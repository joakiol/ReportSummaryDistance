Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 32?41,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsDeveloping and testinga self-assessment and tutoring system?istein E. AnderseniLexIRStreets, 62 Hills RoadCambridge, CB2 1LAand@ilexir.co.ukHelen YannakoudakisCambridge English1 Hills RoadCambridge, CB1 2EUyannakoudakis.h@cambridgeenglish.orgFiona BarkerCambridge English1 Hills RoadCambridge, CB1 2EUbarker.fTim ParishiLexIRStreets, 62 Hills RoadCambridge, CB2 1LAtim@ilexir.co.ukAbstractAutomated feedback on writing may be a use-ful complement to teacher comments in theprocess of learning a foreign language.
Thispaper presents a self-assessment and tutoringsystem which combines an holistic score withdetection and correction of frequent errors andfurthermore provides a qualitative assessmentof each individual sentence, thus making thelanguage learner aware of potentially prob-lematic areas rather than providing a panacea.The system has been tested by learners ina range of educational institutions, and theirfeedback has guided its development.1 IntroductionLearning to write a foreign language well requiresa considerable amount of practice and appropriatefeedback.
Good teachers are essential, but their timeis limited.
As recently shown in a study by Wang etal.
(in press) conducted amongst first-year studentsof English at a Taiwanese university, automatedwriting evaluation can lead to increased learner au-tonomy and higher writing accuracy.
In this pa-per, we investigate the merits of a self-assessmentand tutoring (SAT) system specifically aimed at in-termediate learners of English, at around B2 levelin the Common European Framework of Referencefor Languages (CEFR) (Council of Europe, 2001).There are a large number of students at this level,and they should have sufficient knowledge of thelanguage to benefit from the system whilst at thesame time committing errors which can be identifiedreliably.The system provides automated feedback onlearners?
writing at three different levels of gran-ularity: an overall assessment of their proficiency,a score for each individual sentence, highlightingwell-written passages as well as ones requiring morework, and specific comments on local issues includ-ing spelling and word choice.Computer-based writing tools have been aroundfor a long time, with Criterion (Burstein et al 2003,which also provides a number of features for teach-ers) and ESL Assistant (Gamon et al 2009, notcurrently available) aimed specifically at second-language learners, but the idea of indicating the rel-ative quality of different parts of a text (sentences inour case) has, to the best of our knowledge, not beenimplemented previously.
This kind of non-specificfeedback does not provide a precise diagnosis or im-mediate cure, but might have the advantage of fos-tering learning.In addition to describing the SAT system itself, wepresent a series of three trials in which learners ofEnglish in a number of educational contexts used thesystem as a tool to work on written responses to spe-cific tasks and improve their writing skills.2 SystemThe SAT system is made available to students learn-ing English as a Web service to which they cansign up with a code (?class key?)
provided by theirteacher.
Once they have filled in a short demo-graphic questionnaire, the users can respond to one,two, three or more writing tasks.
The students cansave their work at any time and ask the system toassess the current version of their text, which will32Figure 1: SAT system screen where students can see the automated feedback and revise their piece of writing.
The?score feedback?
and ?error feedback?
views are shown in Figures 2 and 3.give feedback as shown in Figure 1 and describedin more detail in the following subsections.
Assess-ment times are currently around 15sec, which facil-itates incremental and exploratory editing of a textto improve it, giving the students the ability to tryout different ways of correcting a problematic turnof phrase.
The teacher can see which students havesigned up and look at the last saved version of theirresponses.
Finally, the students are asked to answera few questions about their experience with the sys-tem.2.1 Text assessmentThe SAT system provides an overall assessment ofsomeone?s proficiency by automatically analysingand scoring the text as a whole.
There is a largebody of literature with regard to automated text scor-ing systems (Page, 1968; Rudner and Liang, 2002;Attali and Burstein, 2006; Briscoe et al 2010).
Ex-isting systems, overviews of which have been pub-lished in various studies (Dikli, 2006; Williamson,2009; Shermis and Hamner, 2012), involve a largerange of techniques, such as discriminative and gen-erative machine learning, clustering algorithms andvectorial semantics, as well as syntactic parsers.We approach automated text assessment as a su-pervised machine learning problem, which enablesus to take advantage of existing annotated data.
Weuse the publically-available First Certificate in En-glish (FCE) dataset of upper-intermediate learner En-glish (Yannakoudakis et al 2011) and focus on as-sessing general linguistic competence.
Systems thatmeasure English competence directly are easier andfaster to deploy, since they are more likely to be re-usable and generalise better across different genresthan topic-specific ones, which are not immediately33usable when new tasks are added, since the modelcannot be applied until a substantial amount of man-ually annotated responses have been collected for aspecific prompt.Following previous research, we employ discrim-inative ranking, which has been shown to achievestate-of-the-art results on the task of assessingfree-text writing competence (Yannakoudakis et al2011).
The underlying idea is that high-scoring texts(or ?scripts?)
should receive a higher rank than low-scoring ones.
We train a linear ranking perceptron(Bo?s and Opper, 1998) on features derived from pre-vious work (namely, lexical and grammatical prop-erties of text) and compare it to our previous model(Yannakoudakis et al 2011), which is trained usingranking Support Vector Machines (Joachims, 2002).Our new perceptron model achieves 0.740 and 0.765Pearson product-moment (r) and Spearman?s rankcorrelation coefficient (?)
respectively between thegold and predicted scores; this is comparable toour previous SVM model, which achieves 0.741 and0.773, and the differences are not significant.In order to provide scoring feedback1 based onthe predictions of our model, we use visual presen-tations.
Visualisation techniques allow us to go be-yond the mere display of a number, can stimulate thelearners?
visual perceptions, and, when used appro-priately, information can be displayed in an intuitiveand easily interpretable way.
Furthermore, aesthet-ics in computer-based interfaces have been shown tohave an effect on the users.
For example, Ben-Bassatet al(2006) have found an interdependence betweenperceived aesthetics and usability in questionnaire-based assessments, and have shown that users?
pref-erences are not necessarily based only upon perfor-mance; aesthetics also play a role.More specifically, we assign an overall score ona scale from red for a text that looks like it may beat intermediate level or below to green for a text thatshows some evidence of being at upper-intermediatelevel (the level assessed by the FCE exam) or above(i.e., advanced).
This is illustrated in Figure 1 belowthe Overall score section, where an arrow is used toindicate the level of text quality on a colour gradientdefined by the two extreme points, red and green.1Note that ranks can be transformed to scores through linearregression, while correlation remains unaltered as it is invariantto linear transformations.A text with the highest score possible would indi-cate that the learner has potentially shown evidenceof being at a level higher than that assessed by FCE,the latter, of course, being dependent on the extentto which higher-order linguistic skills are elicited bythe prompts.
On the contrary, a very low score in-dicates poor linguistic abilities corresponding to alower level.Although exams that encompass the full range oflanguage proficiency exhibited at different stages oflearning are hard to design, the FCE exam, bench-marked at the B2 level and reserving some of itsscore range for performances beneath and beyond,allows us to roughly estimate someone?s proficiencyas being far below, just below, around or above anupper intermediate level.
The task of predicting at-tainment levels has recently started to receive atten-tion (Dickinson et al 2012; Hawkins and Filipovic?,2012).2.2 Sentence evaluationThe second component of the SAT system automat-ically assesses and scores the quality of individualsentences, independently of their context.
The chal-lenge of assessing intra-sentential quality lies in thelimited linguistic evidence that can be extracted au-tomatically from relatively short sentences for themto be assessed reliably, in addition to the difficultyin acquiring annotated data, since rating a responsesentence by sentence is not something examinerstypically do and would therefore require an addi-tional and expensive manual annotation effort.Previous work has primarily focused on automaticcontent scoring of short answers, ranging from a fewwords to a few sentences (Pulman and Sukkarieh,2005; Attali et al 2008; Mohler et al 2011; Ziaiet al 2012).
On the other hand, scoring of individ-ual sentences with respect to their linguistic quality,specifically in learner texts, has received consider-ably less attention.
Higgins et al(2004) devisedguidelines for the manual annotation of sentences inlearner texts, and evaluated a rule-based approachthat classifies sentences with respect to clarity of ex-pression based on grammar, mechanics and word us-age errors; however, their system performs binaryclassification, whereas we are focusing on scoringsentences.
Writing instruction tools, such as Crite-rion (Burstein et al 2003), give advice on stylistic34and organisational issues and automatically detect avariety of errors in the text, though they do not ex-plicitly allow for an overall evaluation of sentenceswith respect to various writing aspects.
The latter,used in combination with an error feedback compo-nent (see Section 2.3), can be a useful instrumentinforming learners about the severity of their mis-takes; for example, although sentences may containsome errors, they may still maintain a certain levelof acceptability that does not impede communica-tion.
Moreover, indicating problematic regions maybe better from a pedagogic point of view than detect-ing and correcting all errors identified in the text.To date, there is no publically available annotateddataset consisting of sentences marked with a scorerepresenting their linguistic quality.
Manual annota-tion is typically expensive and time-consuming, anda certain amount of annotator training is generallyrequired.
Instead, we exploit already available an-notated data ?
scores and error annotation in the FCEdataset ?
and evaluate various approaches, two ofwhich are: a) to use the script-level model (see Sec-tion 2.1) to predict sentence quality scores, and b) touse the script-level score divided by the total num-ber of (manually annotated) errors in a sentence aspseudo-gold labels to train a sentence-level model.As the models above are expected to contain a cer-tain amount of noise, it is imperative that we iden-tify evaluation measures that are indicative of ourapplication ?
that is, assign higher scores to high-quality sentences compared to low-quality ones ?and not only depend on the labels they have beentrained on.
More specifically, we use correlationwith pseudo-gold scores (rg and ?g; not applicableto the script-level model), correlation with the script-level scores by first averaging predicted sentence-level scores (rs and ?s), correlation with error counts(re and ?e), average precision (AP) and pairwise ac-curacy.
AP is a measure used in information retrievalto evaluate systems that return a ranked list of doc-uments.
Herein, sentences are ranked by their pre-dicted scores, precision is calculated at each correctsentence (that is, containing no errors), and aver-aged over all correct sentences (in other words, wetreat sentences with no errors as the ?relevant doc-uments?).
Pairwise accuracy is calculated based onthe number of times the corrected sentence (avail-able through the error annotation in the FCE dataset)is ranked higher than the original one written by thecandidate, ignoring sentences without errors.
Corre-lation with error counts, average precision and pair-wise accuracy are particularly important as they re-flect more directly the extent to which good and badsentences are discriminated.
Again, in both cases,we employ a linear ranking perceptron.We conducted a series of experiments on a sep-arate development set to evaluate the performanceof features beyond the ones used in the script-levelmodel.
The final results, reported in Table 1, arecalculated on the FCE test set (Yannakoudakis et al2011).Our best configuration is model b, which achievesthe highest results according to most evaluationmeasures with a feature space consisting of 1) er-ror counts identified through the absence of wordtrigrams in a large background corpus, 2) phrase-structure rules, 3) presence of frequent errors, aswell as the number of words defining an error, asdescribed in Section 2.3, 4) the presence of mainverbs, nouns, adjectives, subordinating conjuctionsand adverbs, 5) affixes and 6) the presence of clausalsubjects and modifiers.
The texts were parsed usingRASP (Briscoe et al 2006).Model a, the script-level model, does not work aswell at the sentence level.
However, it does performbetter when evaluated against script-level scores (rsand ?s), and this is expected given that it is traineddirectly on gold script-level scores.
On the otherhand, this evaluation measure is not as indicative ofgood performance in our application as the others,as it does not take into account the varying qualityof individual sentences within a script.Training the script-level model with different fea-ture sets (including those utilised in the sentence-level model) did not yield an improvement in per-formance (the results are omitted due to space re-strictions).
Additional experiments were conductedto investigate the effect of training the sentence-levelmodel with different pseudo-gold labels (e.g., addi-tive/subtractive pseudo-gold scores rather than divi-sive/multiplicative), but the results are not reportedhere as the difference in performance was not sub-stantial.Table 1 shows that better performance can beachieved with our pseudo-gold labels, used to traina model at the sentence level, rather than gold la-35Model a Model brg ?
0.550?g ?
0.646rs 0.572 0.385?s 0.578 0.301re ?0.111 ?0.750?e ?0.078 ?0.702AP 0.393 0.747PairwiseCorrect 0.608 0.703Incorrect 0.359 0.204Table 1: Results on the FCE test set for the script-levelmodel (a) and our model (b).bels at the script level.
To evaluate this further,we trained a sentence-level model using the script-level scores as labels (that is, sentences within thesame script are all assigned the same label/score).However, this did not improve performance (again,the results are omitted due to space restrictions).We also point out that the best-performing featurespace (described above) is based on text propertiesthat are more likely to be present in relatively shortsentences (e.g., the presence of main verbs), com-pared to those used for script-level models in previ-ous work (Yannakoudakis et al 2011), such as wordand part-of-speech bigrams and trigrams, which maybe too sparse for a sentence-level model.Analogously to what we did to present the over-all score, we developed a sentence score feedbackview to indicate the general quality of the sentences,as given by our best model, by highlighting each ofthem with a background colour ranging from greenfor a well-written sentence, via yellow and orangefor a sentence which the system thinks is accept-able, to dark orange and red for a sentence whichmay have a few problems.
Figure 2 shows how theSAT system evaluates and colour-codes a few au-thentic student-written sentences containing errors,as well as their corrected counterparts based on theerror-coding in the FCE test set.
Overall, the systemcorrectly identifies correct and incorrect versions ofeach sentence, attributing a higher score (greenercolour) to the corrected sentence in each pair.2.3 Word-level feedbackBasic spelling checkers have been around since the1970s and grammar checkers since the 1980s (Ku-kich, 1992), but misleading ?corrections?
may be be-wildering (Galletta et al 2005), and the systems donot always focus on the kinds of error frequentlycommitted, even less so in the case of learners aswas pointed out early on by Liou (1992), who testedcommercial grammar checkers on and developed asystem for detecting common errors in Taiwaneselearners?
writing.For word-level feedback within the SAT system,we have implemented a method similar to one wehave used earlier in the context of pre-annotation oflearner corpora (Andersen, 2011).
To ensure highprecision and good coverage of local errors typi-cally committed by learners, error rules are gen-erated from the Cambridge Learner Corpus (CLC)(Nicholls, 2003) to detect word unigrams, bigramsand trigrams which have been annotated as incorrectat least five times and at least ninety per cent of thetimes they occur.
This way, rules can be extractedfrom the existing error annotation in the corpus,obviating the need for manually constructed mal-rules, although the rules obtained by the two differ-ent methods may to some extent be complementary.In addition to corpus-derived rules, many classes ofincorrect but plausible derivational and inflectionalmorphology are detected by means of rules derivedfrom a machine-readable dictionary.
Many mistakesare still not detected, but precision has been found tobe more important in terms of learning effect (Na-gata and Nakatani, 2010), and errors missed by thismodule will often give lower sentence scores.Figure 3 illustrates some types of error detectedby the system.
The feedback text is generated froma small number of templates corresponding to differ-ent categories of error marked up in the CLC.We are currently working on extending this partof the system with more general rules in addition toword n-grams, e.g., part-of-speech tags and gram-matical relations, in order to detect more errors with-out loss in precision.3 TrialsAfter the SAT system had been developed, a seriesof trials were set up in order to test the online sys-36Figure 2: Examples of correct sentences (top) and incorrect ones (bottom) colour-coded by the SAT system.Figure 3: The error feedback view identifies specific words that may have been used incorrectly.
Explanations andsuggested corrections are provided in a separate column.
The system actually proposes two different corrections forand etc., namely etc.
and and so on; the user will have to choose one or the other.
The confusion between the verb seeand the noun sea is identified, but the the is not actually unnecessary; in this case, the system has been led astray bythe surrounding errors.tem and to collect feedback from language learnersand their teachers in a variety of contexts.
Three tri-als were undertaken in November 2012, December2012 and in March 2013, with changes made to thesystem between each pair of trials.English Profile Network member institutionswere contacted who had access to language learnersand who had previously participated in data collec-tion for the English Profile Programme2.
Teachers atuniversities, secondary schools and private languageschools signed up for two or more trials so that theirlearners could use and provide feedback on severaliterations of the SAT system.
Certificates of partici-2See www.englishprofile.orgpation were offered to encourage involvement in thetrials.Ten institutions were involved from nine coun-tries, namely Belgium, the Czech Republic, France,Lithuania, Poland, Romania, Russia, Slovakia andSpain.
Eight universities, one secondary school andone private language school were represented, in-cluding specialist and generalist institutions of ed-ucational sciences, agricultural science, veterinarymedicine and foreign languages.
Each trial had be-tween 4 and 8 institutions taking part, and each in-stitution participated in two or three trials with manystudents undertaking more than one trial.All students who took part in the trials, over 45037in total, were expected to be at or above the upper-intermediate (CEFR B2) level as this was the level atwhich the SAT system was designed to function.Three initial sets of tasks were developed for theplanned system trials, each set consisting of threeshort written prompts which asked the users to writeon a specified topic for a particular purpose, for ex-ample:Daily lifeYour English class is going to make ashort video about daily life in your town.Write a report for your teacher, suggest-ing which activities should be filmed, andwhy.Tasks were based on retired questions from an in-ternational proficiency test at B2 level of the CEFR.Each task was given a short name which was shownin the SAT system in order for the users to select themost interesting or relevant task for themselves.A short set of instructions was produced for bothteachers and students which was emailed to the maincontact in each institution and passed on to their col-leagues, teachers and students who were interestedin taking part in the trial.The trials operated as follows:?
The main institutional contact receives an invi-tation to participate in the trials.?
Interested institutions receive instructions andconfirm the number of class keys required(sign-up codes for the system).?
Main contact and teachers at each institutionlog in and work through the system as if theyare a language learner, by completing a demo-graphic questionnaire, writing 1?3 tasks whichare assessed by the system, and finally complet-ing a short user satisfaction questionnaire.?
Students work through the SAT system eitherwith the support of their teacher in class or re-motely.3.1 SAT system usageDuring Trial 1, on the busiest day there were 155submissions and the highest number of users ona single day was 32.
These figures indicate thatRevisions Count1 2922 2723 1424 785 506 287 158 259 1110 1411?15 2116?20 620?
5Table 2: Number of revisions per task response.all users were submitting their work for assessmentmore than once, which suggests that the system isbeing used in an iterative fashion as envisaged.
Dur-ing Trial 2, the busiest day saw more than twice asmany submissions as during the first trial (442), andthe most people online on any one day almost dou-bled to 62.
Across both trials we collected around3000 submissions in total, including revisions; theaverage number of revisions for a submitted pieceof writing is 3.2 with the highest figure being 54revisions (see Table 2 for details).
This suggeststhat some users write their first response, then makechanges to one word or phrase at a time, resulting insuch a large number of revisions.
When more thanone revision has been submitted, the score given bythe system to the last revision is higher than thatgiven to the initial revision in over 80% of the cases.Current changes to the system allowing system ad-ministrators to check on intermediate versions ofsubmitted texts are underway.3.2 FeedbackIn addition to looking at the writing submitted byusers of the system, there was both numerical andwritten feedback available to the system developers.This was used to suggest changes to the system atsubsequent trials.As can be seen from Table 3, user satisfactionscores were generally high and increased from Trial1 to Trial 2.
In the first pilot, the written feed-back from instructors was generally positive whilst38Trial 1 Trial 2Using the SAT system helps me to write better in English.
3.80 3.92I find the SAT system useful for understanding my mistakes.
3.74 3.96I think the sentence colouring is useful.
3.74 4.15I think the word-level information [error feedback] is useful.
3.86 4.12The SAT system is easy to use.
4.45 4.49The feedback on my writing is clear.
3.80 3.93If you have used the SAT system before, has it improved since the last time?
3.86Table 3: Average feedback scores on a scale from 1 (strongly disagree) to 5 (strongly agree).the learner feedback was mixed, especially when itcomes to sentence evaluation:In summary, I liked this system, becausethe sentence colouring suggests me tothink about my writing style, mistakes,what I should improve, change.
This sys-tem is not like a teacher, who checks allour errors, but makes us develop our crit-ical thinking, which is the most importantfor writing especially.
[...]It?s okay the way of colouring system, theproblem is that it doesn?t tell you specifi-cally what?s wrong with constructions soyou have think what you failed.The fact that the system provides almost immediatefeedback has been appreciated:I like that the paragraphs which I wroteassesed so quickly.
.
.
.
Secondly, I reallylike that student can correct his text till itgets ideal.Users have also made suggestions for improve-ments, which have been essential for deciding whichparts of the system should be developed further.3.3 System changesAs a result of feedback and the team?s extensive useof the system, after each trial changes were madeboth to the on-screen experience and behind thescenes.
After Trial 1, the system was amended toenable users to see paragraph breaks in the correctedversion (which before had not been shown in the as-sessed view of the text).
There was also a new errorview with permanently visible explanations and ex-amples and an additional question on the feedbackquestionnaire which asked whether users felt theWords Count0?
99 540100?199 1,294200?299 928300?399 201400?499 67500?999 261,000?
36Table 4: Number of words per submission.system had improved since the previous time theyused it.
Behind the scenes, the server was upgradedto cope with anticipated demand and code was writ-ten so that administrators could review statistics onusage.At the time of writing the third SAT system trialwas underway.
In the first two trials the total numberof words collected was over 600,000 with an averageresponse length of around 1100 characters or 200words.
Encouragingly, there were many longer re-sponses including twelve over 1080 words in lengthand the longest written to date is 1773 words.
Thesefigures indicate that the system is not restrictive, butencourages and inspires students to write.
Table 4gives an overview of the script length distribution.Following two successful trials, the third trialaimed to involve new and existing users and to pro-vide more detailed teacher feedback.4 ConclusionsIn this paper, we described a tool that provides feed-back to learners of English at three different levelsof granularity: an overall assessment of their profi-ciency, assessment of individual sentences, and di-agnostic feedback on local issues including spellingand word choice.
We argued that the use of visual-39isation techniques is important, as they allow us togo beyond the mere display of a number, can stimu-late the learners?
visual perceptions, and can displayinformation in an intuitive and easily interpretableway.
The usefulness and usability of the tool as awhole, as well as of its components, was confirmedthrough questionnaire-based evaluations, where, forexample, the perceived usefulness of the sentencecolouring received an average of 4.15 on a 5-pointscale.The first component of the SAT system, script-level assessment, uses a machine learner to predicta score for a text and roughly estimate someone?sproficiency level based on lexical and grammaticalfeatures.
The second component allows for an auto-matic evaluation of the linguistic quality of individ-ual sentences.
We proposed a method for generat-ing sentence-level scores, which we use for trainingour model.
Using this method, we were able to learnwhat features can be used to evaluate linguistic qual-ity of (relatively short) sentences.
Indicating prob-lematic regions via highlighting of sentences may bebetter from a pedagogic point of view than detectingand correcting all errors identified in the text.
Thethird component automatically provides diagnosticfeedback on local errors with high precision on thebasis of a few templates, without relying on manu-ally crafted rules.The trials undertaken so far have improved thefunctionality of the system in regard to what is onoffer to teachers and their students, but they havealso provided the basis for further research and de-velopment to enhance the system?s functionality anddesign and move towards wider deployment.
Weplan to continue improving the methodologies usedfor providing feedback to learners, as well as addingfurther functionality, such as L1-specific feedback.Another logical next step would be to continue to-wards lower levels of granularity, moving from thesentence as the unit of assessment to clauses andphrases, which may be particularly beneficial formore advanced language users who write longer andmore complex sentences.AcknowledgementsSpecial thanks to Ted Briscoe and Marek Rei, aswell as to the anonymous reviewers, for their valu-able contributions at various stages.References?istein E. Andersen.
2011.
Semi-automatic ESOL errorannotation.
English Profile Journal, 2.Yigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-Rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3):1?30.Yigal Attali, Don Powers, Marshall Freedman, MarissaHarrison, and Susan Obetz.
2008.
Automated Scoringof short-answer open-ended GRE subject test items.Technical Report 04, ETS.Tamar Ben-Bassat, Joachim Meyer, and Noam Tractin-sky.
2006.
Economic and subjective measuresof the perceived value of aesthetics and usability.ACM Transactions on Computer-Human Interaction,13(2):210?234.Siegfried Bo?s and Manfred Opper.
1998.
Dynamics ofbatch training in a perceptron.
Journal of Physics A:Mathematical and General, 31(21):4835?4850.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In ACL-Coling?06 Interactive Presentation Session, pages 77?80.Ted Briscoe, Ben Medlock, and ?istein E. Andersen.2010.
Automated assessment of ESOL free text exam-inations.
Technical Report UCAM-CL-TR-790, Uni-versity of Cambridge, Computer Laboratory.Jill Burstein, Martin Chodorow, and Claudia Leacock.2003.
Criterion: Online essay evaluation: An appli-cation for automated evaluation of student essays.
InProceedings of the fifteenth annual conference on in-novative applications of artificial intelligence, pages3?10.Council of Europe.
2001.
Common European Frame-work of Reference for Languages: Learning, Teaching,Assessment.
Cambridge University Press.Markus Dickinson, Sandra Ku?bler, and Anthony Meyer.2012.
Predicting learner levels for online exercises ofHebrew.
In Proceedings of the Seventh Workshop onInnovative Use of NLP for Building Educational Ap-plications, pages 95?104.
Association for Computa-tional Linguistics.Semire Dikli.
2006.
An overview of automated scoringof essays.
Journal of Technology, Learning, and As-sessment, 5(1).Dennis F. Galletta, Alexandra Durcikova, Andrea Ever-ard, and Brian M. Jones.
2005.
Does spell-checkingsoftware need a warning label?
Communications ofthe ACM, 48(7):82?86.Michael Gamon, Claudia Leacock, Chris Brockett,William B Dolan, Jianfeng Gao, Dmitriy Belenko, and40Alexandre Klementiev.
2009.
Using statistical tech-niques and web search to correct ESL errors.
CalicoJournal, 26(3):491?511.John A. Hawkins and Luna Filipovic?.
2012.
CriterialFeatures in L2 English: Specifying the Reference Lev-els of the Common European Framework.
EnglishProfile Studies.
Cambridge University Press.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspects of co-herence in student essays.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the ACMConference on Knowledge Discovery and Data Min-ing, pages 133?142.Karen Kukich.
1992.
Techniques for automaticallycorrecting words in text.
ACM Computing Surveys,24(4):377?439.Hsien-Chin Liou.
1992.
An automatic text-analysisproject for EFL writing revision.
System: The Inter-national Journal of Educational Technology and Lan-guage Learning Systems, 20(4):481?492.Michael A.G. Mohler, Razvan Bunescu, and Rada Mi-halcea.
2011.
Learning to grade short answer ques-tions using semantic similarity measures and depen-dency graph alignments.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies.Ryo Nagata and Kazuhide Nakatani.
2010.
Evaluatingperformance of grammatical error detection to maxi-mize learning effect.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics:Posters, COLING ?10, pages 894?900, Stroudsburg,PA, USA.
Association for Computational Linguistics.Diane Nicholls.
2003.
The Cambridge Learner Cor-pus: Error coding and analysis for lexicography andELT.
In Dawn Archer, Paul Rayson, Andrew Wilson,and Tony McEnery, editors, Proceedings of the Cor-pus Linguistics conference, volume 16 of TechnicalPapers, pages 572?581.
University Centre For Com-puter Corpus Research on Lanugage, Lancaster Uni-versity, Lancaster.Ellis B.
Page.
1968.
The use of the computer in analyz-ing student essays.
International Review of Education,14(2):210?225.Stephen G. Pulman and Jana Z. Sukkarieh.
2005.
Au-tomatic short answer marking.
In Proceedings of thesecond workshop on Building Educational Applica-tions Using natural language processing, pages 9?16.Lawrence M. Rudner and Tahung Liang.
2002.
Auto-mated essay scoring using Bayes?
theorem.
The Jour-nal of Technology, Learning and Assessment, 1(2):3?21.Mark D. Shermis and Ben Hamner.
2012.
Contrastingstate-of-the-art automated scoring of essays: analysis.Technical report, The University of Akron and Kaggle.Ying-Jian Wang, Hui-Fang Shang, and Paul Briody.
Inpress.
Exploring the impact of using automated writ-ing evaluation in English as a foreign language univer-sity students?
writing.
Computer Assisted LanguageLearning.David M. Williamson.
2009.
A framework for imple-menting automated scoring.
In Proceedings of the An-nual Meeting of the American Educational ResearchAssociation and the National Council on Measurementin Education, San Diego, CA.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies.Ramon Ziai, Niels Ott, and Detmar Meurers.
2012.Short answer assessment: Establishing links betweenresearch strands.
In Proceedings of the workshop onBuilding Educational Applications Using natural lan-guage processing, pages 190?200.41
