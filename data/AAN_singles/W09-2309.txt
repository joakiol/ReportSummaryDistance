Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 69?77,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsReordering Model Using Syntactic Information of a Source Tree forStatistical Machine TranslationKei Hashimoto?1, Hirohumi Yamamoto?2?3, Hideo Okuma?2?4,Eiichiro Sumita?2?4, and Keiichi Tokuda?1?2?1Nagoya Institute of Technology Department of Computer Science and Engineering/ Gokiso-cho Syouwa-ku Nagoya-city Aichi Japan?2National Institute of Information and Communications Technology?3Kinki University School of Science and Engineering Department of Informaiton?4ATR Spoken Language Communication Research Labs.AbstractThis paper presents a reordering model us-ing syntactic information of a source tree forphrase-based statistical machine translation.The proposed model is an extension of IST-ITG (imposing source tree on inversion trans-duction grammar) constraints.
In the pro-posed method, the target-side word order isobtained by rotating nodes of the source-sideparse-tree.
We modeled the node rotation,monotone or swap, using word alignmentsbased on a training parallel corpus and source-side parse-trees.
The model efficiently sup-presses erroneous target word orderings, espe-cially global orderings.
Furthermore, the pro-posed method conducts a probabilistic evalu-ation of target word reorderings.
In English-to-Japanese and English-to-Chinese transla-tion experiments, the proposed method re-sulted in a 0.49-point improvement (29.31 to29.80) and a 0.33-point improvement (18.60to 18.93) in word BLEU-4 compared withIST-ITG constraints, respectively.
This indi-cates the validity of the proposed reorderingmodel.1 IntroductionStatistical machine translation has been wiedely ap-plied in many state-of-the-art translation systems.
Apopular statistical machine translation paradigms isthe phrase-based model (Koehn et al, 2003; Ochand Ney, 2004).
In phrase-based statistical ma-chine translation, errors in word reordering, espe-cially global reordering, are one of the most se-rious problems.
To resolve this problem, manyword-reordering constraint techniques have beenproposed.
These techniques are categorized intotwo types.
The first type is linguistically syntax-based.
In this approach, tree structures for the source(Quirk et al, 2005; Huang et al, 2006), target (Ya-mada and Knight, 2000; Marcu et al, 2006), or both(Melamed, 2004) are used for model training.
Thesecond type is formal constraints on word permuta-tions.
IBM constraints (Berger et al, 1996), the lex-ical word reordering model (Tillmann, 2004), andinversion transduction grammar (ITG) constraints(Wu, 1995; Wu, 1997) belong to this type of ap-proach.
For ITG constraints, the target-side wordorder is obtained by rotating nodes of the source-side binary tree.
In these node rotations, the sourcebinary tree instance is not considered.
Imposinga source tree on ITG (IST-ITG) constraints (Ya-mamoto et al, 2008) is an extension of ITG con-straints and a hybrid of the first and second type ofapproach.
IST-ITG constraints directly introduce asource sentence tree structure.
Therefore, IST-ITGcan obtain stronger constraints for word reorderingthan the original ITG constraints.
For example, IST-ITG constraints allows only eight word orderings fora four-word sentence, even though twenty-two wordorderings are possible with respect to the originalITG constraints.
Although IST-ITG constraints ef-ficiently suppress erroneous target word orderings,the method cannot assign the probability to the tar-get word orderings.This paper presents a reordering model using syn-tactic information of a source tree for phrase-basedstatistical machine translation.
The proposed re-ordering model is an extension of IST-ITG con-69straints.
In the proposed method, the target-sideword order is obtained by rotating nodes of a source-side parse-tree in a similar fashion to IST-ITG con-straints.
We modeled the rotating positions, mono-tone or swap, from word alignments of a trainingparallel corpus and source-side parse-trees.
The pro-posed method conducts a probabilistic evaluation oftarget word orderings using syntactic information ofthe source tree.The rest of this paper is organized as follows.Section 2 describes the previous approach to re-solving erroneous word reordering.
In Section 3,the reordering model using syntactic information ofa source tree is presented.
Section 4 shows ex-perimental results.
Finally, Section 5 presnts thesummary and some concluding remarks and futureworks.2 Previous WorksFirst, we introduce two previous studies on relatedword reordering constraints, ITG and IST-ITG con-straints.2.1 ITG ConstraintsIn one-to-one word-alignment, the source word fiis translated into the target word ei.
The sourcesentence [f1, f2, ?
?
?
, fN ] is translated into the tar-get sentence which is the reordered target word se-quence [e1, e2, ?
?
?
, eN ].
The number of reorderingsis N !.
When ITG constraints are introduced, thiscombination N !
can be reduced in accordance withthe following constraints.?
All possible binary tree structures are generatedfrom the source word sequence.?
The target sentence is obtained by rotating anynode of the binary trees.When N = 4, the ITG constraints can reducethe number of combinations from 4!
= 24 to22 by rejecting the combinations [e3, e1, e4, e2]and [e2, e4, e1, e3].
For a four-word sentence, thesearch space is reduced to 92% (22/24), but fora 10-word sentence, the search space is only 6%(206,098/3,628,800) of the original full space.2.2 IST-ITG ConstraintsIn ITG constraints, the source-side binary tree in-stance is not considered.
Therefore, if a source sen-tence tree structure is utilized, stronger constraintsthan the original ITG constraints can be created.IST-ITG constraints directly introduce a source sen-tence tree structure.
The target sentence is obtainedwith the following constraints.?
A source sentence tree structure is generatedfrom the source sentence.?
The target sentence is obtained by rotating anynode of the source sentence tree structure.By parsing the source sentence, the parse-tree isobtained.
After parsing the source sentence, abracketed sentence is obtained by removing thenode syntactic labels; this bracketed sentence canthen be converted into a tree structure.
For example,the parse-tree ?
(S1 (S (NP (DT This)) (VP (AUXis) (NP (DT a) (NN pen)))))?
is obtained from thesource sentence ?This is a pen,?
which consists offour words.
By removing the node syntactic labels,the bracketed sentence ?
((This) ((is) ((a) (pen))))?is obtained.
Such a bracketed sentence can be usedto produce constraints.
If IST-ITG constraints isapplied, the number of word orderings in N = 4is reduced to 8, down from 22 with ITG cn-straints.
For example, for the source-side bracketedtree ?
((f1f2) (f3f4)),?
the eight target sequences[e1, e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3],[e2, e1, e4, e3], [e3, e4, e1, e2], [e3, e4, e2, e1],[e4, e3, e1, e2], and [e4, e3, e2, e1] are accepted.
Forthe source-side bracketed tree ?
(((f1f2) f3) f4),?the eight sequences [e1, e2, e3, e4], [e2, e1, e3, e4],[e3, e1, e2, e4], [e3, e2, e1, e4], [e4, e1, e2, e3],[e4, e2, e1, e3], [e4, e3, e1, e2], and [e4, e3, e2, e1] areaccepted.
When the source sentence tree structureis a binary tree, the number of word orderings isreduced to 2N?1.
The parsing results sometimes donot produce binary trees.
In this case, some subtreeshave more than two child nodes.
For a non-binarysubtree, any reordering of child nodes is allowed.
Ifa subtree has three child nodes, six reorderings ofthe nodes are accepted.In phrase-based statistical machine translation, asource ?phrase?
is translated into a target ?phrase?.However, with IST-ITG constraints, ?word?
must be70used for the constraint unit since the parse unit is a?word?.
To absorb different units between transla-tion models and IST-ITG constraints, a new limita-tion for word reordering is applied.?
Word ordering that destroys a phrase is not al-lowed.When this limitation is applied, the translated wordordering is obtained from the bracketed source sen-tence tree by reordering the nodes in the tree, whichis the same as for one-to-one word-alignment.3 Reordering Model Using SyntacticInformation of the Source TreeIn this section, we present a new reordering modelusing syntactic information of a source-side parse-tree.3.1 Abstract of Proposed MethodThe IST-ITG constraints method efficiently sup-presses erroneous target word orderings.
However,IST-ITG constraints cannot evaluate the accuracy ofthe target word orderings; i.e., IST-ITG constraintsassign an equal probability to all target word order-ings.
This paper proposes a reordering model us-ing syntactic information of the source tree as anextension of IST-ITG constraints.
The proposed re-ordering model conducts a probabilistic evaluationof target word orderings using syntactic informationof the source-side parse-tree.In the proposed method, the target-side word or-der is obtained by rotating nodes of the source-side parse-tree in a similar fashion to IST-ITG con-straints.
Reordering probabilities are assigned toeach subtree of source-side parse-tree S by reorder-ing the positions into two types: monotone andswap.
If the subtree has more than two child nodes,the number of child node order is more than two.However, we assume the child node order other thanmonotone to be swap.
The source-side parse-treeS consists of subtrees {s1, s2, ?
?
?
, sK}, where Kis the number of subtrees included in the source-side parse-tree.
The subtree sk is which is repre-sented by the parent node?s syntactic label and theorder, from sentence head to sentence tail, of thechild node?s syntactic labels.
For example, Fig-ure 1 shows a source-side parse-tree for a four-wordSource-side parse-treeSource sentenceSNP VPNPAUXDT NNFigure 1: Example of a source-side parse-tree fo a four-word source sentence consisting of three subtrees.source sentence consisting of three subtrees.
In Fig-ure 1, the subtrees s1, s2, and s3 are represented byS+NP+VP, VP+AUX+NP, and NP+DT+NN, re-spectively.
Each subtree has a probability P (t | sk),where t is monotone (m) or swap (s).
The proba-bility of the target word reordering is calculated asfollows.Pr =K?k=1P (t | sk) (1)Each target candidate is assigned the different re-ordering probability by Equation (1).
Since the pro-posed reordering model uses the syntactic labels,which is not considered in IST-ITG constraints, thedifferent parse-tree assigns the different reorderingprobability.
The proposed model is effective forglobal word reordering, because reordering proba-bilities are also assigned to higher-level subtrees ofthe source-side parse-tree.3.2 Training of the Proposed ModelWe modeled monotone or swap node rotating auto-matically from word alignments of a training paral-lel corpus and source-side parse-trees.
The trainingalgorithm for the proposed reordering model is asfollows.1.
The training process begins with a word-aligned corpus.
We obtained the word align-ments using Koehn et al?s method (2003),71322,342,3,41Figure 2: Example of a source-side parse-tree with wordalignments using the training algorithm of the proposedmodel.which is based on Och and Ney?s work (2004).This involves running GIZA++ (Och and Ney,2003) on the corpus in both directions, and ap-plying refinement rules (the variant they desig-nate is ?final-and?)
to obtain a single many-to-many word alignment for each sentence.2.
Source-side parse-trees are created using asource language phrase structure parser, whichannotates each node with a syntactic label.
Asource-side parse-tree consists of several sub-trees with syntactic labels.
For example, theparse-tree ?
(S1 (S (NP (DT This)) (VP (AUXis) (NP (DT a) (NN pen)))))?
is obtained fromthe source sentence ?This is a pen?
which con-sists of four words.3.
Word alignments and source-side parse-treesare combined.
Leaf nodes are assigned targetword positions obtained from word alignments.Via the bottom-up process, target word posi-tions are assigned to all nodes.
For example,in Figure 2, the left-side (sentence head) childnode of subtree s2 is assigned the target wordposition ?4,?
and the right-side (sentence tail)child node is assigned the target word positions?2?
and ?3,?
which are assigned to the childnodes of subtree s3.4.
The monotone and swap reordering positionsare checked and counted for each subtree.
BySubtree type Monotone probabilityS+PP+,+NP+VP+.
0.764PP+IN+NP 0.816NP+DT+NN+NN 0.664VP+AUX+VP 0.864VP+VBN+PP 0.837NP+NP+PP 0.805NP+DT+JJ+NN 0.653NP+DT+JJ+VBP+NN 0.412NP+DT+NN+CC+VB 0.357Table 1: Example of proposed reordering models.comparing the target word positions, which areassigned in the above step, the reordering posi-tion is determined.
If the target word positionof the left-side child node is smaller than one ofthe right-side child node, the reordering posi-tion determined as monotone.
For example, inFigure 2, the subtrees s1, s2 and s3 are mono-tone, swap, and monotone, respectively.5.
The reordering probability of the subtree canbe directly estimated by counting the reorder-ing positions in the training data.P (t | s) = ct(s)?t ct(s)(2)where ct(s) is the count of reordering positon tincluded all training samples for the subtree s.The parsing results sometimes do not produce bi-nary trees.
For a non-binary subtree, any reorder-ing of child nodes is allowed.
However, the pro-posed reordering model assumes that reordering po-sitions are only two, monotone and swap.
Thatis, the reordering position which the order of childnodes do not change is monotone, and the other po-sitions are swap.
Therefore, the probability of swapP (s | sk) is derived from the probability of mono-tone P (m | sk) as follows.P (s | sk) = 1.0 ?
P (m | sk) (3)Table 1 shows the example of proposed reorderingmodels.If a subtree is represented by a binary-tree, thereare L3 possible subtrees, where L is the number of72Figure 3: Example of a target word order which is notderived from rotating the nodes of source-side parse trees.syntactic labels.
However, in the possible subtrees,there are subtrees observed only a few times in train-ing sentences, especially when the subtree consistsof more than three child nodes.
Although a largenumber of subtree models can capture variations inthe training samples, too many models lead to theover-fitting problem.
Therefore, subtrees where thenumber of training samples is less than a heuristicthreshold and unseen subtrees are clustered to dealwith the data sparseness problem for robust modelestimations.After creating word alignments of a training par-allel corpus, there are target word orders which arenot derived from rotating nodes of source-side parse-trees.
Figure 3 shows a sample which is not derivedfrom rotating nodes.
Some are due to linguistic rea-sons, structual differences such as negation (French?ne...pas?
and English ?not?
), adverb, modal and soon.
Others are due to non-linguistic reasons, er-rors of automatic word alignments, syntactic anal-ysis, or human translation (Fox, 2002).
The pro-posed method discards such problematic cases.
InFigure 3, the subtree s1 is then removed from train-ing samples, and the subtrees s2 and s3 are used astraining samples.3.3 Decoding Using the Proposed ReorderingModelIn this section, we describe a one-pass phrase-baseddecoding algorithm that uses the proposed reorder-ing model in the decoder.
The translation target sen-tence is sequentially generated from left (sentenceFigure 4: Example of a target candidate including aphrase.head) to right (sentence tail), and all reordering isconducted on the source side.
To introduce the pro-posed reordering model into the decoder, the targetcandidate must be checked for whether the reorder-ing position of a subtree is either monotone or swapwhenever a new phrase is selected to extend a targetcandidate.
The checking algorithm is as follows.1.
For old translation candidates, the subtree s,which includes both translated and untranslatedwords, and its untranslated part u are calcu-lated.2.
When a new target phrase e?
is generated, thesource phrase f?
and the untranslated part u cal-culated in the above step are compared.
If thesource phrase f?
does not include the untrans-lated part u and is not included u, the new can-didate is rejected.3.
In the accepted candidate, the reordering po-sitions for all subtrees included the source sideparse-tree are checked by comparing the sourcephrase f?
with the source phrase sequence usedbefore.Subtrees checked reordering positions are assigned aprobability?monotone or swap?by the proposed re-ordering model, and the target word order is evalu-ated by Equation (1).Phrase-based statistical machine translation usesa ?phrase?
as the translation unit.
However, the pro-posed reordering model needs a ?word?
order.
Be-cause ?word?
alignments form the source phrase totarget phrase are not clear, we cannot determine the73Figure 5: Example of a non-binary subtree including aphrase.reordering position of subtree included in a phrase.Therefore, in the decoding process using the pro-posed reordering model, we define that higher prob-ability, monotone or swap, are assigned to subtreesincluded in a source phrase.
For example, in Fig-ure 4, the source sentence [[f1, f2], f3, f4] is trans-lated into the target sentence [[e1, e2], e4, e3], where[f1, f2] and [e1, e2] are used as phrases.
Then, thesource phrase [f1, f2] includes the subtree s2.
If themonotone probabilities of subtrees s1, s2, and s3 are0.8, 0.4 and 0.7, the proposed reordering probabil-ity is 0.8 ?
0.6 ?
0.3 = 0.144.
If a source phraseis [f1, f2, f3, f4] and a source-side parse-tree has thesame tree structure used in Figure 4, the subtrees s1,s2, and s3 are assigned higher reordering probabili-ties.
If the source phrase [f1, f2, f3, f4] used in Fig-ure 4, the subtrees s1, s2, and s3 are assigned higherreordering probabilities.Non-binary subtrees are often observed in thesource-side parse-tree.
When a source phrase f?
isincluded in a non-binary subtree and does not in-clude a non-binary subtree, we cannot determine thereordering position.
For example, the reordering po-sition of subtree s2 in Figure 5, which includes thephrase [f3, f4], can not be determined.
In this case,we define that such subtrees are also to be assigneda higher probability.4 ExperimentsTo evaluate the proposed model, we conducted twoexperiments: English-to-Japanese and English-to-Chinese translation.English JapaneseTrain Sentences 1.0MWords 24.6M 24.6MDev Sentences 2.0KWords 50.1K 58.7KTest Sentences 2.0KWords 49.5K 58.0KTable 2: Statistics of training, development and test cor-pus for E-J translation.4.1 English-to-Japanese Paper AbstractTranslation ExperimentsThe first experiment was the English-to-Japanese(E-J) translation.
Table 2 shows the training, de-velopment and test corpus statistics.
JST Japanese-English paper abstract corpus consists of 1.0Mparallel sentences were used for model training.This corpus was constructed from 2.0M Japanese-English paper abstract corpus belongs to JST byNICT using the method of Uchiyama and Isahara(2007).
For phrase-based translation model training,we used the GIZA++ toolkit (Och and Ney, 2003),and 1.0M bilingual sentences.
For language modeltraining, we used the SRI language model toolkit(Stolcke, 2002), and 1.0M sentences for the trans-lation model training.
The language model type wasword 5-gram smoothed by Kneser-Ney discounting(Kneser and Ney, 1995).
To tune the decoder pa-rameters, we conducted minimum error rate training(Och, 2003) with respect to the word BLEU score(Papineni et al, 2002) using 2.0K development sen-tence pairs.
The test set with 2.0K sentences is used.In the evaluation and development sets, a single ref-erence was used.
For the creation of English sen-tence parse trees and segmentation of the English,we used the Charniak parser (Charniak, 2000).
Weused Chasen for segmentation of the Japanese sen-tences.
For decoding, we used an in-house decoderthat is a close relative of the Moses decoder.
Theperformance of this decoder was configured to bethe same as Moses.
Other conditions were the sameas the default conditions of the Moses decoder.In this experiment, the following three methodswere compared.?
Baseline : The IBM constraints and the lexi-cal reordering model were used for target word74Baseline IST-ITG ProposedBLEU 27.87 29.31 29.80Table 3: BLEU score results for E-J translation.
(1-reference)reordering.?
IST-ITG : The IST-ITG constraints, the IBMconstraints, and the lexical reordering modelwere used for target word reordering.?
Proposed : The proposed reordering model,the IBM constraints, and the lexical reorderingmodel were used for target word reordering.During minimum error training, each method usedeach reordering model and reordering constraint.The proposed reordering model are trained from1.0M bilingual sentences for the translation modeltraining.
The amount of available training samplesrepresented by subtrees was 9.8M.
In the availabletraining samples, there were 54K subtree types.
Theheuristic threshold was 10, and subtrees with train-ing samples of less than 10 were clustered.
The pro-posed reordering model consisted of 5,960 subtreestypes and one clustered model ?other?.
The modelsnot including ?other?
covered 99.29% of all trainingsamples.The BLEU scores are presented in Table 3.In comparing ?Baseline?
method with ?IST-ITG?method, the improvement in BLEU was a 1.44-point.
Furthermore, in comparing ?IST-ITG?method with ?Proposed?
method, the improvementin BLEU was a 0.49-point.
Both the IST-ITG con-straints and the proposed reordering model fixed thephrase position for the global reorderings.
How-ever, the proposed method can conduct a probabilis-tic evaluation of target word reorderings which theIST-ITG constraints cannot.
Therefore, ?Proposed?method resulted in a better BLEU.4.2 NIST MT08 English-to-ChineseTranslation ExperimentsNext, we conducted English-to-Chinese (E-C) news-paper translation experiments for different lan-guage pairs.
The NIST MT08 evaluation campaignEnglish-to-Chinese translation track was used forthe training and evaluation corpora.
Table 4 showsEnglish ChineseTrain Sentences 4.6MWords 79.6M 73.4MDev Sentences 1.6KWords 46.4K 39.0KTest Sentences 1.9KWords 45.7K 47.0K (Ave.)Table 4: Statistics of training, development and test cor-pus for E-C translation.Baseline IST-ITG ProposedBLEU 17.54 18.60 18.93Table 5: BLEU score results for E-C translation.
(4-reference)the training, development and test corpus statistics.For the translation model training, we used 4.6Mbilingual sentences.
For the language model train-ing, we used 4.6M sentences which are used forthe translation model training.
The language modeltype was word 3-gram smoothed by Kneser-Neydiscounting.
A development set with 1.6K sen-tences was used as evaluation data in the Chinese-to-English translation track for the NIST MT07 eval-uation campaign.
A single reference was used inthe development set.
The evaluation set with 1.9Ksentences is the same as the MT08 evaluation data,with 4 references.
In this experiment, the comparedmethods were the same as in the E-J experiment.The proposed reordering model are trained from4.6M bilingual sentences for the translation modeltraining.
The amount of available training samplesrepresented by subtrees was 39.6M.
In the availabletraining samples, there were 193K subtree types.As in the E-J experiments, the heuristic thresholdwas 10.
The proposed reordering model consistedof 18,955 subtree types and one clustered model?other.?
The models not including ?other?
covered99.45% of all training samples.The BLEU scores are presented in Table 5.In comparing ?Baseline?
method with ?IST-ITG?method, the improvement in BLEU was a 1.06-point.
In comparing ?IST-ITG?
method with ?Pro-posed?
method, the improvement in BLEU was a0.33-point.
As in the E-J experiments, ?Proposed?method performed the highest BLEU.
We demon-75strated that the proposed method is effective for mul-tiple language pairs.
However, the improvementof BLEU score in E-C translation is smaller thanthe improvement in E-J translation, because Englishand Chinese are similar sentence structures, such asSVO-languages (Japanese is SOV-language).
Whenthe sentence structures are different, the proposed re-ordering model is effective.5 ConclusionThis paper proposed a new word reordering modelusing syntactic information of a source tree forphrase-based statistical machine translation.
Theproposed model is an extension of the IST-ITG con-straints.
In both IST-ITG constraints and the pro-posed method, the target-side word order is obtainedby rotating nodes of the source-side tree structure.Both the IST-ITG constraints and the proposed re-ordering model fix the phrase position for the globalreorderings.
However, the proposed method canconduct a probabilistic evaluation of target word re-orderings which the IST-ITG constraints cannot.
InE-J and E-C translation experiments, the proposedmethod resulted in a 0.49-point improvement (29.31to 29.80) and a 0.33-point improvement (18.60 to18.93) in word BLEU-4 compared with IST-ITGconstraints, respectively.
This indicates the validityof the proposed reordering model.Future work will focus on a reduction of com-putational cost of decoding including the proposedreordering model, and a simultaneous training oftranslation and reordering models.
Moreover, wewill deal with difference between source and targetin multi level like in Gally et al (2004).The improvement could clearly be seen from vi-sual inspection of the output, a few examples ofwhich are presented in the following Appendix.A Samples from the English-to-JapaneseTranslationA.1 Sentence 1Source: Aggravation was obvious from the latterhalf of March to the end of April, and he contractedthe disease in February to the beginning of May.Baseline: ????????????????????????????????
?Reference: ????????????????????????????
?Proposed: ????????????????????????????????
?A.2 Sentence 2Source: The value of TF, on the other hand, washigher in the reverse order, indicating that high ox-idation rate causes severe defects on the surface ofNi crystallites.Baseline: ????????????????????????????????????????????????????
?Reference: ???????????????????????????????????????????????
?Proposed: ???????????????????????????????????????????????????
?A.3 Sentence 3Source: After diagnosing the pleural effusion andascites, vein catheter was left in place under the echoguide, and after removing the pleural effusion andascites, OK-432 was administered locally.Baseline: ????????????????????????????????????????????????????????
?Reference: ?????????????????????????????????????????????????????????
?Proposed: ?????????????????????????????????????????????????????????????
?A.4 Sentence 4Source: From result of the consideration, it waspointed that radiation from the loop elements wasweak.Baseline: ?????????????????????????????
?Reference: ???????????????????????????
?Proposed: ???????????????????????????
?76ReferencesAdam L. Berger, Peter F. Brown, Stephen A. Della Pietra,Vincent J. Della Pietra, Andrew S. Kehler, and RobertL.
Mercer 1996.
Language translation apparatusand method of using context-based translation models.United States patent, patent number 5510981.Eugene Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In Proceedings of NAACL 2000, pages 132?139.Chasenhttp://chasen-legacy.sourceforge.jp/Heidi J.
Fox, 2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of EMNLP, pages304?311.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT/NAACL-04.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical Syntax-Directed Translation with ExtendedDomain of Locality.
In Proceedings of AMTA.Japanese-English paper abstract corpushttp://www.jst.go.jpReinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language model In Proceed-ings of ICASSP 1995, pages 181?184.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL 2003, pages 127?133.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical MachineTranslation with Syntactified Target Language PhrasesIn Proceedings of EMNLP2006, pages 44?52.Dan Melamed.
2004.
Statistical machine translation byparsing In Proceedings of ACL, pages 653?660.Moseshttp://www.statmt.org/moses/Franz josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1), pages 19?51.Franz josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of ACL,pages 160?167.Franz josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4), pages 417?449.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of ACL, pages 271?279.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318.Andreas Stolcke.
2002.
SRILM - An Ex-tensible Language Model Toolkit In Pro-ceedings of ICSLP2002, pages 901?904.http://www.speech.sri.com/projects/srilm/Christopher Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Proceed-ings of HLT-NAACL, pages 101?104.Masao Uchiyama and Hitoshi Isahara.
2007.
2007.
Ajapanese-english patent parallel corpus.
In MT sum-mit XI, pages 475?482.Dekai Wu.
1995.
Stochastic inversion transductiongrammars, with application to segmentation, bracket-ing, and alignment of parallel corpora.
In Proceedingsof IJCAI, pages 1328?1334.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguiatics, 23(3), pages 377?403.Kenji Yamada and Kevin Knight.
2000.
A syntax-basedstatistical translation model In Proceedings of ACL,pages 523?530.Hirofumi Yamamoto, Hideo Okuma, and EiichiroSumita.
2008.
Imposing Constraints from the SourceTree on ITG Constraints for SMT.
In Proceedings ofACL : HLT Second Workshop on Syntax and Structurein Statistical Translation (SSST-2), pages 1?9.77
