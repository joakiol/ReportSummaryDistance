Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 36?41,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsMeasuring Sentiment Annotation Complexity of TextAditya Joshi1,2,3?Abhijit Mishra1Nivvedan Senthamilselvan1Pushpak Bhattacharyya11IIT Bombay, India,2Monash University, Australia3IITB-Monash Research Academy, India{adityaj, abhijitmishra, nivvedan, pb}@cse.iitb.ac.inAbstractThe effort required for a human annota-tor to detect sentiment is not uniform forall texts, irrespective of his/her expertise.We aim to predict a score that quantifiesthis effort, using linguistic properties ofthe text.
Our proposed metric is calledSentiment Annotation Complexity (SAC).As for training data, since any direct judg-ment of complexity by a human annota-tor is fraught with subjectivity, we rely oncognitive evidence from eye-tracking.
Thesentences in our dataset are labeled withSAC scores derived from eye-fixation du-ration.
Using linguistic features and anno-tated SACs, we train a regressor that pre-dicts the SAC with a best mean error rate of22.02% for five-fold cross-validation.
Wealso study the correlation between a hu-man annotator?s perception of complexityand a machine?s confidence in polarity de-termination.
The merit of our work lies in(a) deciding the sentiment annotation costin, for example, a crowdsourcing setting,(b) choosing the right classifier for senti-ment prediction.1 IntroductionThe effort required by a human annotator to de-tect sentiment is not uniform for all texts.
Com-pare the hypothetical tweet ?Just what I wanted: agood pizza.?
with ?Just what I wanted: a coldpizza.?.
The two are lexically and structurallysimilar.
However, because of the sarcasm in thesecond tweet (in ?cold?
pizza, an undesirable sit-uation followed by a positive sentiment phrase?just what I wanted?, as discussed in Riloff et al(2013)), it is more complex than the first for senti-ment annotation.
Thus, independent of how good?- Aditya is funded by the TCS Research Fellowship Pro-gram.the annotator is, there are sentences which will beperceived to be more complex than others.
Withregard to this, we introduce a metric called senti-ment annotation complexity (SAC).
The SAC of agiven piece of text (sentences, in our case) can bepredicted using the linguistic properties of the textas features.The primary question is whether such complex-ity measurement is necessary at all.
Fort et al(2012) describe the necessity of annotation com-plexity measurement in manual annotation tasks.Measuring annotation complexity is beneficial inannotation crowdsourcing.
If the complexity ofthe text can be estimated even before the annota-tion begins, the pricing model can be fine-tuned(pay less for sentences that are easy to annotate,for example).
Also, in terms of an automatic SAengine which has multiple classifiers in its ensem-ble, a classifier may be chosen based on the com-plexity of sentiment annotation (for example, usea rule-based classifier for simple sentences and amore complex classifier for other sentences).
Ourmetric adds value to sentiment annotation and sen-timent analysis, in these two ways.
The fact thatsentiment expression may be complex is evidentfrom a study of comparative sentences by Gana-pathibhotla and Liu (2008), sarcasm by Riloff etal.
(2013), thwarting by Ramteke et al (2013) orimplicit sentiment by Balahur et al (2011).
Tothe best of our knowledge, there is no general ap-proach to ?measure?
how complex a piece of textis, in terms of sentiment annotation.The central challenge here is to annotate a dataset with SAC.
To measure the ?actual?
time spentby an annotator on a piece of text, we use an eye-tracker to record eye-fixation duration: the timefor which the annotator has actually focused onthe sentence during annotation.
Eye-tracking an-notations have been used to study the cognitive as-pects of language processing tasks like translationby Dragsted (2010) and sense disambiguation by36Joshi et al (2011).
Mishra et al (2013) present atechnique to determine translation difficulty index.The work closest to ours is by Scott et al (2011)who use eye-tracking to study the role of emotionwords in reading.The novelty of our work is three-fold: (a) Theproposition of a metric to measure complexity ofsentiment annotation, (b) The adaptation of pastwork that uses eye-tracking for NLP in the con-text of sentiment annotation, (c) The learning ofregressors that automatically predict SAC usinglinguistic features.2 Understanding Sentiment AnnotationComplexityThe process of sentiment annotation consists oftwo sub-processes: comprehension (where the an-notator understands the content) and sentimentjudgment (where the annotator identifies the sen-timent).
The complexity in sentiment annotationstems from an interplay of the two and we expectSAC to capture the combined complexity of boththe sub-processes.
In this section, we describehow complexity may be introduced in sentimentannotation in different classical layers of NLP.The simplest form of sentiment annotation com-plexity is at the lexical level.
Consider the sen-tence ?It is messy, uncouth, incomprehensible, vi-cious and absurd?.
The sentiment words usedin this sentence are uncommon, resulting in com-plexity.The next level of sentiment annotation com-plexity arises due to syntactic complexity.
Con-sider the review: ?A somewhat crudely con-structed but gripping, questing look at a person soracked with self-loathing, he becomes an enemy tohis own race.?.
An annotator will face difficultyin comprehension as well as sentiment judgmentdue to the complicated phrasal structure in this re-view.
Implicit expression of sentiment introducescomplexity at the semantic and pragmatic level.Sarcasm expressed in ?It?s like an all-star salute todisney?s cheesy commercialism?
leads to difficultyin sentiment annotation because of positive wordslike ?an all-star salute?.Manual annotation of complexity scores maynot be intuitive and reliable.
Hence, we use a cog-nitive technique to create our annotated dataset.The underlying idea is: if we monitor annotationof two textual units of equal length, the more com-plex unit will take longer to annotate, and hence,should have a higher SAC.
Using the idea of ?an-notation time?
linked with complexity, we devise atechnique to create a dataset annotated with SAC.It may be thought that inter-annotator agree-ment (IAA) provides implicit annotation: thehigher the agreement, the easier the piece of textis for sentiment annotation.
However, in case ofmultiple expert annotators, this agreement is ex-pected to be high for most sentences, due to theexpertise.
For example, all five annotators agreewith the label for 60% sentences in our data set.However, the duration for these sentences has amean of 0.38 seconds and a standard deviation of0.27 seconds.
This indicates that although IAA iseasy to compute, it does not determine sentimentannotation complexity of text in itself.3 Creation of dataset annotated withSACWe wish to predict sentiment annotation complex-ity of the text using a supervised technique.
Asstated above, the time-to-annotate is one good can-didate.
However, ?simple time measurement?
isnot reliable because the annotator may spend timenot doing any annotation due to fatigue or distrac-tion.
To accurately record the time, we use aneye-tracking device that measures the ?duration ofeye-fixations1?.
Another attribute recorded by theeye-tracker that may have been used is ?saccadeduration2?.
However, saccade duration is not sig-nificant for annotation of short text, as in our case.Hence, the SAC labels of our dataset are fixationdurations with appropriate normalization.It may be noted that the eye-tracking device isused only to annotate training data.
The actualprediction of SAC is done using linguistic featuresalone.3.1 Eye-tracking Experimental SetupWe use a sentiment-annotated data set consistingof movie reviews by (Pang and Lee, 2005) andtweets from http://help.sentiment140.com/for-students.
A total of 1059 sen-tences (566 from a movie corpus, 493 from a twit-ter corpus) are selected.We then obtain two kinds of annotation fromfive paid annotators: (a) sentiment (positive, nega-tive and objective), (b) eye-movement as recorded1A long stay of the visual gaze on a single location.2A rapid movement of the eyes between positions of reston the sentence.37Figure 1: Gaze-data recording using Translog-IIby an eye-tracker.
They are given a set of instruc-tions beforehand and can seek clarifications.
Thisexperiment is conducted as follows:1.
A sentence is displayed to the annotator onthe screen.
The annotator verbally states thesentiment of this sentence, before (s)he canproceed to the next.2.
While the annotator reads the sentence, aremote eye-tracker (Model: Tobii TX 300,Sampling rate: 300Hz) records the eye-movement data of the annotator.
The eye-tracker is linked to a Translog II soft-ware (Carl, 2012) in order to record the data.A snapshot of the software is shown in fig-ure 1.
The dots and circles represent positionof eyes and fixations of the annotator respec-tively.3.
The experiment then continues in modules of50 sentences at a time.
This is to prevent fa-tigue over a period of time.
Thus, each an-notator participates in this experiment over anumber of sittings.We ensure the quality of our dataset in differentways: (a) Our annotators are instructed to avoidunnecessary head movements and eye-movementsoutside the experiment environment.
(b) To min-imize noise due to head movements further, theyare also asked to state the annotation verbally,which was then manually recorded, (c) Our an-notators are students between the ages 20-24 withEnglish as the primary language of academic in-struction and have secured a TOEFL iBT score of110 or above.We understand that sentiment is nuanced- to-wards a target, through constructs like sarcasm andpresence of multiple entities.
However, we want tocapture the most natural form of sentiment anno-tation.
So, the guidelines are kept to a bare mini-mum of ?annotating a sentence as positive, nega-tive and objective as per the speaker?.
This exper-iment results in a data set of 1059 sentences witha fixation duration recorded for each sentence-annotator pair3The multi-rater kappa IAA for sen-timent annotation is 0.686.3.2 Calculating SAC from eye-tracked dataWe now need to annotate each sentence with aSAC.
We extract fixation durations of the five an-notators for each of the annotated sentences.
Asingle SAC score for sentence s for N annotatorsis computed as follows:SAC(s) =1NN?n=1z(n,dur(s,n))len(s)where,z(n, dur(s, n)) =dur(s,n)??(dur(n))?
(dur(n))(1)In the above formula, N is the total number of an-notators while n corresponds to a specific annota-tor.
dur(s, n) is the fixation duration of annotatorn on sentence s. len(s) is the number of wordsin sentence s. This normalization over numberof words assumes that long sentences may havehigh dur(s, n) but do not necessarily have highSACs.
?
(dur(n)), ?
(dur(n)) is the mean andstandard deviation of fixation durations for anno-tator n across all sentences.
z(n, .)
is a functionthat z-normalizes the value for annotator n to stan-dardize the deviation due to reading speeds.
Weconvert the SAC values to a scale of 1-10 usingmin-max normalization.
To understand how theformula records sentiment annotation complexity,consider the SACs of examples in section 2.
Thesentence ?it is messy , uncouth , incomprehensi-ble , vicious and absurd?
has a SAC of 3.3.
On theother hand, the SAC for the sarcastic sentence ?it?slike an all-star salute to disney?s cheesy commer-cialism.?
is 8.3.4 Predictive Framework for SACThe previous section shows how gold labels forSAC can be obtained using eye-tracking experi-ments.
This section describes our predictive forSAC that uses four categories of linguistic fea-tures: lexical, syntactic, semantic and sentiment-related in order to capture the subprocesses of an-notation as described in section 2.4.1 Experiment SetupThe linguistic features described in Table 3.2 areextracted from the input sentences.
Some of these3The complete eye-tracking data is available at:http://www.cfilt.iitb.ac.in/?cognitive-nlp/.38Feature DescriptionLexical- Word Count- Degree of polysemy Average number of Wordnet senses per word- Mean Word Length Average number of characters per word (commonly used in readability studiesas in the case of Pascual et al (2005))- %ge of nouns and adjs.- %ge of Out-of-vocabulary wordsSyntactic- Dependency Distance Average distance of all pairs of dependent words in the sentence (Lin, 1996)- Non-terminal to Ter-minal ratioRatio of the number of non-terminals to the number of terminals in the con-stituency parse of a sentenceSemantic- Discourse connectors Number of discourse connectors- Co-reference distance Sum of token distance between co-referring entities of anaphora in a sentence- Perplexity Trigram perplexity using language models trained on a mixture of sentencesfrom the Brown corpus, the Amazon Movie corpus and Stanford twitter corpus(mentioned in Sections 3 and 5)Sentiment-related (Computed using SentiWordNet (Esuli et al, 2006))- Subjective WordCount- Subjective Score Sum of SentiWordNet scores of all words- Sentiment Flip Count A positive word followed in sequence by a negative word, or vice versa countsas one sentiment flipTable 1: Linguistic Features for the Predictive Frameworkfeatures are extracted using Stanford Core NLP4tools and NLTK (Bird et al, 2009).
Words thatdo not appear in Academic Word List5and Gen-eral Service List6are treated as out-of-vocabularywords.
The training data consists of 1059 tuples,with 13 features and gold labels from eye-trackingexperiments.To predict SAC, we use Support Vector Regres-sion (SVR) (Joachims, 2006).
Since we do nothave any information about the nature of the rela-tionship between the features and SAC, choosingSVR allows us to try multiple kernels.
We carryout a 5-fold cross validation for both in-domainand cross-domain settings, to validate that the re-gressor does not overfit.
The model thus learned isevaluated using: (a) Error metrics namely, MeanSquared Error estimate, Mean Absolute Error esti-mate and Mean Percentage Error.
(b) the Pearsoncorrelation coefficient between the gold and pre-4http://nlp.stanford.edu/software/corenlp.shtml5www.victoria.ac.nz/lals/resources/academicwordlist/6www.jbauman.com/gsl.htmldicted SAC.4.2 ResultsThe results are tabulated in Table 2.
Our obser-vation is that a quadratic kernel performs slightlybetter than linear.
The correlation values are pos-itive and indicate that even if the predicted scoresare not as accurate as desired, the system is capa-ble of ranking sentences in the correct order basedon their sentiment complexity.
The mean percent-age error (MPE) of the regressors ranges between22-38.21%.
The cross-domain MPE is higher thanthe rest, as expected.To understand how each of the features per-forms, we conducted ablation tests by con-sidering one feature at a time.
Based onthe MPE values, the best features are: Meanword length (MPE=27.54%), Degree of Polysemy(MPE=36.83%) and %ge of nouns and adjectives(MPE=38.55%).
To our surprise, word count per-forms the worst (MPE=85.44%).
This is unliketasks like translation where length has been shown39Kernel Linear Quadratic Cross Domain LinearDomain Mixed Movie Twitter Mixed Movie Twitter Movie TwitterMSE 1.79 1.55 1.99 1.68 1.53 1.88 3.17 2.24MAE 0.93 0.89 0.95 0.91 0.88 0.93 1.39 1.19MPE 22.49% 23.8% 25.45% 22.02% 23.8% 25% 35.01% 38.21%Correlation 0.54 0.38 0.56 0.57 0.37 0.6 0.38 0.46Table 2: Performance of Predictive Framework for 5-fold in-domain and cross-domain validation usingMean Squared Error (MSE), Mean Absolute Error (MAE) and Mean Percentage Error (MPE) estimatesand correlation with the gold labels.to be one of the best predictors in translation dif-ficulty (Mishra et al, 2013).
We believe that forsentiment annotation, longer sentences may havemore lexical clues that help detect the sentimentmore easily.
Note that some errors may be intro-duced in feature extraction due to limitations ofthe NLP tools.5 DiscussionOur proposed metric measures complexity of sen-timent annotation, as perceived by human annota-tors.
It would be worthwhile to study the human-machine correlation to see if what is difficult fora machine is also difficult for a human.
In otherwords, the goal is to show that the confidencescores of a sentiment classifier are negatively cor-related with SAC.We use three sentiment classification tech-niques: Na?
?ve Bayes, MaxEnt and SVM with un-igrams, bigrams and trigrams as features.
Thetraining datasets used are: a) 10000 movie reviewsfrom Amazon Corpus (McAuley et.
al, 2013) andb) 20000 tweets from the twitter corpus (same asmentioned in section 3).
Using NLTK and Scikit-learn7with default settings, we generate six posi-tive/negative classifiers, for all possible combina-tions of the three models and two datasets.The confidence score of a classifier8for giventext t is computed as follows:P : Probability of predicted classConfidence(t) =??
?P if predictedpolarity is correct1?
P otherwise(2)7http://scikit-learn.org/stable/8In case of SVM, the probability of predicted class is com-puted as given in Platt (1999).Classifier (Corpus) CorrelationNa?
?ve Bayes (Movie) -0.06 (73.35)Na?
?ve Bayes (Twitter) -0.13 (71.18)MaxEnt (Movie) -0.29 (72.17)MaxEnt (Twitter) -0.26 (71.68)SVM (Movie) -0.24 (66.27)SVM (Twitter) -0.19 (73.15)Table 3: Correlation between confidence of theclassifiers with SAC; Numbers in parentheses in-dicate classifier accuracy (%)Table 3 presents the accuracy of the classifiersalong with the correlations between the confidencescore and observed SAC values.
MaxEnt has thehighest negative correlation of -0.29 and -0.26.For both domains, we observe a weak yet nega-tive correlation which suggests that the perceptionof difficulty by the classifiers are in line with thatof humans, as captured through SAC.6 Conclusion & Future WorkWe presented a metric called Sentiment Annota-tion Complexity (SAC), a metric in SA researchthat has been unexplored until now.
First, the pro-cess of data preparation through eye tracking, la-beled with the SAC score was elaborated.
Usingthis data set and a set of linguistic features, wetrained a regression model to predict SAC.
Ourpredictive framework for SAC resulted in a meanpercentage error of 22.02%, and a moderate corre-lation of 0.57 between the predicted and observedSAC values.
Finally, we observe a negative corre-lation between the classifier confidence scores anda SAC, as expected.
As a future work, we wouldlike to investigate how SAC of a test sentence canbe used to choose a classifier from an ensemble,and to determine the pre-processing steps (entity-relationship extraction, for example).40ReferencesBalahur, Alexandra and Hermida, Jes?us M and Mon-toyo, Andr?es.
2011.
Detecting implicit expressionsof sentiment in text based on commonsense knowl-edge.
Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis,53-60.Batali, John and Searle, John R. 1995.
The Rediscov-ery of the Mind.
Artif.
Intell., Vol.
77, 177-193.Steven Bird and Ewan Klein and Edward Loper.
2009.Natural Language Processing with Python O?ReillyMedia.Carl, M. 2012.
Translog-II: A Program for Record-ing User Activity Data for Empirical Reading andWriting Research.
In Proceedings of the Eight In-ternational Conference on Language Resources andEvaluation, European Language Resources Associ-ation.Dragsted, B.
2010.
2010.
Co-ordination of readingand writing processes in translation.
Contributionto Translation and Cognition.
Shreve, G. and An-gelone, E.(eds.
)Cognitive Science Society.Esuli, Andrea and Sebastiani, Fabrizio.
2006.
Sen-tiwordnet: A publicly available lexical resource foropinion mining.
Proceedings of LREC, vol.
6, 417-422.Fellbaum, Christiane 1998.
WordNet: An electroniclexical database.
1998.
Cambridge.
MA: MIT Press.Fort, Kar?en and Nazarenko, Adeline and Rosset, So-phie et al2012.
Modeling the complexity of manualannotation tasks: A grid of analysis Proceedings ofthe International Conference on Computational Lin-guistics.Ganapathibhotla, G and Liu, Bing.
2008.
Identifyingpreferred entities in comparative sentences.
22nd In-ternational Conference on Computational Linguis-tics (COLING).Gonz?alez-Ib?a?nez, Roberto and Muresan, Smaranda andWacholder, Nina 2011.
Identifying Sarcasm inTwitter: A Closer Look.
ACL (Short Papers) 581-586.Joachims, T. 2006 Training Linear SVMs in Lin-ear Time Proceedings of the ACM Conference onKnowledge Discovery and Data Mining (KDD).Lin, D. 1996 On the structural complexity of naturallanguage sentences.
Proceeding of the 16th Inter-national Conference on Computational Linguistics(COLING), pp.
729733.Mart?nez-G?omez, Pascual and Aizawa, Akiko.
2013.Diagnosing Causes of Reading Difficulty usingBayesian Networks International Joint Conferenceon Natural Language Processing, 13831391.McAuley, Julian John and Leskovec, Jure 2013 Fromamateurs to connoisseurs: modeling the evolution ofuser expertise through online reviews.
Proceedingsof the 22nd international conference on World WideWeb.Mishra, Abhijit and Bhattacharyya, Pushpak and Carl,Michael.
2013.
Automatically Predicting SentenceTranslation Difficulty Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), 346-351.Narayanan, Ramanathan and Liu, Bing and Choudhary,Alok 2009.
Sentiment Analysis of Conditional Sen-tences.
Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing,180-189.Pang, Bo and Lee, Lillian.
2008.
Opinion mining andsentiment analysis Foundations and trends in infor-mation retrieval, vol.
2, 1-135.Pang, Bo and Lee, Lillian.
2005.
Seeing stars: Ex-ploiting class relationships for sentiment categoriza-tion with respect to rating scales.
Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, 115-124.Platt, John and others.
1999.
Probabilistic outputs forsupport vector machines and comparisons to regular-ized likelihood methods Advances in large marginclassifiers, vol.
10, 61-74.Ramteke, Ankit and Malu, Akshat and Bhattacharyya,Pushpak and Nath, J. Saketha 2013.
Detect-ing Turnarounds in Sentiment Analysis: ThwartingProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), 860-865.Riloff, Ellen and Qadir, Ashequl and Surve, Prafullaand De Silva, Lalindra and Gilbert, Nathan andHuang, Ruihong 2013.
Sarcasm as Contrast be-tween a Positive Sentiment and Negative SituationConference on Empirical Methods in Natural Lan-guage Processing, Seattle, USA.Salil Joshi, Diptesh Kanojia and Pushpak Bhat-tacharyya.
2013.
More than meets the eye: Studyof Human Cognition in Sense Annotation.
NAACLHLT 2013, Atlanta, USA.Scott G. , O Donnell P and Sereno S. 2012.
EmotionWords Affect Eye Fixations During Reading.
Jour-nal of Experimental Psychology:Learning, Memory,and Cognition 2012, Vol.
38, No.
3, 783-792Siegel, Sidney and N. J. Castellan, Jr. 1988.
Nonpara-metric Statistics for the Behavioral Sciences.
Secondedition.
McGraw-Hill.41
