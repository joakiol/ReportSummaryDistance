Paraphrasing Rules for Automatic Evaluation of Translation into JapaneseKANAYAMA HiroshiTokyo Research Laboratory, IBM Japan, Ltd.1623-14 Shimo-tsuruma, Yamato-shi, Kanagawa 242-8502, Japankanayama@trl.ibm.comAbstractAutomatic evaluation of translation qual-ity has proved to be useful when the targetlanguage is English.
In this paper the eval-uation of translation into Japanese is stud-ied.
An existing method based on n-gramsimilarity between translations and refer-ence sentences is difficult to apply to theevaluation of Japanese because of the ag-glutinativeness and variation of semanti-cally similar expressions in Japanese.
Theproposed method applies a set of para-phrasing rules to the reference sentencesin order to increase the similarity scorefor the expressions that differ only in theirwriting styles.
Experimental results showthe paraphrasing rules improved the cor-relation between automatic evaluation andhuman evaluation from 0.80 to 0.93.1 IntroductionEvaluating natural language processing applica-tions?
output is important both for users and devel-opers.
Tasks such as sentential parsing, morpho-logical analysis and named entity recognition areeasy to evaluate automatically because the ?right an-swer?
can be defined deterministically under a spe-cific grammar or assumed criterion.The evaluation of machine translation is not sostraightforward since there are infinite ways to out-put similar meanings and one can not enumerate theright answers exhaustively.
In spite of that, auto-matic translation evaluation is practically importantbecause the evaluation is laborious work for humansand evaluation by humans tends to be arbitrary.
Au-tomatic evaluation is more reliable than human eval-uation because of its consistency for the same trans-lations.BLEU (Papineni et al, 2002b) is one of the meth-ods for automatic evaluation of translation quality.It uses the ratio of co-occurring n-grams betweena translation and single or multiple reference sen-tences.
High correlation is reported between theBLEU score and human evaluations for translationsfrom Arabic, Chinese, French, and Spanish to En-glish (Papineni et al, 2002a).This paper investigates how to apply BLEU to theevaluation of English-to-Japanese translation.
Themain goal of this paper is to design a reliable methodof evaluation for translations from another languageto Japanese (henceforth we call this Japanese trans-lation evaluation).
There are some difficulties in ad-justing BLEU for Japanese: BLEU uses n-grams ofwords, so words in a sentence are assumed to be sep-arated by spaces, while Japanese does not use spacesbetween words.
Moreover, Japanese has more vari-ation in writing styles than English.
A major differ-ence in these languages is that Japanese has politeforms expressed by inflections or auxiliary verbs.
Ifthe style of the translations is not the same as thatof the reference sentences, the evaluation score be-comes low even though the translations are accuratein their meanings and grammar.
To solve these prob-lems, we apply paraphrasing rules to the referencesentences so that the differences in writing styles donot affect the evaluation score.Another goal is derived from this applicationof paraphrasing: to define a ?good paraphrase?.Here paraphrasing means rewriting sentences with-out changing their semantics.
Several methods ofparaphrasing have been studied.
Some of them aimat the preprocessing of machine translation (Mita-mura and Nyberg, 2001; Takahashi et al, 2001).They use paraphrasing to transform the input sen-tences so that the language-transferring routinescan handle them easily.
Another application ofparaphrasing is to canonicalize many expressionsthat have the same semantics, supporting informa-tion retrieval or question answering (Zukerman andRaskutti, 2002; Torisawa, 2002).
Paraphrasing tech-niques in these studies are considered to be useful,but they are difficult to evaluate.Machine translation evaluation requires methodsto judge whether two sentences have the samemeaning even when they are syntactically different.Therefore if a set of paraphrasing rules contributesto more reliable translation evaluation, it can be saidto be ?good?
paraphrasing.
Thus the study in thispaper also presents a new paradigm for evaluatingparaphrases.Section 2 overviews the BLEU metric.
Section 3presents the proposed method of Japanese transla-tion evaluation, and its performance is evaluated inSection 4.
Based on the experimental results, Sec-tion 5 discusses qualitative and quantitative featuresof paraphrasing.2 Background: Overview of BLEUThis section briefly describes the original BLEU(Papineni et al, 2002b)1, which was designed forEnglish translation evaluation, so English sentencesare used as examples in this section.2.1 N-gram precisionBLEU evaluation uses a parallel corpus which con-sists of sentences in the source language and theirtranslations to the target language by professionaltranslators.
We call the professional translations ref-erence sentences.
It is preferable if the corpus hasmultiple reference sentences translated by multipletranslators for each source sentence.Sentences in the source language are also trans-lated by the translation systems to be evaluated.
The1See the cited paper for more detailed definitions.translations are called candidate sentences.
Belowis an example.Example 1Reference 1I had my watch repaired by an office worker.Reference 2A person in the office repaired my watch.Candidate 1I had a man in the office repair a watch.Candidate 2I had the person of an office correct a clock.The BLEU score is based on n-gram precisionshown in Equation (1).
It is the ratio of n-gramswhich appear both in the candidate sentence and inat least one of the reference sentences, among all n-grams in the candidate sentence.pn =?s?cand?ngr?smin(C(ngr), Cr(ngr))?s?cand?ngr?sC(ngr)cand : candidates s : sentence ngr : n-gramC : count in the candidate sentenceCr : count in a corresponding reference sentence(1)Candidate 1 in Example 1 contains 11 unigramsincluding punctuation.
8 unigrams out of these alsoappear in Reference 1 or Reference 2: ?I?, ?had?, ?a?,?in?, ?the?, ?office?, ?watch?
and ?.
?, therefore, the un-igram precision of Candidate 1 is 8/11.
The bigramprecision is 4/10 since ?I had?, ?in the?, ?the office?and ?watch .?
are found.
The only matched trigramis ?in the office?, so the trigram precision is 1/9.On the other hand, the unigram, bigram, and tri-gram precisions of Candidate 2 are 8/11, 2/10, 0/9,respectively, which are lower than those of Candi-date 1.
Indeed Candidate 1 is a better English trans-lation than Candidate 2.In practice the n-gram precision is calculated notfor each sentence but for all of the sentences in thecorpus.2.2 Brevity PenaltyThe n-gram precision is calculated by dividing thenumber of matched n-grams by the number of n-grams in the candidate sentence.
Therefore, a shortcandidate sentence which consists only of frequentlyused words can score a high n-gram precision.
Forexample, if the candidate sentence is just ?The?, itsunigram precision is 1.0 if one of reference sen-tences contains at least one ?the?, and that is usuallytrue.To penalize such a meaningless translation, theBLEU score is multiplied by the brevity penaltyshown in (2).BP ={1 if c > re(1?r/c) if c ?
r (2)where c and r are the total numbers of words inthe candidate sentences and the reference sentenceswhich have the closest numbers of words in eachparallel sentence.2.3 BLEU scoreThe BLEU score is calculated by Equation (3) be-low.
It is the geometric average of the n-gram pre-cisions multiplied by the brevity penalty.
The geo-metric average is used because pn decreases expo-nentially as n increases.
The BLEU score rangesbetween 0 and 1.BLEU = BP ?
( N?n=1pn) 1N(3)The evaluations use unigrams up to N -grams.
If alarge n is used, the fluency of the sentences becomesa more important factor than the correctness of thewords.
Empirically the BLEU score has a high cor-relation with human evaluation when N = 4 for En-glish translation evaluations (Papineni et al, 2002b).3 Japanese Version of BLEU and ItsExtensionThis section describes how to adapt BLEU forJapanese translation evaluation.
The adaptation con-sists of three steps.3.1 Use of Morphological AnalyzerThe first modification is mandatory for using the n-gram metric as in the original BLEU implementa-tion.
Since Japanese has no spaces between words,the words have to be separated by morphologicalanalysis as in Example 2.Example 2Kare ga hon wo yo mi mashi ta .He SUBJ book ACC read INF POLITE PAST .
?He read a book.
?3.2 Distinguish between DifferentParts-of-speechMany English words can be used as various parts-of-speech (POSs), but BLEU doesn?t distinguish be-tween the words with the same surface form in termsof their POSs, since the sentences are not processedby a tagger, so the system can?t handle POSs.
Thisdoesn?t cause a problem because most of the multi-POS words have conceptually similar meanings, asexemplified by the adverb ?fast?
and the adjective?fast?
which have the same basic concept, so match-ing them between the candidate and references rea-sonably reflects the quality of the translation.On the other hand, Japanese homonyms tend tobe completely different if their POSs are different.For example, the postpositional phrasal particle ?ga?and the connective particle ?ga?
should be distin-guished from one another since the former acts asa subject case marker, while the latter connects twoclauses that normally contradict each other.
Fortu-nately the morphological analyzer outputs POS in-formation when the sentence is separated into words,and therefore the words are also distinguished bytheir POSs in the described method.3.3 Paraphrasing RulesExample 3 is another possible translation of thesource sentence of Example 2.Example 3Kare ga hon wo yo n da .He SUBJ book ACC read INF-EUPH PAST .
?He read a book.
?The only difference here is the ending of the sen-tence has a less polite form.
However, when Ex-ample 2 is the only reference sentence, the BLEUevaluation of Example 3 does not score high: 6/8 forunigrams, 4/7 for bigrams, 3/6 for trigrams, and 2/51 $1(verb-c) : n : da ?
$1 : mi : masi : ta2 nai(adj) : .
?
ari : mase : n : .3 $1(noun) : da ?
$1 : dearu4 ni : yo : t : te ?
ni : yo : riTable 1: Examples of paraphrasing rules.
$1 denotesa wild card shared by both sides.
?:?
is a boundary ofmorphemes.
?(verb-c)?
means a consonant verb suchas ?yomu?.
Actually these rules have conditions notdescribed here so that they are not overused.for 4-grams, while its meaning is same as that of thereference sentence.Basically BLEU copes with this problem of varia-tion in writing styles by relying on the number of ref-erence sentences available for each source sentenceand by reflecting the total size of corpus.
That is,if the corpus has multiple reference sentences trans-lated by different translators, multiple writing styleswill tend to be included, and if the corpus is verylarge, such inconsistencies of writing style are sta-tistically not a problem.In Japanese translation evaluation, however, thisproblem can not be resolved using such a quantita-tive solution because the influence of the differencesin writing styles are too large.
For example, whetheror not the translation is given in the polite form de-pends on the translation system2, so the evaluationscore is strongly affected by the degree of matchingof the writing styles between the translation systemand the reference sentences.To cancel out the differences in writing styles,we apply some paraphrasing rules to the referencesentences to generate new sentences with differentwriting styles.
The generated sentences are addedto the reference sentences, and therefore, n-gramsin the candidate sentences can match the referencesentences regardless of their writing styles.
Table 1shows examples of paraphrasing rules.These rules are applied to the reference sentences.If a reference sentence matches to a paraphrasingrule, the sentence is replicated and the replica isrewritten using the matched rule.
For example, theJapanese sentence in Example 2 matches Rule 1 inTable 1 so the Japanese sentence in Example 3 is2Some translation systems allow us to specify such writingstyles but some systems don?t.produced.
In this case, the evaluation is done as ifthere are two reference sentences, therefore, a can-didate sentence gets the same score regardless of itspoliteness.To avoid applying the same rules repeatably, therules are applied in a specific order.
How to generatethe rules is described in Section 4.1.4 Experiments4.1 EnvironmentsTo see how much the three extensions above con-tribute to the evaluation of translation, the correla-tion between the automatic evaluation and the hu-man evaluation is calculated.
We used a bilingualcorpus which consists of 6,871 English sentenceson a technical domain and their translations intoJapanese.100 sentences were randomly selected and trans-lated by 5 machine translation systems S1-S5 anda human H1 who is a native Japanese speaker butdoes not have strong knowledge of the technical do-main.
These 6 translations were evaluated by fivemethods: B1 to B4 are Japanese versions of BLEUwith the extension described in Section 3 and M1 isa manual evaluation.B1: Morphological analysis is applied to translatedJapanese sentences.
Only the technique de-scribed in Section 3.1 is used.B2: Functional words are distinguished by theirPOSs.
This corresponds to the technique inSection 3.1 and 3.2.B3: Paraphrasing rules are applied to the referencesentences as described in Section 3.3.
Herethe applied rules are limited to 51 rules whichrewrite polite forms (e.g.
1 and 2 in Table 1).B4: All 88 paraphrasing rules including other types(e.g.
3 and 4 in Table 1) are applied.M1: Average score of the manual evaluation of alltranslations in the corpus.
The sentences werescored using a 5-level evaluation: 1 (poor) to5 (good).
The evaluator was different from thetranslator of H1.B1 B2 B3 B4 M1S1 0.115 0.114 0.132 0.135 2.38S2 0.130 0.129 0.149 0.151 2.74S3 0.134 0.132 0.148 0.152 2.77S4 0.137 0.135 0.148 0.158 3.16S5 0.183 0.177 0.179 0.180 3.38H1 0.170 0.166 0.179 0.187 4.40correl 0.797 0.803 0.865 0.931 (1.0)Table 2: BLEU scores evaluated by each method.?correl?
means the correlation of each method withthe manual evaluation (M1).00.51S1 S2 S3 S4 S5 H1??
??????
?
??
???
??????
?????
B2?
B3?
B4?
M1Figure 1: BLEU scores normalized as S1 is 0 andH1 is 1.
B1 is omitted since it is close to B2.The paraphrasing rules used in B3 and B4 wereprepared manually by comparing the candidate sen-tences and the reference sentences in the reminder ofthe corpus which are not used for the evaluation.
Theapplication of the rules are unlikely to produce in-correct sentences, because the rules are adjusted byadding the applicable conditions, and the rules thatmay have side effects are not adopted.
This was con-firmed by applying the rules to 200 sentences in an-other corpus.
A total of 189 out of the 200 sentenceswere paraphrased in at least a part, and all of thenewly created sentences were grammatically correctand had the same meaning as the original sentences.4.2 Experimental ResultsTable 2 shows the result of evaluation using the fivemethods.
Comparing the correlation with M1, B2slightly outperformed B1, thus the POS informationimproves the evaluation.
B3 was better than B2 incorrelation by 0.06.
This is because the scores by theB3 evaluation were much higher than the B2 evalu-ation except for S5, since only S5 tends to outputsentences in polite form while the most of referencesentences are written in polite form.
Further im-provement was observed in B4, by applying othertypes of paraphrasing rules.Figure 1 graphically illustrates the correlation be-tween the BLEU evaluations and the human evalua-tions, by normalizing the results so that S1 is 0, H1is 1, and the rest of scores are linearly interpolated.We can see that only B4 ranks all six systems in thesame order as the manual evaluation.5 Discussion5.1 Lexical or Structural Paraphrasing RulesThe paraphrasing rules used here have no lexicalrules that rewrite content words into other expres-sions as in Example 4.Example 4dokusho : suru ?
hon : wo : yo : mu?read?
?read a book?The main reason why we don?t use such rules isthat this type of rules may produce incorrect sen-tences.
For instance, (a) in Example 5 is rewritteninto (b) by the rule in Example 4, but (b) is not cor-rect.Example 5(a) Kare ha watashi no hon wo yo mu.
?He reads my book.?
(b)* Kare ha watashi no dokusho suru.
?He my reads.?
(literally)This error can be decreased if the paraphrasingrules have more strict conditions about surroundingwords, however, using such lexical rules contradictsthe original BLEU?s strategy that the differences inexpressions should be covered by the number of ref-erence sentences.
This strategy is reasonable be-cause complicated rules tend to make the evaluationarbitrary, that is, the evaluation score strongly de-pends on the lexical rules.
To verify that the lex-ical rules are unnecessary, we added 17,478 word-replacing rules to B4.
The rules mainly replace Chi-nese characters or Kana characters with canonicalParaphrasing rule ?correlda(aux) ?
de : a : ru 0.025$1(verb-v) : ru ?
$1(verb-v) : masu 0.022$1(noun) : (dot) : $2(noun) ?
$1 : $2 0.020Table 3: The three best paraphrasing rules whichcontributed to the translation evaluation.
The col-umn ??correl?
means the decrease of the correla-tion in the translation evaluation when the rule is re-moved.
?(verb-v)?
denotes a vowel verb.ones.
With the rules, the correlation with M1 was0.886, which is much lower than B4.This result implies the differences in contentwords do not affect the evaluations.
More specifi-cally, BLEU?s misjudgments because of differencesin content words occur with almost equal probabil-ity for each translation system.
Thus it is enoughto use the structural (i.e.
non-lexical) paraphrasingrules which rewrite only functional words.5.2 Evaluation of Each Paraphrasing RuleThe contribution of the paraphrasing was measuredby the increase of reliability of the translation eval-uation, as described in Section 4.2.
In the sameway, the effect of each single paraphrasing rule canbe also evaluated quantitatively.
Table 3 shows thethree best paraphrasing rules which contributed tothe translation evaluation.
Here the contribution ofa rule to the automatic evaluation is measured bythe increase of correlation with the human evalua-tion when the rule is used.6 Conclusion and Future WorkThis paper has proposed an automatic translationevaluation method applicable to Japanese translationevaluation.
The paraphrasing rules that cancel outthe differences in writing styles contributed to im-prove the reliability of the automatic evaluation.
Theproposed evaluation method with paraphrasing rulesachieved a high correlation of 0.93 with the humanevaluation, while the correlation was 0.80 withoutthe rules.The experiments clarified how much the para-phrasing rules improved the evaluation by compar-ing the correlations.
This means our system canevaluate not only the translation quality but alsothe paraphrasing rules under the assumption that themore properly the semantically similar sentences arejudged as close sentences the more reliable the trans-lation evaluation is.
Therefore the translation evalu-ation gives us an objective evaluation method of theparaphrasing quality that has been difficult to evalu-ate.This paper focuses on non-lexical paraphrasingsince lexical paraphrasing rules make the translationevaluation inconsistent, but if an exhaustive and pre-cise set of paraphrasing rules can be generated, itwill be useful for translation evaluation, and its ap-propriateness should be shown by the reliability ofthe translation evaluation.
In order to develop suchdesirable paraphrasing rules, the automatic acquisi-tion of paraphrasing rules will be our next researchdirection.AcknowledgmentsI am grateful to Dr. Kishore Papineni for the instruc-tion of BLEU.
I would like to thank people in Yam-ato Research Laboratory for helping the evaluation.ReferencesTeruko Mitamura and Eric Nyberg.
2001.
Automaticrewriting for controlled language translation.
In Proc.of NLPRS2001 Workshop on Automatic Paraphrasing,pages 1?12.Kishore Papineni, Salim Roukos, Todd Ward, John Hen-derson, and Florence Reeder.
2002a.
Corpus-basedcomprehensive and diagnostic MT evaluation: InitialArabic, Chinese, French, and Spanish results.
In Proc.of HLT2002, pages 124?127.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002b.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of the 40thACL, pages 311?318.Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, andKentaro Inui.
2001.
Kura: A revision-basedlexico-structural paraphrasing engine.
In Proc.
ofNLPRS2001 Workshop on Automatic Paraphrasing,pages 37?46.Kentaro Torisawa.
2002.
An unsupervised learn-ing method for associative relationships between verbphrases.
In Proc.
of COLING 2002, pages 1009?1015.Ingrid Zukerman and Bhavani Raskutti.
2002.
Lexicalquery paraphrasing for document retrieval.
In Proc.
ofCOLING 2002, pages 1177?1183.
