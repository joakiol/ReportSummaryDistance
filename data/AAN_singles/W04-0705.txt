Applying Coreference to Improve Name RecognitionHeng JI and Ralph GRISHMANDepartment of Computer ScienceNew York University715 Broadway, 7th FloorNew York, NY 10003, U.S.A.hengji@cs.nyu.edu,  grishman@cs.nyu.eduAbstractWe present a novel method of applying theresults of coreference resolution to improveName Recognition for Chinese.
We considerfirst some methods for gauging the confidenceof individual tags assigned by a statisticalname tagger.
For names with low confidence,we show how these names can be filteredusing coreference features to improveaccuracy.
In addition, we present rules whichuse coreference information to correct somename tagging errors.
Finally, we show howthese gains can be magnified by clusteringdocuments and using cross-documentcoreference in these clusters.
These combinedmethods yield an absolute improvement ofabout 3.1% in tagger F score.1 IntroductionThe problem of name recognition andclassification has been intensively studied since1995, when it was introduced as part of the MUC-6 Evaluation (Grishman and Sundheim, 1996).
Awide variety of machine learning methods havebeen applied to this problem, including HiddenMarkov Models (Bikel et al 1997), MaximumEntropy methods (Borthwick et al 1998, Chieuand Ng 2002), Decision Trees (Sekine et al 1998),Conditional Random Fields (McCallum and Li2003), Class-based Language Model (Sun et al2002), Agent-based Approach (Ye et al 2002) andSupport Vector Machines.
However, theperformance of even the best of these models1 hasbeen limited by the amount of labeled training dataavailable to them and the range of features whichthey employ.
In particular, most of these methodsclassify an instance of a name based on theinformation about that instance alone, and verylocal context of that instance ?
typically, one or1The best results reported for Chinese named entityrecognition, on the MET-2 test corpus, are 0.92 to 0.95F-measure for the different name types (Ye et al 2002).two words preceding and following the name.
If aname has not been seen before, and appears in arelatively uninformative context, it becomes veryhard to classify.We propose to use more global information toimprove the performance of name recognition.Some name taggers have incorporated a namecache or similar mechanism which makes use ofnames previously recognized in the document.
Inour approach, we perform coreference analysis andthen use detailed evidence from other phrases inthe document which are co-referential with thisname in order to disambiguate the name.
Thisallows us to perform a richer set of correctionsthan with a name cache.
We then go one stepfurther and process similar documents containinginstances of the same name, and combine theevidence from these additional instances.
At eachstep we are able to demonstrate a small butconsistent improvement in named entityrecognition.The rest of the paper is organized as follows.Section 2 briefly describes the baseline nametagger and coreference resolver used in this paper.Section 3 considers methods for assessing theconfidence of name tagging decisions.
Section 4examines the distribution of name errors, as amotivation for using coreference information.Section 5 shows the coreference features we useand how they are incorporated into a statisticalname filter.
Section 6 describes additional rulesusing coreference to improve name recognition.Section 7 provides the flow graph of the improvedsystem.
Section 8 reports and discusses theexperimental results while Section 9 summarizesthe conclusions.2 Baseline SystemsThe task we consider in this paper is to identifythree classes of names in Chinese text:  persons(PER), organizations (ORG), and geo-politicalentities (GPE).
Geo-political entities are locationswhich have an associated government, such ascities, states, and countries.2  Name recognition inChinese poses extra challenges because neithercapitalization nor word segmentation clues areexplicitly provided, although most of thetechniques we describe are more generallyapplicable.Our study builds on an extraction systemdeveloped for the ACE evaluation, a multi-siteevaluation of information extraction organized bythe U.S. Government.
Following ACEterminology, we will use the term mention to referto a name or noun phrase of one of the types ofinterest, and the term entity for a set of coreferringmentions.
We briefly describe in this section thebaseline Chinese named entity tagger, as well asthe coreference system, used in our experiments.2.1 Chinese Name TaggerOur baseline name tagger consists of an HMMtagger augmented with a set of post-processingrules.
The HMM tagger generally follows theNYMBLE model (Bikel et al 1997), but with alarger number of states (12) to handle nameprefixes and suffixes, and transliterated foreignnames separately.
It operates on the output of aword segmenter from Tsinghua University.
It usesa trigram model with dynamic backoff.
The post-processing rules correct some omissions andsystematic errors using name lists (for example, alist of all Chinese last names; lists of organizationand location suffixes) and particular contextualpatterns (for example, verbs occurring withpeople?s names).
They also deal withabbreviations and nested organization names.2.2 Chinese Coreference ResolverFor this study we have used a rule-basedcoreference resolver.
Table 1 lists the main rulesand patterns used.
We have extensive rules forname-name coreference, including rules specific tothe particular name types.
For these experiments,we do not attempt to resolve pronouns, and weonly resolve names with nominals when the nameand nominal appear in close proximity in a specificstructure, as listed in Table 1.We have used the MUC coreference scoringmetric (Vilain et al 1995) to evaluate this resolver,excluding all pronouns and limiting ourselves tonoun phrases of semantic type PER, ORG, andGPE.
Using a perfect (hand-generated) set ofmentions, we obtain a recall of 82.7% andprecision of 95.1%, for an F score of 88.47%.2This class is used in the U.S. Government?s ACEevaluations;  it excludes locations without governments,such as bodies of water and mountains.Using the mentions generated by our extractionsystem, we obtain a recall of 74.3%, a precision of84.5%, and an F score of 79.07%.33 Confidence MeasuresIn order to decide when we need to rely onglobal (coreference) information for name tagging,we want to have some assessment of theconfidence that the name tagger has in individualtagging decisions.
In this paper, we use two toolsto reach this goal.
The first method is to use threemanually built proper name lists which includecommon names of each type (selected from thehigh frequency names in the user query blog ofCOMPASS, a Chinese search engine, and namelists provided by Linguistic Data Consortium; thePER list includes 147 names, the GPE list 226names, and the ORG list 130 names).
Names onthese lists are accepted without further review.The second method is to have the HMM taggercompute a probability margin for the identificationof a particular name as being of a particular type.Scheffer et al (2001) used a similar method toidentify good candidates for tagging in an activelearner.
During decoding, the HMM tagger seeksthe path of maximal probability through the Viterbilattice.
Suppose we wish to evaluate theconfidence with which words wi, ?, wj areidentified as a name of type T.  We computeMargin (wi,?, wj; T) =  log P1 ?
log P2Here P1 is the maximum path probability and P2 isthe maximum probability among all paths forwhich some word in wi, ?, wj is assigned a tagother than T.A large margin indicates greater confidence inthe tag assignment.
If we exclude names taggedwith a margin below a threshold, we can increasethe precision of name tagging at some cost in recall.Figure 1 shows the trade-off between marginthreshold and name recognition performance.Names with a margin over 3.0 are accepted on thisbasis.3In our scoring, we use the ACE keys and only scorementions which appear in both the key and systemresponse.
This therefore includes only mentionsidentified as being in the ACE semantic categories byboth the key and the system response.
Thus thesescores cannot be directly compared against coreferencescores involving all noun phrases.85878991939597990 1 2 3 4 5 6 7 8 9 10 11 12ThresholdPrecision(%)Figure 1: Tradeoff between Margin Threshold andname recognition performance4   Distribution of Name ErrorsWe consider now names which did not pass theconfidence measure tests: names not on thecommon name list, which were tagged with amargin below the threshold.
We counted theaccuracy of these ?obscure?
names as a function ofthe number of mentions in an entity; the results areshown in Table 2.The table shows that the accuracy of namerecognition increases as the entity includes morementions.
In other words, if a name has morecoref-ed mentions, it is more likely to be correct.This also provides us a linguistic intuition: ifpeople mention an obscure name in a text, theytend to emphasize it later by repeating the samename or describe it with nominal mentions.The table also indicates that the accuracy ofsingle name entities (singletons) is much lowerthan the overall accuracy.
So, although theyconstitute only about 10% of all names, increasingtheir accuracy can significantly improve overallperformance.
Coreference information can play agreat role here.
Take the 157 PER singletons as anexample; 56% are incorrect names.
Among theseincorrect names, 73% actually belong to the othertwo name types.
Many of these can be easily fixedby searching for coreference to other mentionswithout type restriction.
Among the correct names,71% can be confirmed by the presence of a titleword or a Chinese last name.
From theseobservations we can conclude that without strongconfirmation features, singletons are much lesslikely to be correct names.5 Incorporating Coreference Informationinto Name RecognitionWe make use of several features of thecoreference relations a name is involved in; thefeatures are listed in Table 3.
Using these features,we built an independent classifier to predict if aname identified by the baseline name tagger iscorrect or not.
(Note that this classifier is trainedon all name mentions, but during test only?obscure?
names which failed the tests in section 3are processed by this classifier.)
Each namecorresponds to a feature vector which consists ofthe factors described in Table 3.
The PER contextwords are generated from the context patternsdescribed in  (Ji and Luo, 2001).
We used aSupport Vector Machine to implement theclassifier, because of its state-of-the-artperformance and good generalization ability.
Weused a polynomial kernel of degree 3.6 Name Rules based on CoreferenceBesides the factors in the above statistical model,additional coreference information can be used tofilter and in some cases correct the taggingproduced by the HMM.
We developed thefollowing rules to correct names generated by thebaseline tagger.6.1 Name Structure ErrorsSometimes the Name tagger outputs nameswhich are too short (incomplete) or too long.
Wecan make use of the relation among mentions inthe same entity to fix them.
For example, nestedORGs are traditionally difficult to recognizecorrectly.
Errors in ORG names can take thefollowing forms:(1) Head Missed.
Examples: ???????
?/Chinese Art (Group)?, ???????
?/ ChineseStudent (Union)?, ?????????
?/ RussianNuclear Power (Instituition)?Rule 1: If an ORG name x is coref-ed with othermentions with head y (an ORG suffix), and in theoriginal text x is immediately followed by y, thentag xy instead of x; otherwise discard x.
(2) Modifier Missed.
Rule 1 can also be used torestore missed modifiers.
For example, ????????
/ (Edinburgh) University?
; ?????????
/ (Peng Cheng) Limited Corporation?, andsome incomplete translated PER names such as???????
/ (Pa)lestine?.
(3) Name Too LongRule 2: If a name x has no coref-ed mentionsbut part of it, x', is identical to a name in anotherentity y, and y includes at least two mentions; thentag x' instead of x..Rule Type Rule DescriptionIdent(i, j) Mentioni and Mentionj are identicalAbbrev(i, j) Mentioni is an abbreviation of MentionjModifier(i, j) Mentionj = Modifier + ?de?
+ MentioniAllFormal(i, j) Formal and informal ways of referring to the same entity(Ex.
??????
/ American Defense Dept.
&???
?/ Pentagon?
)Substring(i, j) Mentioni is a substring of MentionjPER Title(i, j) Mentionj = Mentioni + title word; orMentionj = LastName + title wordORG Head(i, j) Mentioni and Mentionj have the same headHead(i, j) Mentioni and Mentionj have the same headCapital(i, j) Mentioni: country name;Mentionj: name of the capital of this countryApplied in restricted context.Name &NameGPECountry(i, j) Mentioni and Mentionj are different names referring to the samecountry.(Ex.
???
/ China & ??
/ Huaxia & ???
/ Republic?
)RSub(i, j) Namei is a right substring of NominaljApposition(i, j) Nominalj is the apposite of NameiAllModifier2(i, j) Nominalj = Determiner/Modifier + Namei/ headName &NominalGPERef(i, j) Nominalj = Namei + GPE Ref Word(examples of GPE Ref Word: ???
/ Side?, ???/Government?,????
/ Republic?, ????
?/ Municipality?
)IdentN(i, j) Nominali and Nominalj are identical Nominal&NominalAllModifier3(i, j) Nominalj = Determiner/Modifier + NominaliTable1: Main rules used in the Coreference ResolverNumber of mentionsper entityName Type12 3 4 5 6 7 8 >8PER 43.94 87.07 91.23 87.95 91.57 91.92 94.74 92.31 97.36GPE 55.81 88.8 96.07 100 100 100 100 95.83 97.46ORG 64.71 80.59 89.47 94.29 100 100 -- -- 100Table 2 Accuracy(%) of ?obscure?
name recognitionFactor DescriptionCoreference TypeWeightAverage of weights of coreference relations for which this mentionis antecedent:  0.8 for name-name coreference; 0.5 for apposition;0.3 for other name-nominal coreferenceFirstMentionIs first name mention in the entityHead Includes head word of nameIdiom Name is part of an idiomPER context For PER Name, has context word in textPER title For PER Name, includes title wordMentionWeightORG suffix For ORG Name, includes suffix wordEntity Weight Number of mentions in entity / total number of mentions in allentities in document which include a name mentionTable 3 Coreference factors for name recognition6.2 Name Type ErrorsSome names are mistakenly recognized as othername types.
For example, the name tagger hasdifficulty in distinguishing transliterated PERname and transliterated GPE names.To solve this problem we designed thefollowing rules based on the relation amongentities.Rule 3: If namei is recognized as type1, theentity it belongs to has only one mention; andnamej is recognized as type2, the entity it belongsto has at least two mentions; and namei is identicalwith namej or namei is a substring of namej, thencorrect type1 to type2.For example, if ?????
/ Kremlin?
ismistakenly identified as PER, while ?????
?/ Kremlin Palace?
is correctly identified as ORG,and in coreference results, ?????
/ Kremlin?belongs to a singleton entity, while ??????
/Kremlin Palace?
has coref-ed mentions, then wecorrect the type of ?????
/ Kremlin?
to ORG.Another common mistake gives rise to thesequence ?PER+title+PER?, because our nametagger uses the title word as an important contextfeature for a person name (either preceding orfollowing the title).
But this is an impossiblestructure in Chinese.
We can also use coreferenceinformation to fix it.Rule 4: If ?PER+title+PER?
appears in thename tagger?s output,  then we discard the PERname with lower coref certainty; and checkwhether it is coref-ed to other mentions in a GPEentity or ORG entity; if it is, correct the type.Using this rule we can correctly identify ?[????
/ Sri Lanka GPE] ??
/ Premier [?????
/ Bandaranaike PER]?, instead of ?[????
/Sri Lanka PER] ??
/ Premier [?????
/Bandaranaike PER]?.6.3 Name Abbreviation ErrorsName abbreviations are difficult to recognizecorrectly due to a lack of training data.
Usuallypeople adopt a separate list of abbreviations ordesign separate rules (Sun et al 2002) to identifythem.
But many wrong abbreviation names mightbe produced.
We find that coreferenceinformation helps to select abbreviations.Rule 5: If an abbreviation name has no coref-edmentions and it is not adjacent to anotherabbreviation (ex.
?
?/China ?
/America?
), thenwe discard it.7 System FlowCombining all the methods presented above, theflow of our final system is shown in Figure 2:Figure 2  System Flow8 Experiments8.1 Training and Test DataFor our experiments, we used the BeijingUniversity Insititute of Computational Linguisticscorpus ?
2978 documents from the People?s Dailyin 1998, one million words with name tags ?
andthe training corpus for  the 2003 ACE evaluation,223 documents.
153 of our ACE documents wereused as our test set.
4   The 153 documentscontained 1614 names.
Of the system-taggednames, 959 were considered ?obscure?
:  were noton a name list and had a margin below thethreshold.
These were the names to which therules and classifier were applied.
We ran all thefollowing experiments using the MUC scorer.4The test set was divided into two parts, of 95documents and 58 documents.
We trained two nametagger and classifier models, each time using one partof the test set alng with all the other documents, andevaluated on the other part of the test set.
The resultsreported here are the combined results for the entiretest set.InputNametaggerNominalmentiontaggerCoreferenceResolverCoreferenceRules to fix  nameerrorsSVM classifier to selectcorrect names usingcoreference featuresOutput8.2 Overall Performance ComparisonTable 4 shows the performance of the baselinesystem; Table 5 the system with rule-basedcorrections; and Table 6 the system with bothrules and the SVM classifier.Name Precision Recall FPER 90.9 88.2 89.5GPE 82.3 90.8 86.3ORG 92.1 91.8 91.9ALL 87.8 90.5 89.1Table 4 Baseline Name TaggerName Precision Recall FPER 93.3 87.5 90.3GPE 83.5 90.4 86.8ORG 90.9 92.1 91.5ALL 88.5 90.3 89.4Table 5 Results with Coref Rules AloneName Precision Recall FPER 95.7 84.4 89.7GPE 88.0 91.7 89.8ORG 94.5 91.2 92.8ALL 92.2 89.6 90.9Table 6 Results for Single Document SystemThe gains we observed from coreference withinsingle documents suggested that furtherimprovement might be possible by gatheringevidence from several related documents.
5  Wedid this in two stages.
First, we clustered the 153documents in the test set into 38 topical clusters.Most (29) of the clusters had only two documents;the largest had 28 documents.
We then appliedthe same procedures, treating the entire cluster asa single document.
This yielded another 1.0%improvement in overall F score (Table 7).The improvement in F score was consistent forthe larger clusters (3 or more documents):  the Fscore improved for 8 of those clusters andremained the same for the 9th.
To heighten themulti-document benefit, we took 11 of the small5Borthwick (1999) did use some cross-documentinformation across the entire test corpus, maintainingin effect a name cache for the corpus, in addition to onefor the document.
No attempt was made to select orcluster documents.
(2 document clusters) and enlarged them byretrieving related documents fromsina.com.cn.
In total, we added 52 texts tothese 11 clusters.
The net result was a furtherimprovement of 0.3% in F score (Table 8).6Name Precision Recall FPER 93.3 86.8 90.5GPE 95.2 90.0 92.5ORG 92.9 91.7 92.3ALL 93.8 90.1 91.9Table 7 Results for Mutiple Document SystemName Precision Recall FPER 94.7 87.1 90.7GPE 95.6 89.6 92.5ORG 95.8 90.3 93.0ALL 95.4 89.2 92.2Table 8 Results for Mutiple Document Systemwith additional retrieved texts8.3 Contribution of Coreference FeaturesSince feature selection is crucial to SVMs, wedid experiments to determine how precisionincreased as each feature was added.
The resultsare shown in Figure 3.
We can see that eachfeature in the SVM helps to select correct namesfrom the output of the baseline name tagger,although some (like FirstMention) are morecrucial than others.87888990919293B aselineEdgeTyp eFirstMention HeadId iomPerC ontex tPerTitleOrgSuffixEntityWeightFeaturePrecision(%)Figure 3  Contributions of features6Scores are still computed on the 153 testdocuments ;  the retrieved documents are excludedfrom the scoring.8.4 Comparison to Cache ModelSome named entity systems use a name cache,in which tokens or complete names which havebeen previously assigned a tag are available asfeatures in tagging the remainder of a document.Other systems have made a second tagging passwhich uses information on token sequencestagged in the first pass (Borthwick 1999), or haveused as features information about featuresassigned to other instances of the same token(Chieu and Ng 2002).
Our system, while morecomplex, makes use of a richer set of globalfeatures, involving the detailed structure ofindividual mentions, and in particular makes useof both name ?
name and name ?
nominalrelations.We have compared the performance of ourmethod (applied to single documents) with avoted cache model, which takes into account thenumber of times a particular name has beenpreviously assigned each type of tag:System Precision Recall Fbaseline 88.8 90.5 89.1voted cache 87.6 92.8 90.1current 92.2 89.6 90.9Table 9.
Comparison with voted cacheCompared to a simple voted cache model, ourmodel provides a greater improvement in namerecognition F score; in particular, it cansubstantially increase the precision of namerecognition.
The voted cache model can recoversome missed names, but at some loss in precision.9 Conclusions and Future WorkIn this paper, we presented a novel idea ofapplying coreference information to improvename recognition.
We used both a statistical filterbased on a set of coreference features and rulesfor correcting specific errors in name recognition.Overall, we obtained an absolute improvement of3.1% in F score.
Put another way, we were ableto eliminate about 60% of erroneous name tagswith only a small loss in recall.The methods were tested on a Chinese nametagger, but most of the techniques should beapplicable to other languages.
More generally, itoffers an example of using global and cross-document information to improve local decisionsfor information extraction.
Such methods will beimportant for breaking the ?performance ceiling?in many areas of information extraction.In the future, we plan to experiment withimprovements in coreference resolution (inparticular, adding pronoun resolution) to see if wecan obtain further gains in name recognition.
Wealso intend to explore the production of multipletagging hypotheses by our statistical name tagger,with the alternative hypotheses then rerankedusing global information.
This may allow us toreplace some of our hand-coded error-correctionrules with corpus-trained methods.10 AcknowledgementsThis research was supported by the DefenseAdvanced Research Projects Agency as part of theTranslingual Information Detection, Extractionand Summarization (TIDES) program, underGrant N66001-001-1-8917 from the Space andNaval Warfare Systems Center San Diego, and bythe National Science Foundation under GrantsIIS-0081962 and 0325657.
This paper does notnecessarily reflect the position or the policy of theU.S.
Government.ReferencesDaniel M. Bikel, Scott Miller, Richard Schuartz,and Ralph Weischedel.
1999.
Nymble: a high-performance Learning Name-finder.
Proc.
FifthConf.
On Applied Natural Language Processing,Washington, D.C.Andrew Borthwick.
1999.
A Maximum EntropyApproach to Named Entity Recognition.
Ph.D.Dissertation, Dept.
of Computer Science, NewYork University.Andrew Borthwick, John Sterling, EugeneAgichtein, and Ralph Grishman.
1998.Exploiting Diverse Knowledge Sources viaMaximum Entropy in Named EntityRecognition.
Proc.
Sixth Workshop on VeryLarge Corpora, Montreal, Canada.Hai Leong Chieu and Hwee Tou Ng.
2002.Named Entity Recognition: A MaximumEntropy Approach Using Global Information.Proc.
: 17th Int?l Conf.
on ComputationalLinguistics (COLING 2002), Taipei, Taiwan.Ralph Grishman and Beth Sundheim.
1996.Message understanding conference - 6: A briefhistory.
Proc.
16th Int?l Conference onComputational Linguistics (COLING 96),Copenhagen.Heng Ji, Zhensheng Luo, 2001.
A Chinese NameIdentifying System Based on Inverse NameFrequency Model and Rules.
Natural LanguageProcessing and Knowledge Engineering(NLPKE) Mini Symposium of 2001 IEEEInternational Conference on Systems, Man, andCybernetics (SMC2001)Andrew McCallum and Wei Li.
2003.
Earlyresults for Named Entity Recognition WithConditional Random Fields, Feature Induction,and Web-Enhanced Lexicons.
Proc.
SeventhConf.
on Computational Natural LanguageLearning (CONLL-2003), Edmonton, Canada.Tobias Scheffer, Christian Decomain, and StefanWrobel.
2001.
Active Hidden Markov Modelsfor Information Extraction.
Proc.
Int?lSymposium on Intelligent Data Analysis (IDA-2001).Satoshi Sekine, Ralph Grishman and HiroyukiShinnou.
1998.
A Decision Tree Method forFinding and Classifying Names in JapaneseTexts.
Proc.
Sixth Workshop on Very LargeCorpora; Montreal, Canada.Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhouand Changning Huang.
2002.
Chinese NamedEntity Identification Using Class-basedLanguage Model.
Coling 2002.Marc Vilain, John Burger, John Aberdeen, DennisConnelly, Lynette Hirschman.
1995.
A model --Theoretic Coreference Scoring Scheme.
MUC-6Proceedings, Nov. 1995.Shiren Ye, Tat-Seng Chua, Liu Jimin.
2002.
AnAgent-based Approach to Chinese NamedEntity Recognition.
Coling 2002.
