Towards User-Adaptive Annotation GuidelinesStefanie Dipper, Michael Go?tze, Stavros SkopeteasDept.
of LinguisticsD-14415 Potsdam, Germany{dipper,goetze}@ling.uni-potsdam.deskopetea@rz.uni-potsdam.deAbstractIn this paper we address the issue of user-adaptivity for annotation guidelines.
We showthat different user groups have different needstowards these documents, a fact neglected bymost of current annotation guidelines.
We pro-pose a formal specification of the structure ofannotation guidelines, thus suggesting a mini-mum set of requirements that guidelines shouldfulfill.
Finally, we sketch the use of these speci-fications by exemplary applications, resulting inuser-specific guideline representations.1 IntroductionLinguistic research nowadays makes heavy use ofannotated corpora.
The benefit that researchers maygain from corpora depends to a large extent on doc-umentation of the annotation.
According to Leech?smaxims, the guidelines that were applied in the an-notation of the corpus should be accessible to theuser of the corpus (and thus serve as a kind of doc-umentation), see Leech (1993).1In this paper, we argue that annotation guidelines,which are optimized for use by the annotators of thecorpus, often cannot serve as suitable documenta-tion for users of the annotated corpus.
We illustratethis claim by different types of prototypical corpususers, who have different needs with respect to doc-umentation.
Extending the proposal by MATE (Dy-bkjaer et al, 1998), we sketch a preliminary specifi-cation for annotation guidelines.
We then show howguidelines that are standardized in this way may beadapted to different user needs and serve both asguidelines, applied in the annotation process, anddocumentation, used by different corpus users.1?The annotation scheme should be based on guide-lines which are available to the end user.
Most corporahave a manual which contains full details of the annota-tion scheme and guidelines issued to the annotators.
Thisenables the user to understand fully what each instance ofannotation represents without resorting to guesswork, andto understand in cases of ambiguity why a particular an-notation decision was made at that point.
?, Leech (1993),cited by http://www.ling.lancs.ac.uk/monkey/ihe/linguistics/corpus2/2maxims.htm.This paper grew out of our work in the Son-derforschungsbereich (SFB, collaborative researchcenter) on information structure at the University ofPotsdam.2 In the context of this SFB, several indi-vidual projects collect a large amount of data of di-verse languages and annotate them on various anno-tation levels: phonetics/phonology, morpho-syntax,semantics, and information structure.Within the SFB, guidelines for the different anno-tation levels are being created.
In order to maximizethe profit of these data, we are developing standardrecommendations on the format and content of theSFB annotation guidelines.
These guidelines oughtto serve the SFB annotators as well as the researchcommunity.The paper is organized as follows.
We firstpresent different user profiles with different needstowards annotation guidelines (sec.
2).
We then ana-lyze the form and content of selected existing guide-lines to some detail (sec.
3) and show that theseguidelines fulfill the user needs only inadequately(sec.
4).
Finally, we sketch a formal specification ofthe structure of annotation guidelines and indicatehow XML/XSLT technology can be used to supportuser-adaptive annotation guidelines (sec.
5).2 Guideline UsersAnnotation guidelines are used by different types ofusers with different requirements.
These require-ments depend on (i) the user?s objectives and (ii)the user?s background.2.1 User ObjectivesPeople are interested in annotation guidelines fordifferent reasons.
According to their respective ob-jectives, we define five user profiles.3The annotator Annotators assign linguistic fea-tures to language data, according to criteria and2http://www.ling.uni-potsdam.de/sfb/3In a similar way, Carletta and Isard (1999) define threeuser types: the coder, the coding consumer, and the coding de-veloper.
These classes, however, refer to users of annotationworkbenches rather than annotation guidelines.instructions specified in the annotation guidelines.Important annotation criteria are consistency andspeed.The corpus explorer The group of corpus explor-ers encompasses all those who aim at exploiting lin-guistic data in order to find evidence for or againstlinguistic hypotheses.
These people need to know(i) how to find instances of specific phenomena theyare interested in, and (ii) how to interpret the anno-tations of the phenomena in question.The language engineer Instead of inspecting thedata ?manually?, as the corpus explorer does, thelanguage engineer applies automatic methods to theannotated data to process them further.
This in-cludes a variety of tasks, such as statistical evalu-ations, training and testing of algorithms, and theextraction of various types of linguistic information.The guideline explorer The guidelines per se(i.e., independently of a corpus) are of interest to,e.g., theoretical linguists who want to know theprinciples that underlie the annotation guidelines.
Inaddition, the guidelines may serve as an example forauthors of other annotation guidelines.The guideline author The process of writingguidelines is usually a time-consuming and step-wise process.
Hence, during the process of writ-ing, the authors themselves make use of their ownguidelines to look up related or similar phenomenathat are already covered therein.2.2 User BackgroundA further factor putting constraints on annotationguidelines is the user?s background.
First, (non-)acquaintance with the language of the corpus is animportant factor: if corpora should be useful alsofor people who do not or hardly know the languageof the corpus, annotation guidelines should providetranslations for example sentences and basic infor-mation about linguistic properties of the object lan-guage.Second, (non-)acquaintance with theoreticalanalyses of the phenomena has an impact on re-quirements towards guidelines.
People who are ac-quainted with the linguistic theory that the guide-lines are based on do not need theoretical introduc-tions; an example is the Feldertheorie (field theoryof word order) in German, which serves as the basisof the analyses in the German Verbmobil Treebank(Stegmann et al, 2000).
In addition, people whoknow about alternative (competing) analyses of thephenomena in question may want to know the rea-sons of the chosen analysis.3 Form and Content of GuidelinesWe consider sample guidelines from different typesof annotation; all sample guidelines are availablevia the internet.
These guidelines have been cho-sen to set out the diversity among different lev-els of linguistic analysis?from morphology topragmatics?and among practices established indifferent linguists?
communities?from typologiststo language engineers.4Interlinear morphemic transcription EU-ROTYP (Ko?nig et al, 1993), Leipzig GlossingRules (Bickel et al, 2004).
These guidelines dealwith the annotation of morpheme boundaries andmorpheme-by-morpheme translation (glossing);these guidelines have been created by and fortypologists.5Morphosyntactic annotation Penn Treebank(POS-tagging guidelines, ?POS?)
(Santorini,1995), STTS (Schiller et al, 1999).
These guide-lines have been developed by language engineersfor (semi-)automatic annotation of morphosyntacticinformation.6Syntactic annotation Penn Treebank (bracketingguidelines, ?BG?)
(Bies et al, 1995), SPARKLE(Carroll et al, 1997), VerbMobil, German Treebank(Stegmann et al, 2000).7Semantic/pragmatic annotation PropBank(PropBank Project, 2002), Penn Discourse Tree-bank (Mitsakaki et al, 2004), DAMSL (DialogAct Markup in Several Layers, Allen and Core(1997)).
PropBank and Penn Discourse Treebankare extensions of the Penn Treebank.We focus on three aspects of annotation guide-lines: the components of guideline documents4The sample guidelines also vary with regard to size (e.g.,the Leipzig Glossing Rules comprise 9 pages, the Penn Tree-bank Bracketing Guidelines 317 pages) and status (e.g., theVerbMobil guidelines are completed, whereas guidelines suchas the Penn Discourse Treebank guidelines are still being de-veloped).5We consider only the rules for morphemic transcriptionand not the glossing abbreviations in these documents.6EAGLES provides recommendations for the design ofmorphosyntactic tagsets (Leech and Wilson, 1996).
Tagsetsrepresent only a component of annotation guidelines.
TheSTTS tagset can be viewed as an instantiation of the EAGLESrecommendations.7A very detailed annotation scheme for syntactic, semanticand speech annotation is available in book form for the SU-SANNE corpus (Sampson, 1995).
These guidelines are ad-dressed primarily to the guideline explorer rather than the an-notator.
In this vein, the book provides a detailed discussionof the annotation principles and theoretical background.
We donot include these guideline in our discussion, since they are notavailable electronically.A B C D E F G H I J K L M N O PEUROTYP + + + + + +Leipzig Glossing Rules + + + + + + + +Penn Treebank (POS) + + + + + + + +STTS + + + + + + +Penn Treebank (BG) + + + + + + + + +VerbMobil Treebank + + + + + + + + +SPARKLE + + + + + + + +PropBank + + + + + + + +Penn Discourse Treebank + + + + + +DAMSL + + + + + +Document components:A general principlesB underlying linguistic theoryC tagset declarationD related annotation schemesE tag indexF keyword indexInstruction components:G keywordsH criteriaI examplesJ related instructionsK alternative analysesInstruction ordering:L alphabetical tagsM alphabetical keywordsN content-based structureO default?specific/exceptionalP simple?difficultFigure 1: Features of the sample guidelines(sec.
3.1), the components of an annotation instruc-tion (sec.
3.2), and the ordering of instructions withrespect to each other (sec.
3.3).3.1 Document ComponentsThe document architecture varies to some extent inthe sample guidelines.
In general, however, there is(i) an introductory part, (ii) the main section, and(iii) appendices.
In the following, we sketch pro-totypical components of these parts; to a large ex-tent, these components overlap with the elementsproposed by Dybkjaer et al (1998).
The table infig.
1 presents an overview of most of the guide-line components considered here.
The differencesbetween the guidelines can (partly) be attributed tothe fact that the guidelines address different types ofusers.Introductory part This part comprises basic in-formation such as the name of the guidelines, theannotation goal, the type of source data, the anno-tation markup (e.g., syntactic annotation can be en-coded by brackets vs. graphs, etc.).
In addition, itaddresses general design principles, including gen-eral annotation conventions (A8), and the underly-ing linguistic theory and/or statements about theo-retical problems (B).
A general tagset declarationin the form of an exhaustive list of all admissibletags plus a short description is often included (C).Some guidelines refer to related annotation schemesor standard recommendations like EAGLES (Leechand Wilson, 1996) (D).
Finally, creation notes in-form about the authors, creation date, status of theguidelines, etc.8The letters refer to the table in fig.
1.Main section This section is always devoted tothe presentation of the actual annotation guidelines,which we call ?
(annotation) instructions?.
Thesewill be discussed in detail in sec.
3.2 and 3.3.Appendices Some guidelines provide tutorials inthe form of exercises for practicing the use of the an-notation guidelines.
Different types of indices (i.e.,listings of items, e.g.
tags, and numbers of all pagesthat refer to these items) may be included: alpha-betical index of the tags (E); thematic indices, e.g.an index of keywords such as ?wh-clefts?
(F).
In ad-dition, lists of specific problematic words or con-structions may be given.
Finally, some guidelinesinclude recommendations for annotation tools andmethods.3.2 Instruction ComponentsThe core component of annotation guidelines is rep-resented by the annotation instructions.
We first de-scribe the form and content of an individual instruc-tion before addressing the question of how the setof instructions is ordered/structured (sec.
3.3).
Weillustrate the description by two annotation instruc-tions from the Penn Treebank (POS), displayed infig.
2.An individual instruction always refers to one (ormore) tags that represent the information to be an-notated, e.g., ?VB?.
The instruction usually providessome sort of keywords (G) for the phenomenon inquestion, e.g., ?verb, base form?
(e.g., headers mayprovide such keywords).
The guidelines in the sam-ple include annotation criteria (H) in the form ofa descriptive text (?This tag subsumes .
.
.
?)
andsome illustrative examples (I) (?Do/VB it.?).
Some-Verb, base form?VBThis tag subsumes imperatives, infinitives and subjunctives.EXAMPLES: Imperative: Do/VB it.Infinitive: You should do/VB it.
[.
.
.
]Subjunctive: We suggested that he do/VB it.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.VB or VBPIf you are unsure whether a form is subjunctive (VB) or a present tense verb (VBP), replace the subject by athird person pronoun.
If the verb takes an -s ending, then the original form is a present tense verb (VBP); ifnot, it is a subjunctive (VB).EXAMPLE: I recommended that you do/VB it.(cf.
I recommended that he do/*does it.
)Figure 2: Two instructions from the Penn Treebank POS-tagging guidelines (Santorini, 1995, pp.
5, 21)times, the guidelines also specify how to segmentthe source data.Often, the instructions make reference to other,closely related instructions (whose annotation crite-ria are similar to the current criteria) and emphasizethe differences between them (?If you are unsurewhether .
.
.
?)
(J).
Finally, alternative (competing)analyses may be given (K).93.3 Instruction OrderingGuidelines present annotation instructions in a cer-tain order.
The ordering of instructions is a crucialaspect of the instructions?
presentation: different or-dering principles implement different perspectivesto the guidelines and, consequently, serve require-ments of different groups of users (cf.
sec.
4).The sample guidelines make use of the followingordering principles:Alphabetical order of the tags (L) In the sec-tion on problematic cases, the Penn Treebank (POS)present the tags and their instructions in an alpha-betical order (from ?CC?
to ?WDT?).
(Other guide-lines make use of this type of ordering in an addi-tional tag index.
)Alphabetical order of keywords (M) Canonicalcases in the Penn Treebank (POS) are ordered al-phabetically with respect to keywords (from ?Adjec-tive?
to ?Wh-adverb?
).Content-based structure (N) Instructions are of-ten presented in thematic units, e.g.
all tags encod-ing nominal features are grouped together.
More-over, complex annotation guidelines are usually or-ganized in an hierarchical structure, with chapters,sections, etc., which mirror the complex structure of9The guidelines considered here at most allude implicitelyto alternative analyses: by giving arguments in favour of thechosen analaysis.the described phenomena.
For instance, the Verb-Mobil guidelines contain a chapter about the anno-tation of phrasal constituents, with sections address-ing NPs, PPs, etc., and PP subsections addressingprepositions and circum/postpositions.
In DAMSL,criteria in the form of decision trees guide the anno-tator through the annotation.From default to specific/exceptional cases (O)This is an ordering principle that is usually usedin combination with other principles.
For instance,single sentences are presented before multiple sen-tences in the guidelines of the Penn Discourse Tree-bank.Degree of difficulty (P) Similarly, in combina-tion with other ordering principles, the guidelinesoften proceed from easy to difficult cases.
For in-stance, the Leipzig Glossing Rules first introducemorphemic transcription of prefixes and suffixes.Only later are infixes and circumfixes addressed;these represent a problematic case for interlinearmorphemic translations due to the lack of isomor-phism between the layer of transcription and thelayer of translation.Usually, guidelines make use of several orderingprinciples, e.g., main instructions are structured ac-cording to content, (embedded) subinstructions areordered from default to specific case, and indices areordered alphabetically according to keywords.4 User RequirementsCurrent annotation projects usually do not provideseparate guideline documents for different types ofusers.
Usually, annotation documentation emergesfrom the annotating practice, supporting the an-notator in the annotation task.
At the publish-ing stage, this documentation is often transferredinto a more general document, by adding informa-tion about annotation conventions, format, methods,etc.
?however, the basic structure of the annotationsinstructions remains unaltered.
The obvious conse-quence of this practice is that existing guidelines of-ten ignore the requirements of certain types of users.We illustrate different user requirements by sometypical examples.
These requirements concern (i)document components, (ii) instruction components,and (iii) instruction ordering:Annotator Typical users are annotators who areconfronted with the guidelines for the first time.
(i) Annotators primarily need a tutorial introductionand maybe information about the annotation goals.
(ii) They have to learn specific instructions, sup-ported by didactic examples.
(iii) The appropriate order is from default to excep-tional or from easy to difficult.
Orderings in theform of decision trees may facilitate the acquisition.Corpus explorer A further sample user is a re-searcher who looks for a specific phenomenon inthe corpus:(i) Corpus explorers need an index of phenomena(keywords) to look up the tags that encode the phe-nomenon they are interested in.
Moreover, wheninspecting the encoding of this phenomenon, theymight come across other tags they are not yet famil-iar with.
Hence, they also need an index of tags (ora tagset declaration) to look up the meaning of thesetags.
(ii) They need detailed information about the anno-tation criteria.
Take, for instance, a corpus that isannotated with respect to information-structural cat-egories and imagine a corpus explorer who is inter-ested in topic and focus.
Before looking for data,s/he has to know the exact definitions (criteria) oftopic and focus that have been applied in the anno-tation.
(iii) The easiest way for the corpus explorer to findannotation criteria of phenomena and tags is bymeans of an alphabetic ordering.Language engineer Finally, language engineersmay undertake a statistical evaluation of the corpusdata:(i) They primarily need a tagset declaration, with-out being interested in any details.
In addition, thecircumstances of the annotation are relevant (e.g.,whether the corpus has been annotated manually,twice, etc.).
(ii), (iii) Probably, the language engineer would notneed any information about annotation instructions.Comparing these user requirements and theguideline features in fig.
1, we see that the guide-lines are more oriented towards the annotator thanthe corpus explorer: Features such as indices (E, F)are often missing, whereas the predominant instruc-tion ordering is content-based ordering (N).5 Towards User-Adaptive AnnotationGuidelinesIn what follows, we present a preliminary guidelinespecification that allows for generating user-adaptedguideline representations.
In the second part, weillustrate the applicability of the specification.5.1 Guideline SpecificationFor the specification of user-adaptive guidelines weadopt ideas from the MATE Markup Framework(Dybkjaer et al, 1998), which uses so-called Cod-ing Modules for the specification and representa-tion of annotation schemes.
Building upon MATE,we define semi-formal class specifications, Guide-line modules, which we extend with an Instructionmodule for the annotation instructions.10 In con-trast to MATE, we understand the Guideline moduleas an underlying specification from which differentrepresentations can be generated.
We sketch howthe guidelines can be encoded by XML, which en-ables the generation of user-adapted representationsthrough stylesheet technology (e.g.
XSLT).The Guideline module The guideline module(see fig.
3) constitutes the basis for the specifica-tion of annotation guidelines.
It includes a subsetof the items in the MATE Coding modules and thedocument components introduced and explained insec.
3.1.
Components that can be derived automati-cally, such as the tagset declaration and indices, arenot part of the specification, since these can be gen-erated from the information present in the Instruc-tion module.The Instruction module Annotation instructionsare specified in the Instruction module.
In fig.
4,we sketch a preliminary XML representation of thetwo instructions in fig.
2.
The single elements andattributes specify the instruction components exem-plified there.
In addition, the instruction for the tag?VB?
refers to the second instruction via the ?re-lated?
element, marking it as a ?problematic case?.The second instruction indeed helps the annotatorto decide between the assignment of two tags, ?VB?and ?VBP?.
For both tags, ?criterion?
elements withapplication conditions, the respective annotation ac-tion, and examples are declared.10The MATE Markup Framework neither addresses the en-coding of annotation guidelines nor the issue of user-adaptivityexplicitly.# Component Example1 Guideline title Part-of-speech tagging guidelines for the Penn Treebank, 3rd Revision2 Annotated information Part of speech3 Type of source data English text4 General principles ...(annotation conventions & format)5 Relation to linguistic theories ...6 Related annotation schemes Bies et al (1995):?Bracketing Guidelines for Treebank II Style?, .
.
.7 Annotation instructions ?
INSTRUCTION MODULE8 Creation notes: authors, status, etc.
Beatrice Santorini, 1995, 3rd revisionFigure 3: The Guideline module<instruction tags="VB" keywords="verb, base form" difficulty="easy"id="instr 1"><text>This tag subsumes imperatives, infinitives and subjunctives</text><criterion><condition>verb in base form</condition><action>label VB</action><example comment="imperative">Do/VB it.</example><example comment="infinitive">You should do/VB it.</example><example comment="subjunctive">We suggested that he do/VB it.</example></criterion><related type="problematic case" ref="instr 23"/></instruction>......................................................................................................<instruction tags="VB, VBP" keywords="verb, subjunctive, present tense"difficulty="medium" id="instr 23"><text>If you are unsure whether a form is a subjunctive (VB) or a presenttense verb (VBP), replace the subject by a third person pronoun.</text><criterion><condition>verb does not take an -s ending</condition><action>label VB</action><example>I recommended that you do/VB it.</example><test>I recommend that he do/*does it.</test></criterion><criterion><condition>verb takes an -s ending</condition><action>label VBP</action></criterion></instruction>Figure 4: Instructions of fig.
2 as specified in the Instruction module5.2 Application ExamplesThe exemplary encoding enables the generation ofa number of various types of user-adapted guidelinerepresentations and document components:?
For all user profiles: The ?tags?
and ?keywords?attributes of the instruction elements in fig.
4 al-low us to automatically generate indices as lists oftag:page-number pairs (resp.
keyword:page-numberpairs) and tagset declarations as tag:keyword pairs.?
For the annotator: The ?difficulty?
attribute can beused as a guiding principle for the creation of tu-torial exercises for the annotator, which might startwith easy annotation examples and develop towardsmore difficult instructions.
Furthermore, when theannotator annotates a certain tag, the annotation toolmay display the corresponding ?text?
element as an?online help?
for the annotator.?
For the guideline author: When the author as-signs keywords to the instruction s/he is currentlyworking on, the ?keywords?
attribute can be usedto point to related instructions (marked by the samekeywords).
The formal specification in general canbe used to support the guideline authors, by com-pleteness and consistency checks.6 ConclusionsCurrent guidelines only provide support for a subsetof the potential users.
As we have shown in this pa-per, different user types, such as annotators, corpusexplorers, language engineers, etc., require differentforms of guidelines in order to fulfill their specifictasks related to an annotated corpus.To answer these requirements, we propose a gen-eral guideline structure which serves as the basis forgeneration of user-adapted documents.
With the useof XML/XSLT technology, a broad variety of user-specific applications can be realized.It is clear that the detailed specification we pro-pose make high demands on the guideline authors.However, forcing the authors to fulfill requirementssuch as explicitness (as for the declaration of theexact annotation action), completeness (keywords,examples for every instruction), etc., will resultin high-quality standardized annotation guidelines,which we believe will pay off in greater benefit fromthe annotated corpora.ReferencesJames Allen and Mark Core.
1997.
DAMSL: Di-alog Act Markup in Several Layers.
Draft;http://www.cs.rochester.edu/research/cisd/resources/damsl/RevisedManual/RevisedManual.html.Balthasar Bickel, Bernard Comrie, and Martin Haspel-math.
2004.
The Leipzig Glossing Rules.
Conven-tions for interlinear morpheme by morpheme glosses.Max Planck Institute for Evolutionary Anthropol-ogy and Department of Linguistics, University ofLeipzig; http://www.eva.mpg.de/lingua/files/morpheme.html.Ann Bies, Mark Ferguson, Karen Katz, and RobertMacIntyre.
1995.
Bracketing Guidelines for Tree-bank II Style, Penn Treebank Project.
Departmentof Computer and Information Science, Universityof Pennsylvania; ftp://ftp.cis.upenn.edu/pub/treebank/doc/manual/.Jean Carletta and Amy Isard.
1999.
The MATE annota-tion workbench: User requirements.
In Proceedingsof the ACL Workshop Towards Standards and Toolsfor Discourse Tagging, University of Maryland.John Carroll, Ted Briscoe, Nicoletta Calzolari, StefanoFederici, Simonetta Montemagni, Vito Pirrelli, GregGrefenstette, Antonio Sanfilippo, Glenn Carroll, andMats Rooth.
1997.
SPARKLE Work Package 1: Spec-ification of Phrasal Parsing.
Final Report, 1997-TR-1; http://dienst.iei.pi.cnr.it/.Laila Dybkjaer, Niels Ole Bernsen, Hans Dybkjaer,David McKelvie, and Andreas Mengel.
1998.
TheMATE Markup Framework.
MATE Deliverable D1.2,http://mate.nis.sdu.dk/information/d12/.Ekkehard Ko?nig, Dik Bakker, ?Oesten Dahl, Mar-tin Haspelmath, Maria Koptjevskaja-Tamm, Chris-tian Lehmann, and Anna Siewierska.
1993.
EU-ROTYP Guidelines.
European Science FoundationProgramme in Language Typology.
http://www-uilots.let.uu.nl/ltrc/eurotyp/.Geoffrey Leech and Andrew Wilson.
1996.
EAGLESrecommendations for the morphosyntactic annotationof corpora.
Technical Report EAG-TCWG-MAC/R,ILC-CNR, Pisa; http://www.ilc.cnr.it/EAGLES96/annotate/annotate.html.Geoffrey Leech.
1993.
Corpus annotation schemes.
Lit-erary and Linguistic Computing, 8(4):275?281.Eleni Mitsakaki, Rashmi Prasad, Aravind Joshi,and Bonnie Weber.
2004.
Penn Discourse Tree-bank: Annotation Tutorial.
Institute for Researchin Cognitive Science, University of Pennsylva-nia; http://www.cis.upenn.edu/?pdtb/dltag-webpage-stuff/pdtb-tutorial.pdf.PropBank Project.
2002.
PropBank Annotation Guide-lines.
Version 3; http://www.cis.upenn.edu/?ace/propbank-guidelines-feb02.pdf.Geoffrey Sampson.
1995.
English for the computer:The SUSANNE corpus and analytic scheme.
Oxford:Clarendon Press.Beatrice Santorini.
1995.
Part-of-Speech TaggingGuidelines for the Penn Treebank Project.
3rdRevision, 2nd printing; ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz;Department of Computer and Information Science,University of Pennsylvania.Anne Schiller, Simone Teufel, Christine Sto?ckert, andChristine Thielen.
1999.
Guidelines fu?r das Taggingdeutscher Korpora mit STTS.
http://www.ims.uni-stuttgart.de/projekte/corplex/TagSets/stts-1999.pdf.Rosmary Stegmann, Heike Telljohann, and ErhardHinrichs.
2000.
Stylebook for the German Treebankin VERBMOBIL.
Technical Report 239, Verbmobil;http://verbmobil.dfki.de/cgi-bin/verbmobil/htbin/decode.cgi/share/VM-depot/FTP-SERVER/vm-reports/report-239-00.ps.
