Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164?171,Sydney, July 2006. c?2006 Association for Computational LinguisticsMultilingual Deep Lexical Acquisition for HPSGs via SupertaggingPhil Blunsom and Timothy BaldwinComputer Science and Software EngineeringUniversity of Melbourne, Victoria 3010 Australia{pcbl,tim}@csse.unimelb.edu.auAbstractWe propose a conditional random field-based method for supertagging, and ap-ply it to the task of learning new lexi-cal items for HPSG-based precision gram-mars of English and Japanese.
Us-ing a pseudo-likelihood approximation weare able to scale our model to hun-dreds of supertags and tens-of-thousandsof training sentences.
We show thatit is possible to achieve start-of-the-artresults for both languages using maxi-mally language-independent lexical fea-tures.
Further, we explore the performanceof the models at the type- and token-level,demonstrating their superior performancewhen compared to a unigram-based base-line and a transformation-based learningapproach.1 IntroductionOver recent years, there has been a resurgence ofinterest in the use of precision grammars in NLPtasks, due to advances in parsing algorithm de-velopment, grammar development tools and rawcomputational power (Oepen et al, 2002b).
Pre-cision grammars are defined as implementedgrammars of natural language which capture fine-grained linguistic distinctions, and are generativein the sense of distinguishing between grammat-ical and ungrammatical inputs (or at least havesome in-built notion of linguistic ?markedness?
).Additional characteristics of precision grammarsare that they are frequently bidirectional, and out-put a rich semantic abstraction for each span-ning parse of the input string.
Examples includeDELPH-IN grammars such as the English ResourceGrammar (Flickinger, 2002; Uszkoreit, 2002), thevarious PARGRAM grammars (Butt et al, 1999),and the Edinburgh CCG parser (Bos et al, 2004).Due to their linguistic complexity, precisiongrammars are generally hand-constructed and thusrestricted in size and coverage.
Attempts to(semi-)automate the process of expanding the cov-erage of precision grammars have focused on ei-ther: (a) constructional coverage, e.g.
in the formof error mining for constructional expansion (vanNoord, 2004; Zhang and Kordoni, 2006), or relax-ation of lexico-grammatical constraints to supportpartial and/or robust parsing (Riezler et al, 2002);or (b) lexical coverage, e.g.
in bootstrapping froma pre-existing grammar and lexicon to learn newlexical items (Baldwin, 2005a).
Our particular in-terest in this paper is in the latter of these two,that is the development of methods for automati-cally expanding the lexical coverage of an existingprecision grammar, or more broadly deep lexicalacquisition (DLA hereafter).
In this, we followBaldwin (2005a) in assuming a semi-mature pre-cision grammar with a fixed inventory of lexicaltypes, based on which we learn new lexical items.For the purposes of this paper, we focus specif-ically on supertagging as the mechanism for hy-pothesising new lexical items.Supertagging can be defined as the process ofapplying a sequential tagger to the task of predict-ing the lexical type(s) associated with each wordin an input string, relative to a given grammar.
Itwas first introduced as a means of reducing parserambiguity by Bangalore and Joshi (1999) in thecontext of the LTAG formalism, and has since beenapplied in a similar context within the CCG for-malism (Clark and Curran, 2004).
In both of thesecases, supertagging provides the means to performa beam search over the plausible lexical items fora given string context, and ideally reduces pars-ing complexity without sacrificing parser accu-racy.
An alternate application of supertagging isin DLA, in postulating novel lexical items withwhich to populate the lexicon of a given gram-mar to boost parser coverage.
This can take place164either: (a) off-line for the purposes of roundingout the coverage of a static lexicon, in which casewe are generally interested in globally maximisingprecision over a given corpus and hence predict-ing the single most plausible lexical type for eachword token (off-line DLA: Baldwin (2005b)); or(b) on the fly for a given input string to temporar-ily expand lexical coverage and achieve a spanningparse, in which case we are interested in maximis-ing recall by producing a (possibly weighted) listof lexical item hypotheses to run past the grammar(on-line DLA: Zhang and Kordoni (2005)).
Ourimmediate interest in this paper is in the first ofthese tasks, although we would ideally like to de-velop an off-line method which is trivially portableto the second task of on-line DLA.In this research, we focus particularly onthe Grammar Matrix-based DELPH-IN family ofgrammars (Bender et al, 2002), which includesgrammars of English, Japanese, Norwegian, Mod-ern Greek, Portuguese and Korean.
The Gram-mar Matrix is a framework for streamlining andstandardising HPSG-based multilingual grammardevelopment.
One property of Grammar Matrix-based grammars is that they are strongly lexical-ist and adhere to a highly constrained lexicon-grammar interface via a unique (terminal) lexi-cal type for each lexical item.
As such, lexicalitem creation in any of the Grammar Matrix-basedgrammars, irrespective of language, consists pre-dominantly of predicting the appropriate lexicaltype for each lexical item, relative to the lexicalhierarchy for the corresponding grammar.
In thissame spirit of standardisation and multilingual-ity, the aim of this research is to develop max-imally language-independent supertagging meth-ods which can be applied to any Grammar Matrix-based grammar with the minimum of effort.
Es-sentially, we hope to provide the grammar engi-neer with the means to semi-automatically popu-late the lexicon of a semi-mature grammar, henceaccelerating the pace of lexicon development andproducing a resource of sufficient coverage to bepractically useful in NLP tasks.The contributions of this paper are the devel-opment of a pseudo-likelihood conditional ran-dom field-based method of supertagging, whichwe then apply to the task of off-line DLA forgrammars of both English and Japanese with onlyminor language-specific adaptation.
We show thesupertagger to outperform previously-proposedsupertagger-based DLA methods.The remainder of this paper is structured asfollows.
Section 2 outlines past work relativeto this research, and Section 3 reviews the re-sources used in our supertagging experiments.Section 4 outlines the proposed supertagger modeland reviews previous research on supertagger-based DLA.
Section 5 then outlines the set-up andresults of our evaluation.2 Past ResearchAccording to Baldwin (2005b), research on DLAfalls into the two categories of in vitro methods,where we leverage a secondary language resourceto generate an abstraction of the words we hope tolearn lexical items for, and in vivo methods, wherethe target resource that we are hoping to performDLA relative to is used directly to perform DLA.Supertagging is an instance of in vivo DLA, as itoperates directly over data tagged with the lexicaltype system for the precision grammar of interest.Research on supertagging which is relevant tothis paper includes the work of Baldwin (2005b) intraining a transformation-based learner over datatagged with ERG lexical types.
We discuss thismethod in detail in Section 5.2 and replicate thismethod over our English data set for direct com-parability with this previous research.As mentioned above, other work on supertag-ging has tended to view it as a means of drivinga beam search to prune the parser search space(Bangalore and Joshi, 1999; Clark and Curran,2004).
In supertagging, token-level annotations(gold-standard, automatically-generated or other-wise) for a given DLR are used to train a se-quential tagger, akin to training a POS tagger overPOS-tagged data taken from the Penn Treebank.One related in vivo approach to DLA targetedspecifically at precision grammars is that of Fou-vry (2003).
Fouvry uses the grammar to guidethe process of learning lexical items for unknownwords, by generating underspecified lexical itemsfor all unknown words and parsing with them.Syntactico-semantic interaction between unknownwords and pre-existing lexical items during pars-ing provides insight into the nature of each un-known word.
By combining such fragments of in-formation, it is possible to incrementally arrive ata consolidated lexical entry for that word.
That is,the precision grammar itself drives the incremen-tal learning process within a parsing context.165An alternate approach is to compile out a set ofword templates for each lexical type (with the im-portant qualification that they do not rely on pre-processing of any form), and check for corpus oc-currences of an unknown word in such contexts.That is, the morphological, syntactic and/or se-mantic predictions implicit in each lexical type aremade explicit in the form of templates which rep-resent distinguishing lexical contexts of that lexi-cal type.
This approach has been shown to be par-ticularly effective over web data, where the sheersize of the data precludes the possibility of linguis-tic preprocessing but at the same time amelioratesthe effects of data sparseness inherent in any lexi-calised DLA approach (Lapata and Keller, 2004).Other work on DLA (e.g.
Korhonen (2002),Joanis and Stevenson (2003), Baldwin (2005a))has tended to take an in vitro DLA approach, inextrapolating away from a DLR to corpus or webdata, and analysing occurrences of words throughthe conduit of an external resource (e.g.
a sec-ondary parser or POS tagger).
In vitro DLA canalso take the form of resource translation, in map-ping one DLR onto another to arrive at the lexicalinformation in the desired format.3 Task and ResourcesIn this section, we outline the resources targetedin this research, namely the English ResourceGrammar (ERG: Flickinger (2002), Copestakeand Flickinger (2000)) and the JACY grammar ofJapanese (Siegel and Bender, 2002).
Note that ourchoice of the ERG and JACY as testbeds for exper-imentation in this paper is somewhat arbitrary, andthat we could equally run experiments over anyGrammar Matrix-based grammar for which thereis treebank data.Both the ERG and JACY are implementedopen-source broad-coverage precision Head-driven Phrase Structure Grammars (HPSGs:Pollard and Sag (1994)).
A lexical item in eachof the grammars consists of a unique identifier,a lexical type (a leaf type of a type hierarchy),an orthography, and a semantic relation.
Forexample, in the English grammar, the lexical itemfor the noun dog is simply:dog_n1 := n_-_c_le &[ STEM < "dog" >,SYNSEM [ LKEYS.KEYREL.PRED "_dog_n_1_rel" ] ].in which the lexical type of n - c le encodesthe fact that dog is a noun which does not sub-categorise for any other constituents and which iscountable, "dog" specifies the lexical stem, and" dog n 1 rel" introduces an ad hoc predicatename for the lexical item to use in constructing asemantic representation.
In the context of the ERGand JACY, DLA equates to learning the range oflexical types a given lexeme occurs with, and gen-erating a single lexical item for each.Recent development of the ERG and JACY hasbeen tightly coupled with treebank annotation, andall major versions of both grammars are deployedover a common set of dynamically-updateabletreebank data to help empirically trace the evo-lution of the grammar and retrain parse selectionmodels (Oepen et al, 2002a; Bond et al, 2004).This serves as a source of training and test data forbuilding our supertaggers, as detailed in Table 1.In translating our treebank data into a form thatcan be understood by a supertagger, multiword ex-pressions (MWEs) pose a slight problem.
Both theERG and JACY include multiword lexical items,which can either be strictly continuous (e.g.
hotline) or optionally discontinuous (e.g.
transitiveEnglish verb particle constructions, such as pickup as in Kim picked the book up).Strictly continuous lexical items are describedby way of a single whitespace-delimited lexicalstem (e.g.
STEM < "hot line" >).
Whenfaced with instances of this lexical item, the su-pertagger must perform two roles: (1) predict thatthe words hot and line combine together to forma single lexeme, and (2) predict the lexical typeassociated with the lexeme.
This is performedin a single step through the introduction of theditto lexical type, which indicates that the cur-rent word combines (possibly recursively) with theleft-adjacent word to form a single lexeme, andshares the same lexical type.
This tagging conven-tion is based on that used, e.g., in the CLAWS7part-of-speech tagset.Optionally discontinuous lexical items are lessof a concern, as selection of each of the discontin-uous ?components?
is done via lexical types.
E.g.in the case of pick up, the lexical entry looks asfollows:pick_up_v1 := v_p-np_le &[ STEM < "pick" >,SYNSEM [ LKEYS [ --COMPKEY _up_p_sel_rel,KEYREL.PRED "_pick_v_up_rel" ] ] ].in which "pick" selects for the up p sel relpredicate, which in turn is associated with the stem"up" and lexical type p prtcl le.
In terms oflexical tag mark-up, we can treat these as separate166ERG JACYGRAMMARLanguage English JapaneseLexemes 16,498 41,559Lexical items 26,297 47,997Lexical types 915 484Strictly continuous MWEs 2,581 422Optionally discontinuous MWEs 699 0Proportion of lexemes with more than one lexical item 0.29 0.14Average lexical items per lexeme 1.59 1.16TREEBANKTraining sentences 20,000 40,000Training words 215,015 393,668Test sentences 1,013 1,095Test words 10,781 10,669Table 1.
Make-up of the English Resource Grammar (ERG) and JACY grammars and treebankstags and leave the supertagger to model the mutualinter-dependence between these lexical types.For detailed statistics of the composition of thetwo grammars, see Table 1.For morphological processing (including to-kenisation and lemmatisation), we use the pre-existing machinery provided with each of thegrammars.
In the case of the ERG, this consistsof a finite state machine which feeds into lexicalrules; in the case of JACY, segmentation and lem-matisation is based on a combination of ChaSen(Matsumoto et al, 2003) and lexical rules.
Thatis, we are able to assume that the Japanese datahas been pre-segmented in a form compatible withJACY, as we are able to replicate the automaticpre-processing that it uses.4 SuppertaggingThe DLA strategy we adopt in this research isbased on supertagging, which is a simple in-stance of sequential tagging with a larger, morelinguistically-diverse tag set than is conventionallythe case, e.g., with part-of-speech tagging.
Below,we describe the pseudo-likelihood CRF model webase our supertagger on and outline the featurespace for the two grammars.4.1 Pseudo-likelihood CRF-basedSupertaggingCRFs are undirected graphical models which de-fine a conditional distribution over a label se-quence given an observation sequence.
Here weuse CRFs to model sequences of lexical types,where each input word in a sentence is assigneda single tag.The joint probability density of a sequence la-belling,   (a vector of lexical types), given the in-put sentence,  , is given by: fiffffifl "!$#&%'!fi( fi(1)where we make a first order Markov assumptionover the label sequence.
Here fl ranges over theword indices of the input sentence (  ), ) rangesover the model?s features, and *+-,/.are themodel parameters (weights for their correspond-ing features).
The feature functionsff0are pre-defined real-valued functions over the input sen-tence coupled with the lexical type labels over ad-jacent ?times?
(= sentence locations) fl .
These fea-ture functions are unconstrained, and may repre-sent overlapping and non-independent features ofthe data.
The distribution is globally normalisedby the partition function, ( 1 fi	 , which sums outthe numerator in (1) for every possible labelling:(1fi	132456227ff8ffifl 9!$#&%9!fiWe use a linear chain CRF, which is encoded inthe feature functions of (1).The parameters of the CRF are usually esti-mated from a fully observed training sample, bymaximising the likelihood of these data.
I.e.
*;:=<>@?BA"CEDF?0HG, where G I,     .is the complete set of training data.However, as calculating(1fi	 has complexityquadratic in the number of labels, we need to ap-proximate 8   fi	 in order to scale our model tohundreds of lexical types and tens-of-thousandsof training sentences.
Here we use the pseudo-likelihood approximation 0J < (Li, 1994) in whichthe marginals for a node at time fl are calculatedwith its neighbour nodes?
labels fixed to those ob-167FEATURE DESCRIPTIONWORD CONTEXT FEATURES & lexeme + label& word unigram + labelffflfiffi& previous word unigram + label"!#fi & next word unigram + label&ffflfi %$& previous word bigram + label&"!
fi $& next word bigram + label&#fi'& (clique label pairLEXICAL FEATURES)+*",#.-/	"0&  1-gram prefix + label24365+-7%&  1-gram suffix + label869;:.<&=;>?
:.2@4A4BffiC&& word contains element of character setB'C+ labelTable 2.
Extracted feature types for the CRF modelserved in the training data:DJ<E9"fl2ff0ffifl GF!$#&%0E"fiHff0Hfl'IE"GF!ffJ %(2)J< KDJ< "flMLDJ<Nfl"(3)where O is the lexical type label observed in thetraining data and N ranges over the label set.
Thisapproximation removes the need to calculate thepartition function, thus reducing the complexity tobe linear in the number of labels and training in-stances.Because maximum likelihood estimators forlog-linear models have a tendency to overfit thetraining sample (Chen and Rosenfeld, 1999), wedefine a prior distribution over the model param-eters and derive a maximum a posteriori (MAP)estimate, * :QP J#J<I?fiA"CEDF?8J<ffiG* 	 .We use a zero-mean Gaussian prior, with the prob-ability density function SR/UT56VGWYX[Z\]^Z\/_.This yields a log-pseudo-likelihood objectivefunction of:`J<2a4.b c6de.fhgjiCJ< fiH2g"iCR(4)In order to train the model, we maximize (4).While the log-pseudo-likelihood cannot be max-imised for the parameters, * , in closed form, it isa convex function, and thus we resort to numericaloptimisation to find the globally optimal parame-ters.
We use L-BFGS, an iterative quasi-Newtonoptimisation method, which performs well fortraining log-linear models (Malouf, 2002; Sha andPereira, 2003).
Each L-BFGS iteration requiresthe objective value and its gradient with respect tothe model parameters.As we cannot observe label values for the testdata we must use 01   fi	 when decoding.
TheViterbi algorithm is used to find the maximumposterior probability alignment for test sentences, k?BA C DF?40 fi	 .4.2 CRF featuresOne of the strengths of the CRF model is thatit supports the use of a large number of non-independent and overlapping features of the inputsentence.
Table 2 lists the word context and lexi-cal features used by the CRF model (shared acrossboth grammars).Word context features were extracted from thewords and lexemes of the sentence to be labelledcombined with a proposed label.
A clique labelpair feature was also used to model sequences oflexical types.For the lexical features, we generate a featurefor the unigram, bigram and trigram prefixes andsuffixes of each word (e.g.
for bottles, we wouldgenerate the prefixes b, bo and bot, and the suf-fixes s, es and les); for words in the test data, wegenerate a feature only if that feature-value is at-tested in the training data.
We additionally testeach word for the existence of one or more ele-ments of a range of character sets lnm .
In the caseof English, we focus on five character sets: uppercase letters, lower case letters, numbers, punctua-tion and hyphens.
For the Japanese data, we em-ploy six character sets: Roman letters, hiragana,katakana, kanji, (Arabic) numerals and punctua-tion.
For example, oqpsrqt ?mouldy?
would beflagged as containing katakana character(s), kanjicharacter(s) and hiragana character(s) only.
Notethat the only language-dependent component of168ERG JACYACC ACC   PREC REC F-SCORE ACC ACC   PREC REC F-SCOREBaseline 0.802 0.053 0.184 0.019 0.034 0.866 0.592 0.680 0.323 0.438FNTBL 0.915 0.236 0.370 0.038 0.068 ?
?
?
?
?CRF0.911 0.427 0.339 0.053 0.092 0.920 0.816 0.548 0.414 0.471CRF !	 0.917 0.489 0.509 0.059 0.105 0.932 0.827 0.696 0.424 0.527Table 3.
Results of supertagging for the ERG and JACY (best result in each column in bold)the lexical features is the character sets, whichrequires little or no specialist knowledge of thelanguage.
Note also that for languages with in-fixing, such as Tagalog, we may want to include-gram infixes in addition to-gram prefixes andsuffixes.
Here again, however, the decision aboutwhat range of affixes is appropriate for a given lan-guage requires only superficial knowledge of itsmorphology.5 EvaluationEvaluation is based on the treebank data associ-ated with each grammar, and a random training?test split of 20,000 training sentences and 1,013test sentences in the case of the ERG, and 40,000training sentences and 1,095 test sentences in thecase of the JACY.
This split is fixed for all modelstested.Given that the goal of this research is to ac-quire novel lexical items, our primary focus is onthe performance of the different models at pre-dicting the lexical type of any lexical items whichoccur only in the test data (which may be eithernovel lexemes or previously-seen lexemes occur-ring with a novel lexical type).
As such, we iden-tify all unknown lexical items in the test data andevaluate according to: token accuracy (the pro-portion of unknown lexical items which are cor-rectly tagged: ACC  ); type precision (the propor-tion of correctly hypothesised unknown lexical en-tries: PREC); type recall (the proportion of gold-standard unknown lexical entries for which we geta correct prediction: REC); and type F-score (theharmonic mean of type precision and type recall:F-SCORE).
We also measure the overall token ac-curacy (ACC) across all words in the test data, ir-respective of whether they represent known or un-known lexical items.5.1 Baseline: Unigram SupertaggerAs a baseline model, we use a simple unigram su-pertagger trained based on maximum likelihoodestimation over the relevant training data, i.e.
thetag flfor each token instance of a given word is predicted by:fl?BA"CDF? ffiflIn the instance that  was not observed in thetraining data, we back off to the majority lexicaltype in the training data.5.2 Benchmark: fnTBLIn order to benchmark our results with the CRFmodels, we reimplemented the supertagger modelproposed by Baldwin (2005b) which simply takesFNTBL 1.1 (Ngai and Florian, 2001) off theshelf and trains it over our particular training set.FNTBL is a transformation-based learner that isdistributed with pre-optimised POS tagging mod-ules for English and other European languages thatcan be redeployed over the task of supertagging.Following Baldwin (2005b), the only modifica-tions we make to the default English POS tag-ging methodology are: (1) to set the default lexicaltypes for singular common and proper nouns ton - c le and n - pn le, respectively; and (2)reduce the threshold score for lexical and contexttransformation rules to 1.
It is important to realisethat, unlike our proposed method, the English POStagger implementation in FNTBL has been fine-tuned to the English POS task, and includes a richset of lexical templates specific to English.Note that were only able to run FNTBL over theEnglish data, as encoding issues with the Japaneseproved insurmountable.
We are thus only able tocompare results over the English, although this isexpected to be representative of the relative per-formance of the methods.5.3 ResultsThe results for the baseline, benchmark FNTBLmethod for English and our proposed CRF-basedsupertagger are presented in Table 3, for each ofthe ERG and JACY.
In order to gauge the impactof the lexical features on the performance of ourCRF-based supertagger, we ran the supertaggerfirst without lexical features (CRF#) and thenwith the lexical features (CRFJ).169The first finding of note is that the proposedmodel surpasses both the baseline and FNTBL inall cases.
If we look to token accuracy for un-known lexical types, the CRF is far and away thesuperior method, a result which is somewhat di-minished but still marked for type-level precision,recall and F-score.
Recall that for the purposes ofthis paper, our primary interest is in how success-fully we are able to learn new lexical items, andin this sense the CRF appears to have a clear edgeover the other models.
It is also important to re-call that our results over both English and Japanesehave been achieved with only the bare minimumof lexical feature engineering, whereas those ofFNTBL are highly optimised.Comparing the results for the CRF with andwithout lexical features (CRF  ), the lexicalfeatures appear to have a strong bearing on typeprecision in particular, for both the ERG andJACY.Looking to the raw numbers, the type-level per-formance for all methods is far from flattering.However, it is entirely predictable that the over-all token accuracy should be considerably higherthan the token accuracy for unknown lexical items.A breakdown of type precision and recall for un-known words across the major word classes forEnglish suggests that the CRFJsupertagger ismost adept at learning nominal and adjectival lex-ical items (with an F-score of 0.671 and 0.628, re-spectively), and has the greatest difficulties withverbs and adverbs (with an F-score of 0.333 and0.395, respectively).
In the case of Japanese, con-jugating adjectives and verbs present the least dif-ficulty (with an F-score of 0.933 and 0.886, re-spectively), and non-conjugating adjectives andadverbs are considerably harder (with an F-scoreof 0.396 and 0.474, respectively).It is encouraging to note that type precision ishigher than type recall in all cases (a phenomenonthat is especially noticeable for the ERG), as thismeans that while we are not producing the full in-ventory of lexical items for a given lexeme, overhalf of the lexical items that we produce are gen-uine (with CRFJ).
This suggests that it shouldbe possible to present the grammar developer witha relatively low-noise set of automatically learnedlexical items for them to manually curate and feedinto the lexicon proper.One final point of interest is the ability of theCRF to identify multiword expressions (MWEs).There were no unknown multiword expressionsin either the English or Japanese data, such thatwe can only evaluate the performance of the su-pertagger at identifying known MWEs.
In the caseof English, CRFJidentified strictly continuousMWEs with an accuracy of 0.758, and optionallydiscontinuous MWEs (i.e.
verb particle construc-tions) with an accuracy of 0.625.
For Japanese, theaccuracy is considerably lower, at 0.536 for con-tinuous MWEs (recalling that there were no op-tionally discontinuous MWEs in JACY).6 ConclusionIn this paper we have explored a method forlearning new lexical items for HPSG-based pre-cision grammars through supertagging.
Ourpseudo-likelihood conditional random field-basedapproach provides a principled way of learninga supertagger from tens-of-thousands of trainingsentences and with hundreds of possible tags.We achieve start-of-the-art results for bothEnglish and Japanese data sets with a largelylanguage-independent feature set.
Our model alsoachieves performance at the type- and token-level,over different word classes and at multiword ex-pression identification, superior to a probabilisticbaseline and a transformation based learning ap-proach.AcknowledgementsWe would like to thank Dan Flickinger andFrancis Bond for support and expert assistancewith the ERG and JACY, respectively, and thethree anonymous reviewers for their valuable in-put on this research.
The research in this paperhas been supported by the Australian ResearchCouncil through Discovery Project grant numberDP0663879, and also NTT Communication Sci-ence Laboratories, Nippon Telegraph and Tele-phone CorporationReferencesTimothy Baldwin.
2005a.
Bootstrapping deep lexical re-sources: Resources for courses.
In Proc.
of the ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition,pages 67?76, Ann Arbor, USA.Timothy Baldwin.
2005b.
General-purpose lexical acquisi-tion: Procedures, questions and results.
In Proc.
of the6th Meeting of the Pacific Association for ComputationalLinguistics (PACLING 2005), pages 23?32, Tokyo, Japan.
(Invited Paper).170Srinivas Bangalore and Aravind K. Joshi.
1999.
Supertag-ging: An approach to almost parsing.
Computational Lin-guistics, 25(2):237?65.Emily M. Bender, Dan Flickinger, and Stephan Oepen.2002.
The grammar Matrix.
An open-source starter-kitfor the rapid development of cross-linguistically consis-tent broad-coverage precision grammar.
In Proc.
of theWorkshop on Grammar Engineering and Evaluation at the19th International Conference on Computational Linguis-tics, Taipei, Taiwan.Francis Bond, Sanae Fujita, Chikara Hashimoto, KanameKasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,Takaaki Tanaka, and Shigeaki Amano.
2004.
The Hinokitreebank: A treebank for text understanding.
In Proc.
ofthe First International Joint Conference on Natural Lan-guage Processing (IJCNLP-04), pages 554?9, Hainan Is-land, China.Johan Bos, Stephen Clark, Mark Steedman, James R. Cur-ran, and Julia Hockenmaier.
2004.
Wide-coverage se-mantic representations from a CCG parser.
In Proc.
of the20th International Conference on Computational Linguis-tics (COLING 2004), pages 1240?7, Geneva, Switzerland.Miriam Butt, Tracy Holloway King, Maria-Eugenia Nino,and Frederique Segond.
1999.
A Grammar Writer?sCookbook.
CSLI Publications, Stanford, USA.Stanley F. Chen and Ronald Rosenfeld.
1999.
A sur-vey of smoothing techniques for maximum entropy mod-els.
IEEE Transactions on Speech and Audio Processing,8(1):37?50.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG parsing.
InProc.
of the 20th International Conference on Computa-tional Linguistics (COLING 2004), pages 282?8, Geneva,Switzerland.Ann Copestake and Dan Flickinger.
2000.
An open-sourcegrammar development environment and broad-coverageEnglish grammar using HPSG.
In Proc.
of the 2nd In-ternational Conference on Language Resources and Eval-uation (LREC 2000), Athens, Greece.Dan Flickinger.
2002.
On building a more efficient grammarby exploiting types.
In Oepen et al (Oepen et al, 2002b).Frederik Fouvry.
2003.
Robust Processing for Constraint-based Grammar Formalisms.
Ph.D. thesis, University ofEssex.Eric Joanis and Suzanne Stevenson.
2003.
A general featurespace for automatic verb classification.
pages 163?70, Bu-dapest, Hungary.Anna Korhonen.
2002.
Subcategorization Acquisition.Ph.D.
thesis, University of Cambridge.Mirella Lapata and Frank Keller.
2004.
The web as a base-line: Evaluating the performance of unsupervised web-based models for a range of NLP tasks.
pages 121?8,Boston, USA.Stan Z. Li.
1994.
Markov random field models in computervision.
In ECCV (2), pages 361?370.Rob Malouf.
2002.
A comparison of algorithms for max-imum entropy parameter estimation.
In Proc.
of the6th Conference on Natural Language Learning (CoNLL-2002), pages 49?55, Taipei, Taiwan.Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-taka Hirano, Hiroshi Matsuda, Kazuma Takaoka, andMasayuki Asahara.
2003.
Japanese Morphological Anal-ysis System ChaSen Version 2.3.3 Manual.
Technical re-port, NAIST.Grace Ngai and Radu Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proc.
of the 2nd AnnualMeeting of the North American Chapter of Associationfor Computational Linguistics (NAACL2001), pages 40?7, Pittsburgh, USA.Stephan Oepen, Dan Flickinger, Kristina Toutanova, andChristoper D. Manning.
2002a.
LinGO Redwoods: Arich and dynamic treebank for HPSG.
In Proc.
of TheFirst Workshop on Treebanks and Linguistic Theories(TLT2002), Sozopol, Bulgaria.Stephan Oepen, Dan Flickinger, Jun?ichi Tsujii, and HansUszkoreit, editors.
2002b.
Collaborative Language En-gineering.
CSLI Publications, Stanford, USA.Carl Pollard and Ivan A.
Sag.
1994.
Head-driven PhraseStructure Grammar.
The University of Chicago Press,Chicago, USA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. Maxwell III, and Mark Johnson.
2002.Parsing the Wall Street Journal using a Lexical-FunctionalGrammar and discriminative estimation techniques.
InProc.
of the 40th Annual Meeting of the ACL and 3rd An-nual Meeting of the NAACL (ACL-02), Philadelphia, USA.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proc.
of the 3rd In-ternational Conference on Human Language TechnologyResearch and 4th Annual Meeting of the NAACL (HLT-NAACL 2003), pages 213?20, Edmonton, Canada.Melanie Siegel and Emily M. Bender.
2002.
Efficient deepprocessing of Japanese.
In Proc.
of the 3rd Workshop onAsian Language Resources and International Standard-ization, Taipei, Taiwan.Hans Uszkoreit.
2002.
New chances for deep linguistic pro-cessing.
In Proc.
of the 19th International Conference onComputational Linguistics (COLING 2002), Taipei, Tai-wan.Gertjan van Noord.
2004.
Error mining for wide-coveragegrammar engineering.
In Proc.
of the 42nd Annual Meet-ing of the ACL, Barcelona, Spain.Yi Zhang and Valia Kordoni.
2005.
A statistical approach to-wards unknown word type prediction for deep grammars.In Proc.
of the Australasian Language Technology Work-shop 2005, pages 24?31, Sydney, Australia.Yi Zhang and Valia Kordoni.
2006.
Automated deep lexicalacquisition for robust open texts processing.
In Proceed-ings of the Fifth International Conference on LanguageResources and Evaluation (LREC 2006), Genoa, Italy.171
