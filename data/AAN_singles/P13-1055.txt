Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561?571,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsIdentifying Bad Semantic Neighbors for Improving DistributionalThesauriOlivier FerretCEA, LIST, Vision and Content Engineering Laboratory,Gif-sur-Yvette, F-91191 France.olivier.ferret@cea.frAbstractDistributional thesauri are now widelyused in a large number of Natural Lan-guage Processing tasks.
However, theyare far from containing only interestingsemantic relations.
As a consequence,improving such thesaurus is an impor-tant issue that is mainly tackled indirectlythrough the improvement of semantic sim-ilarity measures.
In this article, we pro-pose a more direct approach focusing onthe identification of the neighbors of athesaurus entry that are not semanticallylinked to this entry.
This identification re-lies on a discriminative classifier trainedfrom unsupervised selected examples forbuilding a distributional model of the entryin texts.
Its bad neighbors are found by ap-plying this classifier to a representative setof occurrences of each of these neighbors.We evaluate the interest of this method fora large set of English nouns with variousfrequencies.1 IntroductionThe work we present in this article focuses on theautomatic building of a thesaurus from a corpus.As illustrated by Table 1, such thesaurus gives foreach of its entries a list of words, called seman-tic neighbors, that are supposed to be semanti-cally linked to the entry.
Generally, each neigh-bor is associated with a weight that characterizesthe strength of its link with the entry and all theneighbors of an entry are sorted according to thedecreasing order of their weight.The term semantic neighbor is very generic andcan have two main interpretations according to thekind of semantic relations it is based on: one re-lies only on paradigmatic relations, such as hy-pernymy or synonymy, while the other consid-ers syntagmatic relations, called collocation rela-tions by (Halliday and Hasan, 1976) in the contextof lexical cohesion or ?non-classical relations?
by(Morris and Hirst, 2004).
The distinction betweenthese two interpretations refers to the distinctionbetween the notions of semantic similarity and se-mantic relatedness as it was done in (Budanitskyand Hirst, 2006) or in (Zesch and Gurevych, 2010)for instance.
However, the limit between these twonotions is sometimes hard to find in existing workas terms semantic similarity and semantic relat-edness are often used interchangeably.
Moreover,semantic similarity is frequently considered as in-cluded into semantic relatedness and the two prob-lems are often tackled by using the same methods.In the remainder of this article, we will use theterm semantic similarity with its generic sense andthe term semantic relatedness for referring morespecifically to similarity based on syntagmatic re-lations.Following work such as (Grefenstette, 1994), awidespread way to build a thesaurus from a cor-pus is to use a semantic similarity measure for ex-tracting the semantic neighbors of the entries ofthe thesaurus.
Three main ways of implement-ing such measures can be distinguished.
The firstone relies on handcrafted resources in which se-mantic relations are clearly identified.
Work basedon WordNet-like lexical networks for building se-mantic similarity measures such as (Budanitskyand Hirst, 2006) or (Pedersen et al, 2004) fallsinto this category.
These measures typically ex-ploit the hierarchical structure of these networks,based on hypernymy relations.
The second ap-proach makes use of a less structured source ofknowledge about words such as the definitions ofclassical dictionaries or the glosses of WordNet.WordNet?s glosses were used to support Lesk-like measures in (Banerjee and Pedersen, 2003)and more recently, measures were also definedfrom Wikipedia or Wiktionaries (Gabrilovich and561Markovitch, 2007).
The last option is the corpus-based approach, based on the distributional hy-pothesis (Firth, 1957): each word is characterizedby the set of contexts from a corpus in which it ap-pears and the semantic similarity of two words iscomputed from the contexts they share.
This per-spective was first adopted by (Grefenstette, 1994)and (Lin, 1998) and then, explored in details in(Curran and Moens, 2002b), (Weeds, 2003) or(Heylen et al, 2008).The problem of improving the results of the?classical?
implementation of the distributionalapproach as it can be found in (Curran and Moens,2002a) for instance was already tackled by somework.
A part of these proposals focus on theweighting of the elements that are part of thecontexts of words such as (Broda et al, 2009),in which the weights of context elements areturned into ranks, or (Zhitomirsky-Geffet and Da-gan, 2009), followed and extended by (Yamamotoand Asakura, 2010), that proposes a bootstrap-ping method for modifying the weights of con-text elements according to the semantic neighborsfound by an initial distributional similarity mea-sure.
However, another part of these proposalsimplies more radical changes.
The use of dimen-sionality reduction techniques, for instance LatentSemantic Analysis in (Pado?
and Lapata, 2007), themulti-prototype (Reisinger and Mooney, 2010) orexamplar-based models (Erk and Pado, 2010), theDeep Learning approach of (Huang et al, 2012) orthe redefinition of the distributional approach in aBayesian framework (Kazama et al, 2010) can beclassified into this second category.The work we present in this article takes placein the framework defined by (Grefenstette, 1994)for implementing the distributional approach butproposes a new method for improving a thesaurusbuilt in this context based on the identification ofits bad semantic neighbors rather than on the adap-tation of the weight of their features.2 PrinciplesOur work shares with (Zhitomirsky-Geffet andDagan, 2009) the use of a kind of bootstrapping asit starts from a distributional thesaurus and to someextent, exploits it for its improvement.
However, itadopts a more indirect approach: instead of select-ing the ?best?
semantic neighbors of an entry inthe thesaurus for adapting the weights of distribu-tional context elements, it focuses on the detectionof its bad semantic neighbors, that is to say theneighbors of the entry that are actually not seman-tically similar to the entry.
In Table 1, waterworksfor the entry cabdriver and hollowness for the en-try machination are two examples of such kind ofneighbors.
By discarding these bad neighbors orat least by downgrading them, the rank of true se-mantic neighbors is expected to be lower.
Thismakes the thesaurus more interesting to use sincethe quality of such thesaurus strongly decreases asthe rank of the neighbors of its entries increases(see Section 4.1 for an illustration), which meansin practice that only the first neighbors of an entrycan be generally exploited.The approach we propose for identifying thebad semantic neighbors of a thesaurus entry re-lies on the distributional hypothesis, as the methodfor the initial building of the thesaurus, but im-plements it in a different way.
This hypothesisroughly specifies that from a semantic viewpoint,the meaning of a word can be characterized by theset of contexts in which this word occurs.
As aconsequence, two words are considered as seman-tically similar if they occur in a large enough setof shared contexts.
In work such as (Curran andMoens, 2002a), this hypothesis is implemented bycollecting for each entry the words it co-occurswith in a large corpus.
This co-occurrence canbe based either on the position of the word in thetext in relation to the entry or on the presenceof a syntactic relation between the entry and theword.
As a result, the distributional representa-tion of a word takes the unstructured form of abag of words or the more structured form of aset of pairs {syntactic relation, word}.
A vari-ant of this approach was proposed in (Kazama etal., 2010) where the distributional representationof a word is modeled as a multinomial distributionwith Dirichlet as prior.However, this approach globally faces a certainlack of diversity and complexity of the features ofits models.
For instance, features such as ngramsof words or ngrams of parts of speech are not con-sidered whereas they are widely used in tasks suchas word sense disambiguation (WSD) for instance,probably because they would lead to very largemodels and because similarity measures such asthe Cosine measure are not necessarily suitablefor heterogeneous representations (Alexandrescuand Kirchhoff, 2007).
Hence, we propose in thisarticle to build a discriminative model for repre-562abnormality defect [0.30], disorder [0.23], deformity [0.22], mutation [0.21], prolapse [0.21], anomaly [0.21] .
.
.agreement accord [0.44], deal [0.41], pact [0.38], treaty [0.36], negotiation [0.35], proposal [0.32], arrangement [0.30] .
.
.cabdriver waterworks [0.23], toolmaker [0.22], weaponeer [0.17], valkyry [0.17], wang [0.17], amusement-park [0.17] .
.
.machination hollowness [0.15], share-price [0.12], clockmaker [0.12], huguenot [0.12], wrangling [0.12], alternation [0.12] .
.
.Table 1: First neighbors of some entries of the distributional thesaurus of section 3.2senting the contexts of a word since this kind ofmodels are known to integrate easily a wide setof different types of features.
This model aimsmore precisely at discriminating from a semanticviewpoint a word in context, i.e.
in a sentence,from all other words and more particularly, fromthose of its neighbors in a distributional thesaurusthat are likely to be actually not semantically sim-ilar to it.
The underlying hypothesis follows thedistributional principles: a word and a synonymshould appear in the same contexts, which meansthat they are characterized by the same features.As a consequence, a model based on these fea-tures that can identify a word in a sentence is likelyto identify also a synonym of this word in a sen-tence, and by extension, to identify a word thatis paradigmatically linked to it.
More precisely,we found that such model is specifically effectivefor discarding the bad neighbors of the entries of adistributional thesaurus.3 Improving a distributional thesaurus3.1 OverviewThe principles presented in the previous sectionface one major problem compared to the ?classi-cal?
distributional approach : the semantic similar-ity of two words can be evaluated directly by com-puting the similarity of their distributional repre-sentations.
However, in our case, since this rep-resentation is a discriminative model, the similar-ity of two words can not be evaluated through thedirect comparison of their models.
These modelshave to be applied to words in context for beingexploited.
As a consequence, for deciding whethera neighbor of a thesaurus entry is a bad neighboror not, the discriminative model of the entry hasto be applied to occurrences of this neighbor intexts.
Hence, the method we propose for improv-ing a distributional thesaurus applies the followingprocess to each of its entries:?
building of a classifier for determiningwhether a word in a sentence corresponds ornot to the entry;?
selection of a set of examples sentences foreach of the neighbors of the entry in the the-saurus;?
application of the classifier to these sen-tences;?
identification of bad neighbors according tothe results of the classifier;?
reranking of entry?s neighbors according tobad neighbors.3.2 Building of the initial thesaurusBefore introducing our method for improving dis-tributional thesauri, we first present the way webuild such a thesaurus.
As in (Lin, 1998) or (Cur-ran and Moens, 2002a), this building is based onthe definition of a semantic similarity measurefrom a corpus.
The corpus used for defining thismeasure was the AQUAINT-2 corpus, a middle-size corpus made of around 380 million wordscoming from news articles.
Although our targetlanguage is English, we chose to limit deliber-ately the level of the tools applied for preprocess-ing texts to part-of-speech tagging and lemmati-zation to make possible the transposition of ourmethod to a large set of languages.
This seemsto be a reasonable compromise between the ap-proach of (Freitag et al, 2005), in which nonenormalization of words is done, and the morewidespread use of syntactic parsers in work suchas (Lin, 1998).
More precisely, we used TreeTag-ger (Schmid, 1994) for performing the linguisticpreprocessing of the AQUAINT-2 corpus.For the extraction of distributional data and thecharacteristics of the distributional similarity mea-sure, we adopted the options of (Ferret, 2010), re-sulting from a kind of grid search procedure per-formed with the extended TOEFL test proposed in(Freitag et al, 2005) as an optimization objective.More precisely, the following characteristics weretaken:?
distributional contexts made of the co-occurrents collected in a 3 word window cen-tered on each occurrence in the corpus of thetarget word.
These co-occurrents were re-stricted to nouns, verbs and adjectives;?
soft filtering of contexts: removal of co-occurrents with only one occurrence;?
weighting function of co-occurrents in con-563texts = Pointwise Mutual Information (PMI)between the target word and the co-occurrent;?
similarity measure between contexts, forevaluating the semantic similarity of twowords = Cosine measure.The building of our initial thesaurus from thesimilarity measure above was performed classi-cally by extracting the closest semantic neighborsof each of its entries.
More precisely, the selectedmeasure was computed between each entry andits possible neighbors.
These neighbors were thenranked in the decreasing order of the values of thismeasure and the first 100 neighbors were kept asthe semantic neighbors of the entry.
Both entriesand possible neighbors were AQUAINT-2 nounswhose frequency was higher than 10.3.3 Building a discriminative model of wordsin contextAs mentioned in section 3.1, the starting point ofour reranking process is the definition of a modelfor determining to what extent a word in a sen-tence, which is not supposed to be known in thecontext of this task, corresponds or not to a refer-ence word E. This task can also be viewed as atagging task in which the occurrences of a targetword T are labeled with two tags: E and notE.In the context of our global objective, we are notof course interested by this task itself but rather bythe fact that such classifier is likely to model thecontexts in which E occurs and as a consequence,is also likely to model its meaning according to thedistributional hypothesis.A step further, such classifier can be viewedas a means for testing whether or not a wordhas the same meaning as E. This is a problemclose to WSD as it is performed in the context ofthe pseudo-word disambiguation paradigm (Galeet al, 1992): a pseudo-word is created with twosenses, E and notE, notE corresponding to oneor several words that are supposed to be represen-tative of a meaning different from the meaning ofE.
The objective is then to build a classifier fordistinguishing the pseudo-senses E and notE.
Asa consequence of this view, we adopt the samekind of features as the ones used for WSD forbuilding our classifier.
More precisely, we follow(Lee and Ng, 2002), a reference work for WSD,by adopting a Support Vector Machines (SVM)classifier with a linear kernel and three kinds offeatures for characterizing each considered occur-rence in a text of the reference word E:?
neighboring words;?
Part-of-Speech (POS) of neighboring words;?
local collocations.Only features based on syntactic relations arenot taken from (Lee and Ng, 2002) since their usewould have not been coherent with the windowbased approach of the building of our initial the-saurus.For the neighboring words features, we con-sider all plain words (common and proper nouns,verbs and adjectives) and adverbs that are presentin the same sentence of an occurrence of E. Eachneighboring word is represented under its lemmaform as a binary feature whose value is equal to 1when it is present in the same sentence as E.For the second type of features, we take moreprecisely the POS of the three words before E andthose of the three words after E. Each pair {POS,position} corresponds to a binary feature for theSVM classifier.
A special empty symbol is usedfor the POS when the position goes beyond the endor the beginning of the current sentence.
Since weanalyze texts with TreeTagger, the tagset is veryclose to the set of Penn Treebank tags.Finally, the local collocations features corre-spond to pairs of words, named collocations, inthe neighborhood of E. A collocation is speci-fied by the notation Ci,j , with i and j referring tothe position of the first and the second word of thecollocation.
In our case, i and j take their valuesin the interval [?3,+3], similarly to POS.
Moreprecisely, the following 11 types of collocationsare extracted for each occurrence of E: C?1,?1,C1,1, C?2,?2, C2,2 C?2,?1, C?1,1, C1,2, C?3,?1,C?2,1, C?1,2 and C1,3.
As for POS, a specialempty symbol stands for words beyond the endor the beginning of the sentence and similarly toneighboring words features, words in collocationsare given under their lemma form.
Each instanceof the 11 types of collocations is represented by atuple ?lemma1, position1, lemma2, position2?
andleads to a binary feature for the SVM classifier.In accordance with the process of section 3.1,a specific SVM classifier is trained for each entryof our initial thesaurus, which requires the unsu-pervised selection of a set of positive and nega-tive examples.
The case of positive examples issimple: a fixed number of sentences containing atleast one occurrence of the target entry are ran-domly chosen in the corpus used for building our564initial thesaurus and the first occurrence of this en-try in the sentence is taken as a positive example.Since we want to characterize words as much aspossible from a semantic viewpoint, the selectionof negative examples is guided by our initial the-saurus.
Choosing a neighbor of the entry with ahigh rank would guarantee in principle few falsenegative examples, that is to say words1 which aresemantically similar to the entry, since the numberof such neighbors strongly decreases as the rankof neighbors increases as we will illustrate it insection 4.1.
In practice, taking neighbors with arather small rank as negative examples is a bet-ter option because these examples are more usefulin terms of discrimination as they are close to thetransition zone between negative and positive ex-amples.
Moreover, in order to limit the risk of se-lecting only false negative examples, three neigh-bors are taken as negative examples, at ranks 10,15 and 202.
For each of these negative examples,a fixed number of sentences is selected follow-ing the same principles as for positive examples,which means that on average, the number of neg-ative examples is equal to three times the numberof positive examples.
This ratio reflects the factthat among the neighbors of an entry, the numberof those that are semantically similar to the entryis far lower than the number of those that are not.3.4 Identification of bad neighbors andthesaurus rerankingOnce a word-in-context classifier was trained foran entry, it is used for identifying the bad neigh-bors of this entry, that is to say the neighbors thatare not semantically similar to it.
As this classifiercan only be applied to words in context, a fixednumber of representative occurrences have to beselected from our reference corpus for each neigh-bor of the entry.
This selection is performed sim-ilarly to the selection of positive and negative ex-amples in the previous section.
The application ofour word-in-context classifier to each of these oc-currences determines whether the context of thisoccurrence is likely to be compatible with the con-text of an occurrence of the entry.In practice, the decision of the classifier is rarely1More precisely, an example here is an occurrence of aword in a text but by extension, we also use the term examplefor referring to the word itself.2It should be noted that these ranks come from the eval-uation of section 4.1 but their choice is not the result of anoptimization process.positive, which is not surprising: even if twowords are semantically equivalent, each one ischaracterized by specific usages, especially in agiven corpus, and some features of our classifier,such as the collocation features, are more likelyto capture such specificities than the unigrams of?classical?
distributional contexts.
As a conse-quence, we consider that a positive outcome of ourclassifier is a significant hint about the presence ofa word that is semantically similar to the entry andwe keep a neighbor as a ?good?
neighbor if at leasta fixed number G of its occurrences, among thoseselected as reference, are tagged positively by ourword-in-context classifier.
Conversely, a neighboris defined as ?bad?
if the number of its referenceoccurrences tagged positively by our classifier islower or equal to G.The neighbors of an entry identified as badneighbors are not fully discarded.
They are ratherdowngraded to the end of the list of neighbors.Among the downgraded neighbors, their initial or-der is left unchanged.
It should be noted thatthe word-in-context classifier is not applied to theneighbors whose occurrences are used for its train-ing as it would frequently lead to downgrade theseneighbors, which is not necessarily optimum as wechose them with a rather low rank.4 Experiments and evaluation4.1 Initial thesaurus evaluationTable 2 shows the results of the evaluation of ourinitial thesaurus, achieved by comparing the se-lected semantic neighbors with two complemen-tary reference resources: WordNet 3.0 synonyms(Miller, 1990) [W], which characterize a semanticsimilarity based on paradigmatic relations, and theMoby thesaurus (Ward, 1996) [M], which gathersa larger set of types of relations and is more rep-resentative of semantic relatedness3.
The fourthcolumn of Table 2, which gives the average num-ber of synonyms and similar words in our refer-ences for the AQUAINT-2 nouns, also illustratesthe difference of these two resources in terms ofrichness.
A fusion of the two resources is alsoconsidered [WM].
As our objective is to evalu-ate the extracted semantic neighbors and not theability to rebuild the reference resources, these re-3The Moby thesaurus includes more precisely bothparadigmatic and syntactic relations but we will sometimesuse the term synonym as a shortcut for referring to all thewords associated to one of its entries.565freq.
ref.
#eval.words#syn.
/wordrecall R-prec.
MAP P@1 P@5 P@10 P@100W 10,473 2.9 24.6 8.2 9.8 11.7 5.1 3.4 0.7all M 9,216 50.0 9.5 6.7 3.2 24.1 16.4 13.0 4.8# 14,670 WM 12,243 38.7 9.8 7.7 5.6 22.5 14.1 10.8 3.8W 3,690 3.7 28.3 11.1 12.5 17.2 7.7 5.1 1.0high M 3,732 69.4 11.4 10.2 4.9 41.3 28.0 21.9 7.9# 4,378 WM 4,164 63.2 11.5 11.0 6.5 41.3 26.8 20.8 7.3W 3,732 2.6 28.6 10.4 12.5 13.6 5.8 3.7 0.7middle M 3,306 41.3 9.3 6.5 3.1 18.7 13.1 10.4 3.8# 5,175 WM 4,392 32.0 9.8 9.3 7.4 20.9 12.3 9.3 3.2W 3,051 2.3 11.9 2.1 3.3 2.6 1.2 0.9 0.3low M 2,178 30.1 2.8 1.2 0.5 2.5 1.5 1.5 0.9# 5,117 WM 3,687 18.9 3.5 2.1 2.4 3.3 1.7 1.5 0.7Table 2: Evaluation of semantic neighbor extractionsources were filtered to discard entries and syn-onyms that are not part of the AQUAINT-2 vo-cabulary (see the difference between the numberof words in the first column and the number ofevaluated words of the third column).
Since thefrequency of words is an important factor in dis-tributional approaches, we give our results glob-ally but also for three ranges of frequencies thatsplit our set of nouns into roughly equal parts:high frequency (frequency > 1000), middle fre-quency (100 < frequency ?
1000) and low fre-quency (10 < frequency ?
100).
These resultstake the form of several measures and start at thefifth column by the proportion of the synonymsand similar words of our references that are foundamong the first 100 extracted neighbors of eachnoun.
As these neighbors are ranked according totheir similarity value with their target word, theevaluation measures are taken from the Informa-tion Retrieval field by replacing documents withsynonyms and queries with target words (see thefour last columns of Table 2).
The R-precision (R-prec.)
is the precision after the first R neighborswere retrieved, R being the number of referencesynonyms; the Mean Average Precision (MAP) isthe average of the precision value after a referencesynonym is found; precision at different cut-offs isgiven for the 1, 5, 10 and 100 first neighbors.
Allthese values are given as percentages.The results of Table 2 lead to three main ob-servations.
First, the level of results heavily de-pends on the frequency range of target words:the best results are obtained for high frequencywords while evaluation measures significantly de-crease for words whose frequency is low.
Sec-ond, the characteristics of the reference resourceshave a significant impact on results.
WordNetprovides a restricted number of synonyms foreach noun while the Moby thesaurus contains foreach entry a large number of synonyms and sim-ilar words.
As a consequence, the precisionsat different cut-offs have a significantly highervalue with Moby as reference than with Word-Net as reference.
Finally, the results of Ta-ble 2 are compatible with those of (Lin, 1998)for instance (R-prec.
= 11.6 and MAP = 8.1with WM as reference for all entries of the the-saurus at http://webdocs.cs.ualberta.ca/lindek/Downloads/sim.tgz) if wetake into account the fact that the thesaurus of Linwas built from a much larger corpus and with syn-tactic co-occurrences.4.2 Implementation issuesThe implementation of the method we have pre-sented in section 3 raises several issues.
One ofthese concerns the occurrences to select from textsof both the entries of the thesaurus and their neigh-bors.
These occurrences are used both for thetraining of our word-in-context classifier and forthe identification of bad neighbors.
In practice, weextract randomly from our reference corpus, i.e.the AQUAINT-2 corpus, a fixed number of sen-tences, equal to 250, for each word of the vocab-ulary of our initial thesaurus and exploit them forthe two tasks.
This extraction is performed on thebasis of the lemma form of these words.
It shouldbe noted that 250 is the upper limit of the num-ber of occurrences by word since the frequencyin the corpus of many words is lower than 250.When this limit is not reached, all the available oc-566currences are taken, which may be no more than11 occurrences for certain low-frequency words.The upper limit of 250 is halfway between the 385training examples on average for the Lexical Sam-ple Task of Senseval 1 and the 118 training exam-ples on average for the same task of Senseval 2.The training of our word-in-context classifieris also an important issue.
As mentioned before,this classifier is a linear SVM.
Hence, only its Cregularization parameter can be optimized.
Sincewe have one specific classifier for each thesaurusentry, such optimization has globally a high cost,even for a linear kernel.
Hence, we have first eval-uated through a 5-fold cross-validation method theresults of these classifiers with a default value ofC, equal to 1.
Table 3 gives their average accu-racy value along with their standard deviation forall the entries of the thesaurus and for the threefrequency ranges of Table 2.all high middle lowaccuracy 86.2 86.1 86.0 86.5standard deviation 6.1 4.2 5.7 7.6Table 3: Results of word-in-context classifiersThis table shows a global high level of resultalong with similar values for all the frequencyranges of entries4.
Hence, we have decided not tooptimize the C parameter and to adopt the defaultvalue of 1 for all the word-in-context classifiers.0 5 10 15 201213141516G thresholdMAP (W)R?prec.
(W)Figure 1: R-precision and MAP for various valuesof the G thresholdThe last and the most important implementationissue is the setting of the threshold G for deter-mining whether a neighbor is likely to be a bad4The standard deviation is a little bit higher for the lowestfrequencies but it should be noted that the low number ofexamples for low frequency entries does not seem to havea strong impact on the results of such classifier.neighbor.
For this setting, we have randomly cho-sen a subset of 859 entries of our initial thesaurusthat corresponds to 10% of the entries with at leastone true neighbor in any of our references.
Fig-ure 1 gives the results of the reranked thesaurusfor these entries in terms of R-precision and MAPagainst reference W5 for various values of G. Al-though the level of these measures does not changea lot for G > 5, the graph of Figure 1 shows thatG = 15 appears to be an optimal value.
Hence,this is the value used for the detailed evaluation ofthe next section.4.3 Evaluation of the reranked thesaurusTable 4 gives the evaluation of the application ofour reranking method to the initial thesaurus ac-cording to the same principles as in section 4.1.The value of each measure comes with its differ-ence with the corresponding value for the initialthesaurus.
As the recall measure and the precisionfor the last rank do not change in a reranking pro-cess, they are not given again.The first thing to notice is that at the globalscale, all measures for all references are signifi-cantly improved6, which means that our hypothe-sis about the possibility for a discriminative clas-sifier to capture the meaning of a word tends tobe validated.
It is an interesting result since thefeatures upon which this classifier was built weretaken from WSD and were not specifically se-lected for this task.
As a consequence, there isprobably some room for improvement.If we go into details, Table 4 clearly shows twomain trends.
First, the improvement of results isparticularly effective for middle frequency entries,then for low frequency and finally, for high fre-quency entries.
Because of their already high levelin the initial thesaurus, results for high frequencyentries are difficult to improve but it is importantto note that our selection of bad neighbors has avery low error rate, which at least preserves theseresults.
This is confirmed by the fact that, withWordNet as reference, only 744 neighbors werefound wrongly downgraded, spread over 686 en-tries, which represents only 5% of all downgradedneighbors.
The second main trend of Table 4 con-5The use of W as reference is justified by the fact that thenumber of synonyms for an entry in W is more compatible,especially for R-precision, with the real use of the resultingthesaurus in an application.6The statistical significance of differences with the initialthesaurus was evaluated by a paired Wilcoxon test with p-value < 0.05 and < 0.01 (?
and ?
for non significance).567freq.
ref.
R-prec.
MAP P@1 P@5 P@10W 9.1 (0.9) 10.7 (0.9) 12.8 (1.1) 5.6 (0.5) 3.7 (0.3)all M 7.2 (0.5) 3.5 (0.3) 26.5 (2.4) 17.9 (1.5) 14.0 (1.0)WM 8.4 (0.7) 6.1 (0.5) 24.8 (2.3) 15.4 (1.3) 11.7 (0.9)W 11.3 (0.2) ?
12.6 (0.1) 17.3 (0.1) ?
7.8 (0.1) ?
5.1 (0.0)high M 10.3 (0.1) 4.9 (0.0) 42.1 (0.8) 28.4 (0.4) 22.1 (0.2)WM 11.1 (0.1) 6.6 (0.1) 42.0 (0.7) 27.2 (0.4) 20.9 (0.1)W 11.8 (1.4) 13.8 (1.3) 15.7 (2.1) 6.5 (0.7) 4.1 (0.4)middle M 7.3 (0.8) 3.6 (0.5) 23.3 (4.6) 16.0 (2.9) 12.4 (2.0)WM 10.3 (1.0) 8.1 (0.7) 25.1 (4.2) 14.6 (2.3) 10.9 (1.6)W 3.2 (1.1) 4.6 (1.3) 3.9 (1.3) 1.8 (0.6) 1.3 (0.4)low M 1.8 (0.6) 0.8 (0.3) 4.4 (1.9) 2.9 (1.4) 2.6 (1.1)WM 3.1 (1.0) 3.3 (0.9) 5.1 (1.8) 2.9 (1.2) 2.3 (0.8)Table 4: Results of the reranking of semantic neighborscerns the type of semantic relations: results withMoby as reference are improved in a larger ex-tent than results with WordNet as reference.
Thissuggests that our procedure is more effective forsemantically related words than for semanticallysimilar words, which can be considered as a lit-tle bit surprising since the notion of context in ourdiscriminative classifier seems a priori more strictthan in ?classical?
distributional contexts.
How-ever, this point must be investigated further as asignificant part of the relations in Moby, even ifthey do no represent the largest part of them, areparadigmatic relations.WordNet respect, admiration, regardMobyadmiration, appreciation, accep-tance, dignity, regard, respect, ac-count, adherence, consideration,estimate, estimation, fame, great-ness, reverence + 79 words moreinitialcordiality, gratitude, admiration,comradeship, back-scratching,perplexity, respect, ruination,appreciation, neighbourliness .
.
.rerankinggratitude, admiration, respect,appreciation, neighborliness, trust,empathy, goodwill, reciprocity,half-staff, affection, self-esteem,reverence, longing, regard .
.
.Table 5: Impact of our reranking for the entry es-teemTable 5 illustrates more precisely the impact ofour reranking procedure for the middle frequencyentry esteem.
Its WordNet row gives all the refer-ence synonyms for this entry in WordNet while itsMoby row gives the first reference related wordsfor this entry in Moby.
In our initial thesaurus, thefirst two neighbors of esteem that are present in ourreference resources are admiration (rank 3) and re-spect (rank 7).
The reranking produces a thesaurusin which these two words appear as the secondand the third neighbors of the entry because neigh-bors without clear relation with it such as back-scratching were downgraded while its third syn-onym in WordNet is raised from rank 22 to rank15.
Moreover, the number of neighbors among thefirst 15 ones that are present in Moby increasesfrom 3 to 5.5 Related workThe building of distributional thesaurus is gener-ally viewed as an application or a mode of eval-uation of work about semantic similarity or se-mantic relatedness.
As a consequence, the im-provement of such thesaurus is generally not di-rectly addressed but is a possible consequenceof the improvement of semantic similarity mea-sures.
However, the extent of this improvementis rarely evaluated as most of the work about se-mantic similarity is evaluated on datasets such asthe WordSim-353 test collection (Gabrilovich andMarkovitch, 2007), which are only partially repre-sentative of the results for thesaurus building.If we consider more specifically the problem ofimproving semantic similarity, and by the way the-sauri, in a given paradigm, (Broda et al, 2009),(Zhitomirsky-Geffet and Dagan, 2009) and (Ya-mamoto and Asakura, 2010), which all take placein the paradigm defined by (Grefenstette, 1994),are the closest works to ours.
(Broda et al, 2009)proposes a new weighting scheme of words indistributional contexts that replaces the weight of568word by a function of its rank in the context, whichis a way to be less dependent on the values of a par-ticular weighting function.
(Zhitomirsky-Geffetand Dagan, 2009) shares with our work the useof bootstrapping by relying on an initial thesaurusto derive means of improving it.
More specifi-cally, (Zhitomirsky-Geffet and Dagan, 2009) as-sumes that the first neighbors of an entry are morerelevant than the others and as a consequence, thattheir most significant features are also representa-tive of the meaning of the entry.
The neighborsof the entry are reranked according to this hypoth-esis by increasing the weight of these features tofavor their influence in the distributional contextsthat support the evaluation of the similarity be-tween the entry and its neighbors.
(Yamamoto andAsakura, 2010) is a variant of (Zhitomirsky-Geffetand Dagan, 2009) that takes into account a largernumber of features for the reranking process.
Onemain difference between all these works and oursis that they assume that the initial thesaurus wasbuilt by relying on distributional contexts repre-sented as bags-of-words.
Our method does notmake this assumption as its reranking is based ona classifier built in an unsupervised way7 from andapplied to the corpus used for building the initialthesaurus.
As a consequence, it could even be ap-plied to other paradigms than (Grefenstette, 1994).If we focus more specifically on the improve-ment of distributional thesauri, (Ferret, 2012) isthe most comparable work to ours, both becauseit is specifically focused on this task and it isbased on the same evaluation framework.
(Fer-ret, 2012) selects in an unsupervised way a setof positive and negative examples of semanticallysimilar words from the initial thesaurus, uses themfor training a classifier deciding whether or not apair of words are semantically similar and finally,applies this classifier to the neighbors of each en-try for reranking them.
One of the objectives of(Ferret, 2012) was to rebalance the initial the-saurus in favor of low frequency entries.
Althoughthis objective was reached, the resulting thesaurustends to have a lower performance than the initialthesaurus for high frequency entries and for syn-onyms.
The problem with high frequency entriescomes from the fact that applying a machine learn-ing classifier to its training examples does not leadto a perfect result.
The problem with synonyms7It is a supervised classifier but its training set is selectedin an unsupervised way.arises from the imbalance between semantic simi-larity and semantic relatedness among training ex-amples: most of selected examples were pairs ofwords linked by semantic relatedness because thiskind of relations are more frequent among seman-tic neighbors than relations based on semantic sim-ilarity.In both cases, the method proposed in (Ferret,2012) faces the problem of relying only on the dis-tributional thesaurus it tries to improve.
This is animportant difference with the method presented inthis article, which mainly exploits the context ofthe occurrences of words in the corpus used for thebuilding the initial thesaurus.
As a consequence, ata global scale, our reranked thesaurus outperformsthe final thesaurus of (Ferret, 2012) for nearly allmeasures.
The only exceptions are the P@1 valuesfor M and WM as reference.
However, it should benoted that values for both MAP and R-precision,which are more reliable measures than P@1, areidentical for the two thesauri and the same refer-ences.6 Conclusion and perspectivesIn this article, we have presented a new approachfor reranking the semantic neighbors of a distribu-tional thesaurus.
This approach relies on the unsu-pervised building of discriminative classifiers ded-icated to the identification of its entries in texts,with the objective to characterize their meaningaccording to the distributional hypothesis.
Theclassifier built for an entry is then applied to aset of occurrences of its neighbors for identifyingand downgrading those that are not semanticallyrelated to the entry.
The proposed method wastested on a large thesaurus of nouns for Englishand led to a significant improvement of this the-saurus, especially for middle and low frequencyentries and for semantic relatedness.
We plan toextend this work by taking into account the no-tion of word sense as it is done in (Reisinger andMooney, 2010) or (Huang et al, 2012): since werely on occurrences of words in texts, this exten-sion should be quite straightforward by turning ourword-in-context classifiers into true word senseclassifiers.AcknowledgmentsThis work was partly supported by the projectANR ASFALDA ANR-12-CORD-0023.569ReferencesAndrei Alexandrescu and Katrin Kirchhoff.
2007.Data-driven graph construction for semi-supervisedgraph-based learning in NLP.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL HLT 2007), pages 204?211, Rochester, New York.Satanjeev Bano Banerjee and Ted Pedersen.
2003.
Ex-tended gloss overlaps as a measure of semantic relat-edness.
In Eighteenth International Conference onArtificial Intelligence (IJCAI-03), Acapulco, Mex-ico.Bartosz Broda, Maciej Piasecki, and Stan Szpakow-icz.
2009.
Rank-Based Transformation in Measur-ing Semantic Relatedness.
In 22nd Canadian Con-ference on Artificial Intelligence, pages 187?190.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.James Curran and Marc Moens.
2002a.
Scaling con-text space.
In 40th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-02), pages231?238, Philadelphia, Pennsylvania, USA.James R. Curran and Marc Moens.
2002b.
Improve-ments in automatic thesaurus extraction.
In Work-shop of the ACL Special Interest Group on the Lexi-con (SIGLEX), pages 59?66, Philadelphia, USA.Katrin Erk and Sebastian Pado.
2010.
Exemplar-basedmodels for word meaning in context.
In 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL 2010), short paper, pages 92?97,Uppsala, Sweden, July.Olivier Ferret.
2010.
Testing semantic similarity mea-sures for extracting synonyms from a corpus.
InSeventh conference on International Language Re-sources and Evaluation (LREC?10), Valletta, Malta.Olivier Ferret.
2012.
Combining bootstrapping andfeature selection for improving a distributional the-saurus.
In 20th European Conference on ArtificialIntelligence (ECAI 2012), pages 336?341, Montpel-lier, France.John R. Firth, 1957.
Studies in Linguistic Analysis,chapter A synopsis of linguistic theory 1930-1955,pages 1?32.
Blackwell, Oxford.Dayne Freitag, Matthias Blume, John Byrnes, Ed-mond Chow, Sadik Kapadia, Richard Rohwer, andZhiqiang Wang.
2005.
New experiments in distribu-tional representations of synonymy.
In Ninth Con-ference on Computational Natural Language Learn-ing (CoNLL), pages 25?32, Ann Arbor, Michigan,USA.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In 20th Interna-tional Joint Conference on Artificial Intelligence (IJ-CAI 2007), pages 6?12.William A Gale, Kenneth W Church, and DavidYarowsky.
1992.
Work on statistical methods forword sense disambiguation.
In AAAI Fall Sympo-sium on Probabilistic Approaches to Natural Lan-guage, pages 54?60.Gregory Grefenstette.
1994.
Explorations in auto-matic thesaurus discovery.
Kluwer Academic Pub-lishers.Michael A. K. Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
Longman, London.Kris Heylen, Yves Peirsmany, Dirk Geeraerts, andDirk Speelman.
2008.
Modelling Word Similarity:An Evaluation of Automatic Synonymy ExtractionAlgorithms.
In Sixth conference on InternationalLanguage Resources and Evaluation (LREC 2008),Marrakech, Morocco.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In 50th Annual Meeting of the Associa-tion for Computational Linguistics (ACL?12), pages873?882.Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,Masaki Murata, and Kentaro Torisawa.
2010.
Abayesian method for robust estimation of distribu-tional similarities.
In 48th Annual Meeting of theAssociation for Computational Linguistics, pages247?256, Uppsala, Sweden.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empir-ical evaluation of knowledge sources and learningalgorithms for word sense disambiguation.
In 2002Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2002), pages 41?48.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In 17th International Confer-ence on Computational Linguistics and 36th AnnualMeeting of the Association for Computational Lin-guistics (ACL-COLING?98), pages 768?774, Mon-tral, Canada.George A. Miller.
1990.
WordNet: An On-Line Lex-ical Database.
International Journal of Lexicogra-phy, 3(4).Jane Morris and Graeme Hirst.
2004.
Non-classical lexical semantic relations.
In Workshopon Computational Lexical Semantics of Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 46?51, Boston, MA.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.570Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity - measuring therelatedness of concepts.
In HLT-NAACL 2004,demonstration papers, pages 38?41, Boston, Mas-sachusetts, USA.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL 2010), pages 109?117, Los Angeles,California, June.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Language Processing.Grady Ward.
1996.
Moby thesaurus.
Moby Project.Julie Weeds.
2003.
Measures and Applications of Lex-ical Distributional Similarity.
Ph.D. thesis, Depart-ment of Informatics, University of Sussex.Kazuhide Yamamoto and Takeshi Asakura.
2010.Even unassociated features can improve lexical dis-tributional similarity.
In Second Workshop onNLP Challenges in the Information Explosion Era(NLPIX 2010), pages 32?39, Beijing, China.Torsten Zesch and Iryna Gurevych.
2010.
Wisdomof crowds versus wisdom of linguists - measuringthe semantic relatdness of words.
Natural LanguageEngineering, 16(1):25?59.Maayan Zhitomirsky-Geffet and Ido Dagan.
2009.Bootstrapping Distributional Feature Vector Quality.Computational Linguistics, 35(3):435?461.571
