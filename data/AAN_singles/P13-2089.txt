Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 499?504,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGenerating Recommendation Dialogs by Extracting Information fromUser ReviewsKevin Reschke, Adam Vogel, and Dan JurafskyStanford UniversityStanford, CA, USA{kreschke,acvogel,jurafsky}@stanford.eduAbstractRecommendation dialog systems helpusers navigate e-commerce listings by ask-ing questions about users?
preferences to-ward relevant domain attributes.
Wepresent a framework for generating andranking fine-grained, highly relevant ques-tions from user-generated reviews.
Wedemonstrate our approach on a new datasetjust released by Yelp, and release a newsentiment lexicon with 1329 adjectives forthe restaurant domain.1 IntroductionRecommendation dialog systems have been devel-oped for a number of tasks ranging from productsearch to restaurant recommendation (Chai et al,2002; Thompson et al, 2004; Bridge et al, 2005;Young et al, 2010).
These systems learn user re-quirements through spoken or text-based dialog,asking questions about particular attributes to fil-ter the space of relevant documents.Traditionally, these systems draw questionsfrom a small, fixed set of attributes, such as cuisineor price in the restaurant domain.
However, thesesystems overlook an important element in users?interactions with online product listings: user-generated reviews.
Huang et al (2012) show thatinformation extracted from user reviews greatlyimproves user experience in visual search inter-faces.
In this paper, we present a dialog-based in-terface that takes advantage of review texts.
Wedemonstrate our system on a new challenge cor-pus of 11,537 businesses and 229,907 user reviewsreleased by the popular review website Yelp1, fo-cusing on the dataset?s 4724 restaurants and bars(164,106 reviews).This paper makes two main contributions.
First,we describe and qualitatively evaluate a frame-1https://www.yelp.com/dataset_challenge/work for generating new, highly-relevant ques-tions from user review texts.
The frameworkmakes use of techniques from topic modeling andsentiment-based aspect extraction to identify fine-grained attributes for each business.
These at-tributes form the basis of a new set of questionsthat the system can ask the user.Second, we use a method based on information-gain for dynamically ranking candidate questionsduring dialog production.
This allows our systemto select the most informative question at each di-alog step.
An evaluation based on simulated di-alogs shows that both the ranking method and theautomatically generated questions improve recall.2 Generating Questions from Reviews2.1 Subcategory QuestionsYelp provides each business with category labelsfor top-level cuisine types like Japanese, Coffee& Tea, and Vegetarian.
Many of these top-levelcategories have natural subcategories (e.g., ramenvs.
sushi).
By identifying these subcategories, weenable questions which probe one step deeper thanthe top-level category label.To identify these subcategories, we run LatentDirichlet Analysis (LDA) (Blei et al, 2003) onthe reviews of each set of businesses in the twentymost common top-level categories, using 10 top-ics and concatenating all of a business?s reviewsinto one document.2 Several researchers have usedsentence-level documents to model topics in re-views, but these tend to generate topics about fine-grained aspects of the sort we discuss in Section2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010).We then manually labeled the topics, discardingjunk topics and merging similar topics.
Table 1displays sample extracted subcategories.Using these topic models, we assign a business2We use the Topic Modeling Toolkit implementation:http://nlp.stanford.edu/software/tmt499Category Topic Label Top Wordspizza crust sauce pizza garlic sausage slice saladItalian traditional pasta sauce delicious ravioli veal dishes gnocchibistro bruschetta patio salad valet delicious brie paninideli sandwich deli salad pasta delicious grocery meatballbrew pub beers peaks ale brewery patio ipa brewgrill steak salad delicious sliders ribs tots drinksbar drinks vig bartender patio uptown dive karaokeAmerican (New) bistro drinks pretzel salad fondue patio sanwich windsorbrunch sandwich brunch salad delicious pancakes patioburger burger fries sauce beef potato sandwich deliciousmediterranean pita hummus jungle salad delicious mediterranean wrapitalian deli sandwich meats cannoli cheeses authentic sausagenew york deli beef sandwich pastrami corned fries waitressDelis bagels bagel sandwiches toasted lox delicious donuts yummymediterranean pita lemonade falafel hummus delicious salad bakerysandwiches sandwich subs sauce beef tasty meats delicioussushi sushi kyoto zen rolls tuna sashimi spicyJapanese teppanyaki sapporo chef teppanyaki sushi drinks shrimp friedteriyaki teriyaki sauce beef bowls veggies spicy grillramen noodles udon dishes blossom delicious soup ramenTable 1: A sample of subcategory topics with hand-labels and top words.to a subcategory based on the topic with high-est probability in that business?s topic distribution.Finally, we use these subcategory topics to gen-erate questions for our recommender dialog sys-tem.
Each top-level category corresponds to a sin-gle question whose potential answers are the set ofsubcategories: e.g., ?What type of Japanese cui-sine do you want?
?2.2 Questions from Fine-Grained AspectsOur second source for questions is based on as-pect extraction in sentiment summarization (Blair-Goldensohn et al, 2008; Brody and Elhadad,2010).
We define an aspect as any noun-phrasewhich is targeted by a sentiment predicate.
Forexample, from the sentence ?The place had greatatmosphere, but the service was slow.?
we ex-tract two aspects: +atmosphere and ?service.Our aspect extraction system has two steps.First we develop a domain specific sentiment lex-icon.
Second, we apply syntactic patterns to iden-tify NPs targeted by these sentiment predicates.2.2.1 Sentiment LexiconCoordination Graph We generate a list ofdomain-specific sentiment adjectives using graphpropagation.
We begin with a seed set combin-ing PARADIGM+ (Jo and Oh, 2011) with ?stronglysubjective?
adjectives from the OpinionFinder lex-icon (Wilson et al, 2005), yielding 1342 seeds.Like Brody and Elhadad (2010), we then constructa coordination graph that links adjectives modify-ing the same noun, but to increase precision werequire that the adjectives also be conjoined byand (Hatzivassiloglou and McKeown, 1997).
Thisreduces problems like propagating positive sen-timent to orange in good orange chicken.
Wemarked adjectives that follow too or lie in thescope of negation with special prefixes and treatedthem as distinct lexical entries.Sentiment Propagation Negative and positiveseeds are assigned values of 0 and 1 respectively.All other adjectives begin at 0.5.
Then a stan-dard propagation update is computed iteratively(see Eq.
3 of Brody and Elhadad (2010)).In Brody and Elhadad?s implementation of thispropagation method, seed sentiment values arefixed, and the update step is repeated until the non-seed values converge.
We found that three modifi-cations significantly improved precision.
First, weomit candidate nodes that don?t link to at least twopositive or two negative seeds.
This eliminatedspurious propagation caused by one-off parsing er-rors.
Second, we run the propagation algorithm forfewer iterations (two iterations for negative termsand one for positive terms).
We found that addi-tional iterations led to significant error propaga-tion when neutral (italian) or ambiguous (thick)terms were assigned sentiment.3 Third, we updateboth non-seed and seed adjectives.
This allows usto learn, for example, that the negative seed deca-dent is positive in the restaurant domain.Table 2 shows a sample of sentiment adjectives3Our results are consistent with the recent finding of Whit-ney and Sarkar (2012) that cautious systems are better whenbootstrapping from seeds.500Negative Sentimentinstitutional, underwhelming, not nice, burn-tish, unidentifiable, inefficient, not attentive,grotesque, confused, trashy, insufferable,grandiose, not pleasant, timid, degrading,laughable, under-seasoned, dismayed, tornPositive Sentimentdecadent, satisfied, lovely, stupendous,sizable, nutritious, intense, peaceful,not expensive, elegant, rustic, fast, affordable,efficient, congenial, rich, not too heavy,wholesome, bustling, lushTable 2: Sample of Learned Sentiment Adjectivesderived by this graph propagation method.
Thefinal lexicon has 1329 adjectives4, including 853terms not in the original seed set.
The lexicon isavailable for download.5Evaluative Verbs In addition to this adjectivelexicon, we take 56 evaluative verbs such as loveand hate from admire-class VerbNet predicates(Kipper-Schuler, 2005).2.2.2 Extraction PatternsTo identify noun-phrases which are targeted bypredicates in our sentiment lexicon, we develophand-crafted extraction patterns defined over syn-tactic dependency parses (Blair-Goldensohn et al,2008; Somasundaran and Wiebe, 2009) generatedby the Stanford parser (Klein and Manning, 2003).Table 3 shows a sample of the aspects generated bythese methods.Adj + NP It is common practice to extract anyNP modified by a sentiment adjective.
However,this simple extraction rule suffers from precisionproblems.
First, reviews often contain sentimenttoward irrelevant, non-business targets (Wayne isthe target of excellent job in (1)).
Second, hypo-thetical contexts lead to spurious extractions.
In(2), the extraction +service is clearly wrong?infact, the opposite sentiment is being expressed.
(1) Wayne did an excellent job addressing ourneeds and giving us our options.
(2) Nice and airy atmosphere, but service could bemore attentive at times.4We manually removed 26 spurious terms which werecaused by parsing errors or propagation to a neutral term.5http://nlp.stanford.edu/projects/yelp.shtmlWe address these problems by filtering out sen-tences in hypothetical contexts cued by if, should,could, or a question mark, and by adopting the fol-lowing, more conservative extractions rules:i) [BIZ + have + adj.
+ NP] Sentiment adjec-tive modifies NP, main verb is have, subjectis business name, it, they, place, or absent.
(E.g., This place has some really great yogurtand toppings).ii) [NP + be + adj.]
Sentiment adjective linkedto NP by be?e.g., Our pizza was much toojalapeno-y.
?Good For?
+ NP Next, we extract aspects us-ing the pattern BIZ + positive adj.
+ for + NP, as inIt?s perfect for a date night.
Examples of extractedaspects include +lunch, +large groups, +drinks,and +quick lunch.Verb + NP Finally, we extract NPs that appearas direct object to one of our evaluative verbs (e.g.,We loved the fried chicken).2.2.3 Aspects as QuestionsWe generate questions from these extracted as-pects using simple templates.
For example, the as-pect +burritos yields the question: Do you want aplace with good burritos?3 Question Selection for DialogTo utilize the questions generated from reviews inrecommendation dialogs, we first formalize the di-alog optimization task and then offer a solution.3.1 Problem StatementWe consider a version of the Information RetrievalDialog task introduced by Kopec?ek (1999).
Busi-nesses b ?
B have associated attributes, comingfrom a set Att.
These attributes are a combinationof Yelp categories and our automatically extractedaspects described in Section 2.
Attributes att ?
Atttake values in a finite domain dom(att).
We denotethe subset of businesses with an attribute att tak-ing value val ?
dom(att), as B|att=val.
Attributesare functions from businesses to subsets of values:att : B ?
P(dom(att)).
We model a user in-formation need I as a set of attribute/value pairs:I = {(att1, val1), .
.
.
, (att|I|, val|I|)}.Given a set of businesses and attributes, a rec-ommendation agent pi selects an attribute to ask501Chinese: Mexican:+beef +egg roll +sour soup +orange chicken +salsa bar +burritos +fish tacos +guacamole+noodles +crab puff +egg drop soup +enchiladas +hot sauce +carne asade +breakfast burritos+dim sum +fried rice +honey chicken +horchata +green salsa +tortillas +quesadillasJapanese: American (New)+rolls +sushi rolls +wasabi +sushi bar +salmon +environment +drink menu +bar area +cocktails +brunch+chicken katsu +crunch +green tea +sake selection +hummus +mac and cheese +outdoor patio +seating area+oysters +drink menu +sushi selection +quality +lighting +brews +sangria +cheese platesTable 3: Sample of the most frequent positive aspects extracted from review texts.Input: Information need ISet of businesses BSet of attributes AttRecommendation agent piDialog length KOutput: Dialog history HRecommended businesses BInitialize dialog history H = ?for step = 0; step < K; step++ doSelect an attribute: att = pi(B,H)Query user for the answer: val = I(att)Restrict set of businesses: B = B|att=valAppend answer: H = H ?
{(att, val)}endReturn (H,B)Algorithm 1: Procedure for evaluating a recom-mendation agentthe user about, then uses the answer value to nar-row the set of businesses to those with the de-sired attribute value, and selects another query.Algorithm 1 presents this process more formally.The recommendation agent can use both the set ofbusinesses B and the history of question and an-swers H from the user to select the next query.Thus, formally a recommendation agent is a func-tion pi : B ?
H ?
Att.
The dialog ends after afixed number of queries K.3.2 Information Gain AgentThe information gain recommendation agentchooses questions to ask the user by selectingquestion attributes that maximize the entropy ofthe resulting document set, in a manner similar todecision tree learning (Mitchell, 1997).
Formally,we define a function infogain : Att?
P(B)?
R:infogain(att, B) =?
?vals?P(dom(att))|Batt=vals||B| log|Batt=vals||B|The agent then selects questions att ?
Att thatmaximize the information gain with respect to theset of businesses satisfying the dialog history H:pi(B,H) = argmaxatt?Attinfogain(att, B|H)4 Evaluation4.1 Experimental SetupWe follow the standard approach of using the at-tributes of an individual business as a simulationof a user?s preferences (Chung, 2004; Young et al,2010).
For every business b ?
B we form an in-formation need composed of all of b?s attributes:Ib =?{att?Att|att(b)6=?
}(att, att(b))To evaluate a recommendation agent, we usethe recall metric, which measures how well an in-formation need is satisfied.
For each informationneed I , let BI be the set of businesses that satisfythe questions of an agent.
We define the recall ofthe set of businesses with respect to the informa-tion need asrecall(BI , I) =?b?BI?
(att,val)?I 1[val ?
att(b)]|BI ||I|We average recall across all information needs,yielding average recall.We compare against a random agent baselinethat selects attributes att ?
Att uniformly at ran-dom at each time step.
Other recommendation di-alog systems such as Young et al (2010) selectquestions from a small fixed hierarchy, which isnot applicable to our large set of attributes.4.2 ResultsFigure 1 shows the average recall for the ran-dom agent versus the information gain agent withvarying sets of attributes.
?Top-level?
repeatedlyqueries the user?s top-level category preferences,?Subtopic?
additionally uses our topic modelingsubcategories, and ?All?
uses these plus the as-pects extracted from reviews.
We see that for suf-ficiently long dialogs, ?All?
outperforms the othersystems.
The ?Subtopic?
and ?Top-level?
systemsplateau after a few dialog steps once they?ve asked50200.10.20.30.40.50.60.70.80.911  2  3  4  5  6  7  8  9  10Average RecallDialog LengthAverage Recall by AgentRandomTop-levelSubtopicAllFigure 1: Average recall for each agent.all useful questions.
For instance, most businessesonly have one or two top-level categories, so af-ter the system has identified the top-level cate-gory that the user is interested in, it has no moregood questions to ask.
Note that the informationgain agent starts dialogs with the top-level and ap-propriate subcategory questions, so it is only forlonger dialogs that the fine-grained aspects boostperformance.Below we show a few sample output dialogsfrom our ?All?
information gain agent.Q: What kind of place do you want?A: American (New)Q: What kind of American (New) do you want:bar, bistro, standard, burgers, brew pub, orbrunch?A: bistroQ: Do you want a place with a good patio?A: YesQ: What kind of place do you want?A: ChineseQ: What kind of Chinese place do you want:buffet, dim sum, noodles, pan Asian, PandaExpress, sit down, or veggie?A: sit downQ: Do you want a place with a good lunchspecial?A: YesQ: What kind of place do you want?A: MexicanQ: What kind of Mexican place do you want:dinner, taqueria, margarita bar, or tortas?A: Margarita barQ: Do you want a place with a good patio?A: Yes5 ConclusionWe presented a system for extracting large setsof attributes from user reviews and selecting rel-evant attributes to ask questions about.
Usingtopic models to discover subtypes of businesses, adomain-specific sentiment lexicon, and a numberof new techniques for increasing precision in sen-timent aspect extraction yields attributes that givea rich representation of the restaurant domain.
Wehave made this 1329-term sentiment lexicon forthe restaurant domain available as useful resourceto the community.
Our information gain recom-mendation agent gives a principled way to dynam-ically combine these diverse attributes to ask rele-vant questions in a coherent dialog.
Our approachthus offers a new way to integrate the advantagesof the curated hand-build attributes used in statisti-cal slot and filler dialog systems, and the distribu-tionally induced, highly relevant categories builtby sentiment aspect extraction systems.6 AcknowledgmentsThanks to the anonymous reviewers and the Stan-ford NLP group for helpful suggestions.
The au-thors also gratefully acknowledge the support ofthe Nuance Foundation, the Defense AdvancedResearch Projects Agency (DARPA) Deep Explo-ration and Filtering of Text (DEFT) Program un-der Air Force Research Laboratory (AFRL) primecontract no.
FA8750-13-2-0040, ONR grantsN00014-10-1-0109 and N00014-13-1-0287 andARO grant W911NF-07-1-0216, and the Centerfor Advanced Study in the Behavioral Sciences.ReferencesSasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-ald, Tyler Neylon, George A Reis, and Jeff Reynar.2008.
Building a sentiment summarizer for localservice reviews.
In WWW Workshop on NLP in theInformation Explosion Era.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
The Journal ofMachine Learning Research, 3:993?1022.Derek Bridge, Mehmet H. Go?ker, Lorraine McGinty,and Barry Smyth.
2005.
Case-based recom-mender systems.
Knowledge Engineering Review,20(3):315?320.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.503In Proceedings of HLT NAACL 2010, pages 804?812.Joyce Chai, Veronika Horvath, Nicolas Nicolov, MargoStys, A Kambhatla, Wlodek Zadrozny, and PremMelville.
2002.
Natural language assistant - a di-alog system for online product recommendation.
AIMagazine, 23:63?75.Grace Chung.
2004.
Developing a flexible spoken dia-log system using simulation.
In Proceedings of ACL2004, pages 63?70.Vasileios Hatzivassiloglou and Kathleen R McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of EACL 1997, pages 174?181.Jeff Huang, Oren Etzioni, Luke Zettlemoyer, KevinClark, and Christian Lee.
2012.
Revminer: An ex-tractive interface for navigating reviews on a smart-phone.
In Proceedings of UIST 2012.Yohan Jo and Alice H Oh.
2011.
Aspect and sentimentunification model for online review analysis.
In Pro-ceedings of the Fourth ACM International Confer-ence on Web Search and Data Mining, pages 815?824.Karin Kipper-Schuler.
2005.
Verbnet: A broad-coverage, comprehensive verb lexicon.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings ACL2003, pages 423?430.I.
Kopec?ek.
1999.
Modeling of the information re-trieval dialogue systems.
In Proceedings of theWorkshop on Text, Speech and Dialogue-TSD 99,Lectures Notes in Artificial Intelligence 1692, pages302?307.
Springer-Verlag.Tom M. Mitchell.
1997.
Machine Learning.
McGraw-Hill, New York.Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In Proceedingsof ACL 2009, pages 226?234.Cynthia A. Thompson, Mehmet H. Goeker, and PatLangley.
2004.
A personalized system for conver-sational recommendations.
Journal of Artificial In-telligence Research (JAIR), 21:393?428.Max Whitney and Anoop Sarkar.
2012.
Bootstrappingvia graph propagation.
In Proceedings of the ACL2012, pages 620?628, Jeju Island, Korea.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005.
Opinionfinder: A system for subjectivityanalysis.
In Proceedings of HLT/EMNLP 2005 onInteractive Demonstrations, pages 34?35.Steve Young, Milica Gas?ic?, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, andKai Yu.
2010.
The hidden information state model:A practical framework for POMDP-based spoken di-alogue management.
Computer Speech and Lan-guage, 24(2):150?174, April.504
