Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 735?745,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsThe Unreasonable Effectiveness of Word Representationsfor Twitter Named Entity RecognitionColin Cherry and Hongyu GuoNational Research Council Canadafirst.last@nrc-cnrc.gc.caAbstractNamed entity recognition (NER) systemstrained on newswire perform very badly whentested on Twitter.
Signals that were reliable incopy-edited text disappear almost entirely inTwitter?s informal chatter, requiring the con-struction of specialized models.
Using well-understood techniques, we set out to improveTwitter NER performance when given a smallset of annotated training tweets.
To lever-age unlabeled tweets, we build Brown clus-ters and word vectors, enabling generaliza-tions across distributionally similar words.
Toleverage annotated newswire data, we employan importance weighting scheme.
Taken alltogether, we establish a new state-of-the-arton two common test sets.
Though it is well-known that word representations are useful forNER, supporting experiments have thus far fo-cused on newswire data.
We emphasize the ef-fectiveness of representations on Twitter NER,and demonstrate that their inclusion can im-prove performance by up to 20 F1.1 IntroductionNamed entity recognition (NER) is the task of find-ing rigid designators as they appear in free text andclassifying them into coarse categories such as per-son or location (Nadeau and Sekine, 2007).
NERenables many other information extraction taskssuch as relation extraction (Bunescu and Mooney,2005) and entity linking (Ratinov et al, 2011).There is considerable excitement at the prospectof porting information extraction technology to so-cial media platforms such as Twitter.
Social mediareacts to world events faster than traditional newssources, and its sub-communities pay close attentionto topics that other sources might ignore.
An earlyexample of the potential inherent in social infor-mation extraction is the Twitter Calendar (Ritter etal., 2012), which detects upcoming events (concerts,elections, video game releases, etc.)
based on theanticipatory chatter of Twitter users.
Unfortunately,processing social media text presents a unique set ofchallenges, especially for technologies designed fornewswire: Twitter posts are short, the language isinformal, capitalization is inconsistent at best, andspelling variations and abbreviations run rampant.Armed with an affordable training set of 1,000annotated tweets, we establish a strong baseline forTwitter NER using well-understood techniques.
Webuild two unsupervised word representations in or-der to leverage a large collection of unannotatedtweets, while a data-weighting technique allows usto benefit from annotated newswire data.
Takentogether, these two simple ideas establish a newstate-of-the-art for both our test sets.
We rigorouslytest the impact of both continuous and cluster-basedword representations on Twitter NER, emphasizingthe dramatic improvement that they bring.
We alsobring the experimental methodology of the domainadaptation community to Twitter NER, testing in-domain, out-of-domain and combined training sce-narios, and revealing that it is not trivial to benefitfrom out-of-domain training data.
Finally, an erroranalysis helps us begin to understand which socialmedia challenges are being addressed by our adap-tations, and which problems persist.2 BackgroundOur work builds on a long line of research in dis-criminative tagging (Collins, 2002), and its applica-tion to named entity recognition (McCallum and Li,2003).
Our baseline tagger draws inspiration fromSarawagi and Cohen (2004), who introduce the no-735tion of semi-Markov tagging for NER, and from deBruijn et al (2011), who apply a similar tagger toclinical information extraction.A number of previous studies have closely ex-amined the use of word representations in NER,where one leverages unlabeled data to build featuresthat help the tagger generalize across similar words.Miller et al (2004) introduce this idea and providethe framework to build representation features fromword clusters, while Lin and Wu (2009) extend thistechnique with phrases and sheer masses of unla-beled data.
Turian et al (2010) introduce continuousvectors as alternative word representations, and pro-vide several experiments comparing these with clus-ters.
Recently, Passos et al (2014) have shown howcontinuous representations can be tailored to NERwith a combination of context- and gazetteer-awareobjectives.
All of these studies employ representa-tions only in newswire scenarios.
Ratinov and Roth(2009) investigate cluster representations in a WebNER task, but the performance of their baseline in-dicates that it is not nearly so drastic a domain shiftas our Twitter task.2.1 Adapting to Social MediaThere has been much recent activity in adaptingNLP tools for social media.
Ritter et al (2011) col-lect training data and adapt tools for a number oftasks, including part-of-speech (POS) tagging, shal-low parsing and NER.
Owoputi et al (2013) extendsa line of research on building robust POS taggers forTwitter, and share our focus on the utility of wordrepresentations in this domain.Liu et al (2011) carry out the first study to specif-ically examine NER on Twitter.
They use a nearest-neighbour word classifier stacked with a CRF, alongwith a boot-strapping scheme for semi-supervisedlearning.
Interestingly, they find no utility in us-ing cluster-based word representations, perhaps be-cause their model directly accounts for a type?sglobal context with bag-of-word features.
Ritter etal.
(2011) also examine Twitter NER, developing asemi-supervised technique that uses labeled LDA toproject information from Freebase gazetteers ontounlabeled tweets.
Plank et al (2014) suggest adistant-supervision scheme, creating artificial train-ing data by projecting reliable NER tags from webpages onto the tweets that link to those pages.Fromreide et al (2014) and Plank et al (2014)point out that NER performance can be over-estimated when a system is tested on data extractedfrom the same pool as its training data.
Temporaleffects and annotation biases can result in gains thatdisappear when shifting to another test set.
We fol-low their lead by testing on data that was annotatedindependently from our training data.3 MethodsOur named entity recognizer is a discriminative,semi-Markov tagger, trained online using large-margin updates.
It differs from word-based CRFsystems in three ways: its inference algorithm, itstag structure, and its learning algorithm.
This tag-ger allows us to develop new systems quickly, but itis important to emphasize that the adaptation strate-gies described later in this section can just as easilybe applied to word-based CRFs.Semi-Markov InferenceSarawagi and Cohen (2004) describe a straight-forward extension to the Viterbi algorithm that en-ables the tagging of contiguous phrases instead ofwords.
Because each phrasal entity is tagged as aunit, we can recover entity boundaries without dis-tinguishing between Begin and Inside tags, leavingthe tagger to track only entity classes and Outsidetags.
This in turn allows us to run our tagger with-out Markov features.
Since most entities are sur-rounded by Outside tags, conditioning on previoustag assignments has only limited utility.
Finally, ourphrasal tags enable useful features that consider en-tire entities, such as phrase-identity indicators.Phrasal and Word-level TagsIn word-based models, it is beneficial to not onlyidentify words that Begin entities, but also those thatare in the middle (Inside) or at the end of entities(Last), as well as entities that consist of exactly oneUnique word (Ratinov and Roth, 2009).
Since wetag entire phrases at once, we can easily assign eachword in the phrase to one of these four entity-relativepositions.
Therefore, even though our tagger tracksonly entity class, its word-level features are anno-tated as if we maintained a full BILUO tag set.736Passive-Aggressive LearningWe train our model with a structured version ofthe Passive-Aggressive (PA) algorithm (Crammer etal., 2006).
The benefits of using PA in place of aCRF are that we require only Viterbi inference, andmemory requirements are minimized, as we updatethe model one training sentence at a time.PA is an online, large-margin learning algorithmthat attempts to separate correct sequences from in-correct ones by a margin of 1.
For each update tothe weight vector w, we select a training sentencex and its gold-standard tag sequence y.
We use dy-namic programming to search for a response y?
thatmaximizes the structured hinge loss:1y?
= arg maxy?6=y=[1 + wT(?
(x, y?)?
?
(x, y))](1)where ?
() maps an (x, y) pair to a feature vector.
Ifthe loss is greater than 0, we update our model:w = w + ?(?
(x, y)?
?
(x, y?
))(2)where ?
is an adaptive learning rate that scales theupdate to the smallest step size that achieves 0 loss:?
= min(C,1 + wT(?
(x, y?)?
?
(x, y))||?
(x, y)?
?
(x, y?
)||)(3)C is a hyper-parameter that truncates large steps toprevent over-fitting.
It is related to the C-parameterof an SVM (Martins et al, 2010).
To furtherguard against over-fitting, we use the average of allvectors w seen during training when tagging newtext (Collins, 2002).FeaturesThe feature function ?
(x, y) must decomposeinto the semi-Markov dynamic program:?
(x, y) =?(s,t,yj)?D(x,y)?
(s, t, yj, x) (4)where D is a derivation decomposing (x, y) into Jentity-tag assignments (s, t, yj), each asserting thatthe phrase xs.
.
.
xt?1is assigned the tag yj.
Taggedspans are non-overlapping, and to eliminate spurious1This can be done by running a 2-best tagger.
If the 1-bestanswer is not correct (y?6= y), then it maximizes the loss, oth-erwise, the 2-best answer maximizes the loss.Phrase:[yj], [yj, xs.
.
.
xt?1],[yj, lc(xs.
.
.
xt?1)], [yj, ss(xs.
.
.
xt?1)]Word, for each i s.t.
s ?
i < t:{[yj, xi+k, k]}2k=?2, {[yj, ers,t(i), xi+k, k]}2k=?2,{[yj, lc(xi+k), k]}2k=?2,{[yj, ers,t(i), lc(xi+k), k]}2k=?2,{[yj, ss(xi+k), k]}2k=?2,{[yj, ers,t(i), ss(xi+k), k]}2k=?2,{[yj, pf(n, xi)]}3n=1, {[yj, ers,t(i), pf(n, xi)]}3n=1,{[yj, sf(n, xi)]}3n=1, {[yj, ers,t(i), sf(n, xi)]}3n=1,Table 1: Baseline features ?
(s, t, yj, x).
[str] stands foran indicator feature with the name str; lc() maps a stringonto its lowercased form; ss() maps a string onto its wordshape (?Apple Inc.?
becomes ?Aa Aa.?
); pf(n, xi) andsf(n, xi) are n-character prefixes and suffixes of xi; anders,t(i) maps an absolute sentence position i (s ?
i < t)to a relative entity position drawn from {B, I, L, U}.ambiguity, constrained so that Outside can tag onlysingle-word spans (t = s+ 1).Our baseline feature set, shown in Table 1, closelymimics the set proposed by Ratnaparkhi (1996),covering word identity, prefixes, suffixes and sur-rounding words.
It has been augmented with phrase-identity indicators and hierarchical word-level tags.These conjoin the entity class yjwith the word?sentity-relative position, backing off to yjalone.Most features look only at a single word xi, whichimproves efficiency by allowing the tagger to re-useword-level scores across many phrasal tags.There are some standard NER features that wechose not to include.
We follow Lin and Wu (2009)in omitting POS tags and gazetteers in order to re-duce our dependence on linguistic resources.
We ex-pect similar information to be provided by unsuper-vised word representations, and we test this assump-tion in Section 5.2.
We omit context aggregation,which accounts for the repetition of entities (Ratinovand Roth, 2009), because Twitter?s short messagelength reduces the utility of document-level features.3.1 Word RepresentationsOur primary tool for domain adaptation will be un-supervised word representations, which convey in-formation about a word?s distributional profile.737Brown clusters, for each i s.t.
s ?
i < t:{[yj, brn(n, xi), n]}n?
{2,4,8,12},{[yj, ers,t(i), brn(n, xi), n]}n?
{2,4,8,12}Word vectors, for each i s.t.
s ?
i < t:{[yj, n] = w2v(n, xi)}300n=1,{[yj, ers,t(i), n] = w2v(n, xi)}300n=1Table 2: Word representation features in ?
(s, t, yj, x).brn(n, xi) maps a word xito the first n bits of its Browncluster bit sequence.
w2v(n, xi) maps xito the nthcom-ponent of its word vector, and [str] = v stands for a real-valued feature with name str and value v.Brown ClustersThe Brown clustering algorithm assigns types toa deterministic, hierarchical clustering, which hasbeen trained to optimize the likelihood of a first-order, class-based language model (Brown et al,1992).
The clusters capture both syntactic and se-mantic regularities, and have been shown to performwell as unsupervised part-of-speech taggers (Blun-som and Cohn, 2011).The clusters are organized into a binary tree struc-ture; therefore, each cluster can be represented as abit string that encodes the branching decisions re-quired to reach its leaf from the root.
By truncatingthe bit string at different prefix lengths, one can ac-cess different granularities of clusters.
Cluster mem-bership can then be used to create indicators similarto the baseline?s word identity features.
This resultsin two feature templates, shown in Table 2.2This technique has been previously applied toboth newswire NER (Miller et al, 2004; Turian etal., 2010; Passos et al, 2014) and Twitter NER (Rit-ter et al, 2011; Plank et al, 2014).
But previouswork on Twitter NER has not directly tested the im-pact of Brown clusters; instead, they generally ap-pear as part of an adapted baseline.Word VectorsAn alternative word representation maps eachword type deterministically to a low-dimensionalcontinuous vector space.
This technique was orig-inally used as the bottom layer for continuous-spacelanguage models (Bengio et al, 2003), where the2We also experimented with templates over clusters and vec-tors for surrounding words, to no benefit.type-to-vector mapping can be learned with back-propagation.
However, Mikolov et al (2013) haveshown that useful vector representations can belearned more efficiently by eschewing the language-modeling objective.
Their skip-gram model, whichwe adopt here, optimizes for each token, the likeli-hood of the tokens in a window surrounding it.
Thistraining process creates a linear classifier that pre-dicts words conditioned on the central token?s vec-tor representation.
The classifier and the word vec-tors are learned simultaneously, but once training iscomplete, the classifier is usually discarded, leavingonly the vectors.These continuous representations project wordsinto a low-dimensional space.
Words that tend tohave similar contexts, and therefore similar syntac-tic and semantic properties, will tend to be near oneanother in this space.
We incorporate these represen-tations into our NER system as real-valued featuresof each word xi, as shown in Table 2.3.2 Data WeightingOur next tool for domain adaptation is a small poolof in-domain, annotated data.
The easiest way tomake use of this data is to append it to our large poolof out-of-domain training data, which is what hasbeen done in previous work on Twitter NER (Rit-ter et al, 2011; Plank et al, 2014).
However, wehave the strong intuition that greater weight shouldbe placed on the in-domain data.Assume that for each training pair (x, y) we alsohave an importance weight ?.
In our case, all out-of-domain pairs will share one value for ?, and allin-domain pairs will share another, higher ?.
Wemodify our PA learner to calculate ?
using a ver-sion of Equation 3 that replaces C with ?C.
Unlikescaling ?
directly, scaling C has the desirable prop-erty of having even high-?
examples stop updatingat precisely 0 loss, just as if we had duplicated thattraining example ?
times (Karampatziakis and Lang-ford, 2010).
If we view C as a regularization term,then this modification can also be interpreted as im-plementing example-specific regularization.
Impor-tance weights can also be incorporated into CRFsby modifying their training objective; however, thisis not a standard feature of most CRF packages.738Data Lines Types Tokens # PER # LOC # ORGFin10 (Train) 1,000 4,865 17,276 192 143 172Fin10Dev (Test) 1,975 7,734 33,770 325 279 287Rit11 (Test) 2,394 8,686 46,469 454 377 280Fro14 (Test) 1,545 5,392 20,666 390 163 200CoNLL (Train) 14,041 20,752 203,621 6,601 7,142 6,322Unlabeled Tweets 98M 57M 1,995M ?
?
?Table 3: Details of our NER-annotated corpora.
A line is a tweet in Twitter and a sentence in newswire.4 Experimental DesignVital statistics for all of our data sets are shown inTable 3.
For in-domain NER data, we use three col-lections of annotated tweets: Fin10 was originallycrowd-sourced by Finin et al (2010), and was man-ually corrected by Fromreide et al (2014), whileRit11 (Ritter et al, 2011) and Fro14 (Fromreide etal., 2014) were built by expert annotators.
We divideFin10 temporally into a training set and a develop-ment set, and we consider Rit11 and Fro14 to be ourtest sets.
This reflects a plausible training scenario,with train and dev drawn from the same pool, butwith distinct tests drawn from later in time.
Thesethree data sets were collected and unified by Plank etal.
(2014), who normalized the tags into three entityclasses: person (PER), location (LOC) and organi-zation (ORG).
The source text has also been normal-ized; notably, all numbers are normalized to NUM-BER, and all URLs and Twitter @user names havebeen normalized to URL and @USER respectively.In the gold-standard, we choose to reverse a taggingnormalization performed by Plank et al (2014), whohad post-processed the data so that all @user namesare tagged as PER.
These tags are trivial to replicate,and we found that they inflate scores quite dramat-ically.
Therefore, all @user names are untagged inboth the gold standard and our system outputs.We use the CoNLL 2003 newswire training set asa source of out-of-domain NER annotations (TjongKim Sang and De Meulder, 2003).
The source texthas been normalized to match the Twitter NER data,and we have removed the MISC tag from the gold-standard, leaving PER, LOC and ORG.Finally, we also use a large corpus of unannotatedtweets, collected from between May 2011 and April2012.
It has been tokenized by the CMU Twok-enizer,3but is otherwise unnormalized.4.1 Hyper-parameter ConfigurationOur NER system is trained for 10 epochs with itsregularization parameter C set to 0.01.We train our word vectors with an in-house im-plementation of word2vec (Mikolov et al, 2013),with vector size set to 300, a hierarchical soft-maxobjective, down-sampling frequent words at a rateof 0.001, a window-size of 10 tokens, and a mini-mum frequency count of 10.
When run on our unan-notated tweets, this produces vector representationsfor 2.5M types.
We generate a random vector, witheach component sampled from the standard normal,to use as the representation for any word that didnot occur in our unlabeled data, including begin- andend-of-sentence markers.
We do not scale the vec-tors before using them as NER features.We train Brown clusters on the same data us-ing the implementation by Liang (2005), with 1,000clusters and a minimum frequency of 10, resultingin cluster assignments for the same 2.5M types.5 ResultsWe evaluate our various NER taggers using theCoNLL 2003 metrics: phrase-level precision, recall,and balanced F-measure (F1).We begin by testing our system on the CoNLLnewswire task, both to confirm that our implemen-tation is reasonable, and to help situate the Twitterresults that appear later.
We train on the unmodi-fied CoNLL training corpus, and report F1 on theCoNLL development and test sets.
We compareour baseline to the baseline from Ratinov and Roth(2009) (RR09), and we compare our representation-enhanced system (+Reps) to their ?All External3http://www.ark.cs.cmu.edu/TweetNLP/739System Dev F1 Test F1RR09 Baseline 89.2 83.6Our Baseline 90.4 84.3RR09 Base + All External 92.5 88.6Our Base + Reps 91.6 88.0Table 4: Performance on newswire (CoNLL) data.System Fin10Dev Rit11 Fro14 AvgCoNLL 27.3 27.1 29.5 28.0+ Brown 38.4 39.4 42.5 40.1+ Vector 40.8 40.4 42.9 41.4+ Reps 42.4 42.2 46.2 43.6Fin10 36.7 29.0 30.4 32.0+ Brown 59.9 53.9 56.3 56.7+ Vector 61.5 56.4 58.4 58.8+ Reps 64.0 58.5 60.2 60.9CoNLL+Fin10 44.7 39.9 44.2 42.9+ Brown 54.9 52.9 58.5 55.4+ Vector 58.9 55.2 59.9 58.0+ Reps 58.9 56.4 61.8 59.0+ Weights 64.4 59.6 63.3 62.4Table 5: Impact of our components on Twitter NER per-formance, as measured by F1, under 3 data scenarios.Knowledge?
system.
Both use Brown clusters, butRR09 uses Wikipedia gazetteers where we use wordvectors.
Results are shown in Table 4.We achieve broadly comparable scores in bothsettings.
Our external knowledge features are notas useful as theirs, which may be due to our lack ofWikipedia gazetteers, or due to a domain mismatchin our unannotated training data.
Their clusters aretrained on the 1996 Reuters corpus, a superset of theCoNLL data, matching it in both era and domain.Conversely, our clusters and vectors are both trainedon tweets from 2011, so it is somewhat surprisingthat they help to the extent that they do.5.1 Performance on TwitterOur primary results are shown in Table 5, where wecompare our word representation and data weightingtechniques under three scenarios: training on out-of-domain data only (CoNLL), on in-domain data only(Fin10), and on both.
Our data weighting technique(+Weights, see Section 3.2) only applies when weuse both training sets.
We used Fin10Dev to deter-System Prec Rec F1CoNLL 43.0 49.8 46.2Fin10 75.3 50.2 60.2CoNLL+Fin10 66.0 58.0 61.8+Weights 73.8 55.4 63.3Table 6: Precision, recall and F1 on the Fro14 test setwith the Base+All Reps feature set.mine our importance weights, selecting ?
= 0.01for CoNLL and ?
= 1 for Fin10.
For these experi-ments, we test Brown clusters (+Brown), word vec-tors (+Vector), and both together (+Reps).Our Twitter NER results are much lower than thenewswire results from Table 4, with our best Twit-ter system scoring more than 25 F1 below our bestCoNLL system.
But the picture would look muchworse without word representations, which boostperformance in every training scenario.
Our bestrepresentation-free system lags nearly 20 F1 behindour best system that uses representations.Looking across scenarios, we note thatCoNLL+Reps outperforms CoNLL+Fin10 on 2out of 3 tests.
This is interesting, as it shows that,given the hypothetical choice between collecting100 million unannotated tweets for word representa-tions, and collecting one thousand annotated tweetsfor NER training, we are better served by the unan-notated data.
Of course, it is even better to use both;their combined benefit in CoNLL+Fin10+Reps ismore than additive.Across all data scenarios and test sets, Brownclusters help less than word vectors.
This contradictsthe observations from Turian et al (2010), who gen-erally found Brown clusters to perform best.
Thismay be because of our domain adaptation scenario,or it could be due to our use of word2vec, whichdid not exist at the time of the Turian study.
Thecombination of Brown clusters and word vectors isconsistently better than using either alone.
The tworepresentations track different sorts of information:our use of a large window leads word2vec to buildtopic-focused vectors (Turney, 2012), while Brownclustering is naturally more local, creating very syn-tactic, part-of-speech-like clusters.
It is easy to seehow both types of information can be useful to NER.Ritter et al (2011) report that including out-of-740????????????????????????????
??
???
???????????????????????????????????????????????????????????????????
?Figure 1: F1 averaged over all 3 test sets as we add Fin10training data to CoNLL.domain data hurts NER performance.
Focusing onthe lines Fin10+Reps and CoNLL+Fin10+Reps, wesee the same problem.
In the presence of word rep-resentations, unweighted CoNLL data hurts perfor-mance when added to a Fin10 system.
Fortunately,the inclusion of importance weights (+Weights) re-verses this trend, giving us our best result on eachtest.
We saw no consistent improvement from im-portance weights on the representation-free system.To better understand the benefits of importanceweights, Table 6 reports detailed scores for theFro14 test set under the Base+Reps feature set, aswe vary training scenarios.
Results on the other testsets are similar.
The Fin10 system achieves high pre-cision but low recall, while the CoNLL+Fin10 doesthe opposite.
This is because the CoNLL data ismuch more entity-dense than the Twitter data, whichbiases systems trained on CoNLL to return too manyentities.
By down-weighting the CoNLL data, wereduce this bias and gain 6.8 points of precision atthe cost of only 2.6 points of recall.Figure 1 gives learning curves as we add Fin10data to CoNLL across several feature sets.
There is asteady improvement for all systems as the in-domaindata grows, and importance weighting increases theimpact of in-domain data even at very low quanti-ties.
Though the curves shows no sign of flatteningout, note that the x-axis is log-scaled.We have access to roughly 100 million tweets forunsupervised representation learning.
Figure 2 pro-vides a learning curve for our Reps+Weights systemas we increase the percentage of unlabeled tweetsused to train both Brown clusters and word vec-tors from 1.5% to 100% of that data, doubling the???????????????????????????????????????????
?
?
????
??
??
????????????????????????????????????????????????????
?Figure 2: F1 averaged over all 3 test sets as we increasethe percentage of tweets used to build representations.Test Set PER LOC ORGFin10Dev 71.3 72.4 48.8Rit11 70.8 61.9 36.9Fro14 69.4 70.2 42.6Table 7: F1 for our All Data+Reps+Weights system, or-ganized by entity class.amount with each step.
With only 1.5 million tweets,average performance is already very good, and wecan see that the benefits of scale are starting to leveloff after we clear 12.5 million.Table 7 reports our best system?s performance byentity class.
For all three test sets, ORG is the mostdifficult.
ORG is perhaps the broadest entity class,but we suspect it is also the most likely to be anno-tated inconsistently, as it is rife with subtle distinc-tions: bands (ORG) versus musicians (PER); com-panies (ORG) versus their products (O); and teams(ORG) versus their home cities (LOC).
During aninspection of 25 incorrect ORG predictions by ourbest system, drawn from a test on Fin10Dev, wefound 10 cases where the gold standard was ques-tionable.
Two of these incorrectly placed ?the?inside a chunk, (?
[the Mariners]?
is wrong; ?the[Mariners]?
is right), while the remaining 8 involvedcompany-product distinctions, which are tricky evenfor human annotators.
The NER task is not alwaysas intuitive as we would like, and organizations tendto highlight these difficulties.5.2 Comparison with Linguistic ResourcesThus far, we have restricted ourselves to a settingwithout access to linguistic resources, but for some741Base +X Reps +X?
42.9 62.4[P]OS Tags 47.1 63.0[G]azetteers 52.8 63.2[P]+[G] 55.6 63.5Table 8: Adding linguistic resources to our baselineand representation-enabled systems, as measured by F1averaged over 3 test sets.
All systems are trained onCoNLL+Fin10, and all but Base+?
use data weighting.languages, such as English, rich resources exist andcan be very useful.
We now examine how wordrepresentations compare and interact with gazetteersand POS taggers.For gazetteers, we use those included with the Illi-nois NER system (Ratinov and Roth, 2009), gen-erating features that indicate when a word appearsas part of a phrase found in a gazetteer.
For POStags, we use the CMU Twitter Tagger (Owoputi etal., 2013), and generate POS tag indicators for thecurrent word and for tags within a 2-word window.For some of our corpora, notably CoNLL and Rit11,the corpus tokenization did not match the POS tag-ger?s tokenization.
We resolve mismatches by al-lowing the POS tagger to further tokenize the inputsentence to better match its assumptions.
After POStagging, we merge any split tokens back to the orig-inal tokenization, picking a representative tag fromamong merged tags according to a priority list (verb> noun > adjective, etc.).
The POS tags may haveperformed better if we had used the tagger?s nativetokenization throughout.4Results of our comparison are shown in Table 8.Comparing Base+?
and Base+[P]+[G], we seethat linguistic resources boost the baseline?s perfor-mance considerably.
Turning to Reps+[P]+[G], wesee that adding word representations to linguistic re-sources provides another substantial boost of 7.9 F1.Conversely, adding linguistic resources to a systemthat already has representations increases F1 by only1.1 points, indicating that not much new informationis being added.
The per-feature analysis indicatesthat much of this boost comes from the gazetteers.4Inconsistent tokenization also hinders the word representa-tions, which were constructed from a corpus tokenized by theCMU Twokenizer.System Rit11 Fro14PHMS14 Baseline 77.4 82.1PHMS14 Dict ?Web 78.5 83.9All Data+Reps+Weights 82.3 86.4All Data+Ling+Reps+Weights 82.6 86.9Table 9: Comparison with the state-of-the-art, reportingtest F1.
Both the gold-standard and the system outputshave @user names deterministically tagged as PER.5.3 Comparison with the State-of-the-ArtIn Table 9, we compare our best system, includinglinguistic resources, to the state-of-the-art results re-ported by Plank et al (2014).5In order to createa fair comparison, we post-process both our systemoutput and the gold-standard to tag all @user namesas PER, just as they do.Like our system, their baseline includes CoNLLand Twitter data, and uses Brown clusters trainedon a comparable number of unlabeled tweets.
Theirstrongest system uses distant supervision over linkedweb-pages to create artificial training data.
But weare able to outperform it with our vector representa-tions and importance weights.
Note that this com-parison is not perfect, as they train on a much largerpool of crowd-sourced, NER-annotated tweets, con-sisting of 170k tokens compared to our 17k.
Thesize of their training data is balanced by the fact thatits annotations were automatically correctly usingMACE (Hovy et al, 2013), where ours were cor-rected manually, making it unclear which group hasthe advantage.
Nonetheless, our results establish anew state-of-the-art for both test sets, and they do sousing only 1k annotated tweets.6 AnalysisWe inspected 100 tweets from the Rit11 test set,focusing on the output from our primary system,Base+Reps+Weights, and our baseline, Base, bothtrained on the CoNLL+Fin10 data.
We noted caseswhere the primary system improved upon the base-line, and cases where it failed to achieve the gold-standard, and placed the phenomena we observedinto bins.
In general, the baseline was observed5We omit the Fin10 test set from this comparison, as Planket al (2014) test on the entirety of Fin10, while we have dividedit into training and development sets.742(a) RT @USER : Christmas:PER was so much better when there was a santa :( #allteensthingsRT @USER : Christmas was so much better when there was a santa :( #allteensthings(b) Lmao .
I have a feeling Imma:ORG get yelled at tomorrow .
Big time .
XD Ehh oh wellLmao .
I have a feeling Imma get yelled at tomorrow .
Big time .
XD Ehh oh well(c) I pray an give God glory even when im in pain , hurting , or crying .I pray an give God:PER glory even when im in pain , hurting , or crying .
(d) Anyone know what days/times that you can smoke hookah at the mix ( cma center ) in corbin:PER .Anyone know what days/times that you can smoke hookah at the mix ( cma center ) in corbin:LOC .Figure 3: (a,b): examples where the baseline (top) is improved by our final system (bottom)(c,d): examples where our final system (top) falls short of the gold-standard (bottom)to rely heavily on local context and capitalization,while the primary system has a much stronger globalprior on a given type?s entity assignment.Reps+Weights improved the baseline in 54 out of100 tweets.
There were 31 cases where the primarysystem corrected a baseline error caused by a mis-leading capitalization cue.
Some of these, such asFigure 3(a) are patched by world knowledge pro-vided by word representations, but many simply re-flect a reduced reliance on capitalization.
We weresurprised to find only 11 cases where Twitter?s infor-mal language led to an error, often due to a vaguelyname-shaped colloquialism, such as in 3(b).
6 ofthese 11 cases were fixed by the primary system.Reps+Weights fell short of the gold-standard in62 of 100 tweets.
We observed 39 recall errors thatwere difficult to divide into smaller bins.
Theseentities were often missed despite clear capitaliza-tion cues, as in Figure 3(c).
This particular exam-ple is actually a symptom of inconsistent annota-tion: CoNLL and Rit11 consistently annotate God asa person, while our Fin10 training data leaves Goduntagged.
The next largest class of errors consistsof 11 problems caused by uniform casing (all capsor all lowercase).
We also have 5 remaining errorsdue to informal language, which are interesting, asthey highlight gaps in our representations.
These in-clude cases where the system generates false entitiesfor variants of rare words (Tidying?
Tidyin), or un-usual lengthenings (Yayaayayay, as opposed to thewell-attested Yayayayay).
We also saw cases whereentities were missed due to creative punctuation (GoV-I-K-I-N-G-S!).
Finally, we found 4 cases wherethe system actually over-relies on its word represen-tations, such as in 3(d), where the global PER in-terpretation of corbin overrides a fairly strong LOCsignal provided by the local context word in.7 DiscussionWe have shown that the combination of Brown clus-ters, word vectors, and a simple data weightingscheme is sufficient to establish a new state-of-the-art on two Twitter NER test sets, using only 1,000annotated tweets.
We have designed our experi-ments to emphasize the dramatic impact of wordrepresentations in this domain, and to clarify the ef-fects of in- and out-of-domain training sets.Word representations learned on a large, unla-beled Twitter corpus have addressed a surprisingnumber of issues with inconsistent capitalizationand informal language.
However, our continuingproblems with uncased tweets and unusual colloqui-alisms demonstrate that there are still many human-readable words that remain a mystery to our sys-tem.
In response to these observations, we wouldlike to investigate more flexible representations, per-haps similar to those of Botha and Blunsom (2014),who use a linear combination of morpheme vectorsto create representations that can generalize acrosswords with similar forms.AcknowledgmentsThanks to the anonymous reviewers for their helpfulcomments, to Alan Ritter for providing us with ourcorpus of unlabeled tweets, and to Barbara Plank forproviding us with our three labeled corpora.743ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal pitman-yor process hmm for unsupervised part ofspeech induction.
In ACL, pages 865?874, Portland,Oregon, USA, June.Jan A. Botha and Phil Blunsom.
2014.
CompositionalMorphology for Word Representations and LanguageModelling.
In ICML, Beijing, China.Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-cent J Della Pietra, and Jenifer C Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional linguistics, 18(4):467?479.Razvan C Bunescu and Raymond J Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In EMNLP, pages 724?731.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In EMNLP.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch.Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, JoelMartin, and Xiaodan Zhu.
2011.
Machine-learnedsolutions for three stages of clinical information ex-traction: the state of the art at i2b2 2010.
Jour-nal of the American Medical Informatics Association,18(5):557?562.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in twitter data with crowd-sourcing.
In Proceedings of the NAACL HLT 2010Workshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk, pages 80?88.Hege Fromreide, Dirk Hovy, and Anders S?gaard.
2014.Crowdsourcing and annotating NER for Twitter #drift.In LREC, pages 2544?2547, Reykjavik, Iceland.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,and Eduard Hovy.
2013.
Learning whom to trustwith mace.
In HLT-NAACL, pages 1120?1130, At-lanta, Georgia, June.Nikos Karampatziakis and John Langford.
2010.
On-line importance weight aware updates.
arXiv preprintarXiv:1011.1576.Percy Liang.
2005.
Semi-supervised learning for natu-ral language.
Ph.D. thesis, Massachusetts Institute ofTechnology.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of the JointConference of the ACL and the AFNLP, pages 1030?1038, Singapore, August.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011.
Recognizing named entities in tweets.
InACL, pages 359?367, Portland, Oregon, USA, June.Andr?e F. T. Martins, Kevin Gimpel, Noah A. Smith,Eric P. Xing, Pedro M. Q. Aguiar, and M?ario A. T.Figueiredo.
2010.
Learning structured classifiers withdual coordinate descent.
Technical Report CMU-ML-10-109, Carnegie Mellon University.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In CoNLL, pages 188?191.
Association for Computa-tional Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In ICLR Workshop.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrimi-native training.
In HLT-NAACL, pages 337?342.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Lingvisti-cae Investigationes, 30(1):3?26.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A Smith.
2013.Improved part-of-speech tagging for online conversa-tional text with word clusters.
In HLT-NAACL, pages380?390.Alexandre Passos, Vineet Kumar, and Andrew McCal-lum.
2014.
Lexicon infused phrase embeddings fornamed entity resolution.
In CoNLL, pages 78?86.Barbara Plank, Dirk Hovy, Ryan McDonald, and AndersS?gaard.
2014.
Adapting taggers to Twitter with not-so-distant supervision.
In COLING, pages 1783?1792,Dublin, Ireland.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL, pages 147?155.Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-son.
2011.
Local and global algorithms for disam-biguation to wikipedia.
In ACL, pages 1375?1384.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In EMNLP, pages 133?142.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An exper-imental study.
In EMNLP, pages 1524?1534, Edin-burgh, Scotland, UK.Alan Ritter, Oren Etzioni, Sam Clark, et al 2012.
Opendomain event extraction from twitter.
In KDD, pages1104?1112.744Sunita Sarawagi and William W Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
In NIPS, pages 1185?1192.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In CoNLL,pages 142?147.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In ACL, pages 384?394.Peter D Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, pages 533?585.745
