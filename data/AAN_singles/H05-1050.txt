Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 395?402, Vancouver, October 2005. c?2005 Association for Computational LinguisticsBootstrapping Without the Boot?Jason Eisner and Damianos KarakosCenter for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218 USA{eisner,damianos}@jhu.eduAbstract?Bootstrapping?
methods for learning require a small amountof supervision to seed the learning process.
We show that itis sometimes possible to eliminate this last bit of supervision,by trying many candidate seeds and selecting the one with themost plausible outcome.
We discuss such ?strapping?
methodsin general, and exhibit a particular method for strapping word-sense classifiers for ambiguous words.
Our experiments on theCanadian Hansards show that our unsupervised technique is sig-nificantly more effective than picking seeds by hand (Yarowsky,1995), which in turn is known to rival supervised methods.1 IntroductionSome of NLP?s most interesting problems have to do withunsupervised learning.
Human language learners are ableto discover word senses, grammatical genders, morpho-logical systems, grammars, discourse registers, and soforth.
One would like to build systems that discover thesame linguistic patterns in raw text.
For that matter, onewould also like to discover patterns in bilingual text (fortranslation), in document collections (for categorizationand retrieval), and in other data that fall outside the scopeof humans?
language learning.There are relatively few successful methods for fullyunsupervised learning from raw text.
For example,the EM algorithm (Dempster et al, 1977) extracts the?wrong?
patterns or gets stuck in local maxima.One of the most promising avenues in recent years hasbeen the use of ?minimally supervised?
methods.
Suchmethods are initialized with some sort of ?seed?
thatgrows into a full classifier (or generative model).
Wesay that a seed is ?fertile?
if it grows into a classifier (ormodel) that performs well on some desired criterion.Ordinarily, it is up to a human to choose a seed thathe or she intuitively expects to be fertile.
While this maybe easy when building a single classifier, it is prohibitivewhen building many classifiers.
For example, we maywish to build?
word-sense classifiers for all words of a language (e.g.,to get sharper lexical translation probabilities in a ma-chine translation system)?
named-entity extractors for many languages?
new clusters or classifiers every day (for an evolvingdocument collection)?We thank David Yarowsky for advice on the choice of dataand for the plant/tank dataset.?
new clusters or classifiers every minute (for the docu-ment sets retrieved by ad hoc queries)?
many distinct classifiers that correspond to differentviews of the data1Even when building a single classifier, a human may notknow how to pick a good seed when working with anunfamiliar language or sublanguage, or when trying toinduce less intuitive hidden variables, such as grammarrules or fine-grained senses.
And there is no reason toexpect humans to have good intuitions about seeds formining non-linguistic data such as consumer purchasingrecords.This paper considers how to remove this last elementof supervision.
Our idea is to guess a number of plausi-ble seeds, build a classifier for each one, and then try todetermine which of the seeds have grown successfully.For example, to discover the two senses of the En-glish word drug, we grow 200 classifiers (from differentseeds) that attempt to partition instances of drug into twoclasses.
We have no direct supervision about which ofthe resulting partitions corresponds to the true sense dis-tinction.
Instead, we rely on clues that tend to signal thata seed was fertile and led to a good partition.
The cluesare not specific to the word drug, but they may have beendemonstrated to be good clues in general for successfullygrown word sense disambiguators.Demonstrated how?
If we consider more than one clue,we may need some data to learn which clues to trust, andtheir relative weights.
Our method is unsupervised in theconventional sense, as it obtains a classifier for drug withno supervision about drug.
However, to learn what goodclassifiers generally look like2 for this task, we first use1A word token or document can be characterized by a 20-bitvector, corresponding to its classifications by 20 different binaryclassifiers.
These vectors are detailed abstract representations ofthe words or documents.
They can be clustered, or all their bitscan be included as potentially relevant features in another task.2Ando and Zhang (2005) independently used this phrase, fora semi-supervised, cross-task learner that differs from our unsu-pervised, cross-instance learner.
Both their work and ours tryto transfer knowledge to a target problem from many artificialsupervised ?auxiliary problems,?
which are generated from un-labeled data (e.g., our pseudoword disambiguation problems).However, in their ?structural learning,?
the target problem issupervised (if inadequately), and the auxiliary problems (super-vised instances of a different task) are a source of useful hiddenfeatures for the classifier.
In our ?strapping,?
the target task isunsupervised, and the auxiliary problems (supervised instances395supervised data for a few other ambiguous words?orambiguous pseudowords, a kind of artificial data wheresupervision comes for free.
This supervision?s effect ondrug might be called cross-instance learning.To take another metaphor, minimally supervised learn-ing is often called ?bootstrapping.?
Our goal is to allow amethod to pull itself up by its own bootstraps3 even whenit has none.
It places its stocking feet in anything handy,pulls on what it hopes to be sturdy straps, and checks tosee how high it got.We dub this family of methods ?bootstrapping withoutthe boot,?
or ?strapping?
for short.
The name is meantto evoke ?bagging?
and ?boosting?
?other methods thattrain and combine multiple classifiers of the same form.However, we are careful to point out that strapping, un-like those theoretically motivated methods, is an unsuper-vised learning technique (in the sense explained above).The clusters or other hidden variables extracted by thewinning classifier may or may not be the ones that onehad hoped to find.
Designing a strapping algorithm for aparticular task requires more art than designing a super-vised learner: one must invent not only appropriate fea-tures for classifying the data, but also appropriate cluesfor identifying ?successful?
classifiers.2 BootstrappingTo show where strapping might be useful, we briefly re-view a range of successful bootstrapping work.
We con-sider different tasks.
Given an instance of the task and aseed s for that instance, one bootstraps a classifier Cs thatcan classify examples of the task instance.2.1 The Yarowsky algorithmYarowsky (1995) sparked considerable interest in boot-strapping with his successful method for word sense dis-ambiguation.
An instance of this task involves a homony-mous word such as drug.
A seed for the instance is a pairof words that are strongly associated, respectively, withthe two senses of drug, such as (trafficking, therapy).
Anexample is a token of drug.For our purposes, a bootstrapping method can be re-garded almost as a black box.
However, we reviewthe details of the Yarowsky algorithm to illustrate howbootstrapping is possible, and why some seeds are bet-ter than others.
We will use these intuitions later in de-signing a method to strap the Yarowsky algorithm on aof the same task) are a source of clues for a meta-classifier thatchooses among classifiers grown from different seeds.
In short,their auxiliary problems help train the target classifier directly,while ours help train only a simple meta-classifier that choosesamong many unsupervised target classifiers.
We use far fewerauxiliary problems but ours must be instances of the target task.3The reference is to Baron Munchausen, a fictional 18th-century adventurer who rescued himself from a pit in this way.It is distinct from the ?bootstrap?
in non-parametric statistics.new instance?i.e., a method for automatically choosingseeds that discover a true sense distinction.A learned classifier for the instance drug is an ordereddecision list of contextual features (such as the presenceof dealer nearby) that strongly indicate one or the othersense of drug.
Given a sample token of drug, the classi-fier picks a sense according to the single highest-rankedfeature that is present in the token?s context.To bootstrap a decision-list classifier from a seed,Yarowsky starts with all examples of drug that can beclassified by using the seed words as the only features.These few examples are used as supervised data to traina longer decision list, which includes the seed words andany other features that suffice to distinguish these exam-ples with high confidence.
This longer decision list cannow classify further examples, which are used to train anew and even longer decision list, and so on.Yarowsky?s method works if it can maintain high ac-curacy as it gradually increases its coverage.
A preciseclassifier at iteration t tends to accurately classify newexamples.
This tends to produce a still-accurate classifierwith greater coverage at iteration t + 1.The method fails if the initial classifier is inaccurate(i.e., if the two seed words do not accurately pick out ex-amples of the two senses).
It may also fail if at somepoint, by bad luck on sparse data, the process learns someinappropriate features.
If the classifier at iteration t issufficiently polluted by bad features, the classifier at iter-ation t + 1 will start trying to distinguish examples thatdo not correspond to different senses, which may lead toeven worse classifiers on subsequent iterations.
However,some alternative seed may have escaped this bad luck bysprouting a different set of examples.2.2 A Few Other Applications of BootstrappingInspired by Yarowsky, Blum and Mitchell (1998) built aclassifier for the task of web page classification.4 Theyconsidered only one instance of this task, namely distin-guishing course home pages from other web pages at acomputer science department.
Their seed consisted of 3positive and 9 negative examples.
Strapping a web pageclassifier would mean identifying seeds that lead to other?natural classes?
of web pages.
Strapping may be usefulfor unsupervised text categorization in general.Riloff et al (2003) learned lists of subjective nounsin English, seeding their method with 20 high-frequency,strongly subjective words.
This seed set was chosen man-ually from an automatically generated list of 850 can-4More precisely, they bootstrapped two Naive Bayesclassifiers?one that looked at page content and the other thatlooked at links to the page.
This ?co-training?
approach has be-come popular.
It was also used by the Cucerzan and Yarowskypapers below, which looked at ?internal?
and ?external?
featuresof a phrase.396didate words.
Strapping their method would identifysubjective nouns in other languages, or other ?naturalclasses?
of English words.Query expansion in IR searches for more documents?similar to?
a designated relevant document.
This prob-lem too might be regarded as searching for a naturalclass?a small subset of documents that share some prop-erty of the original document?and approached using it-erative bootstrapping.
The seed would specify the origi-nal document plus one or two additional words or docu-ments initially associated with the ?relevant?
and/or ?ir-relevant?
classes.
Strapping would guess various differ-ent seeds that extended the original document, then try todetermine which seeds found a cohesive ?relevant set.
?Collins and Singer (1999) bootstrapped a system forclassifying phrases in context.
Again, they consideredonly one instance of this task: classifying English propernames as persons, organizations, or locations.
Their seedconsisted of 7 simple rules (?that New York, California,and U.S. are locations; that any name containing Incor-porated is an organization; and that I.B.M.
and Microsoftare organizations?).
Strapping such a classifier would au-tomatically discover named-entity classes in a differentlanguage, or other phrase classes in English.Cucerzan and Yarowsky (1999) built a similar systemthat identified proper names as well as classifying them.Their seed consisted of a list of 40 to 300 names.
Largeseeds were not necessary for precision but did help recall.Cucerzan and Yarowsky (2003) classified masculinevs.
feminine nouns.
They experimented with several taskinstances, namely different Indo-European languages.
Ineach instance, their seed consisted of up to 30 feminineand 30 masculine words (e.g., girl, princess, father).Many more papers along these lines could be listed.
Arather different task is grammar induction, where a taskinstance is a corpus of text in some language, and thelearned classifier is a parser.
Following Chomsky (1981),we suggest that it may be possible to seed a grammarinduction method with a small number of facts about theword order of the language: the basic clause order (SVO,SOV, etc.
), whether pronominal subjects may be omitted(Chomsky?s ?pro-drop?
parameter), etc.
These facts canfor example be used to construct a starting point for theinside-outside algorithm (Baker, 1979), which like otherEM algorithms is highly sensitive to starting point.
In astrapping method, one would guess a number of differentseeds and evaluate the learned grammars on likelihood,entropy (Wang et al, 2002), correlation with semantics,or plausibility on other linguistic grounds that were notconsidered by the likelihood or the prior.3 StrappingGiven a seed s for some task instance, let Cs denote theclassifier grown from s. Let f(s) denote the true fertilityof a seed s, i.e., the performance of Cs measured againstsome set of correct answers for this instance.
In gen-eral, we do not know the correct answers and hence donot know f(s).
That is why we are doing unsupervisedlearning.Strapping relies on two estimates of f(s).
Let g(s) bea quick estimate that considers only superficial featuresof the seed s. h(s) is a more careful estimate that can becomputed once Cs has been grown.The basic method for strapping a classifier for a newtask instance is very simple:1.
Quickly select a set S of candidate seeds such thatg(s) is high.2.
For each seed s ?
S, learn a classifier Cs and mea-sure h(s).3.
Choose the seed s?
?
S that maximizes h(s?).4.
Return Cs?.Variants on this method are obviously possible.
Forexample, instead of returning a single classifier Cs?, onemight use classifier combination to combine several clas-sifiers Cs that have high h(s).It is clearly important that g and h be good estimatesof f .
Can data help us design g and h?
Unfortunately,f is not known in an unsupervised setting.
However, ifone can get a few supervised instances of the same task,then one can select g and h so g(s) and h(s) approximatef(s) for various seeds s for those instances, where f(s)can be measured directly.
The same g and h can then beused for unsupervised learning on all new task instances.3.1 Selecting Candidate SeedsThe first step in strapping a classifier is to select a set Sof seeds to try.
For strapping to work, it is crucial thatthis set contain a fertile seed.
How can this be arranged?Different strategies are appropriate for different problemsand bootstrapping methods.?
Sometimes a simple heuristic g(s) can help identifyplausibly fertile seeds, as in the pseudocode above.
Instrapping the Yarowsky algorithm, we hope to find seedss = (x, y) such that x and y are strongly associatedwith different senses of the ambiguous target word.
Wechoose s = (x, y) such that x and y were never ob-served in the same sentence, but each of x and y hashigh pointwise mutual information with the ambiguoustarget word and appeared with it at least 5 times.?
If the space of possible seeds is small, it may be pos-sible to try many or all of them.
In grammar induction,for example, perhaps seeding with a few basic word or-der facts is enough.
There are not so many basic wordorders to try.397?
Some methods have many fertile seeds?so many thata small random sample (perhaps filtered by g(s)) islikely to include at least one.
We rely on this forthe Yarowsky algorithm.
If the target word is a truehomonym, there exist many words x associated stronglywith the first sense, and many words y associatedstrongly with the second sense.
It is not difficult to stum-ble into a fertile seed s = (x, y), just as it is not difficultfor a human to think of one.5?
If fertile seeds are few and far between, one couldabandon the use of a candidate set S selected by g(s),and directly use general-purpose search methods to lookfor a seed whose predicted fertility h(s) is high.For example, one could use genetic algorithms tobreed a population of seeds with high h(s).
Orafter evaluating several candidate seeds to obtainh(s1), h(s2), .
.
.
h(sk), one could perform a regressionanalysis that predicts h(s) from superficial features ofs, and use this regression function (a kind of g(s) that isspecific to the task instance) to pick sk+1.Strapping may be harder in cases like gender induc-tion: it is hard to stumble into the kind of detailed seedused by Cucerzan and Yarowsky (2003).
However, wesuspect that fertile seeds exist that are much smaller thantheir lists of 50?60 words.
While their large hand-craftedseed is sure to work, a handful of small seeds (eachconsisting of a few supposedly masculine and femininewords) might be likely to contain at least one that is fer-tile.6 That would be sufficient, assuming we have a wayto guess which seed in the handful is most fertile.
Thatissue is at the core of strapping, and we now turn to it.3.2 Clues for Evaluating Bootstrapped ClassifiersOnce we have identified a candidate seed s and built theclassifier Cs, we must evaluate whether Cs ?looks like?the kind of classifier that tends to do well on our task.This evaluation function h(s) is task-specific.
It mayconsider features of Cs, the growth trajectory of Cs, orthe relation between Cs and other classifiers.For concretness, we consider the Yarowsky method forword-sense disambiguation (WSD).
How can we tell if aseed s = (x, y) was fertile, without using even a smallvalidation set to judge Cs?
There are several types of5Alignment methods in machine translation rely even moreheavily on this property.
While they begin with a small trans-lation lexicon, they are sufficiently robust to the choice of thisinitial seed (lexicon) that it suffices to construct a single seed bycrude automatic means (Brown et al, 1990; Melamed, 1997).Human supervision (or strapping) is unnecessary.6This is particularly likely if one favors function words (inparticular determiners and pronouns), which are strong indica-tors of gender.
Cucerzan and Yarowsky used only content wordsbecause they could be extracted from bilingual dictionaries.clues to fertility, which may be combined into a meta-classifier that identifies fertile seeds.Judge the result of classification with Cs: Even with-out a validation set, the result of running Cs on the train-ing corpus can be validated in various ways, using inde-pendent plausibility criteria that were not considered bythe bootstrapping learner.?
Is the classification reasonably balanced?
(If virtu-ally all examples of the target word are labeled withthe same sense, then Cs has not found a sense dis-tinction.)?
When a document contains multiple tokens of thetarget word, are all examples labeled with the samesense?
This property tends to hold for correct clas-sifiers (Gale et al, 1992a), at least for homonyms.?
True word senses usually correlate with documentor passage topic.
Thus, choose a measure of simi-larity between documents (e.g., the cosine measurein TF/IDF space).
Does the target word tend tohave the same sense in a document and in its nearbyneighbors??
True word senses may also improve performance onsome task.
Is the perplexity of a language modelmuch reduced by knowing whether sense x or sensey (according to Cs) appeared in the current con-text?
(This relates to the previous point.)
Likewise,given a small bilingual text that has been automati-cally (and perhaps poorly) word-aligned, is it easierto predict how the target word will translate whenwe know its sense (according to Cs)?Judge the internal structure of Cs: Does Cs looklike a typical supervised decision list for word-sense dis-ambiguation?
For instance, does it contain many featureswith high log-likelihood ratios?
(If a true sense distinc-tion was discovered, we would expect many contextualfeatures to correlate strongly with the predicted sense.
)Look at the process whereby Cs was learned: Doesthe bootstrapping run that starts from s look like a typicalbootstrapping run from a fertile seed?
For example, didit rapidly add many new examples with high confidence?Once new examples were classified, did their classifica-tions remain stable rather than switching back and forth?Judge the robustness of learning with seed s: Trainseveral versions of Cs, as in ensemble methods (but un-supervised), by restricting each to a random subset of thedata, or a subset of the available features.
Do these ver-sions tend to agree on how to classify the data?
If not,seed s does not reliably find true (or even false) classes.Judge the agreement of Cs with other classifiers:Are there several other classifiers Cs?
that agree stronglywith Cs on examples that they both classify?
If the sense398distinction is real, then many different seeds should beable to find it.3.3 Training the Evaluation Function h(s)Many of the above clues are necessary but not sufficient.For example, a learned classification may be robust with-out being a sense distinction.
We therefore define h(s)from a combination of several clues.In general, h(s) is a classifier or regression functionthat attempts to distinguish fertile from infertile seeds,given the clues.
As mentioned earlier, we train its freeparameters (e.g., coefficients for linear regression) on afew supervised instances of the task.
These supervisedinstances allow us to measure the fertility f(s) of variousseeds, and thus to model the behavior of fertile versusinfertile seeds.
The presumption is that these behaviorpatterns will generalize to new seeds.3.4 Training h(s) on Artificial DataOptionally, to avoid the need for any human annotation atall, the supervised task instances used to train h(s) maybe artificial instances, whose correct classifications areknown without annotation.In the case of word-sense disambiguation, one can au-tomatically construct ambiguous pseudowords (Gale etal., 1992c; Schu?tze, 1998) by replacing all occurences oftwo words or phrases with their conflation.
For example,banana and wine are replaced everywhere by banana-wine.
The original, unconflated text serves as a super-vised answer key for the artificial task of disambiguatingbanana-wine.Traditionally, pseudowords are used as cheap test datato evaluate a disambiguation system.
Our idea is to usethem as cheap development data to tune a system.
Inour case, they tune a few free parameters of h(s), whichsays what a good classifier for this task looks like.
Pseu-dowords should be plausible instances of the task (Gaus-tad, 2001; Nakov and Hearst, 2003): so it is deliberatethat banana and wine share syntactic and semantic fea-tures, as senses of real ambiguous words often do.Cheap ?pseudo-supervised?
data are also available insome other strapping settings.
For grammar induction,one could construct an artificial probabilistic grammar atrandom, and generate text from it.
The task of recoveringthe grammar from the text then has a known answer.4 Experiments4.1 Unsupervised Training/Test DataOur experiments focused on the original Yarowsky algo-rithm.
We attempted to strap word-sense classifiers, us-ing English data only, for English words whose Frenchtranslations are ambiguous.
This has obvious benefits fortraining an English-to-French MT system: separate pa-rameters can be learned for the two senses of drug.7Gale et al (1992b) identified six such words in theCanadian Hansards, a parallel sentence-aligned corpus ofparliamentary debate in English and French: drug, duty,land, language, position, sentence.
We extracted all ex-amples of each word from the 14-million-word Englishportion of the Hansards.8 Note that this is considerablysmaller than Yarowsky?s (1995) corpus of 460 millionwords, so bootstrapping will not perform as well, andmay be more sensitive to the choice of seed.Because we are doing unsupervised learning, we bothtrained and tested these 6 words on the English Hansards.We used the French portion of the Hansards only to createa gold standard for evaluating our results.9 If an Englishsentence containing drug is paired with a French sentencethat contains exactly one of me?dicament or drogue, wetake that as an infallible indicator of its sense.4.2 Comparing ClassifiersSuppose binary classifier 1 assigns class ?+?
to a of nexamples; binary classifier 2 assigns class ?+?
to b of thesame n examples.
Let e be the number of examples wherethe classifiers agree (both ?+?
or both ???
).An unsupervised classifier?s polarity is arbitrary: clas-sifier 1?s ?+?
may correspond to classifier 2?s ???.
So wedefine the overlap as E = max(e, n ?
e), to reflect thebest polarity.To evaluate a learned classifier, we measure its over-lap with the true classification.
The statistical signifi-cance is the probability that this level of overlap wouldbe reached by chance under independent classificationsgiven the values a, b, n:p =?max(a+b?n,0) ?
c ?
?(a+b?E)/2?or?(a+b?(n?E))/2?
?
c ?
min(a,b)?ac?
?n ?
ab ?
c?/?nb?Also, we can measure the agreement between any twolearned classifiers as ?
(log p)/n.
Note that a classifierthat strongly favors one sense will have low agreementwith other classifiers.7To hedge against the possibility of misclassification, onecould interpolate with non-sense-specific parameters.8We are not certain that our version of the Hansards is iden-tical to that in (Gale et al, 1992b).9By contrast, Gale et al (1992b) used the French portion asa source of training supervision.
By contrast, we will assumethat we do not have a large bilingual text such as the Hansards.We train only on the English portion of the Hansards, ignoringthe French.
This mimics the situation where we must constructan MT system with very little bilingual text.
By first discov-ering word senses in unsupervised monolingual data (for eitherlanguage), we can avoid incorrectly mixing up two senses ofdrug in our translation model.3994.3 Generating Candidate Seeds (via g(s))For each target word t, we chose candidate seeds s =(x, y) with a high score g(s), where g(s) = MI(t, x) +MI(t, y), provided that c(x, y) = 0 and c(t, x) ?
5 andc(t, y) ?
5 and 1/9 < c(t, x)/c(t, y) < 9.10The set S of 200 seeds for t was constructed by repeat-edly adding the top-scoring unused seed to S, except thatto increase the variety of words, we disallowed a seeds = (x, y) if x or y already appeared 60 times in S.4.4 Hand-Picked SeedsTo compare, we chose two seeds by hand for each t.The casually hand-picked seed was chosen by intuitionfrom the list of 200 automatically generated seeds.
Thistook about 2 minutes (per seed).The carefully hand-picked seed was not limited to thislist, and took up to 10 minutes to choose, in a data-guidedfashion.
We first looked at some supervised example sen-tences to understand the desired translational sense dis-tinction, and then for each sense chose the highest-MIword that both met some stringent subjective criteria andappeared to retrieve an appropriate initial set of examples.4.5 The Bootstrapping ClassifierOur approximate replication of Yarowsky?s algorithmused only a small set of features:?
Original and lemmatized form of the word immedi-ately preceding the target word t.?
Original and lemmatized form of the word immedi-ately following t.?
Original and lemmatized form of the content wordsthat appear in the same sentence as t.We used the seed to provisionally classify any token ofthe target word that appeared in a sentence with exactlyone of the two seed words.
This formed our initial ?train-ing set?
of disambiguated tokens.
At each iteration of thealgorithm, we trained a decision list on the current train-ing set.
We then used the decision list to reclassify all ktokens in the current training set, and also to augment thetraining set by classifying the additional max(50, k/10)tokens on which the decision list was most confident.1110c(x, y) counts the sentences containing both x and y. MI(t,x) = log c(t, x)c()/c(t)c(x) is pointwise mutual information.11Such a token has some feature with high log-likelihood ra-tio, i.e., it strongly indicates one of the senses in the currenttraining set.
We smoothed using the method of (Yarowsky,1996): when a feature has been observed with only one sense,its log-likelihood ratio is estimated as a linear function of thenumber of occurrences of the seen sense.
Function words aresmoothed with a different linear coefficient than content words,in order to discount their importance.
We borrowed the ac-tual coefficients from (Yarowsky, 1996), though we could havelearned them.4.6 Development Data (for tuning h(s))Before turning to the unsupervised Hansards, we tunedour fertility estimator h(s) to identify good seeds on de-velopment data?i.e., on other, supervised task instances.In the supervised condition, we used just 2 additionaltask instances, plant and tank, each with 4000 hand-annotated instances drawn from a large balanced corpus(Yarowsky, 1995).In the pseudo-supervised condition, we used no hand-annotated data, instead constructing 10 artificial super-vised task instances (section 3.4) from the English por-tion of the Hansards.
To facilitate cross-instance learn-ing, we tried to construct these pseudowords to behavesomething like our ambiguous test words.12 Given a testword t, we randomly selected a seed (x, y) from its candi-date list (section 4.3), excluding any that contained func-tion words.13 Our basic idea was to conflate x and yinto a pseudoword x-y.
However, to get a pseudowordwith only two senses, we tried to focus on the particularsenses of x and y that were selected by t. We constructedabout 500 pseudoword tokens by using only x and y to-kens that appeared in sentences that contained t, or insentences resembling those under a TF-IDF measure.
Werepeated this process twice per test word to obtain 12pseudowords.
We then discarded the 2 pseudowords forwhich no seed beat baseline performance, reasoning thatthey were ill-chosen and unlike real ambiguous words.144.7 Clues to FertilityFor each seed s for each development or test target word,we measured a few clues h1(s), h2(s) .
.
.
h6(s) that wehoped might correlate with fertility.
(In future work, weplan to investigate more clues inspired by section 3.2.)?
The agreeability of Cs with (some of) the other 199classifiers:??1199?s?
6=sagr(Cs, Cs?)??
?1/?The agreement agr(Cs, Cs? )
was defined in section 4.2.We tried 4 values for ?
(namely 1, 2, 5, 10), each result-ing in a different feature.12We used collocates of t. Perhaps better yet would be wordsthat are distributionally similar to t (appear in same contexts).Such words tend to be syntactically and semantically like t.13For an unknown language or domain, a lexicon of functionwords could be constructed automatically (Katz, 1996).14Thus we discarded alcohol-trafficking and addicts-alcohol;note that these were indeed ill-chosen (difficult) since bothwords unluckily corresponded to the same sense of drug.This left us with bound-constituents, customs-pray, claims-value, claims-veterans, culture-unparliamentary, english-learn,competitive-party, financial-party, death-quote, death-page.400?
The robustness of the seed, defined by the agreementof Cs with 10 variant classifiers C(k)s that were trainedwith the same seed but under different conditions:11010?k=1agr(Cs, C(k)s )We simply trained each classifier C(k)s on a random sub-set of the n test examples, chosen by sampling n timeswith replacement.15?
The confidence of Cs on its own training data: its av-erage confidence over the n training tokens, minus theclassifier skew.The decision list?s confidence on a token is the log-likelihood ratio of the single feature used to classify thattoken.
It has the form | log(c/d)| (perhaps smoothed)and was previously used to select data while bootstrap-ping Cs.
Subtracting the skew, | log(a/(n?a))|,16 givesa measurement?
0.
It corrects for confidence that arisesfrom the classifier?s overall bias, leaving only the addedvalue of the relevant contextual feature.4.8 Tuning h(s) and Strapping New ClassifiersFor each of the 2 words or 10 pseudowords t in our de-velopment set (see section 4.6), we ranked its 200 seedss by their true fertility f(s).
We then ran support vec-tor regression17 to learn a single linear function, h(s) =~w ?
(clue vector for Cs), that predicts the fertilities of all2 ?
200 or 10 ?
200 seeds.18Then, for each of our 6 Hansards test instances (sec-tion 4.1), we used h(s) to pick the top-ranked of 200seeds.19 It took about 3 hours total to strap classifiers forall 6 instances, using about 40 machines and unoptimizedPerl code on the 14-million-word Hansards.
For eachof the 6 instances, this involved selecting 200 candidate15We eliminated duplicates, perhaps unfortunately.16As before, a and n ?
a are the numbers of tokens that Csclassifies as ?+?
and ???
respectively.
Thus the skew is the log-likelihood ratio of the decision list?s ?baseline?
feature.17We used cross-validation among the 10 development pseu-dowords to choose the options to SVMlight (Joachims, 1999): alinear kernel, a regularization parameter of 0.3, and a dependentvariable of 10f(s) ?
[1, 10] rather than f(s) ?
[0, 1], whichplaced somewhat more emphasis on modeling the better seeds.Our development objective function was the average over the 10pseudowords of the Spearman rank-order correlation betweenh(s) and f(s).18We augmented the clue vector with binary clues of the formt = plant, t = tank, etc.
The regression weight of such a clueis a learned bias term that models the inherent difficulty of thetask instance t (which varies greatly by t).
This allows the otherregression features to focus on the quality of the seed given t.19We do not have a clue t = .
.
.
for this test instance.
The re-sulting lack of a bias term may subtract a constant from the pre-dicted fertilities?but that does not affect the ranking of seeds.seeds, bootstrapping 11 classifiers Cs, C(1)s , .
.
.
C(10)sfrom each seed, and choosing a particular Cs to return.4.9 ResultsOur results are in Table 1.
On both development and testinstances of the task, g(s) proposed seeds with a goodrange of fertilities.
The correlation of predicted with ac-tual fertility on test data averaged an outstanding 85%.Despite having no knowledge of the desired senses,strapping significantly beat human selection in all 24 ofthe possible comparisons between a hand-picked seed(casual or careful) and a strapped seed (chosen by an h(s)tuned on supervised or pseudo-supervised instances).The h(s) tuned on annotated plant/tank actually chosethe very best of the 200 seeds in 4 of the 6 instances.
Theh(s) tuned on artificial pseudowords did nearly as well,in 2 of 6 instances identifying the very best seed, and in5 of 6 instances ranking it among its top 3 choices.We conclude that our unsupervised clues to fertility ac-tually work.
Furthermore, combining clues via regres-sion was wise, as it tended to work better than any singleclue.
Somewhat better regression weights for the WSDtask were learned from 2 out-of-domain hand-annotatedwords than from 10 in-domain artificial pseudowords.5 Open QuestionsThe work reported here raises many interesting questionsfor future research.In the WSD task, we have only considered word typeswith two unrelated senses (homonyms).
A more generalproblem is to determine when a word type is ambiguousat all, and if so, how many coarse-grained or fine-grainedsenses it has.
Strapping seems naturally suited to thisproblem, since it aims to discover when a sense distinc-tion grown from some seed is a true sense distinction.Then we would like to know how well strapping gen-eralizes to additional bootstrapping scenarios.
Our WSDstrapping experiments were successful using only a sub-set of the techniques proposed in section 3.
Generalizingto other tasks may require other techniques for selectingand evaluating candidate seeds, and perhaps combiningthe resulting classifiers.An interesting question is whether strapping can beused in an active learning context.
Active learning is akind of bootstrapping method that periodically requiresnew seeds: it turns to the user whenever it gets confused.Perhaps some of these seeds can be guessed nondetermin-istically and the guesses evaluated automatically, with orwithout user confirmation.Finally, there may be theoretical guarantees aboutstrapping when something is known about the data.When h(s) is trained to estimate f(s) well on some su-pervised instances, there may be guarantees about howstrapping will perform on unsupervised instances drawn401strapping(unsupervised)8>>>>>>><>>>>>>>:drug duty land language position sentencebaseline / # examples 51.2 / 371 70.1 / 633 76.6 / 1379 87.5 / 1012 81.7 / 2949 50.8 / 501worst seed (of 200) 50.1 (200) traffickers trafficking 50.0 (200) 50.1 (200) claims farming 50.3 (200) 56.1 (200) 50.1 (200) length lifecasually selected (from 200) 56.5 (87) food trafficking 73.4?
(40) 76.2 (24) farm veterans 86.4 (76) 81.7 (41) 80.6?
(40) page prisoncarefully constructed 62.1?
(75) alcohol costs 82.1?
(8.5) 76.6 (20) farm strong 87.9 (25.5) 81.4 (56.5) 86.8?
(27) death quotebest/oracle seed (of 200) 76.1??
(1) alcohol medical 86.2??
(1) 81.3??
(1) acres courts 90.9??
(1) 88.3??
(1) 89.9??
(1) reads servedmost agreeable seed (?=1) 72.6??
(5) abuse information 64.7 (47) 67.5 (36) claims production 86.4 (79) 82.4 (36) 88.7??
(10) life quotemost robust seed 76.1??
(1) alcohol medical 86.2??
(1) 71.7 (29) claims price 85.6 (93) 82.7 (21) 88.8??
(9) commuted nextmost confident seed 66.9?
(32) trafficking used 72.1?
(42) 77.9??
(3) claims courts 89.8??
(10) 84.4??
(8) 89.9??
(1) reads servedh(s)-picked (plant/tank) 76.1??
(1) alcohol medical 86.2??
(1) 81.3??
(1) acres courts 90.3??
(7) 84.5??
(7) 89.9??
(1) reads servedh(s)-picked (10 pseudowd) 70.4??
(10) alcohol found 86.2??
(1) 78.9??
(2) children farm 89.7??
(17) 83.7??
(16) 89.9??
(1) reads servedh(s)-picked, 2nd place 69.1?
(13) alcohol related 85.7??
(2) 77.8??
(4) aboriginal acres 90.9??
(1) 82.8 (19) 89.0??
(7) prison quoteh(s)-picked, 3rd place 76.1??
(1) alcohol medical 84.2?
(4) 77.1??
(5) acres cities 87.5 (28) 88.3??
(1) 88.6??
(15) life readsh(s) rank of oracle seed 3 1 14 2 3 1Spearman rank-order corr.
0.863 0.905 0.718 0.825 0.842 0.937Table 1: [See section 4.9 for highlights.]
Accuracy (as percentage) and rank (in parentheses) of bootstrapped classifiers for variouslychosen seeds, some of which are shown.
* denotes statistically significant agreement with the truth (section 4.2, p < 0.01).?
denotes a seed having significantly better agreement with the truth than does the better of the hand-picked seeds (McNemar?s test,p < 0.03).
In each column, the best performance for an automatic or manual seed appears in boldface.
The ?most .
.
.
?
lines use notuning, the ?plant/tank?
line tunes h(s) on 2 supervised instances, and the subsequent lines tune h(s) on 10 pseudoword instances.The last line gives the Spearman rank-order correlation between seeds?
predicted fertilities h(s) and their actual fertilities f(s).from the same source (cross-instance learning).
Even inthe fully unsupervised case, it may be possible to provethat if the data were generated from a particular kind ofprocess (e.g., a Gaussian mixture), then a certain strap-ping algorithm can recover the hidden variables.6 ConclusionsIn this paper, we showed that it is sometimes possible?indeed, preferable?to eliminate the initial bit of supervi-sion in ?bootstrapping?
algorithms such as the Yarowsky(1995) algorithm for word sense disambiguation.
Our?strapping?
approach tries many candidate seeds as start-ing points and evaluates them automatically.
The eval-uation function can be tuned if desired on other task in-stances, perhaps artificially constructed ones.
It can thenbe used wherever human guidance is impractical.We applied the method to unsupervised disambigua-tion of English words in the Canadian Hansards, as if forEnglish-French translation.
Our results (see section 4.9for several highlights) show that our automatic ?strapped?classifiers consistently outperform the classifiers boot-strapped from manually, knowledgeably chosen seeds.ReferencesR.
K. Ando and T. Zhang.
2005.
A high-performance semi-supervised learning method for text chunking.
In ACL.J.
K. Baker.
1979.
Trainable grammars for speech recogni-tion.
In Jared J. Wolf and Dennis H. Klatt, editors, SpeechCommunication Papers Presented at the 97th meeting of theAcoustical Society of America, MIT, Cambridge, MA, June.A.
Blum and Tom Mitchell.
1998.
Combining labeled and un-labeled data with co-training.
In Proc.
of COLT, July.P.
F. Brown, J. Cook, S.A. Della Pietra, V.G.
Della Pietra, F. Je-linek, J.D.
Lafferty, R.L.
Mercer, and P.S.
Roossin.
1990.
Astatistical approach to machine translation.
CL, 16(2).N.
Chomsky.
1981.
Lectures on Government and Binding.Foris, Dordrecht.M.
Collins and Y.
Singer.
1999.
Unsupervised models fornamed entity classification.
In Proc.
of EMNLP/VLC.S.
Cucerzan and D. Yarowsky.
1999.
Language independentnamed entity recognition combining morphological and con-textual evidence.
In Proc.
of EMNLP/VLC.S.
Cucerzan and D. Yarowsky.
2003.
Minimally supervisedinduction of grammatical gender.
In Proc.
of HLT/NAACL.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.
Maximumlikelihood from incomplete data via the EM algorithm.
J.Royal Statist.
Soc.
Ser.
B, 39(1):1?38.W.
A. Gale, K. W. Church, and D. Yarowsky.
1992a.
One senseper discourse.
In Proc.
of the 4th DARPA Speech and NaturalLanguage Workshop, pages 233?237.W.
A. Gale, K. W. Church, and D. Yarowsky.
1992b.
Us-ing bilingual materials to develop word sense disambiguationmethods.
In Proc.
of the 4th International Conf.
on Theoret-ical and Methodological Issues in Machine Translation.W.
A. Gale, K. W. Church, and D. Yarowsky.
1992c.
Work onstatistical methods for word sense disambiguation.
In Work-ing Notes of the AAAI Fall Symposium on Probabilistic Ap-proaches to Natural Language, pages 54?60.T.
Gaustad.
2001.
Statistical corpus-based word sense disam-biguation: Pseudowords vs. real ambiguous words.
In Proc.of ACL-EACL.T.
Joachims.
1999.
Making large-scale SVM learning practical.In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advancesin Kernel Methods?Support Vector Learning.
MIT Press.S.
M. Katz.
1996.
Distribution of context words and phrases intext and language modelling.
NLE, 2(1):15?59.I.
Dan Melamed.
1997.
A word-to-word model of translationalequivalence.
In Proc.
of ACL/EACL, page 490.P.
Nakov and M. Hearst.
2003.
Category-based pseudowords.In HLT-NAACL?03, pages 67?69, Edmonton, Canada.E.
Riloff, J. Wiebe, and T. Wilson.
2003.
Learning subjec-tive nouns using extraction pattern bootstrapping.
In Proc.of CoNLL, pages 25?32, May?June.H.
Schu?tze.
1998.
Automatic word sense discrimination.
Com-putational Linguistics, 23.S.
Wang, R. Rosenfeld, Y. Zhao, and D. Schuurmans.
2002.The latent maximum entropy principle.
In Proc.
of ISIT.D.
Yarowsky.
1995.
Unsupervised word sense disambiguationrivaling supervised methods.
In Proc.
of ACL.D.
Yarowsky.
1996.
Three Machine Learning Algorithms forLexical Ambiguity Resolution.
Ph.D. thesis, U. of Penn.402
