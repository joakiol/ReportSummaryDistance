Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 536?544,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsFinding Bursty Topics from MicroblogsQiming Diao, Jing Jiang, Feida Zhu, Ee-Peng LimLiving Analytics Research CentreSchool of Information SystemsSingapore Management University{qiming.diao.2010, jingjiang, fdzhu, eplim}@smu.edu.sgAbstractMicroblogs such as Twitter reflect the generalpublic?s reactions to major events.
Bursty top-ics from microblogs reveal what events haveattracted the most online attention.
Althoughbursty event detection from text streams hasbeen studied before, previous work may notbe suitable for microblogs because comparedwith other text streams such as news articlesand scientific publications, microblog postsare particularly diverse and noisy.
To find top-ics that have bursty patterns on microblogs,we propose a topic model that simultaneous-ly captures two observations: (1) posts pub-lished around the same time are more like-ly to have the same topic, and (2) posts pub-lished by the same user are more likely to havethe same topic.
The former helps find event-driven posts while the latter helps identify andfilter out ?personal?
posts.
Our experimentson a large Twitter dataset show that there aremore meaningful and unique bursty topics inthe top-ranked results returned by our mod-el than an LDA baseline and two degeneratevariations of our model.
We also show somecase studies that demonstrate the importanceof considering both the temporal informationand users?
personal interests for bursty topicdetection from microblogs.1 IntroductionWith the fast growth of Web 2.0, a vast amount ofuser-generated content has accumulated on the so-cial Web.
In particular, microblogging sites suchas Twitter allow users to easily publish short in-stant posts about any topic to be shared with thegeneral public.
The textual content coupled withthe temporal patterns of these microblog posts pro-vides important insight into the general public?s in-terest.
A sudden increase of topically similar postsusually indicates a burst of interest in some eventthat has happened offline (such as a product launchor a natural disaster) or online (such as the spreadof a viral video).
Finding bursty topics from mi-croblogs therefore can help us identify the most pop-ular events that have drawn the public?s attention.
Inthis paper, we study the problem of finding burstytopics from a stream of microblog posts generatedby different users.
We focus on retrospective detec-tion, where the text stream within a certain period isanalyzed in its entirety.Retrospective bursty event detection from tex-t streams is not new (Kleinberg, 2002; Fung et al,2005; Wang et al, 2007), but finding bursty topic-s from microblog steams has not been well studied.In his seminal work, Kleinberg (2002) proposed a s-tate machine to model the arrival times of documentsin a stream in order to identify bursts.
This modelhas been widely used.
However, this model assumesthat documents in the stream are all about a giventopic.
In contrast, discovering interesting topics thathave drawn bursts of interest from a stream of top-ically diverse microblog posts is itself a challenge.To discover topics, we can certainly apply standardtopic models such as LDA (Blei et al, 2003), butwith standard LDA temporal information is lost dur-ing topic discovery.
For microblogs, where posts areshort and often event-driven, temporal informationcan sometimes be critical in determining the topic ofa post.
For example, typically a post containing the536word ?jobs?
is likely to be about employment, butright after October 5, 2011, a post containing ?jobs?is more likely to be related to Steve Jobs?
death.
Es-sentially, we expect that on microblogs, posts pub-lished around the same time have a higher probabil-ity to belong to the same topic.To capture this intuition, one solution is to assumethat posts published within the same short time win-dow follow the same topic distribution.
Wang etal.
(2007) proposed a PLSA-based topic model thatexploits this idea to find correlated bursty patternsacross multiple text streams.
However, their modelis not immediately applicable for our problem.
First,their model assumes multiple text streams whereword distributions for the same topic are differenton different streams.
More importantly, their modelwas applied to news articles and scientific publica-tions, where most documents follow the global top-ical trends.
On microblogs, besides talking aboutglobal popular events, users also often talk abouttheir daily lives and personal interests.
In order todetect global bursty events from microblog posts, itis important to filter out these ?personal?
posts.In this paper, we propose a topic model designedfor finding bursty topics from microblogs.
Our mod-el is based on the following two assumptions: (1) Ifa post is about a global event, it is likely to followa global topic distribution that is time-dependent.
(2) If a post is about a personal topic, it is likelyto follow a personal topic distribution that is moreor less stable over time.
Separation of ?global?
and?personal?
posts is done in an unsupervised mannerthrough hidden variables.
Finally, we apply a statemachine to detect bursts from the discovered topics.We evaluate our model on a large Twitter dataset.We find that compared with bursty topics discoveredby standard LDA and by two degenerate variationsof our model, bursty topics discovered by our modelare more accurate and less redundant within the top-ranked results.
We also use some example burstytopics to explain the advantages of our model.2 Related WorkTo find bursty patterns from data streams, Kleinberg(2002) proposed a state machine to model the ar-rival times of documents in a stream.
Different statesgenerate time gaps according to exponential densityfunctions with different expected values, and burstyintervals can be discovered from the underlying statesequence.
A similar approach by Ihler et al (2006)models a sequence of count data using Poisson dis-tributions.
To apply these methods to find burstytopics, the data stream used must represent a singletopic.Fung et al (2005) proposed a method that iden-tifies both topics and bursts from document stream-s.
The method first finds individual words that havebursty patterns.
It then finds groups of words thattend to share bursty periods and co-occur in the samedocuments to form topics.
Weng and Lee (2011)proposed a similar method that first characterizes thetemporal patterns of individual words using wavelet-s and then groups words into topics.
A major prob-lem with these methods is that the word clusteringstep can be expensive when the number of burstywords is large.
We find that the method by Funget al (2005) cannot be applied to our dataset be-cause their word clustering algorithm does not scaleup.
Weng and Lee (2011) applied word clusteringto only the top bursty words within a single day, andsubsequently their topics mostly consist of two orthree words.
In contrast, our method is scalable andeach detected bursty topic is directly associated witha word distribution and a set of tweets (see Table 3),which makes it easier to interpret the topic.Topic models provide a principled and elegan-t way to discover hidden topics from large docu-ment collections.
Standard topic models do not con-sider temporal information.
A number of temporaltopic models have been proposed to consider topicchanges over time.
Some of these models focus onthe change of topic composition, i.e.
word distri-butions, which is not relevant to bursty topic detec-tion (Blei and Lafferty, 2006; Nallapati et al, 2007;Wang et al, 2008).
Some other work looks at thetemporal evolution of topics, but the focus is not onbursty patterns (Wang and McCallum, 2006; Ahmedand Xing, 2008; Masada et al, 2009; Ahmed and X-ing, 2010; Hong et al, 2011).The model proposed by Wang et al (2007) is themost relevant to ours.
But as we have pointed outin Section 1, they do not need to handle the sep-aration of ?personal?
documents from event-drivendocuments.
As we will show later in our experi-ments, for microblogs it is critical to model users?537personal interests in addition to global topical trend-s.To capture users?
interests, Rosen-Zvi et al(2004) expand topic distributions from document-level to user-level in order to capture users?
specif-ic interests.
But on microblogs, posts are short andnoisy, so Zhao et al (2011) further assume that eachpost is assigned a single topic and some words canbe background words.
However, these studies do notaim to detect bursty patterns.
Our work is novel inthat it combines users?
interests and temporal infor-mation to detect bursty topics.3 Method3.1 PreliminariesWe first introduce the notation used in this paper andformally formulate our problem.
We assume thatwe have a stream of D microblog posts, denoted asd1, d2, .
.
.
, dD.
Each post di is generated by a userui, where ui is an index between 1 and U , and U isthe total number of users.
Each di is also associat-ed with a discrete timestamp ti, where ti is an indexbetween 1 and T , and T is the total number of timepoints we consider.
Each di contains a bag of word-s, denoted as {wi,1, wi,2, .
.
.
, wi,Ni}, where wi,j isan index between 1 and V , and V is the vocabularysize.
Ni is the number of words in di.We define a bursty topic b as a word distri-bution coupled with a bursty interval, denoted as(?b, tbs, tbe), where ?b is a multinomial distributionover the vocabulary, and tbs and tbe (1 ?
tbs ?
tbe ?
T )are the start and the end timestamps of the bursty in-terval, respectively.
Our task is to find meaningfulbursty topics from the input text stream.Our method consists of a topic discovery step anda burst detection step.
At the topic discovery step,we propose a topic model that considers both users?topical interests and the global topic trends.
Burstdetection is done through a standard state machinemethod.3.2 Our Topic ModelWe assume that there are C (latent) topics in the textstream, where each topic c has a word distribution?c.
Note that not every topic has a bursty interval.On the other hand, a topic may have multiple burstyintervals and hence leads to multiple bursty topics.We also assume a background word distribution ?Bthat captures common words.
All posts are assumedto be generated from some mixture of these C + 1underlying topics.In standard LDA, a document contains a mixtureof topics, represented by a topic distribution, andeach word has a hidden topic label.
While this is areasonable assumption for long documents, for shortmicroblog posts, a single post is most likely to beabout a single topic.
We therefore associate a singlehidden variable with each post to indicate its topic.Similar idea of assigning a single topic to a short se-quence of words has been used before (Gruber et al,2007; Zhao et al, 2011).
As we will see very soon,this treatment also allows us to model topic distribu-tions at time window level and user level.As we have discussed in Section 1, an importan-t observation we have is that when everything elseis equal, a pair of posts published around the sametime is more likely to be about the same topic than arandom pair of posts.
To model this observation, weassume that there is a global topic distribution ?t foreach time point t. Presumably ?t has a high prob-ability for a topic that is popular in the microblog-sphere at time t.Unlike news articles from traditional media,which are mostly about current affairs, an importantproperty of microblog posts is that many posts areabout users?
personal encounters and interests ratherthan global events.
Since our focus is to find popularglobal events, we need to separate out these ?person-al?
posts.
To do this, an intuitive idea is to comparea post with its publisher?s general topical interestsobserved over a long time.
If a post does not matchthe user?s long term interests, it is more likely re-lated to a global event.
We therefore introduce atime-independent topic distribution ?u for each us-er to capture her long term topical interests.We assume the following generation process forall the posts in the stream.
When user u publishesa post at time point t, she first decides whether towrite about a global trendy topic or a personal top-ic.
If she chooses the former, she then selects a topicaccording to ?t.
Otherwise, she selects a topic ac-cording to her own topic distribution ?u.
With thechosen topic, words in the post are generated fromthe word distribution for that topic or from the back-ground word distribution that captures white noise.5381.
Draw ?B ?
Dirichlet(?
), ?
?
Beta(?
), ?
?Beta(?)2.
For each time point t = 1, .
.
.
, T(a) draw ?t ?
Dirichlet(?)3.
For each user u = 1, .
.
.
, U(a) draw ?u ?
Dirichlet(?)4.
For each topic c = 1, .
.
.
, C,(a) draw ?c ?
Dirichlet(?)5.
For each post i = 1, .
.
.
, D,(a) draw yi ?
Bernoulli(?
)(b) draw zi ?
Multinomial(?ui) if yi = 0 orzi ?
Multinomial(?ti) if yi = 1(c) for each word j = 1, .
.
.
, Nii.
draw xi,j ?
Bernoulli(?)ii.
draw wi,j ?
Multinomial(?B) ifxi,j = 0 or wi,j ?
Multinomial(?zi)if xi,j = 1Figure 2: The generation process for all posts.We use ?
to denote the probability of choosing totalk about a global topic rather than a personal topic.Formally, the generation process is summarized inFigure 2.
The model is also depicted in Figure 1(a).There are two degenerate variations of our modelthat we also consider in our experiments.
The firstone is depicted in Figure 1(b).
In this model, we onlyconsider the time-dependent topic distributions thatcapture the global topical trends.
This model can beseen as a direct application of the model by Wanget al (2007).
The second one is depicted in Fig-ure 1(c).
In this model, we only consider the users?personal interests but not the global topical trends,and therefore temporal information is not used.
Werefer to our complete model as TimeUserLDA, themodel in Figure 1(b) as TimeLDA and the model inFigure 1(c) asUserLDA.
We also consider a standardLDA model in our experiments, where each word isassociated with a hidden topic.LearningWe use collapsed Gibbs sampling to obtain sam-ples of the hidden variable assignment and to esti-mate the model parameters from these samples.
Dueto space limit, we only show the derived Gibbs sam-pling formulas as follows.First, for the i-th post, we know its publisher uiand timestamp ti.
We can jointly sample yi and zibased on the values of all other hidden variables.
Letus use y to denote the set of all hidden variables yand y?i to denote all y except yi.
We use similarsymbols for other variables.
We then havep(yi = p, zi = c|z?i,y?i,x,w) ?Mpi(p) + ?Mpi(?)
+ 2?
?M l(c) + ?M l(?)
+ C??
?Vv=1?E(v)?1k=0 (M c(v) + k + ?)?E(?
)?1k=0 (M c(?)
+ k + V ?
), (1)where l = ui when p = 0 and l = ti when p =1.
Here every M is a counter.
Mpi(0) is the numberof posts generated by personal interests, while Mpi(1)is the number of posts coming from global topicaltrends.
Mpi(?)
= Mpi0 + Mpi1 .
Mui(c) is the number ofposts by user ui and assigned to topic c, and Mui(?)
isthe total number of posts by ui.
M ti(c) is the numberof posts assigned to topic c at time point ti, and M ti(?
)is the total number of posts at ti.
E(v) is the numberof times word v occurs in the i-th post and is labeledas a topic word, while E(?)
is the total number oftopic words in the i-th post.
Here, topic words referto words whose latent variable x equals 1.
M c(v) isthe number of times word v is assigned to topic c,and M c(?)
is the total number of words assigned totopic c. All the counters M mentioned above arecalculated with the i-th post excluded.We sample xi,j for each word wi,j in the i-th postusingp(xi,j = q|y, z,x?{i,j},w)?M?
(q) + ?M?(?)
+ 2?
?M l(wi,j) + ?M l(?)
+ V ?, (2)where l = B when q = 0 and l = zi when q = 1.M?
(0) and M?
(1) are counters to record the numbersof words assigned to the background model and anytopic, respectively, andM?(?)
= M?(0)+M?(1).
MB(wi,j)is the number of times word wi,j occurs as a back-ground word.
M zi(wi,j) counts the number of timesword wi,j is assigned to topic zi, and M zi(?)
is the to-tal number of words assigned to topic zi.
Again, allcounters are calculated with the current word wi,jexcluded.539Figure 1: (a) Our topic model for burst detection.
(b) A variation of our model where we only consider global topicaltrends.
(c) A variation of our model where we only consider users?
personal topical interests.3.3 Burst DetectionJust like standard LDA, our topic model itself finds aset of topics represented by ?c but does not directlygenerate bursty topics.
To identify bursty topics, weuse the following mechanism, which is based on theidea by Kleinberg (2002) and Ihler et al (2006).
Inour experiments, when we compare different mod-els, we also use the same burst detection mechanismfor other models.We assume that after topic modeling, for each dis-covered topic c, we can obtain a series of counts(mc1,mc2, .
.
.
,mcT ) representing the intensity of thetopic at different time points.
For LDA, theseare the numbers of words assigned to topic c.For TimeUserLDA, these are the numbers of postswhich are in topic c and generated by the global top-ic distribution ?ti , i.e whose hidden variable yi is 1.For other models, these are the numbers of posts intopic c.We assume that these counts are generated by twoPoisson distributions corresponding to a bursty stateand a normal state, respectively.
Let ?0 denote theexpected count for the normal state and ?1 for thebursty state.
Let vt denote the state for time point t,where vt = 0 indicates the normal state and vt = 1indicates the bursty state.
The probability of observ-ing a count of mct is as follows:p(mct |vt = l) =e?
?l?mctlmct !,where l is either 0 or 1.
The state sequence(v0, v1, .
.
.
, vT ) is a Markov chain with the follow-ing transition probabilities:p(vt = l|vt?1 = l) = ?l,Method P@5 P@10 P@20 P@30LDA 0.600 0.800 0.750 N/ATimeLDA 0.800 0.700 0.600 0.633UserLDA 0.800 0.700 0.850 0.833TimeUserLDA 1.000 1.000 0.900 0.800Table 1: Precision at K for the various models.Method P@5 P@10 P@20 P@30LDA 0.600 0.800 0.700 N/ATimeLDA 0.400 0.500 0.500 0.567UserLDA 0.800 0.500 0.500 0.600TimeUserLDA 1.000 0.900 0.850 0.767Table 2: Precision at K for the various models after weremove redundant bursty topics.where l is either 0 or 1.?0 and ?1 are topic specific.
In our experiments,we set ?0 = 1T?t mct , that is, ?0 is the averagecount over time.
We set ?1 = 3?0.
For transitionprobabilities, we empirically set ?0 = 0.9 and ?1 =0.6 for all topics.We can use dynamic programming to uncover theunderlying state sequence for a series of counts.
Fi-nally, a burst is marked by a consecutive subse-quence of bursty states.4 Experiments4.1 Data SetWe use a Twitter data set to evaluate our models.The original data set contains 151,055 Twitter usersbased in Singapore and their tweets.
These Twitterusers were obtained by starting from a set of seedSingapore users who are active online and tracing540Bursty Period Top Words Example Tweets LabelNov 29 vote, big, awards, (1) why didnt 2ne1 win this time!
Mnet Asianbang, mama, win, (2) 2ne1.
you deserved that urgh!
Music Awards2ne1, award, won (3) watching mama.
whoohoo (MAMA)Oct 5 ?
Oct 8 steve, jobs, apple, (1) breaking: apple says steve jobs has passed away!
Steve Jobsiphone, rip, world, (2) google founders: steve jobs was an inspiration!
deathchanged, 4s, siri (3) apple 4 life thankyousteveNov 1 ?
Nov 3 reservior, bedok, adlyn, (1) this adelyn totally disgust me.
slap her mum?
girl slappingslap, found, body, queen of cine?
joke please can.
mommom, singapore, steven (2) she slapped her mum and boasted about it on fb(3) adelyn lives in woodlands , later she slap me how?Nov 5 reservior, bedok, adlyn, (1) bedok = bodies either drowned or killed.
suicide nearslap, found, body, (2) another body found, in bedok reservoir?
bedok reservoirmom, singapore, steven (3) so many bodies found at bedok reservoir.
alamak.Oct 23 man, arsenal, united, (1) damn you man city!
we will get you next time!
football gameliverpool, chelsea, city, (2) wtf 90min goal!goal, game, match (3) 6-1 to city.
unbelievable.Table 3: Top-5 bursty topics ranked by TimeUserLDA.
The labels are manually given.
The 3rd and the 4th burstytopics come from the same topic but have different bursty periods.Rank LDA UserLDA TimeLDA1 Steve Jobs?
death MAMA MAMA2 MAMA football game MAMA3 N/A #zamanprimaryschool MAMA4 girl slapping mom N/A girl slapping mom5 N/A iphone 4s N/ATable 4: Top-5 bursty topics ranked by other models.
N/A indicates a meaningless burst.their follower/followee links by two hops.
Becausethis data set is huge, we randomly sampled 2892users from this data set and extracted their tweetsbetween September 1 and November 30, 2011 (91days in total).
We use one day as our time window.Therefore our timestamps range from 1 to 91.
Wethen removed stop words and words containing non-standard characters.
Tweets containing less than 3words were also discarded.
After preprocessing, weobtained the final data set with 3,967,927 tweets and24,280,638 tokens.4.2 Ground Truth GenerationTo compare our model with other alternative models,we perform both quantitative and qualitative evalua-tion.
As we have explained in Section 3, each mod-el gives us time series data for a number of topics,and by applying a Poisson-based state machine, wecan obtain a set of bursty topics.
For each method,we rank the obtained bursty topics by the numberof tweets (or words in the case of the LDA model)assigned to the topics and take the top-30 bursty top-ics from each model.
In the case of the LDA mod-el, only 23 bursty topics were detected.
We mergedthese topics and asked two human judges to judgetheir quality by assigning a score of either 0 or 1.The judges are graduate students living in Singaporeand not involved in this project.
The judges weregiven the bursty period and 100 randomly selectedtweets for the given topic within that period for eachbursty topic.
They can consult external resources tohelp make judgment.
A bursty topic was scored 1if the 100 tweets coherently describe a bursty even-t based on the human judge?s understanding.
Theinter-annotator agreement score is 0.649 using Co-hen?s kappa, showing substantial agreement.
Forground truth, we consider a bursty topic to be cor-rect if both human judges have scored it 1.
Sincesome models gave redundant bursty topics, we al-so asked one of the judges to identify unique bursty541topics from the ground truth bursty topics.4.3 EvaluationIn this section, we show the quantitative evalua-tion of the four models we consider, namely, LDA,TimeLDA, UserLDA and TimeUserLDA.
For eachmodel, we set the number of topics C to 80, ?
to 50Cand ?
to 0.01 after some preliminary experiments.Each model was run for 500 iterations of Gibbs sam-pling.
We take 40 samples with a gap of 5 iterationsin the last 200 iterations to help us assign values toall the hidden variables.Table 1 shows the comparison between thesemodels in terms of the precision of the top-K result-s. As we can see, our model outperforms all othermodels for K <= 20.
For K = 30, the UserLDAmodel performs the best followed by our model.As we have pointed out, some of the bursty topicsare redundant, i.e.
they are about the same burstyevent.
We therefore also calculated precision at Kfor unique topics, where for redundant topics the oneranked the highest is scored 1 and the other onesare scored 0.
The comparison of the performanceis shown in Table 2.
As we can see, in this case,our model outperforms other models with all K. Wewill further discuss redundant bursty topics in thenext section.4.4 Sample Results and DiscussionsIn this section, we show some sample results fromour experiments and discuss some case studies thatillustrate the advantages of our model.First, we show the top-5 bursty topics discoveredby the TimeUserLDA model in Table 3.
As we cansee, all these bursty topics are meaningful.
Some ofthese events are global major events such as SteveJobs?
death, while some others are related to onlineevents such as the scandal of a girl boasting aboutslapping her mother on Facebook.
For comparison,we also show the top-5 bursty topics discovered byother models in Table 4.
As we can see, some ofthem are not meaningful events while some of themare redundant.Next, we show two case studies to demonstratethe effectiveness of our model.Effectiveness of Temporal Models: BothTimeLDA and TimeUserLDA tend to group postspublished on the same day into the same topic.
Wefind that this can help separate bursty topics fromgeneral ones.
An example is the topic on the CircleLine.
The Circle Line is one of the subway lines ofSingapore?s mass transit system.
There were a fewincidents of delays or breakdowns during the periodbetween September and November, 2011.
We showthe time series data of the topic related to the CircleLine of UserLDA, TimeLDA and TimeUserLDA inFigure 3.
As we can see, the UserLDA model de-tects a much large volume of tweets related to thistopic.
A close inspection tells us that the topic underUserLDA is actually related to the subway systemsin Singapore in general, which include a few othersubway lines, and the Circle Line topic is mergedwith this general topic.
On the other hand, TimeL-DA and TimeUserLDA are both able to separate theCircle Line topic from the general subway topic be-cause the Circle Line has several bursts.
What isshown in Figure 3 for TimeLDA and TimeUserLDAis only the topic on the Circle Line, therefore thevolume is much smaller.
We can see that TimeLDAand TimeUserLDA show clearer bursty patterns thanUserLDA for this topic.
The bursts around day 20,day 44 and day 85 are all real events based on ourground truth.Effectiveness of User Models: We have stat-ed that it is important to filter out users?
?person-al?
posts in order to find meaningful global events.We find that our results also support this hypothesis.Let us look at the example of the topic on the MnetAsian Music Awards, which is a major music awardshow that is held by Mnet Media annually.
In 2011,this event took place in Singapore on November 29.Because Korean pop music is very popular in Singa-pore, many Twitter users often tweet about Koreanpop music bands and singers in general.
All our top-ic models give multiple topics related to Korean popmusic, and many of them have a burst on Novem-ber 29, 2011.
Under the TimeLDA and UserLDAmodels, this leads to several redundant bursty top-ics for the MAMA event ranked within the top-30.For TimeUserLDA, however, although the MAMAevent is also ranked the top, there is no redundan-t one within the top-30 results.
We find that this isbecause with TimeUserLDA, we can remove tweet-s that are considered personal and therefore do notcontribute to bursty topic ranking.
We show the top-ic intensity of a topic about a Korean pop singer in54202004006008001000120010  20  30  40  50  60  70  80  90mtUserLDA02004006008001000120010  20  30  40  50  60  70  80  90mtTimeLDA02004006008001000120010  20  30  40  50  60  70  80  90mtTimeUserLDAFigure 3: Topic intensity over time for the topic on the Circle Line.0100020003000400050006000700010  20  30  40  50  60  70  80  90mtUserLDA0100020003000400050006000700010  20  30  40  50  60  70  80  90mtTimeLDA0100020003000400050006000700010  20  30  40  50  60  70  80  90mtTimeUserLDAFigure 4: Topic intensity over time for the topic about a Korean pop singer.
The dotted curves show the topic on SteveJobs?
death.Figure 4.
For reference, we also show the intensityof the topic on Steve Jobs?
death under each mod-el.
We can see that because this topic is related toKorean pop music, it has a burst on day 90 (Novem-ber 29).
But if we consider the relative intensity ofthis burst compared with Steve Jobs?
death, underTimeLDA and UserLDA, this topic is still strong butunder TimeUserLDA its intensity can almost be ig-nored.
This is why with TimeLDA and UserLDAthis topic leads to a redundant burst within the top-30 results but with TimeUserLDA the burst is notranked high.5 ConclusionsIn this paper, we studied the problem of findingbursty topics from the text streams on microblogs.Because existing work on burst detection from tex-t streams may not be suitable for microblogs, weproposed a new topic model that considers both thetemporal information of microblog posts and user-s?
personal interests.
We then applied a Poisson-based state machine to identify bursty periods fromthe topics discovered by our model.
We comparedour model with standard LDA as well as two de-generate variations of our model on a real Twitterdataset.
Our quantitative evaluation showed that ourmodel could more accurately detect unique burstytopics among the top ranked results.
We also usedtwo case studies to illustrate the effectiveness of thetemporal factor and the user factor of our model.Our method currently can only detect bursty top-ics in a retrospective and offline manner.
A more in-teresting and useful task is to detect realtime burstsin an online fashion.
This is one of the directions weplan to study in the future.
Another limitation of thecurrent method is that the number of topics is pre-determined.
We also plan to look into methods thatallow appearance and disappearance of topics alongthe timeline, such as the model by Ahmed and Xing(2010).AcknowledgmentsThis research is supported by the Singapore Nation-al Research Foundation under its International Re-search Centre @ Singapore Funding Initiative andadministered by the IDM Programme Office.
Wethank the reviewers for their valuable comments.ReferencesAmr Ahmed and Eric P. Xing.
2008.
Dynamic non-parametric mixture models and the recurrent Chinese543restaurant process: with applications to evolutionaryclustering.
In Proceedings of the SIAM InternationalConference on Data Mining, pages 219?230.Amr Ahmed and Eric P. Xing.
2010.
Timeline: A dy-namic hierarchical Dirichlet process model for recov-ering birth/death and evolution of topics in text stream.In Proceedings of the 26th Conference on Uncertaintyin Artificial Intelligence, pages 20?29.David M. Blei and John D. Lafferty.
2006.
Dynamictopic models.
In Proceedings of the 23rd InternationalConference on Machine Learning.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,and Hongjun Lu.
2005.
Parameter free bursty eventsdetection in text streams.
In Proceedings of the 31stInternational Conference on Very Large Data Bases,pages 181?192.Amit Gruber, Michal Rosen-Zvi, and Yair Weiss.
2007.Hidden topic Markov model.
In Proceedings of theInternational Conference on Artificial Intelligence andStatistics.Liangjie Hong, Byron Dom, Siva Gurumurthy, andKostas Tsioutsiouliklis.
2011.
A time-dependent top-ic model for multiple text streams.
In Proceedings ofthe 17th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 832?840.Alexander Ihler, Jon Hutchins, and Padhraic Smyth.2006.
Adaptive event detection with time-varyingpoisson processes.
In Proceedings of the 12thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 207?216.Jon Kleinberg.
2002.
Bursty and hierarchical structure instreams.
In Proceedings of the 8th ACM SIGKDD In-ternational Conference on Knowledge Discovery andData Mining, pages 91?101.Tomonari Masada, Daiji Fukagawa, Atsuhiro Takasu,Tsuyoshi Hamada, Yuichiro Shibata, and KiyoshiOguri.
2009.
Dynamic hyperparameter optimizationfor bayesian topical trend analysis.
In Proceedings ofthe 18th ACM Conference on Information and knowl-edge management, pages 1831?1834.Ramesh M. Nallapati, Susan Ditmore, John D. Lafferty,and Kin Ung.
2007.
Multiscale topic tomography.
InProceedings of the 13th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 520?529.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, andPadhraic Smyth.
2004.
The author-topic model for au-thors and documents.
In Proceedings of the 20th con-ference on Uncertainty in artificial intelligence, pages487?494.Xuerui Wang and Andrew McCallum.
2006.
Topics overtime: a non-Markov continuous-time model of topicaltrends.
In Proceedings of the 12th ACM SIGKDD In-ternational Conference on Knowledge Discovery andData Mining, pages 424?433.Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and RichardSproat.
2007.
Mining correlated bursty topic pattern-s from coordinated text streams.
In Proceedings ofthe 13th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 784?793.Chong Wang, David M. Blei, and David Heckerman.2008.
Continuous time dynamic topic models.
In Pro-ceedings of the 24th Conference on Uncertainty in Ar-tificial Intelligence, pages 579?586.Jianshu Weng and Francis Lee.
2011.
Event detection inTwitter.
In Proceedings of the 5th International AAAIConference on Weblogs and Social Media.Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,Ee-Peng Lim, Hongfei Yan, and Xiaoming Li.
2011.Comparing twitter and traditional media using topicmodels.
In Proceedings of the 33rd European confer-ence on Advances in information retrieval, pages 338?349.544
