Proceedings of the 12th Conference of the European Chapter of the ACL, pages 121?129,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsLarge-Coverage Root Lexicon Extraction for HindiCohan Sujay Carlos Monojit Choudhury Sandipan DandapatMicrosoft Research Indiamonojitc@microsoft.comAbstractThis paper describes a method using mor-phological rules and heuristics, for the au-tomatic extraction of large-coverage lexi-cons of stems and root word-forms froma raw text corpus.
We cast the problemof high-coverage lexicon extraction as oneof stemming followed by root word-formselection.
We examine the use of POStagging to improve precision and recall ofstemming and thereby the coverage of thelexicon.
We present accuracy, precisionand recall scores for the system on a Hindicorpus.1 IntroductionLarge-coverage morphological lexicons are an es-sential component of morphological analysers.Morphological analysers find application in lan-guage processing systems for tasks like tagging,parsing and machine translation.
While raw textis an abundant and easily accessible linguistic re-source, high-coverage morphological lexicons arescarce or unavailable in Hindi as in many otherlanguages (Cle?ment et al, 2004).
Thus, the devel-opment of better algorithms for the extraction ofmorphological lexicons from raw text corpora is atask of considerable importance.A root word-form lexicon is an intermediatestage in the creation of a morphological lexicon.In this paper, we consider the problem of extract-ing a large-coverage root word-form lexicon forthe Hindi language, a highly inflectional and mod-erately agglutinative Indo-European language spo-ken widely in South Asia.Since a POS tagger, another basic tool, wasavailable along with POS tagged data to train it,and since the error patterns indicated that POS tag-ging could greatly improve the accuracy of the lex-icon, we used the POS tagger in our experimentson lexicon extraction.Previous work in morphological lexicon extrac-tion from a raw corpus often does not achieve veryhigh precision and recall (de Lima, 1998; Oliverand Tadic?, 2004).
In some previous work the pro-cess of lexicon extraction involves incremental orpost-construction manual validation of the entirelexicon (Cle?ment et al, 2004; Sagot, 2005; Fors-berg et al, 2006; Sagot et al, 2006; Sagot, 2007).Our method attempts to improve on and extendthe previous work by increasing the precision andrecall of the system to such a point that manualvalidation might even be rendered unnecessary.Yet another difference, to our knowledge, is thatin our method we cast the problem of lexicon ex-traction as two subproblems: that of stemming andfollowing it, that of root word-form selection.The input resources for our system are as fol-lows: a) raw text corpus, b) morphological rules,c) POS tagger and d) word-segmentation labelleddata.
We output a stem lexicon and a root word-form lexicon.We take as input a raw text corpus and a setof morphological rules.
We first run a stemmingalgorithm that uses the morphological rules andsome heuristics to obtain a stem dictionary.
Wethen create a root dictionary from the stem dictio-nary.The last two input resources are optional butwhen a POS tagger is utilized, the F-score (har-monic mean of precision and recall) of the rootlexicon can be as high as 94.6%.In the rest of the paper, we provide a briefoverview of the morphological features of theHindi language, followed by a description of ourmethod including the specification of rules, thecorpora and the heuristics for stemming and rootword-form selection.
We then evaluate the systemwith and without the POS tagger.1212 Hindi Orthography and MorphologyThere are some features peculiar to Hindi orthog-raphy and to the character encoding system thatwe use.
These need to be compensated for in thesystem.
It was also found that Hindi?s inflectionalmorphology has certain characteristics that sim-plify the word segmentation rules.2.1 OrthographyHindi is written in the partially-phonemic Devana-gari script.
Most consonant clusters that occur inthe language are represented by characters and lig-atures, while a very few are represented as diacrit-ics.
Vowels that follow consonants or consonantclusters are marked with diacritics.
However, eachconsonant in the Devanagari script also carries animplicit vowel a1 unless its absence is marked by aspecial diacritic ?halant?.
Vowels are representedby vowel characters when they occur at the headof a word or after another vowel.The y sound sometimes does not surface in thepronunciation when it occurs between two vow-els.
So suffixes where the y is followed by e or Ican be written in two ways, with or without the ysound in them.
For instance the suffix ie can alsobe written as iye.Certain stemming rules will therefore need tobe duplicated in order to accommodate the differ-ent spelling possibilities and the different vowelrepresentations in Hindi.
The character encodingalso plays a small but significant role in the easeof stemming of Hindi word-forms.2.2 Unicode RepresentationWe used Unicode to encode Hindi characters.
TheUnicode representation of Devanagari treats sim-ple consonants and vowels as separate units and somakes it easier to match substrings at consonant-vowel boundaries.
Ligatures and diacritical formsof consonants are therefore represented by thesame character code and they can be equated verysimply.However, when using Unicode as the charac-ter encoding, it must be borne in mind that thereare different character codes for the vowel diacrit-ics and for the vowel characters for one and thesame vowel sound, and that the long and short1In the discussion in Section 2 and in Table 1 andTable 2, we have used a loose phonetic transcriptionthat resembles ITRANS (developed by Avinash Chopdehttp://www.aczoom.com/itrans/).Word Form Derivational Segmentation RootkarnA kar + nA karkarAnA kar + A + nA karkarvAnA kar + vA + nA karWord Form Inflectional Segmentation RootkarnA kar + nA karkarAnA karA + nA karAkarvAnA karvA + nA karvATable 1: Morpheme SegmentationlaDkA Nominative ObliqueSingular laDkA laDkePlural laDke laDkonlaDkI Nominative ObliqueSingular laDkI laDkIPlural laDkI laDkiyAnTable 2: Sample Paradigmsforms of the vowels are represented by differentcodes.
These artifacts of the character encodingneed to be compensated for when using substringmatches to identify the short vowel sound as beingpart of the corresponding prolonged vowel soundand when stemming.2.3 MorphologyThe inflectional morphology of Hindi does notpermit agglutination.
This helps keep the num-ber of inflectional morphological rules manage-able.
However, the derivational suffixes are agglu-tinative, leading to an explosion in the number ofroot word-forms in the inflectional root lexicon.The example in Table 1 shows that verbs cantake one of the two causative suffixes A and vA.These being derivational suffixes are not stemmedin our system and cause the verb lexicon to belarger than it would have otherwise.2.4 ParadigmsNouns, verbs and adjectives are the main POS cat-egories that undergo inflection in Hindi accordingto regular paradigm rules.For example, Hindi nouns inflect for case andnumber.
The inflections for the paradigms that thewords laDkA (meaning boy) and laDkI (mean-ing girl) belong to are shown in Table 2.
The rootword-forms are laDkA and laDkI respectively(the singular and nominative forms).122Hindi verbs are inflected by gender, number,person, mood and tense.
Hindi adjectives takeinflections for gender and case.
The number ofinflected forms in different POS categories variesconsiderably, with verbs tending to have a lot moreinflections than other POS categories.3 System DescriptionIn order to construct a morphological lexicon, weused a rule-based approach combined with heuris-tics for stem and root selection.
When used inconcert with a POS tagger, they could extract avery accurate morphological lexicon from a rawtext corpus.
Our system therefore consists of thefollowing components:1.
A raw text corpus in the Hindi language largeenough to contain a few hundred thousandunique word-forms and a smaller labelledcorpus to train a POS tagger with.2.
A list of rules comprising suffix strings andconstraints on the word-forms and POS cate-gories that they can be applied to.3.
A stemmer that uses the above rules, andsome heuristics to identify and reduce in-flected word-forms to stems.4.
A POS tagger to identify the POS category orcategories that the word forms in the raw textcorpus can belong to.5.
A root selector that identifies a root word-form and its paradigm from a stem and a setof inflections of the stem.The components of the system are described inmore detail below.3.1 Text CorporaRules alone are not always sufficient to identifythe best stem or root for a word-form, when thewords being stemmed have very few inflectionalforms or when a word might be stemmed in oneof many ways.
In that case, a raw text corpus canprovide important clues for identifying them.The raw text corpus that we use is the Web-Duniya corpus which consists of 1.4 million sen-tences of newswire and 21.8 million words.
Thecorpus, being newswire, is clearly not balanced.It has a preponderance of third-person formswhereas first and second person inflectional formsare under-represented.Name POS Paradigm Suffixes RootlaDkA noun {?A?,?e?,?on?}
?A?laDkI noun {?I?,?iyAn?}
?I?dho verb {??,?yogI?,?nA?,.
.
.}
?
?chal verb {??,?ogI?,?nA?,.
.
.}
?
?Table 3: Sample Paradigm Suffix SetsSince Hindi word boundaries are clearly markedwith punctuation and spaces, tokenization wasan easy task.
The raw text corpus yielded ap-proximately 331000 unique word-forms.
Whenwords beginning with numbers were removed, wewere left with about 316000 unique word-forms ofwhich almost half occurred only once in the cor-pus.In addition, we needed a corpus of 45,000words labelled with POS categories using the IL-POST tagset (Sankaran et al, 2008) for the POStagger.3.2 RulesThe morphological rules input into the system areused to recognize word-forms that together be-long to a paradigm.
Paradigms can be treated as aset of suffixes that can be used to generate inflec-tional word-forms from a stem.
The set of suffixesthat constitutes a paradigm defines an equivalenceclass on the set of unique word-forms in the cor-pus.For example, the laDkA paradigm in Table 2would be represented by the set of suffix strings{?A?, ?e?, ?on?}
derived from the word-formslaDkA, laDke and laDkon.
A few paradigmsare listed in Table 3.The suffix set formalism of a paradigm closelyresembles the one used in a previous attempt atunsupervised paradigm extraction (Zeman, 2007)but differs from it in that Zeman (2007) considersthe set of word-forms that match the paradigm tobe a part of the paradigm definition.In our system, we represent the morphologicalrules by a list of suffix add-delete rules.
Each rulein our method is a five-tuple {?, ?, ?, ?, ?}
where:?
?
is the suffix string to be matched for therule to apply.?
?
is the portion of the suffix string after whichthe stem ends.?
?
is a POS category in which the string ?
is avalid suffix.123?
?
?
?
??A?
??
Noun N1 ?A??on?
??
Noun N1,N3 ?A??e?
??
Noun N1 ?A??oyogI?
?o?
Verb V5 ?o?Table 4: Sample Paradigm RulesWord Form ?
Match Stem RootlaDkA laDk + A laDk laDkAlaDkon laDk + on laDk laDkAlaDke laDk + e laDk laDkAdhoyogI dh + oyogI dh + o dhoTable 5: Rule Application?
?
is a list of paradigms that contain the suffixstring ?.?
?
is the root suffixThe sample paradigm rules shown in Table 4would match the words laDkA, laDkon, laDkeand dhoyogI respectively and cause them to bestemmed and assigned roots as shown in Table 5.The rules by themselves can identify word-and-paradigm entries from the raw text corpus if a suf-ficient number of inflectional forms were present.For instance, if the words laDkA and laDkonwere present in the corpus, by taking the intersec-tion of the paradigms associated with the match-ing rules in Table 4, it would be possible to inferthat the root word-form was laDkA and that theparadigm was N1.We needed to create about 300 rules for Hindi.The rules could be stored in a list indexed by thesuffix in the case of Hindi because the number ofpossible suffixes was small.
For highly aggluti-native languages, such as Tamil and Malayalam,which can have thousands of suffixes, it would benecessary to use a Finite State Machine represen-tation of the rules.3.3 Suffix EvidenceWe define the term ?suffix evidence?
for a poten-tial stem as the number of word-forms in the cor-pus that are composed of a concatenation of thestem and any valid suffix.
For instance, the suf-fix evidence for the stem laDk is 2 if the word-forms laDkA and laDkon are the only word-forms with the prefix laDk that exist in the corpusand A and on are both valid suffixes.BSE Word-forms Accuracy1 20.5% 79%2 20.0% 70%3 13.2% 70%4 10.8% 81%5 & more 35.5% 80%Table 6: % Frequency and Accuracy by BSEBSE Nouns Verbs Others1 292 6 942 245 2 1363 172 15 664 120 16 715 & more 103 326 112Table 7: Frequency by POS CategoryTable 6 presents word-form counts for differ-ent suffix evidence values for the WebDuniya cor-pus.
Since the real stems for the word-forms werenot known, the prefix substring with the highestsuffix evidence was used as the stem.
We shallcall this heuristically selected stem the best-suffix-evidence stem and its suffix evidence as the best-suffix-evidence (BSE).It will be seen from Table 6 that about 20% ofthe words have a BSE of only 1.
Altogether about40% of the words have a BSE of 1 or 2.
Notethat all words have a BSE of atleast 1 since theempty string is also considered a valid suffix.
Thefraction is even higher for nouns as shown in Table7.It must be noted that the number of nouns witha BSE of 5 or more is in the hundreds only be-cause of erroneous concatenations of suffixes withstems.
Nouns in Hindi do not usually have morethan four inflectional forms.The scarcity of suffix evidence for most word-forms poses a huge obstacle to the extraction of ahigh-coverage lexicon because :1.
There are usually multiple ways to pick astem from word-forms with a BSE of 1 or 2.2.
Spurious stems cannot be detected easilywhen there is no overwhelming suffix evi-dence in favour of the correct stem.3.4 Gold StandardThe gold standard consists of one thousand word-forms picked at random from the intersection of124the unique word-forms in the unlabelled Web-Duniya corpus and the POS labelled corpus.
Eachword-form in the gold standard was manually ex-amined and a stem and a root word-form found forit.For word-forms associated with multiple POScategories, the stem and root of a word-form werelisted once for each POS category because the seg-mentation of a word could depend on its POS cat-egory.
There were 1913 word and POS categorycombinations in the gold standard.The creation of the stem gold standard neededsome arbitrary choices which had to be reflectedin the rules as well.
These concerned some wordswhich could be stemmed in multiple ways.
For in-stance, the noun laDkI meaning ?girl?
could besegmented into the morphemes laDk and I or al-lowed to remain unsegmented as laDkI.
This isbecause by doing the former, the stems of bothlaDkA and laDkI could be conflated whereasby doing the latter, they could be kept separatefrom each other.
We arbitrarily made the choiceto keep nouns ending in I unsegmented and madeour rules reflect that choice.A second gold standard consisting of 1000word-forms was also created to be used in eval-uation and as training data for supervised algo-rithms.
The second gold standard contained 1906word and POS category combinations.
Only word-forms that did not appear in the first gold standardwere included in the second one.3.5 StemmerSince the list of valid suffixes is given, the stem-mer does not need to discover the stems in the lan-guage but only learn to apply the right one in theright place.
We experimented with three heuristicsfor finding the right stem for a word-form.
Theheuristics were:?
Longest Suffix Match (LSM) - Picking thelongest suffix that can be applied to the word-form.?
Highest Suffix Evidence (HSE) - Picking thesuffix which yields the stem with the highestvalue for suffix evidence.?
Highest Suffix Evidence with SupervisedRule Selection (HSE + Sup) - Using labelleddata to modulate suffix matching.3.5.1 Longest Suffix Match (LSM)In the LSM heuristic, when multiple suffixes canbe applied to a word-form to stem it, we choosethe longest one.
Since Hindi has concatenativemorphology with only postfix inflection, we onlyneed to find one matching suffix to stem it.
It isclaimed in the literature that the method of us-ing the longest suffix match works better than ran-dom suffix selection (Sarkar and Bandyopadhyay,2008).
This heuristic was used as the baseline forour experiments.3.5.2 Highest Suffix Evidence (HSE)In the HSE heuristic, which has been applied be-fore to unsupervised morphological segmentation(Goldsmith, 2001), stemming (Pandey and Sid-diqui, 2008), and automatic paradigm extraction(Zeman, 2007), when multiple suffixes can be ap-plied to stem a word-form, the suffix that is pickedis the one that results in the stem with the high-est suffix evidence.
In our case, when computingthe suffix evidence, the following additional con-straint is applied: all the suffixes used to computethe suffix evidence score for any stem must be as-sociated with the same POS category.For example, the suffix yon is only applicableto nouns, whereas the suffix ta is only applicableto verbs.
These two suffixes will therefore neverbe counted together in computing the suffix evi-dence for a stem.
The algorithm for determiningthe suffix evidence computes the suffix evidenceonce for each POS category and then returns themaximum.In the absence of this constraint, the accuracydrops as the size of the raw word corpus increases.3.5.3 HSE and Supervised Rule Selection(HSE + Sup)The problem with the aforementioned heuristics isthat there are no weights assigned to rules.
Sincethe rules for the system were written to be as gen-eral and flexible as possible, false positives werecommonly encountered.
We propose a very sim-ple supervised learning method to circumvent thisproblem.The training data used was a set of 1000 word-forms sampled, like the gold standard, from theunique word-forms in the intersection of the rawtext corpus and the POS labelled corpus.
The setof word-forms in the training data was disjointfrom the set of word-forms in the gold standard.125Rules Accur Prec Recall F-ScoreRules1 73.65% 68.25% 69.4% 68.8%Rules2 75.0% 69.0% 77.6% 73.0%Table 8: Comparison of RulesGold 1 Accur Prec Recall F-ScoreLSM 71.6% 65.8% 66.1% 65.9%HSE 76.7% 70.6% 77.9% 74.1%HSE+Sup 78.0% 72.3% 79.8% 75.9%Gold 2 Accur Prec Recall F-ScoreLSM 75.7% 70.7% 72.7% 71.7%HSE 75.0% 69.0% 77.6% 73.0%HSE+Sup 75.3% 69.3% 78.0% 73.4%Table 9: Comparison of HeuristicsThe feature set consisted of two features: thelast character (or diacritic) of the word-form, andthe suffix.
The POS category was an optional fea-ture and used when available.
If the number of in-correct splits exceeded the number of correct splitsgiven a feature set, the rule was assigned a weightof 0, else it was given a weight of 1.3.5.4 ComparisonWe compare the performance of our rules withthe performance of the Lightweight Stemmer forHindi (Ramanathan and Rao, 2003) with a re-ported accuracy of 81.5%.
The scores we reportin Table 8 are the average of the LSM scoreson the two gold standards.
The stemmer usingthe standard rule-set (Rules1) does not perform aswell as the Lightweight Stemmer.
We then hand-crafted a different set of rules (Rules2) with ad-justments to maximize its performance.
The ac-curacy was better than Rules1 but not quite equalto the Lightweight Stemmer.
However, since ourgold standard is different from that used to eval-uate the Lightweight Stemmer, the comparison isnot necessarily very meaningful.As shown in Table 9, in F-score comparisons,HSE seems to outperform LSM and HSE+Supseems to outperform HSE, but the improvementin performance is not very large in the case of thesecond gold standard.
In terms of accuracy scores,LSM outperforms HSE and HSE+Sup when eval-uated against the second gold standard.POS Correct Incorrect POS ErrorsNoun 749 231 154Verb 324 108 0Adjective 227 49 13Others 136 82 35Table 10: Errors by POS Category3.5.5 Error AnalysisTable 10 lists the number of correct stems, in-correct stems, and finally a count of those incor-rect stems that the HSE+Sup heuristic would havegotten right if the POS category had been avail-able.
From the numbers it appears that a size-able fraction of the errors, especially with nounword-forms, is caused when a suffix of the wrongPOS category is applied to a word-form.
More-over, prior work in Bangla (Sarkar and Bandy-opadhyay, 2008) indicates that POS category in-formation could improve the accuracy of stem-ming.Assigning POS categories to word-forms re-quires a POS tagger and a substantial amount ofPOS labelled data as described below.3.5.6 POS TaggingThe POS tagset used was the hierarchical tagsetIL-POST (Sankaran et al, 2008).
The hierarchicaltagset supports broad POS categories like nounsand verbs, less broad POS types like common andproper nouns and finally, at its finest granularity,attributes like gender, number, case and mood.We found that with a training corpus of about45,000 tagged words (2366 sentences), it was pos-sible to produce a reasonably accurate POS tag-ger2, use it to label the raw text corpus with broadPOS tags, and consequently improve the accuracyof stemming.
For our experiments, we used boththe full training corpus of 45,000 words and a sub-set of the same consisting of about 20,000 words.The POS tagging accuracies obtained were ap-proximately 87% and 65% respectively.The reason for repeating the experiment usingthe 20,000 word subset of the training data was todemonstrate that a mere 20,000 words of labelleddata, which does not take a very great amount of2The Part-of-Speech tagger used was an implementa-tion of a Cyclic Dependency Network Part-of-Speech tagger(Toutanova et al, 2003).
The following feature set was usedin the tagger: tag of previous word, tag of next word, wordprefixes and suffixes of length exactly four, bigrams and thepresence of numbers or symbols.126time and effort to create, can produce significantimprovements in stemming performance.In order to assign tags to the words of the goldstandard, sentences from the raw text corpus con-taining word-forms present in the gold standardwere tagged using a POS tagger.
The POS cate-gories assigned to each word-form were then readoff and stored in a table.Once POS tags were associated with all thewords, a more restrictive criterion for matching arule to a word-form could be used to calculate theBSE in order to determine the stem of the word-form.
When searching for rules, and consequentlythe suffixes, to be applied to a word-form, onlyrules whose ?
value matches the word-form?s POScategory were considered.
We shall call the HSEheuristic that uses POS information in this wayHSE+Pos.3.6 Root SelectionThe stem lexicon obtained by the process de-scribed above had to be converted into a root word-form lexicon.
A root word-form lexicon is in somecases more useful than a stem lexicon, for the fol-lowing reasons:1.
Morphological lexicons are traditionally in-dexed by root word-forms2.
Multiple root word-forms may map to onestem and be conflated.3.
Tools that use the morphological lexicon mayexpect the lexicon to consist of roots insteadof stems.4.
Multiple root word-forms may map to onestem and be conflated.5.
Stems are entirely dependent on the waystemming rules are crafted.
Roots are inde-pendent of the stemming rules.The stem lexicon can be converted into a rootlexicon using the raw text corpus and the morpho-logical rules that were used for stemming, as fol-lows:1.
For any word-form and its stem, list all rulesthat match.2.
Generate all the root word-forms possiblefrom the matching rules and stems.3.
From the choices, select the root word-formwith the highest frequency in the corpus.Relative frequencies of word-forms have beenused in previous work to detect incorrect affix at-tachments in Bengali and English (Dasgupta andNg, 2007).
Our evaluation of the system showedthat relative frequencies could be very effectivepredictors of root word-forms when applied withinthe framework of a rule-based system.4 EvaluationThe goal of our experiment was to build a high-coverage morphological lexicon for Hindi and toevaluate the same.
Having developed a multi-stagesystem for lexicon extraction with a POS taggingstep following by stemming and root word-formdiscovery, we proceeded to evaluate it as follows.The stemming and the root discovery modulewere evaluated against the gold standard of 1000word-forms.
In the first experiment, the precisionand recall of stemming using the HSE+Pos algo-rithm were measured at different POS tagging ac-curacies.In the second experiment the root word-formdiscovery module was provided the entire rawword corpus to use in determining the best pos-sible candidate for a root and tested using the goldstandard.
The scores obtained reflect the perfor-mance of the overall system.For stemming, the recall was calculated as thefraction of stems and suffixes in the gold standardthat were returned by the stemmer for each word-form examined.
The precision was calculated asthe fraction of stems and suffixes returned by thestemmer that matched the gold standard.
The F-score was calculated as the harmonic mean of theprecision and recall.The recall of the root lexicon was measured asthe fraction of gold standard roots that were in thelexicon.
The precision was calculated as the frac-tion of roots in the lexicon that were also in thegold standard.
Accuracy was the percentage ofgold word-forms?
roots that were matched exactly.In order to approximately estimate the accuracyof a stemmer or morphological analyzer that usedsuch a lexicon, we also calculated the accuracyweighted by the frequency of the word-forms ina small corpus of running text.
The gold standardtokens were seen in this corpus about 4400 times.We only considered content words (nouns, verbs,adjectives and adverbs) in this calculation.127Gold1 Accur Prec Recall F-ScoPOS 86.7% 82.4% 86.2% 84.2%Sup+POS 88.2% 85.2% 87.3% 86.3%Gold2 Accur Prec Recall F-ScoPOS 81.8% 77.8% 82.0% 79.8%Sup+POS 83.5% 80.2% 82.6% 81.3%Table 11: Stemming Performance ComparisonsGold 1 Accur Prec Recall F-ScoNo POS 76.7% 70.6% 77.9% 74.1%65% POS 82.3% 77.5% 81.4% 79.4%87% POS 85.4% 80.8% 85.1% 82.9%Gold POS 86.7% 82.4% 86.2% 84.2%Table 12: Stemming Performance at DifferentPOS Tagger Accuracies5 ResultsThe performance of our system using POS tag in-formation is comparable to that obtained by Sarkarand Bandyopadhyay (2008).
Sarkar and Bandy-opadhyay (2008) obtained stemming accuracies of90.2% for Bangla using gold POS tags.
So in thecomparisons in Table 11, we use gold POS tags(row two) and also supervised learning (row three)using the other gold corpus as the labelled trainingcorpus.
We present the scores for the two goldstandards separately.
It must be noted that Sarkarand Bandyopadhyay (2008) conducted their ex-periments on Bangla, and so the results are notexactly comparable.We also evaluate the performance of stemmingusing HSE with POS tagging by a real tagger attwo different tagging accuracies - approximately65% and 87% - as shown in Table 12.
We com-pare the performance with gold POS tags and abaseline system which does not use POS tags.
Wedo not use labelled training data for this section ofthe experiments and only evaluate against the firstgold standard.Table 13 compares the F-scores for root discov-Gold 1 Accur Prec Recall F-ScoNo POS 71.7% 77.6% 78.8% 78.1%65% POS 82.5% 87.2% 88.9% 88.0%87% POS 87.0% 94.1% 95.3% 94.6%Gold POS 89.1% 95.4% 97.9% 96.6%Table 13: Root Finding AccuracyGold 1 Stemming Root Finding65% POS 85.6% 87.0%87% POS 87.5% 90.6%Gold POS 88.5% 90.2%Table 14: Weighted Stemming and Root FindingAccuracies (only Content Words)ery at different POS tagging accuracies against abaseline which excludes the use of POS tags alto-gether.
There seems to be very little prior workthat we can use for comparison here.
To ourknowledge, the closest comparable work is a sys-tem built by Oliver and Tadic?
(2004) in order toenlarge a Croatian Morphological Lexicon.
Theoverall performance reported by Tadic?
et alwasas follows: (precision=86.13%, recall=35.36%,F1=50.14%).Lastly, Table 14 shows the accuracy of stem-ming and root finding weighted by the frequenciesof the words in a running text corpus.
This wascalculated only for content words.6 ConclusionWe have described a system for automatically con-structing a root word-form lexicon from a rawtext corpus.
The system is rule-based and uti-lizes a POS tagger.
Though preliminary, our re-sults demonstrate that it is possible, using thismethod, to extract a high-precision and high-recallroot word-form lexicon.
Specifically, we showthat with a POS tagger capable of labelling word-forms with POS categories at an accuracy of about88%, we can extract root word-forms with an ac-curacy of about 87% and a precision and recall of94.1% and 95.3% respectively.Though the system has been evaluated on Hindi,the techniques described herein can probably beapplied to other inflectional languages.
The rulesselected by the system and applied to the word-forms also contain information that can be used todetermine the paradigm membership of each rootword-form.
Further work could evaluate the accu-racy with which we can accomplish this task.7 AcknowledgementsWe would like to thank our colleagues PriyankaBiswas, Kalika Bali and Shalini Hada, of Mi-crosoft Research India, for their assistance in thecreation of the Hindi root and stem gold standards.128ReferencesLionel Cle?ment, Beno?
?t Sagot and Bernard Lang.2004.
Morphology based automatic acquisition oflarge-coverage lexica.
In Proceedings of LREC2004, Lisbon, Portugal.Sajib Dasgupta and Vincent Ng.
2007.
High-Performance, Language-Independent Morphologi-cal Segmentation.
In Main Proceedings of NAACLHLT 2007, Rochester, NY, USA.Markus Forsberg, Harald Hammarstro?m and AarneRanta.
2006.
Morphological Lexicon Extractionfrom Raw Text Data.
In Proceedings of the 5th In-ternational Conference on Advances in Natural Lan-guage Processing, FinTAL, Finland.John A. Goldsmith.
2001.
Linguistica: An AutomaticMorphological Analyzer.
In Arika Okrent and JohnBoyle, editors, CLS 36: The Main Session, volume36-1, Chicago Linguistic Society, Chicago.Erika de Lima.
1998.
Induction of a Stem Lexicon forTwo-Level Morphological Analysis.
In Proceedingsof the Joint Conferences on New Methods in Lan-guage Processing and Computational Natural Lan-guage Learning, NeMLaP3/CoNLL98, pp 267-268,Sydney, Australia.Antoni Oliver, Marko Tadic?.
2004.
Enlarging theCroatian Morphological Lexicon by Automatic Lex-ical Acquisition from Raw Corpora.
In Proceedingsof LREC 2004, Lisbon, Portugal.Amaresh Kumar Pandey and Tanveer J. Siddiqui.2008.
An Unsupervised Hindi Stemmer withHeuristic Improvements.
In Proceedings of the Sec-ond Workshop on Analytics for Noisy UnstructuredText Data, AND 2008, pp 99-105, Singapore.A Ramanathan and D. D. Rao.
2003.
A LightweightStemmer for Hindi.
Presented at EACL 2003, Bu-dapest, Hungary.Beno?
?t Sagot.
2005.
Automatic Acquisition of aSlovak Lexicon from a Raw Corpus.
In LectureNotes in Artificial Intelligence 3658, Proceedings ofTSD?05, Karlovy Vary, Czech Republic.Beno?
?t Sagot.
2007.
Building a Morphosyntactic Lexi-con and a Pre-Syntactic Processing Chain for Polish.In Proceedings of LTC 2007, Poznan?, Poland.Beno?
?t Sagot, Lionel Cle?ment, E?ric Villemonte de laClergerie and Pierre Boullier.
2006.
The Lefff 2Syntactic Lexicon for French: Architecture, Acqui-sition, Use.
In Proceedings of LREC?06, Genoa,Italy.Baskaran Sankaran, Kalika Bali, Monojit Choudhury,Tanmoy Bhattacharya, Pushpak Bhattacharyya,Girish Nath Jha, S. Rajendran, K. Saravanan, L.Sobha and K.V.
Subbarao.
2008.
A Common Parts-of-Speech Tagset Framework for Indian Languages.In Proceedings of the Sixth International LanguageResources and Evaluation (LREC?08), Marrakech,Morocco.Sandipan Sarkar and Sivaji Bandyopadhyay.
2008.Design of a Rule-based Stemmer for Natural Lan-guage Text in Bengali.
In Proceedings of theIJCNLP-08 Workshop on NLP for Less PrivilegedLanguages, Hyderabad, India.Kristina Toutanova, Dan Klein, Christopher D. Man-ning and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic DependependencyNetwork In Proceedings of HLT-NAACL 2003pages 252-259.Daniel Zeman.
2007.
Unsupervised Acquisition ofMorphological Paradigms from Tokenized Text.
InWorking Notes for the Cross Language EvaluationForum CLEF 2007 Workshop, Budapest, Hungary.129
