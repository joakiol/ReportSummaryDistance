Proceedings of the 7th Workshop on Statistical Machine Translation, pages 304?311,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsThe RWTH Aachen Machine Translation System for WMT 2012Matthias Huck, Stephan Peitz, Markus Freitag, Malte Nuhn and Hermann NeyHuman Language Technology and Pattern Recognition GroupComputer Science DepartmentRWTH Aachen UniversityD-52056 Aachen, Germany<surname>@cs.rwth-aachen.deAbstractThis paper describes the statistical ma-chine translation (SMT) systems developed atRWTH Aachen University for the translationtask of the NAACL 2012 Seventh Workshop onStatistical Machine Translation (WMT 2012).We participated in the evaluation campaignfor the French-English and German-Englishlanguage pairs in both translation directions.Both hierarchical and phrase-based SMT sys-tems are applied.
A number of different tech-niques are evaluated, including an insertionmodel, different lexical smoothing methods,a discriminative reordering extension for thehierarchical system, reverse translation, andsystem combination.
By application of thesemethods we achieve considerable improve-ments over the respective baseline systems.1 IntroductionFor the WMT 2012 shared translation task1 RWTHutilized state-of-the-art phrase-based and hierarchi-cal translation systems as well as an in-house sys-tem combination framework.
We give a survey ofthese systems and the basic methods they implementin Section 2.
For both the French-English (Sec-tion 3) and the German-English (Section 4) languagepair, we investigate several different advanced tech-niques.
We concentrate on specific research direc-tions for each of the translation tasks and present therespective techniques along with the empirical re-sults they yield: For the French?English task (Sec-tion 3.1), we apply a standard phrase-based system.1http://www.statmt.org/wmt12/translation-task.htmlFor the English?French task (Section 3.2), we aug-ment a hierarchical phrase-based setup with a num-ber of enhancements like an insertion model, dif-ferent lexical smoothing methods, and a discrimina-tive reordering extension.
For the German?English(Section 4.3) and English?German (Section 4.4)tasks, we utilize morpho-syntactic analysis to pre-process the data (Section 4.1) and employ sys-tem combination to produce a consensus hypothesisfrom normal and reverse translations (Section 4.2) ofphrase-based and hierarchical phrase-based setups.2 Translation Systems2.1 Phrase-Based SystemThe phrase-based translation (PBT) system usedin this work is an in-house implementation of thestate-of-the-art decoder described in (Zens and Ney,2008).
We use the standard set of models withphrase translation probabilities and lexical smooth-ing in both directions, word and phrase penalty,distance-based distortion model, an n-gram targetlanguage model and three binary count features.
Theparameter weights are optimized with minimum er-ror rate training (MERT) (Och, 2003).2.2 Hierarchical Phrase-Based SystemFor our hierarchical phrase-based translation(HPBT) setups, we employ the open source trans-lation toolkit Jane (Vilar et al, 2010; Stein etal., 2011; Vilar et al, 2012), which has beendeveloped at RWTH and is freely available fornon-commercial use.
In hierarchical phrase-basedtranslation (Chiang, 2007), a weighted synchronouscontext-free grammar is induced from parallel text.304In addition to contiguous lexical phrases, hierar-chical phrases with up to two gaps are extracted.The search is carried out with a parsing-basedprocedure.
The standard models integrated into ourJane systems are: phrase translation probabilitiesand lexical smoothing probabilities in both trans-lation directions, word and phrase penalty, binaryfeatures marking hierarchical phrases, glue rule,and rules with non-terminals at the boundaries,four binary count features, and an n-gram languagemodel.
Optional additional models comprise IBMmodel 1 (Brown et al, 1993), discriminative wordlexicon (DWL) models and triplet lexicon models(Mauser et al, 2009), discriminative reordering ex-tensions (Huck et al, 2011a), insertion and deletionmodels (Huck and Ney, 2012), and several syntacticenhancements like preference grammars (Steinet al, 2010) and string-to-dependency features(Peter et al, 2011).
We utilize the cube pruningalgorithm (Huang and Chiang, 2007) for decodingand optimize the model weights with MERT.2.3 System CombinationSystem combination is used to produce consen-sus translations from multiple hypotheses generatedwith different translation engines.
The basic conceptof RWTH?s approach to machine translation systemcombination is described in (Matusov et al, 2006;Matusov et al, 2008).
This approach includes anenhanced alignment and reordering framework.
Alattice is built from the input hypotheses.
The trans-lation with the best score within the lattice accordingto a couple of statistical models is selected as con-sensus translation.2.4 Other Tools and TechniquesWe employ GIZA++ (Och and Ney, 2003) to trainword alignments.
The two trained alignments areheuristically merged to obtain a symmetrized wordalignment for phrase extraction.
All language mod-els (LMs) are created with the SRILM toolkit (Stol-cke, 2002) and are standard 4-gram LMs with in-terpolated modified Kneser-Ney smoothing (Kneserand Ney, 1995; Chen and Goodman, 1998).
Weevaluate in truecase, using the BLEU (Papineni et al,2002) and TER (Snover et al, 2006) measures.French EnglishEP + NC Sentences 2.1MRunning Words 63.3M 57.6MVocabulary 147.8K 128.5KSingletons 5.4K 5.1K+ 109 Sentences 22.9MRunning Words 728.6M 624.0MVocabulary 1.7M 1.7MSingletons 0.8M 0.8M+ UN Sentences 35.4MRunning Words 1 113.5M 956.4MVocabulary 1.9M 2.0MSingletons 0.9M 1.0MTable 1: Corpus statistics of the preprocessed French-English parallel training data.
EP denotes Europarl, NCdenotes News Commentary.
In the data, numerical quan-tities have been replaced by a single category symbol.3 French-English SetupsWe trained phrase-based translation systems forFrench?English and hierarchical phrase-basedtranslation systems for English?French.
Corpusstatistics for the French-English parallel data aregiven in Table 1.
The LMs are 4-grams trained onthe provided resources for the respective language(Europarl, News Commentary, UN, 109, and mono-lingual News Crawl language model training data).2For French?English we also investigate a smallerEnglish LM on Europarl and News Commentarydata only.
For English?French we experiment withadditional target-side data from the LDC French Gi-gaword Second Edition (LDC2009T28), which is anarchive of newswire text data that has been acquiredover several years by the LDC.3 The LDC FrenchGigaword v2 is permitted for constrained submis-sions in the WMT shared translation task.
As a de-velopment set for MERT, we use newstest2009 in allsetups.3.1 Experimental Results French?EnglishFor the French?English task, the phrase-basedSMT system (PBT) is set up using the standard mod-els listed in Section 2.1.
We vary the training datawe use to train the system and compare the results.2The parallel 109 corpus is often also referred to as WMTGiga French-English release 2.3http://www.ldc.upenn.edu305newstest2008 newstest2009 newstest2010 newstest2011French?English BLEU TER BLEU TER BLEU TER BLEU TERPBT baseline 20.3 63.8 23.0 60.0 23.2 59.1 24.7 57.3+ LM: +109+UN 22.5 61.4 26.2 57.3 26.6 56.1 27.7 54.5+ TM: +109 23.3 60.8 27.6 56.2 27.6 55.4 29.1 53.4Table 2: Results for the French?English task (truecase).
newstest2009 is used as development set.
BLEU and TERare given in percentage.newstest2008 newstest2009 newstest2010 newstest2011English?French BLEU TER BLEU TER BLEU TER BLEU TERHPBT 20.9 66.0 23.6 62.5 25.1 60.2 27.4 57.6+ 109 and UN 22.5 63.2 25.4 59.8 27.0 57.1 29.9 53.9+ LDC Gigaword v2 23.0 63.0 25.9 59.4 27.3 56.9 29.6 54.1+ insertion model 23.0 62.9 26.1 59.2 27.2 56.8 30.0 53.7+ noisy-or lexical scores 23.2 62.5 26.1 59.0 27.6 56.4 30.2 53.4+ DWL 23.3 62.5 26.2 58.9 27.9 55.9 30.4 53.2+ IBM-1 23.4 62.3 26.2 58.8 28.0 55.7 30.4 53.1+ discrim.
RO 23.5 62.2 26.7 58.5 28.1 55.9 30.8 52.8Table 3: Results for the English?French task (truecase).
newstest2009 is used as development set.
BLEU and TERare given in percentage.It should be noted that these setups do not use anyEnglish LDC Gigaword data for LM training at all.Our baseline system uses the Europarl and NewsCommentary data for training LM and phrase table.Corpus statistics are shown in the ?EP+NC?
sectionof Table 1.
This results in a performance of 24.7points BLEU on newstest2011.
Then we add the 109as well as UN data and more monolingual Englishdata from the News Crawl corpus to the data usedfor training the language model.
This system ob-tains a score of 27.7 points BLEU on newstest2011.Our final system uses Europarl, News Commentary,109 and UN data and News Crawl monolingual datafor LM training and the Europarl, News Commen-tary and 109 data (Table 1) for phrase table training.Using these data sets the system reaches 29.1 pointsBLEU.The experimental results are summarized in Ta-ble 2.3.2 Experimental Results English?FrenchFor the English?French task, the baseline system isa hierarchical phrase-based setup including the stan-dard models as listed in Section 2.2, apart from thebinary count features.
We limit the recursion depthfor hierarchical rules with a shallow-1 grammar (deGispert et al, 2010).In a shallow-1 grammar, the generic non-terminalX of the standard hierarchical approach is replacedby two distinct non-terminals XH and XP .
Bychanging the left-hand sides of the rules, lexicalphrases are allowed to be derived from XP only, hi-erarchical phrases from XH only.
On all right-handsides of hierarchical rules, the X is replaced by XP .Gaps within hierarchical phrases can thus solely befilled with purely lexicalized phrases, but not a sec-ond time with hierarchical phrases.
The initial ruleis substituted withS ?
?XP?0,XP?0?S ?
?XH?0,XH?0?
,(1)and the glue rule is substituted withS ?
?S?0XP?1, S?0XP?1?S ?
?S?0XH?1, S?0XH?1?
.
(2)The main benefit of a restriction of the recursiondepth is a gain in decoding efficiency, thus allow-ing us to set up systems more rapidly and to exploremore model combinations and more system config-urations.306The experimental results for English?French aregiven in Table 3.
Starting from the shallow hi-erarchical baseline setup on Europarl and NewsCommentary parallel data only (but Europarl, NewsCommentary, 109, UN, and News Crawl data for LMtraining), we are able to improve translation qual-ity considerably by first adopting more parallel (109and UN) and monolingual (French LDC Gigawordv2) training resources and then employing severaldifferent models that are not included in the baselinealready.
We proceed with individual descriptions ofthe methods we use and report their respective effectin BLEU on the test sets.109 and UN (up to +2.5 points BLEU) While theamount of provided parallel data from Europarland News Commentary sources is rather lim-ited (around 2M sentence pairs in total), theUN and the 109 corpus each provide a substan-tial collection of further training material.
Byappending both corpora, we end up at roughly35M parallel sentences (cf.
Table 1).
We utilizethis full amount of data in our system, but ex-tract a phrase table with only lexical (i.e.
non-hierarchical) phrases from the full parallel data.We add it as a second phrase table to the base-line system, with a binary feature that enablesthe system to reward or penalize the applicationof phrases from this table.LDC Gigaword v2 (up to +0.5 points BLEU)The LDC French Gigaword Second Edition(LDC2009T28) provides some more monolin-gual French resources.
We include a total of28.2M sentences from both the AFP and APWcollections in our LM training data.insertion model (up to +0.4 points BLEU) We addan insertion model to the log-linear model com-bination.
This model is designed as a means toavoid the omission of content words in the hy-potheses.
It is implemented as a phrase-levelfeature function which counts the number of in-serted words.
We apply the model in source-to-target and target-to-source direction.
A target-side word is considered inserted based on lexi-cal probabilities with the words on the foreignlanguage side of the phrase, and vice versa fora source-side word.
As thresholds, we computeindividual arithmetic averages for each wordfrom the vocabulary (Huck and Ney, 2012).noisy-or lexical scores (up to +0.4 points BLEU) Inour baseline system, the tNorm(?)
lexical scor-ing variant as described in (Huck et al, 2011a)is employed with a relative frequency (RF) lex-icon model for phrase table smoothing.
Thesingle-word based translation probabilities ofthe RF lexicon model are extracted from word-aligned parallel training data, in the fashionof (Koehn et al, 2003).
We exchange the base-line lexical scoring with a noisy-or (Zens andNey, 2004) lexical scoring variant tNoisyOr(?
).DWL (up to +0.3 points BLEU) We augmentour system with phrase-level lexical scoresfrom discriminative word lexicon (DWL) mod-els (Mauser et al, 2009; Huck et al, 2011a)in both source-to-target and target-to-source di-rection.
The DWLs are trained on News Com-mentary data only.IBM-1 (up to +0.1 points BLEU) On News Com-mentary and Europarl data, we train IBMmodel-1 (Brown et al, 1993) lexicons in bothtranslation directions and also use them to com-pute phrase-level scores.discrim.
RO (up to +0.4 points BLEU) The modi-fication of the grammar to a shallow-1 versionrestricts the search space of the decoder and isconvenient to prevent overgeneration.
In ordernot to be too restrictive, we reintroduce moreflexibility into the search process by extendingthe grammar with specific reordering rulesXP ?
?XP?0XP?1,XP?1XP?0?XP ?
?XP?0XP?1,XP?0XP?1?
.
(3)The upper rule in Equation (3) is a swap rulethat allows adjacent lexical phrases to be trans-posed, the lower rule is added for symmetryreasons, in particular because sequences as-sembled with these rules are allowed to fill gapswithin hierarchical phrases.
Note that we applya length constraint of 10 to the number of ter-minals spanned by an XP .
We introduce twobinary indicator features, one for each of thetwo rules in Equation (3).
In addition to adding307German EnglishSentences 2.0MRunning Words 55.3M 55.7MVocabulary 191.6K 129.0KSingletons 75.5K 51.8KTable 4: Corpus statistics of the preprocessed German-English parallel training data (Europarl and News Com-mentary).
In the data, numerical quantities have been re-placed by a single category symbol.these rules, a discriminatively trained lexical-ized reordering model is applied (Huck et al,2012).4 German-English SetupsWe trained phrase-based and hierarchical transla-tion systems for both translation directions of theGerman-English language pair.
Corpus statistics forGerman-English can be found in Table 4.
The lan-guage models are 4-grams trained on the respectivetarget side of the bilingual data as well as on the pro-vided News Crawl corpus.
For the English languagemodel the 109 French-English, UN and LDC Giga-word Fourth Edition corpora are used additionally.For the 109 French-English, UN and LDC Gigawordcorpora we apply the data selection technique de-scribed in (Moore and Lewis, 2010).
We examinetwo different language models, one with LDC dataand one without.
All German?English systems areoptimized on newstest2010.
For English?German,we use newstest2009 as development set.
The news-test2011 set is used as test set and the scores for new-stest2008 are included for completeness.4.1 Morpho-Syntactic AnalysisIn order to reduce the source vocabulary size forthe German?English translation, the German textis preprocessed by splitting German compoundwords with the frequency-based method described in(Koehn and Knight, 2003).
To further reduce trans-lation complexity of PBT, we employ the long-rangepart-of-speech based reordering rules proposed byPopovic?
and Ney (2006).4.2 Reverse TranslationFor reverse translations we need to change the wordorder of the bilingual corpus.
For example, if we re-verse both source and target language, the originaltraining example ?der Hund mag die Katze .
?
thedog likes the cat .?
is converted into a new trainingexample ?.
Katze die mag Hund der?
.
cat the likesdog the?.
We call this type of modification of sourceor target language reversion.
A system trained ofthis data is called reverse.
This modification changesthe corpora and hence the language model and align-ment training produce different results.4.3 Experimental Results German?EnglishOur results for the German?English task are shownin Table 5.
For this task, we apply the idea of reversetranslation for both the phrase-based and the hierar-chical approach.
It seems that the reversed systemsperform slightly worse.
However, when we em-ploy system combination using both reverse trans-lation setups (PBT reverse and HPBT reverse) andboth baseline setups (PBT baseline and HPBT base-line), the translation quality is improved by up to 0.4points in BLEU and 1.0 points TER compared to thebest single system.The addition of LDC Gigaword corpora (+GW)to the language model training data of the baselinesetups shows improvements in both BLEU and TER.Furthermore, with the system combination includingthese setups, we are able to report an improvementof up to 0.7 points BLEU and 1.0 points TER over thebest single setup.
Compared to the system combina-tion based on systems which are not using the LDCGigaword corpora, we gain 0.3 points in BLEU and0.4 points in TER.4.4 Experimental Results English?GermanOur results for the English?German task are shownin Table 6.
For this task, we first compare sys-tems using one, two or three language models ofdifferent parts of the data.
The language modelfor systems with only one language model is cre-ated with all monolingual and parallel data.
A lan-guage model with all monolingual data and a lan-guage model with all parallel data is created for thesystems with two language models.
For the systemswith three language models, we also split the paralleldata in two parts consisting of either only Europarldata or only News Commentary data.
For PBT thesystem with two language models performs best forall test sets.
Further, we apply the idea of reverse308newstest2008 newstest2009 newstest2010 newstest2011German?English BLEU TER BLEU TER BLEU TER BLEU TERPBT baseline 21.1 62.3 20.8 61.4 23.7 59.3 21.3 61.3PBT reverse 20.8 62.4 20.6 61.5 23.6 59.2 21.2 61.2HPBT baseline 21.3 62.5 20.9 61.7 23.9 59.4 21.3 61.6HPBT reverse 21.2 63.5 20.9 62.0 23.6 59.2 21.4 61.9system combination (secondary) 21.5 61.6 21.2 60.6 24.3 58.3 21.7 60.3PBT baseline +GW 21.5 61.9 21.2 61.1 24.0 59.0 21.3 61.4PBT reverse 20.8 62.4 20.6 61.5 23.6 59.2 21.2 61.2HPBT baseline +GW 21.6 62.3 21.3 61.3 24.0 59.4 21.6 61.5HPBT reverse 21.2 63.5 20.9 62.0 23.6 59.2 21.4 61.9system combination (primary) 21.9 61.2 21.4 60.5 24.7 58.0 21.9 60.2Table 5: Results for the German?English task (truecase).
+GW denotes the usage of LDC Gigaword data for thelanguage model, newstest2010 serves as development set.
BLEU and TER are given in percentage.newstest2008 newstest2009 newstest2010 newstest2011English?German BLEU TER BLEU TER BLEU TER BLEU TERPBT baseline 1 LM 14.6 71.7 14.8 70.8 15.8 66.9 15.3 70.0PBT baseline 2 LM (*) 14.9 70.9 14.9 70.4 16.0 66.3 15.4 69.5PBT baseline 3 LM 14.8 71.5 14.9 70.5 16.0 66.7 15.1 70.1PBT reverse 2 LM (*) 14.9 71.4 15.1 70.2 15.9 66.5 15.0 69.7HPBT baseline 2 LM (*) 15.1 71.8 15.3 71.1 16.2 67.4 15.4 70.3HPBT baseline 2 LM opt on 4bleu-ter 15.2 68.4 15.0 67.7 15.9 64.6 15.1 67.1HPBT reverse 2 LM (*) 15.4 71.3 15.3 70.7 16.7 66.9 15.5 70.1syscombi of (*) 15.6 69.2 15.4 68.9 16.5 65.0 15.6 68.0Table 6: Results for the English?German task (truecase).
newstest2009 is used as development set.
BLEU and TERare given in percentage.translation for both the phrase-based and the hier-archical approach.
The PBT reverse 2 LM systemsperform slightly worse compared to PBT baseline 2LM.
The HPBT reverse 2 LM performs better com-pared to HPBT baseline 2 LM.
When we employsystem combination using both reverse translationsetups (PBT reverse 2 LM and HPBT reverse 2 LM)and both baseline setups (PBT baseline 2 LM andHPBT baseline 2 LM), the translation quality is im-proved by up to 0.2 points in BLEU and 2.1 points inTER compared to the best single system.5 ConclusionFor the participation in the WMT 2012 shared trans-lation task, RWTH experimented with both phrase-based and hierarchical translation systems.
Severaldifferent techniques were evaluated and yielded con-siderable improvements over the respective base-line systems as well as over our last year?s setups(Huck et al, 2011b).
Among these techniques arean insertion model, the noisy-or lexical scoring vari-ant, additional phrase-level lexical scores from IBMmodel 1 and discriminative word lexicon models, adiscriminative reordering extension for hierarchicaltranslation, reverse translation, and system combi-nation.AcknowledgmentsThis work was achieved as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The Mathemat-309ics of Statistical Machine Translation: Parameter Es-timation.
Computational Linguistics, 19(2):263?311,June.Stanley F. Chen and Joshua Goodman.
1998.
An Em-pirical Study of Smoothing Techniques for LanguageModeling.
Technical Report TR-10-98, ComputerScience Group, Harvard University, Cambridge, Mas-sachusetts, USA, August.David Chiang.
2007.
Hierarchical Phrase-Based Trans-lation.
Computational Linguistics, 33(2):201?228.Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,Eduardo R. Banga, and William Byrne.
2010.
Hierar-chical Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars.
Compu-tational Linguistics, 36(3):505?533.Liang Huang and David Chiang.
2007.
Forest Rescoring:Faster Decoding with Integrated Language Models.
InProceedings of the 45th Annual Meeting of the Associ-ation for Computational Linguistics, pages 144?151,Prague, Czech Republic, June.Matthias Huck and Hermann Ney.
2012.
Insertionand Deletion Models for Statistical Machine Trans-lation.
In Proceedings of the North American Chap-ter of the Association for Computational Linguistics -Human Language Technologies conference, Montreal,Canada, June.Matthias Huck, Saab Mansour, Simon Wiesler, and Her-mann Ney.
2011a.
Lexicon Models for HierarchicalPhrase-Based Machine Translation.
In InternationalWorkshop on Spoken Language Translation, pages191?198, San Francisco, California, USA, December.Matthias Huck, Joern Wuebker, Christoph Schmidt,Markus Freitag, Stephan Peitz, Daniel Stein, ArnaudDagnelies, Saab Mansour, Gregor Leusch, and Her-mann Ney.
2011b.
The RWTH Aachen MachineTranslation System for WMT 2011.
In EMNLP 2011Sixth Workshop on Statistical Machine Translation,pages 405?412, Edinburgh, UK, July.Matthias Huck, Stephan Peitz, Markus Freitag, and Her-mann Ney.
2012.
Discriminative Reordering Exten-sions for Hierarchical Phrase-Based Machine Transla-tion.
In 16th Annual Conference of the European As-sociation for Machine Translation, Trento, Italy, May.Reinhard Kneser and Hermann Ney.
1995.
ImprovedBacking-Off for M-gram Language Modeling.
In Pro-ceedings of the International Conference on Acoustics,Speech, and Signal Processing, volume 1, pages 181?184, May.Philipp Koehn and Kevin Knight.
2003.
Empirical Meth-ods for Compound Splitting.
In Proceedings of Euro-pean Chapter of the ACL (EACL 2009), pages 187?194.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
InProc.
of the Human Language Technology Conf.
(HLT-NAACL), pages 127?133, Edmonton, Canada,May/June.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing Consensus Translation from Multi-ple Machine Translation Systems Using Enhanced Hy-potheses Alignment.
In Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL), pages 33?40, Trento, Italy, April.E.
Matusov, G. Leusch, R.E.
Banchs, N. Bertoldi,D.
Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,J.B.
Marino, M. Paulik, S. Roukos, H. Schwenk, andH.
Ney.
2008.
System Combination for MachineTranslation of Spoken and Written Language.
IEEETransactions on Audio, Speech and Language Pro-cessing, 16(7):1222?1237.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.
Ex-tending Statistical Machine Translation with Discrimi-native and Trigger-Based Lexicon Models.
In Proc.
ofthe Conf.
on Empirical Methods for Natural LanguageProcessing (EMNLP), pages 210?218, Singapore, Au-gust.Robert C. Moore and William Lewis.
2010.
Intelli-gent Selection of Language Model Training Data.
InACL (Short Papers), pages 220?224, Uppsala, Swe-den, July.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51, March.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 160?167, Sapporo,Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, Penn-sylvania, USA, July.Jan-Thorsten Peter, Matthias Huck, Hermann Ney, andDaniel Stein.
2011.
Soft String-to-Dependency Hier-archical Machine Translation.
In International Work-shop on Spoken Language Translation, pages 246?253, San Francisco, California, USA, December.Maja Popovic?
and Hermann Ney.
2006.
POS-basedWord Reorderings for Statistical Machine Translation.In International Conference on Language Resourcesand Evaluation, pages 1278?1283, Genoa, Italy, May.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human Anno-tation.
In Proceedings of the 7th Conference of the310Association for Machine Translation in the Americas,pages 223?231, Cambridge, Massachusetts, USA, Au-gust.Daniel Stein, Stephan Peitz, David Vilar, and HermannNey.
2010.
A Cocktail of Deep Syntactic Featuresfor Hierarchical Machine Translation.
In Conf.
of theAssociation for Machine Translation in the Americas(AMTA), Denver, Colorado, USA, October/November.Daniel Stein, David Vilar, Stephan Peitz, Markus Fre-itag, Matthias Huck, and Hermann Ney.
2011.
AGuide to Jane, an Open Source Hierarchical Trans-lation Toolkit.
The Prague Bulletin of MathematicalLinguistics, (95):5?18, April.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.
onSpeech and Language Processing (ICSLP), volume 2,pages 901?904, Denver, Colorado, USA, September.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2010.
Jane: Open source hierarchical transla-tion, extended with reordering and lexicon models.
InACL 2010 Joint Fifth Workshop on Statistical MachineTranslation and Metrics MATR, pages 262?270, Upp-sala, Sweden, July.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2012.
Jane: an advanced freely available hier-archical machine translation toolkit.
Machine Trans-lation, pages 1?20.
http://dx.doi.org/10.1007/s10590-011-9120-y.Richard Zens and Hermann Ney.
2004.
Improve-ments in Phrase-Based Statistical Machine Transla-tion.
In Proc.
Human Language Technology Conf.
/North American Chapter of the Association for Com-putational Linguistics Annual Meeting (HLT-NAACL),pages 257?264, Boston, Massachusetts, USA, May.Richard Zens and Hermann Ney.
2008.
Improvementsin Dynamic Programming Beam Search for Phrase-based Statistical Machine Translation.
In Interna-tional Workshop on Spoken Language Translation,pages 195?205, Honolulu, Hawaii, USA, October.311
