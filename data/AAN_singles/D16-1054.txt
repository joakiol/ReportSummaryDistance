Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 562?572,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsOn Generating Characteristic-rich Question Sets for QA EvaluationYu Su1, Huan Sun2, Brian Sadler3, Mudhakar Srivatsa4Izzeddin Gu?r1, Zenghui Yan1 and Xifeng Yan11University of California, Santa Barbara, Department of Computer Science2The Ohio State University, Department of Computer Science and Engineering3U.S.
Army Research Lab, 4IBM Research{ysu,izzeddingur,zyan,xyan}@cs.ucsb.edu, sun.397@osu.edubrian.m.sadler6.civ@mail.mil, msrivats@us.ibm.comAbstractWe present a semi-automated framework forconstructing factoid question answering (QA)datasets, where an array of question character-istics are formalized, including structure com-plexity, function, commonness, answer cardi-nality, and paraphrasing.
Instead of collectingquestions and manually characterizing them,we employ a reverse procedure, first generat-ing a kind of graph-structured logical formsfrom a knowledge base, and then convertingthem into questions.
Our work is the firstto generate questions with explicitly specifiedcharacteristics for QA evaluation.
We con-struct a new QA dataset with over 5,000 log-ical form-question pairs, associated with an-swers from the knowledge base, and show thatdatasets constructed in this way enable fine-grained analyses of QA systems.
The datasetcan be found in https://github.com/ysu1989/GraphQuestions.1 IntroductionFactoid question answering (QA) has gained greatattention recently, owing to the fast growth of largeknowledge bases (KBs) such as DBpedia (Lehmannet al, 2014) and Freebase (Bollacker et al, 2008),which avail QA systems of comprehensive and pre-cise knowledge of encyclopedic scope (Yahya et al,2012; Berant et al, 2013; Cai and Yates, 2013;Kwiatkowski et al, 2013; Berant and Liang, 2014;Fader et al, 2014; Reddy et al, 2014; Bao et al,2014; Zou et al, 2014; Yao and Van Durme, 2014;Yih et al, 2015; Sun et al, 2015; Dong et al, 2015;Yao, 2015; Berant and Liang, 2015).
With the blos-soming of QA systems, evaluation is becoming anincreasingly important problem.
QA datasets, con-sisting of a set of questions with ground-truth an-swers, are critical for both comparing existing sys-tems and gaining insights to develop new systems.Questions have rich characteristics, constitutingdimensions along which question difficulty varies.Some questions are difficult due to their com-plex semantic structure (?Who was the coach whenMichael Jordan stopped playing for the ChicagoBulls??
), while some others may be difficult becausethey require a precise quantitative analysis over theanswer space (?What is the best-selling smartphonein 2015??).
Many other characteristics shall be con-sidered too, e.g., what topic a question is about(questions about common topics may be easier toanswer) and how many answers there are (it is harderto achieve a high recall in case of multiple answers).Worse still, due to the flexibility of natural language,different people often describe the same question indifferent ways, i.e., paraphrasing.
It is important fora QA system to be robust to paraphrasing.A QA dataset explicitly specifying such ques-tion characteristics allows for fine-grained inspec-tion of system performance.
However, to thebest of our knowledge, none of the existing QAdatasets (Voorhees and Tice, 2000; Berant et al,2013; Cai and Yates, 2013; Lopez et al, 2013; Bor-des et al, 2015; Serban et al, 2016) provides ques-tion characteristics.
In this work, we make the firstattempt to generate questions with explicitly speci-fied characteristics, and examine the impact of vari-ous question characteristics in QA.We present a semi-automated framework (Fig-ure 1) to construct QA datasets with characteristic562Figure 1: Running example of our framework.
Graph queries are first generated from a knowledge base.
After refinement (notshown), graph queries are sent to human annotators and converted into natural language questions.
Answers are collected from theknowledge base.specification from a knowledge base.
The frame-work revolves around an intermediate graph queryrepresentation, which helps to formalize questioncharacteristics and collect answers.
We first auto-matically generate graph queries from a knowledgebase, and then employ human annotators to convertgraph queries into questions.Automating graph query generation brings withit the challenge of assessing the quality of graphqueries and filtering out bad ones.
Our frameworktackles the challenge by combining structured infor-mation in the knowledge base and statistical infor-mation from the Web.
First, we identify redundantcomponents in a graph query and develop techniquesto remove them.
Furthermore, based on the fre-quency of entities, classes, and relations mined fromthe Web, we quantify the commonness of a graphquery and filter out too rare ones.We employ a semi-automated approach for theconversion from graph query to natural languagequestion, which provides two levels of paraphras-ing: Common lexical forms of an entity (e.g.,?Queen Elizabeth?
and ?Her Majesty the Queen?for ElizabethII) mined from the Web are usedas entity paraphrases, and the remaining parts of aquestion are paraphrased by annotators.
As a result,dozens of paraphrased questions can be produced fora single graph query.To demonstrate the usefulness of question char-acteristics in QA evaluation, we construct a newdataset with over 5,000 questions based on Freebaseusing the proposed framework, and extensively eval-uate several QA systems.
A couple of new find-ings about system performance and question diffi-culty are discussed.
For example, different fromthe results based on previous QA datasets (Yao etal., 2014), we find that semantic parsing in gen-eral works better than information extraction on ourdataset.
Information extraction based QA systemshave trouble dealing with questions requiring aggre-gation or with multiple answers.
A holistic under-standing of the whole question is often needed forhard questions.
The experiments point out an arrayof issues that future QA systems may need to solve.2 Related WorkEarly QA research has extensively studied problemslike question taxonomy, answer type, and knowl-edge sources (Burger et al, 2001; Hirschman andGaizauskas, 2001; Voorhees and Tice, 2000).
Thiswork mainly targets factoid questions with one ormore answers that are guaranteed to exist in a KB.A few KB-based QA datasets have been pro-posed recently.
QALD (Lopez et al, 2013) andFREE917 (Cai and Yates, 2013) contain hundredsof hand-crafted questions.
QALD also indicateswhether a question requires aggregation.
Bothbased on single Freebase triples, SIMPLEQUES-TIONS (Bordes et al, 2015) employ human an-notators to formulate questions, while Serban etal.
(2016) use a recurrent neural network to auto-matically formulate questions.
They are featuredby a large size, but the questions only concern sin-gle triples, while our framework can generate ques-563tions involving multiple triples and various func-tions.
Wang et al (2015) generate question-answerpairs for closed domains like basketball.
Theyalso first generate logical forms (?-DCS formu-lae (Liang, 2013) in their case), and then convert log-ical forms into questions via crowdsourcing.
Logi-cal forms are first converted into canonical questionsto help crowdsourcing workers.
Different from pre-vious works, we put a particular focus on generatingquestions with diversified characteristics in a sys-tematic way, and examining the impact of differentquestion characteristics in QA.Another attractive way for QA dataset construc-tion is to collect questions from search enginelogs (Bendersky and Croft, 2009).
For exam-ple, WEBQUESTIONS (Berant et al, 2013) containsthousands of popular questions from Google search,and Yih et al (2016) have manually annotated thesequestions with logical forms.
However, automaticcharacterization of questions is hard, while man-ual characterization is costly and requires exper-tise.
Moreover, users?
search behavior is shaped bysearch engines (Aula et al, 2010).
Due to the inade-quacy of current search engines to answer advancedquestions, users may adapt themselves accordinglyand mostly ask simple questions.
Thus questionscollected in this way, to some extent, may still notwell reflect the true distribution of user informationneeds, nor does it fully exploit the potential of KB-based QA.
Collecting answers is yet another chal-lenge for this approach.
Yih et al (2016) show thatonly 66% of the WEBQUESTIONS answers, whichwere collected via crowdsourcing, are completelycorrect.
On the other hand, although questions gen-erated from a KB may not follow the distribution ofuser information needs, it has the advantage of ex-plicit question characteristics, and enables program-matic configuration of question generation.
Also,answer collecting is automated without involvinghuman labor and errors.3 Background3.1 Knowledge BaseIn this work, we mainly concern knowledge basesstoring knowledge about entities and relations in theform of triples (simply knowledge bases hereafter).Suppose E is a set of entities, L a set of literals (I =E ?
L is also called individuals), C a set of classes,and R a set of directed relations, a knowledge baseK consists of two parts: an ontologyO ?
C?R?Cand a modelM ?
E ?
R ?
(C ?
E ?
L).
In otherwords, an ontology specifies classes and relationsbetween classes, and a model consists of facts aboutindividuals.
Such knowledge bases can be naturallyrepresented as a directed graph, e.g., Figure 1(a).Literal classes such as Datetime are represented asdiamonds, and other classes are rounded rectangles.Individuals are shaded.
We assume relations aretyped, i.e., each relation is associated with a set ofdomain and range classes.
Facts of a relation mustbe compatible with its domain and range constraints.Without loss of generality, we use Freebase (June2013 version) in this work for compatibility with theto-be-tested QA systems.
It has 24K classes, 65Krelations, 41M entities, and 596M facts.3.2 Graph QueryMotivated by the graph-structured nature of knowl-edge bases, we adopt a graph-centric approach.
Wehinge on a formal representation named graph query(e.g., Figure 1(c)), developed on the basis of Yih etal.
(2015) and influenced by ?-DCS (Liang, 2013).Syntax.
A graph query q is a connected directedgraph built on a given knowledge base K. It com-prises three kinds of nodes: (1) Question node (dou-ble rounded rectangle), a free variable.
(2) Un-grounded node (rounded rectangle or diamond), anexistentially quantified variable.
(3) Grounded node(shaded rounded rectangle or diamond), an individ-ual.
In addition, there are functions (shaded circle)such as < and count applied on a node.
Nodes aretyped, each associated with a class.
Nodes are con-nected by directed edges representing relations.
En-tities on the grounded nodes are called topic entities.Semantics.
Graph query is a strict subset of ?-calculus.
For example, the graph query in Fig-ure 1(c) can be written in ?-calculus (an existentiallyquantified variable is imposed by <):?x.?y.
?z.type(x, DeceasedPerson)?
type(y, DeceasedPerson)?
type(z, Datetime) ?
parents(x, y)?
causeOfDeath(x, LungCancer)?
causeOfDeath(y, LungCancer)?
dateOfDeath(x, z)?
< (z, 1960).564The answer of a graph query q, denoted as JqKK,can be easily obtained from K. For example, if Kis stored in a RDF triplestore, then q can be au-tomatically converted into a SPARQL query andrun against K to get the answer.
Compared withYih et al (2015), graph queries are not constrainedto be tree-structured, which grants us a higher ex-pressivity.
For example, linguistic phenomena likeanaphora (e.g., Figure 1(d)) become easier to model.4 Automatic Graph Query GenerationOur framework proceeds as follows: (1) Generatequery templates from a knowledge base, groundthe templates to generate graph queries, and col-lect answers (this section).
(2) Refine graph queriesto retain high-quality ones (Section 5).
(3) Con-vert graph queries into questions via crowdsourcing(Section 6).We now describe an algorithm to generate thequery template shown in Figure 1(b) (excluding thefunction for now).
For simplicity, we will focus onthe case of a single question node.
Nevertheless,the proposed framework can be extended to generategraph queries with multiple question nodes.
The al-gorithm takes as input an ontology (Figure 1(a)) andthe desired number of edges.
All the operations areconducted in a random manner to avoid systematicbiases in query generation.
The DeceasedPersonclass is first selected as the question node.
We theniteratively grow it by adding neighboring nodes andedges in the ontology.
In each iteration, an exist-ing node is selected, and a new edge, which mightintroduce a new node, is appended to it.
For exam-ple, the relation causeOfDeath, whose domain in-cludes DeceasedPerson, is first appended to thequestion node, and then one of its range classes,CauseOfDeath, is added as a new node.
When anode with the class CauseOfDeath already exists,it is possible to add an edge without introducing anew node.
The same relation or class can be addedmultiple times, e.g., ?parent of parent?.Topic entities like LungCancer play an impor-tant role in a question.
A query template containssome template nodes that can be grounded withdifferent topic entities to generate different graphqueries.
We randomly choose a few nodes as tem-plate.
It may cause problems.
For example, ground-Figure 2: Mutual exclusivity example.
Entities on differentnodes should be different.ing one node may make some others redundant.
Weconduct a formal study on this in Section 5.1.Functions such as counting and comparatives arepervasive in real-life questions, e.g., ?how many?,?the most recent?, and ?people older than?, butare scarce in existing QA datasets.
We incorpo-rate functions as an important question characteris-tic, and consider nine common functions, groupedinto three categories: counting (count), superlative(max, min, argmax, argmin), and comparative (>,?, <, ?).
More functions can be incorporated inthe future.
See Appendix A for examples.
We ran-domly add functions to compatible nodes in querytemplates.
In the running example, the < functionimposes the constraint that only people who passedaway before a certain date should be considered.Each query will have at most one function.We then ground the template nodes with indi-viduals to generate graph queries.
A grounding isvalid if the individuals conform with the class ofthe corresponding template nodes, and the resultedanswer is not empty.
For example, by groundingCauseOfDeath with LungCancer and Datetimewith 1960, we get the graph query in Figure 1(c).
Aquery template can render multiple groundings.Finally, we convert a graph query into a SPARQLquery and execute it using Virtuoso Open-Source 7to collect answers.
We further impose mutual exclu-sivity in SPARQL queries, that is, the entities on anytwo nodes in a graph query should be different.
Con-sider the example in Figure 2, which is asking forthe siblings of Natasha Obama.
Wihout mutual ex-clusivity, however, Natash Obama herself will alsobe included as an answer, which is not desired.5 Query RefinementSince graph queries are randomly generated, someof them may not correspond to an interesting ques-tion.
Next we study two query characteristics, re-dundancy and commonness, based on which we pro-vide mechanisms for automatic query refinement.565Figure 3: Query minimization example: (a) Graph query withredundant components.
(b) Graph query after minimization.Figure 4: Uncommon query example.
It is uncommon to askfor somebody?s great-great-grandparents.5.1 Query Redundancy and MinimizationSome components (nodes and edges) in a graphquery may not effectively impose any constraint onthe answer.
The query in Figure 3(a) is to ?findthe US president whose child is Natasha Obama,and Natasha Obama was born on 2001-06-10?.Intuitively, the bold-faced clause does not changethe answer of the question.
Correspondingly, thedateOfBirth edge and the date node are redun-dant.
As a comparison, removing any componentfrom the query in Figures 3(b) will change the an-swer.
Formally, given a knowledge base K, a com-ponent in a graph query q is redundant iff.
removingit does not change the answer JqKK.Redundancy can be desired or not.
In a question,redundant information may be inserted to reduceambiguity.
In Figure 3(a), if one uses ?Natasha?to refer to NatashaObama, there comes ambiguitysince it may be matched with many other entities.The additional information ?who was born on 2001-06-10?
then helps.
Next we describe an algorithm toremove redundancy from queries.
One can choose toeither only generate queries with no redundant com-ponent, or intentionally generate redundant queriesand test QA systems in presence of redundancy.We manage to generate minimal queries, forwhich there exists no sub-query having the same an-swer.
An important theorem, as we prove in Ap-pendix B, is the equivalency of minimality and non-redundancy: A query is minimal iff.
it has no redun-dant component.
This renders a simple algorithmfor query minimization, which directly detects andremoves the redundant components in a query.
Wefirst examine every edge (in an arbitrary order), andremove an edge if it is redundant.
Redundant nodeswill then become disconnected to the question nodeand are thus eliminated.
It is easy to prove that theproduced query (e.g., Figure 3(b)) is minimal, andhas the same answer as the original query.5.2 Commonness CheckingWe now quantify the commonness of graph queries.The benefits of this study are two-fold.
First, it pro-vides a refinement mechanism to reduce too rarequeries.
Second, commonness is itself an importantquestion characteristic.
It is interesting to examineits impact on question difficulty.
Consider the ex-ample in Figure 4, which asks for ?the great-great-grandparents of Ernest Solvay?.
It is minimal andlogically plausible.
Few users, however, are likelyto come up with it.
Ernest Solvay is famous for theSolvay Conferences, but few people outside the sci-ence community may know him.
Although Personand parents are common, asking for the great-great-grandparents is quite uncommon.A query is more common if users would morelikely come up with it.
We define the commonnessof a query q as its probability p(q) of being pickedamong all possible queries from a knowledge base.The problem then boils down to estimating p(q).It is hard, if not impossible, to exhaust the wholequery space.
We thus make the following simplifica-tion.
We break down query commonness by compo-nents, assuming mutual independence between com-ponents, and omit functions:p(q) =?i?Iqp(i)?
?c?Cqp(c)?
?r?Rqp(r), (1)where Iq, Cq,Rq are the multi-set of the individuals,classes, and relations in q, respectively.
Repeatingcomponents are thus accumulated (c.f.
Figure 4).We propose a data-driven method, using statisti-cal information from the Web, to estimate p(i), p(c),and p(r).
Other methods like domain-knowledgebased estimation are also applicable if available.
Westart with entity probability p(e) (excluding literalsfor now).
If users mention an entity more frequently,its probability of being observed in a question shouldbe higher.
We use a large entity linking dataset,FACC1 (Gabrilovich et al, 2013), which identifiesaround 10 billion mentions of Freebase entities inover 1 billion web documents.
The estimated link-ing precision and recall are 80-85% and 70-85%, re-566Figure 5: Question generation and paraphrasing.spectively.
Suppose entity e has n(e) mentions, thenp(e) = n(e)?e?
?E n(e?).
For a class c, probability p(c)is higher if it has more frequently mentioned enti-ties.
If we use e ?
c to indicate e is an instance ofc, then p(c) =?e?c n(e)?c??C?e?c?
n(e).
Estimating p(r) re-quires relation extraction from texts, which is hard.We make the following simplification: If (e1, r, e2)is a fact in the knowledge base, we increase n(r) by1 if e1 and e2 co-occur in a document.
This sufficesto distinguish common relations from uncommonones.
We then define p(r) = n(r)?r?
?R n(r?).
Finally,we use frequency information from the knowledgebase to smooth the probabilities, e.g., to avoid zeroprobabilities.
The probability of literals are solelydetermined by the frequency information from theknowledge base.
Refer to Appendix C for the re-sulted probability distributions.6 Natural Language ConversionIn order to ensure naturalness and diversity, we em-ploy human annotators to manually convert graphqueries into natural language questions.
We man-age to provide two levels of paraphrasing (Fig-ure 5).
Each query is sent to multiple annotators forsentence-level paraphrasing.
In addition, we use dif-ferent lexical forms of an entity mined from FACC1for entity-level paraphrasing.
We provide a rankedlist of common lexical forms and the correspondingfrequency for each topic entity.
For example, thelexical form list for UnitedStatesOfAmerica is?us?
(108M), ?united states?
(44M), ?usa?
(22M),etc.
Finally, graph queries are automatically trans-lated into SPARQL queries to collect answers.Natural language generation (NLG) (Wen et al,2015; Serban et al, 2016; Dus?ek and Jurc??
?c?ek, 2015)would be a good complement to our framework, thecombination of which can lead to a fully-automatedpipeline to generate QA datasets.
For example,Serban et al (2016) automatically convert Free-base triples into questions with a neural network.More sophisticated NLG techniques able to handlegraph queries involving multiple relations and vari-ous functions are an interesting future direction.7 ExperimentsWe have constructed a new QA dataset, namedGRAPHQUESTIONS, using the proposed frame-work, and tested several QA systems to show thatit enables fine-grained inspection of QA systems.7.1 Dataset ConstructionWe first randomly generated a set of minimal graphqueries, and removed the ones whose common-ness is below a certain threshold.
The remaininggraph queries were then screened by graduate stu-dents, and a canonical question was generated foreach query, with each being verified by at least twostudents.
We recruited 160 crowdsourcing work-ers from Amazon MTurk to generate sentence-levelparaphrases of the canonical questions.
Trivial para-phrases (e.g., ?which city?
vs. ?what city?)
weremanually removed to retain a high diversity in para-phrasing.
At most 3 entity-level paraphrases wereused for each sentence-level paraphrase.7.2 Dataset AnalysisGRAPHQUESTIONS contains 500 graph queries,2,460 sentence-level paraphrases, and 5,166 ques-tions2.
The dataset presents a high diversity andcovers a wide range of domains including People,Astronomy, Medicine, etc.
Specifically, it con-tains 148, 506, 596, 376 and 3,026 distinct domains,classes, relations, topic entities, and words, respec-tively.
We evenly split GRAPHQUESTIONS into atraining set and a testing set.
All the paraphrases ofthe same graph query are in the same set.While there are other question characteristicsderivable from graph query, we will focus on thefollowing ones: structure complexity, function, com-monness, paraphrasing, and answer cardinality.
We2For each query template, we only generate one graph query,but one can also generate multiple graph queries, and easily getthe corresponding questions by replacing the topic entities.
Thiswill significantly increase the total number of questions, and canbe helpful in training.567# of edges Function log10(p(q)) |A|1 2 3 none count super.
comp.
[?40, 30) [?30, 20) [?20, 10) [?10, 0) 1 > 1# of graph queries 321 144 35 350 61 42 47 60 135 283 22 332 168# of questions 3094 1648 424 3855 710 332 269 653 1477 2766 270 3487 1679Table 1: Characteristic statistics.
|A| is answer cardinality.
Refer to Appendix D for paraphrase and other fine-grained distributions.Question Domain Answer # of edges Function log10(p(q)) |A|Find terrorist organizations involved inSeptember 11 attacks.The September 11 attacks were carried out withthe involvement of what terrorist organizations?
Terrorism alQaeda 1 none -16.67 1Who did nine eleven?How many children of Eddard Stark were bornin Winterfell?Winterfell is the home of how many of EddardStark?s children?
FictionalUniverse 3 2 count -23.34 1What?s the number of Ned Stark?s childrenwhose birthplace is Winterfell?In which month does the average rainfall of NewYork City exceed 86 mm?Rainfall averages more than 86 mm in New YorkCity during which months?
Travel March, August.
.
.
3 comp.
-37.84 7List the calendar months when NYC averages inexcess of 86 millimeters of rain?Table 2: Example questions and characteristics.
Three sentence-level paraphrases are shown for each graph query, with the lastone also involving entity-level paraphrasing.
Topic entities are bold-faced.
More examples can be found in Appendix D.use the number of edges to quantify structure com-plexity, and limit it to at most 3.
Commonness islimited to log10(p(q)) ?
?40 (c.f.
Eq.
1).
Asshown in Section 7.4.2, such questions are alreadyvery hard for existing QA systems.
Nevertheless,the proposed framework can be used to generatequestions with different characteristic distributions.Some statistics are shown in Table 1 and more fine-grained statistics can be found in Appendix D.Several example questions are shown in Ta-ble 2.
Sentence-level paraphrasing requires to han-dle both commands (the first example) and ?Wh?questions, light verbs (?Who did nine eleven??
),and changes of syntactic structure (?The Septem-ber 11 attacks were carried out with the involve-ment of what terrorist organizations??).
Entity-level paraphrasing tests the capability of QA systemson abbreviation (?NYC?
for New York City),world knowledge (?Her Majesty the Queen?
forElizabethII), or even common typos (?Shaks-peare?
for WilliamShakespeare).
Numbers anddates are also common, e.g., ?Which computer oper-ating system was released on Sept. the 20th, 2008?
?We compare several QA datasets constructedfrom Freebase, shown in Table 3.
Datasets focus-ing on single-relation questions are of a larger scale,but are also of a significant lack in question charac-teristics.
Overall GRAPHQUESTIONS presents thehighest diversity in question characteristics.7.3 SetupWe evaluate three QA systems whose source codeis publicly available: SEMPRE (Berant et al, 2013),PARASEMPRE (Berant and Liang, 2014), and JA-CANA (Yao and Van Durme, 2014).
SEMPREand PARASEMPRE follow the semantic parsingparadigm.
SEMPRE conducts a bottom-up beam-based parsing on questions to find the best logicalform.
PARASEMPRE, in a reverse manner, enumer-ates a set of logical forms, generates a canonicalutterance for each logical form, and ranks logicalforms according to how well the canonical utter-ance paraphrases the input question.
In contrast, JA-CANA follows the information extraction paradigm,and builds a classifier to directly predict whether anindividual is the answer.
They all use Freebase.The main metric for answer quality is the aver-age F1 score, following Berant and Liang (2014).Because a question can have more than one an-swer, individual precision, recall, and F1 scores arefirst computed on each question and then averaged.When a system generates no response for a question,568Dataset # of Questions # of Multi-relation Function (count/super./comp.)
Commonness Paraphrase Multi-answerGRAPHQUESTIONS (this work) 5166 2072 710 / 332 / 269 + + +WEBQUESTIONSSP (Yih et al, 2016)1 4737 2075 2 / 168 / 334 - - +FREE917 (Cai and Yates, 2013) 917 229 185 / 0 / 0 - - +Serban et al (2016) 30M 0 0 / 0 / 0 - - -SIMPLEQUESTIONS (Bordes et al, 2015) 108K 0 0 / 0 / 0 - - -Table 3: Comparison of QA datasets constructed from Freebase.
GRAPHQUESTIONS is the richest in question characteristics.System F1 Time/sSEMPRE 10.80 56.19PARASEMPRE 12.79 18.43JACANA 5.08 2.01Table 4: Overall performance on GRAPHQUESTIONS.precision is 1, recall is 0, and F1 is 0.
Average run-time is used for efficiency.
Results are shown in per-centage.
Systems are trained on the training set us-ing the suggested configurations (Appendix E).
Weuse student?s t test at p = 0.05 for significance test.7.4 Results7.4.1 Overall EvaluationCompared with the scores on WEBQUESTIONS(30%-40%), the scores on GRAPHQUESTIONS arelower (Table 4).
This is because GRAPHQUES-TIONS contains questions over a broader range ofdifficulty levels.
For example, it is more diverse intopics (Appendix D); also the scores become muchcloser when excluding paraphrasing (Section 7.4.2).JACANA achieves a comparable F1 score withSEMPRE and PARASEMPRE on WEBQUES-TIONS (Yao et al, 2014).
On GRAPHQUESTIONS,however, SEMPRE and PARASEMPRE significantlyoutperform JACANA (both p < 0.0001).
Thefollowing experiments will give more insightsabout where the performance difference comesfrom.
On the other hand, JACANA is much faster,showing an advantage of information extraction.The semantic parsing systems spend a lot of time onexecuting SPARQL queries.
Bypassing SPARQLand directly working on the knowledge base may bea promising way to speed up semantic parsing onlarge knowledge bases (Yih et al, 2015).2WEBQUESTIONSSP is WEBQUESTIONS with manuallyannotated logical forms.
Only those with a full logical formare included (4737 / 5810).7.4.2 Fine-grained EvaluationWith explicitly specified question characteristics,we are able to further inspect QA systems.Structure Complexity.
We first break down systemperformance by structure.
Answer quality is in gen-eral sensitive to the complexity of question struc-ture: As the number of edges increases, F1 scoredecreases (Figure 6(a)).
The tested systems oftenfail to take into account auxiliary constraints in aquestion.
For example, for ?How many childrenof Ned Stark were born in Winterfell??
SEMPREfails to identify the constraint ?born in Winterfell?,so it also considers Ned Stark?s bastard son, JonSnow, as an answer, who was not born in Winter-fell.
Answering questions involving multiple rela-tions using large knowledge bases remain an openproblem.
The large size of knowledge bases pro-hibits exhaustive search, so smarter algorithms areneeded to efficiently prune the answer space.
Be-rant and Liang (2015) point out an interesting direc-tion, leveraging agenda-based parsing with imitationlearning for efficient search in the answer space.Function.
In terms of functions, while SEMPRE andPARASEMPRE perform well on count questions, allthe tested systems perform poorly on questions withsuperlatives or comparatives (Figure 6(b)).
JACANAhas trouble dealing with functions because it doesnot conduct quantitative analysis over the answerspace.
SEMPRE and PARASEMPRE do not generatelogical forms with superlatives and comparatives, sothey cannot answer such questions well.Commonness.
Not surprisingly, more commonquestions are in general easier to answer (Fig-ure 6(c)).
An interesting observation is that SEM-PRE?s performance gets worse on the most commonquestions.
The cause is likely rooted in how theQA systems construct their candidate answer sets.PARASEMPRE and JACANA exhaustively constructcandidate sets, while SEMPRE employs a bottom-upbeam search, making it more sensitive to the size of5691 2 300.050.10.150.2(a) # of edgesAverage F1SEMPREPARASEMPREJACANAnone count super.
comp.00.050.10.150.2(b) FunctionAverage F1SEMPREPARASEMPREJACANA?35 ?25 ?15 ?500.050.10.150.20.25(c) commonnessAverage F1SEMPREPARASEMPREJACANA1 3 5 7 9 11 13 15 1700.050.10.150.20.250.30.35(d) Rank of paraphrasesAverage F1SEMPREPARASEMPREJACANAFigure 6: Performance breakdown by (a) structure complexity, (b) function, (c) commonness, and (d) paraphrase.
Note that in (c)x = ?5 indicates the commonness range ?10 ?
log10(p(q)) < 0.the candidate answer space.
Common entities likeUnitedStatesOfAmerica are often featured bya high degree in knowledge bases (e.g., 1 millionneighboring entities), which dramatically increasesthe size of the candidate answer space.
During SEM-PRE?s iterative beam search, many correct logicalforms may have fallen off beam before getting intothe final candidate set.
We checked the percentage ofquestions for which the correct logical form is in thefinal candidate set, and found that it decreased from19.8% to 16.7% when commonness increased from-15 to -5, providing an evidence for the intuition.Paraphrasing.
It is critical for a system to toler-ate the wording varieties of users.
We make thefirst effort to evaluate QA systems on paraphras-ing.
For each system, we rank, in descending or-der, all the paraphrases derived from the same graphquery by their F1 score achieved by the system, andthen compute the average F1 score of each rank.
InFigure 6(d), the decreasing rate of a curve thus de-scribes a system?s robustness to paraphrasing; thehigher, the less robust.
All the systems achieve areasonable score on the top-1 paraphrases, i.e., whena system can choose the paraphrase it can best an-swer.
The F1 scores drop quickly in general.
Onthe fourth-ranked paraphrases, the F1 score of SEM-PRE, PARASEMPRE, and JACANA are respectivelyonly 37.65%, 53.2%, and 36.2% of their score onthe top-1 paraphrases.
Leveraging paraphrasing inits model, PARASEMPRE does seem to be more ro-bust.
The results show that how to handle para-phrased questions is still a challenging problem.Answer Cardinality.
SEMPRE and JACANA get asignificantly lower F1 score (both p < 0.0001)on multi-answer questions (Table 5), mainly com-ing from a decrease on recall.
The decrease ofPARASEMPRE is not significant (p=0.29).
The par-ticularly significant decrease of JACANA demon-System |A| Prec.
Rec.
F1SEMPRE 1 59.81 16.11 12.68> 1 62.38 9.17 6.78PARASEMPRE 1 17.42 17.58 13.25> 1 19.65 17.23 11.82JACANA 1 14.77 6.56 6.56> 1 11.80 1.43 1.98Table 5: Performance breakdown by answer cardinality |A|.strates the difficulty of training a classifier that canpredict all of the answers correctly; semantic pars-ing is more robust in this case.
The precision ofSEMPRE is high because it generates no response formany questions.
Note that under the current defini-tion, the average F1 score is not the harmonic meanof the average precision and recall (c.f.
Section 7.3).8 ConclusionWe proposed a framework to generate characteristic-rich questions for question answering (QA) evalua-tion.
Using the proposed framework, we constructeda new and challenging QA dataset, and extensivelyevaluated several QA systems.
The findings pointout an array of issues that future QA research mayneed to solve.9 AcknowledgementsThis research was sponsored in part by the ArmyResearch Laboratory under cooperative agreementsW911NF09-2-0053, NSF IIS 0954125, and NSFIIS 1528175.
The views and conclusions containedherein are those of the authors and should not be in-terpreted as representing the official policies, eitherexpressed or implied, of the Army Research Lab-oratory or the U.S. Government.
The U.S. Gov-ernment is authorized to reproduce and distributereprints for Government purposes notwithstandingany copyright notice herein.570ReferencesAnne Aula, Rehan M. Khan, and Zhiwei Guan.
2010.How does search behavior change as search becomesmore difficult?
In Proceedings of CHI.Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.2014.
Knowledge-based question answering as ma-chine translation.
In Proceedings of ACL.Michael Bendersky and W. Bruce Croft.
2009.
Analysisof long queries in a large scale search log.
In Proceed-ings of the 2009 workshop on Web Search Click Data.Jonathan Berant and Percy Liang.
2014.
Semantic pars-ing via paraphrasing.
In Proceedings of ACL.Jonathan Berant and Percy Liang.
2015.
Imitation learn-ing of agenda-based semantic parsers.
Transactions ofthe Association for Computational Linguistics, 3:545?558.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of EMNLP.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A collabo-ratively created graph database for structuring humanknowledge.
In Proceedings of SIGMOD.Antoine Bordes, Nicolas Usunier, Sumit Chopra, andJason Weston.
2015.
Large-scale simple questionanswering with memory networks.
arXiv preprintarXiv:1506.02075.John Burger, Claire Cardie, Vinay Chaudhri, RobertGaizauskas, Sanda Harabagiu, David Israel, ChristianJacquemin, Chin-Yew Lin, Steve Maiorano, GeorgeMiller, et al 2001.
Issues, tasks and programstructures to roadmap research in question & answer-ing (q&a).
In Document Understanding ConferencesRoadmapping Documents, pages 1?35.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexicon ex-tension.
In Proceedings of ACL.Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2015.
Ques-tion answering over Freebase with multi-column con-volutional neural networks.
In Proceedings of ACL.Ondr?ej Dus?ek and Filip Jurc???c?ek.
2015.
Training a nat-ural language generator from unaligned data.
In Pro-ceedings of ACL.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated and ex-tracted knowledge bases.
In Proceedings of KDD.Evgeniy Gabrilovich, Michael Ringgaard, and AmarnagSubramanya.
2013.
FACC1: Freebase annotationof ClueWeb corpora, version 1 (release date 2013-06-26, format version 1, correction level 0).
http://lemurproject.org/clueweb09/ and http://lemurproject.org/clueweb12/.Lynette Hirschman and Robert Gaizauskas.
2001.
Natu-ral language question answering: the view from here.natural language engineering, 7(4):275?300.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers with on-the-fly ontology matching.
In Proceedings of EMNLP.Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,Dimitris Kontokostas, Pablo N Mendes, SebastianHellmann, Mohamed Morsey, Patrick van Kleef,So?ren Auer, et al 2014.
DBpedia - a large-scale, mul-tilingual knowledge base extracted from wikipedia.Semantic Web Journal, 6(2):167?195.Percy Liang.
2013.
Lambda dependency-based compo-sitional semantics.
arXiv preprint arXiv:1309.4408.Vanessa Lopez, Christina Unger, Philipp Cimiano, andEnrico Motta.
2013.
Evaluating question answeringover linked data.
Web Semantics: Science, Servicesand Agents on the World Wide Web, 21:3?13.Siva Reddy, Mirella Lapata, and Mark Steedman.
2014.Large-scale semantic parsing without question-answerpairs.
Transactions of the Association for Computa-tional Linguistics, 2:377?392.Iulian Vlad Serban, Alberto Garc?
?a-Dura?n, Caglar Gul-cehre, Sungjin Ahn, Sarath Chandar, Aaron Courville,and Yoshua Bengio.
2016.
Generating factoid ques-tions with recurrent neural networks: The 30m factoidquestion-answer corpus.
In Proceedings of ACL.Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai,Jingjing Liu, and Ming-Wei Chang.
2015.
Open do-main question answering via semantic enrichment.
InProceedings of WWW.Ellen M Voorhees and Dawn M Tice.
2000.
Building aquestion answering test collection.
In Proceedings ofSIGIR.Yushi Wang, Jonathan Berant, and Percy Liang.
2015.Building a semantic parser overnight.
In Proceedingsof ACL.Tsung-Hsien Wen, Milica Gas?ic?, Nikola Mrks?ic?, Pei-HaoSu, David Vandyke, and Steve Young.
2015.
Semanti-cally conditioned LSTM-based natural language gen-eration for spoken dialogue systems.
In Proceedingsof EMNLP.Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,Maya Ramanath, Volker Tresp, and Gerhard Weikum.2012.
Deep answers for naturally asked questions onthe web of data.
In Proceedings of WWW.Xuchen Yao and Benjamin Van Durme.
2014.
Informa-tion extraction over structured data: Question answer-ing with Freebase.
In Proceedings of ACL.Xuchen Yao, Jonathan Berant, and Benjamin Van Durme.2014.
Freebase QA: Information extraction or seman-tic parsing?
In Proceedings of ACL.Xuchen Yao.
2015.
Lean question answering over Free-base from scratch.
In Proceedings of NAACL.571Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jian-feng Gao.
2015.
Semantic parsing via staged querygraph generation: Question answering with knowl-edge base.
In Proceedings of ACL.Wen-tau Yih, Matthew Richardson, Christopher Meek,Ming-Wei Chang, and Jina Suh.
2016.
The value ofsemantic parse labeling for knowledge base questionanswering.
In Proceedings of ACL.Lei Zou, Ruizhe Huang, Haixun Wang, Jeffrey Xu Yu,Wenqiang He, and Dongyan Zhao.
2014.
Natural lan-guage question answering over rdf: a graph data drivenapproach.
In Proceedings of SIGMOD.572
