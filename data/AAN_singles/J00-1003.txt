Practical Experiments with RegularApproximation of Context-Free LanguagesMark- Jan  Nederhof "German Research CenterIntelligencefor ArtificialSeveral methods are discussed that construct afinite automaton given a context-free grammar,including both methods that lead to subsets and those that lead to supersets of the originalcontext-free language.
Some of these methods of regular approximation are new, and some othersare presented here in a more refined form with respect to existing literature.
Practical experimentswith the different methods of regular approximation are performed for spoken-language input:hypotheses from a speech recognizer are filtered through afinite automaton.1.
IntroductionSeveral methods of regular approximation of context-free languages have been pro-posed in the literature.
For some, the regular language is a superset of the context-freelanguage, and for others it is a subset.
We have implemented a large number of meth-ods, and where necessary, refined them with an analysis of the grammar.
We alsopropose a number of new methods.The analysis of the grammar is based on a sufficient condition for context-freegrammars to generate regular languages.
For an arbitrary grammar, this analysis iden-tifies sets of rules that need to be processed in a special way in order to obtain a regularlanguage.
The nature of this processing differs for the respective approximation meth-ods.
For other parts of the grammar, no special treatment is needed and the grammarrules are translated to the states and transitions of a finite automaton without affectingthe language.Few of the published articles on regular approximation have discussed the appli-cation in practice.
In particular, little attention has been given to the following twoquestions: First, what happens when a context-free grammar grows in size?
What isthen the increase of the sizes of the intermediate r sults and the obtained minimal de-terministic automaton?
Second, how "precise" are the approximations?
That is, howmuch larger than the original context-free language is the language obtained by asuperset approximation, and how much smaller is the language obtained by a subsetapproximation?
(How we measure the "sizes" of languages in a practical setting willbecome clear in what follows.
)Some considerations with regard to theoretical upper bounds on the sizes of theintermediate r sults and the finite automata have already been discussed in Nederhof(1997).
In this article we will try to answer the above two questions in a practical set-ring, using practical linguistic grammars and sentences taken from a spoken-languagecorpus.?
DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany.
E-mail: nederhof@dfki.de?
2000 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 1The structure of this paper is as follows: In Section 2 we recall some standarddefinitions from language theory.
Section 3 investigates a sufficient condition for acontext-free grammar to generate a regular language.
We also present he constructionof a finite automaton from such a grammar.
In Section 4, we discuss several meth-ods to approximate the language generated by a grammar if the sufficient conditionmentioned above is not satisfied.
These methods can be enhanced by a grammar trans-formation presented in Section 5.
Section 6 compares the respective methods, whichleads to conclusions in Section 7.2.
PreliminariesThroughout this paper we use standard formal language notation (see, for example,Harrison \[1978\]).
In this section we review some basic definitions.A context-free grammar G is a 4-tuple (G ,N,P ,S) ,  where G and N are two finitedisjoint sets of terminals and nonterminals, respectively, S E N is the start symbol, andP is a finite set of rules.
Each rule has the form A ~ ~ with A E N and ~ E V*, whereV denotes N U ~.
The relation ~ on N x V* is extended to a relation on V* x V* asusual.
The transitive and reflexive closure of 4 is denoted by 4* .The language generated by G is given by the set {w E G* I S 4"  w}.
By definition,such a set is a context-free language.
By reduction of a grammar we mean the elimi-nation from P of all rules A 4 ,7  such that S --+* c~Afl --* a"/fl 7"  w does not hold forany~, f lEV*andwEG* .We generally use symbols A, B, C .
.
.
.
to range over N, symbols a, b, c , .
.
.
to rangeover ~, symbols X, Y, Z to range over V, symbols a, fl,"7 .
.
.
.
to range over V* andsymbols v, w, x .
.
.
.
to range over G* We write ?
to denote the empty string.A rule of the form A --, B is called a unit rule.A (nondeterministic) finite automaton .T is a 5-tuple (K, G, A, s, F), where K is afinite set of states, of which s is the initial state and those in F c K are the final states,is the input alphabet, and the transition relation A is a finite subset of K x Z* x K.We define a configuration to be an element of K x G*.
We define the binary relationt- between configurations as: (q, vw) F- (q', w) if and only if (q, v, q') E A.
The transitiveand reflexive closure of ~- is denoted by F-*.Some input v is recognized if (s, v) t-* (q, c), for some q E F. The language acceptedby .T is defined to be the set of all strings v that are recognized.
By definition, alanguage accepted by a finite automaton is called a regular language.3.
Finite Automata in the Absence of Self-EmbeddingWe define a spine in a parse tree to be a path that runs from the root down to someleaf.
Our main interest in spines lies in the sequences of grammar symbols at nodesbordering on spines.A simple example is the set of parse trees such as the one in Figure 1, for agrammar of palindromes.
It is intuitively clear that the language is not regular: thegrammar symbols to the left of the spine from the root to E "communicate" with thoseto the right of the spine.
More precisely, the prefix of the input up to the point whereit meets the final node c of the spine determines the suffix after that point, in sucha way that an unbounded quantity of symbols from the prefix need to be taken intoaccount.A formal explanation for why the grammar may not generate a regular languagerelies on the following definition (Chomsky 1959b):18Nederhof Experiments with Regular ApproximationS--' ,a S aS-->b S bS---~ ~Sa S aY\b S bFigure 1Grammar of palindromes, and a parse tree.DefinitionA grammar is sel f -embedding if there is some A E N such that A --+* c~Afl, for somea?eandf l?e .If a grammar is not self-embedding, this means that when a section of a spine ina parse tree repeats itself, then either no grammar symbols occur to the left of thatsection of the spine, or no grammar symbols occur to the right.
This prevents the"unbounded communication" between the two sides of the spine exemplified by thepalindrome grammar.We now prove that grammars that are not self-embedding generate regular lan-guages.
For an arbitrary grammar, we define the set of reeursive nonterminals as:BN = {A E N I  Ag\]}mWe determine the partition N" of N consisting of subsets N1, N2 .
.
.
.
, Nk, for some k > 0,of mutual ly recursive nonterminals:H = {N1,N2 .
.
.
.
,Nk}NIUN2U.
.
.UNk=NVi\[Ni 7L O\]Vi, j\[i  j =~ Ni N Nj = 0\]and for all A, B E N:3i\[A E Ni AB  @ Nil - ~oQ, fll, O~2,fl2\[a ---~* alBfll AB  ---+* c?2Afl2\],We now define the function recursive from N" to the set {left, right, self, cyclic}.
Forl< iKk :recursive(Ni) -- left, if ~LeftGenerating(Ni)= right, if LeftGenerating(Ni)-- self, if LeftGenerating(Ni)= cyclic, if -,LeftGenerating(Ni)/x RightGenerating(Ni)/x ~RightGenerating(Ni)/x RightGenerating(Ni)/x ~RightGenerating( Ni )whereLeftGenerating(Ni) = 3(A --* aBfl) E P\[A E Ni A B E Ni /X ~ 7~ e\]RightGenerating(Ni) = 3(A --* aBfl) E P\[A E Ni /x B E Ni /~ fl  ?\]19Computational Linguistics Volume 26, Number 1When recursive(Ni) = left, Ni consists of only left-recursive nonterminals, which doesnot mean it cannot also contain right-recursive nonterminals, but in that case rightrecursion amounts to application of unit rules.
When recursive(Ni) = cyclic, it is onlysuch unit rules that take part in the recursion.That recursive(Ni) = self, for some i, is a sufficient and necessary condition for thegrammar to be self-embedding.
Therefore, we have to prove that if recursive(Ni) E{left, right, cyclic}, for all i, then the grammar generates a regular language.
Our proofdiffers from an existing proof (Chomsky 1959a) in that it is fully constructive: Fig-ure 2 presents an algorithm for creating a finite automaton that accepts the languagegenerated by the grammar.The process is initiated at the start symbol, and from there the process descendsthe grammar in all ways until terminals are encountered, and then transitions arecreated labeled with those terminals.
Descending the grammar is straightforward inthe case of rules of which the left-hand side is not a recursive nonterminal: the sub-automata found recursively for members in the right-hand side will be connected.In the case of recursive nonterminals, the process depends on whether the nontermi-nals in the corresponding set from H are mutually left-recursive or right-recursive;if they are both, which means they are cyclic, then either subprocess can be ap-plied; in the code in Figure 2 cyclic and right-recursive subsets Ni are treated uni-formly.We discuss the case in which the nonterminals are left-recursive.
One new state iscreated for each nonterminal in the set.
The transitions that are created for terminalsand nonterminals not in Ni are connected in a way that is reminiscent of the con-struction of left-corner parsers (Rosenkrantz and Lewis 1970), and specifically of oneconstruction that focuses on sets of mutually recursive nonterminals (Nederhof 1994,Section 5.8).An example is given in Figure 3.
Four states have been labeled according to thenames they are given in procedure make~fa.
There are two states that are labeled qB.This can be explained by the fact that nonterminal B can be reached by descendingthe grammar from S in two essentially distinct ways.The code in Figure 2 differs from the actual implementation in that sometimes, for anonterminal, a separate finite automaton is constructed, namely, for those nonterminalsthat occur as A in the code.
A transition in such a subautomaton may be labeled byanother nonterminal B, which then represents he subautomaton corresponding to B.The resulting representation is similar to extended context-free grammars (Purdomand Brown 1981), with the exception that in our case recursion cannot occur, by virtueof the construction.The representation forthe running example is indicated by Figure 4, which showstwo subautomata, l beled S and B.
The one labeled S is the automaton on the top level,and contains two transitions labeled B, which refer to the other subautomaton.
Notethat this representation is more compact han that of Figure 3, since the transitionsthat are involved in representing the sublanguage of strings generated by nonterminalB are included only once.The compact representation consisting of subautomata c n be turned into a sin-gle finite automaton by substituting subautomata A for transitions labeled A in otherautomata.
This comes down to regular substitution i the sense of Berstel (1979).
Theadvantage of this way of obtaining a finite automaton over a direct construction of anondeterministic automaton is that subautomata may be determinized and minimizedbefore they are substituted into larger subautomata.
Since in many cases determinizedand minimized automata re much smaller, this process avoids much of the combina-20Nederhof Experiments with Regular Approximationlet K = O, A = O, s = fresh_state, f = fresh_state, F = {f};make_fa( s, S, f) .procedure  makeffa(qo, a, ql):i f  a= ethen  let A = A U {(q0,e, ql)}elsei f  a = a, some a E ,Uthen  let A = A U {(q0, a, ql)}elsei f  a = Xfl, some X E V, fl C V* such  that  IflI > 0then  let q = fresh_state;makeffa(qo, X, q);makeffa( q, t ,  ql )else let A = a; (* a must consist of a single nonterminal *)i f  there  ex ists  i such  that  A C Nithen  for  each  B E Ni do let qB = fresh_state nd;if recursive(Ni) = leftthen  for  each  (C -+ X I ' .
'Xm)  E P such  that  CENi  AX1, .
.
.
,Xm~Nido make_fa(qo, XI  " .
Xm, qc )end;for  each  (C --+ DX1 ... X,~) C P such  thatC,D ~ Ni A X1 , .
.
.
,Xm ~ Nido make ffa( qD , X I " " X,~ , qc )end;let A = A U {(qA, e, ql)}else (* recursive(g,) C {right, cyclic} *)for  each  (C -+ X1 .
.
.Xm)  E P such  that  CENi  A X1 , .
.
.
,Xm~Nido make_fa(qc, X1 .
.
.
Xm, ql)end;for  each  (C --~ XI ".. XmD) E P such  thatC, D E Ni AX I , .
.
.
,Xm ~ Nido makc_fa(qc, XI ".. Xm, qD)end;let A = A U {(qo, e, qa)}endelse for  each  (A -+ fl) C P do make_fa(qo,fl, ql) end (* A is not recursive *)endendend.p rocedure  fresh_state():create some object q such that q ~ K;let K=KU{q};re turn  qend.Figure 2Transformation from a grammar G = (E, N,P, S) that is not self-embedding into an equivalentfinite automaton 3 v = (K, E, A, s, F).21Computational Linguistics Volume 26, Number 1S --* AaA --* SBA ~ BbB --* BcB ---* dcqBFigure 3N = {S,A,B}\]kf : {N1, N2}N1 = {S,A} recursive(N1) = leftN2 -- {B} recursive(N2) = left__qA a dApplication of the code from Figure 2 on a small grammar.SFigure 4B.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 ciiiI, , .
d ~ (~) ,f w  - w  = iqB IThe automaton from Figure 3 in a compact representation.torial explosion that takes place upon naive construction of a single nondeterministicfinite automaton.
1Assume we have a list of subautomata A1 .
.
.
.
.
Am that is ordered from lower-levelto higher-level automata; i.e., if an automaton Ap occurs as the label of a transitionof automaton Aq, then p < q; Am must be the start symbol  S. This order is a naturalresult of the way that subautomata are constructed uring our depth-first raversal ofthe grammar, which is actually postorder in the sense that a subautomaton is outputafter all subautomata occurring at its transitions have been output.Our implementat ion constructs a minimal deterministic automaton by repeatingthe following for p = 1 , .
.
.
,m:..Make a copy of Ap.
Determinize and minimize the copy.
If it has fewertransitions labeled by nonterminals than the original, then replace Ap byits copy.Replace each transition in Ap of the form (q, Ar, q') by (a copy of)automaton Ar in a straightforward way.
This means that new e-transitionsconnect q to the start state of Ar and the final states of Ar to qt.1 The representation in Mohri and Pereira (1998) is even more compact than ours for grammars that arenot self-embedding.
However, in this paper we use our representation as an intermediate r sult inapproximating an unrestricted context-free grammar, with the final objective of obtaining a singleminimal deterministic automaton.
For this purpose, Mohri and Pereira's representation ffers littleadvantage.22Nederhof Experiments with Regular Approximation3.
Again determinize and minimize Ap and store it for later reference.The automaton obtained for Am after step 3 is the desired result.4.
Methods of Regular ApproximationThis section describes a number of methods for approximating a context-free gram-mar by means of a finite automaton.
Some published methods did not mention self-embedding explicitly as the source of nonregularity for the language, and suggestedthat approximations should be applied globally for the complete grammar.
Wherethis is the case, we adapt the method so that it is more selective and deals withself-embedding locally.The approximations are integrated into the construction of the finite automatonfrom the grammar, which was described in the previous ection.
A separate incarnationof the approximation process is activated upon finding a nonterminal A such thatA E Ni and recursive(Ni) = self, for some i.
This incarnation then only pertains tothe set of rules of the form B --* c~, where B E Ni.
In other words, nonterminals notin Ni are treated by this incarnation of the approximation process as if they wereterminals.4.1 Superset Approximation Based on RTNsThe following approximation was proposed in Nederhof (1997).
The presentationhere, however, differs substantially from the earlier publication, which treated the ap-proximation process entirely on the level of context-free grammars: a self-embeddinggrammar was transformed in such a way that it was no longer self-embedding.
Afinite automaton was then obtained from the grammar by the algorithm discussedabove.The presentation here is based on recursive transition networks (RTNs) (Woods1970).
We can see a context-free grammar as an RTN as follows: We introduce twostates qA and q~ for each nonterminal A, and m + 1 states q0 .
.
.
.
.
qm for each ruleA --* X1 .. ?
Xm.
The  states for a rule A ~ X 1 .
.
.
X m are connected with each other andto the states for the left-hand side A by one transition (qA, c, q0), a transition (qi-1, Xi, qi)for each i such that 1 < i < m, and one transition (qm, e,q~A).
(Actually, some epsilontransitions are avoided in our implementation, but we will not be concerned with suchoptimizations here.
)In this way, we obtain a finite automaton with initial state qA and final state q~ foreach nonterminal A and its defining rules A --* X1 ?
?
?
Xm.
This automaton can be seenas one component of the RTN.
The complete RTN is obtained by the collection of allsuch finite automata for different nonterminals.An approximation now results if we join all the components in one big automaton,and if we approximate the usual mechanism of recursion by replacing each transition(q, A, q') by two transitions (q, c, qA) and (q~, e, q').
The construction is illustrated inFigure 5.In terms of the original grammar, this approximation can be informally explainedas follows: Suppose we have three rules B --* c~Afl, B I ~ c~IAfl ~, and A ~ % Top-down,left-to-right parsing would proceed, for example, by recognizing a in the first rule;it would then descend into rule A ~ % and recognize "y; it would then return tothe first rule and subsequently process ft.
In the approximation, however, the finiteautomaton "forgets" which rule it came from when it starts to recognize % so that itmay subsequently recognize fl' in the second rule.23Computational Linguistics Volume 26, Number 1(b)a B b(a)d A eqB Y '~ ; i  = i ~  'B A---~ a B b >t~ qA---~c A " ' "~"~ fB- - -~dA eB--~ f(c)a b~ ~ t /Figure 5Application of the RTN method for the grammar in (a).
The RTN is given in (b), and (c)presents the approximating finite automaton.
We assume A is the start symbol and thereforeqA becomes the initial state and q~ becomes the final state in the approximating automaton.For the sake of presentational convenience, the above describes a constructionworking on the complete grammar.
However, our implementation applies the con-struction separately for each nonterminal in a set Ni such that recursive(Ni) = self,which leads to a separate subautomaton of the compact representation (Section 3).See Nederhof (1998) for a variant of this approximation that constructs finite trans-ducers rather than finite automata.We have further implemented a parameterized version of the RTN approximation.A state of the nondeterministic automaton is now also associated to a list H of lengthIHI strictly smaller than a number d, which is the parameter to the method.
This listrepresents a history of rule positions that were encountered in the computation leadingto the present state.More precisely, we define an item to be an object of the form \[A ~ a ?
fl\],where A ~ aft is a rule from the grammar.
These are the same objects as the "dot-ted" productions of Earley (1970).
The dot indicates a position in the right-handside.The unparameterized RTN method had one state qI for each i tem/,  and two statesqA and q~ for each nonterminal A.
The parameterized RTN method has one state qrHfor each item I and each list of items H that represents a valid history for reachingI, and two states qaH and q~H for each nonterminal A and each list of items H thatrepresents a valid history for reaching A.
Such a valid history is defined to be a list24Nederhof Experiments with Regular ApproximationH with 0 < \[HI < d that represents a series of positions in rules that could have beeninvoked before reaching I or A, respectively.
More precisely, if we set H =/1 .. .
In, theneach Im (1 < m < n) should be of the form \[Am ~ olin ?
Bmflm\] and for 1 < m < n weshould have Am -- Bm+l.
Furthermore, for a state qiH with I = \[A --* a ?
fl\] we demandA = B1 if n > 0.
For a state qAH we demand A -- B1 if n > 0.
(Strictly speaking, statesqAH and qrH, with \[HI < d - 1 and I = \[A --+ a ?
fl\], will only be needed if AIH \] is thestart symbol in the case IH\[ > 0, or if A is the start symbol in the case H = c.)The transitions of the automaton that pertain to terminals in right-hand sidesof rules are very similar to those in the case of the unparameterized method: For astate qIH with I of the form \[A ~ a ?
aft\], we create a transition (q~H, a, qi,H), withI' = \[A ~ aa ?
fl\].Similarly, we create epsilon transitions that connect left-hand sides and right-handsides of rules: For each state qAa there is a transition (qAH, e, qIH) for each item I =\[A --* ?
a\], for some a, and for each state of the form qI,u, with I' = \[A ~ a ?\], thereis a transition (qFa, c, q~H).For transitions that pertain to nonterminals in the right-hand sides of rules, weneed to manipulate the histories.
For a state qIH with I of the form \[A ~ a ?
Bfl\], wecreate two epsilon transitions.
One is (qIH, c, qBn,), where H' is defined to be IH  if\[IH\[ < d, and to be the first d - 1 items of IH ,  otherwise.
Informally, we extend thehistory by the item I representing the rule position that we have just come from, butthe oldest information in the history is discarded if the history becomes too long.
Thesecond transition is (q'BH,, ~, q~'H), with I' = \[A --* aB ?
fl\].If the start symbol is S, the initial state is qs and the final state is q~ (after thesymbol S in the subscripts we find empty lists of items).
Note that the parameterizedmethod with d -- 1 concurs with the unparameterized method, since the lists of itemsthen remain empty.An example with parameter d -- 2 is given in Figure 6.
For the unparameterizedmethod, each I = \[A --* a ?
fl\] corresponded to one state (Figure 5).
Since reaching Acan have three different histories of length shorter than 2 (the empty history, since A isthe start symbol; the history of coming from the rule position given by item \[A -~ c ?
A\];and the history of coming from the rule position given by item \[B ~ d ?
Ae\]), in Figure 6we now have three states of the form qI~ for each I -- \[A ~ a ?
fl\], as well as threestates of the form qA~r and q~H"The higher we choose d, the more precise the approximation is, since the historiesallow the automaton to simulate part of the mechanism of recursion from the originalgrammar, and the maximum length of the histories corresponds to the number oflevels of recursion that can be simulated accurately.4.2 Refinement of RTN Superset ApproximationWe rephrase the method of Grimley-Evans (1997) as follows: First, we construct heapproximating finite automaton according to the unparameterized RTN method above.Then an additional mechanism is introduced that ensures for each rule A --~ X1 ?
.. Xmseparately that the list of visits to the states qo,.
.
?
?
qm satisfies ome reasonable criteria:a visit to qi, with 0 < i < m, should be followed by one to qi+l or q0.
The latter optionamounts to a nested incarnation of the rule.
There is a complementary condition forwhat should precede a visit to qi, with 0 < i < m.Since only pairs of consecutive visits to states from the set {q0 .
.
.
.
.
qm} are consid-ered, finite-state techniques suffice to implement such conditions.
This can be realizedby attaching histories to the states as in the case of the parameterized RTN methodabove, but now each history is a set rather than a list, and can contain at most oneitem \[A --* a ?
fl\] for each rule A ---* aft.
As reported by Grimley-Evans (1997) and con-25Computational Linguistics Volume 26, Number 1A~a B bA~c AB---,'d A eB--->fFigure 6ac/aH = \[A----> c .A l  qA~_  .
.
.
.
.
g ~',x I I  , a ,,H= \[B-->d.A el qA e',,b, Ebi ,, ,, .
.
.
.b ",,'" \ -qA .d " ,,, , Z e _qB Q~___ H = \[A --~ a .B  b\] - -  5 .
.
"'L qBHApplication of the parameterized RTN method with d = 2.
We again assume A is the startsymbol.
States qm have not been labeled in order to avoid cluttering the picture.f irmed by our own experiments, the nondeterministic finite automata resulting fromthis method may be quite large, even for small grammars.
The explanation is that thenumber of such histories is exponential in the number of rules.We have refined the method with respect o the original publication by applyingthe construction separately for each nonterminal in a set Ni such that recursive(Ni) =self.4.3 Subset Approximation by Transforming the GrammarPutting restrictions on spines is another way to obtain a regular language.
Severalmethods can be defined.
The first method we present investigates spines in a verydetailed way.
It eliminates from the language only those sentences for which a sub-derivation is required of the form B --~* aBfl, for some a ~ ?
and fl ~ e. The motivationis that such sentences do not occur frequently in practice, since these subderivationsmake them difficult for people to comprehend (Resnik 1992).
Their exclusion willtherefore not lead to much loss of coverage of typical sentences, especially for simpleapplication domains.We express the method in terms of a grammar transformation i Figure 7.
Theeffect of this transformation is that a nonterminal A is tagged with a set of pairs(B, Q), where B is a nonterminal occurring higher in the spine; for any given B, atmost one such pair (B, Q) can be contained in the set.
The set Q may contain theelement l to indicate that something to the left of the part of the spine from B to A26Nederhof Experiments with Regular ApproximationWe are given a grammar G = (E,N, P, S).
The following is to be performed for eachset Ni EAf  such that recursive(Ni) = self.. For each A E Ni and each F E 2 (Nix2~l''}), add the following nonterminalto N.?
A F .2.
For each A E Ni, add the following rule to P.?
A---~A 0.. For each (A --* o~0A1o~1A2... C~m-lAmCrm) E P such that A, A1 .
.
.
.
,Am E Niand no symbols from c~0 .
.
.
.
, am are members of Ni, and each F such that(A, (l, r}) ~ F, add the following rule to P.a F F1 Fm o~0A 1 oq.
.
.
A m O~m, where, for 1 G j _< m,- -  F j= {(B, QUC~U~F) I (B,Q) E F'};F' = FU {(A, 0)} if -~3Q\[(A,Q) E F\], and F' = Fotherwise;- -   = 0 if c~0AlC~l...Aj-I~j-1 = c, and ~ = {l} otherwise;- -  QJr = 0 if o/.jaj+lOLj+l...AmOL m = ?,  and QJr = {r}otherwise.4.
Remove from P the old rules of the form A --* c~, where A E Ni.5.
Reduce the grammar.Figure 7Subset approximation by transforming the grammar.was generated.
Similarly, r E Q indicates that something to the right was generated.
IfQ = {l, r}, then we have obtained a derivation B --** c~Afl, for some c~ ~ c and fl ~ ~,and further occurrences of B below A should be blocked in order to avoid a derivationwith self-embedding.An example is given in Figure 8.
The original grammar is implicit in the depictedparse tree on the left, and contains at least the rules S --+ A a, A --, b B, B -* C, andC --* S. This grammar is self-embedding, since we have a subderivation S --~* bSa.We explain how FB is obtained from FA in the rule A ~ --* b B r'.
We first constructF' = {(S, {r}), (A, 0)} from FA = {(S, (r})} by adding (A, 0), since no other pair of theform (A, Q) was already present.
To the left of the occurrence of B in the original ruleA --* b B we find a nonempty string b.
This means that we have to add l to all secondcomponents of pairs in F', which gives us FB = {(S, (l, r}), (A, {l})}.In the transformed grammar, the lower occurrence of S in the tree is tagged withthe set {(S, {I, r}), (A, {l}), (B, 0), (C, 0)}.
The meaning is that higher up in the spine, wewill find the nonterminals S, A, B, and C. The pair (A, (1}) indicates that since we sawA on the spine, something to the left has been generated, namely, b.
The pair (B, 0)indicates that nothing either to the left or to the right has been generated since wesaw B.
The pair (S, {1, r}) indicates that both to the left and to the right something hasbeen generated (namely, b on the left and a on the right).
Since this indicates that an27Computational Linguistics Volume 26, Number 1s(a) s (b) s Fs Fs  =A a a / \  / \FB b B b B 'BIC ~F c Fc  =5 '  ' - -  sX0{(S, {l, r}), (A, {/})}{(S, {l, r}), (A, {/}), (B, 0)}{(S, {l, r}), (A, {/}), (B, 0), (C, 0)}Figure 8A parse tree m a self-embedding grammar (a), and the corresponding parse tree in thetransformed grammar (b), for the transformation from Figure 7.
For the moment we ignorestep 5 of Figure 7, i.e., reduction of the transformed grammar.offending subderivation S --** c~Sfl has been found, further completion of the parsetree is blocked: the transformed grammar will not have any rules with left-hand sideS {(S'{I'r})'(A'{I})'(B'O)'(C'O)}.
In fact, after the grammar is reduced, any parse tree that isconstructed can no longer even contain a node labeled by S {(s'U'r})'(a'{O)'(B'?)'(c'?
)}, orany nodes with labels of the form A r such that (A, {l,r}) c F.One could generalize this approximation i such a way that not all self-embeddingis blocked, but only self-embedding occurring, say, twice in a row, in the sense of asubderivation of the form A --** a lA f l l  --+* oqol2Afl2fll.
We will not do so here, becausealready for the basic case above, the transformed grammar can be huge due to thehigh number of nonterminals of the form A F that may result; the number of suchnonterminals i exponential in the size of Ni.We therefore present, in Figure 9, an alternative approximation that has a lowercomplexity.
By parameter d, it restricts the number of rules along a spine that maygenerate something to the left and to the right.
We do not, however, restrict pure leftrecursion and pure right recursion.
Between two occurrences of an arbitrary rule, weallow left recursion followed by right recursion (which leads to tag r followed by tagrl), or right recursion followed by left recursion (which leads to tag l followed bytag lr).An example is given in Figure 10.
As before, the rules of the grammar are implicitin the depicted parse tree.
At the top of the derivation we find S. In the transformedgrammar, we first have to apply S --* S -r'?.
The derivation starts with a rule S --* A a,which generates a string (a) to the right of a nonterminal (A).
Before we can apply zeroor more of such rules, we first have to apply a unit rule S T,?
--* S r,?
in the transformedgrammar.
For zero or more rules that subsequently generate something on the left,such as A ~ b B, we have to obtain a superscript containing rl, and in the examplethis is done by applying A r,?
~ A rl,?.
Now we are finished with pure left recursion andpure right recursion, and apply B rl,O ---+ B ?,0.
This allows us to apply one unconstrainedrule, which appears in the transformed grammar as B ?,?
---* c S T'I d.28Nederhof Experiments with Regular ApproximationWe are given a grammar G = (G, N, P, S).
The following is to be performed for eachset Ni C .IV" such that recursive(Ni) = self.
The value d stands for the maximum numberof unconstrained rules along a spine, possibly alternated with a series of left-recursiverules followed by a series of right-recursive rules, or vice versa.1.
For each A c Ni, each Q E { T, l, r, It, rl, 3_ }, and each f such that0 < f < d, add the following nonterminals to N.?
AQ,f.2.
For each A E Ni, add the following rule to P.?
A ---+ A T'0.3.
For each A E Ni and f such that 0 G f G d, add the following rules to P.?
AT, f  ___+ Al,f.?
ATd: __+ Ar,f.?
Aid ---+ Alr,f.?
Ar,f ---, A~l,/.?
Atr,f __+ A?,d .?
Arl,f ___+ A?,f.4.
For each (A -+ Ba) ~ P such that A, B c Ni and no symbols from ~ aremembers of Ni, eachf  such that 0 <f  G d, and each Q E {r, lr}, add thefollowing rule to P.?
AQd ~ BQ/a.5.
For each (A --+ c~B) E P such that A, B E Ni and no symbols from c~ aremembers of Ni, eachf  such that 0 Gf  < d, and each Q c {l, rl}, add thefollowing rule to P.?
Aqd ~ c~BQ,f.6.
For each (A -~ o~0AloqA2... O~m-lAmC~m) C P such that A, A1 .
.
.
.
.
Am E Niand no symbols from s0 .
.
.
.
.
C~m are members of Ni, and each f such that0 < f G d, add the following rule to P, provided m = 0 v f  < d.?
A?/  c~0Alq-d+lc~l AT,f+1 - - -4  .
.
.~ l  m ' OLm .7.
Remove from P the old rules of the form A ~ c~, where A E Ni.8.
Reduce the grammar.Figure 9A simpler subset approximation by transforming the grammar.Now the counter f has been increased from 0 at the start of the subderivation to1 at the end.
Depending on the value d that we choose, we cannot build derivationsby repeating subderivation S --+* b c S d a an unlimited number of times: at somepoint the counter will exceed d. If we choose d = 0, then already the derivation at29Computational Linguistics Volume 26, Number 1SS !T,O(a) /~  (b)!r,OSo\ A a ' arl, Ob B b B rl'O:ot tt tt tt t Figure 10A parse tree in a self-embedding grammar (a), and the corresponding parse tree in thetransformed grammar (b), for the simple subset approximation from Figure 9.Figure 10 (b) is no longer possible, since no nonterminal in the transformed grammarwould contain 1 in its superscript.Because of the demonstrated increase of the counter f ,  this transformation is guar-anteed to remove self-embedding from the grammar.
However, it is not as selective asthe transformation we saw before, in the sense that it may also block subderivationsthat are not of the form A --** ~Afl.
Consider for example the subderivation fromFigure 10, but replacing the lower occurrence of S by any other nonterminal C that ismutually recursive with S, A, and B.
Such a subderivation S ---** b c C d a would alsobe blocked by choosing d = 0.
In general, increasing d allows more of such derivationsthat are not of the form A ~"  o~Afl but also allows more derivations that are of thatform.The reason for considering this transformation rather than any other that elim-inates self-embedding is purely pragmatic: of the many variants we have tried thatyield nontrivial subset approximations, this transformation has the lowest complex-ity in terms of the sizes of intermediate structures and of the resulting finite au-tomata.In the actual implementation, wehave integrated the grammar transformation a dthe construction of the finite automaton, which avoids reanalysis of the grammar todetermine the partition of mutually recursive nonterminals after transformation.
Thisintegration makes use, for example, of the fact that for fixed Ni and fixed f,  the set ofnonterminals of the form A,f ,  with A c Ni, is (potentially) mutually right-recursive.A set of such nonterminals can therefore be treated as the corresponding case fromFigure 2, assuming the value right.The full formulation of the integrated grammar transformation a d constructionof the finite automaton is rather long and is therefore not given here.
A very similarformulation, for another grammar transformation, is given in Nederhof (1998).30Nederhof Experiments with Regular Approximation4.4 Superset Approximation through Pushdown AutomataThe distinction between context-free languages and regular languages can be seen interms of the distinction between pushdown automata and finite automata.
Pushdownautomata maintain a stack that is potentially unbounded in height, which allows morecomplex languages to be recognized than in the case of finite automata.
Regular ap-proximation can be achieved by restricting the height of the stack, as we will see inSection 4.5, or by ignoring the distinction between several stacks when they becometoo high.More specifically, the method proposed by Pereira and Wright (1997) first con-structs an LR automaton, which is a special case of a pushdown automaton.
Then,stacks that may be constructed in the course of recognition of a string are computedone by one.
However, stacks that contain two occurrences of a stack symbol are iden-tified with the shorter stack that results by removing the part of the stack between thetwo occurrences, including one of the two occurrences.
This process defines a congru-ence relation on stacks, with a finite number of congruence classes.
This congruencerelation directly defines a finite automaton: each class is translated to a unique state ofthe nondeterministic finite automaton, shift actions are translated to transitions labeledwith terminals, and reduce actions are translated to epsilon transitions.The method has a high complexity.
First, construction of an LR automaton, ofwhich the size is exponential in the size of the grammar, may be a prohibitively ex-pensive task (Nederhof and Satta 1996).
This is, however, only a fraction of the effortneeded to compute the congruence classes, of which the number is in turn exponen-tial in the size of the LR automaton.
If the resulting nondeterministic automaton isdeterminized, we obtain a third source of exponential behavior.
The time and spacecomplexity of the method are thereby bounded by a triple exponential function in thesize of the grammar.
This theoretical nalysis eems to be in keeping with the highcosts of applying this method in practice, as will be shown later in this article.As proposed by Pereira and Wright (1997), our implementation applies the ap-proximation separately for each nonterminal occurring in a set Ni that reveals self-embedding.A different superset approximation based on LR automata was proposed by Baker(1981) and rediscovered by Heckert (1994).
Each individual stack symbol is now trans-lated to one state of the nondeterministic finite automaton.
It can be argued theoret-ically that this approximation differs from the unparameterized RTN approximationfrom Section 4.1 only under certain conditions that are not likely to occur very oftenin practice.
This consideration is confirmed by our experiments obe discussed later.Our implementation differs from the original algorithm in that the approximation isapplied separately for each nonterminal in a set Ni that reveals elf-embedding.A generalization f this method was suggested by Bermudez and Schimpf (1990).For a fixed number d > 0 we investigate sequences of d top-most elements of stacksthat may arise in the LR automaton, and we translate these to states of the finiteautomaton.
More precisely, we define another congruence r lation on stacks, such thatwe have one congruence class for each sequence of d stack symbols and this classcontains all stacks that have that sequence as d top-most elements; we have a separateclass for each stack that contains fewer than d elements.
As before, each congruenceclass is translated to one state of the nondeterministic finite automaton.
Note that thecase d = 1 is equivalent to the approximation i  Baker (1981).If we replace the LR automaton by a certain type of automaton that performs top-down recognition, then the method in Bermudez and Schimpf (1990) amounts to theparameterized RTN method from Section 4.1; note that the histories from Section 4.1in fact function as stacks, the items being the stack symbols.31Computational Linguistics Volume 26, Number 14.5 Subset Approximation through Pushdown AutomataBy restricting the height of the stack of a pushdown automaton, one obstructs recogni-tion of a set of strings in the context-free language, and therefore a subset approxima-tion results.
This idea was proposed by Krauwer and des Tombe (1981), Langendoenand Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) andrecently by Johnson (1998).
Since the latest publication in this area is more explicit inits presentation, we will base our treatment on this, instead of going to the historicalroots of the method.One first constructs a modified left-corner ecognizer from the grammar, in theform of a pushdown automaton.
The stack height is bounded by a low number;Johnson (1998) claims a suitable number would be 5.
The motivation for using theleft-corner strategy is that the height of the stack maintained by a left-corner parseris already bounded by a constant in the absence of self-embedding.
If the artificialbound imposed by the approximation method is chosen to be larger than or equal tothis natural bound, then the approximation may be exact.Our own implementation is more refined than the published algorithms mentionedabove, in that it defines a separate left-corner recognizer for each nonterminal A suchthat A E Ni and recursive(Ni) = self, some i.
In the construction of one such recognizer,nonterminals that do not belong to Ni are treated as terminals, as in all other methodsdiscussed here.4.6 Superset Approximation by N-gramsAn approximation from Seyfarth and Bermudez (1995) can be explained as follows.Define the set of all terminals reachable from nonterminal A to be ~A = {a I 3c~, iliA --**o~afl\]}.
We now approximate the set of strings derivable from A by G~, which is theset of strings consisting of terminals from GA. Our implementation is made slightlymore sophisticated by taking ~A to be {X \] 3B, c~,fl\[B E Ni A B ~ oLXfl A X ~ Ni\]}, foreach A such that A E Ni and recursive(Ni) = self, for some i.
That is, each X E ~A isa terminal, or a nonterminal not in the same set Ni as A, but immediately reachablefrom set Ni, through B E Ni.This method can be generalized, inspired by Stolcke and Segal (1994), who deriveN-gram probabilities from stochastic ontext-free grammars.
By ignoring the probabil-ities, each N = 1, 2, 3 .
.
.
.
gives rise to a superset approximation that can be describedas follows: The set of strings derivable from a nonterminal A is approximated by theset of strings al .. .
an such that?
for each substring v = ai+l .
.
.
ai+N (0 < i < n -- N) we have A --+* wvy, forsome w and y,?
for each prefix v = al .
.
.
ai (0 < i < n) such that i < N we have A -** vy,for some y, and?
for each suffix v = ai+l ...  an (0 < i < n) such that n - i < N we havea ---~* wv, for some w.(Again, the algorithms that we actually implemented are more refined and take intoaccount he sets Ni.
)The approximation from Seyfarth and Bermudez (1995) can be seen as the case N =1, which will henceforth be called the unigram method.
We have also experimentedwith the cases N = 2 and N = 3, which will be called the bigram and trigram methods.32Nederhof Experiments with Regular Approximation5.
Increasing the PrecisionThe methods of approximation described above take as input the parts of the grammarthat pertain to self-embedding.
It is only for those parts that the language is affected.This leads us to a way to increase the precision: before applying any of the abovemethods of regular approximation, we first transform the grammar.This grammar transformation copies grammar ules containing recursive nonter-minals and, in the copies, it replaces these nonterminals by new nonrecursive nonter-minals.
The new rules take over part of the roles of the old rules, but since the newrules do not contain recursion and therefore do not pertain to self-embedding, theyremain unaffected by the approximation process.Consider for example the palindrome grammar from Figure 1.
The RTN methodwill yield a rather crude approximation, amely, the language {a, b}*.
We transformthis grammar in order to keep the approximation process away from the first threelevels of recursion.
We achieve this by introducing three new nonterminals S\[1\], S\[2\]and S\[3\], and by adding modified copies of the original grammar ules, so that weobtain:S\[1\]S\[2\]S\[3\]SThe new start symbol is S\[1\].aS\[2\]a \] bS\[2\] b I ?aS\[3\]a \] bS\[3\] b I caSa l bSb  i caSa  i bSb  i eThe new grammar generates the same language as before, but the approximationprocess leaves unaffected the nonterminals S\[1\], S\[2\], and S\[3\] and the rules definingthem, since these nonterminals are not recursive.
These nonterminals amount o theupper three levels of the parse trees, and therefore the effect of the approximationon the language is limited to lower levels.
If we apply the RTN method then weobtain the language that consists of (grammatical) palindromes of the form ww R, wherew E {?, a, b} U {a, b} 2 U {a, b} 3, plus (possibly ungrammatical) strings of the form wvw R,where w E {a, b} 3 and v E {a, b}*.
(w R indicates the mirror image of w.)The grammar transformation i its full generality is given by the following, whichis to be applied for fixed integer j > 0, which is a parameter of the transformation,and for each Ni such that recursive(Ni) = self.For each nonterminal A E Ni we introduce j new nonterminals All\] .
.
.
.
.
A~\].
Foreach A --, X1 .
.
.Xm in P such that A E Ni, and h such that 1 ~ h < j, we addA\[h\] --* X ' I .
.
.
X"  to P, where for 1 < k < m:X~k = Xk\[h + 1\] if X k E Ni /X h < j= Xk otherwiseFurther, we replace all rules A --* X1 .
.
.
Xm such that A ~ Ni by A --* X~ ... X~m, wherefor 1 < k < m:X~ -- Xk\[1\] i fXkENi= Xk otherwiseIf the start symbol S was in Ni, we let S\[1\] be the new start symbol.A second transformation, which shares some characteristics with the one above,was presented in Nederhof (1997).
One of the earliest papers uggesting such transfor-mations as a way to increase the precision of approximation is due to ~ulik and Cohen(1973), who only discuss examples, however; no general algorithms were defined.33Computational Linguistics Volume 26, Number 1550500450?
400 -5350._N_ 300250E 200E 1501005000I I I I I I50 100 150 200 250 300 350corpus size (# sentences)E1801601401201008060402005 10 15 20 25 30length (# words)Figure 11The test material.
The left-hand curve refers to the construction of the grammar f om 332sentences, the right-hand curve refers to the corpus of 1,000 sentences used as input to thefinite automata.6.
Empirical ResultsIn this section we investigate empirically how the respective approximation methodsbehave on grammars of different sizes and how much the approximated languagesdiffer from the original context-free languages.
This last question is difficult o answerprecisely.
Both an original context-free language and an approximating regular lan-guage generally consist of an infinite number of strings, and the number of stringsthat are introduced in a superset approximation or that are excluded in a subset ap-proximation may also be infinite.
This makes it difficult to attach numbers to the"quality" of approximations.We have opted for a pragmatic approach, which does not require investigation ofthe entire infinite languages of the grammar and the finite automata, but looks at acertain finite set of strings taken from a corpus, as discussed below.
For this finite setof strings, we measure the percentage that overlaps with the investigated languages.For the experiments, we took context-free grammars for German, generated auto-matically from an HPSG and a spoken-language corpus of 332 sentences.
This corpusconsists of sentences possessing grammatical phenomena ofinterest, manually selectedfrom a larger corpus of actual dialogues.
An HPSG parser was applied on these sen-tences, and a form of context-free backbone was selected from the first derivation thatwas found.
(To take the first derivation is as good as any other strategy, given that wehave at present no mechanisms for relative ranking of derivations.)
The label occur-ring at a node together with the sequence of labels at the daughter nodes was thentaken to be a context-free rule.
The collection of such rules for the complete corpusforms a context-free grammar.
Due to the incremental nature of this construction ofthe grammar, we can consider the subgrammars obtained after processing the first psentences, where p = 1, 2, 3 .
.
.
.
.
332.
See Figure 11 (left) for the relation between p andthe number of rules of the grammar.
The construction is such that rules have at mosttwo members in the right-hand side.As input, we considered a set of 1,000 sentences, obtained independently from the332 sentences mentioned above.
These 1,000 sentences were found by having a speechrecognizer provide a single hypothesis for each utterance, where utterances come fromactual dialogues.
Figure 11 (right) shows how many sentences of different lengths thecorpus contains, up to length 30.
Above length 25, this number quickly declines, butstill a fair quantity of longer strings can be found, e.g., 11 strings of a length between34Nederhof Experiments with Regular Approximation51 and 60 words.
In most cases however such long strings are in fact composed of anumber of shorter sentences.Each of the 1,000 sentences were input in their entirety to the automata, lthoughin practical spoken-language systems, often one is not interested in the grammaticalityof complete utterances, but tries to find substrings that form certain phrases bearinginformation relevant to the understanding of the utterance.
We will not be concernedhere with the exact way such recognition of substrings could be realized by means offinite automata, since this is outside the scope of this paper.For the respective methods of approximation, we measured the size of the com-pact representation f the nondeterministic automaton, the number of states and thenumber of transitions of the minimal deterministic automaton, and the percentageof sentences that were recognized, in comparison to the percentage of grammaticalsentences.
For the compact representation, we counted the number of lines, which isroughly the sum of the numbers of transitions from all subautomata, not consideringabout three additional lines per subautomaton for overhead.We investigated the size of the compact representation because it is reasonablyimplementation independent, barring optimizations of the approximation algorithmsthemselves that affect he sizes of the subautomata.
For some methods, we show thatthere is a sharp increase in the size of the compact representation for a small increasein the size of the grammar, which gives us a strong indication of how difficult itwould be to apply the method to much larger grammars.
Note that the size of thecompact representation is a (very) rough indication of how much effort is involved indeterminization, minimization, and substitution of the subautomata into each other.For determinization a d minimization of automata, we have applied programs fromthe FSM library described in Mohri, Pereira, and Riley (1998).
This library is consideredto be competitive with respect o other tools for processing of finite-state machines.When these programs cannot determinize or minimize in reasonable time and spacesome subautomata constructed by a particular method of approximation, then this canbe regarded as an indication of the impracticality of the method.We were not able to compute the compact representation for all the methodsand all the grammars.
The refined RTN approximation from Section 4.2 proved to bequite problematic.
We were not able to compute the compact representation for anyof the automatically obtained grammars in our collection that were self-embedding.We therefore liminated individual rules by hand, starting from the smallest self-embedding rammar in our collection, eventually finding grammars small enough tobe handled by this method.
The results are given in Table 1.
Note that the size of thecompact representation increases ignificantly for each additional grammar rule.
Thesizes of the finite automata, fter determinization a d minimization, remain relativelysmall.Also problematic was the first approximation from Section 4.4, which was basedon LR parsing following Pereira and Wright (1997).
Even for the grammar of 50 rules,we were not able to determinize and minimize one of the subautomata accordingto step 1 of Section 3: we stopped the process after it had reached a size of over 600megabytes.
Results, as far as we could obtain them, are given in Table 2.
Note the sharpincreases in the size of the compact representation, resulting from small increases, from44 to 47 and from 47 to 50, in the number of rules, and note an accompanying sharpincrease in the size of the finite automaton.
For this method, we see no possibilityof accomplishing the complete approximation process, including determinization a dminimization, for grammars in our collection that are substantially larger than 50 rules.Since no grammars of interest could be handled by them, the above two methodswill be left out of further consideration.35Computational Linguistics Volume 26, Number 1Table 1Size of the compact representation a d number of states and transitions,for the refined RTN approximation (Grimley-Evans 1997).Grammar Size Compact Representation # of States # of Transitions10 133 11 1412 427 17 2613 1,139 17 3414 4,895 17 3615 16,297 17 4016 51,493 19 5217 208,350 19 5218 409,348 21 5919 1,326,256 21 61Table 2Size of the compact representation a d number of states and transitions,for the superset approximation based on LR automata following Pereiraand Wright (1997).Grammar Size Compact Representation # of States # of Transitions35 15,921 350 2,12544 24,651 499 4,35247 151,226 5,112 35,75450 646,419 ?
?Below, we refer to the unparameterized and parameterized approximations basedon RTNs (Section 4.1) as RTN and RTNd, respectively, for d = 2,3; to the subsetapproximation from Figure 9 as Subd, for d = 1, 2, 3; and to the second and thirdmethods from Section 4.4, which were based on LR parsing following Baker (1981)and Bermudez and Schimpf (1990), as LR and LRd, respectively, for d = 2, 3.
We referto the subset approximation based on left-corner parsing from Section 4.5 as LCd, forthe maximal stack height of d = 2, 3, 4; and to the methods discussed in Section 4.6 asUnigram, Bigram, and Trigram.We first discuss the compact representation f the nondeterministic automata.
InFigure 12 we use two different scales to be able to represent the large variety of values.For the method Subd, the compact representation is of purely theoretical interest forgrammars larger than 156 rules in the case of Sub1, for those larger than 62 rulesin the case of Sub2, and for those larger than 35 rules in the case of Sub3, sincethe minimal deterministic automata could thereafter no longer be computed with areasonable bound on resources; we stopped the processes after they had consumedover 400 megabytes.
For LC3, LC4, RTN3, LR2, and LR3, this was also the case forgrammars larger than 139, 62, 156, 217, and 156 rules, respectively.
The sizes of thecompact representation seem to grow moderately for LR and Bigram, in the upperpanel, yet the sizes are much larger than those for RTN and Unigram, which areindicated in the lower panel.The numbers of states for the respective methods are given in Figure 13, againusing two very different scales.
As in the case of the grammars, the terminals of ourfinite automata re parts of speech rather than words.
This means that in general therewill be nondeterminism during application of an automaton on an input sentence dueto lexical ambiguity.
This nondeterminism can be handled efficiently using tabular36Nederhof Experiments with Regular Approximationr'~E O o70000060000050000040000030000020000010000000i i \] ; ; / i i, / !
/' LC4!
/ / ," LR3--x---.i \[ / ., RTN3 -~i I i ," LC3i \[ i " LR2 .
.
.
.
.!
/ j  Trigram -~'---, / ; LC2 -e---i / i RTN2i / / LR -+ --:t \] / Bigram-E3---~: ~ ;'~ /'!
/" / /  4-/ ," ,4- ...... + ...." / '  / '~  4-"""  ,' / .
- "  _ .
.+  .
.
.
.
.
.
.
.
4- .....' ~ - ~  "" - - - -E3-  .
.
.
.
.
.
.
.
.
.
.
.
E}  .
.
.
.
.
.
.
E} - - - -n  .
.
.
.
- - ' -- - .
.
.
.
.
.
.
.
.
1 .
.
.
.
.
.
.
.
I I I I I I50 100 150 200 250 300 350 400 450 500 550grammar size2000015000"3&10000o5000IJ ://"/ /"//.'
,.
',; ,.
: ,"i ... ,',,..,'...." ,,,'/,,: ,,, .
.
, .
..: ....z~ ,'" .
.. "~/ ... ,' .- .
......'.....-, , ?
y~..-" ' " " ' ', , ,~::'~ .....0 50 100 150 200Figure 12Size of the compact representation.RTN2 .-a--LC2 -e - -LR -+ --'Bigram -~---Sub3 -x .
.
.
.Sub2 "a .....Sub1 -~ .....RTNUnigram -~,---.....,0.I I I I I I250 300 350 400 450 500 550grammar sizetechniques, provided the number of states is not too high.
This consideration favorsmethods that produce low numbers of states, such as Trigram, LR, RTN, Bigram, andUnigram.37Computational Linguistics Volume 26, Number 1100009000800070006000500040003000200010000Sub2 ..A .....LC4 -8 - -Sub1 --~ .....LC3 -x - -RTN3LC2 -e - -f/ j/\[//~~~i100?~- '~-~ ~0 200 300 400 500 600grammar size100 , , , , ,LC3RTN3LR3 --x- --LC2 -e - -x RTN280 ,,,,,," TrigramLR2 -.~--~----LR -+ --RTN -B~Bigram -a--~- -~-  Unigram -~---60 .~?
"/40: ~ ~  d~3._-.D .D  ---\[\] ' --- .
.
.
.
.
.
.
.
.
~E\]" * .
.
.
.
.
.
\ [ \ ]  .
.
.
.
.
.
.
.
.
.
.
-20 " ~" --e-- "<>.
.e -  .
.
.
.
.
.
.
.
.
e .
.
.
.
.
-e .
.
.
.
.
.
.
~ .
.
.
.
.
.0 i i i i i0 100 200 300 400 500 600grammar sizeFigure 13Number of states of the deterrninized and minimized automata.Note that the numbers of states for LR and RTN differ very little.
In fact, forsome of the smallest and for some of the largest grammars  in our collection, theresulting automata were identical.
Note, however, that the intermediate results for LR38Nederhof Experiments with Regular Approximation(Figure 12) are much larger.
It should therefore be concluded that the "sophistication"of LR parsing is here merely an avoidable source of inefficiency.The numbers of transitions for the respective methods are given in Figure 14.Again, note the different scales used in the two panels.
The numbers of transitionsroughly correspond to the storage requirements for the automata.
It can be seen that,again, Trigram, LR, RTN, Bigram, and Unigram perform well.The precision of the respective approximations i  measured in terms of the per-centage of sentences in the corpus that are recognized by the automata, in comparisonto the percentage of sentences that are generated by the grammar, as presented by Fig-ure 15.
The lower panel represents an enlargement of a section from the upper panel.Methods that could only be applied for the smaller grammars are only presented inthe lower panel; LC4 and Sub2 have been omitted entirely.The curve labeled G represents he percentage of sentences generated by the gram-mar.
Note that since all approximation methods compute ither supersets or subsets, aparticular automaton cannot both recognize some ungrammatical sentences and rejectsome grammatical sentences.Unigram and Bigram recognize very high percentages of ungrammatical sentences.Much better results were obtained for RTN.
The curve for LR would not be distin-guishable from that for RTN in the figure, and is therefore omitted.
(For only two ofthe investigated grammars was there any difference, the largest difference occurringfor grammar size 217, where 34.1 versus 34.5 percent of sentences were recognizedin the cases of LR and RTN, respectively.)
Trigram remains very close to RTN (andLR); for some grammars a lower percentage is recognized, for others a higher per-centage is recognized.
LR2 seems to improve slightly over RTN and Trigram, but datais available only for small grammars, due to the difficulty of applying the method tolarger grammars.
A more substantial improvement is found for RTN2.
Even smallerpercentages are recognized by LR3 and RTN3, but again, data is available only forsmall grammars.The subset approximations LC3 and Sub1 remain very close to G, but here againonly data for small grammars is available, since these two methods could not beapplied on larger grammars.
Although application of LC2 on larger grammars requiredrelatively few resources, the approximation is very crude: only a small percentage ofthe grammatical sentences are recognized.We also performed experiments with the grammar transformation from Section 5,in combination with the RTN method.
We found that for increasing j, the interme-diate automata soon became too large to be determinized and minimized, with abound on the memory consumption of 400 megabytes.
The sizes of the automata thatwe were able to compute are given in Figure 16.
RTN+j, for j = 1, 2, 3,4, 5, repre-sents the (unparameterized) RTN method in combination with the grammar transfor-mation with parameter j.
This is not to be confused with the parameterized RTNdmethod.Figure 17 indicates the number of sentences in the corpus that are recognized byan automaton divided by the number of sentences in the corpus that are generatedby the grammar.
For comparison, the figure also includes curves for RTNd, whered = 2, 3 (cf.
Figure 15).
We see that j = 1, 2 has little effect.
For j = 3,4, 5, however,the approximating language becomes ubstantially smaller than that in the case ofRTN, but at the expense of large automata.
In particular, if we compare the sizes ofthe automata for RTN+j in Figure 16 with those for RTNd in Figures 13 and 14, thenFigure 17 suggests the large sizes of the automata for RTN+j are not compensatedadequately by a reduction of the percentage of sentences that are recognized.
RTNdseems therefore preferable to RTN+j.39Computational Linguistics Volume 26, Number 19000080000700006000050000 o400003000020000100OO00z~100 200 300  400  500grammar  sizeiSub2 --~ ......LC4Sub1 --~ .....LC3LC2RTN2I60050004000 -0000 / /! '
2000 - // -.
/ / ' / '  /';'" .13 - - - _  D -/ ";Y" _ .
.
.
.
.
.
~-  .
.
.
.
.
43"-1000 I I .
:..i~ ..- .
.
.
.
(3 .
.
.
..D -''0LC3 -x---RTN3LR3 --x- --LC2  -e~RTN2Tr ig ram -~---LR2  -~,- -LR -?
--RTNB igram -B - -Un igram -~- --0 100 200  300  400  500grammar  sizeFigure 14Number of transitions of the determinized and minimized automata.6007.
Conc lus ionsIf we apply the finite automata with the intention of filtering out incorrect sentences,for example from the output from a speech recognizer, then it is al lowed that a40Nederhof Experiments with Regular Approximation10080-o 6O&oo400020Unigram -4- --Bigram -\[\]--,e - - -~  Trigram -x--.-,,' RTN(LR)  -~- -e- .
.
.
.
.
.
?, -- --e- -- -~- .
.
.
.
.
'~ RTN2G -~---~a--.
. '
LC2 -e - -?
13 -* -?3,.O .
.
.
.
.
.
.
.
~" ~3 .
.
.
.
.
(3""'" 13 .
.
.
.
.
.
E l - - - - \ [ \ ] ' - ' -\ [ \ ]  .
.
.
.
.
.
.
Ey ?/ , - _ .
lq___  +/' ,' 4 .
.
-  I6 ,~'  0 0 .
_ .
- - -e - -~O 0?
C\] / /i" I I100 200 300 400 500 600grammar size5',\[::4"E03i , ' i  \[\]/ , -?
j:/,D / / i -  ,/ /  .
.-/ '0 i I I I I I I40 60 80 100 120 140 160grammar sizeFigure 15Percentage of sentences that are recognized./ i/ Bigram -D--RTN(LR)Trigram -x-.-LR2 -,~- -RTN2 -~- -LR3 --x- --RTN3 -+- - -G - - t - - .LC3 -x - -Sub1 --~ .....LC2 -e - -certain percentage of ungrammatical input is recognized.
Recognizing ungrammat-ical input merely makes filtering less effective; it does not affect the functionalityof the system as a whole, provided we assume that the grammar specifies exactlythe set of sentences that can be successfully handled by a subsequent phase of pro-41Computational Linguistics Volume 26, Number 1100008000600040002000: !
!i i i iRTN+5 -~--RTN+4 -0--RTN+3 -+--RTN+2 -o--RTN+I .....a %0, / ~ , ,  ,,; /  .4 -  -"50 100 150 200 250 300 350 400 450 500 550grammar size250000200000o15000010000050000i i i , i i:ia iij}!
oi i i i iRTN+5 -z~--RTN+4 -o--RTN+3 -+--RTN+2 -ra--RTN+I "~'",+" ~-E\],\[3^ _ ~ /  ,, 4z ' '+0 :"~'~"" .. .
.
.
.
.
.
.
.
.
.
.0 50 100 150 200 250 300 350 400 450 500 550grammar sizeFigure 16Number of states and number of transitions of the determinized and minimized automata.1.61.51.41.3s1.21.1i iRTNRTN2 -a---RTN3RTN+I -~<--RTN+2 -D--RTN+3 -+--RTN+4 -o--RTN+5 -a--150 100 150 200 250 300 350 400grammar sizeFigure 17Number of recognized sentences divided by number of grammatical sentences.cessing.
Also allowed is that "pathological" grammatical sentences are rejected thatseldom occur in practice; an example are sentences requiring multiple levels of self-embedding.Of the methods we considered that may lead to rejection of grammatical sen-tences, i.e., the subset approximations, none seems of much practical value.
The mostserious problem is the complexity of the construction of automata from the compactrepresentation for large grammars.
Since the tools we used for obtaining the minimal42Nederhof Experiments with Regular Approximationdeterministic automata re considered to be of high quality, it seems unlikely thatalternative implementations could succeed on much larger grammars, especially con-sidering the sharp increases in the sizes of the automata for small increases in the sizeof the grammar.
Only LC2 could be applied with relatively few resources, but this is avery crude approximation, which leads to rejection of many more sentences than justthose requiring self-embedding.Similarly, some of the superset approximations are not applicable to large gram-mars because of the high costs of obtaining the minimal deterministic automata.
Someothers provide rather large languages, and therefore do not allow very effective ill-tering of ungrammatical input.
One method, however, seems to be excellently suitedfor large grammars, namely, the RTN method, considering both the unparameterizedversion and the parameterized version with d = 2.
In both cases, the size of the au-tomaton grows moderately in the grammar size.
For the unparameterized version, thecompact representation also grows moderately.
Furthermore, the percentage of recog-nized sentences remains close to the percentage of grammatical sentences.
It seemstherefore that, under the conditions of our experiments, this method is the most suit-able regular approximation that is presently available.AcknowledgmentsThis paper could not have been writtenwithout he wonderful help of Hans-UlrichKrieger, who created the series of grammarsthat are used in the experiments.
I also oweto him many thanks for countlessdiscussions and for allowing me to pursuethis work.
I am very grateful to theanonymous referees for their inspiringsuggestions.This work was funded by the GermanFederal Ministry of Education, Science,Research and Technology (BMBF) in theframework of the VERBMOBIL Project underGrant 01 IV 701 V0.ReferencesBaker, Theodore P. 1981.
Extendinglookahead for LR parsers.
Journal ofComputer and System Sciences, 22:243-259.Bermudez, Manuel E. and Karl M. Schimpf.1990.
Practical arbitrary lookahead LRparsing.
Journal of Computer and SystemSciences, 41:230-250.Berstel, Jean.
1979.
Transductions andContext-Free Languages.
B. G. Teubner,Stuttgart.Black, Alan W. 1989.
Finite state machinesfrom feature grammars.
In InternationalWorkshop on Parsing Technologies, pages277-285, Pittsburgh, PA.Chomsky, Noam.
1959a.
A note on phrasestructure grammars.
Information andControl, 2:393-395.Chomsky, Noam.
1959b.
On certain formalproperties of grammars.
Information andControl, 2:137-167.Culik, Karel II and Rina Cohen.
1973.LR-regular grammars--An extension ofLR(k) grammars.
Journal of Computer andSystem Sciences, 7:66-96.Earley, Jay.
1970.
An efficient context-freeparsing algorithm.
Communications of theACM, 13(2):94-102, February.Grimley-Evans, Edmund.
1997.Approximating context-free grammarswith a finite-state calculus.
In Proceedingsof the 35th Annual Meeting of the Associationfor Computational Linguistics an 8thConference ofthe European Chapter of theAssociation for Computational Linguistics,pages 452-459, Madrid, Spain.Harrison, Michael A.
1978.
Introduction toFormal Language Theory.
Addison-Wesley.Heckert, Erik.
1994.
Behandlung vonSyntaxfehlern fiir LR-Sprachen ohneKorrekturversuche.
Ph.D. thesis,Ruhr-Universit/it Bochum.Johnson, Mark.
1998.
Finite-stateapproximation of constraint-basedgrammars using left-comer grammartransforms.
In COLING-ACL "98: 36thAnnual Meeting of the Association forComputational Linguistics and 17thInternational Conference on ComputationalLinguistics, volume 1, pages 619-623,Montreal, Quebec, Canada.Krauwer, Steven and Louis des Tombe.
1981.Transducers and grammars as theories oflanguage.
Theoretical Linguistics, 8:173-202.Langendoen, D. Terence and YedidyahLangsam.
1987.
On the design of finitetransducers for parsing phrase-structurelanguages.
In Alexis Manaster-Ramer,editor, Mathematics of Language.
JohnBenjamins, Amsterdam, pages 191-235.Mohri, Mehryar and Fernando C. N.43Computational Linguistics Volume 26, Number 1Pereira.
1998.
Dynamic ompilation ofweighted context-free grammars.
InCOLING-ACL "98: 36th Annual Meeting ofthe Association for Computational Linguisticsand 17th International Conference onComputational Linguistics, volume 2, pages891-897, Montreal, Quebec, Canada.Mohri, Mehryar, Femando C. N. Pereira,and Michael Riley.
1998.
A rational designfor a weighted finite-state transducerlibrary.
In Derick Wood and Sheng Yu,editors, Automata Implementation.
LectureNotes in Computer Science, Number 1436.Springer Verlag, pages 144-158.Nederhof, Mark-Jan. 1994.
Linguistic Parsingand Program Transformations.
Ph.D. thesis,University of Nijmegen.Nederhof, Mark-Jan. 1997.
Regularapproximations of CFLs: A grammaticalview.
In Proceedings ofthe InternationalWorkshop on Parsing Technologies,pages 159-170, Massachusetts Institute ofTechnology.Nederhof, Mark-Jan. 1998.
Context-freeparsing through regular approximation.In Proceedings ofthe International Workshopon Finite State Methods in Natural LanguageProcessing, pages 13-24, Ankara, Turkey.Nederhof, Mark-Jan and Giorgio Satta.
1996.Efficient abular LR parsing.
In Proceedingsof the 34th Annual Meeting, pages 239-246,Santa Cruz, CA.
Association forComputational Linguistics.Pereira, Fernando C. N. and Rebecca N.Wright.
1997.
Finite-state approximationof phrase-structure grammars.
InEmmanuel Roche and Yves Schabes,editors, Finite-State Language Processing.MIT Press, pages 149-173.Pulman, S. G. 1986.
Grammars, parsers, andmemory limitations.
Language andCognitive Processes, 1(3):197-225.Purdom, Paul Walton, Jr. and Cynthia A.Brown.
1981.
Parsing extended LR(k)grammars.
Acta Informatica, 15:115-127.Resnik, Philip.
1992.
Left-corner parsing andpsychological p ausibility.
In COLING '92:Papers presented tothe Fifteenth \[sic\]International Conference on ComputationalLinguistics, pages 191-197, Nantes, France.Rosenkrantz, D. J. and P. M. Lewis, II.
1970.Deterministic left comer parsing.
In IEEEConference Record of the 11th AnnualSymposium on Switching and AutomataTheory, pages 139-152.Seyfarth, Benjamin R. and Manuel E.Bermudez.
1995.
Suffix languages in LRparsing.
International Journal of ComputerMathematics, 55:135-153.Stolcke, Andreas and Jonathan Segal.
1994.Precise N-gram probabilities fromstochastic context-free grammars.
InProceedings ofthe 32nd Annual Meeting,pages 74-79, Las Cruces, NM.
Associationfor Computational Linguistics.Woods, W. A.
1970.
Transition etworkgrammars for natural language analysis.Communications of the ACM,13(10):591-606.44
