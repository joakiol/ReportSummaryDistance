Scoring Algorithms for Wordspotting SystemsRobert W. Morris and Jon A. Arrowood and Peter S. CardilloNexidia Inc.3060 Peachtree Rd Suite 730Atlanta, Georgia 30305-2240{rmorris,jarrowood,pcardillo}@nexidia.comMark A. ClementsCenter for Signal & Image ProcessingGeorgia Institute of TechnologyAtlanta, Georgia 30332-0250clements@ece.gatech.eduAbstractWhen evaluating wordspotting systems, onenormally compares receiver operating charac-teristic curves and different measures of accu-racy.
However, there are many other factorsthat are relevant to the system?s usability forsearching speech.
In this paper, we discussboth measures of quality for confidence scoresand propose algorithms for producing scoresthat are optimal with respect to these criteria.1 IntroductionIn order to evaluate any system, it is useful to have objec-tive quality measures that can be automatically applied tosystems for comparison.
For wordspotting systems, thesemeasures are oriented towards recall accuracy.
Most ofthese measures are based on receiver operating character-istic (ROC) curves and functions of these curves.
How-ever, there are many other factors that are relevant to thesystems usability.When a user enters a query to the Nexidia wordspot-ter (Clements et al, 2001), the system returns a sorted re-sult list that marks the times where the query matches theaudio.
In addition, scores are associated with each result.These scores are related to the likelihood that the taggedaudio matches the query.
Although this score gives anindication of the strength of the match, users have haddifficulty interpreting the scores.We found that most users want to use the score in oneof two ways.
The first application is to provide a scorethreshold for monitoring applications.
Alternatively, peo-ple also assume that the score reflects the probability thatthe tagged audio segment is actually a match.However, without any objective quality measure ofthese scores, it was difficult to evaluate different scoregeneration algorithms.
In this paper, we discuss bothmeasures of quality for confidence scores and proposealgorithms for producing scores that are optimal with re-spect to these criteria.2 AssumptionsIn order to derive a scoring algorithm, a key assumptionmust be made by the wordspotting algorithm: each matchmust have a numeric score associated with it.
In addition,there must be some theoretical basis for an additive de-composition of this score.
This decomposition is givenbyR(q) =L?l=1R(q)l , (1)where R(q) is the score returned by query q, and R(q)l isthe score associated with the lth phoneme in the query.With this assumption, we also assume that these compo-nents can be modeled with a Gaussian distribution withdependence on whether the match is truly a hit or a miss.The distributions are then given byR(q)l |Hit ?
N(?H(S(q)l), ?2H)(2)R(q)l |Miss ?
N(?M(S(q)l), ?2M), (3)where S(q)l is the lth phoneme in query q.
In this model,the means, ?
are dependent on the phoneme, but the vari-ance, ?2, is not.
Using the additive model, the raw scoresare distributed byR(q)|Hit ?
N( L?l=1?H(S(q)l), L?2H)(4)R(q)|Miss ?
N( L?l=1?M(S(q)l), L?2M).
(5)3 Performance MeasuresWe propose two scoring evaluation measures.
In each ofthese methods, the raw score is modified by some scoringfunction F ().
The first measure evaluates a scoring algo-rithms usefulness for setting detection thresholds.
Thismethod assumes that the scoring function calculates thecdf of the missed score distributions.
The measurementis based on the Kolmogorov-Smirnov test statistic, whichis given byKS = maxi????F(R(i)M)?
iN????
(6)where R(i)M are the raw scores for the false alarms in de-scending order.A metric for measuring scoring algorithms based onresult confidence is given byB = 1NHNH?n=1[1?F(R(n)H)]2+ 1NMNM?n=1[F(R(n)M)]2,(7)where NM and NH are the number of hits and misses.This value is equal to zero when all hits are scored to oneand all misses are scored as zero.
On the other hand, B isequal to 0.5 if F (R) is set to 0.5 regardless of the input.4 AlgorithmsIf one is interested in setting a detection threshold basedon false alarms per hour, then one can set the score usingthe cumulative density function of the misses.
This yieldsthe scoreFC(R(q))= Pr(x < R(q))= Q[1?L?M(R(q) ?L?l=1?M(S(q)l))], (8)whereQ is the cdf of the unit normal distribution.
To set athreshold for K false alarms per hour, then the thresholdshould be set to?
= 1.0?
KKT, (9)where KT is the range of false alarms per hour that themiss model is trained.If one is looking at a list of scores, one might be inter-ested in the probability that the score was generated by atrue match.
By Bayes law, the conditional probability canbe calculated byFB(R(q))= Pr(Hit|R(q))= PHp(R(q)|Hit)PHp(R(q)|Hit) + (1?PH)p(R(q)|Miss),(10)where PH is the prior probability of a hit.5 Model TrainingEach of the scoring methods described above requiremodels of how the phonemes relate to the scores throughthe parameters: ?M , ?H , ?2M , and ?2H .
For this purpose,a series of hits and misses over the desired range of falsealarms rates must be collected from the wordspotter.
Withthese scores, it is possible to train the miss and hit mod-els independently.
For this reason, only the miss modeltraining is described here.Given the model in Equation 5, the following distribu-tion holds with N observations:p(R|S, ?M , ?2M ) =N?n=1N(R(n) ?L?l=1?M(S(n)l), L?2M).
(11)The maximum likelihood solution for ?M and ?2M is adifficult optimization problem.
However, if the phonemecomponents R(n)l from Equation 1, the distribution sim-plifies to observations of the Gaussian components.
Byusing the Expectation Maximization (EM) algorithm, theoverall likelihood in Equation 11 can be iteratively max-imized (Dempster et al, 1977).Similarly, the training problem can also be viewed in aBayesian framework, where a Minimum Mean SquaredError (MMSE) estimate can be calculated.
Like themaximum likelihood estimate, this requires an iterativemethod where the components of the score are generated.This can be computed by a Gibbs sampler (Gamerman,1997).In addition to providing a mechanism for creatingmeaningful scores, these models can be useful for otherpurposes.
For example, one can analyze the mean vectorsto determine which phonemes provide better discrimina-tion for wordspotting.
These can also be used to diagnoseproblems in performance that are phoneme specific.6 ResultsThe experiments for this algorithm were conducted us-ing the Nexidia wordspotting system trained on broadcastquality North American English speech.
The effect of us-ing different scoring algorithms was accomplished usinga nine hour subset of the HUB-4 1996 North AmericanEnglish broadcast corpus.
This data was chosen since thiscorpus is widely available and is disjoint from the train-ing data used for the wordspotter.
From this corpus, 8500search terms were randomly selected from the transcripts.These queries were equally distributed in length from 4to 20 phonemes, and then split into a testing and trainingset.
For each search term, results ranging from the topscore down to the 90th false alarm were collected.
Theresults from the training terms were then used to train thescore models using both the EM algorithm and a Gibbssampler.These trained models were then then used to gener-ate both FB and FC for all of the test queries.
In addi-tion, the ?Standard?
scores were generated.
These scoresare what the Nexidia wordspotting product reveals to theusers, and are calculated by scaling the raw scores bythe number of phonemes and mapping these from zeroto one.The resulting scores from these tests are listed in Ta-ble 1.
As expected, the CFAR based score performedwell on the KS metric, while the Bayesian score wasmore accurate on the B measure.
Both of these methodsperformed much better than the previous ad-hoc ?Stan-dard?
method.
However, performance improvements onone measure resulted in very poor scores on the other.This is due to the fact that the objective of each mea-sure is very different.
In addition, the estimation schemehad little effect on the overall scores.
Since the EM al-gorithm requires a small fraction of the computation thatthe Gibbs sampler requires, this method is preferable.Table 1: Comparison of different scoring algorithmsbased on two scoring measurementsAlgorithm Performance MeasureKS BGibbs CFAR 0.312 0.350Bayes 0.790 0.197EM CFAR 0.322 0.351Bayes 0.789 0.196Standard 0.633 0.496To illustrate the differences between the three scoringalgorithms, the hits and misses were also collected andplotted in Figure 6.
In each subplot, there are histogramsof the hits and misses.
In all three cases, most of the hitstend to have scores close to one.
However, the missesin the standard scoring scheme are concentrated from 0.5to 0.8.
When the Bayes scoring method is used, half ofthe hits are very close to 1.0, while half of the misses arevery close to 0.0.
The other half of the scores are dis-tributed along the score range.
Finally, the misses fromthe CFAR scoring algorithm are distributed evenly alongentire range of scores.
Because the normal score assump-tion does not strictly hold, this distribution is not perfectlyflat at the start and the end, but it is fairly close.7 ConclusionsSeveral methods for for both generating and evaluatingscores from wordspotting systems have been proposed.These methods can operate on any system that generatesscores where an additive model based on phonemes isvalid.
The scores that are produced by the algorithmsdescribed can be used to both give intuitive confidencelevels, as well as provide a simple mechanisms for settingthresholds in monitoring environments.
These methodshave been shown to provide superior performance whencompared to their relevant metrics.ReferencesM.
A. Clements, P. S. Cardillo, and M. S. Miller.
Pho-netic searching vs. LVCSR: How to find what you re-ally want in audio archives, in AVIOS 2001.Dani Gamerman.
1997.
Markov Chain Monte Carlo:Stochastic Simulation for Bayesian Inference, vol-ume 1.
Chapman & Hall, Boca Raton, FL.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society Se-ries B, 39(1):1?38.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.4ScoreHistogram of hits and misses using standard scoringMissesHits0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.5ScoreHistogram of hits and misses using Bayes scoringMissesHits0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.4ScoreHistogram of hits and misses using CFAR scoringMissesHitsFigure 1: Comparison of different scoring methods on Broadcast English queries.
Scores are derived from resultsranging from zero to ten false alarms per hour.
