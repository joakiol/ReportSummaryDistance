The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 272?280,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsThe UI System in the HOO 2012 Shared Task on Error CorrectionAlla Rozovskaya Mark Sammons Dan RothCognitive Computation GroupUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{rozovska,mssammon,danr}@illinois.eduAbstractWe describe the University of Illinois (UI) sys-tem that participated in the Helping Our Own(HOO) 2012 shared task, which focuses oncorrecting preposition and determiner errorsmade by non-native English speakers.
Thetask consisted of three metrics: Detection,Recognition, and Correction, and measuredperformance before and after additional revi-sions to the test data were made.
Out of 14teams that participated, our system scored firstin Detection and Recognition and second inCorrection before the revisions; and first inDetection and second in the other metrics af-ter revisions.
We describe our underlying ap-proach, which relates to our previous work inthis area, and propose an improvement to theearlier method, error inflation, which resultsin significant gains in performance.1 IntroductionThe task of correcting grammar and usage mistakesmade by English as a Second Language (ESL) writ-ers is difficult: many of these errors are context-sensitive mistakes that confuse valid English wordsand thus cannot be detected without considering thecontext around the word.Below we show examples of two common ESLmistakes considered in this paper:1.
?Nowadays ?
*/the Internet makes us closer and closer.?2.
?I can see at*/on the list a lot of interesting sports.
?In (1), the definite article is incorrectly omitted.In (2), the writer uses an incorrect preposition.This paper describes the University of Illinois sys-tem that participated in the HOO 2012 shared taskon error detection and correction in the use of prepo-sitions and determiners (Dale et al, 2012).
Fourteenteams took part in the the competition.
The scoringincluded three metrics: Detection, Recognition, andCorrection, and our team scored first or second ineach metric (see Dale et al (2012) for details).The UI system consists of two components, a de-terminer classifier and a preposition classifier, witha common pre-processing step that corrects spellingmistakes.
The determiner system builds on the ideasdescribed in Rozovskaya and Roth (2010c).
Thepreposition classifier uses a combined system, build-ing on work described in Rozovskaya and Roth(2011) and Rozovskaya and Roth (2010b).Both the determiner and the preposition systemsapply the method proposed in our earlier work,which uses the error distribution of the learner datato generate artificial errors in training data.
The orig-inal method was proposed for adding artificial er-rors when training on native English data.
In thistask, however, we apply this method when trainingon annotated ESL data.
Furthermore, we introducean improvement that is conceptually simple but veryeffective and which also proved to be successful inan earlier error correction shared task (Dale and Kil-garriff, 2011; Rozovskaya et al, 2011).
We identifythe unique characteristics of the error correction taskand analyze the limitations of existing approaches toerror correction that are due to these characteristics.Based on this analysis, we propose the error infla-tion method (Sect.
6.2).In this paper, we first briefly discuss the task (Sec-272tion 2) and present our overall approach (Section3.
Next, we describe the spelling correction mod-ule (Section 4).
Section 5 provides an overview ofthe training approaches for error correction tasks.We present the inflation method in Section 6.
Next,we describe the determiner error correction system(Section 7), and the preposition error correctionmodule (Section 8).
In Section 9, we present theperformance results of our system in the competi-tion.
We conclude with a brief discussion (Section10).2 Task DescriptionThe HOO 2012 shared task focuses on correctingdeterminer and preposition errors made by non-native speakers of English.
These errors are some ofthe most common and also some of the most difficultfor ESL learners (Leacock et al, 2010); even veryadvanced learners make these mistakes (Rozovskayaand Roth, 2010a).The training data released by the task organizerscomes from the publicly available FCE corpus (Yan-nakoudakis et al, 2011).
The original FCE data setcontains 1244 essays written by non-native Englishspeakers and is corrected and error-tagged using adetailed error classification schema.
The HOO train-ing data contains 1000 of those files.1 The test datafor the task consists of an additional set of 100 stu-dent essays, different from the 1244 above.Since the HOO task focuses on determiner andpreposition mistakes, only annotations markingpreposition and determiner mistakes were kept.Note that while the other error annotations wereremoved, the errors still remain in the HOO data.More details can be found in Dale et al (2012).3 System OverviewOur system consists of two components that addressindividually article2 and preposition errors and usethe same pre-processing.1In addition, the participating teams were allowed to use fortraining the remaining 244 files of this corpus, as well as anyother data.
We also use a publicly available data set of nativeEnglish, Google Web 1T corpus (Brants and Franz, 2006), inone of our models.2We will use the terms ?article-?
and ?determiner errors?
in-terchangeably: article errors constitute the majority of deter-miner errors, and we address only article mistakes.The first pre-processing step is correcting spellingerrors.
Since the essays were written by students ofEnglish as a Second language, and these essays werecomposed on-the-fly, they contain a large number ofspelling errors.
These errors add noise to the contextaround the target word (article or preposition).
Goodcontext is crucial for robust detection and correctionof article and preposition mistakes.After spelling errors are corrected, we run a sen-tence splitter, part-of-speech tagger3 and shallowparser4 (Punyakanok and Roth, 2001) on the data.Both the article and the preposition systems use fea-tures based on the output of these tools.We made a 244-document subset of the FCE dataa held-out set for development.
The results in Sec-tions 7 and 8 give performance on this held-out set,where we use the HOO data (1000 files) for train-ing.
The actual performance in the task (Section 9)reflects the system trained on the whole set of 1244documents.Our article and preposition modules build on theelements of the systems described in Rozovskayaand Roth (2010b), Rozovskaya and Roth (2010c)and Rozovskaya and Roth (2011).
All article sys-tems are trained using the Averaged Perceptron(AP) algorithm (Freund and Schapire, 1999), im-plemented within Learning Based Java (Rizzolo andRoth, 2010).
Our preposition systems combine theAP algorithm with the Na?
?ve Bayes (NB) classifierwith prior parameters adapted to the learner data(see Section 5).
The AP systems are trained usingthe inflation method (see Section 6.2).We submitted 10 runs.
All of our runs achievedcomparable performance.
Sections 7 and 8 describeour modules.4 Correcting Spelling ErrorsAnalysis of the HOO data made clear the need fora variety of corrections beyond the immediate scopeof the current evaluation.
When a mistake occurs inthe vicinity of a target (i.e.
preposition or article) er-ror, it may result in local cues that obscure the natureof the desired correction.3http://cogcomp.cs.illinois.edu/page/software view/POS4http://cogcomp.cs.illinois.edu/page/software view/Chunker273The following example illustrates such a problem:?In my opinion your parents should be arrive in thefirst party of the month becouse we could be go inmeeting with famous writer, travelled and journalistwho wrote book about Ethiopia.
?In this sample sentence, there are multiple errorsin close proximity: the misspelled word becouse; theverb form should be arrive; the use of the word partyinstead of part; the verb travelled instead of a nounform; an incorrect preposition in (in meeting).The context thus contains a considerable amountof noise that is likely to negatively affect system per-formance.
To address some of these errors, we run astandard spell-checker over the data.We use Jazzy5, an open-source Java spell-checker.The distribution, however, comes only with a USEnglish dictionary, which also has gaps in its cov-erage of the language.
The FCE corpus prefers UKEnglish spelling, so we use a mapping from US toUK English6 to automatically correct the originaldictionary.
We also keep the converted US spelling,since our preposition module makes use of nativeEnglish data, where the US spelling is prevalent.The Jazzy API allows the client to query a word,and get a list of candidate corrections sorted in or-der of edit distance from the original term.
Wetake the first suggestion and replace the originalword.
The resulting substitution may be incorrect,which may in turn mislead the downstream correc-tion components.
However, manual evaluation ofthe spelling corrections suggested about 80% wereappropriate, and experimental evaluation on the cor-pus development set indicated a modest overall im-provement when the spell-checked documents wereused in place of the originals.5 Training for Correction TasksThe standard approach to correcting context-sensitive ESL mistakes follows the methodology ofthe context-sensitive spelling correction task that ad-dresses such misspellings as their and there (Carl-son et al, 2001; Golding and Roth, 1999; Goldingand Roth, 1996; Carlson and Fette, 2007; Banko andBrill, 2001).Following Rozovskaya and Roth (2010c), we dis-5http://jazzy.sourceforge.net/6http://www.tysto.com/articles05/q1/20050324uk-us.shtmltinguish between two training paradigms in ESL er-ror correction, depending on whether the author?soriginal word choice is used in training as a feature.In the standard context-sensitive spelling correctionparadigm, the decision of the classifier depends onlyon the context around the author?s word, e.g.
arti-cle or preposition, and the author?s word itself is nottaken into consideration in training.Mistakes made by non-native speakers obey cer-tain regularities (Lee and Seneff, 2008; Rozovskayaand Roth, 2010a).
Adding knowledge about typ-ical errors to a model significantly improves itsperformance (Gamon, 2010; Rozovskaya and Roth,2010c; Dahlmeier and Ng, 2011).
Typical errorsmay refer both to speakers whose first language isL1 and to specific authors.
For example, non-nativespeakers whose first language does not have articlestend to make more articles errors in English (Ro-zovskaya and Roth, 2010a).Since non-native speakers?
mistakes are system-atic, the author?s word choice (the source word)carries a lot of information.
Models that use thesource word in training (Han et al, 2010; Gamon,2010; Dahlmeier and Ng, 2011) learn which errorsare typical for the learner and thus significantly out-perform systems that only look at context.
We callthese models adapted.
Training adapted models re-quires annotated data, since in native English datathe source word is always correct and thus cannot beused by the classifier.In this work, we use two methods of adapting amodel to typical errors that have been proposed ear-lier.
Both methods were originally developed formodels trained on native English data: they use asmall amount of annotated ESL data to generate er-ror statistics.
The artificial errors method is basedon generating artificial errors7 in correct native En-glish training data.
The method was implementedwithin the Averaged Perceptron (AP) algorithm (Ro-zovskaya and Roth, 2010c; Rozovskaya and Roth,2010b), a discriminative learning algorithm, and thisis the algorithm that we use in this work.
The NB-priors method is a special adaptation technique forthe Na?
?ve Bayes algorithm (Rozovskaya and Roth,2011).
While NB-priors improves both precision7For each task, only relevant errors are generated ?
for ex-ample, article mistakes for the article correction task.274and recall, the artificial errors approach suffersfrom low recall due to error sparsity (Sec.
6.1).In this work, in the preposition correction task,we use the NB-priors method without modifications(as described in the original paper).
We use the ar-tificial errors approach both for article and prepo-sition error correction but with two important mod-ifications: we train on annotated ESL data insteadof native data, and use the proposed error inflationmethod (described in Section 6) to increase the errorrate in training.6 Error InflationIn this section, we show why AP (Freund andSchapire, 1999), a discriminative classifier, is sen-sitive to the error sparsity of the data, and proposea method that addresses the problems raised by thissensitivity.6.1 Error Sparsity and Low RecallThe low recall of the AP algorithm is related to thenature of the error correction tasks, which exhibitlow error rates.
Even for ESL writers, over 90% oftheir preposition and article usage is correct, whichmakes the errors very sparse (Rozovskaya and Roth,2010c).
The low recall problem is, in fact, a specialcase of a more general problem where there is oneor a small group of dominant features that are verystrongly correlated with the label.
In this case, thesystem tends to predict the label that matches thisfeature, and tends to not predict it when that fea-ture is absent.
In error correction, which tends tohave a very skewed label distribution, this results invery few errors being detected by the system: whentraining on annotated data with naturally occurringerrors and using the source word as a feature, thesystem will learn that in the majority of cases thesource word corresponds to the label, and will tendto over-predict it, which will result in low recall.In the artificial errors approach, errors are sim-ulated according to real observed mistakes.
Ta-ble 1 shows a sample confusion matrix based onpreposition mistakes in the FCE corpus; we showfour rows, but the entire table contains 17 rows andcolumns, one for each preposition, and each entryshows Prob(pi|pj), the probability that the author?spreposition is pi given that the correct prepositionis pj .
The matrix also shows the preposition countfor each source and label in the data set.
Given theentire matrix and the counts, it is also possible togenerate the matrix in the other direction and obtainProb(pj |pi), the probability that the correct prepo-sition is pj given that the author?s preposition is pi.This other matrix is used for adapting NB with thepriors method.The confusion matrix is sparse and shows that thedistribution of alternatives for each source preposi-tion is very different from that of the others.
Thisstrongly suggests that these errors are systematic.Additionally, most prepositions are used correctly,so the error rate is very low (the error rate can beestimated by looking at the matrix diagonal in thetable; for example, the error rate for the prepositionabout is lower than for into, since 94.4% of the oc-currences of label about are correct, but only 76.8%of label into are correct).The artificial errors thus model the two proper-ties that we mentioned: the confusability of differ-ent preposition pairs and the low error rate, and theartificial errors are similarly sparse.6.2 The Error Inflation MethodTwo extreme choices for solving the low recall prob-lem due to error sparsity are: (1) training without thesource word feature or (2) training with this feature,where the classifier relies on it too much.
Modelstrained without the source feature have very poorprecision.
While the NB-priors method does havegood recall, our expectation is that with the right ap-proach, a discriminative classifier will also improverecall, but maintain higher precision as well.We wish to reduce the confidence that the systemhas in the source word, while preserving the knowl-edge the model has about likely confusions and con-texts of confused words.
To accomplish this, we re-duce the proportion of correct examples, i.e.
exam-ples where the source and the label are the same,by some positive constant < 1.0 and distribute theextra probability mass among the typical errors inan appropriate proportion by generating additionalerror examples.
This inflates the proportion of ar-tificial errors in the training data, and hence the er-ror rate, while keeping the probability distributionamong likely corrections the same.
Increasing theerror rate improves the recall, while the typical er-275Label Sourceson about into with as at by for from in of over to(648) (700) (54) (733) (410) (880) (243) (1394) (515) (2213) (1954) (98) (1418)on (598) 0.846 0.003 0.003 0.008 0.013 - 0.003 0.022 - 0.076 0.013 0.001 0.009about (686) 0.004 0.944 - 0.007 - - - 0.022 0.005 0.002 0.016 0.001 -into (55) 0.001 - 0.768 - - - 0.011 0.011 - 0.147 - - 0.053with (710) 0.001 0.006 - 0.934 - 0.001 0.007 0.004 0.001 0.027 0.003 - 0.015Table 1: Confusion matrix for preposition errors.
Based on data from the FCE corpus for top 17 most frequent Englishprepositions.
The left column shows the correct preposition.
Each row shows the author?s preposition choices for that label andProb(source|label).
The sources among, between, under and within are not shown for lack of space; they all have 0 probabilitiesin the matrix.
The numbers next to the targets show the count of the label (or source) in the data set.ror knowledge ensures that high precision is main-tained.
This method causes the classifier to rely onthe source feature less and increases the contribu-tion of the features based on context.
The learningalgorithm therefore finds a more optimal balance be-tween the source feature and the context features.Algorithm 1 shows the pseudo-code for generat-ing training data; it takes as input training examples,the confusion matrix CM as shown in Table 1, andthe inflation constant, and generates artificial sourcefeatures for correct training examples.8 An infla-tion constant value of 1.0 simulates learner mistakeswithout inflation.
Table 2 shows the proportion ofartificial errors created in training using the inflationmethod for different inflation rates.Algorithm 1 Data Generation with InflationInput: Training examples E with correct sources, confusion matrixCM , inflation constant COutput: Training examples E with artificial errorsfor Example e in E doInitialize lab?
e.label, e.source?
e.labelRandomize targets ?
CM [lab]Initialize flag?
Falsefor target t in targets doif flag equals True thenBreakend ifif t equals lab thenProb(t) = CM [lab][t] ?
CelseProb(t) = 1.0?CM [lab][lab]?C1.0?CM [lab][lab] ?
CM [lab][t]end ifx?
Random[0, 1]if x < Prob(t) thene.source?
tflag?
Trueend ifend forend forreturn E8When training on native English data, all examples are cor-rect.
When training on annotated learner data, some exampleswill contain naturally occurring mistakes.Inflation rate1.0 (Regular) 0.9 0.8 0.7 0.6 0.57.7% 15.1% 22.6% 30.1% 37.5% 45.0%Table 2: Artificial errors.
Proportion of generated artificialpreposition errors in training using the inflation method (basedon the FCE corpus).7 DeterminersTable 4 shows the distribution of determiner errorsin the HOO training set.
Even though the majorityof determiner errors involve article mistakes, 14% oferrors are personal and possessive pronouns.9 Mostof the determiner errors involve omitting an article.Similar error patterns have been observed in otherESL corpora (Rozovskaya and Roth, 2010a).Our system focuses on article errors.
Becausethe majority of determiner errors are omissions, it isvery important to target this subset of mistakes.
Oneapproach would be to consider every space as a pos-sible article insertion point.
However, this methodwill likely produce a lot of noise.
The standardapproach is to consider noun-phrase-initial contexts(Han et al, 2006; Rozovskaya and Roth, 2010c).Error type ExampleRepl.
15.7% ?Can you send me the*/a letter back writingwhat happened to you recently.?Omis.
57.5% ?Nowadays ?
*/the Internet makes us closer andcloser.?Unnec.
26.8% ?One of my hobbies is the*/?
photography.
?Table 4: Distribution of determiner errors in the HOOtraining data.9e.g.
?Pat apologized to me for not keeping the*/my secrets.
?276Feature Type DescriptionWord n-grams wB, w2B, w3B, wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB, w2BwBwA, wBwAw2A,wAw2Aw3A, w4Bw3Bw2BwB, w3w2BwBwA, w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4APOS features pB, p2B, p3B , pA, p2A, p3A, pBpA, p2BpB, pAp2A, pBwB, pAwA, p2Bw2B, p2Aw2A, p2BpBpA, pBpAp2A,pAp2Ap3ANP1 headWord, npWords, NC, adj&headWord, adjTag&headWord, adj&NC, adjTag&NC, npTags&headWord, npTags&NCNP2 headWord&headPOS, headNumberwordsAfterNP headWord&wordAfterNP, npWords&wordAfterNP, headWord&2wordsAfterNP, npWords&2wordsAfterNP,headWord&3wordsAfterNP, npWords&3wordsAfterNPwordBeforeNP wB&fi ?i ?
NP1Verb verb, verb&fi ?i ?
NP1Preposition prep&fi ?i ?
NP1Table 3: Features used in the article error correction system.
wB and wA denote the word immediately before and afterthe target, respectively; and pB and pA denote the POS tag before and after the target.
headWord denotes the head of the NPcomplement.
NC stands for noun compound and is active if second to last word in the NP is tagged as a noun.
Verb features areactive if the NP is the direct object of a verb.
Preposition features are active if the NP is immediately preceded by a preposition.
adjfeature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective.
npWords and npTags denoteall words (POS tags) in the NP.7.1 Determiner FeaturesThe features are presented in Table 3.
The modelalso uses the source article as a feature.7.2 Training the Determiner SystemModel Detection CorrectionAP (natural errors) 30.75 28.97AP (inflation) 34.62 32.02Table 5: Article development results: AP with inflation.
Theperformance shows the F-Score for the 244 held-out documentsof the original FCE data set.
AP with inflation uses the constantvalue of 0.8.The article classifier is based on the artificial er-rors approach (Rozovskaya and Roth, 2010c).
Theoriginal method trains a system on native Englishdata.
The current setting is different, since the FCEcorpus contains annotated learner errors.
Since theerrors are sparse, we use the error inflation method(Section 6.2) to boost the proportion of errors intraining using the error distribution obtained fromthe same training set.
The effectiveness of thismethod is demonstrated by the system performance:we obtain the top or second result in every metric.Note also that the article system does not use addi-tional data for training.Table 5 compares the performance of the systemtrained on natural errors with the performance of thesystem trained with the inflation method.
We foundthat any value of the inflation constant between 0.9and 0.5 will give a boost in performance.
We useseveral values; the top determiner model uses the in-flation constant of 0.8.8 PrepositionsTable 6 shows the distribution of the three types ofpreposition errors in the HOO training data.
TheFCE annotation distinguishes between prepositionmistakes and errors involving the infinitive markerto, e.g.
?He wants ?
*/to go there.
?, which are anno-tated as verb errors.
Since in the competition onlyarticle and preposition annotations are kept, theseerrors are not annotated, and thus we do not targetthese mistakes.Error type ExampleRepl.
57.9% ?I can see at*/on the list a lot of interestingsports.?Omis.
24.0% ?I will be waiting ?
*/for your call.?Unnec.
18.1% ?Despite of */?
being tiring , it was rewarding?Table 6: Distribution of preposition errors in the HOOtraining data.To detect missing preposition errors, we use a setof rules, mined from the training data, to identifypossible locations where a preposition might havebeen incorrectly omitted.
Below we show examplesof such contexts.?
?I will be waiting ?
*/for your call.??
?But now we use planes to go ?
*/to far places.
?8.1 Preposition FeaturesAll features used in the preposition module are lex-ical: word n-grams in the 4-word window around277Feature Type DescriptionWord n-ngram features in the 4-word windowaround the targetwB, w2B, w3B , wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB,w2BwBwA, wBwAw2A, wAw2Aw3A, w4Bw3Bw2BwB, w3w2BwBwA,w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4APreposition complement features compHead, wB&compHead, w2BwB&compHeadTable 7: Features used in the preposition error correction system.
wB and wA denote the word immediately before andafter the target, respectively; the other features are defined similarly.
compHead denotes the head of the preposition complement.wB&compHead, w2BwB&compHead are feature conjunctions of compHead with wB and w2BwB, respectively.the target preposition, and three features that use thehead of the preposition complement (see Table 7).The NB-priors classifier, which is part of our model,can only make use of the word n-gram features; ituses n-gram features of lengths 3, 4, and 5.
AP istrained on the HOO data and uses n-grams of lengths2, 3, and 4, the head complement features, and theauthor?s preposition as a feature.Model Detection CorrectionAP (inflation) 34.64 27.51NB-priors 38.76 26.57Combined 41.27 29.35Table 8: Preposition development results: performance ofindividual and combined systems.
The performance showsthe F-Score for the 244 held-out documents of the original FCEdata set.8.2 Training the Preposition SystemWe train two systems.
The first one is an AP modeltrained on the FCE data with inflation (similar tothe article system).
Correcting preposition errors re-quires more data to achieve performance compara-ble to article error correction, due to the task com-plexity (Gamon, 2010).
Moreover, given that thedevelopment and test data are quite different,10 itmakes sense to use a model that is independent ofthose, to avoid overfitting.
We combine the APmodel with a model trained on native English data.Our second system is an NB-priors classifier trainedon the the Google Web 1T 5-gram corpus (Brantsand Franz, 2006).
We use training data to replace theprior parameters of the model (see Rozovskaya andRoth, 2011 for more detail).
The NB-priors modeldoes not target preposition omissions.10The data contains essays written on prompts, so that thetraining data may contain several essays written on the sameprompt and thus will be very similar in content.
In contrast,we expected that the test data will likely contain essays on adifferent set of prompts.The NB-priors model outperforms the AP classi-fier.
The two models are also very different due tothe different learning algorithms and the type of thedata used in training.
Our final preposition modelis thus a combination of these two, where we takeas the base the decisions of the NB-priors classifierand add the AP model predictions for cases whenthe base model does not flag a mistake.
Table 8shows the results.
The combined model improvesboth the detection and correction scores.
Our prepo-sition system ranked first in detection and recogni-tion and second in correction.Model Detection CorrectionAP (natural errors) 13.50 12.73AP (inflation) 21.31 32.02Table 9: Preposition development results: AP with infla-tion.
The performance shows the F-Score for the 244 held-outdocuments of the original FCE data set.
AP with inflation usesthe constant value of 0.7.9 Test PerformanceA number of revisions were made to the test databased on the input from the participating teams af-ter the initial results were obtained, where each teamsubmitted proposed edits to correct annotation mis-takes.
We show both results.Table 10 shows results before the revisions weremade.
Row 1 shows the performance of the de-terminer system for the three metrics.
This systemachieved the best score in correction, and the secondbest scores in detection and recognition.
The systemis described in Section 7.2, with the exception thatthe final system for the article correction is trainedon the entire FCE data set.Table 10 (row 2) presents the results on prepo-sition error correction.
The system is described inSection 8.2 and is a combined model of AP trainedwith inflation on the FCE data set and NB-priorsmodel trained on the Google Web 1T corpus.
The278Model Detection Recognition CorrectionPrecision Recall F-Score Precision Recall F-Score Precision Recall F-ScoreArticles 40.00 37.79 38.862 38.05 35.94 36.972 35.61 33.64 34.601Prepositions 38.21 45.34 41.471 31.05 40.25 35.061 20.36 24.15 22.092Combined 37.22 43.71 40.201 34.23 36.64 35.391 26.39 28.26 27.292Table 10: Performance on test before revisions.
Results are shown before revisions were made to the data.
The rank of thesystem is shown as a superscript.Model Detection Recognition CorrectionPrecision Recall F-Score Precision Recall F-Score Precision Recall F-ScoreArticles 43.90 39.30 41.472 45.98 34.93 39.702 41.46 37.12 39.172Prepositions 41.43 47.54 44.271 37.14 42.62 39.691 26.79 30.74 28.632Combined 43.56 42.92 43.241 38.97 39.96 39.462 32.58 33.40 32.992Table 11: Performance on test after revisions.
Results are shown after revisions were made to the data.
The rank of the systemis shown as a superscript.preposition system achieved the best scores in detec-tion and recognition, scoring second in correction.Row 3 shows the performance of the combinedsystem.
This system was ranked first in detectionand recognition, and second in correction.Table 11 shows our performance after the revi-sions were applied.10 DiscussionThe HOO 2012 shared task follows the HOO 2011pilot shared task (Dale and Kilgarriff, 2011), wherethe data was fully corrected and error-tagged andthe participants could address any types of mistakes.The current task allows for comparison of individ-ual systems for each error type considered.
This isimportant, since to date it has been difficult to com-pare different systems due to the lack of a bench-mark data set.The data used for the shared task has many errorsbesides the preposition and determiner errors; theannotations for these have been removed.
One un-desirable consequence of this approach is that somecomplex errors that involve either an article or apreposition mistake but depend on other correctionson neighboring words, e.g.
a noun of a verb, mayresult in ungrammatical sequences.Clearly, the task of annotating all requisite correc-tions is a daunting task, and it is preferable to iden-tify subsets of these corrections that can be tackledsomewhat independently of the rest, and these morecomplex cases present a problem.To address these conflicting needs, we proposethat the scope of all ?final?
corrections be marked,without necessarily specifying all individual correc-tions necessary to transform the original text intocorrect English.
Edits that plausibly require correc-tions to their context to resolve correctly could thenbe treated as out of scope, and ignored by spellingcorrection systems even though in other contexts,those same edits would be in scope.11 ConclusionWe have demonstrated how a competitive system forpreposition and determiner error correction can bebuilt using techniques that address the error sparsityof the data and the overfitting problem.
We built onour previous work and presented the error inflationmethod that can be applied to the earlier proposedartificial errors approach to boost recall.
Our de-terminer system used error inflation and trained amodel using only the annotated FCE corpus.
Ourpreposition system combined the FCE-trained sys-tem with a native-data model that was adapted tolearner errors, using the NB-priors approach pro-posed earlier.
Both of the systems showed compet-itive performance, scoring first or second in everytask ranking.AcknowledgmentsThe authors thank Jeff Pasternack for his assistance and VivekSrikumar for helpful feedback.
This research is supported bya grant from the U.S. Department of Education and is partlysupported by the Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-018.ReferencesM.
Banko and E. Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
In Proc.279of 39th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 26?33, Toulouse,France, July.T.
Brants and A. Franz.
2006.
Web 1T 5-gram Version 1.Linguistic Data Consortium, Philadelphia, PA.A.
Carlson and I. Fette.
2007.
Memory-based context-sensitive spelling correction at web scale.
In Proc.
ofthe IEEE International Conference on Machine Learn-ing and Applications (ICMLA).A.
Carlson, J. Rosen, and D. Roth.
2001.
Scaling upcontext sensitive text correction.
In Proceedings of theNational Conference on Innovative Applications of Ar-tificial Intelligence (IAAI), pages 45?50.D.
Dahlmeier and H. T. Ng.
2011.
Grammatical er-ror correction with alternating structure optimization.In Proc.
of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (ACL), pages 915?923, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.R.
Dale and A. Kilgarriff.
2011.
Helping Our Own:The HOO 2011 pilot shared task.
In Proc.
of the 13thEuropean Workshop on Natural Language Generation(ENLG), pages 242?249, Nancy, France.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
A re-port on the preposition and determiner error correctionshared task.
In Proc.
of the NAACL HLT 2012 Sev-enth WorkshopWorkshop on Innovative Use of NLP forBuilding Educational Applications, Montreal, Canada,June.
Association for Computational Linguistics.Y.
Freund and R. E. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37(3):277?296.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing.
In Proc.
of the 2010 An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (NAACL), pages 163?171, LosAngeles, California, June.A.
R. Golding and D. Roth.
1996.
Applying Win-now to context-sensitive spelling correction.
In Proc.of the International Conference on Machine Learning(ICML), pages 182?190.A.
R. Golding and D. Roth.
1999.
A Winnow basedapproach to context-sensitive spelling correction.
Ma-chine Learning, 34(1-3):107?130.N.
Han, M. Chodorow, and C. Leacock.
2006.
Detectingerrors in English article usage by non-native speakers.Journal of Natural Language Engineering, 12(2):115?129.N.
Han, J. Tetreault, S. Lee, and J. Ha.
2010.
Us-ing an error-annotated learner corpus to develop andESL/EFL error correction system.
In Proc.
of the Sev-enth conference on International Language Resourcesand Evaluation (LREC), Valletta, Malta, May.
Euro-pean Language Resources Association (ELRA).C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault.2010.
Automated Grammatical Error Detection forLanguage Learners.
Morgan and Claypool Publish-ers.J.
Lee and S. Seneff.
2008.
An analysis of grammaticalerrors in non-native speech in English.
In Proc.
of the2008 Spoken Language Technology Workshop, Goa.V.
Punyakanok and D. Roth.
2001.
The use of classi-fiers in sequential inference.
In The Conference onAdvances in Neural Information Processing Systems(NIPS), pages 995?1001.
MIT Press.N.
Rizzolo and D. Roth.
2010.
Learning based java forrapid development of nlp systems.
In Proceedings ofthe International Conference on Language Resourcesand Evaluation (LREC), Valletta, Malta, 5.A.
Rozovskaya and D. Roth.
2010a.
Annotating ESL er-rors: Challenges and rewards.
In Proc.
of the NAACLHLT 2010 Fifth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 28?36,Los Angeles, California, June.
Association for Com-putational Linguistics.A.
Rozovskaya and D. Roth.
2010b.
Generating confu-sion sets for context-sensitive error correction.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages961?970, Cambridge, MA, October.
Association forComputational Linguistics.A.
Rozovskaya and D. Roth.
2010c.
Training paradigmsfor correcting errors in grammar and usage.
In Proc.
ofthe Annual Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL), pages 154?162, Los Angeles, California, June.
Association forComputational Linguistics.A.
Rozovskaya and D. Roth.
2011.
Algorithm selec-tion and model adaptation for ESL correction tasks.In Proc.
of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (ACL), pages 924?933, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.A.
Rozovskaya, M. Sammons, J. Gioja, and D. Roth.2011.
University of Illinois system in HOO text cor-rection shared task.
In Proc.
of the 13th EuropeanWorkshop on Natural Language Generation (ENLG).H.
Yannakoudakis, T. Briscoe, and B. Medlock.
2011.
Anew dataset and method for automatically grading esoltexts.
In Proc.
of the 49th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages180?189, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.280
