Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 356?366,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsUser Edits Classification Using Document Revision HistoriesAmit BronnerInformatics InstituteUniversity of Amsterdama.bronner@uva.nlChristof MonzInformatics InstituteUniversity of Amsterdamc.monz@uva.nlAbstractDocument revision histories are a usefuland abundant source of data for naturallanguage processing, but selecting relevantdata for the task at hand is not trivial.In this paper we introduce a scalable ap-proach for automatically distinguishing be-tween factual and fluency edits in documentrevision histories.
The approach is basedon supervised machine learning using lan-guage model probabilities, string similar-ity measured over different representationsof user edits, comparison of part-of-speechtags and named entities, and a set of adap-tive features extracted from large amountsof unlabeled user edits.
Applied to con-tiguous edit segments, our method achievesstatistically significant improvements overa simple yet effective edit-distance base-line.
It reaches high classification accuracy(88%) and is shown to generalize to addi-tional sets of unseen data.1 IntroductionMany online collaborative editing projects such asWikipedia1 keep track of complete revision histo-ries.
These contain valuable information about theevolution of documents in terms of content as wellas language, style and form.
Such data is publiclyavailable in large volumes and constantly grow-ing.
According to Wikipedia statistics, in August2011 the English Wikipedia contained 3.8 millionarticles with an average of 78.3 revisions per ar-ticle.
The average number of revision edits permonth is about 4 million in English and almost 11million in total for all languages.21http://www.wikipedia.org2Average for the 5 years period between August 2006and August 2011.
The count includes edits by registeredExploiting document revision histories hasproven useful for a variety of natural languageprocessing (NLP) tasks, including sentence com-pression (Nelken and Yamangil, 2008; Yamangiland Nelken, 2008) and simplification (Yatskar etal., 2010; Woodsend and Lapata, 2011), informa-tion retrieval (Aji et al 2010; Nunes et al 2011),textual entailment recognition (Zanzotto and Pen-nacchiotti, 2010), and paraphrase extraction (Maxand Wisniewski, 2010; Dutrey et al 2011).The ability to distinguish between factualchanges or edits, which alter the meaning, and flu-ency edits, which improve the style or readability,is a crucial requirement for approaches exploit-ing revision histories.
The need for an automatedclassification method has been identified (Nelkenand Yamangil, 2008; Max and Wisniewski, 2010),but to the best of our knowledge has not been di-rectly addressed.
Previous approaches have eitherapplied simple heuristics (Yatskar et al 2010;Woodsend and Lapata, 2011) or manual annota-tions (Dutrey et al 2011) to restrict the data tothe type of edits relevant to the NLP task at hand.The work described in this paper shows that it ispossible to automatically distinguish between fac-tual and fluency edits.
This is very desirable asit does not rely on heuristics, which often gener-alize poorly, and does not require manual anno-tation beyond a small collection of training data,thereby allowing for much larger data sets of re-vision histories to be used for NLP research.In this paper, we make the following novel con-tributions:We address the problem of automated classi-fication of user edits as factual or fluency editsusers, anonymous users, software bots and reverts.
Source:http://stats.wikimedia.org.356by defining the scope of user edits, extracting alarge collection of such user edits from the En-glish Wikipedia, constructing a manually labeleddataset, and setting up a classification baseline.A set of features is designed and integrated intoa supervised machine learning framework.
It iscomposed of language model probabilities andstring similarity measured over different represen-tations, including part-of-speech tags and namedentities.
Despite their relative simplicity, the fea-tures achieve high classification accuracy whenapplied to contiguous edit segments.We go beyond labeled data and exploit largeamounts of unlabeled data.
First, we demonstratethat the trained classifier generalizes to thousandsof examples identified by user comments as spe-cific types of fluency edits.
Furthermore, we in-troduce a new method for extracting features froman evolving set of unlabeled user edits.
Thismethod is successfully evaluated as an alternativeor supplement to the initial supervised approach.2 Related WorkThe need for user edits classification is implicit instudies of Wikipedia edit histories.
For example,Viegas et al(2004) use revision size as a simpli-fied measure for the change of content, and Kitturet al(2007) use metadata features to predict useredit conflicts.Classification becomes an explicit requirementwhen exploiting edit histories for NLP research.Yamangil and Nelken (2008) use edits as train-ing data for sentence compression.
They makethe simplifying assumption that all selected editsretain the core meaning.
Zanzotto and Pennac-chiotti (2010) use edits as training data for textualentailment recognition.
In addition to manuallylabeled edits, they use Wikipedia user commentsand a co-training approach to leverage unlabelededits.
Woodsend and Lapata (2011) and Yatskaret al(2010) use Wikipedia comments to identifyrelevant edits for learning sentence simplification.The work by Max and Wisniewski (2010) isclosely related to the approach proposed in thispaper.
They extract a corpus of rewritings, dis-tinguish between weak semantic differences andstrong semantic differences, and present a typol-ogy of multiple subclasses.
Spelling correctionsare heuristically identified but the task of auto-matic classification is deferred.
Follow-up workby Dutrey et al(2011) focuses on automatic para-phrase identification using a rule based approachand manually annotated examples.Wikipedia vandalism detection is a user ed-its classification problem addressed by a yearlycompetition (since 2010) in conjunction with theCLEF conference (Potthast et al 2010; Potthastand Holfeld, 2011).
State-of-the-art solutions in-volve supervised machine learning using variouscontent and metadata features.
Content featuresuse spelling, grammar, character- and word-levelattributes.
Many of them are relevant for our ap-proach.
Metadata features allow detection by pat-terns of usage, time and place, which are gener-ally useful for the detection of online maliciousactivities (West et al 2010; West and Lee, 2011).We deliberately refrain from using such features.A wide range of methods and approaches hasbeen applied to the similar tasks of textual en-tailment and paraphrase recognition, see Androut-sopoulos and Malakasiotis (2010) for a compre-hensive review.
These are all related becauseparaphrases and bidirectional entailments repre-sent types of fluency edits.A different line of research uses classifiers topredict sentence-level fluency (Zwarts and Dras,2008; Chae and Nenkova, 2009).
These could beuseful for fluency edits detection.
Alternatively,user edits could be a potential source of human-produced training data for fluency models.3 Definition of User Edits ScopeWithin our approach we distinguish between editsegments, which represent the comparison (diff)between two document revisions, and user edits,which are the input for classification.An edit segment is a contiguous sequence ofdeleted, inserted or equal words.
The differencebetween two document revisions (vi, vj) is repre-sented by a sequence of edit segments E. Eachedit segment (?, wm1 ) ?
E is a pair, where ?
?
{deleted , inserted , equal} and wm1 is a m-wordsubstring of vi, vj or both (respectively).A user edit is a minimal set of sentences over-lapping with deleted or inserted segments.
Giventhe two sets of revision sentences (Svi , Svj ), let?
(?, wm1 ) = {s ?
Svi ?
Svj | wm1 ?
s 6= ?}
(1)be the subset of sentences overlapping with agiven edit segment, and let?
(s) = {(?, wm1 ) ?
E | wm1 ?
s 6= ?}
(2)357be the subset of edit segments overlapping with agiven sentence.A user edit is a pair (pre ?
Svi , post ?
Svj )where?s ?
pre ?
post ??
?
{deleted , inserted} ?wm1(?, wm1 ) ?
?(s)?
?
(?, wm1 ) ?
pre ?
post (3)?s ?
pre ?
post ??
?
{deleted , inserted} ?wm1(?, wm1 ) ?
?
(s) (4)Table 1 illustrates different types of edit seg-ments and user edits.
The term replaced segmentrefers to adjacent deleted and inserted segments.Example (1) contains a replaced segment becausethe deleted segment (?1700s?)
is adjacent to theinserted segment (?18th century?).
Example (2)contains an inserted segment (?and largest profes-sional?
), a replaced segment (?(est.?
?
?estab-lished in?)
and a deleted segment (?)?).
User editsof both examples consist of a single pre sentenceand a single post sentence because deleted and in-serted segments do not cross any sentence bound-ary.
Example (3) contains a replaced segment (?.He?
?
?who?).
In this case the deleted segment(?.
He?)
overlaps with two sentences and there-fore the user edit consists of two pre sentences.4 Features for Edits ClassificationWe design a set of features for supervised classi-fication of user edits.
The design is guided by twomain considerations: simplicity and interoperabil-ity.
Simplicity is important because there are po-tentially hundreds of millions of user edits to beclassified.
This amount continues to grow at rapidpace and a scalable solution is required.
Interop-erability is important because millions of user ed-its are available in multiple languages.
Wikipediais a flagship project, but there are other collabora-tive editing projects.
The solution should prefer-ably be language- and project-independent.
Con-sequently, we refrain from deeper syntactic pars-ing, Wikipedia-specific features, and language re-sources that are limited to English.Our basic intuition is that longer edits are likelyto be factual and shorter edits are likely to befluency edits.
The baseline method is thereforecharacter-level edit distance (Levenshtein, 1966)between pre- and post-edited text.Six feature categories are added to the baseline.Most features take the form of threefold counts re-ferring to deleted, inserted and equal elements of(1) Revisions 368209202 & 378822230pre (?By the mid 1700s, Medzhybizh was the seat ofpower in Podilia Province.?
)post (?By the mid 18th century, Medzhybizh wasthe seat of power in Podilia Province.?
)diff (equal , ?By the mid?)
, (deleted, ?1700s?)
,(inserted , ?18th century?)
, (equal , ?, Medzhy-bizh was the seat of power in Podilia Province.?
)(2) Revisions 148109085 & 149440273pre (?Original Society of Teachers of the AlexanderTechnique (est.
1958).?
)post (?Original and largest professional Society ofTeachers of the Alexander Technique estab-lished in 1958.?
)diff (equal , ?Original?)
, (inserted , ?and largestprofessional?)
, (equal , ?Society of Teachers ofthe Alexander Technique?)
, (deleted , ?(est.?)
,(inserted , ?
established in?)
, (equal , ?1958?)
,(deleted , ?)?)
, (equal , ?.?
)(3) Revisions 61406809 & 61746002pre (?Fredrik Modin is a Swedish ice hockey leftwinger.?
, ?He is known for having one of thehardest slap shots in the NHL.?
)post (?Fredrik Modin is a Swedish ice hockey leftwinger who is known for having one of the hard-est slap shots in the NHL.?
)diff (equal , ?Fredrik Modin is a Swedish ice hockeyleft winger?)
, (deleted , ?.
He?)
, (inserted ,?who?)
, (equal , ?is known for having one ofthe hardest slap shots in the NHL.?
)Table 1: Examples of user edits and the corre-sponding edit segments (revision numbers corre-spond to the English Wikipedia).each user edit.
For instance, example (1) in Table1 has one deleted token, two inserted tokens and14 equal tokens.
Many features use string similar-ity calculated over alternative representations.Character-level features include counts ofdeleted, inserted and equal characters of differenttypes, such as word & non-word characters or dig-its & non-digits.
Character types may help iden-tify edits types.
For example, the change of dig-its may suggest a factual edit while the change ofnon-word characters may suggest a fluency edit.Word-level features count deleted, insertedand equal words using three parallel represen-tations: original case, lower case, and lemmas.Word-level edit distance is calculated for eachrepresentation.
Table 2 illustrates how edit dis-tance may vary across different representations.358Rep.
User Edit DistWords pre Branch lines were built in Kenya 4post A branch line was built in KenyaLowcase pre branch lines were built in kenya 3post a branch line was built in kenyaLemmas pre branch line be build in Kenya 1post a branch line be build in KenyaPoS tags pre NN NNS VBD VBN IN NNP 2post DT NN NN VBD VBN IN NNPNE tags pre LOCATION 0post LOCATIONTable 2: Word- and tag-level edit distance mea-sured over different representations (examplefrom Wikipedia revisions 2678278 & 2682972).Fluency edits may shift words, which sometimesmay be slightly modified.
Fluency edits may alsoadd or remove words that already appear in con-text.
Optimal calculation of edit distance withshifts is computationally expensive (Shapira andStorer, 2002).
Translation error rate (TER) pro-vides an approximation but it is designed for theneeds of machine translation evaluation (Snoveret al 2006).
To have a more sensitive estima-tion of the degree of edit, we compute the minimalcharacter-level edit distance between every pair ofwords that belong to different edit segments.
Foreach pair of edit segments (?, wm1 ), (?
?, w?k1) over-lapping with a user edit, if ?
6= ??
we compute:?w ?
wm1 : minw??w?k1EditDist(w,w?)
(5)Binned counts of the number of words with a min-imal edit distance of 0, 1, 2, 3 or more charac-ters are accumulated per edit segment type (equal,deleted or inserted).Part-of-speech (PoS) features include countsof deleted, inserted and equal PoS tags (per tag)and edit distance at the tag level between PoS tagsbefore and after the edit.
Similarly, named-entity(NE) features include counts of deleted, insertedand equal NE tags (per tag, excluding OTHER)and edit distance at the tag level between NE tagsbefore and after the edit.
Table 2 illustrates theedit distance at different levels of representation.We assume that a deleted NE tag, e.g.
PERSONor LOCATION, could indicate a factual edit.
Itcould however be a fluency edit where the NE isreplaced by a co-referent like ?she?
or ?it?.
Evenif we encounter an inserted PRP PoS tag, the fea-tures do not capture the explicit relation betweenthe deleted NE tag and the inserted PoS tag.
Thisis an inherent weakness of these features whencompared to parsing-based alternatives.An additional set of counts, NE values, de-scribes the number of deleted, inserted and equalnormalized values of numeric entities such asnumbers and dates.
For instance, if the word?100?
is replaced by ?200?
and the respective nu-meric values 100.0 and 200.0 are normalized, thecounts of deleted and inserted NE values will beincremented and suggest a factual edit.
If on theother hand ?100?
is replaced by ?hundred?
and thelatter is normalized as having the numeric value100.0, then the count of equal NE values will beincremented, rather suggesting a fluency edit.Acronym features count deleted, inserted andequal acronyms.
Potential acronyms are extractedfrom word sequences that start with a capital letterand from words that contain multiple capital let-ters.
If, for example, ?UN?
is replaced by ?UnitedNations?, ?MicroSoft?
by ?MS?
or ?Jean Pierre?by ?J.P?, the count of equal acronyms will be in-cremented, suggesting a fluency edit.The last category, language model (LM) fea-tures, takes a different approach.
These featureslook at n-gram based sentence probabilities be-fore and after the edit, with and without normal-ization with respect to sentence lengths.
The ratioof the two probabilities, P?ratio(pre, post) is com-puted as follows:P?
(wm1 ) ?m?i=1P (wi|wi?1i?n+1) (6)P?norm(wm1 ) = P?
(wm1 )1m (7)P?ratio(pre, post) =P?norm(post)P?norm(pre)(8)log P?ratio(pre, post) = logP?norm(post)P?norm(pre)(9)= log P?norm(post)?
log P?norm(pre)=1|post |log P?
(post)?1|pre|log P?
(pre)Where P?
is the sentence probability estimated asa product of n-gram conditional probabilities andP?norm is the sentence probability normalized bythe sentence length.
We hypothesize that the rel-ative change of normalized sentence probabilitiesis related to the edit type.
As an additional feature,the number of out of vocabulary (OOV) words be-fore and after the edit is computed.
The intuition359Dataset Labeled SubsetNumber of User Edits:923,820 (100%) 2,008 (100%)Edit Segments Distribution:Replaced 535,402 (57.96%) 1,259 (62.70%)Inserted 235,968 (25.54%) 471 (23.46%)Deleted 152,450 (16.5%) 278 (13.84%)Character-level Edit Distance Distribution:1 202,882 (21.96%) 466 (23.21%)2 81,388 (8.81%) 198 (9.86%)3-10 296,841 (32.13%) 645 (32.12%)11-100 342,709 (37.10%) 699 (34.81%)Word-level Edit Distance Distribution:1 493,095 (53.38%) 1,008 (54.18%)2 182,770 (19.78%) 402 (20.02%)3 77,603 (8.40%) 161 (8.02%)4-10 170,352 (18.44%) 357 (17.78%)Labels Distribution:Fluency - 1,008 (50.2%)Factual - 1,000 (49.8%)Table 3: Dataset of nearly 1 million user editswith single deleted, inserted or replaced segments,of which 2K are labeled.
The labels are almostequally distributed.
The distribution over edit seg-ment types and edit distance intervals is detailed.is that unknown words are more likely to be in-dicative of factual edits.5 Experiments5.1 Experimental SetupFirst, we extract a large amount of user edits fromrevision histories of the English Wikipedia.3 Theextraction process scans pairs of subsequent re-visions of article pages and ignores any revisionthat was reverted due to vandalism.
It parses theWikitext and filters out markup, hyperlinks, tablesand templates.
The process analyzes the clean textof the two revisions4 and computes the differencebetween them.5 The process identifies the overlapbetween edit segments and sentence boundariesand extracts user edits.
Features are calculatedand user edits are stored and indexed.
LM featuresare calculated against a large English 4-gram lan-3Dump of all pages with complete edit history as of Jan-uary 15, 2011 (342GB bz2), http://dumps.wikimedia.org.4Tokenization, sentence split, PoS & NE tags by StanfordCoreNLP, http://nlp.stanford.edu/software/corenlp.shtml.5Myers?
O(ND) difference algorithm (Myers, 1986),http://code.google.com/p/google-diff-match-patch.guage model built by SRILM (Stolcke, 2002) withmodified interpolated Kneser-Ney smoothing us-ing the AFP and Xinhua portions of the EnglishGigaword corpus (LDC2003T05).We extract a total of 4.3 million user edits ofwhich 2.52 million (almost 60%) are insertionsand deletions of complete sentences.
Althoughthese may include fluency edits such as sentencereordering or rewriting from scratch, we assumethat the large majority is factual.
Of the remaining1.78 million edits, the majority (64.5%) containssingle deleted, inserted or replaced segments.
Wedecide to focus on this subset because sentenceswith multiple non-contiguous edit segments aremore likely to contain mixed cases of unrelatedfactual and fluency edits, as illustrated by exam-ple (2) in Table 1.
Learning to classify contigu-ous edit segments seems to be a reasonable wayof breaking down the problem into smaller parts.We filter out user edits with edit distance longerthan 100 characters or 10 words that we assume tobe factual.
The resulting dataset contains 923,820user edits: 58% replaced segments, 25.5% in-serted segments and 16.5% deleted segments.Manual labeling of user edits is carried out bya group of annotators with near native or nativelevel of English.
All annotators receive the samewritten guidelines.
In short, fluency labels areassigned to edits of letter case, spelling, gram-mar, synonyms, paraphrases, co-referents, lan-guage and style.
Factual labels are assigned toedits of dates, numbers and figures, named enti-ties, semantic change or disambiguation, additionor removal of content.
A random set of 2,676 in-stances is labeled: 2,008 instances with a majorityagreement of at least two annotators are selectedas training set, 270 instances are held out as de-velopment set, 164 trivial fluency corrections of asingle letter?s case and 234 instances with no clearagreement among annotators are excluded.
Thelast group (8.7%) emphasizes that the task is, toa limited extent, subjective.
It suggests that auto-mated classification of certain user edits would bedifficult.
Nevertheless, inter-rater agreement be-tween annotators is high to very high.
Kappa val-ues between 0.74 to 0.84 are measured betweensix pairs of annotators, each pair annotated a com-mon subset of at least 100 instances.
Table 3 de-scribes the resulting dataset, which we also makeavailable to the research community.66Available for download at http://staff.360Character-level Edit Distance?
?
4 > 4?Fluency (725) Factual (821)Factual (179) Fluency (283)Figure 1: A decision tree that uses character-leveledit distance as a sole feature.
The tree correctlyclassifies 76% of the labeled user edits.Feature set SVM RF LogitBaseline 76.26% 76.26% 76.34%+ Char-level 83.71%?
84.45%?
84.01%?+ Word-level 78.38%??
81.38%??
78.13%?
?+ PoS 76.58%?
76.97% 78.35%?
?+ NE 82.71%?
83.12%?
82.38%?+ Acronyms 76.55% 76.61% 76.96%+ LM 76.20% 77.42% 76.52%All Features 87.14%??
87.14%?
85.64%?
?Table 4: Classification accuracy using the base-line, each feature set added to the baseline, andall features combined.
Statistical significance atp < 0.05 is indicated by ?
w.r.t the baseline (us-ing the same classifier), and by ?
w.r.t to anotherclassifier marked by ?
(using the same features).Highest accuracy per classifier is marked in bold.5.2 Feature AnalysisWe experiment with three classifiers: SupportVector Machines (SVM), Random Forests (RF)and Logistic Regression (Logit).7 SVMs (Cortesand Vapnik, 1995) and Logistic Regression (orMaximum Entropy classifiers) are two widelyused machine learning techniques.
SVMs havebeen applied to many text classification problems(Joachims, 1998).
Maximum Entropy classifiershave been applied to the similar tasks of para-phrase recognition (Malakasiotis, 2009) and tex-tual entailment (Hickl et al 2006).
RandomForests (Breiman, 2001) as well as other decisiontree algorithms are successfully used for classi-fying Wikipedia edits for the purpose of vandal-ism detection (Potthast et al 2010; Potthast andHolfeld, 2011).Experiments begin with the edit-distance base-science.uva.nl/?abronner/uec/data.7Using Weka classifiers: SMO (SVM), RandomForest &Logistic (Hall et al 2009).
Classifier?s parameters are tunedusing the held-out development set.Feature set SVM RF Logitflu.
/ fac.
flu.
/ fac.
flu.
/ fac.Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67+ Char-level 0.85 / 0.82 0.83 / 0.86 0.86 / 0.82+ Word-level 0.88 / 0.69 0.81 / 0.82 0.86 / 0.70+ PoS 0.85 / 0.68 0.78 / 0.76 0.84 / 0.72+ NE 0.86 / 0.79 0.79 / 0.87 0.87 / 0.78+ Acronyms 0.87 / 0.66 0.83 / 0.70 0.86 / 0.68+ LM 0.85 / 0.67 0.79 / 0.76 0.84 / 0.69All Features 0.88 / 0.86 0.86 / 0.88 0.87 / 0.84Table 5: Fraction of correctly classified edits pertype: fluency edits (left) and factual edits (right),using the baseline, each feature set added to thebaseline, and all features combined.line.
Then each one of the feature groups is sep-arately added to the baseline.
Finally, all featuresare evaluated together.
Table 4 reports the per-centage of correctly classified edits (classifiers?accuracy), and Table 5 reports the fraction of cor-rectly classified edits per type.
All results are for10-fold cross validation.
Statistical significanceagainst the baseline and between classifiers is cal-culated at p < 0.05 using paired t-test.The first interesting result is the highly predic-tive power of the single-feature baseline.
It con-firms the intuition that longer edits are mainly fac-tual.
Figure 1 shows that the edit distance of 72%of the user edits labeled as fluency is between 1 to4, while the edit distance of 82% of those labeledas factual is greater than 4.
The cut-off value isfound by a single-node decision tree that uses editdistance as a sole feature.
The tree correctly clas-sifies 76% of the instances.
This result impliesthat the actual challenge is to correctly classifyshort factual edits and long fluency edits.Character-level features and named-entity fea-tures lead to significant improvements over thebaseline for all classifiers.
Their strength lies intheir ability to identify short factual edits suchas changes of numeric values or proper names.Word-level features also significantly improve thebaseline but their contribution is smaller.
PoSand acronym features lead to small statistically-insignificant improvements over the baseline.The poor contribution of LM features is sur-prising.
It might be due to the limited contextof n-grams, but it might be that LM probabili-ties are not a good predictor for the task.
Re-moving LM features from the set of all features361Fluency Edits Misclassified as FactualEquivalent or redundant in context 14Paraphrases 13Equivalent numeric patterns 7Replacing first name with last name 4Acronyms 4Non specific adjectives or adverbs 3Other 5Factual Edits Misclassified as FluencyShort correction of content 35Opposites 3Similar names 3Noise (unfiltered vandalism) 3Other 6Table 6: Error types based on manual examina-tion of 50 fluency edit misclassifications and 50factual edit misclassifications.leads to a small decrease in classification accu-racy, namely 86.68% instead of 87.14% for SVM.This decrease is not statistically significant.The highest accuracy is achieved by both SVMand RF and there are few significant differencesamong the three classifiers.
The fraction of cor-rectly classified edits per type (Table 5) revealsthat for SVM and Logit, most fluency edits arecorrectly classified by the baseline and most im-provements over the baseline are attributed to bet-ter classification of factual edits.
This is not thecase for RF, where the fraction of correctly classi-fied factual edits is higher and the fraction of cor-rectly classified fluency edits is lower.
This in-sight motivates further experimentation.
Repeat-ing the experiment with a meta-classifier that usesa majority voting scheme, achieves an improvedaccuracy of 87.58%.
This improvement is not sta-tistically significant.5.3 Error AnalysisTo have better understanding of errors made bythe classifier, 50 fluency edit misclassificationsand 50 factual edit misclassifications are ran-domly selected and manually examined.
The er-rors are grouped into categories as summarized inTable 6.
These explain certain limitations of theclassifier and suggest possible improvements.Fluency edit misclassifications: 14 instances(28%) are phrases (often co-referents) that are ei-ther equivalent or redundant in the given context.Correctly Classified Fluency Edits?Adventure education makes intentional use of intention-ally uses challenging experiences for learning.?
?He served as president from October 1 , 1985 and retiredthrough his retirement on June 30 , 2002.?
?In 1973, he helped organize assisted in organizing hisfirst ever visit to the West.
?Correctly Classified Factual Edits?Over the course of the next two years five months, theunit completed a series of daring raids.?
?Scottish born David Tennant has reportedly said hewould like his Doctor to wear a kilt.?
?This family joined the strip in late 1990 around March1991.
?Table 7: Examples of correctly classified user ed-its.
Deleted segments are struck out, inserted arebold (revision numbers are omitted for brevity).For example: ?in 1986?
?
?that year?, ?whenshe returned??
?when Ruffa returned?
and ?thecore member of the group are??
?the core mem-bers are?.
13 (26%) are paraphrases misclassifiedas factual edits.
Examples are: ?made cartoons??
?produced animated cartoons?
and ?with theimplication that they are similar to?
?
?imply-ing a connection to?.
7 modify numeric patternsthat do not change the meaning such as the year?37?
?
?1937?.
4 replace a first name of a per-son with the last name.
4 contain acronyms, e.g.
?Display PostScript?
?
?Display PostScript (orDPS)?.
Acronym features are correctly identifiedbut the classifier fails to recognize a fluency edit.3 modify adjectives or adverbs that do not changethe meaning such as ?entirely?
and ?various?.Factual edit misclassifications: the big major-ity, 35 instances (70%), could be characterized asshort corrections, often replacing a similar word,that make the content more accurate or moreprecise.
Examples (context is omitted): ?city??
?village?, ?emigrated?
?
?immigrated?
and?electrical??
?electromagnetic?.
3 are oppositesor antonyms such as ?previous?
?
?next?
and?lived?
?
?died?.
3 are modifications of similarperson or entity names, e.g.
?Kelly?
?
?Kate?.3 are instances of unfiltered vandalism, i.e.
noisyexamples.
Other misclassifications include verbtense modifications such as ?is?
?
?was?
and?consists?
?
?consisted?.
These are difficult to362Comment Test Set Classified asSize Fluency Edits?grammar?
1,122 88.9%?spelling?
2,893 97.6%?typo?
3,382 91.6%?copyedit?
3,437 68.4%Random set 5,000 49.4%Table 8: Classifying unlabeled data selected byuser comments that suggest a fluency edit.
TheSVM classifier is trained using the labeled data.User comments are not used as features.classify because the modification of verb tense ina given context is sometimes factual and some-times a fluency edit.These findings agree with the feature analy-sis.
Fluency edit misclassifications are typicallylonger phrases that carry the same meaning whilefactual edit misclassifications are typically sin-gle words or short phrases that carry differentmeaning.
The main conclusion is that the clas-sifier should take into account explicit contentand context.
Putting aside the consideration ofsimplicity and interoperability, features based onco-reference resolution and paraphrase recogni-tion are likely to improve fluency edits classi-fication, and features from language resourcesthat describe synonymy and antonymy relationsare likely to improve factual edits classification.While this conclusion may come at no surprise, itis important to highlight the high classification ac-curacy that is achieved without such capabilitiesand resources.
Table 7 presents several examplesof correct classification produced by our classifier.6 Exploiting Unlabeled DataWe extracted a large set of user edits but our ap-proach has been limited to a restricted number oflabeled examples.
This section attempts to findwhether the classifier generalizes beyond labeleddata and whether unlabeled data could be used toimprove classification accuracy.6.1 Generalizing Beyond Labeled DataThe aim of the next experiment is to test how wellthe supervised classifier generalizes beyond thelabeled test set.
The problem is the availabilityof test data.
There is no shared task for user ed-its classification and no common test set to eval-Replaced by Frequency Edit class?second?
144 Factual?First?
38 Fluency?last?
31 Factual?1st?
22 Fluency?third?
22 FactaulTable 9: User edits replacing the word ?first?
withanother single word: most frequent 5 out of 524.Replaced by Frequency Replaced by Frequency?Adams?
7 ?Squidward?
6?Joseph?
7 ?Alexander?
5?Einstein?
6 ?Davids?
5?Galland?
6 ?Haim?
5?Lowe?
6 ?Hickes?
5Table 10: Fluency edits replacing the word ?He?with proper noun: most frequent 10 out of 1,381.uate against.
We resort to Wikipedia user com-ments.
It is a problematic option because it is un-reliable.
Users may add a comment when submit-ting an edit, but it is not mandatory.
The com-ment is a free text with no predefined structure.It could be meaningful or nonsense.
The com-ment is per revision.
It may refer to one, someor all edits submitted for a given revision.
Nev-ertheless, we identify several keywords that rep-resent certain types of fluency edits: ?grammar?,?spelling?, ?typo?, and ?copyedit?.
The first threeclearly indicate grammar and spelling corrections.The last indicates a correction of format and style,but also of accuracy of the text.
Therefore it onlyrepresents a bias towards fluency edits.We extract unlabeled edits whose comment isequal to one of the keywords and construct a testset per keyword.
An additional test set consists ofrandomly selected unlabeled edits with any com-ment.
The five test sets are classified by the SVMclassifier trained using the labeled data and the setof all features.
To remove any doubt, user com-ments are not part of any feature of the classifier.The results in Table 8 show that most unlabelededits whose comments are ?grammar?, ?spelling?or ?typo?
are indeed classified as fluency ed-its.
The classification of edits whose comment is?copyedit?
is biased towards fluency edits, but asexpected the result is less distinct.
The classifica-tion of the random set is balanced, as expected.363Feature set SVM RF LogitBaseline 76.26% 76.26% 76.34%All Features 87.14%??
87.14%?
85.64%?
?Unlabeled only 78.11%?
83.49%??
78.78%?
?Base + unlabeled 80.86%??
85.45%??
81.83%?
?All + unlabeled 87.23% 88.35%???
85.92%?Table 11: Classification accuracy using featuresfrom unlabeled data.
The first two rows are identi-cal to Table 4.
Statistical significance at p < 0.05is indicated by: ?
w.r.t the baseline; ?
w.r.t all fea-tures excluding features from unlabeled data; and?
w.r.t to another classifier marked by ?
(using thesame features).
The best result is marked in bold.6.2 Features from Unlabeled DataThe purpose of the last experiment is to exploitunlabeled data in order to extract additional fea-tures for the classifier.
The underlying assumptionis that reoccurring patterns may indicate whethera user edit is factual or a fluency edit.We could assume that fluency edits would re-occur across many revisions, while factual editswould only appear in revisions of specific docu-ments.
However, this assumption does not nec-essarily hold.
Table 9 gives a simple example ofsingle word replacements for which the most re-occurring edit is actually factual and other factualand fluency edits reoccur in similar frequencies.Finding user edits reoccurrence is not trivial.We could rely on exact matches of surface forms,but this may lead to data sparseness issues.
Flu-ency edits that exchange co-referents and propernouns, as illustrated by the example in Table 10,may reoccur frequently but this fact could notbe revealed by exact matching of specific propernouns.
On the other hand, using a bag of wordapproach may find too many unrelated edits.We introduce a two-step method that measuresthe reoccurrence of edits in unlabeled data us-ing exact and approximate matching over multi-ple representations.
The method provides a set offrequencies that is fed into the classifier and al-lows for learning subtle patterns of reoccurrence.Staying consistent with our initial design consid-erations, the method is simple and interoperable.Given a user edit (pre, post), the method doesnot compare pre with post in any way.
It onlycompares pre with pre-edited sentences of otherunlabeled edits and post with post-edited sen-tences of other unlabeled edits.
The first step is toselect candidates using a bag of words approach.The second step is a comparison of the user editwith each one of the candidates while increment-ing counts of similarity measures.
These accountfor exact matches between different representa-tions (original and low case, lemmas, PoS and NEtags) as well as for approximate matches usingcharacter- and word-level edit distance betweenthose representations.
An additional feature is thenumber of distinct documents in the candidate set.We compute the set of features for the labeleddataset based on the unlabeled data.
The numberof candidates is set to 1,000 per user edit.
Were-train the classifiers using five configurations:Baseline and All Features are identical to the firstexperiment.
Unlabeled only uses the new featureset without any other feature.
Base + Unlabeledadds the new feature set to the baseline.
All + Un-labeled uses all available features.
All results arefor 10-fold cross validation with statistical signif-icance at p < 0.05 by paired t-test, see Table 11.We find that features extracted from unlabeleddata outperform the baseline and lead to statisti-cally significant improvements when added to it.The combination of all features allows RandomForests to achieve the highest statistically signifi-cant accuracy level of 88.35%.7 ConclusionsThis work addresses the task of user edits clas-sification as factual or fluency edits.
It adoptsa supervised machine learning approach anduses character- and word- level features, part-of-speech tags, named entities, language modelprobabilities, and a set of features extracted fromlarge amounts of unlabeled data.
Our experimentswith contiguous user edits extracted from revisionhistories of the English Wikipedia achieve highclassification accuracy and demonstrate general-ization to data beyond labeled edits.Our approach shows that machine learningtechniques can successfully distinguish betweenuser edit types, making them a favorable alterna-tive to heuristic solutions.
The simple and adap-tive nature of our method allows for application tolarge and evolving sets of user edits.Acknowledgments.
This research was fundedin part by the European Commission through theCoSyne project FP7-ICT-4-248531.364ReferencesA.
Aji, Y. Wang, E. Agichtein, and E. Gabrilovich.2010.
Using the past to score the present: Extend-ing term weighting models through revision historyanalysis.
In Proceedings of the 19th ACM inter-national conference on Information and knowledgemanagement, pages 629?638.I.
Androutsopoulos and P. Malakasiotis.
2010.
A sur-vey of paraphrasing and textual entailment meth-ods.
Journal of Artificial Intelligence Research,38(1):135?187.L.
Breiman.
2001.
Random forests.
Machine learn-ing, 45(1):5?32.J.
Chae and A. Nenkova.
2009.
Predicting the fluencyof text with shallow structural features: case stud-ies of machine translation and human-written text.In Proceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 139?147.C.
Cortes and V. Vapnik.
1995.
Support-vector net-works.
Machine learning, 20(3):273?297.C.
Dutrey, D. Bernhard, H. Bouamor, and A. Max.2011.
Local modifications and paraphrases inWikipedia?s revision history.
Procesamiento delLenguaje Natural, Revista no 46:51?58.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The WEKA datamining software: an update.
ACM SIGKDD Explo-rations Newsletter, 11(1):10?18.A.
Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,and Y. Shi.
2006.
Recognizing textual entailmentwith LCCs GROUNDHOG system.
In Proceedingsof the Second PASCAL Challenges Workshop.T.
Joachims.
1998.
Text categorization with supportvector machines: Learning with many relevant fea-tures.
Machine Learning: ECML-98, pages 137?142.A.
Kittur, B.
Suh, B.A.
Pendleton, and E.H. Chi.
2007.He says, she says: Conflict and coordination inWikipedia.
In Proceedings of the SIGCHI confer-ence on Human factors in computing systems, pages453?462.V.I.
Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707?710.P.
Malakasiotis.
2009.
Paraphrase recognition usingmachine learning to combine similarity measures.In Proceedings of the ACL-IJCNLP 2009 StudentResearch Workshop, pages 27?35.A.
Max and G. Wisniewski.
2010.
Min-ing naturally-occurring corrections and paraphrasesfrom Wikipedia?s revision history.
In Proceedingsof LREC, pages 3143?3148.E.W.
Myers.
1986.
An O(ND) difference algorithmand its variations.
Algorithmica, 1(1):251?266.R.
Nelken and E. Yamangil.
2008.
MiningWikipedia?s article revision history for trainingcomputational linguistics algorithms.
In Proceed-ings of the AAAI Workshop on Wikipedia and Arti-ficial Intelligence: An Evolving Synergy, pages 31?36.S.
Nunes, C. Ribeiro, and G. David.
2011.
Termweighting based on document revision history.Journal of the American Society for InformationScience and Technology, 62(12):2471?2478.M.
Potthast and T. Holfeld.
2011.
Overview of the 2ndinternational competition on Wikipedia vandalismdetection.
Notebook for PAN at CLEF 2011.M.
Potthast, B. Stein, and T. Holfeld.
2010.
Overviewof the 1st international competition on Wikipediavandalism detection.
Notebook Papers of CLEF,pages 22?23.D.
Shapira and J. Storer.
2002.
Edit distance withmove operations.
In Combinatorial Pattern Match-ing, pages 85?98.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings ofAssociation for Machine Translation in the Ameri-cas, pages 223?231.A.
Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In Proceedings of the interna-tional conference on spoken language processing,volume 2, pages 901?904.F.B.
Viegas, M. Wattenberg, and K. Dave.
2004.Studying cooperation and conflict between authorswith history flow visualizations.
In Proceedings ofthe SIGCHI conference on Human factors in com-puting systems, pages 575?582.A.G.
West and I. Lee.
2011.
Multilingual vandalismdetection using language-independent & ex postfacto evidence.
Notebook for PAN at CLEF 2011.A.G.
West, S. Kannan, and I. Lee.
2010.
DetectingWikipedia vandalism via spatio-temporal analysisof revision metadata.
In Proceedings of the ThirdEuropean Workshop on System Security, pages 22?28.K.
Woodsend and M. Lapata.
2011.
Learning tosimplify sentences with quasi-synchronous gram-mar and integer programming.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing, pages 409?420.E.
Yamangil and R. Nelken.
2008.
Mining Wikipediarevision histories for improving sentence compres-sion.
In Proceedings of ACL-08: HLT, Short Pa-pers, pages 137?140.M.
Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, andL.
Lee.
2010.
For the sake of simplicity: Unsu-pervised extraction of lexical simplifications fromWikipedia.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 365?368.365F.M.
Zanzotto and M. Pennacchiotti.
2010.
Expand-ing textual entailment corpora from Wikipedia us-ing co-training.
In Proceedings of the 2nd Work-shop on Collaboratively Constructed Semantic Re-sources, COLING 2010.S.
Zwarts and M. Dras.
2008.
Choosing the righttranslation: A syntactically informed classificationapproach.
In Proceedings of the 22nd InternationalConference on Computational Linguistics-Volume1, pages 1153?1160.366
