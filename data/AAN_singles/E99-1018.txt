Proceedings ofEACL '99POS Disambiguation and Unknown Word Guessing withDecision TreesGiorgos S. OrphanosComputer Engineering & Informatics Dept.and Computer Technology InstituteUniversity of Patras26500 Rion, Patras, Greecegeoffan@cti.grDimitris N. ChristodoulaldsComputer Engineering & Informatics Dept.and Computer Technology InstituteUniversity of Patras26500 Rion, Patras, Greecedxri@cti.grAbstractThis paper presents a decision-treeapproach to the problems of part-of-speech disambiguation and unknownword guessing as they appear in ModemGreek, a highly inflectional language.
Thelearning procedure is tag-set independentand reflects the linguistic reasoning on thespecific problems.
The decision treesinduced are combined with a high-coverage lexicon to form a tagger thatachieves 93,5% overall disambiguationaccuracy.1 IntroductionPart-of-speech (POS) taggers are softwaredevices that aim to assign unambiguousmorphosyntactic tags to words of electronictexts.
Although the hardest part of the taggingprocess is performed by a computationallexicon, a POS tagger cannot solely consist of alexicon due to: (i) morphosyntactic ambiguity(e.g., 'love' as verb or noun) and (ii) theexistence of unknown words (e.g., proper nouns,place names, compounds, etc.).
When thelexicon can assure high coverage, unknownword guessing can be viewed as a decision takenupon the POS of open-class words (i.e., Noun,Verb, Adjective, Adverb or Participle).Towards the disambiguation of POS tags,two main approaches have been followed.
Onone hand, according to the linguistic approach,experts encode handcrafted rules or constraintsbased on abstractions derived from languageparadigms (usually with the aid of corpora)(Green and Rubin, 1971; Voutilainen 1995).
Onthe other hand, according to the data-drivenapproach, a frequency-based language model isacquired from corpora and has the forms of n-grams (Church, 1988; Cutting et al, 1992), rules(Hindle, 1989; Brill, 1995), decision trees(Cardie, 1994; Daelemans et al, 1996) or neuralnetworks (Schmid, 1994).In order to increase their robusmess, mostPOS taggers include a guesser, which tries toextract the POS of words not present in thelexicon.
As a common strategy, POS guessersexamine the endings of unknown words (Cuttinget al 1992) along with their capitalization, orconsider the distribution of unknown words overspecific parts-of-speech (Weischedel et aL,1993).
More sophisticated guessers furtherexamine the prefixes of unknown words(Mikheev, 1996) and the categories ofcontextual tokens (Brill, 1995; Daelemans et aL,1996).This paper presents a POS tagger for ModemGreek (M. Greek), a highly inflectionallanguage, and focuses on a data-driven approachfor the induction of decision trees used asdisambiguation/guessing devices.
Based on ahigh-coverage 1 lexicon, we prepared a taggedcorpus capable of showing off the behavior ofall POS ambiguity schemes present in M.
Greek(e.g., Pronoun-Clitic-Article, Pronoun-Clitic,Adjective-Adverb, Verb-Noun, etc.
), as well asthe characteristics of unknown words.Consequently, we used the corpus for theinduction of decision trees, which, along with1 At present, he lexicon is capable of assigning fullmorphosyntactic attributes (i.e., POS, Number,Gender, Case, Person, Tense, Voice, Mood) to-870.000 Greek word-forms.134Proceedings ofEACL '99the lexicon, are integrated into a robust POStagger for M. Greek texts.The disambiguating methodology followed ishighly influenced by the Memory-Based Tagger(MBT) presented in (Daelemans et aL, 1996).Our main contribution is the successfulapplication of the decision-tree methodology toM.
Greek with three improvements/custom-izations: (i) injection of linguistic bias to thelearning procedure, (ii) formation of tag-setindependent training patterns, and (iii) handlingof set-valued features.2 Tagger ArchitectureFigure 1 illustrates the functional components ofthe tagger and the order of processing:Raw Text- -  I I I words with one tag I I I re?reun~ownl I ~an w?r , 4;;Disambiguator Itags"  I &Guesser IIwords with one tagTa ed TextFigure 1.
Tagger ArchitectureRaw text passes through the Tokenizer, where itis converted to a stream of tokens.
Non-wordtokens (e.g., punctuation marks, numbers, dates,etc.)
are resolved by the Tokenizer and receive atag corresponding to their category.
Word tokensare looked-up in the Lexicon and those foundreceive one or more tags.
Words with more thanone tags and those not found in the Lexicon passthrough the Disambiguator/Guesser, where thecontextually appropriate ag is decided/guessed.The Disambiguator/Guesser i  a 'forest' ofdecision trees, one tree for each ambiguityscheme present in M. Greek and one tree forunknown word guessing.
When a word with twoor more tags appears, its ambiguity scheme isidentified.
Then, the corresponding decision treeis selected, which is traversed according to thevalues of morphosyntactic features extractedfrom contextual tags.
This traversal returns thecontextually appropriate POS.
The ambiguity isresolved by eliminating the tag(s) with differentPOS than the one returned by the decision tree.The POS of an unknown word is guessed bytraversing the decision tree for unknown words,which examines contextual features along withthe word ending and capitalization and returnsan open-class POS.3 Training SetsFor the study and resolution of lexical ambiguityin M. Greek, we set up a corpus of 137.765tokens (7.624 sentences), collecting sentencesfrom student writings, literature, newspapers,and technical, financial and sports magazines.We made sure to adequately cover all POSambiguity schemes present in M. Greek, withoutshowing preference to any scheme, so as to havean objective view to the problem.
Subsequently,we tokenized the corpus and inserted it into adatabase and let the lexicon assign amorphosyntactic tag to each word-token.
We didnot use any specific tag-set; instead, we let thelexicon assign to each known word allmorphosyntactic attributes available.
Table 1shows a sample sentence after this initial tagging(symbolic names appearing in the tags areexplained in Appendix A).26382638263826382638263826382638Table 1.
An example-sentence from the tagged corpus1 Ot The Art (MscFemSglNom)2 axuvff\]o~t~ answers ......... vrb(,B_SglActPS?sjv + iB~,SglKctFutlnd)+Nra% ( FemP1 rNomAc cVoc)3 ~oI) of " Prn ( C MScNtrsngGen)- +~ Clt + Art (MscNtrSngGen)4 ~.
Mr. Abr5 n=~0~o~ eap,dopoulos "ou" Cap N~ + vrb + Adj + Pep +Aav6 .illaV were Vrb (-c--sg!
~ir I c~Ind!i .,_i~i/,7 aa~iq clear Adj (MscFemPlrNomAccVoc)8 I .
!N1212ArtNnn135Proceedings ofEACL '99Table 2.
A fragment from the training set Verb-Noun.Examplel ~: .~.~::~,~i:.~:::.~::~::~:: I ::~ i:~:::~.i~  ~:-~:< ~./Tiig~,.:..
~:S ,.
;;;.i:~  ~: , ; ; / " \ [Manui l: i~iD~.:i:l:~,i:~i~';~::;::i~i:ii%~!
::~:~?~J~":.~~::~i~ :~i~:~i~.
': !~:.~ i::~ :~::~i':i~i~.
:~".
;~il;:.,;~< :!
'~ ;: "?~::' '.~::!
;~ ~ s:~-:i'.:.
~-.'~'~'.~.~'.
:~ ;~:~:!.:',~t~-':i.
'~ ~.
l1 Adj (FemSglNomAcc) ;Vrb(_B_SglPntActZmv) + ~Prn( C FemSglGen) + Clt + NnnNnn (FemSglNomAccVoc) ~rt  ( FemSglGen )Nnn (FemSglNomAccVoc)" "  "i  iqzm "+ Vrb  + 'Ad j  +-Pep !Vrb ( _B_Sg lFutPs tAct IndS jv )  + i,, .
"N '~" -.
+ Adv Nnn (FemPlrNomAccVoc)4 Prn  (_A_SglGenAcc) + Vrb  (_B_SglFutPstAct IndSjv)  + Adj (FemSglNomAccVoc)  Vrb ,............ Pps ............ Nnn  ( Nt rSgl P i rNomGenAccVoc  )5 Ar t  (FemPlrAcc) .......... ~?r b 'i-_B~Sg-i-~ ?~P" -s tJ%c-E Z nclS jv  ~ - ?
..... ~ p~~-c"fise~Er~i-6%n3 "~" -6iE- " i~  "Nnn(FemPl rNomAccVoc)  ',+ Art  (MscNtrSglGen).... 6 " Pci ......... V rb  (B_Sg lPntFcsFutPs tAct IndS jv )  !Prn (A_Sg lGenAcc)  + Pps Vrb~+ Nrns (MscSglNom).... 7 ................... 3/rb (B_Sg lFutPs tAct IndS jv )  + ~rb  (_C_P l rPntFcsAct IndS jv)  .. N~-~Nnn ( FemPl rNomAccVoc  )' ? '
V rb  8 Pcl ~Vrb (_B_SglFutPstAct IndSjv)  + iNnn  ( Nt rSgl P1 rNomGenAc cVoc) !9 Adj (FemSglNomAcc) Nrb  (_C_SglPntFcsAct IndSjv)  + ~t  (MscSglAcc + Nnn.l~nn (FemSglNomAccVoc) ~t rSg lNomAcc  )?
10 Pcl + Adv Mrb(  B Sg lPntFcsFutPs tActXndS jv )~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
V rb: i+ Nnn (MscSglNom) '~To words with POS ambiguity (e.g., tokens #2and #3 in Table 1) we manually assigned theircontextually appropriate POS.
To unknownwords (e.g., token #5 in Table 1), which bydefault received a disjunct of open-class POSlabels, we manually assigned their real POS anddeclared explicitly their inflectional ending.At a next phase, for all words relative to aspecific ambiguity scheme or for all unknownwords, we collected from the tagged corpus theirautomatically and manually assigned tags alongwith the automatically assigned tags of theirneighboring tokens.
This way, we created atraining set for each ambiguity scheme and atraining set for unknown words.
Table 2 shows a10-example fragment from the training set forthe ambiguity scheme Verb-Noun.
For reasonsof space, Table 2 shows the tags of only theprevious (column Tagi_l) and next (columnTagi+~) tokens in the neighborhood of anambiguous word, whereas more contextual tagsactually comprise a training example.
A trainingexample also includes the manually assigned tag(column Manual Tagi) along with theautomatically assigned tag 2 (column Tagi) of theambiguous word.
One can notice that somecontextual tags are missing (e.g., Tagi_~ ofExample 7; the ambiguous word is the first inthe sentence), or some contextual tags mayexhibit POS ambiguity (e.g., Tagi+l of Example1), an incident implying that the learner mustlearn from incomplete/ambiguous examples,since this is the case in real texts.If we consider that a tag encodes 1 to 5morphosyntaetic features, each feature takingone or a disjunction of 2 to 11 values, then thetotal number of different tags counts up toseveral hundreds 3.
This fact prohibits the feedingof the training algorithms with patterns that havethe fo rm:  (Tagi_2, Tagi_b Tagi, Tagi.~, Manual_Tagi),which is the ease for similar systems that learnPOS disambiguation (e.g., Daelemans et al,1996).
On the other hand, it would be inefficient(yielding to information loss) to generate asimplified tag-set in order to reduce its size.
The'what the training patterns should look like'bottleneck was surpassed by assuming a set offunctions that extract from a tag the value(s) ofspecific features, e.g.
:Gender (Ar t  (MscSg lAcc  + Nt rSg lNomAcc) )  =MSC + Nt rWith the help of these functions, the trainingexamples hown in Table 2 are interpreted topatterns that look like:(POS(Tagi_2), POS(Tagi_l), Gender(Tagi), POS(TagH),Gender(Tagi+l), Manual_Tagi),2 In case the learner needs to use morphosyntacticinformation of the word being disambiguated.3 The words of the corpus received from the lexicon690 different tags having the form shown in Table 2.136Proceedings of EACL '99that is, a sequence of feature-values extractedfrom the previous/current/next tags along withthe manually assigned POS label.Due to this transformation, two issuesautomatically arise: (a) A feature-extractingfunction may return more than one feature value(as in the Gander(...) example); consequently,the training algorithm should be capable ofhandling set-valued features.
(b) A feature-extracting function may return no value, e.g.Gender(Vrb( C P l rPntkc t lndS jv ) )  = None,thus we added an extra value -the value None--to each feature 4.To summarize, the training material weprepared consists of: (a) a set of trainingexamples for each ambiguity scheme and a setof training examples for unknown words 5, and(b) a set of features accompanying eachexample-set, denoting which features (extractedfrom the tags of training examples) willparticipate in the training procedure.
Thisconfiguration offers the following advantages:1.
A training set is examined only for thefeatures that are relative to thecorresponding ambiguity scheme, thusaddressing its idiosyncratic needs.2.
What features are included to each feature-set depends on the linguistic reasoning onthe specific ambiguity scheme, introducingthis way linguistic bias to the learner.3.
The learning is tag-set independent, since itis based on specific features and not on theentire tags.4.
The learning of a particular ambiguityscheme can be fine-tuned by including newfeatures or excluding existing features fromits feature-set, without affecting the learningof the other ambiguity schemes.4 Decision Trees4.1 Tree InductionIn the previous section, we stated the use oflinguistic reasoning for the selection of feature-4 e.g.
: Gender ={Masculine, Feminine, Neuter, None}.5 The training examples for unknown words, exceptcontextual tags, also include the capitalization featureand the suffixes of unknown words.sets suitable to the idiosyncratic properties of thecorresponding ambiguity schemes.Formally speaking, let FS be the feature-setattached to a training set TS.
The algorithm usedto transform TS into a decision tree belongs tothe TDIDT (Top Down Induction of DecisionTrees) family (Quinlan, 1986).
Based on thedivide and conquer principle, it selects the bestFbe, t feature from FS, partitions TS according tothe values of Fbest and repeats the procedure foreach partition excluding Fbest from FS,continuing recursively until all (or the majorityof) examples in a partition belong to the sameclass C or no more features are left in FS.During each step, in order to find the featurethat makes the best prediction of class labels anduse it to partition the training set, we select thefeature with the highest gain ratio, aninformation-based quantity introduced byQuinlan (1986).
The gain ratio metric iscomputed as follows:Assume a training set TS with patternsbelonging to one of the classes C1, C2, ... Ck.The average information eeded to identify theclass of a pattern in TS is:info(TS) - ?
freq(Cj,TS) = x log 2 (freq(Cj' TS))j=l ITS I ITS INow consider that TS is partitioned into TSI,TSz, ...
TS., according to the values of a featureF from FS.
The average information eeded toidentify the class of a pattern in the partitionedTS is:info F (TS ) = ?1TS l  I x in fo (TS i )i=l \[TSIThe quantity:gain(F) = info(TS) - info F (TS)measures the information relevant toclassification that is gained by partitioning TSin accordance with the feature F. Gain ratio is anormalized version of information gain:gain ratio(F) = gain(F)split info(F)Split info is a necessary normalizing factor, sincegain favors features with many values, andrepresents he potential information generated bydividing TS into n subsets:split info(F) = -?
ITsi I?
l?g2 (IIT:~ I)i=1 ITS\[ \[137Proceedings of EACL '99Taking into consideration the formula thatcomputes the gain ratio, we notice that the bestfeature is the one that presents the minimumentropy in predicting the class labels of thetraining set, provided the information of thefeature is not split over its values.The recursive algorithm for the decision treeinduction is shown in Figure 2.
Its parametersare: a node N, a training set TS and a feature setFS.
Each node constructed, in a top-down left-to-right fashion, contains a default class label C(which characterizes the path constructed sofar) and if it is a non-terminal node it alsocontains a feature F from FS according towhich further branching takes place.
Everyvalue vi of the feature F tested at a non-terminalnode is accompanied by a pattern subset TSj(i.e., the subset of patterns containing the valuevi).
If two or more values of F are found in atraining pattern (set-valued feature), the trainingpattern is directed to all correspondingbranches.
The algorithm is initialized with aroot node, the entire training set and the entirefeature set.
The root node contains a dummy 6feature and a blank class label.InduceTree( Node N ,  TrainingSet TS,  FeatureSet FS )BeginFor each value v= of the feature F tested by node N DoBeginCreate the subset TSl and assign it to vi;If TSi is empty Then continue; /* goto For */If all pattems in TS~ belong to the same class C ThenCreate under vi a leaf node N' with label C;ElseBeginFind the most frequent class C in TS~;If FS is empty ThenCreate under vj a leaf node N' with label C;ElseBeginFind the feature F' ~th the highest gain ratio;Create under vja non-terminal node N' withlabel C and set N' to test F';Create the feature subset FS' = FS - {F'};InduceTree( N ' ,  TSi ,  FS' );EndEndEndEndFigure 2.
Tree-Induction Algorithm6 The dummy feature contains the sole value None.4.2 Tree TraversalEach tree node, as already mentioned, contains aclass label that represents the 'decision' beingmade by the specific node.
Moreover, when anode is not a leaf, it also contains an ordered listof values corresponding to a particular featuretested by the node.
Each value is the origin of asubtree hanging under the non-terminal node.The tree is traversed from the root to the leaves.Each non-terminal node tests one after the otherits feature-values over the testing pattern.
Whena value is found, the traversal continues throughthe subtree hanging under that value.
If none ofthe values is found or the current node is a leaf,the traversal is finished and the node's classlabel is returned.
For the needs of the POSdisambiguation/guessing problem, tree nodescontain POS labels and test morphosyntacticfeatures.
Figure 3 illustrates the tree-traversalalgorithm, via which disarnbiguation/guessing isperformed.
The lexical and/or contextualfeatures of an ambiguous/unknown wordconstitute a testing pattern, which, along withthe root of the decision tree corresponding to thespecific ambiguity scheme, are passed to thetree-traversal lgorithm.ClassLabel TraverseTree( Node N, TestingPattem P )BeginIf N is a non-terminal node ThenFor each value vl of the feature F tested by N DoIf vl is the value of F in P ThenBeginN' = the node hanging under vj;Return TraverseTree( N ' ,  P );EndRetum the class label of N;EndFigure 3.
Tree-Traversal Algorithm4.3 Subtree OrderingThe tree-traversal gorithm of Figure 3 can bedirectly implemented by representing thedecision tree as nested if-statements (seeAppendix B), where each block of codefollowing an if-statement corresponds to asubtree.
When an if-statement succeeds, thecontrol is transferred to the inner block and,since there is no backtracking, no other feature-values of the same level are tested.
To classify apattern with a set-valued feature, only one value138Proceedings of EACL '99from the set steers the traversal; the value that istested first.
A fair policy suggests to test first themost important (probable) value, or,equivalently, to test first the value that leads tothe subtree that gathered more training patternsthan sibling subtrees.
This policy can beincarnated in the tree-traversal lgorithm if wepreviously sort the list of feature-values testedby each non-terminal node, according to thealgorithm of Figure 4, which is initialized withthe root of the tree.OrderSubtrees( Node N )BeginIf N is a non-terminal node ThenBeginSort the feature-values and sub-trees of node Naccording to the number of training pattems eachsub-tree obtained;For each child node N' under node N DoOrderSubtrees( N' );EndEndFigure 4.
Subtree-Ordering Algor i thmThis ordering has a nice side-effect: itincreases the classification speed, as the mostprobable paths are ranked first in the decisiontree.4.4 Tree CompactionA tree induced by the algorithm of Figure 2 maycontain many redundant paths from root toleaves; paths where, from a node and forward,the same decision is made.
The tree-traversaldefinitely speeds up by eliminating the tails ofthe paths that do not alter the decisions takenthus far.
This compaction does not affect theperformance of the decision tree.
Figure 5illustrates the tree-compaction algorithm, whichis initialized with the root of the tree.CompactTree( Node N )BeginFor each child node N' under node N DoBeginIf N' is a leaf node ThenBeginIf N' has the same class label with N ThenDelete N';EndElseBeginCompactTree( N' );If N' is now a leaf node Andhas the same class label with N ThenDelete N';EndEndEndFigure 5.
Tree-Compaction AlgorithmTable 3.
Statistics and Evaluation MeasurementsPOSAmbiguity SchemesPronoun-Article 7,13 34,19 14,5 1,96Pronoun-Article-Clit ic 4,70 22,54 39,1 4,85pron0un-Prep0sit ion 2,14 10,26 12,2 1,35Adjective-Adverb 1,53 7,33 31,1 13,4Pronoun-Clit ic 1,4i 6,76 38,0 5,78Prepos i t ion-Part ic le -Conjunct i0n i ,~21 ~ 4 ,89  20 ,8  8,942"49 .
.
.
.
12 ,1  6,93 Verb-Noun .
.
.
.
.
.
.
0<52 .
.
.
.
.
.
.
.
.
.
.Adje.ctive-Ad~ erb-NOun .. .
.
0,51 .......... 2,44 ............. 5!,.
0 .... 30 ,4Adjective-~o~ 0~,46 .................... ~,20 ............... 38,2 .
.
.
.
.
.
18~2Par6 ic ie -con  ~unct iOn  .
.
.
.
.
.
.
.
.
.
.
.
0,3.9 .
.
.
.
.
.
.
.
!
,8.7 .
.
.
.
.
.
.
.
I t3 .8 .
.
1 ,38Adverb-Conjunction " 0,36 .
.
.
.
.
.
.
1,72 , 22,.8 " i~8,1Pronoun-Adverb 0,34 1,63 4,31 4,31Verb-Adverb 0,0"6 .
.
.
.
.
0,28 16,8 1,99Other 0,29 1,39 30,1 12,3Tota l  POS Ambiguity 20,85 \[ 24,1 5,48Unknown Words 2,53 1 38,6 15,8Totals 23,38 25,6  6 ,61139Proceedings ofEACL '995 EvaluationTo evaluate our approach, we first partitionedthe datasets described in Section 3 into trainingand testing sets according to the 10-fold cross-validation methodL Then, (a) we found the mostfrequent POS in each training set and (b) weinduced a decision tree from each training set.Consequently, we resolved the ambiguity of thetesting sets with two methods: (a) we assignedthe most frequent POS acquired from thecorresponding training sets and (b) we used theinduced ecision trees.Table 3 concentrates the results of ourexperiments.
In detail: Column (1) shows inwhat percentage the ambiguity schemes and theunknown words occur in the corpus.
The totalproblematic word-tokens in the corpus are23,38%.
Column (2) shows in what percentageeach ambiguity scheme contributes to the totalPOS ambiguity.
Column (3) shows the errorrates of method (a).
Column (4) shows the errorrates of method (b).To compute the total POS disambiguationerror rates of the two methods (24,1% and5,48% respectively) we used the contributionpercentages shown in column (2).6 Discussion and Future GoalsWe have shown a uniform approach to the dualproblem of POS disambiguation and unknownword guessing as it appears in M. Greek,reinforcing the argument that "machine-learningresearchers should become more interested inNLP as an application area" (Daelemans et al,1997).
As a general remark, we argue that thelinguistic approach as good performance whenthe knowledge or the behavior of a language canbe defined explicitly (by means of lexicons,syntactic grammars, etc.
), whereas empirical(corpus-based statistical) learning should applywhen exceptions, complex interaction orambiguity arise.
In addition, there is always theopportunity to bias empirical learning withlinguistically motivated parameters, o as to7 In this method, adataset ispartitioned 10 times into90% training material and 10% testing material.Average accuracy provides a reliable estimate of thegeneralization accuracy.meet he needs of the specific language problem.Based on these statements, we combined a high-coverage lexicon and a set of empiricallyinduced decision trees into a POS taggerachieving ~5,5% error rate for POSdisambiguation and ~16% error rate forunknown word guessing.The decision-tree approach outperforms boththe naive approach of assigning the mostfrequent POS, as well as the ~20% error rateobtained by the n-gram tagger for M. Greekpresented in (Dermatas and Kokkinakis, 1995).Comparing our tree-induction algorithm andIGTREE, the algorithm used in MBT(Daelemans et al, 1996), their main differenceis that IGTREE produces oblivious decisiontrees by supplying an a priori ordered list of bestfeatures instead of re-computing the best featureduring each branching, which is our case.
Afterapplying IGTREE to the datasets described inSection 3, we measured similar performance(-7% error rate for disambiguation and -17%for guessing).
Intuitively, the global search forbest features performed by IGTREE has similarresults to the local searches over the fragmenteddatasets performed by our algorithm.Our goals hereafter aim to cover thefollowing:?
Improve the POS tagging results by: a)finding the optimal feature set for eachambiguity scheme and b) increasing thelexicon coverage.?
Analyze why IGTREE is still so robustwhen, obviously, it is built on lessinformation.?
Apply the same approach to resolve Gender,Case, Number, etc.
ambiguity and to guesssuch attributes for unknown words.ReferencesBrill E. (1995).
Transformation-Based Error-DrivenLearning and Natural Language Processing: ACase Study in Part of Speech Tagging.Computational Linguistics, 21(4), 543-565.Cardie C. (1994).
Domain-Specific KnowledgeAcquisition for Conceptual Sentence Analysis.Ph.D.
Thesis, University of Massachusetts,Amherst, MA.Church K. (1988).
A Stochastic parts program andnoun phrase parser for unrestricted text.
In140Proceedings ofEACL '99Proceedings of 2nd Conference on Applied NaturalLanguage Processing, Austin, Texas.Cutting D., Kupiec J., Pederson J. and Sibun P.(1992).
A practical part-of-speech tagger.
InProceedings of 3rd Conference on Applied NaturalLanguage Processing, Trento, Italy.Daelemans W., Zavrel J., Berck P. and GiUis S.(1996).
MBT: A memory-based part of speechtagger generator, In Proceedings of 4th Workshopon Very Large Corpora, ACL SIGDAT, 14-27.Daelemans W., Van den Bosch A. and Weijters A.(1997).
Empirical Learning of Natural LanguageProcessing Tasks.
In W. Daelemans, A.
Van denBosch, and A. Weijters (eels.)
Workshop Notes ofthe ECML/Mlnet Workshop on Empirical Learningof Natural Language Processing Tasks, Prague, 1-10.Dermatas E. and Kokkinakis G. (1995).
AutomaticStochastic Tagging of Natural Language Texts.Computational Linguistics, 21(2), 137-163.Greene B. and Rubin G. (1971).
Automatedgrammatical tagging of English.
Deparlment ofLinguistics, Brown University.Hindle D. (1989).
Acquiring disambiguation rulesfrom text.
In Proceedings of A CL '89.Quinlan J. R. (1986).
Induction of Decision Trees.Machine Learning, 1, 81-106.Mikheev A.
(1996).
Learning Part-of-SpeechGuessing Rules from Lexicon: Extension to Non-Concatenative Operations.
In Proceedings ofCOLING '96.Schmid H. (1994) Part-of-speech tagging with neuralnetworks.
In Proceedings of COLING'94.Voutilainen A.
(1995).
A syntax-based part-of-speechanalyser.
In Proceedings of EA CL "95.Weischedel R., Meteer M., Schwartz R., Ramshaw L.and Palmucci J.
(1993).
Coping with ambiguity andunknown words through probabilistic models.Computational Linguistics, 19(2), 359-382.Appendix A: Feature Values/ShortcutsPart-Of-Speech = {Article/Art, Noun/Nnn, Adjective/Adj,Pronoun/Pm, VerbNrb, Pardciple/Pcp, Adverb/Adv,Conjunction/Cnj, Preposition/Pps, Particle/Pcl, Clitic/CIt}Number = {Singular/Sng, Plural/Plu}Gender = {Masculine/Msc, Feminine/Fern, Neuter/Ntr}Case = {Nominative/Nom, Genitive/Gen, Dative/Dat,Accusative/Acc, Vocative/Voc}Person = {First/A_, Second/B_, Third/C_}Tense = {Present\]Pnt, Future/Fut, Future Perfect/Fpt, FutureContinuous/Fcs, Past/Pst, Present Perfect/Pnp, PastPerfect/Psp}Voice = {Active/Act, Passive/Psv}Mood = {Indicative/Ind, Imperative/Imv, Subjanctive/Sjv}Capitalization = {Capital/Cap}Appendix B: A decision tree for thescheme Adverb-Adjective/* 'disamb_.AdvAdj.c' file, automatically generated from atraining corpus *1#include "../tagger/tagger.h"int disamb_AdvAdj(void *'I'L) /* TL means Woken List' */{if(POS(TL, -1, Vrb)) /*-1: previous token */if(POS(TL, 1, Nnn)) return Adj; /*+1: next token */else return Adv;else if(POS('rL,-1, Pm))if(POS(TL, 1, None)) return Adv;else if(POS(TL, 1, Pps)) retum Adv;else if(POS(TL, 1, Pcp)) return Adv;else retum Adj;else if(POS(TL, -1, Art)) return Adj;else if(POS(TL, -1, None))if(POS(TL, 1, Nnn)) return Adj;else return Adv;else if(POS(TL, -1, Cnj))if(POS(TL, 1, Nnn)) retum Adj;else return Adv;else if(POS(TL, -1, Adv))if(POS(TL, 1, Nnn)) return Adj;else if(POS(TL, 1, Adv)) return Adj;else return Adv;else if(POS(TL, -1, Adj))if(POS(TL, 1, Cnj)) return Adv;else if(POS(TL, 1, Pcp)) retum Adv;else retum Adj;else if(POS(TL, -1, Nnn))if(POS(TL, 1, Nnn)) retum Adj;else if(POS(TL, 1, Exc)) return Adj;else return Adv;else if(POS(TL, -1, Pps))if(POS(TL, 1, Pm)) return Adv;else if(POS(TL, 1, None)) return Adv;else if(POS(TL, 1, Art)) return Adv;else if(POS(TL, 1, Pcl)) return Adv;else if(POS(TL, 1, CIt)) return Adv;else if(POS(TL, 1, Vrb)) retum Adv;else if(POS(TL, 1, Pps)) return Adv;else if(POS('rL, 1, Pcp)) retum Adv;else return Adj;else if(POS(TL,-1, Pcl))if(POS(TL, 1, Nnn)) return Adj;else if(POS('l'L, 1, Adj)) return Adj;else return Adv;else if(POS(TL,-1, Pcp))if(POS('I'L, 1, Nnn)) return Adj;else if(POS(TL, 1, Vrb)) return Adj;else return Adv;else return Adv;}141
