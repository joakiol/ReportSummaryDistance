JIn: Proceedings of CoNLL-2000 and LLL-2000, pages 199-208, Lisbon, Portugal, 2000.Inductive Logic Programming forCorpus-Based Acquisition of Semantic LexiconsPasca le  S4bi l lotIRISA - Campus de Beaulieu - 35042 Rennes cedex - Francesebillot@irisa, frP ier re t te  Bou i l lonTIM/ ISSCO - ETI - Universit4 de Gen~ve - 40 Bvd du Pont-d'Arve -CH-1205 Geneva-  SwitzerlandPierrette.
Bouillon@issco.
unige, chC4ci le FabreERSS - Universit@ de Toulouse II - 5 all@es A. Machado - 31058 Toulouse cedex - Francecfabre@univ-tlse2, frAbst ractIn this paper, we propose an Inductive LogicProgramming learning method which aims atautomatically extracting special Noun-Verb (N-V) pairs from a corpus in order to build upsemantic lexicons based on Pustejovsky's Gen-erative Lexicon (GL) principles (Pustejovsky,1995).
In one of the components of this lex-ical model, called the qualia structure, wordsare described in terms of semantic roles.
Forexample, the relic role indicates the purpose orfunction of an item (cut for knife), the agen-tive role its creation mode (build for house),etc.
The qualia structure of a noun is mainlymade up of verbal associations, encoding rela-tional information.
The Inductive Logic Pro-gramming learning method that we have devel-oped enables us to automatically extract froma corpus N-V pairs whose elements axe linkedby one of the semantic relations defined in thequalia structure in GL, and to distinguish them,in terms of surrounding categorial context fromN-V pairs also present in sentences ofthe corpusbut not relevant.
This method has been theoret-ically and empirically validated, on a technicalcorpus.
The N-V pairs that have been extractedwill further be used in information retrieval ap-plications for index expansion 1.1This works is funded by the Agence universi-taire de la Francophonie (AUF) (Action de recherchepartag4e "Acquisition automatique d'dldments du Lex-Keywords:  Lexicon learning, GenerativeLexicon, Inductive Logic Programming, Infor-mation indexing.1 In t roduct ionInformation retrieval (IR) systems aim at pro-viding a user who asks a query to a database ofdocuments with the most relevant exts.
Thequality of these systems is usually measuredwith the help of two criteria: the recall rate,which corresponds to the proportion of relevantanswers that have been given by the systemcompared to the total number of relevant an-swers in the database, and the precision rate,which denotes the proportion of relevant an-swers that are present among the given answers.In these IR systems, texts and queries areusually represented by indexes, that is, a col-lection of some of the words that they contain.The quality of the systems therefore highly de-pends on the type of indexing language that hasbeen chosen.
Two kinds of indexes exist: sim-ple indexes, which correspond to simple nouns(N), verbs (V) and/or adjectives (A) that oc-cur in a text or a query 2, and complex indexes,which correspond to the compounds (for exam-ple, NN compounds) present in the document orique Gdndratif pour amdliorer les performances desyst~mes de recherche d'information", r@seau FRAN-CIL).2All the simple N, V and/or A can be kept as indexes,or the most frequent ones for a given text, or those whosefrequencies in this text are especially high compared totheir frequencies in the database, etc.199the question.
The solutions that are given fora user query are the texts whose indexes bettermatch the query index.In order to obtain the hightest performances,IR systems usually offer some possibilities toexpand both query and text indexes.
Tra-ditional index expansion concerns morpho-syntactic similarities; for example, the same in-dex words in plural and singular forms can bematched.
Some other systems deal with a kindof semantic similarities: if they possess a lin-guistic knowledge database, they can, for ex-ample, expand a nominal index by followingsynonymy or hyperonymy links.
These systemsare however usually limited to intra-categorialexpansion, especially N-to-N one.
Here wedeal with a new kind of expansion that hasbeen proven particularly useful (Grefenstette,1997; Fabre and S~billot, 1999) for documentdatabase questioning.
It concerns N-V linksand aims at allowing matching between ominaland verbal formulations that are semanticallyclose.
For example, our objective is to permit amatching between a query index disk store andthe text formulation to sell disks, related by thetypical function of a store.N-V index expansion however has to be con-trolled in order to ensure that the same con-cept is involved in the two formulations.
Wehave chosen Pustejovsky's Generative Lexicon(GL) framework (Pustejovsky, 1995; Bouillonand Busa, 2000) to define what a relevant N-V link is, that is, what is a N-V pair in whichthe N and the V are related by a semantic linkwhich is close, and which can therefore be usedto expand indexes.In GL formalism, lexical entries consist instructured sets of predicates that define a word.In one of the components of this lexical model,called the qualia structure, words are describedin terms of semantic roles.
The telic role in-dicates the purpose or function of an item (forexample, cut for knife), the agentive role its cre-ation mode (build for house), the constitutiverole its constitutive parts (handle for handcup)and the formal role its semantic ategory (con-tain (information) for book).
The qualia struc-ture of a noun is mainly made up of verbal as-sociations, encoding relational information.
Weassert hat these N-V links are especially rele-vant for index expansion in IR systems (Fabreand S~billot, 1999), and what we call a relevantN-V pair afterwards in the paper is a pair com-posed of a N and a V which are related by one ofthe four semantic relations defined in the qualiastructure in GL.GL is however currently just a formalism; nogenerative l xicons exist that are precise noughfor every domain and every application (for eg.IR), and the cost of a manual construction ofa lexicon based on GL principles is prohibitive.Moreover the real N-V links that are the key-point of this formalism cannot be defined a pri-ori and have to be acquired from corpora ofthe studied domain.
The aim of this paper istherefore to present a machine learning method,developed in the Inductive Logic Programmingframework, that enables us to automatically ex-tract from a corpus N-V pairs whose elementsare linked by one of the semantic relations de-fined in the qualia structure in GL, and to dis-tinguish them, in terms of surrounding cate-gorial (Part-of-Speech, POS) context from N-V pairs also present in sentences of the corpusbut not relevant.
It will be divided in threeparts.
Section 2 focusses on the motivation ofthis project regarding the use of GL.
Section 3explains the machine learning method that wehave developed.
Section 4 is dedicated to itstheoretical nd empirical validations, and to theresults of its application to a technical corpus.2 Mot ivat ionAs stated in the introduction, our work makestwo strong claims: firstly N-V associations de-fined in GL are relevant for IR and secondlythis information can be acquired from a corpuson the basis of surrounding POS context.
Thesepresuppositions have to be motivated before ex-plaining the learning method:1.
The aim of GL is to define underspec-ified lexical representations that will acquiretheir specifications in context.
For example, thequalia structure of book indicates that its de-fault function is read and that it is created bythe act of writing.
But this information has tobe enriched in context in order to characterizehow words are used in specific domains.
Forexample, the qualia structure of book will alsohave to indicate that the book can be shelved orindexed if this information is necessary to inter-pret texts from information science domain.
GL200is therefore a theory of words in context.
It canalso be seen as a way to structure informationin corpora and, in that sense, the relations itdefines are therefore privileged information forIR.
In this perspective, GL has been preferredto existing lexical resources uch as WordNet(Fellbaum, 1998) for two main reasons: lexicalrelations that we want to exhibit - namely N-Vlinks - are unavailable in WordNet, which fo-cuses on paradigmatic lexical relations; Word-Net is a domain-independent, static resource,which can not be used as such to describe lexi-cal associations in specific texts, considering thegreat variability of semantic associations fromone domain to another.2.
In GL, the qualia structures are not arbi-trary repository of information.
They containthe information ecessary to explain the syn-tactic behaviour of the item.
We would there-fore expect that there are strong connectionsbetween some specific syntactic phenomena andsome specific qualia relations.
For example, themiddle construction seems to be only possible ifa telic relation holds between the N and V (Bas-sac and Bouillon, 2000) (for example: ?
?thisbook writes well vs this book reads well).
Sim-ilarly, imperative constructions (e.g.
open thedoor, follow the links) or adjectival sentences (abook difficult to write/read) may also indicatea qualia relation.
These are some of the con-structions that we want to identify primilarlyin corpora by the learning method.3 The  mach ine  learn ing  methodTrying to infer lexical semantic informationfrom corpora is not new: lots of works havealready been conducted on this subject, espe-cially in the statistical learning domain (see(Grefenstette, 1994b), for e.g., or (Habert etal., 1997) and (Pichon and S~billot, 1997) forsurveys of this field).
Following Harris's frame-work (Harris et al, 1989), such research tries toextract both syntagmatic and paradigmatic n-formation, respectively studying the words thatappear in the same window-based or syntacticcontexts as a considered lexical unit (first or-der word affinities (Grefenstette, 1994a)), or thewords that generate the same contexts as thekey word (second order word affinities).
For ex-ample, (Briscoe and Carroll, 1997) and (Faureand N~dellec, 1999) try to automatically learnverbal argument structures and selectional re-strictions; (Agarwal, 1995) and (Bouaud et al,1997) build semantic classes; (Hearst, 1992)and (Morin, 1997) focus on particular lexi-cal relations, like hyperonymy.
Some of theseworks are concerned with automatically ob-taining more complete lexical semantic repre-sentations ((Grefenstette, 1994b; Pichon andS~billot, 1999).
Among these studies, (Puste-jovsky et al, 1993) presents a research whoseaim is to acquire GL nominal qualia structuresfrom a corpus; this work is however quite dif-ferent from ours because it supposes that thequalia structure contents are initialized and areonly refined with the help of the corpus by usingthe type coercion 3 mechanism.In order to automatically acquire N-V pairswhose elements are linked by one of the seman-tic relations defined in the qualia structure inGL, we have decided to use a machine learningmethod.
This section is devoted to the expla-nation of this choice and to the description ofthe method that we have developed.Machine learning aims at automaticallybuilding programs from examples that areknown to be positive or negative examples oftheir runnings.
According to Mitchell (Mitchell,1997), "a computer program is said to learnfrom experience E with respect to some classof tasks T and performance measure P, if  itsperformance at tasks in T, as measured by P,improve with experience E".Among different machine learning techniques,we have chosen the Inductive Logic Program-ming framework (ILP) (Muggleton and De-Raedt, 1994) to learn from a textual corpus N-Vpairs that are related in terms of one of the re-lations defined in the qualia structure in GL.Programs that are infered from a set of factsand a background knowledge are here logic pro-grams, that is, sets of Horn clauses.
In the ILPframework, the main idea is to obtain a set ofgeneralized clauses that is sufficiently genericto cover the majority of the positive examples(E+), and sufficiently specific to rightly corre-spond to the concept we want to learn and tocover no (or a few - some noise can be allowed)negative xample(s) (E - ) .
For our experiment,3A semantic operation that converts an argument tothe type which is expected by a function, where it wouldotherwise result in a type error.201we furnish a set of N-V pairs related by one ofthe qualia relations within a POS context (E+),and a set of N-V pairs that are not semanticallylinked (E-),  and the method infers general rules(clauses) that explain these E +.
This particularexplanatory characteristic of ILP has motivatedour choice: ILP does not just provide a predic-tor (this N-V pair is relevant, this one is not)but also a data-based theory.
Contrary to somestatistical methods, it does not just give rawresults but explains the concept hat is learnt 4.We use Progol (Muggleton, 19915) for ourproject, Muggleton's ILP implementation thathas already been proven well suited to deal witha big amount of data in multiple domains, andto lead to results comparable to other ILP im-plementations (Roberts et al, 1998).In this section we briefly describe the corpuson which our experiment has been conducted.We then explain the elaboration of E + and E -for Progol.
We finally present he generalizedclauses that we obtain.
The validation of themethod is detailed in section 4.3.1 The corpusThe French corpus used in this project isa 700 kBytes handbook of helicopter main-tenance, given to us by MATRA CCRA@rospatiale, which contains more than 104000word occurrences 5.
The MATRA CCR corpushas some special characteristics that are espe-cially well suited for our task: it is coherent;it contains lots of concrete terms (screw, door,etc.)
that are frequently used in sentences to-gether with verbs indicating their telic (screwsmust be tightened, etc.)
or agentive roles.This corpus has been POS-tagged with thehelp of annotation tools developed in the MUL-TEXT project (Armstrong, 1996); sentences andwords are first segmented with MtSeg; wordsare analyzed and lemmatized with Mmorph (Pe-titpierre and Russell, 1998; Bouillon et al,1998), and finally disambiguated by the Tatootool, a Hidden Markov Model tagger (Arm-strong et al, 1995).
Each word therefore onlyreceive one POS-tag, with less than 2% of er-4Learning with ILP has already been successfullyused in natural language processing, for example incor-pus POS-tagging (Cussens, 1996) or semantic nterpre-tation (Mooney, 1999).5104212 word occurrences.rors.3.2 Example  const ruct ionThe first task consists in building up E + andE -  for Progol, in order for it to infer gener-alized clauses that explain what, in the POScontext of N-V pairs, distinguishes the relevantpairs from the not relevant ones.
Work has tobe done to determine what is the most appro-priate context for this task.
We just presenthere the solution we have finally chosen.
Sec-tion 4 describes methods and measures to eval-uate the "quality" of the learning that enableus to choose between the different contextualpossibilities.
Here is our methodology for theconstruction of the examples.We first consider all the nouns of the MA-TRA CCR corpus.
More precisely, we only dealwith a 81314 word occurrence subcorpus of theMATRA CCR corpus, which is formed by allthe sentences that contain at least one N andone V. This subcorpus contains 1489 differentN (29633 noun occurrences) and 567 differentV (9522 verb occurrences).
For each N of thissubcorpus, the 10 most strongly associated V, interms of Chi-square, are selected.
This first stepboth produces pairs that are really bound byone qualia relation ((dcrou, serrer)) 6 and pairsthat are fully irrelevant ((roue, prescrire)) 7.Each pair is manually annotated as relevantor irrelevant according to Pustejovsky's qualiastructure principles.
A Perl program is thenused to find the occurrences of these N-V pairsin the sentences of the corpus.For each occurrence of each pair that is sup-posed to be used to build one E +, that is foreach of the previous pairs that has been glob-ally annotated as relevant, a manual control hasto be done to ensure that the N and the V reallyare in the expected relation within the studiedsentence.
After this control, a second Perl pro-gram automatically produces the E +.
Here isthe form of the positive examples:POSITiVE(category_before_N, category_after.N,category_before_V, V_type, distance, position).where V_type indicates if the V is an infinitiveform, etc., distance corresponds to the number6(nut, tighten).7(wheel, prescribe)202of verbs between the N and the V, and positionis POS (for positive) if the V appears before theN in the sentence, NEG if the N appears beforethe V.For example,POSITIVE(VRBINF, P_DE, VID, VRBINF~ 0,POS).means that a N-V pair, in which the N issurrounded with an infinitive verb on its left(VRBINF) and a preposition de s (P.DE) on itsright, in which the V is preceded by nothing 9(VID) 1?
and is an infinitive one (VRBINF), inwhich no verb exists between the N and the V(0), and in which the V appears before the Nin the sentence (POS), is a relevant pair (for ex-ample, in ouvrir la porte de ...).The E -  are elaborated in the same way thanthe E +, with the same Perl program.
E -  andE + forms are identical, except he presence of asign :- before the predicate POSITIVE to denoteaE- ::-POSITIVE (category_before.N,category_after_N, category_before_V, V_type,distance, position).These E -  are automatically built from theprevious highly correlated N-V pairs that havebeen manually annotated as irrelevant.
For ex-ample,:-POSITIVE(VID, P_PAR, NC, VRBPP, 0, NEG).means that a N-V pair, in which the N has noth-ing on its left (VID) and a preposition par n(P_PAR) on its right, in which the V is precededby a noun (NC) and is a past participle (VRBPP),in which no verb exists between the N and theV (0), and in which the V appears after the Nin the sentence (NEG), is an irrelevant pair (forexample, in freinage par goupilles fendues).4031 E + and about 7000 E -  are automati-cally produced this way from the corpus.sOl.9Or by one of the three categories that we do notconsider for example laboration, that is, determiners,adverbs and adjectives.1?Empty.nBy.3.3 Learn ing  w i th  the  he lp  of  P rogo lThese E + and E -  are then furnish to Progolin order for it to try to infer generalized clausesthat explain the concept "qualia pair" versus"not qualia pair".
We do not discuss here ei-ther parameter setting that concerns the choiceof the example POS context, or evaluation cri-teria; this discussion is postponed to next sec-tion; we simply present he learning method andthe type of generalized clauses that we have ob-tained.Some information have to be given to Progolfor it to know what are the categories that canundergo a generalization.
For example, if twoE + are identical but possess different locativeprepositions as second arguments (for eg.
sur 12and sous13), must Progol produce a generaliza-tion corresponding to the same clause exceptthat the second argument is replaced by thegeneral one: locative-preposition, or by a stillmore general one: preposition?The background knowledge used by Progol isknowledge on the domain.
For example here, itcontains the fact that a verb can be found inthe corpus in an infinitive or a conjugated form,etc.verbe( V ) :- infinitif( V ).verbe( V ) :- conjugue( V ).and that an infinitive form is denoted by thetag VERBINF, and a conjugated form by the tagsVERB-PL and VER.B-SG, etc.infinitif( verbinf ).conjugue( verb-pl ).conjugue( verb-sg ).When Progol is provided with all this knowl-edge, learning can begun.
The output of Progolis of two kinds: some clauses that have not atall been generalized (that is, some of the E+),and some generalized clauses; we call the set ofthese generalized clauses G, and it is this set Gthat interests us here.
Here is an example of oneof the generalized clauses that we have obtainedin our experiment:POSITIVE(A, C, C, D, E, F) :-PREPOSITIONLIEU(A), VIDE(C), VERBINF(D),PRES(E).
(1)12On"13Under.203which means that N-V pairs (i) in which thecategory before the N is a locative preposition(PREPOSITIONLIEU(A)), (ii) in which there isnothing after the N and before the V (VIDE(C)for the second and third arguments), (iii) inwhich the V is an infinitive one (VERBINF(D)),and (iv) in which there is no verb between the Nand the V (proximity denoted by P:aEs(E)14),are relevant.
No constraint is set on N/V orderin the sentences.This generalized clause covers, for example,the following E+:POSITIVE(P_SUR, VID, VID, VERBINF, 0, POS).which corresponds to the relevant pair (prise,brancher) 15 that is detected in the corpus in thesentence "Brancher les connecteurs sur les prises~lectriques.
".Some of the generalized clauses in G coverlots of E +, others far less.
We now present amethod to detect what the "good" c, lauses are,that is, the clauses that explain the concept thatwe want to learn, and a measure of the "quality"of the learning that has been conducted.4 Learn ing  va l idat ion  and  resu l tsThis section is dedicated to two aspects ofthe validation of our machine learning method.First we define the theoretical validation of thelearning, that is, we focus on the determinationof a means to detect what are the "good" gen-eralized clauses, and of a measure of the qualityof the concept learning; this parameter settingand evaluation criterion phase explains how wehave chosen the precise POS context for N-Vpairs in the E + and E -  (as described in subsec-tion 3.2): the six contextual elements in exam-ples are the combination that leads to the bestresults in terms of the learning quality measurethat we have chosen.
The second step of thevalidation is the empirical one.
We have appliedthe generalized clauses that have been selectedto the Mat ra  CCR corpus and  haw~ evaluatedthe quality of the results in terms of pairs thatare indicated relevant or not.
Here  are thesetwo phases.14Close(E).l~(plug, to plug in).4.1 Theoret ical  val idationAs we have previously noticed, among the gen-eralized clauses produced from our E + and E -by Progol (set G), some of them cover a lot ofE +, others only a few of them.
What we wantis to get a way to automatically find what arethe generalized clauses that have to be kept inorder to explain the concept we want to learn.We have first defined a measure of the theo-retical generality of the clauses 16.
The theoreti-cal generality of a generalized clause is the num-ber of not generalized clauses (E +) that thisclause can cover.
For example, bothPOSITIVE(P_AUTOURDE, VID, VID, VERBINF,0, NEG).andPOSITIVE(P_CHEZ, VID, VID, VERBINF, 0,POS).can be covered by clause (1) (cf.
subsec-tion 3.3).
During the study of, for example,the distribution of the number of clauses in Gon these different heoretical generality values,our "hope" is to obtain a gaussian-like graphin order to automatically select all the clausespresent under the gaussian plot, or to calculatetwo thresholds that cover 95% of these clausesand to reject the other 5%.
This distribution ishowever not a gaussian one.Our second try has not only concerned thetheoretical coverage of G clauses but also theirempirical coverage.
This second measure thatwe have defined is the number of E + that arereally covered by each clause of G. We then con-sider the distribution of the empirical coverageof G clauses on the theoretical coverages of theseclauses, that is, we consider the graph in which,for each different heoretical measure value forG clauses, we draw a line whose length corre-sponds to the total number of E + covered bythe G clauses that have this theoretical cover-age value.
Here two gaussians clearly appear(cf.
figure 1), one for rather specific lauses andthe other for more general ones.
We have there-fore decided to keep all the generalized clausesproduced by Progol.16We thank J. Nicolas, INRIA researcher at IRISA, forhis help on this point.2048007006005OO400ul 300200100!
!iiii~i~ii!ii!i!iiii!iiii!iiii~iiiii!iilTheoretical coverageFigure 1: Distribution ofThe second point concerns the determinationof a measure of the quality of the learning for theparameter setting.
We are especially interestedin the percentage ofE + that are covered by thegeneralized clauses, and if we permit some noisein Progol parameter adjustment to allow moregeneralizations, by the percentage of E -  thatare rejected by these generalized clauses.
Themeasure of the recall and the precision rates ofthe learning method can be summarized in aPearson coefficient:Pearson = (TP ,TN) - (FP ,FN)x /P rP*PrN*AP*ANwhere A = actual, Pr = predicated, P -- pos-itive, N= negative, T= true, F= false; the moreclose to 1 this value is, the better the learningis.The results for our learning method with arate of Progol noise equal to 0 are the following:from the 4031 initial E + and the 6922 initial E- ,the 109 generalized clauses produced by Progolcover 2485 E + and 0 E-; 1546 E + and 6922 E-positive examples on clausesare therefore uncovered; the value of the Pear-son coefficient is 0.71.
(NB: Figure 1 illustratesthese results).We have developed a Perl program whose roleis to find which Progol noise rate leads to thebest results.
This Progol noise rate is equal to37.
With this rate, the results are the following:from the 4031 initial E + and the 6922 initial E-,the 66 generalized clauses produced by Progolcover 3547 E + and 348 E-; 484 E + and 6574 E-are therefore uncovered; the value of the Pear-son coefficient is 0.84.
The stability of the setof learnt generalized clauses has been tested.4.2 Empir ica l  val idat ionIn order to evaluate the empirical validity of ourlearning method, we have applied the 66 gen-eralized clauses to the Matra CCR corpus andhave studied the appropriateness of the pairsthat are stated relevant or irrelevant by them.Of course, it is impossible to test all the N-Vcombinations present in such a corpus.
Ourevaluation has focussed on some of the signif-205icant nouns of the domain.A Perl program presents to one expert all theN-V pairs that appear in one sentence in a partof the corpus and include one of the studiednouns.
The expert manually tags each pair asrelevant or not.
This tagging is then comparedto the results obtained for these N-V pairs ofthe same part of the corpus by the applicationof the generalized clauses learnt wit\]h Progol.The results for seven significant nouns (vis,@crou, porte, voyant, prise, capot, bouchon) 17are presented in table 1.
In the left column, oneN-V pair is considered as tagged "relevant" bythe generalized clauses if at least one of themcovers this pair; in the right one, at least sixdifferent clauses of G must cover a pair for itto be said correctly detected by the generalizedclauses; the aim of this second test is to reducenoise in the results.1 occurrence 6 occurrencescorrectly found: 49 correctly found: 23incorrectly found: 54 incorrectly found: 4not found: 10 not found: 36Pearson = 0.5138 Pearson = 0.5209Table 1: Empirical validation on Matra CCRcorpusThe results are quite promising, especially ifwe compare them to those obtain by Chi-squarecorrelation (cf.
table 2).
This comparison isinteresting because Chi-square is the first stepof our selection of N-V couples in the corpus (cf.subsection 3.2).correctly found: 38incorrectly found: 124not found: 21Pearson = 0.1206Table 2: Chi-square results on Matra CCR cor-pus5 Conc lus ionThe Inductive Logic Programming learningmethod that we have proposed in order to de-fine what is a N-V pair whose elements are17(screw, nut, door, indicator signal, plug, cowl, cap).bound by one of the qualia relations in Puste-jovsky's Generative Lexicon formalism leads tovery promising results: 83.05% of relevant pairs(after one occurrence) are detected for seven sig-nificant nouns; these results have to be com-pared with the 64% results of Chi-square.
Itis worth noticing that beyond this simple com-parison with one of the possible pure statis-tics based method is, the interest of using ILPlearning is its explanatory characteristic; and itis this characteristic that have motivated ourchoice: contrary to statistical methods, our ILPmethod does not just extract statistically corre-lated pairs but it permits to automatically earnrules that distinguish relevant pairs from others.The fact that noise has to be used in Progol toobtain these results however means that some-thing is missing in our E + to fully define theconcept "qualia pair" versus "not qualia pair";some E -  have to be covered to define it better.A piece of information, maybe syntactic and/orsemantic is missing in our E + to fully character-ize it.
This fact can be easily illustrated by thefollowing example: 'Verbinf det N' structuresare generally relevant (ouvrir la porte 19, etc.
),except when the N indicates a collection of ob-jects (nettoyer l'ensemble du rdservoir 2?)
or apart of an object (vider le fond du rdservoir21).A simple POS-tagging of the sentences offersno difference between them.
We are currentlyworking on a semantic tagging of the MatraCCR corpus in order to improve the results.Another future work concerns the automaticdistinction between the various qualia roles dur-ing learning.
The last phase of the project willdeal with the real use of the N-V pairs obtainedby the machine learning method within one in-formation retrieval system and the evaluation ofthe improvement of its performances.Re ferencesRajeev Agarwal.
1995.
Semantic Feature Extractionfrom Technical Texts with Limited Human Inter-vention.
Ph.D. thesis, Mississippi State Univer-sity, USA.Susan Armstrong, Pierrette Bouillon, andGilbert Robert.
1995.
Tagger Overview.lSThis comparison could be extended to other corpusfrequency based technics (mutual information, etc.
).19Open the door.2?Clean the whole tank.21Empty the tank bottom.206Technical report, ISSCO, (http://issco-www.unige.ch/staff/robert/tatoo/tagger.html).Susan Armstrong.
1996.
Multext: MultilingualText Tools and Corpora.
In H. Feldweg andW.
Hinrichs, editors, Lexikon und Text, pages107-119.
Tiibingen: Niemeyer.Christian Bassac and Pierrette Bouillon.
2000.
ThePolymorphism of Verbs Exhibiting Middle Tran-sitive Alternations in English.
Technical report,ISSCO.Jacques Bouaud, Beno~t Habert, Adeline Nazarenko,and Pierre Zweigenbaum.
1997.
Regroupe-ments issus de d@pendances syntaxiques en cor-pus : cat@gorisation et confrontation avec deuxmod@lisations conceptuelles.
In Proceedings ofIngdnierie de la Connaissance, Roscoff, France.Pierrette Bouillon and Federica Busa.
2000.
Gener-ativity in the Lexicon.
CUP:Cambridge, In Press.Pierrette Bouillon, Sabine Lehmann, SandraManzi, and Dominique Petitpierre.
1998.DSveloppement de lexiques ~ grande @chelle.In Proceedings of colloque de Tunis 1997 "Lamgmoire des mots', Tunis, Tunisie.Ted Briscoe and John Carroll.
1997.
Automatic Ex-traction of Subcategorisation from Corpora.
InProceedings of 5th ACL conference on AppliedNatural Language Processing, Washington, USA.James Cussens.
1996.
Part-of-Speech Disambigua-tion using ILP.
Technical report, Oxford Univer-sity Computing Laboratory.C@cile Fabre and Pascale S~billot.
1999.
Seman-tic Interpretation of Binominal Sequences and In-formation Retrieval.
In Proceedings of Interna-tional ICSC Congress on Computational Intelli-gence: Methods and Applications, CIMA '99, Sym-posium on Advances in Intelligent Data AnalysisAIDA '99, Rochester, N.Y., USA.David Faure and Claire N@dellec.
1999.
KnowledgeAcquisition of Predicate Argument Structuresfrom Technical Texts using Machine Learning:the System ASIUM.
In Dieter Fensel Rudi Studer,editor, Proceedings of 11th European WorkshopEKAW'99, Dagstuhl, Germany.
Springer-Verlag.Christiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press, Cam-bridge, MA.Gregory Grefenstette.
1994a.
Corpus-Derived First,Second and Third-Order Word Affinities.
InProceedings of EURALEX'9~, Amsterdam, TheNetherlands.Gregory Grefenstette.
1994b.
Explorations in Auto-matic Thesaurus Discovery.
Dordrecht: KluwerAcademic Publishers.Gregory Grefenstette.
1997.
SQLET: Short QueryLinguistic Expansion Techniques, Palliating One-Word Queries by Providing Intermediate Struc-ture to Text.
In McGill-University, editor, Pro-ceedings of Recherche d'Informations Assistdepar Ordinateur, RIAO'g7, Montr@al, Qu@bec,Canada.Beno~t Habert, Adeline Nazarenko, and Andr@Salem.
1997.
Les linguistiques de corpus.
Ar-mand Collin/Masson, Paris.Zelig Harris, Michael Gottfried, Thomas Ryckman,Paul Mattick(Jr), Anne Daladier, Tzvee N. Har-ris, and Suzanna Harris.
1989.
The Form ofInformation in Science, Analysis of ImmunologySublanguage.
Kluwer Academic Publisher, Dor-drecht.Marti A. Hearst.
1992.
Automatic Acquisitionof Hyponyms from Large Text Corpora.
InProceedings of 15th International Conference onComputational Linguistics, COLING-92, Nantes,France.Tom M. Mitchell.
1997.
Machine Learning.McGraw-Hill.Raymond Mooney.
1999.
Learning for Semantic In-terpretation: Scaling Up without Dumbing Down.In Proceedings of Learning Language in Logic,LLL99, Bled, Slovenia.Emmanuel Morin.
1997: Extraction de lienss@mantiques entre termes dans des corpus detextes techniques : application ~ l'hyponymie.In Proceedings of Traitement Automatique desLangues Naturelles, TALN'97, Grenoble, France.Stephen Muggleton and Luc De-Raedt.
1994.
In-ductive Logic Programming: Theory and Meth-ods.
Journal of Logic Programming, 19-20:629-679.Stephen Muggleton.
1995.
Inverse Entailment andProgol.
New Generation Computing, 13(3-4):245-286.Dominique Petitpierre and Graham Russell.
1998.Mmorph - the Multext Morphology Program.Technical report, ISSCO.Ronan Pichon and Pascale S~billot.
1997.
Acquisi-tion automatique d'informations lexicales ~ partirde corpus : un brian.
Research report n?3321, IN-RIA, Rennes.Ronan Pichon and Pascale S@billot.
1999.
FromCorpus to Lexicon: from Contexts to SemanticFeatures.
In Proceedings of Practical Applicationsin Language Corpora, PALC'99, to appear, Lodz,Poland.James Pustejovsky, Peter Anick, and Sabine Bergler.1993.
Lexical Semantic Techniques for CorpusAnalysis.
Computational Linguistics, 19(2).James Pustejovsky.
1995.
The Generative Lexicon.Cambridge:MIT Press.Sam Roberts, Wim Van-Laer, Nico Jacobs, StephenMuggleton, and Jeremy Broughton.
1998.
AComparison of ILP and Propositional Systems on207Propositional Data.
In Springer-Verlag, editor,Proceedings of 8th International Workshop on In-ductive Logic Programming, ILP-98, :Berlin, Ger-many.
LNAI 1446.208
