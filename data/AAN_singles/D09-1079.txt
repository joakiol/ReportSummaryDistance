Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 756?764,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPStream-based Randomised Language Models for SMTAbby LevenbergSchool of InformaticsUniversity of Edinburgha.levenberg@sms.ed.ac.ukMiles OsborneSchool of InformaticsUniversity of Edinburghmiles@inf.ed.ac.ukAbstractRandomised techniques allow very biglanguage models to be represented suc-cinctly.
However, being batch-basedthey are unsuitable for modelling an un-bounded stream of language whilst main-taining a constant error rate.
We present anovel randomised language model whichuses an online perfect hash functionto efficiently deal with unbounded textstreams.
Translation experiments overa text stream show that our online ran-domised model matches the performanceof batch-based LMs without incurring thecomputational overhead associated withfull retraining.
This opens up the possibil-ity of randomised language models whichcontinuously adapt to the massive volumesof texts published on the Web each day.1 IntroductionLanguage models (LM) are an integral featureof statistical machine translation (SMT) systems.They assign probabilities to generated hypothe-ses in the target language informing lexical selec-tion.
The most common form of LMs in SMTsystems are smoothed n-gram models which pre-dict a word based on a contextual history of n?
1words.
For some languages (such as English) tril-lions of words are available for training purposes.This fact, along with the observation that ma-chine translation quality improves as the amountof monolingual training material increases, haslead to the introduction of randomised techniquesfor representing large LMs in small space (Talbotand Osborne, 2007; Talbot and Brants, 2008).Randomised LMs (RLMs) solve the problem ofrepresenting large, static LMs but they are batchoriented and cannot incorporate new data with-out fully retraining from scratch.
This propertymakes current RLMs ill-suited for modelling themassive volume of textual material published dailyon the Web.
We present a novel RLM which iscapable of incremental (re)training.
We use ran-dom hash functions coupled with an online perfecthashing algorithm to represent n-grams in smallspace.
This makes it well-suited for dealing withan unbounded stream of training material.
To ourknowledge this is the first stream-based RLM re-ported in the machine translation literature.
Aswell as introducing the basic stream-based RLM,we also consider adaptation strategies.
Perplex-ity and translation results show that populatingthe language model with material chronologicallyclose to test points yields good results.
As withprevious randomised language models, our experi-ments focus on machine translation but we also ex-pect that our findings are general and should helpinform the design of other stream-based models.Section 2 introduces the incrementally retrain-able randomised LM and section 3 considers re-lated work; Section 4 then considers the questionof how unbounded text streams should be mod-elled.
Sections 5 and 6 show stream-based trans-lation results and properties of our novel data-structure.
Section 7 concludes the paper.2 Online Bloomier Filter LMOur online randomised LM (O-RLM) is basedon the dynamic Bloomier filter (Mortensen et al,2005).
It is a variant of the batch-based Bloomierfilter LM of Talbot and Brants (2008) which werefer to as the TB-LM henceforth.
As with theTB-LM, the O-RLM uses random hash functionsto represent n-grams as fingerprints which is themain source of space savings for the model.2.1 Online Perfect HashingThe key difference in our model as compared tothe TB-LM is we use an online perfect hashing756Figure 1: Inserting an n-gram into the dynamic Bloomier filter.
Above: an n-gram is hashed to its targetbucket.
Below: the n-gram is transformed into a fingerprint and the same target bucket is scanned.
If acollision occurs that n-gram is diverted to the overflow dictionary; otherwise the fingerprint is stored inthe bucket.function instead of having to precompute the per-fect hash offline prior to data insertion.The online perfect hash function uses two datastructures: A and D. A is the main, randomiseddata structure and is an array of b dictionariesA0, .
.
.
, Ab?1.
D is a lossless data structure whichhandles collisions in A.
Each of the dictionaries inA is referred to as a ?bucket?.
In our implementa-tion the buckets are equally sized arrays of w-bitcells.
These cells hold the fingerprints and valuesof n-grams (one n-gram-value pair per cell).To insert an n-gram x and associated valuev(x) into the model, we select a bucket Aibyhashing x into the range i ?
[0, .
.
.
, b ?
1].Each bucket has an associated random hash func-tion, hAi, drawn from a universal hash func-tion (UHF) family h (Carter and Wegman, 1977),which is then used to generate the n-gram finger-print: f(x) = hAi(x).If the bucket Aiis not full we conduct a scan ofits cells.
If the fingerprint f(x) is not already en-coded in the bucket Aiwe add the fingerprint andvalue to the first empty cell available.
We allocatea preset number of the least significant bits of eachw-bit cell to hold v(x) and the remaining most sig-nificant bits for f(x) but this is arbitrary.
Any en-coding scheme, such as the packed representationof Talbot and Brants (2008), is viable here.However, if f(x) ?
Aialready (there is a colli-sion) we store the n-gram x and associated valuev(x) in the lossless overflow dictionary D instead.D also holds the n-grams that were hashed to anybuckets that are already full.To query for the value of an n-gram, we firstcheck if the gram is in the overflow dictionary D.If it is, we return the associated value.
Otherwisewe query A using the same hash functions andprocedure as insertion.
If we find a matching fin-gerprint in the appropriate bucket Aiwe have ahit with high probability.
Deletions and updatesare symmetric to querying except we reset the cellto the null value or update its value respectively.As with other randomised models we constructqueries with the appropriate sanity checks to lowerthe error rate efficiently (Talbot and Brants, 2008).2.2 Data InsertionInitially we seed the language model with a largecorpus S in the usual manner associated withbatch LMs.
Then, when processing the stream,we aggregate n-gram counts for some consecu-tive portion, or epoch, of the input stream.
Wecan vary the size of stream window.
For examplewe might batch-up a day or week?s worth of mate-rial.
Intuitively, smaller windows produce resultsthat are sensitive to small variation in the stream,while longer windows (corresponding to data overa longer time period) average out local spikes.
Theexact window size is a matter of experimentation.In our MT experiments (section 5) we can com-pute counts within the streaming window exactlybut randomised approaches (such as the approxi-mate counting schemes from section 3) can easilybe employed instead.757These n-grams and counts are then consideredfor insertion into the online model.
If we decideto insert an n-gram, we either update the count ofthat n-gram if we previously inserted it or else weinsert it as a new entry.
Note that there is someprobability we may encounter a false positive andupdate some other n-gram in the model.2.3 PropertiesThe online perfect hash succeeds by associatingeach n-gram with only one cell in A rather thanhaving it depend on cells (or bits) which may beshared by other n-grams as with the TB-LM.
Sinceeach n-gram?s encoding in the model uses distinctbits and is independent of all other events it cannot corrupt other n-grams when deleted.Adding the overflow dictionary D means thatwe use more space than the TB-LM for the samesupport.
It is shown in Mortensen et al (2005) thatthe expected size of D is a small fraction of the to-tal number of events and its space usage comprisesless than O(|S|) bits with high probability.There is a nonzero probability for false posi-tives.
Since the overflow dictionary D has no er-rors, the expected error rate for our dynamic struc-ture is the probability of a random collision in thehash range of each hAifor each bucket cell com-pared.
In our setup we havePr(falsepos) =|Ai|2|f(x)|where |f(x)| is the number of bits of each w-bitcell used for the fingerprint f(x).
w also primar-ily governs space used in the model.
The O-RLMassumes only valid updates and deletions are per-formed (i.e.
we do not remove or update entriesthat were never inserted prior).The O-RLM takes time linear to the input sizefor training and uses worst-case constant time forquerying and deletions where the constant is de-pendent on the number of cells per bucket in A.The number of bucket cells also effects the overallerror rate significantly since smaller ranges reducethe probability of a collision.
However, too fewcells per bucket will result in many full bucketswhen the bucket hash function is not highly IID.2.4 Basic RLM ComparisonsTable 1 compares expected versus observed falsepositive rates for the Bloom filter, TB-LM, and O-RLM obtained by querying a model of approxi-mately 280M events with 100K unseen n-grams.LM Expected Observed RAMLossless 0 0 7450MBBloom 0.0039 0.0038 390MBTB-LM 0.0039 0.0033 640MBO-RLM 0.0039 0.0031 705MBTable 1: Example false postive rates and corre-sponding memory usage for all randomised LMs.We see the bit-based Bloom filter uses signifi-cantly less memory than the cell-based alternativesand the O-RLM consumes more memory than theTB-LM for the same expected error rate.3 Related Work3.1 Randomised Language ModelsTalbot and Osborne (2007) used a Bloom filter(Bloom, 1970) to encode a smoothed LM.
ABloom filter (BF) represents a set S from arbitrarydomain U and supports membership queries suchas?Is x ?
S??.
The BF uses an array of m bits andk independent UHFs each with range 0, .
.
.
,m?1.For insertion, each item is hashed through the khash functions and the resulting target bits are setto one.
During testing, an event x ?
U is passedthrough the same k hash functions and if any bittested is zero then x was not in the support S.The Bloomier filter directly represents key-value pairs by using a table of cells and a family ofk associated hash functions (Chazelle et al, 2004).Each key-value pair is associated with k cells inthe table via a perfect hash function.
Talbot andBrants (2008) used a Bloomier filter to encode aLM.
Before data can be added to the Bloomier fil-ter, a greedy perfect hashing of all entries needs tobe computed in advance; this attempts to associateeach event in the support with one unique table cellso no other entry collides with it.
The procedurecan fail and might need to be repeated many times.Neither of these two randomised language mod-els are suitable for modelling a stream.
Given thefact that the stream is of unbounded size, we areforced to delete items if we wish to maintain aconstant error rate and account for novel n-grams.However, the Bloom filter LM nor the BloomierFilter LM support deletions.
The bit sharing of theBloom filter (BF) LM (Talbot and Osborne, 2007)means deletions may corrupt shared stored events.The Bloomier filter LM (Talbot and Brants, 2008)has a precomputed matching of keys shared be-tween a constant number of cells in the filter array.758Deleting items from a Bloomier Filter without re-computing the perfect hash will corrupt it.3.2 Probabilistic CountingConcurrent work has used approximate countingschemes based on Morris (1978) to estimate insmall space frequencies over a high volume in-put text stream (Van Durme and Lall, 2009; Goyalet al, 2009).
The space savings are due to com-pact storage of counts and retention of only asmall subset of the available n-grams in the datastream.
Since the final LMs are still lossless (mod-ulo counts), the resulting LM needs significantspace.
It is trivial to use probabilistic countingwithin our framework.3.3 Compact Exact Language ModelsRandomised algorithms are not the only com-pact representation schemes.
Church et al (2007)looked at Golomb Coding and Brants et al (2007)used tries in a distributed setting.
These methodsare less succinct than randomised approaches.3.4 Adaptive Language ModelsThere is a large literature on adaptive LMs fromthe speech processing domain (Bellegarda, 2004).The primary difference between the O-RLM andother adaptive LMs is that we add and remove n-grams from the model instead of adapting only theparameters of the current support set.3.5 Domain adaptation in MachineTranslationWithin MT there has been a variety of approachesdealing with domain adaption (for example (Wuet al, 2008; Koehn and Schroeder, 2007).
Typi-cally LMs are interpolated with one another, yield-ing good results.
These models are usually stat-ically trained, exact and unable to deal with anunbounded stream of monolingual data.
Domainadaptation has similarities with streaming, in thatour stream may be non-stationary.
A crucial dif-ference however is that the stream is of unboundedlength, whereas domain adaptation usually as-sumes some finite and fixed training set.4 Stream-based translationStreaming algorithms have numerous applicationsin mainstream computer science (Muthukrishnan,2003) but to date there has been very little aware-ness of this field within computational linguistics.Figure 2: Stream-based translation.
The onlineRLM uses data from the target stream and the lasttest point in the source stream for adaptation.A text stream can be thought of as a unboundedsequence of documents that are time-stamped andwe have access to them in strict chronological or-der.
The volume of the stream is so large we canafford only a limited number of passes over thedata (typically one).Text streams naturally arise on the Web whenmillions of new documents are published each dayin many languages.
For instance, 18 thousandwebsites continuously publish news stories in 40languages and there are millions of multilingualblog postings per day.
There are over 30 billione-mails sent daily and social networking sites, in-cluding services such as Twitter, generate an adun-dance of textual data in real time.
Web crawlersthat spidered all these new documents would pro-duce an unbounded input stream.The stream-based translation scenario is as fol-lows: we assume that each day we see a sourcestream of many new newswire stories that needtranslation.
We also assume a stream of newswirestories in the target language.
Intuitively, since theconcurrent streams are from the same domain, wecan use the contexts provided in the target streamto help with the translation of the source stream(Figure 2).
From a theoretical perspective, sincewe cannot represent the entirety of the stream andwish to maintain a constant error rate, we areforced to throw some information away.Given that the incoming text stream contains fartoo much data to store in its entirety an immediatequestion we would like to answer is: within ourLM, which subset of the target text stream should75918020022024026028030020  25  30  35  40  45  50perplexityweeksReuters 96-97 LM subsets51-week baseline20-week subset test 120-week subset test 2Figure 3: Perplexity results using streamed data.Perplexity decreases as we retrain LMs using datachronologically closer to the (two) test dates.we represent in our model?Using perplexity, we investigated this questionusing a text stream based on Reuter?s RCV1 textcollection (Rose et al, 2002).
This contains 800ktime-stamped newswire stories from a full calen-der year (8.20.1996 - 8.19.1997).
We used theSRILM (Stolcke, 2002) to construct an exact tri-gram model built using all the RCV1 data with theexception of the final week which we held out astest data.
This served as an oracle since we storeall of the stream.We then trained multiple exact LMs of muchsmaller sizes, coined subset LMs, to simulatememory constraints.
For a given date in the RCV1stream, these subset LMs were trained using afixed window of previously seen documents up tothat data.
Then we obtained perplexity results foreach subset LM against our test set.Figure 3 shows an example.
For this experimentsubset LMs were trained using a sliding windowof 20 weeks with the window advancing over aperiod of three weeks each time.
The two arcscorrespond to two different test sets drawn fromdifferent days.
The arcs show that recency has aclear effect: populating LMs using material closerto the test data date produces improved perplexityperformance.
The LM chronologically closest toa given test set has perplexity closest to the resultsof the significantly larger baseline LM which usesall the stream.
As expected, using all of the datayields the lowest perplexity.We note that this is a robust finding, since wealso observe it in other domains.
For example, weEpoch Stream Window1 08.20.1996 to 01.01.19972 01.02.1997 to 04.23.19973 04.24.1997 to 08.18.1997Table 2: The stream timeline is divided into win-dowed epochs for our recency experiments.conducted the same tests over a stream of 18 bil-lion tokens drawn from 80 million time-stampedblog posts downloaded from the web with match-ing results.
The effect of recency on perplexity hasalso been observed elsewhere (see, for example,Rosenfeld (1995) and Whittaker (2001)).Our experiments show that a possible way totackle stream-based translation is to always focusthe attention of the LM on the most recent partof the stream.
This means we remove data fromthe model that came from the receding parts of thestream and replace it with the present.5 SMT Experiments5.1 Experimental SetupWe used publicly available resources for all ourtests: for decoding we used Moses (Koehn andHoang, 2007) and our parallel data was taken fromthe Spanish-English section of Europarl.
For testmaterial, we translated 63 documents (800 sen-tences) from three randomly selected dates spacedthroughout the RCV1 year (January 2nd, April24, and August 19).1 This effectively divided thestream into three epochs between the test dates (table 2).
We held out 300 sentences for minimumerror rate training (MERT) (Och, 2003) and opti-mised the parameters of the feature functions ofthe decoder for each experimental run.The RCV1 is not a large corpus when comparedto the entire web but it is multilingual, chronologi-cal, and large enough to enable us to test the effectof recency in a translation setting.5.2 AdaptionWe looked at a number of ways of adapting theO-RLM:1.
(Random) Randomly sample the stream andfor each new n-gram encountered, insert1As RCV1 is not a parallel corpus we translated the ref-erence documents ourselves.
This parallel corpus is availablefrom the authors.760Order Full Epoch 1 Epoch 31 1.25M 0.6M 0.7M2 14.6 M 6.8M 7.0M3 50.6 M 21.3M 21.7M4 90.3 M 34.8M 35.4M5 114.7M 41.8M 42.6MTotal 271.5M 105M 107.5MTable 3: Distinct n-grams (in millions) encoun-tered in the full stream and example epochs.it and remove some previously inserted n-gram, irrespective of whether it was ever re-quested by the decoder or is a prefix.2.
(Conservative) For each new n-gram en-countered in the stream, insert it in the filterand remove one previously inserted n-gramwhich was never requested by the decoder.To preserve consistency we do not removelower-order grams that are needed to estimatebackoff probability for higher-order smooth-ing.
Counts are updated for n-grams alreadyin the model if the new count observed islarger than the current one.3.
(Severe) Differs from the conservative ap-proach only in that we delete all unused n-grams (i.e.
all those not requested by the de-coder in the previous translation task) fromthe O-RLM before adapting with data fromthe stream.
This means the data structure issparsely populated for all runs.All the TB-LMs and O-RLMs were unpruned 5-gram models and used Stupid-backoff smoothing(Brants et al, 2007) 2 with the backoff parameterset to 0.4 as suggested.
The number of distinct n-grams encountered in the stream for two epochs isshown in Table 3.Table 6 shows translation results using theseadaption strategies.
In practice, the random ap-proach does not work while the conservative andsevere adaption techniques produce equivalent re-sults due to the small proportion of data in themodel that is queried during decoding.
All the MTexperiments that follow use the severe method andthe overflow dictionary always holds less than 1%of the total elements in the model.2Smoothing text input data streams poses an interestingproblem we hope to investigate in the future.Date Lossless TB-LM O-RLMJan 37.83 37.12 37.17Apr 34.88 34.21 34.79Aug 29.05 28.52 28.44Avg 33.92 33.28 33.46Table 4: Baseline translation results in BLEU us-ing data from the first stream epoch with a losslessLM (4.5GB RAM), the TB-LM and the O-RLM(300MB RAM).
All LMs are static.5.3 Training RegimesWe now consider stream-based translation.
Ourfirst naive approach is to continually add new datafrom the stream to the training set without delet-ing anything.
Given a constant memory bound thisstrategy only increases the error rate over time asdiscussed.
Our second, computationally demand-ing approach is, before each test point, to rebuildthe TB-LM from scratch using the stream datafrom the most recent epoch as the training set.This is batch retraining.
The final approach in-crementally retrains online.
This utilizes the sametraining data as above (the stream data from thelast epoch) but instead of full retraining it replacesn-grams currently in the model with unseen n-grams and counts encountered in the data stream.5.4 Streaming Translation ResultsEach table shows translation results for the threedifferent test times in the stream.
All results re-ported use the case-sensitive BLEU score.For our baselines we use static LMs trained onthe first epoch?s data to test all three translationpoints in the source stream.
This is the tradi-tional approach.
We trained an exact, modifiedKneser-Ney smoothed LM (here we do not en-force a memory constraint) and also used the TB-LM and O-RLM to verify our structures adequecy.Results are shown in table 4.
The exact modelgives better performance overall due to the moresophisticated smoothing used.Table 5 shows results for a set of stream-basedLMs using the TB-LM and the O-RLM with mem-ory bounds of 200MB and 300MB.
As expected,the naive models performance degrades over timeas we funnel more data into the TB-LM and theerror rises.
The batch retrained TB-LMs and O-RLMs have constant error rates of 128and 1212andso outperform the naive approach.
Since the train-ing data is identical we see (approximately) equal761Naive TB-LM Batch Retrained TB-LM O-RLMDate 200MB 300MB 200MB 300MB 200MB 300MBJan 35.94 37.12 35.94 37.12 36.44 37.17Apr 33.55 35.79 36.01 35.99 35.87 36.10Aug 22.44 26.07 28.97 29.38 29.00 29.18Avg 30.64 32.99 33.64 34.16 33.77 34.15Table 5: Translation results for stream-based LMs in BLEU.
Performance degrades with time using theNaive approach.
The batch retrained TB-LM and stream-based O-RLM use constant error rates of 128and 1212.performance from the batch retrained and onlinemodels.
We also see some improvement comparedto the static baselines when the LMs use the mostrecent data from the target language stream withrespect to the current translation point.The key difference is that each time we batchretrain the TB-LM, we must compute a perfecthashing of the new training set.
This is computa-tionally demanding since the perfect hashing algo-rithm uses Monte Carlo randomisation which failsroutinely and must be repeated.
To make the al-gorithm tractable the training data set must be di-vided into lexically sorted subsets as well.
Thisrequires extra passes over the data which may notbe trivial in a streaming environment.In contrast, the O-RLM is incrementally re-trained online.
This makes it more resource ef-ficient since we find bits in the model for the n-grams dynamically without using more memorythan we intially set.
Note that even though the O-RLM is theoretically less space efficient than theTB-LM, when using the same amount of memorytranslation performance is comparable.6 O-RLM PropertiesThe previous experiments confirm that the O-RLM can be employed as a LM in an SMT settingbut it is useful to get insight into the intrinsic prop-erties of the data structure.
Many of the propertiesof the model, such as the number of bits per fin-gerprint, follow directly from the TB-LM but therelationship between the overflow dictionary andthe randomised buckets is novel.Figures 4 and 5 shows properties of the O-RLMwhile varying only the number of cells in eachbucket and keeping all other model parametersconstant.
We test membership of n-grams in anunseen corpus against those stored in the table.Our tests were conducted over a larger stream of1.25B n-grams from the Gigaword corpus(Graff,Date Severe Random ConservativeJan 36.44 36.44 36.44Apr 35.87 31.08 35.51Aug 29.00 19.31 29.14Avg 33.77 29.11 33.70Table 6: Adaptation results measured in BLEU.Random deletions degrade performance whenadapting a 200MB O-RLM.2003).
We set our space usage to match the 3.08bytes per n-gram reported in Talbot and Brants(2008) and held out just over 1M unseen n-gramsto test the error rates of our models.In Figure 4 we see a direct correlation betweenmodel error and cells per buckets.
As the num-ber of cells decreases the false positive rate dropsas well since fewer cells to compare against perbucket means a lower chance of producing colli-sions.
If the range is decreased too much thoughmore data is diverted to the overflow dictionarydue to many buckets reaching capacity when in-serting and adapting.
Clearly this is less space ef-ficient.
Figure 5 shows the relationship betweenthe percent of data in the overflow dictionary andthe total cells per bucket.7 ConclusionsOur experiments have shown that for stream-basedtranslation, using recent data can benefit perfor-mance but simply adding entries to a randomisedrepresentation will only reduce translation perfor-mance over time.
We have presented a novel ran-domised language model based on dynamic per-fect hashing that supports online insertions anddeletions.
As a consequence, it is considerablyfaster and more efficient than batch retraining.While not advocating the idea that only smallamounts of data are needed for language mod-76200.0010.0020.0030.0040.0050.0060.00750  100  150  200  250falsepositiveratescells per bucketO-RLM Error rateFigure 4: The O-RLM error rises in correlationwith the number of cells per bucket.00.0020.0040.0060.0080.010.0120.0140.0160.01850  100  150  200  250%of datainoverflowdictionarycells per bucketOverflow Dictionary SizeFigure 5: Too few cells per bucket causes a higherpercentage of the data to be stored in the overflowdictionary due to full buckets.elling, within a bounded amount of space our re-sults show that it is better to have a low error rateand store a wisely chosen fraction of the data thanhaving a high error rate and storing more of it.Clearly tradeoffs will vary between applications.This is the first stream-based randomised lan-guage model and associated machine translationsystem reported in the literature.
Clearly there aremany interesting open questions for future work.For example, can we use small randomised repre-sentations called sketches to compactly representside-information on the stream telling us which as-pects of it we should insert into our data?
Howcan we efficiently deal with smoothing in this set-ting?
Our adaptation scheme is simple and ourdata stream is tractable.
Currently we are con-ducting tests over much larger, higher variancetext streams from crawled blog data.
In the fu-ture we will also consider randomised representa-tions of other adaptive LMs in the literature usinga static background LM in conjunction with ouronline one.
We ultimately hope to deploy large-scale LMs which continuously adapt to the vastamount of material published on the Web withoutincurring significant computational overhead.AcknowledgementsThe authors would like to thank David Talbot,Adam Lopez and Phil Blunsom for their valu-able comments and insight.
This work was sup-ported in part under the GALE program of the De-fense Advanced Research Projects Agency, Con-tract No.
HR0011-06-C-0022.ReferencesJerome R. Bellegarda.
2004.
Statistical languagemodel adaptation: review and perspectives.
SpeechCommunication, 42:93?108.Burton H. Bloom.
1970.
Space/time trade-offs inhash coding with allowable errors.
Commun.
ACM,13(7):422?426.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 858?867.J.
Lawrence Carter and Mark N. Wegman.
1977.
Uni-versal classes of hash functions (extended abstract).In STOC ?77: Proceedings of the ninth annual ACMsymposium on Theory of computing, pages 106?112,New York, NY, USA.
ACM Press.Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, andAyellet Tal.
2004.
The bloomier filter: an ef-ficient data structure for static support lookup ta-bles.
In SODA ?04: Proceedings of the fifteenth an-nual ACM-SIAM symposium on Discrete algorithms,pages 30?39, Philadelphia, PA, USA.
Society for In-dustrial and Applied Mathematics.Kenneth Church, Ted Hart, and Jianfeng Gao.
2007.Compressing trigram language models with Golombcoding.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 199?207,Prague, Czech Republic, June.
Association for Com-putational Linguistics.763Amit Goyal, Hal Daume?
III, and Suresh Venkatasub-ramanian.
2009.
Streaming for large scale NLP:Language modeling.
In North American Chap-ter of the Association for Computational Linguistics(NAACL), Boulder, CO.David Graff.
2003.
English Gigaword.
Linguistic DataConsortium (LDC-2003T05).Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 868?876.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 224?227,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Robert Morris.
1978.
Counting large numbersof events in small registers.
Commun.
ACM,21(10):840?842.Christian Worm Mortensen, Rasmus Pagh, and MihaiPa?trac?cu.
2005.
On dynamic range reporting in onedimension.
In STOC ?05: Proceedings of the thirty-seventh annual ACM symposium on Theory of com-puting, pages 104?111, New York, NY, USA.
ACM.S.
Muthukrishnan.
2003.
Data streams: algorithmsand applications.
In SODA ?03: Proceedings of thefourteenth annual ACM-SIAM symposium on Dis-crete algorithms, pages 413?413, Philadelphia, PA,USA.
Society for Industrial and Applied Mathemat-ics.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Tony Rose, Mark Stevenson, and Miles Whitehead.2002.
The reuters corpus volume 1 - from yester-days news to tomorrows language resources.
In InProceedings of the Third International Conferenceon Language Resources and Evaluation, pages 29?31.Ronald Rosenfeld.
1995.
Optimizing lexical and n-gram coverage via judicious use of linguistic data.In In Proc.
European Conf.
on Speech Technology,pages 1763?1766.A.
Stolcke.
2002.
Srilm ?
an extensible language mod-eling toolkit.
In Proc.
Intl.
Conf.
on Spoken Lan-guage Processing, 2002.David Talbot and Thorsten Brants.
2008.
Randomizedlanguage models via perfect hash functions.
In Pro-ceedings of ACL-08: HLT, pages 505?513, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.David Talbot and Miles Osborne.
2007.
SmoothedBloom filter language models: Tera-scale LMs onthe cheap.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 468?476.Benjamin Van Durme and Ashwin Lall.
2009.
Prob-abilistic counting with randomized storage.
InTwenty-First International Joint Conference on Ar-tificial Intelligence (IJCAI-09), Pasadena, CA, July.E.
W. D. Whittaker.
2001.
Temporal adaptation of lan-guage models.
In In Adaptation Methods for SpeechRecognition, ISCA Tutorial and Research Workshop(ITRW), pages 203?206.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine transla-tion with domain dictionary and monolingual cor-pora.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling2008), pages 993?1000.
Coling 2008 OrganizingCommittee, August.764
