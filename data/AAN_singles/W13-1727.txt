Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207?215,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsLinguistic Profiling based on General?purpose Features and NativeLanguage IdentificationAndrea Cimino, Felice Dell?Orletta, Giulia Venturi and Simonetta MontemagniIstituto di Linguistica Computazionale ?Antonio Zampolli?
(ILC?CNR)ItaliaNLP Lab - www.italianlp.itvia G. Moruzzi, 1 ?
Pisa (Italy){name.surname}@ilc.cnr.itAbstractIn this paper, we describe our approach to na-tive language identification and discuss the re-sults we submitted as participants to the FirstNLI Shared Task.
By resorting to a wide set ofgeneral?purpose features qualifying the lexi-cal and grammatical structure of a text, ratherthan to ad hoc features specifically selectedfor the NLI task, we achieved encouraging re-sults, which show that the proposed approachis general?purpose and portable across differ-ent tasks, domains and languages.1 IntroductionSince the seminal work by Koppel et al(2005),within the Computational Linguistics communitythere has been a growing interest in the NLP?basedNative Language Identification (henceforth, NLI)task.
However, so far, due to the unavailabilityof balanced and wide?coverage benchmark corporaand the lack of evaluation standards it has been dif-ficult to compare the results achieved for this taskwith different methods and techniques (Tetreault etal., 2013).
The First Shared Task on Native Lan-guage Identification (Tetreault et al 2013) can beseen as an answer to the above mentioned problems.In this paper, we describe our approach to na-tive language identification and discuss the resultswe submitted as participants to the First NLI SharedTask.
Following the guidelines by the Shared TaskOrganizers based on the previous literature on thistopic, Native Language Identification is tackled asa text classification task combining NLP?enabledfeature extraction and machine learning: see e.g.Tetreault et al(2013) and Brooke and Hirst (2012).Interestingly, the same methodological paradigm isshared by other tasks like e.g.
author recognition andverification (see e.g.
van Halteren (2004), author-ship attribution (see Juola (2008) for a survey), genreidentification (Mehler et al 2011) as well as read-ability assessment (see Dell?Orletta et al(2011a) foran updated survey), all relying on feature extractionfrom automatically parsed texts and state?of?the?artmachine learning algorithms.
Besides obvious dif-ferences at the level of the typology of selected lin-guistic features and of learning techniques, these dif-ferent tasks share a common approach to the prob-lems they tackle: i.e.
they succeed in determiningthe language variety, the author, the text genre or thelevel of readability of a text by exploiting the distri-bution of different types of linguistic features auto-matically extracted from texts.Our approach to NLI relies on multi?level lin-guistic analysis, covering morpho?syntactic taggingand dependency parsing.
In the NLI literature, therange of features used is wide and includes char-acteristics of the linguistic structure underlying theL2 text, encoded in terms of sequences of charac-ters, words, grammatical categories or of syntac-tic constructions, as well as of the document struc-ture: note however that, in most part of the cases,the exploited features are task?specific.
In our ap-proach, we decided to resort to a wide set of fea-tures ranging across different levels of linguistic de-scription (i.e.
lexical, morpho?syntactic and syntac-tic) without any a priori selection: the same set offeatures was successfully exploited in NLI?relatedtasks, i.e.
focusing on the linguistic form rather than207the content of texts, such as readability assessment(Dell?Orletta et al 2011a) or the classification oftextual genres (Dell?Orletta et al 2012).The exploitation of general features qualifying thelexical and grammatical structure of a text, ratherthan ad hoc features specifically selected for the taskat hand, is not the only peculiarity of our approachto NLI.
Following Biber (1993), we start from theassumption that ?linguistic features from all levelsfunction together as underlying dimensions of vari-ation?.
This choice stems from studies on linguis-tic variation, in particular from Biber and Conrad(2009) who claim that linguistic varieties ?
called?registers?
from a functional perspective ?
differ ?intheir characteristic distributions of pervasive linguis-tic features, not the single occurrence of an indi-vidual feature?.
This is to say that by carrying outthe linguistic analysis of collections of essays eachwritten by different L1 native speakers, we need toquantify the extent to which a given feature occursin each collection, in order to reconstruct the lin-guistic profile underlying each L1 collection: dif-ferences lie at the level of the distribution of linguis-tic features, which can be common and pervasive insome L1 collections but comparatively rare in oth-ers.
This approach is the basis of so?called ?linguis-tic profiling?
of texts, within which ?the occurrencesof a large number of linguistic features in a text, ei-ther individual items or combinations of items, arecounted?
(van Halteren, 2004) with the final aim ofreconstructing the profile of a text.We carried out native language identification intwo steps.
The first step consisted of the identifi-cation of the set of linguistic features characteriz-ing the essays written by different L1 native speak-ers, i.e.
the linguistic profiling of the different sec-tions of TOEFL11 corpus (Blanchard et al 2013)distributed as training and development data.
Inthe second step, the features which turned out tohave highly discriminative power were used for theclassification of essays written by different L1 na-tive speakers.
Essay classification has been carriedout by experimenting with different approaches: i.e.a single?classifier method and two different multi?model ensemble approaches.The paper is organised as follows: after introduc-ing the set of used linguistic features (Section 2),Section 3 illustrates a selection of the linguisticprofiling results obtained with respect to the train-ing section of the TOEFL11 corpus; Section 4 de-scribes the different classification approaches wefollowed and the feature selection process; in Sec-tion 5 achieved results are reported and discussed.2 FeaturesIn this study, we focused on a wide set of featuresranging across different levels of linguistic descrip-tion.
Differing from previous work on NLI, no apriori selection of features was carried out.
Insteadof focusing on particular classes of errors or on dif-ferent types of stylistic idiosyncrasies, we took intoaccount a wide range of features which are typicallyused in studies focusing on the ?form?
of a text,e.g.
on issues of genre, style, authorship or read-ability.
As previously pointed out, this represents apeculiarity of our approach.
This choice makes theselected features language?independent, domain?independent and reusable across different types oftasks, as empirically demonstrated in Dell?Orlettaet al(2011a) where the same set of features hasbeen successfully exploited for readability assess-ment, and in Dell?Orletta et al(2012) where the fea-tures have been used for the classification of differ-ent types of textual genre.
Note that in both cases thelanguage dealt with was Italian: for the NLI SharedTask we had to specialize the feature extraction pro-cess with respect to the English language as well asto the annotation scheme used to represent the un-derlying linguistic structure.The whole set of features we started with is de-scribed below, organised into four main categories:namely, raw text and lexical features as well asmorpho-syntactic and syntactic features.
This pro-posed four?fold partition closely follows the differ-ent levels of linguistic analysis automatically car-ried out on the text being evaluated, i.e.
tokeniza-tion, lemmatization, morpho-syntactic tagging anddependency parsing.2.1 Raw and Lexical Text FeaturesSentence Length, calculated as the average numberof words per sentence.Word Length, calculated as the average number ofcharacters per word.Document Length, calculated as the total number208of words per document.Character bigrams.Word n-grams, including both unigrams and bi-grams.Type/Token Ratio: the Type/Token Ratio (TTR) isa measure of vocabulary variation which has shownto be a helpful measure of lexical variety withina text as well as style marker in an authorship at-tribution scenario: a text characterized by a lowtype/token ratio will contain a great deal of repeti-tion whereas a high type/token ratio reflects vocabu-lary richness and variation.
Due to its sensitivity tosample size, TTR has been computed for text sam-ples of equivalent length (the first 50 tokens).2.2 Morpho?syntactic FeaturesCoarse grained Part-Of-Speech n-grams: distri-bution of unigrams and bigrams of coarse?grainedPoS, corresponding to the main grammatical cate-gories (e.g.
noun, verb, adjective, etc.
).Fine grained Part-Of-Speech n-grams: distribu-tion of unigrams and bigrams of fine?grained PoS,which represent subdivisions of the coarse?grainedtags (e.g.
the class of nouns is subdivided into propervs common nouns, verbs into main verbs, gerundforms, past particles, etc.
).Verbal chunks: distribution of sequences of verbalPoS (also including adverbs).
This feature can beseen as a proxy to capture different aspects of verbalpredication, with particular attention to idiosyncraticusages of verbal mood, tense, person and adverbialmodification.Lexical density: ratio of content words (verbs,nouns, adjectives and adverbs) to the total numberof lexical tokens in a text.2.3 Syntactic FeaturesDependency types n-grams: distribution of uni-grams and bigrams of dependency types calculatedwith respect to i) the hierarchical parse tree structureand ii) the surface linear ordering of words.Dependency triples: distribution of triplets repre-senting a dependency relation consisting of a syn-tactic head (h), the dependency relation type (t) andthe dependent (d).
Two different variants of this fea-ture are distinguished, based on the fact that eitherthe coarse?grained PoS or the word?form of h and dis considered: we will refer to the former as Coarsegrained Part-Of-Speech dependency triples and tothe latter as Lexical dependency triples.
In bothcases, the relative ordering of h and d, i.e.
whether hprecedes or follows d at the level of the linear order-ing of words within the sentence, is also considered.Dependency Subtrees: distribution of dependencysubtrees consisting of a dependency relation (repre-sented as the dependency triple {h, t, d}), the headfather and the dependency relation linking the two.As in the previous case, two different variants of thisfeature are distinguished, based on the fact that ei-ther the coarse grained PoS or the word?forms ofthe nodes in the dependency subtree are considered.Parse tree depth features: this set of features ismeant to capture different aspects of the parse treedepth and includes: a) the depth of the whole parsetree, calculated in terms of the longest path fromthe root of the dependency tree to some leaf; b)the average depth of embedded complement ?chains?governed by a nominal head and including eitherprepositional complements or nominal and adjecti-val modifiers; c) the probability distribution of em-bedded complement ?chains?
by depth.
These fea-tures represent reliable indicators of sentence com-plexity, as stated by, among others, Yngve (1960),Frazier (1985) and Gibson (1998), and they can thusallow capturing specific difficulties of L2 learners.Coarse grained Part-Of-Speech of sentence root:this feature refers to coarse grained POS of the syn-tactic root of a sentence.Arity of verbal predicates: this feature refers tothe number of dependencies (corresponding to eithersubcategorized arguments or modifiers) governed bythe same verbal head.
In the NLI context, it can al-low capturing improper verbal usage by L2 learnersdue to language transfer (e.g.
with pro?drop lan-guages as L1).Subordination features: this set of features ismeant to capture different aspects of the use of sub-ordination and includes: a) the distribution of sub-ordinate vs main clauses; b) the average depth of?chains?
of embedded subordinate clauses and c)the probability distribution of embedded subordinateclauses ?chains?
by depth.
Similarly to parse treedepth, this set of features can be taken to reflect thestructural complexity of sentences and can thus beindicative of specific difficulties of L2 learners.Length of dependency links: measured in terms209of the words occurring between the syntactic headand the dependent.
This is another feature whichreflects the syntactic complexity of sentences (Lin,1996; Gibson, 1998) and which can be successfullyexploited to capture syntactic idiosyncracies of L2learners due to L1 interferences.2.4 Other featuresTwo further features have been considered for NLIpurposes, which were included in the distributeddatasets.
For each document, we have also consid-ered i) the English language proficiency level (high,medium, or low) based on human assessment by lan-guage specialists, and ii) the topic of the essays.3 Linguistic Profiling of TOEFL11 CorpusIn this section, we illustrate the results of linguis-tic profiling carried out on the training and devel-opment sets extracted from the TOEFL11 corpus.This corpus, described in Blanchard et al(2013),contains 1,100 essays per 11 languages (for a to-tal of 12,100 essays) sampled as evenly as possi-ble from 8 prompts (i.e., topics) along with scorelevels (low/medium/high) for each essay.
The con-sidered L1s are: Arabic, Chinese, French, German,Hindi, Italian, Japanese, Korean, Spanish, Telugu,and Turkish.
For the specific purposes of the NLIShared Task, a total of 9,900 essays has been dis-tributed as training data (900 essays per L1), 1,100as development data (100 per L1) and the remaining1,100 essays have been used as test data.We started from the automatic linguistic annota-tion of training and development data whose outputhas been searched for with respect to the features il-lustrated in Section 2.3.1 Linguistic Pre?processingBoth training and development data were au-tomatically morpho-syntactically tagged by thePOS tagger described in Dell?Orletta (2009) anddependency?parsed by the DeSR parser usingMulti?Layer Perceptron as learning algorithm (At-tardi et al 2009), a state?of?the?art linear?timeShift?Reduce dependency parser.
Feature extractionis carried out against the output of the multi?levelautomatic linguistic analysis carried out during thepre?processing stage: lexical and grammatical pat-terns corresponding to the wide typology of selectedfeatures are looked for within each annotation layerand quantified.3.2 Linguistic ProfilingGenerally speaking, linguistic profiling makes itpossible to identify (groups of) texts which are sim-ilar, at least with respect to the ?profiled?
features(van Halteren, 2004).
In what follows we reportthe results of linguistic profiling obtained with re-spect to the 11 L1 sub?corpora considered in thisstudy.
Figure 1 shows the results obtained with re-spect to a selection of the features described in Sec-tion 2.
These results refer to the combined trainingand development data sets: note, however, that wealso calculated the values of these features in the twodatasets separately and it turned out that they do notvary significantly between the two sets.
This factcan be taken as a proof both of the reliability of ourapproach to linguistic profiling and of the relevanceof these features for NLI purposes.Starting from raw textual features (Figures 1(a)and 1(b)), both average sentence length and aver-age word length vary significantly across L1s.
Inparticular, if on the one hand the essays written byArabic and Spanish L1 speakers contain the shortestwords and the longest sentences, on the other handthe Hindi and Telugu L1 essays are characterized bythe longest words; the L1 Japanese and Korean cor-pora contain the shortest sentences.Let us focus now on the distribution of unigramsof coarse grained Parts?Of?Speech.
If we considerthe distributions of determiners and nouns, two fea-tures typically used for NLI purposes (Wong andDras, 2009) which also represent stylistic markersassociated with different linguistic varieties (Biberand Conrad, 2009), it can be noticed (see Fig-ures 1(c) and 1(d)) that for Japanese and Korean theessays show the lowest percentage of determiners,while for Hindi and Telugu they are characterizedby the highest percentage of nouns.For what concerns syntactic features, we observethat essays by Japanese and Korean speakers arecharacterized by quite a different distribution withrespect to the other L1 corpora.
In particular, theyshow the shallowest parse trees, the shortest depen-dency links as well as the shortest ?chains?
of em-bedded complements governed by a nominal head.On the other hand, the essays by Spanish and Ara-210(a) Average word length (b) Average sentence length(c) Distribution of Determiners (d) Distribution of Nouns(e) Average parse tree depth (f) Average depth of embedded complement ?chains?
(g) Average length of the longest dependency link (h) Arity of verbal predicatesFigure 1: Results of linguistic profiling carried out on the combined training and development sections of the TOEFL11corpus.211bic speakers contain the deepest parse trees, for Ital-ian and Spanish we observe the longest dependencylinks and for Hindi and Telugu the longest sequencesof embedded complements.
Moreover, while theessays by Italians are characterised by the highestvalue of arity of verbal predicates, for Hindi, Teluguand Korean essays much lower values are recorded.Interestingly, these linguistic profiling resultsshow similar trends across the 11 languages at dif-ferent levels of linguistic analysis.
For instance, itcan be noted that Japanese and Korean or Italianand Spanish, which belong to two different languagefamilies, show similar distributions of features.
Sim-ilarities have also been recorded in the sub?corporaby Hindi and Telugu speakers, even if these lan-guages do not belong to the same family; we canhypothesize that this might originate from languagecontact phenomena.4 System Description4.1 Machine Learning ClassifierOur approach to Native Language Identification hasbeen implemented in a software prototype, i.e.
aclassifier operating on mopho?syntactically taggedand dependency parsed texts which assigns to eachdocument a score expressing its probability of be-longing to a given L1 class.
The highest score rep-resents to the most probable class.
Given a set offeatures and a training corpus, the classifier creates astatistical model using the feature statistics extractedfrom the training corpus.
This model is used in theclassification of unseen documents.
The set of fea-tures and the machine learning algorithm can be pa-rameterized through a configuration file.For each feature, we have implemented three dif-ferent variants, depending on whether the featurevalue is encoded in terms of: i ) presence/absenceof the feature (binary variant), ii ) the normalizedfrequency (normalized frequency variant), and iii )the normalized tf*idf value (normalized tf*idf vari-ant).
Since the binary feature variant outperformedthe other two, in all the experiments carried out onthe development set reported in Section 5 we illus-trate the results obtained using this variant only.
Thisis in line with the results obtained by Brooke andHirst (2012) and Tetreault et al(2013).
Accordingto (Brooke and Hirst, 2012), a possible explanationis that ?in these relatively short texts, there is highvariability in normalized frequencies, and a simplermetric, by having less variability, is easier for theclassifier to leverage?.
Support Vector Machines(SVM) using LIBSVM (Chang and Lin, 2001) andMaximum Entropy (ME) using MaxEnt1 have beenused as machine learning algorithms.We experimented two classification approaches: asingle classifier method and two ensemble systems,combining the output of several classifiers.The single classifier uses the set of features re-sulting from the feature selection process describedin Section 4.2 and the SVM using linear kernel asmachine learning algorithm.
This choice is due tothe fact that in all the experiments the linear SVMoutperformed the SVM using polynomial kernel.There are two possible explanations for this fact,namely: a) the number of features is much higherthan the number of training instances, accordinglyit might not be necessary to map data to a higherdimensional space, therefore the nonlinear mappingdoes not improve the performance; b) Weston et al(2000) showed that SVMs can indeed suffer in highdimensional spaces where many features are irrele-vant.
Note that in Section 5, we report the results ofthis classifier using different sets of features corre-sponding to the lexical, morpho?syntactic and syn-tactic levels of linguistic analysis.The two ensemble systems combine the outputsof the component classifiers following two differentstrategies.
The first one is based on the majority vot-ing method (henceforth, VoteComb ): the combina-tion strategy is seen as a classical voting problemwhere for each essay is assigned the L1 class thathas been selected from the majority of classifiers.
Incase of ties, the L1 class predicted from the best indi-vidual model (as resulting from the experiments car-ried out on the development set) is selected.
The sec-ond strategy combines the outputs of the componentclassifiers via another classifier (henceforth referredto as meta?classifier): we will refer to this secondstrategy as ClassComb.
The meta?classifier uses asa feature the probability score predicted from eachcomponent classifier for each L1 class.
Differentlyfrom the component classifiers, the meta?classifieris based on polynomial kernel SVM.
In both en-1https://github.com/lzhang10/maxent#readme212semble systems, the component classifiers use linearSVM and ME as machine learning algorithms andexploit different sets of features among the ones re-sulting from the feature selection process describedbelow.4.2 Features Selection ProcessSince our approach to NLI relies on a wide num-ber of general?purpose features, a feature selectionprocess was necessary in order to prune irrelevantand redundant features which could negatively af-fect the classification results.
The selection processstarts taking into account all the n features describedin Section 2.
In each iteration, for each feature fi wegenerate a configuration ci such that fi is disabledand all the other features are enabled.
When an it-eration finishes, we obtain for each ci a correspond-ing accuracy score score(ci) which is computed asthe average of the accuracy obtained by the classi-fier on the development set (ad) and on an internaldevelopment set (ai), corresponding to the 10% ofthe training set, used in order to reduce the overfit-ting risk.
Being cb the best configuration among allthe ci configurations, if score(cb) ?
of the accuracyscores resulting from the previous iterations the pro-cess stops.
Otherwise:1. store in F the pair ?fb, disabled?
;2. for each configuration ci, if score(ci) ?
of theaccuracy scores resulting from the previous it-erations, we store in F the pair ?fi, enabled?;3.
set C = ?cb, score(cb)?where F is a map containing elementsfeature ?
{disabled, enabled} and C is apair that contains the current best configuration cband the corresponding score score(cb).
In eachiteration, we consider only the features which donot occur in F .
At the initialization step F is emptyand C contains the configuration where all theconsidered features are enabled.In spite of the fact that the described selectionprocess does not guarantee to obtain the global opti-mum, it however permitted us to obtain an improve-ment of about 8% with respect to the starting modelindiscriminately using all features.Table 1 lists the features resulting from the fea-ture selection process.
It can be noted that someLexical features:Word n-gramsMorpho?syntactic features:Coarse grained Part-Of-Speech unigramsFine grained Part-Of-Speech bigramsSyntactic features:Dependency types unigramsLexical dependency triplesParse tree depth featuresCoarse grained Part-Of-Speech of sentence rootArity of verbal predicatesSubordination featuresLength of dependency linksTable 1: Features resulting from the feature selection pro-cess.of them coincide with those typically used for NLIpurposes: this is the case of n?grams of words,Parts-Of-Speech and syntactic dependencies.
Inter-estingly, to our knowledge, other features such as ar-ity of verbal predicates, length of dependency linksas well as subordination and parse tree depth fea-tures have not been used for NLI so far, in spite oftheir being widely exploited in the syntactic com-plexity literature (as discussed in Section 2).5 ResultsTable 2 reports the overall Accuracy achieved withthe different classifier models in the NLI classifi-cation task on the official test set as well as theF-measure score recorded for each L1 class.
Thefirst two lines show the accuracies of the two com-bination models, while the last three report the re-sults obtained by the single classifier using i) the setof features resulting by the features selection pro-cess (Best Single), ii) the selected lexical featuresonly (see Table 1) (Lexical ) and iii) the lexical andmorpho?syntactic features (Lex+Morph ).The two combination models outperform allthe single model classifiers: note that ClassCombachieved much better results with respect to Vote-Comb.
By comparing these results with the F-measure scores obtained on the distributed develop-ment data (see Table 3), it can be seen that the rank-ing of the scores achieved by the different classifiersremains the same even if on the test data we obtaineda performance of -2,2% with respect to the develop-213Accuracy ARA CHI FRE GER HIN ITA JAP KOR SPA TEL TURClassComb 77,9 73,8 77,5 83,2 87,3 71,1 86,0 78,8 74,2 70,8 76,2 78,0VoteComb 77,2 74,3 77,0 80,0 87,0 72,8 81,6 79,6 73,8 67,7 77,6 77,6Best Single 76,6 71,9 77,6 75,8 85,7 73,2 82,0 80,0 74,0 69,0 76,9 76,5Lex+Morph 76,4 77,2 76,2 78,6 85,9 72,1 80,4 76,8 71,9 68,0 76,4 76,4Lexical 76,2 71,1 76,5 79,0 87,6 74,5 80,8 77,7 70,8 66,7 79,2 73,4Table 2: Classification results of different classifiers on official test data.ment test set.Let us consider now the results obtained by thesingle model classifiers.
In all cases the Best Singleoutperforms the other two models demonstrating thereliability of the features selection process and thata combination of lexical, morpho?syntactic and syn-tactic features leads to better results.Although the best performing model is the Class-Comb, this is not true for all the 11 languages.
InTable 2, the best results for each L1 are bolded.
In-terestingly, even though Lexical is the worst model,it is the best performing one for three L1s while thebest model, i.e.
ClassComb, for five only.It can be noted that with respect to the devel-opment data set the syntactic features used by theBest Single model allow an increment of +1% asopposed to the Lexical model: this represents amuch higher increase if compared with the resultobtained on the test data, which is +0,4%.
This isan unexpected result since the feature selection de-scribed in Section 4.2 was carried out on an internaldevelopment set in order to prevent the risk of over-fitting on the distributed development data.Classifier AccuracyClassComb 80,1VoteComb 79,3Best Single 78,8Lex+Morph 78,2Lexical 77,8Table 3: Classification results of different classifiers ondistributed development data.6 ConclusionIn this paper, we reported our participation resultsto the First Native Language Identification SharedTask.
By resorting to a wide set of general?purpose features qualifying the lexical and grammat-ical structure of a text, rather than to ad hoc fea-tures specifically selected for the task at hand, weachieved encouraging results.
After a feature se-lection process, new features which to our knowl-edge have never been exploited so far for NLI pur-poses turned out to contribute significantly to thetask.
Interestingly, the same set of features westarted from has been previously successfully ex-ploited in other related tasks, such as readabilityassessment and genre classification, operating onthe Italian language.
The obtained results suggestthat our approach is general?purpose and portableacross different domains and languages.
Further di-rections of research currently include: i) comparisonof results obtained with general purpose features andwith NLI?specific features (e.g.
typical errors or dif-ferent types of stylistic idiosyncrasies specific to L2learners), with a view to combining them to achievebetter results; ii) design and development of new en-semble classification methods as well as new fea-ture selection methods considering not only classesof features but also individual features; iii) testingour approach to NLI on different L2s (e.g.
Italian) .ReferencesGiuseppe Attardi, Felice Dell?Orletta, Maria Simi andJoseph Turian.
2009.
Accurate dependency parsingwith a stacked multilayer perceptron.
In Proceedingsof EVALITA, Evaluation of NLP and Speech Tools forItalian, Reggio Emilia, Italy.Douglas Biber.
1993.
Using Register?diversified Cor-pora for General Language Studies.
ComputationalLinguistics Journal, 19(2): 219?241.Douglas Biber and Susan Conrad.
2009.
Genre, Register,Style.
Cambridge: CUP.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Educational TestingService.214Julian Brooke and Graeme Hirst.
2012.
Robust, Lexical-ized Native Language Identification.
In Proceedingsof COLING 2012, Mumbai, India, 391?408.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIBSVM:a library for support vector machines.
Software avail-able at http://www.csie.ntu.edu.tw/ cjlin/libsvmWalter Daelemans.
2012.
Explanation in ComputationalStylometry.
In A. Gelbukh (ed.)
CICLing 2012, PartII, LNCS 7817, Springer?Verlag, 451?462.Felice Dell?Orletta.
2009.
Ensemble system for Part-of-Speech tagging.
In Proceedings of Evalita?09, Eval-uation of NLP and Speech Tools for Italian, ReggioEmilia, December.Felice Dell?Orletta, Simonetta Montemagni and GiuliaVenturi.
2011a.
READ-IT: Assessing Readability ofItalian Texts with a View to Text Simplification.
InProceedings of the Workshop on ?Speech and Lan-guage Processing for Assistive Technologies?
(SLPAT2011), Edinburgh, July 30, 73?83.Felice Dell?Orletta, Simonetta Montemagni and GiuliaVenturi.
2012.
Genre?oriented Readability Assess-ment: a Case Study.
In Proceedings of the Workshopon Speech and Language Processing Tools in Educa-tion (SLP-TED), 91?98.Lyn Frazier.
1985.
Syntactic complexity.
In D.R.Dowty, L. Karttunen and A.M. Zwicky (eds.
), NaturalLanguage Parsing, Cambridge University Press, Cam-bridge, UK.Edward Gibson.
1998.
Linguistic complexity: Localityof syntactic dependencies.
In Cognition, 68(1), pp.
1?76.Patrick Juola.
2008.
Authorship Attribution.
Now Pub-lishers Inc.Moshe Koppel, Jonathan Schler and Kfir Zigdon.
2005.Automatically determining an anonymous author?s na-tive language.
In Intelligence and Security Informat-ics, vol.
3495, LNCS, Springer?Verlag, 209?217.Dekan Lin.
1996.
On the structural complexity of naturallanguage sentences.
In Proceedings of COLING 1996,pp.
729?733.Ryan McDonald and Joakim Nivre.
2007.
Character-izing the Errors of Data-Driven Dependency ParsingModels.
In Proceedings of EMNLP-CoNLL, 2007,122?131.Alexander Mehler, Serge Sharoff and Marina Santini(Eds.).
2011.
Genres on the Web.
ComputationalModels and Empirical Studies.
Springer Series: Text,Speech and Language Technology.Sze?Meng Jojo Wong and Mark Dras.
2009.
ContrastiveAnalysis and Native Language Identification.
In Pro-ceedings of the Australasian Language Technology As-sociation Workshop.Hans van Halteren.
2004.
Linguistic profiling for authorrecognition and verification.
In Proceedings of theAssociation for Computational Linguistics (ACL04),200?207.Joel Tetreault, Daniel Blanchard, Aoife Cahill and Mar-tin Chodorow.
2012.
Native Tongues, Lost andFound: Resources and Empirical Evaluations in Na-tive Language Identification.
In Proceedings of COL-ING 2012, Mumbai, India, 2585?2602.Joel Tetreault, Daniel Blanchard and Aoife Cahill.
2013.Summary Report on the First Shared Task on NativeLanguage Identification.
In Proceedings of the EighthWorkshop on Building Educational Applications Us-ing NL, Atlanta, GA, USA.Victor H.A.
Yngve.
1960.
Amodel and an hypothesis forlanguage structure.
In Proceedings of the AmericanPhilosophical Society, 444?466.Jason Weston, Sayan Mukherjee, Oliver Chapelle, Mas-similiano Pontil, Tomaso Poggio and Vladimir Nau-movich Vapnik.
2000.
Feature selection for SVMs.
InAdvances in Neural Information Processing Systems13, MIT Press, 668?674.215
