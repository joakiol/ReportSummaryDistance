mmmm/|mmm//|lmmmMorphemes  as Necessary  Conceptfor  S t ruc tures  D iscoveryf rom Untagged CorporaHervd D~jeanGREYC - CNRS - UPRESA 6072Universit~ de Caen - Basse NormandieHerve .
Dej ean@inf  o. un icaen ,  f rAbstractThis paper describes an overview of a methodwhich allows discovery of syntactic structuresfrom untagged corpora.
It is composed ofthreemain steps: the discovery of the grammaticalmorphemes of the language.
Then the con-struction of the chunks which axe a multilin-gual conceptual level allowing the bypass ofthe limping notion of words.
And Finally thediscovery of the relations between chunks.
Wegive an overview of the ditferent procedures re-alized and we especially describe the discov-ery of morphemes.
This operation is dividedinto three steps: the discovery of the most fre-quent morphemes of the language.
Then thediscovery of the other morphemes, and finallythe segmentation f the words of the corpus.We concluded with the procedure of correctionwhich required the chunk level.
The conceptsand algorithms were tested on a twenty nat-ural languages like English, German, Turkish,Vietnamese, Swahili, Finnish, Latin, Indone-sian.1 Introduct ionThe method presented in this paper is inspired bythe distributional pproach developed by AmericanstructuraIists between 1940 and 1950 (Harris, 1951).This approach is characterized by two facts: (a) theuse of corpora nd (b) the use of the notion of distri-bution instead of the sense of elements.
The distri-bution of an element is the set of the environmentsin which the element occurs.
Other works describesystems that induce structures from corpora, butthey use tagged corpora (Brill, 1993), or grammat-ical informations (Brent, 1993), or work with artifi-cial samples (Elman, 1990).
Our originality lies inthe fazt that we only use untagged and non artificialcorpora without specific knowledge about the studiedlanguage.
We try to discover the structures of a nat-ural language from raw texts of this language (on100,000 words).
We show that this kind of discov-ery is possible if we have some expectations of thestructure of Natural Languages and if we use someformal properties.The method relies on structural inguistic con-cepts: the morpheme, the chunk and the linearity ofthe language, i.e.
the corpus is composed of a unidi-mensional sequence of elements.
We first give herean overview of the concepts and general principles,from morphemes to syntactic structures discovery.Then we explain in detail how the segmentation iscarried out.2 The General  Structure ofSentencesNatural Languages are a linear object.
It meansthat sentences are sequences of sounds.
In the caseof written sentences, we consider them as sequencesof letters (or characters).
We also consider that lan-guages are not only sequences ofsound but are struc-tured in several structural evels.
We claim thatthese different levels are formally indicated in thesentences.
How?
Since sentences are unidimensionalobject, a simple way is the use of boundaries in-dicators between the elements which composed thesentences.
Applying this principle on several Lan-guages, we find out three multilingual and hierar-chical levels: the morpheme level, the chunk leveland the clause level.
One usehll formal criterium inthe discovery of these structures is the position ofwords and morphemes relatively to beginnings andends of sentences.The morpheme l vel is already well known in Lin-guistics.
The morphemes are the basic elements ofthe structure.
In this paper, we call morphemes theatTlxes of the language.
These elements are discov-ered during the operation of words segmentation.The morphemes contain as much structural infor-mations as grammatical words and are essential tothe discovery of the syntactic structures.
Section 3explains how we list them.The higher level is the chunk one.
We note thatD~jean 295 Morphemes for Structures DiscoveryHerr6 D6jean (1998) Morphemes as Necessary Concept for Structures Discovery from Untagged Corpora.
In D.M.W.Powers (ed.)
NeMLaP3/CoNLL98 Workshop on Paradigms and Grounding inLanguage Learning, ACL, pp 295-298.some elements have a specific behaviour: they neveroccur at the beginning or at the end of the sentences.For example, the English word the never ends thesentences.
There exists in all the studied languagessimilar elements (words or morphemes) that we canconsider as indicating the beginning or the end ofstructures, the grammatical words as well as themorphemes are consider as boundaries indicators.We systematically consider grammatical words ei-ther as beginning indicator or as ending indicator.
Inpractice, we tie them to their nearest lexical element(the following lexical for beginnings and precedinglexical for endings) In the same way, prefixes areconsidered as beginning and suffixes as ending.
Forexample, both postpositions and inflexional suffixesare consider as ending indicators.
The structuresgenerated by these elements correspond to a lexicalelement (the nucleus of the chunk) surrounded bygrammatical e ements (words or morphemes, gener-ally a combination of both).The chunks may be viewed as non recursivephrases.
Though each chunk of the corpus has notsystematically boundaries indicators, there generallyexists enough chunks which are delimited in order toallow the discovery of these boundaries.
The discov-ery of such indicators is automatically realized for alarge part.The last level is the clause level.
By workingon boundaries indicators, we have noted that someindicators have a more specific behaviour.
Theymainly occur at the beginning or at the end ofsentences.
Furthermore, since some chunks havethe same behaviour, clause boundaries are indi-cated either by morpheme, sole grammatical wordsor chunks.
These elements always characterize el-ements of clauses: conjunctions or verbal phrases.For instance, English conjunction but begins sen-tences 672 times out of 760 occurrences.
This be-haviour is specific to clause boundaries indicators.German clause is, most of the times, dosed eitherby grammatical words such as her, zur~ck, verbalparticles, or by verbal phrases.
In Turkish, the con-junction area (but) occurs 763 times and begins 743times.
The Turkish clause is closed by verbal chunks,which implies that all the verbal morphemes (-tit,yor) axe well marked as absolute ndings.
All thelanguages which have a SOV or OSV structure offerobvious end boundaries for clauses, and languageswhich have VSO or VOS structure offer beginningsboundaries for clauses.
These formal informationsdo not cover all the formal characteristics of the lan-guages, but they offer enough informations in orderto discover the different syntactic relations betweenchunks, and offer a good starting point in order tofind specific structures of a language, the position ofthe finite verb in German for instance.In practice, we note that some languages priv-ilege beginning indicators (prepositional languagesas many European ones), others privilege endingindicators (postpositional languages as Turkish orJapanese) either at chunk level as at clause level,but they generally use the two methods.
Some lan-guages (Asian tonal languages) have a low numberof boundaries indicators that complicates the chunksand clauses discovery.
For the moment, we havestopped this study at clause level (or sequences ofclause), but there perhaps exists higher levels.3 The  Morphemes  D iscoveryWe now explain in details how the morphemes of aparticular language are found.
We refer the read-ers to other works dealing with this problem (Brent,Murthy, and Lunsberg, 1995), (de Marcken, 1995).Our aim is not the realization of a morphologicalanalysis of each word of the corpus, but the pro-duction of the list of the morphemes for a givenlanguage.
We do not try to discover all the mor-phemes contained in the corpus, since only the hun-dred most frequent ones are necessary in order toclimb to chunks level.
The method is inspired bythe works of Zellig Harris.
His algorithm is based onthe number of different letters which follow a givensequence ofletters.
The increase of this number indi-cates a morpheme boundary.
For instance, after theEnglish sequence direc, we only find, in our corpus,one letter t. After direct, we find four letters: /, l, o,and e (directly, director, directed, direction).
This in-crease indicates a boundary between the root (directand the SUffLxes (-ion, -ly, -or and -ed).
The algo-rithm works well when the corpus contains enoughoccurrences of a stem family.
But, it may gener-ate wrong segmentations.
For example from thelist started, startled, startling, the algorithm outputsthis segmentation: start-ed, start-Ied~ start-ling.
Theerrors occur when two kinds of stem families are usedfor the segmentation.
(Harris, 1955) exposes everalvariations more or less complex.
Their implementa-tion does not furnish great improvements.Our idea for improving the segmentation is to di-vide into three steps this operation.
The first stepcomputes the list of the most frequent morphemes.The second steps extends the list by segmentingwords with the help of the morphemes already gener-ated.
The third step consists in the segmentation fall the words with the morphemes obtained at thesecond step.
The algorithm is illustrated with thesuffixes egmentation, but the discovery of prefixesis totally symmetric: we just reverse the letters ofD~jean 296 Morphemes for Structures DiscoveryI111II1111111IIIImBmIImmmmBIImImmmIm.-: ?the words.3.1 The  discovery of  the most frequentmorphemesThe discovery of the most frequent morphemes isbased on Harris algorithm.
We try to find begin-nings or endings of words which have the followingproperty: after a given sequence of letters, we countthe number of different letters.
If this number ishigher than a threshold (half of the letters of the al-phabet), we arrive at a morpheme boundary, exceptin the case we are in the sequence which correspondsto a longer morpheme, a case we can detect.
For ex-ample, before the sequence on, we found 18 differentletters, thus on may be a morpheme.
But 292 ofthese words in the corpus end with ion out of 367which end with on.
Since the longest sequence ionrepresents more than 50% of the word ended by on,we consider that on is a part of the morpheme -ion I.
We only keep on the sequences which have afrequency higher than 100.Table 1: The most frequent morphemes.EnglishFrenchGermanTurkishSwahiliSwahiliVietnamese-e -s -ed -ing -al -ation -ly -ic -ent-s -e -es -ent -er -ds -re -ation -ique-en -e -te -ten -er -es -lich -el-m -in -lar -ler -dan -den -inl -ml-wa -ia -u -eni -o -isha -ana -wewa- m- ku- ali- ni- aka- ki- vi-NONE3.2 The discovery of other morphemesOnce these morphemes are found, we use them inorder to segment words and to find out other mor-phemes thanks to the following rule: For a givensequence of letters (light in Table 3.2), we check onif the next sequences of letters correspond to mor-phemes already found.
If half of them belongs to themorphemes found (like -s -ed -ing -ly -er,  then theothers (-hess -en -est) are also considered as mor-phemes.This algorithm also generates wrong morphemes,but the frequency of them is very low (1 or 2).
Thus,we only keep on new morphemes which have a fre-quency higher than a given threshold (5 in practice).The morphemes with a frequency lower than thisthreshold are not found.
The morphemes list maygreatly depend on the type of corpus used.
The num-ber of morphemes depends on the morphology of thelanguage.
In Vietnamese, no morpheme is found.1Form the sequence on, we generate the morpheme-ation.Table 2: Second ste'Morphemes found-S-ed-ing-ly-er) of the morphemes discovery.words New Morphemeslightlightslightedlightinglightlylighterlightness -hesslightest -estlighten -enIn English, a list of fifty morphemes is generated(Table 3).
The Turkish list contains more than 500morphemes.
We note that morphemes have a sim-ilar behaviour as words: a small number of thempossesses a high frequency and corresponds to themajor occurrences of the corpus.
We do not tryto generate all the morphemes of the corpus, sincethe hundred most frequent morphemes are sufficientfor the construction of the higher level (the chunklevel).
Some morphemes of the list given in Table 3are composed of a sequence of morphemes (ful-ly,ence-s).
In highly morphological languages, most ofthe morphemes correspond to sequence of elemen-tary morphemes.
We do not try to resegment theseelements now.
Because of the presence of one lettermorphemes, the resegmentation inevitably lead tothe segmentation of the morphemes in letters.
Wewait the chunk level in order to refine these mor-phemes (Section 4).Table 3: Final English MorphemessuflLxes:-y -ward -ure -s -ry -ously -ous -ors -or -ness-ments -ment -ly -less -ively -ive -ity -ious -ions -ion-ings -ingly -ing -in -ily -ies -ic -ible -fully -ful -est -es-ers -er -ence -ences -en -ement -ements -ely -ed -e-ations -ation -ance -ances -ally -al -age -ably -able- 'Sprefixes: dis- in- pro- re- un-3.3 The segmentation of the wordsOnce the list of the morphemes i found, we useit for segmenting all the words of the corpus.
Wesegment he words by applying the longest matchalgorithm: we segment each word with the longestmorpheme which matches beginning or ending of theword.
In order to allow the chunks discovery, thereare some words which are not segmented: the mostfrequent ones (5% of the words).
They generally cot-D~\]ean 297 Morphemes for Structures Discoveryrespond to grammatical words, and we do not seg-ment hem in order to make easier the chunks discov-ery.
The following section explains how the lexicalwords which appear in this list are segmented.
Wecheck on the segmentation of 500 words randomlyselected and we obtain 8 segmentations we consideras wrong (as compla-in, forse-en or in German wordantwortest 2 segmented in antwor-test with the mor-pheme -test (correct in lern-test 3 , preterit 2 pers.
).Harris' algorithm realizes the segmentation ofwords during the discovery of morphemes.
The dis-sociation of the two phase allows a more correctsegmentation.
With Harris algorithm, the wordsstartling, startled and started generate the follow-ing segmentation: start-ed, start-led, start-ling (Sec-tion 3).
With our method, the segmentation isstartl-ing, startl-ed and start-ed since -ling and -ledare not morphemes.
It may be generated some errors(as antwortest) but only for few words.4 The  cor rect ion  o f  wordssegmentat ionWe now explain how the frequent lexical words andthe morphemes composed of a sequence of othermorphemes are segmented.
The method use the con-textual informations discovered in the chunk level.During the construction of chunks, we generate bi-grams of morphemes (Table 4).
We use these bi-grams in order to refine the segmentation.
Eachword or morpheme occurring in a context corre-sponding to chunk structure will be segmented.
Forexample, the German word Hauses (house) occur-ring in des Hauses is segmented in des Haus.esthanks to the context des S-es 4.
The algorithm isthe same for sequences of morphemes.
The Frenchsequence antes is segmented is ante-s thanks to thecontexts les S-s.Table 4: Segmentation correction.bigrams correct segmentationGermandes S-es des Hauses des Haus-esich S-te ida machte ich mach-teFrenchles S-s les S-es les S-e-sles S-s les S-antes les S-ante-s~(you) answer.
Antwort-en: to answerZ(you) learned.
Lern-en: to learn.aS for Stem5 The  necess i ty  o f  morphemes  in  aprocedure  o f  d i scoveryThe morphemes level allows the emergence of struc-tures which hardly appear at word level: structureswhich are marked by morphemes like the concor-dance structures.
For example, the French structure(les-S-s S-s) or German one ( des-S-en S-es) are eas-ily found thanks to their frequencies.
Other struc-tures are also easily found like adverb-verb structurein English, characterized by the high frequency ofthe bigrams (S-ly S-ed).
Another useful morphemesare inflectional ones which mark relations betweenchunks at clause level.
The relations between chunksare discover since bigrams composed of grammati-cal words and morphemes belonging to contiguouschunks.
Frequent bigrams generally correspond torelations between two chunks (like S-ed S-ly).
A po-sitional criterium allows the elimination of bad fre-quent bigrams like (o\]-S S-ed) (Noun Complement- Verb sequence): since this bigram ne/rer begins asentence, we consider that the structure is not com-plete and requires another chunk in order to com-plete the relational structure (the-S of-S S-ed).We conclude by claiming that morphemic level isessential nd unavoidable in a procedure of syntacticstructures discovery.Re ferencesBrent, Mickael.
1993.
From grammar to lexicon:Unsupervised learning of lexical syntax.
Compu-tational Linguistics, 19:243-262.Brent, Mick~l, Sreerama K. Murthy, and AndrewLunsberg.
1995.
Discoveringmorphemic suifi.xes :A case study in mdl induction.
In Fifth Inter-national Workshop on AI and Statistics,Ft.
Laud-erdale, florida.Brill, Eric.
1993.
Automatic grammar inductionand parsing free text : a transformation-based ap-proach.
In ACL93.de Marcken; Carl.
1995.
The unsupervised acquisi-tion of a lexicon from continous preech.
Techni-cal report, MIT Artificial Intelligence Lab.
Memo1558.Elman, J.L.
1990.
Finding struture in time.
Cogni-tive Science, 14:179--211.Harris, Zellig.
1951.
Structural Linguistics.
TheUniversity of Chicago Press.Harris, Zellig.
1955.
From phonemes to morphemes.Language, 31(2):190-222.D~jean 298 Morphemes for Structures DiscoveryIIIII!IIIIIIIIIIIIIIIIIIIIII
