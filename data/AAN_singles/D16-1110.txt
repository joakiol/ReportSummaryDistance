Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1042?1047,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsReal-Time Speech Emotion and Sentiment Recognition for InteractiveDialogue SystemsDario Bertero, Farhad Bin Siddique, Chien-Sheng Wu,Yan Wan, Ricky Ho Yin Chan and Pascale FungHuman Language Technology CenterDepartment of Electronic and Computer EngineeringThe Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong[dbertero, fsiddique]@connect.ust.hk, b01901045@ntu.edu.tw,ywanad@connect.ust.hk, eehychan@ust.hk, pascale@ece.ust.hkAbstractIn this paper, we describe our approach of en-abling an interactive dialogue system to rec-ognize user emotion and sentiment in real-time.
These modules allow otherwise con-ventional dialogue systems to have ?empathy?and answer to the user while being aware oftheir emotion and intent.
Emotion recognitionfrom speech previously consists of feature en-gineering and machine learning where the firststage causes delay in decoding time.
We de-scribe a CNN model to extract emotion fromraw speech input without feature engineer-ing.
This approach even achieves an impres-sive average of 65.7% accuracy on six emotioncategories, a 4.5% improvement when com-pared to the conventional feature based SVMclassification.
A separate, CNN-based senti-ment analysis module recognizes sentimentsfrom speech recognition results, with 82.5 F-measure on human-machine dialogues whentrained with out-of-domain data.1 IntroductionInteractive dialogue systems and chatbots have beenaround for a while.
Some, though not all, systemshave statistical and machine learning modules to en-able them to improve overtime.
With the pervasive-ness of such systems on mobile devices, expecta-tions of user experience have also increased.
Weexpect human-machine dialogues to get closer tohuman-human dialogues.
One important factor isthat we expect machines to understand our emotionsand intent and respond with empathy.We propose a module of emotion and sentimentrecognition for an interactive dialogue system.
Thismodule enables the system to assess the user?scurrent emotional state and sentiment, and therebydecide the appropriate response at every dialoguestate.
The dialogue management system handles themixed-initiative dialogues while taking into accountuser emotion and sentiment, in addition to querycontent.
Emotion and sentiment recognition enablesour system to handle user queries previously unseenin training data.
Positive user queries containingpositive emotion and sentiment label would havea positive response, and similarly a negativelylabeled statement would have a negative response.Examples are shown below:User: I lost my job.Response: Sorry to hear that.
Success is in nevergiving up.User: I just graduated from college!Response: Congratulations!
I am happy for you.User: I went on a vacation last month and it waspretty bad, I lost all my luggageResponse: That doesn?t sound so good.
Hope yournext vacation will be a good one.User: My last vacation was amazing, I loved it!Response: That sounds great.
I would like to travelwith you.Meanwhile, dialogue systems like this need tohave real-time recognition of user emotion and sen-timent.
Previous approaches of emotion recognitionfrom speech involve feature engineering (Schuller etal., 2009; Schuller et al, 2010) as a first step whichinvariably causes delay in decoding.
So we are in-terested in investigating a method to avoid featureengineering and instead use a Convolutional Neural1042Network to extract emotion from raw audio input di-rectly.2 Speech RecognitionOur acoustic data is obtained from various publicdomain corpora and LDC corpora, comprised of1385hrs of speech.
We use Kaldi speech recogni-tion toolkit (Povey et al, 2011) to train our acous-tic models.
We train deep neural network hiddenMarkov models (DNN-HMMs) using the raw audiotogether with encode-decode parallel audio.
We ap-ply layer-wise training of restricted Boltzmann ma-chines (RBM) (Hinton, 2010), frame cross-entropytraining with mini-batch stochastic gradient descent(SGD) and sequence discriminative training usingstate Minimum Bayes Risk (sMBR) criterion.The text data, of approximately 90 million sen-tences, includes acoustic training transcriptions, fil-tered sentences of Google 1 billion word LM bench-mark (Chelba et al, 2013), and other multiple do-mains (web news, music, weather).
Our decoder al-lows streaming of raw audio or CELP encoded datathrough TCP/IP or HTTP protocol, and performs de-coding in real time.
The ASR system achieves 7.6%word error rate on our clean speech test data1.3 Real-Time Emotion Recognition fromTime-Domain Raw Audio InputIn recent years, we have seen successful systemsthat gave high classification accuracies on bench-mark datasets of emotional speech (Mairesse et al,2007) or music genres and moods (Schermerhornand Scheutz, 2011).Most of such work consists of two main steps,namely feature extraction and classifier learning,which is tedious and time-consuming.
Extractinghigh and low level features (Schuller et al, 2009),and computing over windows of audio signals typi-cally takes a few dozen seconds to do for each ut-terance, making the response time less than real-time instantaneous, which users have come to ex-pect from interactive systems.
It also requires a lotof hand tuning.
In order to bypass feature engineer-ing, the current direction is to explore methods thatcan recognize emotion or mood directly from time-domain audio signals.
One approach that has shown1https://catalog.ldc.upenn.edu/LDC94S13Agreat potential is using Convolutional Neural Net-works.
In the following sections, we compare an ap-proach of using CNN without feature engineering toa method that uses audio features with a SVM clas-sifier.3.1 DatasetFor our experiments on emotion recognition withraw audio, we built a dataset from the TED-LIUMcorpus release 2 (Rousseau et al, 2014).
It includes207 hours of speech extracted from 1495 TED talks.We annotated the data with an existing commercialAPI followed by manual correction.
We use these 6categories: criticism, anxiety, anger, loneliness, hap-piness, and sadness.
We obtained a total of 2389segments for the criticism category, 3855 for anxi-ety, 12708 for anger, 3618 for loneliness, 8070 forhappy and 1824 for sadness.
The segments have anaverage length slightly above 13 seconds.3.2 Convolutional Neural Network modelThe Convolutional Neural Network (CNN) modelusing raw audio as input is shown in Figure 1.
Theraw audio samples are first down-sampled at 8 kHz,in order to optimize between the sampling rate andrepresentation memory efficiency in case of longersegments.
The CNN is designed with a single fil-ter for real-time processing.
We set a convolutionwindow of size 200, which corresponds to 25 ms,and an overlapping step size of 50, equal to around6 ms.
The convolution layer performs the featureextraction, and models the variations among neigh-boring, overlapping frames.
The subsequent max-pooling combines the contributions of all the frames,and gives as output a segment-based vector.
This isthen fed into a fully connected layer before the finalsoftmax layer.
These last layers perform a similarfunction as those of a fully connected Deep Neu-ral Network (DNN), mapping the max-pooling out-put into a probabilistic distribution over the desiredemotional output categories.During decoding the processing time increaseslinearly with the length of the audio input segment.Thus the largest time contribution is due to the com-putations inside the network (He and Sun, 2015),which with a single convolution layer can be per-formed in negligible time for single utterances.1043Figure 1: Convolutional Neural Network model for emotionclassification from raw audio samples.Figure 2: Convolutional neural network model for sentimentclassification4 Sentiment Inference from Speech andTextConvolutional Neural Networks (CNNs) have re-cently achieved remarkably strong performance alsoon the practically important task of sentence classi-fication (Johnson and Zhang, 2014; Kalchbrenner etal., 2014; Kim, 2014).
In our approach, we use aCNN-based classifier with Word2Vec to analyze thesentiment of recognized speech.We train a CNN with one layer of convolution andmax pooling (Collobert et al, 2011) on top of wordembedding vectors trained on the Google News cor-pus (Mikolov et al, 2013) of size 300.
We applyon top of the word vectors a convolutional slidingwindow of size 3, 4 and 5 to represent multiple fea-tures.
We then apply a max-pooling operation overthe output vectors of the convolutional layer, that al-lows the model to pick up the most valuable infor-mation wherever it happens in the input sentence,and give as output a fixed-length sentence encodingEmotion class SVM CNNCriticism/Cynicism 55.0 61.2Defensiveness/Anxiety 56.3 62.0Hostility/Anger 72.8 72.9Loneliness/Unfulfillment 61.1 66.6Love/Happiness 50.9 60.1Sadness/Sorrow 71.1 71.4Average 61.2 65.7Table 1: Accuracy obtained, percentage, in the ConvolutionalNeural Network model for emotion classification from raw au-dio samples.vector.We employ two distinct CNN channels: the firstuses word embedding vectors directly as input,while the second fine-tunes them via back propaga-tion (Kim, 2014).
All the hidden layer dimensionsare set to 100.
The final softmax layer takes as inputthe concatenated sentence encoding vectors of thetwo channels, and gives as output is the probabil-ity distribution over a binary classification for senti-ment analysis of text transcribed from speech by ourspeech recognizer.To improve the performance of sentiment classi-fication in real time conversation, we compare theperformance on the Movie Review dataset used inKim (2014) with the Twitter sentiment 1402 dataset.This twitter dataset contains a total of 1.6M sen-tences with positive and negative sentiment labels.Before training the CNN model we apply some pre-processing as mentioned in Go et al (2009).5 Experiments5.1 Experimental setupFor the speech emotion detection module we setupour experiments as binary classification tasks, inwhich each segment is classified as either part ofa particular emotion category or not.
For each cat-egory the negative samples were chosen randomlyfrom the clips that did not belong to the positivecategory.
We took 80% of the data as training set,and 10% each as development and test set.
The de-velopment set was used to tune the hyperparametersand determine the early stopping condition.
We im-plemented our CNN with the THEANO framework2www.sentiment140.com1044Corpus Average Length Size Vocabulary Size Words in Word2vecMovie Review 20 10662 18765 16448Twitter 12.97 1600000 273761 79663Table 2: Corpus statistics for text sentiment experiments with CNN.Model Accuracy Precision Recall F-scoreCNN model (trained on Movie Review dataset) 67.8% 91.2% 63.5% 74.8LIWC (keyword based) 73.5% 80.3% 77.3% 77.7CNN model (trained on Twitter dataset) 72.17% 78.64% 86.69% 82.5Table 3: Sentiment analysis result on human-machine dialogue when trained from Twitter and Movie Review dataset(Bergstra et al, 2010).
We chose rectified linearas the non-linear function for the hidden layers, asit generally provided better performance over otherfunctions.
We used standard backpropagation train-ing, with momentum set to 0.9 and initial learningrate to 10?5.
As a baseline we used a linear-kernelSVM model from the LibSVM (Chang and Lin,2011) library with the INTERSPEECH 2009 emo-tion feature set (Schuller et al, 2009), extracted withopenSMILE (Eyben et al, 2010).
These features arecomputed from a series of input frames and output asingle static summary vector, e.g, the smooth meth-ods, maximum and minimum value, mean value ofthe features from the frames (Liscombe et al, 2003).A similar one-layer CNN setup was used alsofor the sentiment module, again with rectified lin-ear as the activation function.
As our dataset con-tains many neutral samples, we trained two distinctCNNs: one for positive sentiment and one for nega-tive, and showed the average results among the twocategories.
For each of the two training corpora wetook 10% as development set.
We used as baselinea method that uses positive and emotion keywordsfrom the Linguistic Inquiry and Word Count (LIWC2015) dictionary (Pennebaker et al, 2015).5.2 Results and discussion5.2.1 Speech emotion recognitionResults obtained by this module are shown in Ta-ble 1.
In all the emotion classes considered our CNNmodel outperformed the SVM baseline, sometimesmarginally (in the angry and sad classes), sometimesmore significantly (happy and criticism classes).
Itis particularly important to point out that our CNNdoes not use any kind of preprocessed features.
Thelower results for some categories, even on the SVMbaseline, may be a sign of inaccuracy in manual la-beling.
We plan to work to improve both the dataset,with hand-labeled samples, and periodically retrainthe model as ongoing work.Processing time is another key factor of our sys-tem.
We ran an evaluation of the time needed toperform all the operations required by our system(down-sampling, audio samples extraction and clas-sification) on a commercial laptop.
The system weused is a Lenovo x250 laptop with a Intel i5 CPU,8 Gb RAM, an SSD hard disk and running LinuxUbuntu 16.04.
Our classifier took an average of162 ms over 10 segments randomly chosen fromour corpus of length greater than 13 s, which corre-sponds to 13 ms per second of speech, hence achiev-ing real-time performance on typical utterances.
Thekey of the low processing time is the lightweightstructure of the CNN, which uses only one filter.
Wereplicated the evaluations with the same 10 segmentson a two-filter CNN, where the second filter spansover 250 ms windows.
Although we obtained higherperformance with this structure in our preliminaryexperiments, the processing time raised to 6.067 s,which corresponds to around 500 ms per second ofspeech.
This is over one order of magnitude higherthan the one filter configuration, making it less suit-able for time constrained applications such as dia-logue systems.5.2.2 Sentiment inference from ASRResults obtained by this module are shown in Ta-ble 3.
Our CNN model got a 6.1% relative im-provement on F-score over the baseline when trainedwith the larger Twitter dataset.
The keyword basedmethod got a slightly better accuracy and precisionand a much lower recall on our relatively smallhuman-machine dialogue dataset (821 short utter-1045ances).
However, we noticed that the keyword basedmethod accuracy fell sharply when tested on thelarger Twitter dataset we used to train the CNN,yielding only 45% accuracy.
We also expect to im-prove our CNN model in the future training it withmore domain specific data, something not possiblewith a thesaurus based method.6 ConclusionIn this paper, we have introduced the emotion andsentiment recognition module for an interactive di-alog system.
We described in detail the two partsinvolved, namely speech emotion and sentimentrecognition, and discussed the results achieved.
Wehave shown how deep learning can be used forsuch modules in this architecture, ranging fromspeech recognition, emotion recognition to senti-ment recognition from dialogue.
More importantly,we have shown that by using a CNN with a singlefilter, it is possible to obtain real-time performanceon speech emotion recognition at 65.7% accuracy,directly from time-domain audio input, bypassingfeature engineering.
Sentiment analysis with CNNalso leads to a 82.5 F-measure when trained fromout-of-domain data.
This approach of creating emo-tionally intelligent systems will help future robots toacquire empathy, and therefore rather than commit-ting harm, they can act as friends and caregivers tohumans.AcknowledgmentsThis work was partially funded by the Hong KongPhd Fellowship Scheme, and partially by grant#16214415 of the Hong Kong Research GrantsCouncil.ReferencesJames Bergstra, Olivier Breuleux, Fre?de?ric Bastien, Pas-cal Lamblin, Razvan Pascanu, Guillaume Desjardins,Joseph Turian, David Warde-Farley, and Yoshua Ben-gio.
2010.
Theano: a cpu and gpu math expressioncompiler.
In Proceedings of the Python for scien-tific computing conference (SciPy), volume 4, page 3.Austin, TX.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:a library for support vector machines.
ACM Trans-actions on Intelligent Systems and Technology (TIST),2(3):27.Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,Thorsten Brants, Phillipp Koehn, and Tony Robin-son.
2013.
One billion word benchmark for measur-ing progress in statistical language modeling.
arXivpreprint arXiv:1312.3005.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Florian Eyben, Martin Wo?llmer, and Bjo?rn Schuller.2010.
Opensmile: the munich versatile and fast open-source audio feature extractor.
In Proceedings of the18th ACM international conference on Multimedia,pages 1459?1462.
ACM.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, 1:12.Kaiming He and Jian Sun.
2015.
Convolutional neuralnetworks at constrained time cost.
In Proceedings ofthe IEEE Conference on Computer Vision and PatternRecognition, pages 5353?5360.Geoffrey Hinton.
2010.
A practical guide to trainingrestricted boltzmann machines.
Momentum, 9(1):926.Rie Johnson and Tong Zhang.
2014.
Effective use ofword order for text categorization with convolutionalneural networks.
arXiv preprint arXiv:1412.1058.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
arXiv preprint arXiv:1404.2188.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
arXiv preprint arXiv:1408.5882.Jackson Liscombe, Jennifer Venditti, and Julia BellHirschberg.
2003.
Classifying subject ratings of emo-tional speech using acoustic features.
In Proceedingsof Eurospeech, pages 725?728.
ISCA.Franc?ois Mairesse, Marilyn A Walker, Matthias R Mehl,and Roger K Moore.
2007.
Using linguistic cuesfor the automatic recognition of personality in conver-sation and text.
Journal of artificial intelligence re-search, pages 457?500.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In Advances in neural information processing systems,pages 3111?3119.J.W.
Pennebaker, R.J. Booth, R.L.
Boyd, and M.E.Francis.
2015.
Linguistic inquiry and word count:Liwc2015.
Austin, TX: Pennebaker Conglomerates.Daniel Povey, Arnab Ghoshal, Gilles Boulianne, LukasBurget, Ondrej Glembek, Nagendra Goel, Mirko Han-nemann, Petr Motlicek, Yanmin Qian, Petr Schwarz,et al 2011.
The kaldi speech recognition toolkit.
In1046IEEE 2011 workshop on automatic speech recognitionand understanding, number EPFL-CONF-192584.IEEE Signal Processing Society.Anthony Rousseau, Paul Dele?glise, and Yannick Este`ve.2014.
Enhancing the ted-lium corpus with selecteddata for language modeling and more ted talks.
InLREC, pages 3935?3939.Paul Schermerhorn and Matthias Scheutz.
2011.
Dis-entangling the effects of robot affect, embodiment,and autonomy on human team members in a mixed-initiative task.
In Proceedings from the InternationalConference on Advances in Computer-Human Interac-tions, pages 236?241.Bjo?rn Schuller, Stefan Steidl, and Anton Batliner.
2009.The interspeech 2009 emotion challenge.
In INTER-SPEECH, volume 2009, pages 312?315.
Citeseer.Bjo?rn Schuller, Stefan Steidl, Anton Batliner, FelixBurkhardt, Laurence Devillers, Christian A Mu?ller,and Shrikanth S Narayanan.
2010.
The interspeech2010 paralinguistic challenge.
In INTERSPEECH,volume 2010, pages 2795?2798.1047
