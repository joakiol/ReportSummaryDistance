Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60?63,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPHuman Evaluation of Article and Noun Number Usage:Influences of Context and Construction VariabilityJohn LeeSpoken Language SystemsMIT CSAILCambridge, MA 02139, USAjsylee@csail.mit.eduJoel TetreaultEducational Testing ServicePrinceton, NJ 08541jtetreault@ets.orgMartin ChodorowHunter College of CUNYNew York, NY 10021martin.chodorow@hunter.cuny.eduAbstractEvaluating systems that correct errors innon-native writing is difficult because ofthe possibility of multiple correct answersand the variability in human agreement.This paper seeks to improve the best prac-tice of such evaluation by analyzing thefrequency of multiple correct answers andidentifying factors that influence agree-ment levels in judging the usage of articlesand noun number.1 IntroductionIn recent years, systems have been developed withthe long-term goal of detecting, in the writing ofnon-native speakers, usage errors involving arti-cles, prepositions and noun number (Knight andChander, 1994; Minnen et al, 2000; Lee, 2004;Han et al, 2005; Peng and Araki, 2005; Brockettet al, 2006; Turner and Charniak, 2007).
Thesesystems should, ideally, be evaluated on a cor-pus of learners?
writing, annotated with accept-able corrections.
However, since such corpora areexpensive to compile, many researchers have in-stead resorted to measuring the accuracy of pre-dicting what a native writer originally wrote inwell-formed text.
This type of evaluation effec-tively makes the assumption that there is one cor-rect form of native usage per context, which maynot always be the case.Two studies have already challenged this ?sin-gle correct construction?
assumption by compar-ing the output of a system to the original text.In (Tetreault and Chodorow, 2008), two humanjudges were presented with 200 sentences and, foreach sentence, they were asked to select whichpreposition (either the writer?s preposition, or thesystem?s) better fits the context.
In 28% of thecases where the writer and the system differed, thehuman raters found the system?s prediction to beequal to or better than the writer?s original prepo-sition.
(Lee and Seneff, 2006) found similar re-sults on the sentence level in a task that evaluatedmany different parts of speech.Percentage Article Number Example42.5% null singular stone22.7% the singular the stone17.6% null plural stones11.4% a/an singular a stone5.7% the plural the stonesTable 1: Distribution of the five article-numberconstructions of head nouns, based on 8 millionexamples extracted from the MetaMetrics LexileCorpus.
The various constructions are illustratedwith the noun ?stone?.2 Research QuestionsIt is clear that using what the author wrote asthe gold standard can underestimate the sys-tem?s performance, and that multiple correct an-swers should be annotated.
Using this annotationscheme, however, raises two questions that havenot yet been thoroughly researched: (1) what is thehuman agreement level on such annotation?
(2)what factors might influence the agreement level?In this paper, we consider two factors: the contextof a word, and the variability of its usage.In the two studies cited above, the human judgeswere shown only the target sentence and did nottake into account any constraint on the choice ofword that might be imposed by the larger con-text.
For PP attachment, human performance im-proves when given more context (Ratnaparkhi etal., 1994).
For other linguistic phenomena, suchas article/number selection for nouns, a larger con-text window of at least several sentences may berequired, even though some automatic methods forexploiting context have not been shown to boostperformance (Han et al, 2005).The second factor, variability of usage, may be60Three years ago John Small, a sheep farmer in the MendipHills, read an editorial in his local newspaper which claimedthat foxes never killed lambs.
He drove down to the pa-per?s office and presented [?
], killed the night before,to the editor.NO-CONTEXT IN-CONTEXTlamb: no noa lamb: yes yes*the lamb: yes nolambs: yes yesthe lambs: yes noTable 2: An example of a completed annotationitem.expressed as the entropy of the distribution of theword?s constructions.
Table 1 shows the over-all distribution of five article/number constructionsfor head nouns, i.e.
all permissible combinationsof number (singular or plural), and article (?a/an?,?the?, or the ?null article?).
A high entropy nounsuch as ?stone?
can appear freely in all of these, ei-ther as a count noun or a non-count noun.
Thiscontrasts with a low entropy noun such as ?pollu-tion?
which is mostly limited to two constructiontypes (?pollution?
and ?the pollution?
).In this paper, we analyze the effects of varyingcontext and noun entropy on human judgments ofthe acceptability of article-number constructions.As a result of this study, we hope to advance thebest practice in annotation for evaluating error de-tection systems.
?3 describes our annotation task.In ?4, we test the ?single correct construction?
as-sumption for article and noun number.
In ?5, weinvestigate to what extent context and entropy con-strain the range of acceptable constructions and in-fluence the level of human agreement.3 Annotation Design3.1 Annotation SchemeTwo native speakers of English participated inan annotation exercise, which took place in twostages: NO-CONTEXT and IN-CONTEXT.
Bothstages used a common set of sentences, each con-taining one noun to be annotated.
That noun wasreplaced by the symbol [?
], and the five possibleconstructions, as listed in Table 1, were displayedbelow the sentence to be judged.In the NO-CONTEXT stage, only the sentencein question and the five candidate constructions(i.e., the bolded parts in Table 2) were shown tothe raters.
They were asked to consider each ofthe five constructions, and to select yes if it wouldnull a theanaphoric not anaphoricsingular 2 2 2 2plural 2 n/a 2 2Table 3: For each noun, two sentences were se-lected from each configuration of number, articleand anaphor.yield a good sentence in some context, and no oth-erwise1.The IN-CONTEXT stage began after a few days?break.
The raters were presented with the samesentences, but including the context, which con-sisted of the five preceding sentences, some ofwhich are shown in Table 2.
The raters were againasked to select yes if the choice would yield agood sentence given the context, and no other-wise.
Among the yes constructions, they wereasked to mark with an asterisk (yes*) the con-struction(s) most likely to have been used in theoriginal text.3.2 Annotation ExampleIn Table 2, ?lambs?
are mentioned in the context,but only in the generic sense.
Therefore, the [?
]in the sentence must be indefinite, resulting in yesfor both ?a lamb?
and ?lambs?.
Of these two con-structions, the singular was judged more likely tohave been the writer?s choice.If the context is removed, then the [?]
in thesentence could be anaphoric, and so ?the lamb?and ?the lambs?
are also possible.
Finally, regard-less of context, the null singular ?lamb?
is not ac-ceptable.3.3 Item SelectionAll items were drawn from the Grade 10 materialin the 2.5M-sentence MetaMetrics Lexile corpus.To avoid artificially inflating the agreement level,we excluded noun phrases whose article or num-ber can be predicted with very high confidence,such as proper nouns, pronouns and non-countnouns.
Noun phrases with certain words, such asnon-article determiners (e.g., this car), possessivepronouns (e.g., his car), cardinal numbers (e.g.,one car) or quantifiers (e.g., some cars), also fallinto this category.
Most of these preclude the arti-cles a and the.1Originally, a third response category was offered to therater to mark constructions that fell in a grey area betweenyes and no.
This category was merged with yes.61Rater NO-CONTEXT IN-CONTEXTyes no yes noR1 62.4% 37.6% 29.3% 70.7%R2 51.8% 48.2% 39.2% 60.8%Table 4: Breakdown of the annotations by raterand by stage.
See ?4 for a discussion.Once these easy cases were filtered out, the headnouns in the corpus were divided into five sets ac-cording to their dominant construction.
Each setwas then ranked according to the entropy of thedistribution of their constructions.
Low entropytypically means that there is one particular con-struction whose frequency dwarfs the others?, suchas the singular definite for ?sun?.
High entropymeans that the five constructions are more evenlyrepresented in the corpus; these are mostly genericobjects that can be definite or indefinite, singularor plural, such as ?stone?.
For each of the fiveconstructions, the three nouns with the highest en-tropies, and three with the lowest, were selected.This yielded a total of 15 ?high-entropy?
and 15?low-entropy?
nouns.For each noun, 14 sentences were drawn ac-cording to the breakdown in Table 3, ensuring abalanced representation of the article and num-ber used in the original text, and the presence ofanaphoric references2.
A total of 368 items3 weregenerated.4 Multiple Correct ConstructionsWe first establish the reliability of the annotationby measuring agreement with the original text,then show how and when multiple correct con-structions can arise.
All results in this section arefrom the IN-CONTEXT stage.Since the items were drawn from well-formedtext, each noun?s original construction should bemarked yes.
The two raters assigned yes to theoriginal construction 80% and 95% of the time,respectively.
These can be viewed as the upperbound of system performance if we assume therecan be only one correct construction.
A stricterceiling can be obtained by considering how of-ten the yes* constructions overlap with the orig-2For practical reasons, we have restricted the study of con-text to direct anaphoric references, i.e., where the same headnoun has already occurred in the context.3In theory, there should be 420 items, but some of the con-figurations in Table 3 are missing for certain nouns, mostlythe low-entropy ones.NO-CONTEXT IN-CONTEXTR1:?
R2:?
yes no yes noyes 846 302 462 77no 108 584 260 1041Table 5: The confusion tables of the two raters forthe two stages.inal one4.
The yes* items overlapped with theoriginal 72% and 83% of the time, respectively.These relatively high figures serve as evidence ofthe quality of the annotation.Both raters frequently found more than onevalid construction ?
18% of the time if onlyconsidering yes*, and 49% if considering bothyes and yes*.
The implication for auto-matic system evaluation is that one could po-tentially underestimate a system?s performanceby as much as 18%, if not more.
For bothraters, the most frequent combinations of yes*constructions were {null-plural,the-plural}, {a-singular,the-singular}, {a-singular,null-plural},and {the-singular,the-plural}.
From the stand-point of designing a grammar-checking system,a system should be less confident in proposingchange from one construction to another withinthe same construction pair.5 Sources of Variation in AgreementIt is unavoidable for agreement levels to be af-fected by how accepting or imaginative the in-dividual raters are.
In the NO-CONTEXT stage,Rater 1 awarded more yes?s than Rater 2, per-haps attributable to her ability to imagine suitablecontexts for some of the less likely constructions.In the IN-CONTEXT stage, Rater 1 used yesmoresparingly than Rater 2.
This reflects their differentjudgments on where to draw the line among con-structions in the grey area between acceptable andunacceptable.We have identified, however, two other factorsthat led to variations in the agreement level: theamount of context available, and the distributionof the noun itself in the English language.
Carefulconsideration of these factors should lead to betteragreement.Availability of Context As shown in Table 4, forboth raters, the context sharply reduced the num-ber of correct constructions.
The confusion tables4Both raters assigned yes* to an average of 1.2 construc-tions per item.62for the two raters are shown in Table 5.
For theNO-CONTEXT stage, they agreed 78% of the timeand the kappa statistic was 0.55.
When context isprovided, human judgment can be expected to in-crease.
Indeed, for the IN-CONTEXT stage, agree-ment rose to 82% and kappa to 0.605.Another kind of context ?
previous mentionof the noun ?
also increases agreement.
Amongnouns originally constructed with ?the?, the kappastatistics for those with direct anaphora was 0.63,but only 0.52 for those without6.Most previous research on article-number pre-diction has only used features extracted from thetarget sentence.
These results suggest that usingfeatures from a wider context should improveperformance.Noun Construction Entropy For the low-entropynouns, we found a marked difference in humanagreement among the constructions depending ontheir frequencies.
For the most frequent construc-tion in a noun?s distribution, the kappa was 0.78;for the four remaining constructions, which aremuch more rare, the kappa was only 0.527.
Theyprobably constitute ?border-line?
cases for whichthe line between yes and no was often hard todraw, leading to the lower kappa.Entropy can thus serve as an additional factorwhen a system decides whether or not to mark ausage as an error.
For low-entropy nouns, the sys-tem should be more confident of predicting a fre-quent construction, but more wary of suggestingthe other constructions.6 Conclusions & Future WorkWe conducted a human annotation exercise on ar-ticle and noun number usage, making two obser-vations that can help improve the evaluation pro-cedure for this task.
First, although the contextsubstantially reduces the range of acceptable an-swers, there are still often multiple acceptable an-swers given a context; second, the level of humanagreement is influenced by the availability of the5This kappa value is on the boundary between ?moderate?and ?substantial?
agreement on the scale proposed in (Landisand Koch, 1977).
The difference between the kappa valuesfor the NO-CONTEXT and IN-CONTEXT stages approachesstatistical significance, z = 1.71, p < 0.10.6The difference between these kappa values is statisticallysignificant, z = 2.06, p < 0.05.7The two kappa values are significantly different, z =4.35, p < 0.001.context and the distribution of the noun?s construc-tions.These observations should help improve notonly the evaluation procedure but also the designof error correction systems for articles and nounnumber.
Entropy, for example, can be incorpo-rated into the estimation of a system?s confidencein its prediction.
More sophisticated contextualfeatures, beyond simply noting that a noun hasbeen previously mentioned (Han et al, 2005; Lee,2004), can also potentially reduce uncertainty andimprove system performance.AcknowledgmentsWe thank the two annotators, Sarah Ohls and Wa-verely VanWinkle.ReferencesC.
Brockett, W. Dolan, and M. Gamon.
2006.
Cor-recting ESL Errors using Phrasal SMT Techniques.Proc.
ACL.N.-R. Han, M. Chodorow, and C. Leacock.
2005.Detecting Errors in English Article Usage by Non-Native Speakers.
Natural Language Engineering,1(1):1?15.K.
Knight and I. Chander.
1994.
Automated Postedit-ing of Documents.
Proc.
AAAI.J.
R. Landis and G. G. Koch.
1977.
The Measurementof Observer Agreement for Categorical Data.
Bio-metrics 33:159?174.J.
Lee.
2004.
Automatic Article Restoration.
Proc.HLT-NAACL Student Research Workshop.J.
Lee and S. Seneff.
2006.
Automatic Grammar Cor-rection for Second-Language Learners.
Proc.
Inter-speech.G.
Minnen, F. Bond, and A. Copestake.
2000.Memory-based Learning for Article Generation.Proc.
CoNLL/LLL.J.
Peng and K. Araki.
2005.
Correction of Arti-cle Errors in Machine Translation Using Web-basedModel.
Proc.
IEEE NLP-KE.A.
Ratnaparkhi, J. Reynar, and S. Roukos.
1994.
AMaximum Entropy Model for Prepositional PhraseAttachment.
Proc.
ARPA Workshop on Human Lan-guage Technology.J.
Tetreault and M. Chodorow.
2008.
Native Judg-ments of Non-Native Usage.
Proc.
COLING Work-shop on Human Judgements in Computational Lin-guistics.J.
Turner and E. Charniak.
2007.
Language Modelingfor Determiner Selection.
Proc.
HLT-NAACL.63
