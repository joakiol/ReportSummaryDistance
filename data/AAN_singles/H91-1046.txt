AA Trell is-Based Algor i thmFor Est imating The Parameters OfHidden Stochastic Context-Free GrammarJulian KupiecXerox Palo Alto Research Center3333 Coyote Hill RoadPalo Alto, CA 94304ABSTRACTThe paper presents a new algorithm for estimating the pa-rameters of a hidden stochastic ontext-free grammar.
In con-trast to the Inside/Outside (I/O) algorithm it does not requirethe grammar to be expressed in Chomsky normal form, and thuscan operate directly on more natural representations of a gram-mar.
The algorithm uses a trellis-based structure as opposed tothe binary branching tree structure used by the I/O algorithm.The form of the trellis is an extension of that used by the For-ward/Backward algorithm, and as a result the algorithm reducesto the latter for components hat can be modeled as finite-statenetworks.
In the same way that a hidden Markov model (HMM)is a stochastic analogue of a finlte-state network, the represen-tation used by the new algorithm is a stochastic analogue of arecursive transition etwork, in which a state may be simple oritself contain an underlying structure.INTRODUCTIONThe algorithm described in this paper is concerned withusing hidden Markov methods for estimation of the param-eters of a stochastic ontext-free grammar from free text.The Forward/Backward (F/B) algorithm (Baum, 1972) iscapable of estimating the parameters of a hidden Markovmodel (i.e.
a hidden stochastic regular grammar) and hasbeen used with success to train text taggers (Jehnek, 1985).In the tagging apphcation the observed symbols are wordsand their underlying lexical categories are the hidden statesof the model.A context-free grammar comprises both lexical (termi-nai) categories and grammatical (nonterminai) categories.One iterative method of estimation in this case involvesparsing each sentence in the training corpus and for eachderivation, accumulating counts of the number of times eachrule is used.
This method has been used by Fujisald et ai.
(1989), and Chitrao & Grishman (1990).
A more efficientmethod is the Inside/Outside algorithm, devised by Baker(1979) for grammars that are expressed in Chomsky nor-mal form.
The algorithm described in this paper relaxesthe requirement for a grammar to be expressed in a nor-mal form, and it is based on a trellis representation that isclosely related to the F /B algorithm, and which reduces toit for finite-state networks.The development of the algorithm has various motiva-tions.
Grammars must provide a large coverage to accom-modate the diversity of expression present in large collec-tions of unrestricted text.
As a result they become moreambiguous.
A stochastic grammar provides the capabilityto resolve ambiguity on a probabilistic basis, providing apractical approach to the problem.
It also provides a way ofmodeling conditional dependence for incomplete grammars,or in the absence of any specific structural information.
Thelatter is exemplified by the approach taken in many currenttaggers, which have a uniform model of second-order depen-dency between word categories.
Kupiec (1989) has experi-mented with the inclusion of networks to model mixed-orderdependencies.The use of hidden Markov methods is motivated by theflexibility they afford.
Text corpora from any domain can beused for training, and there are no restrictions on a grammardue to conventions used during labehng.
The methods alsolend themselves to multi-hngual application.The representation used by the algorithm can be relatedto constituent structures used in other parsers uch as chartparsers, providing a means of embedding this technique inthem.REPRESENTATIONThe representation of a grammar and the basic trellisstructure are discussed in this section.
The starting point is241the conventional HMM network in which symbols are gen-erated at states (rather than on transitions) as described inLevinson et al (1983).
Such a network is represented bythe parameter set (A, B, I) comprising the transition, out-put and initial matrices.
The states in this kind of networkwill be referred to as terminal states from now on, and Willbe represented pictorially with single circles.
As a short-hand convenience in what follows, if the circle contains asymbol, then it is assumed that only that symbol is evergenerated by the state.
(The probability of generating it isthen unity, and zero for all other symbols.)
A single sym-bol is generated by a transition to a terminal state.
Forthe grammars considered here, terminal states correspondto lexical categories.To this parameter set we will add four other parameters(N, F, To2, L).
The boolean Top indicates whether the net-work is to be considered as the top-level network.
Only onenetwork may be assigned as the top-level network, and it isanalogous to the root symbol of a grammar.
The parameterF is the set of final states, specifying the allowable statesin which a network can be considered to have accepted asequence of observations.
A different ype of state will nowbe introduced, called a nonterminal state.
It represents areference to another network and is indicated iagrammati-cally with two concentric ircles.
When a transition is madeto a nonterminal state, the state does not generate any ob-servations per se, but terminal nodes within the referrednetwork do.
A nonterminal state may be associated with asequence of observation symbols, corresponding to the se-quence accepted by the underlying network.
The parameterN is a matrix which indicates whether a state is a terminalor nonterminal state.
Terminal states have a null entry inthe matrix, and nonterminal states have a reference to thenetwork which they represent.
A grammar is usually com-posed of several networks, so each one is referred to with aunique label L.Figure 1 shows how rules in Chomsky normal form arerepresented asnetworks using the above scheme.
The lexicalform of the rules is included, illustrating how the left handside of a rule corresponds to a network label, and the net-work structure is associated with the right-hand side.
Ter-minal states are labeled in lower case and nonterminals inupper case.
The numbers associated with the states aretheir initial probabilities which are also rule probabilities.For terminal nodes in the top-level network, initial prob-abilities have the same meaning as in the F /B algorithm.For all other networks, an initial probabihty corresponds toa production probability.
States which have a non-zero ini-tial probability will be termed "Initial states" from now on.Any sequence recognized by a network must start on an ini-tial state and end on a final state.
In Figure 1, final statesare designated with the annotation "F ' .
Figure 2 shows howthe terminal symbols in Figure 1 may be represented in amore compact style, by a single state having different Bmatrix probabilities for the symbols x and y.Network A()-~~)F  0.5~ F0.3A-> BCA -> xG0.2FA -> yFigure 1: Network and Rules for Chomsky Normal Form?F0.5Figure 2: Representation for Terminal SymbolsTermino logyA grammar is represented as a set A/" of networks, anda component network labeled n is composed of parameters(A, B, I, N,F, Top, n).
To strictly identify an element in theparameter set each element must be a function of its as-sociated network (e.g.
A(n), I(n) etc.).
In the followingsections however, where the reference is obvious this nota-tion has been omitted to make formulae less cumbersome.Thus, given a network n E A\[, an element of its transitionmatrix A, from state i to state j is written a(i, j).
Likewisethe initial probability for state i is I(i).
Assuming that sen-tences are used as text units, an observation sequence mayconsist of Y + 1 words, indexed from 0 to Y:(WO, ~/)1, ~/)2...'tOY)242NetworkState: 1 State: 2Trellis Diagramstart1end2W 0 W 1 W 2Figure 3: An Example Network and Trellis DiagramIt is useful to define a lookup function W(y) which re-turns the index k of the vocabulary entry vk matching theword Wy at positioh y in the sentence.
The vocabulary entrymay be a word or an equivalence class based on categories(Kupiec, 1989).
An element of the output matrix B, rep-resenting the the probability of seeing word wy in terminalstate j is then b(j,W(y)).
In addition, three sets will bementioned:1.
Term(n) The set of terminal states in network n.2.
Nonterm(n) This is the set of nonterminal states innetwork n.3.
Final(n) The set F of final states in network n.The predicate Top(n) indicates that n is a top-level network,and ~ Top(n) indicates it isn't.
Finally, the function N(p, n)returns the network to which state p in network n refers to.
(If p is a terminal state it returns a null value).TRELL IS  D IAGRAMFigure 3 shows the form of the trellis diagrams that areused for the computation of probabilities.
In the F/B al-gorithm a single trellis is used, whose dimensions are thenumber of states in the network and the length of the sen-tence.
A single trellis spans the whole sentence.
In the newalgorithm each network has an associated set of trellises,for subsequences starting at different positions in a sentenceand ending at subsequent ones.
(Only a single trellis start-ing at w0 is shown in Figure 3.)
It can be seen that terminalstate 2 has corresponding nodes in the trellis diagram, butnonterminal state 1 is represented by pairs of nodes.
Onenode of the pair is called the start node and the other istermed the end node.
Paths exist in the trellis for possiblestate transitions between successive words.
However, it isalso implicitly understood that paths also exist between thestart node and subsequent end nodes for each nonterminalstate.
These implicit paths are shown as broken lines in Fig-ure 3 and correspond to paths that enter network A at sometime, and return from it at the same or a later time.
Theprobabilities associated with the implicit paths are assignedby reference to the trellis diagrams of the appropriate net-work.
An implicit path from a start node at position x toan end node at position y for a nonterminal state p can bethought of as a constituent labeled p, that dominates thewords from positions x through to y (inclusive) in a sen-tence.
A network n is deemed to include the sequence wx...w~ if paths exists through the network which will generatethis sequence or a longer one which includes it as a prefix.Thus it is not necessary to be at a final state of n at wordw, to include w .... wy.The algorithm makes use of one set of trellis diagrams tocompute "alpha" probabilities, and another for "beta" prob-abilities.
These are both sprit into terminal, nonterminal-start and nonterminal-end probabilities, corresponding tothe three different ypes of nodes in the trellis diagram.
Forthe alpha set, these are labeled at, a,ts and a,t~ respec-tively.at(x, y, j, n): The probability of generating the words w;...wu inclusive and network n includes them, and being atthe node for terminal state j at position y.,~,(*,u,j,n) = \ [~/a t (x ,y - -1 ,  i ,n)a( i , j ) \ ]  b(j,W(y))+ \ [~cent?
(z ,y - l ,q ,n )a (q , j ) \ ]b ( j ,W(y) )L q0 < y _< Y j , i  6 Term(n)o < ?
< u q ?
Non~erm(n)  (1)~,(~, ~, j, n) = x(j)b(j, w(~))o < ~ < r j e Term(n) (2)243It can be seen that if x = 0 and there are no nonterminalstates, the previous expressions are as in the F/B algorithm.a,t~(x, y,p, n): The probability of generating the wordswz...w~_l inclusive and network n includes them, and beingat the start node of nonterminal state p at position y.crnts(x,y,p,n) = Eat (x ,y -  l, i ,n)a(i,p)i+ ~, ,~.
,~(x ,  y - 1, q, n)a(q, p)q0 < y < Y p,q ?
Nonterm(n)0 <_ x < y i ?
Term(n) (3)o~..(x,x,p,n) = I(p)o < x < Y p ?
Nonterm(n)  (4)Otnte(X, y,p, n): The probability of generating the wordsw .... wu inclusive and network n includes them, and beingat the end node of nonterminal state p at position y.ot,,t~(z,y,p,n) = E a.t~(x,v,p,n)atot,~t(v,y,N(p,n))0 < y < Y p ?
Nonterm(n)o < x < y (5)a,ot.
l (v,  y, n)O<y<YO<v<y~,~t( , , ,  u, i,n) + ~-~ o,.,.
(,,, u,p, n)i pi ?
Term(n) & i ?
Final(n)p ?
Nonterm(n) & p ?
Final(n) (6)The quantity Oltotat(V, y  n) refers to the probability thatnetwork n generates the words w .... w u inclusive and beingin a final state of n at position y.
'The OLtota!
probabilitiescorrespond to the "Inner" (bottom-up) probabilities of theI/O algorithm.
If the network topology for Chomsky normalform shown in Figure 1 is substituted in equation (6), thereeursion for the inner probabilities of the I /O algorithmwill be produced after further substitutions using equations0)46)-In the previous equations (5) and (6) it can be seen thatthe a,te probabilities for a network are defined in terms ofother ones.
They will never be self-referential f the grammaris cycle-free, (i.e.
there are no derivations A ~ A for anynonterminal production A).
In the new algorithm cycles canbe detected and self-referencing avoided.
This is a similarsituation to a chart parser where once a constituent witha given label, start and end position is built, no furtherinstances of it are added.The alpha probabilities are all computed first.
The betaprobabilities can then be calculated, which unlike the F/Balgorithm involve the alpha probabilities because prefixes ofa sentence must be accounted for as well as suffixes.
Thebeta probabilities are described below.
For convenience inlater equations the following functions fl~bo,,, and B,id, arefirst defined:~obo~.
(x, U, n) =m6.Af r:N(r,m)=n O~v~xr ?
Nonterrn(m) (7),e,ia,(~, y, i, n) =a(l, i)Z,(., y + 1, i, n)b(i, W(y + 1))i+E a(l'q) E atot,~,(y+ 1,v,N(q,n))fl ,  te(x,v,q,n)q y<v<Yi ?
Term(n)q ?
Nonterm(n) (8)Bit(x, y, j, n): The probability of generating the prefix w0?
..w,-1 and suffix wu+l...wr given that network n includeswx...wy and is in terminal state j at position y.
The indica-tor function Ind 0 is used in subsequent equations.
Its valueis unity when its argument is true and zero otherwise.
Inaddition, elements that are not explicitly referenced by theranges in the equations are assumed to be zero.f l t (x ,y , j ,n )  = ~side(X ,y , j ,n )+ Ind(j ?
Final(n))flabove(x, y, n)0 < y < Y j ?
Term(n)o < x < ~ (9)~,(~, Y, j, n)O<x<Ygobo~,(x, Y, n)j 6 Term(n)j 6 Final(n) & ,~ Top(n) (10)flt(O,Y,j,n) = 1.0j 6 Term(n)j E Final(n) & Top(n) (11)The previous equations reduce to the definitions for fl in theF/B algorithm when x = 0 and there are no nonterminalstates in the network.244Pnte(~,  y, p, n): The probability of generating the prefixWO...W,-1 and suffix wy+l ... WY given that network n in-cludes wX...wy and is at  the end node of state p at  positionY.Pnte(x, YtPj n) = Pside(~, Y,P, n)It can be seen that the values for Pnte(x,y,p, n) are de-fined in terms of those in other networks which referencen via Pabove.
As a result this computation has a topdownorder.
In contrast, the ant,(z,y,p, n)  probabilities involveother networks that are referred to by network n and soassigned in a bottom-up order.
If the network topologyfor Chomsky normal form is substituted in equation (12),the recursion for the "Outer" probabilities of the 110 al-gorithm can be derived after further substitutions.
The pprobabilities for final states then correspond to the outerprobabilities.Pnts(x, y,p, n): The probability of generating the prefixwo ... w,-1 and suffix w y  ... wy given that network n includesw,...wy-1 and is at  at  the start node of state p at positionYRE-ESTIMATION FORMULAEOnce the alpha and beta probabilities are available, it isstraightforward to obtain new parameter estimates (A, B,  0.The total probability P of a sentence is found from the t o plevel network nTop.There are four different kinds of transition:1.
Terminal node i to terminal node j.2.
Terminal node i to nonterminal start node p.3.
Nonterminal end node p to nonterminal start node q .4 .
Nonterminal end node p to terminal node i .The expected total number of times a transition is madefrom state i to state j  conditioned on the observed sentenceis E($i,j).
The following formulae give E($) for each of theabove cases:0 = x Top(n)0 < x < Y Top(n)x < y < YA new estimate a(;, j )  for a typical transition is then:Only B matrix elements for terminal states are used,and are re-estimated a s  follows.
The expected total numberof times the k'th vocabulary entry vk is generated in statea conditioned on the observed sentence is E(qi,k).
A newestimate for 6(i, k) can then be found:The initial state matrix I is re-estimated as follows:O-~x,O<x<Yi(p)O=xO<x<Yi 6 Term(n) & Top(n)i 6 Term(n) & ~ Top(n) (24)p ~ ,~,,.
(~, ~,p, n)/~.,.
(~, :,p, n)Xp c Uonterm(n) ~ Top(n)p C Nonterm(n) & ~ Top(n) (25)IMPLEMENTATIONInspection of the preceding equations indicates that insimilar fashion to the I/O algorithm, this algorithm has cu-bic complexity in both the length of a sentence and thenumber of states in the grammar.
It has been implementedas a computer program, and verification was conducted infour stages, to facilitate debugging:1.
Using top-level networks having only terminal states,check for exact numerical agreement of re-estimatedparameters with those obtained by applying the F/Balgorithm to the same examples.2.
Create examples involving nonterminals, but whichhave finite-state quivalents, and verify as in stage 1.3.
Create examples with several references to a given net-work, then build a finite-state quivalent in which thereferences are supplanted by network copies havingtied parameters.
Verify as in stage 1.4.
Test using examples in Chomsky normal form andcompare with results from the I /O algorithm.Unscaled arithmetic was employed to simplify the initial im-plementation.
Subsequent versions will include logarithmicscaling to prevent inaccuracies due to arithmetic underflow.The representation would also benefit from the inclusion ofa probability matrix for final states, rather than their usesimply as constraints.As the representation used by the algorithm is a supersetof that used by the F/B algorithm, it conveniently permits"Staged Training".
Components that are finite-state net-works can be pre-trained using the F/B algorithm, and theninserted into a context-free superstructure.
This may bedone to obtain improved initial estimates for the algorithm,and/or to reduce the total amount of computation i volved.Lari and Young (1990) describe xperiments using the I/Oalgorithm in which such pre-training was found useful.
Us-ing the algorithm, the parameters of a context-free grammarcan be trained from a corpus of untagged text.
Values forthe production probabilities are directly available, and noconversion of the rules to or from Chomsky normal form isneeded.
Once trained, a grammar can be used to predict hemost likely syntactic structure of new sentences using a cor-responding analogue of the Cocke-Younger-Kasami parser.CONCLUSIONAn iterative algorithm for estimating the parameters of ahidden stochastic context-free grammar has been described,which is a generalization f the F/B algorithm and the I /Oalgorithm.
The algorithm reduces to the F/B algorithmfor finite-state grammars, and to the I /O algorithm when acontext-free grammar is expressed in Chomsky normal form.ACKNOWLEDGEMENTSI would like to thank Phil Chou and John Maxwell ofXerox PARC, for their helpful comments on this paper.REFERENCES\[1\] Baker, J.K. (1979).
Trainable Grammars for Speech Recog-nition.
Speech Communication Papers for the 97th Meetingo\] the Acoustical Society of America (D.H. Klatt & J.J. Woff,eds), pp.
547-550.\[2\] Bourn, L.E.
(1972).
An Inequality and Associated Maximiza-tion Technique in Statistical Estimation for ProbabilisticFunctions of a Markov Process.
Inequalities, 3, pp.
1-8.\[3\] Chitrao, M.V.
& Grishman, R. (1990).
Statistical Parsingof Messages.
Proceedings of the DARPA Speech and NaturalLanguage Workshop.\[4\] Fujisakl, T., Jelinek, F., Cocke, J., Black, E. & Nishino, T.(1989).
A Probabilistic Parsing Method for Sentence Disam-biguation.
International Workshop on Parsing Technologies,Pittsburgh, PA. pp.
85-94.\[5\] Jelinek, F. (1985).
Markov Source Modeling of Text Gener-ation.
Impact of Processing Techniques on Communication(J.K. Skwirzinski, ed), Nijhoff, Dordrecht.\[6\] Kupiec, J.M.
(1989).
Augmenting a Hidden Markov Modelfor Phrase-Dependent Word Tagging.
Proceedings o\] theDARPA Speech and Natural Language Workshop, CapeCod, MA pp.
92-98.
Morgan Kaufmaxm.\[7\] Lari, K. & Young, S.J.
(1990).
The Estimation of Stochas-tic Context-Free Grammars Using the Inside-Outside Algo-rithm.
Computer Speech and Language, 4, pp.
35-56.\[8\] Levinson, S.E., Rabiner, L.R.
& Sondhi, M.M.
(1983).
An In-troduction to the Application of the Theory of ProbabilisticFunctions of a Markov Process to Automatic Speech Recog-nition.
Bell System Technical Journal, 62, pp.
1035-1074.246
