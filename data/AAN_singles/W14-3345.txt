Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 355?361,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsParmesan: Meteor without Paraphrases with Paraphrased ReferencesPetra Baran?c?
?kov?aInstitute of Formal and Applied LinguisticsCharles University in Prague, Faculty of Mathematics and PhysicsMalostransk?e n?am?est??
25, Prague, Czech Republicbarancikova@ufal.mff.cuni.czAbstractThis paper describes Parmesan, our sub-mission to the 2014 Workshop on Sta-tistical Machine Translation (WMT) met-rics task for evaluation English-to-Czechtranslation.
We show that the Czech Me-teor Paraphrase tables are so noisy thatthey actually can harm the performance ofthe metric.
However, they can be veryuseful after extensive filtering in targetedparaphrasing of Czech reference sentencesprior to the evaluation.
Parmesan first per-forms targeted paraphrasing of referencesentences, then it computes the Meteorscore using only the exact match on thesenew reference sentences.
It shows sig-nificantly higher correlation with humanjudgment than Meteor on the WMT12 andWMT13 data.1 IntroductionThe metric for automatic evaluation of machinetranslation (MT) Meteor1(Denkowski and Lavie,2011) has shown high correlation with humanjudgment since its appearance.
It outperforms tra-ditional metrics like BLEU (Papineni et al., 2002)or NIST (Doddington, 2002) as it explicitly ad-dresses their weaknesses ?
it takes into account re-call, distinguishes between functional and contentwords, allows language-specific tuning of param-eters and many others.Another important advantage of Meteor is thatit supports not only exact word matches betweena hypothesis and its corresponding reference sen-tence, but also matches on the level of stems, syn-onyms and paraphrases.
The Meteor Paraphrasetables (Denkowski and Lavie, 2010) were createdautomatically using the pivot method (Bannardand Callison-Burch, 2005) for six languages.1We use the the version 1.4., which was recently outdatedas the new version 1.5. was released for WMT14The basic setting of Meteor for evaluationof Czech sentences offers two levels of matches- exact and paraphrase.
In this paper, we show theimpact of the quality of paraphrases on the perfor-mance of Meteor.
We demonstrate that the CzechMeteor Paraphrase tables are full of noise and theiraddition to the metric worsens its correlation withhuman judgment.
However, they can be very use-ful (after extensive filtering) in creating new refer-ence sentences by targeted paraphrasing.Parmesan2starts with a simple greedy algo-rithm for substitution of synonymous words froma hypothesis in its corresponding reference sen-tence.
Further, we apply Depfix (Rosa et al., 2012)to fix grammar errors that might arise by the sub-stitutions.Our method is independent of the evaluationmetric used.
In this paper, we use Meteor forits consistently high correlation with human judg-ment and we attempt to tune it further by mod-ifying its paraphrase tables.
We show that re-ducing the size of the Meteor Paraphrase tablesis very beneficial.
On the WMT12 and WMT13data, the Meteor scores computed using only theexact match on our new references significantlyoutperform Meteor with both exact and paraphrasematch on original references.
However, this resultwas not confirmed by this year?s data.We perform our experiments on English-to-Czech translations, but the method is largely lan-guage independent.2 Related WorkOur paraphrasing work is inspired by Kauchak andBarzilay (2006).
They are trying to improve theaccuracy of MT evaluation of Chinese-to-Englishtranslation by targeted paraphrasing, i.e.
makinga reference closer in wording to a hypothesis (MToutput) while keeping its meaning and correctness.2PARaphrasing for MEteor SANs paraphrases355Having a hypothesis H = h1, ..., hnandits corresponding reference translation R =r1, ..., rm, they select a set of candidates C ={?ri, hj?|ri?
R \ H,hj?
H \ R}.C is reduced to pairs of words appearingin the same WordNet (Miller, 1995) synset only.For every pair ?ri, hj?
?
C, hjis eval-uated in the context r1, ..., ri?1,, ri+1, ..., rmand if confirmed, the new reference sentencer1, ..., ri?1, hj, ri+1, ..., rmis created.
This way,several reference sentences might be created, allwith a single changed word with respect to theoriginal one.In Baran?c?
?kov?a et al.
(2014), we experimentwith several methods of paraphrasing of Czechsentences and filtering the Czech Meteor tables.We show that the amount of noise in the multi-word paraphrases is very high and no automaticfiltering method we used outperforms omittingthem completely.
We present an error analysisbased method of filtering paraphrases consistingof pairs of single words, which is used in subsec-tion 3.1.
From several methods of paraphrasing,we achieved the best results a with simple greedymethod, which is presented in section 4.3 DataWe perform our experiments on data sets fromthe English-to-Czech translation task of WMT12(Callison-Burch et al., 2012), WMT13 (Bojar etal., 2013) and WMT14 (Bojar et al., 2014).
Thedata sets contain 13/143/10 files with Czech out-puts of MT systems.
In addition, each data set con-tains one file with corresponding reference sen-tences and one with original English source sen-tences.
We perform morphological analysis andtagging of the hypotheses and the reference sen-tences using Mor?ce (Spoustov?a et al., 2007).The human judgment of hypotheses is availableas a relative ranking of performance of five sys-tems for a sentence.
We calculated the score forevery system by the ?> others?
method (Bojar etal., 2011), which was the WMT12 official sys-tem score.
It is computed aswinswins+loses.
We referto this interpretation of human judgment as silverstandard to distinguish it from the official systemscores, which were computed differently each year(here referred to as gold standard).3We use only 12 of them because two of them (FDA.2878and online-G) have no human judgments.WMT12 WMT13 WMT14WordNet 0.26 0.22 0.24filtered Meteor 1.53 1.29 1.39together 1.59 1.34 1.44Table 1: Average number of one-word paraphrasesper sentence found in WordNet, filtered Meteor ta-bles and their union over all systems.3.1 Sources of ParaphrasesWe use two available sources of Czech para-phrases ?
the Czech WordNet 1.9 PDT (Pala andSmr?z, 2004) and the Meteor Paraphrase Tables(Denkowski and Lavie, 2010).The Czech WordNet 1.9 PDT contains para-phrases of high quality, however, their amount isinsufficient for our purposes.
It contains 13k pairsof synonymous lemmas and only one paraphraseper four sentences on average is found in the data(see Table 1).
For that reason, we employ theCzech Meteor Paraphrase tables, too.
They arequite the opposite of Czech WordNet ?
they arelarge in size, but contain a lot of noise.We attempt to reduce the noise in the Czech Me-teor Paraphrase tables in the following way.
Wekeep only pairs consisting of single words sincewe were not successful in reducing the noise ef-fectively for the multi-word paraphrases (?
).Using Mor?ce, we first perform morphologicalanalysis of all one-word pairs and replace the wordforms with their lemmas.
We keep only pairs ofdifferent lemmas.
Further, we dispose of pairs ofwords that differ in their parts of speech (POS)or contain an unknown word (typically a foreignword).In this way we have reduced 684k paraphrasesin the original Czech Meteor Paraphrase tablesto only 32k pairs of lemmas.
We refer to this tableas filtered Meteor.4 Creating New ReferencesWe create new references similarly to Kauchakand Barzilay (2006).
Let HL, RLbe sets of lem-mas from a hypothesis and a corresponding refer-ence sentence, respectively.
Then we select candi-dates for paraphrasing in the following way: CL={(r, h)|r ?
RLr HL, h ?
HLr RL, rPOS=hPOS}, where rPOSand hPOSdenote the partof speech of the respective lemma.Further, we restrict the set CLto pairs appearingin our paraphrase tables only.
If a word has several356Source The location alone is classic.HypothesisSamotn?e m?
?sto je klasick?e .Actual placeneutis classicneut.The place alone is classic.ReferenceU?z poloha je klasick?a .Already positionfemis classicfem.The position itself is classic.Before DepfixU?z m?
?sto je klasick?a .Already placeneutis classicfem.
*The place itself is classic.New referenceU?z m?
?sto je klasick?e .Already placeneutis classicneut.The place itself is classic.Figure 1: Example of the targeted paraphrasing.
The hypothesis is grammatically correct and has verysimilar meaning as the reference sentence.
The new reference is closer in wording to the hypothesis,but the agreement between the noun and the adjective is broken.
Depfix resolves the error and the finalreference is correct.
Number of overlapping unigrams increased from 2 to 4.metric reference WMT12 WMT13BLEUoriginal 0.751 0.835new 0.834 0.891METEORoriginal 0.833 0.817new 0.927 0.8911 - TERoriginal 0.274 0.760new 0.283 0.781Table 2: Pearson?s correlation of different metricswith the silver standard.paraphrases in CL, we give preference to thosefound in WordNet or even better in both WordNetand filtered Meteor.We proceed word by word from the beginningof the reference sentence to its end.
If a lemmaof a word appears as the first member of a pairin restricted CL, it is replaced by the word formfrom hypothesis that has its lemma as the secondelement of that pair, i.e., by the paraphrase fromthe hypothesis.
Otherwise, the original word thereference sentence is kept.When integrating paraphrases to the referencesentence, it may happen that the sentence becomesungrammatical, e.g., due to a broken agreement(see Figure 1).
Therefore, we apply Depfix (Rosaet al., 2012) ?
a system for automatic correctionof grammatical errors that appear often in English-to-Czech MT outputs.Depfix analyses the input sentences usinga range of natural language processing tools.
Itfixes errors using a set of linguistically-motivatedrules and a statistical component it contains.5 Choosing a metricOur next step is choosing a metric that correlateswell with human judgment.
We experiment withthree common metrics ?
BLEU, Meteor and TER.Based on the results (see Table 2), we decided toemploy Meteor in WMT14 as our metric becauseit shows consistently highest correlations.6 Meteor settingsBased on the positive impact of filtering MeteorParaphrase Tables for targeted lexical paraphras-ing of reference sentences (see the column Ba-sic in Table 4), we experiment with the filteringthem yet again, but this time as an inner part of theMeteor evaluation metric (i.e.
for the paraphrasematch).We experiment with seven different settings thatare presented in Table 3.
All of them are cre-ated by reducing the original Meteor Paraphrasetables, except for the setting referred to as Word-Net in the table.
In this case, the paraphrase tableis generated from one-word paraphrases in CzechWordNet to all their possible word forms foundin CzEng (Bojar et al., 2012).Prior paraphrasing reference sentences and us-ing Meteor with the No paraphr.
setting forcomputing scores constitutes Parmesan ?
our sub-mission to the WMT14 for evaluation English-to-Czech translation.
In the tables with results,357setting size description of the paraphrase tableBasic 684k The original Meteor Paraphrase TablesOne-word 181k Basic without multi-word pairsSame POS 122k One-word + only same part-of-speech pairsDiff.
Lemma 71k Same POS + only forms of different lemmaSame Lemma 51k Same POS + only forms of same lemmaNo paraphr.
0 No paraphrase tables, i.e., exact match onlyWordNet 202k Paraphrase tables generated from Czech WordNetTable 3: Different paraphrase tables for Meteor and their size (number of paraphrase pairs).WMT12reference Basic One-word Same POS Same Lemma Diff.
Lemma No paraphr.
WordNetOriginal 0.833 0.836 0.840 0.838 0.863 0.861 0.863Before Depfix 0.905 0.908 0.911 0.911 0.931 0.931 0.931New 0.927 0.930 0.931 0.932 0.950 0.951 0.951WMT13references Basic One-word Same POS Same Lemma Diff.
Lemma No paraphr.
WordNetOriginal 0.817 0.820 0.823 0.821 0.850 0.848 0.850Before Depfix 0.865 0.867 0.869 0.868 0.895 0.895 0.894New 0.891 0.892 0.893 0.892 0.915 0.915 0.915Table 4: Pearson?s correlation of Meteor and the silver standard.Parmesan scores are highlighted by the box andthe best scores are in bold.7 Results7.1 WMT12 and WMT13The results of our experiments are presented in Ta-ble 44as Pearson?s correlation coefficient of theMeteor scores and the human judgment.
The re-sults in both tables are very consistent.
There isa clear positive impact of the prior paraphrasingof the reference sentences and of applying Depfix.The results also show that independently of a ref-erence sentence used, reducing the Meteor para-phrase tables in evaluation is always beneficial.We use a freely available implementation5of Meng et al.
(1992) to determine whether thedifference in correlation coefficients is statisticallysignificant.
The tests show that Parmesan per-forms better than original Meteor with 99% cer-tainty on the data from WMT12 and WMT13.Diff.
Lemma and WordNet settings give thebest results on the original reference sentences.That is because they are basically a limited version4The results of WMT13 using the gold standard are inTable 5.5http://www.cnts.ua.ac.be/?vincent/scripts/rtest.pyof the paraphrase tables we use for creating ournew references, which contain both all differentlemmas of the same part of speech from MeteorParaphrase tables and all lemmas from the Word-Net.The main reason of the worse performanceof the metric when employing the Meteor Para-phrase tables is the noise.
It is especially apparentfor multi-word paraphrases (Baran?c?
?kov?a et al.,2014); however, there are problems among one-word paraphrases as well.
Significant amount ofthem are pairs of different word forms of a singlelemma, which may award even completely non-grammatical sentences.
This is reflected in the lowcorrelation of the Same Lemma setting.Even worse is the fact that the metric may awardeven parts of the hypothesis left untranslated, asthe original Meteor Paraphrase tables contain En-glish words and their Czech translations as para-phrases.
There are for example pairs: p?senice -wheat6, v?udce - leader, va?rit - cook, poloostrov- peninsula.
For these reasons, the differencesamong the systems are more blurred and the met-ric performs worse than without using the para-phrases.6In all examples the Czech word is the correct translationof the English side.358WMT13references Basic One-word Same POS Same Lemma Diff.
Lemma No paraphr.
WordNetOriginal 0.856 0.859 0.862 0.860 0.885 0.883 0.884Before Depfix 0.894 0.896 0.898 0.897 0.918 0.917 0.917New 0.918 0.918 0.919 0.919 0.933 0.933 0.933Table 5: Pearson?s correlation of Meteor and the gold standard ?
Expected Wins (Bojar et al., 2013).
Theresults corresponds very well with the silver standard in Table 4.frequency Basic No paraphr.WMT12 0.75 0.837 0.869WMT13 0.61 0.818 0.852Table 6: The frequency column shows averagenumber of substitution per sentence using the orig-inal Meteor Paraphrase tables only.
The rest showsPearson?s correlation with the silver standard us-ing these paraphrases.We also experimented with paraphrasing usingthe original Meteor Paraphrase tables for a com-parison.
We used the same pipeline as it is de-scribed in Section 4, but used only original one-word paraphrases from the Meteor Paraphrase ta-bles.
Even though the paraphrase tables are muchlarger than our filtered Meteor tables, the amountof substituted words is much smaller (see Table 6)due to not being lemmatized.
The Basic settingin Table 6 corresponds well with the setting One-word in Table 4 on original reference sentences.The results for No paraphr.
setting in Table 6 out-performs all correlations with original referencesbut cannot compete with our new reference sen-tences created by the filtered Meteor and Word-Net.7.2 WMT14The WMT14 data did not follow similar patternsas data from two previous years.
The results arepresented in Table 7 (the silver standard) and inTable 8 (the gold standard).While reducing the Meteor tables during theevaluation is still beneficial, this is not entirelyvalid about the prior paraphrasing of referencesentences.
The baseline correlation of Meteoris rather high and paraphrasing sometimes helpsand sometimes harms the performance of the met-ric.
Nevertheless, the differences in correlation be-tween the original references and the new ones arevery small (0.012 at most).In contrast to WMT12 and WMT13, the firstphase of paraphrasing before applying Depfixcauses a drop in correlation.
On the other hand,applying Depfix is again always beneficial.With both standards, the best result is achievedon the original reference with the No paraphr.and the WordNet setting.
Parmesan outperformsMeteor by a marginal difference (0.005) on the sil-ver standard, whereas using the gold standard, Me-teor is better by exactly the same margin.
How-ever, the correlation of the two standards is 0.997.There is a distinctive difference between thedata from previous years and this one.
In theWMT14, the English source data for translatingto Czech are sentences originally English or pro-fessionally translated from Czech to English.
Inthe previous years, on the other hand, the sourcedata were equally composed from all competinglanguages, i.e., only fifth/sixth of data is originallyEnglish.One more language involved in the transla-tion seems as a possible ground for the benefi-cial effect of prior paraphrasing of reference sen-tences.
Therefore, we experiment with limitingthe WMT12 and WMT13 data to only sentencesthat are originally Czech or English.
However,Parmesan on this limited translations again signifi-cantly outperforms Meteor and the results (see Ta-ble 9) follow similar patterns as on the whole datasets.8 Conclusion and Future WorkWe have demonstrated a negative effect of noisein the Czech Meteor Paraphrase tables to the per-formance of Meteor.
We have shown that large-scale reduction of the paraphrase tables can bevery beneficial for targeted paraphrasing of ref-erence sentences.
The Meteor scores computedwithout the Czech Meteor Paraphrase tables onthese new reference sentences correlates signifi-cantly better with the human judgment than orig-inal Meteor on the WMT12 and WMT13 data.However, the WMT14 data has not confirmed359WMT14reference Basic One-word Same POS Same Lemma Diff.
Lemma No paraphr.
WordNetOriginal 0.963 0.967 0.965 0.968 0.970 0.973 0.973Before Depfix 0.957 0.958 0.959 0.959 0.965 0.965 0.965New 0.968 0.965 0.969 0.969 0.968 0.968 0.968Table 7: Pearson?s correlation of Meteor and the silver standard.WMT14reference Basic One-word Same POS Same Lemma Diff.
Lemma No paraphr.
WordNetOriginal 0.967 0.968 0.969 0.972 0.972 0.974 0.974Before Depfix 0.958 0.959 0.959 0.960 0.963 0.963 0.963New 0.966 0.966 0.966 0.967 0.962 0.962 0.962Table 8: Pearson?s correlation of Meteor and the gold standard ?
TrueSkill (Bojar et al., 2014).
Note thatas opposed to official WMT14 results, the version 1.4 of Meteor is still used in this table.WMT12reference Basic One-word Same POS Same Lemma Diff.
Lemma No paraphr.
WordNetOriginal 0.781 0.779 0.782 0.772 0.807 0.798 0.801Before Depfix 0.872 0.872 0.874 0.874 0.898 0.899 0.899New 0.897 0.897 0.897 0.897 0.923 0.923 0.923WMT13reference Basic One-word Same POS Same Lemma Diff.
Lemma No paraphr.
WordNetOriginal 0.805 0.810 0.813 0.813 0.842 0.840 0.844Before Depfix 0.843 0.846 0.849 0.848 0.879 0.877 0.877New 0.874 0.877 0.878 0.877 0.877 0.902 0.902Table 9: Pearson?s correlation of Meteor and the silver standard on sentences originally Czech or Englishonly.
In this case, the interpretation of human judgment was computed only on those sentences as well.this result and the improvement was very small.Furthermore, Parmesan performs even worse thanMeteor on the gold standard.In the future, we plan to thoroughly examine thereason for the different performance on WMT14data.
We also intend to make more sophisticatedparaphrases including word order changes andother transformation that cannot be expressed bysimple substitution of two words.
We are also con-sidering extending Parmesan to more languages.AcknowledgmentI would like to thank Ond?rej Bojar for his help-ful suggestions.
This research was partially sup-ported by the grants SVV project number 260 104and FP7-ICT-2011-7-288487 (MosesCore).
Thiswork has been using language resources devel-oped and/or stored and/or distributed by the LIN-DAT/CLARIN project of the Ministry of Edu-cation, Youth and Sports of the Czech Republic(project LM2010013).ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 597?604, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Petra Baran?c?
?kov?a, Rudolf Rosa, and Ale?s Tamchyna.2014.
Improving Evaluation of English-Czech MTthrough Paraphrasing.
In Proceedings of the 9th In-ternational Conference on Language Resources andEvaluation (LREC 2014), Reykjav?
?k, Iceland.
Euro-pean Language Resources Association.Ond?rej Bojar, Milo?s Ercegov?cevi?c, Martin Popel, andOmar F. Zaidan.
2011.
A Grain of Salt for theWMT Manual Evaluation.
In Proceedings of theSixth Workshop on Statistical Machine Translation,WMT ?11, pages 1?11, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Ond?rej Bojar, Zden?ek?Zabokrtsk?y, Ond?rej Du?sek, Pe-tra Galu?s?c?akov?a, Martin Majli?s, David Mare?cek, Ji?r??Mar?s?
?k, Michal Nov?ak, Martin Popel, and Ale?s Tam-360chyna.
2012.
The Joy of Parallelism with CzEng1.0.
In Proc.
of LREC, pages 3921?3928.
ELRA.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Ond?rej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, and Lucia Specia.2014.
Findings of the 2014 Workshop on StatisticalMachine Translation.
In Proceedings of the NinthWorkshop on Statistical Machine Translation, Bal-timore, USA, June.
Association for ComputationalLinguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Seventh Workshop on Statis-tical Machine Translation, pages 10?51, Montr?eal,Canada.Michael Denkowski and Alon Lavie.
2010.METEOR-NEXT and the METEOR Paraphrase Ta-bles: Improved Evaluation Support For Five TargetLanguages.
In Proceedings of the ACL 2010 JointWorkshop on Statistical Machine Translation andMetrics MATR.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-occurrence Statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for Automatic Evaluation.
In Proceedingsof the main conference on Human Language Tech-nology Conference of the North American Chap-ter of the Association of Computational Linguistics,HLT-NAACL ?06, pages 455?462, Stroudsburg, PA,USA.
Association for Computational Linguistics.Xiao-Li Meng, Robert Rosenthal, and Donald B Ru-bin.
1992.
Comparing correlated correlation coeffi-cients.
Psychological bulletin, 111(1):172.George A. Miller.
1995.
WordNet: A LexicalDatabase for English.
COMMUNICATIONS OFTHE ACM, 38:39?41.Karel Pala and Pavel Smr?z.
2004.
Building CzechWordNet.
In Romanian Journal of Information Sci-ence and Technology, 7:79?88.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Rudolf Rosa, David Mare?cek, and Ond?rej Du?sek.2012.
DEPFIX: A System for Automatic Correc-tion of Czech MT Outputs.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, WMT ?12, pages 362?368, Stroudsburg, PA,USA.
Association for Computational Linguistics.Drahom?
?ra Spoustov?a, Jan Haji?c, Jan Votrubec, PavelKrbec, and Pavel Kv?eto?n.
2007.
The Best of TwoWorlds: Cooperation of Statistical and Rule-BasedTaggers for Czech.
In Proceedings of the Work-shop on Balto-Slavonic Natural Language Process-ing, ACL 2007, pages 67?74, Praha.361
