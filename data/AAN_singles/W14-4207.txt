Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 56?65,October 29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsMeasuring Language Closeness by Modeling RegularityJavad Nouri and Roman YangarberDepartment of Computer ScienceUniversity of Helsinki, Finlandfirst.last@cs.helsinki.fiAbstractThis paper addresses the problems of mea-suring similarity between languages?where the term language covers any of thesenses denoted by language, dialect or lin-guistic variety, as defined by any theory.We argue that to devise an effective way tomeasure the similarity between languagesone should build a probabilistic model thattries to capture as much regular correspon-dence between the languages as possible.This approach yields two benefits.
First,given a set of language data, for any twomodels, this gives a way of objectivelydetermining which model is better, i.e.,which model is more likely to be accurateand informative.
Second, given a model,for any two languages we can determine,in a principled way, how close they are.The better models will be better at judg-ing similarity.
We present experiments ondata from three language families to sup-port these ideas.
In particular, our resultsdemonstrate the arbitrary nature of termssuch as language vs. dialect, when appliedto related languages.1 IntroductionIn the context of building and applying NLP toolsto similar languages, language varieties, or di-alects,1we are interested in principled ways ofcapturing the notion of language closeness.Starting from scratch to develop resources andtools for languages that are close to each otheris expensive; the hope is that the cost can be re-duced by making use of pre-existing resources andtools for related languages, which are richer in re-sources.1We use the term language to mean any of: language,dialect, or linguistic variety, according to any definition.In the context of this workshop, we assume thatwe deal with some method, ?Method X,?
that isapplied to two (or more) related languages.
Forexample, Method X may involve adapting/portinga linguistic resource from one language to another;or may be trying to translate between the lan-guages; etc.
We also assume that the success ofMethod X directly depends in some way on howsimilar?or close?the languages are: that is, thesimilarity between the languages is expected to bea good predictor of how successful the applicationof the method will be.
Thus, in such a setting, it isworthwhile to devote some effort to devising goodways of measuring similarity between languages.This is the main position of this paper.We survey some of the approaches to measur-ing inter-language similarity in Section 2.
We as-sume that we are dealing with languages that arerelated genetically (i.e., etymologically).
Relatedlanguages may be (dis)similar on many levels; inthis paper, we focus on similarity on the lexicallevel.
This is admittedly a potential limitation,since, e.g., for Method X, similarity on the level ofsyntactic structure may be more relevant than sim-ilarity on the lexical level.
However, as is done inother work, we use lexical similarity as a ?general?indicator of relatedness between the languages.2Most of the surveyed methods begin with align-ment at the level of individual phonetic segments(phones), which is seen as an essential phase inthe process of evaluating similarity.
Alignmentprocedures are applied to the input data, whichare sets of words which are judged to be similar(cognate)?drawn from the related languages.Once an alignment is obtained using somemethod, the natural question arises: how effectiveis the particular output alignment?Once the data is aligned (and, hopefully, aligned2This is a well-studied subject in linguistics, with gen-eral consensus that the lexical level has stronger resistance tochange than other levels.56well), it becomes possible to devise measures forcomputing distances between the aligned words.One of the simplest of such measures is the Leven-shtein edit distance (LED), which is a crude countof edit operations needed to transform one wordinto one another.
Averaging across LEDs betweenindividual word pairs gives an estimate of the dis-tance between the languages.
The question thenarises: how accurate is the obtained distance?LED has obvious limitations.
LED charges anedit operation for substituting similar as well asdissimilar phones?regardless of how regular (andhence, probable) a given substitution is.
Con-versely, LED charges nothing for substituting aphone x in language A for the same phone in lan-guage B, even if x in A regularly (e.g., always!
)corresponds to y in B.
More sophisticated variantsof LED are then proposed, which try to take intoaccount some aspects of the natural alignment set-ting (such as assigning different weights to dif-ferent edit operations, e.g., by saying that it ischeaper to transform t into d than t into w).Thus, in pursuit of effective similarity mea-sures, we are faced with a sequence of steps:procedures for aligning data produce alignments;from the individual word-level alignments we de-rive distance measures; averaging distances acrossall words we obtain similarity measures betweenlanguages; we then require methods for compar-ing and validating the resulting language distancemeasures.
At various phases, these steps involvesubjectivity?typically in the form of gold stan-dards.
We discuss the kinds of subjectivity en-countered with this approach in detail in Sec-tion 2.1.As an alternative approach, we advocate view-ing closeness between languages in terms of regu-larity in the data: if two languages are very close,it means that either the differences between themare very few, or?if they are many?then theyare very regular.3As the number of differencesgrows and their nature becomes less regular, thelanguages grow more distant.
The goal then is tobuild probabilistic models that capture regularityin the data; to do this, we need to devise algorithmsto discover as much regularity as possible.This approach yields several advantages.
First,a model assigns a probability to observed data.This has deep implications for this task, since it3In the former case, the differences form a short list; in thelatter, the rules describing the differences form a short list.allows us to quantify uncertainty in a principledfashion, rather than commit to ad-hoc decisionsand prior assumptions.
We will show that prob-abilistic modeling requires us to make fewer sub-jective judgements.
Second, the probabilities thatthe models assign to data allow us to build natu-ral distance measures.
A pair of languages whosedata have a higher probability under a given modelare closer than a pair with a lower probability, ina well-defined sense.
This also allows us to de-fine distance between individual word pairs.
Thesmarter the model?i.e., the more regularity it cap-tures in the data?the more we will be able totrust in the distance measures based on the model.Third?and equally important for this problemsetting?this offers a principled way of comparingmethods: if model X assigns higher probability toreal data than model Y, then model X is better, andcan be trusted more.
The key point here is thatwe can then compare models without any ?groundtruth?
or gold-standard, pre-annotated data.One way to see this is by using the model topredict unobserved data.
We can withhold oneword pair (wA, wB) from languages A and B be-fore building the model (so the model does notsee the true correspondence); once the model isbuilt, show it wA, and ask what is the correspond-ing word in B. Theoretically, this is simple: thebest guess for w?Bis simply the one that maxi-mizes the probability of the pair pM(wA, w?B) un-der the model, over all possible strings w?Bin B.4Measuring the distance between wBand w?Btellshow good M is at predicting unseen data.
Now, ifmodel M1consistently predicts better than M2, itis very difficult to argue thatM1is in any sense theworse model; and it is able to predict better onlybecause it has succeeded in learning more aboutthe data and the regularities in it.Thus we can compare different models for mea-suring linguistic similarity.
And this can be donein a principled fashion?if the distances are basedon probabilistic models.The paper is organized as follows.
We continuewith a discussion of related work.
In Section 3we present one particular approach to modeling,based on information-theoretic principles.
In Sec-tion 4 we show some applications of these modelsto several linguistic data sets, from three differentlanguage families.
We conclude with plans for fu-4In practice, this can be done efficiently, using heuristicsto constrain the search over all strings w?Bin B.57ture work, in Section 5.2 Related workIn this section we survey related work on similar-ity measures between languages, and contrast theprinciples on which this work relies against theprinciples which we advocate.2.1 SubjectivityTypically, alignment-based approaches use severalkinds of inputs that have a subjective nature.One such input is the data itself, which is tobe aligned.
For a pair of closely related di-alects, deciding which words to align may ap-pear ?self-evident.?
However, as we take di-alects/languages that are progressively more dis-tant, such judgements become progressively lessself-evident; therefore, in all cases, we shouldkeep in mind that the input data itself is a source ofsubjectivity in measuring similarity based on datathat is comprised of lists of related words.Another source of subjectivity in some of therelated work is gold-standard alignments, whichaccompany the input data.
Again, for very closelanguages, the ?correct?
alignment may appear tobe obvious.
However, we must recognize that thisnecessarily involves subjective judgements fromthe creators of the gold-standard alignment.Further, many alignment methods pre-supposeone-to-one correspondence between phones.
Onone hand, this is due to limitations of the meth-ods themselves (there exist methods for aligningphones in other than one-to-one fashion); on an-other hand, it violates accepted linguistic under-standing that phones do not need to correspondin a one-to-one fashion among close languages.Another potential source of subjectivity comes inthe form of prior assumptions or restrictions onpermissible alignments.5Another common as-sumption is insistence on consonant-to-consonantand vowel-to-vowel alignments.
More relaxed as-sumptions may come in the form of prior proba-bilities of phone alignments.
Although these mayappear ?natural?
in some sense, it is important tokeep in mind that they are ad hoc, and reflect asubjective judgement which may not be correct.After alignment and computation of languagedistance, the question arises: which of the dis-tance measures is more accurate?
Again, one way5One-to-one alignment is actually one such restriction.to answer this question is to resort to gold stan-dards.
For example, this can be done via phylo-genetic clustering; if method A says language l1is closer to l2than to l3, and method B says theopposite (that l1is closer to l3), and if we ?know?the latter to be true?from a gold standard?thenwe can prefer method B.
Further, if we have agold-standard tree for the group of languages, wecan apply tree-distance measures6to check howthe trees generated by a given method differ fromthe gold-standard.
The method that deviates leastfrom the gold standard is then considered best.2.2 Levenshtein-based algorithmsThe Levenshtein algorithm is a dynamic program-ming approach for aligning a word pair (A,B) us-ing a least expensive set of insertion, deletion andsubstitution operations required for transformingA into B.
While the original Levenshtein edit dis-tance is based on these three operations withoutany restrictions, later algorithms adapt this methodby additional edit operations or restrictions.Wieling et al.
(2009) compare several align-ment algorithms applied to dialect pronunciationdata.
These algorithms include several adaptationsof the Levenshtein algorithm and the Pair Hid-den Markov Model.
They evaluate the algorithmsby comparing the resulting pairwise alignments toalignments generated from a set of manually cor-rected multiple alignments.
Standard Levenshteinedit distance is used for comparing the output ofeach algorithm to the gold standard alignment, todetermine which algorithm is preferred.All alignment algorithms based on Levenshteindistance evaluated by Wieling et al.
(2009) restrictaligning vowels with consonants.VC-sensitive Levenshtein algorithm: uses thestandard Levenshtein algorithm, prohibits aligningvowels with consonants, and assigns unit cost forall edit operations.
The only sense in which it cap-tures regularities is the assumption that the samesymbol in two languages represents same sound,which results in assigning a cost of 0 to aligninga symbol to itself.
It also prevents the algorithmfrom finding vowel-to-consonant correspondences(found in some languages), such as u?v, u?l, etc.Levenshtein algorithm with Swap: adds an editoperation to enable the algorithm to capture phe-nomena such as metathesis, via a transposition:6Tree-distance measures are developed in the context ofwork on phylogenetic trees in biological/genetic applications.58aligning ab inA to ba inB costs a single edit oper-ation.
This algorithm also forbids aligning vowelsto consonants, except in a swap.Levenshtein algorithm with generated segmentdistances based on phonetic features: The abovealgorithms assign unit cost for all edit opera-tions, regardless of how the segments are related.Heeringa (2004) uses a variant where the distancesare obtained from differences between phoneticfeatures of the segment pairs.
The authors observethat this is subjective because one could choosefrom different possible feature sets.Levenshtein algorithm with generated segmentdistances based on acoustic features: To avoidsubjectivity of feature selection, Heeringa (2004)experiments with assigning different costs to dif-ferent segment pairs based on how phoneticallyclose they are; segment distances are calculatedby comparing spectrograms of recorded pronun-ciations.
These algorithms do not attempt to dis-cover regularity in data, since they only considerthe word pair at a time, using no information aboutthe rest of the data.Levenshtein algorithm with distances based onPMI: Wieling et al.
(2009) use Point-wise MutualInformation (PMI) as the basis for segment dis-tances.
They assign different costs to segments,and use the entire dataset for each alignment.
PMIfor outcomes x and y of random variables X andY is defined as:pmi(x, y) = log2p(x, y)p(x)p(y)(1)PMI is calculated using estimated probabilities ofthe events.
Since greater PMI shows higher ten-dency of x and y to co-occur, it is reversed andnormalized to obtain a dissimilarity measure tobe used as segment distance.
Details about thismethod are in (Wieling and Nerbonne, 2011).2.3 Other distance measuresEllison and Kirby (2006) present a distance mea-sure based on comparing intra-language lexicaonly, arguing that there is no well-founded com-mon language-independent phonetic space to beused for comparing word forms across languages.Instead, they focus on inferring the distances bycomparing how meanings in language A are likelyto be confused for each other, and comparing it tothe confusion probabilities in language B.Given a lexicon containing mappings from a setof meanings M to a set of forms F , confusionprobability P (m1|m2;L) for each pair of mean-ings (m1,m2) in L is the probability of confus-ing m1for m2.
This probability is formulatedbased on an adaptation of neighborhood activationmodel, and depends on the edit distance betweenthe corresponding forms in the lexicon.
Followingthis approach, they construct a confusion probabil-ity matrix for each language, which can be viewedas a probability distribution.
Inter-language dis-tances are then calculated as the distance betweenthe corresponding distributions, using symmetricKullback-Liebler distance and Rao distance .
Theinferred distances are used to construct a phylo-genetic tree of the Indo-European languages.
Theapproach is evaluated by comparing the resultingtaxonomy to a gold-standard tree, which is re-ported to be a good fit.As with other presented methods, althoughthis method can be seen as measuring distancesbetween languages, there remain two problems.First, they do not reflect the genetic differencesand similarities?and regularities?between thelanguages in a transparent, easily interpretableway.
Second, they offer no direct way to com-pare competing approaches, except indirectly, andusing (subjective) gold-standards.3 Methods for measuring languageclosenessWe now discuss an approach which follows theproposal outlined in Section 1, and allows us tobuild probabilistic models for measuring closenessbetween languages.
Other approaches that rely onprobabilistic modeling would serve equally well.A comprehensive survey of methods for measur-ing language closeness may be found in (Wiel-ing and Nerbonne, 2015).
Work that is proba-bilistically oriented, similarly to our proposed ap-proaches, includes (Bouchard-C?ot?e et al., 2007;Kondrak, 2004) and others.
We next review twotypes of models (some of which are describedelsewhere), which are based on information-theoretic principles.
We discuss how these modelssuit the proposed approach, in the next section.3.1 1-1 symbol modelWe begin with our ?basic?
model, describedin (Wettig and Yangarber, 2011; Wettig etal., 2011), which makes several simplifyingassumptions?which the subsequent, more ad-vanced models relax (Wettig et al., 2012; Wettig59et al., 2013).7The basic model is based on align-ment, similarly to much of the related work men-tioned above: for every word pair in our data set?the ?corpus?
?it builds a complete alignment forall symbols (Wettig et al., 2011).
The basic modelconsiders pairwise alignments only, i.e., two lan-guages at a time; we call them the source andthe target languages.
Later models relax this re-striction by using N-dimensional alignment, withN > 2 languages aligned simultaneously.
Thebasic model allows only 1-1 symbol alignments:one source symbol8may correspond to one tar-get symbol?or to the empty symbol  (which wemark as ?.?).
More advanced models align sub-strings of more than one symbol to each other.
Thebasic model also ignores context, whereas in re-ality symbol correspondences are heavily condi-tioned on their context.
Finally, the basic modeltreats the symbols as atoms, whereas more ad-vanced models treat the symbols as vectors of dis-tinctive features.We distinguish between the raw, observed dataand complete data?i.e., complete with the align-ment; the hidden data is where the insertions anddeletions occur.
For example, if we ask what isthe ?correct?
alignment between Finnish vuosi andKhanty al (cognate words from these two Uraliclanguages, both meaning ?year?
):v u o .
s i v u o s i| | | | | | | | | | |.
a .
l .
.
.
.
a l .are two possible alignments, among many oth-ers.
From among all alignments, we seek the bestalignment: one that is globally optimal, i.e., onethat is consistent with as many regular sound cor-respondences as possible.
This leads to a chicken-and-egg problem: on one hand, if we had the bestalignment for the data, we could simply read offa set of rules, by observing which source sym-bol corresponds frequently to which target sym-bol.
On the other hand, if we had a completeset of rules, we could construct the best align-ment, by using dynamic programming (`a la one ofthe above mentioned methods, since the costs ofall possible edit operations are determined by therules).
Since at the start we have neither, the rulesand the alignment are bootstrapped in tandem.7The models can be downloaded from ety-mon.cs.helsinki.fi8In this paper, we equate symbols with sounds: we assumeour data to be given in phonetic transcription.Following the Minimum Description Length(MDL) principle, the best alignment is the one thatcan be encoded (i.e., written down) in the shortestspace.
That is, we aim to code the complete data?for all word pairs in the given language pair?ascompactly as possible.
To find the optimal align-ment, we need A. an objective function?a way tomeasure the quality of any given alignment?andB.
a search algorithm, to sift through all possiblealignments for one that optimizes the objective.We can use various methods to code the com-plete data.
Essentially, they all amount to measur-ing how many bits it costs to ?transmit?
the com-plete set of alignment ?events?, where each align-ment event e is a pair of aligned symbols (?
: ?
)e = (?
: ?)
?
?
?{.,#}?
T ?
{.,#}drawn from the source alphabet ?
and the targetalphabet T , respectively.9One possible codingscheme is ?prequential?
coding, or the Bayesianmarginal likelihood, see, e.g., (Kontkanen et al.,1996), used in (Wettig et al., 2011); another is nor-malized maximum likelihood (NML) code, (Ris-sanen, 1996), used in (Wettig et al., 2012).Prequential coding gives the total code lengthLbase(D) = ?
?e?Elog c(e)!+ log[?e?Ec(e) +K ?
1]!?
log(K ?
1)!
(2)for data D. Here, c(e) denotes the event count,and K is the total number of event types.To find the optimal alignments, the algorithmstarts with aligning word pairs randomly, and theniteratively searching for the best alignment givenrest of the data for each word pair at a time.
To dothis, we first exclude the current alignment fromour complete data.
The best alignment in the re-aligning process is found using a Dynamic Pro-gramming matrix, with source word symbols inthe rows and target word symbols as the columns.Each possible alignment of the word pair corre-sponds to a path from top-left cell of the matrixto the bottom-right cell.
Each cell V (?i, ?j) holdsthe cost of aligning sub-string ?1..?iwith ?1..?j,and is computed as:V (?i, ?j) = min{V (?i, ?j?1) +L(.
: ?j)V (?i?1, ?j) +L(?i: .
)V (?i?1, ?j?1) +L(?i: ?j)(3)9Note, that the alphabets need not be the same, or evenhave any symbols in common.
We add a special end-of-wordsymbol, always aligned to itself: (# : #).
Empty alignments(.
: .)
are not allowed.60where L(e) is the cost of coding event e. The costof aligning the full word pair, is then found in thebottom-right cell, and the corresponding path ischosen as the new alignment, which is registeredback into the complete data.We should mention that due to vulnerability ofthe algorithm to local optima, we use simulatedannealing with (50) random restarts.3.2 Context modelContext model is described in detail in (Wettig etal., 2013).
We use a modified version of this modelto achieve faster run-time.One limitation of the basic model describedabove is that it uses no information about the con-text of the sounds, thus ignoring the fact that lin-guistic sound change is regular and highly dependson context.
The 1-1 model also treats symbols ofthe words as atoms, ignoring how two sounds arephonetically close.
The context model, addressesboth of these issues.Each sound is represented as a vector of distinc-tive phonetic features.
Since we are using MDL asthe basis of the model here, we need to code (i.e.,transmit) the data.
This can be done by coding onefeature at a time on each level.To code a feature F on a level L, we construct adecision tree.
First, we collect all instances of thesounds in the data of the corresponding level thathave the current feature, and then build a countmatrix based on how many instances take eachvalue.
Here is an example of such a matrix forfeature V (vertical articulation of a vowel).V Close Mid-close Mid-open Open10 25 33 24This shows that there are 10 close vowels, 25mid-close vowels, etc.This serves as the root node of the tree.
The treecan then query features of the sounds in the currentcontext by choosing from a set of candidate con-texts.
Each candidate is a triplet (L,P, F ), repre-senting Level, Position, and Feature respectively.L can be either source or target, since we aredealing with a pair of language varieties at a time.P is the position of the sound that is being queriedrelative to current sound, and F is the feature be-ing queried.
Examples of a Position are previ-ous vowel, previous position, itself, etc.
The treeexpands depending on the possible responses tothe query, resulting in child nodes with their owncount matrix.
The idea here is to make the matri-ces in the child nodes as sparse as possible in orderto code them with fewer bits.This process continues until the tree cannot beexpanded any more.
Finally the data in each leafnode is coded using prequential coding as beforewith the same cost explained in Equation 2.Code length for the complete data consists ofcost of encoding the trees and the cost of encodingthe data given the trees.
The search algorithm re-mains the same as the 1-1 algorithm, but uses theconstructed trees to calculate the cost of events.This method spends much time rebuilding thetrees on each iteration; its run-time is very high.In the modified version used in this paper, the treesare not allowed to expand initially, when the modelhas just started and everything is random due tosimulated annealing.
Once the simulated anneal-ing phase is complete, the trees are expanded fullynormally.
Our experiments show that this resultsin trees that are equally good as the original ones.3.3 Normalized Compression DistanceThe cost of coding the data for a language pair un-der a model reflects the amount of regularity themodel discovered, and thus is a means of measur-ing the distance between these languages.
How-ever the cost also depends on the size of the datafor the language pair; thus, a way of normalizingthe cost is needed to make them comparable acrosslanguage pairs.
We use ?Normalized Compres-sion Distance?
(NCD), described in (Cilibrasi andVitanyi, 2005) to achieve this.Given a model that can compress a languagepair (a, b) with cost C(a, b), NCD of (a, b) is:NCD(a, b) =C(a, b)?min(C(a), C(b))max(C(a), C(b))(4)Since NCD of different pairs are comparable un-der the same model, it can be used as a distancemeasure between language varieties.3.4 Prediction of unobserved dataThe models mentioned above are also able topredict unobserved data as described in Sec-tion 1 (Wettig et al., 2013).For the basic 1-1 model, since no informa-tion about the context is used, prediction sim-ply means looking for the most probable symbolin target language for each symbol of wA.
Forthe context model, a more sophisticated dynamic-programming heuristic is needed to predict the un-seen word, (Hiltunen, 2012).
The predicted word6110000150002000025000300001000015000200002500030000Context model1x1 modelMDLcostazb bas blkx chv hak jak kaz krg nogx qum shr sjg tat tof trk trm tuv uig uzb y = xFigure 1: Model comparison: MDL costs.w?Bis then compared to the real correspondingword wBto measure how well the model per-formed on the task.Feature-wise Levenshtein edit distance is usedfor this comparison.
The edit distances for allword pairs are normalized, resulting in Normal-ized Feature-wise Edit Distance (NFED) whichcan serve as a measure of model quality.4 ExperimentsTo illustrate the principles discussed above, weexperiment with the two principal model types de-scribed above?the baseline 1-1 model and thecontext-sensitive model, using data from three dif-ferent language families.4.1 DataWe use data from the StarLing data bases,(Starostin, 2005), for the Turkic and Uralic lan-guage families, and for the Slavic branch of theIndo-European family.
For dozens of languagefamilies, StarLing has rich data sets (going be-yond Swadesh-style lists, as in some other lexicaldata collections built for judging language and di-alect distances).
The databases are under constantdevelopment, and have different quality.
Somedatasets, (most notably the IE data) are drawnfrom multiple sources, which use different nota-tion, transcription, etc., and are not yet unified.The data we chose for use is particularly clean.For the Turkic family, StarLing at present con-tains 2017 cognate sets; we use 19 (of the total 27)languages, which have a substantial amount of at-tested word-forms in the data collection.00.10.20.30.40.50.6  00.10.20.30.40.50.6Context model1x1 modelNormalizedFeaturewise EditDistance (NFED)azb bas blkx chv hak jak kaz krg nogx qum shr sjg tat tof trk trm tuv uig uzb y = xFigure 2: Model comparison: NFED.4.2 Model comparisonWe first demonstrate how the ?best?
model can bechosen from among several models, in a principledway.
This is feasible if we work with probabilis-tic models?models that assign probabilities to theobserved data.
If the model is also able to performprediction (of unseen data), then we can measurethe model?s predictive power and select the bestmodel using predictive power as the criterion.
Wewill show that in the case of the two probabilisticmodels presented above, these two criteria yieldthe same result.We ran the baseline 1-1 model and the contextmodel against the entire Turkic dataset, i.e., the19 ?
18 language pairs,10(with 50 restarts foreach pair, a total of 17100 runs).
For each lan-guage pair, we select the best out of 50 runs foreach model, according to the cost it assigns to thislanguage pair.
Figure 1 shows the costs obtainedby the best run: each point denotes a languagepair; X-coordinate is the cost according to the 1-1 model, Y-coordinate is the cost of the contextmodel.
The Figure shows that all 19?18 points liebelow the diagonal (x=y), i.e., for every languagepair, the context model finds a code with lowercost?as is expected, since the context model is?smarter,?
uses more information from the data,and hence finds more regularity in it.Next, for each language pair, we take the runthat found the lowest cost, and use it to imputeunseen data, as explained in Section 3?yieldingNFED, the distance from the imputed string to the10Turkic languages in tables and figures are:azb:Azerbaijani, bas:Bashkir, blk:Balkar, chv:Chuvash,hak:Khakas, jak:Yakut, kaz:Kazakh, krg:Kyrgyz, nog:Nogaj,qum:Qumyk, shr:Shor, sjg:Sary Uyghur, tat:Tatar,tof:Tofalar, trk:Turkish, trm:Turkmen, tuv:Tuva, uig:Uyghur,uzb:Uzbek.62ru ukr cz slk pl usrb lsrb bulg scrru 0 .41 .41 .39 .41 .51 .53 .48 .40ukr .41 0 .48 .46 .51 .49 .50 .48 .47cz .40 .48 0 .29 .38 .45 .52 .50 .39slk .38 .45 .29 0 .38 .41 .44 .45 .38pl .43 .51 .39 .41 0 .48 .50 .52 .45usrb .50 .48 .44 .40 .46 0 .29 .49 .48lsrb .52 .51 .49 .44 .47 .30 0 .52 .50bulg .46 .47 .48 .45 .51 .47 .49 0 .41scr .40 .47 .38 .38 .43 .49 .51 .44 0Table 1: NCDs for 9 Slavic languages, StarLingdatabase: context modelactual, correct string in the target language.
Thisagain yields 19?18 points, shown in Figure 2; thistime the X and Y values lie between 0 and 1, sinceNFED is normalized.
(In the figure, the points arelinked with line segments as follows: for any pair(a,b) the point (a,b) is joined by a line to the point(b,a).
This is done for easier identification, sincethe point (a,b) displays the legend symbol for onlylanguage a.)
Overall, many more points lie belowthe diagonal, (approximately 10% of the points areabove).
The context model performs better, and itwould therefore be a safer/wiser choice, if we wishto measure language closeness; which agrees withthe result obtained using raw compression costs.The key point here is that this compari-son method can accommodate any probabilisticmodel: for any new candidate model we check?over the same datasets?what probability valuesdoes the model assign to each data point.
Probabil-ities and (compression) costs are interchangeable:information theory tells us that for a data set D andmodel M, the probability P of data D under modelM and the cost (code length) L of D under M arerelated by: LM(D) = ?
logPM(D).
If the newmodel assigns higher probability (or lower cost) toobserved data, it is preferable?obviating the needfor gold-standards, or subjective judgements.4.3 Language closenessWe next explore various datasets using the contextmodel?the better model we have available.Uralic: We begin with Uralic data from Star-Ling.11The Uralic database contains data frommore than one variant of many languages: we ex-tracted data for the top two dialects?in terms ofcounts of available word-forms?for Komi, Ud-11We use data from the Finno-Ugric sub-family.
Thelanguage codes are: est:Estonian, fin:Finnish, khn:Khanty,kom:Komi, man:Mansi, mar:Mari, mrd:Mordva, saa:Saami,udm:Udmurt.Language pair NCDkom s kom p .18kom p kom s .19udm s udm g .20udm g udm s .21mar b mar kb .28mar kb mar b .28mrd m mrd e .29mrd e mrd m .29est fin .32fin est .32man p man so .34khn v khn dn .35khn dn khn v .36man so man p .36saa n saa l .37saa l saa n .37Table 2: Comparison of Uralic dialect/languagepairs, sorted by NCD: context model.murt, Mari, Mordva, Mansi, Khanty and Saami.Table 2 shows the normalized compression dis-tances for each of the pairs; the NCD costs forFinnish and Estonian are given for comparison.It is striking that the pairs that score belowFinnish/Estonian are all ?true?
dialects, whereasthose that score above are not.
E.g., the Mansivariants Pelym and Sosva, (Honti, 1998), andDemjanka and Vakh Khanty, (Abondolo, 1998),are mutually unintelligible.
The same is true forNorth and Lule Saami.Turkic: We compute NCDs for the Turkic lan-guages under the context model.
Some of the Tur-kic languages are known to form a much tighterdialect continuum, (Johanson, 1998), which is ev-ident from the NCDs in Table 3.
E.g., Tofa is most-closely related to the Tuvan language and forms adialect continuum with it, (Johanson, 1998).
Turk-ish and Azerbaijani closely resemble each otherand are mutually intelligible.
In the table we high-light language pairs with NCD ?
0.30.Slavic: We analyzed data from StarLing for 9Slavic languages.12The NCDs are shown in Ta-ble 1.
Of all pairs, the normalized compressioncosts for (cz, slk) and (lsrb, usrb) fall below the.30 mark, and indeed these pairs have high mutualintelligibility, unlike all other pairs.When the data from Table 1 are fed intothe NeighborJoining algorithm, (Saitou and Nei,1987), it draws the phylogeny in Figure 3, whichclearly separates the languages into the 3 ac-cepted branches of Slavic: East (ru, ukr), South12The Slavic languages from StarLing: bulg:Bulgarian,cz:Czech, pl:Polish, ru:Russian, slk:Slovak, scr:Serbo-Croatian, ukr:Ukrainian, lsrb/usrb:Lower and Upper Sorbian.630 0.05 0.1 0.15 0.2 0.25SCRBULGRUUKRSLKCZPLUSRBLSRBFigure 3: NeighborJoining tree for Slavic lan-guages in Table 1.
(scr, bulg) and West (pl, cz, slk, u/lsrb).
Thephylogeny also supports later separation (rela-tive time depth > 0.05) of the pairs with highermutual intelligibility?Upper/Lower Sorbian, andCzech/Slovak.135 Conclusions and future workWe have presented a case for using probabilisticmodeling when we need reliable quantitative mea-sures of language closeness.
Such needs arise, forexample, when one attempts to develop methodswhose success directly depends on how close thelanguages in question are.
We attempt to demon-strate two main points.
One is that using proba-bilistic models provides a principled and naturalway of comparing models?to determine whichcandidate model we can trust more when mea-suring how close the languages are.
It also letsus compare models without having to build gold-standard datasets; this is important, since gold-standards are subjective, not always reliable, andexpensive to produce.
We are really interested inregularity, and the proof of the model?s quality isin its ability to assign high probability to observedand unobserved data.The second main point of the paper is show-ing how probabilistic models can be employed tomeasure language closeness.
Our best-performingmodel seems to provide reasonable judgementsof closeness when applied to languages/linguisticvariants from very different language families.
Forall of Uralic, Turkic and Slavic data, those that fell13We should note that the NCDs produce excellent phylo-genies also for the Turkic and Uralic data; not included heredue to space constraints.below the 0.30 mark on the NCD axis are knownto have higher mutual intelligibility, while thosethat are above the mark have lower or no mutualintelligibility.
Of course, we do not claim that 0.30is a magic number; for a different model the lineof demarcation may fall elsewhere entirely.
How-ever, it shows that the model (which we selectedon the basis of its superiority according to our se-lection criteria) is quite consistent in predicting thedegree of mutual intelligibility, overall.Incidentally, these experiments demonstrate, ina principled fashion, the well-known arbitrary na-ture of the terms language vs. dialect?this dis-tinction is simply not supported by real linguis-tic data.
More importantly, probabilistic methodsrequire us to make fewer subjective judgements,with no ad hoc priors or gold-standards, which inmany cases are difficult to obtain and justify?andrather rely on the observed data as the ultimate andsufficient truth.AcknowledgmentsThis research was supported in part by: theFinUgRevita Project of the Academy of Finland,and by the National Centre of Excellence ?Al-gorithmic Data Analysis (ALGODAN)?
of theAcademy of Finland.ReferencesDaniel Abondolo.
1998.
Khanty.
In Daniel Abon-dolo, editor, The Uralic Languages, pages 358?386.Routledge.Alexandre Bouchard-C?ot?e, Percy Liang, Thomas Grif-fiths, and Dan Klein.
2007.
A probabilistic ap-proach to diachronic phonology.
In Proceedings ofthe Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Nat-ural Language Learning (EMNLP-CoNLL:2007),pages 887?896, Prague, Czech Republic.Rudi Cilibrasi and Paul M.B.
Vitanyi.
2005.
Cluster-ing by compression.
IEEE Transactions on Infor-mation Theory, 51(4):1523?1545.T.
Mark Ellison and Simon Kirby.
2006.
Measuringlanguage divergence by intra-lexical comparison.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th AnnualMeeting of the Association for Computational Lin-guistics (COLING/ACL-2006), pages 273?280, Syd-ney, Australia.Wilbert Heeringa.
2004.
Measuring Dialect Pronunci-ation Differences using Levenshtein Distance.
Ph.D.thesis, Rijksuniversiteit Groningen.64azb bas blk chv hak jak kaz krg nog qum shr sjg tat tof trk trm tuv uig uzbazb 0 .40 .35 .60 .48 .59 .38 .39 .35 .34 .43 .41 .38 .45 .27 .33 .46 .40 .37bas .39 0 .28 .58 .45 .59 .27 .31 .24 .26 .40 .41 .21 .46 .39 .38 .45 .36 .34blk .35 .30 0 .57 .42 .57 .26 .28 .22 .19 .36 .40 .27 .42 .34 .36 .42 .35 .30chv .59 .59 .56 0 .62 .67 .56 .58 .55 .53 .60 .57 .56 .60 .56 .60 .62 .61 .58hak .47 .44 .41 .63 0 .58 .41 .40 .37 .40 .27 .43 .43 .39 .46 .50 .40 .46 .46jak .57 .57 .57 .70 .58 0 .56 .57 .55 .54 .55 .54 .57 .51 .58 .57 .56 .58 .57kaz .38 .28 .27 .57 .42 .57 0 .24 .16 .24 .38 .39 .29 .44 .37 .39 .41 .36 .33krg .38 .31 .27 .60 .40 .57 .23 0 .21 .26 .35 .40 .32 .41 .36 .39 .40 .35 .33nog .35 .25 .22 .57 .39 .55 .15 .22 0 .19 .36 .38 .26 .43 .33 .35 .41 .35 .31qum .34 .27 .19 .57 .41 .55 .23 .26 .19 0 .35 .37 .26 .41 .33 .35 .41 .33 .31shr .43 .40 .36 .63 .28 .55 .38 .36 .35 .34 0 .40 .40 .36 .43 .44 .38 .42 .42sjg .43 .42 .41 .58 .45 .55 .40 .41 .39 .38 .40 0 .42 .44 .43 .43 .43 .41 .41tat .36 .22 .27 .60 .44 .59 .28 .32 .26 .26 .40 .41 0 .45 .38 .38 .45 .36 .33tof .47 .45 .42 .61 .39 .50 .42 .42 .42 .41 .36 .42 .45 0 .48 .46 .24 .44 .43trk .28 .40 .35 .58 .48 .59 .37 .36 .33 .34 .43 .42 .39 .47 0 .34 .46 .40 .38trm .32 .40 .36 .62 .51 .59 .39 .40 .36 .35 .44 .43 .39 .46 .34 0 .49 .41 .36tuv .46 .46 .41 .63 .40 .56 .41 .40 .41 .41 .38 .42 .45 .23 .45 .48 0 .45 .46uig .40 .39 .34 .60 .49 .58 .36 .36 .36 .33 .43 .40 .38 .45 .41 .42 .46 0 .33uzb .37 .36 .31 .60 .48 .58 .34 .34 .32 .32 .43 .41 .34 .44 .38 .36 .47 .33 0Table 3: Normalized compression distances for 19 Turkic languages (StarLing database): context modelSuvi Hiltunen.
2012.
Minimum description lengthmodeling of etymological data.
Master?s thesis,University of Helsinki.L?aszl?o Honti.
1998.
Ob?
Ugrian.
In Daniel Abon-dolo, editor, The Uralic Languages, pages 327?357.Routledge.Lars Johanson.
1998.
The history of Turkic.
InLars Johanson &?Eva?Agnes Csat?o, editor, The Tur-kic Languages, pages 81?125.
London, New York:Routledge.
Classification of Turkic languages (atTurkiclanguages.com).Grzegorz Kondrak.
2004.
Combining evidence incognate identification.
In Proceedings of the Sev-enteenth Canadian Conference on Artificial Intelli-gence (Canadian AI 2004), pages 44?59, London,Ontario.
Lecture Notes in Computer Science 3060,Springer-Verlag.Petri Kontkanen, Petri Myllym?aki, and Henry Tirri.1996.
Constructing Bayesian finite mixture modelsby the EM algorithm.
Technical Report NC-TR-97-003, ESPRIT NeuroCOLT: Working Group on Neu-ral and Computational Learning.Jorma Rissanen.
1996.
Fisher information andstochastic complexity.
IEEE Transactions on Infor-mation Theory, 42(1):40?47.Naruya Saitou and Masatoshi Nei.
1987.
Theneighbor-joining method: a new method for recon-structing phylogenetic trees.
Molecular biology andevolution, 4(4):406?425.Sergei A. Starostin.
2005.
Tower of Babel: StarLingetymological databases.
http://newstar.rinet.ru/.Hannes Wettig and Roman Yangarber.
2011.
Proba-bilistic models for alignment of etymological data.In Proceedings of NoDaLiDa: the 18th Nordic Con-ference on Computational Linguistics, Riga, Latvia.Hannes Wettig, Suvi Hiltunen, and Roman Yangarber.2011.
MDL-based Models for Alignment of Et-ymological Data.
In Proceedings of RANLP: the8th Conference on Recent Advances in Natural Lan-guage Processing, Hissar, Bulgaria.Hannes Wettig, Kirill Reshetnikov, and Roman Yan-garber.
2012.
Using context and phonetic featuresin models of etymological sound change.
In Proc.EACL Workshop on Visualization of Linguistic Pat-terns and Uncovering Language History from Mul-tilingual Resources, pages 37?44, Avignon, France.Hannes Wettig, Javad Nouri, Kirill Reshetnikov, andRoman Yangarber.
2013.
Information-theoreticmodeling of etymological sound change.
In LarsBorin and Anju Saxena, editors, Approaches to mea-suring linguistic differences, volume 265 of Trendsin Linguistics, pages 507?531.
de Gruyter Mouton.Martijn Wieling and John Nerbonne.
2011.
Measur-ing linguistic variation commensurably.
In Dialec-tologia Special Issue II: Production, Perception andAttitude, pages 141?162.Martijn Wieling and John Nerbonne.
2015.
Advancesin dialectometry.
In Annual Review of Linguistics,volume 1.
To appear.Martijn Wieling, Jelena Proki?c, and John Nerbonne.2009.
Evaluating the pairwise string alignment ofpronunciations.
In Proceedings of the EACL 2009Workshop on Language Technology and Resourcesfor Cultural Heritage, Social Sciences, Humanities,and Education, pages 26?34, Athens, Greece.65
