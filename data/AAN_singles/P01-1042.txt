Joint and conditional estimation of tagging and parsing models?Mark JohnsonBrown UniversityMark Johnson@Brown.eduAbstractThis paper compares two different waysof estimating statistical language mod-els.
Many statistical NLP tagging andparsing models are estimated by max-imizing the (joint) likelihood of thefully-observed training data.
How-ever, since these applications only re-quire the conditional probability distri-butions, these distributions can in prin-ciple be learnt by maximizing the con-ditional likelihood of the training data.Perhaps somewhat surprisingly, modelsestimated by maximizing the joint weresuperior to models estimated by max-imizing the conditional, even thoughsome of the latter models intuitivelyhad access to ?more information?.1 IntroductionMany statistical NLP applications, such as tag-ging and parsing, involve finding the valueof some hidden variable Y (e.g., a tag or aparse tree) which maximizes a conditional prob-ability distribution P?
(Y |X), where X is agiven word string.
The model parameters ?are typically estimated by maximum likelihood:i.e., maximizing the likelihood of the training?I would like to thank Eugene Charniak and the othermembers of BLLIP for their comments and suggestions.
Fer-nando Pereira was especially generous with comments andsuggestions, as were the ACL reviewers; I apologize for notbeing able to follow up all of your good suggestions.
This re-search was supported by NSF awards 9720368 and 9721276and NIH award R01 MH60922-01A2.data.
Given a (fully observed) training cor-pus D = ((y1, x1), .
.
.
, (yn, xn)), the maximum(joint) likelihood estimate (MLE) of ?
is:??
= argmax?n?i=1P?
(yi, xi).
(1)However, it turns out there is another maximumlikelihood estimation method which maximizesthe conditional likelihood or ?pseudo-likelihood?of the training data (Besag, 1975).
Maximumconditional likelihood is consistent for the con-ditional distribution.
Given a training corpusD, the maximum conditional likelihood estimate(MCLE) of the model parameters ?
is:??
= argmax?n?i=1P?(yi|xi).
(2)Figure 1 graphically depicts the difference be-tween the MLE and MCLE.
Let ?
be the universeof all possible pairs (y, x) of hidden and visiblevalues.
Informally, the MLE selects the modelparameter ?
which make the training data pairs(yi, xi) as likely as possible relative to all otherpairs (y?, x?)
in ?.
The MCLE, on the other hand,selects the model parameter ?
in order to make thetraining data pair (yi, xi) more likely than otherpairs (y?, xi) in ?, i.e., pairs with the same visiblevalue xi as the training datum.In statistical computational linguistics, max-imum conditional likelihood estimators havemostly been used with general exponential or?maximum entropy?
models because standardmaximum likelihood estimation is usually com-putationally intractable (Berger et al, 1996; DellaPietra et al, 1997; Jelinek, 1997).
Well-known computational linguistic models such as(MLE)(MCLE)?
Y = yi, X = xi?X = xiY = yi, X = xiFigure 1: The MLE makes the training data (yi, xi) aslikely as possible (relative to ?
), while the MCLE makes(yi, xi) as likely as possible relative to other pairs (y?, xi).Maximum-Entropy Markov Models (McCallumet al, 2000) and Stochastic Unification-basedGrammars (Johnson et al, 1999) are standardlyestimated with conditional estimators, and itwould be interesting to know whether conditionalestimation affects the quality of the estimatedmodel.
It should be noted that in practice, theMCLE of a model with a large number of featureswith complex dependencies may yield far betterperformance than the MLE of the much smallermodel that could be estimated with the samecomputational effort.
Nevertheless, as this papershows, conditional estimators can be used withother kinds of models besides MaxEnt models,and in any event it is interesting to ask whetherthe MLE differs from the MCLE in actual appli-cations, and if so, how.Because the MLE is consistent for the jointdistribution P(Y,X) (e.g., in a tagging applica-tion, the distribution of word-tag sequences), itis also consistent for the conditional distributionP(Y |X) (e.g., the distribution of tag sequencesgiven word sequences) and the marginal distribu-tion P(X) (e.g., the distribution of word strings).On the other hand, the MCLE is consistent for theconditional distribution P(Y |X) alone, and pro-vides no information about either the joint or themarginal distributions.
Applications such as lan-guage modelling for speech recognition and EMprocedures for estimating from hidden data ei-ther explicitly or implicitly require marginal dis-tributions over the visible data (i.e., word strings),so it is not statistically sound to use MCLEs forsuch applications.
On the other hand, applicationswhich involve predicting the value of the hiddenvariable from the visible variable (such as taggingor parsing) usually only involve the conditionaldistribution, which the MCLE estimates directly.Since both the MLE and MCLE are consistentfor the conditional distribution, both converge inthe limit to the ?true?
distribution if the true dis-tribution is in the model class.
However, giventhat we often have insufficient data in computa-tional linguistics, and there are good reasons tobelieve that the true distribution of sentences orparses cannot be described by our models, thereis no reason to expect these asymptotic results tohold in practice, and in the experiments reportedbelow the MLE and MCLE behave differently ex-perimentally.A priori, one can advance plausible argumentsin favour of both the MLE and the MCLE.
Infor-mally, the MLE and the MCLE differ in the fol-lowing way.
Since the MLE is obtained by maxi-mizing?i P?(yi|xi)P?
(xi), the MLE exploits in-formation about the distribution of word strings xiin the training data that the MCLE does not.
Thusone might expect the MLE to converge faster thanthe MCLE in situations where training data is notover-abundant, which is often the case in compu-tational linguistics.On the other hand, since the intended applica-tion requires a conditional distribution, it seemsreasonable to directly estimate this conditionaldistribution from the training data as the MCLEdoes.
Furthermore, suppose that the model classis wrong (as is surely true of all our current lan-guage models), i.e., the ?true?
model P(Y,X) 6=P?
(Y,X) for all ?, and that our best models areparticularly poor approximations to the true dis-tribution of word strings P(X).
Then ignoringthe distribution of word strings in the training dataas the MCLE does might indeed be a reasonablething to do.The rest of this paper is structured as fol-lows.
The next section formulates the MCLEsfor HMMs and PCFGs as constrained optimiza-tion problems and describes an iterative dynamic-programming method for solving them.
Becauseof the computational complexity of these prob-lems, the method is only applied to a simplePCFG based on the ATIS corpus.
For this ex-ample, the MCLE PCFG does perhaps produceslightly better parsing results than the standardMLE (relative-frequency) PCFG, although the re-sult does not reach statistical significance.It seems to be difficult to find model classes forwhich the MLE and MCLE are both easy to com-pute.
However, often it is possible to find twoclosely related model classes, one of which hasan easily computed MLE and the other which hasan easily computed MCLE.
Typically, the modelclasses which have an easily computed MLE de-fine joint probability distributions over both thehidden and the visible data (e.g., over word-tag pair sequences for tagging), while the modelclasses which have an easily computed MCLE de-fine conditional probability distributions over thehidden data given the visible data (e.g., over tagsequences given word sequences).Section 3 investigates closely related jointand conditional tagging models (the lat-ter can be regarded as a simplification ofthe Maximum Entropy Markov Models ofMcCallum et al (2000)), and shows that MLEsoutperform the MCLEs in this application.
Thefinal empirical section investigates two differentkinds of stochastic shift-reduce parsers, andshows that the model estimated by the MLEoutperforms the model estimated by the MCLE.2 PCFG parsingIn this application, the pairs (y, x) consist of aparse tree y and its terminal string or yield x (itmay be simpler to think of y containing all of theparse tree except for the string x).
Recall thatin a PCFG with production set R, each produc-tion (A??)
?
R is associated with a parameter?A??.
These parameters satisfy a normalizationconstraint for each nonterminal A:??:(A??)?R?A??
= 1 (3)For each production r ?
R, let fr(y) be the num-ber of times r is used in the derivation of the treey.
Then the PCFG defines a probability distribu-tion over trees:P?
(Y ) =?(A??)?R?A??fA??
(Y )The MLE for ?
is the well-known ?relative-frequency?
estimator:??A??
=?ni=1 fA??(yi)?ni=1???:(A???
)?R fA???
(yi).Unfortunately the MCLE for a PCFG is morecomplicated.
If x is a word string, then let ?
(x) bethe set of parse trees with terminal string or yieldx generated by the PCFG.
Then given a trainingcorpus D = ((y1, x1), .
.
.
, (yn, xn)), where yi isa parse tree for the string xi, the log conditionallikelihood of the training data log P(~y|~x) and itsderivative are given by:log P(~y|~x) =n?i=1?
?log P?
(yi) ?
log?y??(xi)P?(y)???
log P(~y|~x)??A?
?= 1?A??n?i=1(fA??
(yi) ?
E?(fA?
?|xi))Here E?
(f |x) denotes the expectation of f withrespect to P?
conditioned on Y ?
?(x).
Theredoes not seem to be a closed-form solution forthe ?
that maximizes P(~y|~x) subject to the con-straints (3), so we used an iterative numerical gra-dient ascent method, with the constraints (3) im-posed at each iteration using Lagrange multipli-ers.
Note that?ni=1 E?(fA?
?|xi) is a quantitycalculated in the Inside-Outside algorithm (Lariand Young, 1990) and P(~y|~x) is easily computedas a by-product of the same dynamic program-ming calculation.Since the expected production counts E?
(f |x)depend on the production weights ?, the entiretraining corpus must be reparsed on each itera-tion (as is true of the Inside-Outside algorithm).This is computationally expensive with a largegrammar and training corpus; for this reason theMCLE PCFG experiments described here wereperformed with the relatively small ATIS tree-bank corpus of air travel reservations distributedby LDC.In this experiment, the PCFGs were alwaystrained on the 1088 sentences of the ATIS1 corpusand evaluated on the 294 sentences of the ATIS2corpus.
Lexical items were ignored; the PCFGsgenerate preterminal strings.
The iterative algo-rithm for the MCLE was initialized with the MLEparameters, i.e., the ?standard?
PCFG estimatedfrom a treebank.
Table 1 compares the MLE andMCLE PCFGs.The data in table 1 shows that compared to theMLE PCFG, the MCLE PCFG assigns a higherconditional probability of the parses in the train-ing data given their yields, at the expense of as-signing a lower marginal probability to the yieldsthemselves.
The labelled precision and recallparsing results for the MCLE PCFG were slightlyhigher than those of the MLE PCFG.
BecauseMLE MCLE?
log P(~y) 13857 13896?
log P(~y|~x) 1833 1769?
log P(~x) 12025 12127Labelled precision 0.815 0.817Labelled recall 0.789 0.794Table 1: The likelihood P(~y) and conditional likelihoodP(~y|~x) of the ATIS1 training trees, and the marginal likeli-hood P(~x) of the ATIS1 training strings, as well as the la-belled precision and recall of the ATIS2 test trees, using theMLE and MCLE PCFGs.both the test data set and the differences are sosmall, the significance of these results was esti-mated using a bootstrap method with the differ-ence in F-score in precision and recall as the teststatistic (Cohen, 1995).
This test showed that thedifference was not significant (p ?
0.1).
Thus theMCLE PCFG did not perform significantly bet-ter than the MLE PCFG in terms of precision andrecall.3 HMM taggingAs noted in the previous section, maximizing theconditional likelihood of a PCFG or a HMM canbe computationally intensive.
This section andthe next pursues an alternative strategy for com-paring MLEs and MCLEs: we compare similiar(but not identical) model classes, one of whichhas an easily computed MLE, and the other ofwhich has an easily computed MCLE.
The appli-cation considered in this section is bitag POS tag-ging, but the techniques extend straight-forwardlyto n-tag tagging.
In this application, the data pairs(y, x) consist of a tag sequence y = t1 .
.
.
tmand a word sequence x = w1 .
.
.
wm, where tjis the tag for word wj (to simplify the formu-lae, w0, t0, wm+1 and tm+1 are always taken tobe end-markers).
Standard HMM tagging modelsdefine a joint distribution over word-tag sequencepairs; these are most straight-forwardly estimatedby maximizing the likelihood of the joint train-ing distribution.
However, it is straight-forwardto devise closely related HMM tagging modelswhich define a conditional distribution over tagsequences given word sequences, and which aremost straight-forwardly estimated by maximizingthe conditional likelihood of the distribution oftag sequences given word sequences in the train-ing data.
(4) ?
?
?
// Tj //Tj+1 //?
?
?Wj Wj+1(5) ?
?
?
// Tj // Tj+1 // ?
?
?WjOOWj+1OO(6) ?
?
?
// Tj //Tj+1 //?
?
?==|||||||||||| Wj;;xxxxxxxxxxWj+1==|||||||||||(7) ?
?
?
//!
!DDDDDDDDDDD Tj //##FFFFFFFFFFTj+1 //!!BBBBBBBBBBBB?
?
?WjOOWj+1OOFigure 2: The HMMs depicted as ?Bayes net?
graphicalmodels.All of the HMM models investigated in thissection are instances of a certain kind of graph-ical model that Pearl (1988) calls ?Bayes nets?
;Figure 2 sketches the networks that correspond toall of the models discussed here.
(In such a graph,the set of incoming arcs to a node depicting a vari-able indicate the set of variables on which thisvariable is conditioned).Recall the standard bitag HMM model, whichdefines a joint distribution over word and tag se-quences:P(Y,X) =m+1?j=1P?
(Tj |Tj?1)P?
(Wj |Tj) (4)As is well-known, the MLE for (4) sets P?
to theempirical distributions on the training data.Now consider the following conditional modelof the conditional distribution of tags given words(this is a simplified form of the model describedin McCallum et al (2000)):P(Y |X) =m+1?j=1P0(Tj |Wj , Tj?1) (5)The MCLE of (5) is easily calculated: P0 shouldbe set the empirical distribution of the trainingdata.
However, to minimize sparse data prob-lems we estimated P0(Tj |Wj, Tj?1) as a mixtureof P?
(Tj |Wj), P?
(Tj |Tj?1) and P?
(Tj |Wj , Tj?1),where the P?
are empirical probabilities and the(bucketted) mixing parameters are determined us-ing deleted interpolation from heldout data (Je-linek, 1997).These models were trained on sections 2-21of the Penn tree-bank corpus.
Section 22 wasused as heldout data to evaluate the interpola-tion parameters ?.
The tagging accuracy of themodels was evaluated on section 23 of the tree-bank corpus (in both cases, the tag tj assigned toword wj is the one which maximizes the marginalP(tj|w1 .
.
.
wm), since this minimizes the ex-pected loss on a tag-by-tag basis).The conditional model (5) has the worst perfor-mance of any of the tagging models investigatedin this section: its tagging accuracy is 94.4%.
Thejoint model (4) has a considerably lower errorrate: its tagging accuracy is 95.5%.One possible explanation for this result is thatthe way in which the interpolated estimate of P0is calculated, rather than conditional likelihoodestimation per se, is lowering tagger accuracysomehow.
To investigate this possibility, two ad-ditional joint models were estimated and tested,based on the formulae below.P(Y,X) =m+1?j=1P?
(Wj |Tj)P1(Tj |Wj?1, Tj?1) (6)P(Y,X) =m+1?j=1P0(Tj |Wj, Tj?1)P?
(Wj |Tj?1) (7)The MLEs for both (6) and (7) are easy to cal-culate.
(6) contains a conditional distribution P1which would seem to be of roughly equal com-plexity to P0, and it was estimated using deletedinterpolation in exactly the same way as P0, soif the poor performance of the conditional modelwas due to some artifact of the interpolation pro-cedure, we would expect the model based on (6)to perform poorly.
Yet the tagger based on (6)performs the best of all the taggers investigated inthis section: its tagging accuracy is 96.2%.
(7) is admitted a rather strange model, sincethe right hand term in effect predicts the follow-ing word from the current word?s tag.
However,note that (7) differs from (5) only via the pres-ence of this rather unusual term, which effectivelyconverts (5) from a conditional model to a jointmodel.
Yet adding this term improves tagging ac-curacy considerably, to 95.3%.
Thus for bitag tag-ging at least, the conditional model has a consid-erably higher error rate than any of the joint mod-els examined here.
(While a test of significancewas not conducted here, previous experience withthis test set shows that performance differencesof this magnitude are extremely significant statis-tically).4 Shift-reduce parsingThe previous section compared similiar joint andconditional tagging models.
This section com-pares a pair of joint and conditional parsing mod-els.
The models are both stochastic shift-reduceparsers; they differ only in how the distributionover possible next moves are calculated.
Theseparsers are direct simplifications of the StructuredLanguage Model (Jelinek, 2000).
Because theparsers?
moves are determined solely by the toptwo category labels on the stack and possibly thelook-ahead symbol, they are much simpler thanstochastic LR parsers (Briscoe and Carroll, 1993;Inui et al, 1997).
The distribution over treesgenerated by the joint model is a probabilisticcontext-free language (Abney et al, 1999).
Aswith the PCFG models discussed earlier, theseparsers are not lexicalized; lexical items are ig-nored, and the POS tags are used as the terminals.These two parsers only produce trees withunary or binary nodes, so we binarized the train-ing data before training the parser, and debina-rize the trees the parsers produce before evaluat-ing them with respect to the test data (Johnson,1998).
We binarized by inserting n?
2 additionalnodes into each local tree with n > 2 children.We binarized by first joining the head to all of theconstituents to its right, and then joining the re-sulting structure with constituents to the left.
Thelabel of a new node is the label of the head fol-lowed by the suffix ?-1?
if the head is (containedin) the right child or ?-2?
if the head is (containedin) the left child.
Figure 3 depicts an example ofthis transformation.The Structured Language Model is describedin detail in Jelinek (2000), so it is only reviewedhere.
Each parser?s stack is a sequence of node(b)(a)VPRBusuallyVBZ-1RBonlyVBZ-2VBZ-2VBZeatsNPpizzaADVPquicklyADVPquicklyVPRBusuallyRBonlyVBZeatsNPpizzaFigure 3: The binarization transformation used in the shift-reduce parser experiments transforms tree (a) into tree (b).labels (possibly including labels introduced by bi-narization).
In what follows, s1 refers to the topelement of the stack, or ???
if the stack is empty;similarly s2 refers to the next-to-top element ofthe stack or ???
if the stack contains less than twoelements.
We also append a ???
to end of the ac-tual terminal string being parsed (just as with theHMMs above), as this simplifies the formulationof the parsers, i.e., if the string to be parsed isw1 .
.
.
wm, then we take wm+1 = ?.A shift-reduce parse is defined in terms ofmoves.
A move is either shift(w), reduce1(c) orreduce2(c), where c is a nonterminal label and wis either a terminal label or ???.
Moves are par-tial functions from stacks to stacks: a shift(w)move pushes a w onto the top of stack, while areducei(c) move pops the top i terminal or non-terminal labels off the stack and pushes a c ontothe stack.
A shift-reduce parse is a sequence ofmoves which (when composed) map the emptystack to the two-element stack whose top elementis ???
and whose next-to-top element is the startsymbol.
(Note that the last move in a shift-reduceparse must always be a shift(?)
move; this cor-responds to the final ?accept?
move in an LRparser).
The isomorphism between shift-reduceparses and standard parse trees is well-known(Hopcroft and Ullman, 1979), and so is not de-scribed here.A (joint) shift-reduce parser is defined bya distribution P(m|s1, s2) over next moves mgiven the top and next-to-top stack labels s1and s2.
To ensure that the next move is infact a possible move given the current stack,we require that P(reduce1(c)|?, ?)
= 0 andP(reduce2(c)|c?, ?)
= 0 for all c, c?, and thatP(shift(?
)|s1, s2) = 0 unless s1 is the start sym-bol and s2 = ?.
Note that this extends to aprobability distribution over shift-reduce parses(and hence parse trees) in a particularly simpleway: the probability of a parse is the product ofthe probabilities of the moves it consists of.
As-suming that P meets certain tightness conditions,this distribution over parses is properly normal-ized because there are no ?dead?
stack configura-tions: we require that the distribution over movesbe defined for all possible stacks.A conditional shift-reduce parser differs onlyminimally from the shift-reduce parser justdescribed: it is defined by a distributionP(m|s1, s2, t) over next moves m given the topand next-to-top stack labels s1, s2 and the nextinput symbol w (w is called the look-ahead sym-bol).
In addition to the requirements on Pabove, we also require that if w?
6= w thenP(shift(w?
)|s1, s2, w) = 0 for all s1, s2; i.e.,shift moves can only shift the current look-aheadsymbol.
This restriction implies that all non-zeroprobability derivations are derivations of the parsestring, since the parse string forces a single se-quence of symbols to be shifted in all derivations.As before, since there are no ?dead?
stack con-figurations, so long as P obeys certain tightnessconditions, this defines a properly normalized dis-tribution over parses.
Since all the parses are re-quired to be parses of of the input string, this de-fines a conditional distribution over parses giventhe input string.It is easy to show that the MLE for the jointmodel, and the MCLE for the conditional model,are just the empirical distributions from the train-ing data.
We ran into sparse data problems usingthe empirical training distribution as an estimatefor P(m|s1, s2, w) in the conditional model, soin fact we used deleted interpolation to interpo-late P?
(m|s1, s2, w), and P?
(m|s1, s2) to estimateP(m|s1, s2, w).
The models were estimated fromsections 2?21 of the Penn treebank, and tested onthe 2245 sentences of length 40 or less in section23.
The deleted interpolation parameters were es-timated using heldout training data from sectionJoint SR Conditional SR PCFGPrecision 0.666 0.633 0.700Recall 0.650 0.639 0.657Table 2: Labelled precision and recall results for joint andconditional shift-reduce parsers, and for a PCFG.22.We calculated the most probable parses usinga dynamic programming algorithm based on theone described in Jelinek (2000).
Jelinek notes thatthis algorithm?s running time is n6 (where n is thelength of sentence being parsed), and we foundexhaustive parsing to be computationally imprac-tical.
We used a beam search procedure whichthresholded the best analyses of each prefix of thestring being parsed, and only considered analyseswhose top two stack symbols had been observedin the training data.
In order to help guard againstthe possibility that this stochastic pruning influ-enced the results, we ran the parsers twice, oncewith a beam threshold of 10?6 (i.e., edges whoseprobability was less than 10?6 of the best edgespanning the same prefix were pruned) and againwith a beam threshold of 10?9.
The results ofthe latter runs are reported in table 2; the labelledprecision and recall results from the run with themore restrictive beam threshold differ by less than0.001, i.e., at the level of precision reported here,are identical with the results presented in table 2except for the Precision of the Joint SR parser,which was 0.665.
For comparision, table 2 alsoreports results from the non-lexicalized treebankPCFG estimated from the transformed trees insections 2-21 of the treebank; here exhaustiveCKY parsing was used to find the most probableparses.All of the precision and recall results, includingthose for the PCFG, presented in table 2 are muchlower than those from a standard treebank PCFG;presumably this is because the binarization trans-formation depicted in Figure 3 loses informa-tion about pairs of non-head constituents in thesame local tree (Johnson (1998) reports similiarperformance degradation for other binarizationtransformations).
Both the joint and the condi-tional shift-reduce parsers performed much worsethan the PCFG.
This may be due to the pruningeffect of the beam search, although this seemsunlikely given that varying the beam thresholddid not affect the results.
The performance dif-ference between the joint and conditional shift-reduce parsers bears directly on the issue ad-dressed by this paper: the joint shift-reduce parserperformed much better than the conditional shift-reduce parser.
The differences are around a per-centage point, which is quite large in parsing re-search (and certainly highly significant).The fact that the joint shift-reduce parser out-performs the conditional shift-reduce parser issomewhat surprising.
Because the conditionalparser predicts its next move on the basis of thelookahead symbol as well as the two top stackcategories, one might expect it to predict this nextmove more accurately than the joint shift-reduceparser.
The results presented here show that thisis not the case, at least for non-lexicalized pars-ing.
The label bias of conditional models may beresponsible for this (Bottou, 1991; Lafferty et al,2001).5 ConclusionThis paper has investigated the difference be-tween maximum likelihood estimation and max-imum conditional likelihood estimation for threedifferent kinds of models: PCFG parsers, HMMtaggers and shift-reduce parsers.
The results forthe PCFG parsers suggested that conditional es-timation might provide a slight performance im-provement, although the results were not statis-tically significant since computational difficultyof conditional estimation of a PCFG made itnecessary to perform the experiment on a tinytraining and test corpus.
In order to avoid thecomputational difficulty of conditional estima-tion, we compared closely related (but not identi-cal) HMM tagging and shift-reduce parsing mod-els, for some of which the maximum likelihoodestimates were easy to compute and for others ofwhich the maximum conditional likelihood esti-mates could be easily computed.
In both cases,the joint models outperformed the conditionalmodels by quite large amounts.
This suggeststhat it may be worthwhile investigating meth-ods for maximum (joint) likelihood estimationfor model classes for which only maximum con-ditional likelihood estimators are currently used,such as Maximum Entropy models and MEMMs,since if the results of the experiments presentedin this paper extend to these models, one mightexpect a modest performance improvement.As explained in the introduction, because max-imum likelihood estimation exploits not just theconditional distribution of hidden variable (e.g.,the tags or the parse) conditioned on the visiblevariable (the terminal string) but also the marginaldistribution of the visible variable, it is reason-able to expect that it should outperform maxi-mum conditional likelihood estimation.
Yet itis counter-intuitive that joint tagging and shift-reduce parsing models, which predict the next tagor parsing move on the basis of what seems tobe less information than the corresponding con-ditional model, should nevertheless outperformthat conditional model, as the experimental re-sults presented here show.
The recent theoreti-cal and simulation results of Lafferty et al (2001)suggest that conditional models may suffer fromlabel bias (the discovery of which Lafferty et.
al.attribute to Bottou (1991)), which may provide aninsightful explanation of these results.None of the models investigated here are state-of-the-art; the goal here is to compare two dif-ferent estimation procedures, and for that rea-son this paper concentrated on simple, easily im-plemented models.
However, it would also beinteresting to compare the performance of jointand conditional estimators on more sophisticatedmodels.ReferencesSteven Abney, David McAllester, and FernandoPereira.
1999.
Relating probabilistic grammars andautomata.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguis-tics, pages 542?549, San Francisco.
Morgan Kauf-mann.Adam L. Berger, Vincent J. Della Pietra, andStephen A. Della Pietra.
1996.
A maximumentropy approach to natural language processing.Computational Linguistics, 22(1):39?71.J.
Besag.
1975.
Statistical analysis of non-lattice data.The Statistician, 24:179?195.Le?on Bottou.
1991.
Une Approche the?orique del?Apprentissage Connexionniste: Applications a` laReconnaissance de la Parole.
Ph.D. thesis, Univer-site?
de Paris XI.Ted Briscoe and John Carroll.
1993.
Generalizedprobabilistic LR parsing of natural language (cor-pora) with unification-based methods.
Computa-tional Linguistics, 19:25?59.Paul R. Cohen.
1995.
Empirical Methods for Artifi-cial Intelligence.
The MIT Press, Cambridge, Mas-sachusetts.Stephen Della Pietra, Vincent Della Pietra, and JohnLafferty.
1997.
Inducing features of random fields.IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 19(4):380?393.John E. Hopcroft and Jeffrey D. Ullman.
1979.
Intro-duction to Automata Theory, Languages and Com-putation.
Addison-Wesley.K.
Inui, V. Sornlertlamvanich, H. Tanaka, and T. Toku-naga.
1997.
A new formalization of probabilisticGLR parsing.
In Proceedings of the Fifth Interna-tional Workshop on Parsing Technologies (IWPT-97), pages 123?134, MIT.Frederick Jelinek.
1997.
Statistical Methods forSpeech Recognition.
The MIT Press, Cambridge,Massachusetts.Frederick Jelinek.
2000.
Stochastic analysis of struc-tured language modeling.
Technical report, Centerfor Language and Speech Modeling, Johns HopkinsUniversity.Mark Johnson, Stuart Geman, Stephen Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In TheProceedings of the 37th Annual Conference of theAssociation for Computational Linguistics, pages535?541, San Francisco.
Morgan Kaufmann.Mark Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24(4):613?632.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Machine Learning: Proceedingsof the Eighteenth International Conference (ICML2001).K.
Lari and S.J.
Young.
1990.
The estimationof Stochastic Context-Free Grammars using theInside-Outside algorithm.
Computer Speech andLanguage, 4(35-56).Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum Entropy Markov Mod-els for information extraction and segmentation.
InMachine Learning: Proceedings of the SeventeenthInternational Conference (ICML 2000), pages 591?598, Stanford, California.Judea Pearl.
1988.
Probabalistic Reasoning in In-telligent Systems: Networks of Plausible Inference.Morgan Kaufmann, San Mateo, California.
