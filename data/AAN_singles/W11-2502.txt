Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 11?21,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsComparison of the Baseline Knowledge-, Corpus-, and Web-based SimilarityMeasures for Semantic Relations ExtractionAlexander PanchenkoCenter for Natural Language Processing (CENTAL)Universite?
catholique de Louvain, Belgiumalexander.panchenko@student.uclouvain.beAbstractUnsupervised methods of semantic relationsextraction rely on a similarity measure be-tween lexical units.
Similarity measures differboth in kinds of information they use and inthe ways how this information is transformedinto a similarity score.
This paper is makinga step further in the evaluation of the avail-able similarity measures within the contextof semantic relation extraction.
We compare21 baseline measures ?
8 knowledge-based, 4corpus-based, and 9 web-based metrics withthe BLESS dataset.
Our results show thatexisting similarity measures provide signifi-cantly different results, both in general per-formances and in relation distributions.
Weconclude that the results suggest developing acombined similarity measure.1 IntroductionSemantic relations extraction aims to discovermeaningful lexico-semantic relations such as syn-onyms and hyponyms between a given set of lexi-cally expressed concepts.
Automatic relations dis-covery is a subtask of automatic thesaurus con-struction (see Grefenstette (1994), and Panchenko(2010)).A set of semantic relations R between a set ofconcepts C is a binary relation R ?
C ?
T ?
C,where T is a set of semantic relation types.
A re-lation r ?
R is a triple ?ci, t, cj?
linking two con-cepts ci, cj ?
C with a semantic relation of typet ?
T .
We are dealing with six types of semanticrelations: hyperonymy, co-hyponymy, meronymy,event (associative), attributes, and random: T ={hyper, coord,mero, event, attri, random}.
Wedescribe analytically and compare experimentallymethods, which discover set of semantic relationsR?
for a given set of concepts C. A semantic relationextraction algorithm aims to discover R?
?
R.One approach for semantic relations extractionis based on the lexico-syntactic patterns which areconstructed either manually (Hearst, 1992) or semi-automatically (Snow et al, 2004).
The alternativeapproach, adopted in this paper, is unsupervised (seee.g.
Lin (1998a) or Sahlgren (2006)).
It relies ona similarity measure between lexical units.
Vari-ous measures are available.
We compare 21 base-line measures: 8 knowledge-based, 4 corpus-based,and 9 web-based.
We would like to answer on twoquestions: ?What metric is most suitable for the un-supervised relation extraction?
?, and ?Does variousmetrics capture the same semantic relations??.
Thesecond question is particularly interesting for devel-oping of a meta-measure combining several metrics.This information may also help us choose a measurewell-suited for a concrete application.We extend existing surveys in three ways.
First,we ground our comparison on the BLESS dataset1,which is open, general, and was never used beforefor comparing all the considered metrics.
Secondly,we face corpus-, knowledge-, and web-based, whichwas never done before.
Thirdly, we go further thanmost of the comparisons and thoroughly comparethe metrics with respect to relation types they pro-vide.
We report empirical relation distributions for1http://sites.google.com/site/geometricalmodels/sharedevaluation11each measure and check if they are significantly dif-ferent.
Next, we propose a way to find the measureswith the most and the least similar relation distribu-tions.
Finally, we report information about redun-dant measures in an original way ?
in a form of anundirected graph.2 Methodology2.1 Similarity-based Semantic RelationsDiscoveryWe use an unsupervised approach to calculate setof semantic relations R between a given set ofconcepts C (see algorithm 1).
The method usesone of 21 similarity measures described in sections2.2 to 2.4.
First, it calculates the concept?conceptsimilarity matrix S with a measure sim.
Sincesome similarity measures output scores outsidethe interval [0; 1] we transform them with thefunction normalize as following: S?
(S?min(S))max(S) .If we deal with a dissimilarity measure, we ad-ditionally transform its score S to similarity asfollowing: S ?
1 ?
normalize(S).
Finally, thefunction threshold calculates semantic relations Rbetween concepts C with the k-NN thresholding:?|C|i=1 {?ci, t, cj?
: cj ?
top k% concepts ?
sij ?
?}
.Here k is the percent of the top similar conceptsto a concept ci, and ?
is a small value whichensures than nearly-zero pairwise similarities sijwill be ignored.
Thus, the method links eachconcept ci with k% of its nearest neighbours.Algorithm 1: Computing semantic relationsInput: Concepts C, Sim.parameters P ,Threshold k, Min.similarity value ?Output: Unlabeled semantic relations R?1 S?
sim(C,P ) ;2 S?
normalize(S) ;3 R??
threshold(S, k, ?)
;4 return R?
;Below we list the pairwise similarity measuressim used in our experiments with references to theoriginal papers, where all details can be found.2.2 Knowledge-based MeasuresThe knowledge-based metrics use a hierarchical se-mantic network in order to calculate similarities.Some of the metrics also use counts derived froma corpus.
We evaluate eight knowledge-based mea-sures listed below.
Let us describe them in the fol-lowing notations: cr is the root concept of the net-work; h is the height of the network; len(ci, cj) isthe length of the shortest path in the network be-tween concepts; cij is a lowest common subsumerof concepts ci and cj ; P (c) is the probability of theconcept, estimated from a corpus (see below).
Then,the Inverted Edge Count measure (Jurafsky and Mar-tin, 2009, p. 687) issij = len(ci, cj)?1; (1)Leacock and Chodorow (1998) measure issij = ?loglen(ci, cj)2h; (2)Resnik (1995) measure issij = ?log(P (cij)); (3)Jiang and Conrath (1997) measure issij = (2log(P (cij))?
(log(P (ci))+log(P (cj))))?1;(4)Lin (1998b) measure issij = (2log(P (cij))log(P (ci) + log(P (cj)); (5)Wu and Palmer (1994) measure issij =2len(cr, cij)len(ci, cij) + len(cj , cij) + 2len(cr, cij).
(6)Extended Lesk (Banerjee and Pedersen, 2003) mea-sure issij =?ci?Ci?cj?Cjsimg(ci, cj), (7)where simg is a gloss-based similarity measure, andsetCi includes concept ci and all concepts which aredirectly related to it.Gloss Vectors measure (Patwardhan and Peder-sen, 2006) is calculated as a cosine (9) between con-text vectors vi and vj of concepts ci and cj .
A con-text vector calculated as following:vi =?
?j:cj?Gifj .
(8)12Here fj is a first-order co-occurrence vector, derivedfrom the corpus of all glosses, and Gi is concate-nation of glosses of the concept ci and all conceptswhich are directly related to it.We experiment with measures relying on theWORDNET 3.0 (Miller, 1995) as a semantic net-work and SEMCOR as a corpus (Miller et al, 1993).2.3 Corpus-based measuresWe use four measures, which rely on the bag-of-word distributional analysis (BDA) (Sahlgren,2006).
They calculate similarity of concepts ci, cjas similarity of their feature vectors fi, fj withthe following formulas (Jurafsky and Martin, 2009,p.
699): cosinesij =fi ?
fj?fi?
?fj?, (9)Jaccardsij =?Nk=1min(fik, fjk)?Nk=1max(fik, fjk), (10)Manhattansij =N?k=1|fik ?
fjk|, (11)Euclidiansij =???
?N?k=1(fik ?
fjk)2.
(12)The feature vector fi is a first-order co-occurrencevector.
The context of a concept includes allwords from a sentence where it occurred, whichpass a stop-word filter (around 900 words) and astop part-of-speech filter (nouns, adjectives, andverbs are kept).
The frequencies fij are normalizedwith Poinwise Mutual Information (PMI): fij =log(fij/(count(ci)count(fj))).
In our experimentswe use two general English corpora (Baroni et al,2009): WACYPEDIA (800M tokens), and PUKWAC(2000M tokens).
These corpora are POS-taggedwith the TreeTagger (Schmid, 1994).2.4 Web-based measuresThe web-based metrics use the Web text search en-gines in order to calculate the similarities.
They relyon the number of times words co-occur in the doc-uments indexed by an information retrieval system.Let us describe these measures in the following no-tation: hi is the number of documents (hits) returnedby the system by the query ?ci?
; hij is the numberof hits returned by the query ?ci AND cj?
; and Mis number of documents indexed by the system.
Weuse two web-based measures: Normalized GoogleDistance (NGD) (Cilibrasi and Vitanyi, 2007):sij =max(log(hi, hj))?
log(hij)log(M)?min(log(hi), log(hj)), (13)and PMI-IR similarity (Turney, 2001) :sij = log(hij?i?j hihjhihj?i hij).
(14)We experiment with 5 NGD measures based on Ya-hoo, YahooBoss 2, Google, Google over Wikipedia,and Factiva 3; and with 4 PMI-IR measures basedon YahooBoss, Google, Google over Wikipedia, andFactiva.
We perform search among all indexed docu-ments or within the domain wikipedia.org (wedenote the latter measures with the postfix -W).2.5 Classification of the measuresIt might help to understand the results if we men-tion that (1) - (6) are measures of semantic similar-ity, while (7) and (8) are measures of semantic relat-edness.
Semantic relatedness is a more general no-tion than semantic similarity (Budanitsky and Hirst,2001).
A measure of semantic similarity uses onlyhierarchical and equivalence relations of the seman-tic network, while a measure of semantic related-ness also use relations of other types.
Furthermore,measures (1), (2), (3), are ?pure?
semantic similar-ity measures since they use only semantic network,while (3), (4), and (5) combine information from asemantic network and a corpus.The corpus-based and web-based measures arecalculated differently, but they are both clearly dis-tributional in nature.
In that respect, the web-basedmeasures use the Web as a corpus.
Figure 1 contains2http://developer.yahoo.com/search/boss/3http://www.factiva.com/13Figure 1: Classification of the measures used in the paper.a more precise classification of the considered mea-sures, according to their properties.
Finally, both (8)and (9)-(12), rely on the vector space model.2.6 Experimental SetupWe experiment with the knowledge-based measuresimplemented in the WORDNET::SIMILARITY pack-age (Pedersen et al, 2004).
Our own implemen-tation is used in the experiments with the corpus-based measures and the web-based measures rely-ing on the YAHOO BOSS search engine API.
Weuse the MEASURES OF SEMANTIC RELATEDNESSweb service 4 to assess the other web measures.The evaluation was done with the BLESS setof semantic relations.
It relates 200 target con-cepts to some 8625 relatum concepts with 26554 se-mantic relations (14440 are correct and 12154 arerandom).
Every relation has one of the followingsix types: hyponymy, co-hyponymy, meronymy, at-tribute, event, and random.
The distribution of re-lations among those types is given in table 1.
Eachconcept is a single English word.3 Results3.1 Comparing General Performance of theSimilarity MeasuresIn our evaluation semantic relations extraction wasviewed as a retrieval task.
Therefore, for every met-ric we calculated precision, recall, and F1-measurewith respect to the golden standard.
Let R?
be set ofextracted semantic relations, and R be set of seman-tic relations in the BLESS.
ThenPrecision =|R ?
R?||R?|, Recall =|R ?
R?||R|.An extracted relation ?ci, t?, cj?
?
R?
matches a re-lation from the evaluation dataset ?ci, t, cj?
?
R if4http://cwl-projects.cogsci.rpi.edu/msr/Figure 2: Precision-recall graph of the six similarity mea-sures (kNN threshold value k = 0?
52%).t 6= random.
Thus, an extracted relation is correctif it has any type in BLESS, but random.General performance of the measures is presentedin table 1 (columns 2-4).
The Resnik measure (3) isthe best among the knowledge-based measures; theNGD (13) measure relying on the Yahoo search en-gine is the best results among the web-based mea-sures.
Finally, the cosine measure (9) (BDA-Cos) isthe best among all the measures.
The table 2 demon-strate some extracted relations discovered with theBDA-Cos measure.In table 1 we ranked the measures based on theirF-measure when precision is fixed at 80% (see fig-ure 2).
We have chosen this precision level, be-cause it is a point when automatically extractedrelations start to be useful.
It is clear from theprecision-recall graph (figure 2) that if another pre-cision level is fixed then ranking of the metrics willchange.
Analysis of this and similar plots for othermeasures shows us that: (1) the best knowledge-based metric is Resnik; (2) the BDA-Cos is thebest among the corpus-based measures, but BDA-Jaccard is very close to it; (3) the three best web-based measures are NGD-Google (within the preci-sion range 100-90%), NGD-Factiva (within the pre-cision range 90%-87%), and NGD-Yahoo (startingfrom the precision level 87%).
In these settings,choose of the most suitable metric may depend onthe application.
For instance, if just a few preciserelations are needed then NGD-Google is a goodchoice.
On the other hand, if we tolerate a slightlyless precision, and if we need many relations thenthe BDA-Cos is the best choice.Figure 3 depicts learning curve of the BDA-Cos14Figure 3: Learning curves of the BDA-Cos on the Wa-Cypedia and PukWaC corpora (0.1M?2000M tokens).Figure 4: Percent of co-hyponyms among all correctlyextracted relations for the six best measures.measure.Dependence of the F-measure at the preci-sion level of 80% from the corpus size is not linear.F-measure improves up to 44% when we increasecorpus size from 1M to 10M tokens; increasing cor-pus from 10M to 100M tokens gives the improve-ment of 16%; finally, increasing corpus from 100Mto 2000M tokens gives the improvement of only 3%.3.2 Comparing Relation Distributions of theSimilarity MeasuresIn this section, we are trying to figure out whattypes of semantic relations the measures find.
Wecompare distributions of semantic relations againstthe BLESS dataset.
Generally, if two measureshave equal general performances, one may want tochoose a metric which provides more relations of acertain type, depending on the application.
This in-formation may be also valuable in order to decidewhich metrics to combine in a meta-metric.Distribution of Relation Types.
In this sec-tion, we estimate empirical relation distribution ofthe metrics over five relation types: hyponymy, co-hyponymy, meronymy, attribute, and event.
To do sowe calculate percents of correctly extacted relationsof type t for a each measure:Percent =R?t|R ?
R?|, where?t?TR?t = |R ?
R?|.Here |R ?
R?| is a set of all correctly extracted rela-tions, and R?t is a set of extracted relations of type t.Figure 4 demonstrates that percent of extracted rela-tions of certain type depends on the value of k (c.f.section 2.1).
For instance, if k = 10% then 77%of extracted relations by Resnik are co-hyponyms,but if k = 40% then the same measure outputs 40%of co-hyponyms.
We report relations distribution attwo levels of the threshold k ?
10% and 40%.The empirical distributions are reported incolumns 5-9 of the table 1.
Each of those columnscorrespond to one semantic relation type t, and con-tains two numbers: p10 ?
percent of relations of typet when k = 10%, and p40 ?
percent of relations oftype t when k = 40%.
We represent those two val-ues in the following format: p10|p40.
For instance,77|40 behind the Resnik measure means that whenk = 10% it extracts 77% of co-hypernyms, andwhen k = 40% it extracts 40% of co-hypernyms.If the threshold k is 10% then the biggest frac-tion of extracted relations are co-hyponyms ?
from35% for BDA-Manhattan to 77% for Resnik mea-sure.
At this threshold level, the knowledge-basedmeasures mostly return co-hyponyms (60% in aver-age) and hyperonyms (23% in average).
The corpus-based metrics mostly return co-hyponyms (38% inaverage) and event relations (26% in average).
Theweb-based measures return many (48% in average)co-hyponymy relations.If the threshold k is 40% then relation distributionof all the measures significantly changes.
Most ofthe relations returned by the knowledge-based mea-sures are co-hyponyms (36%) and meronyms (24%).The majority of relations discovered by the corpus-based metrics are co-hyponyms (33% ), event rela-tions (26%), and meronyms (20.33%).
The web-based measures at this threshold value return manyevent relations (32%).15General Performance Semantic Relations DistributionMeasure k Recall F1 hyper,% coord,% attri,% mero,% event,%Resnik 40% 0.59 0.68 9 | 14 77 | 40 4 | 8 6 | 22 4 | 15Inv.Edge-Counts 38% 0.56 0.66 22 | 15 61 | 40 4 | 8 7 | 22 6 | 15Leacock-Chodorow 38% 0.56 0.66 22 | 15 61 | 40 4 | 8 7 | 22 6 | 15Wu Palmer 37% 0.54 0.65 20 | 15 64 | 42 3 | 8 7 | 22 5 | 13Lin 36% 0.53 0.64 30 | 16 52 | 31 4 | 7 8 | 29 5 | 16Gloss Overlap 36% 0.53 0.63 5 | 6 52 | 34 7 | 12 18 | 21 18 | 27Jiang-Conrath 35% 0.52 0.63 38 | 16 45 | 30 4 | 6 8 | 29 5 | 18Extended Lesk 30% 0.45 0.57 21 | 14 39 | 30 1 | 9 29 | 28 9 | 19BDA-Cos 52% 0.76 0.78 9 | 7 42 | 27 11 | 20 15 | 17 23 | 30BDA-Jaccard 51% 0.75 0.77 10 | 7 45 | 27 8 | 16 16 | 20 20 | 27BDA-Manhattan 37% 0.54 0.65 7 | 6 35 | 24 17 | 22 10 | 15 31 | 34BDA-Euclidian 21% 0.30 0.44 7 | 7 31 | 18 20 | 26 12 | 13 30 | 37NGD-Yahoo 46% 0.68 0.74 7 | 6 51 | 30 9 | 18 17 | 20 15 | 25NGD-Factiva 47% 0.66 0.72 10 | 8 44 | 28 8 | 19 23 | 22 16 | 25NGD-YahooBOSS 35% 0.51 0.63 13 | 10 54 | 36 4 | 10 14 | 20 15 | 22NGD-Google 33% 0.48 0.60 1 | 7 41 | 28 45 | 19 2 | 19 11 | 28NGD-Google-W 29% 0.43 0.56 8 | 9 45 | 31 8 | 14 20 | 21 19 | 25PMI-YahooBOSS 29% 0.43 0.56 15 | 12 53 | 38 3 | 9 15 | 20 13 | 20PMI-Factiva 25% 0.28 0.44 8 | 8 42 | 30 10 | 17 21 | 20 18 | 24PMI-Google 12% 0.18 0.29 8 | 8 55 | 35 7 | 15 17 | 21 12 | 22PMI-Google-W 9% 0.13 0.23 12 | 11 47 | 38 7 | 11 20 | 20 13 | 19Random measure 8 | 9 24 | 25 20 | 19 22 | 20 26 | 27BLESS dataset 9 25 20 19 27Table 1: Columns 2-4: Recall and F-measure when Precision= 0.8 (correct relations of all types vs random relations).Columns 5-9: percent of extracted relations of a certain type with respect to all correctly extracted relations, whenthreshold k equal 10% or 40%.
The best measure are sorted by F-measure; the best measures are in bold.ant banana fork missile salmoncockroach (coord) mango (coord) prong (mero) warhead (mero) trout (coord)grasshopper (coord) pineapple (coord) spoon (coord) weapon (hyper) mackerel (coord)silverfish (coord) papaya (coord) knife (coord) deploy (event) herring (coord)wasp (coord) pear (coord) lift (event) nuclear (attri) fish (event)insect (hyper) ripe (attri) fender (random) bomb (coord) tuna (coord)arthropod (hyper) peach (coord) plate (coord) destroy (event) oily (attri)industrious (attri) coconut (coord) rake (coord) rocket (coord) poach (event)ladybug (coord) fruit (hyper) shovel (coord) arm (hyper) catfish (coord)bee (coord) apple (coord) handle (mero) propellant (mero) catch (event)beetle (coord) apricot (coord) sharp (attri) bolster (random) fresh (attri)locust (coord) strawberry (coord) spade (coord) launch (event) cook (event)dragonfly (coord) ripen (event) napkin (coord) deadly (attri) cod (coord)hornet (coord) plum (coord) cutlery (hyper) country (random) smoke (event)creature (hyper) grapefruit (coord) head (mero) strike (event) seafood (hyper)crawl (event) cherry (coord) scissors (coord) defuse (event) eat (event)Table 2: Examples of the discovered semantic relations with the bag-of-words distributional analysis (BDA-Cos).16Interestingly, for the most of the measures, per-cent of extracted hyponyms and co-hyponyms de-creases as the value of k increase, while the percentof other relations increases.
In order to make it clear,we grayed cells of the table 1 when p10 ?
p40.Similarity to the BLESS Distribution.
In thissection, we check if relation distributions (see ta-ble 1) are completely biased by the distribution inthe evaluation dataset.
We compare relation dis-tributions of the metrics with the distribution inthe BLESS on the basis of the ?2 goodness of fittest 5 (Agresti, 2002) with df = 4.
A random simi-larity measure is completely biased by the distribu-tion in the evaluation dataset: ?2 = 5.36, p = 0.252for k = 10% and ?2 = 3.17, p = 0.53 for k = 40%.On the other hand, distributions of all the 21 mea-sures are significantly different from the distributionin the BLESS (p < 0.001).
The value of chi-squarestatistic varies from ?2 = 89.94 (NGD-Factiva,k = 10%) to ?2 = 4000 (Resnik, k = 10%).Independence of Relation Distributions.
In thissection, we check whether relation distributions ofthe various measures are significantly different.
Inorder to do so, we perform the chi-square indepen-dence test on the table 1.
Our experiments shownthat there is a significant interaction between thetype of the metric and the relations distribution:?2 = 10487, p < 0.001, df = 80 for all the metrics;?2 = 2529, df = 28, p < 0.001 for the knowledge-based metrics; ?2 = 245, df = 12, p < 0.001 forthe corpus-based metrics; and ?2 = 3158, df =32, p < 0.001 for the web-based metrics.
Thus,there is a clear dependence between the type of mea-sure and the type of relation it extracts.Most Similar and Dissimilar Measures.
In thissection, we would like to find the most similar anddisimilar measures.
This information is particularlyuseful for the combination of the metrics.
In order tofind redundant measures, we calculate distance xijbeween measures simi and simj , based on the ?2-statistic:xij = xji =?t?
T(|R?it| ?
|R?jt |)2|R?jt |, (15)where R?it is ensemble of correctly extracted rela-5Here and below, we calculate the ?2 statistic from the table1 (columns 5-9), where percents are replaced with frequencies.tions of type t with measure simi.
We calculatethese distances for all pairs of measures and thenrank the pairs according to the value of xij .
Ta-ble 3 present list of the most similar and dissimi-lar metrics obtained this way.
Figure 7 reports in acompact way all the pairwise similarities (xij)21?21between the 21 metrics.
In this graph, an edgelinks two measures, which have the distance valuexij < 220.
The graph was drawn with the Fruchter-man and Reingold (1991) force-directed layout al-gorithm.
One can see that relation distributions ofthe web- and corpus-based measures are quite sim-ilar.
The knowledge-based measures are much dif-ferent from them, but similar among themselves.Distribution of Similarity Scores.
In this sec-tion, we compare distributions of similarity scoresacross relation types with the following procedure:(1) Pick a closest relatum concept cj per relationtype t for each target concept ci.
(2) Convert sim-ilarity scores associated to each target concept to z-scores.
(3) Summarize the distribution of similari-ties across relations by plotting the z-scores groupedby relations in a box plot.
(4) Verify the statisticalsignificance of the differences in similarity scoresacross relations by performing the Tukey?s HSD test.Figure 6 presents the distributions of similaritiesacross various relation types for Resnik, BDA-Cos,and NGD-Yahoo.
First, meaningful relation typesfor these three measures are significantly different(p < 0.001) from random relations.
The only ex-ception is the Resnnik measure ?
its similarity scoresfor the attribute relations are not significantly differ-ent (p = 0.178) from random relations.
Thus, thebest three measures provide scores which let us sep-arate incorrect relations from the correct ones if anappropriate threshold k is set.
Second, the similar-ity scores have highest values for the co-hyponymyrelations.
Third, BDA-Cos, BDA-Jaccard, NGD-Yahoo, NGD-Factiva, and PMI-YahooBoss providethe best scores.
They let us clearly (p < 0.001) sep-arate meaningful relations from the random ones.From the other hand, the poorest scores were pro-vided by BDA-Manhattan, BDA-Euclidian, NGD-YahooBoss, and NGD-Google, because their scoreslet us clearly separate only co-hyponyms from therandom relations.Corpus Size.
Table 1 presented relation distribu-tion of the BDA-Cos trained on the 2000M token17Figure 5: Semantic relations distribution function of cor-pus size (BDA-Cos measure, PukWaC corpus).corpus UKWAC.
Figure 5 shows the relation dis-tribution function of the corpus size.
First, if cor-pus size increases then percent of attribute relationsdecreases, while percent of co-hyponyms increases.Second, corpus size does not drastically influencethe distribution for big corpora.
For instance, ifwe increase corpus size from 100M to 2000M to-kens then the percent of relations change on 3% forattributes, on 3% co-hyponyms, on 1% events, on0.7% hyperonyms, and on 0.4% meronyms.4 Related WorkPrior research provide us information about gen-eral performances of the measures considered inthis paper, but not necessarily on the task of se-mantic relations extraction.
For instance, Mihal-cea et al (2006) compare two corpus-based (PMI-IRand LSA) and six knowledge-based measures on thetask of text similarity computation.
The authors re-port that PMI-IR is the best measure; that, similarlyto our results, Resnik is the best knowledge-basedmeasure; and that simple average over all 8 mea-sures is even better than PMI-IR.
Budanitsky andHirst (2001) report that Jiang-Conrath is the bestknowledge-based measure for the task of spellingcorrection.
Patwardhan and Pedersen (2006) eval-uate six knowledge-based measures on the task ofword sense disambiguation and report the same re-sult.
This contradicts our results, since we foundResnik to be the best knowledge-based measure.Peirsman et al (2008) compared general per-formances and relation distributions of distribu-tional methods using a lexical database.
Sahlgren(2006) evaluated syntagmatic and paradigmatic bag-of-word models.
Our findings mostly fits well theseand other (e.g.
Curran (2003) or Bullinaria and Levy(2007)) results on the distributional analysis.
Lind-sey et al (2007) compared web-based measures.Authors suggest that a small search domain is betterthan the whole Internet.
Our results partially confirmthis observation (NGD-Factiva outperforms NGD-Google), and partially contradicts it (NGD-Yahoooutperforms NGD-Factiva).Van de Cruys (2010) evaluates syntactic, and bag-of-words distributional methods and suggests thatthe syntactic models are the best for the extraction oftight synonym-like similarity.
Wandmacher (2005)reports that LSA produces 46.4% of associative rela-tions, 15.2% of synonyms, antonyms, hyperonyms,co-hyponyms, and meronyms, 5.6% of syntactic re-lations, and 32.8% of erroneous relations.
We can-not compare these results to ours, since we did notevaluate neither LSA nor syntactic models.A common alternative to our evaluation method-ology is to use the Spearman?s rank correlationcoefficient (Agresti, 2002) to compare the resultswith the human judgments, such as those obtainedby Rubenstein and Goodenough (1965) or Millerand Charles (1991).5 Conclusion and Future WorkThis paper has compared 21 similarity measures be-tween lexical units on the task of semantic relationextraction.
We compared their general performancesand figured out that Resnik, BDA-Cos, and NGD-Yahoo provide the best results among knowledge-, corpus-, and web-based measures, correspond-ingly.
We also found that (1) semantic relation dis-tributions of the considered measures are signifi-cantly different; (2) all measures extract many co-hyponyms; (3) the best measures provide the scoreswhich let us clearly separate correct relations fromthe random ones.The analyzed measures provide complimentarytypes of semantic information.
This suggests de-veloping a combined measure of semantic similar-ity.
A combined measure is not presented here sincedesigning an integration technique is a complex re-search goal on its own right.
We will address thisproblem in our future research.18Figure 6: Distribution of similarities accross relation types for Resnik, BDA-Cos, and NGD-Yahoo measures.Most Similar Measures Most Disimilar Measuressimi simj xij simi simj xijLeacock-Chodorow Inv.Edge-Counts 0 NGD-Google Extended Lesk 39935.16BDA-Jaccard BDA-Cos 7.17 Jiang-Conrath NGD-Google 27478.90NGD-YahooBOSS PMI-YahooBOSS 19.58 Lin NGD-Google 17527.22Wu-Palmer Inv.Edge-Counts 24.00 NGD-Google Wu-Palmer 17416.95Wu-Palmer Leacock-Chodorow 24.00 NGD-Google PMI-YahooBOSS 13390.66BDA-Manhattan BDA-Euclidian 25.37 Inv.Edge-Counts NGD-Google 12012.79PMI-Google-W NGD-Factiva 27.65 Leacock-Chodorow NGD-Google 12012.79PMI-Google NGD-Yahoo 33.42 NGD-Google Resnik 11750.41NGD-Google-W NGD-Factiva 40.03 NGD-Google NGD-YahooBOSS 11556.69NGD-W PMI-Factiva 42.17 BDA-Euclidian Extended Lesk 8411.66Gloss Overlap NGD-Yahoo 53.64 NGD-Factiva NGD-Google 8066.75NGD-Factiva PMI-Factiva 58.13 BDA-Euclidian Resnik 6829.71Lin Jiang-Conrath 58.42 PMI-Google-W NGD-Google 6574.62Gloss Overlap NGD-Google-W 62.46 BDA-Manhattan Extended Lesk 6428.47Table 3: List of the most and least similar measures (k = 10%).Figure 7: Measures grouped according to similarity of their relation distributions with (15).
An edge links measuressimi and simj if xij < 220.
The knowledge-, corpus-, and web-based measures are marked in red, blue, and greencorrespondingly and with the prefixes ?K?,?C?,and ?W?.
The best measures are marked with a big circle.196 AcknowledgmentsI would like to thank Thomas Franc?ois who kindlyhelped with the evaluation methodology, and mysupervisor Dr. Ce?drick Fairon.
The two anony-mous reviewers, Ce?drick Fairon, Thomas Franc?ois,Jean-Leon Bouraoui, and Andew Phillipovich pro-vided comments and remarks, which considerablyimproved quality of the paper.
This research is sup-ported by Wallonie-Bruxelles International.ReferencesAlan Agresti.
Categorical Data Analysis (Wiley Se-ries in Probability and Statistics).
Wiley seriesin probability and statistics.
Wiley Interscience,Hoboken, NJ, 2 edition, 2002.Satanjeev Banerjee and Ted Pedersen.
Extendedgloss overlaps as a measure of semantic related-ness.
In International Joint Conference on Ar-tificial Intelligence, volume 18, pages 805?810,2003.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
The wacky wide web: A col-lection of very large linguistically processed web-crawled corpora.
Language Resources and Eval-uation, 43(3):209?226, 2009.Alexander Budanitsky and Graeme Hirst.
Se-mantic distance in WordNet: An experimental,application-oriented evaluation of five measures.In Workshop on WordNet and Other Lexical Re-sources, volume 2, 2001.John A. Bullinaria and Joseph P. Levy.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Be-havior Research Methods, 39(3):510, 2007.Rudi L. Cilibrasi and Paul M. B. Vitanyi.
TheGoogle Similarity Distance.
IEEE Trans.
onKnowl.
and Data Eng., 19(3):370?383, 2007.James R. Curran.
From distributional to semanticsimilarity.
PhD thesis, University of Edinburgh,2003.Thomas M. J. Fruchterman and Edward M. Rein-gold.
Graph drawing by force-directed placement.Software: Practice and Experience, 21(11):1129?1164, 1991.Gregory Grefenstette.
Explorations in AutomaticThesaurus Discovery (The Springer InternationalSeries in Engineering and Computer Science).Springer, 1 edition, 1994.
ISBN 0792394682.Marti A. Hearst.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedingsof the 14th conference on Computational linguis-tics, pages 539?545, Morristown, NJ, USA, 1992.Association for Computational Linguistics.Jay J. Jiang and David W. Conrath.
Semantic Simi-larity Based on Corpus Statistics and Lexical Tax-onomy.
In International Conference Research onComputational Linguistics (ROCLING X), pages19?33, 1997.Daniel Jurafsky and James H. Martin.
Speech andLanguage Processing: An Introduction to Nat-ural Language Processing, Computational Lin-guistics, and Speech Recognition.
Prentice Hall,2009.Claudia Leacock and Martin Chodorow.
Combin-ing Local Context and WordNet Similarity forWord Sense Identification.
An Electronic LexicalDatabase, pages 265?283, 1998.Dekang Lin.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of the 17th interna-tional conference on Computational linguistics-Volume 2, pages 768?774.
Association for Com-putational Linguistics, 1998a.Dekang Lin.
An Information-Theoretic Definitionof Similarity.
In In Proceedings of the 15th Inter-national Conference on Machine Learning, pages296?304, 1998b.Robert Lindsey, Vladislav D. Veksler, AlexGrintsvayg, and Wayne D. Gray.
Be wary of whatyour computer reads: the effects of corpus selec-tion on measuring semantic relatedness.
In 8thInternational Conference of Cognitive Modeling,ICCM, 2007.Rado Mihalcea, Corley Corley, and Carlo Strappa-rava.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceed-ings of the National Conference on Artificial In-telligence, volume 21, page 775.
Menlo Park,CA; Cambridge, MA; London; AAAI Press; MITPress, 2006.20George A. Miller.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41, 1995.George A. Miller and Walter G. Charles.
Contextualcorrelates of semantic similarity.
Language andCognitive Processes, 6(1):1?28, 1991.George A. Miller, Claudia Leacock, Randee Tengi,and Ross T. Bunker.
A semantic concordance.In Proceedings of the workshop on Human Lan-guage Technology, pages 303?308.
Associationfor Computational Linguistics, 1993.Alexander Panchenko.
Can we automatically re-produce semantic relations of an information re-trieval thesaurus?
In 4th Russian Summer Schoolin Information Retrieval, pages 13?18.
VoronezhState University, 2010.Siddharth Patwardhan and Ted Pedersen.
UsingWordNet-based context vectors to estimate the se-mantic relatedness of concepts.
Making Sense ofSense: Bringing Psycholinguistics and Computa-tional Linguistics Together, page 1, 2006.Ted Pedersen, Siddharth Patwardhan, and JasonMichelizzi.
WordNet:: Similarity: measuring therelatedness of concepts.
In Demonstration Papersat HLT-NAACL 2004 on XX, pages 38?41.
Asso-ciation for Computational Linguistics, 2004.Yves Peirsman, Kris Heylen, and Dirk Speelman.Putting things in order.
First and second ordercontext models for the calculation of semanticsimilarity.
Proceedings of the 9th Journe?es in-ternationales d?Analyse statistique des Donne?esTextuelles (JADT 2008), pages 907?916, 2008.Philip Resnik.
Using Information Content to Eval-uate Semantic Similarity in a Taxonomy.
In Pro-ceedings of the 14th International Joint Confer-ence on Artificial Intelligence., volume 1, pages448?453, 1995.H.
Rubenstein and J.B. Goodenough.
Contextualcorrelates of synonymy.
Communications of theACM, 8(10):627?633, 1965.Magnus Sahlgren.
The Word-Space Model: Us-ing distributional analysis to represent syntag-matic and paradigmatic relations between wordsin high-dimensional vector spaces.
PhD thesis,Stockholm University, 2006.Helmut Schmid.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
pages 44?49, 1994.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.Learning syntactic patterns for automatic hyper-nym discovery.
Advances in Neural InformationProcessing Systems (NIPS), 17:1297?1304, 2004.Peter Turney.
Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.
In Proceedings of thetwelfth european conference on machine learning(ecml-2001), 2001.Tim Van de Cruys.
Mining for Meaning: The Ex-traction of Lexicosemantic Knowledge from Text.PhD thesis, University of Groningen, 2010.Tonio Wandmacher.
How semantic is Latent Seman-tic Analysis?
Proceedings of TALN/RECITAL,2005.Zhibiao Wu and Martha Palmer.
Verbs seman-tics and lexical selection.
In Proceedings of the32nd annual meeting on Association for Compu-tational Linguistics, pages 133?138.
Associationfor Computational Linguistics, 1994.21
