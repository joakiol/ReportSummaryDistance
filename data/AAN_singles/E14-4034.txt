Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 174?179,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsSimple and Effective Approach for Consistent Training of HierarchicalPhrase-based Translation ModelsStephan Peitz1and David Vilar2and Hermann Ney11Lehrstuhl f?ur Informatik 6Computer Science Department2Pixformance GmbHRWTH Aachen University D-10587 Berlin, GermanyD-52056 Aachen, Germany david.vilar@gmail.com{peitz,ney}@cs.rwth-aachen.deAbstractIn this paper, we present a simple ap-proach for consistent training of hierarchi-cal phrase-based translation models.
Inorder to consistently train a translationmodel, we perform hierarchical phrase-based decoding on training data to findderivations between the source and tar-get sentences.
This is done by syn-chronous parsing the given sentence pairs.After extracting k-best derivations, wereestimate the translation model proba-bilities based on collected rule counts.We show the effectiveness of our proce-dure on the IWSLT German?English andEnglish?French translation tasks.
Ourresults show improvements of up to 1.6points BLEU.1 IntroductionIn state of the art statistical machine translationsystems, the translation model is estimated by fol-lowing heuristic: Given bilingual training data,a word alignment is trained with tools such asGIZA++(Och and Ney, 2003) or fast align (Dyeret al., 2013).
Then, all valid translation pairs areextracted and the translation probabilities are com-puted as relative frequencies (Koehn et al., 2003).However, this extraction method causes severalproblems.
First, this approach does not consider,whether a translation pair is extracted from a likelyalignment or not.
Further, during the extractionprocess, models employed in decoding are notconsidered.For phrase-based translation, a successful ap-proach addressing these issues is presented in(Wuebker et al., 2010).
By applying a phrase-based decoder on the source sentences of the train-ing data and constraining the translations to thecorresponding target sentences, k-best segmenta-tions are produced.
Then, the phrases used forthese segmentations are extracted and counted.Based on the counts, the translation model prob-abilities are recomputed.
To avoid over-fitting,leave-one-out is applied.However, for hierarchical phrase-based transla-tion an equivalent approach is still missing.In this paper, we present a simple and effec-tive approach for consistent reestimation of thetranslation model probabilities in a hierarchicalphrase-based translation setup.
Using a heuristi-cally extracted translation model as starting point,the training data are parsed bilingually.
From theresulting hypergraphs, we extract k-best deriva-tions and the rules applied in each derivation.
Thisis done with a top-down k-best parsing algorithm.Finally, the translation model probabilities are re-computed based on the counts of the extractedrules.
In our procedure, we employ leave-one-outto avoid over-fitting.
Further, we consider all mod-els which are used in translation to ensure a con-sistent training.Experimental results are presented on theGerman?English and English?French IWSLTshared machine translation task (Cettolo et al.,2013).
We are able to gain improvements of up to1.6% BLEU absolute and 1.4% TER over a com-petitive baseline.
On all tasks and test sets, theimprovements are statistically significant with atleast 99% confidence.The paper is structured as follow.
First, we re-vise the state of the art hierarchical phrase-basedextraction and translation process.
In Section 3,we propose our training procedure.
Finally, ex-perimental results are given in Section 4 and weconclude with Section 5.2 Hierarchical Phrase-based TranslationIn hierarchical phrase-based translation (Chiang,2005), discontinuous phrases with ?gaps?
areallowed.
The translation model is formalizedas a synchronous context-free grammar (SCFG)174and consists of bilingual rules, which are basedon bilingual standard phrases and discontinuousphrases.
Each bilingual rule rewrites a genericnon-terminal X into a pair of strings?f and e?with both terminals and non-terminals in both lan-guagesX ?
?
?f, e??.
(1)In a standard hierarchical phrase-based translationsetup, obtaining these rules is based on a heuristicextraction from automatically word-aligned bilin-gual training data.
Just like in the phrase-basedapproach, all bilingual rules of a sentence pairare extracted given an alignment.
The standardphrases are stored as lexical rules in the rule set.In addition, whenever a phrase contains a sub-phrase, this sub-phrase is replaced by a genericnon-terminal X .
With these hierarchical phraseswe can define the hierarchical rules in the SCFG.The rule probabilities which are in general definedas relative frequencies are computed based on thejoint counts C(X ?
?
?f, e??)
of a bilingual ruleX ?
?
?f, e?
?pH(?f |e?)
=C(X ?
?
?f, e??)?
?f?C(X ?
?
?f?, e??).
(2)The translation probabilities are computed insource-to-target as well as in target-to-source di-rection.
In the translation processes, these proba-bilities are integrated in the log-linear combinationamong other models such as a language model,word lexicon models, word and phrase penalty andbinary features marking hierarchical phrases, gluerule and rules with non-terminals at the bound-aries.The translation process of hierarchical phrase-based approach can be considered as parsing prob-lem.
Given an input sentence in the source lan-guage, this sentence is parsed using the source lan-guage part of the SCFG.
In this work, we performthis step with a modified version of the CYK+ al-gorithm (Chappelier and Rajman, 1998).
The out-put of this algorithm is a hypergraph, which rep-resents all possible derivations of the input sen-tence.
A derivation represents an application ofrules from the grammar to generate the given in-put sentence.
Using the the associated target partof the applied rule, for each derivation a transla-tion can be constructed.
In a second step, the lan-guage model score is incorporated.
Given the hy-pergraph, this is done with the cube pruning algo-rithm presented in (Chiang, 2007).3 Translation Model TrainingWe propose following pipeline for consistent hi-erarchical phrase-based training: First we train aword alignment, from which the baseline trans-lation model is extracted as described in the pre-vious section.
The log-linear parameter weightsare tuned with MERT (Och, 2003) on a develop-ment set to produce the baseline system.
Next,we perform decoding on the training data.
As thetranslations are constrained to the given target sen-tences, we name this step forced decoding in thefollowing.
Details are given in the next subsection.Given the counts CFD(X ?
?
?f, e??)
of the rules,which have been applied in the forced decodingstep, the translation probabilities pFD(?f |e?)
for thetranslation model are recomputed:pFD(?f |e?)
=CFD(X ?
?
?f, e??)?
?f?CFD(X ?
?
?f?, e??).
(3)Finally, using the translation model with thereestimated probabilities, we retune the log-linearparameter weights and obtain our final system.3.1 Forced DecodingIn this section, we describe the forced decodingfor hierarchical phrase-based translation in detail.Given a sentence pair of the training data, weconstrain the translation of the source sentence toproduce the corresponding target sentence.
Forthis constrained decoding process, the languagemodel score is constant as the translation is fixed.Hence, the incorporation of the a language modelis not needed.
This results in a simplification ofthe decoding process as we do not have to employthe cube pruning algorithm as described in the pre-vious section.
Consequently, forced decoding forhierarchical phrase-based translation is equivalentto synchronous parsing of the training data.
Dyer(2010) has described an approach to reduce theaverage-case run-time of synchronous parsing bysplitting one bilingual parse into two successivemonolingual parses.
We adopt this method andfirst parse the source sentence and then the targetsentence with CYK+.If the given sentence pair has been parsed suc-cessfully, we employ a top-down k-best parsingalgorithm (Chiang and Huang, 2005) on the re-sulting hypergraph to find the k-best derivationsbetween the given source and target sentence.
Inthis step, all models of the translation process are175included (except for the language model).
Further,leave-one-out is applied to counteract overfitting.Note, that the model weights of the baseline sys-tem are used to perform forced decoding.Finally, we extract and count the rules whichhave been applied in the derivations.
These countsare used to recompute the translation probabilities.3.2 RecombinationIn standard hierarchical phrase-based decoding,partial derivations that are indistinguishable fromeach other are recombined.
In (Huck et al., 2013)two schemes are presented.
Either derivations thatproduce identical translations or derivations withidentical language model context are recombined.As in forced decoding the translation is fixed anda language model is missing, both schemes are notsuitable.However, a recombination scheme is necessaryto avoid derivations with the same applicationof rules.
Further, recombining such derivationsincreases simultaneously the amounts of consid-ered derivations during k-best parsing.
Given twoderivations with the same set of applied rules, theorder of application of the rules may be different.Thus, we propose following scheme for recom-bining derivations in forced decoding: Derivationsthat produce identical sets of applied rules are re-combined.
Figure 1 shows an example for k = 3.Employing the proposed scheme, derivations d1and d2are recombined since both share the sameset of applied rules ({r1, r3, r2}).d1: {r1, r3, r2}d2: {r3, r2, r1}d3: {r4, r5, r1, r2}(a)d1: {r1, r3, r2}d3: {r4, r5, r1, r2}d4: {r6, r5, r2, r3}(b)Figure 1: Example search space before (a) and af-ter (b) applying recombination.4 Experiments4.1 SetupThe experiments were carried out on the IWSLT2013 German?English shared translation task.11http://www.iwslt2013.orgGerman English English FrenchSentences 4.32M 5.23MRun.
Words 108M 109M 133M 147MVocabulary 836K 792K 845K 888KTable 1: Statistics for the bilingual trainingdata of the IWSLT 2013 German?English andEnglish?French task.It is focusing the translation of TED talks.
Bilin-gual data statistics are given in Table 1.
The base-line system was trained on all available bilingualdata and used a 4-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen andGoodman, 1998), trained with the SRILM toolkit(Stolcke, 2002).
As additional data sources for theLM we selected parts of the Shuffled News andLDC English Gigaword corpora based on cross-entropy difference (Moore and Lewis, 2010).
Inall experiments, the hierarchical search was per-formed as described in Section 2.To confirm the efficacy of our approach, addi-tional experiments were run on the IWSLT 2013English?French task.
Statistics are given in Ta-ble 1.The training pipeline was set up as describedin the previous section.
Tuning of the log-linearparameter weights was done with MERT on a pro-vided development set.
As optimization criterionwe used BLEU (Papineni et al., 2001).Forced decoding was performed on the TEDtalks portion of the training data (?140K sen-tences).
In both tasks, around 5% of the sentencescould not be parsed.
In this work, we just skippedthose sentences.We report results in BLEU [%] and TER [%](Snover et al., 2006).
All reported results are av-erages over three independent MERT runs, andwe evaluated statistical significance with MultE-val (Clark et al., 2011).4.2 ResultsFigure 2 shows the performance of setups us-ing translation models with reestimated translationprobabilities.
The setups vary in the k-best deriva-tion size extracted in the forced decoding (fd) step.Based on the performance on the development set,we selected two setups with k = 500 using leave-one-out (+l1o) and k = 750 without leave-one-out (-l1o).
Table 2 shows the final results forthe German?English task.
Performing consistenttranslation model training improves the translation176dev*eval11 testBLEU[%]TER[%]BLEU[%]TER[%]BLEU[%]TER[%]baseline 33.1 46.8 35.7 44.1 30.5 49.7forced decoding -l1o 33.2 46.3 36.3 43.4 31.2 48.8forced decoding +l1o 33.6 46.2 36.6 43.0 31.8 48.3Table 2: Results for the IWSLT 2013 German?English task.
The development set used for MERT ismarked with an asterisk (*).
Statistically significant improvements with at least 99% confidence over thebaseline are printed in boldface.dev*eval11 testBLEU[%]TER[%]BLEU[%]TER[%]BLEU[%]TER[%]baseline 28.1 55.7 37.5 42.7 31.7 49.5forced decoding +l1o 28.8 55.0 39.1 41.6 32.4 49.0Table 3: Results for the IWSLT 2013 English?French task.
The development set used for MERT ismarked with an asterisk (*).
Statistically significant improvements with at least 99% confidence over thebaseline are printed in boldface.31.53232.53333.5341  10  100  1000  10000BLEU[%]kdev fd +l1odev fd -l1odev baselineFigure 2: BLEU scores on the IWSLTGerman?English task of setups using trans-lation models trained with different k-bestderivation sizes.
Results are reported on dev with(+l1o) and without leave-one-out (-l1o).quality on all test sets significantly.
We gain animprovement of up to 0.7 points in BLEU and 0.9points in TER.
Applying leave-one-out results inan additional improvement by up to 0.4 % BLEUand 0.5 % TER.
The results for English?Frenchare given in Table 3.
We observe a similar im-provement by up to 1.6 % BLEU and 1.1 % TER.The improvements could be the effect of do-main adaptation since we performed forced decod-ing on the TED talks portion of the training data.Thus, rules which were applied to decode the in-domain data might get higher translation probabil-ities.Furthermore, employing leave-one-out seems toavoid overfitting as the average source rule lengthin training is reduced from 5.0 to 3.5 (k = 500).5 ConclusionWe have presented a simple and effective approachfor consistent training of hierarchical phrase-basedtranslation models.
By reducing hierarchical de-coding on parallel training data to synchronousparsing, we were able to reestimate the trans-lation probabilities including all models appliedduring the translation process.
On the IWSLTGerman?English and English?French tasks, thefinal results show statistically significant improve-ments of up to 1.6 points in BLEU and 1.4 pointsin TER.Our implementation was released as part of Jane(Vilar et al., 2010; Vilar et al., 2012; Huck et al.,2012; Freitag et al., 2014), the RWTH AachenUniversity open source statistical machine trans-lation toolkit.2AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreements no 287658 and no 287755.2http://www.hltpr.rwth-aachen.de/jane/177ReferencesMauro Cettolo, Jan Nieheus, Sebastian St?uker, LuisaBentivogli, and Marcello Federico.
2013.
Report onthe 10th iwslt evaluation campaign.
In Proc.
of theInternational Workshop on Spoken Language Trans-lation, Heidelberg, Germany, December.J.-C. Chappelier and M. Rajman.
1998.
A general-ized CYK algorithm for parsing stochastic CFG.
InProceedings of the First Workshop on Tabulation inParsing and Deduction, pages 133?137, April.Stanley F. Chen and Joshuo Goodman.
1998.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard University, Cam-bridge, MA, August.David Chiang and Liang Huang.
2005.
Better k-bestParsing.
In Proceedings of the 9th Internation Work-shop on Parsing Technologies, pages 53?64, Octo-ber.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pages 263?270,Ann Arbor, Michigan, June.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228,June.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis test-ing for statistical machine translation: Controllingfor optimizer instability.
In 49th Annual Meet-ing of the Association for Computational Linguis-tics:shortpapers, pages 176?181, Portland, Oregon,June.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A Simple, Fast, and Effective Reparameter-ization of IBM Model 2.
In Proceedings of NAACL-HLT, pages 644?648, Atlanta, Georgia, June.Chris Dyer.
2010.
Two monolingual parses are betterthan one (synchronous parse).
In In Proc.
of HLT-NAACL.Markus Freitag, Matthias Huck, and Hermann Ney.2014.
Jane: Open source machine translation sys-tem combination.
In Conference of the EuropeanChapter of the Association for Computational Lin-guistics, Gothenburg, Sweden, April.
To appear.Matthias Huck, Jan-Thorsten Peter, Markus Freitag,Stephan Peitz, and Hermann Ney.
2012.
Hierar-chical Phrase-Based Translation with Jane 2.
ThePrague Bulletin of Mathematical Linguistics, 98:37?50, October.Matthias Huck, David Vilar, Markus Freitag, and Her-mann Ney.
2013.
A performance study of cubepruning for large-scale hierarchical machine transla-tion.
In Proceedings of the NAACL 7th Workshop onSyntax, Semantics and Structure in Statistical Trans-lation, pages 29?38, Atlanta, Georgia, USA, June.Reinerd Kneser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processingw, volume 1,pages 181?184, May.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisti-cal Phrase-Based Translation.
In Proceedings of the2003 Meeting of the North American chapter of theAssociation for Computational Linguistics (NAACL-03), pages 127?133, Edmonton, Alberta.R.C.
Moore and W. Lewis.
2010.
Intelligent Selectionof Language Model Training Data.
In ACL (ShortPapers), pages 220?224, Uppsala, Sweden, July.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 160?167, Sapporo,Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
IBM ResearchReport RC22176 (W0109-022), IBM Research Di-vision, Thomas J. Watson Research Center, P.O.
Box218, Yorktown Heights, NY 10598, September.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proceedings of the 7th Conference of the As-sociation for Machine Translation in the Americas,pages 223?231, Cambridge, Massachusetts, USA,August.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Speech and Language Processing (ICSLP), vol-ume 2, pages 901?904, Denver, CO, September.David Vilar, Daniel Stein, Matthias Huck, and Her-mann Ney.
2010.
Jane: Open source hierarchi-cal translation, extended with reordering and lexi-con models.
In ACL 2010 Joint Fifth Workshop onStatistical Machine Translation and Metrics MATR,pages 262?270, Uppsala, Sweden, July.David Vilar, Daniel Stein, Matthias Huck, and Her-mann Ney.
2012.
Jane: an advanced freely avail-able hierarchical machine translation toolkit.
Ma-chine Translation, 26(3):197?216, September.178Joern Wuebker, Arne Mauser, and Hermann Ney.2010.
Training phrase translation models withleaving-one-out.
In Proceedings of the 48th AnnualMeeting of the Assoc.
for Computational Linguistics,pages 475?484, Uppsala, Sweden, July.179
