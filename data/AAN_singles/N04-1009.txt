A Probabilistic Rasch Analysis of Question Answering EvaluationsRense LangeIntegrated Knowledge Systemsrlange@iknowsys.orgWarren R. GreiffThe MITRE Corporationgreiff@mitre.orgJuan MoranUniversity of Illinois, Urbana-Champaignjmoran@ncsa.uiuc.eduLisa FerroThe MITRE Corporationlferro@mitre.orgAbstractThe field of Psychometrics routinely grappleswith the question of what it means to measurethe inherent ability of an organism to performa given task, and for the last forty years, thefield has increasingly relied on probabilisticmethods such as the Rasch model for test con-struction and the analysis of test results.
Be-cause the underlying issues of measuringability apply to human language technologiesas well, such probabilistic methods can be ad-vantageously applied to the evaluation ofthose technologies.
To test this claim, Raschmeasurement was applied to the results of 67systems participating in the Question Answer-ing track of the 2002 Text REtrieval Confer-ence (TREC) competition.
Satisfactory modelfit was obtained, and the paper illustrates thetheoretical and practical strengths of Raschscaling for evaluating systems as well as ques-tions.
Most important, simulations indicatethat a test invariant metric can be defined bycarrying forward 20 to 50 equating questions,thus placing the yearly results on a commonscale.1 IntroductionFor a number of years, objective evaluation of state-of-the-art computational systems on realistic languageprocessing tasks has been a driving force in the advanceof Human Language Technology (HLT).
Often, suchevaluations are based on the use of simple sum-scores(i.e., the number of correct answers) and derivativesthereof (e.g., percentages), or on ad-hoc ways to rank ororder system responses according to their correctness.Unfortunately, research in other areas indicates thatsuch approaches rarely yield a cumulative body ofknowledge, thereby complicating theory formation andpractical decision making alike.
In fact, although it isoften taken for granted that sums or percentages ade-quately reflect systems?
performance, this assumptiondoes not agree with many models currently used in edu-cational testing (cf., Hambleton and Swaminathan,1985; Stout, 2002).
To address this situation, we presentthe use of Rasch (1960/1980) measurement to the HLTresearch community, in general, and to the QuestionAnswering (QA) research community, in particular.Rasch measurement has evolved over the last fortyyears to rigorously quantify performance aspects in suchdiverse areas as educational testing, cognitive develop-ment, moral judgment, eating disorders (see e.g., Bondand Fox, 2001), as well as olfactory screening for Alz-heimer?s disease (Lange et al, 2002) and model glidercompetitions (Lange, 2003).
In each case, the majorcontribution of Rasch measurement is to decomposeperformance into two additive sources: the difficulty ofthe task and the ability of the person or system perform-ing this task.
While Rasch measurement is new to theevaluation of the performance of HLT systems, we in-tend to demonstrate that this approach applies here aswell, and that it potentially provides significant advan-tages over traditional evaluation approaches.Our principal theoretical argument in favor ofRasch modeling is that the decomposition of perform-ance into task difficulty and system ability creates thepotential for formulating detailed and testable hypothe-ses in other areas of language technology.
For QA, theexistence of a well-defined, precise, mathematical for-mulation of question difficulty and system ability canprovide the basis for the study of the dimensions inher-ent in the answering task, the formal characterization ofquestions, and the methodical analysis of the strengthsand weaknesses of competing algorithmic approaches.As Bond and Fox (2001, p. 3) explain: ?The goal is tocreate abstractions that transcend the raw data, just as inthe physical sciences, so that inferences can be madeabout constructs rather than mere descriptions about rawdata.?
Researchers are then in a position to formulateinitial theories, validate the consequences of theories onreal data, refine theories in light of empirical data, andfollow up with revised experimentation in a dialecticprocess that forms the essence of scientific discovery.Rasch modeling offers a number of direct practicaladvantages as well.
Among these are:?
Quantification of question difficulty and systemability on a single scale with a common metric.?
Support for the creation of tailor-made questionsand the compilation of questions that suit well-defined evaluation objectives.?
Equating (calibration) of distinct question corporaso that systems participating in distinct evaluationcycles can be directly compared.?
Assessment of the degree to which independentevaluations assess the same system abilities.?
Availability of rigorous statistical techniques forthe following:- analysis of fit of the data produced from systems?performance to the Rasch modeling assumptions;- identification of individual systems whose per-formance behavior does not conform to the per-formance patterns of the population as a whole;- identification of individual test questions that ap-pear to be testing facets distinct from those evalu-ated by the test as a whole;- assessment of the reliability of the test ?
that is,the degree to which we can expect estimates ofsystems?
abilities to be replicated if these systemsare given another test of equivalent questions;- identification of unmodeled sources of variationin the data through a variety of methods, includ-ing bias tests and analysis of residual terms.The remainder of the paper is organized as follows.First, we present in section 2 the basic concepts ofRasch modeling.
We continue in section 3 with an ap-plication of Rasch modeling to the data resulting fromthe QA track of the 2002 Text REtrieval Conference(TREC) competition.
We fit the model to the data, ana-lyze the resulting fit, and demonstrate some of the bene-fits that can be derived from this approach.
In section 4we present simulation results on test equating.
Finally,we conclude with a summary of our findings and pre-sent ideas for continuing research into the application ofRasch models to technology development and scientifictheory formation in the various fields of human lan-guage processing.2 The Rasch Model for Binary DataFor binary results, Rasch?s (1960/1980) measurementrequires that the outcome of an encounter between com-puter systems (1, ?, s, ?, ns) and questions (1, ?, q, ?,nq) should depend solely on the differences betweenthese systems?
abilities (Ss) and the questions?
difficul-ties (Qq).
Together with mild and standard scaling as-sumptions, the preceding implies that:11 ?
?+= )e(P sq SQsq              (1)In a QA context, Psq is the probability that a system withthe ability Ss will answer a question with difficulty Qqcorrectly.
For a rigorous derivation of Equation 1 and anoverview of the assumptions involved, we refer thereader to work by Fisher (1995).
Fisher also proves thatsum-scores (and hence percentages of correct answers)are sufficient performance statistics if and only if theassumptions of the Rasch model are satisfied.10 8 6 4 2 0 2 4 6 800.250.50.751Latent Dimension (Logits)P(correct)0.55?
2Figure 1.
Three Sample Rasch ResponseCurvesThe binary Rasch model has several interestingproperties.
First, as is illustrated by the three solid linesin Figure 1, Equation 1 defines a set of non-intersectinglogistic response-curves such that Psq = 0.5 whenever Ss= Qq.
In the following, such points are also referred toas question?s locations.
For instance, the locations of thethree questions depicted in Figure 1 are -5, 0, and 2.Second, for each pair of systems i and j with Si > Sj, forany question q, system i has a better chance of respond-ing correctly than system j, i.e., Piq > Pjq.
Thus, thequestions that are answered correctly by less capablesystems always form a probabilistic subset of those an-swered correctly by more capable systems.
Third, restat-ing Equation 1 as is shown below highlights that thequestion and system parameters are additive and ex-pressed in a common metric:qsPPQS)ln(sqsq ?=?1  (2)Given the left-hand side of Equation 2, this metric?sunits are called Logits.
Note that this equation furtherimplies that Ss and Qq are determined up to an additiveconstant only (i.e., their common origin is arbitrary).Finally, efficient maximum-likelihood proceduresexist to estimate Ss and Qq independently, together withtheir respective standard errors SEs and SEq (see e.g.,Wright and Stone, 1979).
These procedures do not re-quire any assumptions about the magnitudes or the dis-tribution of the Ss in order to estimate the Qq, and vice-versa.
Accordingly, systems?
abilities can be deter-mined in a ?question free?
fashion, as different sets ofquestions from the same pool will yield statisticallyequivalent Ss estimates.
Analogously, questions?
loca-tions can be estimated in a ?system free?
fashion, assimilarly spaced Qq estimates should be obtained acrossdifferent samples of systems.
In this paper, the modelparameters, together with their errors of estimate, willbe computed via the Winsteps Rasch scaling software(Linacre, 2003).0.0 0.5 1.0 1.5 2.0Question Outfit (Mean-Square)-4-202QuestionlocationsQq(Logits) -QuestionDifficulty2.
Questions w ith Outfit > 2.0 are show n as X1.
The size of the dots is proportional to SEqNote:3.
Symbols have been slightly jittered along theEasyHardX-axis to reveal overlapping pointsFigure 2.
Questions by Qq, Outfitq, and SEq3 Analysis of TREC Evaluation DataWe used the results from the Question Answering trackof the 2002 TREC competition to test the feasibility ofapplying Rasch modeling to QA evaluation.
Sixty-sevensystems participated, and answered 500 questions byreturning a single precise response extracted from a 3-gigabyte corpus of texts.
While the NIST judges as-sessed each answer as correct, incorrect, non-exact, orunsupported, we created binary responses by treatingeach of these last three assessments as incorrect.
Tenquestions were excluded from all analyses, as thesewere not answered correctly by any system.1 The final1 When all respondents answer some question q correctly (ordata set thus consisted of 67 systems?
responses to 490questions.0.0 0.5 1.0 1.5 2.0System Outfit (Mean-Square)-4-202SystemlocationsSs(Logits) -SystemCapability Outfit = 2.68LowAbilityHighAbilityNote:1.
The size of the dots is proportional to SEs2.
The circled dot is displaced to fit the graphFigure 3.
Systems by Ss, Outfits, and SEs3.1 Question Difficulty and System AbilityMaximum-likelihood estimates of the questions?
diffi-culty and the systems?
abilities were computed via Win-steps.
Figure 2 displays the results associated with thequestions, whereas Figure 3 addresses the systems.
Eachdot in these displays corresponds to one question or onesystem.
For questions, the Y-value gives the estimate ofthe questions?
difficulty (i.e., Qq); for systems, the Y-value reflects the estimate of systems?
ability (Ss).
Forquestions, lower values correspond to easier questions,while higher values to difficult questions.
For systems,higher values correspond to greater ability.
As is cus-tomary, the mean difficulty of the questions is set atzero, thereby implicitly fixing the origin of the esti-mated system abilities at -1.98.
As was noted earlier, aincorrectly), the parameter Qq cannot be estimated.
Note thatraw-score approaches implicitly ignore such questions as wellsince including them does not affect the order of systems?
?number right.?
Of course, by changing the denominator,percentages of right or wrong questions are affected.constant value can be added to each Qq and Ss withoutthereby affecting the validity of the results.
The X-axesof Figures 2 and 3 refer to the quality of fit, as describedin section 3.3 below.As an example, consider a question with difficultylevel -2.
This means that a system whose ability level is-2 has a probability of .5 (odds=1) of getting this ques-tion correct.
The odds of a system with ability of -1 get-ting this same question correct would increase by afactor2 of 2.72, thus increasing the probability of a cor-rect answer to Psq = 2.72/(1+2.72) = .73.
For a systemat ability level 0, the odds increase by another factor of2.72 to 7.39, giving a probability of .88.
On the otherhand, a system with an ability of -3, would have theeven odds decrease by a factor of 2.72 to .369, yieldingPsq = .27.
In other words, increasing (decreasing) ques-tions?
difficulties or decreasing (increasing) systems?abilities by the same amounts affects the log-odds in thesame fashion.
The preceding thus illustrates that ques-tion difficulty and system ability have additive proper-ties on the log-odds, or, Logit, scale.3The smoothed densities in Figure 4 summarize thelocations of the 490 questions (dotted distribution) andthe 67 systems (solid).
It can be seen that question dif-ficulties range approximately from -3 to +3, and thatmost questions fall in a region about -1.
Systems?
abili-ties mostly cover a lower range such that the questions?locations (MeanQ = 0 Logits) far exceed those of thesystems (MeanS = -1.98 Logits).
In other words, mostquestions are very hard for these systems.
The vast ma-jority of systems (those located near -1 or below) haveonly a small chance (below 15%) of answering a sig-nificant portion of the questions (those located above 1),and an even smaller chance (below 5%) on a non-negligible number of questions (those above 2).
Ofthose systems, a large portion (those at -2 or below) willhave even less of a chance on these questions.-5 -3 -1 1 3 5Rasch Dimension (Logits)0.00.10.20.30.40.5Density(SandQ)0.00.20.40.60.81.0SEQandSESSESSEQQuestion DensitySystem DensityFigure 4.
Smoothed System and Question LocationDensities2 The value of e, since we are working with natural logarithms.3 Note that a number of measures used in the physical scienceslikewise achieve additivity by adopting a log scale.3.2 Standard Error of EstimateThe two U-shaped curves in Figure 4 reflect the esti-mates of error, SEq for questions (dotted curve) and SEsfor systems (solid curve), as a function of their esti-mated locations (X-axis).
As is also reflected by the sizeof the dots in Figure 3, it can be seen that SEs is smallerfor intermediate and high performing systems (i.e., Ssbetween -3 and 1 Logits) than for low performing sys-tems (Ss < -3 Logits).
This pattern suits the ?horse-race?nature of the TREC evaluation well since the top per-forming systems are assessed with nearly optimal preci-sion.
While the most capable system shows somewhatgreater SEs, calculation shows its performance is stillsignificantly higher than that of the runner-up (z =10.64, p < .001).Figure 4 further shows that SEq increases dramati-cally beyond -1 Logits (this is also reflected in the sizeof the dots in Figure 2).
For instance, the standard errorof estimate SEq exceeds 0.5 Logits for questions with Qq> 1 Logit.
Thus, the locations of the hardest questionsare known with very poor precision only.3.3 Question and System FitAccording to the Rasch model, a system, A, with mid-dling performance is expected to perform well on theeasier questions and poorly on the harder questions.However, it is possible that some system, B, achievedthe same score on the test by doing poorly on the easyquestions and well on the harder questions.
While thebehavior of system A agrees with the model, system Bdoes not.
Accordingly, the fit of system B is said to bepoor as this system contradicts the knowledge embodiedin the data as a whole.
Analogous comments can bemade with respect to questions.
Rasch modeling formal-izes the preceding account in a statistical fashion.
Inparticular, for each response to Question q by System s,Equation 1 allows the computation of a standardizedresidual zsq, which is the difference between an observeddatum (i.e., 0 or 1) and the probability estimate Psq afterdivision by its standard deviation.
Since the zsq followan approximately normal distribution, unexpected re-sults (e.g., |zsq|>3) are easily identified.
The overall fitfor systems (across questions) and for questions (acrosssystems) is quantified by their Outfit.4 For instance, forSystem s:)n/(zOutfit qqsqs 12 ?
?=  (3)Since the summed z2sq in Equation 3 define a ?2 statisticwith expected value nq ?
1, the Outfit statistic ranges4 Additionally, systems?
(or questions?)
?Infit?
statistic is de-fined by weighting the z2sq contributions inversely to theirdistance to the contributing questions (or systems).
As such,Infit statistics are less sensitive to outlying observations.
Sincethis paper focuses on overall model fit, Infit statistics are notreported.from 0 to ?, with an expected value of 1.
As a rule ofthumb, for rather small samples such as the present,Outfit values in the range 0.6 to 1.6 are considered asbeing within the expected range of variation.Figure 2 shows 46 questions whose Outfit exceeds1.6 (those to the right of the rightmost dashed verticalline) and the Outfit values of 24 of these exceed 2.0(shown in the graph by Xs, plotted at the right with hori-zontal jitter).
These are questions on which low per-forming systems performed surprisingly well, and/orhigh performing systems performed unexpectedlypoorly.
Thus, there is a clear indication that these ques-tions have characteristics that differentiate them fromtypical questions.
Such questions are worthy of individ-ual attention by the system developers.Questions and systems with uncharacteristicallysmall Outfit values are of interest as well.
For instance,in the present context it seems plausible that some ques-tions simply cannot be answered by systems lackingcertain capabilities (e.g., pronominal anaphora resolu-tion, acronym expansion, temporal phrase recognition),while such questions are easily answered by systemsthat possess such capabilities.
We might find that thesequestions would be answered by the vast majority, if notall, of the high performing systems and very few if anyof the low ability systems.
The estimated fit would bemuch better (small Outfit) than expected by chance.Again, the identification and analysis of such overfittingquestions and, similiarly, underfitting systems maygreatly enhance our understanding of both.3.4 Example of System with large OutfitNote that Figure 3 above shows that the best performingsystem also exhibits the largest Outfit (2.68), and weinvestigated this system?s residuals in detail.
Table 1indicates that this system failed (Datum = 0) on eightquestions (q) where its modeled probability of successwas very high (Psq > 0.98).
Thus, the misfit results fromthis system?s failure to answer correctly questions thatproved quite easy for most other systems.q Qq Datum Psq Residual z1411 -1.51 0 0.98 -0.98 -7.391418 -1.96 0 0.99 -0.99 -9.261465 -1.74 0 0.99 -0.99 -8.281672 -1.59 0 0.98 -0.98 -7.671671 -1.51 0 0.98 -0.98 -7.391686 -2.11 0 0.99 -0.99 -9.971697 -1.89 0 0.99 -0.99 -8.921841 -1.66 0 0.98 -0.98 -7.97Table 1.
Misfit Diagnosis of Best Performing Sys-temThese are the eight questions listed in Table 1:1411 What Spanish explorer discovered the MississippiRiver?1418 When was the Rosenberg trial?1465 What company makes Bentley cars?1642 What do you call a baby sloth?1671 Where is Big Ben?1686 Who defeated the Spanish armada?1697 Where is the Statue of Liberty?1841 What?s the final line in the Edgar Allen Poe poem?The Raven?
?This situation should be highly relevant to the sys-tem?s developers.
Informally speaking, the best systemstudied here ?should have gotten these questions right,?and it might thus prove fruitful to determine exactlywhy the system failed.
Even if no obvious mistakes canbe identified, doing so could reveal strategies for systemimprovement by focusing on seemingly ?easy?
issuesfirst.
Alternatively, it might turn out that precisely thoseaspects of the system that enable it do so well overallalso cause it to falter on the easier questions.
Ascertain-ing this might or might not be of help to the system?sdesigners, but it would certainly foster the developmentof a scientific theory of automatic question answering.3.5 Impact of MisfitThe existence of misfitting entities raises the possibilitythat the estimated Rasch system abilities are distorted bythe question and system misfit.
We therefore recom-puted systems?
locations by iteratively removing theworst fitting questions until 372 questions with Outfitq<1.6 remained.
In support of the robustness of the Raschmodel, Figure 5 shows that the correlation between thetwo sets of estimates is nearly perfect (r = 0.99), indi-cating that the original and the ?purified?
questions pro-duce essentially equivalent system evaluations.
Thus,the observed misfit had negligible effect on the systemparameter estimates.-4 -2 0 2Using best 372 fitting items only (Logits)-4-202Usingallitems(Logits)r = 0.99Figure 5.
Effect of Removing Misfitting Questionson the Estimated System Abilities Ss4 Test Equating SimulationA major motivation for introducing Rasch models ineducational assessment was that this allows for the cali-bration, or equating, of different tests based on a limitedset of common (i.e., repeated) questions.
The purpose ofequating is to achieve equivalent test scores across dif-ferent test sets.
Thus, equating opens the door to cali-brating the difficulty of questions and the performanceof systems across the test sets used in different years.Since appropriate data from different years arelacking, a simulation study was performed based ondifferent subsets of the 490 available questions.
Weshow how system abilities can be expressed in the samemetric, even though systems are evaluated with a com-pletely different set of questions.
To rule out the possi-bility that such a correspondence might come about bychance (e.g., equally difficult sets of questions mightaccidentally be produced), a worst-case scenario is used.The simulation also provides a powerful means to dem-onstrate the superiority of the Rasch Logit metric com-pared to raw scores as indices of system performance.To this end, we divide the available questions fromTREC 2002 into two sets of equal size.
The Easy setcontains the easiest questions (lowest Qq) as identifiedin earlier sections.
For the simulation, this will be thetest set for one year?s evaluation.
A second, Hard set,serves as the test set for a subsequent evaluation, and itcontains the remaining 50% of the questions (highestQq).
By design, the difference in questions?
difficultiesis far more extreme than is likely to be encountered inpractice.
The Rasch model is then fitted to the responsesto the Easy set of questions.
Next, based on questions?difficulties and their fit to the Rasch model, a subset ofthe Easy questions is selected for inclusion in the sec-ond test in conjunction with the Hard question set.These questions are said to comprise the Equating set,as they serve to fix the overall locations of the questionsin the Hard set.Normally, this second test would be administered toa new set of systems (some completely new, others im-proved versions of systems evaluated previously).
How-ever, for the purposes of this simulation, we?administer?
the second test to the same systems.
TheRasch model is then applied to the Hard and Equatingquestions combined, while fixing the locations of theEquating questions to those derived while scaling theEasy set.
The Winsteps software achieves this by shift-ing the locations in the Hard set to be consistent withthe Equating set ?
but without adjusting the spacing ofthe questions in the Hard or Easy sets.
If the assump-tions of the Rasch model hold, then the Easy and Hardquestion sets will now behave as if their levels had beenestimated simultaneously.
Since the same systems areused in the simulation, and the questions have beencalibrated to be on the same scale, the estimated systemabilities Ss as derived from the Easy and Hard questionsets should be statistically identical.
That is, these twoestimates should show a high linear correlation and theyshould have very similar means and standard deviations(see e.g., Wright and Stone, 1979, p. 83-126).Common wisdom in the Rasch scaling communityholds that relatively few questions are needed to achievesatisfactory equating results.
For this reason, the simula-tion study was performed three times, using Equatingsets with 20, 30, and 50 questions, respectively (i.e.,about 4, 6, and 10% of the total number of questions inthe present study).4.1 FindingsThe simulation results are summarized in Table 2,whose rows reflect the sizes of the respective Equatingsets (i.e., 20, 30, and 50).
Each first sub-row reports theRasch scaling results, while the second sub-row reportsthe raw-score (i.e., number correct) findings.
The col-umns report a number of basic statistics, including themean (M) and standard deviations (SD) of the Logit andraw-score values in the Easy and Hard sets, and thecorrelation (rlinear) between systems?
estimated abilitiesbased on the Easy and Hard sets.Size ofEquat-ing Set Index Measy SDeasy Mhard SDhard rlinear20 Rasch  -0.66 1.10 -0.65 1.23 0.90# Correct 92.40 47.53 27.39 30.70 0.7730 Rasch  -0.68 1.10 -0.66 1.21 0.92# Correct 92.88 47.92 29.82 31.80 0.8050 Rasch  -0.78 1.11 -0.77 1.18 0.94# Correct 94.76 49.62 31.01 32.29 0.82Table 2.
Results of the Simulation StudyThe major findings are as follows.
First, inspectionof the rlinear columns indicates that Rasch equating con-sistently produced higher correlations between systems?estimated abilities as estimated via the Easy and Hardquestion sets than did the raw scores for each set.
Sec-ond, for obvious reasons the raw-score estimates basedon the Easy sets are considerably higher than thosebased on the Hard sets.
However, Table 2 also showsthat the standard deviations of the number correct esti-mates obtained for the Easy sets exceed those of theHard sets as well (sometimes by over 100%).
In otherwords, when raw scores (or percentages) are used, the?spacing?
of the systems is affected by the difficulty ofthe questions.Third, the Rasch approach by contrast producesvery similar means and standard deviations for the ca-pability estimates based on the Easy and Hard questionsets.
This holds regardless of the size of the Equatingsets.
For instance, when 50 equating questions are used,the estimated abilities based on the Easy and Hard setshave nearly identical SD (i.e., 1.11 and 1.18 Logits, re-spectively).
The corresponding averages for this caseare -0.78 and -0.77 Logits, i.e., a standardized difference(effect size) of less than 0.01 SD.
Similarly small effectssizes are obtained for the other cases.
Further, given thesuperior values of rlinear, it thus appears that Raschequating provides an acceptable equating mechanismeven when maximally different question sets are used.In fact, already for Equating sets of size 20 a correlationof  0.90 is produced.Fourth, as a check on the results, scatter plots of thevarious cases summarized in Table 2 were produced.The left panel of Figure 6 shows the Rasch capabilityestimates obtained for the Hard question set plottedagainst those for the Easy set, and it can be seen thatthese estimates are highly correlated (rlinear = 0.94).
Thecorresponding raw scores are plotted in the right panelof Figure 6.
In addition to showing a lower correlation(rlinear = 0.82), raw scores also clearly posses a non-linear component, and in fact the quadratic trend ishighly significant (tquad = 13.10, p < .001).
Thus, inaddition to being question-dependent, raw score andpercentage based comparisons suffer from pronouncednon-linearity.Despite the favorable results, we remind the readerthat the above simulations represented a worse-casescenario.
Indeed, more realistic simulations not reportedhere indicate that Rasch equating can further be im-proved by omitting misfitting questions and by usingless extreme question sets.5 ConclusionsIn this paper we have described the Rasch model forbinary data and applied it to the 2002 TREC QA results.We addressed the estimation of question difficulty andsystem ability, the estimation of standard errors forthese parameters, and how to assess the fit of individualquestions and systems.
Finally, we presented a simula-tion which demonstrated the advantage of using Raschmodeling for calibration of question sets.Based on our findings, we recommend that testequating be introduced in formal evaluations of HLT.
Inparticular, for the QA track of the TREC competition,we propose that NIST include a set of questions to bereused in the following year for calibration purposes.For instance, after evaluating the systems?
performancein the 2004 competition, NIST would select a set ofquestions consistent with the criteria outlined above.Using twenty to fifty questions from a set of 500 willprobably be sufficient, especially when misfitting ques-tions are eliminated.
When the results are released to theparticipants, they would be asked not to look at theseequating questions, and not to use them to train theirsystems in the future.
These equating questions wouldthen be included in the 2005 question set so as to placethe 2004 and 2005 results on the same Logit scale.
Theprocess would continue in each consecutive year.The approach outlined above serves several pur-poses.
For instance, the availability of equated testswould increase the confidence that the testing indeedmeasures progress, and not simply the unavoidable-4 -2 0 2 4Estimated capablity based on "Easy" questions (Logits)Estimatedcapabilitybasedon"Hard"questions(Logits)rlinear = 0.94Y= X-4-20240 50 100 150Raw  score on "Easy" questions050100150Rawscoreon"Hard"questionsrlinear = 0.82Y= XY = 24.32 - 0.55X + 0.0051 X2Y = 0.016 + 0.998 XFigure 6.
Systems?
Performance on Easy vs. Hard Questions Based on Rasch Scaling (left) and Raw Scores (right)variations in difficulty across each year?s question set.Additionally, it would support the goal of making eachcompetition increasingly more challenging by correctlyidentifying easy and  difficult questions.
Further, cali-brated questions could be combined into increasinglylarge corpora, and these corpora could then be used toprovide researchers with immediate performance feed-back in the same metric as the NIST evaluation scale.The availability of large corpora of equated questionsmight also provide the basis for the development ofmethods to predict question difficulty, thus stimulatingimportant theoretical research in QA.The work presented here only begins to scratch thesurface of adopting a probabilistic approach such as theRasch model for the evaluation of human languagetechnologies.
First, as was discussed above, questionsdisplaying unexpectedly large or small Outfit values canbe identified for further study.
The questions themselvescan be analyzed in terms of both content and linguisticexpression.
With the objective of beginning to form atheory of question difficulty, questions can be analyzedin concert with the occurrence of correct answers in thedocument corpus and the incorrect answers returned bysystems.
Also, experimentation with more complexscaling models could be conducted to uncover informa-tion other than questions?
difficulty levels.
For example,so-called 2-parameter IRT models (see e.g., Hambletonand Swaminathan, 1985) would allow for the estimationof a discrimination parameter together with the diffi-culty parameter for each question.
More direct informa-tion concerning the diagnosis of systems?
skill defectsare described in Stout (2002).It is also possible to incorporate into the modelother factors and variables affecting a system?s per-formance.
Rasch modeling can be extended to manyother HLT evaluation contexts since Rasch measure-ment procedures exist to deal with multi-level re-sponses, counts, proportions, and rater effects.
Ofparticular interest is application to technology areas thatuse metrics other than percent of items processed cor-rectly.
Measures such as average precision, R-precisionand precision at fixed document cutoff, which are usedin Information Retrieval (Voorhees and Harman, 1999),metrics such as BiLingual Evaluation Understudy(BLUE) (Papineni et al, 2002) used in Machine Trans-lation, and F-measure (Van Rijsbergen, 1979) com-monly used for evaluation of a variety of NLP tasks arejust a few of the variety of metrics used for evaluationof language technologies that can benefit from Raschscaling and related techniques.ReferencesBond, T.G.
and Fox, C.M.
(2001).
Applying the RaschModel: Fundamental Measurement in the HumanSciences.
New Jersey: Lawrence Erlbaum Associates.Fischer, G.H.
(1995).
Derivations of the Rasch model.In G.H.
Fischer & I.W.
Molenaar (Eds.
), Rasch mod-els: Foundations, recent developments, and applica-tions.
(pp.
15-38) New York: Springer.Hambleton, R.K. and Swaminathan, H. (1985).
Itemresponse theory: Principles and applications.
Boston:Kluwer ?
Nijhoff.Lange, R. (2003).
Model Sailplane Competition: FromAwarding Points to Measuring Performance Skills.RC Soaring Digest, August Issue.
(This paper is alsoavailable as: http://www.iknowsys.org/Download/ -Model_Sailplanes.pdf).Lange, R., Donathan, C.L., and Hughes, L.F. (2002).Assessing olfactory abilities with the University ofPensylvania smell identification test: A Rasch scalingapproach.
Journal of Alzheimer?s Disease, 4, 77-91.Linacre, J. M. (2003).
WINSTEPS Rasch measurementcomputer program.
Chicago, IL: Winsteps.com.Papineni, K., Roukos, S., Ward, T, Henderson, J. andReeder F. (2002).
Corpus-based Comprehensive andDiagnostic MT Evaluation: Initial Arabic, Chinese,French, and Spanish Results.
Proceedings of the 2002Conference on Human Language Technology (pp.124-127).
San Diego, CA, 2002.Stout W.F.
(2002).
Psychometrics from practice to the-ory and back.
Psychometrika, 67, 485-518.http://www.psychometricsociety.org/journal/online/ARTICLEstout2002.pdfRasch, G. (1960/1980).
Probabilistic models for someintelligence and attainment tests.
Chicago, IL: MESAPress.Van Rijsbergen, C. J.
(1979).
Information Retrieval.Dept.
of Computer Science, University of Glasgow.Voorhees, E. M. and Harman, D. K.
(eds.).
1999.
Pro-ceedings of the Eighth Text REtrieval Conference(TREC-8), NIST Special Publication 500-246.Wright, B.D.
and Stone, M.H.
(1979).
Best test design.Chicago, IL: MESA Press.
