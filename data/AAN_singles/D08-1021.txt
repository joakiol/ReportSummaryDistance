Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 196?205,Honolulu, October 2008. c?2008 Association for Computational LinguisticsSyntactic Constraints on Paraphrases Extracted from Parallel CorporaChris Callison-BurchCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, Marylandccb cs jhu eduAbstractWe improve the quality of paraphrases ex-tracted from parallel corpora by requiring thatphrases and their paraphrases be the same syn-tactic type.
This is achieved by parsing the En-glish side of a parallel corpus and altering thephrase extraction algorithm to extract phraselabels alongside bilingual phrase pairs.
In or-der to retain broad coverage of non-constituentphrases, complex syntactic labels are intro-duced.
A manual evaluation indicates a 19%absolute improvement in paraphrase qualityover the baseline method.1 IntroductionParaphrases are alternative ways of expressing thesame information.
Being able to identify or gen-erate paraphrases automatically is useful in a widerange of natural language applications.
Recent workhas shown how paraphrases can improve questionanswering through query expansion (Riezler et al,2007), automatic evaluation of translation and sum-marization by modeling alternative lexicalization(Kauchak and Barzilay, 2006; Zhou et al, 2006;Owczarzak et al, 2006), and machine translationboth by dealing with out of vocabulary words andphrases (Callison-Burch et al, 2006) and by expand-ing the set of reference translations for minimum er-ror rate training (Madnani et al, 2007).
While all ap-plications require the preservation of meaning whena phrase is replaced by its paraphrase, some addi-tionally require the resulting sentence to be gram-matical.In this paper we examine the effectiveness ofplacing syntactic constraints on a commonly usedparaphrasing technique that extracts paraphrasesfrom parallel corpora (Bannard and Callison-Burch,2005).
The paraphrasing technique employs variousaspects of phrase-based statistical machine transla-tion including phrase extraction heuristics to obtainbilingual phrase pairs from word alignments.
En-glish phrases are considered to be potential para-phrases of each other if they share a common for-eign language phrase among their translations.
Mul-tiple paraphrases are frequently extracted for eachphrase and can be ranked using a paraphrase proba-bility based on phrase translation probabilities.We find that the quality of the paraphrases thatare generated in this fashion improves significantlywhen they are required to be the same syntactic typeas the phrase that they are paraphrasing.
This con-straint:?
Eliminates a trivial but pervasive error thatarises from the interaction of unaligned wordswith phrase extraction heuristics.?
Refines the results for phrases that can take ondifferent syntactic labels.?
Applies both to phrases which are linguisticallycoherent and to arbitrary sequences of words.?
Results in much more grammatical outputwhen phrases are replaced with their para-phrases.A thorough manual evaluation of the refined para-phrasing technique finds a 19% absolute improve-196ment in the number of paraphrases that are judgedto be correct.This paper is structured as follows: Section 2describes related work in syntactic constraints onphrase-based SMT and work utilizing syntax inparaphrase discovery.
Section 3 details the prob-lems with extracting paraphrases from parallel cor-pora and our improvements to the technique.
Sec-tion 4 describes our experimental design and evalu-ation methodology.
Section 5 gives the results of ourexperiments, and Section 6 discusses their implica-tions.2 Related workA number of research efforts have focused on em-ploying syntactic constraints in statistical machinetranslation.
Wu (1997) introduced the inversiontransduction grammar formalism which treats trans-lation as a process of parallel parsing of the sourceand target language via a synchronized grammar.The synchronized grammar places constraints onwhich words can be aligned across bilingual sen-tence pairs.
To achieve computational efficiency, theoriginal proposal used only a single non-terminal la-bel rather than a linguistic grammar.Subsequent work used more articulated parsesto improve alignment quality by applying cohesionconstraints (Fox, 2002; Lin and Cherry, 2002).
Iftwo English phrases are in disjoint subtrees in theparse, then the phrasal cohesion constraint preventsthem from being aligned to overlapping sequencesin the foreign sentence.
Other recent work has incor-porated constituent and dependency subtrees into thetranslation rules used by phrase-based systems (Gal-ley et al, 2004; Quirk et al, 2005).
Phrase-basedrules have also been replaced with synchronous con-text free grammars (Chiang, 2005) and with treefragments (Huang and Knight, 2006).A number of techniques for generating para-phrases have employed syntactic information, eitherin the process of extracting paraphrases from mono-lingual texts or in the extracted patterns themselves.Lin and Pantel (2001) derived paraphrases basedon the distributional similarity of paths in depen-dency trees.
Barzilay and McKeown (2001) incor-porated part-of-speech information and other mor-phosyntactic clues into their co-training algorithm.They extracted paraphrase patterns that incorporatethis information.
Ibrahim et al (2003) generatedstructural paraphrases capable of capturing long-distance dependencies.
Pang et al (2003) employeda syntax-based algorithm to align equivalent Englishsentences by merging corresponding nodes in parsetrees and compressing them down into a word lat-tice.Perhaps the most closely related work is a recentextension to Bannard and Callison-Burch?s para-phrasing method.
Zhao et al (2008b) extended themethod so that it is capable of generating richerparaphrase patterns that include part-of-speech slots,rather than simple lexical and phrasal paraphrases.For example, they extracted patterns such as con-sider NN ?
take NN into consideration.
To ac-complish this, Zhao el al.
used dependency parseson the English side of the parallel corpus.
Theirwork differs from the work presented in this paperbecause their syntactic constraints applied to slotswithin paraphrase patters, and our constraints applyto the paraphrases themselves.3 Paraphrasing with parallel corporaBannard and Callison-Burch (2005) extract para-phrases from bilingual parallel corpora.
They givea probabilistic formation of paraphrasing which nat-urally falls out of the fact that they use techniquesfrom phrase-based statistical machine translation:e?2 = argmaxe2:e2 6=e1p(e2|e1) (1)wherep(e2|e1) =?fp(f |e1)p(e2|f, e1) (2)?
?fp(f |e1)p(e2|f) (3)Phrase translation probabilities p(f |e1) and p(e2|f)are commonly calculated using maximum likelihoodestimation (Koehn et al, 2003):p(f |e) =count(e, f)?f count(e, f)(4)where the counts are collected by enumerating allbilingual phrase pairs that are consistent with the197conseguido.opportunitiesequalcreatetofailedhasprojecteuropeantheoportunidadesdeigualdadlahanoeuropeoproyectoelFigure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanishphrase la igualdad aligns with equal, create equal, and to create equal.word alignments for sentence pairs in a bilingualparallel corpus.
Various phrase extraction heuristicsare possible.
Och and Ney (2004) defined consistentbilingual phrase pairs as follows:BP (fJ1 , eI1, A) = {(fj+mj , ei+ni ) :?
(i?, j?)
?
A : j ?
j?
?
j +m?
i ?
i?
?
i+ n??
(i?, j?)
?
A : j ?
j?
?
j +m?
?
i ?
i?
?
i+ n}where fJ1 is a foreign sentence, eI1 is an English sen-tence and A is a set of word alignment points.The heuristic allows unaligned words to be in-cluded at the boundaries of the source or target lan-guage phrases.
For example, when enumerating theconsistent phrase pairs for the sentence pair given inFigure 1, la igualdad would align not only to equal,but also to create equal, and to create equal.
In SMTthese alternative translations are ranked by the trans-lation probabilities and other feature functions dur-ing decoding.The interaction between the phrase extractionheuristic and unaligned words results in an unde-sirable effect for paraphrasing.
By Bannard andCallison-Burch?s definition, equal, create equal, andto create equal would be considered paraphrases be-cause they are aligned to the same foreign phrase.Tables 1 and 2 show how sub- and super-phrases cancreep into the paraphrases: equal can be paraphrasedas equal rights and create equal can be paraphrasedas equal.
Obviously when e2 is substituted for e1 theresulting sentence will generally be ungrammatical.The first case could result in equal equal rights, andthe second would drop the verb.This problem is pervasive.
To test its extent we at-tempted to generate paraphrases for 900,000 phrasesusing Bannard and Callison-Burch?s method trainedon the Europarl corpora (as described in Section 4).It generated a total of 3.7 million paraphrases forequalequal .35 equally .02same .07 the .02equality .03 fair .01equals .02 equal rights .01Table 1: The baseline method?s paraphrases of equal andtheir probabilities (excluding items with p < .01).create equalcreate equal .42 same .03equal .06 created .02to create a .05 conditions .02create .04 playing .02to create equality .03 creating .01Table 2: The baseline?s paraphrases of create equal.
Mostare clearly bad, and the most probable e2 6= e1 is a sub-string of e1.400,000 phrases in the list.1 We observed that 34%of the paraphrases (excluding the phrase itself) weresuper- or sub-strings of the original phrase.
Themost probable paraphrase was a super- or sub-stringof the phrase 73% of the time.There are a number of strategies that might beadopted to alleviate this problem:?
Bannard and Callison-Burch (2005) rank theirparaphrases with a language model when theparaphrases are substituted into a sentence.?
Bannard and Callison-Burch (2005) sum overmultiple parallel corpora C to reduce the prob-lems associated with systematic errors in the1The remaining 500,000 phrases could not be paraphrasedeither because e2 6= e1 or because they were not consistentlyaligned to any foreign phrases.198word alignments in one language pair:e?2 = argmaxe2?c?C?fp(f |e1)p(e2|f) (5)?
We could change the phrase extraction heuris-tic?s treatment of unaligned words, or we couldattempt to ensure that we have fewer unaligneditems in our word alignments.?
The paraphrase criterion could be changedfrom being e2 6= e1 to specifying that e2 is notsub- or super-string of e1.In this paper we adopt a different strategy.
Theessence of our strategy is to constrain paraphrasesto be the same syntactic type as the phrases that theyare paraphrasing.
Syntactic constraints can apply intwo places: during phrase extraction and when sub-stituting paraphrases into sentences.
These are de-scribed in sections 3.1 and 3.2.3.1 Syntactic constraints on phrase extractionWhen we apply syntactic constraints to the phraseextraction heuristic, we change how bilingual phrasepairs are enumerated and how the component proba-bilities of the paraphrase probability are calculated.We use the syntactic type s of e1 in a refined ver-sion of the paraphrase probability:e?2 = argmaxe2:e2 6=e1?s(e2)=s(e1)p(e2|e1, s(e1)) (6)where p(e2|e1, s(e1)) can be approximated as:?c?C?f p(f |e1, s(e1))p(e2|f, s(e1))|C|(7)We define a new phrase extraction algorithm that op-erates on an English parse tree P along with foreignsentence fJ1 , English sentence eI1, and word align-ment A.
We dub this SBP for syntactic bilingualphrases:SBP (fJ1 , eI1, A, P ) = {(fj+mj , ei+ni , s(ei+ni )) :?
(i?, j?)
?
A : j ?
j?
?
j +m?
i ?
i?
?
i+ n??
(i?, j?)
?
A : j ?
j?
?
j +m?
?
i ?
i?
?
i+ n??
subtree ?
P with label s spanning words (i, i+ n)}equalJJ equal .60 similar .02same .14 equivalent .01fair .02ADJP equal .79 the same .01necessary .02 equal in law .01similar .02 equivalent .01identical .02Table 3: Syntactically constrained paraphrases for equalwhen it is labeled as an adjective or adjectival phrase.The SBP phrase extraction algorithm produces tu-ples containing a foreign phrase, an English phraseand a syntactic label (f, e, s).
After enumeratingthese for all phrase pairs in a parallel corpus, we cancalculate p(f |e1, s(e1)) and p(e2|f, s(e1)) as:p(f |e1, s(e1)) =count(f, e1, s(e1))?f count(f, e1, s(e1))p(e2|f, s(e1)) =count(f, e2, s(e1))?e2 count(f, e2, s(e1))By redefining the probabilities in this way we parti-tion the space of possible paraphrases by their syn-tactic categories.In order to enumerate all phrase pairs with theirsyntactic labels we need to parse the English side ofthe parallel corpus (but not the foreign side).
Thislimits the potential applicability of our refined para-phrasing method to languages which have parsers.Table 3 gives an example of the refined para-phrases for equal when it occurs as an adjective oradjectival phrase.
Note that most of the paraphrasesthat were possible under the baseline model (Table1) are now excluded.
We no longer get the nounequality, the verb equals, the adverb equally, the de-termier the or the NP equal rights.
The paraphrasesseem to be higher quality, especially if one considerstheir fidelity when they replace the original phrase inthe context of some sentence.We tested the rate of paraphrases that were sub-and super-strings when we constrain paraphrasesbased on non-terminal nodes in parse trees.
Thepercent of the best paraphrases being substringsdropped from 73% to 24%, and the overall percentof paraphrases subsuming or being subsumed by theoriginal phrase dropped from 34% to 12%.
How-ever, the number of phrases for which we were able199SBARQWHADVPWRBHowSQVBPdoNPPRPweVPVBcreateNPJJequalNNSrights.
?Figure 2: In addition to extracting phrases that are domi-nated by a node in the parse tree, we also generate labelsfor non-syntactic constituents.
Three labels are possiblefor create equal.to generated paraphrases dropped from 400,000 to90,000, since we limited ourselves to phrases thatwere valid syntactic constituents.
The number ofunique paraphrases dropped from several million to800,000.The fact that we are able to produce paraphrasesfor a much smaller set of phrases is a downside tousing syntactic constraints as we have initially pro-posed.
It means that we would not be able to gen-erate paraphrases for phrases such as create equal.Many NLP tasks, such as SMT, which could benefitfrom paraphrases require broad coverage and mayneed to paraphrases for phrases which are not syn-tactic constituents.Complex syntactic labelsTo generate paraphrases for a wider set of phrases,we change our phrase extraction heuristic again sothat it produces phrase pairs for arbitrary spans inthe sentence, including spans that aren?t syntacticconstituents.
We assign every span in a sentence asyntactic label using CCG-style notation (Steedman,1999), which gives a syntactic role with elementsmissing on the left and/or right hand sides.SBP (fJ1 , eI1, A, P ) = {(fj+mj , ei+ni , s) :?
(i?, j?)
?
A : j ?
j?
?
j +m?
i ?
i?
?
i+ n??
(i?, j?)
?
A : j ?
j?
?
j +m?
?
i ?
i?
?
i+ n?
?s ?
CCG-labels(ei+ni , P )}The function CCG-labels describes the set of CCG-labels for the phrase spanning positions i to i+ n increate equalVP/(NP/NNS) create equal .92creating equal .08VP/(NP/NNS) PP create equal .96promote equal .03establish fair .01VP/(NP/NNS) PP PP create equal .80creating equal .10provide equal .06create genuinely fair .04VP/(NP/(NP/NN) PP) create equal .83create a level playing .17VP/(NP/(NP/NNS) PP) create equal .83creating equal .17Table 4: Paraphrases and syntactic labels for the non-constituent phrase create equal.a parse tree P .
It generates three complex syntacticlabels for the non-syntactic constituent phrase createequal in the parse tree given in Figure 2:1.
VP/(NP/NNS) ?
This label corresponds to the in-nermost circle.
It indicates that create equal isa verb phrase missing a noun phrase to its right.That noun phrase in turn missing a plural noun(NNS) to its right.2.
SQ\VBP NP/(VP/(NP/NNS)) ?
This label corre-sponds to the middle circle.
It indicates thatcreate equal is an SQ missing a VBP and a NPto its left, and the complex VP to its right.3.
SBARQ\WHADVP (SQ\VBP NP/(VP/(NP/NNS)))/.
?This label corresponds to the outermost cir-cle.
It indicates that create equal is an SBARQmissing a WHADVP and the complex SQ to itsleft, and a punctuation mark to its right.We can use these complex labels instead of atomicnon-terminal symbols to handle non-constituentphrases.
For example, Table 4 shows the para-phrases and syntactic labels that are generated forthe non-constituent phrase create equal.
The para-phrases are significantly better than the paraphrasesgenerated for the phrase by the baseline method (re-fer back to Table 2).The labels shown in the figure are a fraction ofthose that can be derived for the phrase in the paral-lel corpus.
Each of these corresponds to a different200syntactic context, and each has its own set of associ-ated paraphrases.We increase the number of phrases that are para-phrasable from the 90,000 in our initial definitionof SBP to 250,000 when we use complex CCG la-bels.
The number of unique paraphrases increasesfrom 800,000 to 3.5 million, which is nearly asmany paraphrases that were produced by the base-line method for the sample.3.2 Syntactic constraints when substitutingparaphrases into a test sentenceIn addition to applying syntactic constraints to ourphrase extraction algorithm, we can also apply themwhen we substitute a paraphrase into a sentence.
Todo so, we limit the paraphrases to be the same syn-tactic type as the phrase that it is replacing, based onthe syntactic labels that are derived from the phrasetree for a test sentence.
Since each phrase normallyhas a set of different CCG labels (instead of a sin-gle non-termal symbol) we need a way of choosingwhich label to use when applying the constraint.There are several different possibilities for choos-ing among labels.
We could simultaneously choosethe best paraphrase and the best label for the phrasein the parse tree of the test sentence:e?2 = argmaxe2:e2 6=e1argmaxs?CCG-labels(e1,P )p(e2|e1, s) (8)Alternately, we could average over all of the labelsthat are generated for the phrase in the parse tree:e?2 = argmaxe2:e2 6=e1?s?CCG-labels(e1,P )p(e2|e1, s) (9)The potential drawback of using Equations 8 and9 is that the CCG labels for a particular sentence sig-nificantly reduces the paraphrases that can be used.For instance, VP/(NP/NNS) is the only label for theparaphrases in Table 4 that is compatible with theparse tree given in Figure 2.Because the CCG labels for a given sentence areso specific, many times there are no matches.
There-fore we also investigated a looser constraint.
Wechoose the highest probability paraphrase with anylabel (i.e.
the set of labels extracted from all parsetrees in our parallel corpus):e?2 = argmaxe2:e2 6=e1argmaxs??
?T in CCCG-labels(e1,T )p(e2|e1, s) (10)Equation 10 only applies syntactic constraints dur-ing phrase extraction and ignores them during sub-stitution.In our experiments, we evaluate the quality of theparaphrases that are generated using Equations 8, 9and 10.
We compare their quality against the Ban-nard and Callison-Burch (2005) baseline.4 Experimental designWe conducted a manual evaluation to evaluate para-phrase quality.
We evaluated whether paraphrasesretained the meaning of their original phrases andwhether they remained grammatical when they re-placed the original phrase in a sentence.4.1 Training materialsOur paraphrase model was trained using the Eu-roparl corpus (Koehn, 2005).
We used ten par-allel corpora between English and (each of) Dan-ish, Dutch, Finnish, French, German, Greek, Ital-ian, Portuguese, Spanish, and Swedish, with approx-imately 30 million words per language for a total of315 million English words.
Automatic word align-ments were created for these using Giza++ (Och andNey, 2003).
The English side of each parallel corpuswas parsed using the Bikel parser (Bikel, 2002).
Atotal of 1.6 million unique sentences were parsed.A trigram language model was trained on these En-glish sentences using the SRI language modelingtoolkit (Stolcke, 2002).The paraphrase model and language model for theBannard and Callison-Burch (2005) baseline weretrained on the same data to ensure a fair comparison.4.2 Test phrasesThe test set was the English portion of test setsused in the shared translation task of the ACL-2007 Workshop on Statistical Machine Translation(Callison-Burch et al, 2007).
The test sentenceswere also parsed with the Bikel parser.The phrases to be evaluated were selected suchthat there was an even balance of phrase lengths(from one word long up to five words long), withhalf of the phrases being valid syntactic constituentsand half being arbitrary sequences of words.
410phrases were selected at random for evaluation.
30items were excluded from our results subsequentto evaluation on the grounds that they consisted201solely of punctuation and stop words like determin-ers, prepositions and pronouns.
This left a total of380 unique phrases.4.3 Experimental conditionsWe produced paraphrases under the following eightconditions:1.
Baseline ?
The paraphrase probability definedby Bannard and Callison-Burch (2005).
Calcu-lated over multiple parallel corpora as given inEquation 5.
Note that under this condition thebest paraphrase is the same for each occurrenceof the phrase irrespective of which sentence itoccurs in.2.
Baseline + LM ?
The paraphrase probability(as above) combined with the language modelprobability calculated for the sentence with thephrase replaced with the paraphrase.3.
Extraction Constraints ?
This condition se-lected the best paraphrase according to Equa-tion 10.
It chooses the single best paraphraseover all labels.
Conditions 3 and 5 only applythe syntactic constraints at the phrase extractionstage, and do not require that the paraphrasehave the same syntactic label as the phrase inthe sentence that it is being subtituted into.4.
Extraction Constraints + LM ?
As above, butthe paraphrases are also ranked with a languagemodel probability.5.
Substitution Constraints ?
This conditioncorresponds to Equation 8, which selects thehighest probability paraphrase which matchesat least one of the syntactic labels of the phrasein the test sentence.
Conditions 5?8 apply thesyntactic constraints both and the phrase ex-traction and at the substitution stages.6.
Syntactic Constraints + LM ?
As above, butincluding a language model probability as well.7.
Averaged Substitution Constraints ?
Thiscondition corresponds to Equation 9, which av-erages over all of the syntactic labels for thephrase in the sentence, instead of choosing thesingle one which maximizes the probability.MEANING5 All of the meaning of the original phrase is re-tained, and nothing is added4 The meaning of the original phrase is retained, al-though some additional information may be addedbut does not transform the meaning3 The meaning of the original phrase is retained, al-though some information may be deleted withouttoo great a loss in the meaning2 Substantial amount of the meaning is different1 The paraphrase doesn?t mean anything close tothe original phraseGRAMMAR5 The sentence with the paraphrase inserted is per-fectly grammatical4 The sentence is grammatical, but might soundslightly awkward3 The sentence has an agreement error (such as be-tween its subject and verb, or between a pluralnoun and singular determiner)2 The sentence has multiple errors or omits wordsthat would be required to make it grammatical1 The sentence is totally ungrammaticalTable 5: Annotators rated paraphrases along two 5-pointscales.8.
Averaged Substitution Constraints + LM ?As above, but including a language modelprobability.4.4 Manual evaluationWe evaluated the paraphrase quality through a sub-stitution test.
We retrieved a number of sentenceswhich contained each test phrase and substituted thephrase with automatically-generated paraphrases.Annotators judged whether the paraphrases had thesame meaning as the original and whether the re-sulting sentences were grammatical.
They assignedtwo values to each sentence using the 5-point scalesgiven in Table 5.
We considered an item to havethe same meaning if it was assigned a score of 3 orgreater, and to be grammatical if it was assigned ascore of 4 or 5.We evaluated several instances of a phrase whenit occurred multiple times in the test corpus,since paraphrase quality can vary based on context(Szpektor et al, 2007).
There were an average of3.1 instances for each phrase, with a maximum of6.
There were a total of 1,195 sentences that para-202phrases were substituted into, with a total of 8,422judgements collected.
Note that 7 different para-phrases were judged on average for every instance.This is because annotators judged paraphrases foreight conditions, and because we collected judg-ments for the 5-best paraphrases for many of theconditions.We measured inter-annotator agreement with theKappa statistic (Carletta, 1996) using the 1,391items that two annotators scored in common.
Thetwo annotators assigned the same absolute score47% of the time.
If we consider chance agreement tobe 20% for 5-point scales, then K = 0.33, which iscommonly interpreted as ?fair?
(Landis and Koch,1977).
If we instead measure agreement in termsof how often the annotators both judged an item tobe above or below the thresholds that we set, thentheir rate of agreement was 80%.
In this case chanceagreement would be 50%, so K = 0.61, which is?substantial?.4.5 Data and codeIn order to allow other researchers to recreate our re-sults or extend our work, we have prepared the fol-lowing materials for download2:?
The complete set of paraphrases generated forthe test set.
This includes the 3.7 million para-phrases generated by the baseline method andthe 3.5 million paraphrases generated with syn-tactic constraints.?
The code that we used to produce these para-phrases and the complete data sets (includingall 10 word-aligned parallel corpora along withtheir English parses), so that researchers canextract paraphrases for new sets of phrases.?
The manual judgments about paraphrase qual-ity.
These may be useful as development ma-terial for setting the weights of a log-linear for-mulation of paraphrasing, as suggested in Zhaoet al (2008a).5 ResultsTable 6 summarizes the results of the manual eval-uation.
We can observe a strong trend in the syn-tactically constrained approaches performing better2Available from http://cs.jhu.edu/?ccb/.correct correct bothmeaning grammar correctBaseline .56 .35 .30Baseline+LM .46 .44 .36Extraction Constraints .62 .57 .46Extraction Const+LM .60 .65 .50Substitution Constraints .60 .60 .50Substitution Const+LM .61 .68 .54Avg Substitution Const .62 .61 .51Avg Substit Const+LM .61 .68 .55Table 6: The results of the manual evaluation for eachof the eight conditions.
Correct meaning is the percent oftime that a condition was assigned a 3, 4, or 5, and correctgrammar is the percent of time that it was given a 4 or 5,using the scales from Table 5.than the baseline.
They retain the correct meaningmore often (ranging from 4% to up to 15%).
Theyare judged to be grammatical far more frequently(up to 26% more often without the language model,and 24% with the language model) .
They performnearly 20% better when both meaning and grammat-icality are used as criteria.3Another trend that can be observed is that incor-porating a language model probability tends to resultin more grammatical output (a 7?9% increase), butmeaning suffers as a result in some cases.
Whenthe LM is applied there is a drop of 12% in correctmeaning for the baseline, but only a slight dip of 1-2% for the syntactically-constrained phrases.Note that for the conditions where the paraphraseswere required to have the same syntactic type as thephrase in the parse tree, there was a reduction in thenumber of paraphrases that could be applied.
Forthe first two conditions, paraphrases were posited for1194 sentences, conditions 3 and 4 could be appliedto 1142 of those sentences, but conditions 5?8 couldonly be applied to 876 sentences.
The substitutionconstraints reduce coverage to 73% of the test sen-tences.
Given that the extraction constraints havebetter coverage and nearly identical performance on3Our results show a significantly lower score for the base-line than reported in Bannard and Callison-Burch (2005).
Thisis potentially due to the facts that in this work we evaluatedon out-of-domain news commentary data, and we randomly se-lected phrases.
In the pervious work the test phrases were drawnfrom WordNet, and they were evaluated solely on in-domainEuropean parliament data.203the meaning criterion, they might be more suitablein some circumstances.6 ConclusionIn this paper we have presented a novel refinementto paraphrasing with bilingual parallel corpora.
Weillustrated that a significantly higher performancecan be achieved by constraining paraphrases to havethe same syntactic type as the original phrase.
Athorough manual evaluation found an absolute im-provement in quality of 19% using strict criteriaabout paraphrase accuracy when comparing againsta strong baseline.
The syntactically enhanced para-phrases are judged to be grammatically correct overtwo thirds of the time, as opposed to the baselinemethod which was grammatically correct under halfof the time.This paper proposed constraints on paraphrases attwo stages: when deriving them from parsed paral-lel corpora and when substituting them into parsedtest sentences.
These constraints produce para-phrases that are better than the baseline and whichare less commonly affected by problems due to un-aligned words.
Furthermore, by introducing com-plex syntactic labels instead of solely relying onnon-terminal symbols in the parse trees, we are ableto keep the broad coverage of the baseline method.Syntactic constraints significantly improve thequality of this paraphrasing method, and their useopens the question about whether analogous con-straints can be usefully applied to paraphrases gen-erated from purely monolingual corpora.
Our im-provements to the extraction of paraphrases fromparallel corpora suggests that it may be usefully ap-plied to other NLP applications, such as generation,which require grammatical output.AcknowledgmentsThanks go to Sally Blatz, Emily Hinchcliff andMichelle Bland for conducting the manual evalua-tion and to Michelle Bland and Omar Zaidan forproofreading and commenting on a draft of this pa-per.This work was supported by the National ScienceFoundation under Grant No.
0713448.
The viewsand findings are the author?s alone.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceedingsof ACL.Dan Bikel.
2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceedingsof HLT.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of HLT/NAACL.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-)evaluation of machine translation.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249?254.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of EMNLP.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT/NAACL.Bryant Huang and Kevin Knight.
2006.
Relabeling syn-tax trees to improve syntax-based machine translationquality.
In Proceedings of HLT/NAACL.Ali Ibrahim, Boris Katz, and Jimmy Lin.
2003.
Extract-ing structural paraphrases from aligned monolingualcorpora.
In Proceedings of the Second InternationalWorkshop on Paraphrasing (ACL 2003).David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedings ofEMNLP.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT/NAACL.Philipp Koehn.
2005.
A parallel corpus for statisticalmachine translation.
In Proceedings of MT-Summit,Phuket, Thailand.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33:159?174.Dekang Lin and Colin Cherry.
2002.
Word align-ment with cohesion constraint.
In Proceedings ofHLT/NAACL.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules from text.
Natural Language Engineering,7(3):343?360.204Nitin Madnani, Necip Fazil Ayan, Philip Resnik, andBonnie Dorr.
2007.
Using paraphrases for parame-ter tuning in statistical machine translation.
In Pro-ceedings of the ACL Workshop on Statistical MachineTranslation.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Karolina Owczarzak, Declan Groves, Josef Van Gen-abith, and Andy Way.
2006.
Contextual bitext-derivedparaphrases in automatic MT evaluation.
In Proceed-ings of the SMT Workshop at HLT-NAACL.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProceedings of HLT/NAACL.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of ACL.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of ACL.Mark Steedman.
1999.
Alternative quantier scope in ccg.In Proceedings of ACL.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing, Denver,Colorado, September.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acquisi-tion.
In Proceedings of ACL.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3).Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and ShengLi.
2008a.
Combining multiple resources to improveSMT-based paraphrasing model.
In Proceedings ofACL/HLT.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2008b.
Pivot approach for extracting paraphrasepatterns from bilingual corpora.
In Proceedings ofACL/HLT.Liang Zhou, Chin-Yew Lin, and Eduard Hovy.
2006.
Re-evaluating machine translation results with paraphrasesupport.
In Proceedings of EMNLP.205
