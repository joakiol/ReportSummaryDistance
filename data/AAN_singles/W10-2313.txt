Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 83?87,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsDistinguishing between Positive and Negative Opinions with ComplexNetwork FeaturesDiego R. Amancio, Renato Fabbri, Osvaldo N. Oliveira Jr.,Maria G. V. Nunes and Luciano da F. CostaUniversity of Sa?o Paulo, Sa?o Carlos, Sa?o Paulo, Brazildiego.amancio@usp.br, renato.fabbri@gmail.com, chu@ifsc.usp.br,gracan@icmc.usp.br, ldfcosta@gmail.comAbstractTopological and dynamic features of com-plex networks have proven to be suitablefor capturing text characteristics in recentyears, with various applications in natu-ral language processing.
In this article weshow that texts with positive and negativeopinions can be distinguished from eachother when represented as complex net-works.
The distinction was possible byobtaining several metrics of the networks,including the in-degree, out-degree, short-est paths, clustering coefficient, between-ness and global efficiency.
For visu-alization, the obtained multidimensionaldataset was projected into a 2-dimensionalspace with the canonical variable analysis.The distinction was quantified using ma-chine learning algorithms, which allowedan recall of 70% in the automatic dis-crimination for the negative opinions, evenwithout attempts to optimize the patternrecognition process.1 IntroductionThe use of statistical methods is well estab-lished for a number of natural language pro-cessing tasks (Manning and Schuetze, 2007), insome cases combined with a deep linguistic treat-ment in hybrid approaches.
Representing text asgraphs (Antiqueira et al, 2007), in particular, hasbecome popular with the advent of complex net-works (CN) (Newman, 2003; Albert and Barabasi,2002), especially after it was shown that largepieces of text generate scale-free networks (Ferreri Cancho and Sole, 2001; Barabasi, 2009).
Thisscale-free nature of such networks is probably themain reason why complex networks concepts arecapable of capturing features of text, even in theabsence of any linguistic treatment.
Significantly,the scale-free property has also allowed CN to beapplied in diverse fields (Costa et al, 2008), fromneuroscience (Sporns, 2002) to physics (Gfeller,2007), from linguistics (Dorogovtsev and Mendes,2001) to computer science (Moura et al, 2003), tomention a few areas.
Other frequently observedunifying principles that natural networks exhibitare short paths between any two nodes and highclustering coefficients (i.e.
the so-called small-world property), correlations in node degrees, anda large number of cycles or specific motifs.The topology and the dynamics of CN can beexploited in natural language processing, whichhas led to several contributions in the literature.For instance, metrics of CN have been used to as-sess the quality of written essays by high schoolstudents (Antiqueira et al, 2007).
Furthermore,degrees, shortest paths and other metrics of CNwere used to produce strategies for automatic sum-marization (Antiqueira et al, 2009), whose resultsare among the best for methods that only employstatistics.
The quality of machine translation sys-tems can be examined using local mappings of lo-cal measures (Amancio et al, 2008).
Other re-lated applications include lexical resources anal-ysis (Sigman and Cecchi, 2002), human-inducedwords association (Costa, 2004), language evolu-tion (Dorogovtsev andMendes, 2002), and author-ship recognition (Antiqueira et al, 2006).In this paper, we model texts as complex net-works with each word being represented by anode and co-occurrences of words defining theedges (see next section).
Unlike traditional meth-ods of text mining and sentiment detection of re-views (Tang et al, 2009; Pennebaker et al, 2003),the method described here only takes into accountthe relationships between concepts, regardless ofthe semantics related to each word.
Specifically,we analyze the topology of the networks in orderto distinguish between texts with positive and neg-ative opinions.
Using a corpus of 290 pieces of83Before pre-processing After pre-processingThe projection of the projectionnetwork data into two network data twodimensions is crucial dimension be crucialfor big networks big networkTable 1: Adjacency list obtained from the sentence?The projection of the network data into two di-mensions is crucial for big networks?.text with half of positive opinions, we show thatthe network features allows one to achieve a rea-sonable distinction.2 Methodology2.1 Representing texts as complex networksTexts are modeled as complex networks here byconsidering each word (concept) as a node and es-tablishing links by co-occurrence of words, disre-garding the punctuation.
In selecting the nodes,the stopwords were removed and the remainingwords were lemmatized to combine words withthe same canonical form but different inflectionsinto a single node.
Additionally, the texts werelabeled using the MXPost part-of-speech Tag-ger based on the Ratnaparki?s model (Ratnaparki,1996), which helps to resolve problems of am-biguity.
This is useful because the words withthe same canonical form and same meaning aregrouped into a single node, while words that havethe same canonical form but distinct meaningsgenerate distinct nodes.
This pre-processing isdone by accessing a computational lexicon, whereeach word has an associated rule for the genera-tion of the canonical form.
For illustrative means,Table 1 shows the pre-processed form of the sen-tence ?The projection of the network data into twodimensions is crucial for big networks?
and Figure1 shows the network obtained for the same sen-tence.Several CN metrics have been used to analyzetextual characteristics, the most common of whichare out-degree (kout), in-degree (kin), cluster co-efficient (C) and shortest paths (l).
Here we alsouse the betweenness (?)
and the global efficiency(?).
The out-degree corresponds to the numberof edges emanating from a given node, wherethe weight of each link between any two nodesmay also be considered, being referred to as out-strength.
Analogously, the node?s in-degree is de-fined as the number of edges arriving at a givenFigure 1: Network obtained from the sentence?The projection of the network data into two di-mensions is crucial for big networks?.node.
The network?s kout and kin are evaluatedby calculating the average among all the nodes,note that such global measures kout and kin arealways equal.
Regarding the adjacency matrix torepresent the network, for a given node i, its koutand kin are calculated by eqs 1 and 2, where Nrepresents the number of distinct words in the pre-processed text:kout(i) =N?j=1Wji (1)kin(i) =N?j=1Wij (2)The cluster coefficient (C) is defined as follows.Let S be the set formed by nodes receiving edgesof a given node i, and Nc is the cardinality of thisset.
If the nodes of this set form a completely con-nected set, then there are Nc(Nc-1) edges in thissub graph.
However, if there are only B edges,then the coefficient is given by eq.
(3):C(i) = BNc(Nc ?
1)(3)If Nc is less than 1, then C is defined as zero.Note that this measure quantifies how the nodesconnected to a specific node are linked to eachother, with its value varying between zero and one.The shortest paths are calculated from all pairsof nodes within the network.
Let dij be the min-imum distance between any two words i and j inthe network.
The shortest path length l of a node iis given in equation 4.84l(i) = 1N ?
1?j ?=idij (4)Another measure often used in network analy-sis is the global efficiency (?
), which is defined inequation 5, and may be interpreted as the speedwith which information is exchanged between anytwo nodes, since a short distance dij contributesmore significantly than a long distance.
Note thatthe formula below prevents divergence; therefore,it is especially useful for networks with two ormore components.
The inverse of ?, named har-monic mean of geodesic distances, has also beenused to characterize complex networks.?
= 1N(N ?
1)?i?=j1dij(5)While l and ?
use the length of shortest paths,the betweenness uses the number of shortest paths.Formally, the betweenness centrality for a givenvertex v is given in equation 6, where the numera-tor represents the number of shortest paths passingthrough the vertices i, v and j and the denomina-tor represents the number of shortest paths pass-ing through the vertices i and j.
In other words,if there are many shortest paths passing through agiven node, this node will receive a high between-ness centrality.?
(v) =?i?j?
(i, v, j)?
(i, j)(6)2.2 CorpusThe corpus used in the experiments was ob-tained from the Brazilian newspaper Folha de Sa?oPaulo1, from which we selected 290 articles over a10-year period from a special section where a pos-itive opinion is confronted with a negative opinionabout a given topic.
For this study, we selectedthe 145 longest texts with positive opinion and the145 longest text with negative opinions2, in orderto have meaningful statistical data for the CN anal-ysis.2.3 Machine Learning MethodsIn order to discriminate the topological featuresfrom distinct networks we first applied a techniquefor reducing the dimension of the dataset, thecanonical variable analysis (McLachlan, 2004).1http://www.folha.com.br2The average size of the selected corpus is 600 words.The projection of network data into a lower di-mension is crucial for visualization, in additionto avoids the so-called ?curse of dimensional-ity?
(Bishop, 2006).
To calculate the axes pointsfor projecting the data, a criterion must be es-tablished with which the distances between datapoints are defined.
Let S be the overall disper-sion of the measurements, as shown in equation 7,where ?
is the number of instances (?
= 290),?
?xc isthe set of metrics for a particular instance and ??
?x ?is the average of all ?
?xc.S =??c=1(?
?xc ?
??
?x ?)(?
?xc ?
??
?x ?
)T(7)Considering that two classes (C1 = positiveopinions and C2 = negative opinions) are used, thescatter matrix Si is obtained for each class Ci, ac-cording to equation 8, where ??
?x ?i is the analo-gous of ??
?x ?
when only the instances belonging toclass Ci is taken into account.Si =?c?Ci(?
?xc ?
??
?x ?i)(?
?xc ?
??
?x ?i)T(8)The intraclass matrix, i.e.
the matrix that givesthe dispersion inside C1 and C2, is defined as inequation 9.
Additionally, we define the interclassmatrix, i.e.
the matrix that provides the dispersionbetween C1 and C2, as shown in equation 10.Sintra = S1 + S2 (9)Sinter = S ?
Sintra (10)The principal axes for the projection are thenobtained by computing the eigenvector associ-ated with the largest eigenvalues of the ma-trix ?
(McLachlan, 2004) defined in equation11.
Since the data were projected in a two-dimensional space, the two principal axes were se-lected, corresponding to the two largest eigenval-ues.?
= S?1intraSinter (11)Finally, to quantify the efficiency of separa-tion with the projection using canonical variableanalysis, we implemented three machine learn-ing algorithms (decision tree, using the C4.5 algo-rithm (Quinlan, 1993); rules of decision, using the85RIP algorithm (Cohen, 1995), and Naive Bayesalgorithm (John and Langley, 1995)) and eval-uated the accuracy rate using the 10-fold-cross-validation (Kohavi, 1995).3 Results and DiscussionThe metrics out-degree (kout), in-degree (kin),shortest paths (l), cluster coefficient (C), between-ness (?)
and global efficiency (?)
were computedfor each of the 145 texts for positive and nega-tive opinions, as described in the Methodology.The mean values and the standard deviations ofthese metrics were used as attributes for each text.This generated a dataset described in 10 attributes,since the average kin is equal to the average koutand the standard deviation of ?
is not defined (inother words, it is always zero).
Figure 2 shows theprojection of the dataset obtained with canonicalvariable analysis, illustrating that texts with dif-ferent opinions can be distinguished to a certainextent.
That is to say, the topological features ofnetworks representing positive opinion tend to dif-fer from those of texts with negative opinion.The efficiency of this methodology for charac-terizing different opinions can be quantified usingmachine learning algorithms to process the datafrom the projection.
The results are illustrated inTable 2.
Again, the distinction between classes isreasonably good, since the accuracy rate reached62%.
Indeed, this rate seems to be a good result,since the baseline method3 tested showed an ac-curacy rate of 53%.
One also should highlightthe coverage found for the class of negative re-views by using the C4.5 algorithm, for which avalue of 82% (result not shown in the Table 2) wasobtained.
This means that if an opinion is nega-tive, the probability of being classified as negativeis only 18%.
Thus, our method seems especiallyuseful when a negative view should be classifiedcorrectly.Method Correctly classifiedC4.5 58%Rip 60%Naive Bayes 62%Table 2: Percentage of correctly classified in-stances.3The baseline method used as attributes the frequency ofeach word in each text.
Then, the algorithm C4.5 was runwith the same parameters used for the methodology based oncomplex networks.-0.120 -0.110 -0.100 -0.090-0.180-0.175-0.170-0.165-0.160-0.155-0.150FIRST PRINCIPAL AXISSECONDPRINCIPALAXISPOSITIVENEGATIVEPROJECTION OF POSITIVE ANDNEGATIVE OBSERVATIONSFigure 2: Projection obtained by using the methodof canonical variables.
A reasonable distinctioncould be achieved between positive and negativeopinions.4 Conclusion and Further WorkThe topological features of complex networksgenerated with texts appear to be efficient in dis-tinguishing between attitudes, as indicated herewhere texts conveying positive opinions could bedistinguished from those of negative opinions.The metrics of the CN combined with a projec-tion technique allowed a reasonable separation ofthe two types of text, and this was confirmed withmachine learning algorithms.
An 62% accuracywas achieved (the baseline reached 53%), eventhough there was no attempt to optimize the met-rics or the methods of analysis.
These promis-ing results are motivation to evaluate other typesof subtleties in texts, including emotional states,which is presently being performed in our group.Acknowledgements: Luciano da F. Costais grateful to FAPESP (05/00587-5) and CNPq(301303/06-1 and 573583/2008-0) for the finan-cial support.
Diego R. Amancio is grateful toFAPESP sponsorship (proc.
09/02941-1) and Re-nato Fabbri is grateful to CAPES sponsorship.
Wealso thank Dr. Oto Araujo Vale very much for sup-plying the corpus.86ReferencesC.
D. Manning and H. Schuetze.
1999.
Foundations ofStatistical Natural Language Processing.
The MITPress, First Edition.L.
Antiqueira, M. G. V. Nunes, O. N. Oliveira Jr. and L.da F. Costa.
2007.
Strong correlations between textquality and complex networks features.
Physica A,373:811?820.M.
E. J. Newman.
2003.
The Structure and Functionof Complex Networks.
SIAM Review, 45:167?256.R.
Z. Albert and A.L.
Barabasi.
2002.
Statistical Me-chanics of Complex Networks.
Rev.
Modern Phys.,74:47?97.R.
Ferrer i Cancho and R. V. Sole.
2001.
The smallworld of human language.
Proceedings of the RoyalSociety of London B, 268:2261.A.L.
Barabasi.
2009.
Scale-Free Networks: a decadeand beyond.
Science, 24 325 5939 412?413.L.
F. da Costa, O. N. Oliveira Jr., G. Travieso, F.A.
Rodrigues, P. R. Villas Boas, L. Antiqueira, M.P.
Viana, L. E. C. da Rocha.
2008.
Analyzingand Modeling Real-World Phenomena with Com-plex Networks: A Survey of Applications.
arXiv0711.3199.O.
Sporns.
2002.
Network analysis, complexity, andbrain function.
Complexity, 8(1):56?60.D.
Gfeller, P. LosRios, A. Caflisch and F. Rao.
2007.Complex network analysis of free-energy land-scapes.
Proceedings of the National Academy ofScience USA, 104 (6):1817?1822S.
N. Dorogovtsev and J. F. F.Mendes.
2001.
Lan-guage as an evolving word web.
Proceedings of theRoyal Society of London B, 268:2603.A.
P. S. de Moura, Y. C. Lai and A. E. Motter.
2003.Signatures of small-world and scale-free propertiesin large computer programs.
Physical Review E,68(1):017102.L.
Antiqueira, O. N. Oliveira Jr., L. da F. Costa andM.
G. V. Nunes.
2009.
A Complex Network Ap-proach to Text Summarization.
Information Sci-ences, 179:(5) 584?599.M.
Sigman and G.A.
Cecchi.
2002.
Global Organi-zation of the Wordnet Lexicon.
Proceedings of theNational Academy of Sciences, 99:1742?1747.L.
F. Costa.
2004.
What?s in a name ?
InternationalJournal of Modern Physics C, 15:371?379.S.
N. Dorogovtsev and J. F. F. Mendes.
2002.
Evo-lution of networks.
Advances in Physics, 51:1079?1187.L.
Antiqueira, T. A. S. Pardo, M. G. V. Nunes, O. N.Oliveira Jr. and L. F. Costa.
2006.
Some issues oncomplex networks for author characterization.
Pro-ceeedings of the Workshop in Information and Hu-man Language Technology.H.
Tang, S. Tan and X. Cheng.
2009.
A survey onsentiment detection of reviews.
Expert Systems withApplications, 36:7 10760?10773.J.
W. Pennebaker, M. R. Mehl and K. G. Niederhoffer.2003.
Psychological aspects of natural language.use: our words, our selves.
Annual review of psy-chology, 54 547-77.D.
R. Amancio, L. Antiqueira, T. A. S. Pardo, L.F. Costa, O. N. Oliveira Jr. and M. G. V. Nunes.2008.
Complex networks analysis of manual andmachine translations.
International Journal of Mod-ern Physics C, 19(4):583-598.A.
Ratnaparki.
1996.
A Maximum Entropy Part-Of-Speech Tagger.
Proceedings of the Empirical Meth-ods in Natural Language Processing Conference,University of Pennsylvania.G.
J. McLachlan.
2004.
Discriminant Analysis andStatistical Pattern Recognition.
Wiley.C.
M. Bishop.
2006.
Pattern Recognition and MachineLearning.
Springer-Verlag New York.R.
Quinlan.
1993.
C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann Publishers.W.
W. Cohen.
1995.
Fast Effective Rule Induction.12 International converence on Machine Learning,115?223.G.
H. John and P. Langley.
1995.
Estimating Continu-ous Distribution in Bayesian Classifiers.
11 Confer-ence on Uncertainty in Artificial Intelligence, 338?345.R.
Kohavi.
1995.
A study of cross-validation and boot-strap for accuracy estimation and model selection.Proceedings of the Fourteenth International JointConference on Artificial Intelligence 2, 12:1137-1143.87
