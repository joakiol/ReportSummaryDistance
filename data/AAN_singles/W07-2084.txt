Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 382?385,Prague, June 2007. c?2007 Association for Computational LinguisticsUC3M: Classification of Semantic Relations between Nominals usingSequential Minimal OptimizationIsabel Segura BedmarComputer Science DepartmentUniversity Carlos III of Madridisegura@inf.uc3m.esDoaa SamyComputer Science DepartmentUniversity Carlos III of Madriddsamy@inf.uc3m.esJose L. MartinezComputer Science DepartmentUniversity Carlos III of Madridjlmartinez@inf.uc3m.esAbstractThis paper presents a method for auto-matic classification of semantic relationsbetween nominals using SequentialMinimal Optimization.
We participatedin the four categories of SEMEVAL task4 (A: No Query, No Wordnet; B: Word-Net, No Query; C: Query, No WordNet;D: WordNet and Query) and for all train-ing datasets.
Best scores were achievedin category B using a set of feature vec-tors including lexical file numbers ofnominals obtained from WordNet and anew feature WordNet Vector designedfor the task1.1 IntroductionThe survey of the state-of-art reveals an increas-ing interest in automatically discovering the un-derlying semantics in natural language.
In thisinterdisciplinary field, the growing interest isjustified by the number of applications whichcan directly benefit from introducing semanticinformation.
Question Answering, InformationRetrieval and Text Summarization are examplesof these applications (Turney and Littman, 2005;Girju et al, 2005).In the present work and for the purpose of theSEMEVAL task 4, our scope is limited to thesemantic relationships between nominals.
Bythis definition, we understand it is the process ofdiscovering the underlying relations betweentwo concepts expressed by two nominals.1 This work has been partially supported by the Re-gional Government of Madrid Ander the ResearchNetwork MAVIR (S-0505/TIC-0267)Within the framework of SEMEVAL, nomi-nals can occur either on the phrase, clause or thesentence level.
This fact constitutes the majorchallenge in this task since most of the previousresearch limited their approaches to certain typesof nominals mainly the ?compound nomi-nals?
(Girju et al 2005).The paper is divided as follows; section 2 is abrief introduction to SMO used as the classifierfor the task.
Section 3 is dedicated to the de-scription of the set of features applied in our ex-periments.
In section 4, we discuss the experi-ment?s results compared to the baselines of theSEMEVAL task and the top scores.
Finally, wesummarize our approach, pointing out conclu-sions and future directions of our work.2 Sequential Minimal OptimizationWe decided to use Support Vector Machine(SVM), as one of the most successful MachineLearning techniques, achieving the best per-formances for many classification tasks.
Algo-rithm performance and time efficiency are keyissues in our task, considering that our final goalis to apply this classification in a Question An-swering System.Sequential Minimal Optimization (SMO) is afast method to train SVM.
SMO breaks the largequadratic programming (QP) optimization prob-lem needed to be resolved in SVM into a seriesof smallest possible QP problems.
These smallQP problems are analytically solved, avoiding,in this way, a time-consuming numerical QPoptimization as an inner loop.
We used Weka(Witten and Frank, 2005) an implementation ofthe SMO (Platt, 1998).3823 FeaturesPrior to the classification of semantic rela-tions, characteristics of each sentence are auto-matically extracted using GATE (Cunninghamet al, 2002).
GATE is an infrastructure for de-veloping and deploying software components forLanguage Engineering.
We used the followingGATE components: English Tokenizer, Part-Of-Speech (POS) tagger and Morphological ana-lyser.The set of features used for the classificationof semantic relations includes information fromdifferent levels: word tokens, POS tags, verblemmas, semantic information from WordNet,etc.
Semantic features are only applied in cate-gories B and D.On the lexical level, the set of word featuresinclude the two nominals, their heads in case oneof the nominals in question or both are com-pound nominals ( e.g.
the relation between<e1>tumor shrinkage</e1> and <e2>radiationtherapy </e2> is actually between the head ofthe first ?shrinkage?
and ?radiation_therapy?
).More features include: the two words before thefirst nominal, the two words after the secondnominal, and the word list in-between (Wang etal., 2006).On the POS level, we opted for using a set ofPOS features since word features are often toosparse.
This set includes POS tags of the twowords occurring before the first nominal and thetwo words occurring after the second nominaltogether with the tag list of the words in-between (Wang et al, 2006).
POS tags of nomi-nals are considered redundant information.Information regarding verbs and prepositions,occurring in-between the two nominals, is highlyconsidered.
In case of the verb, the system takesinto account the verb token and the informationconcerning the voice and lemma.
In the sameway, the system keeps track of the prepositionsoccurring between both nominals.
In addition, afeature, called numinter, indicating the numberof words between nominals is considered.Other important feature is the path from thefirst nominal to the second nominal.
This featureis built by the concatenation of the POS tags be-tween both nominals.The feature related to the query provided foreach sentence is only considered in the catego-ries C and D according to the SEMEVAL re-strictions.On the semantic level, we used features ob-tained from WordNet.
In addition to the Word-Net sense keys, provided for each nominal, weextracted its synset number and its lexical filenumber.Based on the work of Rosario, Hearst andFillmore (2002), we suppose that these lexicalfile numbers can help to determine if the nomi-nals satisfy the restrictions for each relation.
Forexample, in the relation Theme-Tool, the themeshould be an object, an event, a state of being, anagent, or a substance.
Else, it is possible to af-firm that the relation is false.For the Part-Whole relation and due to itsrelevance in this classification task, a featureindicating metonymy relation in WordNet wastaken into account.Furthermore, we designed a new feature,called WordNet vector.
For constructing thisvector, we selected the synsets of the third levelof depth in WordNet and we detected if each isancestor or not of the nominal.
It is a binary vec-tor, i.e.
if the synset is ancestor of the nominal itis assigned the value 1, else it is assigned thevalue 0.
In this way, we worked with two vec-tors, one for each nominal.
Each vector has adimension of 13 coordinates.
Each coordinaterepresents one of the 13 nodes in the third levelof depth in WordNet.
Our initial hypothesis con-siders that this representation for the nominalscould perform well on unseen data.4 Experiment ResultsCross validation is a way to test the ability ofthe model to classify unseen examples.
Wetrained the system using 10-fold cross-validation; the fold number recommended forsmall training datasets.
For each relation and foreach category (A, B, C, D) we selected the set offeatures that obtained the best results using theindicated cross validation.We submitted 16 sets of results as we partici-pated in the four categories (A, B, C, D).
Wealso used all the possible sizes of the trainingdataset (1: 1 to 35, 2:1 to 70, 3:1 to 106, 4:1 to140).383A: No Query, NoWordNetB: No Query,WordNetC: No Query, NoWordNetD: Query, Word-NetPrec Rec F Prec Rec F Prec Rec F Prec Rec FCause-Effect 50.0 51.2 50.6 66.7   73.2 69.8 42.9   36.6   39.5   59.0   56.1   57.5Instrument-Agency 47.5 50.0 48.7 73.7   73.7   73.7   51.4   50.0   50.7   67.5   71.1   69.2Product-Producer 65.3 51.6 57.7 83.7   66.1   73.9   67.4   50.0   57.4   74.5   61.3   67.3Origin-Entity 50.0 27.8 35.7 63.0   47.2   54.0   54.5   33.3   41.4   63.3   52.8   57.6Theme-Tool 50.0 27.6 35.6 50.0   48.3   49.1   47.4   31.0   37.5   40.9   31.0   35.3Part-Whole 26.5 34,6 30.0 72.4   80.8   76.4   34.0   61.5   43.8   57.1   76.9   65.6Content-Container 48.4 39.5 43.5 57.6   50.0   53.5   48.6   44.7   46.6   63.6   55.3   59.2Avg for UC3M 48.2 40.3   43.1   66.7 62.8  64.3  49.4   43.9   45.3   60.9   57.8   58.8Avg for all systems 59.2 58.7 58.0 65.3 64.4 63.6 59.9 59.0 58.4 64.9 60.4 60.6Max Avg F   64.8   72.4   65.1   62.6Table  1 Scores for A4, B4, C4 and D4For some learning algorithms such as decisiontrees and rule learning, appropriate selection offeatures is crucial.
For the SVM model, this isnot so important due to its learning mechanism,where irrelevant features are usually balancedbetween positive and negative examples for agiven binary classification problem.
However, inthe experiments we observed that certain fea-tures have strong influence on the results, and itsinclusion or elimination from the vector, influ-enced remarkably the outcomes.In this section, we will briefly discuss the ex-periments in the four categories highlighting themost relevant observations.In category A, we expected to obtain betterresults, but the overall performance of the sys-tem has decreased in the seven relations.
Thisshows that our system has over-fitted the train-ing set.
The contrast between the F score valuesin the cross-validation and the final test resultsdemonstrates this fact.
For all the relations in thecategory A4, we obtained an average ofF=43.1% [average score of all participatingteams: F=58.0% and top average score:F=64.8%].In Product-Producer relation, only two fea-tures were used: the two heads of the nominals.In training, we obtained an average F= 60% us-ing cross-validation, while in the final test data,we achieved an average score F=57.7%.
For therelation Theme-Tool, other set of features wasemployed: nominals, their heads, verb, preposi-tion and the list of word between both nominals.Based on the results of the 10-fold cross valida-tion, we expected to obtain an average of theF=70%.
Nevertheless, the score obtained is F=30%.In category B, our system has achieved betterscores.
Our average score F is 64.3% and it isabove the average of participating teams(F=63.6%) and the baseline.Best results in this category were achieved inthe relations: Instrument-Agency (F=73.7%),Product-Producer (F=73.9%), Part-Whole(F=76.4%).
However, for the relation Theme-Tool the system obtained lower scores(F=49.1%).It is obvious that introducing WordNet infor-mation has improved notably the results com-pared with the results obtained in the categoryA.In categories C and D, only three groups haveparticipated.
In category C (as in category A),the system results have decreased obviously(F=45.3%) with respect to the expected scores inthe 10-fold cross validation.
Moreover, the scoreobtained is lower than the average score of allparticipants (F=58.4%) and the best score(F=65.1%).
For example, in training the Instru-ment-Agent relation, the system achieved anaverage F=78% using 10-fold cross-validation,while for the final score it only obtainedF=50.7%.Results reveal that the main reason behind thelow scores in A and C, is the absence of infor-mation from WordNet.
Hence, the vector designneeds further consideration in case no semanticinformation is provided.In category D, both WordNet senses andquery were used, we achieved an average scoreF=58.8%.
The average score for all participantsis F=60.6% and the best system achievedF=62.6%.
However, the slight difference showsthat our system worked relatively well in thiscategory.384Both run time and accuracy depend criticallyon the values given to two parameters: the upperbound on the coefficient?s values in the equationfor the hyperplane (-C), and the degree of thepolynomials in the non-linear mapping (-E)(Witten and Frank, 2005).
Both are set to 1 bydefault.
The best settings for a particular datasetcan be found only by experimentation.We made numerous experiments to find thebest value for the parameter C (C=1, C=10,C=100, C=1000, C=10000), but the results werenot remarkably affected.
Probably, this is due tothe small size of the training set.5 Conclusions and Future WorkIn our first approach to automatic classifica-tion of semantic relations between nominals andas expected from the training phase, our systemachieved its best performance using WordNetinformation.
In general, we obtained betterscores in category 4 (size of training: 1 to 140),i.e., when all the training examples are used.On the other hand, overfitting the trainingdata (most probably due to the small size oftraining dataset) is the main reason behind thelow scores obtained by our system.These facts lead us to the conclusion that se-mantic features from WordNet, in general, playa key role in the classification task.
However,the relevance of WordNet-related features var-ies.
For example, lexical file numbers proved tobe highly effective, while the use of the Word-Net Vector did not improve significantly the re-sults.
Thus, we consider that a level 3 WordNetVector is rather abstract to represent each nomi-nal.
Developing a WordNet Vector with a deeperlevel (> 3) could be more effective as the repre-sentation of nouns is more descriptive.Query features, on the other hand, did not im-prove the performance of the system.
This is dueto the fact that the same query could representboth positive and negative examples of the rela-tion.
However, to improve results in categoriesA and C, more features need to introduced, es-pecially context and syntactic information suchas chunks or dependency relations.To improve results across the whole dataset,wider use of semantic information is necessary.For example, the immediate hypernym for eachsynset obtained from WordNet could help inimproving the system performance (Nastase etal., 2006).
Besides, information regarding theentity features could help in the classification ofsome relations like Origin-Entity or Product-Producer.
Other semantic resources such asVerbNet, FrameNet, PropBank, etc.
could alsobe used.Furthermore, we consider introducing a WordSense Disambiguation module to obtain the cor-responding synsets of the nominals.
Also, in-formation concerning the synsets of the list ofthe context words could be of great value for theclassification task (Wang et al, 2006).ReferencesHamish Cunningham, Diana Maynard and KalinaBontcheva, Valentin Tablan, Cristian Ursu.
2002.
TheGATE User Guide.
http://gate.ac.uk/Roxana Girju, Dan Moldovan, Marta Tatu and Daniel An-tohe.
2005.
On the semantics of noun compunds.
Com-puter Speech and Language 19 pp.
479-496.Vivi Nastase, Jelber Sayyad-Shirbad, Marina Sokolova andStan Szpakowicz.
2006.
Learning noun-modifier seman-tic relations with corpus-based and WordNet-based fea-tures.
In Proc.
of the 21st National Conference on Artifi-cial Intelligence (AAAI 2006).
Boston, MA.John C. Platt.
1998.
Sequential Minimal Optimization: AFast Algorithm for Training Support Vector Machines,Microsoft Research Technical Report MSR-TR-98-14.Barbara Rosario, Marti A. Hearst, and Charles Fillmore.2002.
?The descent of hierarchy, and selection in rela-tions semantics?.
In Proceedings of the 40 th AnnualMeeting of the Association for Computacional Linguis-tics (ACL?02), Philadelphia, PA, pages 417-424.Ian H. Witten, Eibe Frank.
2005.
Data Mining: Practicalmachine learning tools and techniques.
Morgan Kauf-mann.Peter D. Turney and Michael L. Littman.
2005.
Corpus-based learning of analogies and semantic rela-tions.
Ma-chine Learning, in press.Ting Wang, Yaoyong Li, Kalina Bontcheva, Hamish Cun-ningham and Ji Wang.
2006.
Automatic Extraction ofHierarchical Relations from Text.
In Proceedings of theThird European Semantic Web Conference (ESWC2006), Lecture Notes in Computer Science 4011,Springer, 2006.385
