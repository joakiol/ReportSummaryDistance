Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 542?551,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsReNew: A Semi-Supervised Framework for Generating Domain-SpecificLexicons and Sentiment AnalysisZhe ZhangDepartment of Computer ScienceNorth Carolina State UniversityRaleigh, NC 27695-8206zzhang13@ncsu.eduMunindar P. SinghDepartment of Computer ScienceNorth Carolina State UniversityRaleigh, NC 27695-8206singh@ncsu.eduAbstractThe sentiment captured in opinionated textprovides interesting and valuable informa-tion for social media services.
However,due to the complexity and diversity oflinguistic representations, it is challeng-ing to build a framework that accuratelyextracts such sentiment.
We propose asemi-supervised framework for generat-ing a domain-specific sentiment lexiconand inferring sentiments at the segmentlevel.
Our framework can greatly reducethe human effort for building a domain-specific sentiment lexicon with high qual-ity.
Specifically, in our evaluation, work-ing with just 20 manually labeled reviews,it generates a domain-specific sentimentlexicon that yields weighted average F-Measure gains of 3%.
Our sentiment clas-sification model achieves approximately1% greater accuracy than a state-of-the-artapproach based on elementary discourseunits.1 IntroductionAutomatically extracting sentiments from user-generated opinionated text is important in build-ing social media services.
However, the complex-ity and diversity of the linguistic representationsof sentiments make this problem challenging.High-quality sentiment lexicons can improvethe performance of sentiment analysis models overgeneral-purpose lexicons (Choi and Cardie, 2009).More advanced methods such as (Kanayama andNasukawa, 2006) adopt domain knowledge by ex-tracting sentiment words from the domain-specificcorpus.
However, depending on the context, thesame word can have different polarities even in thesame domain (Liu, 2012).In respect to sentiment classification, Pang etal.
(2002) infer the sentiments using basic features,such as bag-of-words.
To capture more complexlinguistic phenomena, leading approaches (Naka-gawa et al, 2010; Jo and Oh, 2011; Kim et al,2013) apply more advanced models but assumeone document or sentence holds one sentiment.However, this is often not the case.
Sentimentscan change within one document, one sentence,or even one clause.
Also, existing approaches in-fer sentiments without considering the changes ofsentiments within or between clauses.
However,these changes can be successfully exploited for in-ferring fine-grained sentiments.To address the above shortcomings of lexiconand granularity, we propose a semi-supervisedframework named ReNew.
(1) Instead of us-ing sentences, ReNew uses segments as the basicunits for sentiment classification.
Segments canbe shorter than sentences and therefore help cap-ture fine-grained sentiments.
(2) ReNew leveragesthe relationships between consecutive segments toinfer their sentiments and automatically generatesa domain-specific sentiment lexicon in a semi-su-pervised fashion.
(3) To capture the contextualsentiment of words, ReNew uses dependency re-lation pairs as the basic elements in the generatedsentiment lexicon.SentimentSegment1 2 3 4 5NEGNEUPOStransition cuetransition cueFigure 1: Segments in a Tripadvisor review.Consider a part of a review from Tripadvisor.1We split it into six segments with sentiment labels.1http://www.tripadvisor.com/ShowUserReviews-g32655-d81765-r100000013542?.
.
.
(1: POS) The hotel was clean andcomfortable.
(2: POS) Service wasfriendly (3: POS) even providing us alate-morning check-in.
(4: POS) Theroom was quiet and comfortable, (5:NEG) but it was beginning to show afew small signs of wear and tear.
.
.
.
?Figure 1 visualizes the sentiment changeswithin the text.
The sentiment remains the sameacross Segments 1 to 4.
The sentiment transi-tion between Segments 4 and 5 is indicated by thetransition cue ?but?
?which signals conflict andcontradiction.
Assuming we know Segment 4 ispositive, given the fact that Segment 5 starts with?but,?
we can infer with high confidence that thesentiment in Segment 5 changes to neutral or nega-tive even without looking at its content.
After clas-sifying the sentiment of Segment 5 as NEG, weassociate the dependency relation pairs {?sign?,?wear?}
and {?sign?, ?tear?}
with that sentiment.ReNew can greatly reduce the human effort forbuilding a domain-specific sentiment lexicon withhigh quality.
Specifically, in our evaluation ontwo real datasets, working with just 20 manu-ally labeled reviews, ReNew generates a domain-specific sentiment lexicon that yields weighted av-erage F-Measure gains of 3%.
Additionally, oursentiment classification model achieves approxi-mately 1% greater accuracy than a state-of-the-art approach based on elementary discourse units(Lazaridou et al, 2013).The rest of this paper is structured as follows.Section 2 introduces some essential background.Section 3 illustrates ReNew.
Section 4 presentsour experiments and results.
Section 5 reviewssome related work.
Section 6 concludes this pa-per and outlines some directions for future work.2 BackgroundLet us introduce some of the key terminology usedin ReNew.
A segment is a sequence of wordsthat represents at most one sentiment.
A seg-ment can consist of multiple consecutive clauses,up to a whole sentence.
Or, it can be shorterthan a clause.
A dependency relation defines abinary relation that describes whether a pairwisesyntactic relation among two words holds in a sen-tence.
In ReNew, we exploit the Stanford typeddependency representations (de Marneffe et al,2006) that use triples to formalize dependency re-lations.
A domain-specific sentiment lexicon con-tains three lists of dependency relations, associ-ated respectvely with positive, neutral, or negativesentiment.Given a set of reviews, the tasks of senti-ment analysis in ReNew are (1) splitting each re-view into segments, (2) associating each segmentwith a sentiment label (positive, neutral, nega-tive), and (3) automatically generating a domain-specific sentiment lexicon.
We employ Condi-tional Random Fields (Lafferty et al, 2001) to pre-dict the sentiment label for each segment.
Given asequence of segments x?
= (x1, ?
?
?
,xn) and a se-quence of sentiment labels y?
= (y1, ?
?
?
, yn), theCRFs model p(y?|x?)
as follows.p(y?|x?)
=1Z(x?)expJ?j(?j?
Fj(x?, y?
))Fj(x?, y?)
=n?i=1fj(yi?1, yi, x?, i)where ?
is a set of weights learned in the train-ing process to maximize p(y?|x?).
Z(x?)
is a nor-malization constant that is the sum of all possiblelabel sequences.
And, Fjis a feature function thatsums fjover i ?
(1,n), where n is the length ofy?, and fjcan have arbitrary dependencies on theobservation sequence x?
and neighboring labels.3 FrameworkBootstrapping ProcessSentiment LabelingorLearner RetrainingSeed InformationLexiconGeneratorDomainSpecificLexiconGeneralLexiconSegmentationLabeledDataUnlabeledDataFigure 2: The ReNew framework schematically.Figure 2 illustrates ReNew.
Its inputs include543a general sentiment lexicon and a small labeledtraining dataset.
We use a general sentiment lexi-con and the training dataset as prior knowledge tobuild the initial learners.On each iteration in the bootstrapping process,additional unlabeled data is first segmented.
Sec-ond, the learners predict labels for segments basedon current knowledge.
Third, the lexicon gener-ator determines which newly learned dependencyrelation triples to promote to the lexicon.
At theend of each iteration, the learners are retrained viathe updated lexicon so as to classify better on thenext iteration.
After labeling all of the data, weobtain the final version of our learners along witha domain-specific lexicon.3.1 Rule-Based Segmentation AlgorithmAlgorithm 1 Rule-based segmentation.Require: Review dataset T1: for all review r in T do2: Remove HTML tags3: Expand typical abbreviations4: Mark special name-entities5: for all sentence m in r do6: while m contains a transition cue and mis not empty do7: Extract subclause p that contains thetransition cue8: Add p as segment s into segment list9: Remove p from m10: end while11: Add the remaining part in m as segments into segment list12: end for13: end forThe algorithm starts with a review dataset T.Each review r from dataset T is first normalizedby a set of hard-coded rules (lines 2?4) to removeunnecessary punctuations and HTML tags, expandtypical abbreviations, and mark special name enti-ties (e.g., replace a URL by #LINK# and replace amonetary amount ?$78.99?
by #MONEY#).After the normalization step, it splits each re-view r into sentences, and each sentence into sub-clauses (lines 6?10) provided transition cues oc-cur.
In effect, the algorithm converts each reviewinto a set of segments.Note that ReNew captures and uses the senti-ment changes.
Therefore, our segmentation algo-rithm considers only two specific types of transi-tion cues including contradiction and emphasis.3.2 Sentiment LabelingReNew starts with a small labeled training set.Knowledge from this initial training set is not suf-ficient to build an accurate sentiment classificationmodel or to generate a domain-specific sentimentlexicon.
Unlabeled data contains rich knowledge,and it can be easily obtained.
To exploit this re-source, on each iteration, the sentiment labelingcomponent, as shown in Figure 3, labels the databy using multiple learners and a label integrator.We have developed a forward (FR) and a back-ward relationship (BR) learner to learn relation-ships among segments.Sentiment LabelingLabelIntegratorreverseorderForwardRelationshipLearnerBackwardRelationshipLearnerUnlabeledsegmentsLabeledsegmentsFigure 3: Sentiment labeling.3.2.1 FR and BR LearnersThe FR learner learns the relationship between thecurrent segment and the next.
Given the senti-ment label and content of a segment, it tries to findthe best possible sentiment label of the next seg-ment.
The FR Learner tackles the following situa-tion where two segments are connected by a tran-sition word, but existing knowledge is insufficientto infer the sentiment of the second segment.
Forinstance, consider the following review sentence.2(1) The location is great, (2) but the staff waspretty ho-hum about everything from checking in,to AM hot coffee, to PM bar.The sentence contains two segments.
We caneasily infer the sentiment polarity of Segment 1based on the word ?great?
that is commonly in-cluded in many general sentiment lexicons.
ForSegment 2, without any context information, itis difficult to infer its sentiment.
Although the2http://www.tripadvisor.com/ShowUserReviews-g60763-d93589-r10006597544word ?ho-hum?
indicates a negative polarity, itis not a frequent word.
However, the conjunc-tion ?but?
clearly signals a contrast.
So, giventhe fact that the former segment is positive, a pre-trained FR learner can classify the latter as neg-ative.
The Backward Relationship (BR) learnerdoes the same but with the segments in each re-view in reverse order.3.2.2 Label IntegratorGiven the candidate sentiment labels suggested bythe two learners, the label integrator first selectsthe label with confidence greater than or equal toa preset threshold.
Segments are left unlabeled iftheir candidate labels belong to mutually exclusivecategories with the same confidence.3.3 Lexicon GeneratorIn each iteration, after labeling a segment, the lexi-con generator identifies new triples automatically.As shown in Figure 4, this module contains twoparts: a Triple Extractor and a Lexicon Integra-tor.
For each sentiment, the Triple Extractor (TE)extracts candidate dependency relation triples us-ing a novel rule-based approach.
The LexiconIntegrator (LI) evaluates the proposed candidatesand promotes the most supported candidates to thecorresponding sentiment category in the domain-specific lexicon.Lexicon GeneratorTripleExtractorLexiconIntegratorDomainSpecificLexiconLabeledsegmentsFigure 4: Lexicon generator module.3.3.1 Triple Extractor (TE)The TE follows the steps below, for segmentsthat contain only one clause, as demonstratedin Figure 5 for ?The staff was slow and defi-nitely not very friendly.?
The extracted triples areroot nsubj(slow, staff), nsubj(slow, staff), andnsubj(not friendly, staff).1.
Generate a segment?s dependency parse tree.2.
Identify the root node of each clause in thesegment.3.
Remove all triples except those marked E inTable 1.4.
Apply the rules in Table 2 to add or modifytriples.5.
Suggest the types of triples marked L in Ta-ble 1 to the lexicon integrator.Table 1: Dependency relation types used in ex-tracting (E) and domain-specific lexicon (L).Types Explanation E Lamod adjectival modifier?
?acomp adjectival complement?
?nsubj nominal subject?
?neg negation modifier?conj and words coordinated by ?and?
?or similarprep with words coordinated by ?with?
?root root node?root amod amod root node?root acomp acomp root node?root nsubj nsubj root node?neg pattern ?neg?
pattern?Table 1 describes all seven types of triples usedin the domain-specific lexicon.
Among them,amod, acomp, and nsubj are as in (de Marneffeet al, 2006).
And, root amod captures the rootnode of a sentence when it also appears in the ad-jectival modifier triple, similarly for root acompand root nsubj.
We observe that the word of theroot node is often related to the sentiment of a sen-tence and this is especially true when this wordalso appears in the adjectival modifier, adjectivalcomplement, or negation modifier triple.Zhang et al (2010) propose the no pattern thatdescribes a word pair whose first word is ?No?followed by a noun or noun phrase.
They showthat this pattern is a useful indicator for sentimentanalysis.
In our dataset, in addition to ?No,?
weobserve the frequent usage of ?Nothing?
followedby an adjective.
For example, users may express anegative feeling about a hotel using sentence suchas ?Nothing special.?
Therefore, we create theneg pattern to capture a larger range of possibleword pairs.
In ReNew, neg pattern is ?No?
or?Nothing?
followed by a noun or noun phrase oran adjective.3.3.2 Lexicon Integrator (LI)The Lexicon Integrator promotes candidate tripleswith a frequency greater than or equal to a preset545Thestaffwasslownsubjdefinitelynot veryfriendlydetcopadvmodadvmodnegconj_androotstaffslownsubjnotfriendlynegconj_androotstaffslownsubjnot_friendlyconj_androotstaffslownsubjnot_friendlyroot(a)(b)(c)(d)nsubjFigure 5: Extracting sentiment triples from a seg-ment that contains one clause.
(a) The initial de-pendency parse tree.
(b) Remove nonsentimenttriples.
(c) Handle negation triples.
(d) Build rela-tionships.threshold.
The frequency list is updated in eachiteration.
The LI first examines the prior knowl-edge represented as an ordered list of the gover-nors of all triples, each is attached with an orderedlist of its dependents.
Then, based on the triplespromoted in this iteration, the order of the gover-nors and their dependents is updated.
Triples arenot promoted if their governors or dependents ap-pear in a predetermined list of stopwords.The LI promotes triples by respecting mutualexclusion and the existing lexicon.
In particular,it does not promote triples if they exist in multiplesentiment categories or if they already belong to adifferent sentiment category.Finally, for each sentiment, we obtain sevensorted lists corresponding to amod, acomp,nsubj, root amod, root acomp, root nsubj, andneg pattern.
These lists form the domain-specificsentiment lexicon.Table 2: Rules for extracting sentiment triples.Rule Function Condition ResultR1Handle Negation word wi; wi= wdep+ ??
?neg(wgov,wdep); + wiwi= wgov;R2Build Relationships word wiand wj; amod(wgov,wi)(conj and, amod) conj and(wi,wj); amod(wgov,wj)amod(wgov,wi);R3Build Relationships word wiand wj; acomp(wgov,wi)(conj and, acomp) conj and(wi,wj); acomp(wgov,wj)acomp(wgov,wi);R4Build Relationships word wiand wj; nsubj(wi,wdep)(conj and, nsubj) conj and(wi,wj); nsubj(wj,wdep)nsubj(wi,wdep);3.4 Learner RetrainingAt the end of each iteration, ReNew retrains eachlearner as shown in Figure 6.
Newly labeled seg-ments are selected by a filter.
Then, given an up-dated lexicon, learners are retrained to performbetter on the next iteration.
Detailed descriptionof the filter and learner are presented below.3.4.1 FilterThe filter seeks to prevent labeling errors fromaccumulating during bootstrapping.
In ReNew,newly acquired training samples are segmentswith labels that are predicted by old learners.
Eachpredicted label is associated with a confidencevalue.
The filter is applied to select those labeledsegments with confidence greater than or equal toa preset threshold.Learner RetrainingFilterDomainSpecificLexiconLearnerFeatureExtractorClassificationModelLabeledsegmentsFigure 6: Retrain a relationship learner.3.4.2 LearnerAs Section 3.2 describes, ReNew uses learners tocapture different types of relationships among seg-ments to classify sentiment by leveraging theserelationships.
Each learner contains two com-ponents: a feature extractor and a classificationmodel.
To train a learner, the feature extractorfirst converts labeled segments into feature vectors546Table 3: A list of transition types used in ReNew.Transition Types ExamplesAgreement, Addition, and Similarity also, similarly, as well as, .
.
.Opposition, Limitation, and Contradiction but, although, in contrast, .
.
.Cause, Condition, and Purpose if, since, as/so long as, .
.
.Examples, Support, and Emphasis including, especially, such as, .
.
.Effect, Consequence, and Result therefore, thus, as a result .
.
.Conclusion, Summary, and Restatement overall, all in all, to sum up, .
.
.Time, Chronology, and Sequence until, eventually, as soon as, .
.
.for training a CRF-based sentiment classificationmodel.
The feature extractor generates five kindsof features as below.Grammar: part-of-speech tag of every word, thetype of phrases and clauses (if known).Opinion word: To exploit a general sentimentlexicon, we use two binary features indicat-ing the presence or absence of a word in thepositive or negative list in a general sentimentlexicon.Dependency relation: The lexicon generated byReNew uses the Stanford typed dependencyrepresentation as its structure.Transition cue: For tracking the changes of thesentiment, we exploit seven types of transi-tion cues, as shown in Table 3.Punctuation, special name-entity, and seg-ment position: Some punctuation symbols,such as ?!
?, are reliable carriers of senti-ments.
We mark special named-entities, suchas time, money, and so on.
In addition,we use segment positions (beginning, middle,and end) in reviews as features.4 ExperimentsTo assess ReNew?s effectiveness, we prepare twohotel review datasets crawled from Tripadvisor.One dataset contains a total of 4,017 unlabeled re-views regarding 802 hotels from seven US cities.The reviews are posted by 340 users, each ofwhom contributes at least ten reviews.
The otherdataset contains 200 reviews randomly selectedfrom Tripadvisor.
We collected ground-truth la-bels for this dataset by inviting six annotatorsin two groups of three.
Each group labeled thesame 100 reviews.
We obtained the labels foreach segment consist as positive, neutral, or nega-tive.
Fleiss?
kappa scores for the two groups were0.70 and 0.68, respectively, indicating substantialagreement between our annotators.The results we present in the remainder of thissection rely upon the following parameter values.The confidence thresholds used in the Label In-tegrator and filter are both set to 0.9 for positivelabels and 0.7 for negative and neutral labels.
Theminimum frequency used in the Lexicon Integra-tor for selecting triples is set to 4.4.1 Feature Function EvaluationOur first experiment evaluates the effects of dif-ferent combinations of features.
To do this, wefirst divide all features into four basic feature sets:T (transition cues), P (punctuations, special name-entities, and segment positions), G (grammar), andOD (opinion words and dependency relations).We train 15 sentiment classification models usingall basic features and their combinations.
Figure 7shows the results of a 10-fold cross validation onthe 200-review dataset (light grey bars show theaccuracy of the model trained without using tran-sition cue features).0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68OD+G+POD+POD+GG+PODGPTAccuracyFeaturew/o transition cues (T)w/ transition cues (T)Figure 7: Accuracy using different features.The feature OD yields the best accuracy, fol-lowed by G, P, and T. Although T yields the worstaccuracy, incorporating it improves the resultingaccuracy of the other features, as shown by thedark grey bars.
In particular, the accuracy of ODis markedly improved by adding T. The modeltrained using all the feature sets yields the best ac-curacy.4.2 Relationship Learners EvaluationOur second experiment evaluates the impact of therelationship learners and the label integrator.
Tothis end, we train and compare sentiment classifi-cation models using three configurations.
The firstconfiguration (FW-L) uses only the FR learner; thesecond (BW-L) only the BR learner.
ALL-L usesboth the FR and BR learners, together with a labelintegrator.
We evaluate them with 10-fold cross547validation on the 200-review dataset.Accuracy Macro F-score Micro F-score0.460.480.50.520.540.560.580.60.620.640.660.68FW-LBW-LBothFigure 8: Comparison among the learners.Figure 8 reports the accuracy, macro F-score,and micro F-score.
It shows that the BR learnerproduces better accuracy and a micro F-score thanthe FR learner but a slightly worse macro F-score.Jointly considering both learners with the label in-tegrator achieves better results than either alone.The results demonstrate the effectiveness of oursentiment labeling component.4.3 Domain-Specific Lexicon AssessmentOur third experiment evaluates the quality of thedomain-specific lexicon automatically generatedby ReNew.
To do this, we first transform eachof the 200 labeled reviews into feature vectors.Then we retrain Logistic Regression models us-ing WEKA (Hall et al, 2009).
Note that we useonly the features extracted from the lexicons them-selves.
This is important because to compare onlythe lexicons?
impact on sentiment classification,we need to avoid the effect of other factors, suchas syntax, transition cues, and so on.
We com-pare models trained using (1) our domain-specificlexicon, (2) Affective Norms for English Words(ANEW) (Bradley and Lang, 1999), and (3) Lin-guistic Inquiry and Word Count (LIWC) (Tausczikand Pennebaker, 2010).
ANEW and LIWC arewell-known general sentiment lexicons.Table 4 shows the results obtained by 10-foldcross validation.
Each weighted average is com-puted according to the number of segments ineach class.
The table shows the significant advan-tages of the lexicon generated by ReNew.
ANEWachieves the highest recall for the positive class,but the lowest recalls in the negative and neutralclasses.
Regarding the neutral class, both ANEWand LIWC achieve poor results.
The weighted av-erage measures indicate our lexicon has the high-est overall quality.Our domain-specific lexicon contains dis-tinguishable aspects associated with sentimentwords.
For example, the aspect ?staff?
is associ-ated with positive words (e.g., ?nice,?
?friendli,??help,?
?great,?
and so on) and negative words(e.g., ?okai,?
?anxiou,?
?moodi,?
?effici,?
and soon).
We notice that some positive words also occuron the negative side.
This may be for two reasons.First, some sentences that contain positive wordsmay convey a negative sentiment, such as ?Thestaff should be more efficient.?
Second, the boot-strapping process in ReNew may introduce somewrong words by mistakenly labeling the sentimentof the segments.
These challenges suggest usefuldirections for the future work.4.4 Lexicon Generation and SentimentClassificationOur fourth experiment evaluates the robustness ofReNew?s lexicon generation process as well as theperformance of the sentiment classification mod-els using these lexicons.
We first generate tendomain-specific lexicons by repeatedly followingthese steps: For the first iteration, (1) build a train-ing dataset by randomly selecting 20 labeled re-views (about 220 segments) and (2) train the learn-ers using the training dataset and LIWC.
For eachiteration thereafter, (1) label 400 reviews from theunlabeled dataset (4,071 reviews) and (2) updatethe lexicon and retrain the learners.
After labelingall of the data, output a domain-specific lexicon.To evaluate the benefit of using domain-specificsentiment lexicons, we train ten sentiment classifi-cation models using the ten lexicons and then com-pare them, pairwise, against models trained withthe general sentiment lexicon LIWC.
Each modelconsists of an FR learner, a BR learner, and a la-bel integrator.
Each pairwise comparison is eval-uated on a testing dataset with 10-fold cross vali-dation.
Each testing dataset consists of 180 ran-domly selected reviews (about 1,800 segments).For each of the pairwise comparisons, we conducta paired t-test to determine if the domain-specificsentiment lexicon can yield better results.Figure 9 shows the pairwise comparisons of ac-curacy between the two lexicons.
Each groupof bars represents the accuracy of two sentimentclassification models trained using LIWC (CRFs-General) and the generated domain-specific lexi-con (CRFs-Domain), respectively.
The solid linecorresponds to a baseline model that takes the ma-548Table 4: Comparison results of different lexicons.ANEW LIWC ReNewPrecision Recall F-Measure Precision Recall F-Measure Precision Recall F-MeasurePositive 0.59 0.994 0.741 0.606 0.975 0.747 0.623 0.947 0.752Negative 0.294 0.011 0.021 0.584 0.145 0.232 0.497 0.202 0.288Neutral 0 0 0 0 0 0 0.395 0.04 0.073Weighted average 0.41 0.587 0.44 0.481 0.605 0.489 0.551 0.608 0.518jority classification strategy.
Based on the dis-tribution of the datasets, the majority class of alldatasets is positive.
We can see that models usingeither the general lexicon or the domain-specificlexicon achieve higher accuracy than the baselinemodel.
Domain-specific lexicons produce signif-icantly higher accuracy than general lexicons.
Inthe figures below, we indicate significance to 10%,5%, and 1% as ??
?, ??
?, and ???
?, respectively.P1(??)
P2(??)
P3(?)
P4(?)
P5(?)
P6(?)
P7(??)
P8(?)
P9(?)
P10(??
)0.570.590.610.630.650.670.69Comparing PairsAccuracyCRFs-GeneralCRFs-DomainBaselineFigure 9: Accuracy with different lexicons.P1(?)
P2(??)
P3(?)
P4() P5(?)
P6(?)
P7(??)
P8(?)
P9() P10(??
)0.420.430.440.450.460.470.48Comparing PairsMacroF-scoreCRFs-GeneralCRFs-DomainFigure 10: Macro F-score with different lexicons.Figure 10 and 11 show the pairwise compar-isons of macro and micro F-score together withthe results of the paired t-tests.
We can see that thedomain-specific lexicons (dark-grey bars) consis-tently yield better results than their correspondinggeneral lexicons (light-grey bars).P1(??)
P2(??)
P3(?)
P4(?)
P5(?)
P6(?)
P7(??)
P8(?)
P9(?)
P10(??
)0.540.550.560.570.580.590.6Comparing PairsMicroF-scoreCRFs-GeneralCRFs-DomainFigure 11: Micro F-score with different lexicons.ReNew starts with LIWC and a labeled datasetand generates ten lexicons and sentiment classifi-cation models by iteratively learning 4,017 unla-beled reviews without any human guidance.
Theabove results show that the generated lexiconscontain more domain-related information than thegeneral sentiment lexicons.
Also, note that the la-beled datasets we used contain only 20 labeled re-views.
This is an easy requirement to meet.4.5 Comparison with Previous WorkOur fifth experiment compares ReNew withLazaridou et al?s (2013) approach for sentimentclassification using discourse relations.
Like Re-New, Lazaridou et al?s approach works on thesub sentential level.
However, it differs from Re-New in three aspects.
First, the basic units oftheir model are elementary discourse units (EDUs)from Rhetorical Structure Theory (RST) (Mannand Thompson, 1988).
Second, their model con-siders the forward relationship between EDUs,whereas ReNew captures both forward and back-ward relationship between segments.
Third, theyuse a generative model to capture the transitiondistributions over EDUs whereas ReNew uses adiscriminative model to capture the transition se-quences of segments.EDUs are defined as minimal units of text andconsider many more relations than the two types549Table 5: Comparison of our framework with pre-vious work on sentiment classification.Method AccuracyEDU-Model (Lazaridou et al) 0.594ReNew (our method) 0.605of transition cues underlying our segments.
Weposit that EDUs are too fine-grained for sentimentanalysis.
Consider the following sentence fromLazaridou et al?s dataset with its EDUs identified.
(1) My husband called the front desk (2) to com-plain.Unfortunately, EDU (1) lacks sentiment andEDU (2) lacks the topic.
Although Lazaridou etal.
?s model can capture the forward relationshipbetween any two consecutive EDUs, it cannot han-dle such cases because their model assumes thateach EDU is associated with a topic and a senti-ment.
In contrast, ReNew finds just one segmentin the above sentence.Just to compare with Lazaridou et al, we ap-ply our sentiment labeling component at the levelof EDUs.
Their labeled dataset contains 65 re-views, corresponding to 1,541 EDUs.
Since thisdataset is also extracted from Tripadvisor, we usethe domain-specific lexicon automatically learnedby ReNew based on our 4,071 unlabeled reviews.Follow the same training and testing regimen (10-fold cross validation), we compare ReNew withtheir model.
As shown in Table 5, ReNew outper-forms their approach on their dataset: AlthoughReNew is not optimized for EDUs, it achieves bet-ter accuracy.5 Related WorkTwo bodies of work are relevant.
First, to gener-ate sentiment lexicons, existing approaches com-monly generate a sentiment lexicon by extend-ing dictionaries or sentiment lexicons.
Hu andLiu (2004), manually collect a small set of sen-timent words and expand it iteratively by search-ing synonyms and antonyms in WordNet (Miller,1995).
Rao and Ravichandran (2009) formalizethe problem of sentiment detection as a semi-supervised label propagation problem in a graph.Each node represents a word, and a weighted edgebetween any two nodes indicates the strength ofthe relationship between them.
Esuli and Sebas-tiani (2006) use a set of classifiers in a semi-supervised fashion to iteratively expand a manu-ally defined lexicon.
Their lexicon, named Senti-WordNet, comprises the synset of each word ob-tained from WordNet.
Each synset is associatedwith three sentiment scores: positive, negative,and objective.Second, for sentiment classification, Nakagawaet al (2010) introduce a probabilistic model thatuses the interactions between words within onesentence for inferring sentiments.
Socher et al(2011) introduce a semi-supervised approach thatuses recursive autoencoders to learn the hierarchi-cal structure and sentiment distribution of a sen-tence.
Jo and Oh (2011) propose a probabilis-tic generative model named ASUM that can ex-tract aspects coupled with sentiments.
Kim et al(2013) extend ASUM by enabling its probabilis-tic model to discover a hierarchical structure ofthe aspect-based sentiments.
The above worksapply sentence-level sentiment classification andtheir models are not able to capture the relation-ships between or among clauses.6 Conclusions and Future WorkThe leading lexical approaches to sentiment anal-ysis from text are based on fixed lexicons that arepainstakingly built by hand.
There is little a priorijustification that such lexicons would port acrossapplication domains.
In contrast, ReNew seeksto automate the building of domain-specific lexi-cons beginning from a general sentiment lexiconand the iterative application of CRFs.
Our resultsare promising.
ReNew greatly reduces the humaneffort for generating high-quality sentiment lexi-cons together with a classification model.
In fu-ture work, we plan to apply ReNew to additionalsentiment analysis problems such as review qual-ity analysis and sentiment summarization.AcknowledgmentsThanks to Chung-Wei Hang, Chris Healey, JamesLester, Steffen Heber, and the anonymous review-ers for helpful comments.
This work is supportedby the Army Research Laboratory in its Net-work Sciences Collaborative Technology Alliance(NS-CTA) under Cooperative Agreement NumberW911NF-09-2-0053 and by an IBM Ph.D. Schol-arship and an IBM Ph.D. fellowship.550ReferencesMargaret M. Bradley and Peter J. Lang.
1999.
Affec-tive norms for English words (ANEW): Instructionmanual and affective ratings.
Technical Report C-1,The Center for Research in Psychophysiology, Uni-versity of Florida, Gainesville, FL.Yejin Choi and Claire Cardie.
2009.
Adapting a po-larity lexicon using integer linear programming fordomain-specific sentiment classification.
In Pro-ceedings of the 14thConference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 590?598, Singapore.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5thConference on LanguageResources and Evaluation (LREC), pages 449?454,Genoa, Italy.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-WordNet: A publicly available lexical resource foropinion mining.
In Proceedings of the 5thCon-ference on Language Resources and Evaluation(LREC), pages 417?422, Genoa, Italy.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An up-date.
SIGKDD Explorations Newsletter, 11(1):10?18, November.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thInternational Conference on Knowledge Discoveryand Data Mining (KDD), pages 168?177, Seattle.Yohan Jo and Alice Haeyun Oh.
2011.
Aspect and sen-timent unification model for online review analysis.In Proceedings of the 4thACM International Con-ference on Web Search and Data Mining (WSDM),pages 815?824, Hong Kong.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.Fully automatic lexicon expansion for domain-oriented sentiment analysis.
In Proceedings of the11thConference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 355?363,Sydney.Suin Kim, Jianwen Zhang, Zheng Chen, Alice H.Oh, and Shixia Liu.
2013.
A hierarchical aspect-sentiment model for online reviews.
In Proceed-ings of the 27thAAAI Conference on Artificial In-telligence (AAAI), pages 804?812, Bellevue.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the 18thInter-national Conference on Machine Learning (ICML),pages 282?289, San Francisco.Angeliki Lazaridou, Ivan Titov, and CarolineSporleder.
2013.
A Bayesian model for jointunsupervised induction of sentiment, aspect anddiscourse representations.
In Proceedings of the51stAnnual Meeting of the Association for Compu-tational Linguistics (ACL), pages 1630?1639, Sofia,Bulgaria.Bing Liu.
2012.
Sentiment Analysis and OpinionMining.
Synthesis Lectures on Human LanguageTechnologies.
Morgan & Claypool Publishers, SanRafael, CA.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.George A. Miller.
1995.
WordNet: A lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classifica-tion using CRFs with hidden variables.
In Proceed-ings of the 11thAnnual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), pages 786?794, Los Angeles.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification us-ing machine learning techniques.
In Proceedings ofthe 7thConference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 79?86,Philadelphia.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceed-ings of the 12thConference of the European Chap-ter of the Association for Computational Linguistics(EACL), pages 675?682, Athens.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of the16thConference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 151?161,Edinburgh.Yla R. Tausczik and James W. Pennebaker.
2010.
Thepsychological meaning of words: LIWC and com-puterized text analysis methods.
Journal of Lan-guage and Social Psychology, 29(1):24?54, March.Lei Zhang, Bing Liu, Suk Hwan Lim, and EamonnO?Brien-Strain.
2010.
Extracting and ranking prod-uct features in opinion documents.
In Proceedingsof the 23rdInternational Conference on Compu-tational Linguistics (COLING), pages 1462?1470,Beijing.551
