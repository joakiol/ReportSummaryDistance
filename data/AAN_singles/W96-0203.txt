!Unsupervised Learning of Syntactic Knowledge:methods and measuresa.
Basili (*), A. Marziali (*), M.T.
Pazienza (*), P. Ve lard i (#)(*) Dipartimento di Informatica, Sistemi eProduzione, Universita' di Roma Tor Vergata(ITALY) {basili, pazienza}@info .utovrm.
it(#) Istituto di Informatica, Universita' di Ancona(ITALY)velardi@anvax 1. c ineca, itAbstractSupervised methods for ambiguity resolution learn in"sterile" environments, in absence of syntactic noise.However, in many language engineering applicationsmanually tagged corpora are not available nor easilyimplemented.
On the other side, the "exportability" ofdisambiguation cues acquired from a given, noise-free,domain (e.g.
the Wall Street Journal) to other domainsis not obvious.Unsupervised methods of lexical learning have, justas well, many inherent limitations.
First, the type ofsyntactic ambiguity phenomena occurring in real do-mains are much more complex than the standard VN PP patterns analyzed in literature.
Second, espe-cially in sublanguages, yntactic noise seems to be asystematic phenomenon, because many ambiguities oc-cur within identical phrases.
In such cases there is littlehope to acquire a higher statistical evidence of the cor-rect attachment.
Class-based models may reduce thisproblem only to a certain degree, depending upon therichness of the sublanguage, and upon the size of theapplication corpus.Because of these inherent difficulties, we believe thatsyntactic learning should be a gradual process, in whichthe most difficult decisions are made as late as possible,using increasingly refined levels of knowledge.In this paper we present an incremental, class-based,unsupervised method to reduce syntactic ambiguity.We show that our method achieves a considerable com-pression of noise, preserving only those ambiguous pat-terns for which shallow techniques do not allow reliabledecisions.Unsupervised vs. supervised models ofsyntactic learningSeveral corpus-based methods for syntactic ambiguityresolution have been recently presented in the litera-ture.
In (Hindle and Rooth, 1993) hereafter H&R, lexi-calized rules are derived according to the probability ofnoun-preposition r verb-preposition bigrams for am-biguous tructures like verb-noun-preposition-noun se-quences.
This method has been criticised because itdoes not consider the PP object in the attachment de-cision scheme.
However collecting bigrams rather thantrigrams reduces the well known problem of data sparse-ness.In subsequent s udies, trigrams rather than bigramswere collected from corpora to derive disambiguationcues.
In (Collins and Brooks,1995) the problems of datasparseness is approached with a supervised back-offmodel, with interesting results.
In (Resnik and Hearst,1993) class-based trigrams are obtained by generalizingthe PP head, using WordNet synonymy sets.
In (Rat-naparkhi et al 1994) word classes are derived automat-ically with a clustering procedure.
(Franz, 1995) uses aloglinear model to estimate preferred attachments ac-cording to the linguistic features of co-occurring words(e.g.
bigrams, the accompanying noun determiner,etc.).
(Brill and Resnik, 1994) use transformation-based error-driven learning (Brill, 1992) to derive dis-ambiguation rules based on simple context information(e.g.
right and left adjacent words or POSs).All these approaches need extensive collections ofpositive examples (i.e.
hand corrected attachment in-stances) in order to trigger the acquisition process.Probabilistic, backed-off or loglinear models rely en-tirely on noise-free data, that is, correct parse trees orbracketed structures.
In general the training set is theparsed Wall Street Journal (Marcus et al 1993), withfew exceptions, and the size of the training samplesis around 10-20,000 test cases.
Some methods do notrequire manually validated PP attachments, but word2Scollocations are collected from large sets of noise-freedata.
Unfortunately, in language ngineering applica-tions, manually tagged corpora are not widely avail-able nor easily implemented 1.
On the other side, the"exportability" of disambiguation cues obtained in agiven domain (e.g.
WSJ) to other domains is not obvi-ous.Unsupervised methods have, on their side, seriouslimitations:* First, the type of occurring syntactic ambiguityphenomena re in the average much more com-plex than the standard verb-noun-preposition-nounpatterns analyzed in literature.
H&R method hasbeen proved very weak on complex phenomenalike verb-noun-preposition-noun-preposition-noun se-quences (see (Franz,1995)).
Other methods (super-vised or not) do not consider more complex ambigu-ous structures.. Second, in real environments, and especially in sunlanguages, syntactic noise seems to be a systematicphenomenon.
Many ambiguities occur within severalidentical phrases, hence the "wrong" and the "right"associations may gain the same statistical evidence.Therefore, there are intrinsic limitations to the pos-sibility of using purely statistical approaches to am-biguity resolution.The nature of ambiguous phenomena in untaggedcorpora has not been studied in detail in the literaturealthough one such analysis would be very useful on alanguage ngineering standpoint.
Accordingly, section2 is devoted to an experimental nalysis of complex-ity and recurrence of ambiguous phenomena in sub-languages.
This analysis demonstrates that syntacticdisambiguation i large cannot be afforded by the useof knowledge induced exclusively from the corpus.
Wethink that corpus based techniques are useful to signifi-cantly reduce, not to eliminate, the ambiguous phenom-ena.
In section 3, we describe an unsupervised, class-based, incremental, syntactic disambiguation methodthat is aimed at reducing noisy collocates, to the ex-tent that this is allowed by the observation of corpusphenomena.
The approach that we support is to re-duce syntactic ambiguity through an incremental pro-cess.
Decisions are deferred until enough evidence hasbeen gained of a noisy phenomenon.
First, a kernel ofshallow grammatical competence is used to extract acollection of noise-prone syntactic ollocates.
Then, aglobal data analysis is performed to review local choicesand derive new statistical distributions.
This incremen-tal process can be iterated to the point that the system1 It is not just a matter of time, but also of requiredlinguistic skills (see for example (Marcus et al 1993)).reaches a kernel of "hard" cases for which there is nomore evidence for a reliable decision.
The output ofthe last iteration represents a less noisy environmenton which additional learning process can be triggered(e.g.
sense disambiguation, acquisition of subcatego-rization frames, ...).
These later inductive phases mayrely on some level of a priori knowledge, like for exam-ple the naive case relations used in the ARIOSTO_LEXsystem (Basili et al 1993c , 1996).Complex i ty  and  recur rence  o fambiguous  pat terns  in  corporaIn the previous section we pointed out that unsuper-vised lexical learning methods must cope with complexand repetitive ambiguities.
We now describe an exper-iment to measure these phenomena in corpora.
In thisexperiment, we wish to demonstrate that:The type of syntactic ambiguities are much morecomplex than V N PP or N N PP sentences.
In arealistic environment, he correct attachment mustbe selected among several possibilities, not just two.The fundamental ssumption of most common statis-tical analyses is that the events being analyzed (pro-ductive word pairs or triples in our case) are indepen-dent.
Instead, ambiguous patterns are highly repeti-tive, especially in sublanguages.
This means that inmany cases, unless we work in absence of noise, the"correct" and "wrong" associations in an ambiguousphrase acquires the same or similar statistical evi-dence.To conduct he experiment, we used a shallow syntac-tic analyzer (SSA) (Basili et al 1994) to extract wordassociations from two very different corpora in Italian(a scientific corpus of environmental bstracts, calledENEA, and a legal corpus on taxation norms, calledLD) 2Given a corpus, SSA produces an extensive databaseof elementary syntactic links (esl).
Typical eslclasses express the following dependency relations:noun-preposition-noun (N_P_N), verb-preposition-noun(V_P_N), adjective-conjunction-adjective (Adj_C_Adj)and others.
An esl has the following structure:esl( h, mod(p, w) ),where h is the head of the underlying phrasal structureand rood(p, w) denotes the head modifier, and w as themodifier head.Ambiguity is generated by multiple morphologicderivations and intrinsic language ambiguities (PP ref-erences, coordination, etc.).
Given a sentence, SSA2SSA is based on a DCG model with controlled skip rules24Iproduces in general a noise-prone set of esl's, some ofwhich represent colliding interpretations.
The defini-tion of Collision Set (CS) is the following:DEF(Co l l i s ion  Set):  A Collision Set (CS) is the setof syntactic groups, derived from a given sentence thatshare the same modifier, mod O.To smooth the weight of ambiguous esl's in lexicallearning, each detected esl is weighted by a measurecalled plausibility.
To simplify, the plausibility of a de-tected esl is roughly inversely proportional to the num-ber of mutually excluding syntactic structures in thetext segment hat generated the esl (see (Basili et al1993a) for details).In the following, we show examples of collision setsextracted from the LD (an English word by word trans-lation is provided for the sentence fragments that gen-erated a collision set).
It is important o observe thatthe complexity does not arise simply from the numberof colliding tuples but also from the structure of am-biguous patterns (e.g.
non consecutive word strings, asin the second example).
Bold characters identify therood(p, w) shared by colliding tuples.
Local plausibilityvalues are reported on the right.1.
Examples of Simple Collision sets:1.1 Minimal  A t tachment  (consecutive wordstrings):su richiesta del ministro per le finanze , il \[ ( servizio divigilanza sulle aziende) di credito \] (* service of control ofagencies of credit ) controlla l'esattezza delle attestazionicontenute nel certificato .g_N_p_N(2,azienda,di,credito) 0.333g_N _p_N (4,vigilanza,di,credi to) 0.333g_N_p_N (6,servizio,di,credito) 0.3331.2 Non-Minimal  At tachment  (non consecutiveword strings)i sostituti d imposta devono \[(presentare la dichiarazionedi-cui-a quarto comma dell'articolo 9, relativamente ai paga-menti fatti e agli utili distribuiti nell'anno 1974) entro il15- aprile- 1975 \].
(* must present the declaration of whichat comma 4th of item 9, relatively to the payment done andthe profit distributed in the year 1974,within april 15,1974 )g_N_p_N (17,articolo,entro,x_15_aprile_1975) 0.166g_N_p_N(7,distribuire,entro,x_l 5_aprile_ 1975) 0.166g_Adv_p_N(14,relativamente,entro,x_15_aprile_1975)0.166g_N_p_N(19,comma,entro,x_15_aprile_ 1975) 0.166g_V_p..N(24,presentare,entro,x_15_aprile_ 1975) 0.166To measure the complexity of the ambiguous truc-tures, we collected from fragments of the two corporaall the ambigous collision sets, i.e.
those with morethan one esl.
10,433 collision sets were found in theENEA corpus and 30,130 in the LD 3.
Figure 1 plotsthe percentage of colliding esl~s vs. the cardinality ofcollision sets.
The average size of ambiguous collisionsets is about 4 in both corpora.Of course SSA introduces additional noise due to itsshallow nature (see referred papers for an evaluationof performances4), but as far as our experiment is con-cerned (measuring the complexity of collision sets) SSAstill provides a good testbed.
In fact, some esl can bemissed in a collision set, or some spurious attachmentcan be detected, but in the average, these phenomenaare sufficiently rare and in any case they tend to beequally probable.C~30 -?20"10"OI1t~7 10 13 16CS SizeFigure 1: Percentage ofcollision sets Vs. number of collid-ing tuples for the LD.In the second experiment we measure the recurrenceof ambiguous patterns.
This phenomenon is knownto be typical in sublanguage, but was never analyzedin detail.
A straightforward measure of recurrence isprovided by the average Mutual Information of collid-ing esl's.
This figure measures the probability of co-occurrence of two esl's in a collision set.
If the Mu-tual Information is high, it means that the measuredphenomena (productive word tuples) do not indepen-dently occur in collision sets, i.e.
they systematicallyoccur in reciprocal ambiguity in the corpus.
The conse-quence is that statistically based lexical learning meth-ods are faced not only with the problem of data sparse-ness (events that are never or rarely encountered), butalso with the problem of systematic ambiguity (events3The LD test corpus is larger, and in addition, the legallanguage is more verbous and less concise than the scientificstyle that characterizes the ENEA corpus.4We measured an average of 80% precision and 75% recallover three corpora, one of which in English.25Table 1: Mutual Information of co-occurring esl'sAverage MIITaverage frequency of esl'sLD ENEA(30,130 CS) (10,433 CS)13.65 12.91.8 0.843.2 .721.9 1.43Table 3: Mutual Information of right-generalized sl's intwo domainsAverage MIIT0.
2LD(all esl's)11.53.109.62LD(high freq.esl's)2.154.66ENEA(all esl's )11.002.657.05(hisTable 2: Mutual Information of esl's occurring with fre-quency higher than averageLD ENEAAverage MI 11.60 11.60a 2.05 1.12a z 4.23 1.27that occur always in the same sequence).
This phe-nomenon is likely to be more relevant in sublanguages(medicine, law, engineering) than in narrative texts, butsublanguages are at the basis of many important appli-cations.The average Mutual Information was evaluated byfirst computing, in the standard way, the Mutual Infor-mation of all the pairs of esl's that co-occurred in atleast one collision set:Prob(esli, eslj)Mr(est,, esl~) = log2 Prob(esl~)Prob(esl~) (1)where the probability is evaluated over the space ofcollision sets with cardinality > 1.Tables 1 and 2 summarize the results of the exper-iment.Tables 1 and 2 show the average MI ,  standarddeviation and variance for the two domains.
The valuesin 1 shows that the average MI is close to the perfectcorrelation 5 and has a small variance, especially in theENEA corpus that is in technical style.
This resultcould be biased by the esl's occurring just once in thecollision sets, hence we repeated the computation forthe pair of esFs occurring at a frequency higher thanthe average (> 2, in both domains).
The results arereported in Table 2.
It is seen that the values remainrather high, still with a small variance.Clustering the esl~s would seem an obvious way toreduce this problem.
Therefore, in a subsequent exper-iment we clustered the head of PPs in the collision setsusing a set of high-level semantic tags (for a discussionSTwo esl's occurring exactly as the average (1.9 in LD)are in perfect correlation when their MI is equal to 13.8.on semantic tagging see (Basili et al 1992, 1993b) 6.
Forexample, the eslV_P_N ( to_present, within, apriL15_1974 )is generalized as:V_P_N ( to_present, within, TEMPORAL.ENT ITY) .Because of sense ambiguity, the collision sets became20,353 in the ENEA corpus, and 42,681 in the LD.
Theaverage frequency of "right-generalized" esl~s is now4.28 in the ENEA and 4.64 in the LD.
The results aresummarised in Table 3.Notice that the phenomenon of systematic ambiguityis much less striking (lower MI  and higher variance),though it is not eliminated.
It is also important hatthe two corpora, though very different in style, behavein the same way as far as systematic ambiguity is con-cerned.For example, consider the following sentence frag-ment:... imposta sul reddito delle persone ... ( *... tax on theincome of people ...)that occurs in the LD corpus almost 200 times.The global plausibility of the syntactic collocates (i)imposta-di-persona (tax-of-people) and (ii) reddito-di-persona (income-of-people)is (i) 91.66 and (ii) 93.69.Therefore a reliable decision is not allowed by the setof syntactic observations found in the corpus.
Further-more, similar sentences, like for example... imposta sul reddito delle societa'... (*tax on theincome of companies...),always have a HUMAN_ENTITY as head modifier.Therefore, the fact that (reddito di persona) is correctcannot be captured even when comparing the general-ized patterns (reddito di HUMAN_ENTITY) and (im-posta di HUMAN_ENTITY).6Class based approaches are widely employed.
Clustersare created by means of distributional techniques in (Rat-naparkhi et al 1994), while in (Resnik and Hearst, 1993)low level synonim sets in WordNet are used.
Instead, we usehigh level tags (human, time, abstraction etc.
), manually as-signed in Itafian domains and automatically assigned fromWordNet in English domains.
For sake of brevity, we do notre-discuss the matter here.
See aforementioned papers.26The conclusion we may derive from these two exper-iments is that most syntactic disambiguation methodspresented in literature are tested in an unrealistic en-vironment.
This does not mean that they don't work,but simply that their applicability to real domains isyet to be proven.
Application corpora are noisy, maynot be very large, and include repetitive and complexambiguities that are an obstacle to reliable statisticallearning.The experiments also stress the importance of classbased models of lexical learning.
Clustering "similar"phenomena is an obvious way of reducing the problemsjust outlined.
Unfortunately, Table 3 shows that gener-alization improves, but not eliminates, the problem ofrepetitive patterns.An  incrementa l  a rch i tec ture  fo runsuperv ised  reduct ion  o f  syntact i cambigu i tyThe previous section shows that we need to be morerealistic in approaching the problem of syntactic am-biguity resolution in large.
Certain results can be ob-tained with purely statistical methods, but there aremany complex cases for which there seems to be a clearneed for less shallow techniques.The approach that we have undertaken is to attackthe problem of syntactic ambiguity through increasinglyrefined learning phases.
The first stage is noise com-pression, in which we adopt an incremental syntacticlearning method, to create a more suitable frameworkfor subsequent steps of learning.
Noise compression isperformed essentially by the use of shallow NLP andstatistical techniques.
This method is described here-after, while the subsequent s eps, that use deeper (rule-based) levels of knowledge, are implemented into theARIOSTO_LEX lexical learning system, described in(Basili et al, 1993b, 1933c and 1996).A feedback  a lgor i thm for  no ise  reductionThe process of incremental noise reduction works asfollows:1.
First, use a surface grammatical competence (i.e.SSA) to derive the (noise prone) set of observations.2.
Cluster the collocational data according to semanticcategories.3.
Apply class based disambiguation perators to reducethe initial source of noise, by first disambiguating thenon-persistent ambiguity phenomena.4.
Derive new statistical distributions.5.
Repeat step 2.-4. on the remaining (i.e.
persistent)ambiguous phenomena.The incremental disambiguation activity stops whenno more evidence can be derived to solve new ambigu-ous cases .In order to accomplish the outlined noise reductionprocess we need: (i) a disambiguation operator and(ii) a disambiguation strategy to eliminate at each step"some" noisy collocations.The class based disambiguation operator is theMutual Conditioned Plausibility (MCPI) (Basili etal.,1993a).
Given an esl, the value of its correspond-ing MCPl  is defined by the following:DEF(Mutual Conditioned Plausibility): The MutualConditioned Plausibility (MCP1) of a prepositional t-tachment esl(w, mod(p, n)), is:M C Pl(esl( w, rood(p, n ) ) =~yer  pl(esl(w, mod(p, y) ) )~vh,yer pl(esl(h, mod(p, y) ) ) ~v~ pl(esl(w, mod(p, y) ) ) (2)where F is the high level semantic tag assigned to the mod-ifier n and pl 0 is the plausibility function.
Examplesof the generalized esl's were presented in the previoussection.
For example to the computation of the MCPIof esl(reddito,(di, persona)) contribute esl's likeesl (reddito, (di, pro f essionista ) ), esl ( reddito, ( di, azienda)where professionista, persona and azienda are in-stances of HUMAN_ENTIT IY .After a first scan of the corpus by the SSA and af-ter the computation of global MCPI values, a primaryknowledge base is available.
This knowledge is fullycorpus driven, and it is obtained without a preliminarytraining set of hand tagged patterns.
Each esl in a colli-sion set has its own MCP1 value, that has been globallyderived from the corpus.
The MCPI is thus employedto remove the less plausible attachments proposed bythe grammar, with a consequent reduction in size of therelated collision sets.
When more than one esl remainin a collision set the system is not forced to decide, anda further disambiguation step is attempted later.After the first scan of the corpus by means of the SSAgrammar, the corpus is re-written as a set of possiblyambiguous Collision Sets, i.e.
if C is the corpus andCSi a Collision Set, we have:C = CSo  U CSx U ... W CS~ U ...CSNCSiNCS j={O},  fo r i?
j , i , j=O,  1,2,...Nwhere N is the total number of collision sets found inthe corpus.The cardinality of a generic collision set is directlyproportional to the degree of ambiguity of its members.The feedback algorithm tries to reduce the cardinality27Table 4: A general feedback algorithm for noise reduction Table 5: Disambiguation Algorithm: Learning Phase(1) Use SSA to derive all the syntactic observations Ofrom the corpus;Set the initial performance index PFC' to 0;(2) REPEAT(2.1) Substitute PCF with PCF'(2.2) Evaluate the MCPI for each esl E 0(2.3) Use MCP1 on a subset of the corpus (testset)and evaluate the current performanceindex PCF'(2.3) IF PCF' > PCF THEN:(2.3.1) Rewrite the collision sets of Oremoving hell esllsinto a new set of observation O'(2.3.2) Replace O with O'UNTIL PCF' > PCF(3) STOPLet CS = { el,e2,.. .eN } be anycollision set in the corpus, where e~s are esl'sLet -~ be the prior probability (pprior).Let MCPI(ei) be the Mutual ConditionalPlausibility (2) of eiThe posterior probability of el, pposti,is defined asMCPI(ei) pposti = z--,=~'jN1 MCP|(ej)Let a E \[0, 1\] be a given learning threshold.For each ei in CS DO:IF ~ < 1 -aTHEN ppriorREMOVE ei from CS, i.e.
PUT itin the hell setOTHERWISE ei is a limbo esl.IF Vi ?
j ei is in hellMOVE ej in the paradise setof all CSi step by step: esl with "lower" MCPI val-ues (as globally derived from all the corpus) are filteredout; the MCP1 values are then redistributed among theremaining esl~s.
In a picturesque way, we can say thatdiscarded esl~s are damned (the hell is the right place),while survived esl~s are waiting for next judgment (thelimbo is the right place for this wait state); at the end ofthe algorithm, if there is a single winner esl, it will gainthe paradise.
Persistently ambiguous esl of the corpusmay remain still ambiguous within the correspondingcollision sets: limbo will be their place forever.
The al-gorithm will try to obtain as many paradise esl~s (i.e.singleton CS) as possible but is robust against persis-tently ambiguous phenomena.The general feedback algorithm is illustrated in Table4.
It should be noted that the above feedback strategyhas three main phases: (step 2.2) statistical inductionof syntactic preference scores; (step 2.3) testing phase(which is necessary in order to quantify the performanceof disambiguation criteria derived from the current sta-tistical distributions); (step 2.3.1) learning phase, to fil-ter out the syntactically odd esl~s (i.e.
esl with locallylow MCP1 values).Learn ing  and  Test ing  d i sambiguat ion  cuesAccording to the disambiguate as late as possible strat-egy, the learning and testing phases have different ob-jectives:During the learning phase, the objective is to takeonly highly reliable decisions, by eliminating thoseesl's with a very low plausibility, while delaying un-reliable choices.?
During the test phase, the objective is to evaluate theability of the system at separating, within each colli-sion set, correct from wrong attachment candidates.This results in two different disambiguation algo-rithms: the learning phase is used only to remove hellesl's from the collision sets, without forcing any par-adise choice (e.g.
a maximum likelihood candidate).
Inthe test phase eslls are classified as (locally) correct andwrong according to their relative values of MCPI.The learning phase, called ith -learning step, is guidedby the following algorithm:1.
Identify all Collision Sets of the corpus, CSi, i =1,2, ...N;2.
Apply the preference criterion to each CSi in orderto classify hell, limbo or paradise esl' s;3.
Redistribute plausibility values among the limboesl's of each CSi;Step 2 is further specified in Table 5.In step 3 of the Learning algorithm, the new plausi-bility values are redistributed among the survived esl'saccording to the following rule:pli (CSi)pli+l (esl(h, mod(p, w))) = pli pli+l (CSi+i) (3)where i is the learning step and CSi+i (C CSi) does notcontain esl's that have been placed in hell during stepi.After each learning step the upgraded plausibility val-ues provide newer MCP1 scores that are more reliablebecause the hell esFs have been discarded.28Table 6: Disambiguation Algorithm: Learning PhaseLet CS= { e l ,e2 , .
.
.eN  } be anycollision set the test setand Ncases be the number of test cases.Let -~ be the prior probability (pprior).Let MCPl(ei) be the MutualConditional Plausibility (2) of eiThe posterior probability of el, pposti, is defined as-.
__ MCPI(ei)ppos , -Let r E \[0, 11 be a given test threshold.For each CS and for each ei E CS DO:e2_.ez/.t N IF -prior > 1 + r THE(F  ei is correct,  i.e.
manually validated, THEN++TruePositives;OTHERWISE++ FalsePositives;OTHERWISE IF ~ < 1 - r THENIF e~ is correct pp~,Or~HEN++ FalseNegatives;OTHERWISE++True Negatives;++Ncasesprecision =TruePositives-~ TrueNe~ativesTruePositives+ TrueNegatives+ FalsePositives?
FalseNegativesrecall =TruePositives-~ TrueNe~tativesNcasescoverage  =TruePositives~TrueNe~atives+ Fal ePositives+FalseNegativesNcasesThe evaluation of each learning step is carried on bytesting the syntactic disambiguation  a selected set ofcorpus sentences where ambiguities have been manuallysolved.The general test algorithm is defined in Table 6.In Table 6, notice that precision and recall evalu-ate the ability of the system both at eliminating trulywrong esl's and accepting truly correct esl~s, since, asremarked in section 2, our objective is noise compres-sion, rather than full syntactic disambiguation.
Noticealso that, because of their different classification objec-tives, learning and testing use different decision thresh-olds.Experimental Results.To evaluate numerically the benefits of the feedback al-gorithm, several experiments and performance indexeshave been evaluated.
The corpus selected for experi-menting the incremental technique is the LD: the sizeof the corpus is about 500,000 words.
The SSA gram-mar in LD has about 25 DCG rules and it generates29Table 7: Performance values of the MCP1 without learningr Coverage Recall i Precision0.0 99.8% 0.75 0.7490.05 95.0% 0.72 0.750.1 87.4% 0.69 0.790.2 77.8% 0.62 0.800.5 49.9% 0.42 0.84240,493 esl's from the whole corpus.
Of these only 10%of esl's are initially unambiguous, while all the remain-ing are limbo esl's.
A testset of 1,154 hand correctedcollision sets was built.
5,285 different esl's are in thetestset.
An average of 25.9% correct groups have beenfound in the testset, again demonstrating a great levelof ambiguity in the source data.At first, we need to study the system classificationparameters, ~r and r (see Tables (5) and (6)) .
Dur-ing the learning phase, we wish to eliminate as manyhell esl's as possible, because the more noise has beeneliminated from the source syntactic data, the more re-liable is the application of the later inductive operators(i.e.
ARIOSTO lexical learning system).
However weknow from the experiments in section 2 that the com-petence that we are using (shallow NLP and statisticaloperators) is insufficient o cope with highly repetitiveambiguities.
The threshold o" is therefore a crucial pa-rameter, because it must establish the best trade-offbetween precision of choices (i.e.
it must classify as helltruly noisy eslls) and impact on noise compression (i.e.it must remove as much noise as possible).Table 7 shows the results.To select he best value for ~r, we measured the valuesof recall and precision (defined in Table 6) according todifferent values for r. These measures have been derivedfrom the early (thus noisy) state of knowledge wherejust the SSA grammar, and no learning, was applied tothe corpus.According to the results of Table 7, r = 0.2 wasselected for the better trade-off between recall, pre-cision and coverage.
The learning steps have then beperformed with a threshold value o" = 0.2 over the LDcorpus.
In each phase the corresponding recall andprecision have been measured.The results of the experiment are summarised in Fig-ure 2.
Figure 2.A plots recall versus precision thathave been obtained in the early (prior to learning) stage(Step 0), after 1 (Step 1) and 2 (Step 2) learning iter-ations.
Each measure is evaluated for a different valueof the testing threshold r, that varies from 0.5 to 0.0from left to right in Fig.
2.A.Figure 2.B plots the Information Gain (Kononenkoand Bratko, 1991) an information theory index that,roughly speaking, measures the quality of the statisti-cal distributions of the correct vs. wrong esl's.
Fig-Table 8: Performance values of the LA without learningr Coverage Recall Precision'0.0 100% 0.610 0.6100.05 96.5% 0.594 0.615'0.1 93.8% 0.578 0.6160.2 86.4% 0.544 0.631"0.5 71.9% 0.465 0.647ure 2.C measures the Data Compression, that is themere reduction of eis's in the corpus.
The compres-sion is measured as the ratio between hell's els's andthe number of the observed esl's.
Figure 2.D plotsthe Coverage, i.e.
the number of decided cases overthe total number of possible decisions.
Finally, Table8 reports the performance (at the Step 0 phase) of theH&R Lexical Association (LA) 7.
We experiment thisdisambiguation operator just because the HLzR methodhas, among the others, the merit of being easily repro-ducible.The first four figures give a global overview of themethod.
In Fig.
2.A (Step 1), a significant improvementin precision can be observed.
For r = 0.5 the improve-ment in recall (.5) and precision (.85) is more sensible.Furthermore a better coverage (60 %) is shown in Fig.2.D (Step 1).
A further index to evaluate the status ofthe system knowledge about the PP-attachment prob-lem is the Information Gain ((Kononenko and Bratko,1991) and (Basili et al 1996)).
The posterior probabil-ity (see algorithms in Table 5 and 6) improves overthe "blind" prior probability as much as it increasesthe confidence of correct eslls and decreases the con-fidence of wrong esl~s.
The improvement is quantifiedby means of the number of saved bits necessary to de-scribe the correct decisions when moving from prior toposterior probability.
The Information Gain does notdepend on the selected thresholds, since it acts on allthe probability values, and it is related to the com-plexity of the learning task.
It gives a measure of theglobal trend of the statistical decision model.
A signif-icant improvement measured over the testset (12% to24% relative increment) is shown by Fig.
2.B as aresult of the learning steps.
As discussed in (Basili eta1.,1994), the Information Gain produces performanceresults that may contrast with precision and recall.In fact, in the learning step 2, we observed ecreasedperformance of precision and recall.
The overlearn-ing effect is common of feedback algorithms.
Further-more, the small size of the corpus is likely to anticipate7Unfike H&R, we did not use the t-score as a decisioncriteria, but forced the system to decide according to differ-ent values of the thresholds r for sake of readabifity of thecomparison.
Technical details of our treatment of the LAoperator within our grammatical framework can be foundin (Basili et a1,1994),30this phenomenon.
The problem is clearly due to thehighly repetitive ambiguities.
The system quickly re-moves from the corpus syntactically wrong esl's withlow MCP1.
But now let's consider a collision set withtwo esl's that almost constantly occur together.
TheirMCPI tends to acquire exactly the same value.
Thus,they will stay in the limbo forever.
But if one of the two,accidentally the wrong, has an even minimal additionalevidence with respect to its competitor, this initiallysmall advantage may be emphasized by the plausibilityredistribution rule 38 .
Hence once the learning algo-rithm reaches the "hard cases" and is still forced todiscriminate, it gets at stuck, and may take accidentaldecisions.
This phenomenon occurs very early in ourdomains, and this could be easily foreseen according tothe high correlation between esl's that we measured.For the current experimental setup, our data showa significant reduction of noise with a significant 40%compression of the data after step 1, and a correspon-dent slight improvement in precision-recall, given thecomplexity of the task (see the Lexical Association per-formance in Table 8, for a comparison).
However, thephenomena that we analyzed in Section 2 have a neg-ative impact on the possibility of a longer incrementallearning process.
We do not believe that experiment-ing over different domains would give different results.In fact, the Legal and Environmental sublanguages arevery different in style, and not so narrow in scope.Rather, we believe that the size of the corpora may bein fact too small.
We could hope in a higher variabilityof language patterns by training over 1-2 million wordscorpora.Swhereas, for more independent phenomena, 3 shouldemphasize the right attachments.io,s6 !?
~ o~o,s2O,800,780,760,740,720,47777777~711777,,,7Y ................. 'f, i ................ ........... ,\]~ ~ :::\]\]\]:\]\]\]::~ili~!
:e,,O;iii21221iill "7"~ ~21772117211 _:::::tl:::: \];tep !
__j .
.
.
.
~!.....
~.--- {;tep 2 :" I =.......... / ........... i .............. / .............. ~,O,5 0,6 0,7 0,8- (A): Precision vs. Recallfor learning phases Step O, Step I and Step2and a=0.2  -Data  Compress ion1,0 .................................................................0,8 .................................................................. i0,6 ............................................................... :0,20'4 ~ io,o ?"
10 1 2Learn ing Step?
(C): Data Compression i three learning steps -In format ion Gain0~ .
.
.
.0,2 / t ? '
? '
? '
?
? "
" ? "
-~  i0 ,1"  .............................. i ..............................0,0' i?
0 2Learn ing  StepReca l l- (B): Information Gainin the three learning steps -Coverageo,9 ~ i ..!? "
"=:~;.r=""~i:;; ~ ~  .
.
.
.
.
.
.
.
.
i0,4 i I i i i I I0,0 0,l 02- 0,3 0,4 0.5 0,6Test Threshold x?
(D): Cm, erage in three learning steps -Figure 2: Incrementa l  Learn ing :  Exper imenta l  Resu l ts31Further improvements could also be obtained usinga more refined discriminator than MCP1, but there isno free lunch.
If the corpus is our unique source ofknowledge, it is not possible to learn things for whichthere is no evidence.
Only if we can rely on some a-priori model of the world, even a naive model 9 to guidedifficult choices, then we can hope in a better coverageof repetitive phenomena.ConclusionsAs a conclusion we may claim that corpus-driven lexicallearning should result from the interaction of cooperat-ing inductive processes triggered by several knowledgesources.
The described method is a combination of nu-merical techniques (e.g.
the probability driven MCP1disambiguation operator) and some logical devices:?
a shallow syntactic analyzer that embodies a surfaceand portable grammatical competence helpful in trig-gering the overall induction;?
a naive semantic type system to obviate the problemof data sparseness and to give the learning systemsome explanatory powerThe interaction of such components has been ex-ploited in an incremental process.
In the experiments,the performance over a typical NLP task 10 (i.e.
PP-disambiguation) has been significantly improved by thisa cooperative approach.
Moreover, on the language n-gineering standpoint he main consequences are a sig-nificant data compression and a corresponding improve-ment of the overall system efficiency.One of the purposes of this paper was to show that,despite the good results recently obtained in the fieldof corpus-driven lexical learning, we must still demon-strate that NLP techniques, after the advent of lexicalstatistics, are industrially competitive.
And one goodway for doing so, is by measuring ourselves with the fullcomplexities of language.
More effort should thus be de-Voted in evaluating the performance of lexical learningmethods in real world, noisy domains.REFERENCES(Basili et a1.,1992) Basiti, R., Pazienza, M.T., Velardi, P.,Computational Lexicons: the Neat Examples and theOdd Exemplars, Proc.
of Third Int.
Conf.
on AppliedNatural Language Processing, Trento, Italy, 1-3 April,1992.9tike for example the coarse selectional restrictions usedby the ARIOSTO_LEX system (see refereed papers)1?although inherently hard for an unsupervised noise-prone framework(Basili et al1993a) Basili, R., A. Marziali, M.T.
Pazienza,Modelling syntactic uncertainty in lexical acquisitionfrom texts, Journal of Quantitative Linguistics, vol.1,n.1, 1994.
(Basili et al1993b) Basili, R., M.T.
Pazienza, P. Velardi,What can be learned from raw texts ?, Journal of Ma-chine Translation, 8:147-173,1993.
(Basili et a1,1993c) Basiti, R., M.T.
Pazienza, P. Velardi,Acquisition of selectional patterns, Journal of MachineTranslation, 8:175-201,1993.
(Basili et al,1994a) Basiti, R., M.T.
Pazienza, P.Velardi, A(not-so) shallow parser for collocational analysis, Proc.of Coting '94, Kyoto, Japan, 1994.
(Basili et al,1994b) Basiti, R., M.H.Candito, M.T.Pazienza, P. Velardi, Evaluating the information gainof probability-based PP-disambiguation methods, Proc.of International Conference on New Methods in Lan-guage Processing, Manchester, September 1994.
(Basiti et a1.,1996), Basili, R., M.T.
Pazienza, P.Velardi,An Empirical Symbolic Approach to Natural LanguageProcessing, Artificial Intelligence, to appear on vol.
85,August 1996(Brill 1992) Brill, E., A simple rule-based part of speechtagger, in Proc.
of the 3rd Conf.
on Applied NaturalLanguage Processing, ACL, Trento Italy(Brill and Resnik,1994) Brill E., Resnik P., A rule-based ap-proach to prepositional phrase attachment disambigua-tion, in Proc.
of COLING 94, 1198-1204(Collins and Brooks,1995) Collins M. and Brooks J., Prepo-sitional Phrase Attachment trough a Backed-off Model,3rd.
Workshop on Very Large Corpora, MT, 1995(Franz,1995), Franz A., A statistical approach to learn-ing prepositional phrase attachment disambiguation, iProc.
of IJCAI Workshop on New Approaches toLearning for Natural Language Processing, Montreal1995.
(Hindle and Rooth,1993) Hindle D. and Rooth M., Struc-tural Ambiguity and Lexical Relations, ComputationalLinguistics, 19(1): 103-120.
(Kononenko and Bratko, 1991) Kononenko I., I. Bratko,Information-Based Evaluation Criterion for Classi-fier's Performance, Machine Learning, 6,67-80, 1991.
(Marcus et al 1993) Marcus M., Santorini B. andMarcinkiewicz M., Building a large annotated corpusin English: The Penn Tree Bank, Computational Lin-guistics, 19(2): 313-330.
(Ratnaparkhi et al 1994), Ratnaparkhi, Rynar and Roukos,A maximum entropy model for prepositional phrase at-tachment.
In ARPA Workshop on Human languageTechnology, plainsboro, N J, 1994.
(Resnik and Hearst, 1993) Resnik P. and Hearst M., Struc-tural Ambiguity and Conceptual Relations, in Proc.
of1st Workshop on Very Large Corpora, 1993.32
