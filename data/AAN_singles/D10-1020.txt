Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 196?206,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsCrouching Dirichlet, Hidden Markov Model:Unsupervised POS Tagging with Context Local Tag GenerationTaesun Moon, Katrin Erk, and Jason BaldridgeDepartment of LinguisticsUniversity of Texas at Austin1 University Station B5100Austin, TX 78712-0198 USA{tsmoon,katrin.erk,jbaldrid}@mail.utexas.eduAbstractWe define the crouching Dirichlet, hiddenMarkov model (CDHMM), an HMM for part-of-speech tagging which draws state prior dis-tributions for each local document context.This simple modification of the HMM takesadvantage of the dichotomy in natural lan-guage between content and function words.
Incontrast, a standard HMM draws all prior dis-tributions once over all states and it is knownto perform poorly in unsupervised and semi-supervised POS tagging.
This modificationsignificantly improves unsupervised POS tag-ging performance across several measures onfive data sets for four languages.
We also showthat simply using different hyperparametervalues for content and function word states ina standard HMM (which we call HMM+) issurprisingly effective.1 IntroductionHidden Markov Models (HMMs) are simple, ver-satile, and widely-used generative sequence models.They have been applied to part-of-speech (POS) tag-ging in supervised (Brants, 2000), semi-supervised(Goldwater and Griffiths, 2007; Ravi and Knight,2009) and unsupervised (Johnson, 2007) trainingscenarios.
Though discriminative models achievebetter performance in both semi-supervised (Smithand Eisner, 2005) and supervised (Toutanova et al,2003) learning, there has been only limited work onunsupervised discriminative sequence models (e.g.,on synthetic data and protein sequences (Xu et al,2006)), and none to POS tagging.The tagging accuracy of purely unsupervisedHMMs is far below that of supervised and semi-supervised HMMs; this is unsurprising as it is stillnot well understood what kind of structure is beingfound by an unconstrained HMM (Headden III et al,2008).
However, HMMs are fairly simple directedgraphical models, and it is straightforward to ex-tend them to define alternative generative processes.This also applies to linguistically motivated HMMsfor recovering states and sequences that correspondmore closely to those implicitly defined by linguistswhen they label sentences with parts-of-speech.One way in which a basic HMM?s structure is apoor model for POS tagging is that there is no inher-ent distinction between (open-class) content wordsand (closed-class) function words.
Here, we proposetwo extensions to the HMM.
The first, HMM+, is avery simple modification where two different hyper-parameters are posited for content states and func-tion states, respectively.
The other is the crouch-ing Dirichlet, hidden Markov model (CDHMM), anextended HMM that captures this dichotomy basedon the statistical evidence that comes from context.Content states display greater variance across lo-cal context (e.g.
sentences, paragraphs, documents),and we capture this variance by adding a componentto the model for content states that is based on la-tent Dirichlet alocation (Blei et al, 2003).
This ex-tension is in some ways similar to the LDAHMMof Griffiths et al (2005).
Both models are compos-ite in that two distributions do not mix with eachother.
Unlike the LDAHMM, the generation of con-tent states is folded into the CDHMM process.We compare the HMM+ and CDHMM against abasic HMM and LDAHMM on POS tagging on amore extensive and diverse set of languages thanprevious work in monolingual unsupervised POStagging: four languages from three families (Ger-manic: English and German; Romance: Portuguese;196and Mayan: Uspanteko).
The CDHMM easily out-performs all other models, including HMM+, acrossthree measures (accuracy, F-score, and variationof information) for unsupervised POS tagging onmost data sets.
However, the HMM+ is surpris-ingly competitive, outperforming the basic HMMand LDAHMM, and rivaling or even passing theCDHMM on some measures and data sets.2 BackgroundThe Bayesian formulation for a basic HMM (Gold-water and Griffiths, 2007) is:?t|?
?
Dir(?)?t|?
?
Dir(?
)wi|ti = t ?
Mult(?t)ti|ti?1 = t ?
Mult(?t)Dir is the conjugate Dirichlet prior to Mult (a multi-nomial distribution).
The state transitions are gen-erated by Mult(?t) whose prior ?t is generated byDir(?)
with a symmetric (i.e.
uniform) hyperparam-eter ?.
Emissions are generated by Mult(?t) witha prior ?t generated by Dir(?)
with a symmetrichyperparameter ?.
Hyperparameter values smallerthan one encourage posteriors that are peaked, withsmaller values increasing this concentration.
It isnot necessary that the hyperparameters be symmet-ric, but this is a common approach when one wantsto be na?
?ve about the data.
This is particularly ap-propriate in unsupervised POS tagging with regardto novel data since there won?t be a priori groundsfor favoring certain distributions over others.There is considerable work on extensions toHMM-based unsupervised POS tagging (see ?6),but here we concentrate on the LDAHMM (Grif-fiths et al, 2005), which models topics and statesequences jointly.
The model is a composite of aprobabilistic topic model and an HMM in which asingle state is allocated for words generated fromthe topic model.
A strength of this model is that itis able to use less supervision than previous topicmodels since it does not require a stopword list.While the topic model component still uses the bags-of-words assumption, the joint model infers whichwords are more likely to carry topical content andwhich words are more likely to contribute to thelocal sequence.
This model is competitive with astandard topic model, and its output is also compet-itive when compared with a standard HMM.
How-ever, Griffiths et al (2005) note that the topic modelcomponent inevitably loses some finer distinctionswith respect to parts-of-speech.
Though many con-tent states such as adjectives, verbs, and nouns canvary a great deal across documents, the topic stategroups these words together.
This leads to assign-ment of word tokens to clusters that are a poorer fitfor POS tagging.
This paper shows that a model thatconflates the LDAHMM topics with content statescan significantly improve POS tagging.3 ModelsWe aim to model the fact that in many languageswords can generally be grouped into function wordsand content words and that these groups oftenhave significantly different distributions.
There arefew function words and they appear frequently,while there are many content words appearing infre-quently.
Another difference in distribution is oftenimplied in information retrieval by the use of stop-word filters and tf-idf values to remove or reduce theinfluence of words which occur frequently but havelow variance (i.e.
their global probability is similarto their local probability in a document).A difference in distribution is also revealed whenthe parts-of-speech are known.
When no smoothingparameters are added, the joint probability of a wordthat is not ?the?
or ?a?
occurring with a DT tag (inthe Penn Treebank) is almost always zero.
Similarlypeaked distributions are observed for other functioncategories such as MD and CC.
On the other hand,the joint probability of any word occurring with NNis much less likely to be zero and the distribution ismuch less likely to be peaked.We attempt to account for these two distributionalproperties?that certain words have higher varianceacross contexts (e.g.
a document) and that certaintags have more peaked emission distributions?in asequence model.
To do this, we define the crouchingDirichlet, hidden Markov model1 (CDHMM).
Thismodel, like LDAHMM, captures items of high vari-ance across contexts, but it does so without losing1We call our model a ?crouching Dirichlet?
model since itinvolves a Dirichlet prior that generates distributions for certainstates as if it were ?crouching?
on the side.197wi????ti?
?
?
?
?
????
?Figure 1: Graphical representation of relevant vari-ables and dependencies at a given time step i. Ob-served word wi is dependent on hidden state ti.Edges to priors ?, ?, ?
may or may not be activateddepending on the value of ti.
The edge to transitionprior ?
is always activated.
Hyperparameters to pri-ors are represented by dots.
See ?3.1 for details.sequence distinctions, namely, a given word?s lo-cal function via its part-of-speech.
We also definethe HMM+, a simple adaptation of a basic HMMwhich accounts for the latter property by using dif-ferent priors for emissions from content and functionstates.3.1 CDHMMThe CDHMM incorporates an LDA-like module toits graphical structure in order to capture wordsand tags which have high variance across contexts.Such tags correspond to content states.
Like theLDAHMM, the model is composite in that distribu-tions over a single random variable are composedof several different distribution functions which de-pend on the value of the underlying variable.We posit the following model (see fig.
1 for a dia-gram of dependencies and all variables involved at asingle time step).
We observe a sequence of tokensw=(w1, .
.
.
, wN ) that we assume is generated byan underlying state sequence t=(t1, .
.
.
, tN ) over astate alphabet T with first order Markov dependen-cies.
T is a union of disjoint content states C andfunction states F .
In this composite model, the pri-ors for the emission and transition for each step inthe sequence depend on whether state t at step i ist?C or t?F .
If t?C , the word emission is depen-dent on ?
(the content word prior) and the state tran-sition is dependent on ?
(the ?topic?
prior) and ?
(thetransition prior).
If t?F , the word emission proba-bility is dependent on ?
(the function word prior)and the state transition on ?
(again, the transitionprior).
Therefore, if t?F , the transition and emis-sion structure is identical to the standard BayesianHMM.To elaborate, three prior distributions are definedglobally for this model: (1) ?t, the transition priorsuch that p(t?|t, ?t) = ?t?|t (2) ?t, the function wordprior such that p(w|t, ?t) = ?w|t (3) ?t, the contentword prior such that p(w|t, ?t) = ?w|t.
Locally foreach context d (documents in our case), we define?d, the topic prior such that p(t|?d) = ?t|d for t?C .The generative story is as follows:1.
For each state t?T(a) Draw a distribution over states ?t ?Dir(?
)(b) If t?C , draw a distribution over words?t ?
Dir(?
)(c) If t?F , draw a distribution over words?t ?
Dir(?)2.
For each context d(a) Draw a distribution ?d ?
Dir(?)
overstates t?C(b) For each word wi in di.
draw ti from ?ti?1 ?
?dii.
if ti?C , then draw wi from ?ti , elsedraw wi from ?tiFor each context d, we draw a prior distribution?d?formally identical to the LDA topic prior?thatis defined only for the states t?C .
This prior is thenused to weight the draws for states at each word,from ?ti?1 ?
?d, where we have defined the vectorvalued operation ?
as follows:(?ti?1 ?
?d)ti ={1Z ?ti|ti?1 ?
?ti|d ti?C1Z ?ti|ti?1 ti?Fwhere (?ti?1 ?
?d)ti is the element corresponding tostate ti in the vector ?ti?1 ?
?d.
Z is a normalizationconstant such that the probability mass sums to one.198p(ti|t?i,w) ??????Nwi|ti+?Nti+W?Nti|di+?Ndi+C??Nti|ti?1+???Nti+1|ti+I[ti?1=ti=ti+1]+?
?Nti+T?+I[ti=ti?1]ti ?
CNwi|ti+?Nti+W??Nti|ti?1+???Nti+1|ti+I[ti?1=ti=ti+1]+?
?Nti+T?+I[ti=ti?1]ti ?
FFigure 2: Conditional distribution for ti in the CDHMM.The important thing to note is that the draw forstates at each word is proportional to a compositeof (a) the product of the individual elements of thetopic and transition priors when ti?C and (b) thetransition priors when ti?F .
The draw is propor-tional to the product of topic and transition priorswhen ti?C because we have made a product of ex-perts (PoE) factorization assumption (Hinton, 2002)for tractability and to reduce the size of our model.Without such an assumption, the transition parame-ters would lie in a partitioned space of size O(|C|4)as opposed to O(|T |2) for the current model.
Fur-thermore, this combination of a composite hiddenstate space with a product of experts assumption al-lows us to capture high variance for certain states.To summarize, the CDHMM is a compositemodel where both the observed token and the hiddenstate variable are composite distributions.
For thehidden state, this means that there is a ?topical?
ele-ment with high variance across contexts that is em-bedded in the state sequence for a subset of events.We embed this element through a PoE assumptionwhere transitions into content states are modeled asa product of the transition probability and the localprobability of the content state.Inference.
We use a Gibbs sampler (Gao andJohnson, 2008) to learn the parameters of this andall other models under consideration.
In this infer-ence regime, two distributions are of particular in-terest.
One is the posterior density and the other isthe conditional distribution, neither of which can belearned in closed form.Letting ?
= (?, ?, ?, ?)
and h = (?, ?, ?, ?
), theposterior density is given asp(?|w, t;h) ?
p(w, t|?)p(?
;h)Note that p(w, t|?)
is equal toD?dNd?i(?wi|ti?ti|d?ti|ti?1)I[ti?C](?wi|ti?ti|ti?1)I[ti?F ] (1)where I[?]
is the indicator function, D is the numberof documents in the corpus and Nd is the number oftokens in document d.Another important measure is the conditional dis-tribution which is conditioned on all the randomvariables except the hidden state variable of interestand which is derived by integrating out the priors:p(ti|t?i,w;h) ?
p(ti|t?i;h)p(wi|t,w?i;h) (2)where t?i is the joint random variable t without tiand w?i is w without wi.There are two well-known approaches to conduct-ing Gibbs sampling for HMMs.
The default methodis to sample ?
based on the posterior, then sampleeach ti based on the conditional distribution.
An-other approach is to sample directly from the con-ditional distribution without sampling from the pos-terior since the conditional distribution incorporatesthe posterior through integration.
This is called acollapsed Gibbs sampler, which is the method em-ployed for the models in this study.The full conditional distribution for tag transitionsfor the Gibbs sampler is given in Figure 2.
At eachtime step, we decrement all counts for the currentvalue of ti, sample a new value for ti from a multino-mial proportional to the conditional distribution andassign that value to ti.
?, ?
are the hyperparametersfor the word emission priors of the content states andfunction states, respectively.
?
is the hyperparame-ter for the state transition priors.
?
is the hyperpa-rameter for the state prior given that it is in somecontext d. Note that we have overridden notation so199that C and T here refer to the size of the alphabet.W is the size of the vocabulary.
Notation such asNti|ti?1 refers to the counts of the events indicatedby the subscript, minus the current token and tag un-der consideration.
Nti|ti?1 is the number of times tihas occurred after ti?1 minus the tag for wi.
Nwi|tiis the number of times wi has occurred with ti minusthe current value.
Nti and Ndi are the counts for thegiven tag and document minus the current value.In its broad outline, the CDHMM is not muchmore complicated than an HMM since the decompo-sition (eqn.
1) is nearly identical to that of an HMMwith the exception that conditional probabilities fora subset of the states?the content states?are local.An inference algorithm can be derived that involvesno more than adding a single term to the standardMCMC algorithm for HMMs (see Figure 2).3.2 HMM+The CDHMM explicitly posits two different typesof states: function states and content states.
Hav-ing made this distinction, there is a very simple wayto capture the difference in emission distributionsfor function and content states within an otherwisestandard HMM: posit different hyperparameters forthe two types.
One type has a small hyperparame-ter to model a sparse distribution for function wordsand the other has a relatively large hyperparameterto model a distribution with broader support.
Thisextension, which we refer to as HMM+, provides animportant benchmark to compare with the CDHMMto see how much is gained by its additional ability tomodel the fact that function words occur frequentlybut have low variance across contexts.As with the CDHMM, we use Gibbs sampling toestimate the model parameters while holding the twodifferent hyperparameters fixed.
The conditionaldistribution for tag transitions for this model is iden-tical to that in fig.
2 except that it does not have thesecond term Nti|di+?Ndi+C?
in the first case where ti?C .We are not aware of a published instance of suchan extension to the HMM?which our results showto be surprisingly effective.
Goldwater and Griffiths(2007) posits different hyperparameters for individ-ual states, but not for different groups of states.corpus tokens docs avg.
tagsWSJ 974254 1801 541 43Brown 797328 343 2325 80Tiger 447079 1090 410 58Floresta 197422 1956 101 19Uspanteko 70125 29 2418 83Table 2: Number of tokens, documents, average to-kens per document and total tag types for each cor-pus.4 Data and ExperimentsData.
We use five datasets from four languages(English, German, Portuguese, Uspanteko) for eval-uating POS tagging performance.?
English: the Brown corpus (Francis et al, 1982)and the Wall Street Journal portion of the PennTreebank (Marcus et al, 1994).?
German: the Tiger corpus (Brants et al, 2002).?
Portuguese: the full Bosque subset of the Florestacorpus (Afonso et al, 2002).?
Uspanteko (an endangered Mayan language ofGuatemala): morpheme-segmented and POS-tagged texts collected and annotated by theOKMA language documentation project (Pixabajet al, 2007); we use the cleaned-up version de-scribed in Palmer et al (2009).Table 2 provides the statistics for these corpora.We lowercase all words, do not remove any punc-tuation or hapax legomena, and we do not replacenumerals with a single identifier.
Due to the natureof the models, document boundaries are retained.Evaluation We report values for three evaluationmetrics on all five corpora, using their full tagsets.?
Accuracy: We use a greedy search algorithm tomap each unsupervised tag to a gold label suchthat accuracy is maximized.
We evaluate on a1-to-1 mapping between unsupervised tags andgold labels, as well as many-to-1 (M-to-1), cor-responding to the evaluation mappings used inJohnson (2007).
The 1-to-1 mapping provides astricter evaluation.
The many-to-one mapping, onthe other hand, may be more adequate as unsu-pervised tags tend to be more fine-grained than200Model Accuracy Pairwise P/R Scores VI1-to-1 M-to-1 P R FWSJ(50) HMM 0.34 (0.01) 0.49 (0.03) 0.51 (0.03) 0.19 (0.01) 0.28 (0.01) 3.72 (0.08)LDAHMM 0.30 (0.04) 0.45 (0.04) 0.25 (0.07) 0.27 (0.03) 0.26 (0.04) 3.64 (0.14)HMM+ 0.42 (0.04) 0.46 (0.05) 0.24 (0.03) 0.49 (0.03) 0.32 (0.03) 2.65 (0.15)CDHMM 0.44 (0.01) 0.58 (0.02) 0.31 (0.01) 0.43 (0.03) 0.36 (0.02) 2.73 (0.08)Brown(50) HMM 0.32 (0.01) 0.50 (0.02) 0.60 (0.02) 0.18 (0.00) 0.28 (0.01) 3.82 (0.05)LDAHMM 0.28 (0.06) 0.41 (0.08) 0.25 (0.10) 0.28 (0.05) 0.25 (0.05) 3.71 (0.21)HMM+ 0.43 (0.06) 0.48 (0.07) 0.29 (0.05) 0.50 (0.04) 0.37 (0.05) 2.63 (0.19)CDHMM 0.48 (0.02) 0.62 (0.02) 0.32 (0.03) 0.54 (0.04) 0.40 (0.03) 2.48 (0.06)Tiger(50) HMM 0.29 (0.02) 0.49 (0.02) 0.49 (0.04) 0.14 (0.01) 0.22 (0.02) 3.91 (0.06)LDAHMM 0.31 (0.04) 0.50 (0.04) 0.26 (0.07) 0.24 (0.02) 0.25 (0.04) 3.51 (0.11)HMM+ 0.41 (0.08) 0.44 (0.05) 0.25 (0.05) 0.58 (0.10) 0.35 (0.06) 2.70 (0.25)CDHMM 0.47 (0.01) 0.61 (0.02) 0.45 (0.01) 0.58 (0.03) 0.50 (0.02) 2.72 (0.04)Usp.
(50) HMM 0.36 (0.01) 0.49 (0.02) 0.39 (0.01) 0.18 (0.00) 0.25 (0.00) 3.63 (0.04)LDAHMM 0.35 (0.02) 0.47 (0.02) 0.26 (0.04) 0.23 (0.03) 0.24 (0.02) 3.52 (0.09)HMM+ 0.32 (0.02) 0.35 (0.03) 0.12 (0.02) 0.52 (0.05) 0.20 (0.02) 3.13 (0.06)CDHMM 0.39 (0.02) 0.50 (0.02) 0.16 (0.02) 0.39 (0.03) 0.23 (0.02) 3.00 (0.06)Flor.
(50) HMM 0.30 (0.01) 0.58 (0.03) 0.62 (0.05) 0.18 (0.01) 0.28 (0.01) 3.51 (0.06)LDAHMM 0.36 (0.06) 0.59 (0.04) 0.55 (0.10) 0.29 (0.07) 0.38 (0.08) 3.22 (0.15)HMM+ 0.35 (0.04) 0.52 (0.02) 0.28 (0.04) 0.43 (0.06) 0.34 (0.04) 2.58 (0.07)CDHMM 0.36 (0.01) 0.64 (0.02) 0.37 (0.02) 0.27 (0.01) 0.31 (0.01) 2.73 (0.05)Table 1: Evaluation on WSJ, Brown, Tiger, Floresta and Uspanteko for models with 50 states.
For VI, loweris bettergold part-of-speech tags.
In particular, they tendto form semantically coherent sub-classes of goldparts of speech.?
Pairwise Precision and Recall: Viewing taggingas a clustering task over tokens, we evaluate pair-wise precision (P ) and recall (R) between themodel tag sequence (M ) and gold tag sequence(G) by counting the true positives (tp), false pos-itives (fp) and false negatives (fn) between thetwo and setting P = tp/(tp + fp) and R =tp/(tp+ fn).
tp is the number of token pairs thatshare a tag in M as well as in G, fp is the numbertoken pairs that share the same tag in M but havedifferent tags in G, and fn is the number tokenpairs assigned a different tag in M but the samein G (Meila, 2007).
We also provide the f -scorewhich is the harmonic mean of P and R.?
Variation of Information (VI): The variation ofinformation is an information theoretic metricthat measures the amount of information lost andgained in going from tag sequenceM toG (Meila,2007).
It is defined as V I(M,G) = H(M) +H(G) ?
2I(M,G) where H denotes entropy andI mutual information.
Goldwater and Griffiths(2007) noted that this measure can point out mod-els that have more consistent errors in the formof lower VI, even when accuracy figures are thesame.We also report learning curves on M-to-1 with ge-ometrically increasing training set sizes of 8, 16, 32,64, 128, 256, 512, 1024, and all documents, or asmany as possible given the corpus.5 ExperimentsIn this section we discuss our parameter settings andexperimental results.5.1 Models and ParametersWe compare four different models:?
HMM: a standard HMM?
HMM+: an HMM in which the hyperparametersfor the word emissions are asymmetric, such thatcontent states have different word emission priorscompared to function states.?
LDAHMM: an HMM with a distinguished statethat generates words from a topic model (Griffithset al, 2005)201WSJ Brown Tiger Floresta Uspanteko20304050HMM+acc0.00.10.20.30.40.50.6WSJ Brown Tiger Floresta UspantekoLDAHMMacc0.00.10.20.30.40.50.6WSJ Brown Tiger Floresta UspantekoCDHMMacc0.00.10.20.30.40.50.6Figure 3: Averaged many-to-one accuracy on the full tagset for the models HMM+, LDAHMM, CDHMMwhen the number of states is set at 20, 30, 40 and 50 states.?
CDHMM: our HMM with context-based emis-sions, where the context used is the documentWe implemented all of these models, ensuring per-formance differences are due to the models them-selves rather than implementation details.For all models, the transition hyperparameters ?are set to 0.1.
For the LDAHMM and HMM all emis-sion hyperparameters are set to 0.0001.
These fig-ures are the MCMC settings that provided the bestresults in Johnson (2007).
For the models that distin-guish content and function states (HMM+, CDHMM),we fixed the number of content states at 5 and set thefunction state emission hyperparameters ?
= 0.0001and the content state emission hyperparameters ?
=0.1.
For the models with an LDA or LDA-like com-ponent (LDAHMM, CDHMM), we set the topic orcontent-state hyperparameter ?
= 1.For decoding, we use maximum posterior decod-ing to obtain a single sample after the required burn-in, as has been done in other unsupervised HMMexperiments.
We use this sample for evaluation.5.2 ResultsResults for all models on the full tagset are providedin table 1.2 Each number is the mean accuracy often randomly initialized samples after a single chainburn-in of 1000 iterations.
The model with a sta-tistically significant (p < 0.05) best score for eachmeasure and data set is given in plain bold.
In cases2Similar results are obtained with reduced tagsets, as is com-monly done in other work on unsupervised POS-tagging.where the differences for the best models are not sig-nificantly different from each other, but are signifi-cantly better from the others, the top model scoresare given in bold italic.CDHMM is extremely strong on the accuracy met-ric: it wins or ties for all datasets for both 1-to-1 andM-to-1 measures.
For pairwise f -score, it obtainsthe best score for two datasets (WSJ and Tiger), andties with HMM+ on Brown (we return to Uspantekoand Floresta below in an experiment that varies thenumber of states).
For VI, HMM+ and CDHMM botheasily outperform the other models, with CDHMMwinning Brown and Uspanteko and HMM+ winningFloresta.In the case of Uspanteko, the absolute differencein mean performance between models is smalleroverall but still significant.
This is due to the reducedvariance between samples for all models.
This isstriking because the non-CDHMM models have muchhigher standard deviation on other corpora but havesharply reduced standard deviation only for Uspan-teko.
The most likely explanation is that the Uspan-teko corpus is much smaller than the other corpora.3Nonetheless, CDHMM comes out strongest on mostmeasures.A simple baseline for accuracy is to choose themost frequent tag for all tokens; this gives accura-cies of 0.14 (WSJ), 0.14 (Brown), 0.21 (Tiger), 0.203which is interesting in itself since the weak law of largenumbers implies that sample standard deviation decreases withsample size, which in our case is the number of tokens ratherthan the 10 samples under discussion202Model Accuracy P/R Scores VI1-to-1 M-to-1 P R FUsp.
(100) HMM 0.36 (0.01) 0.58 (0.01) 0.56 (0.02) 0.16 (0.00) 0.25 (0.01) 3.53 (0.04)LDAHMM 0.35 (0.01) 0.58 (0.02) 0.45 (0.04) 0.17 (0.01) 0.24 (0.01) 3.46 (0.06)HMM+ 0.35 (0.02) 0.41 (0.02) 0.18 (0.01) 0.36 (0.03) 0.24 (0.01) 3.25 (0.08)CDHMM 0.40 (0.01) 0.59 (0.01) 0.25 (0.02) 0.27 (0.02) 0.26 (0.01) 3.05 (0.03)Flor.
(20) HMM 0.31 (0.02) 0.48 (0.03) 0.40 (0.03) 0.21 (0.01) 0.28 (0.02) 3.54 (0.10)LDAHMM 0.35 (0.06) 0.46 (0.06) 0.27 (0.07) 0.45 (0.08) 0.33 (0.05) 3.10 (0.10)HMM+ 0.37 (0.04) 0.50 (0.03) 0.30 (0.02) 0.45 (0.06) 0.36 (0.03) 2.62 (0.06)CDHMM 0.44 (0.02) 0.55 (0.02) 0.30 (0.01) 0.53 (0.03) 0.39 (0.02) 2.39 (0.07)Table 3: Evaluation for Uspanteko and Floresta.
Experiments in this table use state sizes that correspondmore closely to the size of the tag sets in the respective corpora.
(Floresta), and 0.11 (Uspanteko).
Clearly, all of themodels easily outperform this baseline.Number of states.
Figure 3 shows the change inaccuracy for the different models for different cor-pora when the overall number of states is variedbetween 20 and 50.
The figure shows results forM-to-1.
All models with the exception of HMM+show improvements as the number of states is in-creased.
This brings up the valid concern (Clark,2003; Johnson, 2007) that a model could posit avery large number of states and obtain high M-to-1 scores.
However, it is neither the case here norin any of the studies we cite.
Furthermore, as isstrongly suggested with HMM+, it does not seem asif all models will benefit from assuming a large num-ber of states.Looking at the results by number of states on VIand f -score for CDHMM(Figure 5), it is clear thatFloresta displays the reverse pattern of all other datasets where performance monotonically deterioratesas state sizes are increased.
Though the exact reasonis unknown, we believe it is partially due to the factthat Floresta has 19 tags.
We therefore wonderedwhether positing a state size that more closely ap-proximated the size of the gold tag set performs bet-ter.
Since the discrepancy is greatest for Uspantekoand Floresta, we present tabulated results for exper-iments with state settings of 100 and 20 states re-spectively (table 3).
With the exception of VI (wherelower is better) for Uspanteko, the scores generallyimprove when the model state size is closer to thegold size.
M-to-1 goes down for Floresta when 20states are posited, but this is to be expected since thisscore is defined, to a certain extent, to do better withWSJ Brown Tiger Floresta Uspanteko20304050F?SCOREf?score0.00.10.20.30.40.5WSJ Brown Tiger Floresta UspantekoVIvi01234Figure 5: f -score and VI for CDHMM by number ofstateslarger models.Variance.
As we average performance figuresover ten runs for each model, it is also instructiveto consider standard deviation across runs.
Standarddeviation is lowest for the CDHMM models and thevanilla HMM.
Standard deviation is high for HMM+and LDAHMM.
This is not surprising for LDAHMM,since it has fifty topic parameters in addition to thenumber of states posited, and random initial condi-tions would have greater effect on the outcome thanfor the other models.
It is unexpected, however, thatHMM+ has high variance over different chains.
Themodel shares the large content emission hyperpa-rameter ?
= 0.1 with CDHMM.
At this point, it canonly be assumed that the additional LDA componentacts as a regularization factor for CDHMM and re-duced the volatility in having a large emission hy-perparameter.2030 1 2 3 4 5 60.30.40.50.6Brown WSJ TigerUspantekoFloresta0 1 2 3 4 5 6 7 80.30.40.50.60 1 2 3 4 5 6 7 80.30.40.50.60 1 2 3 4 5 6 7 80.30.40.50.60 1 20.30.40.50.6hmmhmm+ldahmmFigure 4: Learning curves on M-to-1 evaluation.
The staples at each point represent two standard deviations.Learning curves We present learning curves ondifferent sizes of subcorpora in Figure 4.
The graphsare box plots of the full M-1 accuracy figures on10 randomly initialized training runs for seven sub-corpora in Brown, nine in WSJ, Tiger, Floresta andthree in Uspanteko.Comparing the graphs, the performance of HMM+shows the strongest improvement for English andGerman data as the amount of training data in-creases.
Also, it is evident that CDHMM posts con-sistent performance gains across data sets as it trainson more data.
This stands in opposition to HMM andLDAHMM which do not seem able to take advantageof more information for WSJ and Floresta.
Thissuggests that performance for CDHMM and HMM+could improve if the training corpora were aug-mented with out-of-corpus raw data.
One exceptionto the consistent improvement over increased data isthe performance of the models on Uspanteko, whichuniformly flatline.
One reason might be that the tagsare labeled over segmented morphemes instead ofwords like the other corpora.
Another could be thatUspanteko has a relatively large number of tags in avery small corpus.6 Related workUnsupervised POS tagging is an active area of re-search.
Most recent work has involved HMMs.Given that an unconstrained HMM is not well under-stood in POS tagging, much work has been done onexamining the mechanism and the properties of theHMM as applied to natural language data (Johnson,2007; Gao and Johnson, 2008; Headden III et al,2008).
Conversely, there has also been work focusedon improving the HMM as an inference procedurethat looked at POS tagging as an example (Graca etal., 2009; Liang and Klein, 2009).
NonparametricHMMs for unsupervised POS tag induction (Snyderet al, 2008; Van Gael et al, 2009) have seen partic-ular activity due to the fact that model size assump-tions are unnecessary and it lets the data ?speak foritself.
?There is also work on alternative unsupervisedmodels that are not HMMs (Schu?tze, 1993; Abendet al, 2010; Reichart et al, 2010b) as well as re-search on improving evaluation of unsupervised tag-gers (Frank et al, 2009; Reichart et al, 2010a).Though they did not concentrate on unsupervisedmethods, Haghighi and Klein (2006) conducted anunsupervised experiment that utilized certain to-ken features (e.g.
character suffixes of 3 or less,204has initial capital, etc.
; the features themselves arefrom Smith and Eisner (2005)) to learn parametersin an undirected graphical model which was theequivalent of an HMM in directed models.
It wasalso the first study to posit the one-to-one evalua-tion criterion which has been repeated extensivelysince (Johnson, 2007; Headden III et al, 2008;Graca et al, 2009).Finkel et al (2007) is an interesting variant of un-supervised POS tagging where a parse tree is as-sumed and POS tags are induced from this structurenon-parametrically.
It is the converse of unsuper-vised parsing which assumes access to a tagged cor-pus and induces a parsing model.Other models more directly influenced or closelyparallel our work.
Griffiths et al (2005) is the workthat inspired the current approach where a set ofstates is designated to capture variance across con-texts.
The primary goal of that model was to inducea topic model given data that had not been filteredof noise in the form of function words.
As such,distinguishing between topic states such that theymodel different syntactic states was not attempted,and we have seen in sec.
3 that such an extension isnot entirely straightforward.4 Boyd-Graber and Blei(2009) has some parallels to our model in that a hid-den variable over topics is distributed according toa normalized product between a context prior and asyntactic prior.
However, it assumes a much greateramount of information than we do in that a parse treeas well as (possibly) POS tags are taken as observed.The model has a very different goal from ours aswell, which is to infer a syntactically informed topicmodel.
Teichert and Daume?
III (2010) is anotherstudy with close similarities to our own.
This studymodels distinctions between closed class words andopen class words within a modified HMM.
It is un-clear from their formulation how the distinction be-tween open class and closed class words is learned.There is also extensive literature on learning se-quence structure from unlabeled text (Smith andEisner, 2005; Goldberg et al, 2008; Ravi andKnight, 2009) which assume access to a tag dic-tionary.
Goldwater and Griffiths (2007) deservesmention for examining a semi-supervised model4We tested a variant of LDAHMM in which more than onestate can generate topics.
It did not achieve good results.that sampled emission hyperparameters for eachstate rather than a single symmetric hyperparame-ter.
They showed that this outperformed a symmet-ric model.
An interesting heuristic model is Zhaoand Marcus (2009) that uses a seed set of closedclass words to classify open class words.7 ConclusionWe have shown that a hidden Markov model thatallocates a subset of the states to have distribu-tions conditioned on localized domains can signif-icantly improve performance in unsupervised part-of-speech tagging.
We have also demonstrated thatsignificant performance gains are possible simplyby setting a different emission hyperparameter fora subgroup of the states.
It is encouraging that theseresults hold for both models not just on the WSJ butacross a diverse set of languages and measures.We believe our proposed extensions to the HMMare a significant contribution to the general HMMand unsupervised POS tagging literature in that bothcan be implemented with minimum modificationof existing MCMC inferred HMMs, have (nearly)equivalent run times, produce output that is easy tointerpret since they are based on a generative frame-work, and bring about considerable performance im-provements at the same time.AcknowledgmentsThe authors would like to thank Elias Ponvert andthe anonymous reviewers.
This work was supportedby a grant from the Morris Memorial Trust Fund ofthe New York Community Trust.ReferencesO.
Abend, R. Reichart, and A. Rappoport.
2010.
Im-proved unsupervised POS induction through prototypediscovery.
In Proceedings of ACL, pages 1298?1307.S.
Afonso, E. Bick, R. Haber, and D. Santos.
2002.
Flo-resta sinta?(c)tica?
: a treebank for Portuguese.
In Pro-ceedings of LREC, pages 1698?1703.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
LatentDirichlet alocation.
The Journal of Machine LearningResearch, 3:993?1022.J.
L. Boyd-Graber and D. Blei.
2009.
Syntactic topicmodels.
In Proceedings of NIPS, pages 185?192.205S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.2002.
The TIGER treebank.
In Proceedings of theWorkshop on Treebanks and Linguistic Theories.T.
Brants.
2000.
TnT: a statistical part-of-speech tag-ger.
In Proceedings of conference on Applied naturallanguage processing, pages 224?231.A.
Clark.
2003.
Combining distributional and morpho-logical information for part of speech induction.
InProceedings of EACL, pages 59?66.J.
R. Finkel, T. Grenager, and C. D. Manning.
2007.
Theinfinite tree.
In Proceedings of ACL, pages 272?279.W.N.
Francis, H. Kuc?era, and A.W.
Mackie.
1982.
Fre-quency analysis of English usage: Lexicon and gram-mar.
Houghton Mifflin Harcourt.S.
Frank, S. Goldwater, and F. Keller.
2009.
Evaluatingmodels of syntactic category acquisition without usinga gold standard.
In Proceedings of CogSci.J.
Gao and M. Johnson.
2008.
A comparison of Bayesianestimators for unsupervised Hidden Markov ModelPOS taggers.
In Proceedings of EMNLP, pages 344?352.Y.
Goldberg, M. Adler, and M. Elhadad.
2008.
EMcan find pretty good HMM POS-taggers (when givena good start).
In Proceedings of ACL, pages 746?754.S.
Goldwater and T. L. Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.
InProceedings of ACL, pages 744?751.J.
Graca, K. Ganchev, B. Taskar, and F. Pereira.
2009.Posterior vs parameter sparsity in latent variable mod-els.
In Proceedings of NIPS, pages 664?672.T.
L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-baum.
2005.
Integrating topics and syntax.
In Pro-ceedings of NIPS, pages 537?544.A.
Haghighi and D. Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofHLT/NAACL, pages 320?327.W.
P. Headden III, D. McClosky, and E. Charniak.2008.
Evaluating unsupervised part-of-speech taggingfor grammar induction.
In Proceedings of COLING,pages 329?336.G.E.
Hinton.
2002.
Training products of experts by min-imizing contrastive divergence.
Neural Computation,14(8):1771?1800.M.
Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers.
In Proceedings of EMNLP-CoNLL,pages 296?305.P.
Liang and D. Klein.
2009.
Online EM for unsuper-vised models.
In Proceedings of HLT/NAACL, pages611?619.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a large annotated corpus of English:The Penn Treebank.
Comp.
ling., 19(2):313?330.M.
Meila.
2007.
Comparing clusterings?an informa-tion based distance.
Journal of Multivariate Analysis,98(5):873?895.A.
Palmer, T. Moon, and J. Baldridge.
2009.
Evaluat-ing automation strategies in language documentation.In Proceedings of the NAACL-HLT 2009 Workshopon Active Learning for Natural Language Processing,pages 36?44.T.
C. Pixabaj, M. A. Vicente Me?ndez, M. VicenteMe?ndez, and O.
A. Damia?n.
2007.
Text Collections inFour Mayan Languages.
Archived in The Archive ofthe Indigenous Languages of Latin America.S.
Ravi and K. Knight.
2009.
Minimized models forunsupervised part-of-speech tagging.
In Proceedingsof ACL and AFNLP, pages 504?512.R.
Reichart, O. Abend, and A. Rappoport.
2010a.
Typelevel clustering evaluation: New measures and a POSinduction case study.
In Proceedings of CoNLL, pages77?87.R.
Reichart, R. Fattal, and A. Rappoport.
2010b.
Im-proved unsupervised POS induction using intrinsicclustering quality and a Zipfian constraint.
In Proceed-ings of CoNLL, pages 57?66.H.
Schu?tze.
1993.
Part-of-speech induction from scratch.In Proceedings of ACL, pages 251?258.N.A.
Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In Pro-ceedings of ACL, pages 354?362.B.
Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.2008.
Unsupervised multilingual learning for POStagging.
In Proceedings of EMNLP, pages 1041?1050.A.R.
Teichert and H. Daume?
III.
2010.
UnsupervisedPart of Speech Tagging Without a Lexicon.
In NIPSWorkshop on Grammar Induction, Representation ofLanguage and Language Learning 2010.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proceedings ofNAACL, pages 173?180.J.
Van Gael, A. Vlachos, and Z. Ghahramani.
2009.
Theinfinite HMM for unsupervised PoS tagging.
In Pro-ceedings of EMNLP, pages 678?687.L.
Xu, D. Wilkinson, F. Southey, and D. Schuurmans.2006.
Discriminative unsupervised learning of struc-tured predictors.
In Proceedings of ICML, pages1057?1064.Q.
Zhao and M. Marcus.
2009.
A simple unsuper-vised learner for POS disambiguation rules given onlya minimal lexicon.
In Proceedings of EMNLP, pages688?697.206
