A Part of Speech Estimation Method for Japanese UnknownWords using a Statistical Model of Morphology and ContextMasaak i  NAGATANTT  Cyber  Space Laborator ies1-1 Hikari-no-oka Yokosuka-Shi Kanagawa,  239-0847 Japannagata@nttn ly ,  i s l .
n t t .
co. jpAbst rac tWe present a statistical model of Japanese unknownwords consisting of a set of length and spellingmodels classified by the character types that con-stitute a word.
The point is quite simple: differ-ent character sets should be treated ifferently andthe changes between character types are very im-portant because Japanese script has both ideogramslike Chinese (kanji) and phonograms like English(katakana).
Both word segmentation accuracy andpart of speech tagging accuracy are improved by theproposed model.
The model can achieve 96.6% tag-ging accuracy if unknown words are correctly seg-mented.1 In t roduct ionIn Japanese, around 95% word segmentation ac-curacy is reported by using a word-based lan-guage model and the Viterbi-like dynamic program-ming procedures (Nagata, 1994; Yamamoto, 1996;Takeuchi and Matsumoto, 1997; Haruno and Mat-sumoto, 1997).
About the same accuracy is reportedin Chinese by statistical methods (Sproat et al,1996).
But there has been relatively little improve-ment in recent years because most of the remainingerrors are due to unknown words.There are two approaches to solve this problem:to increase the coverage of the dictionary (Fung andWu, 1994; Chang et al, 1995; Mori and Nagao,1996) and to design a better model for unknownwords (Nagata, 1996; Sproat et al, 1996).
We takethe latter approach.
To improve word segmenta-tion accuracy, (Nagata, 1996) used a single generalpurpose unknown word model, while (Sproat et al,1996) used a set of specific word models uch as forplurals, personal names, and transliterated foreignwords.The goal of our research is to assign a correct partof speech to unknown word as well as identifying itcorrectly.
In this paper, we present anovel statisticalmodel for Japanese unknown words.
It consists ofa set of word models for each part of speech andword type.
We classified Japanese words into nineorthographic types based on the character types thatconstitute a word.
We find that by making differentmodels for each word type, we can better model thelength and spelling of unknown words.In the following sections, we first describe the lan-guage model used for Japanese word segmentation.We then describe a series of unknown word mod-els, from the baseline model to the one we propose.Finally, we prove the effectiveness of the proposedmodel by experiment.2 Word  Segmentat ion  Mode l2.1 Baseline Language Model and SearchAlgorithmLet the input Japanese character sequence be C =Cl...Cm, and segment i  into word sequence W =wl ... wn 1 .
The word segmentation task can be de-fined as finding the word segmentation 12d that max-imize the joint probability of word sequence givencharacter sequence P(WIC ).
Since the maximiza-tion is carried out with fixed character sequence C,the word segmenter only has to maximize the jointprobability of word sequence P(W).= arg mwax P(WIC) = arg mwax P(W) (1)We call P(W) the segmentation model.
We canuse any type of word-based language model forP(W), such as word ngram and class-based ngram.We used the word bigram model in this paper.
So,P(W) is approximated by the product of word bi-gram probabilities P(wi\[wi- 1).P(W)P(wz I<bos>) 1-I ,~2 P(wi \[wi-1 )P(<eos> Iwn) (2)Here, the special symbols <bos> and <eos> indi-cate the beginning and the end of a sentence, re-spectively.Basically, word bigram probabilities of the wordsegmentation model is estimated by computing the1 In this paper, we define a word as a combination of itssurface form and part of speech.
Two words are consideredto be equal only if they have the same surface form and partof speech.277Table 1: Examples of word bigrams including un-known word tagsword bigram frequency?
)/no/particle<U-verb><U-number><U-adjectival-verb><U-adjective><U-adverb><U-noun>b/shi/inflectionH/yen/suffixt~/na/inflection~/i/inflection/to/particle67831052407405182139relative frequencies of the corresponding events inthe word segmented training corpus, with appropri-ate smoothing techniques.
The maximization searchcan be efficiently implemented by using the Viterbi-like dynamic programming procedure described in(Nagata, 1994).2.2 Mod i f i ca t ion  to  Hand le  UnknownWordsTo handle unknown words, we made a slight modi-fication in the above word segmentation model.
Wehave introduced unknown word tags <U-t> for eachpart of speech t. For example, <U-noun> and <U-verb> represents an unknown noun and an unknownverb, respectively.If wl is an unknown word whose part of speechis t, the word bigram probability P(wi\[wl-a)  is ap-proximated as the product of word bigram probabil-ity P(<U-t>\[wi_ l )  and the probability of wi givenit is an unknown word whose part of speech is t,P(wi\[<U-t>).P(wi lwi -1)  = P (<U- t> lw i -1 )P (w i l<U- t>,w i -a )P (<U- t>\ [w i_ l )P (w i l<U- t>)  (3)Here, we made an assumption that the spellingof an unknown word solely depends on its part ofspeech and is independent of the previous word.This is the same assumption made in the hiddenMarkov model, which is called output independence.The probabilities P(<U- t> lw i_ l  ) can be esti-mated from the relative frequencies in the trainingcorpus whose infrequent words are replaced withtheir corresponding unknown word tags based ontheir part of speeches 2Table 1 shows examples of word bigrams includingunknown word tags.
Here, a word is represented bya list of surface form, pronunciation, and part ofspeech, which are delimited by a slash '/ ' .
The first2 Throughout in this paper, we use the term "infrequentwords" to represent words that appeared only once in thecorpus.
They are also called "hapax legomena" or "hapaxwords".
It is well known that the characteristics of hapaxlegomena are similar to those of unknown words (Baayen andSproat, 1996).example "?
)/no/particle <U-noun>" will appear inthe most frequent form of Japanese noun phrases "A?
B", which corresponds to "B of A" in English.As Table 1 shows, word bigrams whose infrequentwords are replaced with their corresponding part ofspeech-based unknown word tags are very importantinformation source of the contexts where unknownwords appears.3 Unknown Word  Mode l3.1 Base l ine  Mode lThe simplest unknown word model depends only onthe spelling.
We think of an unknown word as a wordhaving a special part of speech <UNK>.
Then, theunknown word model is formally defined as the jointprobability of the character sequence wi = cl .. ?
ckif it is an unknown word.
Without loss of generality,we decompose it into the product of word lengthprobability and word spelling probability given itslength,P(wi \ [<UNK>)  = P(cx .
.
.
ck\[<VNK>) =P(k I<UNK>)P(c l  .
.
.
cklk, <UNK>) (4)where k is the length of the character sequence.We call P (k I<UNK> ) the word length model, andP(cz .
.
.
ck Ik, <UNK>) the word spelling model.In order to estimate the entropy of English,(Brown et al, 1992) approximated P(k I<UNK> )by a Poisson distribution whose parameter is theaverage word length A in the training corpus, andP(cz .
.
.
cklk, <UNK>) by the product of characterzerogram probabilities.
This means all characters inthe character set are considered to be selected inde-pendently and uniformly.
)kP(Cl .
.
.ck I<UNK> ) -~ -~.
e -~p k (5)where p is the inverse of the number of characters inthe character set.
If we assume JIS-X-0208 is usedas the Japanese character set, p = 1/6879.Since the Poisson distribution is a single parame-ter distribution with lower bound, it is appropriateto use it as a first order approximation to the wordlength distribution.
But the Brown model has twoproblems.
It assigns a certain amount of probabilitymass to zero-length words, and it is too simple toexpress morphology.For Japanese word segmentation and OCR errorcorrection, (Nagata, 1996) proposed a modified ver-sion of the Brown model.
Nagata also assumed theword length probability obeys the Poisson distribu-tion.
But he moved the lower bound from zero toone.
()~ - I) k-1P(k\ ]<UNK>) ~ (k -  1)!
e-()~-l) (6)278Instead of zerogram, He approximated the wordspelling probability P(Cl...ck\[k, <UNK>) by theproduct of word-based character bigram probabili-ties, regardless of word length.P(cl...  cklk, <UNK>)P(Cll<bow> ) YI~=2 P(cilc,_~)P( <eow>lc~) (7)where <bow> and <eow> are special symbols thatindicate the beginning and the end of a word.3.2 Correction of Word SpellingProbabilit iesWe find that Equation (7) assigns too little proba-bilities to long words (5 or more characters).
This isbecause the lefthand side of Equation (7) representsthe probability of the string cl ... Ck in the set of allstrings whose length are k, while the righthand siderepresents he probability of the string in the set ofall possible strings (from length zero to infinity).Let Pb(cz ...ck\]<UNK>) be the probability ofcharacter string Cl...ck estimated from the char-acter bigram model.Pb(cl... ckI<UNK>) --P(Cl\]<bow>) 1-I~=2 P(c~lc,-1)P( <e?w>lck) (8)Let Pb (kl <UNK>) be the sum of the probabilitiesof all strings which are generated by the characterbigram model and whose length are k. More appro-priate estimate for P(cl.
.
.
cklk, <UNK>) is,P(cl... cklk, <UNK>) ~ Pb(cl ... ckI<UNK>)Pb(kI<UNK>)(9)But how can we estimate Pb(kI<UNK>)?
It isdifficult o compute it directly, but we can get a rea-sonable stimate by considering the unigram case.If strings are generated by the character unigrammodel, the sum of the probabilities of all length kstrings equals to the probability of the event thatthe end of word symbol <eow> is selected after acharacter other than <eow> is selected k - 1 times.Pb(k\[<UNK>) ~ (1 -P(<eow>))k-ZP(<eow>)(10)Throughout in this paper, we used Equation (9)to compute the word spelling probabilities.3.3 Japanese Orthography and WordLength Distr ibutionIn word segmentation, one of the major problems ofthe word length model of Equation (6) is the decom-position of unknown words.
When a substring of anunknown word coincides with other word in the dic-tionary, it is very likely to be decomposed into thedictionary word and the remaining substring.
Wefind the reason of the decomposition is that the word0.50.450.40.350.30.250.20.150.10.050Word Length Distribution, i iProbs from Raw Counts (hapax words)Estimates by Poisson (hapax words) -+---/ /I I i i2 4 6 8 10Word Character LengthFigure 1: Word length distribution of unknownwords and its estimate by Poisson distribution0.50.450 .40350.30.250.20.150.10.0500Unknown Word Length OistflbutlonkanJlkatakana ~2 4 6 8 10Word Character LengthFigure 2: Word length distribution of kanji wordsand katakana wordslength model does not reflect the variation of theword length distribution resulting from the Japaneseorthography.Figure 1 shows the word length distribution of in-frequent words in the EDR corpus, and the estimateof word length distribution by Equation (6) whoseparameter (A = 4.8) is the average word length ofinfrequent words.
The empirical and the estimateddistributions agree fairly well.
But the estimatesby Poisson are smaller than empirical probabilitiesfor shorter words (<= 4 characters), and larger forlonger words (> characters).
This is because we rep-279Table 2: Character type configuration of infrequentwords in the EDR corpusTable 3: Examples of common character bigrams foreach part of speech in the infrequent wordscharacter type sequencekanjikatakanakatakana-kanjikanji-hiraganahiraganakanji-katakanakat akana-symbol-katakananumberkanji-hiragana-kanjialphabetkanji-hir agana-kanji-hir aganahiragana-kanjipercent45.1%11.4%6.5%5.6%3.7%3.4%3.0%2.6%2.4%2.0%1.7%1.3%examples=~y~T'I/y Yt .
*ag, ~$OO7~ ,  ~V~VSOP?~,~, ~ ~-~,~!resented all unknown words by one length model.Figure 2 shows the word length distribution ofwords consists of only kanji characters and wordsconsists of only katakana characters.
It shows thatthe length of kanji words distributes around 3 char-acters, while that of katakana words distributesaround 5 characters.
The empirical word length dis-tribution of Figure 1 is, in fact, a weighted sum ofthese two distributions.In the Japanese writing system, there are at leastfive different types of characters other than punc-tuation marks: kanji, hiragana, katakana, Romanalphabet, and Arabic numeral.
Kanji which means'Chinese character' is used for both Chinese originwords and Japanese words semantically equivalentto Chinese characters.
Hiragana nd katakana resyllabaries: The former is used primarily for gram-matical function words, such as particles and inflec-tional endings, while the latter is used primarily totransliterate Western origin words.
Roman alphabetis also used for Western origin words and acronyms.Arabic numeral is used for numbers.Most Japanese words are written in kanji, whilemore recent loan words are written in katakana.Katakana words are likely to be used for techni-cal terms, especially in relatively new fields likecomputer science.
Kanji words are shorter thankatakana words because kanji is based on a large(> 6,000) alphabet of ideograms while katakana isbased on a small (< 100) alphabet of phonograms.Table 2 shows the distribution of character typesequences that constitute the infrequent words inthe EDR corpus.
It shows approximately 65% ofwords are constituted by a single character type.Among the words that are constituted by more thantwo character types, only the kanji-hiragana ndhiragana-kanji sequences are morphemes and othersare compound words in a strict sense although theypart of speech character bigram frequencynounnumberadjectival-verbverbadjectiveadverb<eow><bow> 1<eow>~'J <eow>b <eow>0 <eow>13434843272136963are identified as words in the EDR corpus 3Therefore, we classified Japanese words into 9word types based on the character types that consti-tute a word: <sym>,  <num>,  <alpha>, <hira>,<kata>,  and <kan> represent a sequence of sym-bols, numbers, alphabets, hiraganas, katakanas, andkanjis, respectively.
<kan-hira> and <hira-kan>represent a sequence of kanjis followed by hiraganasand that of hiraganas followed by kanjis, respec-tively.
The rest are classified as <misc>.The resulting unknown word model is as follows.We first select the word type, then we select thelength and spelling.P(Cl ...ckI<UNK>) =P( <WT>I<UNK> )P(kI<WT> , dUNK>)P(cl... cklk, <WT>,  <UNK>)  (11)3.4 Par t  of  Speech  and  Word  Morpho logyIt is obvious that the beginnings and endings ofwords play an important role in tagging part ofspeech.
Table 3 shows examples of common char-acter bigrams for each part of speech in the infre-quent words of the EDR corpus.
The first examplein Table 3 shows that words ending in ' - - '  are likelyto be nouns.
This symbol typically appears at theend of transliterated Western origin words writtenin katakana.It is natural to make a model for each part ofspeech.
The resulting unknown word model is asfollows.P(Cl .. ?
ck\]<U-t>) =P(k\]<U-t>)P(Cl... cklk, <U-t>)  (12)By introducing the distinction of word type to themodel of Equation (12), we can derive a more sophis-ticated unknown word model that reflects both word3 When a Chinese character is used to represent a seman-tically equivalent Japanese verb, its root is written in theChinese character and its inflectional suffix is written in hi-ragana.
This results in kanji-hiragana sequence.
When aChinese character is too difficult to read, it is transliteratedin hiragana.
This results in either hiragana-kanji or kanji-hiragana sequence.280type and part of speech information.
This is the un-known word model we propose in this paper.
It firstselects the word type given the part of speech, thenthe word length and spelling.P(cl... c l<U-t>) =P( <WT>I<U-t> )P(kI<WT>, <U-t>)P(Cl... cklk, <WT>, <U-t>) (13)Table 4: The amount of training and test setssentencesword tokenschar tokenstraining set100,0002,460,1883,897,718test set-1 test set-2100,000 5,0002,465,441 122,0643,906,260 192,818The first factor in the righthand side of Equa-tion (13) is estimated from the relative frequencyof the corresponding events in the training corpus.p(<WT>I<U_t> ) = C(<WT>, <U-t>)C(<U-t>)(14)Here, C(.)
represents he counts in the corpus.
Toestimate the probabilities of the combinations ofword type and part of speech that did not appearedin the training corpus, we used the Witten-Bellmethod (Witten and Bell, 1991) to obtain an esti-mate for the sum of the probabilities of unobservedevents.
We then redistributed this evenly among allunobserved events aThe second factor of Equation (13) is estimatedfrom the Poisson distribution whose parameter'~<WT>,<U-t> is the average length of words whoseword type is <WT> and part of speech is <U-t>.P(kI<WT>, <U-t>) =( )~<WW>,<U-t>- l )  u-1 e - - (A<WW>,<U.
t>- l )  (15) (k-l)!If the combinations ofword type and part of speechthat did not appeared inthe training corpus, we usedthe average word length of all words.To compute the third factor of Equation (13), wehave to estimate the character bigram probabilitiesthat are classified by word type and part of speech.Basically, they are estimated from the relative fre-quency of the character bigrams for each word typeand part of speech.f(cilci-1, <WT>, <U-t>) =C(<WT>,<U- t>,c i _  1 ,cl)C(<WT>,<U-t>,ci_l) (16)However, if we divide the corpus by the combina-tion of word type and part of speech, the amount ofeach training data becomes very small.
Therefore,we linearly interpolated the following five probabili-ties (Jelinek and Mercer, 1980).P(c~lci_l, <WT>, <U-t>) =4 The Witten-Bel l  method est imates the probabil ity of ob-serving novel events to be r/(n+r), where n is the total num-ber of events seen previously, and r is the number  of symbolsthat  are distinct.
The probabil ity of the event observed ct imes is c/(n + r).oqf(ci, <WT>, <U-t>)+a2f (c i  1Ci-1, <WT>,  <U-t>)+a3f(ci) + aaf(cilci_,) + ~5(1/V) (17)Where~1+(~2+~3+cq+c~5 --- 1. f(ci, <WT>, <U-t>) andf(ci\[ci-t, <WT>, <U-t>) are the relative frequen-cies of the character unigram and bigram for eachword type and part of speech, f(ci) and f(cilci_l)are the relative frequencies ofthe character unigramand bigram.
V is the number of characters (not to-kens but types) appeared in the corpus.4 Exper iments4.1 Training and Test Data for theLanguage ModelWe used the EDR Japanese Corpus Version 1.0(EDR, 1991) to train the language model.
It is amanually word segmented and tagged corpus of ap-proximately 5.1 million words (208 thousand sen-tences).
It contains a variety of Japanese sentencestaken from newspapers, magazines, dictionaries, en-cyclopedias, textbooks, etc..In this experiment, we randomly selected two setsof 100 thousand sentences.
The first 100 thousandsentences are used for training the language model.The second 100 thousand sentences are used for test-ing.
The remaining 8 thousand sentences are usedas a heldout set for smoothing the parameters.For the evaluation of the word segmentation ac-curacy, we randomly selected 5 thousand sentencesfrom the test set of 100 thousand sentences.
Wecall the first test set (100 thousand sentences) "testset-l" and the second test set (5 thousand sentences)"test set-T'.
Table 4 shows the number of sentences,words, and characters of the training and test sets.There were 94,680 distinct words in the trainingtest.
We discarded the words whose frequency wasone, and made a dictionary of 45,027 words.
Af-ter replacing the words whose frequency was onewith the corresponding unknown word tags, therewere 474,155 distinct word bigrams.
We discardedthe bigrams with frequency one, and the remaining175,527 bigrams were used in the word segmentationmodel.As for the unknown word model, word-based char-acter bigrams are computed from the words with281Table 5: Cross entropy (CE) per word and characterperplexity (PP) of each unknown word modelunknown word model CE per word char PPPoisson+zerogram 59.4 2032Poisson+bigram 37.8 128WT+Poisson+bigram 33.3 71frequency one (49,653 words).
There were 3,120 dis-tinct character unigrams and 55,486 distinct char-acter bigrams.
We discarded the bigram with fre-quency one and remaining 20,775 bigrams were used.There were 12,633 distinct character unigrams and80,058 distinct character bigrams when we classifiedthem for each word type and part of speech.
Wediscarded the bigrams with frequency one and re-maining 26,633 bigrams were used in the unknownword model.Average word lengths for each word type and partof speech were also computed from the words withfrequency one in the training set.4.2 Cross Entropy and PerplexityTable 5 shows the cross entropy per word and char-acter perplexity of three unknown word model.
Thefirst model is Equation (5), which is the combina-tion of Poisson distribution and character zerogram(Poisson + zerogram).
The second model is thecombination of Poisson distribution (Equation (6))and character bigram (Equation (7)) (Poisson + bi-gram).
The third model is Equation (11), which is aset of word models trained for each word type (WT+ Poisson + bigram).
Cross entropy was computedover the words in test set-1 that were not foundin the dictionary of the word segmentation model(56,121 words).
Character perplexity is more intu-itive than cross entropy because it shows the averagenumber of equally probable characters out of 6,879characters in JIS-X-0208.Table 5 shows that by changing the word spellingmodel from zerogram to big-ram, character perplex-ity is greatly reduced.
It also shows that by makinga separate model for each word type, character per-plexity is reduced by an additional 45% (128 -~ 71).This shows that the word type information is usefulfor modeling the morphology of Japanese words.4.3 Part  of Speech Predict ion Accuracywithout ContextFigure 3 shows the part of speech prediction accu-racy of two unknown word model without context.It shows the accuracies up to the top 10 candidates.The first model is Equation (12), which is a set ofword models trained for each part of speech (POS+ Poisson + bigram).
The second model is Equa-tion (13), which is a set of word models trained forPart of Speech Estimation Accuracy0.95 ~"~ .
.
.
.
.
.
~'**""0.9 / ' " "0.850.8 ~- / ~ + WT + Poisson + bigram -e-- N I// POS + Poisson + bigram --~---0.75 \ [ /0.651 2 3 4 5 6 7 8 9 10RankFigure 3: Accuracy of part of speech estimationeach part of speech and word type (POS + WT +Poisson + bigram).
The test words are the same56,121 words used to compute the cross entropy.Since these unknown word models give the prob-ability of spelling for each part of speech P(wlt), weused the empirical part of speech probability P(t)to compute the joint probability P(w, t).
The partof speech t that gives the highest joint probability isselected.= argmtaxP(w,t ) = P(t)P(wlt ) (18)The part of speech prediction accuracy of the firstand the second model was 67.5% and 74.4%, respec-tively.
As Figure 3 shows, word type informationimproves the prediction accuracy significantly.4.4 Word Segmentat ion AccuracyWord segmentation accuracy is expressed in termsof recall and precision as is done in the previousresearch (Sproat et al, 1996).
Let the number ofwords in the manually segmented corpus be Std, thenumber of words in the output of the word segmenterbe Sys, and the number of matched words be M.Recall is defined as M/Std, and precision is definedas M/Sys.
Since it is inconvenient touse both recalland precision all the time, we also use the F-measureto indicate the overall performance.
It is calculatedbyF= (f~2+l.0) xPxRf~2 x P + R (19)where P is precision, R is recall, and f~ is the relativeimportance given to recall over precision.
We set282Table 6: Word segmentation accuracy of all wordsrec prec FPoisson+bigram 94.5 93.1 93.8WT+Poisson+bigram 94.4 93.8 94.1POS+Poisson+bigram 94.4 93.6 94.0POS+WT+Poisson+bigram 94.6 93.7 94.1Table 7: Word segmentation accuracy of unknownwords64.1%.Other than the usual recall/precision measures,we defined another precision (prec2 in Table 8),which roughly correspond to the tagging accuracyin English where word segmentation is trivial.
Prec2is defined as the percentage of correctly tagged un-known words to the correctly segmented unknownwords.
Table 8 shows that tagging precision is im-proved from 88.2% to 96.6%.
The tagging accuracyin context (96.6%) is significantly higher than thatwithout context (74.4%).
This shows that the wordbigrams using unknown word tags for each part ofspeech are useful to predict he part of speech.rec prec FPoisson + bigram 31.8 65.0 42.7WT+Poisson+bigram 45.5 62.0 52.5POS+Poisson+bigram 39.7 61.5 48.3POS+WT+Poisson+bigram 42.0 66.4 51.4f~ = 1.0 throughout this experiment.
That is, weput equal importance on recall and precision.Table 6 shows the word segmentation accuracy offour unknown word models over test set-2.
Com-pared to the baseline model (Poisson + bigram), byusing word type and part of speech information, theprecision of the proposed model (POS + WT + Pois-son + bigram) is improved by a modest 0.6%.
Theimpact of the proposed model is small because theout-of-vocabulary rate of test set-2 is only 3.1%.To closely investigate the effect of the proposedunknown word model, we computed the word seg-mentation accuracy of unknown words.
Table 7shows the results.
The accuracy of the proposedmodel (POS + WT + Poisson + bigram) is signif-icantly higher than the baseline model (Poisson +bigram).
Recall is improved from 31.8% to 42.0%and precision is improved from 65.0% to 66.4%.Here, recall is the percentage of correctly seg-mented unknown words in the system output o theall unknown words in the test sentences.
Precisionis the percentage of correctly segmented unknownwords in the system's output o the all words thatsystem identified as unknown words.Table 8 shows the tagging accuracy of unknownwords.
Notice that the baseline model (Poisson +bigram) cannot predict part of speech.
To roughlyestimate the amount of improvement brought by theproposed model, we applied a simple tagging strat-egy to the output of the baseline model.
That is,words that include numbers are tagged as numbers,and others are tagged as nouns.Table 8 shows that by using word type and partof speech information, recall is improved from 28.1%to 40.6% and precision is improved from 57.3% to5 Re la ted  WorkSince English uses spaces between words, unknownwords can be identified by simple dictionary lookup.So the topic of interest is part of speech estimation.Some statistical model to estimate the part of speechof unknown words from the case of the first letterand the prefix and suffix is proposed (Weischedel etal., 1993; Brill, 1995; Ratnaparkhi, 1996; Mikheev,1997).
On the contrary, since Asian languages likeJapanese and Chinese do not put spaces betweenwords, previous work on unknown word problem isfocused on word segmentation; there are few studiesestimating part of speech of unknown words in Asianlanguages.The cues used for estimating the part of speech ofunknown words for Japanese in this paper are ba-sically the same for English, namely, the prefix andsuffix of the unknown word as well as the previousand following part of speech.
The contribution ofthis paper is in showing the fact that different char-acter sets behave differently in Japanese and a betterword model can be made by using this fact.By introducing different length models based oncharacter sets, the number of decomposition errorsof unknown words are significantly reduced.
In otherwords, the tendency of over-segmentation s cor-rected.
However, the spelling model, especially thecharacter bigrams in Equation (17) are hard to es-timate because of the data sparseness.
This is themain reason of the remaining under-segmented andover-segmented errors.To improve the unknown word model, feature-based approach such as the maximum entropymethod (Ratnaparkhi, 1996) might be useful, be-cause we don't have to divide the training data intoseveral disjoint sets (like we did by part of speechand word type) and we can incorporate more lin-guistic and morphological knowledge into the sameprobabilistic framework.
We are thinking of re-implementing our unknown word model using themaximum entropy method as the next step of ourresearch.283Table 8: Part of speech tagging accuracy of unknown words (the last column represents the percentage ofcorrectly tagged unknown words in the correctly segmented unknown words)rec prec F prec2Poisson+bigram 28.1 57.3 37.7 88.2WT+Poisson+bigram 37.7 51.5 43.5 87.9POS+Poisson+bigram 37.5 58.1 45.6 94.3POS+WT+Poisson+bigram 40.6 64.1 49.7 96.66 Conc lus ionWe present a statistical model of Japanese unknownwords using word morphology and word context.
Wefind that Japanese words are better modeled by clas-sifying words based on the character sets (kanji, hi-ragana, katakana, etc.)
and its changes.
This isbecause the different character sets behave differ-ently in many ways (historical etymology, ideogramvs.
phonogram, etc.).
Both word segmentation ac-curacy and part of speech tagging accuracy are im-proved by treating them differently.Re ferencesHarald Baayen and Richard Sproat.
1996.
Estimat-ing lexical priors for low-frequency morphologi-cally ambiguous forms.
Computational Linguis-tics, 22(2):155-166.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural anguage processing: A casestudy in part-of-speech tagging.
ComputationalLinguistics, 21(4):543-565.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, Jennifer C. Lal, and Robert L.Mercer.
1992.
An estimate of an upper bound forthe entropy of English.
Computational Linguis-tics, 18(1):31-40.Jing-Shin Chang, Yi-Chung Lin, and Keh-Yih Su.1995.
Automatic construction of a Chinese elec-tronic dictionary.
In Proceedings of the ThirdWorkshop on Very Large Corpora, pages 107-120.EDR.
1991.
EDR electronic dictionary version1 technical guide.
Technical Report TR2-003,Japan Electronic Dictionary Research Institute.Pascale Fung and Dekai Wu.
1994.
Statistical aug-mentation of a Chinese machine-readable dictio-nary.
In Proceedings of the Second Workshop onVery Large Corpora, pages 69-85.Masahiko Haruno and Yuji Matsumoto.
1997.Mistake-driven mixture of hierachical tag contexttrees.
In Proceedings of the 35th ACL and 8thEA CL, pages ~ 230-237.F.
Jelinek and R. L. Mercer.
1980.
Interpolated esti-mation of Markov source parameters from sparsedata.
In Proceedings of the Workshop on PatternRecognition in Practice, pages 381-397.Andrei Mikheev.
1997.
Automatic rule induction forunknown-word guessing.
Computational Linguis-tics, 23(3):405-423.Shinsuke Mori and Makoto Nagao.
1996.
Word ex-traction from corpora and its part-of-speech esti-mation using distributional analysis.
In Proceed-ings of the 16th International Conference on Com-putational Linguistics, pages 1119-1122.Masaaki Nagata.
1994.
A stochastic Japanese mor-phological analyzer using a forward-dp backward-A* n-best search algorithm.
In Proceedings of the15th International Conference on ComputationalLinguistics, pages 201-207.Masaaki Nagata.
1996.
Context-based spelling cor-rection for Japanese OCR.
In Proceedings of the16th International Conference on ComputationalLinguistics, pages 806-811.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof Conference on Empirical Methods in NaturalLanguage Processing, pages 133-142.Richard Sproat, Chilin Shih, William Gale, andNancy Chang.
1996.
A stochastic finite-stateword-segmentation algorithm for Chinese.
Com-putational Linguistics, 22(3):377-404.Koichi Takeuchi and Yuji Matsumoto.
1997.
HMMparameter learning for Japanese morphologicalanalyzer.
Transaction of Information Processingof Japan, 38(3):500-509.
(in Japanese).Ralph Weischedel, Marie Meteer, Richard Schwartz,Lance Ramshaw, and Jeff Palmucci.
1993.
Cop-ing with ambiguity and unknown words throughprobabilistic models.
Computational Linguistics,19(2):359-382.Ian H. Witten and Timothy C. Bell.
1991.
Thezero-frequency problem: Estimating the proba-bilities of novel events in adaptive text compres-sion.
IEEE Transaction on Information Theory,37(4):1085-1094.Mikio Yamamoto.
1996.
A re-estimation method forstochastic language modeling from ambiguous ob-servations.
In Proceedings of the Fourth Workshopon Very Large Corpora, pages 155-167.284
