Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 80?87,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn annotation scheme for citation functionSimone Teufel Advaith Siddharthan Dan TidharNatural Language and Information Processing GroupComputer LaboratoryCambridge University, CB3 0FD, UK{Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.ukAbstractWe study the interplay of the discourse struc-ture of a scientific argument with formal ci-tations.
One subproblem of this is to clas-sify academic citations in scientific articles ac-cording to their rhetorical function, e.g., as arival approach, as a part of the solution, oras a flawed approach that justifies the cur-rent research.
Here, we introduce our anno-tation scheme with 12 categories, and presentan agreement study.1 Scientific writing, discourse structureand citationsIn recent years, there has been increasing interest inapplying natural language processing technologies toscientific literature.
The overwhelmingly large num-ber of papers published in fields like biology, geneticsand chemistry each year means that researchers needtools for information access (extraction, retrieval, sum-marization, question answering etc).
There is also in-creased interest in automatic citation indexing, e.g.,the highly successful search tools Google Scholar andCiteSeer (Giles et al, 1998).1 This general interest inimproving access to scientific articles fits well with re-search on discourse structure, as knowledge about theoverall structure and goal of papers can guide better in-formation access.Shum (1998) argues that experienced researchers areoften interested in relations between articles.
Theyneed to know if a certain article criticises another andwhat the criticism is, or if the current work is basedon that prior work.
This type of information is hardto come by with current search technology.
Neitherthe author?s abstract, nor raw citation counts help usersin assessing the relation between articles.
And eventhough CiteSeer shows a text snippet around the phys-ical location for searchers to peruse, there is no guar-antee that the text snippet provides enough informationfor the searcher to infer the relation.
In fact, studiesfrom our annotated corpus (Teufel, 1999), show that69% of the 600 sentences stating contrast with otherwork and 21% of the 246 sentences stating researchcontinuation with other work do not contain the cor-responding citation; the citation is found in preceding1CiteSeer automatically citation-indexes all scientific ar-ticles reached by a web-crawler, making them available tosearchers via authors or keywords in the title.Nitta and Niwa 94Resnik 95Brown et al 90aRose et al 90Church and Gale 91Dagan et al 94Li and Abe 96Hindle 93Hindle 90Dagan et al93Pereira et al 93Following Pereira et al we measureword similarity by the relative entropyor Kulbach?Leibler (KL) distance, bet?ween the corresponding conditionaldistributions.His notion of similarityseems to agree with ourintuitions in many cases,but it is not clear how itcan  be used directly toconstruct word classesand corresponding models of association.Figure 1: A rhetorical citation mapsentences (i.e., the sentence expressing the contrast orcontinuation would be outside the CiteSeer snippet).We present here an approach which uses the classifica-tion of citations to help provide relational informationacross papers.Citations play a central role in the process of writinga paper.
Swales (1990) argues that scientific writingfollows a general rhetorical argumentation structure:researchers must justify that their paper makes a con-tribution to the knowledge in their discipline.
Severalargumentation steps are required to make this justifica-tion work, e.g., the statement of their specific goal inthe paper (Myers, 1992).
Importantly, the authors alsomust relate their current work to previous research, andacknowledge previous knowledge claims; this is donewith a formal citation, and with language connectingthe citation to the argument, e.g., statements of usage ofother people?s approaches (often near textual segmentsin the paper where these approaches are described), andstatements of contrast with them (particularly in thediscussion or related work sections).
We argue that theautomatic recognition of citation function is interest-ing for two reasons: a) it serves to build better citationindexers and b) in the long run, it will help constraininterpretations of the overall argumentative structure ofa scientific paper.Being able to interpret the rhetorical status of a ci-tation at a glance would add considerable value to ci-tation indexes, as shown in Fig.
1.
Here differencesand similarities are shown between the example paper(Pereira et al, 1993) and the papers it cites, as well as80the papers that cite it.
Contrastive links are shown ingrey ?
links to rival papers and papers the current pa-per contrasts itself to.
Continuative links are shown inblack ?
links to papers that are taken as starting point ofthe current research, or as part of the methodology ofthe current paper.
The most important textual sentenceabout each citation could be extracted and displayed.For instance, we see which aspect of Hindle (1990) thePereira et al paper criticises, and in which way Pereiraet al?s work was used by Dagan et al (1994).We present an annotation scheme for citations, basedon empirical work in content citation analysis, whichfits into this general framework of scientific argumentstructure.
It consists of 12 categories, which allow usto mark the relationships of the current paper with thecited work.
Each citation is labelled with exactly onecategory.
The following top-level four-way distinctionapplies:?
Weakness: Authors point out a weakness in citedwork?
Contrast: Authors make contrast/comparison withcited work (4 categories)?
Positive: Authors agree with/make use of/showcompatibility or similarity with cited work (6 cat-egories), and?
Neutral: Function of citation is either neutral, orweakly signalled, or different from the three func-tions stated above.We first turn to the point of how to classify citationfunction in a robust way.
Later in this paper, we willreport results for a human annotation experiment withthree annotators.2 Annotation schemes for citationsIn the field of library sciences (more specifically, thefield of Content Citation Analysis), the use of informa-tion from citations above and beyond simple citationcounting has received considerable attention.
Biblio-metric measures assesses the quality of a researcher?soutput, in a purely quantitative manner, by countinghow many papers cite a given paper (White, 2004;Luukkonen, 1992) or by more sophisticated measureslike the h-index (Hirsch, 2005).
But not all citationsare alike.
Researchers in content citation analysis havelong stated that the classification of motivations is acentral element in understanding the relevance of thepaper in the field.
Bonzi (1982), for example, points outthat negational citations, while pointing to the fact thata given work has been noticed in a field, do not meanthat that work is received well, and Ziman (1968) statesthat many citations are done out of ?politeness?
(to-wards powerful rival approaches), ?policy?
(by name-dropping and argument by authority) or ?piety?
(to-wards one?s friends, collaborators and superiors).
Re-searchers also often follow the custom of citing some1.
Cited source is mentioned in the introduction ordiscussion as part of the history and state of theart of the research question under investigation.2.
Cited source is the specific point of departure forthe research question investigated.3.
Cited source contains the concepts, definitions,interpretations used (and pertaining to the disci-pline of the citing article).4.
Cited source contains the data (pertaining to thediscipline of the citing article) which are usedsporadically in the article.5.
Cited source contains the data (pertaining to thediscipline of the citing particle) which are usedfor comparative purposes, in tables and statistics.6.
Cited source contains data and material (fromother disciplines than citing article) which isused sporadically in the citing text, in tables orstatistics.7.
Cited source contains the method used.8.
Cited source substantiated a statement or assump-tion, or points to further information.9.
Cited source is positively evaluated.10.
Cited source is negatively evaluated.11.
Results of citing article prove, verify, substantiatethe data or interpretation of cited source.12.
Results of citing article disprove, put into ques-tion the data as interpretation of cited source.13.
Results of citing article furnish a new interpreta-tion/explanation to the data of the cited source.Figure 2: Spiegel-Ru?sing?s (1977) Categories for Cita-tion Motivationsparticular early, basic paper, which gives the founda-tion of their current subject (?paying homage to pio-neers?).
Many classification schemes for citation func-tions have been developed (Weinstock, 1971; Swales,1990; Oppenheim and Renn, 1978; Frost, 1979; Chu-bin and Moitra, 1975), inter alia.
Based on such an-notation schemes and hand-analyzed data, different in-fluences on citation behaviour can be determined, butannotation in this field is usually done manually onsmall samples of text by the author, and not confirmedby reliability studies.
As one of the earliest such stud-ies, Moravcsik and Murugesan (1975) divide citationsin running text into four dimensions: conceptual oroperational use (i.e., use of theory vs. use of techni-cal method); evolutionary or juxtapositional (i.e., ownwork is based on the cited work vs. own work is an al-ternative to it); organic or perfunctory (i.e., work is cru-cially needed for understanding of citing article or justa general acknowledgement); and finally confirmativevs.
negational (i.e., is the correctness of the findingsdisputed?).
They found, for example, that 40% of thecitations were perfunctory, which casts further doubton the citation-counting approach.Other content citation analysis research which is rel-81evant to our work concentrates on relating textual spansto authors?
descriptions of other work.
For example, inO?Connor?s (1982) experiment, citing statements (oneor more sentences referring to other researchers?
work)were identified manually.
The main problem encoun-tered in that work is the fact that many instances of cita-tion context are linguistically unmarked.
Our data con-firms this: articles often contain large segments, par-ticularly in the central parts, which describe other peo-ple?s research in a fairly neutral way.
We would thusexpect many citations to be neutral (i.e., not to carryany function relating to the argumentation per se).Many of the distinctions typically made in contentcitation analysis are immaterial to the task consideredhere as they are too sociologically orientated, and canthus be difficult to operationalise without deep knowl-edge of the field and its participants (Swales, 1986).
Inparticular, citations for general reference (backgroundmaterial, homage to pioneers) are not part of our an-alytic interest here, and so are citations ?in passing?,which are only marginally related to the argumentationof the overall paper (Ziman, 1968).Spiegel-Ru?sing?s (1977) scheme (Fig.
2) is an exam-ple of a scheme which is easier to operationalise thanmost.
In her scheme, more than one category can applyto a citation; for instance positive and negative evalu-ation (category 9 and 10) can be cross-classified withother categories.
Out of 2309 citations examined, 80%substantiated statements (category 8), 6% discussedhistory or state of the art of the research area (cate-gory 1) and 5% cited comparative data (category 5).Category DescriptionWeak Weakness of cited approachCoCoGM Contrast/Comparison in Goals or Meth-ods (neutral)CoCoR0 Contrast/Comparison in Results (neutral)CoCo- Unfavourable Contrast/Comparison (cur-rent work is better than cited work)CoCoXY Contrast between 2 cited methodsPBas author uses cited work as starting pointPUse author uses tools/algorithms/dataPModi author adapts or modifiestools/algorithms/dataPMot this citation is positive about approach orproblem addressed (used to motivate workin current paper)PSim author?s work and cited work are similarPSup author?s work and cited work are compat-ible/provide support for each otherNeut Neutral description of cited work, or notenough textual evidence for above cate-gories or unlisted citation functionFigure 3: Our annotation scheme for citation functionOur scheme (given in Fig.
3) is an adaptation of thescheme in Fig.
2, which we arrived at after an analysisof a corpus of scientific articles in computational lin-guistics.
We tried to redefine the categories such thatthey should be reasonably reliably annotatable; at thesame time, they should be informative for the appli-cation we have in mind.
A third criterion is that theyshould have some (theoretical) relation to the particu-lar discourse structure we work with (Teufel, 1999).Our categories are as follows: One category (Weak)is reserved for weakness of previous research, if it is ad-dressed by the authors (cf.
Spiegel-Ru?sing?s categories10, 12, possibly 13).
The next three categories describecomparisons or contrasts between own and other work(cf.
Spiegel-Ru?sing?s category 5).
The difference be-tween them concerns whether the comparison is be-tween methods/goals (CoCoGM) or results (CoCoR0).These two categories are for comparisons without ex-plicit value judgements.
We use a different category(CoCo-) when the authors claim their approach is bet-ter than the cited work.Our interest in differences and similarities betweenapproaches stems from one possible application wehave in mind (the rhetorical citation search tool).
Wedo not only consider differences stated between the cur-rent work and other work, but we also mark citations ifthey are explicitly compared and contrasted with otherwork (not the current paper).
This is expressed in cat-egory CoCoXY.
It is a category not typically consid-ered in the literature, but it is related to the other con-trastive categories, and useful to us because we thinkit can be exploited for search of differences and rivalapproaches.The next set of categories we propose concerns pos-itive sentiment expressed towards a citation, or a state-ment that the other work is actively used in the cur-rent work (which is the ultimate praise).
Like Spiegel-Ru?sing, we are interested in use of data and methods(her categories 4, 5, 6, 7), but we cluster different us-ages together and instead differentiate unchanged use(PUse) from use with adaptations (PModi).
Workwhich is stated as the explicit starting point or intellec-tual ancestry is marked with our category PBas (hercategory 2).
If a claim in the literature is used tostrengthen the authors?
argument, this is expressed inher category 8, and vice versa, category 11.
We col-lapse these two in our category PSup.
We use twocategories she does not have definitions for, namelysimilarity of (aspect of) approach to other approach(PSim), and motivation of approach used or problemaddressed (PMot).
We found evidence for prototypi-cal use of these citation functions in our texts.
How-ever, we found little evidence for her categories 12 or13 (disproval or new interpretation of claims in citedliterature), and we decided against a ?state-of-the-art?category (her category 1), which would have been inconflict with our PMot definition in many cases.Our fourteenth category, Neut, bundles truly neutraldescriptions of other researchers?
approaches with allthose cases where the textual evidence for a citationfunction was not enough to warrant annotation of thatcategory, and all other functions for which our schemedid not provide a specific category.
As stated above, wedo in fact expect many of our citations to be neutral.82Citation function is hard to annotate because it inprinciple requires interpretation of author intentions(what could the author?s intention have been in choos-ing a certain citation?).
Typical results of earlier cita-tion function studies are that the sociological aspect ofciting is not to be underestimated.
One of our most fun-damental ideas for annotation is to only mark explicitlysignalled citation functions.
Our guidelines explicitlystate that a general linguistic phrase such as ?better?or ?used by us?
must be present, in order to increaseobjectivity in finding citation function.
Annotators areencouraged to point to textual evidence they have forassigning a particular function (and are asked to typethe source of this evidence into the annotation tool foreach citation).
Categories are defined in terms of cer-tain objective types of statements (e.g., there are 7 casesfor PMot).
Annotators can use general text interpreta-tion principles when assigning the categories, but arenot allowed to use in-depth knowledge of the field orof the authors.There are other problematic aspects of the annota-tion.
Some concern the fact that authors do not al-ways state their purpose clearly.
For instance, severalearlier studies found that negational citations are rare(Moravcsik and Murugesan, 1975; Spiegel-Ru?sing,1977); MacRoberts and MacRoberts (1984) argue thatthe reason for this is that they are potentially politicallydangerous, and that the authors go through lengths todiffuse the impact of negative references, hiding a neg-ative point behind insincere praise, or diffusing thethrust of criticism with perfunctory remarks.
In ourdata we found ample evidence of this effect, illustratedby the following example:Hidden Markov Models (HMMs) (Huanget al 1990) offer a powerful statistical ap-proach to this problem, though it is unclearhow they could be used to recognise the unitsof interest to phonologists.
(9410022, S-24)2It is also sometimes extremely hard to distinguishusage of a method from statements of similarity be-tween a method and the own method.
This happensin cases where authors do not want to admit they areusing somebody else?s method:The same test was used in Abney and Light(1999).
(0008020, S-151)Unication of indices proceeds in the samemanner as unication of all other typedfeature structures (Carpenter 1992).
(0008023, S-87)In this case, our annotators had to choose betweencategories PSim and PUse.It can also be hard to distinguish between continu-ation of somebody?s research (i.e., taking somebody?s2In all corpus examples, numbers in brackets correspondto the official Cmp lg archive number, ?S-?
numbers to sen-tence numbers according to our preprocessing.research as starting point, as intellectual ancestry, i.e.PBas) and simply using it (PUse).
In principle, onewould hope that annotation of all usage/positive cate-gories (starting with P), if clustered together, should re-sult in higher agreement (as they are similar, and as theresulting scheme has fewer distinctions).
We would ex-pect this to be the case in general, but as always, casesexist where a conflict between a contrast (CoCo) and achange to a method (PModi) occur:In contrast to McCarthy, Kay and Kiraz,we combine the three components into a sin-gle projection.
(0006044, S-182)The markable units in our scheme are a) all full cita-tions (as recognized by our automatic citation proces-sor on our corpus), and b) all names of authors of citedpapers anywhere in running text outside of a formalcitation context (i.e., without date).
Our citation pro-cessor recognizes these latter names after parsing thecitation list an marks them up.
This is unusual in com-parison to other citation indexers, but we believe thesenames function as important referents comparable inimportance to formal citations.
In principle, one couldgo even further as there are many other linguistic ex-pressions by which the authors could refer to other peo-ple?s work: pronouns, abbreviations such as ?Muellerand Sag (1990), henceforth M & S?, and names of ap-proaches or theories which are associated with partic-ular authors.
If we could mark all of these up auto-matically (which is not technically possible), annota-tion would become less difficult to decide, but techni-cal difficulty prevent us from recognizing these othercases automatically.
As a result, in these contexts it isimpossible to annotate citation function directly on thereferent, which sometimes causes problems.
Becausethis means that annotators have to consider non-localcontext, one markable may have different competingcontexts with different potential citation functions, andproblems about which context is ?stronger?
may oc-cur.
We have rules that context is to be constrained tothe paragraph boundary, but for some categories paper-wide information is required (e.g., for PMot, we needto know that a praised approach is used by the authors,information which may not be local in the paragraph).Appendix A gives unambiguous example caseswhere the citation function can be decided on the ba-sis of the sentence alone, but Fig.
4 shows a more typ-ical example where more context is required to inter-pret the function.
The evaluation of the citation Hin-dle (1990) is contrastive; the evaluative statement isfound 4 sentences after the sentence containing the ci-tation3.
It consists of a positive statement (agreementwith authors?
view), followed by a weakness, under-lined, which is the chosen category.
This is marked onthe nearest markable (Hindle, 3 sentences after the ci-tation).3In Fig.
4, markables are shown in boxes, evaluative state-ments underlined, and referents in bold face.83S-5 Hindle (1990)/Neut proposed dealing with thesparseness problem by estimating the likelihood of un-seen events from that of ?similar?
events that have beenseen.S-6 For instance, one may estimate the likelihood of aparticular direct object for a verb from the likelihoodsof that direct object for similar verbs.S-7 This requires a reasonable definition of verb simi-larity and a similarity estimation method.S-8 In Hindle/Weak ?s proposal, words are similarif we have strong statistical evidence that they tend toparticipate in the same events.S-9 His notion of similarity seems to agree with our in-tuitions in many cases, but it is not clear how it can beused directly to construct word classes and correspond-ing models of association.
(9408011)Figure 4: Annotation example: influence of contextA naive view on this annotation scheme could con-sider the first two sets of categories in our scheme as?negative?
and the third set of categories ?positive?.There is indeed a sentiment aspect to the interpretationof citations, due to the fact that authors need to makea point in their paper and thus have a stance towardstheir citations.
But this is not the whole story: manyof our ?positive?
categories are more concerned withdifferent ways in which the cited work is useful to thecurrent work (which aspect of it is used, e.g., just adefinition or the entire solution?
), and many of the con-trastive statements have no negative connotation at alland simply state a (value-free) difference between ap-proaches.
However, if one looks at the distribution ofpositive and negative adjectives around citations, onenotices a (non-trivial) connection between our task andsentiment classification.There are written guidelines of 25 pages, which in-struct the annotators to only assign one category percitation, and to skim-read the paper before annotation.The guidelines provide a decision tree and give deci-sion aids in systematically ambiguous cases, but sub-jective judgement of the annotators is nevertheless nec-essary to assign a single tag in an unseen context.
Weimplemented an annotation tool based on XML/XSLTtechnology, which allows us to use any web browser tointeractively assign one of the 12 tags (presented as apull-down list) to each citation.3 DataThe data we used came from the CmpLg (Computationand Language archive; 320 conference articles in com-putational linguistics).
The articles are in XML format.Headlines, titles, authors and reference list items areautomatically marked up with the corresponding tags.Reference lists are parsed, and cited authors?
namesare identified.
Our citation parser then applies regu-lar patterns and finds citations and other occurrences ofthe names of cited authors (without a date) in runningtext and marks them up.
Self-citations are detected byoverlap of citing and cited authors.
The citation pro-cessor developped in our group (Ritchie et al, 2006)achieves high accuracy for this task (96% of citationsrecognized, provided the reference list was error-free).On average, our papers contain 26.8 citation instancesin running text4.4 Human Annotation: resultsIn order to machine learn citation function, we arein the process of creating a corpus of scientific arti-cles with human annotated citations, according to thescheme discussed before.
Here we report preliminaryresults with that scheme, with three annotators who aredevelopers of the scheme.In our experiment, the annotators independently an-notated 26 conference articles with this scheme, on thebasis of guidelines which were frozen once annotationstarted5.
The data used for the experiment contained atotal of 120,000 running words and 548 citations.The relative frequency of each category observed inthe annotation is listed in Fig.
5.
As expected, the dis-tribution is very skewed, with more than 60% of thecitations of category Neut.6 What is interesting is therelatively high frequency of usage categories (PUse,PModi, PBas) with a total of 18.9%.
There isa relatively low frequency of clearly negative cita-tions (Weak, CoCoR-, total of 4.1%), whereas theneutral?contrastive categories (CoCoGM, CoCoR0,CoCoXY) are slightly more frequent at 7.6%.
Thisis in concordance with earlier annotation experiments(Moravcsik and Murugesan, 1975; Spiegel-Ru?sing,1977).We reached an inter-annotator agreement of K=.72(n=12;N=548;k=3)7.
This is comparable to aggreementon other discourse annotation tasks such as dialogueact parsing and Argumentative Zoning (Teufel et al,1999).
We consider the agreement quite good, consid-ering the number of categories and the difficulties (e.g.,non-local dependencies) of the task.The annotators are obviously still disagreeing onsome categories.
We were wondering to what de-gree the fine granularity of the scheme is a prob-lem.
When we collapsed the obvious similar cat-egories (all P categories into one category, andall CoCo categories into another) to give four toplevel categories (Weak, Positive, Contrast,Neutral), this only raised kappa to 0.76.
This4As opposed to reference list items, which are fewer.5The development of the scheme was done with 40+ dif-ferent articles.6Spiegel-Ru?sing found that out of 2309 citations she ex-amined, 80% substantiated statements.7Following Carletta (1996), we measure agreement inKappa, which follows the formula K = P (A)?P (E)1?P (E) whereP(A) is observed, and P(E) expected agreement.
Kapparanges between -1 and 1.
K=0 means agreement is only asexpected by chance.
Generally, Kappas of 0.8 are consideredstable, and Kappas of .69 as marginally stable, according tothe strictest scheme applied in the field.84Neut PUse CoCoGM PSim Weak CoCoXY PMot PModi PBas PSup CoCo- CoCoR062.7% 15.8% 3.9% 3.8% 3.1% 2.9% 2.2% 1.6% 1.5% 1.1% 1.0% 0.8%Figure 5: Distribution of the categoriesWeak CoCo- CoCoGM CoCoR0 CoCoXY PUse PBas PModi PMot PSim PSup NeutWeak 5 3CoCo- 1 3CoCoGM 23 3CoCoR0 4CoCoXY 1PUse 86 6 2 1 12PBas 3 2PModi 3PMot 13 4PSim 3 20 5PSup 1 2 1Neut 6 10 6 4 17 1 6 4 287Figure 6: Confusion matrix between two annotatorspoints to the fact that most of our annotators disagreedabout whether to assign a more informative categoryor Neut, the neutral fall-back category.
Unfortunately,Kappa is only partially sensitive to such specialised dis-agreements.
While it will reward agreement with in-frequent categories more than agreement with frequentcategories, it nevertheless does not allow us to weightdisagreements we care less about (Neut vs more in-formative category) less than disagreements we do carea lot about (informative categories which are mutuallyexclusive, such as Weak and PSim).Fig.
6 shows a confusion matrix between the two an-notators who agreed most with each other.
This againpoints to the fact that a large proportion of the confu-sion involves an informative category and Neut.
Theissue with Neut and Weak is a point at hand: au-thors seem to often (deliberately or not) mask their in-tended citation function with seemingly neutral state-ments.
Many statements of weakness of other ap-proaches were stated in such caged terms that our anno-tators disagreed about whether the signals given were?explicit?
enough.While our focus is not sentiment analysis, it is pos-sible to conflate our 12 categories into three: positive,weakness and neutral by the following mapping:Old Categories New CategoryWeak, CoCo- NegativePMot, PUse, PBas, PModi, PSim, PSup PositiveCoCoGM, CoCoR0, CoCoXY, Neut NeutralThus negative contrasts and weaknesses are groupedinto Negative, while neutral contrasts are groupedinto Neutral.
All the positive classes are conflatedinto Positive.
This resulted in kappa=0.75 for threeannotators.Fig.
7 shows the confusion matrix between two an-notators for this sentiment classification.
Fig.
7 is par-ticularly instructive, because it shows that annotatorsWeakness Positive NeutralWeakness 9 1 12Positive 140 13Neutral 4 30 339Figure 7: Confusion matrix between two annotators;categories collapsed to reflect sentimenthave only one case of confusion between positive andnegative references to cited work.
The vast majority ofdisagreements reflects genuine ambiguity as to whetherthe authors were trying to stay neutral or express a sen-timent.Distinction KappaPMot v. all others .790CoCoGM v. all others .765PUse v. all others .761CoCoR0 v. all others .746Neut v. all others .742PSim v. all others .649PModi v. all others .553CoCoXY v. all others .553Weak v. all others .522CoCo- v. all others .462PBas v. all others .414PSup v. all others .268Figure 8: Distinctiveness of categoriesIn an attempt to determine how well each cate-gory was defined, we created artificial splits of thedata into binary distinctions: each category versus asuper-category consisting of all the other collapsed cat-egories.
The kappas measured on these datasets aregiven in Fig.
8.
The higher they are, the better the anno-tators could distinguish the given category from all theother categories.
We can see that out of the informa-85tive categories, four are defined at least as well as theoverall distinction (i.e.
above the line in Fig.
8: PMot,PUse, CoCoGM and CoCoR0.
This is encouraging,as the application of citation maps is almost entirelycentered around usage and contrast.
However, the se-mantics of some categories are less well-understood byour annotators: in particular PSup (where the difficultylies in what an annotator understands as ?mutual sup-port?
of two theories), and (unfortunately) PBas.
Theproblem with PBas is that its distinction from PUse isbased on subjective judgement of whether the authorsuse a part of somebody?s previous work, or base them-selves entirely on this previous work (i.e., see them-selves as following in the same intellectual framework).Another problem concerns the low distinctivity for theclearly negative categories CoCo- and Weak.
This isin line with MacRoberts and MacRoberts?
hypothesisthat criticism is often hedged and not clearly lexicallysignalled, which makes it more difficult to reliably an-notate such citations.5 ConclusionWe have described a new task: human annotation ofcitation function, a phenomenon which we believe tobe closely related to the overall discourse structure ofscientific articles.
Our annotation scheme concentrateson contrast, weaknesses of other work, similarities be-tween work and usage of other work.
One of its prin-ciples is the fact that relations are only to be marked ifthey are explicitly signalled.
Here, we report positiveresults in terms of interannotator agreement.Future work on the annotation scheme will concen-trate on improving guidelines for currently suboptimalcategories, and on measuring intra-annotator agree-ment and inter-annotator agreement with naive annota-tors.
We are also currently investigating how well ourscheme will work on text from a different discipline,namely chemistry.
Work on applying machine learningtechniques for automatic citation classification is cur-rently underway (Teufel et al, 2006); the agreementof one annotator and the system is currently K=.57,leaving plenty of room for improvement in comparisonwith the human annotation results presented here.6 AcknowledgementsThis work was funded by the EPSRC projectsCITRAZ (GR/S27832/01, ?Rhetorical Citation Mapsand Domain-independent Argumentative Zoning?)
andSCIBORG (EP/C010035/1, ?Extracting the Sciencefrom Scientific Publications?
).ReferencesSusan Bonzi.
1982.
Characteristics of a literature as predic-tors of relatedness between cited and citing works.
JASIS,33(4):208?216.Jean Carletta.
1996.
Assessing agreement on classificationtasks: The kappa statistic.
Computational Linguistics,22(2):249?254.Daryl E. Chubin and S. D. Moitra.
1975.
Content analysisof references: Adjunct or alternative to citation counting?Social Studies of Science, 5(4):423?441.Carolyn O.
Frost.
1979.
The use of citations in literary re-search: A preliminary classification of citation functions.Library Quarterly, 49:405.C.
Lee Giles, Kurt D. Bollacker, and Steve Lawrence.
1998.Citeseer: An automatic citation indexing system.
In Pro-ceedings of the Third ACM Conference on Digital Li-braries, pages 89?98.Jorge E. Hirsch.
2005.
An index to quantify an individ-ual?s scientific research output.
Proceedings of the Na-tional Academy of Sciences of the United Stated of Amer-ica (PNAS), 102(46).Terttu Luukkonen.
1992.
Is scientists?
publishing behaviourreward-seeking?
Scientometrics, 24:297?319.Michael H. MacRoberts and Barbara R. MacRoberts.
1984.The negational reference: Or the art of dissembling.
So-cial Studies of Science, 14:91?94.Michael J. Moravcsik and Poovanalingan Murugesan.
1975.Some results on the function and quality of citations.
So-cial Studies of Science, 5:88?91.Greg Myers.
1992.
In this paper we report...?speech actsand scientific facts.
Journal of Pragmatics, 17(4):295?313.John O?Connor.
1982.
Citing statements: Computer recogni-tion and use to improve retrieval.
Information Processingand Management, 18(3):125?131.Charles Oppenheim and Susan P. Renn.
1978.
Highly citedold papers and the reasons why they continue to be cited.JASIS, 29:226?230.Anna Ritchie, Simone Teufel, and Steven Robertson.
2006.Creating a test collection for citation-based IR experi-ments.
In Proceedings of HLT-06.Simon Buckingham Shum.
1998.
Evolving the web for sci-entific knowledge: First steps towards an ?HCI knowledgeweb?.
Interfaces, British HCI Group Magazine, 39:16?21.Ina Spiegel-Ru?sing.
1977.
Bibliometric and content analy-sis.
Social Studies of Science, 7:97?113.John Swales.
1986.
Citation analysis and discourse analysis.Applied Linguistics, 7(1):39?56.John Swales, 1990.
Genre Analysis: English in Academicand Research Settings.
Chapter 7: Research articles in En-glish, pages 110?176.
Cambridge University Press, Cam-bridge, UK.Simone Teufel, Jean Carletta, and Marc Moens.
1999.
Anannotation scheme for discourse-level argumentation in re-search articles.
In Proceedings of the Ninth Meeting of theEuropean Chapter of the Association for ComputationalLinguistics (EACL-99), pages 110?117.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006.Automatic classification of citation function.
In Proceed-ings of EMNLP-06.Simone Teufel.
1999.
Argumentative Zoning: InformationExtraction from Scientific Text.
Ph.D. thesis, School ofCognitive Science, University of Edinburgh, UK.Melvin Weinstock.
1971.
Citation indexes.
In Encyclopediaof Library and Information Science, volume 5, pages 16?40.
Dekker, New York, NY.Howard D. White.
2004.
Citation analysis and discourseanalysis revisited.
Applied Linguistics, 25(1):89?116.John M. Ziman.
1968.
Public Knowledge: An Essay Con-cerning the Social Dimensions of Science.
CambridgeUniversity Press, Cambridge, UK.86A Annotation examplesWeak However, Koskenniemi himself understood that his initial implementation had signif-icant limitations in handling non-concatenative morphotactic processes.
(0006044, S-4)CoCoGM The goals of the two papers are slightly different: Moore ?s approach is designed toreduce the total grammar size (i.e., the sum of the lengths of the productions), whileour approach minimizes the number of productions.
(0008021, S-22)CoCoR0 This is similar to results in the literature (Ramshaw and Marcus 1995).
(0008022, S-147)CoCo- For the Penn Treebank, Ratnaparkhi (1996) reports an accuracy of 96.6% using theMaximum Entropy approach, our much simpler and therefore faster HMM approachdelivers 96.7%.
(0003055, S-156)CoCoXY Unlike previous approaches (Ellison 1994, Walther 1996), Karttunen ?s approachis encoded entirely in the nite state calculus, with no extra-logical procedures forcounting constraint violations.
(0006038, S-5)PBas Our starting point is the work described in Ferro et al (1999) , which used a fairlysmall training set.
(0008004, S-11)PUse In our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen etal.
1996).
(0003060, S-105)PModi In our experiments, we have used a conjugate-gradient optimization program adaptedfrom the one presented in Press et al (0008028, S-72)PMot It has also been shown that the combined accuracy of an ensemble of multiple clas-siers is often signicantly greater than that of any of the individual classiers thatmake up the ensemble (e.g., Dietterich (1997)).
(0005006, S-9)PSim Our system is closely related to those proposed in Resnik (1997) and Abney andLight (1999).
(0008020, S-24)PSup In all experiments the SVM Light system outperformed other learning algorithms,which conrms Yang and Liu ?s (1999) results for SVMs fed with Reuters data.
(0003060, S-141)Neut The cosine metric and Jaccard?s coefcient are commonly used in information re-trieval as measures of association (Salton and McGill 1983).
(0001012, S-29)87
