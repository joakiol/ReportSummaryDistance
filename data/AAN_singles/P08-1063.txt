Proceedings of ACL-08: HLT, pages 550?558,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRobustness and Generalization of Role Sets: PropBank vs. VerbNetBen?at Zapirain and Eneko AgirreIXA NLP GroupUniversity of the Basque Country{benat.zapirain,e.agirre}@ehu.esLlu?
?s Ma`rquezTALP Research CenterTechnical University of Catalonialluism@lsi.upc.eduAbstractThis paper presents an empirical study on therobustness and generalization of two alterna-tive role sets for semantic role labeling: Prop-Bank numbered roles and VerbNet thematicroles.
By testing a state?of?the?art SRL sys-tem with the two alternative role annotations,we show that the PropBank role set is morerobust to the lack of verb?specific semanticinformation and generalizes better to infre-quent and unseen predicates.
Keeping in mindthat thematic roles are better for applicationneeds, we also tested the best way to generateVerbNet annotation.
We conclude that taggingfirst PropBank roles and mapping into Verb-Net roles is as effective as training and taggingdirectly on VerbNet, and more robust for do-main shifts.1 IntroductionSemantic Role Labeling is the problem of analyzingclause predicates in open text by identifying argu-ments and tagging them with semantic labels indi-cating the role they play with respect to the verb.Such sentence?level semantic analysis allows to de-termine ?who?
did ?what?
to ?whom?, ?when?
and?where?, and, thus, characterize the participants andproperties of the events established by the predi-cates.
This kind of semantic analysis is very inter-esting for a broad spectrum of NLP applications (in-formation extraction, summarization, question an-swering, machine translation, etc.
), since it opensthe door to exploit the semantic relations among lin-guistic constituents.The properties of the semantically annotated cor-pora available have conditioned the type of researchand systems that have been developed so far.
Prop-Bank (Palmer et al, 2005) is the most widely usedcorpus for training SRL systems, probably becauseit contains running text from the Penn Treebank cor-pus with annotations on all verbal predicates.
Also,a few evaluation exercises on SRL have been con-ducted on this corpus in the CoNLL-2004 and 2005conferences.
However, a serious criticisms to thePropBank corpus refers to the role set it uses, whichconsists of a set of numbered core arguments, whosesemantic translation is verb-dependent.
While Arg0and Arg1 are intended to indicate the general rolesof Agent and Theme, other argument numbers donot generalize across verbs and do not correspondto general semantic roles.
This fact might compro-mise generalization and portability of SRL systems,especially when the training corpus is small.More recently, a mapping from PropBank num-bered arguments into VerbNet thematic roles hasbeen developed and a version of the PropBank cor-pus with thematic roles has been released (Loper etal., 2007).
Thematic roles represent a compact set ofverb-independent general roles widely used in lin-guistic theory (e.g., Agent, Theme, Patient, Recipi-ent, Cause, etc.).
We foresee two advantages of us-ing such thematic roles.
On the one hand, statisti-cal SRL systems trained from them could generalizebetter and, therefore, be more robust and portable,as suggested in (Yi et al, 2007).
On the other hand,roles in a paradigm like VerbNet would allow for in-ferences over the assigned roles, which is only pos-sible in a more limited way with PropBank.In a previous paper (Zapirain et al, 2008), we pre-sented a first comparison between the two previousrole sets on the SemEval-2007 Task 17 corpus (Prad-han et al, 2007).
The SemEval-2007 corpus only550comprised examples about 50 different verbs.
Theresults of that paper were, thus, considered prelim-inary, as they could depend on the small amount ofdata (both in training data and number of verbs) orthe specific set of verbs being used.
Now, we ex-tend those experiments to the entire PropBank cor-pus, and we include two extra experiments on do-main shifts (using the Brown corpus as test set) andon grouping VerbNet labels.
More concretely, thispaper explores two aspects of the problem.
First,having in mind the claim that general thematic rolesshould be more robust to changing domains andunseen predicates, we study the performance of astate-of-the-art SRL system trained on either codi-fication of roles and some specific settings, i.e.
in-cluding/excluding verb-specific information, label-ing unseen verb predicates, or domain shifts.
Sec-ond, assuming that application scenarios would pre-fer dealing with general thematic role labels, we ex-plore the best way to label a text with thematic roles,namely, by training directly on VerbNet roles or byusing the PropBank SRL system and perform a pos-terior mapping into thematic roles.The results confirm our preliminary findings (Za-pirain et al, 2008).
We observe that the PropBankroles are more robust in all tested experimental con-ditions, i.e., the performance decrease is more se-vere for VerbNet.
Besides, tagging first PropBankroles and then mapping into VerbNet roles is as ef-fective as training and tagging directly on VerbNet,and more robust for domain shifts.The rest of the paper is organized as follows: Sec-tion 2 contains some background on PropBank andVerbNet role sets.
Section 3 presents the experimen-tal setting and the base SRL system used for the roleset comparisons.
In Section 4 the main compara-tive experiments on robustness are described.
Sec-tion 5 is devoted to analyze the posterior mapping ofPropBank outputs into VerbNet thematic roles, andincludes results on domain?shift experiments usingBrown as test set.
Finally, Sections 6 and 7 containa discussion of the results.2 Corpora and Semantic Role SetsThe PropBank corpus is the result of adding a se-mantic layer to the syntactic structures of Penn Tree-bank II (Palmer et al, 2005).
Specifically, it pro-vides information about predicate-argument struc-tures to all verbal predicates of the Wall Street Jour-nal section of the treebank.
The role set is theory?neutral and consists of a set of numbered core ar-guments (Arg0, Arg1, ..., Arg5).
Each verb has aframeset listing its allowed role labels and mappingeach numbered role to an English-language descrip-tion of its semantics.Different senses for a polysemous verb have dif-ferent framesets, but the argument labels are seman-tically consistent in all syntactic alternations of thesame verb?sense.
For instance in ?Kevin broke [thewindow]Arg1 ?
and in ?
[The door]Arg1 broke into amillion pieces?, for the verb broke.01, both Arg1 ar-guments have the same semantic meaning, that is?broken entity?.
Nevertheless, argument labels arenot necessarily consistent across different verbs (orverb senses).
For instance, the same Arg2 label isused to identify the Destination argument of a propo-sition governed by the verb send and the Beneficiaryargument of the verb compose.
This fact might com-promise generalization of systems trained on Prop-Bank, which might be focusing too much on verb?specific knowledge.
It is worth noting that the twomost frequent arguments, Arg0 and Arg1, are in-tended to indicate the general roles of Agent andTheme and are usually consistent across differentverbs.
However, this correspondence is not total.According to the study by (Yi et al, 2007), Arg0corresponds to Agent 85.4% of the time, but alsoto Experiencer (7.2%), Theme (2.1%), and Cause(1.9%).
Similarly, Arg1 corresponds to Theme in47.0% of the occurrences but also to Topic (23.0%),Patient (10.8%), and Product (2.9%), among others.Contrary to core arguments, adjuncts (Temporal andLocation markers, etc.)
are annotated with a closedset of general and verb-independent labels.VerbNet (Kipper et al, 2000) is a computationalverb lexicon in which verbs are organized hier-archically into classes depending on their syntac-tic/semantic linking behavior.
The classes are basedon Levin?s verb classes (Levin, 1993) and each con-tains a list of member verbs and a correspondencebetween the shared syntactic frames and the se-mantic information, such as thematic roles and se-lectional constraints.
There are 23 thematic roles(Agent, Patient, Theme, Experiencer, Source, Ben-eficiary, Instrument, etc.)
which, unlike the Prop-551Bank numbered arguments, are considered as gen-eral verb-independent roles.This level of abstraction makes them, in princi-ple, better suited (compared to PropBank numberedarguments) for being directly exploited by generalNLP applications.
But, VerbNet by itself is not anappropriate resource to train SRL systems.
As op-posed to PropBank, the number of tagged examplesis far more limited in VerbNet.
Fortunately, in thelast years a twofold effort has been made in orderto generate a large corpus fully annotated with the-matic roles.
Firstly, the SemLink1 resource (Loperet al, 2007) established a mapping between Prop-Bank framesets and VerbNet thematic roles.
Sec-ondly, the SemLink mapping was applied to a repre-sentative portion of the PropBank corpus and man-ually disambiguated (Loper et al, 2007).
The re-sulting corpus is currently available for the researchcommunity and makes possible comparative studiesbetween role sets.3 Experimental Setting3.1 DatasetsThe data used in this work is the benchmark corpusprovided by the SRL shared task of CoNLL-2005(Carreras and Ma`rquez, 2005).
The dataset, of over1 million tokens, comprises PropBank sections 02?21 for training, and sections 24 and 23 for develop-ment and test, respectively.
From the input informa-tion, we used part of speech tags and full parse trees(generated using Charniak?s parser) and discardednamed entities.
Also, we used the publicly avail-able SemLink mapping from PropBank into Verb-Net roles (Loper et al, 2007) to generate a replicateof the CoNLL-2005 corpus containing also the Verb-Net annotation of roles.Unfortunately, SemLink version 1.0 does notcover all propositions and arguments in the Prop-Bank corpus.
In order to have an homogeneous cor-pus and not to bias experimental evaluation, we de-cided to discard all incomplete examples and keeponly those propositions that were 100% mapped intoVerbNet roles.
The resulting corpus contains 56% ofthe original propositions, that is, over 50,000 propo-sitions in the training set.
This subcorpus is muchlarger than the SemEval-2007 Task 17 dataset used1http://verbs.colorado.edu/semlink/in our previous experimental work (Zapirain et al,2008).
The difference is especially noticeable inthe diversity of predicates represented.
In this case,there are 1,709 different verbs (1,505 lemmas) com-pared to the 50 verbs of the SemEval corpus.
Webelieve that the size and richness of this corpus isenough to test and extract reliable conclusions onthe robustness and generalization across verbs of therole sets under study.In order to study the behavior of both role setsin out?of?domain data, we made use of the Prop-Banked Brown corpus (Marcus et al, 1994) for test-ing, as it is also mapped into VerbNet thematic rolesin the SemLink resource.
Again, we discarded thosepropositions that were not entirely mapped into the-matic roles (45%).3.2 SRL SystemOur basic Semantic Role Labeling system representsthe tagging problem as a Maximum Entropy MarkovModel (MEMM).
The system uses full syntacticinformation to select a sequence of constituentsfrom the input text and tags these tokens with Be-gin/Inside/Outside (BIO) labels, using state-of-the-art classifiers and features.
The system achieves verygood performance in the CoNLL-2005 shared taskdataset and in the SRL subtask of the SemEval-2007English lexical sample task (Zapirain et al, 2007).Check this paper for a complete description of thesystem.When searching for the most likely state se-quence, the following constraints are observed2:1.
No duplicate argument classes for Arg0?Arg5PropBank (or VerbNet) roles are allowed.2.
If there is a R-X argument (reference), thenthere has to be a X argument before (referent).3.
If there is a C-X argument (continuation), thenthere has to be a X argument before.4.
Before a I-X token, there has to be a B-X or I-Xtoken.5.
Given a predicate, only the arguments de-scribed in its PropBank (or VerbNet) lexical en-try (i.e., the verbal frameset) are allowed.2Note that some of the constraints are dependent of the roleset used, i.e., PropBank or VerbNet552Regarding the last constraint, the lexical entriesof the verbs were constructed from the training dataitself.
For instance, the verb build appears withfour different PropBank core roles (Arg0?3) and fiveVerbNet roles (Product, Material, Asset, Attribute,Theme), which are the only ones allowed for thatverb at test time.
Note that in the cases where theverb sense was known we could constraint the pos-sible arguments to those that appear in the lexical en-try of that sense, as opposed of using the argumentsthat appear in all senses.4 On the Generalization of Role SetsWe first seek a basic reference of the comparativeperformance of the classifier on each role set.
Wedevised two settings based on our dataset.
In thefirst setting (?SemEval?)
we use all the available in-formation provided in the corpus, including the verbsenses in PropBank and VerbNet.
This informationwas available both in the training and test, and wasthus used as an additional feature by the classifierand to constrain further the possible arguments whensearching for the most probable Viterbi path.
We callthis setting ?SemEval?
because the SemEval-2007competition (Pradhan et al, 2007) was performedusing this configuration.Being aware that, in a real scenario, the sense in-formation will not be available, we devised the sec-ond setting (?CoNLL?
), where the hand-annotatedverb sense information was discarded.
This is thesetting used in the CoNLL 2005 shared task (Car-reras and Ma`rquez, 2005).The results for the first setting are shown in the?SemEval setting?
rows of Table 1.
The correct,excess, missed, precision, recall and F1 measuresare reported, as customary.
The significance inter-vals for F1 are also reported.
They have been ob-tained with bootstrap resampling (Noreen, 1989).F1 scores outside of these intervals are assumed tobe significantly different from the related F1 score(p < 0.05).
The results for PropBank are slightlybetter, which is reasonable, as the number of labelsthat the classifier has to learn in the case of VerbNetshould make the task harder.
In fact, given the smalldifference, one could think that VerbNet labels, be-ing more numerous, are easier to learn, perhaps be-cause they are more consistent across verbs.In the second setting (?CoNLL setting?
row inthe same table) the PropBank classifier degradesslightly, but the difference is not statistically signif-icant.
On the contrary, the drop of 1.6 points forVerbNet is significant, and shows greater sensitivityto the absence of the sense information for verbs.One possible reason could be that the VerbNet clas-sifier is more dependant on the argument filter (i.e.,the 5th constraint in Section 3.2, which only allowsroles that occur in the verbal frameset) used in theViterbi search, and lacking the sense informationmakes the filter less useful.
In fact, we have attestedthat the 5th constrain discard more than 60% of thepossible candidates for VerbNet, making the task ofthe classifier easier.In order to test this hypothesis, we run the CoNLLsetting with the 5th constraint disabled (that is, al-lowing any argument).
The results in the ?CoNLLsetting (no 5th)?
rows of Table 1 show that the dropfor PropBank is negligible and not significant, whilethe drop for VerbNet is more important, and statisti-cally significant.Another view of the data is obtained if we com-pute the F1 scores for core arguments and adjunctsseparately (last two columns in Table 1).
The per-formance drop for PropBank in the first three rowsis equally distributed on both core arguments and ad-juncts.
On the contrary, the drop for VerbNet rolesis more acute in core arguments (3.7 points), whileadjuncts with the 5th constraint disabled get resultsclose to the SemEval setting.
These results confirmthat the information in the verbal frameset is moreimportant in VerbNet than in PropBank, as only corearguments are constrained in the verbal framesets.The explanation could stem from the fact that cur-rent SRL systems rely more on syntactic informationthan pure semantic knowledge.
While PropBank ar-guments Arg0?5 are easier to distinguish on syntac-tic grounds alone, it seems quite difficult to distin-guish among roles like Theme and Topic unless wehave access to the specific verbal frameset.
This cor-responds nicely with the performance drop for Verb-Net when there is less information about the verb inthe algorithm (i.e., sense or frameset).We further analyzed the results by looking at eachof the individual core arguments and adjuncts.
Ta-ble 2 shows these results on the CoNLL setting.
Theperformance for the most frequent roles is similar553PropBankExperiment correct excess missed precision recall F1 F1 core F1 adj.SemEval setting 6,022 1,378 1,722 81.38 77.76 79.53 ?0.9 82.25 72.48CoNLL setting 5,977 1,424 1,767 80.76 77.18 78.93 ?0.9 81.64 71.90CoNLL setting (no 5th) 5,972 1,434 1,772 80.64 77.12 78.84 ?0.9 81.49 71.50No verbal features 5,557 1,828 2,187 75.25 71.76 73.46 ?1.0 74.87 70.11Unseen verbs 267 89 106 75.00 71.58 73.25 ?4.0 76.21 64.92VerbNetExperiment correct excess missed precision recall F1 F1 core F1 adj.SemEval setting 5,927 1,409 1,817 80.79 76.54 78.61 ?0.9 81.28 71.83CoNLL setting 5,816 1,548 1,928 78.98 75.10 76.99 ?0.9 79.44 70.20CoNLL setting (no 5th) 5,746 1,669 1,998 77.49 74.20 75.81 ?0.9 77.60 71.67No verbal features 4,679 2,724 3,065 63.20 60.42 61.78 ?0.9 59.19 69.95Unseen verbs 207 136 166 60.35 55.50 57.82 ?4.3 55.04 63.41Table 1: Basic results using PropBank (top) and VerbNet (bottom) role sets on different settings.for both.
Arg0 gets 88.49, while Agent and Expe-riencer get 87.31 and 87.76 respectively.
Arg2 gets79.91, but there is more variation on Theme, Topicand Patient (which get 75.46, 85.70 and 78.64 re-spectively).Finally, we grouped the results according to thefrequency of the verbs in the training data.
Table 3shows that both PropBank and VerbNet get decreas-ing results for less frequent verbs.
PropBank getsbetter results in all frequency ranges, except for themost frequent, which contains a single verb (say).Overall, the results on this section point out at theweaknesses of the VerbNet role set regarding robust-ness and generalization.
The next sections examinefurther its behavior.4.1 Generalization to Unseen PredicatesIn principle, the PropBank core roles (Arg0?4) geta different interpretation depending of the verb, thatis, the meaning of each of the roles is described sepa-rately for each verb in the PropBank framesets.
Still,the annotation criteria used with PropBank tried tomake the two main roles (Arg0 and Arg1, which ac-count for most of the occurrences) consistent acrossverbs.
On the contrary, in VerbNet al roles are com-pletely independent of the verb, in the sense that theinterpretation of the role does not vary across verbs.But, at the same time, each verbal entry lists the pos-sible roles it accepts, and the combinations allowed.This experiment tests the sensitivity of the two ap-proaches when the SRL system encounters a verbwhich does not occur in the training data.
In prin-ciple, we would expect the VerbNet semantic la-bels, which are more independent across verbs, to bemore robust at tagging new predicates.
It is worthnoting that this is a realistic scenario, even for theverb-specific PropBank labels.
Predicates which donot occur in the training data, but do have a Prop-Bank lexicon entry, could appear quite often in thetext to be analyzed.For this experiment, we artificially created a testset for unseen verbs.
We chose 50 verbs at random,and split them into 40 verbs for training and 10 fortesting (yielding 13,146 occurrences for training and2,723 occurrences for testing; see Table 4).The results obtained after training and testing theclassifier are shown in the last rows in Table 1.
Notethat they are not directly comparable to the other re-sults mentioned so far, as the train and test sets aresmaller.
Figures indicate that the performance of thePropBank argument classifier is considerably higherthan the VerbNet classifier, with a ?15 point gap.This experiment shows that lacking any informa-tion about verbal head, the classifier has a hard timeto distinguish among VerbNet roles.
In order to con-firm this, we performed the following experiment.4.2 Sensitivity to Verb-dependent FeaturesIn this experiment we want to test the sensitivity ofthe role sets when the classifier does not have any in-formation of the verb predicate.
We removed fromthe training and testing data all the features whichmake any reference to the verb, including, amongothers: the surface form, lemma and POS of theverb, and all the combined features that include theverb form (please, refer to (Zapirain et al, 2007) fora complete description of the feature set).The results are shown in the ?No verbal features?554CoNLL setting No verb featuresPBank VNet PBank VNetcorr.
F1 corr.
F1 F1 F1Overall 5977 78.93 5816 76.99 73.46 61.78Arg0 1919 88.49 84.02Arg1 2240 79.81 73.29Arg2 303 65.44 48.58Arg3 10 52.63 14.29Actor1 44 85.44 0.00Actor2 10 71.43 25.00Agent 1603 87.31 77.21Attribut.
25 71.43 50.79Cause 51 62.20 5.61Experien.
215 87.76 86.69Location 31 64.58 25.00Patient1 38 67.86 5.71Patient 208 78.64 25.06Patient2 21 67.74 43.33Predicate 83 62.88 28.69Product 44 61.97 2.44Recipient 85 79.81 62.73Source 29 60.42 30.95Stimulus 39 63.93 13.70Theme 1021 75.46 52.14Theme1 20 57.14 4.44Theme2 21 70.00 23.53Topic 683 85.70 73.58ADV 132 53.44 129 52.12 52.67 53.31CAU 13 53.06 13 52.00 53.06 45.83DIR 22 53.01 27 56.84 40.00 46.34DIS 133 77.78 137 79.42 77.25 78.34LOC 126 61.76 126 61.02 59.56 57.34MNR 109 58.29 111 54.81 52.99 51.49MOD 249 96.14 248 95.75 96.12 95.57NEG 124 98.41 124 98.80 98.41 98.01PNC 26 44.07 29 44.62 38.33 41.79TMP 453 75.00 450 73.71 73.06 73.89Table 2: Detailed results on the CoNLL setting.
Refer-ence arguments and verbs have been omitted for brevity,as well as those with less than 10 occ.
The last twocolumns refer to the results on the CoNLL setting withno verb features.Freq.
PBank VNet Freq.
PBank VNet0-50 74,21 71,11 500-900 77,97 75,7750-100 74,79 71,83 > 900 91,83 92,23100-500 77,16 75,41Table 3: F1 results split according to the frequency of theverb in the training data.Train affect, announce, ask, attempt, avoid, believe, build, care,cause, claim, complain, complete, contribute, describe,disclose, enjoy, estimate, examine, exist, explain, express,feel, fix, grant, hope, join, maintain, negotiate, occur,prepare, promise, propose, purchase, recall, receive,regard, remember, remove, replace, sayTest allow, approve, buy, find, improve, kill, produce, prove,report, rushTable 4: Verbs used in the unseen verb experimentrows of Table 1.
The performance drops more than5 points in PropBank, but the drop for VerbNet isdramatic, with more than 15 points.A closer look at the detailed role-by-role perfor-mances can be done if we compare the F1 rows in theCoNLL setting and in the ?no verb features?
settingin Table 2.
Those results show that both Arg0 andArg1 are quite robust to the lack of target verb in-formation, while Arg2 and Arg3 get more affected.Given the relatively low number of Arg2 and Arg3arguments, their performance drop does not affectso much the overall PropBank performance.
In thecase of VerbNet, the picture is very different.
Focus-ing on the most frequent roles first, while the perfor-mance drop for Experiencer, Agent and Topic is of1, 10 and 12 points respectively, the other roles getvery heavy losses (e.g.
Theme and Patient drop 23and 50 points), and the rest of roles are barely found.It is worth noting that the adjunct labels get verysimilar performances in both PropBank and Verb-Net cases.
In fact, Table 1 in the last two rows showsvery clearly that the performance drop is caused bythe core arguments.The better robustness of the PropBank roles canbe explained by the fact that, when creating Prop-Bank, the human PropBank annotators tried to beconsistent when tagging Arg0 and Arg1 acrossverbs.
We also think that both Arg0 and Arg1 canbe detected quite well relying on unlexicalized syn-tactic features only, that is, not knowing which arethe verbal and nominal heads.
On the other hand,distinguishing between Arg2?4 is more dependanton the subcategorization frame of the verb, and thusmore sensitive to the lack of verbal information.In the case of VerbNet, the more fine-grained dis-tinction among roles seems to depend more on themeaning of the predicate.
For instance, distinguish-ing between Agent?Experiencer, or Theme?Topic?Patient.
The lack of the verbal head makes it muchmore difficult to distinguish among those roles.
Thesame phenomena can be observed among the rolesnot typically realized as Subject or Object such asRecipient, Source, Product, or Stimulus.5 Mapping into VerbNet Thematic RolesAs mentioned in the introduction, the interpretationof PropBank roles depends on the verb, and that555Test on WSJ all core adj.PropBank to VerbNet (hand) 79.17 ?0.9 81.77 72.50VerbNet (SemEval setting) 78.61 ?0.9 81.28 71.84PropBank to VerbNet (MF) 77.15 ?0.9 79.09 71.90VerbNet (CoNLL setting) 76.99 ?0.9 79.44 70.88Test on BrownPropBank to VerbNet (MF) 64.79 ?1.0 68.93 55.94VerbNet (CoNLL setting) 62.87 ?1.0 67.07 54.69Table 5: Results on VerbNet roles using two differentstrategies.
Topmost 4 rows for the usual test set (WSJ),and the 2 rows below for the Brown test set.makes them less suitable for NLP applications.
Onthe other hand, VerbNet roles have a direct inter-pretation.
In this section, we test the performanceof two different approaches to tag input sentenceswith VerbNet roles: (1) train on corpora tagged withVerbNet, and tag the input directly; (2) train on cor-pora tagged with PropBank, tag the input with Prop-Bank roles, and use a PropBank to VerbNet mappingto output VerbNet roles.The results for the first approach are already avail-able (cf.
Table 1).
For the second approach, wejust need to map PropBank roles into VerbNet rolesusing SemLink (Loper et al, 2007).
We devisedtwo experiments.
In the first one we use the hand-annotated verb class in the test set.
For each predi-cate we translate PropBank roles into VerbNet rolesmaking use of the SemLink mapping informationcorresponding to that verb lemma and its verbalclass.For instance, consider an occurrence of allow in atest sentence.
If the occurrence has been manuallyannotated with the VerbNet class 29.5, we can usethe following entry in SemLink to add the VerbNetrole Predicate to the argument labeled with Arg1,and Agent to the Arg0 argument.<predicate lemma="allow"><argmap pb-roleset="allow.01" vn-class="29.5"><role pb-arg="1" vn-theta="Predicate" /><role pb-arg="0" vn-theta="Agent" /></argmap></predicate>The results obtained using the hand-annotatedVerbNet classes (and the SemEval setting for Prop-Bank), are shown in the first row of Table 5.
If wecompare these results to those obtained by VerbNetin the SemEval setting (second row of Table 5), theyare 0.5 points better, but the difference is not statis-tically significant.experiment corr.
F1Grouped (CoNLL Setting) 5,951 78.11?0.9PropBank to VerbNet to Grouped 5,970 78.21?0.9Table 6: Results for VerbNet grouping experiments.In a second experiment, we discarded the senseannotations from the dataset, and tried to predict theVerbNet class of the target verb using the most fre-quent class for the verb in the training data.
Sur-prisingly, the accuracy of choosing the most fre-quent class is 97%.
In the case of allow the mostfrequent class is 29.5, so we would use the sameSemLink entry as above.
The third row in Table 5shows the results using the most frequent VerbNetclass (and the CoNLL setting for PropBank).
Theperformance drop compared to the use of the hand-annotated VerbNet class is of 2 points and statisti-cally significant, and 0.2 points above the results ob-tained using VerbNet directly on the same conditions(fourth row of the same Table).The last two rows in table 5 show the results whentesting on the the Brown Corpus.
In this case, thedifference is larger, 1.9 points, and statistically sig-nificant in favor of the mapping approach.
Theseresults show that VerbNet roles are less robust todomain shifts.
The performance drop when mov-ing to an out?of?domain corpus is consistent withpreviously published results (Carreras and Ma`rquez,2005).5.1 Grouping experimentsVerbNet roles are more numerous than PropBankroles, and that, in itself, could cause a drop in per-formance.
Motivated by the results in (Yi et al,2007), we grouped the 23 VerbNet roles in 7 coarserrole groups.
Note that their groupings are focusedon the roles which map to PropBank Arg2.
In ourcase we are interested in a more general groupingwhich covers all VerbNet roles, so we added twoadditional groups (Agent-Experiencer and Theme-Topic-Patient).
We re-tagged the roles in the datasetswith those groups, and then trained and tested ourSRL system on those grouped labels.
The resultsare shown in the first row of Table 6.
In order tojudge if our groupings are easier to learn, we cansee that he performance gain with respect to the un-grouped roles (fourth row of Table 5) is small (76.99556vs.
78.11) but significant.
But if we compare themto the results of the PropBank to VerbNet mapping,where we simply substitute the fine-grained roles bytheir corresponding groups, we see that they still lagbehind (second row in Table 6).Although one could argue that better motivatedgroupings could be proposed, these results indicatethat the larger number of VerbNet roles does not ex-plain in itself the performance difference when com-pared to PropBank.6 Related WorkAs far as we know, there are only two other worksperforming comparisons of alternative role sets ona common test data.
Gildea and Jurafsky (2002)mapped FrameNet frame elements into a set of ab-stract thematic roles (i.e., more general roles such asAgent, Theme, Location), and concluded that theirsystem could use these thematic roles without degra-dation in performance.
(Yi et al, 2007) is a closely related work.
Theyalso compare PropBank and VerbNet role sets, butthey focus on the performance of Arg2.
They showthat splitting Arg2 instances into subgroups based onVerbNet thematic roles improves the performance ofthe PropBank-based classifier.
Their claim is thatsince VerbNet uses argument labels that are moreconsistent across verbs, they would provide moreconsistent training instances which would general-ize better, especially to new verbs and genres.
In factthey get small improvements in PropBank (WSJ)and a large improvement when testing on Brown.An important remark is that Yi et al use a com-bination of grouped VerbNet roles (for Arg2) andPropBank roles (for the rest of arguments).
In con-trast, our study compares both role sets as they stand,without modifications or mixing.
Another differenceis that they compare the systems based on the Prop-Bank roles ?by mapping the output VerbNet labelsback to PropBank Arg2?
while in our case we de-cided to do just the contrary (i.e., mapping PropBankoutput into VerbNet labels and compare there).
Aswe already said, we think that VerbNet?based labelscan be more useful for NLP applications, so our tar-get is to have a SRL system that provides VerbNetannotations.
While not in direct contradiction, bothstudies show different angles of the complex relationbetween the two role sets.7 Conclusion and Future workIn this paper we have presented a study of the per-formance of a state-of-the-art SRL system trainedon two alternative codifications of roles (PropBankand VerbNet) and some particular settings, e.g., in-cluding/excluding verb?specific information in fea-tures, labeling of infrequent and unseen verb pred-icates, and domain shifts.
We observed that Prop-Bank labeling is more robust in all previous experi-mental conditions, showing less performance dropsthan VerbNet labels.Assuming that application-based scenarios wouldprefer dealing with general thematic role labels, weexplore the best way to label a text with VerbNetthematic roles, namely, by training directly on Verb-Net roles or by using the PropBank SRL systemand performing a posterior mapping into thematicroles.
While results are similar and not statisticallysignificant in the WSJ test set, when testing on theBrown out?of?domain test set the difference in favorof PropBank plus mapping step is statistically signif-icant.
We also tried to map the fine-grained VerbNetroles into coarser roles, but it did not yield better re-sults than the mapping from PropBank roles.
As aside-product, we show that a simple most frequentsense disambiguation strategy for verbs is sufficientto provide excellent results in the PropBank to Verb-Net mapping.Regarding future work, we would like to exploreways to improve the performance on VerbNet roles,perhaps using selectional preferences.
We also wantto work on the adaptation to new domains of bothroles sets.AcknowledgementsWe are grateful to Martha Palmer and Edward Loperfor kindly providing us with the SemLink map-pings.
This work has been partially funded bythe Basque Government (IT-397-07) and by theMinistry of Education (KNOW TIN2006-15049,OpenMT TIN2006-15307-C03-02).
Ben?at is sup-ported by a PhD grant from the University of theBasque Country.557ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 shared task: Semantic role label-ing.
In Ido Dagan and Daniel Gildea, editors, Proceed-ings of the Ninth Conference on Computational Nat-ural Language Learning (CoNLL-2005), pages 152?164, Ann Arbor, Michigan, USA, June.
Associationfor Computational Linguistics.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class based construction of a verb lexicon.
InProceedings of the 17th National Conference on Arti-ficial Intelligence (AAAI-2000), Austin, TX, July.Beth Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
The University ofChicago Press, Chicago.Edward Loper, Szu-Ting Yi, and Martha Palmer.
2007.Combining lexical resources: Mapping between prop-bank and verbnet.
In Proceedings of the 7th In-ternational Workshop on Computational Linguistics,Tilburg, the Netherlands.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The penn tree-bank: annotating predicate argument structure.
InHLT ?94: Proceedings of the workshop on HumanLanguage Technology, pages 114?119, Morristown,NJ, USA.
Association for Computational Linguistics.Eric W. Noreen.
1989.
Computer-Intensive Methods forTesting Hypotheses.
John Wiley & Sons.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?105.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
Semeval-2007 task-17: En-glish lexical sample, SRL and all words.
In Proceed-ings of the Fourth International Workshop on Seman-tic Evaluations (SemEval-2007), pages 87?92, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Szu-Ting Yi, Edward Loper, and Martha Palmer.
2007.Can semantic roles generalize across genres?
In Pro-ceedings of the Human Language Technology Con-ferences/North American Chapter of the Associa-tion for Computational Linguistics Annual Meeting(HLT/NAACL-2007).Ben?at Zapirain, Eneko Agirre, and Llu?
?s Ma`rquez.
2007.Sequential SRL Using Selectional Preferences.
AnApproach with Maximum Entropy Markov Models.
InProceedings of the 4th International Workshop on Se-mantic Evaluations (SemEval-2007), pages 354?357.Ben?at Zapirain, Eneko Agirre, and Llu?
?s Ma`rquez.
2008.A Preliminary Study on the Robustness and General-ization of Role Sets for Semantic Role Labeling.
InProceedings of the 9th International Conference onComputational Linguistics and Intelligent Text Pro-cessing (CICLing-2008), pages 219?230, Haifa, Israel,February.558
