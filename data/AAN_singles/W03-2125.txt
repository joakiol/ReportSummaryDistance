A procedure assistant for astronautsin a functional programming architecture,with step previewing and spoken correction of dialogue movesGregory Aist1, Manny Rayner1, John Dowding1,Beth Ann Hockey1, Susana Early2, and Jim Hieronymus31Research Institute for Advanced Computer Science2Foothill/DeAnza College3NASA Ames Research CenterM/S T35B-1, Moffett Field CA 94035{aist, mrayner, jdowding, bahockey, jimh}@riacs.edu; searly@mail.arc.nasa.govAbstractWe present a demonstration of a proto-type system aimed at providing supportwith procedural tasks for astronauts onboard the International Space Station.Current functionality includes navigationwithin the procedure, previewing steps,requesting a list of images or a particularimage, recording voice notes and spokenalarms, setting parameters such as audiovolume.
Dialogue capabilities includehandling spoken corrections for an entiredialogue move, reestablishing context inresponse to a user request, responding touser barge-in, and help on demand.
Thecurrent system has been partially reim-plemented for better efficiency and in re-sponse to feedback from astronauts andastronaut training personnel.
Added fea-tures include visual and spoken step pre-viewing, and spoken correction ofdialogue moves.
The intention is to intro-duce the system into astronaut training asa prelude to flight on board the Interna-tional Space Station.1 IntroductionAstronauts on board the International Space Sta-tion engage in a wide variety of tasks on orbit in-cluding medical procedures, extra vehicularactivity (E V A), scientific payloads, and stationrepair and maintenance.
These human space flightactivities require extensive and thorough proce-dures.
These procedures are written down in theform of a number of steps and, with various notes,cautions, and warnings interspersed throughout theprocedure.
Each step may have one or more substeps.
Procedures also include branch points, call-outs to other procedures, and instructions to com-municate with mission control.
Since December2001, the RIALIST group has been developing aspoken dialogue system for providing assistancewith space station procedures.
Aist and Hockey(2002) and Aist et al (2002) described the firstversion of the system, which operated on a simpli-fied (and invented) procedure for unpacking andoperating a digital camera and included speechinput and speech output only.
Aist et al (2003)described a second version of the system with anXML-based display, and that included support fornot only procedures, but also voice notes and re-corded alarms, and parameter settings such as in-creasing and decreasing volume.
In this paper, wedescribe the third version of the system, with areimplemented architecture based on a functionalspecification of the domain-specific aspects of thesystem combined with an event-driven generic ar-chitectural framework.
We also describe two newfeatures: previewing of steps, and spoken correc-tion of dialogue moves.2 System DescriptionThe March 2003 version of the Intelligent Proce-dure Assistant is shown in Figure 1, just afterloading a procedure.
The March 2003 version pro-vides the following functions:Loading a procedure by specifying its name, forexample, ?Load water procedure.
?Sequential navigation through individual steps, forexample, ?Next step?
or ?Previous step.
?Navigation to arbitrary steps, for example, ?Go tostep two point one.
?Setting system parameters, such as ?Increase vol-ume?
or ?Decrease volume.
?Handling annotations, such as voice notes oralarms (?Record a voice note?
), or pictures (?Showthe small waste water bag.?
).Previewing steps; for example, ?Read step three?.Issuing spoken corrections (of entire commands),for example, ?I meant go to step two.
?We will discuss previewing steps and issuing spo-ken corrections in turn.2.1 Previewing steps (Reading mode)Besides acting on the current step, astronauts indi-cated that they would like a spoken preview of thenext step.
Currently this functionality is imple-mented as displaying a second procedure windowin the upper right corner of the screen.
Further-more, steps are prefixed with a spoken indicationof previewing, for example, ?Reading mode.
Notebefore step two??
To transition back into normal(execution) mode, the user may say ?Stop read-ing.?
Figure 2 shows the resulting display for thereading mode.2.2 Issuing spoken correctionsIn the March 2003 version of the Checklist system,the user may issue a spoken correction in the caseof an incorrectly given command, or in the case ofa speech recognition error (e.g.
?read me stepthree?
?
?repeat step three?).
The dialogue historyis represented as a list of the prior dialogue states.Currently we model a correction as a change in theinformation state, a rollback of the previous actionplan, and then an application of the new actionplan.
Figure 3 shows the display after issuing acorrection, ?I meant the wash cloth?.
Readingmode has been exited, and a picture of the wash-cloth is displayed.Figure 1.
Loading a procedure.Figure 2.
Preview mode, step three.Figure 3.
A subsequent correction, resulting in areturn to execution mode, and the implementationof the other command.Figure 4.
Checklist dialogue system architecture.3  Architecture, or, How to writea dialogue system in three easy stepsThere are three main sections to the dialogue han-dling code: the input manager, dialogue manager,and the output manager (Figure 4).
These aresimilar divisions to those proposed in Allen et al(2000).
Here, we also adopt a further division ofthe code into application-specific code and genericcode.
Application-specific code computes the fol-lowing function for each component, as a compila-tion step:Input manager: Input ?
EventDialogue manager: (Event, State)?
(Action, State)Output manager: Action ?
(Output, Inverse)The Output and Inverse computed by the Inputmanager are the multimodal output plans and theirmultimodal inverses, respectively.
The multi-modal inverses are used when applying a correc-tion ?
in conjunction with a return to a previousstate on the history list.The generic code is an interpretation (or execu-tion) step; the input manager?s code collects in-coming events and dispatches the events to thedialogue manager.
The dialogue manager?s codecollects the incoming events, retrieves the previousstate, applies the application-specific function,saves the new state, and then dispatches the newaction.
The output manager takes the action, ap-plies the application-specific function to computethe output and its inverse, and then dispatches theoutput plan one action at a time.
Each action is rep-resented as an OAA solvable, and dispatched se-quentially to be handled by the appropriate agentsuch as the text-to-speech agent.The entire dialogue manager is side-effectfree.
(With the minor exception of loading a procedurefile, which causes a change in the ?last accessed?time of the file.)
In a more typical dialogue systemarchitecture such as that shown in Figure 5, theside effects are represented separately.
The inte-gration of side effects into the output plan haspositive benefits for robustness, since they will berepresented in one place (and thus modified at thesame time when programming changes are made).Figure 5.
A more typical dialogue system ar-chitecture, with the side effects executed separatelyfrom the spoken output.4 Related Research and Future WorkRudnicky, Reed, and Thayer (1996) describe asystem for supporting vehicle maintenance withspeech interfaces.
Schreckenghost et al (2003)describe a scenario involving similar tasks (lifeSpeechRecognizerParserInputManagerOutputManagerSpeechSynthesizerVisualDisplayDialogueManagerI: input ?
eventD: (event, state)?
(action, state)O: action?
(output, inverse)SpeechRecognizerParser InputManagerOutputManagerSpeechSynthesizerDBDialogueManagersupport / maintenance related) but with the com-puter in more control of the actual task.
S & KElectronics (n.d.) mention a procedure develop-ment environment for rapidly developing and veri-fying on-orbit procedures(http://sk-web.sk-tech.com/proj.html).Possible future work includes adding proceduresinvolving inventory management and robot armassistance, automating dialogue system construc-tion from XML procedures, integrating with te-lemetry to monitor execution of the procedure anddevelop error recovery options, improving natural-ness of the speech output, modeling dialogue toinclude dialogue moves and expected user re-sponses, and improving speech recognition to berobust to ISS noise.ReferencesG.
Aist.
J. Dowding, B.
A. Hockey, and J. Hieronymus.2002.
An intelligent procedure assistant for astronauttraining and support.
Proceedings of the 40thAnnualMeeting of the Association for Computational Lin-guistics, refereed demonstration track.G.
Aist and B.
A. Hockey.
2002.
Generating Trainingand Assistive Dialogues for Astronauts from Interna-tional Space Station Technical Documentation.
ITS2002 Workshop on Integrating Technical and Train-ing Documentation.
Presented along with systemdemonstration.G.
Aist, J. Dowding, B.
A. Hockey, M. Rayner, J. Hi-eronymus, D. Bohus, B. Boven, N. Blaylock, E.Campana, S. Early, G. Gorrell, and S. Phan.
2003.European Association for Computational Linguistics(EACL) 2003 meeting, Software Demonstration, Bu-dapest, Hungary, April 2003.J.
Allen, D. Byron, M. Dzikovska, G. Ferguson, L.Galescu, and A. Stent.
2000.
An architecture for ageneric dialogue shell.
Natural Language Engineer-ing, Special issue on Best Practice in Spoken Lan-guage Dialogue Systems Engineering, pp.
323-340.A.
Rudnicky, S. Reed, and E. H. Thayer.
1996.SpeechWear: A mobile speech system.http://www.speech.cs.cmu.edu/air/papers/speechwear.psD.
Schreckenghost, C. Thronesbery, P. Bonasso, D.Kortenkamp and C. Martin, Intelligent Control ofLife Support for Space Missions, in IEEE IntelligentSystems Magazine, September/October, 2002.Portions of the dialogue systems described in this paperwere constructed with Rayner, Hockey, and Dowding?sRegulus open source toolkit.
Interested readers may findthe toolkit and supporting documentation online at:http://sourceforge.net/projects/regulus/.
