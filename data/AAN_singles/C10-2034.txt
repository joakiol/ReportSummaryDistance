Coling 2010: Poster Volume, pages 294?302,Beijing, August 2010A Novel Method for Bilingual Web Page Acquisition fromSearch Engine Web RecordsYanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin Yao, Qiaoming ZhuSchool of Computer Science & Technology, Soochow University{20094227002, hongy, 20074227065071, jyao, qmzhu}@suda.edu.cnAbstractA new approach has been developedfor acquiring bilingual web pages fromthe result pages of search engines,which is composed of two challengingtasks.
The first task is to detect webrecords embedded in the result pagesautomatically via a clustering methodof a sample page.
Identifying theseuseful records through the clusteringmethod allows the generation of highlyeffective features for the next taskwhich is high-quality bilingual webpage acquisition.
The task ofhigh-quality bilingual web pageacquisition is a classification problem.One advantage of our approach is that itis search engine and domainindependent.
The test is based on 2516records extracted from six searchengines automatically and annotatedmanually, which gets a high precisionof 81.3% and a recall of 94.93%.
Theexperimental results indicate that ourapproach is very effective.1 IntroductionThere have been extensive studies on parallelresource extraction from parallel monolingualweb pages of some bilingual web sites (Chenand Nie, 2000; Resnik and Smith, 2003; Zhanget al, 2006; Shi et al, 2006).
Candidate parallelweb pages are acquired by making use of URLstrings or HTML tags, then the translationequivalence of the candidate pairs are verifiedvia content-based features.However, we observe that bilingualresources may exist not only in two parallelmonolingual web pages, but also in singlebilingual web pages.
For example, many newsweb pages and English learning pages arebilingual.
Based on this observation,researchers have proposed methods to improveparallel sentences extraction within a bilingualweb page.
Jiang (2009) uses an adaptivepattern-based method to mine interestingbilingual data based on the observation thatbilingual data usually appears collectivelyfollowing similar patterns.
Because the WorldWide Web is composed of billions of pages, itis a challenging task to locate valuablebilingual pages.To acquire bilingual web pagesautomatically, a novel and effective method isproposed in this paper by making use of searchengines, such as Baidu (http://www.baidu.com).By submitting parallel sentence pairs to thegiven search engine, lots of result pages withweb records are returned, most of which arelinked to bilingual web pages.
We first identifyand extract all result records automatically byselecting and analyzing a sample page with aclustering method, and then select high-qualitybilingual web pages from candidates withclassification algorithms.Our method has the following advantages:1.
Former researchers extract parallel corpusfrom specific bilingual web sites.
Since searchengines index amounts of web pages, and weaim to acquire bilingual pages based on them,our method expands the corpus source greatly.2.
For one search engine, only one sampleresult page is used to generate the recordwrapper.
Then the wrapper is used to identifyweb records from other result pages of the samesearch engine.
Compared with existing datarecord extraction technologies, such as MDR(Liu et al, 2003; Zhai and Liu, 2006), ourmethod is more effective and efficient.2943.
We model the issue of verificationbilingual pages as a binary-class classificationproblem.
The records acquired automaticallyand annotated manually are utilized to train andtest the classifier.
This work is domain andsearch engine independent.
That is to say, therecords acquired from any search engine in anydomain are used indiscriminately as trainingand testing dataset.The rest of the paper is organized as follows.Related works are introduced in section 2.Section 3 provides an overview of our solution.The work about bilingual page acquisition andverification is introduced in section 4 and 5.Section 6 presents the experiments and results.Finally section 7 concludes the paper.2 Related WorkAs far as we know, there is no publicationavailable on acquiring bilingual web pages.Most existing studies, such as Nie (1999),Resnik and Smith (2003) and Shi (2006), mineparallel web documents within bilingual websites first and then extract bilingual sentencesfrom mined parallel documents using sentencealignment method.In this paper, the candidate bilingual webpages are acquired by analyzing web recordsembedded in the search engines?
result pages.Therefore, record extraction from result pagesis a critical technique in our method.
Manyresearches, such as Laender (2002), have beendeveloped various solutions in web informationextraction from kinds of perspectives.Earlier web information extraction systems(Baumgartner et al, 2001; Liu et al, 2000; Zhaiand Liu, 2005) require users to provide labeleddata so that the extraction rules could be learned.Yet such semi-automatic methods are notscalable enough to the whole Web whichchanges at any time.
That?s why more and moreresearchers focus on fully or nearly fullyautomatic solutions.Structured data objects are normally databaserecords retrieved from underlying webdatabases and displayed on the web pages withsome fixed templates, so automatic extractionmethods try to find such patterns and use themto extract more data.
Several approaches havesucceeded to address the problem automaticallywithout human assistance.
IEPAD (Chang andLui, 2001) identifies sub-strings that appearmany times in a document.
By traversing theDOM tree of the Web page, MDR extracts thedata-rich sub-tree indirectly by detecting theexistence of multiple similar generalized-nodes.The key limitation is its greedy manner ofidentifying a data region.
DEPTA (Zhai and Liu,2005) uses visual information (locations on thescreen at which the tags are rendered) to inferthe structural relationship among tags and toconstruct a tag tree.
NET (Liu and Zhai, 2005)extracts flat or nested data records bypost-order or pre-order traversal of the tag tree.ViNTs (Zhao et al, 2005) considers the webpage as a tag tree, and utilizes both visualcontent features as well as tag tree structures.
Itassumes that data records are located in aminimum data-rich sub-tree and separated byseparators of tag forests.
Zhao (2006) explicitlyaims at extracting all dynamic sections fromweb pages, and extracting records in eachsection, whereas ViNTs focuses on recordextraction from a single section.
Miao (2009)figures out how tag paths format the whole page.Compared with the previous method, itcompares pairs of tag path occurrence patternsto estimate how likely these tag paths representthe same list of objects instead of comparingone pair of individual sub-trees in the record.
Itbrings some noise.
We follow this method andmake appropriate improvement for our task.3 Basic Concepts and Overview3.1 Basic ConceptsSome basic concepts are introduced below.Figure 1.
An example of search engine return295Tag Path: The path of a tag consists of allnodes from the tree root <html> to itself.
Weuse tag path to specify the location of the tag.The tag paths are classified into two types: texttag paths and non-text tag paths.Data Record: When a page is considered asstrings of tokens, data records are enwrappedby one or more tag paths, which compose thevisually repeating pattern in a page.
This paperaims to extract such structured data records thatare produced by computer programs followingsome fixed templates, while whose contents areusually retrieved from backend databases.
Forexample, there are four records in Figure 1.3.2 Method OverviewWe can get much more bilingual web pages bysubmitting parallel sentence pairs to the searchengine than submitting monolingual queries.Based on this observation, our work is asshown in Figure 2.
The algorithm consists oftwo steps: 1) Record wrapper generation.
Bysubmitting parallel sentence pairs to searchengines, result pages containing lots of webrecords are returned.
In order to generate recordwrappers, we select and analyze a sample pageand then apply clustering method to tag pathswith similar patterns.
We apply these wrappersto extract more records, which are linked tocandidate bilingual web pages.
2) High-qualitybilingual page acquisition.
In order to acquirehigh-quality bilingual pages from candidates, abinary classifier is constructed to decidewhether the candidate pages are bilingual or not.In order to improve the classifier, some usefulresources are used, such as a dictionary andtranslation equivalents.However, a result page often contains someinformation irrelevant to the query, such asinformation related to the hosting site of thesearch engine, which increases the difficulty ofrecord extraction.
Besides, there are also manyirrelevant records irrelevant to the query.
Soour focus is to acquire plenty of features tofilter out the irrelevant pages from thecandidates.In this paper, the first result page is chosen asthe sample page and Affinity Propagation (AP)clustering is used.
The reason lies in Frey andDueck (2007), which proves that to produce thegroups of tag paths; the AP algorithm does notrequire the three restrictions: 1) the samplesmust be of a specific kind, 2) the similarityvalues must be in a specific range, and 3) thesimilarity matrix must be symmetric.
In orderto decide the type of a page, the SupportVector Machines (SVM) (Cortes and Vapnik,1995) classifier on Fuzzy C-means isconstructed combining with word-overlap,length and frequency measures.
SVM iswell-fitted to treat such classification problemsthat involve interrelated features likes ours,while most probabilistic classifiers, such asNa?ve Bayes classifier, strongly assume featureindependence (DuVerle and Prendinger, 2009).Figure 2.
Overview of the method4 Bilingual Page Acquisition4.1 Result Page ExtractionThe result pages of a search engine consist of aranked list of document summaries linked to theactual documents or web pages.
A webdocument summary typically contains the titleand URL of the web page, links to live andcached versions of the page and, mostimportantly, a short text summary, or a snippet,to convey the contents of the page.
Suchsnippets embedded in result pages of searchengines are query-dependent summaries.
White(2001) finds the result pages are sensitive to thecontent and language of the query.
If the queryis monolingual, the returned search results aremostly monolingual, while the result pages arebilingual if the query is bilingual.
In order toacquire more bilingual web pages, we submitparallel translation pairs.
Figure 1 gives anexample result page from Baidu, in which thesnapshot consists of four records related to thequery, which consists of ?I see.?
and itstranslation ???????.
The results have296more effective advantages than submitting thequery ?I see.?
or ???????
respectively.4.2 Clustering With Path SimilarityGiven a web page, we get the occurrencepositions of each tag path the same as thesequence in the preorder traversal of the page?sDOM tree.
Certainly, there are many tag pathswhich appear several times in the whole page.So an inverted mapping from HTML tag pathsto their positions is built easily.
For example,there are 599 tag paths formatting the samplepage in Figure 1, and after the inverted mapping,we acquire 86 unique tag paths in all.
Only tickoff one part of the results as shown in Table 1,where Pi represents the ith unique tag path, andthe vector Si is defined to store the occurrencepositions of Pi in the third column.As introduced above, detecting visuallyrepeating tag paths is a clustering problem.Above all, a factor in determining the clusteringperformance is the choice of similarityfunctions, which captures the likelihood thattwo data samples belong to the same cluster.
Inour case, the similarity scores between two tagpaths aim to capture how their positions areclose to each other and how they interleaveeach other.With the purpose of characterizing how closetwo tag paths appear, we only acquire thedistance between paths?
average positions,which is easy to obtain by the acquiredoccurrence vectors.
For example, the averageposition of P11 and P15 in Table 1 is 227 and215, so the distance between them is 12.L UniqueTag  Path (Pi)Occurrences (Si) of Pi1 \html 13 \html\head\#text 3,4,7,8,99 \html\body\table84,93,115,146,180,217,258,292,335,372,406,43711 \html\body\table\tr15,85,94,116,147,181,218,259,293,336,373,407,43814\html\body\table\tr\td\#text18,21,24,27,55,79,87,91,97,111,11315\html\body\table\tr\td\a19,88,118,149,183,220,261,295,338,375,409,440Table 1.
Unique tag paths of the sample pageHowever, the most difficult problem is howto capture the interleaving characteristicbetween two tag paths.
Before doing that,another vector Oi is produced.
Oi(k) indicateswhether the tag path Pi occurs in the position kor not by its value.
In addition, the value isbinary that 0 or 1, and 0 shows Pi doesn?t occurin the position k, while 1 shows the opposite.
Ofparticular note, the length of each Oi is equal tothe total number of HTML tags that formattingthe whole web page.
Take the tag path P3(?\html\head\#text?)
in Table 1 as an example,whose position vector O3 is (0, 0, 1, 1, 0, 0, 1, 1,1, 0?
0), and the vector?s length is 599,because there are totally 599 tag pathsformatting the sample page in Figure 1.Based on the position vectors, we capturehow tag path Pi and Pj interleave each other bya segmentjiOOD /  of Oi divided by Oj.
Weaim to find such tag paths that divide each otherin average.
In other words if the variance ofcounts in the segmentjiOOD /  is stable, theyare likely to be grouped in the same cluster.
So,we define the interleaving measureP in termsof the variances ofjiOOD /  and ij OOD /  as:)}( ),(   max{),( // ijji OOOOji DVarDVarOO  P (1)wherejiOOD /  is acquired by Oj as follows: ifvalue of Oj(k) is 1, Oi(k) is a separator tosegment itself into several regions.
The value ofevery element in the segment is the count of Pithat occurs in every region, which is thenumber of 1 in the region.Figure 3.
An Example of tag pathsIn addition, there may be many consecutiveseparators in Oi, and we integrate them into one.Besides, the segment is a non-empty set.
So ifthere is no occurrence of Pi in one region, we297will ignore this special region.
Figure 3 showsthree tag paths.
P1 and P2 are likely to belongto the same cluster because of their regularoccurrences, whereas the occurrences of P3 arecomparatively irregular.
By our method,31 / OOD  = {1, 1, 1} and13 /OOD = {1, 2, 1}.
Weintegrate separators once and ignore an emptyregion in the process of getting31 / OOD .Both the score of the closeness measure andthe interleaving measure for any two tag pathsare non-negative real numbers.
And a smallervalue of either measure indicates a highfrequency that the two tag paths appearregularly.
The measure ),( ji PPV  definedbelow is inversely proportional of these twomeasures.HPHVu),(),(),(jijiji OOSScPP (2)where H  is a non-negative term that avoidsdividing by 0 and normalizes the similarityvalue so that it falls into the range (0, 1].
In ourexperiment, we choose H = 10.
By Equation 2,we calculate the similarity value of any pair oftag paths.
As expected, the pairwise similaritymatrix is fed into the AP clustering algorithmdirectly, and each cluster acquired from APclustering contains n tag paths, which indicatesthat those n paths appear repeatedly togetherwith high frequency, and the tag paths that haveno remarkable relation are spilt into differentclusters.
For the given sample page in Figure 1,the number of identified clusters is 16.We observe that HTML code of most datarecords contain more than three HTML tags, sowe only examine the clusters containing four ormore visual signals.
In the clustering result ofsample page in Figure 1, there are threeclusters?
sizes less than four.
Meanwhile, wealso note that:1.
The feature page of a common searchengine usually contains 10 or more web recordswith similar layout pattern.
So we define athreshold T=3.
If an ancestor tag path doesn?toccur more than T times, we believe these tagpath dose not lead a record.2.
Usually the content of the result pagesreturned by search engines is completely relatedto the queries, which means the data recordsthat we are interested in are distributed in thewhole page as main component.
So theoccurrence position of valuable tag paths mustbe global optimization.
In this paper, the scopebetween beginning and ending occurrence mustbe wider than three quarters of the length of theweb page.Thus, we get essential clusters fit with aboveobservations, which is denoted by C= {C1,C2?CM}.
Once we have the essential clusters,we apply them in new web page of the samesearch engine to identify data records.4.3 Data Record ExtractionBased on the essential clusters, we extract theexact data records from the real content of texttag path that follow the ancestor tag path.In order to describe the extraction process indetails, we firstly define DaI as the child tagpaths of an ancestor tag path Pa, and supposethat (Pos1?
Posi?
Posm) is the occurrencevector of Pa, which means at each position Posithe tag path Pi occurs.
Da(i) is such a tag path setthat the position Pos of every path in it is Posi<Pos<Posi+1.
In the meantime, such path stringsmust begin with the same prefix of Pa.
Such asin Table 2, Da(i) contains tag paths from Posi toPosi+1-1, and we obtain the ith recordsembedded in the result pages by acquiring thereal content of all text tag paths in Da(i).Occurrenceof PaDaI ofPaChild tag pathPos1 Pa:\html\body\table\trPos1+1 Pt:\html\body\table\tr\???
?
?Pos2-1Da(1)Pk: ???
?
?Posi Pa:\html\body\table\trPosi+1 Pt:\html\body\table\tr\???
?
?Posi+1-1Da(i)Pn: ???
?
?Posm Pa:\html\body\table\tr??Da(m)?
?Table 2.
Collection of child tag paths forancestor tag path2985 Bilingual Web Page VerificationBased on the previous work, we capture a list ofrecords based on a holistic analysis of a resultpage, and each record contains snippets andURLs related to the query.
In this section, weaim to decide whether the candidate pages thatreturned records are linked to are bilingual ornot by putting some statistical features(collected from snippets) into an effective SVMclassification.To the acquired snippets, some necessarypreprocessing is made before we acquireuseful features.
We remove most of the noisethat affect the precision and robustness of theentire system by such methods as recovery ofabbreviation words, deletion of noisy words,amendment for half or full punctuations andsimplified or traditional characters, and so on.The snippet is described with more regularcontents after preprocessing.
We cut thesnippet into several segments by its language.Each segment of the snippet is just representedin one language, which is either English orChinese in this paper and different from itsadjacent segments.
So the source snippets aretransferred into such language strings thatconsist of C and E, where C stands for Chineseand E stands for English.
It is unlikely thatcontinuous C or E exists in the same languagestring.
We store the real text Tc (Te) that each C(E) stands for.
We take the snippet ?I see.
????
?I quit!
??????
as example, itslanguage string is ?ECEC?
and real text stringis TeTcTeTc, where the two Te stand for ?I see?and ?I quit?, the two Tc stand for ?????
?and ?????
?.Note that different feature functions for theclassifier will lead to different results, it isimportant to choose feature functions that willhelp to discriminate different classes.
In thispaper, the SVM classifier involvesword-overlap, length and frequency features.We define these three features based on thesnippet itself as follows:(1) Word-Overlap measureWord overlap judges the similarity ofChinese term and English term.
In this paper,we acquire the word-overlap score between anytwo adjacent language segments.
The similarityScore(c_res,e_res) of Chinese term and Englishterm is based on word-overlap as following:1 1( ( , ))( _ , _ )pi ji j qMax Sim c eScore c res e resId d?
(3)where the denominator is normalization factor,and in our experiment we select p+q as its value,where p stands for the length of Chinese termand q stands for the length of English term.
Inaddition, ci stands for the ith word of Chineseterm and ej stands for the jth word of Englishterm.
Sim(ci,ej) in Liu (2003) and Deng (2004)stands for the similarity of Chinese word ci andEnglish word ej.In our experiment, the Chinese and Englishsub-snippets are equivalent to Chinese andEnglish sentences of the bilingual pages.
In thesegmented snippet, with regard to eachsub-snippet T, which is at even position in thelanguage string, we separately evaluate theintermediate score for snippet T with its leftand right neighbors by Equation 3.
Especiallywhen T doesn?t have right or left neighbor, thescore for T with its null neighbor is 0.
So forevery sub-snippet that needs to be scored theword-overlap score, there are two candidatescores with its adjacent neighbors.
Then wechoose the higher value as one item of anintermediate result vector.
Either the length ofthe language string is 2 u n or 2 u n+1, thelength of intermediate vector is n, and the finalscore is computed as follows:mnInVsScorenkku?1)( (4)where Score(s) stands for the final score ofsnippet s on the word-overlap measure, andvector InV is the intermediate result vector asmentioned before.
The length of the vector InVis n, and m is the number of its items that is notequal to zero.
m/n is used as a useful measureof length, because it indicates how manyparallel pairs are there in the same snippet.
(2) Length-Based measureWe acquire three scores about lengthmeasure.
Take the language string ?ECECEC?as example, we use ?E1C1E2C2E3C3?
to replaceit for simple description.
We acquire one scoreof the length measure as follows:)())()(()( 1sLeneLencLensScoremi?(5)299where s and m stand for the same as in Equation4.
In addition, c and e stands for suchsub-snippet that Score(c,e) contributes to?nkkInV1.
The function Len(s) is to compute thenumber of words in the sentence.We acquire the length of language string.
Ifthe length is too long or too short, theassociated web page is unlikely to be abilingual page.
At the same time, we are notinterested in some language strings althoughthe lengths of them are appropriate.
So we alsostore the variances of lengths about eachsub-snippet.
(3) Frequency-Based measureAccording to the result pages, queries oftenoccur in the title, snippet, or advertisements.They are highlighted to make them easier toidentify.
Hence we aim to acquire thefrequency of the query in one whole snippet asa feature.Based on the three measures above, anumber of records (containing snippets andURLs) for training and testing can be convertedthem into a 6-dimensional feature space.
In ourexperiments, nonlinear SVM with GaussianRadial Basis Function (RBF) kernel is used.The performance of the SVM classifierindicates that it is a reliable way to verifywhether the page is bilingual or not by thecontent of snippet.6 Experiments and Results6.1 The Data SetTo acquire enough experimental data, wecollect from Google, Baidu, Yahoo, Youdao,Bing and Tecent Soso, and the effectiveness ofour algorithm is evaluated based on the data setfrom these six search engines.Result records of search engines arecollected by program and by human beingswith submitting different queries respectively.They are used for checking the performance ofrecord extraction.
When evaluating the methodof verification bilingual web pages, 2300records (60% are positive instances) arechosen for training the SVM classifier, andother 230 are selected randomly as test recordsfrom the whole record set.The training data is annotated by human intwo methods.
The first method is motivated bythe content of each source snippet.
Theannotators assign the type of web pages byscanning the text of every snippet.
If the snippetcontains many parallel term pairs, we annotatethe page as bilingual or monolingual if notparallel.
We also use another annotationmethod, which is to reach the URL by theInternet Explorer.
By checking the content ofthe real web page, annotators decide the type ofthe candidate pages.
And the biggest differencebetween the two public hand-classified datasetappears when some snippets of candidatepages have no clues in their content to predictclassifications.6.2 Evaluation On Bilingual PageAcquisitionThe entire system is evaluated by measuringthe performance of the binary SVM classifier.And how the classifier performance changeswith three features is shown in Table 3, whereW, L and F separately stand for theword-overlap, length and frequency measures.In order to improve the performance ofword-overlap measure, we use not only thebilingual dictionary but also translationequivalents, which are extracted from parallelcorpora.
Because the bilingual dictionarydoesn?t contain all necessary entries, theclassifier with only word-overlap measureaccepts many wrong pairs.Feature W W +L W +L+FPrecision 70.2% 81.02% 85.10%Table 3.
SVM Classifier Performance changeswith more features added to the classifierTable 3 shows that the length feature and thefrequency feature have a significant effect onbilingual web page verification because of thenatural relationship among queries, snippetsand true web pages.#1 #2NP(%) R(%) P(%) R(%)1 85.1 92.3 75% 84.82 80.7 95.1 72.8 85.73 78.1 97.4 71.0 93.0aver 81.3 94.93 72.93 87.83Table 4.
Performance versus training data types300Three experiments of verification bilingualweb pages based on two different trainingdatasets are conducted whose results areshown in Table 4.
#1 stands for the data setannotated by snippets, and #2 stands for thetraining data annotated by URLs.
Precision andrecall are used to evaluate our method.
Theaverage precision based on training dataset #2is 73%, which is lower than the precision of81.3% resulting from the dataset #1, because inmany cases, some snippets are weakly relatedwith real text in the real pages introduced bysearch engine summarization algorithm.
Fromthe table, we also see that the recalls in dataset#1 and #2 are both relatively high, whichmeans our classifier can select high-qualitybilingual pages with high accuracy.6.3 Evaluation On Web RecordExtractionRecord extraction has significant effect onbilingual web page collection.
A usefulintermediate evaluation of the whole scheme isconducted by measuring the performance ofrecord extraction.We built a prototype system to test thealgorithm of record extraction based on theclustering of similar records.
On a laptop with aPentium M 1.7G processor, the process ofconstructing records wrapper for a given searchengine is done in 10 to 30 seconds.
Once thewrapper is built, the record extraction from anew result page is done in a small fraction of asecond.In order to test the robustness of thegenerated wrapper, we compare the recordsextracted by our method with the test recordsacquired manually.
The precision and recallmeasures are used to evaluate the result.
98%of all the records are extracted by program,with a precision of 99%.
The precisionindicates that the generated wrappers in ourexperiment are quite robust to acquire records.The recall is lower than the precision, whichindicates that it sometimes misses a few records.The reason for this is that in the extraction step,the records different from more common onesare eliminated.We compare our performance with the workin Zhao (2006), which addresses the issue ofdifferentiating dynamic sections and recordsbased on the sample result pages.
It generatessection wrappers by identifying sectionboundary markers in nine steps.
It is morecomplicated in computation than ours becauseit renders each result page and extracts itscontent lines by a traversal of the DOM tree,while we use tag structure of a page.
Theaccordance is making full use of the samplepages for given search engines.
The methodalso gets a high precision of 98.8% and a recallof 98.7%.7 ConclusionThe paper presents a novel method to acquirebilingual web pages automatically via searchengines.
In order to improve the efficiency andeffectiveness, the snippets of search enginesrather than the contents of the massive pagesare analyzed to locate bilingual pages.Bilingual web page verification is modeled asa classification problem with word-overlap,length and frequency measures.
Based on thesimilarity of HTML structures, AP clusteringis used to extract web records from resultpages of search engines.
Experiments showthat our algorithm has good performance inprecision and recall.As a valuable resource for up-to-datebilingual terms and sentences, bilingual webpages are counterpart to parallel monolingualweb pages.
Our method brings an efficient andeffective solution to bilingual languageengineering.ReferencesAdelberg B., NoDoSE.
1998.
A tool for semi-Automatically extracting structured and sem-istructured data from text documents.
In:Proc.ACM SIGMOD Conference on  man-agement of Data, Seattle, WA (1998).Baumgartner R., S. Flesca and G. Gottlob.2001.Visual Web Information Extraction withLixto.
Proceedings of the 27th InternationalConference on Very Large Data Bases,pp.119-128, September 11-14, 2001Chang C., S. Lui.
2001.
Information Extractionbased on Pattern Discovery.
In Proceedingsof the 10th international conference onWorld Wide Web.
pp.681-688, May 01-05,2001, Hong Kong.Chen Jiang and Jian-Yun Nie.
2000.
Web301Parallel text mining for Chinese-Englishcross-language information retrieval.
Proce-edings of RIAO2000 Content-Based Multi-media Information Access, CID, ParisCortes, C. and V. Vapnik.
1995.
Support-vectornetwork.
Machine Learning 20, pp.273-297.Deng Dan.
2004.
Research on Chinese-Englishword alignment.
Institute of ComputingTechnology Chinese Academy of Sciences,Master Thesis.
(in Chinese).DuVerle David, Helmut Prendinger.
2009.
ANovel Discourse Parser Based on SupportVector Machine Classification.
The 47thAnnual Meeting of the Association forComputational Linguistics.
pp.
665-673Frey B. J. and D. Dueck.
2007.
Clustering bypassing messages between data points.Science, 315(5814):972-976.Laender A, B. Ribeiro-Neto, A. da Silva, J.Teixeira.
2002.
A Brief Survey of Web DataExtraction Tools.
ACM SIGMOD Record.Volume 31, Number 2.Liu B. and Y. Zhai.
2005.
System for extractingWeb data from flat and nested data records.In Proceedings of the Conference on WebInformation Systems Engineering,pp.487-495.Liu B., R. Grossman and Y. Zhai.
2003.
MiningData Records in Web Pages.
In Proceedingsof the ninth ACM SIGKDD internationalconference on Knowledge Discovery andData mining, Washington, D.C, pp.601-606.Liu Feifan, Jun Zhao, Bo Xu.
2003.
BuildingLarge-Scale Domain Independent Chinese-English Bilingual Corpus and the Researcheson Sentence Alignment.
Joint Symposium onComputational Linguistics.Liu L., C. Pu and W. Han.
2000.
An XML-Enabled Wrapper Construction System forWeb Information Sources.
Proceedings ofthe 16th International Conference on DataEngineering, pp.611.Long Jiang, Shiquan Yang, Ming Zhou, Xiao-hua Liu and Qingsheng Zhou.
2009.
MiningBilingual Data from the Web with Adaptive-ly Learnt Patterns.
The 47th Annual Meetingof the Association for Computational Lingui-stics.
pp.
870-878 (2009)Miao Gengxin, Junichi Tatemura, Wang-PinHsiung, Arsany Sawires, Louise E. Moser.2009.
Extracting data records from the webusing tag path clustering.
In Proceedings ofthe 18th International Conference on WorldWide Web, Spain, Madrid.Nie Jian-Yun, Michel Simard, Pierre Isabelle,Richard Durand 1999.
Cross-LanguageInformation Retrieval based on ParallelTexts and Automatic Mining of ParallelTexts in the Web.
SIGIR-1999; 74-81.Resnik Philip and Noah A. Smith.
2003.
Theweb as a Parallel Corpus.
ComputationalLinguistics.Shi Lei, Cheng Niu, Ming Zhou, and JianfengGao.
2006.
A DOM Tree Alignment Modelfor Mining Parallel Data from the Web.
InJoint Proceedings of the Association forComputational Linguistics and the Internati-onal Conference on Computational Linguist-ics, Sydney, Australia.White, R., Jose, J.
& Ruthven, R. 2001.Query-biased web page summarisation: a task-oriented evaluation.
In Proceedings of the24th ACM SIGIR Conference on Researchand Development of Information Retrieval.New Orleans, Louisiana, United States, pp.412-413.Zhai Y., B. Liu.
2005.
Extracting Web DataUsing Instance-Based Learning.
Web Infor-mation Systems Engineering.Zhai Y., B. Liu.
2005.
Web Data ExtractionBased on Partial Tree Alignment.
InProceedings of the 14th internationalconference on World Wide Web.
May 10-14,2005, Chiba, Japan.Zhang Ying, Ke Wu, Jianfeng Gao, Phil Vines.2006.
Automatic Acquisition of Chinese-English Parallel Corpus from the web.
InProceedings of 28th European Conferenceon Information Retrieval.Zhao H., W. Meng, Z. Wu, V. Raghavan, C.Yu.
2006.
Automatic Extraction of DynamicRecord Sections From Search Engine ResultPages.
In Proceedings of the 32nd Internatio-nal conference on Very large databases.302
