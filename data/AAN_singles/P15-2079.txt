Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 483?488,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsTagging Performance Correlates with Author AgeDirk Hovy1and Anders S?gaard1Center for Language TechnologyUniversity of Copenhagen, DenmarkNjalsgade 140, DK-2300 Copenhagen S{dirk.hovy,soegaard}@hum.ku.dkAbstractMany NLP tools for English and Germanare based on manually annotated articlesfrom the Wall Street Journal and Frank-furter Rundschau.
The average readers ofthese two newspapers are middle-aged (55and 47 years old, respectively), and the an-notated articles are more than 20 years oldby now.
This leads us to speculate whethertools induced from these resources (suchas part-of-speech taggers) put older lan-guage users at an advantage.
We show thatthis is actually the case in both languages,and that the cause goes beyond simple vo-cabulary differences.
In our experiments,we control for gender and region.1 IntroductionOne of the main challenges in natural languageprocessing (NLP) is to correct for biases in themanually annotated data available to system en-gineers.
Selection biases are often thought of interms of textual domains, motivating work in do-main adaptation of NLP models (Daume III andMarcu, 2006; Ben-David et al., 2007; Daume III,2007; Dredze et al., 2007; Chen et al., 2009; Chenet al., 2011, inter alia).
Domain adaptation prob-lems are typically framed as adapting models thatwere induced on newswire to other domains, suchas spoken language, literature, or social media.However, newswire is not just a domain withparticular conventions.
It is also a source of infor-mation written by and for particular people.
Thereader base of most newspapers is older, richer,and more well-educated than the average popu-lation.
Also, many newspapers have more read-ers in some regions of their country.
In addition,1Both authors contributed equally to the paper, andflipped a heavily biased coin until they were both satisfiedwith the order.newswire text is much more canonical than otherdomains, and includes fewer neologisms and non-standard language.
Both, however, are frequentin the language use of young adults, who are themain drivers of language change (Holmes, 2013;Nguyen et al., 2014).In this paper, we focus on the most widely usedmanually annotated resources for English and Ger-man, namely the English Penn Treebank and theTIGER Treebank for German.
The English tree-bank consists of manually annotated Wall StreetJournal articles from 1989.
The TIGER Treebankconsists of manually annotated Frankfurter Rund-schau articles from the early 1990s.
Both newspa-pers have regionally and demographically biasedreader bases, e.g., with more old than young read-ers.
We discuss the biases in ?2.In the light of recent research (Volkova et al.,2013; Hovy, 2015; J?rgensen et al., 2015), we ex-plore the hypothesis that these biases transfer toNLP tools induced from these resources.
As a re-sult, these models perform better on texts writtenby certain people, namely those whose languageis closer to the training data.
Language dynamicsbeing what they are, we expect English and Ger-man POS taggers to perform better on texts writtenby older people.
To evaluate this hypothesis, wecollected English and German user reviews froma user review site used by representative samplesof the English and German populations.
We anno-tated reviews written by users whose age, gender,and location were known with POS tags.
The re-sulting data set enables us to test whether there aresignificant performance differences between ages,genders, and regions, while controlling for the tworespective other, potentially confounding, factors.Contribution We show that age bias leadsto significant performance differences in off-the-shelf POS taggers for English and German.
Wealso analyze the relevant linguistic differences be-tween the age groups, and show that they are not483solely lexical, but instead extend to the grammat-ical level.
As a corollary, we also present severalnew evaluation datasets for English and Germanthat allow us to control for age, gender, and loca-tion.2 Data2.1 Wall Street Journal and FrankfurterRundschauThe Wall Street Journal is a New York City-basednewspaper, in print since 1889, with about twomillion readers.
It employs 2,000 journalists in85 news bureaus across 51 countries.
Wall StreetJournal is often considered business-friendly, butconservative.
In 2007, Rupert Murdoch boughtthe newspaper.
The English Penn Treebank con-sists of manually annotated articles from 1989, in-cluding both essays, letters and errata, but the vastmajority are news pieces.1Frankfurter Rundschau is a German languagenewspaper based in Frankfurt am Main.
Its first is-sue dates back to 1945, shortly after the end of thesecond world war.
It has about 120,000 readers.
Itis often considered a left-wing liberal newspaper.According to a study conducted by the newspa-per itself,2its readers are found in ?comfortable?higher jobs, well-educated, and on average in theirmid-forties.
While the paper is available interna-tionally, most of its users come from the Rhine-Main region.2.2 The Trustpilot CorpusThe Trustpilot Corpus (Hovy et al., 2015a) con-sists of user reviews scraped from the multi-lingual website trustpilot.com.
The re-viewer base has been shown to be representativeof the populations in the countries for which largereviewer bases exist, at least wrt.
age, gender, andgeographical spread (Hovy et al., 2015a).
The lan-guage is more informal than newswire, but lesscreative than social media posts.
This is similar tothe language in the reviews section of the EnglishWeb Treebank.3For the experiments below, weannotated parts of the British and German sections1http://www.let.rug.nl/?bplank/metadata/genre_files_updated.html2http://www.fr-online.de/wir-ueber-uns/studie-wer-sind-unsere-leser-,4353508,4356262.html3https://catalog.ldc.upenn.edu/LDC2012T13of the Trustpilot Corpus with the tag set proposedin Petrov et al.
(2011).2.3 POS annotationsWe use an in-house interface to annotate the En-glish and German data.
For each of the two lan-guages, we annotate 600 sentences.
The data issampled in the following way: we first extract allreviews associated with a location, split and to-kenize the review using the NLTK tokenizer forthe respective language, and discard any sentenceswith fewer than three or more than 100 tokens.
Wethen map each review to the NUTS region corre-sponding to the location.
If the location name isambiguous, we discard it.We then run two POS taggers (TreeTagger4, anda model implemented in CRF++5) to obtain log-likelihoods for each sentence in the English andGerman sub corpora.
We normalize by sentencelength and compute the average score for each re-gion under each tagger.We single out the two regions in England andGermany with the highest, respectively lowest, av-erage log-likelihoods from both taggers.
We dothis to be able to control for dialectal variation.In each region, we randomly sample 200 reviewswritten by women under 35, 200 reviews writtenby men under 35, 200 reviews written by womenover 45, and 200 reviews written by men over 45.This selection enables us to study and control forgender, region, and age.While sociolinguistics agrees on languagechange between age groups (Barke, 2000; Schleret al., 2006; Barbieri, 2008; Rickford and Price,2013), it is not clear where to draw the line.
Theage groups selected here are thus solely based onthe availability of even-sized groups that are sepa-rated by 10 years.3 Experiments3.1 Training data and modelsAs training data for our POS tagging models, weuse manually annotated data from the Wall StreetJournal (English Penn Treebank) and FrankfurterRundschau (TIGER).
We use the training and testsections provided in the CoNLL 2006?7 sharedtasks, but we convert all tags to the universal POStag set (Petrov et al., 2011).4http://www.cis.uni-muenchen.de/?schmid/tools/TreeTagger/5http://taku910.github.io/crfpp/484Our POS taggers are trained using TreeTaggerwith default parameters, and CRF++ with defaultparameters and standard POS features (Owoputiet al., 2013; Hovy et al., 2015b).
We use two dif-ferent POS tagger induction algorithms in orderto be able to abstract away from their respectiveinductive biases.
Generally, TreeTagger (TREET)performs better than CRF++ on German, whereasCRF++ performs best on English.3.2 Resultscountry group TREET CRF++ avg.DEU35 87.42 85.93 86.68O45 89.39 87.04 88.22male 88.53 86.11 87.32female 88.21 86.78 87.50highest reg.
88.46 86.49 87.48lowest reg.
88.85 87.41 88.13ENU35 87.92 88.23 88.08O45 88.26 88.40 88.33male 88.19 88.55 88.37female 87.97 88.08 88.03highest reg.
88.27 88.57 88.42lowest reg.
88.24 88.52 88.38Table 1: POS accuracy on different demographicgroups for English and German.
Significant dif-ferences per tagger in boldTable 1 shows the accuracies for both algo-rithms on the three demographic groups (age, gen-der, region) for German and English.
We see thatthere are some consistent differences between thegroups.
In both languages, results for both taggersare better for the older group than for the youngerone.
In three out of the four cases, this differenceis statistically significant at p < 0.05, accordingto a bootstrap-sample test.
The difference betweenthe genders is less pronounced, although we do seeCRF++ reaching a significantly higher accuracyfor women in German.
For regions, we find thatwhile the models assign low log-likelihood scoresto some regions, this is not reflected in the accu-racy.As common in NLP, we treat American (train-ing) and British English (test data) as variants.
Itis possible that this introduces a confounding fac-tor.
However, since we do not see marked effectsfor gender or region, and since the English resultsclosely track the German data, this seems unlikely.We plan to investigate this in future work.4 AnalysisThe last section showed the performance differ-ences between various groups, but it does not tellus where the differences come from.
In this sec-tion, we try to look into potential causes, and ana-lyze the tagging errors for systematic patterns.
Wefocus on age, since this variable showed the largestdifferences between groups.Holmes (2013) argues that people between 30and 55 years use standard language the most,because of societal pressure from their workplace.Nguyen et al.
(2014) made similar observationsfor Twitter.
Consequently, both young and retiredpeople often depart from the standard linguisticnorms, young people because of innovation, olderpeople because of adherence to previous norms.Our data suggests, however, that young people doso in ways that are more challenging for off-the-shelf NLP models induced on age-biased data.But what exactly are the linguistic differences thatlead to lower performance for this group?The obvious cause for the difference betweenage groups would be lexical change, i.e., the useof neologisms, spelling variation, or linguisticchange at the structural level in the youngergroup.
The resulting vocabulary differencesbetween age groups would result in an increasedout-of-vocabulary (OOV) rate in the youngergroup, which in turn negatively affects modelperformance.While we do observe an unsurprising corre-lation between sentence-level performance andOOV-rate, the young reviewers in our sample donot use OOV words more often than the olderage group.
Both groups differ from the trainingdata roughly equally.
This strongly suggests thatage-related differences in performance are not aresult of OOV items.In order to investigate whether the differ-ences extend beyond the vocabulary, we comparethe tag bigram distributions, both between thetwo age groups and between each group andthe training data.
We measure similarity by KLdivergence between the distributions, and inspectthe 10 tag bigrams which are most prevalentfor either group.
We use Laplace smoothing to485Figure 1: Tag bigrams with highest differences between distributions in English data.account for missing bigrams and ensure a properdistribution.For the English age groups, we find that a) thetwo Trustpilot data sets have a smaller KL diver-gence with respect to each other (1.86e ?
6) thaneither has with the training data (young: 3.24e?5,old.
: 2.36e?5, respectively).
We do note however,b), that the KL divergence for the older groups ismuch smaller than for the younger group.
Thismeans that there is a cross-domain effect, which isbigger, measured this way, than the difference inage groups.
The age group difference in KL diver-gence, however, suggests that the two groups usedifferent syntactic constructions.Inspecting the bigrams which are most preva-lent for each group, we find again that a) theTrustpilot groups show more instances involvingverbs, such as PRON?VERB, VERB?ADV, andVERB?DET, while the English Penn Treebankdata set has a larger proportion of instances ofnominal constructions, such as NOUN?VERB,NOUN?ADP, and NOUN?NOUN.On the other hand, we find that b) the youngergroup has more cases of verbal constructions andthe use of particles, such as PRT?VERB, VERB?PRT, PRON?PRT, and VERB?ADP, while theolder group?similar to the treebank?shows moreinstances of nominal constructions, i.e., againNOUN?VERB, ADJ?NOUN, NOUN?ADP,and NOUN?NOUN.The heatmaps in Figure 1 show all pairwisecomparisons between the three distributions.
Inthe interest of space and visibility, we select the 10bigrams that differ most from each other betweenthe two distributions under comparison.
Thecolor indicates in which of the two distributionsa bigram is more prevalent, and the degree ofshading indicates the size of the difference.For German, we see similar patterns.
TheTrustpilot data shows more instances of ADV?ADV, PRON?VERB, and ADV?VERB, whilethe TIGER treebank contains more NOUN?DET,NOUN?ADP, and NOUN?NOUN.Again, the younger group is more dissimilarto the CoNLL data, but less so than for English,with CONJ?PRON, NOUN?VERB, VERB?VERB, and PRON?DET, while the oldergroup shows more ADV?ADJ, ADP?NOUN,NOUN?ADV, and ADJ?NOUN.In all of these cases, vocabulary does notfactor into the differences, since we are at thePOS level.
The results indicate that there existfundamental grammatical differences between theage groups, which go well beyond mere lexicaldifferences.
These findings are in line with theresults in Johannsen et al.
(2015), who showedthat entire (delexicalized) dependency structurescorrelate with age and gender, often across severallanguages.4.1 Tagging Error AnalysisAnalyzing the tagging errors of our model can giveus an insight into the constructions that differ mostbetween groups.In German, most of the errors in the youngergroup occur with adverbs, determiners, and verbs.Adverbs are often confused with adjectives, be-cause adverbs and adjectives are used as modi-fiers in similar ways.
The taggers also frequentlyconfused adverbs with nouns, especially sentence-initially, presumably largely because they are cap-italized.
Sometimes, such errors are also due to486spelling mistakes and/or English loanwords.
De-terminers are often incorrectly predicted to be pro-nouns, presumably due to homography: in Ger-man, der, die, das, etc.
can be used as determin-ers, but also as relative pronouns, depending onthe position.
Verbs are often incorrectly predictedto be nouns.
This last error is again mostly dueto capitalization, homographs, and, again, Englishloanwords.
Another interesting source is sentence-initial use of verbs, which is unusual in canoni-cal German declarative sentences, but common ininformal language, where pronouns are dropped,i.e, ?
[Ich] Kann mich nicht beschweren?
([I] Can?tcomplain).Errors involving verbs are much less frequentin the older group, where errors with adjectivesand nouns are more frequent.For English, the errors in the younger andolder group are mostly on the same tags (nouns,adjectives, and verbs).
Nouns often get mis-tagged as VERB, usually because of homographydue to null-conversion (ordering, face, needs).Adjectives are also most commonly mis-taggedas VERB, almost entirely due to homography inparticiples (?ed, ?ing).
We see more emoticons(labeled X) in the younger group, and some ofthem end up with incorrect tags (NOUN or ADV).There are no mis-tagged emoticons in the oldergroup, who generally uses fewer emoticons (seealso Hovy et al.
(2015a)).5 ConclusionIn this position paper, we show that some of thecommon training data sets bias NLP tools towardsthe language of older people.
I.e., there is a statis-tically significant correlation between tagging per-formance and age for models trained on CoNLLdata.
A study of the actual differences betweenage groups shows that they go beyond the vocabu-lary, and extend to the grammatical level.The results suggest that NLP?s focus on a lim-ited set of training data has serious consequencesfor model performance on new data sets, but alsodemographic groups.
Due to language dynam-ics and the age of the data sets, performance de-grades significantly for younger speakers.
SincePOS tagging is often the first step in any NLPpipeline, performance differences are likely to in-crease downstream.
As a result, we risk disadvan-taging younger groups when it comes to the bene-fits of NLP.The case study shows that our models are sus-ceptible to the effects of language change and de-mographic factors.
Luckily, the biases are not in-herent to the models, but reside mostly in the data.The problem can thus mostly be addressed withmore thorough training data selection that takesdemographic factors into account.
It does high-light, however, that we also need to develop morerobust technologies that are less susceptible to databiases.AcknowledgementsWe would like to thank the anonymous reviewersfor their comments, which helped improve the pa-per.
This research is funded by the ERC StartingGrant LOWLANDS No.
313695.ReferencesFederica Barbieri.
2008.
Patterns of age-based lin-guistic variation in American English.
Journal ofsociolinguistics, 12(1):58?88.Andrew J Barke.
2000.
The Effect of Age on theStyle of Discourse among Japanese Women.
In Pro-ceedings of the 14th Pacific Asia Conference on Lan-guage, Information and Computation, pages 23?34.Shai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2007.
Analysis of representationsfor domain adaptation.
In NIPS.Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.2009.
Extracting discriminative concepts for do-main adaptation in text mining.
In KDD.Minmin Chen, Killiang Weinberger, and John Blitzer.2011.
Co-training for domain adaptation.
In NIPS.Hal Daume III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26:101?126.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of ACL.Mark Dredze, John Blitzer, Partha Pratim Taluk-dar, Kuzman Ganchev, Jo?ao Graca, and FernandoPereira.
2007.
Frustratingly Hard Domain Adap-tation for Dependency Parsing.
In EMNLP-CoNLL.Janet Holmes.
2013.
An introduction to sociolinguis-tics.
Routledge.Dirk Hovy, Anders Johannsen, and Anders S?gaard.2015a.
User review-sites as a source for large-scalesociolinguistic studies.
In Proceedings of WWW.487Dirk Hovy, Barbara Plank, H?ector Mart?
?nez Alonso,and Anders S?gaard.
2015b.
Mining for unambigu-ous instances to adapt pos taggers to new domains.In Proceedings of NAACL-HLT.Dirk Hovy.
2015.
Demographic factors improve clas-sification performance.
In Proceedings of ACL.Anders Johannsen, Dirk Hovy, and Anders S?gaard.2015.
Cross-lingual syntactic variation over age andgender.
In Proceedings of CoNLL.Anna J?rgensen, Dirk Hovy, and Anders S?gaard.2015.
Challenges of studying and processing di-alects in social media.
In Workshop on Noisy User-generated Text (W-NUT).Dong Nguyen, Dolf Trieschnigg, A. Seza Dogru?oz, Ri-lana Gravel, Mariet Theune, Theo Meder, and Fran-ciska De Jong.
2014.
Predicting Author Genderand Age from Tweets: Sociolinguistic Theories andCrowd Wisdom.
In Proceedings of COLING 2014.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL.Slav Petrov, Dipanjan Das, and Ryan McDonald.2011.
A universal part-of-speech tagset.
CoRRabs/1104.2086.John Rickford and Mackenzie Price.
2013.
Girlz iiwomen: Age-grading, language change and stylisticvariation.
Journal of Sociolinguistics, 17(2):143?179.Jonathan Schler, Moshe Koppel, Shlomo Argamon,and James W Pennebaker.
2006.
Effects of ageand gender on blogging.
In AAAI Spring Sympo-sium: Computational Approaches to Analyzing We-blogs, volume 6, pages 199?205.Svitlana Volkova, Theresa Wilson, and DavidYarowsky.
2013.
Exploring demographic languagevariations to improve multilingual sentiment anal-ysis in social media.
In Proceedings of EMNLP,pages 1815?1827.488
