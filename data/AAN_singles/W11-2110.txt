Proceedings of the 6th Workshop on Statistical Machine Translation, pages 104?107,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsMorphemes and POS tags for n-gram based evaluation metricsMaja Popovic?German Research Center for Artificial Intelligence (DFKI)Language Technology (LT), Berlin, Germanymaja.popovic@dfki.deAbstractWe propose the use of morphemes for auto-matic evaluation of machine translation out-put, and systematically investigate a set of Fscore and BLEU score based metrics calculatedon words, morphemes and POS tags along withall corresponding combinations.
Correlationsbetween the new metrics and human judg-ments are calculated on the data of the third,fourth and fifth shared tasks of the Statisti-cal Machine Translation Workshop.
Machinetranslation outputs in five different Europeanlanguages are used: English, Spanish, French,German and Czech.
The results show that theF scores which take into account morphemesand POS tags are the most promising metrics.1 IntroductionRecent investigations have shown that the n-grambased evaluation metrics calculated on Part-of-Speech (POS) sequences correlate very well withhuman judgments (Callison-Burch et al, 2008;Callison-Burch et al, 2009; Popovic?
and Ney, 2009)clearly outperforming the widely used metrics BLEUand TER.
The BLEU score measured on morphemesis shown to be useful for evaluation of morpholog-ically rich languages (Luong et al, 2010).
We pro-pose the use of morphemes for a set of n-gram basedautomatic evaluation metrics and investigate the cor-relation of the novel metrics with human judgments.We carry out a systematic comparison between theF and BLEU based metrics calculated on variouscombinations of words, morphemes and POS tags.The focus of this work is not a comparison of themorpheme and POS based metrics with the standardevaluation metrics1 as in (Popovic?
and Ney, 2009),but rather a comparison within the proposed set ofmetrics in order to decide which score(s) should besubmitted to the WMT 2011 evaluation task.
Thereare fifteen evaluation metrics in total, which can bedivided in three groups: the metrics calculated onsingle units, i.e.
words, morphemes or POS tagsalone, the metrics calculated on pairs, i.e.
wordsand POS tags, words and morphemes as well as mor-phemes and POS tags, and the metrics which take ev-erything into account ?
lexical, morphological andsyntactic information, i.e.
words, morphemes andPOS tags.Spearman?s rank correlation coefficients on thedocument (system) level between all the metricsand the human ranking are computed on the En-glish, French, Spanish, German and Czech textsgenerated by various translation systems in theframework of the third (Callison-Burch et al,2008), fourth (Callison-Burch et al, 2009) andfifth (Callison-Burch et al, 2010) shared translationtasks.2 Evaluation metricsWe carried out a systematic comparison between thefollowing metrics:?
single unit (word/morpheme/POS) metrics:?
WORDFStandard F score: takes into account allword n-grams which have a counterpart1Apart from the standard BLEU score which is tightly re-lated.104both in the corresponding reference and inthe hypothesis.?
MORPHFMorpheme F score: takes into account allmorpheme n-grams which have a counter-part both in the corresponding referenceand in the hypothesis.?
POSFPOS F score: takes into account all POSn-grams which have a counterpart both inthe corresponding reference and in the hy-pothesis.?
BLEUThe standard BLEU score (Papineni et al,2002).?
POSBLEUThe standard BLEU score calculated onPOS tags.?
MORPHBLEUThe standard BLEU score calculated onmorphemes.?
pairwise metrics:?
WPFF score of word and POS n-grams.?
WMFF score of word and morpheme n-grams.?
MPFF score of morpheme and POS n-grams.?
WPBLEUArithmetic mean of BLEU and POSBLEUscores.?
WMBLEUArithmetic mean of BLEU and MOR-PHBLEU scores.?
MPBLEUArithmetic mean of MORPHBLEU andPOSBLEU scores.?
metrics taking everything into account:?
WMPFF score on word, morpheme and POS n-grams.?
WMPBLEUArithmetic mean of BLEU, MORPHBLEUand POSBLEU scores.?
WMPFBLEUArithmetic mean of all F and BLEU scores.The prerequisite for POS based metrics is avail-ability of an appropriate POS tagger for the targetlanguage.
It should be noted that the POS tags can-not be only basic but must have all details (e.g.
verbtenses, cases, number, gender, etc.).
For the mor-pheme based metrics, a tool for splitting words intomorphemes is necessary.All the F scores and the BLEU scores are based onfour-grams (i.e.
the value of maximal n is 4).
Pre-liminary experiments on the morpheme based mea-sures showed that there is no improvement by us-ing six-grams, seven-grams or eight-grams.
As forthe n-gram averaging, BLEU scores use geometricmean.
However, it is also argued not to be optimalbecause the score becomes equal to zero even if onlyone of the n-gram counts is equal to zero.
In ad-dition, previous experiments on the syntax-orientedn-gram metrics (Popovic?
and Ney, 2009) showedthat there is no significant difference between arith-metic and geometric mean in the terms of correlationcoefficients.
Therefore, arithmetic averaging with-out weights is used for all F-scores.
For the WMPFscore, an additional experiment with weights is car-ried out as well.3 Experiments on WMT 2008, WMT 2009and WMT 2010 test dataExperimental set-upThe evaluation metrics were compared with humanrankings by means of Spearman correlation coeffi-cients ?.
Spearman?s rank correlation coefficient isequivalent to Pearson correlation on ranks, and itsadvantage is that it makes fewer assumptions aboutthe data.
The possible values of ?
range between 1(if all systems are ranked in the same order) and -1(if all systems are ranked in the reverse order).
Thusthe higher the value of ?
for an automatic metric, themore similar is to the human metric.The scores were calculated for outputs of transla-tions from Spanish, French, German and Czech intoEnglish and vice versa.
Spanish, French, Germanand English POS tags were produced using the Tree-Tagger2, and the Czech texts are tagged using the2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/105COMPOST tagger (Spoustova?
et al, 2009).
In thisway, all references and hypotheses were providedwith detailed POS tags.The words of all outputs were split into mor-phemes using the Morfessor tool (Creutz and La-gus, 2005).
The tool is corpus-based and language-independent: it takes a text as input and producesa segmentation of the word forms observed in thetext.
The obtained results are not strictly linguistic,however they often resemble a linguistic morphemesegmentation.
Once a morpheme segmentation hasbeen learnt from some text, it can be used for seg-menting new texts.
In our experiments, for each doc-ument, first a corresponding reference translationhas been split, and then this segmentation is used forsplitting all translation hypotheses.
In this way, pos-sible discrepancies between reference and hypothe-sis segmentation of the same word are avoided.
Ef-fects of the training on the large(r) monolingual cor-pora have not been investigated yet.In Table 1, an English reference sentence can beseen along with its morpheme and POS equivalents.words Another leading role in the filmis played by Matt Damon .morphemes An other lead ing role in the filmis play ed by Ma tt Da mon .POS tags DT VBG NN IN DT NNVBZ VBN IN NP NP SENTTable 1: Example of an English sentence with its corre-sponding morpheme and POS sequences.Comparison of metricsFor each evaluation metric described in Section 2,the system level Spearman correlation coefficients ?were calculated for each document.
In total, 33 cor-relation coefficients were obtained for each metric ?four English outputs from the WMT 2010 task, fivefrom the WMT 2009 and eight from the WMT 2008task, together with sixteen outputs in other four tar-get languages.
The obtained correlation results werethen summarised into the following three values:?
meana correlation coefficient averaged over all trans-lation outputs;?
rank>percentage of documents where the particularmetric has better correlation than the other met-rics investigated in this work;?
rank?percentage of documents where the particularmetric has better or equal correlation than theother metrics investigated in this work.These values for each metric are presented in Ta-ble 2.metric mean rank> rank?WORDF 0.550 24.2 42.6MORPHF 0.608 40.0 58.0POSF 0.673 63.4 78.0BLEU 0.566 20.6 38.6MORPHBLEU 0.567 29.9 44.6POSBLEU 0.674 54.7 66.9WPF 0.627 44.0 66.9WMF 0.587 37.0 53.9MPF 0.669 51.9 77.4WPBLEU 0.629 41.0 57.4WMBLEU 0.557 23.6 41.0MPBLEU 0.634 44.6 66.6WMPF 0.645 46.3 71.1WMPBLEU 0.610 32.7 54.7WMPFBLEU 0.628 35.8 61.6WMPF?
0.668 51.9 78.8Table 2: Average correlation mean (column 1), rank>(column 2) and rank?
(column 3) for each evaluationmetric.
Bold represents the best value in the particu-lar metric group.
The most promising metrics are theF scores containing POS and morpheme information,namely WMPF?, MPF and POSF, as well as the POSBLEUscore.
The standard BLEU score has very low values.It can be observed that the morpheme based met-rics outperform the word based metrics, however notthe POS based metrics.
As for pairwise metrics, theMPF score seems to be very promising.
Adding theactual original words unfortunately deteriorates thesystem level correlations, nevertheless omitting thewords can possibly lead to the poor sentence levelcorrelations.
Therefore an additional experiment iscarried out with the most promising metric contain-ing words, namely the WMPF score: a weighted106WMPF?
score is introduced, with word weight of0.2, morpheme weight of 0.3 and POS weight of0.5.
WMPF?
clearly outperforms the simple WMPFscore without weights, and it is comparable to themorpheme-POS F score MPF as well as POS-basedmetrics POSF and POSBLEU.
Apart from that, it canbe observed that, in general, the F scores are bet-ter than the BLEU scores.
The combination of all Fand all BLEU scores (WMPFBLEU) is better than theWMPBLEU score, but does not yield any improve-ments over the WMPF score.The most promising metrics are the F scores con-taining POS and morpheme information, namelyPOSF, MPF and WMPF?
together with the WMPF,as well as the POSBLEU score.
The standard BLEUscore has the third lowest average correlation and thelowest rank values.4 ConclusionsThe results presented in this article show that the useof morphemes improves n-gram based automaticevaluation metrics, particularly in combination withsyntactic information in the form of detailed POStags.
Especially promising are the weighted WMPFand the MPF scores, which have been submitted tothe WMT 2011 evaluation task.
Weights for thesetwo metrics should be further investigated in fu-ture work, as well as the possible impact of differ-ent morpheme splittings (such as training on largertexts).AcknowledgmentsThis work has partly been developed within theTARAX ?U project3 financed by TSB Technologies-tiftung Berlin ?
Zukunftsfonds Berlin, co-financedby the European Union ?
European fund for regionaldevelopment.ReferencesChris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
FurtherMeta-Evaluation of Machine Translation.
In Proceed-ings of the 3rd ACL 08 Workshop on Statistical Ma-chine Translation (WMT 08), pages 70?106, Colum-bus, Ohio, June.3http://taraxu.dfki.de/Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR(WMT 10), pages 17?53, Uppsala, Sweden, July.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using morfessor 1.0.
Technical Re-port Report A81, Computer and Information Science,Helsinki University of Technology, Helsinki, Finland,March.Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.2010.
A Hybrid Morpheme-Word Representationfor Machine Translation of Morphologically RichLanguages.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 10), pages 148?157, Cambridge, MA, Octo-ber.Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL 02), pages 311?318, Philadel-phia, PA, July.Maja Popovic?
and Hermann Ney.
2009.
Syntax-orientedevaluation measures for machine translation output.
InProceedings of the 4th EACL 09 Workshop on Sta-tistical Machine Translation (WMT 09), pages 29?32,Athens, Greece, March.Drahom?
?ra ?Johanka?
Spoustova?, Jan Hajic?, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervised train-ing for the averaged perceptron POS tagger.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 763?771,Athens, Greece, March.107
