Proceedings of NAACL HLT 2007, Companion Volume, pages 93?96,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsDetection of Non-native Sentences using Machine-translated Training DataJohn LeeSpoken Language SystemsMIT CSAILCambridge, MA 02139, USAjsylee@csail.mit.eduMing Zhou, Xiaohua LiuNatural Language Computing GroupMicrosoft Research AsiaBeijing, 100080, China{mingzhou,xiaoliu}@microsoft.comAbstractTraining statistical models to detect non-native sentences requires a large corpusof non-native writing samples, which isoften not readily available.
This paperexamines the extent to which machine-translated (MT) sentences can substituteas training data.Two tasks are examined.
For the na-tive vs non-native classication task, non-native training data yields better perfor-mance; for the ranking task, however,models trained with a large, publicly avail-able set of MT data perform as well asthose trained with non-native data.1 IntroductionFor non-native speakers writing in a foreign lan-guage, feedback from native speakers is indispens-able.
While humans are likely to provide higher-quality feedback, a computer system can offer bet-ter availability and privacy.
A system that can dis-tinguish non-native (?ill-formed?)
English sentencesfrom native (?well-formed?)
ones would providevaluable assistance in improving their writing.Classifying a sentence into discrete categories canbe difficult: a sentence that seems fluent to one judgemight not be good enough to another.
An alternativeis to rank sentences by their relative fluency.
Thiswould be useful when a non-native speaker is un-sure which one of several possible ways of writing asentence is the best.We therefore formulate two tasks on this problem.The classification task gives one sentence to the sys-tem, and asks whether it is native or non-native.
Theranking task submits sentences with the same in-tended meaning, and asks which one is best.To tackle these tasks, hand-crafting formal ruleswould be daunting.
Statistical methods, however,require a large corpus of non-native writing sam-ples, which can be difficult to compile.
Sincemachine-translated (MT) sentences are readily avail-able in abundance, we wish to address the questionof whether they can substitute as training data.The next section provides background on relatedresearch.
Sections 3 and 4 describe our experiments,followed by conclusions and future directions.2 Related ResearchPrevious research has paid little attention to rank-ing sentences by fluency.
As for classification, oneline of research in MT evaluation is to evaluate thefluency of an output sentence without its referencetranslations, such as in (Corston-Oliver et al, 2001)and (Gamon et al, 2005).
Our task here is simi-lar, but is applied on non-native sentences, arguablymore challenging than MT output.Evaluation of non-native writing has encom-passed both the document and sentence levels.
Atthe document level, automatic essay scorers, suchas (Burstein et al, 2004) and (Ishioka and Kameda,2006), can provide holistic scores that correlate wellwith those of human judges.At the sentence level, which is the focus of thispaper, previous work follows two trends.
Some re-searchers explicitly focus on individual classes of er-93rors, e.g., mass vs count nouns in (Brockett et al,2006) and (Nagata et al, 2006).
Others implicitly doso with hand-crafted rules, via templates (Heidorn,2000) or mal-rules in context-free grammars, suchas (Michaud et al, 2000) and (Bender et al, 2004).Typically, however, non-native writing exhibits awide variety of errors, in grammar, style and wordcollocations.
In this research, we allow unrestrictedclasses of errors1, and in this regard our goal is clos-est to that of (Tomokiyo and Jones, 2001).
How-ever, they focus on non-native speech, and assumethe availability of non-native training data.3 Experimental Set-Up3.1 DataOur data consists of pairs of English sentences, onenative and the other non-native, with the same ?in-tended meaning?.
In our MT data (MT), both sen-tences are translated, by machine or human, fromthe same sentence in a foreign language.
In our non-native data (JLE), the non-native sentence has beenedited by a native speaker2.
Table 1 gives some ex-amples, and Table 2 presents some statistics.MT (Multiple-Translation Chinese and Multiple-Translation Arabic corpora) English MT out-put, and human reference translations, of Chi-nese and Arabic newspaper articles.JLE (Japanese Learners of English Corpus) Tran-scripts of Japanese examinees in the StandardSpeaking Test.
False starts and disfluencieswere then cleaned up, and grammatical mis-takes tagged (Izumi et al, 2003).
The speakingstyle is more formal than spontaneous English,due to the examination setting.3.2 Machine Learning FrameworkSVM-Light (Joachims, 1999), an implementationof Support Vector Machines (SVM), is used for theclassification task.For the ranking task, we utilize the ranking modeof SVM-Light.
In this mode, the SVM algorithmis adapted for learning ranking functions, origi-nally used for ranking web pages with respect to a1Except spelling mistakes, which we consider to be a sepa-rate problem that should be dealt with in a pre-processing step.2The nature of the non-native data constrains the ranking totwo sentences at a time.query (Joachims, 2002).
In our context, given a setof English sentences with similar semantic content,say s1, .
.
.
, sn, and a ranking based on their fluency,the learning algorithm estimates the weights ~w tosatisfy the inequalities:~w ?
?
(sj) > ~w ?
?
(sk) (1)where sj is more fluent than sk, and where ?
mapsa sentence to a feature vector.
This is in contrast tostandard SVMs, which learn a hyperplane boundarybetween native and non-native sentences from theinequalities:yi(~w ?
?
(si) + w0) ?
1 ?
0 (2)where yi = ?1 are the labels.
Linear kernels areused in our experiments, and the regularization pa-rameter is tuned on the development sets.3.3 FeaturesThe following features are extracted from each sen-tence.
The first two are real numbers; the rest areindicator functions of the presence of the lexicaland/or syntactic properties in question.Ent Entropy3 from a trigram language modeltrained on 4.4 million English sentences withthe SRILM toolkit (Stolcke, 2002).
The tri-grams are intended to detect local mistakes.Parse Parse score from Model 2 of the statisti-cal parser (Collins, 1997), normalized by thenumber of words.
We hypothesize that non-native sentences are more likely to receivelower scores.Deriv Parse tree derivations, i.e., from each parentnode to its children nodes, such as S ?
NP VP.Some non-native sentences have plausible N -grams, but have derivations infrequently seenin well-formed sentences, due to their unusualsyntactic structures.DtNoun Head word of a base noun phrase, and itsdeterminer, e.g., (the, markets) from the humannon-native sentence in Table 1.
The usage of ar-ticles has been found to be the most frequent er-ror class in the JLE corpus (Izumi et al, 2003).3Entropy H(x) is related to perplexity PP (x) by the equa-tion PP (x) = 2H(x).94Type SentenceNative Human New York and London stock markets went upNon-native Human The stock markets in New York and London were increasing togetherMT The same step of stock market of London of New York risesTable 1: Examples of sentences translated from a Chinese source sentence by a native speaker, by a non-native speaker, and by a machine translation system.Data Set Corpus # sentences (for classification) # pairs (fortotal native non-native ranking)MT train LDC{2002T01, 2003T18, 2006T04} 30075 17508 12567 91795MT dev LDC2003T17 (Zaobao only) 1995 1328 667 2668MT test LDC2003T17 (Xinhua only) 3255 2184 1071 4284JLE train Japanese Learners of English 9848 4924 4924 4924JLE dev 1000 500 500 500JLE test 1000 500 500 500Table 2: Data sets used in this paper.Colloc An in-house dependency parser extractsfive types of word dependencies4: subject-verb,verb-object, adjective-noun, verb-adverb andpreposition-object.
For the human non-nativesentence in Table 1, the unusual subject-verbcollocation ?market increase?
is a useful cluein this otherwise well-formed sentence.4 Analysis4.1 An Upper BoundTo gauge the performance upper bound, we first at-tempt to classify and rank the MT test data, whichshould be less challenging than non-native data.
Af-ter training the SVM on MT train, classificationaccuracy on MT test improves with the additionof each feature, culminating at 89.24% with allfive features.
This result compares favorably withthe state-of-the-art5.
Ranking performance reaches96.73% with all five features.We now turn our attention to non-native test data,and contrast the performance on JLE test usingmodels trained by MT data (MT train), and bynon-native data (JLE train).4Proper nouns and numbers are replaced with special sym-bols.
The words are further stemmed using Porter?s Stemmer.5Direct comparison is impossible since the corpora were dif-ferent.
(Corston-Oliver et al, 2001) reports 82.89% accuracyon English software manuals and online help documents, and(Gamon et al, 2005) reports 77.59% on French technical docu-ments.Test Set: Train SetJLE test MT train JLE trainEnt+ 57.2 57.7Parse (+) 48.6 (+) 70.6(-) 65.8 (-) 44.8+Deriv 58.4 64.7(+) 54.6 (+)72.2(-) 62.2 (-) 57.2+DtNoun 59.0 66.4(+) 57.6 (+) 72.8(-) 60.4 (-) 60.0+Colloc 58.6 65.9(+) 54.2 (+) 72.6(-) 63.2 (-) 59.2Table 3: Classication accuracy on JLE test.
(-)indicates accuracy on non-native sentences, and (+)indicates accuracy on native sentences.
The overallaccuracy is their average.4.2 ClassificationAs shown in Table 3, classification accuracy on JLEtest is higher with the JLE train set (66.4%)than with the larger MT train set (59.0%).
TheSVM trained on MT train consistently misclas-sifies more native sentences than non-native ones.One reason might be that speech transcripts have aless formal style than written news sentences.
Tran-scripts of even good conversational English do notalways resemble sentences in the news domain.4.3 RankingIn the ranking task, the relative performance be-tween MT and non-native training data is reversed.95Test Set: Train SetJLE test MT train JLE trainEnt+Parse 72.8 71.4+Deriv 73.4 73.6+DtNoun 75.4 73.8+Colloc 76.2 74.6Table 4: Ranking accuracy on JLE test.As shown in Table 4, models trained on MT trainyield higher ranking accuracy (76.2%) than thosetrained on JLE train (74.6%).
This indicates thatMT training data can generalize well enough to per-form better than a non-native training corpus of sizeup to 10000.The contrast between the classification and rank-ing results suggests that train/test data mismatch isless harmful for the latter task.
Weights trained onthe classification inequalities in (2) and on the rank-ing inequalities in (1) both try to separate native andMT sentences maximally.
The absolute boundarylearned in (2) is inherently specific to the natureof the training sentences, as we have seen in ?4.2.In comparison, the relative scores learned from (1)have a better chance to carry over to other domains,as long as some gap still exists between the scoresof the native and non-native sentences.5 Conclusions & Future WorkWe explored two tasks in sentence-level fluencyevaluation: ranking and classifying native vs. non-native sentences.
In an SVM framework, we exam-ined how well MT data can replace non-native datain training.For the classification task, training with MT datais less effective than with non-native data.
How-ever, for the ranking task, models trained on pub-licly available MT data generalize well, performingas well as those trained with a non-native corpus ofsize 10000.In the future, we would like to search for moresalient features through a careful study of non-nativeerrors, using error-tagged corpora such as (Izumi etal., 2003).
We also plan to explore techniques forcombining large MT training corpora and smallernon-native training corpora.
Our ultimate goal is toidentify the errors in the non-native sentences andpropose corrections.ReferencesE.
Bender, D. Flickinger, S. Oepen, A. Walsh, and T.Baldwin.
2004.
Arboretum: Using a Precision Gram-mar for Grammar Checking in CALL.
Proc.
In-STIL/ICALL Symposium on Computer Assisted Learn-ing.C.
Brockett, W. Dolan, and M. Gamon.
2006.
Correct-ing ESL Errors using Phrasal SMT Techniques.
Proc.ACL.J.
Burstein, M. Chodorow and C. Leacock.
2004.
Auto-mated Essay Evaluation: The Criterion online WritingService.
AI Magazine, 25(3):27?36.M.
Collins.
1997.
Three Generative, Lexicalised Modelsfor Statistical Parsing.
Proc.
ACL.S.
Corston-Oliver, M. Gamon and C. Brockett.
2001.
AMachine Learning Approach to the Automatic Evalu-ation of Machine Translation.
Proc.
ACL.M.
Gamon, A. Aue, and M. Smets.
2005.
Sentence-Level MT Evaluation without Reference Translations:Beyond Language Modeling.
Proc.
EAMT.G.
Heidorn.
2000.
Intelligent Writing Assistance.Handbook of Natural Language Processing.
RobertDale, Hermann Moisi and Harold Somers (ed.).
Mar-cel Dekker, Inc.T.
Ishioka and M. Kameda.
2006.
Automated JapaneseEssay Scoring System based on Articles Written byExperts.
Proc.
ACL.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H.Isahara.
2003.
Automatic Error Detection in theJapanese Learners?
English Spoken Data.
Proc.
ACL.T.
Joachims.
1999.
Making Large-Scale SVM LearningPractical.
Advances in Kernel Methods - Support Vec-tor Learning.
B. Scho?lkopf, C. Burges and A.
Smola(ed.
), MIT-Press.T.
Joachims.
2002.
Optimizing Search Engines usingClickthrough Data.
Proc.
SIGKDD.L.
Michaud, K. McCoy and C. Pennington.
2000.
An In-telligent Tutoring System for Deaf Learners of WrittenEnglish.
Proc.
4th International ACM Conference onAssistive Technologies.R.
Nagata, A. Kawai, K. Morihiro, and N. Isu.
2006.
AFeedback-Augmented Method for Detecting Errors inthe Writing of Learners of English.
Proc.
ACL.A.
Stolcke.
2002.
SRILM ?
An Extensible LanguageModeling Toolkit Proc.
ICSLP.L.
Tomokiyo and R. Jones.
2001.
You?re not from ?roundhere, are you?
Na?
?ve Bayes Detection of Non-nativeUtterance Text.
Proc.
NAACL.96
