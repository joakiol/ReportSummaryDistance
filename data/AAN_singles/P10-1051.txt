Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495?503,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsMinimized models and grammar-informed initializationfor supertagging with highly ambiguous lexiconsSujith Ravi1 Jason Baldridge2 Kevin Knight11University of Southern CaliforniaInformation Sciences InstituteMarina del Rey, California 90292{sravi,knight}@isi.edu2Department of LinguisticsThe University of Texas at AustinAustin, Texas 78712jbaldrid@mail.utexas.eduAbstractWe combine two complementary ideasfor learning supertaggers from highly am-biguous lexicons: grammar-informed tagtransitions and models minimized via in-teger programming.
Each strategy on itsown greatly improves performance overbasic expectation-maximization trainingwith a bitag Hidden Markov Model, whichwe show on the CCGbank and CCG-TUTcorpora.
The strategies provide further er-ror reductions when combined.
We de-scribe a new two-stage integer program-ming strategy that efficiently deals withthe high degree of ambiguity on thesedatasets while obtaining the full effect ofmodel minimization.1 IntroductionCreating accurate part-of-speech (POS) taggersusing a tag dictionary and unlabeled data is aninteresting task with practical applications.
Ithas been explored at length in the literature sinceMerialdo (1994), though the task setting as usu-ally defined in such experiments is somewhat arti-ficial since the tag dictionaries are derived fromtagged corpora.
Nonetheless, the methods pro-posed apply to realistic scenarios in which onehas an electronic part-of-speech tag dictionary ora hand-crafted grammar with limited coverage.Most work has focused on POS-tagging forEnglish using the Penn Treebank (Marcus et al,1993), such as (Banko and Moore, 2004; Gold-water and Griffiths, 2007; Toutanova and John-son, 2008; Goldberg et al, 2008; Ravi and Knight,2009).
This generally involves working with thestandard set of 45 POS-tags employed in the PennTreebank.
The most ambiguous word has 7 dif-ferent POS tags associated with it.
Most methodshave employed some variant of Expectation Max-imization (EM) to learn parameters for a bigramor trigram Hidden Markov Model (HMM).
Raviand Knight (2009) achieved the best results thusfar (92.3% word token accuracy) via a MinimumDescription Length approach using an integer pro-gram (IP) that finds a minimal bigram grammarthat obeys the tag dictionary constraints and cov-ers the observed data.A more challenging task is learning supertag-gers for lexicalized grammar formalisms such asCombinatory Categorial Grammar (CCG) (Steed-man, 2000).
For example, CCGbank (Hocken-maier and Steedman, 2007) contains 1241 dis-tinct supertags (lexical categories) and the mostambiguous word has 126 supertags.
This pro-vides a much more challenging starting pointfor the semi-supervised methods typically ap-plied to the task.
Yet, this is an important tasksince creating grammars and resources for CCGparsers for new domains and languages is highlylabor- and knowledge-intensive.
Baldridge (2008)uses grammar-informed initialization for HMMtag transitions based on the universal combinatoryrules of the CCG formalism to obtain 56.1% accu-racy on ambiguous word tokens, a large improve-ment over the 33.0% accuracy obtained with uni-form initialization for tag transitions.The strategies employed in Ravi and Knight(2009) and Baldridge (2008) are complementary.The former reduces the model size globally givena data set, while the latter biases bitag transitionstoward those which are more likely based on a uni-versal grammar without reference to any data.
Inthis paper, we show how these strategies may becombined straightforwardly to produce improve-ments on the task of learning supertaggers fromlexicons that have not been filtered in any way.1We demonstrate their cross-lingual effectivenesson CCGbank (English) and the Italian CCG-TUT1See Banko and Moore (2004) for a description of howmany early POS-tagging papers in fact used a number ofheuristic cutoffs that greatly simplify the problem.495corpus (Bos et al, 2009).
We find a consistent im-proved performance by using each of the methodscompared to basic EM, and further improvementsby using them in combination.Applying the approach of Ravi and Knight(2009) naively to CCG supertagging is intractabledue to the high level of ambiguity.
We deal withthis by defining a new two-stage integer program-ming formulation that identifies minimal gram-mars efficiently and effectively.2 DataCCGbank.
CCGbank was created by semi-automatically converting the Penn Treebank toCCG derivations (Hockenmaier and Steedman,2007).
We use the standard splits of the dataused in semi-supervised tagging experiments (e.g.Banko and Moore (2004)): sections 0-18 for train-ing, 19-21 for development, and 22-24 for test.CCG-TUT.
CCG-TUT was created by semi-automatically converting dependencies in the Ital-ian Turin University Treebank to CCG deriva-tions (Bos et al, 2009).
It is much smaller thanCCGbank, with only 1837 sentences.
It is splitinto three sections: newspaper texts (NPAPER),civil code texts (CIVIL), and European law textsfrom the JRC-Acquis Multilingual Parallel Corpus(JRC).
For test sets, we use the first 400 sentencesof NPAPER, the first 400 of CIVIL, and all of JRC.This leaves 409 and 498 sentences from NPAPERand CIVIL, respectively, for training (to acquire alexicon and run EM).
For evaluation, we use twodifferent settings of train/test splits:TEST 1 Evaluate on the NPAPER section of testusing a lexicon extracted only from NPAPERsection of train.TEST 2 Evaluate on the entire test using lexi-cons extracted from (a) NPAPER + CIVIL,(b) NPAPER, and (c) CIVIL.Table 1 shows statistics for supertag ambiguityin CCGbank and CCG-TUT.
As a comparison, thePOS word token ambiguity in CCGbank is 2.2: thecorresponding value of 18.71 for supertags is in-dicative of the (challenging) fact that supertag am-biguity is greatest for the most frequent words.3 Grammar informed initialization forsupertaggingPart-of-speech tags are atomic labels that in and ofthemselves encode no internal structure.
In con-Data Distinct Max Type ambig Tok ambigCCGbank 1241 126 1.69 18.71CCG-TUTNPAPER+CIVIL 849 64 1.48 11.76NPAPER 644 48 1.42 12.17CIVIL 486 39 1.52 11.33Table 1: Statistics for the training data used to ex-tract lexicons for CCGbank and CCG-TUT.
Dis-tinct: # of distinct lexical categories; Max: # ofcategories for the most ambiguous word; Typeambig: per word type category ambiguity; Tokambig: per word token category ambiguity.trast, supertags are detailed, structured labels; auniversal set of grammatical rules defines how cat-egories may combine with one another to projectsyntactic structure.2 Because of this, properties ofthe CCG formalism itself can be used to constrainlearning?prior to considering any particular lan-guage, grammar or data set.
Baldridge (2008) usesthis observation to create grammar-informed tagtransitions for a bitag HMM supertagger based ontwo main properties.
First, categories differ intheir complexity and less complex categories tendto be used more frequently.
For example, two cat-egories for buy in CCGbank are (S[dcl]\NP)/NPand ((((S[b]\NP)/PP)/PP)/(S[adj]\NP))/NP; theformer occurs 33 times, the latter once.
Second,categories indicate the form of categories foundadjacent to them; for example, the category forsentential complement verbs ((S\NP)/S) expectsan NP to its left and an S to its right.Categories combine via rules such as applica-tion and composition (see Steedman (2000) for de-tails).
Given a lexicon containing the categoriesfor each word, these allow derivations like:Ed might see a catNP (S\NP)/(S\NP) (S\NP)/NP NP/N N>B >(S\NP)/NP NP>S\NP>SOther derivations are possible.
In fact, every pairof adjacent words above may be combined di-rectly.
For example, see and a may combinethrough forward composition to produce the cate-gory (S\NP)/N, and Ed?s category may type-raiseto S/(S\NP) and compose with might?s category.Baldridge uses these properties to define tag2Note that supertags can be lexical categories of CCG(Steedman, 2000), elementary trees of Tree-adjoining Gram-mar (Joshi, 1988), or types in a feature hierarchy as in Head-driven Phrase Structure Grammar (Pollard and Sag, 1994).496transition distributions that have higher likeli-hood for simpler categories that are able tocombine.
For example, for the distributionp(ti|ti?1=NP ), (S\NP)\NP is more likely than((S\NP)/(N/N))\NP because both categories maycombine with a preceding NP but the former issimpler.
In turn, the latter is more likely than NP: itis more complex but can combine with the preced-ing NP.
Finally, NP is more likely than (S/NP)/NPsince neither can combine, but NP is simpler.By starting EM with these tag transition dis-tributions and an unfiltered lexicon (word-to-supertag dictionary), Baldridge obtains a taggingaccuracy of 56.1% on ambiguous words?a largeimprovement over the accuracy of 33.0% obtainedby starting with uniform transition distributions.We refer to a model learned from basic EM (uni-formly initialized) as EM, and to a model withgrammar-informed initialization as EMGI .4 Minimized models for supertaggingThe idea of searching for minimized models isrelated to classic Minimum Description Length(MDL) (Barron et al, 1998), which seeks to se-lect a small model that captures the most regularityin the observed data.
This modeling strategy hasbeen shown to produce good results for many nat-ural language tasks (Goldsmith, 2001; Creutz andLagus, 2002; Ravi and Knight, 2009).
For tagging,the idea has been implemented using Bayesianmodels with priors that indirectly induce sparsityin the learned models (Goldwater and Griffiths,2007); however, Ravi and Knight (2009) show abetter approach is to directly minimize the modelusing an integer programming (IP) formulation.Here, we build on this idea for supertagging.There are many challenges involved in using IPminimization for supertagging.
The 1241 distinctsupertags in the tagset result in 1.5 million tag bi-gram entries in the model and the dictionary con-tains almost 3.5 million word/tag pairs that are rel-evant to the test data.
The set of 45 POS tags forthe same data yields 2025 tag bigrams and 8910dictionary entries.
We also wish to scale our meth-ods to larger data settings than the 24k word tokensin the test data used in the POS tagging task.Our objective is to find the smallest supertaggrammar (of tag bigram types) that explains theentire text while obeying the lexicon?s constraints.However, the original IP method of Ravi andKnight (2009) is intractable for supertagging, sowe propose a new two-stage method that scales tothe larger tagsets and data involved.4.1 IP method for supertaggingOur goal for supertagging is to build a minimizedmodel with the following objective:IPoriginal: Find the smallest supertag gram-mar (i.e., tag bigrams) that can explain the en-tire text (the test word token sequence).Using the full grammar and lexicon to performmodel minimization results in a very large, diffi-cult to solve integer program involving billions ofvariables and constraints.
This renders the mini-mization objective IPoriginal intractable.
One wayof combating this is to use a reduced grammarand lexicon as input to the integer program.
Wedo this without further supervision by using theHMM model trained using basic EM: entries arepruned based on the tag sequence it predicts onthe test data.
This produces an observed grammarof distinct tag bigrams (Gobs) and lexicon of ob-served lexical assignments (Lobs).
For CCGbank,Gobs and Lobs have 12,363 and 18,869 entries,respectively?far less than the millions of entriesin the full grammar and lexicon.Even though EM minimizes the model some-what, many bad entries remain in the grammar.We prune further by supplying Gobs and Lobs asinput (G,L) to the IP-minimization procedure.However, even with the EM-reduced grammar andlexicon, the IP-minimization is still very hard tosolve.
We thus split it into two stages.
The firststage (Minimization 1) finds the smallest grammarGmin1 ?
G that explains the set of word bigramtypes observed in the data rather than the wordsequence itself, and the second (Minimization 2)finds the smallest augmentation of Gmin1 that ex-plains the full word sequence.Minimization 1 (MIN1).
We begin with a sim-pler minimization problem than the original one(IPoriginal), with the following objective:IPmin 1: Find the smallest set of tag bigramsGmin1 ?
G, such that there is at least onetagging assignment possible for every word bi-gram type observed in the data.We formulate this as an integer program, creat-ing binary variables gvari for every tag bigramgi = tjtk in G. Binary link variables connect tagbigrams with word bigrams; these are restricted497::titj::Input Grammar (G) word bigrams:w1w2w2w3::wiwj::MIN 1::titj::Input Grammar (G) word bigrams:w1w2w2w3::wiwj::word sequence:w1w2w3w4w5t1t2t3::tksupertagstag bigrams chosen in first minimization step (Gmin1)(does not explain the word sequence)word sequence:w1w2w3w4w5t1t2t3::tksupertagstag bigrams chosen in second minimization step (Gmin2)MIN 2IP Minimization 1IP Minimization 2Figure 1: Two-stage IP method for selecting minimized models for supertagging.to the set of links that respect the lexicon L pro-vided as input, i.e., there exists a link variablelinkjklm connecting tag bigram tjtk with word bi-gram wlwm only if the word/tag pairs (wl, tj) and(wm, tk) are present in L. The entire integer pro-gramming formulation is shown Figure 2.The IP solver3 solves the above integer programand we extract the set of tag bigrams Gmin1 basedon the activated grammar variables.
For the CCG-bank test data, MIN1 yields 2530 tag bigrams.However, a second stage is needed since there isno guarantee that Gmin1 can explain the test data:it contains tags for all word bigram types, but itcannot necessarily tag the full word sequence.
Fig-ure 1 illustrates this.
Using only tag bigrams fromMIN1 (shown in blue), there is no fully-linked tagpath through the network.
There are missing linksbetween words w2 and w3 and between words w3and w4 in the word sequence.
The next stage fillsin these missing links.Minimization 2 (MIN2).
This stage uses theoriginal minimization formulation for the su-pertagging problem IPoriginal, again using an in-teger programming method similar to that pro-posed by Ravi and Knight (2009).
If applied tothe observed grammar Gobs, the resulting integerprogram is hard to solve.4 However, by using thepartial solution Gmin1 obtained in MIN1 the IPoptimization speeds up considerably.
We imple-ment this by fixing the values of all binary gram-mar variables present in Gmin1 to 1 before opti-mization.
This reduces the search space signifi-3We use the commercial CPLEX solver.4The solver runs for days without returning a solution.Minimize:?
?gi?GgvariSubject to constraints:1.
For every word bigram wlwm, there exists at leastone tagging that respects the lexicon L.??
tj?L(wl), tk?L(wm)linkjklm ?
1where L(wl) and L(wm) represent the set of tags seenin the lexicon for words wl and wm respectively.2.
The link variable assignments are constrained to re-spect the grammar variables chosen by the integer pro-gram.linkjklm ?
gvariwhere gvari is the binary variable corresponding to tagbigram tjtk in the grammar G.Figure 2: IP formulation for Minimization 1.cantly, and CPLEX finishes in just a few hours.The details of this method are described below.We instantiate binary variables gvari and lvarifor every tag bigram (in G) and lexicon entry (inL).
We then create a network of possible taggingsfor the word token sequence w1w2....wn in thecorpus and assign a binary variable to each linkin the network.
We name these variables linkcjk,where c indicates the column of the link?s sourcein the network, and j and k represent the link?ssource and destination (i.e., linkcjk corresponds totag bigram tjtk in column c).
Next, we formulatethe integer program given in Figure 3.Figure 1 illustrates how MIN2 augments thegrammar Gmin1 (links shown in blue) with addi-498Minimize:?
?gi?GgvariSubject to constraints:1.
Chosen link variables form a left-to-right paththrough the tagging network.
?c=1..n?2?k?j linkcjk =?j link(c+1)kj2.
Link variable assignments should respect the chosengrammar variables.for every link: linkcjk ?
gvariwhere gvari corresponds to tag bigram tjtk3.
Link variable assignments should respect the chosenlexicon variables.for every link: linkcjk ?
lvarwctjfor every link: linkcjk ?
lvarwc+1tkwhere wc is the cth word in the word sequence w1...wn,and lvarwctj is the binary variable corresponding to theword/tag pair wc/tj in the lexicon L.4.
The final solution should produce at least one com-plete tagging path through the network.?
?j,klink1jk ?
15.
Provide minimized grammar from MIN1as partialsolution to the integer program.
?gi?Gmin1 gvari = 1Figure 3: IP formulation for Minimization 2.tional tag bigrams (shown in red) to form a com-plete tag path through the network.
The minimizedgrammar set in the final solution Gmin2 containsonly 2810 entries, significantly fewer than theoriginal grammar Gobs?s 12,363 tag bigrams.We note that the two-stage minimization pro-cedure proposed here is not guaranteed to yieldthe optimal solution to our original objectiveIPoriginal.
On the simpler task of unsupervisedPOS tagging with a dictionary, we comparedour method versus directly solving IPoriginal andfound that the minimization (in terms of grammarsize) achieved by our method is close to the opti-mal solution for the original objective and yieldsthe same tagging accuracy far more efficiently.Fitting the minimized model.
The IP-minimization procedure gives us a minimalgrammar, but does not fit the model to the data.In order to estimate probabilities for the HMMmodel for supertagging, we use the EM algorithmbut with certain restrictions.
We build the transi-tion model using only entries from the minimizedgrammar set Gmin2, and instantiate an emissionmodel using the word/tag pairs seen in L (pro-vided as input to the minimization procedure).
Allthe parameters in the HMM model are initializedwith uniform probabilities, and we run EM for 40iterations.
The trained model is used to find theViterbi tag sequence for the corpus.
We refer tothis model (where the EM output (Gobs, Lobs) wasprovided to the IP-minimization as initial input)as EM+IP.Bootstrapped minimization.
The quality of theobserved grammar and lexicon improves consid-erably at the end of a single EM+IP run.
Raviand Knight (2009) exploited this to iteratively im-prove their POS tag model: since the first mini-mization procedure is seeded with a noisy gram-mar and tag dictionary, iterating the IP procedurewith progressively better grammars further im-proves the model.
We do likewise, bootstrapping anew EM+IP run using as input, the observed gram-mar Gobs and lexicon Lobs from the last taggingoutput of the previous iteration.
We run this untilthe chosen grammar set Gmin2 does not change.54.2 Minimization with grammar-informedinitializationThere are two complementary ways to usegrammar-informed initialization with the IP-minimization approach: (1) using EMGI outputas the starting grammar/lexicon and (2) using thetag transitions directly in the IP objective function.The first takes advantage of the earlier observationthat the quality of the grammar and lexicon pro-vided as initial input to the minimization proce-dure can affect the quality of the final supertaggingoutput.
For the second, we modify the objectivefunction used in the two IP-minimization steps tobe:Minimize:?
?gi?Gwi ?
gvari (1)where, G is the set of tag bigrams provided as in-put to IP, gvari is a binary variable in the integerprogram corresponding to tag bigram (ti?1, ti) ?G, and wi is negative logarithm of pgii(ti|ti?1)as given by Baldridge (2008).6 All other parts of5In our experiments, we run three bootstrap iterations.6Other numeric weights associated with the tag bi-grams could be considered, such as 0/1 for uncombin-499the integer program including the constraints re-main unchanged, and, we acquire a final tagger inthe same manner as described in the previous sec-tion.
In this way, we combine the minimizationand GI strategies into a single objective functionthat finds a minimal grammar set while keepingthe more likely tag bigrams in the chosen solution.EMGI+IPGI is used to refer to the method thatuses GI information in both ways: EMGI outputas the starting grammar/lexicon and GI weights inthe IP-minimization objective.5 ExperimentsWe compare the four strategies described in Sec-tions 3 and 4, summarized below:EM HMM uniformly initialized, EM training.EM+IP IP minimization using initial grammarprovided by EM.EMGI HMM with grammar-informed initializa-tion, EM training.EMGI+IPGI IP minimization using initial gram-mar/lexicon provided by EMGI and addi-tional grammar-informed IP objective.For EM+IP and EMGI+IPGI , the minimizationand EM training processes are iterated until theresulting grammar and lexicon remain unchanged.Forty EM iterations are used for all cases.We also include a baseline which randomlychooses a tag from those associated with eachword in the lexicon, averaged over three runs.Accuracy on ambiguous word tokens.
Weevaluate the performance in terms of tagging accu-racy with respect to gold tags for ambiguous wordsin held-out test sets for English and Italian.
Weconsider results with and without punctuation.7Recall that unlike much previous work, we donot collect the lexicon (tag dictionary) from thetest set: this means the model must handle un-known words and the possibility of having missinglexical entries for covering the test set.Precision and recall of grammar and lexicon.In addition to accuracy, we measure precision andable/combinable bigrams.7The reason for this is that the ?categories?
for punctua-tion in CCGbank are for the most part not actual categories;for example, the period ?.?
has the categories ?.?
and ?S?.As such, these supertags are outside of the categorial system:their use in derivations requires phrase structure rules that arenot derivable from the CCG combinatory rules.Model ambig ambig all all-punc -puncRandom 17.9 16.2 27.4 21.9EM 38.7 35.6 45.6 39.8EM+IP 52.1 51.0 57.3 53.9EMGI 56.3 59.4 61.0 61.7EMGI+IPGI 59.6 62.3 63.8 64.3Table 2: Supertagging accuracy for CCGbank sec-tions 22-24.
Accuracies are reported for foursettings?
(1) ambiguous word tokens in the testcorpus, (2) ambiguous word tokens, ignoringpunctuation, (3) all word tokens, and (4) all wordtokens except punctuation.recall for each model on the observed bitag gram-mar and observed lexicon on the test set.
We cal-culate them as follows, for an observed grammaror lexicon X:Precision =|{X} ?
{Observedgold}||{X}|Recall =|{X} ?
{Observedgold}||{Observedgold}|This provides a measure of model performance onbitag types for the grammar and lexical entry typesfor the lexicon, rather than tokens.5.1 English CCGbank resultsAccuracy on ambiguous tokens.
Table 2 givesperformance on the CCGbank test sections.
Allmodels are well above the random baseline, andboth of the strategies individually boost perfor-mance over basic EM by a large margin.
For themodels using GI, accuracy ignoring punctuation ishigher than for all almost entirely due to the factthat ?.?
has the supertags ?.?
and S, and the GIgives a preference to S since it can in fact combinewith other categories, unlike ?.?
?the effect is thatnearly every sentence-final period (?5.5k tokens) istagged S rather than ?.
?.EMGI is more effective than EM+IP; however,it should be kept in mind that IP-minimizationis a general technique that can be applied toany sequence prediction task, whereas grammar-informed initialization may be used only withtasks in which the interactions of adjacent labelsmay be derived from the labels themselves.
In-terestingly, the gap between the two approachesis greater when punctuation is ignored (51.0 vs.59.4)?this is unsurprising because, as noted al-ready, punctuation supertags are not actual cate-500EM EM+IP EMGI EMGI+IPGIGrammarPrecision 7.5 32.9 52.6 68.1Recall 26.9 13.2 34.0 19.8LexiconPrecision 58.4 63.0 78.0 80.6Recall 50.9 56.0 71.5 67.6Table 3: Comparison of grammar/lexicon ob-served in the model tagging vs. gold taggingin terms of precision and recall measures for su-pertagging on CCGbank data.gories, so EMGI is unable to model their distribu-tion.
Most importantly, the complementary effectsof the two approaches can be seen in the improvedresults for EMGI+IPGI , which obtains about 3%better accuracy than EMGI .Accuracy on all tokens.
Table 2 also gives per-formance when taking all tokens into account.
TheHMM when using full supervision obtains 87.6%accuracy (Baldridge, 2008),8 so the accuracy of63.8% achieved by EMGI+IPGI nearly halves thegap between the supervised model and the 45.6%obtained by basic EM semi-supervised model.Effect of GI information in EM and/or IP-minimization stages.
We can also consider theeffect of GI information in either EM training orIP-minimization to see whether it can be effec-tively exploited in both.
The latter, EM+IPGI ,obtains 53.2/51.1 for all/no-punc?a small gaincompared to EM+IP?s 52.1/51.0.
The former,EMGI+IP, obtains 58.9/61.6?a much larger gain.Thus, the better starting point provided by EMGIhas more impact than the integer program that in-cludes GI in its objective function.
However, wenote that it should be possible to exploit the GIinformation more effectively in the integer pro-gram than we have here.
Also, our best model,EMGI+IPGI , uses GI information in both stagesto obtain our best accuracy of 59.6/62.3.P/R for grammars and lexicons.
We can ob-tain a more-fine grained understanding of how themodels differ by considering the precision and re-call values for the grammars and lexicons of thedifferent models, given in Table 3.
The basic EMmodel has very low precision for the grammar, in-dicating it proposes many unnecessary bitags; it8A state-of-the-art, fully-supervised maximum entropytagger (Clark and Curran, 2007) (which also uses part-of-speech labels) obtains 91.4% on the same train/test split.achieves better recall because of the sheer num-ber of bitags it proposes (12,363).
EM+IP prunesthat set of bitags considerably, leading to betterprecision at the cost of recall.
EMGI ?s higher re-call and precision indicate the tag transition dis-tributions do capture general patterns of linkagebetween adjacent CCG categories, while EM en-sures that the data filters out combinable, but un-necessary, bitags.
With EMGI+IPGI , we againsee that IP-minimization prunes even more entries,improving precision at the loss of some recall.Similar trends are seen for precision and recallon the lexicon.
IP-minimization?s pruning of inap-propriate taggings means more common words arenot assigned highly infrequent supertags (boostingprecision) while unknown words are generally as-signed more sensible supertags (boosting recall).EMGI again focuses taggings on combinable con-texts, boosting precision and recall similarly toEM+IP, but in greater measure.
EMGI+IPGI thenprunes some of the spurious entries, boosting pre-cision at some loss of recall.Tag frequencies predicted on the test set.
Ta-ble 4 compares gold tags to tags generated byall four methods for the frequent and highly am-biguous words the and in.
Basic EM wandersfar away from the gold assignments; it has littleguidance in the very large search space availableto it.
IP-minimization identifies a smaller set oftags that better matches the gold tags; this emergesbecause other determiners and prepositions evokesimilar, but not identical, supertags, and the gram-mar minimization pushes (but does not force)them to rely on the same supertags wherever pos-sible.
However, the proportions are incorrect;for example, the tag assigned most frequently toin is ((S\NP)\(S\NP))/NP though (NP\NP)/NPis more frequent in the test set.
EMGI ?s tagscorrect that balance and find better proportions,but also some less common categories, such as(((N/N)\(N/N))\((N/N)\(N/N)))/N, sneak in be-cause they combine with frequent categories likeN/N and N. Bringing the two strategies togetherwith EMGI+IPGI filters out the unwanted cate-gories while getting better overall proportions.5.2 Italian CCG-TUT resultsTo demonstrate that both methods and their com-bination are language independent, we apply themto the Italian CCG-TUT corpus.
We wantedto evaluate performance out-of-the-box because501Lexicon Gold EM EM+IP EMGI EMGI+IPGIthe?
(41 distinct tags in Ltrain) (14 tags) (18 tags) (9 tags) (25 tags) (12 tags)NP[nb]/N 5742 0 4544 4176 4666((S\NP)\(S\NP))/N 14 5 642 122 107(((N/N)\(N/N))\((N/N)\(N/N)))/N 0 0 0 698 0((S/S)/S[dcl])/(S[adj]\NP) 0 733 0 0 0PP/N 0 1755 0 3 1: : : : : :in?
(76 distinct tags in Ltrain) (35 tags) (20 tags) (17 tags) (37 tags) (14 tags)(NP\NP)/NP 883 0 649 708 904((S\NP)\(S\NP))/NP 793 0 911 320 424PP/NP 177 1 33 12 82((S[adj]\NP)/(S[adj]\NP))/NP 0 215 0 0 0: : : : : :Table 4: Comparison of tag assignments from the gold tags versus model tags obtained on the test set.The table shows tag assignments (and their counts for each method) for the and in in the CCGbank testsections.
The number of distinct tags assigned by each method is given in parentheses.
Ltrain is thelexicon obtained from sections 0-18 of CCGbank that is used as the basis for EM training.Model TEST 1 TEST 2 (using lexicon from:)NPAPER+CIVIL NPAPER CIVILRandom 9.6 9.7 8.4 9.6EM 26.4 26.8 27.2 29.3EM+IP 34.8 32.4 34.8 34.6EMGI 43.1 43.9 44.0 40.3EMGI+IPGI 45.8 43.6 47.5 40.9Table 5: Comparison of supertagging results forCCG-TUT.
Accuracies are for ambiguous wordtokens in the test corpus, ignoring punctuation.bootstrapping a supertagger for a new language isone of the main use scenarios we envision: in sucha scenario, there is no development data for chang-ing settings and parameters.
Thus, we determineda train/test split beforehand and ran the methodsexactly as we had for CCGbank.The results, given in Table 5, demonstrate thesame trends as for English: basic EM is far moreaccurate than random, EM+IP adds another 8-10%absolute accuracy, and EMGI adds an additional 8-10% again.
The combination of the methods gen-erally improves over EMGI , except when the lex-icon is extracted from NPAPER+CIVIL.
Table 6gives precision and recall for the grammars andlexicons for CCG-TUT?the values are lower thanfor CCGbank (in line with the lower baseline), butexhibit the same trends.6 ConclusionWe have shown how two complementarystrategies?grammar-informed tag transitions andIP-minimization?for learning of supertaggersfrom highly ambiguous lexicons can be straight-EM EM+IP EMGI EMGI+IPGIGrammarPrecision 23.1 26.4 44.9 46.7Recall 18.4 15.9 24.9 22.7LexiconPrecision 51.2 52.0 54.8 55.1Recall 43.6 42.8 46.0 44.9Table 6: Comparison of grammar/lexicon ob-served in the model tagging vs. gold taggingin terms of precision and recall measures for su-pertagging on CCG-TUT.forwardly integrated.
We verify the benefits ofboth cross-lingually, on English and Italian data.We also provide a new two-stage integer program-ming setup that allows model minimization to betractable for supertagging without sacrificing thequality of the search for minimal bitag grammars.The experiments in this paper use large lexi-cons, but the methodology will be particularly use-ful in the context of bootstrapping from smallerones.
This brings further challenges; in particular,it will be necessary to identify novel entries con-sisting of seen word and seen category and to pre-dict unseen, but valid, categories which are neededto explain the data.
For this, it will be necessaryto forgo the assumption that the provided lexiconis always obeyed.
The methods we introduce hereshould help maintain good accuracy while open-ing up these degrees of freedom.
Because the lexi-con is the grammar in CCG, learning new word-category associations is grammar generalizationand is of interest for grammar acquisition.502Finally, such lexicon refinement and generaliza-tion is directly relevant for using CCG in syntax-based machine translation models (Hassan et al,2009).
Such models are currently limited to lan-guages for which corpora annotated with CCGderivations are available.
Clark and Curran (2006)show that CCG parsers can be learned from sen-tences labeled with just supertags?without fullderivations?with little loss in accuracy.
The im-provements we show here for learning supertag-gers from lexicons without labeled data may beable to help create annotated resources more ef-ficiently, or enable CCG parsers to be learned withless human-coded knowledge.AcknowledgementsThe authors would like to thank Johan Bos, JoeyFrazee, Taesun Moon, the members of the UT-NLL reading group, and the anonymous review-ers.
Ravi and Knight acknowledge the supportof the NSF (grant IIS-0904684) for this work.Baldridge acknowledges the support of a grantfrom the Morris Memorial Trust Fund of the NewYork Community Trust.ReferencesJ.
Baldridge.
2008.
Weakly supervised supertaggingwith grammar-informed initialization.
In Proceed-ings of the 22nd International Conference on Com-putational Linguistics (Coling 2008), pages 57?64,Manchester, UK, August.M.
Banko and R. C. Moore.
2004.
Part of speechtagging in context.
In Proceedings of the Inter-national Conference on Computational Linguistics(COLING), page 556, Morristown, NJ, USA.A.
R. Barron, J. Rissanen, and B. Yu.
1998.
Theminimum description length principle in coding andmodeling.
IEEE Transactions on Information The-ory, 44(6):2743?2760.J.
Bos, C. Bosco, and A. Mazzei.
2009.
Converting adependency treebank to a categorial grammar tree-bank for Italian.
In Proceedings of the Eighth In-ternational Workshop on Treebanks and LinguisticTheories (TLT8), pages 27?38, Milan, Italy.S.
Clark and J. Curran.
2006.
Partial training fora lexicalized-grammar parser.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 144?151, NewYork City, USA, June.S.
Clark and J. Curran.
2007.
Wide-coverage efficientstatistical parsing with CCG and log-linear models.Computational Linguistics, 33(4).M.
Creutz and K. Lagus.
2002.
Unsupervised discov-ery of morphemes.
In Proceedings of the ACLWork-shop on Morphological and Phonological Learning,pages 21?30, Morristown, NJ, USA.Y.
Goldberg, M. Adler, and M. Elhadad.
2008.
EM canfind pretty good HMM POS-taggers (when given agood start).
In Proceedings of the ACL, pages 746?754, Columbus, Ohio, June.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics, 27(2):153?198.S.
Goldwater and T. L. Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the ACL, pages 744?751,Prague, Czech Republic, June.H.
Hassan, K. Sima?an, and A.
Way.
2009.
A syntac-tified direct translation model with linear-time de-coding.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing, pages 1182?1191, Singapore, August.J.
Hockenmaier and M. Steedman.
2007.
CCGbank:A corpus of CCG derivations and dependency struc-tures extracted from the Penn Treebank.
Computa-tional Linguistics, 33(3):355?396.A.
Joshi.
1988.
Tree Adjoining Grammars.
In DavidDowty, Lauri Karttunen, and Arnold Zwicky, ed-itors, Natural Language Parsing, pages 206?250.Cambridge University Press, Cambridge.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2).B.
Merialdo.
1994.
Tagging English text with aprobabilistic model.
Computational Linguistics,20(2):155?171.C.
Pollard and I.
Sag.
1994.
Head Driven PhraseStructure Grammar.
CSLI/Chicago UniversityPress, Chicago.S.
Ravi and K. Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 504?512, Suntec, Singapore, August.M.
Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA.Kristina Toutanova and Mark Johnson.
2008.
ABayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In Proceedings of the Ad-vances in Neural Information Processing Systems(NIPS), pages 1521?1528, Cambridge, MA.
MITPress.503
