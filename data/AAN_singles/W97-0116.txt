Acquiring German PrepositionalSubcategorization Frames from CorporaEr ika  F.  de L imaG1VID - German Nat iona l  Research  Centerfor In fo rmat ion  TechnologyDoHvostrasse 1564293 Darmstadt ,  Germanydelima@darmsCadt, gmd.
deJuly 7, 1997AbstractThis paper presents a procedure to automaticafly learn Germanprepositional subcategofization frames fzom text corpora.
It is basedon shallow parsing techniques employed to identify high-accuracy cuesfor prepositional frames, the EM algorithm to solve the PP attachmentproblem implicit in the task, and a method to rank the evidence forsubcategorization provided by the collected data.1 IntroductionThe description of lexical forms in both computation and hun~-orientedlexica include prepositional subcategoriza~ion i formation.
For instance inGerman, the verb arbeiten ('to work') subcategorizes fora PP headed by thepreposition an ('on'), and the verb erinnern ('to remind'), for an accusativeNP and a PP headed by an:(1) Mary arbeitet an der Frage P .~ NP.Mary works on the question(2) Mary exinnert ihren Freund an den Terrain.Mary reminds her friend on the deadline'Mary reminds her friend of the deadline.
'Subcategorization information is usually compiled by hand.
A procedure toautomatically learn prepositional subcategorization would enable the acqui-153sition of broad-coverage lexica which reflect evolving usage and which areless subject o lexical gaps.Learning prepositional subcategorization automatically is not a trivialt~LSk; it entails a PP attachment decision problem, and requires being ableto distinguish complement from adjunct prepositional cues.
For instance in(2) above, it is (syntactically) possible to attach the prepositional phrase\[pp an den Termini (to the noun phrase object as well as to the verb phrase.Sentence (2) cannot be considered conclusive vidence of a verbal framebased on syntactical information alone.In (3) the prepositional phrase \[pp in der Nacht\] ('at night') is an adjunctPP which may occur with any (aspectuaUy compatible) verb.
It is notspecific of the verb arbeiten ('to work') and should not be considered evidenceof subcategorization.
(3) Mary arbeitete in der Nacht.Mary worked in the night'Mary worked at night.
'This paper proposes a method to automatically acquire German preposi-tional subcategorization frames (SFs) fzom text corpora.
It is based on shal-low parsing techniques employed to identify high-accuracy ues for prepo-sitional SFs, and a method to rank the evidence for subcategorization pro-vided by the collected ata.
The PP attachment problem implicit in thetask is dealt with by using the EM algorithm to rank alternative frames.The subcategorization frames considered are shown in figure 1.2 MethodThe automatic extraction of German prepositional SFs is based on the ob-servation that certain constructs involving so-called pronominal adverbs arehigh-accuracy ues for prepositional subcategorization.
Pronominal adverbsare compounds in German consisting of the adverbs da(r)- and wo(r)- andcertain prepositions.
For instance in (4c), the pronominal adverb daran('about it') is used as a pro-form for the personal pronoun es ('it') as theobject of the preposition an ('about').
(Note that the usage of the pronoun(4b) is ungrammatical.)
In (4d), the pronominal adverb daran occurs ina correlative construct with a subordinate daft ('that') clause immediatelyfollowing it.
(4) a. Mary denkt an Johns Ankuft.Mary thlnk~ on John's arrival'Mary thinks about John's arrival .
'154Example SF DescriptionPP\[auf\] V\[warten\]'wait for' Verb with PPPP\[an\] NPA V\[erlnnern\]'remind NP of' Verb with accusative object and PPPP\[fiir\] NPD V\[danken\]'thauk NP for' Verb with dative object and PPPP\[auf\] sich V\[vorbereiten\]'to prepare oneself for' reflexive verb with PPPP\[auf\] N\[Hoffmtmg\]'hope for' Noun with PPPP\[auf\] A\[stolz\]'proud of" Adjective with PPFigure 1: Subcategorization flames learned by the systemb.
*Mary denkt an es.Mary thinks on itc.
Mary denkt daran.Mary thinks on it'Mary thlulcs about it.'d.
Mary denkt damn, da6 John bald aukommt.Mary thinks on it that John soon arrives'Mary thinks about the fact that John will arrive soon.
'Unlike prepositional phrases, pronominal adverb correlative constructs pro-vide reliable cues for prepositional subcategorization.
For instance the oc-currence of the pronominal dverb damn in the correlative construct in (4d)can be used to infer that the verb denken ('to think') subcategorizes for aPP headed by the preposition an (~about').In the next section, a learning procedure is described which makes useof pronomln~.1 adverb correlative constructs to infer prepositional subcate-gorization.
It consists of four components: SF detection, mapping, disam-biguation, and ranldng.1552.1 SF  Detect ionThis component makes use of shallow parsing tte,,hn~ques to detect possibleprepositional SF ~ructures; a standard CFG parser is used with a hand-written grammar d~qn~ng pairs of main and subordinate clauses in correla-tive constructs such as (4d).
Main clauses covered by the grammar includecopular constructs as well as active and passive verb-second and verb-finalconstructs.
Subordinate clauses considered include those headed by daft('that'), indirect interrogative clauses, and infinitival clauses.The internal structure of the clause pair consists of phrase-like con-stituents; these include nominative (NC)~ prepositional (PC), adjectival(AC), verbal (VC), and clausal constituents.
Their deiq-ltion is non-standard;for instance, all prepositional phrases, whether complement or not, are leftunattached.
As an example, the shallow parse structure for the sentencefragment in (5) is shown in (5') below.
(5) Er lobte die Reaktion der 5ffentlichen Meinung in RuBlandhe praised the reaction the public opinion in Russiaals Beweis dafiir, daB...as proof for it that'He praised the reaction of the public opinion in Russia as proof ofthe fact that .
.
.
'(e) Is Sr\]\[vc lobte\]\[NC die R ktion\]\[A'c der 5~ent\]ichen Meinung\]\[PC in Ru.Bland\]\[PC Ms Beweis\]\ [Pc\[sc daS.
.
.
\]2.2 SF  Mapp ingThe SF Mapping component maps a shallow parse structure of a main clausein a pronominal dverb correlative construct to a set of putative subcatego-rization frames reflecting structural as wen as morphological mbiguities inthe original sentence.
Alternative SFs usually stem from an ambiguity in theattachment ofthe pronominal dverb PP.
The mapping is defined as follows.
(In the following, p denotes the preposition within the pronomlnal adverb156IIII,IIIiIIIIin a correlative construct main clause, VC the main verbal constituent inthe clause; v in VC\[v\] denotes the head Iemm~ of the verbal constituent,analogously for NC\[n\].)VC\[v\]/NC\[n\].
An active verb-second or verb-final clause with one NC ism~tpped to {PP\[p\] V\[v\]} if the NC precedes the finite verb/auxi~ary in theclause, otherwise to {PP\[p\] V\[v\], PP\[p\] N\[n\]}.For instance, sentence (6) is a verb-second clause with an adverbial inthe first position in the clause and one NC following the verb.
In thisconstruct, the PP headed by the pronominal adverb may potentially beattached to the verb phrase or to the nominal phrase immediately precedingit.
According to this rule, this sentence is mapped to {PP\[an\] V\[arbeiten\],PP\[anl N\[Student\]}.
(6) Jetzt arbeitet der Student daran, ...Now works the student on it'The student is now working on ... 'VC\[v\]/NCl\[nl\]/NC2\[n2\].
An active verb-second orverb-final clause withtwo nominal constituents NC1 and NC~ such that NC,2 follows NC1 in theclause is mapped to {PP\[p\] NPA V\[v\], PP\[p\] N\[n2\]}, if the head of NC2 is anoun, and to {PP\[p\] NPA V\[v\]} otherwise.Sentences (Ta,b) are examples to which this rule applies.
In (Ta) theverb erinnern ('to remind') subcategorizes for an accusative NP and a PPheaded by the preposition an ('on'), while in (To), the verb nehmen ('totake') is a support verb and Racksicht ('consideration') a noun which sub-categorizes for a PP headed by the preposition auf.
Since their shallowstructure is ambiguous, they are each mapped to a SF set reflecting bothattar hment alternatives; (Ta) is mapped to the set {PP\[an\] NPA V\[erirmern\],PP\[an\] N\[Freund\]}, and (Tb) to the set {PP\[auf\] NPA V\[nehmen\], PP\[auf\]N\[R~eksicht\]}.
(7) a. Mary erinnert ihren Freund daran, daB...Mary reminds her friend on it that'Mary reminds her friend of the fact that ... 'b.
Mary nimmt keine Rficksicht darauf, daft...Mary takes no consideration on it that'Mary shows no consideration for the fact that ... 'Copula/NCl\[nl\]/NC2\[n2\].
A copula clause with two nominal constituentsNCt\[nl\] and NC2\[n2\] such that NC2 follows NC1 and n2 is a noun is mappedto {PP\[p\] N\[n2\]}.
For instance (8) is mapped with this rule to {PP\[auf\]\[nin is\]}.157(8) Weft dies ein Hinweis darauf ist, da6...because this an indication on in is that'Because this is an indication (of the fact) that .
.
.
'Copula/NC\[n\]/AC\[a\].
A copula clause with one nominal and one ad-jectival constituent is mapped to {PP\[p\] N\[n\], PP\[p\] A\[a\],}.
For instance,with this rule the clause in (9) is mapped to {PP\[auf\] A\[stolz\], PP\[auf\]N\[Student\]}(9) Stolz ist der Student darauf, da6...proud is the student on it that'The student is proud of the fact that .
.
.
'PCs.
Any clause in wt~ch a PC immediately precedes the prronomlna\]adverb is mapped as in the appropriate rule with the additional element'PP\[p\] N\[n\]' in the set, where n is the head of the NC within the prepositionalconstituent.
For instance, (10) is mapped to {PP\[an\] V\[arbeiten\], PP\[an\]N\[Woche\]} with the VC/NC and PC rules.
(10) Mary arbeitet seit zwei Wochen daran, ...Mary works since two weeks on it'Mary has been working for two weeks on .
.
.
'Morphology.
Any clause in wldch a possible locus of attachment is mor-phologicaUy ambiguous i mapped with the appropriate rule applied to allmorphology alternatives.
For instance, (11) is mapped with the VC/NC andMorphology rules to {PP\[an\] V\[denken\], PP\[an\] V\[gedenken\]}, since g~achtis the past participle of both the verbs nken ('to think') and g~enken ('toconsider').
(11) Er hat daran gedacht, dat3 ...he has on it thought/considered that'He thought of .
.
.
'Passive/VC\[v\]/NC\[n\].
This rule is applied to 'werden ('to be') passiveverb-second or verb-final clause with one NC.
In case n is not the pronounes ('it'), the clause is mapped to (PP\[p\] NPA V\[v\]} ifNC precedes the verb,and to {PP\[p\] NPA V\[v\], PP\[p\] N\[n\]} otherwise.
In case n is the pronoun~, the clause is mapped to {PP\[p\] NPA V\[v\], PP\[p\] V\[v\]}.
For instance,(12) is mapped to {PP\[an\] NPA V\[erinnern\]}.
(12) Mary wird daran erinnert, da6...Mary is on it reminded that'Mary is reminded (of the fact) that .
.
.
'158IIi!2.3 SF D is~rnb iguat ionThe dis~rnhiguation component uses the expectation-maTirni~tion (EM)algorithm to assign probabilities to each frame in an SF alternative, givenall SF sets obtained for a given corpus.
The EM algorithm (Dempster, Laird,and Rubin, 1977) is a general iterative method to obtain maximum likelihoodestimators in incomplete data situations.
See (Vardi and Lee, 1993) for ageneral description of the algorithms as well as numerous examples of itsapplication.
The EM algorithm has been used to induce valence informationin (Carrol and Rooth, 1997).In the current setting, the algorithm is employed to rank the frames ina given SF set by using the relative vidence obtained for each frame in theset.
The algorithm is shown below.Algorithm.
Let F be a set o f  frames.
Further, let ~q be a finite set ofnonempty subsets of ~(F), and let F0 = I.J X.XE8Initialization step: for each frame z in F0:c0C~) = E (I(z,x).
go(X)) XE8Step k + 1 (k >= 0):Ck+l(Z) = ek(Z) "t- E (Pk(z,X) "ge(X))xE$Where ge is a ftmetion from S to the natural n-tubers mapping a set X tothe number of times it was produced by the SF mapping for a given corpusC.
Fm-ther, I, Pk, and Pk are run.
ions defined as follows:x:  e ?
\[0,1\]{ l~ i f zEX(z,X) ~ 0 elseF ?
\[0,1\]l ...e~l_.
if z e X and lX\[ > l(z,X) ~ ~Ex2" p~C~)0 elsepk : F --r \[0,1\]x~EFODefinition.
A frame z is best in the set X at the iteration k if z E X andp~(z) is an absolute maximum in U Pk(~)-~EX159In the algorithm above, 8 denotes the set of SF sets produced by the SFmapping for a given corpus C. In the initialization step, co assigns an initial"weight" to each frame, depending on its relative frequency of occurrence,and on whether the structures in which it occurred are ambiguous.
Theweight ck(x) of a frame x is used to estimate its probability pk(x).
Ineach iteration of the algoritBrn, the weight of a frame ?
is calculated byconsidering the totality of alternatives in which ~c occurs (i.e., the sets forwhich z E X and IX\[ > 1), and its probability within each alternative.The best frames in a set are the most probable frames given the evidenceprovided by the data.
In the experiment described in section 3~ the 6n~:lnumber of iterations was set empirically.2.4 SF l~k lngThis component ranks the SFs obtained by the previous component of thesystem.
Let ?c be the set of head lemmata (verbs, nouns and adjectives)in the subcategorization cues (i.e., best frames in the SF sets) for a givencorpus C. Let .~" be the set {NPA V\[-\], NPD V\[-\], V\[-\], PP\[an\] V\[.\], PP\[an\]NPA V\[-\], ...} of SF structures.
(Roughly, an SF structure is an SF with-out its head lernm~) The analysis of SF cues is performed by creating acontingency table cont~inlng the following counts for each lemma L E ?cand prepositional structure S E yr: k(L S) (k(L S)) is the count of lemm~ Lwith (without) structure S, and k(L S) (k(L S)) is the count of all \]~mm~tain ?c except L with (without) structure S.If a lemma L occurs independently of a structure S, then one wouldexpect that the distribution of L given that S is present and that of L giventhat S is not present have the same underlying parameter.
The log likelihoodstatistic is used to test this hypothesis.
This statistic is given by -2  log A =2(log L(p1, kl, hi) ?log L(p2, k2, n2)-log L(p, kl, R1)--log L(p, k2, n2)), wherelog LCo, k, n) = k logp + (n - k)log(1 -- p), and Pl = ~,  P2 = ~,  P = ,~',~;(For a detailed escription of the statistic used, see (Dunning, 1993)).In the formulae above, kl is k(L S), nl is the total number of occurrencesof S, k2 is/c(L S), and n2 the total number of occurrences ofstructures otherthan S. A large value of -2  log A for a lemma L and structure S m~n~ thatthe outcome is such that the hypothesis that the two distributions havethe same underlying parameter is ,mllicely, and that a lemm~ L is highlyassociated with a structure S in a given corpus.
This value is used torank the subcategorization cues produced by the previous components ofthe system.1603 Resu l tsThe method escribed in the previous ection was applied to 1 year of thenewspaper FFrankfu~er Allgemeine Zeitung containing approximately 36mil-lion word-like tokens.
A total of 16795 sentences matched the pronominaladverb correlative construct grammar described in section 2.1.3.1 SF D isambiguat ionOf the 16795 sets produced by the SF mapping, 5581 contained more thanone SF, i.e., reflected some form of ambiguity in the original sentence, ofwhich 4365 were unique.
A random set of 400 sets was obtained from theseunique ambiguous sets.
The disambiguation component produced a decisionfor 359 of these 400 sets.
These results were compared to the blind judge-ments of a single judge; 305 were found to be correct, 23 incorrect.
Therem~inlng 31 sets were considered to contain incorrect SFs solely.
Althoughan error rate of over 15% is not negligible, it is comparable to other PPattachment experiments (Collins and Brooks, 1995).3.2 Acqu i red  D ic t ionaryThe system acquired a dictionary of 1663 unique subcategorization frames.Figure 2 and 3 show the 30 most and 10 least plausible frames according tothe system.
Starred structures are considered to be errors.Examination ofthe r~n~ed SF table shows that frames with a low -2  logvalue consist mostly of errors.
The cues produced by the system are notperfect predictors of subcategorization.
False cues stem from incorrect de-cisions in the disambiguation component as well as parsing and mappingerrors, spurious adjuncts, or actual errors in the original text.In figures 3, two errors are due to the disambiguation component (nehmen,AmO; three errors stem from mistaking reflexive verbs for verbs t~ki~g anyaccusative object (sich treffen mit ('to meet with'), sich bekennen zu ('de-clare oneself for'), sich halten an ('to comply with')).
These stem from theg~arnm~.r specification, and can be avoided with further development of thedetection component.By far the most frequent type of error was the inclusion of an accusativeor dative NP in a verbal frame when the verb in fact only takes a PP.
Forinstance of the errors in the 31 sets (out of the 400 ambiguous sets exam-ined) containing incorrect SFs only, about 42% were due to the fact that anadditional accusative/dative NP was incorrectly included in a verbal frame,161-21ogA k(LS) k(ZS) k(L.~) k(Z~) L S13270.3167 1225 1691 284.2 3897009 hinweisen PP\[atLf\]V\[-\] ('pointto')6757.6162 498 337 1183 39007494857.2234 482 328 7909 38940484241.0161 429 529 5792 38960173307.6279 406 2510 3479 38963723179.3391 339 234 11293 38909013156.5375 342 433 8013 38939793118.6878 255 158 2810 389954,42897.6673 293 888 2766 3898820254.8.1622 385 796 20598 38809882253.9826 234 2682 860 38989912002.4658 174 128 3706 38987591605.2355 146 629 1155 39008371193.6521 190 383 25066 38771281042.8259 115 298 4968 3897386876.6903 64 121 610 3901972813.5798 74 761 510 3901422789.1838 78 122 4432 3898135777.0291 121 837 7860 3893949776.2428 62 92 1368 3901245766.8966 122 1059 6794 3894792766.6407 65 89 2105 3900508764.9588 135 640 16398 3885594686.0675 48 393 107 3902219684.1054 85 40 27806 3874836677.7402 70 1111 660 3900926656.856,1 67 455 1435 3900810577.1696 58 383 1371 3900955569.0955 43 78 815 3901831555.6635 61 89 7787 3894830Figure 2:aus~e.hellrechnenerinnernverweisenbestehensorrgenPP\[von\] V\[-\] ('assume')PP\[mit\] V\[- l ('reckon with')PP\[an\] V\[-\] ('remind of')PP\[auf\] V\[-\] ('refer to')PP\[in\] V\[.\] ('lie in')PP\[fftr\] V\[.\] ('care for')aussprechenPP\[fiir\] sich V\[.\] ('speak for')beitragen PP\[zu\] V\[-\] ('contribute to')fdhren PP\[zu\] V\[.\] ('lead to')an~ommen PP\[auf\] V\[-\] ('depend on')begrfmden PP\[mit\] NPA V\[-\]('substantiate NP with')pl~dieren PP\[fiir\] V\[-\] ('plead for')liegen PP\[in\] V\[-\] ('lie in')einsetzen PP\[fdr\] sich V\[.\] ('support')hindern PP\[an\] NPA V\[.\]('hinder NP from')PP\[von\] V\[-\] ('depend on')PP\[auf\] N\[-\] ('reference to')PP\[an\] V\[-\] ('think of')a-fimerk.~mPP\[auf\] A\[.\] ('attentive to')PP\[zu\] V\[-\] ('serve for')PP\[auf\] A\[.\] ('proud of')PP\[ffir\] V\[-\] ('speak for')abh~agenHinweisdenkendienenstolzsprechenhinweg-t~uschensehenneigenBeweisPP\[fiber\] V\[-\] ('obscure')PP\[in\] NPA V\[-\] ('see NP in')PP\[zu\] v\[.\] ('tend to')PP\[fiir\] N\[-\] ('proof of')nachdenken PP\[fiber\] V\[.\] ('thlnk about')abhalten PP\[von\] NPA V\[.\]('prevent NP from')Interesse PP\[an\] N\[-\] ('interest in')30 most plausible frames162-21og~ k(LS) k(LS) lc(L,~) I~(L,~) L S0.0126 1 301 115270.0117 1 463 93510.0112 4 831 177230.0087 1 204 208660.0054 1 159 226500.0047 1 184 225650.0029 1 809 50820.0011 1 957 39380.0005 1 204 186330.0002 1 521 75803890938 treffen *PP\[mit\] NPA V\[.\]3892952 bekennen*PP\[zu\] NPA V\[-\]3884209 wissen PP\[von\] V\[-\]('know of')3881696 nehmen *PP\[auiJ NPA V\[-\]3879957 fmden *PP\[durch\] NPA V.D3880017 halten *PP\[an\] NPA V\[.\]3896875 einsetzen *PP\[mit\] V\[.\]3897871 verdienen PP\[an\] V\[-\]('make a profit on')3883929 bringen PP\[auf~ NPA V\[-\]3894665 Amt *PP\[fSr\] N\[-\]Figure 3:10 least plausible framesalthough the prepos/tion i the frame was subcategorized for.
These stemfrom erroneous alternatives in the segmentation f nornin~!
constituents adefined by the grammar and could be eliminated with further developed ofthe detection component.Yet another type of error stems from pronominal dverbs which are con-junction/adverb homographs, or which are used anaphorically, while theverb in the main clause subcategorizes for a daj~ ('that') clause, so the sen-tence is erroneously considered to be a correlative construct.
This is thesource of most errors for flames involving the preposition gegen ('against'),bei ('by') and nach ('to'), and cannot be avoided given the learning strategy.Given the fact that the cues produced by the system are not perfectpredictors of subcategorization, a test of significance could be introduced inorder to filter out potentially erroneous cues.
However, it was observed thattruly "new" prepositional frames--frames not listed in broad coverage pub-lished dictionaries, or even considered to be erroneous by a native speakeruntil confronted with examples from the corpus--behaved with respect otheir rank;ngs very much like errors.
So the current version of the learn-ing procedure relies on manual post-editing assisted by the SF ranking andexamples from the corpus in order to discard f~!se frames.3.3 P rec i s ion  and  Recal lEvaluating the acqui~d ictionary is not straightfoward; linguists often dis-agree on the criteria for the complement/adjunct distinction.
Instead of163attempting a definition, the acquired ictionary was compared to a broadcoverage published ictionary cont~iniug explicit information on preposi-tional subcategorization.A random set of 300 verbs occurring more than 1000 times in the corpuswas obtained, z The prepositional SFs for these verbs which were listed in(W~brig, Kraemer, and Zimmerman, 1980) and in the acquired lexicon werenoted.
There was a total of 307 verbal prepositional frames listed in eitherdictionary.
Of these, 136 were listed only in the published ictionary, and121 only in the acquired ictionary.These prepositional SFs were used to calculate a lower bound for theprecision and recall rates of the system; A SF is considered correct if andonly if it is listed in the published ictionary.
2 A lower bound for the recallrate of the system is given by the number of learned correct frames dividedby the number of frames listed in the published ictionary, or 52/173.
Thisrecall rate is a lower-bound for the actual rate with respect o the corpus,since there are prepositional SFs listed in the published ictionary with noinstance in the corpus.A lower bound for the precision of the system is given by the number oflearned correct frames divided by the number of learned frames, or 52/188.This rate is a lower-bound for the actual precision rate of the system, sinceit does not take the fact into account hat the system did learn true SFsnot listed in the published ictionary, so the precision rate of the systemis actually higher.
Further, not all prepositions contributed equally to theprecision and recall rates.
For instance the precision and recall for theprepositions aus  ('out') was 60% and 42%, that of t~on ('off) 50% and 53%,while that of geeger~ ('against') 6% and 11%, respectively.4 Related WorkThe automatic extraction of English subcategorization frames has been con-sidered in (Brent, 1991; Brent, 1993), where a procedure is presented thattakes untamed text as input and generates a list of verbal subcategorizationframes.
The procedure uses a very simple heuristics to identify verbs; thesynt~t ic  types of nearby phrases are identified by relying on local morpho-syntactic ues.
Once potential verbs and SFs are identifled, a final com-1There was a total of 15178 unique verbs (known to the morphology) occurring in thecorpus, of which 913 occurred more than 1000 times.=No dictionary isexempt from errors (of omission).
However it (hopefully) provides a1,=iform classification for PP subcategorization.164portent attempts to determine when a lexical form occurs with a cue oftenenough so that it is unlikely to be due to errors; an automatically computederror rate is used to filter out potentially erroneous cues.
Prepositionalframes are not considered, since, according to the author, "it is not clearhow a machine learning system could do this \[determine which PPs arearguments and which are adjuncts\].
"In (Manning, 1991) another method is introduced for producing a dictio-nary of English verbal subcategorization frames.
This method makes use of astochastic tagger to determine part of speech, and a Finite state parser whichr~m.~ on the output of the ta~er, identifying auxiliary sequences, noting pu-tative complements after verbs and collecting histogram-type fr quencies ofpossible SFs.
The final component assesses the frames encountered by theparser by using the same model as (Brent, 1993), with the error rate set em-pirically.
Prepositional verbal frames are learned by the system by relyingon PPs as cues for subcategorization; since the system cannot differenti-ate between complement and adjunct prepositional cues, it learns frequentprepositional djuncts as well.In order to evaluate the acquired dictionary, M~nn~ng compares theframes obtained for 40 random verbs to those in a published ictionary,yielding for these verbs an overall precision and recall rates of 90~ and 43%respectively.
However, if only the prepositional frames listed for these verbsare considered, the rates drop to appro~mately 84% and 25%, respectively.In the experiment described, the error bounds for the filtering procedurewere chosen with the aim of "get\[ing\] a highly accurate dictionary at the ex-pense of recall."
His system did not consider nomlnal and adjectival frames.
(Carrol and Rooth, 1997) present a learnln~ procedure for English sub-categorization i formation.
Unlike previous approaches, it is based on aprobabilistic ontext free grammar.
The system uses expected frequenciesof head words and frames--calculated using a hand-written grammar andoccurrences in a text corpuswto iteratively estimate probability parametersfor a PCFG using the expectation maximi~.ation algorithm.
These parame-ters are used to rh~racterize v rbal, nominal and adjectival SFs.
The modeldoes not distinguish between complements and adjunct prepositional cues.5 Conc lus ionThis paper presents a method for learning German prepositional subcatego-rization frames.
Although other attempts have been made to learn Englishverbal/prepositional SFs from text corpora, no previous work considered a165ipartially free word-order language such as German, nor differentiated be-tween complement and adjunct prepositional cues.The overall precision rate for the system described in this paper is lowerthan that of similar systems developed for English, since no test of signif-icance was used to filter out possibly erroneous cues.
In the experimentdescribed in the previous ection, truly new prepositional frames behavedwith respect o frequency of occu~ence very much like errors, and wouldpossibly have been discarded by a filtering mechanism.A problem in the current version of the system was the fact that segmen-tation of nominal constituents was not optimally handled by the detectioncomponent, leading to a large mlmher of verbal frames with correct preposi-tions, but with an additional erroneous accusative/dative NC in the frame.So the precision of the system can be significantly improved with furtherdevelopment of the detection component.Further, the system should be extended to handle other types of pronom-inal adverb cues, such as pro-forms for interrogative, personal and relativepronouns; possibly PPs headed by prepositions should also be considered.Finally, the method-low-level parsing together with a procedure to ranlcalternatives obtained-should beextended to other frames as well.ReferencesBrent, Michael R. 1991.
Automatic acquisition of subcategofization frames~om untamed text.
In Proceedings of the ~h Annual Meeding of theAGL, pages 209-214.Brent, Michael R. 1993.
From grammar to lexicon: Unsupervised learningof lexical syntax.
Computational Linguistics, 19(2):243-262.Carrol, Glenn and Mats Rooth.
1997.
Valence induction with a head-lexiccMiz~ed PCPG.
http://www2.1ras.uni-stuttgart.de/,,,mats.Collins, Michael and James Brooks.
1995.
Prepositional phrase attachmentthrough a backed-off model.
In Proceeding8 of the Third Wor~hop onVery Large Corpora.Dempster, A.P., N.M. Laird, and D.B.
Rubin.
1977.
Maximuln likelihoodfrom inclomplete data via the em algorithm.
J.R.Sta~s.
Soc.
B, 39:1-38.Dnnnln.% Ted.
1993.
Accurate methods for the statistics of surprise andcoincidence.
Computational Linguistics, 19(1):61-74.166Mauling, Christopher D. 1991.
Automatic acquisition of a large subcate-gorization dictionary from corpora.
In Proceedings of the ~9th AnnualMeeding of the ACL, pages 235-242.Vardi, Y. and D. Lee.
1993.
From image deblurring to optimal invest-ments: Maximum likelihood solutions for positive linear inverse prob-lems.
Y.R.Statis.
Soc.
B, 55(3):569--612.Wahrig, Gerhard, Hildegard Kraemer, and Harald Zimmerman.
1980.Brockhaus Wahrig Deutsches W~irterbuch in secl~ B~nden.
F.A.
Brock-haus und Deutsche Verlags-Anstalt GmbH, Wiesbaden.167
