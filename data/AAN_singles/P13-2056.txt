Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 312?317,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCross-lingual Projections between Languages from Different FamiliesMo Yu1 Tiejun Zhao1 Yalong Bai1 Hao Tian2 Dianhai Yu21School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China{yumo,tjzhao,ylbai}@mtlab.hit.edu.cn2Baidu Inc., Beijing, China{tianhao,yudianhai}@baidu.comAbstractCross-lingual projection methods can ben-efit from resource-rich languages to im-prove performances of NLP tasks inresources-scarce languages.
However,these methods confronted the difficulty ofsyntactic differences between languagesespecially when the pair of languagesvaries greatly.
To make the projectionmethod well-generalize to diverse lan-guages pairs, we enhance the projec-tion method based on word alignmentsby introducing target-language word rep-resentations as features and proposing anovel noise removing method based onthese word representations.
Experimentsshowed that our methods improve the per-formances greatly on projections betweenEnglish and Chinese.1 IntroductionMost NLP studies focused on limited languageswith large sets of annotated data.
English andChinese are examples of these resource-rich lan-guages.
Unfortunately, it is impossible to buildsufficient labeled data for all tasks in all lan-guages.
To address NLP tasks in resource-scarcelanguages, cross-lingual projection methods wereproposed, which make use of existing resourcesin resource-rich language (also called source lan-guage) to help NLP tasks in resource-scarce lan-guage (also named as target language).There are several types of projection methods.One intuitive and effective method is to build acommon feature space for all languages, so thatthe model trained on one language could be di-rectly used on other languages (McDonald et al,2011; Ta?ckstro?m et al, 2012).
We call it di-rect projection, which becomes very popular re-cently.
The main limitation of these methods isthat target language has to be similar to sourcelanguage.
Otherwise the performance will de-grade especially when the orders of phrases be-tween source and target languages differ a lot.Another common type of projection methodsmap labels from resource-rich language sentencesto resource-scarce ones in a parallel corpus us-ing word alignment information (Yarowsky et al,2001; Hwa et al, 2005; Das and Petrov, 2011).We refer them as projection based on word align-ments in this paper.
Compared to other types ofprojection methods, this type of methods is morerobust to syntactic differences between languagessince it trained models on the target side thus fol-lowing the topology of the target language.This paper aims to build an accurate projec-tion method with strong generality to various pairsof languages, even when the languages are fromdifferent families and are typologically divergent.As far as we know, only a few works focusedon this topic (Xia and Lewis 2007; Ta?ckstro?met al, 2013).
We adopted the projection methodbased on word alignments since it is less affectedby language differences.
However, such methodsalso have some disadvantages.
Firstly, the modelstrained on projected data could only cover wordsand cases appeared in the target side of parallelcorpus, making it difficult to generalize to test datain broader domains.
Secondly, the performancesof these methods are limited by the accuracy ofword alignments, especially when words betweentwo languages are not one-one aligned.
So the ob-tained labeled data contains a lot of noises, makingthe models built on them less accurate.This paper aims to build an accurate projectionmethod with strong generality to various pairs oflanguages.
We built the method on top of projec-tion method based on word alignments because ofits advantage of being less affected by syntacticdifferences, and proposed two solutions to solvethe above two difficulties of this type of methods.312Firstly, we introduce Brown clusters of targetlanguage to make the projection models coverbroader cases.
Brown clustering is a kind of wordrepresentations, which assigns word with similarfunctions to the same cluster.
They can be ef-ficiently learned on large-scale unlabeled data intarget language, which is much easier to acquireeven when the scales of parallel corpora of minorlanguages are limited.
Brown clusters have beenfirst introduced to the field of cross-lingual projec-tions in (Ta?ckstro?m et al, 2012) and have achievedgreat improvements on projection between Euro-pean languages.
However, their work was basedon the direct projection methods so that it do notwork very well between languages from differentfamilies as will be shown in Section 3.Secondly, to reduce the noises in projection, wepropose a noise removing method to detect andcorrect noisy projected labels.
The method wasalso built on Brown clusters, based on the assump-tion that instances with similar representations ofBrown clusters tend to have similar labels.
As faras we know, no one has done any research on re-moving noises based on the space of word repre-sentations in the field of NLP.Using above techniques, we achieved a projec-tion method that adapts well on different languagepairs even when the two languages differ enor-mously.
Experiments of NER and POS taggingprojection from English to Chinese proved the ef-fectiveness of our methods.In the rest of our paper, Section 2 describes theproposed cross-lingual projection method.
Evalu-ations are in Section 3.
Section 4 gives concludingremarks.2 Proposed Cross-lingual ProjectionMethodsIn this section, we first briefly introduce the cross-lingual projection method based on word align-ments.
Then we describe how the word represen-tations (Brown clusters) were used in the projec-tion method.
Section 2.3 describes the noise re-moving methods.2.1 Projection based on word alignmentsIn this paper we consider cross-lingual projec-tion based on word alignment, because we wantto build projection methods that can be used be-tween language pairs with large differences.
Fig-ure 1 shows the procedure of cross-lingual projec-tion methods, taking projection of NER from En-glish to Chinese as an example.
Here English isthe resource-rich language and Chinese is the tar-get language.
First, sentences from the source sideof the parallel corpus are labeled by an accuratemodel in English (e.g., ?Rongji Zhu?
and ?GanLuo?
were labeled as ?PER?
), since the sourcelanguage has rich resources to build accurate NERmodels.
Then word alignments are generated fromthe parallel corpus and serve as a bridge, so thatunlabeled words in the target language will get thesame labels with words aligning to them in thesource language, e.g.
the first word ??(??)?
?in Chinese gets the projected label ?PER?, since itis aligned to ?Rongji?
and ?Zhu?.
In this way, la-bels in source language sentences are projected tothe target sentences.... ...... ...O inspected??
(O)O have?
(O)O others??
(O)O andPER Yi ?
(O)PER Wu ??
(PER)O ,PER Gan?
(O)PER Luo?(??)?
(PER)O ,PER RongjiPER ZhuFigure 1: An example of projection of NER.
La-bels of Chinese sentence (right) in brackets areprojected from the source sentence.From the projection procedure we can see that alabeled dataset of target language is built based onthe projected labels from source sentences.
Theprojected dataset has a large size, but with a lotof noises.
With this labeled dataset, models of thetarget language can be trained in a supervised way.Then these models can be used to label sentencesin target language.
Since the models are trainedon the target language, this projection approach isless affected by language differences, comparingwith direct projection methods.2.2 Word Representation features forCross-lingual ProjectionOne disadvantage of above method is that the cov-erage of projected labeled data used for training313Words wi,i?
{?2:2}, wi?1/wi,i?
{0,1}Cluster ci,i?
{?2:2}, ci?1/ci,i?
{?1,2}, c?1/c1Transition y?1/y0/{w0, c0, c?1/c1}Table 1: NER features.
ci is the cluster id of wi.target language models are limited by the cover-age of parallel corpora.
For example in Figure 1,some Chinese politicians in 1990?s will be learnedas person names, but some names of recent politi-cians such as ?Obama?, which did not appeared inthe parallel corpus, would not be recognized.To broader the coverage of the projected data,we introduced word representations as features.Same or similar word representations will be as-signed to words appearing in similar contexts,such as person names.
Since word representationsare trained on large-scale unlabeled sentences intarget language, they cover much more words thanthe parallel corpus does.
So the information of aword in projected labeled data will apply to otherwords with the same or similar representations,even if they did not appear in the parallel data.In this work we use Brown clusters as word rep-resentations on target languages.
Brown clusteringassigns words to hierarchical clusters according tothe distributions of words before and after them.Taking NER as an example, the feature templatemay contain features shown in Table 1.
The clusterid of the word to predict (c0) and those of contextwords (ci, i ?
{?2,?1, 1, 2}), as well as the con-junctions of these clusters were used as features inCRF models in the same way the traditional wordfeatures were used.
Since Brown clusters are hi-erarchical, the cluster for each word can be rep-resented as a binary string.
So we also use prefixof cluster IDs as features, in order to compensatefor clusters containing small number of words.
Forlanguages lacking of morphological changes, suchas Chinese, there are no pre/suffix or orthographyfeatures.
However the cluster features are alwaysavailable for any languages.2.3 Noise Removing in Word RepresentationSpaceAnother disadvantage of the projection method isthat the accuracy of projected labels is badly af-fected by non-literate translation and word align-ment errors, making the data contain many noises.For example in Figure 1, the word ???
(Wu Yi)?was not labeled as a named entity since it wasnot aligned to any words in English due to thealignment errors.
A more accurate model will betrained if such noises can be reduced.A direct way to remove the noises is to mod-ify the label of a word to make it consistent withthe majority of labels assigned to the same word inthe parallel corpus.
The method is limited when aword with low frequency has many of its appear-ances incorrectly labeled because of alignment er-rors.
In this situation the noises are impossible toremove according to the word itself.
The error inFigure 1 is an example of this case since the otherfew occurrences of the word ???
(Wu Yi)?
alsohappened to fail to get the correct label.Such difficulties can be easily solved when weturned to the space of Brown clusters, based onthe observation that words in a same cluster tendto have same labels.
For example in Figure 1, theword ???
(Wu Yi)?, ??(??)?
(Zhu Rongji)?and ???
(Luo Gan)?
are in the same cluster, be-cause they are all names of Chinese politiciansand usually appear in similar contexts.
Having ob-served that a large portion of words in this clusterare person names, it is reasonable to modified thelabel of ???
(Wu Yi)?
to ?PER?.The space of clusters is also less sparse so it isalso possible to use combination of the clusters tohelp noise removing, in order to utilize the contextinformation of data instances.
For example, wecould represent a instance as bigram of the clusterof target word and that of the previous word.
Andit is reasonable that its label should be same withother instances with the same cluster bigrams.The whole noise removing method can be rep-resented as following: Suppose a target word wiwas assigned label yi during projection with prob-ability of alignment pi.
From the whole projectedlabeled data, we can get the distribution pw(y) forthe word wi, the distribution pc(y) for its clusterci and the distribution pb(y) for the bigram ci?1ci.We choose y?i = y?, which satisfiesy?
= argmaxy(?y,yipi + ?x?
{w,c,b}px(y)) (1)?y,yi is an indicator function, which is 1 wheny equals to yi.
In practices, we set pw/c/b(y) to 0for the ys that make the probability less than 0.5.With the noise removing method, we can build amore accurate labeled dataset based on the pro-jected data and then use it for training models.3143 Experimental Results3.1 Data PreparationWe took English as resource-rich language andused Chinese to imitate resource-scarce lan-guages, since the two languages differ a lot.
Weconducted experiments on projections of NER andPOS tagging.
The resource-scarce languages wereassumed to have no training data.
For the NERexperiments, we used data from People?s Daily(April.
1998) as test data (55,177 sentences).
Thedata was converted following the style of PennChinese Treebank (CTB) (Xue et al, 2005).
Forevaluation of projection of POS tagging, we usedthe test set of CTB.
Since English and Chinesehave different annotation standards, labels in thetwo languages were converted to the universalPOS tag set (Petrov et al, 2011; Das and Petrov,2011) so that the labels between the source and tar-get languages were consistent.
The universal tagset made the task of POS tagging easier since thefine-grained types are no more cared.The Brown clusters were trained on ChineseWikipedia.
The bodies of all articles are retainedto induce 1000 clusters using the algorithm in(Liang, 2005) .
Stanford word segmentor (Tsenget al, 2005) was used for Chinese word segmenta-tion.
When English Brown clusters were in need,we trained the word clusters on the tokenized En-glish Wikipedia.We chose LDC2003E14 as the parallel corpus,which contains about 200,000 sentences.
GIZA++(Och and Ney, 2000) was used to generate wordalignments.
It is easier to obtain similar amountof parallel sentences between English and minorlanguages, making the conclusions more generalfor problems of projection in real applications.3.2 Performances of NER ProjectionTable 2 shows the performances of NER projec-tion.
We re-implemented the direct projectionmethod with projected clusters in (Ta?ckstro?m etal., 2012).
Although their method was proven towork well on European language pairs, the resultsshowed that projection based on word alignments(WA) worked much better since the source and tar-get languages are from different families.After we add the clusters trained on ChineseWikipedia as features as in Section 2.2, a greatimprovement of about 9 points on the average F1-score of the three entity types was achieved, show-ing that the word representation features help toSystem avgPrecavgRecavgF1Direct projection 47.48 28.12 33.91Proj based on WA 71.6 37.84 47.66+clusters(from en) 63.96 46.59 53.75+clusters(ch wiki) 73.44 47.63 56.60Table 2: Performances of NER projection.recall more named entities in the test set.
The per-formances of all three categories of named entitieswere improved greatly after adding word repre-sentation features.
Larger improvements were ob-served on person names (14.4%).
One of the rea-sons for the improvements is that in Chinese, per-son names are usually single words.
Thus Brown-clustering method can learn good word representa-tions for those entities.
Since in test set, most enti-ties that are not covered are person names, Brownclusters helped to increase the recall greatly.In (Ta?ckstro?m et al, 2012), Brown clusterstrained on the source side were projected to thetarget side based on word alignments.
Rather thanbuilding a same feature space for both the sourcelanguage and the target language as in (Ta?ckstro?met al, 2012), we tried to use the projected clus-ters as features in projection based on word align-ments.
In this way the two methods used exactlythe same resources.
In the experiments, we triedto project clusters trained on English Wikipediato Chinese words.
They improved the perfor-mance by about 6.1% and the result was about20% higher than that achieved by the direct pro-jection method, showing that even using exactlythe same resources, the proposed method out-performed that in (Ta?ckstro?m et al, 2012) muchon diverse language pairs.Next we studied the effects of noise removingmethods.
Firstly, we removed noises according toEq(1), which yielded another huge improvementof about 6% against the best results based on clus-ter features.
Moreover, we conducted experimentsto see the effects of each of the three factors.
Theresults show that both the noise removing methodsbased on words and on clusters achieved improve-ments between 1.5-2 points.
The method based onbigram features got the largest improvement of 3.5points.
It achieved great improvement on personnames.
This is because a great proportion of thevocabulary was made up of person names, some ofwhich are mixed in clusters with common nouns.315While noise removing method based on clustersfailed to recognize them as name entities, clusterbigrams will make use of context information tohelp the discrimination of these mixed clusters.System PER LOC ORG AVGBy Eq(1) 59.77 55.56 72.26 62.53By clusters 49.75 53.10 72.46 58.44By words 49.00 54.69 70.59 58.09By bigrams 58.39 55.01 66.88 60.09Table 3: Performances of noise removing methods3.3 Performances of POS ProjectionIn this section we test our method on projectionof POS tagging from English to Chinese, to showthat our methods can well extend to other NLPtasks.
Unlike named entities, POS tags are asso-ciated with single words.
When one target wordis aligned to more than one words with differentPOS tags on the source side, it is hard to decidewhich POS tag to choose.
So we only retained thedata labeled by 1-to-1 alignments, which also con-tain less noises as pointed out by (Hu et al, 2011).The same feature template as in the experimentsof NER was used for training POS taggers.The results are listed in Table 4.
Because of thegreat differences between English and Chinese,projection based on word alignments worked bet-ter than direct projection did.
After adding wordcluster features and removing noises, an error re-duction of 12.7% was achieved.POS tagging projection can benefit more fromour noise removing methods than NER projectioncould, i.e.
noise removing gave rise to a higherimprovement (2.7%) than that achieved by addingcluster features on baseline system (1.5%).
Onepossible reason is that our noise removing meth-ods assume that labels are associated with singlewords, which is more suitable for POS tagging.Methods AccuracyDirect projection (Ta?ckstro?m) 62.71Projection based on WA 66.68+clusters (ch wiki) 68.23+cluster(ch)&noise removing 70.92Table 4: Performances of POS tagging projection.4 Conclusion and perspectivesIn this paper we introduced Brown clusters oftarget languages to cross-lingual projection andproposed methods for removing noises on pro-jected labels.
Experiments showed that both thetwo techniques could greatly improve the perfor-mances and could help the projection method wellgeneralize to languages differ a lot.Note that although projection methods based onword alignments are less affected by syntactic dif-ferences, the topological differences between lan-guages still remain an importance reason for thelimitation of performances of cross-lingual projec-tion.
In the future we will try to make use of repre-sentations of sub-structures to deal with syntacticdifferences in more complex tasks such as projec-tion of dependency parsing.
Future improvementsalso include combining the direct projection meth-ods based on joint feature representations with theproposed method as well as making use of pro-jected data from multiple languages.AcknowledgmentsWe would like to thank the anonymous review-ers for their valuable comments and helpful sug-gestions.
This work was supported by NationalNatural Science Foundation of China (61173073),and the Key Project of the National High Technol-ogy Research and Development Program of China(2011AA01A207).ReferencesP.F.
Brown, P.V.
Desouza, R.L.
Mercer, V.J.D.
Pietra,and J.C. Lai.
1992.
Class-based n-gram mod-els of natural language.
Computational linguistics,18(4):467?479.D.
Das and S. Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 600?609.P.L.
Hu, M. Yu, J. Li, C.H.
Zhu, and T.J. Zhao.2011.
Semi-supervised learning framework forcross-lingual projection.
In Web Intelligenceand Intelligent Agent Technology (WI-IAT), 2011IEEE/WIC/ACM International Conference on, vol-ume 3, pages 213?216.
IEEE.R.
Hwa, P. Resnik, A. Weinberg, C. Cabezas, andO.
Kolak.
2005.
Bootstrapping parsers via syntacticprojection across parallel texts.
Natural languageengineering, 11(3):311?326.316W.
Jiang and Q. Liu.
2010.
Dependency parsing andprojection based on word-pair classification.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, ACL, volume 10,pages 12?20.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Ph.D. thesis, Massachusetts Institute ofTechnology.R.
McDonald, S. Petrov, and K. Hall.
2011.
Multi-source transfer of delexicalized dependency parsers.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages62?72.
Association for Computational Linguistics.F.J.
Och and H. Ney.
2000.
Giza++: Training of statis-tical translation models.S.
Petrov, D. Das, and R. McDonald.
2011.
Auniversal part-of-speech tagset.
arXiv preprintarXiv:1104.2086.O.
Ta?ckstro?m, R. McDonald, and J. Uszkoreit.
2012.Cross-lingual word clusters for direct transfer of lin-guistic structure.O Ta?ckstro?m, R McDonald, and J Nivre.
2013.
Tar-get language adaptation of discriminative transferparsers.
Proceedings of NAACL-HLT.H.
Tseng, P. Chang, G. Andrew, D. Jurafsky, andC.
Manning.
2005.
A conditional random fieldword segmenter for sighan bakeoff 2005.
In Pro-ceedings of the Fourth SIGHAN Workshop on Chi-nese Language Processing, volume 171.
Jeju Island,Korea.F Xia and W Lewis.
2007.
Multilingual struc-tural projection across interlinear text.
In Proc.
ofthe Conference on Human Language Technologies(HLT/NAACL 2007), pages 452?459.N.
Xue, F. Xia, F.D.
Chiou, and M. Palmer.
2005.
Thepenn chinese treebank: Phrase structure annotationof a large corpus.
Natural Language Engineering,11(2):207.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.Inducing multilingual text analysis tools via robustprojection across aligned corpora.
In Proceedingsof the first international conference on Human lan-guage technology research, pages 1?8.
Associationfor Computational Linguistics.317
