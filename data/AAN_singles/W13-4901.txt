Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 1?11,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsWorking with a small dataset - semi-supervised dependency parsing for IrishTeresa Lynn1,2, Jennifer Foster1, Mark Dras2 and Josef van Genabith11NCLT/CNGL, Dublin City University, Ireland2Department of Computing, Macquarie University, Sydney, Australia1{tlynn,jfoster,josef}@computing.dcu.ie2{teresa.lynn,mark.dras}@mq.edu.au,AbstractWe present a number of semi-supervised pars-ing experiments on the Irish language carriedout using a small seed set of manually parsedtrees and a larger, yet still relatively small, setof unlabelled sentences.
We take two pop-ular dependency parsers ?
one graph-basedand one transition-based ?
and compare re-sults for both.
Results show that using semi-supervised learning in the form of self-trainingand co-training yields only very modest im-provements in parsing accuracy.
We also tryto use morphological information in a targetedway and fail to see any improvements.1 IntroductionDeveloping a data-driven statistical parser relies onthe availability of a parsed corpus for the languagein question.
In the case of Irish, the only parsedcorpus available to date is a dependency treebank,which is currently under development and still rel-atively small, with only 803 gold-annotated trees(Lynn et al 2012a).
As treebank development isa labour- and time-intensive process, in this studywe evaluate various approaches to bootstrapping astatistical parser with a set of unlabelled sentencesto ascertain how accurate parsing output can beat this time.
We carry out a number of differ-ent semi-supervised bootstrapping experiments us-ing self-training, co-training and sample-selection-based co-training.
Our studies differ from previoussimilar experiments as our data is taken from a work-in-progress treebank.
Thus, aside from the currentsmall treebank which is used for training the initialseed model and for testing, there is no additionalgold-labelled data available to us to directly com-pare supervised and semi-supervised approaches us-ing training sets of comparable sizes.In the last decade, data-driven dependency pars-ing has come to fore, with two main approachesdominating ?
transition-based and graph-based.
Inclassic transition-based dependency parsing, thetraining phase consists of learning the correct parseraction to take given the input string and the parsehistory, and the parsing phase consists of the greedyapplication of parser actions as dictated by thelearned model.
In contrast, graph-based depen-dency parsing involves the non-deterministic con-struction of a parse tree by predicting the maximum-spanning-tree in the digraph for the input sentence.In our study, we employ Malt (Nivre et al 2006),a transition-based dependency parsing system, andMate (Bohnet, 2010), a graph-based parser.In line with similar experiments carried out onEnglish (Steedman et al 2003), we find that co-training is more effective than self-training.
Co-training Malt on the output of Mate proves to be themost effective method for improving Malt?s perfor-mance on the limited data available for Irish.
Yet, theimprovement is relatively small (0.6% over the base-line for LAS, 0.3% for UAS) for the best co-trainedmodel.
The best Mate results are achieved through anon-iterative agreement-based co-training approach,in which Mate is trained on trees produced by Maltwhich exhibit a minimum agreement of 85% withMate (LAS increase of 1.2% and UAS of 1.4%).The semi-supervised parsing experiments do notexplicitly take into account the morphosyntacticproperties of the Irish language.
In order to examinethe effect of this type of information during parsing,we carry out some orthogonal experiments where we1reduce word forms to lemmas and introduce mor-phological features in certain cases.
These changesdo not bring about an increase in parsing accuracy.The paper is organised as follows.
Section 2 isan overview of Irish morphology.
In Section 3 ourprevious work carried out on the development of anIrish dependency treebank is discussed followed inSection 4 by a description of some of our prior pars-ing results.
Section 5 describes the self-training, co-training and sample-selection-based co-training ex-periments, Section 6 presents the preliminary pars-ing experiments involving morphological features,and, finally, Section 7 discusses our future work.2 Irish as a morphologically rich languageIrish is a Celtic language of the Indo-European lan-guage family.
It has a VSO word order and is rich inmorphology.
The following provides an overview ofthe type of morphology present in the Irish language.It is not a comprehensive summary as the rules gov-erning morphological changes are too extensive andat times too complex to document here.Inflection in Irish mainly occurs through suffixa-tion, but initial mutation through lenition and eclip-sis is also common (Christian-Brothers, 1988).
Aprominent feature of Irish (also of Scottish andManx), which influences inflection, is the existenceof two sets of consonants, referred to as ?broad?
and?slender?
consonants (O?
Siadhail, 1989).
Conso-nants can be slenderised by accompanying the con-sonant with a slender vowel, either e or i. Broaden-ing occurs through the use of broad vowels; a, o oru.
For example, buail ?to hit?
becomes ag bualadh?hitting?
in the verbal noun form.
In general, thereneeds to be vowel harmony (slender or broad) be-tween stem endings and the initial vowel in a suffix.A process known as syncopation also occurswhen words with more than one syllable have avowel-initial suffix added.
For example imir ?toplay?
inflects as imr?
?m ?I play?.Nouns While Old Irish employed several gram-matical cases, Modern Irish uses only three: Nomi-native, Genitive and Vocative.
The nominative formis sometimes regarded as the ?common form?
as it isnow also used to account for accusative and dativeforms.
Nouns in Irish are divided into five classes, ordeclensions, depending on the manner in which thegenitive case is formed.
In addition, there are twogrammatical genders in Irish - masculine and fem-inine.
Case, declension and gender are expressedthrough noun inflection.
For example, pa?ipe?ar ?pa-per?
is a masculine noun in the first declension.
Bothlenition and slenderisation are used to form the geni-tive singular form: pha?ipe?ir.
In addition, possessiveadjectives cause noun inflection through lenition,eclipsis and prefixation.
For example, teach ?house?,mo theach ?my house?, a?r dteach ?our house?
; ainm?name?, a hainm ?her name?.Verbs Verbs can incorporate their subject, inflect-ing for person and number through suffixation.
Suchforms are referred to as synthetic verb forms.
Inaddition, verb tense is often indicated through var-ious combinations of initial mutation, syncopationand suffixation.
For example, scr?
?obh ?write?
can in-flect as scr?
?obhaim ?I write?.
The past tense of theverb tug ?give?
is thug ?gave?.
Lenition occurs af-ter the negative particle n??.
For example, tugaim ?Igive?
; n??
thugaim ?I do not give?
; n?
?or thug me?
?Idid not give?.
Eclipsis occurs following clitics suchas interrogative particles (an, nach); complementis-ers (go, nach); and relativisers (a, nach) (Stenson,1981).
For example, an dtugann se??
?does he give??
;nach dtugann se??
?does he not give?
?.Adjectives In general, adjectives follow nouns andagree in number, gender and case.
Depending onthe noun they modify, adjectives can also inflect.Christian-Brothers (1988) note eight declensions ofadjectives.
They can decline for genitive singularmasculine, genitive singular feminine and nomina-tive plural.
For example, bacach ?lame?
inflects asbacaigh (Gen.Sg.Masc), baca??
(Gen.Fem.Sg) andbacacha (Nom.PL).
Comparative adjectives are alsoformed through inflection.
For example, la?idir?strong?, n?
?os la?idre ?stronger?
; de?anach ?late?, isde?ana??
?latest?.Prepositions Irish has simple and compoundprepositions.
Most of the simple prepositions caninflect for person and number (known as preposi-tional pronouns or pronominal prepositions), thusincluding a nominal element.
For example, com-pare bh??
se?
ag labhairt le fear ?he was speakingwith a man?
with bh??
se?
ag labhairt leis ?he wasspeaking with him?.
These forms are used quite fre-2quently, not only with regular prepositional attach-ment where pronominal prepositions operate as ar-guments of verbs or modifiers of nouns and verbs,but also in idiomatic use where they express emo-tions and states, e.g.
ta?
bro?n orm (lit.
?be-worry-on me?)
?I am worried?
or ta?
su?il agam (lit.
?be-expectation-with me?)
?I hope?.
Noted by Greene(1966) as a noun-centered language, nouns are of-ten used to convey the meaning that verbs oftenwould.
Pronominal prepositions are often used inthese types of structures.
For example, bhain me?geit aisti (lit.
extracted-I-shock-from her) ?I fright-ened her?
; bhain me?
mo cho?ta d?
?om (lit.
extracted-I-coat-from me) ?I took off my coat?
; bhain me?
u?sa?idas (lit.
extracted-I-use-from it) ?I used it?
; bhainme?
triail astu (lit.
extracted-I-attempt-from them)?Itried them?.Derivational morphology There are also someinstances of derivational morphology in Irish.
U?
?Dhonnchadha (2009) notes that all verb stems andagentive nouns can inflect to become verbal nouns.Verbal adjectives are also derived from verb stemsthrough suffixation.
For example, the verb du?n?close?
undergoes suffixation to become du?nadh?closing?
(verbal noun) and du?nta ?closed?
(verbaladjective).
An emphatic suffix -sa/-se (both broadand slender form) can attach to nouns or pronouns.It can also be attached to any verb that has been in-flected for person and number and also to pronom-inal prepositions.
For example mo thuairim ?myopinion?
?mo thuairimse ?my opinion; tu?
?you?(sg)?
tusa ?you?
; cloisim ?I hear??
cloisimse ?I hear?
;liom ?with me??
liomsa ?with me?.
In addition, thediminutive suffix -?
?n can attach to all nouns to forma derived diminutive form.
The rules of slenderisa-tion apply here also.
For example, buachaill ?boy?becomes buachaill?
?n ?little boy?, and tamall ?while?becomes tamaill?
?n ?short while?.3 The Irish Dependency TreebankIrish is the official language of Ireland, yet Englishis the primary language for everyday use.
Irish istherefore considered an EU minority language andis lacking in linguistic resources that can be used todevelop NLP applications (Judge et al 2012).Recently, in efforts to address this issue, we havebegun work on the development of a dependencytreebank for Irish (Lynn et al 2012a).
The treebankhas been built upon a gold standard 3,000 sentencePOS-tagged corpus1 developed by U??
Dhonnchadha(2009).
Our labelling scheme is based on an ?LFG-inspired?
dependency scheme developed for Englishby C?etinog?lu et al(2010).
This scheme was adoptedwith the aim of identifying functional roles whileat the same time circumventing outstanding, unre-solved issues in Irish theoretical syntax.2 The Irishlabelling scheme has 47 dependency labels in the la-bel set.
The treebank is in the CoNLL format withthe following fields: ID, FORM, LEMMA, CPOSTAG,POSTAG, HEAD and DEPREL.
The coarse-grainedpart of speech of a word is marked by the la-bel CPOSTAG, and POSTAG marks the fine-grainedpart of speech for that word.
For example, prepo-sitions are tagged with the CPOSTAG Prep andone of the following POSTAGs: Simple: ar ?on?,Compound: i ndiaidh ?after?, Possessive: ina?in its?, Article: sa ?in the?.At an earlier stage of the treebank?s develop-ment, we carried out on an inter-annotator agree-ment (IAA) study.
The study involved four stages.
(i) The first experiment (IAA-1) involved the as-sessment of annotator agreement following the in-troduction of a second annotator.
The results re-ported a Kappa score of 0.79, LAS of 74.4% andUAS of 85.2% (Lynn et al 2012a).
(ii) We thenheld three workshops that involved thorough anal-ysis of the output of IAA-1, highlighting disagree-ments between annotators, gaps in the annotationguide, shortcomings of the labelling scheme and lin-guistic issues not yet addressed.
(iii) The annotationguide, labelling scheme and treebank were updatedaccordingly, addressing the highlighted issues.
(iv)Finally, a second inter-annotator agreement exper-iment (IAA-2) was carried out presenting a Kappascore of 0.85, LAS of 79.2% and UAS of 87.8%(Lynn et al 2012b).We found that the IAA study was valuable in thedevelopment of the treebank, as it resulted in im-1A tagged, randomised subset of the NCII, (New Corpus forIreland - Irish http://corpas.focloir.ie/), comprised of text frombooks, news data, websites, periodicals, official and governmentdocuments.2For example there are disagreements over the existence ofa VP in Irish and whether the language has a VSO or an under-lying SVO structure.3provement of the quality of the labelling scheme,the annotation guide and the linguistic analysis ofthe Irish language.
Our updated labelling schemeis now hierarchical, allowing for a choice betweenworking with fine-grained or coarse-grained labels.The scheme has now been finalised.
A full list ofthe labels can be found in Lynn et al(2012b).
Thetreebank currently contains 803 gold-standard trees.4 Preliminary Parsing ExperimentsIn our previous work (Lynn et al 2012a), we car-ried out some preliminary parsing experiments withMaltParser and 10-fold cross-validation using 300gold-standard trees.
We started out with the fea-ture template used by C?etinog?lu et al(2010) and ex-amined the effect of omitting LEMMA, WORDFORM,POSTAG and CPOSTAG features and combinationsof these, concluding that it was best to include allfour types of information.
Our final LAS and UASscores were 63.3% and 73.1% respectively.
Follow-ing the changes we made to the labelling schemeas a result of the second IAA study (describedabove), we re-ran the same parsing experiments onthe newly updated seed set of 300 sentences - theLAS increased to 66.5% and the UAS to 76.3%(Lynn et al 2012b).In order to speed up the treebank creation, we alsoapplied an active learning approach to bootstrappingthe annotation process.
This work is also reported inLynn et al(2012b).
The process involved training aMaltParser model on a small subset of the treebankdata, and iteratively, parsing a new set of sentences,selecting a 50-sentence subset to hand-correct, andadding these new gold sentences to the training set.We compared a passive setup, in which the parsesthat were selected for correction were chosen at ran-dom, to an active setup, in which the parses thatwere selected for correction were chosen based onthe level of disagreement between two parsers (Maltand Mate).
The active approach to annotation re-sulted in superior parsing results to the passive ap-proach (67.2% versus 68.1% LAS) but the differ-ence was not statistically significant.5 Semi-Supervised Parsing ExperimentsIn order to alleviate data sparsity issues broughtabout by our lack of training material, we experi-ment with automatically expanding our training setusing well known semi-supervised techniques.5.1 Self-Training5.1.1 Related WorkSelf-training, the process of training a system onits own output, has a long and chequered history inparsing.
Early experiments by Charniak (1997) con-cluded that self-training is ineffective because mis-takes made by the parser are magnified rather thansmoothed during the self-training process.
The self-training experiments of Steedman et al(2003) alsoyielded disappointing results.
Reichart and Rap-paport (2007) found, on the other hand, that self-training could be effective if the seed training setwas very small.
McClosky et al(2006) also re-port positive results from self-training, but the self-training protocol that they use cannot be consideredto be pure self-training as the first-stage Charniakparser (Charniak, 2000) is retrained on the output ofthe two-stage parser (Charniak and Johnson, 2005)They later show that the extra information broughtby the discriminative reranking phase is a factorin the success of their procedure (McClosky et al2008).
Sagae (2010) reports positive self-training re-sults even without the reranking phase in a domainadaptation scenario, as do Huang and Harper (2009)who employ self-training with a PCFG-LA parser.5.1.2 Experimental SetupThe labelled data available to us for this experi-ment comprises the 803 gold standard trees referredto in Section 3.
This small treebank includes the150-tree development set and 150-tree test set usedin experiments by Lynn et al(2012b).
We use thesame development and test sets for this study.
Asfor the remaining 503 trees, we remove any treesthat have more than 200 tokens.
The motivation forthis is two-fold: (i) we had difficulties training Mateparser with long sentences due to memory resourceissues, and (ii) in keeping with the findings of Lynnet al(2012b), the large trees were sentences fromlegislative text that were difficult to analyse for au-tomatic parsers and human annotators.
This leavesus with 500 gold-standard trees as our seed trainingdata set.For our unlabelled data, we take the next 1945sentences from the gold standard 3,000-sentence4A is a parser.M iA is a model of A at step i.P iA is a set of trees produced using MiA.U is a set of sentences.U i is a subset of U at step i.L is the manually labelled seed training set.LiA is labelled training data for A at step i.Initialise:L0A ?
L.M0A ?
Train(A,L0A)for i = 1?
N doU i ?
Add set of unlabelled sentences from U .P iA ?
Parse(Ui , M iA)Li+1A ?
LiA + PiAM i+1A ?
Train(A,Li+1A )end forFigure 1: Self-training algorithmPOS-tagged corpus referred to in Section 3.
Whenwe remove sentences with more than 200 tokens, weare left with 1938 sentences in our unlabelled set.The main algorithm for self-training is given inFigure 1.
We carry out two separate experimentsusing this algorithm.
In the first experiment we useMalt.
In the second experiment, we substitute Matefor Malt.3The steps are as follows: Initialisation involvestraining the parser on a labelled seed set of 500 goldstandard trees (L0A), resulting in a baseline parsingmodel: M iA.
We divide the set of gold POS-taggedsentences (U ) into 6 sets, each containing 323 sen-tences U i.
For each of the six iterations in this ex-periment i = [1?6], we parse U i.
Each time, the setof newly parsed sentences (PA) is added to the train-ing set LiA to make a larger training set of Li+1A .
Anew parsing model (M i+1A ) is then induced by train-ing with the new training set.5.1.3 ResultsThe results of our self-training experiments arepresented in Figure 2.
The best Malt model wastrained on 2115 trees, at the 5th iteration (70.2%LAS).
UAS scores did not increase over the baseline(79.1%).
The improvement in LAS over the baselineis not statistically significant.
The best Mate modelwas trained on 1792 trees, at the 4th iteration (71.2%3Versions used: Maltparser v1.7 (stacklazy parsing algo-rithm); Mate tools v3.3 (graph-based parser)Figure 2: Self-Training Results on the Development SetLAS, 79.2% UAS).
The improvement over the base-line is not statistically significant.5.2 Co-Training5.2.1 Related WorkCo-training involves training a system on the out-put of a different system.
Co-training has foundmore success in parsing than self-training, and itis not difficult to see why this might be the caseas it can be viewed as a method for combining thebenefits of individual parsing systems.
Steedmanet al(2003) directly compare co-training and self-training and find that co-training outperforms self-training.
Sagae and Tsujii (2007) successfully em-ploy co-training in the domain adaption track of theCoNLL 2007 shared task on dependency parsing.5.2.2 Experimental SetupIn this and all subsequent experiments, we useboth the same training data and unlabelled data thatwe refer to in Section 5.1.2.Our co-training algorithm is given in Figure 3 andit is the same as the algorithm provided by Steedmanet al(2003).
Again, our experiments are carried outusing Malt and Mate.
This time, the experiments arerun concurrently as each parser is bootstrapped fromthe other parser?s output.5A and B are two different parsers.M iA and MiB are models of A and B at step i.P iA and PiB are a sets of trees produced using MiA and MiB .U is a set of sentences.U i is a subset of U at step i.L is the manually labelled seed training set.LiA and LiB are labelled training data for A and B at step i.Initialise:L0A ?
L0B ?
L.M0A ?
Train(A,L0A)M0B ?
Train(B,L0B)for i = 1?
N doU i ?
Add set of unlabelled sentences from U .P iA ?
Parse(Ui , M iA)P iB ?
Parse(Ui , M iB)Li+1A ?
LiA + PiBLi+1B ?
LiB + PiAM i+1A ?
Train(A,Li+1A )M i+1B ?
Train(B,Li+1B )end forFigure 3: Co-training algorithmThe steps are as follows: Initialisation involvestraining both parsers on a labelled seed set of 500gold standard trees (L0A and L0B), resulting in twoseparate baseline parsing models: M iA (Malt) andM iB (Mate).
We divide the set of gold POS-taggedsentences (U ) into 6 sets, each containing 323 sen-tences U i.
For each of the six iterations in this ex-periment i = [1?
6], we use Malt and Mate to parseU i.
This time, the set of newly parsed sentences P iB(Mate output) is added to the training set LiA to makea larger training set of Li+1A (Malt training set).
Con-versely, the set of newly parsed sentences P iA (Maltoutput) is added to the training set LiB to make alarger training set of Li+1B (Mate training set).
Twonew parsing models (M i+1A and Mi+1B ) are then in-duced by training Malt and Mate respectively withtheir new training sets.5.2.3 ResultsThe results of our co-training experiment are pre-sented in Figure 4.
The best Malt model was trainedon 2438 trees, at the final iteration (71.0% LASand 79.8% UAS).
The improvement in UAS overthe baseline is statistically significant.
Mate?s bestmodel was trained on 823 trees on the second iter-ation (71.4% LAS and 79.9% UAS).
The improve-ment over the baseline is not statistically significant.Figure 4: Co-Training Results on the Development Set5.3 Sample-Selection-Based Co-Training5.3.1 Related WorkSample selection involves choosing training itemsfor use in a particular task based on some criteriawhich approximates their accuracy in the absence ofa label or reference.
In the context of parsing, Re-hbein (2011) chooses additional sentences to add tothe parser?s training set based on their similarity tothe existing training set ?
the idea here is that sen-tences that are similar to training data are likely tohave been parsed properly and so are ?safe?
to addto the training set.
In their parser co-training experi-ments, Steedman et al(2003) sample training itemsbased on the confidence of the individual parsers (asapproximated by parse probability).In Active Learning research, the Query By Com-mittee selection method (Seung et al 1992) is usedto choose items for annotation ?
if a committee oftwo or more systems disagrees on an item, this is ev-idence that the item needs to be prioritised for man-ual correction (see for example Lynn et al(2012b)).Steedman et al(2003) discuss a sample selectionapproach based on differences between parsers ?
ifparser A and parser B disagree on an analysis, parserA can be improved by being retrained on parser B?sanalysis, and vice versa.
In contrast, Ravi et al(2008) show that parser agreement is a strong in-6dicator of parse quality, and in parser domain adap-tation, Sagae and Tsujii (2007) and Le Roux et al(2012) use agreement between parsers to choosewhich automatically parsed target domain items toadd to the training set.Sample selection can be used with both self-training and co-training.
We restrict our attentionto co-training since our previous experiments havedemonstrated that it has more potential than self-training.
In the following set of experiments, we ex-plore the role of both parser agreement and parserdisagreement in sample selection in co-training.5.3.2 Agreement-Based Co-TrainingExperimental Setup The main algorithm foragreement-based co-training is given in Figure 5.Again, Malt and Mate are used.
However, this algo-rithm differs from the co-training algorithm in Fig-ure 3 in that rather than adding the full set of 323newly parsed trees (P iA and PiB) to the training setat each iteration, selected subsets of these trees (P iA?and P iB?)
are added instead.
To define these subsets,we identify the trees that have 85% or higher agree-ment between the two parser output sets.
As a re-sult, the number of trees in the subsets differ at eachiteration.
For iteration 1, 89 trees reach the agree-ment threshold; iteration 2, 93 trees; iteration 3, 117trees; iteration 4, 122 trees; iteration 5, 131 trees;iteration 6, 114 trees.
The number of trees in thetraining sets is much smaller compared with thosein the experiments of Section 5.2.Results The results for agreement-based co-training are presented in Figure 6.
Malt?s bestmodel was trained on 1166 trees at the final iteration(71.0% LAS and 79.8% UAS).
Mate?s best modelwas trained on 1052 trees at the 5th iteration (71.5%LAS and 79.7% UAS).
Neither result represents astatistically significant improvement over the base-line.5.3.3 Disagreement-based Co-TrainingExperimental Setup This experiment uses thesame sample selection algorithm we used foragreement-based co-training (Figure 5).
For this ex-periment, however, the way in which the subsetsof trees (P iA?
and PiB?)
are selected differs.
Thistime we choose the trees that have 70% or higherdisagreement between the two parser output sets.A and B are two different parsers.M iA and MiB are models of A and B at step i.P iA and PiB are a sets of trees produced using MiA and MiB .U is a set of sentences.U i is a subset of U at step i.L is the manually labelled seed training set.LiA and LiB are labelled training data for A and B at step i.Initialise:L0A ?
L0B ?
L.M0A ?
Train(A,L0A)M0B ?
Train(B,L0B)for i = 1?
N doU i ?
Add set of unlabelled sentences from U .P iA ?
Parse(Ui , M iA)P iB ?
Parse(Ui , M iB)P iA?
?
a subset of X trees from PiAP iB ?
?
a subset of X trees from PiBLi+1A ?
LiA + PiB ?Li+1B ?
LiB + PiA?M i+1A ?
Train(A,Li+1A )M i+1B ?
Train(B,Li+1B )end forFigure 5: Sample selection Co-training algorithmAgain, the number of trees in the subsets differ ateach iteration.
For iteration 1, 91 trees reach the dis-agreement threshold; iteration 2, 93 trees; iteration3, 73 trees; iteration 4, 74 trees; iteration 5, 68 trees;iteration 6, 71 trees.Results The results for our disagreement-basedco-training experiment are shown in Figure 7.
Thebest Malt model was trained with 831 trees at the4th iteration (70.8% LAS and 79.8% UAS).
Mate?sbest models were trained on (i) 684 trees on the 2nditeration (71.0% LAS) and (ii) 899 trees on the 5thiteration (79.4% UAS).
Neither improvement overthe baseline is statistically significant.5.3.4 Non-Iterative Agreement-basedCo-TrainingIn this section, we explore what happens whenwe add the additional training data at once ratherthan over several iterations.
Rather than testing thisidea with all our previous setups, we choose sample-selection-based co-training where agreement be-tween parsers is the criterion for selecting additionaltraining data.Experimental Setup Again, we also follow thealgorithm for agreement-based co-training as pre-sented in Figure 5.
However, two different ap-7Figure 6: Agreement-based Co-Training Results on theDevelopment Setproaches are taken this time, involving only one it-eration in each.
For the first experiment (ACT1a),the subsets of trees (P iA?
and PiB?)
that are added tothe training data are chosen based on an agreementthreshold of 85% between parsers, and are takenfrom the full set of unlabelled data (where U i = U ),comprising 1938 trees.
In this instance, the subsetconsists of 603 trees, making a final training set of1103 trees.For the second experiment (ACT1b), only treesmeeting a parser agreement threshold of 100% areadded to the training data.
253 trees (P iA?
and PiB?
)out of 1938 trees (U i = U ) meet this threshold.
Thefinal training set consists of 753 trees.Results ACT1a proved to be the most accurateparsing model for Mate overall.
The addition of603 trees that met the agreement threshold of 85%increased the LAS and UAS scores over the base-line by 1.0% and 1.3% to 71.8 and 80.4 respec-tively.
This improvement is statistically significant.Malt showed a LAS improvement of 0.93% anda UAS improvement of 0.42% (71.0% LAS and79.6% UAS).
The LAS improvement over the base-line is statistically significant.The increases for ACT1b, where 100% agreementtrees are added, are less pronounced and are not sta-Figure 7: Disagreement-based Co-Training Results onthe Development Settistically significant.
Results showed a 0.5% LASand 0.2% UAS increase over the baseline with Malt,based on the 100% agreement threshold (adding 235trees).
Mate performs at 0.5% above the LAS base-line and 0.1% above the UAS baseline.5.4 AnalysisWe perform an error analysis for the Malt and Matebaseline, self-trained and co-trained models on thedevelopment set.
We observe the following trends:?
All Malt and Mate parsing models confuse thesubj and obj labels.
A few possible rea-sons for this stand out: (i) It is difficult forthe parser to discriminate between analytic verbforms and synthetic verb forms.
For example,in the phrase pho?sfainn thusa ?I would marryyou?, pho?sfainn is a synthetic form of the verbpo?s ?marry?
that has been inflected with the in-corporated pronoun ?I?.
Not recognising this,the parser decided that it is an intransitive verb,taking ?thusa?, the emphatic form of the pro-noun tu?
?you?, as its subject instead of object.
(ii) Possibly due to a VSO word order, whenthe parser is dealing with relative phrases, itcan be difficult to ascertain whether the follow-ing noun is the subject or object.
For example,an chail?
?n a chonaic me?
inne?
?the girl whom8I saw yesterday/ the girl who saw me yester-day?.4 (iii) There is no passive verb form inIrish.
The autonomous form is most closelylinked with passive use and is used when theagent is not known or mentioned.
A ?hidden?or understood subject is incorporated into theverbform.
Casadh eochair i nglas ?a key wasturned in a lock?
(lit.
somebody turned a keyin a lock).
In this sentence, eochair ?key?
is theobject.?
For both parsers, there is some confusion be-tween the labelling of obl and padjunct,both of which mark the attachment betweenverbs and prepositions.
Overall, Malt?s con-fusion decreases over the 6 iterations of self-training, but Mate begins to incorrectly choosepadjunct over obl instead.
Mixed resultsare obtained using the various variants of co-training.?
Mate handles coordination better than Malt.5 Itis not surprising then that co-training Malt us-ing Mate parses improves Malt?s coordinationhandling whereas the opposite is the case whenco-training Mate on Malt parses, demonstrat-ing that co-training can both eliminate and in-troduce errors.?
Other examples of how Mate helps Malt duringco-training is in the distinction between topand comp relations, between vparticleand relparticle, and in the analysis ofxcomps.?
Distinguishing between relative and cleft par-ticles is a frequent error for Mate, and there-fore Malt also begins to make this kind of errorwhen co-trained using Mate.
Mate improvesusing sample-selection-based co-training withMalt.?
The sample-selection-based co-training vari-ants show broadly similar trends to the basicco-training.4Naturally ambiguous Irish sentences like this require con-text for disambiguation.5Nivre and McDonald (2007) make a similar observationwhen they compare the errors made by graph and transitionbased dependency parsers.Parsing Models LAS UASDevelopment SetMalt Baseline: 70.0 79.1Malt Best (co-train) : 71.0 80.2Mate Baseline: 70.8 79.1Mate Best (85% threshold ACT1a): 71.8 80.4Test SetMalt Baseline: 70.2 79.5Malt Best (co-train) : 70.8 79.8Mate Baseline: 71.9 80.1Mate Best (85% threshold ACT1a): 73.1 81.5Table 1: Results for best performing models5.5 Test Set ResultsThe best performing parsing model for Malt onthe development set is in the final iteration of thebasic co-training approach in Section 5.2.
Thebest performing parsing model for Mate on the de-velopment set is the non-iterative 85% thresholdagreement-based co-training approach described inSection 5.3.4.
The test set results for these opti-mal development set configurations are also shownin Table 1.
The baseline model for Malt obtainsa LAS of 70.2%, the final co-training iteration aLAS of 70.8%.
The baseline model for Mate ob-tains a LAS of 71.9%, and the non-iterative 85%agreement-based co-trained model obtains a LAS of73.1%.6 Parsing Experiments UsingMorphological FeaturesAs well as the size of the dataset, data sparsity isalso confounded by the number of possible inflectedforms for a given root form.
With this in mind,and following on from the discussion in Section 5.4,we carry out further parsing experiments in an at-tempt to make better use of morphological informa-tion during parsing.
We attack this in two ways: byreducing certain words to their lemmas and by in-cluding morphological information in the optionalFEATS (features) field.
The reasoning behind re-ducing certain word forms to lemmas is to furtherreduce the differences between inflected forms ofthe same word, and the reasoning behind includingmorphological information is to make more explicitthe similarity between two different word forms in-flected in the same way.
All experiments are car-9Parsing Models (Malt) LAS UASBaseline: 70.0 79.1Lemma (Pron Prep): 69.7 78.9Lemma + Pron Prep Morph Features: 69.6 78.9Form + Pron Prep Morph Features: 69.8 79.1Verb Morph Features: 70.0 79.1Table 2: Results with morphological features on the de-velopment setried out with MaltParser and our seed training setof 500 gold trees.
We focus on two phenomena:prepositional pronouns or pronominal prepositions(see Section 2) and verbs with incorporated subjects(see Section 2 and Section 5.4).In the first experiment, we include extra mor-phological information for pronominal prepositions.We ran three parsing experiments: (i) replacing thevalue of the surface form (FORM) of pronominalprepositions with their lemma form (LEMMA), forexample agam?ag, (ii) including morphological in-formation for pronominal prepositions in the FEATScolumn.
For example, in the case of agam ?at me?,we include Per=1P|Num=Sg, (iii) we combineboth approaches of reverting to lemma form and alsoincluding the morphological features.
The resultsare given in Table 2.In the second experiment, we include morpholog-ical features for verbs with incorporated subjects:imperative verb forms, synthetic verb forms and au-tonomous verb forms such as those outlined in Sec-tion 5.4.
For each instance of these verb types, weincluded incorpSubj=true in the FEATS col-umn.
The results are also given in Table 2.The experiments on the pronominal prepositionsshow a drop in parsing accuracy while the experi-ments carried out using verb morphological infor-mation showed no change in parsing accuracy.6 Inthe case of inflected prepositions, perhaps we havenot seen any improvement because we have not fo-cused on a phenomenon which is critical for parsing.More experimentation is necessary.7 Concluding RemarksWe have presented two sets of experiments whichaim to improve dependency parsing performance for6Although the total number of correct attachments are thesame, the parser output is different.a minority language with a very small treebank.
Inthe first set of experiments, the main focus of the pa-per, we tried to overcome the limited treebank sizeby increasing the parsers?
training sets using auto-matically parsed sentences.
While we do manageto achieve statistically significant improvements insome settings, it is clear from the results that thegains in parser accuracy through semi-supervisedbootstrapping methods are fairly modest.
Yet, in theabsence of more gold labelled data, it is difficult toknow now whether we would achieve similar or im-proved results by adding the same amount of goldtraining data.
This type of analysis will be interest-ing at a later date when the unlabelled trees used inthese experiments are eventually annotated and cor-rected manually.The second set of experiments tries to mitigatesome of the data sparseness issues by exploitingmorphological characteristics of the language.
Un-fortunately, we do not see any improvements but wemay get different results if we repeat these experi-ments using the larger semi-supervised training setsfrom the first set of experiments.There are many directions this parsing researchcould take us in the future.
Our unlabelled data con-sisted of sentences annotated with gold POS tags.In the future we would like to take advantage ofthe fully unlabelled, untagged data in the New Cor-pus for Ireland ?
Irish, which consists of 30 millionwords.
We would also like to experiment with a fullyunsupervised parser using this dataset.
Our Malt fea-ture models are manually optimised ?
it would be in-teresting to experiment with optimising them usingMaltOptimizer (Ballesteros, 2012).
An additionalavenue of research would be to exploit the hierar-chical nature of the dependency scheme to arrive atmore flexible way of measuring agreement or dis-agreement in sample selection.AcknowledgementsWe thank the three anonymous reviewers for theirhelpful feedback.
This work is supported by Sci-ence Foundation Ireland (Grant No.
07/CE/I1142)as part of the Centre for Next Generation Localisa-tion (www.cngl.ie) at Dublin City University.10ReferencesMiguel Ballesteros.
2012.
Maltoptimizer: A sys-tem for maltparser optimization.
In Proceedings ofthe Eighth International Conference on Linguistic Re-sources and Evaluation (LREC), pages 2757?2763, Is-tanbul, Turkey.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of COL-ING.O?zlem C?etinog?lu, Jennifer Foster, Joakim Nivre, DeirdreHogan, Aoife Cahill, and Josef van Genabith.
2010.LFG without c-structures.
In Proceedings of the 9thInternational Workshop on Treebanks and LinguisticTheories.Eugene Charniak and Mark Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd ACL.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of AAAI.Eugene Charniak.
2000.
A maximum entropy inspiredparser.
In Proceedings of the First Annual Meetingof the North American Chapter of the Association forComputational Linguistics (NAACL-00).Christian-Brothers.
1988.
New Irish Grammar.
Dublin:C J Fallon.David Greene.
1966.
The Irish Language.
Dublin: TheThree Candles.Zhongqiang Huang and Mary Harper.
2009.
Self-training PCFG grammars with latent annotationsacross languages.
In Proceedings of EMNLP.John Judge, Ailbhe N??
Chasaide, Rose N??
Dhubhda,Kevin P. Scannell, and Elaine U??
Dhonnchadha.
2012.The Irish Language in the Digital Age.
Springer Pub-lishing Company, Incorporated.Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-soul Samed Zadeh Kaljahi, and Anton Bryl.
2012.DCU-Paris13 systems for the sancl 2012 shared task.In Working Notes of SANCL.Teresa Lynn, O?zlem C?etinog?lu, Jennifer Foster, Elaine U?
?Dhonnchadha, Mark Dras, and Josef van Genabith.2012a.
Irish treebanking and parsing.
In Proceedingsof the Eight International Conference on LanguageResources and Evaluation, pages 1939?1946.Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U??Dhonnchadha.
2012b.
Active learning and the Irishtreebank.
In Proceeedings of the Australasian Lan-guage Technology Workshop (ALTA), pages 23?32.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Main Conference, pages 152?159, NewYork City, USA, June.
Association for ComputationalLinguistics.David McClosky, Eugene Charniak, and Mark Johnson.2008.
When is self-training effective for parsing?
InProceedings of COLING.Joakim Nivre and Ryan McDonald.
2007.
Characteriz-ing the errors of data-driven dependency parsing mod-els.
In Proceedings of EMNLP-CoNLL, Prague, CzechRepublic.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of the Fifth InternationalConference on Language Resources and Evaluation(LREC2006).M?
?chea?l O?
Siadhail.
1989.
Modern Irish: Grammaticalstructure and dialectal variation.
Cambridge: Cam-bridge University Press.Sujith Ravi, Kevin Knight, and Radu Soricut.
2008.
Au-tomatic prediction of parser accuracy.
In Proceedingsof EMNLP, Hawaii.Ines Rehbein.
2011.
Data point selection for self-training.
In Proceedings of the Second Workshopon Statistical Parsing of Morphologically Rich Lan-guages (SPMRL 2011), Dublin, Ireland.Roi Reichart and Ari Rappaport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In Proceedings ofACL.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with LR models and parserensembles.
In Proceedings of the CoNLL shared tasksession of EMNLP-CoNLL.Kenji Sagae.
2010.
Self-training without reranking forparser domain adapation and its impact on semanticrole labelling.
In Proceedings of the ACL Workshopon Domain Adaptation for NLP.Sebastian Seung, Manfred Opper, and Haim Sompolin-sky.
1992.
Query by committee.
In Proceedingsof the Fifth Annual ACM Workshop on ComputationalLearning Theory.Mark Steedman, Miles Osborne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrap-ping statistical parsers from small datasets.
In Pro-ceedings of the tenth conference on European chapterof the Association for Computational Linguistics - Vol-ume 1, EACL ?03, pages 331?338, Stroudsburg, PA,USA.
Association for Computational Linguistics.Nancy Stenson.
1981.
Studies in Irish Syntax.
Tu?bingen:Gunter Narr Verlag.Elaine U??
Dhonnchadha.
2009.
Part-of-Speech Taggingand Partial Parsing for Irish using Finite-State Trans-ducers and Constraint Grammar.
Ph.D. thesis, DublinCity University.11
