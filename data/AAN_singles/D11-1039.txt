Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 421?432,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsBootstrapping Semantic Parsers from ConversationsYoav Artzi and Luke ZettlemoyerComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195{yoav,lsz}@cs.washington.eduAbstractConversations provide rich opportunities forinteractive, continuous learning.
When some-thing goes wrong, a system can ask for clari-fication, rewording, or otherwise redirect theinteraction to achieve its goals.
In this pa-per, we present an approach for using con-versational interactions of this type to inducesemantic parsers.
We demonstrate learningwithout any explicit annotation of the mean-ings of user utterances.
Instead, we modelmeaning with latent variables, and introducea loss function to measure how well potentialmeanings match the conversation.
This lossdrives the overall learning approach, which in-duces a weighted CCG grammar that could beused to automatically bootstrap the semanticanalysis component in a complete dialog sys-tem.
Experiments on DARPA Communica-tor conversational logs demonstrate effectivelearning, despite requiring no explicit mean-ing annotations.1 IntroductionConversational interactions provide significant op-portunities for autonomous learning.
A well-definedgoal allows a system to engage in remediations whenconfused, such as asking for clarification, reword-ing, or additional explanation.
The user?s responseto such requests provides a strong, if often indirect,signal that can be used to learn to avoid the orig-inal confusion in future conversations.
In this pa-per, we show how to use this type of conversationalfeedback to learn to better recover the meaning ofuser utterances, by inducing semantic parsers fromunannotated conversational logs.
We believe thatthis style of learning will contribute to the long termgoal of building self-improving dialog systems thatcontinually learn from their mistakes, with little orno human intervention.Many dialog systems use a semantic parsing com-ponent to analyze user utterances (e.g., Allen et al,2007; Litman et al, 2009; Young et al, 2010).
Forexample, in a flight booking system, the sentenceSent: I want to go to Seattle on FridayLF: ?x.to(x, SEA) ?
date(x, FRI)might be mapped to the logical form (LF) meaningrepresentation above, a lambda-calculus expressiondefining the set of flights that match the user?s de-sired constraints.
This LF is a representation of thesemantic content that comes from the sentence, andwould be input to a context-dependent understand-ing component in a full dialog system, for exampleto find the date that the symbol FRI refers to.To induce semantic parsers from interactions, weconsider user statements in conversational logs andmodel their meaning with latent variables.
Wedemonstrate that it is often possible to use the dia-log that follows a statement (including remediationssuch as clarifications, simplifications, etc.)
to learnthe meaning of the original sentence.
For example,consider the first user utterance in Figure 1, wherethe system failed to understand the user?s request.To complete the task, the system must use a reme-diation strategy.
Here, it takes the initiative by ask-ing for and confirming each flight constraint in turn.This strategy produces an unnatural conversation butprovides supervision for learning the meaning of the421original utterance.
We can easily record representa-tions of the meanings the system intended to conveyat each step, as seen in Figure 1, and use this indirectsupervision for learning.Learning from this weak signal is challenging.
Inany specific conversation, the system?s remediationscan fail to recover aspects of the original user mean-ing and can introduce spurious constraints, for ex-ample when users change their goals mid conversa-tion.
To learn effectively, the model must accumu-late evidence from many interactions to best recoverthe meaning of each specific sentence.We will learn semantic parsers defined by prob-abilistic Combinatory Categorial Grammars (PC-CGs), which include both a lexicon and a weightedlinear model for parse selection.
The lexicon speci-fies the meanings of individual words and phrases,while the parameters of a parsing model definehow to best combine word- and phrase-level mean-ings to analyze complete sentences.
To learn with-out labeled meaning representations, we make useof a variant of the loss-sensitive Perceptron algo-rithm (Singh-Miller and Collins, 2007).
We defineloss functions to provide a rough measure of (1) howwell a candidate meaning for a utterance matchesthe conversation that follows it and (2) how well thecandidate matches our expectations about the typesof things that are often said in the dialog?s domain.These notions of loss drive not only the parameterestimation but also the grammar induction processthat constructs the CCG lexicon.
Experiments onconversation logs from the DARPA Communicatorcorpus (Walker et al, 2002) demonstrate the feasi-bility of our approach.This paper makes the following contributions:?
A formalization of the problem of learningthe meaning of user statements from conversa-tional feedback, without requiring annotation.?
A new loss-sensitive learning algorithm for thisproblem that induces semantic parsers fromconversation logs.?
Loss functions to measure the quality of hypo-thetical utterance meanings within the conver-sation in which they appear.?
An evaluation on logs from two dialog systemsSYSTEM: how can i help you?
(OPEN TASK)USER: i would like to fly from atlanta georgia to lon-don england on september twenty fourth in the earlyevening i would like to return on october first departingfrom london in the late morningSYSTEM: leaving what city?
(ASK:?x.from(fl, x))USER: atlanta georgiaSYSTEM: leaving atlanta.
(CONFIRM:from(fl, ATL))going to which city?
(ASK:?x.to(fl, x))USER: londonSYSTEM: arriving in london england.
(CONFIRM:to(fl, LON)) what date would you like todepart atlanta?
(ASK:?x.from(fl, ATL) ?
depart-date(fl, x))USER: september twenty fourth in the early evening[conversation continues]Figure 1: Conversational excerpt from a DARPA Com-municator travel-planning dialog.
Each system statementis labeled with representations of its speech act and log-ical meaning, in parentheses.
The user utterances haveno labels.
Conversations of this type provide the trainingdata to learn semantic parsers for user utterances.that demonstrate effective learning from con-versations alone.2 ProblemOur goal is to learn a function that maps a sentencex to a lambda-calculus expression z.
We assume ac-cess to logs of conversations with automatically gen-erated annotation of system utterance meanings, butno explicit labeling of each user utterance meaning.We define a conversation C = (~U,O) to be a se-quence of utterances ~U = [u0, .
.
.
, um] and a setof conversational objects O.
An object o ?
Ois an entity that is being discussed, for examplethere would be a unique object for each flight legdiscussed in a travel planning conversation.
Eachutterance ui = (s, x, a, z) represents the speakers ?
{User, System} producing the natural lan-guage statement x which asserts a speech act a ?
{ASK,CONFIRM, .
.
.}
with meaning represen-tation z.
For example, from the second system ut-terance in Figure 1 the question x =?Leaving whatcity??
is an a=ASK speech act with lambda-calculusmeaning z = ?x.from(fl, x).
This meaning repre-sents the fact that the system asked for the departurecity for the conversational object o = fl represent-ing the flight leg that is currently being discussed.We will learn from conversations where the speech422acts a and logical forms z for user utterances are un-labeled.
Such data can be generated by recordinginteractions, along with each system?s internal rep-resentation of its own utterances.Finally, since we will be analyzing sentences at aspecific point in a complete conversation, we defineour training data as a set {(ji, Ci)|i = 1 .
.
.
n}.
Eachpair is a conversation Ci and the index ji of the userutterance x in Ci whose meaning we will attempt tolearn to recover.
In general, the same conversationC can be used in multiple examples, each with a dif-ferent sentence index.
Section 8 provides the detailsof how the data was gathered for our experiments.3 Overview of ApproachWe will present an algorithm for learning a weightedCCG parser, as defined in Section 5, that can be usedto map sentences to logical forms.
The approachinduces a lexicon to represent the meanings of wordsand phrases while also estimating the parameters ofa weighted linear model for selecting the best parsegiven the lexicon.Learning As defined in Section 2, the algorithmtakes a set of n training examples, {(ji, Ci) : i =1, .
.
.
, n}.
For each example, our goal is to learn toparse the user utterance x at position ji in Ci.
Thetraining data contains no direct evidence about thelogical form z that should be paired with x, or theCCG analysis that would be used to construct z. Wemodel all of these choices as latent variables.To learn effectively in this complex, latent space,we introduce a loss function L(z, j, C) ?
R thatmeasures how well a logical form z models themeaning for the user utterance at position j in C. InSection 6, we will present the details of the loss weuse, which is designed to be sensitive to remedia-tions in C (system requests for clarification, etc.)
butalso be robust to the fact that conversations often donot uniquely determine which z should be selected,for example when the user prematurely ends the dis-cussion.
Then, in Section 7, we present an approachfor incorporating this loss function into a completealgorithm that induces a CCG lexicon and estimatesthe parameters of the parsing model.This learning setup focuses on a subproblem indialog; semantic interpretation.
We do not yet learnto recover user speech acts or integrate the logicalform into the context of the conversation.
These areimportant areas for future work.Evaluation We will evaluate performance on atest set {(xi, zi)|i = 1, .
.
.
,m} of m sentences xithat have been explicitly labeled with logical formszi.
This data will allow us to directly evaluate thequality of the learned model.
Each sentence is an-alyzed with the learned model alone; the loss func-tion and any conversational context are not used dur-ing evaluation.
Parsers that perform well in this set-ting will be strong candidates for inclusion in a morecomplete dialog system, as motivated in Section 1.4 Related WorkMost previous work on learning from conversationalinteractions has focused on the dialog sub-problemsof response planning (e.g., Levin et al, 2000; Singhet al, 2002) and natural language generation (e.g.,Lemon, 2011).
We are not aware of previous workon inducing semantic parsers from conversations.There has been significant work on supervisedlearning for inducing semantic parsers.
Varioustechniques were applied to the problem includ-ing machine translation (Papineni et al, 1997;Ramaswamy and Kleindienst, 2000; Wong andMooney, 2006; 2007; Matuszek et al, 2010), higher-order unification (Kwiatkowski et al, 2010), parsing(Ruifang and Mooney, 2006; Lu et al, 2008), induc-tive logic programming (Zelle and Mooney, 1996;Thompson and Mooney, 2003; Tang and Mooney,2000), probabilistic push-down automata (He andYoung, 2005; 2006) and ideas from support vec-tor machines and string kernels (Kate and Mooney,2006; Nguyen et al, 2006).
The algorithms we de-velop in this paper build on previous work on su-pervised learning of CCG parsers (Zettlemoyer andCollins, 2005; 2007), as we describe in Section 5.3.There is also work on learning to do semanticanalysis with alternate forms of supervision.
Clarkeet al (2010) and Liang et al (2011) describe ap-proaches for learning semantic parsers from ques-tions paired with database answers, while Gold-wasser et al (2011) presents work on unsuper-vised learning.
Our approach provides an alterna-tive method of supervision that could complementthese approaches.
Additionally, there has been sig-nificant recent work on learning to do other, re-423I want to go from Boston to New York and then to ChicagoS/N (N\N)/NP NP (N\N)/NP NP CONJ[] (N\N)/NP NP?f.f ?y.?f.
?x.f(x) ?
from(x, y) BOS ?y.?f.
?x.f(x) ?
to(x, y) NYC ?y.?f.
?x.f(x) ?
to(x, y) CHI> > >(N\N) (N\N) (N\N)?f.
?x.f(x) ?
from(x,BOS) ?f.
?x.f(x) ?
to(x,NY C) ?f.
?x.f(x) ?
to(x,CHI)<B(N\N)?f.
?x.f(x) ?
from(x,BOS) ?
to(x,NY C)<?>(N\N)?f.
?x[].f(x) ?
from(x[1], BOS) ?
to(x[1], NY C) ?
before(x[1], x[2]) ?
to(x[2], CHI)N?x[].from(x[1], BOS) ?
to(x[1], NY C) ?
before(x[1], x[2]) ?
to(x[2], CHI)>S?x[].from(x[1], BOS) ?
to(x[1], NY C) ?
before(x[1], x[2]) ?
to(x[2], CHI)Figure 2: An example CCG parse.
This parse shows the construction of a logical form with an array-typed variable x[]that specifies a list of flight legs, indexed by x[1] and x[2].
The top-most parse steps introduce lexical items while thelower ones create new nonterminals according the CCG combinators (>, <, etc.
), see Steedman (2000) for details.lated, natural language semantic analysis tasks fromcontext-dependent database queries (Miller et al,1996; Zettlemoyer and Collins, 2009), groundedevent streams (Chen et al, 2010; Liang et al, 2009),environment interactions (Branavan et al, 2009;2010; Vogel and Jurafsky, 2010), and even unanno-tated text (Poon and Domingos, 2009; 2010).Finally, the DARPA Communicator data (Walkeret al, 2002) has been previously studied.
Walker andPassonneau (2001) introduced a schema of speechacts for evaluation of the DARPA Communicatorsystem performance.
Georgila et al (2009) extendedthis annotation schema to user utterances using anautomatic process.
Our speech acts extend this workto additionally include full meaning representations.5 Mapping Sentences to Logical FormWe will use a weighted linear CCG grammar for se-mantic parsing, as briefly reviewed in this section.5.1 Combinatory Categorial GrammarsCombinatory categorial grammars (CCGs) are alinguistically-motivated model for a wide range oflanguage phenomena (Steedman, 1996; 2000).
ACCG is defined by a lexicon and a set of combina-tors.
The grammar defines a set of possible parsetrees, where each tree includes syntactic and seman-tic information that can be used to construct logicalforms for sentences.The lexicon contains entries that define categoriesfor words or phrases.
For example, the secondlexical entry in the parse in Figure 2 is:from := (N\N)/NP : ?y.?f.
?x.f(x) ?
from(x, y)Each category includes both syntactic and seman-tic information.
For example, the phrase ?from?is assigned the category with syntax (N\N)/NPand semantics ?y.?f.
?x.f(x) ?
from(x, y).
Theoutermost syntactic forward slash specifies that theentry must first be combined with an NP to theright (the departure city), while the inner back slashspecifies that it will later modify a noun N to theleft (to add a constraint to a set of flights).
Thelambda-calculus semantic expression is designedto build the appropriate meaning representation ateach of these steps, as seen in the parse in Figure 2.In general, we make use of typed lambda cal-culus to represent meaning (Carpenter, 1997), bothin the lexicon and in intermediate parse tree nodes.We also introduce an extension for modeling array-typed variables to represent lists of individual en-tries.
These constructions are used, for example, tomodel sentences describing a sequence of segmentswhile specifying flight preferences.Figure 2 shows how a CCG parse builds a logicalform for a complete sentence with an array-typedvariable.
Each intermediate node in the tree is con-structed with one of a small set of CCG combina-tor rules, see the explanation from Steedman (1996;2000).
We make use of the standard application,composition and coordination combinators, as wellas type-shifting rules introduced by Zettlemoyer andCollins (2007) to model spontaneous, unedited text.5.2 Weighted Linear CCGsA weighted linear CCG (Clark and Curran, 2007)provides a ranking on the space of possible parsesunder the grammar, which can be used to selectthe best logical form for a sentence.
This type ofmodel is closely related to several other approaches(Ratnaparkhi et al, 1994; Johnson et al, 1999;424Lafferty et al, 2001; Collins, 2004; Taskar et al,2004).
Let x be a sentence, y be a CCG parse, andGEN(x; ?)
be the set of all possible CCG parses forx given the lexicon ?.
Define ?
(x, y) ?
Rd to bea d-dimensional feature?vector representation and?
?
Rd to be a parameter vector.
The optimal parsefor sentence x isy?
(x) = arg maxy?GEN(x;?)
?
?
?
(x, y)and the final output logical form z is the lambda-calculus expression at the root of y?
(x).We compute y?
(x) with a CKY-style chart pars-ing algorithm.
Since each chart entry contains afull lambda-calculus meaning expression, we useN -best pruning to control the number of options weconsider at each span.
Learning a model of this forminvolves learning the parameters ?
and the lexicon?.
We will show that this is possible from conversa-tional logs that do not contain any explicit labelingof the logical forms for user utterances.5.3 Supervised learning with GENLEXPrevious work on lexical induction, including theGENLEX approach which we briefly review here,has required labeled logical meaning representa-tions.
In Section 7, we will introduce a new wayof using GENLEX to learn from unannotated con-versation logs.The supervised CCG learning algorithms ofZettlemoyer and Collins (2005; 2007) induce aweighted CCG from training examples (xi, zi) fori = 1 .
.
.
n, where xi is a sentence and zi is thecorresponding lambda-calculus meaning representa-tion.
The output from the algorithm is a pair (?,?
)containing the learned parameters and CCG lexicon.They defined the function GENLEX(x, z) to mapa sentence x and its meaning z to a large set of poten-tial lexical entries.
These entries are generated withrules that consider the logical form z and guess po-tential CCG categories.
For example, the rule p ?
(N\N)/NP : ?y.?f.
?x.f(x) ?
p(x, y) introducescategories commonly used to model certain types ofprepositions.
This rule would, for example, intro-duce the category (N\N)/NP : ?y.?f.
?x.f(x) ?to(x, y) for any logical form z that contains the con-stant to.
GENLEX uses a small set of rules of thistype to generate categories that are paired with allExample Conversation:(1) SYSTEM: how can i help you?
(OPEN TASK)(2) USER: i would like to fly from boston arriving to new yorkand back from new york to boston(3) SYSTEM: leaving boston (CONFIRM:from(fl1, BOS))on what date?
(ASK:?x.departdate(fl1, x))(4) USER: september twenty four(5) SYSTEM: leaving boston (CONFIRM:from(fl1, BOS))on september twenty four (CONFIRM:depart-date(fl1, sept 24)) to new york(CONFIRM:to(fl1, NY C)) is new york your finaldestination?
(ASK:?
?x.fr(x,NY C))(6) USER: yes i want to return to boston(7) SYSTEM: sorry, i didn?t understand.
(APOL-OGY) where are you flying to from new york?
(ASK:?x.fr(fl2, NY C) ?
to(fl2, x))[conversation ends]Candidate Logical Expressions for Utterance #2:(a) ?x.to(x,BOS) ?
from(x,NY C)(b) ?x.from(x,BOS) ?
to(x,NY C)(c) ?x.to(x,BOS) ?
to(x,NY C)(d) ?x[].from(x[1], BOS) ?
to(x[1], NY C)?
before(x[1], x[2]) ?
return(x[2])?
from(x[2], NY C) ?
to(x[2], BOS))(e) ?x[].from(x[1], BOS) ?
to(x[1], NY C)?
before(x[1], x[2]) ?
return(x[2])?
from(x[2], BOS) ?
to(x[2], NY C)Figure 3: Conversation reflecting an interaction as seenin the DARPA Communicator travel-planning dialogs.possible substrings in x to form an overly generallexicon.
The complete learning algorithm then si-multaneously selects a small subset of all entriesgenerated by GENLEX and estimates parameter val-ues ?.
Zettlemoyer and Collins (2005) present amore detailed explanation.6 Measuring LossIn Section 7, we will present a loss-sensitive learn-ing algorithm that models the meaning of user utter-ances as latent variables to be estimated from con-versational interactions.We first introduce a loss function to measure thequality of potential meaning representations.
Thisloss function L(z, j, C) ?
R indicates how well alogical expression z represents the meaning of thej-th user utterance in conversation C. For example,425consider the first user utterance (j = 2) in Figure 3,which is a request for a return trip from Boston toNew York.
We would like to assign the lowest lossto the meaning representation (d) in Figure 3 thatcorrectly encodes all of the stated constraints.We make use of a loss function with two parts:L(z, j, C) = Lc(z, j, C) + Ld(z).
The conversa-tion loss Lc (defined in Section 6.1) measures howwell the candidate meaning representation fits theconversation, for example incorporating informa-tion recovered through conversational remediationsas motivated in Section 1.
The domain loss Ld (de-scribed in Section 6.2) measures how well a logi-cal form z matches domain expectations, such asthe fact that flights can only have a single origin.These functions guide the types of meaning repre-sentations we expect to see, but in many cases willfail to specify a unique best option, for examplein conversations where the user prematurely termi-nates the interaction.
In Section 7, we will present acomplete, loss-driven learning algorithm that is ro-bust to these types of ambiguities while inducing aweighted CCG parser from conversations.6.1 Conversation LossWe will use a conversation loss function Lc(z, j, C)that provides a rough indication of how well the log-ical expression z represents a potential meaning forthe user utterance at position j in C. For example,the first user utterance (j = 2) in Figure 3 is a re-quest for a return trip from Boston to New Yorkwhere the user has explicitly mentioned both legs.The figure also shows five options (a-e) for the logi-cal form z.
We want to assign the lowest loss to op-tion (d), which includes all of the stated constraints.The loss is computed in four steps for a user ut-terance x at position j by (1) selecting a subset ofsystem utterances in the conversation C, (2) extract-ing and computing loss for semantic content fromselected system utterances, (3) aligning the subex-pressions in z to the extracted semantic content, and(4) computing the minimal loss value from the bestalignment.
In Figure 3, the loss for the candidatelogical forms is computed by considering the seg-ment of system utterances up until the conversationend.
Within this segment, the matching for expres-sion (d) involves mapping the origin and departureconstraints for the first leg (Boston - New York) ontothe earlier system confirmations while also align-ing the ones for the second leg to system utteranceslater in the selected portion of the conversation.
Fi-nally, the overall score depends on the quality of thealignment, for example how many of the constraintsmatch to confirmations.
This section presents thefull approach.Segmentation For a user utterance at position j,we select all system utterances from j ?
1 until thesystem believes it has completed the current subtask,as indicated by a reset action or final offer.
We callthis selected segment C?.
In Figure 3, C?
ends with areset, but in a successful interaction it would haveended with the offer of a specific flight.Extracting Properties A property is a predicate-entity-value triplet, where the entity can be a vari-able from z or a conversational object.
For example,?from, fl, BOS?
is a property where fl is a ob-ject from C?
and ?from, x,BOS?
is a property fromz = ?x.from(x,BOS).
We define PC?
to be theset of properties from logical forms for system ut-terances in C?.
Similarly, we define Pz to be the setof properties in z.Scoring System Properties For each systemproperty p ?
PC?
we compute its position valuepos(p), which is a normalized weighted averageover all the positions where it appears in a logi-cal form.
For each mention the weight is obtainedfrom its speech act.
For example, properties that areexplicitly confirmed contribute more to the averagethan those that were merely offered to the user in aselect statement.We use pos(p) to compute a loss loss(p) foreach property p ?
PC?
.
We first define P eC?
to be allproperties in PC?
with entity e. For entity e and po-sition d, we define the entity-normalization function:ne(d) =d?minp?P eC?
pos(p)maxp?P eC?
pos(p)?minp?P eC?
pos(p).For a given property p ?
PC?
with an entity e wecompute the loss value:loss(p) = n?1e (1?
ne(pos(p)))?
1 .Where n?1e is the inverse of ne.
This loss value is de-signed to, first, provide less loss for later propertiesso that it, for example, favors the last property in aseries of statements that finally resolves a confusion426in the conversation.
Second, the loss value is lowerfor objects mentioned closer to the user utterance x,thereby preferring objects discussed sooner.Matching Properties An alignment A maps vari-ables in z to conversational objects in C?, for exam-ple the flight legs fl1 and fl2 being discussed inFigure 3.
We will use alignments to match prop-erties of z and C?.
To do this we extend the align-ment function A to apply to properties, for exampleA(?from, x,BOS?)
= ?from,A(x), BOS?.Scoring Alignments Finally, we compute theconversation loss Lc(z, j, C) as follows:Lc(z, j, C) = minA?pu?Pz?ps?PC?s(A(pu), ps) .The function s(A(pu), ps) ?
R computes the com-patibility of the two input properties.
It is zero ifA(pu) 6= ps.
Otherwise, it returns loss(ps).We approximate the min computation in Lc overalignments A as follows.
For a logical form z atposition j, we align the outer-most variable to theconversational object in C?
that is being discussed atj.
The remaining variables are aligned greedily tominimize the loss, by selecting a single conversa-tional object for each in turn.Finally, for each aligned variable, we increase theloss by one for each unmatched property from Pz .This increases the loss of logical forms that includespurious information.
However, since a conversationmight stop prematurely and therefore won?t discussthe entire user request, we only increase the loss forvariables that are already aligned.
For this purpose,we define an aligned variable to be one that has atleast one property matched successfully.6.2 Domain LossWe also make use of a domain loss functionLd(z) ?R.
The function takes a logical form z and returnsthe number of violations there are in z to a set ofconstraints on logical forms that occur commonly inthe dialog domain.
For example, in a travel domain,a violation might occur if a flight leg has two differ-ent destination cities.
The set of possible violationsmust be specified for each dialog system, but can of-ten be compiled from existing resources, such as adatabase of valid flight ticketing options.In our experiments, we will use a set of eightsimple constraints to check for violations in flightInputs: Training set {(ji, Ci) : i = 1 .
.
.
n} where each exam-ple includes the index ji of a sentence xi in the conversationCi.
Initial lexicon ?0.
Number of iterations T .
Margin ?.Beam size k for lexicon generation.
Loss function L(x, j, C),as described in Section 6.Definitions: GENLEX(x, C) takes as input a sentence and aconversation and returns a set of lexical items as described inSection 7.
GEN(x; ?)
is the set of all possible CCG parsesfor x given the lexicon ?.
LF (y) returns the logical formz at the root of the parse tree y.
Let ?i(y) be shorthand forthe feature function ?
(xi, y) defined in Section 5.
DefineLEX(y) to be the set of lexical entries used in parse y. Fi-nally, let MINLi(Y ) be {y|?y?
?
Y,L(LF (y), ji, Ci) ?L(LF (y?
), ji, Ci)}, the set of minimal loss parses in Y .Algorithm:?
= 0?
, ?
= ?0For t = 1 .
.
.
T, i = 1 .
.
.
n :Step 1: (Lexical generation)a.
Set ?
= ?
?GENLEX(xi, Ci)b.
Let Y be the k highest scoring parses of xi using ?c.
Select new lexical entries from the lowest loss parses?i =?y?MINLi(Y ){l|l ?
LEX(y)}d. Set lexicon to ?
= ?
?
?iStep 2: (Update parameters)a.
Define Gi = MINLi(GEN(xi,?, ?))
andLmin to be the minimal lossb.
Set Bi = GEN(xi,?, ?)?Gic.
Set the relative loss function: ?i(y) = L(y, Ci)?Lmind.
Construct sets of margin violating good and bad parses:Ri = {r|r ?
Gi ??y?
?
Bi s.t.
??,?i(r)?
?i(y?)?
< ?
?i(r)}Ei = {e|e ?
Bi ??y?
?
Gi s.t.
??,?i(y?)?
?i(e)?
< ??i(e)}e.
Apply the additive update:?
= ?
+?r?Ri1|Ri|?i(r)?
?e?Ei1|Ei|?i(e)Output: Parameters ?
and lexicon ?Figure 4: The learning algorithm.itineraries, which can have multiple legs.
Theseinclude, for example, checking that the legs haveunique origins and destinations that match across theentire itinerary.
For example, in Figure 3 the logicalforms (a), (b) and (d) will have no violations; theydescribe valid flights.
Example (c) has a single vio-lation: a flight has two origins.
Example (e) violatesa more complex constraint: the second flight?s originis different from the first flight?s destination.7 LearningFigure 4 presents the complete learning algorithm.We assume access to training examples, {(ji, Ci) :i = 1, .
.
.
, n}, where each example includes the in-427dex ji of a sentence xi in the conversation Ci.
The al-gorithm learns a weighted CCG parser, described inSection 5, including both a lexicon ?
and parameters?.
The approach is online, considering each examplein turn and performing two steps: (1) expanding thelexicon and (2) updating the parameters.Step 1: Lexical Induction We introduce new lex-ical items by selecting candidates from the functionGENLEX , following previous work (Zettlemoyerand Collins, 2005; 2007) as reviewed in Section 5.3.However, we face the new challenge that there isno labeled logical-form meaning z.
Instead, let ZC?be set of all logical forms that appear in systemutterances in the relevant conversation segment C?.We will now define the conversational lexicon set:GENLEX(x, C?)
=?z?ZC?GENLEX(x, z)where we use logical forms from system utterancesto guess possible CCG categories for analyzing theuser utterance.
This approach will overgeneralize,when the system talks about things that are unrelatedto what the user said, and will also often be incom-plete, for example when the system does not repeatparts of the original content.
However, it provides away of guessing lexical items that can be combinedwith previously learned ones, which can fill in anygaps and help select the best analysis.Step 1(a) in Figure 4 uses GENLEX to tem-porarily create a large set of potential categoriesbased on the conversation.
Steps (b-d) select a smallsubset of these entries to add to the current lexicon?
: we find the k-best parses under the model, re-rank them according to loss, find the lexical itemsused in the best trees, and add them to ?.
Thisapproach favors lexical items that are used in high-scoring but low-loss analyses, as computed given thecurrent model.Step 2: Parameter Updates Given the loss func-tion L(x, i, C), we use a variant of a loss-sensitiveperceptron to update the parameters (Singh-Millerand Collins, 2007).
In Steps (a-c), for the currentexample i, we compute the relative loss function ?ithat scales with the loss achieved by the best andworst possible parses under the model.
In contrastto previous work, we do not only compute the lossover a fixed n-best list of possible outputs, but in-stead use the current model score to recompute theoptions at each update.
Then, Steps (d-e) find the setRi of least loss analyses and Ei of higher-loss can-didates whose models scores are not separated by atleast ?
?i, where ?
is a margin scale constant.
Thefinal update (Step f) is additive and increases the pa-rameters for features indicative of the analyses withless loss while down weighting those for parses thatwere not sufficiently separated.Discussion This algorithm uses the conversationto drive learning in two ways: it guides the lexi-cal items that are proposed while also providing theconversational feedback that defines the loss used toupdate the parameters.
The resulting approach is,at every step, using information about how the con-versation progressed after a user utterance to recon-struct the meaning of the original statement.8 Data SetsFor evaluation, we used conversation logs from theLucent and BBN dialog systems in the DARPACommunicator corpus (Walker et al, 2002).
We se-lected these systems since they provide significantopportunities for learning.
They asked relativelyopen ended questions, allowing for more complexuser responses, while also using a number of sim-ple remediating strategies to recover from misun-derstandings.
The original conversational logs in-cluded unannotated transcripts of system and userutterances.
Inspired by the speech act labeling ap-proach of Walker and Passonneau (2001), we wrotea set of scripts to label the speech acts and logicalforms for system statements.
This could be donewith high accuracy since the original text was gener-ated with templates.
These labels represent what thesystem explicitly said and do not require complex,potentially error-prone annotation of the full state ofthe original dialog system.
The set of speech acts in-cludes confirmations, information requests, selects,offers, instructions, and a miscellaneous category.The data sets include a total of 376 conversations,divided into training and testing sets.
Table 1 pro-vides details about the training and testing sets, aswell as general data set statistics.
We developed oursystem using 4-fold cross validation on the trainingsets.
Although there are approximately 12,000 user428Lucent BBN# Conversations 214 162Total # of utterances 11,974 12,579Avg.
utterances per conversation 55.95 77.65Avg.
tokens per user utterance 3.24 2.39Total # of training utterances 208 67Total # of testing utterances 96 67Avg.
tokens per selected utterance 11.72 9.53Table 1: Data set statistics for Lucent and BBN systems.utterances in the data sets, the vast majority are sim-ple, short phrases (such as ?yes?
or ?no?)
which arenot useful for learning a semantic parser.
We se-lect user utterances with a small set of heuristics, in-cluding a threshold (6 for Lucent, 4 for BBN) on thenumber of words and requiring that at least one nounphrase is present from our initial lexicon.
This ap-proach was manually developed to perform well onthe training sets, but is not perfect and does intro-duce a small amount of noise into the data.9 Experimental SetupThis section describes our experimental setup andcomparisons.
We follow the setup of Zettlemoyerand Collins (2007) where possible, including fea-ture design, initialization of the semantic parser, andevaluation metrics, as reviewed below.Features and Parser The features include indica-tors for lexical item use, properties of the logicalform that is being constructed, and indicators forparsing operators used to build the tree.
The parserattempts to boost recall with a two-pass strategy thatallows for word skipping if the initial parse fails.Initialization and Parameters We use an initiallexicon that includes a list of domain-specific nounphrases, such as city and airport names, and a listof domain-independent categories for closed-classwords such as ?the?
and ?and?.
We also used a timeand number parser to expand this lexicon for eachinput sentence with the BIU Number Normalizer.1The learning parameters were tuned using the devel-opment sets: the margin constant ?
is set to 0.5, weuse 6 iterations and take the top 30 parses for lexicalgeneration (step 1, figure 4).
The parser used for pa-rameter update (step 2, figure 4) has a beam of 250.The parameter vector is initialized to 0?.1http://www.cs.biu.ac.il/?nlp/downloads/Evaluation Metrics For evaluation, we measureperformance against gold standard labels.
We reportboth the number of exact matches, fully correct log-ical forms, and a partial-credit number.
We measurepartial-credit accuracy by mapping logical forms toattribute-value pairs (for example, the expressionfrom(x, LA) will be mapped to from = LA) andreport precision and recall on attribute sets.
Thismore lenient measure does not test the overall struc-ture of the logical expression, only its components.Systems We compare performance with the fol-lowing systems:Full Supervision: We measured how a fully super-vised approach would perform on our data by hand-labeling the training data and using a 0-1 loss func-tion that tests if the output logical form matches thelabeled one.
For lexicon generation, the labels wereused instead of the conversation.No Conversation Baseline: We also report resultsfor a no conversation baseline.
This baseline sys-tem is constructed by making two modifications tothe full approach.
We remove the conversation lossfunction and apply the GENLEX templates to everypossible logical constant, instead of only those in theconversation.
This baseline allows us to measure theimportance of having access to the conversations bycompletely ignoring the context for each sentence.Ablations: In addition to the baseline above, wealso do ablation tests by turning off various individ-ual components of the complete algorithm.10 ResultsTable 2 shows exact match results for the develop-ment sets, including different system configurations.We report mean results across four folds.
To ver-ify their contributions, we include results where weablate the conversational loss and domain loss func-tions.
Both are essential.The test results are listed in Table 3.
The fullmethod significantly outperforms the baseline, indi-cating that we are making effective use of the con-versational feedback, although we do not yet matchthe fully supervised result.
The poor baseline per-formance is not surprising, given the difficulty of thetask and lack of guidance when the conversations areremoved.
The partial-credit numbers also demon-strate an empirical trend that we observed; in many429Exact Match Metric Lucent BBNPrec.
Rec.
F1 Prec.
Rec.
F1Without conversational loss 0.35 0.34 0.35 0.66 0.54 0.59Without domain loss 0.42 0.42 0.42 0.69 0.56 0.61Our Approach 0.63 0.61 0.62 0.77 0.64 0.69Supervised method 0.76 0.75 0.75 0.81 0.67 0.73Table 2: Mean exact-match results for cross fold evaluation on the development sets.Exact Match Metric Lucent BBNPrec.
Rec.
F1 Prec.
Rec.
F1No Conversations Baseline 0 0 0 0.16 0.15 0.15Our Approach 0.58 0.55 0.56 0.85 0.75 0.79Supervised method 0.7 0.68 0.69 0.87 0.78 0.82Partial Credit Metric Lucent BBNPrec.
Rec.
F1 Prec.
Rec.
F1No Conversations Baseline 0.26 0.35 0.29 0.26 0.33 0.29Our Approach 0.68 0.63 0.65 0.97 0.57 0.72Supervised method 0.75 0.68 0.72 0.96 0.68 0.79Table 3: Exact- and partial-match results on the test sets.cases where we do not produce the correct logicalform, the output is often close to correct, with onlyone or two missed flight constraints.The difference between the two systems is evi-dent.
The BBN system presents a simpler approachto the dialog problem by creating a more constrainedconversation.
This is done by handling one flightat a time, in the case of flight planing, and pos-ing simple and close ended questions to the user.Such an approach encourages the user to make sim-pler requests, with relatively few constraints in eachrequest.
In contrast, the Lucent system presents aless-constrained approach: interactions start with anopen ended prompt and the conversations flow in amore natural, less constrained fashion.
BBN?s sim-plified approach makes it easier for learning, givingus superior performance when compared to the Lu-cent system, despite the smaller training set.
This istrue for both our approach and supervised learning.We compared the logical forms recovered by thebest conversational model to the labeled ones in thetraining set.
Many of the errors came from caseswhere the dialog system never fully recovered fromconfusions in the conversation.
For example, the Lu-cent system almost never understood user utterancesthat specified flight arrival times.
Since it was unableto consistently recover and introduce this constraint,the user would often just recalculate and specify adeparture time that would achieve the original goal.This type of failure provides no signal for our learn-ing algorithm, whereas the fully supervised algo-rithm would use labeled logical forms to resolve theconfusion.
Interestingly, the test set had more sen-tences that suffered such failures than the develop-ment set, which contributed to the performance gap.11 DiscussionWe presented a loss-driven learning approach thatinduces the lexicon and parameters of a CCG parserfor mapping sentences to logical forms.
The losswas defined over the conversational context, withoutrequiring annotation of user utterances meaning.The overall approach assumes that, in aggregate,the conversations contain sufficient signal (remedia-tions such as clarification, etc.)
to learn effectively.In this paper, we satisfied this requirement by us-ing logs from automated systems that deployed rea-sonably effective recovery strategies.
An importantarea for future work is to consider how this learningcan be best integrated into a complete dialog system.This would include designing remediation strategiesthat allow for the most effective learning and consid-ering how similar techniques could be used simulta-neously for other dialog subproblems.AcknowledgmentsThe research was supported by funding from theDARPA Computer Science Study Group.
Thanksto Dan Weld, Raphael Hoffmann, Jonathan Berant,Hoifung Poon and Mark Yatskar for their sugges-tions and comments.
We also thank Shachar Mirkinfor providing access to the BIU Normalizer.430ReferencesAllen, J., M. Manshadi, M. Dzikovska, and M. Swift.2007.
Deep linguistic processing for spoken dialoguesystems.
In Proceedings of the Workshop on Deep Lin-guistic Processing.Branavan, SRK, H. Chen, L.S.
Zettlemoyer, and R. Barzi-lay.
2009.
Reinforcement learning for mapping in-structions to actions.
In Proceedings of the Joint Con-ference of the Association for Computational Linguis-tics and the International Joint Conference on NaturalLanguage Processing.Branavan, SRK, L.S.
Zettlemoyer, and R. Barzilay.
2010.Reading between the lines: learning to map high-levelinstructions to commands.
In Proceedings of the Asso-ciation for Computational Linguistics.Carpenter, B.
1997.
Type-Logical Semantics.
The MITPress.Chen, D.L., J. Kim, and R.J. Mooney.
2010.
Training amultilingual sportscaster: using perceptual context tolearn language.
Journal of Artificial Intelligence Re-search 37(1):397?436.Clark, S. and J.R. Curran.
2007.
Wide-coverage efficientstatistical parsing with CCG and log-linear models.Computational Linguistics 33(4):493?552.Clarke, J., D. Goldwasser, M. Chang, and D. Roth.
2010.Driving semantic parsing from the world?s response.In Proceedings of the Conference on ComputationalNatural Language Learning.Collins, M. 2004.
Parameter estimation for statisticalparsing models: Theory and practice of distribution-free methods.
In New Developments in Parsing Tech-nology.Georgila, K., O.
Lemon, J. Henderson, and J.D.
Moore.2009.
Automatic annotation of context and speech actsfor dialogue corpora.
Natural Language Engineering15(03):315?353.Goldwasser, D., R. Reichart, J. Clarke, and D. Roth.2011.
Confidence driven unsupervised semantic pars-ing.
In Proceedings.
of the Association of Computa-tional Linguistics.He, Y. and S. Young.
2005.
Semantic processing usingthe hidden vector state model.
Computer Speech andLanguage 19:85?106.He, Y. and S. Young.
2006.
Spoken language understand-ing using the hidden vector state model.
Speech Com-munication 48(3-4).Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.1999.
Estimators for stochastic ?unification-based?grammars.
In Proceedings of the Association for Com-putational Linguistics.Kate, R.J. and R.J. Mooney.
2006.
Using string-kernelsfor learning semantic parsers.
In Proceedings of theAssociation for Computational Linguistics.Kwiatkowski, T., L.S.
Zettlemoyer, S. Goldwater, andM.
Steedman.
2010.
Inducing probabilistic ccg gram-mars from logical form with higher-order unification.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing.Lafferty, J., A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof the International Conference on Machine Learning.Lemon, O.
2011.
Learning what to say and how to sayit: Joint optimisation of spoken dialogue managementand natural language generation.
Computer Speech &Language 25(2):210?221.Levin, E., R. Pieraccini, and W. Eckert.
2000.
A stochas-tic model of human-machine interaction for learningdialog strategies.
IEEE Transactions on Speech andAudio Processing 8(1):11?23.Liang, P., M.I.
Jordan, and D. Klein.
2009.
Learning se-mantic correspondences with less supervision.
In Pro-ceedings of the Joint Conference of the Associationfor Computational Linguistics the International JointConference on Natural Language Processing.Liang, P., M.I.
Jordan, and D. Klein.
2011.
Learningdependency-based compositional semantics.
In Pro-ceedings of the Association for Computational Lin-guistics.Litman, D., J. Moore, M.O.
Dzikovska, and E. Farrow.2009.
Using Natural Language Processing to AnalyzeTutorial Dialogue Corpora Across Domains Modali-ties.
In Proceeding of the Conference on Artificial In-telligence in Education.Lu, W., H.T.
Ng, W.S.
Lee, and L.S.
Zettlemoyer.
2008.A generative model for parsing natural language tomeaning representations.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.Matuszek, C., D. Fox, and K. Koscher.
2010.
Follow-ing directions using statistical machine translation.
InProceeding of the international conference on Human-robot interaction.Miller, S., D. Stallard, R.J. Bobrow, and R.L.
Schwartz.1996.
A fully statistical approach to natural languageinterfaces.
In Proceedings of the Association for Com-putational Linguistics.Nguyen, L., A. Shimazu, and X. Phan.
2006.
Seman-tic parsing with structured SVM ensemble classifica-tion models.
In Proceedings of the joint conference431of the International Committee on Computational Lin-guistics and the Association for Computational Lin-guistics.Papineni, K.A., S. Roukos, and T.R.
Ward.
1997.
Feature-based language understanding.
In Proceedings of theEuropean Conference on Speech Communication andTechnology.Poon, H. and P. Domingos.
2009.
Unsupervised semanticparsing.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.Poon, H. and P. Domingos.
2010.
Unsupervised ontologyinduction from text.
In Proceedings of the Associationfor Computational Linguistics.Ramaswamy, G.N.
and J. Kleindienst.
2000.
Hierarchi-cal feature-based translation for scalable natural lan-guage understanding.
In Proceedings of the Interna-tional Conference on Spoken Language Processing.Ratnaparkhi, A., S. Roukos, and R.T. Ward.
1994.
Amaximum entropy model for parsing.
In Proceedingsof the International Conference on Spoken LanguageProcessing.Ruifang, G. and R.J. Mooney.
2006.
Discriminativereranking for semantic parsing.
In Porceedings of theAssociation for Computational Linguistics.Singh, S.P., D.J.
Litman, M.J. Kearns, and M.A.
Walker.2002.
Optimizing dialogue management with re-inforcement learning: Experiments with the NJFunsystem.
Journal of Artificial Intelligence Research16(1):105?133.Singh-Miller, N. and M. Collins.
2007.
Trigger-basedlanguage modeling using a loss-sensitive perceptronalgorithm.
In IEEE International Conference onAcoustics, Speech and Signal Processing.Steedman, M. 1996.
Surface Structure and Interpreta-tion.
The MIT Press.Steedman, M. 2000.
The Syntactic Process.
The MITPress.Tang, L.R.
and R.J. Mooney.
2000.
Automated construc-tion of database interfaces: Integrating statistical andrelational learning for semantic parsing.
In Proceed-ings of the Joint Conference on Empirical Methodsin Natural Language Processing and Very Large Cor-pora.Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Thompson, C.A.
and R.J. Mooney.
2003.
Acquiringword-meaning mappings for natural language inter-faces.
Journal of Artificial Intelligence Research 18:1?44.Vogel, A. and D. Jurafsky.
2010.
Learning to follow nav-igational directions.
In Proceedings of the Associationfor Computational Linguistics.Walker, M. and R. Passonneau.
2001.
DATE: a dia-logue act tagging scheme for evaluation of spoken di-alogue systems.
In Proceedings of the First Inter-national Conference on Human Language TechnologyResearch.Walker, M., A. Rudnicky, R. Prasad, J. Aberdeen,E.
Bratt, J. Garofolo, H. Hastie, A.
Le, B. Pellom,A.
Potamianos, et al 2002.
DARPA Communicator:Cross-system results for the 2001 evaluation.
In Pro-ceedings of the International Conference on SpokenLanguage Processing.Wong, Y.W.
and R.J. Mooney.
2006.
Learning for se-mantic parsing with statistical machine translation.
InProceedings of the Human Language Technology Con-ference of the North American Association for Compu-tational Linguistics.Wong, Y.W.
and R.J. Mooney.
2007.
Learning syn-chronous grammars for semantic parsing with lambdacalculus.
In Proceedings of the Association for Com-putational Linguistics.Young, S., M. Gasic, S. Keizer, F. Mairesse, J. Schatz-mann, B. Thomson, and K. Yu.
2010.
The hiddeninformation state model: A practical framework forPOMDP-based spoken dialogue management.
Com-puter Speech & Language 24(2):150?174.Zelle, J.M.
and R.J. Mooney.
1996.
Learning to parsedatabase queries using inductive logic programming.In Proceedings of the National Conference on Artifi-cial Intelligence.Zettlemoyer, L.S.
and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Pro-ceedings of the Conference on Uncertainty in ArtificialIntelligence.Zettlemoyer, L.S.
and M. Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to logicalform.
In Proceedings of the Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning.Zettlemoyer, L.S.
and Michael Collins.
2009.
Learningcontext-dependent mappings from sentences to logicalform.
In Proceedings of the Joint Conference of theAssociation for Computational Linguistics and Inter-national Joint Conference on Natural Language Pro-cessing.432
