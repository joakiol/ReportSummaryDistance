High Precision Extraction of Grammatical RelationsJohn CarrollCognitive and Computing SciencesUniversity of SussexFalmer, BrightonBN1 9QH, UKTed BriscoeComputer LaboratoryUniversity of CambridgeJJ Thomson AvenueCambridge CB3 0FD, UKAbstractA parsing system returning analyses in the form ofsets of grammatical relations can obtain high pre-cision if it hypothesises a particular relation onlywhen it is certain that the relation is correct.
Weoperationalise this technique?in a statistical parserusing a manually-developed wide-coverage gram-mar of English?by only returning relations thatform part of all analyses licensed by the grammar.We observe an increase in precision from 75% toover 90% (at the cost of a reduction in recall) on atest corpus of naturally-occurring text.1 Introduction1Head-dependent relationships (possibly labelledwith a relation type) have been advocated as a use-ful level of representation for grammatical struc-ture in a number of different large-scale language-processing tasks.
For instance, in recent work onstatistical treebank grammar parsing (e.g.
Collins,1999) high levels of accuracy have been reachedusing lexicalised probabilistic models over head-dependent tuples.
Bouma, van Noord and Mal-ouf (2001) create dependency treebanks semi-auto-matically in order to induce dependency-based sta-tistical models for parse selection.
Lin (1998),Srinivas (2000) and others have evaluated the ac-curacy of both phrase structure-based and depen-dency parsers by matching head-dependent rela-tions against ?gold standard?
relations, rather thanmatching (labelled) phrase structure bracketings.Research on unsupervised acquisition of lexical in-formation from corpora, such as argument structureof predicates (Briscoe and Carroll, 1997; McCarthy,2000), word classes for disambiguation (Clark andWeir, 2001), and collocations (Lin 1999), has usedgrammatical relation/head/dependent tuples.
Such1A previous version of this paper was presented atIWPT?01; this version contains new experiments and results.tuples also constitute a convenient intermediate rep-resentation in applications such as information ex-traction (Palmer et al, 1993; Yeh, 2000), and docu-ment retrieval on the Web (Grefenstette, 1997).A variety of different approaches have been takenfor robust extraction of relation/head/dependent tu-ples, or grammatical relations, from unrestrictedtext.
Dependency parsing is a natural techniqueto use, and there has been some work in that areaon robust analysis and disambiguation (e.g.
Laf-ferty, Sleator and Temperley, 1992; Srinivas, 2000).Finite-state approaches (e.g.
Karlsson et al, 1995;A?
?t-Mokhtar and Chanod, 1997; Grefenstette, 1998)have used hand-coded transducers to recognise lin-ear configurations of words and part of speech la-bels associated with, for example, subject/object-verb relationships.
An intermediate step may be tomark nominal, verbal etc.
?chunks?
in the text and toidentify the head word of each of the chunks.
Sta-tistical finite-state approaches have also been used:Brants, Skut and Krenn (1997) train a cascade ofHidden Markov Models to tag words with theirgrammatical functions.
Approaches based on mem-ory based learning have also used chunking as afirst stage, before assigning grammatical relation la-bels to heads of chunks (Argamon, Dagan and Kry-molowski, 1998; Buchholz, Veenstra and Daele-mans, 1999).
Blaheta and Charniak (2000) assumea richer input representation consisting of labelledtrees produced by a treebank grammar parser, anduse the treebank again to train a further procedurethat assigns grammatical function tags to syntac-tic constituents in the trees.
Alternatively, a hand-written grammar can be used that produces ?shal-low?
and perhaps partial phrase structure analysesfrom which grammatical relations are extracted (e.g.Carroll, Minnen and Briscoe, 1998; Lin, 1998).Recently, Schmid and Rooth (2001) have de-scribed an algorithm for computing expected gov-ernor labels for terminal words in labelled headedparse trees produced by a probabilistic context-freegrammar.
A governor label (implicitly) encodes agrammatical relation type (such as subject or ob-ject) and a governing lexical head.
The labels areexpected in the sense that each is weighted by thesum of the probabilities of the trees giving rise toit, and are computed efficiently by processing theentire parse forest rather than individual trees.
Theset of terminal/relation/governing-head tuples willnot typically constitute a globally coherent analy-sis, but may be useful for interfacing to applicationsthat primarily accumulate fragments of grammati-cal information from text (such as for instance in-formation extraction, or systems that acquire lexicaldata from corpora).
The approach is not so suit-able for applications that need to interpret completeand consistent sentence structures (such as the anal-ysis phase of transfer-based machine translation).Schmid and Rooth have implemented the algorithmfor parsing with a lexicalised probabilistic context-free grammar of English and applied it in an opendomain question answering system, but they do notgive any practical results or an evaluation.In the paper we investigate empirically Schmidand Rooth?s proposals, using a wide-coverage pars-ing system applied to a test corpus of naturally-occurring text, extend it with various thresholdingtechniques, and observe the trade-off between pre-cision and recall in grammatical relations returned.Using the most conservative threshold results in aparser that returns only grammatical relations thatform part of all analyses licensed by the grammar.In this case, precision rises to over 90%, as com-pared with a baseline of 75%.2 The Analysis SystemIn this investigation we extend a statistical shallowparsing system for English developed originally byCarroll, Minnen and Briscoe (1998).
Briefly, thesystem works as follows: input text is labelled withpart-of-speech (PoS) tags by a tagger, and theseare parsed using a wide-coverage unification-based?phrasal?
grammar of English PoS tags and punctu-ation.
For disambiguation, the parser uses a prob-abilistic LR model derived from parse tree struc-tures in a treebank, augmented with a set of lexicalentries for verbs, acquired automatically from a 10million word sample of the British National Corpus(Leech, 1992), each entry containing subcategori-sation frame information and an associated proba-bility.
The parser is therefore ?semi-lexicalised?
inthat verbal argument structure is disambiguated lex-ically, but the rest of the disambiguation is purelystructural.The coverage of the grammar?the proportion ofsentences for which at least one complete spanninganalysis is found?is around 80% when applied tothe SUSANNE corpus (Sampson, 1995).
In addition,the system is able to perform parse failure recov-ery, finding the highest scoring sequence of phrasalfragments (following the approach of Kiefer et al,1999), and the system has produced at least partialanalyses for over 98% of the sentences in the writtenpart of the British National Corpus.The parsing system reads off grammatical rela-tion tuples (GRs) from the constituent structure treethat is returned from the disambiguation phase.
In-formation is used about which grammar rules in-troduce subjects, complements, and modifiers, andwhich daughter(s) is/are the head(s), and which thedependents.
In Carroll et al?s evaluation the systemachieves GR accuracy that is comparable to pub-lished results for other systems: extraction of non-clausal subject relations with 83% precision, com-pared with Grefenstette?s (1998) figure of 80%; andoverall F-score2 of unlabelled head-dependent pairsof 80%, as opposed to Lin?s (1998) 83%3 and Srini-vas?s (2000) 84% (this with respect only to binaryrelations, and omitting the analysis of control rela-tionships).
Blaheta and Charniak (2000) report anF-score of 87% for assigning grammatical functiontags to constituents, but the task, and therefore thescoring method, is rather different.For the work reported in this paper we have ex-tended Carroll et al?s basic system, implementinga version of Schmid and Rooth?s expected gover-nor technique (see section 1 above) but adapted forunification-based grammar and GR-based analyses.Each sentence is analysed as a set of weighted GRswhere the weight associated with each grammati-cal relation is computed as the sum of the proba-bilities of the parses that relation was derived from,divided by the sum of the probabilities of all parses.So, if we assume that Schmid and Rooth?s examplesentence Peter reads every paper on markup has 2parses, one where on markup attaches to the preced-ing noun having overall probability     and theother where it has verbal attachment with probabil-ity     , then some of the weighted GRs would be2We use the F 	 measure defined asfiffflffi "!#!%$"&'()*ffiff,+-ffi) "!#!%.
.3Our calculation, based on table 2 of Lin (1998).1.0 ncsubj(reads, Peter, )0.7 ncmod(on, paper, markup)0.3 ncmod(on, reads, markup)Figure 1 contains a more extended example of aweighted GR analysis for a short sentence from theSUSANNE corpus, and also gives a flavour of the re-lation types that the system returns.
The GR schemeis decribed in detail by Carroll, Briscoe and Sanfil-ippo (1998).3 Empirical Results3.1 Weight ThresholdingOur first experiment compared the accuracy of theparser when extracting GRs from the highest rankedanalysis (the standard probabilistic parsing setup)against extracting weighted GRs from all parses inthe forest.
To measure accuracy we use the pre-cision, recall and F-score measures of parser GRsagainst ?gold standard?
GR annotations in a 10,000-word test corpus of in-coverage sentences derivedfrom the SUSANNE corpus and covering a range ofwritten genres4.
GRs are in general compared us-ing an equality test, except that in a specific, limitednumber of cases (described by Carroll, Minnen andBriscoe, 1998) the parser is allowed to return moregeneric relation types.When a parser GR has a weight of less than one,we proportionally discount its contribution to theprecision and recall scores.
Thus, given a set /of GRs with associated weights produced by theparser, i.e./ 0 132465785:9<;465>=@?BACEDGFGDH=%ICAKJ?
?LMN=JADHOFP=%ACRQTSU8)57FPCVDNWD YX465[Z]\(^and a set _ of gold-standard (unweighted) GRs, wecompute the weighted match between _ and the el-ements of / as`0 abdcfehg iejhk"l45m285Kn_o9where m 2pq9r0s\ if p is true and   otherwise.
Theweighted precision and recall are then`tbucfevg ie*jhkwl465JxVO`;'_P;respectively, expressed as percentages.
We arenot aware of any previous published work using4At http://www.cogs.susx.ac.uk/lab/nlp/carroll/greval.html.Table 1: GR accuracy comparing extraction fromjust the highest-ranked parse compared to weightedGR extraction from all parses.Precision (%) Recall (%) F-scoreBest parse 76.25 76.77 76.51All parses 74.63 75.33 74.98weighted precision and recall measures, althoughthere is an option for associating weights with com-plete parses in the distributed software implement-ing the PARSEVAL scheme (Harrison et al, 1991)for evaluating parser accuracy with respect to phrasestructure bracketings.
The weighted measures makesense for application tasks that can deal with sets ofmutually-inconsistent GRs.In this initial experiment, precision and recallwhen extracting weighted GRs from all parses wereboth one and a half percentage points lower thanwhen GRs were extracted from just the highestranked analysis (see table 1)5.
This decrease inaccuracy might be expected, though, given that atrue positive GR may be returned with weight lessthan one, and so will not receive full credit from theweighted precision and recall measures.However, these results only tell part of the story.An application using grammatical relation analysesmight be interested only in GRs that the parser isfairly confident of being correct.
For instance, in un-supervised acquisition of lexical information (suchas subcategorisation frames for verbs) from text, theusual methodology is to (partially) analyse the text,retaining only reliable hypotheses which are thenfiltered based on the amount of evidence for themover the corpus as a whole.
Thus, Brent (1993)only creates hypotheses on the basis of instancesof verb frames that are reliably and unambiguouslycued by closed class items (such as pronouns) sothere can be no other attachment possibilities.
In re-cent work on unsupervised learning of prepositionalphrase disambiguation, Pantel and Lin (2000) derivetraining instances only from relevant data appearingin syntactic contexts that are guaranteed to be unam-biguous.
In our system, the weights on GRs indicatehow certain the parser is of the associated relationsbeing correct.
We therefore investigated whethermore highly weighted GRs are in fact more likely5Ignoring the weights on GRs, standard (unweighted) eval-uation results for all parses are: precision 36.65%, recall89.42% and F-score 51.99.1.0 aux( , continue, will) 0.4490 iobj(on, place, tax-payers)1.0 detmod( , burden, a) 0.3276 ncmod(on, burden, tax-payers)1.0 dobj(do, this, ) 0.2138 ncmod(on, place, tax-payers)1.0 dobj(place, burden, ) 0.0250 xmod(to, continue, place)1.0 ncmod( , burden, disproportionate) 0.0242 ncmod( , Fulton, tax-payers)1.0 ncsubj(continue, Failure, ) 0.0086 obj2(place, tax-payers)1.0 ncsubj(place, Failure, ) 0.0086 ncmod(on, burden, Fulton)1.0 xcomp(to, Failure, do) 0.0020 mod( , continue, place)0.9730 clausal(continue, place) 0.0010 ncmod(on, continue, tax-payers)0.9673 ncmod( , tax-payers, Fulton)Figure 1: Weighted GRs for the sentence Failure to do this will continue to place a disproportionate burdenon Fulton taxpayers.1007550Recall(%)50 75 100Precision (%)yyyzzThreshold=0{{{ |Threshold=1 }}}}E~Figure 2: Weighted GR accuracy as the threshold isvaried.to be correct than ones with lower weights.
We didthis by setting a threshold on the output, such thatany GR with weight lower than the threshold is dis-carded.Figure 2 plots weighted recall and precision asthe threshold is varied between zero and one Theresults are intriguing.
Precision increases monoton-ically from 74.6% at a threshold of zero (the situ-ation as in the previous experiment where all GRsextracted from all parses in the forest are returned)to 90.4% at a threshold of one.
(The latter thresh-old has the effect of allowing only those GRs thatform part of every single analysis to be returned).The influence of the threshold on recall is equallydramatic, although since we have not escaped theusual trade-off with precision the results are some-what less positive.
Recall decreases from 75.3%to 45.2%, initially rising slightly, then falling at agradually increasing rate.
Between thresholds 0.99and 1.0 there is only a two percentage point differ-ence in precision, but recall differs by almost four-teen percentage points6.
Over the whole range, asthe threshold is increased from zero, precision risesfaster than recall falls until the threshold reaches0.65; here the F-score attains its overall maximumof 77.It turns out that the eventual figure of over 90%precision is not due to ?easier?
relation types (suchas the dependency between a determiner and anoun) being returned and more difficult ones (forexample clausal complements) being ignored.
Themajority of relation types are produced with fre-quency consistent with the overall 45% recall fig-ure.
Exceptions are arg mod (encoding the Englishpassive ?by-phrase?)
and iobj (indirect object), forwhich no GRs at all are produced.
The reason forthis is that both types of relation originate froman occurrence of a prepositional phrase in contextswhere it could be either a modifier or a complementof a predicate.
This pervasive ambiguity means thatthere will always be disagreement between analysesover the relation type (but not necessarily over theidentity of the head and dependent themselves).3.2 Parse UnpackingSchmid and Rooth?s algorithm computes expectedgovernors efficiently by using dynamic program-ming and processing the entire parse forest ratherthan individual trees.
In contrast, we unpack thewhole parse forest and then extract weighted GRsfrom each tree individually.
Our implementationis certainly less elegant, but in practical terms for6Roughly, each percentage point increase or decrease inprecision and recall is statistically significant at the 95% level.In this and all significance tests in this paper we use a one-tailedpaired t-test (with 499 degrees of freedom).sentences where there are relatively small numbersof parses the speed is still acceptable.
However,throughput goes down linearly with the numberof parses, and when there are many thousands ofparses?and particularly also when the sentence islong and so each tree is large?the parsing systembecomes unacceptably slow.One possibility to improve the situation would beto extract GRs directly from forests.
At first glancethis looks a possibility: although our parse forestsare produced by a probabilistic LR parser using aunification-based grammar, they are similar in con-tent to those computed by a probabilistic context-free grammar, as assumed by Schmid and Rooth?salgorithm.
However, there are problems.
If the testfor being able to pack local ambiguities in the unifi-cation grammar parse forest is feature structure sub-sumption, unpacking a parse apparently encoded inthe forest can fail due to non-local inconsistency infeature values (Oepen and Carroll, 2000)7, so everygovernor tuple hypothesis would have to be checkedto ensure that the parse it came from was globallyvalid.
It is likely that this verification step wouldcancel out the efficiency gained from using an algo-rithm based on dynamic programming.
This prob-lem could be side-stepped (but at the cost of lesscompact parse forests) by instead testing for featurestructure equivalence rather than subsumption.
Asecond, more serious problem is that some of our re-lation types encode more information than is presentin a single governor tuple (the non-clausal subjectrelation, for instance, encoding whether the surfacesubject is the ?deep?
object in a passive construc-tion); this information can again be less local andviolate the conditions required for the dynamic pro-gramming approach.Another possibility is to compute only the  high-est ranked parses and extract weighted GRs fromjust those.
The basic case where ?0?\ is equivalentto the standard approach of computing GRs fromthe highest probability parse.
Table 2 shows the ef-fect on accuracy as  is increased in stages to \     ,using a threshold for GR extraction of \ ; also shownis the previous setup (labelled ?unlimited?)
in whichall parses in the forest are considered.8 (All differ-ences in precision in the table are significant to atleast the 95% level, except between \     parses and7The forest therefore also ?leaks?
probability mass since itcontains derivations that are in fact not legal.8At ff???????
parses, the (unlabelled) weighted precisionof head-dependent pairs is 91.0%.Table 2: Weighted GR accuracy using a thresholdof 1, with respect to the maximum number ofranked parses considered.Maximum Precision Recall F-scoreParses (%) (%)1 76.25 76.77 76.512 80.15 73.30 76.575 84.94 67.03 74.9310 86.73 62.47 72.63100 89.59 51.45 65.361000 90.24 46.08 61.00unlimited 90.40 45.21 60.27an unlimited number).
The results demonstrate thatlimiting processing to a relatively small, fixed num-ber of parses?even as low as 100?comes withina small margin of the accuracy achieved using thefull parse forest.
These results are striking, in viewof the fact that the grammar assigns more than   parses to over a third of the sentences in the testcorpus, and more than a thousand parses to a fifth ofthem.
Another interesting observation is that the re-lationship between precision and recall is very closeto that seen when the threshold is varied (as in theprevious section); there appears to be no loss in re-call at a given level of precision.
We therefore feelconfident in unpacking a limited number of parsesfrom the forest and extracting weighted GRs fromthem, rather than trying to process all parses.
Wehave tentatively set the limit to be \     , as a reason-able compromise in our system between throughputand accuracy.3.3 Parse WeightingThe way in which the GR weighting is carried outdoes not matter when the weight threshold is equalto 1 (since then only GRs that are part of every anal-ysis are returned, each with a weight of one).
How-ever, we wanted to see whether the precise methodfor assigning weights to GRs has an effect on accu-racy, and if so, to what extent.
We therefore tried analternative approach where each GR receives a con-tribution of 1 from every parse, no matter what theprobability of the parse is, normalising in this caseby the number of parses considered.
This tends toincrease the numbers of GRs returned for any giventhreshold, so when comparing the two methods wefound thresholds such that each method obtained thesame precision figure (of roughly 83.38%).
We thencompared the recall figures (see table 3).
The recallTable 3: Accuracy at the same level of precision us-ing different weighting methods, with a 1000-parsetree limit.Weighting Precision Recall F-scoreMethod (%) (%)Probabilistic (at 88.38 59.19 70.90threshold 0.99)Equally (at 88.39 55.17 67.94threshold 0.768)for the probabilistic weighting scheme is 4% higher(statistically significant at the 99.95% level).3.4 Maximal Consistent Relation SetsIt is interesting to see what happens if we com-pute for each sentence the maximal consistent set ofweighted GRs.
(We might want to do this if we wantcomplete and coherent sentence analyses, interpret-ing the weights as confidence measures over sub-analysis segments).
We use a ?greedy?
algorithm tocompute consistent relation sets, taking GRs sortedin order of decreasing weight and adding a GR tothe set if and only if there is not already a GR inthe set with the same dependent.
(But note thatthe correct analysis may in fact contain more thanone GR with the same dependent, such as the nc-subj ... Failure GRs in Figure 1, and in these casesthis method will introduce errors).
The weightedprecision, recall and F-score at threshold zero are79.31%, 73.56% and 76.33 respectively.
Precisionand F-score are significantly better (at the 95.95%level) than the baseline.3.5 Parser BootstrappingOne of our primary research goals is to explore un-supervised acquisition of lexical knowledge.
Theparser we use in this work is ?semi-lexicalised?,using subcategorisation probabilities for verbs ac-quired automatically from (unlexicalised) parses.
Inthe future we intend to acquire other types of lexico-statistical information (for example on PP attach-ment) which we will feed back into the parser?s dis-ambiguation procedure, bootstrapping successivelymore accurate versions of the parsing system.
Thereis still plenty of scope for improvement in accu-racy, since compared with the number of correctGRs in top-ranked parses there are roughly a fur-ther 20% that are correct but present only in lower-ranked parses.
There appears to be less room forimprovement with argument relations (ncsubj, dobjetc.)
than with modifier relations (ncmod and sim-ilar).
This indicates that our next efforts should bedirected to collecting information on modification.4 Discussion and Further WorkWe have extended a shallow parsing system for En-glish that returns analyses in the form of sets ofgrammatical relations, presenting an investigationinto the extraction of weighted relations from prob-abilistic parses.
We observed that setting a thresh-old on the output such that any relation with weightlower than the threshold is discarded allows a trade-off to be made between recall and precision, andfound that by setting the threshold at 1 the preci-sion of the system was boosted dramatically, froma baseline of 75% to over 90%.
With this setting,the system returns only relations that form part ofall analyses licensed by the grammar: the systemcan have no greater certainty that these relations arecorrect, given the knowledge that is available to it.Although we believe this technique to be wellsuited to probabilistic parsers, it could also poten-tially benefit any parsing system that can repre-sent ambiguity and return analyses that are com-posed of a collection of elementary units.
Sucha system need not necessarily be statistical, sinceparse probabilities make no difference when check-ing that a given sub-analysis segment forms partof all possible global analyses.
Moreover, a non-statistical parsing system could use the the tech-nique to construct a reliable annotated corpus au-tomatically, which it could then be trained on.AcknowledgementsWe are grateful to Mats Rooth for early discus-sions about his expected governor label work.
Thisresearch was supported by UK EPSRC projectsGR/N36462/93 ?Robust Accurate Statistical Parsing(RASP)?
and by EU FP5 project IST-2001-34460?MEANING: Developing Multilingual Web-scaleLanguage Technologies?.ReferencesA?
?t-Mokhtar, S. and J-P. Chanod (1997) Subject and ob-ject dependency extraction using finite-state transduc-ers.
In Proceedings of the ACL/EACL?97 Workshopon Automatic Information Extraction and Building ofLexical Semantic Resources, 71?77.
Madrid, Spain.Argamon, S., I. Dagan and Y. Krymolowski (1998) Amemory-based approach to learning shallow naturallanguage patterns.
In Proceedings of the 36th An-nual Meeting of the Association for ComputationalLinguistics, 67?73.
Montreal.Blaheta, D. and E. Charniak (2000) Assigning functiontags to parsed text.
In Proceedings of the 1st Con-ference of the North American Chapter of the ACL,234?240.
Seattle, WA.Bouma, G., G. van Noord and R. Malouf (2001)Alpino: wide-coverage computational analysis ofDutch.
Computational Linguistics in the Netherlands2000.
Selected Papers from the 11th CLIN Meeting.Brants, T., W. Skut and B. Krenn (1997) Tagging gram-matical functions.
In Proceedings of the 2nd Confer-ence on Empirical Methods in Natural Language Pro-cessing, 64?74.
Providence, RI.Brent, M. (1993) From grammar to lexicon: unsuper-vised learning of lexical syntax.
Computational Lin-guistics, 19(3), 243?262.Briscoe, E. and J. Carroll (1997) Automatic extractionof subcategorization from corpora.
In Proceedings ofthe 5th ACL Conference on Applied Natural LanguageProcessing, 356?363.
Washington, DC.Buchholz, S., J. Veenstra and W. Daelemans (1999) Cas-caded grammatical relation assignment.
In Proceed-ings of the Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing and VeryLarge Corpora, College Park, MD.
239?246.Carroll, J., E. Briscoe and A. Sanfilippo (1998) Parserevaluation: a survey and a new proposal.
In Proceed-ings of the 1st International Conference on LanguageResources and Evaluation, 447?454.
Granada, Spain.Carroll, J., G. Minnen and E. Briscoe (1998) Can sub-categorisation probabilities help a statistical parser?.In Proceedings of the 6th ACL/SIGDAT Workshop onVery Large Corpora.
Montreal, Canada.Clark, S. and D. Weir (2001) Class-based probability es-timation using a semantic hierarchy.
In Proceedingsof the 2nd Conference of the North American Chapterof the ACL.
Pittsburgh, PA.Collins, M. (1999) Head-driven statistical models fornatural language parsing.
PhD thesis, University ofPennsylvania.Grefenstette, G. (1997) SQLET: Short query linguisticexpansion techniques, palliating one-word queries byproviding intermediate structure to text.
In Proceed-ings of the RIAO?97, 500?509.
Montreal, Canada.Grefenstette, G. (1998) Light parsing as finite-state filter-ing.
In A. Kornai (Eds.
), Extended Finite State Modelsof Language.
Cambridge University Press.Harrison, P., S. Abney, E. Black, D. Flickinger, C.Gdaniec, R. Grishman, D. Hindle, B. Ingria, M. Mar-cus, B. Santorini, & T. Strzalkowski (1991) Evalu-ating syntax performance of parser/grammars of En-glish.
In Proceedings of the ACL?91 Workshop onEvaluating Natural Language Processing Systems,71?78.
Berkeley, CA.Karlsson, F., A. Voutilainen, J. Heikkila?
and A. Anttila(1995) Constraint Grammar: a Language-Independ-ent System for Parsing Unrestricted Text.
Berlin, Ger-many: de Gruyter.Kiefer, B., H-U.
Krieger, J. Carroll and R. Malouf (1999)A bag of useful techniques for efficient and robustparsing.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics, 473?480.
University of Maryland.Lafferty, J., D. Sleator and D. Temperley (1992) Gram-matical trigrams: A probabilistic model of link gram-mar.
In Proceedings of the AAAI Fall Symposium onProbabilistic Approaches to Natural Language, 89?97.
Cambridge, MA.Leech, G. (1992) 100 million words of English: theBritish National Corpus.
Language Research, 28(1),1?13.Lin, D. (1998) Dependency-based evaluation of MINI-PAR.
In Proceedings of the The Evaluation of Pars-ing Systems: Workshop at the 1st InternationalConference on Language Resources and Evaluation.Granada, Spain (also available as University of Sus-sex technical report CSRP-489).Lin, D. (1999) Automatic identification of non-compositional phrases.
In Proceedings of the 37thAnnual Meeting of the Association for ComputationalLinguistics, 317?324.
College Park, MD.McCarthy, D. (2000) Using semantic preferences toidentify verbal participation in role switching alter-nations.
In Proceedings of the 1st Conference of theNorth American Chapter of the ACL, 256?263.
Seat-tle, WA.Oepen, S. and J. Carroll (2000) Ambiguity packing inconstraint-based parsing ?
practical results.
In Pro-ceedings of the 1st Conference of the North AmericanChapter of the ACL, 162?169.
Seattle, WA.Palmer, M., R. Passonneau, C. Weir and T. Finin (1993)The KERNEL text understanding system.
ArtificialIntelligence, 63, 17?68.Pantel, P. and D. Lin (2000) An unsupervised approachto prepositional phrase attachment using contextuallysimilar words.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Lin-guistics, 101?108.
Hong Kong.Sampson, G. (1995) English for the Computer.
OxfordUniversity Press.Schmid, H. and M. Rooth (2001) Parse forest computa-tion of expected governors.
In Proceedings of the 39thAnnual Meeting of the Association for ComputationalLinguistics, 458?465.
Toulouse, France.Srinivas, B.
(2000) A lightweight dependency analyzerfor partial parsing.
Natural Language Engineering,6(2), 113?138.Yeh, A.
(2000) Using existing systems to supplementsmall amounts of annotated grammatical relationstraining data.
In Proceedings of the 38th Annual Meet-ing of the Association for Computational Linguistics,126?132.
Hong Kong.
