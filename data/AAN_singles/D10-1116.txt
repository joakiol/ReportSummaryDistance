Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1194?1203,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsPractical Linguistic Steganography using Contextual Synonym Substitutionand Vertex Colour CodingChing-Yun ChangUniversity of CambridgeComputer LaboratoryChing-Yun.Chang@cl.cam.ac.ukStephen ClarkUniversity of CambridgeComputer LaboratoryStephen.Clark@cl.cam.ac.ukAbstractLinguistic Steganography is concerned withhiding information in natural language text.One of the major transformations used in Lin-guistic Steganography is synonym substitu-tion.
However, few existing studies have stud-ied the practical application of this approach.In this paper we propose two improvementsto the use of synonym substitution for encod-ing hidden bits of information.
First, we usethe Web 1T Google n-gram corpus for check-ing the applicability of a synonym in context,and we evaluate this method using data fromthe SemEval lexical substitution task.
Second,we address the problem that arises from wordswith more than one sense, which creates a po-tential ambiguity in terms of which bits areencoded by a particular word.
We develop anovel method in which words are the verticesin a graph, synonyms are linked by edges, andthe bits assigned to a word are determined bya vertex colouring algorithm.
This methodensures that each word encodes a unique se-quence of bits, without cutting out large num-ber of synonyms, and thus maintaining a rea-sonable embedding capacity.1 IntroductionSteganography is concerned with hiding informa-tion in a cover medium, in order to facilitate covertcommunication, such that the presence of the infor-mation is imperceptible to a user (human or com-puter).
Much of the existing research in steganog-raphy has used images as cover media; however,given the ubiquitous nature of electronic text, inter-est is growing in using natural language as the covermedium.
Linguistic Steganography?lying at the in-tersection of Computational Linguistics and Com-puter Security?is concerned with making changesto a cover text in order to embed information, in sucha way that the changes do not result in ungrammati-cal or unnatural text.A related area is natural language watermarking,in which changes are made to a text in order to iden-tify it, for example for copyright purposes.
An inter-esting watermarking application is ?traitor tracing?,in which documents are changed in order to embedindividual watermarks.
These marks can then beused to later identify particular documents, for ex-ample if a set of documents?identical except forthe changes used to embed the watermarks?
havebeen sent to a group of individuals, and one of thedocuments has been leaked to a newspaper.In terms of security, a linguistic stegosystemshould impose minimum embedding distortion tothe cover text so that the resulting stegotext in whicha message is camouflaged is inconspicuous, result-ing in high imperceptibility.
In addition, sincesteganography aims at covert communication, a lin-guistic stegosystem should allow sufficient embed-ding capacity, known as the payload.
There is a fun-damental tradeoff between imperceptibility and pay-load, since any attempt to embed more informationvia changes to the cover text increases the chanceof introducing anomalies into the text and thereforeraising the suspicion of an observer.A linguistic transformation is required to em-bed information.
Transformations studied in pre-vious work include lexical substitution (Chapmanand Davida, 1997; Bolshakov, 2004; Taskiran et al,2006; Topkara et al, 2006b), phrase paraphrasing(Chang and Clark, 2010), sentence structure manip-ulations (Atallah et al, 2001a; Atallah et al, 2001b;1194Liu et al, 2005; Meral et al, 2007; Murphy, 2001;Murphy and Vogel, 2007; Topkara et al, 2006a) andsemantic transformations (Atallah et al, 2002; Vy-bornova and Macq, 2007).
Many of these transfor-mations require some sophisticated NLP tools; forexample, in order to perform semantic transforma-tions on text, word sense disambiguation, seman-tic role parsing and anaphora resolution tools maybe required.
However, the current state-of-the-art inlanguage technology is arguably not good enoughfor secure linguistic steganography based on sophis-ticated semantic transformations, and the level of ro-bustness required to perform practical experimentshas only just become available.
Hence many exist-ing linguistic stegosystems are proof-of-concept im-plementations with little practical evaluation of theimperceptibility or payload.1.1 Synonym substitutionSynonym substitution is a relatively straightforwardlinguistic steganography method.
It substitutes se-lected words with the same part of speech (PoS) syn-onyms, and does not involve operating on the sen-tence structure so the modification can be guaran-teed to be grammatical.
Another advantage of thismethod is that many languages are profuse in syn-onyms, and so there is a rich source of informationcarriers compared with other text transformations.There are two practical difficulties associated withhiding bits using synonym subsitution.
The first isthat words can have more than one sense.
In terms ofWordNet (Fellbaum, 1998), which is the electronicdictionary we use, words can appear in more thanone synset.
This is a problem because a word may beassigned different bit strings in the different synsets,and the receiver does not know which of the sensesto use, and hence does not know which hidden bitstring to recover.
Our solution to this problem is anovel vertex colouring method which ensures thatwords are always assigned the same bit string, evenwhen they appear in different synsets.The second problem is that many synonyms areonly applicable in certain contexts.
For example, thewords in the WordNet synset {bridge, span} sharethe meaning of ?a structure that allows people or ve-hicles to cross an obstacle such as a river or canalor railway etc.?.
However, bridge and span cannotbe substutited for each other in the sentence ?sus-Figure 1: An example of the basic algorithmpension bridges are typically ranked by the lengthof their main span?, and doing so would likely raisethe suspicion of an observer due to the resultinganomaly in the text.Our solution to this problem is to perform a con-textual check which utilises the Web 1T n-gramGoogle n-gram corpus.1 We evaluate the methodusing the data from the English Lexical Substitu-tion task for SemEval-2007.2 The resulting preci-sion of our lexical substitution system can be seenas an indirect measure of the imperceptibility of thestegosystem, whereas the recall can be seen as anindirect measure of the payload.The paper is organised so that the contextualcheck is described first, and this is evaluated inde-pendently of the steganographic application.
Thenthe vertex colouring method is presented, and finallywe show how the contextual check can be integratedwith the vertex colouring coding method to give acomplete stegsosystem.
For readers unfamiliar withlinguistic steganogaphy, Section 2 has some exam-ples of how bits can be hidden using textual trans-formations.
Also, Chang and Clark (2010) is a re-cent NLP paper which describes the general linguis-tic steganography framework.2 Related WorkIn the original work on linguistic steganography inthe late 1990s, Winstein proposed an informationhiding algorithm using a block coding method to en-code synonyms, so that the selection of a word froma synset directly associates with part of the secretbitstring (Bergmair, 2007).
Figure 1 illustrates theembedding procedure of this approach.
In this ex-ample, the bitstring to be embedded is 010, which1www.ldc.upenn.edu/Catalog/docs/LDC2006T13/readme.txt2http://www.dianamccarthy.co.uk/task10index.html1195Figure 2: An example of applying the basic algorithm tooverlapping synsetscan be divided into two codewords, 0 and 10, and theinformation carriers in the cover text are the wordsfinished and project.
According to the encoding dic-tionary, complete represents 0, and task represents10; hence these words are chosen and replace theoriginal words in the cover text (with suitable suffix-ation).
The stego sentence ?He completed the task?is then sent to the receiver.
In order to recover themessage, the receiver only needs a copy of the en-coding dictionary, and the decoding algorithm sim-ply reverses the process used to encode the hiddenbits.
Note that the receiver does not need the origi-nal cover text to recover the information.This algorithm requires synonym sets to be dis-joint; i.e.
no word may appear in more than one syn-onym set, since overlapping synsets may cause am-biguities during the decoding stage.
Figure 2 showswhat happens when the basic algorithm is applied totwo overlapping synonym sets.
As can be seen fromthe example, composition is represented by two dif-ferent codewords and thus the secret bitstring can-not be reliably recovered, since the receiver does notknow the original cover word or the sense of theword.
In order to solve this problem, we proposea novel coding method based on vertex colouring,described in Section 4.In addition to the basic algorithm, Winstein pro-posed the T-Lex system using synonym substitutionas the text transformation.
In order to solve theproblem of words appearing in more than one syn-onym set, Winstein defines interchangeable wordsas words that belong to the same synsets, and onlyuses these words for substitution.
Any words that arenot interchangeable are discarded and not availablefor carrying information.
The advantage in this ap-proach is that interchangeable words always receivethe same codeword.
The disadvantage is that manysynonyms need to be discarded in order to achievethis property.
Winstein calculates that only 30% ofWordNet can be used in such a system.Another stegosystem based on synonym substi-tution was proposed by Bolshakov (2004).
In or-der to ensure both sender and receiver use thesame synsets, Bolshakov applied transitive closureto overlapping synsets to avoid the decoding ambi-guity.
Applying transitive closure leads to a mergerof all the overlapping synsets into one set which isthen seen as the synset of a target word.
Considerthe overlapping synsets in Figure 2 as an example.After applying transitive closure, the resulting setis {?authorship?, ?composition?, ?paper?, ?penning?,?report?, ?theme?, ?writing?
}.Bolshakov (2004) also uses a method to deter-mine whether a substitution is applicable in context,using a collocation-based test.
Finally, the colloca-tionally verified synonyms are encoded by using theblock coding method.
This is similar to our use ofthe Google n-gram data to check for contextual ap-plicability.The disadvantage of Bolshakov?s system is thatall words in a synonym transitive closure chain needto be considered, which can lead to very large setsof synonyms, and many which are not synonymouswith the original target word.
In contrast, our pro-posed method operates on the original synonym setswithout extending them unnecessarily.3 Proposed Method and Experiments3.1 ResourcesWe use WordNet (Fellbaum, 1998) to provide setsof synonyms (synsets) for nouns, verbs, adjectivesand adverbs.
Since the purpose of using WordNet isto find possible substitutes for a target word, thosesynsets containing only one entry are not useful andare ignored by our stegosystem.
In addition, ourstegosystem only takes single word substitution intoconsideration in order to avoid the confusion of find-ing information-carrying words during the decodingphase.
For example, if the cover word ?complete?
isreplaced by ?all over?, the receiver would not knowwhether the secret message is embedded in the word?over?
or the phrase ?all over?.
Table 1 shows thestatistics of synsets used in our stegosystem.1196noun verb adj adv# of synsets 16,079 4,529 6,655 964# of entries 30,933 6,495 14,151 2,025average set size 2.56 2.79 2.72 2.51max set size 25 16 21 8Table 1: Statistics of synsets used in our stegosystemFor the contextual check we use the Google Web1T 5-gram Corpus (Brants and Franz, 2006) whichcontains counts for n-grams from unigrams throughto five-grams obtained from over 1 trillion word to-kens of English Web text.
The corpus has been usedfor many tasks such as spelling correction (Islam andInkpen, 2009; Carlson et al, 2008) and multi-wordexpression classification (Kummerfeld and Curran,2008).
Moreover, for the SemEval-2007 EnglishLexical Substitution Task, which is similar to oursubstitution task, six out of ten participating teamsutilised the Web 1T corpus.3.2 Synonym Checking MethodIn order to measure the degree of acceptability in asubstitution, the proposed filter calculates a substi-tution score for a synonym by using the observedfrequency counts in the Web n-gram corpus.
Themethod first extracts contextual n-grams around thesynonym and queries the n-gram frequency countsfrom the corpus.
For each n, the total count fn is cal-culated by summing up individual n-gram frequen-cies, for every contextual n-gram containing the tar-get word.
We define a count function Count(w) =?5n=2 log(fn) where log(0) is defined as zero.
IfCount(w) = 0, we assume the word w is unrelatedto the context and therefore is eliminated from con-sideration.
We then find the maximum Count(w)called max from the remaining words.
The mainpurpose of having max is to score each word rela-tive to the most likely synonym in the group, so evenin less frequent contexts which lead to smaller fre-quency counts, the score of each synonym can stillindicate the degree of feasibility.
The substitutionscore is defined as Score(w) = Count(w)?max.The hypothesis is that a word with a high score ismore suitable for the context, and we apply a thresh-old so that synonyms having a score lower than thethreshold are discarded.Figure 3 demonstrates an example of calculat-f2=525,856 high pole 3,544pole .
522,312f3=554 very high pole 84high pole .
470f4=72 a very high pole 72very high pole .
0f5=0 not a very high pole 0a very high pole .
0Count(?pole?)=log(f2)+log(f3)+log(f4)+log(f5)=23Score(?pole?)=Count(?pole?
)/max=0.44>0.37Figure 3: An example of using the proposed synonymchecking methoding the substitution score for the synonym ?pole?given the cover sentence ?This is not a very highbar.?
First of all, various contextual n-grams are ex-tracted from the sentence and the Web n-gram cor-pus is consulted to obtain their frequency counts.Count(?pole?)
is then calculated using the n-gramfrequencies.
Suppose the threshold is 0.37, and themax score is 52.
Since Count(?pole?)
is greaterthan zero and the substitution score Score(?pole?
)is 0.44, the word ?pole?
is determined as acceptablefor this context (even though it may not be, depend-ing on the meaning of ?bar?
in this case).3.3 Evaluation DataIn order to evaluate the proposed synonym check-ing method, we need some data to test whether ourmethod can pick out acceptable substitutions.
TheEnglish Lexical Substitution task for SemEval-2007has created human-annotated data for developingsystems that can automatically find feasible substi-tutes given a target word in context.
This data com-prises 2010 sentences selected from the English In-ternet Corpus3, and consists of 201 words: nouns,verbs, adjectives and adverbs each with ten sen-tences containing that word.
The five annotatorswere asked to provide up to three substitutes for atarget word in the context of a sentence, and werepermitted to consult a dictionary or thesaurus oftheir choosing.We use the sentences in this gold standard as thecover text in our experiments so that the substi-tutes provided by the annotators can be the positivedata for evaluating the proposed synonym check-3http://corpus.leeds.ac.uk/internet.html1197noun verb adj adv# of target words 59 54 57 35# of sentences 570 527 558 349# of positives 2,343 2,371 2,708 1,269# of negatives 1,914 1,715 1,868 884Table 2: Statistics of experimental dataing methods.
Since we only take into considera-tion the single word substitutions for the reason de-scribed earlier, multi-word substitutes are removedfrom the positive data.
Moreover, we use Word-Net as the source of providing candidate substitutesin our stegosystem, so if a human-provided sub-stitute does not appear in any synsets of its targetword in WordNet, there is no chance for our sys-tem to replace the target word with the substitute andtherefore, the substitute can be eliminated.
Table 2presents the statistics of the positive data for our ex-periments.Apart from positive data, we also need some neg-ative data to test whether our method has the abilityto filter out bad substitutions.
Since the annotatorswere allowed to refer to a dictionary or thesaurus,we assume that annotators used WordNet as one ofthe reference resources while generating candidates.Hence we assume that, if a word in the correct synsetfor a target word is not in the set produced by the hu-man annotators, then it is inappropriate for that con-text and a suitable negative example.
This method isappropriate because our steganography system hasto distinguish between good and bad synonyms fromWordNet, given a particular context.For the above reasons, we extract the negativedata for our experiments by first matching positivesubstitutes of a target word to all the synsets thatcontain the target word in WordNet.
The synsetthat includes the most positive substitutes is usedto represent the meaning of the target word.
Ifthere is more than one synset containing the high-est number of positives, all the synsets are takeninto consideration.
We then randomly select up tosix single-word synonyms other than positive substi-tutes from the chosen synset(s) as negative instancesof the target word.
Figure 4 shows an example ofautomatically collected negative data from WordNetgiven a target word and its positive substitutes.
Thesynset {?remainder?, ?balance?, ?residual?, ?residue?,Figure 4: An example of automatic negative datanoun verb adj adv# of true negatives 234 201 228 98# of false negatives 9 20 28 16Table 3: Annotation results for negative data?residuum?, ?rest?}
is selected for negative data col-lection since it contains one of the positives whilethe other synsets do not.
We assume the selectedsynset represents the meaning of the original word,and those synonyms in the synset which are not an-notated as positives must have a certain degree ofmismatch to the context.
Therefore, from this exam-ple, ?balance?, ?residue?, ?residuum?
and ?rest?
areextracted as negatives to test whether our synonymchecking method can pick out bad substitutions froma set of words sharing similar or the same meaning.In order to examine whether the automaticallycollected instances are true negatives and henceform a useful test set, a sample of automatically gen-erated negatives was selected for human evaluation.For each PoS one sentence of each different targetword is selected, which results in roughly 13% ofthe collected negative data, and every negative sub-stitute of the selected sentences was judged by thesecond author.
As can be seen from the annota-tion results shown in Table 3, most of the instancesare true negatives, and only a few cases are incor-rectly chosen as false negatives.
Since the main pur-pose of the data set is to test whether the proposedsynonym checking method can guard against inap-propriate synonym substitutions and be integratedin the stegosystem, it is reasonable to have a fewfalse negatives in our experimental data.
Also, itis more harmless to rule out a permissible substitu-1198PoS Acc% P% R% F% Thresholdnoun 70.2 70.0 80.2 74.7 0.58verb 68.1 69.7 79.5 74.3 0.56adj 72.5 72.7 85.7 78.7 0.48adv 73.7 76.4 80.1 78.2 0.54Table 4: Performance of the synonym checking methodtion than including an inappropriate replacement fora stegosystem in terms of the security.
Table 2 givesthe statistics of the automatically collected negativedata for our experiments.Note that, although we use the data from the lex-ical substitution task, our task is different: the pos-sible substitutions for a target word need to be fixedin advance for linguistic steganography (in order forthe receiver to be able to recover the hidden bits),whereas for the lexical substitution task participantswere asked to discover possible replacements.3.4 ResultsThe performance of the proposed checking methodis evaluated in terms of accuracy, precision, recalland balanced F-measure.
Accuracy represents thepercentage of correct judgements over all accept-able and unacceptable substitutions.
Precision is thepercentage of substitutions judged acceptable by themethod which are determined to be suitable syn-onyms by the human judges.
Recall is the percent-age of substitutions determined to be feasible by thehuman annotators which are also judged acceptableby the method.
The interpretation of the measuresfor a stegosystem is that a higher precision value im-plies a better security level since good substitutionsare less likely to be seen as suspicious by the ob-server; whereas a larger recall value means a greaterpayload capacity since words are being substitutedwhere possible and therefore embedding as much in-formation as possible.In order to derive sensible threshold values foreach PoS, 5-fold cross-validation was implementedto conduct the experiments.
For each fold, 80% ofthe data is used to find the threshold value whichmaximises the accuracy, and that threshold is thenapplied to the remaining 20% to get the final result.Table 4 gives the results for the synonym checkingmethod and the average threshold values over the 5folds.
In addition, we are interested in the effect ofFigure 5: System performance under various thresholdsvarious thresholds on the system performance.
Fig-ure 5 shows the precision and recall values with re-spect to different thresholds for each PoS.
From thegraphs we can clearly see the trade-off between pre-cision and recall.
Although a higher precision canbe achieved by using a higher threshold value, forexample noun?s substitutions almost reach 90% pre-cision with threshold equal to 0.9, the large drop inrecall means many applicable synonyms are beingeliminated.
In other words, the trade-off betweenprecision and recall implies the trade-off betweenimperceptibility and payload capacity for linguisticsteganography.
Therefore, the practical thresholdsetting would depend on how steganography userswant to trade off imperceptibility for payload.1199Figure 6: An example of coloured synonym graph4 Proposed Stegosystem4.1 The Vertex Coloring Coding MethodIn this section, we propose a novel coding methodbased on vertex colouring by which each synonym isassigned a unique codeword so the usage of overlap-ping synsets is not problematic for data embeddingand extracting.
A vertex colouring is a labelling ofthe graph?s vertices with colours subject to the con-dition that no two adjacent vertices share the samecolour.
The smallest number of colours requiredto colour a graph G is called its chromatic num-ber ?
(G), and a graph G having chromatic number?
(G) = k is called a k-chromatic graph.
The mainidea of the proposed coding method is to representoverlapping synsets as an undirected k-chromaticgraph called a synonym graph which has a vertexfor each word and an edge for every pair of wordsthat share the same meaning.
A synonym is thenencoded by a codeword that represents the colourassigned by the vertex colouring of the synonymgraph.
Figure 6 shows the use of four differentcolours, represented by ?00?, ?01?, ?10?
and ?11?, tocolour the 4-chromatic synonym graph of the twooverlapping synsets in Figure 2.
Now, the over-lapped word ?composition?
receives a unique code-word no matter which synset is considered, whichmeans the replacement of ?paper?
to ?composition?in Figure 2 will not cause an ambiguity since the re-ceiver can apply the same coding method to deriveidentical codewords used by the sender.99.6% of synsets in WordNet have size less than8, which means most of the synsets cannot exhaustmore than a 2-bit coding space (i.e.
we can onlyencode at most 2 bits using a typical synset).
There-fore, we restrict the chromatic number of a synonymgraph G to 1 < ?
(G) ?
4, which implies the max-imum size of a synset is 4.
When ?
(G) = 2, eachFigure 7: Examples of 2,3,4-chromatic synonym graphsvertex is assigned a single-bit codeword either ?0?or ?1?
as shown in Figure 7(a).
When ?
(G) = 3,the overlapping set?s size is either 2 or 3, which can-not exhaust the 2-bit coding space although code-words ?00?, ?01?
and ?10?
are initially assigned toeach vertex.
Therefore, only the most significantbits are used to represent the synonyms, which wecall codeword reduction.
After the codeword reduc-tion, if a vertex has the same codeword, say ?0?, asall of its neighbors, the vertex?s codeword must bechanged to ?1?
so that the vertex would be able to ac-commodate either secret bit ?0?
or ?1?, which we callcodeword correction.
Figure 7(b) shows an exampleof the process of codeword reduction and codewordcorrection for ?
(G) = 3.
For the case of ?
(G) = 4,codeword reduction is applied to those vertices thatthemselves or their neighboring vertices have no ac-cess to all the codewords ?00?, ?01?, ?10?
and ?11?.For example, vertices a, b, c, e and f in Figure 7(c)meet the requirement of needing codeword reduc-tion.
The codeword correction process is then fur-ther applied to vertex f to rectify its accessibility.1200Figure 8 describes a greedy algorithm for con-structing a coded synonym graph using at most4 colours, given n synonyms w1, w2,.
.
.
, wn inthe overlapping synsets.
Let us define a functionE(wi, wj) which returns an edge between wi andwj if wi and wj are in the same synset; otherwisereturns false.
Another function C(wi) returns thecolour of the synonym wi.
The procedure loopsthrough all the input synonyms.
For each iteration,the procedure first finds available colours for the tar-get synonym wi.
If there is no colour available,namely all the four colours have already been givento wi?s neighbors, wi is randomly assigned one ofthe four colours; otherwise, wi is assigned one ofthe available colours.
After adding wi to the graphG, the procedure checks whether adding an edge ofwi to graph G would violate the vertex colouring.After constructing the coloured graph, codeword re-duction and codeword correction as previously de-scribed are applied to revise improper codewords.4.2 Proposed Lexical StegosystemFigure 9 illustrates the framework of our lexicalstegosystem.
Note that we have preprocessed Word-Net by excluding multi-word synonyms and single-entry synsets.
A possible information carrier is firstfound in the cover sentence.
We define a possi-ble information carrier as a word in the cover sen-tence that belongs to at least one synset in Word-Net.
The synsets containing the target word, and allother synsets which can be reached via the synonymrelation, are then extracted from WordNet (i.e.
webuild the connected component of WordNet whichcontains the target word according to the synonymrelation).
Words in these sets are then examinedby the Google n-gram contextual checking methodto eliminate inappropriate substitutions.
If there ismore than one word left and if words which pass thefilter all belong to the same synset, the block cod-ing method is used to encode the words; otherwisethe vertex colouring coding is applied.
Finally, ac-cording to the secret bitstring, the system selects thesynonym that shares an edge with the target wordand has as its codeword the longest potential matchwith the secret bitstring.We use the connected component of WordNetcontaining the target word as a simple method to en-sure that both sender and receiver colour-code theINPUT: a synonym list w1, w2,.
.
.
, wn and anempty graph GOUTPUT: a coded synonym graph G using atmost four coloursFOR every synonym wi in the input listinitialize four colours as available for wiFOR every wj in graph GIF E(wi, wj) THENset C(wj) as unavailableEND IFEND FORIF there is a colour available THENassign one of the available coloursto wiELSEassign one of the four colours to wiEND IFADD wi to graph GFOR every wj in graph GIF E(wi, wj) and C(wi) is notequal to C(wj) THENADD edge E(wi, wj) to GEND IFEND FOREND FORcodeword reductioncodeword correctionOUTPUT graph GFigure 8: Constructing a coloured synonym graphsame graph.
It is important to note, however, thatthe sender only considers the synonyms of the targetword as potential substitutes; the connected compo-nent is only used to consistently assign the codes.For the decoding process, the receiver does notneed the original text for extracting secret data.
Aninformation carrier can be found in the stegotext byreferring to WordNet in which related synonyms areextracted.
Those words in the related sets undergothe synonym checking method and then are encodedby either block coding or vertex colouring codingscheme depending on whether the remaining wordsare in the same synset.
Finally, the secret bitstring isimplicit in the codeword of the information carrierand therefore can be extracted.We demonstrate how to embed secret bit 1 in the1201Figure 9: Framework of the proposed lexical stegosystemsentence ?it is a shame that we could not reach thenext stage.?
A possible information carrier ?shame?is first found in the sentence.
Table 5 lists the re-lated synsets extracted from WordNet.
The scoreof each word calculated by the synonym checkingmethod using the Web 1T Corpus is given as a sub-script.
Assume the threshold score is 0.27.
The out-put of the synonym checking method is shown at theright side of Table 5.
Since the remaining words donot belong to the same synset, the vertex colouringcoding method is then used to encode the words.Figure 10(a) is the original synset graph in whicheach vertex is assigned one of the four colours; Fig-ure 10(b) is the graph after applying codeword re-duction.
Although both ?disgrace?
and ?pity?
are en-coded by ?1?, ?pity?
is chosen to replace the coverword since it has a higher score.
Finally, the stego-text is generated, ?it is a pity that we could not reachthe next stage.
?As a rough guide to the potential payload withthis approach, we estimate that, with a threshold of0.5 for the contextual check, the payload would beslightly higher than 1 bit per newspaper sentence.5 ConclusionsOne of the contributions of this paper is to develop anovel lexical stegosystem based on vertex colouringcover sentence:It is a shame that we could not reach the next stageoriginal synsets retained synsets{commiseration.28, {commiseration,pity.97, ruth.13, pathos.31} pity, pathos}{pity.97, shame1} {pity, shame}{compassion.49, pity.97} {compassion, pity}{condolence.27, {commiseration}commiseration.28} {pathos, poignancy}{pathos.31, poignancy.31} {shame, disgrace}{shame1, disgrace.84, {compassion}ignominy.24} {poignancy}{compassion.49,compassionateness0}{poignance.12,poignancy.31}Table 5: Synsets of ?shame?
before and after applying thesynonym checking methodFigure 10: Synonym graph of ?shame?coding which improves the data embedding capacitycompared to existing systems.
The vertex colouringcoding method represents synonym substitution as asynonym graph so the relations between words canbe clearly observed.
In addition, an automatic sys-tem for checking synonym acceptability in context isintegrated in our stegosystem to ensure informationsecurity.
For future work, we would like to exploremore linguistic transformations that can meet the re-quirements of linguistic steganography ?
retainingthe meaning, grammaticality and style of the origi-nal text.
In addition, it is crucial to have a full eval-uation of the linguistic stegosystem in terms of im-perceptibility and payload capacity so we can knowhow much data can be embedded before the covertext reaches its maximum distortion which is toler-ated by a human judge.1202ReferencesMikhail J. Atallah, Craig J. McDonough, Victor Raskin,and Sergei Nirenburg.
2001a.
Natural language pro-cessing for information assurance and security: anoverview and implementations.
In Proceedings of the2000 workshop on New security paradigms, pages 51?65, Ballycotton, County Cork, Ireland.Mikhail J. Atallah, Victor Raskin, Michael C. Crogan,Christian Hempelmann, Florian Kerschbaum, DinaMohamed, and Sanket Naik.
2001b.
Natural lan-guage watermarking: design, analysis, and a proof-of-concept implementation.
In Proceedings of the 4thInternational Information Hiding Workshop, volume2137, pages 185?199, Pittsburgh, Pennsylvania.Mikhail J. Atallah, Victor Raskin, Christian F. Hempel-mann, Mercan Karahan, Umut Topkara, Katrina E.Triezenberg, and Radu Sion.
2002.
Natural languagewatermarking and tamperproofing.
In Proceedings ofthe 5th International Information Hiding Workshop,pages 196?212, Noordwijkerhout, The Netherlands.Richard Bergmair.
2007.
A comprehensive bibliogra-phy of linguistic steganography.
In Proceedings of theSPIE Conference on Security, Steganography, and Wa-termarking of Multimedia Contents, volume 6505.Igor A. Bolshakov.
2004.
A method of linguisticsteganography based on collocationally-verified syn-onym.
In Information Hiding: 6th International Work-shop, volume 3200, pages 180?191, Toronto, Canada.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram corpus version 1.1.
Technical report, Google Re-search.Andrew Carlson, Tom M. Mitchell, and Ian Fette.
2008.Data analysis project: Leveraging massive textual cor-pora using n-gram statistics.
Technical report, Schoolof Computer Science, Carnegie Mellon University.Ching-Yun Chang and Stephen Clark.
2010.
Linguis-tic steganography using automatically generated para-phrases.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages591?599, Los Angeles, California, June.
Associationfor Computational Linguistics.Mark Chapman and George I. Davida.
1997.
Hiding thehidden: A software system for concealing ciphertextas innocuous text.
In Proceedings of the First Interna-tional Conference on Information and CommunicationSecurity, volume 1334, pages 335?345, Beijing.Christiane Fellbaum.
1998.
WordNet: An electronic lex-ical database.
MIT Press, first edition.Aminul Islam and Diana Inkpen.
2009.
Real-wordspelling correction using Google Web IT 3-grams.
InEMNLP ?09: Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 1241?1249, Morristown, USA.
Association forComputational Linguistics.Jonathan K Kummerfeld and James R Curran.
2008.Classification of verb particle constructions with theGoogle Web 1T Corpus.
In Proceedings of the Aus-tralasian Language Technology Association Workshop2008, pages 55?63, Hobart, Australia, December.Yuling Liu, Xingming Sun, and Yong Wu.
2005.
A nat-ural language watermarking based on Chinese syntax.In Advances in Natural Computation, volume 3612,pages 958?961, Changsha, China.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
In Pro-ceedings of the 4th International Workshop on Seman-tic Evaluations, pages 48?53, Prague, Czech Republic.Hasan M. Meral, Emre Sevinc, Ersin Unkar, BulentSankur, A. Sumru Ozsoy, and Tunga Gungor.
2007.Syntactic tools for text watermarking.
In Proceed-ings of the SPIE Conference on Security, Steganogra-phy, and Watermarking of Multimedia Contents, vol-ume 6505, San Jose, CA.Brian Murphy and Carl Vogel.
2007.
The syntax of con-cealment: reliable methods for plain text informationhiding.
In Proceedings of the SPIE Conference on Se-curity, Steganography, and Watermarking of Multime-dia Contents, volume 6505, San Jose, CA.Brian Murphy.
2001.
Syntactic information hiding inplain text.
Master?s thesis, Trinity College Dublin.Cuneyt M. Taskiran, Mercan Topkara, and Edward J.Delp.
2006.
Attacks on linguistic steganography sys-tems using text analysis.
In Proceedings of the SPIEConference on Security, Steganography, and Water-marking of Multimedia Contents, volume 6072, pages97?105, San Jose, CA.Mercan Topkara, Umut Topkara, and Mikhail J. Atallah.2006a.
Words are not enough: sentence level naturallanguage watermarking.
In Proceedings of the ACMWorkshop on Content Protection and Security, pages37?46, Santa Barbara, CA.Umut Topkara, Mercan Topkara, and Mikhail J. Atal-lah.
2006b.
The hiding virtues of ambiguity: quan-tifiably resilient watermarking of natural language textthrough synonym substitutions.
In Proceedings of the8th Workshop on Multimedia and Security, pages 164?174, Geneva, Switzerland.M.
Olga Vybornova and Benoit Macq.
2007.
Amethod of text watermarking using presuppositions.In Proceedings of the SPIE Conference on Secu-rity, Steganography, and Watermarking of MultimediaContents, volume 6505, San Jose, CA.1203
