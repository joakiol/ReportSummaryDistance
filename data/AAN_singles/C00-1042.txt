Statistical Morphological Disambiguation forAgglutinative LanguagesDi lek  Z. Hakkani-Tfir,  Kemal  Oflazer, GSkhan T i i r\ ])el)aIt lncn(;  of COmlmtcr  Engin(',ering,B i lkent  Univers i ty,Ankara ,  06533, TU l l .KEY{hakkani, ko, tur}~cs,  b i lkent ,  edu.
t rAbstractIn this 1)aper, we present sta.tistical models formorphological disambiguation i Tm'kish.
Turkishpresents an interesting problem for statistical ,nodclssince the potential tag set size is very large becauseof the productive, derivational morl/hology.
\Ve pro-pose to handle this by breaking Ul) 1;11(; morhosyn-tactic tags into inflectional groups, each of whichcontains the inflectional features ti)r each (internm-diate) derived tbrm.
Our statistical models score theprobability of each morhosyntactic tag by consider-ing statistics over the individual inflection groupsin a trigram model.
Among the three models thatwe have deveh)l)ed and tested, (;11(; simplest modelignoring the lo(:al mort)hota(:ties within words l)er-tbrms the best.
Ollr })('.st; rigram model 1)erfornlswith 93.95% accuracy on otir test data getting all 1;11olllorhosyllta(;ti(; aild semantic fc.atul'es correct.
If weare just interested in syntactically relevant featuresalld igilore a very sinall set of semantic features, then(;tie accuracy increases to 95.07%.1 I n t roduct ionRe(:ent advances in (:onltmter har(lware and avail-al)ility of very large corpora have made (;t1(` - al)pli-cation of s(;atistical techniques to natural languageprocessing a t)asible and a very at)pealing resem'charea.
Many useflll results have 1)cell obtained byapplyilig these techniques to English (and similarlanguages) in parsing, word sense dismnbiguation,part-of  speech (POS) tagging, speech recognition,et;c. However, languages like Turkish, Czech, Hun-garian and Finnish, displ W a substantially differentbehavior than English.
Unlike English, these lan-guages have agglutinative or inflective morphologyand relatively free constituent order.
Such languageshave.
received little previous attention in statisticalprocessing.In this lmper, we t)resent our work on modelingTurkish using statistical methods, and present re-suits on morphological disainbiguation.
The meth-ods developed here are certainly al)plicable to otheragglutinative languages, especially those involvingproductive derivational phenomena.
The Iml)er isorganized as follows: After a brief overview of re-lated previous work, we smnma.rize relevant aspectsof Turkish and present details of various statisticalmodels for nlorlfliological disanfl/iguation for Turk-ish.
We then present results and analyses fronl ourexperiments.2 Related  WorkThere has been a large numl)er of studies in tag-ging and mori)hological disambiguation using vari-ous techniques.
POS taggiug systems have used ei-ther a statistical or a rule-based approach, hi thestatistical api)roach, a large corpus \]ms been used totrain a t)rotmbilistic model wlfieh then has been usedto tag new text, assigning the most likely tag for agiven word in a given context (e.g., Church (1.988),Cutting el; al.
(1992)).
In the rule-based approach,a large mmfl>e.r of hand-craft;ed linguisiic constraintsare used to elinfinate impossible tags or morpho-logical t)arse.s tbr a given word in a given context(Ka.rlsson et al, 1995).
Brill (1995a) has presenteda transfl)rnmtioi>based lea.rning at)l)roach, whi(:h in-duces disanlbiguation rules from tagged corpora.Morphologi(:al disanlbiguation i inflecting or ag-glutinative languages with COlnl)lex morphology in-volves more than determining the major or minorImrts-of-sl)cech of the lexiea.l items.
Typically, roofphology marks a mlmber of inflectional or deriva-tioiml features and this involves ambiguity.
For in-stance, a given word nlay be chopl)ed up in difl'erentways into mort)heroes , a given mort)heine may inarkdifferent features depending on the morphotactics,or lexicalized variants of derived words may interactwith productively derived versions (see Ottazer andTiir (1997) for the difl'erent kinds of morphologicalambiguities in Turkish.)
We assume that all syn-tactically relevant fcat'urcs of word forms have to bedetermined correctly for morphological disambigua.-tion.In this context, there have l)een some interestingprevious studies for difl'erent languages.
Levingerct al.
(1995) have reported on an approach thatlearns morpholexical probabilities fi'om an mltaggedeorlms mid have.
used the resulting infornlation in285morphological disambiguation in Hebrew.
Haji~:and Hla(lk~i (1998) have used ntaximunl entropymodeling approach for morphological dismnbigua-tion in Czech.
Ezeiza et al (1998) have combinedstochastic and rule-based isambiguation methodsfor Basque.
Megyesi (19991 has adapted Brill's POStagger with extended lexical templates to Itungar-tan.Previous ai)proaches to morphological dismnbi-guation of Turkish text; had employed a constraint-based approach (Otlazer and KuruSz, 1994; Oflazerand Tiir, 1996; Oflazer and Tiir, 1997).
Althoughresults obtained earlier in these at)preaches were rea-sonable, the fact that tim constraint rules were handcrafted posed a rather serious impediment o thegenerality and improvement of these systems.3 Turk i shTurkish is a flee constituent order language.
Tlmorder of the constituents may clmnge freely accord-ing to tim discourse context and the syntactic role ofthe constituents i indicated by their case marking.Turkish has agglutinative morphology with produc-tive inflectional and derixmtional suflixations.
Thenumber of word forms one can derive from a Turkishroot; form mW be in the millions (ttankmner, 19891.Hence, the number of distinct word forms, i.e., thevocabulary size, can be very large.
For instance, Ta-ble 1 shows the size of the vocabulary for I and 10million word corl)ora of Turkish, collected from on-line newspaI)ers.
This large vocabulary is the reasonCorpus  s ize  Vocabu lary  s ize1M words 106,54710M words 41.7,775Table 1: Vocabulary sizes for two Turkish corpora.for a serious data sparseness problem and also sig-nificantly increases the number of parameters to beestimated even for a bigram language model.
Thesize of the vocabulary also causes the perplexity tobe large (although this is not an issue in morpho-logical disambiguation).
Table 2 lists tlm trainingand test set perplexities of trigram language modelstrained on 1 and 10 million word corpora for ri51rkish.For each corpus, tile first cohmm is the perplexityfor the data the language model is trained on, andthe second column is the pert)lexity for previouslyunseen test data of 1 million words.
Another ma-jor reason for the high perplexity of Turkish is thehigh percentage of out-of  vocabulary words (wordsin the test; data which did not occur in the trainingdata); this results from the productivity of the wordtbrmation process.Training Training Set Test Set (1M words)Data Perplexity Perplexity1M words 66.13 t449.8110M words 94.08 1084.13Table 2: The pert)lexity of Turkish corpora usingword-based trigram language models.The issue of large vocabulary brought in by pro-ductive inflectional and derivational processes alsomakes tagset design an important issue.
111 lan-guages like English, the nunlber of POS tags that canbe assigned to the words in a text; is rather linfited(less than 100, though some researchers have usedlarge tag sets to refine g,:anularity, but they are stillsmall compared to Turkish.)
But, such a finite tagsetal)proach for languages like Turkish may lead to aninevitable loss of information.
The reason for thisis that the lnorphological features of intermediatederivations can contain markers for syntactic rela-tionshil)s. Thus, leaving out this information witlfina fixed-tagset scheme may prevent crucial syntac-tic information fl'om being represented (Oilazer etal., 1999).
For examl)le , it; is not clear what POStag sllould be assigned to the word sa.~lamlaqtwmak(below), without losing any information, the cate-gory of the root; (Adjective), tile final category ofthe word as a whole (Noun) or one of the interme-diate categories (Verb).
1s a\[flam +laq + t,r + ma~:saglam+kdj ^DB+yerb+Become^DB+gerb+Caus+Pos ^DB+Noun+Inf +A3sg+Pnon+lqomto ca'ass (,s'ometh, i.ng) to become stron 9 /to strength, or,/fortify (somcth, ing)Ignoring the fact that the root; word is an adjec-tive may sever any relationslfips with a.n adverbialmodifier modi~ying the root.
Thus instead of a sim-I)le POS tag, wc use the full rno~Tflt, oIogical a'nahtscsof the words, rcprcscntcd as a combination of \]'ca-tures (including any dcrivational markers) as theirmorphosyntactic tags.
For instance in the exami)leabove, we would use everything including the root;form as the morphosyntactic tag.In order to alleviate the data sparseness problelnwe break down the flfll tags.
We represent each wordas a sequence of inflectional groups (IGs hereafter),separated by "DBs denoting derivation boundaries,as described by Oflazer (1999).
Thus a morphologi-cal parse would be represented in the following gen-eral tbrm:tThe morphological features other than the POSs are:+Become: become verb, +Cans: causative verb, +Pos: Positivepolarity, +Inf: marker that derives an infinitive form fl'om averb, +Aasg: 3sg number-person agreement, +Pnon: No pos-sessive agreement, m~d +Nora: Nominative case.
"DB's markderivational boundaries.286Full ~\[hgs (No roots)hfltectional GrouI)sPossil)leOO9,129O1)served10,5312,194%fl)le 3: Numbers of q2tgs and iGsroot+IGi  ~DB+IG2 ~DB+- - -^DB+IG.where  IGi denotes relevant inflectional fea l ;urcs  o fthe inflectional groul)s, including the 1)art-ofsl)eechfor the root or any of the derived forms.For exalnlfle , the infinitive, tbtm s(u.~lamla.#'trmakgiven above would be ret)resented with the adjectivereading of the root sa.rflam mM the tbllowing 4 IGs:1.
Adj2.
Verb+Become3.
Verb+Caus+Posd.
Noun+Inf+A3sg+Pnon+NomTable 3 1)rovides a ( ' ,oml )ar i son  of the mnnl)er dis-l, in(:t full morl)hosyntactic tags (ignoring the rootwords in this case) mid IGs, generativ(dy 1)ossil)lea.nd observed in a (:ortms of 1M words (consideringa\]\[ ambiguities).
One can see thai; the' nmnber ob-served till tags ignoring the root words is very high,significantly higher than quoted tbr Czech by Ita.ji5and Itladk5 (1998).4 Stat i s t i ca l  Morpho log ica lD isambiguat ionMorphoh)gica.1 disambiguation is the prol)lcun oftinding the.
corresponding s(;qucnce, of morl/hologicalparses (including l;he root), 7' = t~ ~ = l l ,12, .
.
.
, l , , ,given a sequence of words 1?
= 'w~' = 1u 1 , 'W2, ...~ 'lU n.Our at)proach ix to model the (listrilmtion of lilOr-phological I)arscs give, n the words, using a hiddenMarkov model, and then to seek the variable 7', I.hatmaxilnizes .I'(TII'V)::/' T P (W)  )= ~.~, , laxP(T)  ?
P (WlT) (2 )TThe term P(W)  ix a constant for all choices of T, andcan thus be ignored when choosing the most prob-able 7'.
\Ve C~lll further simplify the t)roblem usingthe aSSUlnlil;ion that words arc indc'i)endent of eachother given their tags.
In Tm'ldsh we can use theadditional simplification that \ ] ' (wi l t i )  = l since l,iillcludes tim root fbrm and all morphosyntactic t~a-tures to uniquely determine the word f'orm.
2 Since2'l'hat is, we assume that  there is no morphological gen-eration ambiguity.
This  is ahnost  always true.
There area tb.w word fin'ms like flelirkcne and horde, which have them o, l , -~/so  z'( ' , , , ; ItT) : P(*,,~I*~) = 1, w(; ~u~ ,v r i , , :7 bP(WIT) = I \ ]P(w; I t~')  = 1i - -  \]a, l ld~/rgl~.,x P(SqW) = arg?11Hix \ ] ) (T )  (3)7' !1'N o wP(~') = P(t , , l t~' - '  ) x P ( t , - i l t~  '-2) x .
.
.xP(t~lt l)  ?
l)(t l)Simplifying fin%her with the trigram tag model, weget:P(T) = P(t,dt,,_.,.,Z,_,) ?l ' ( t , ,_ l  It,,-:~, t,,-.,) x .
.
.v(l,:~l~,,~._,) ?
e(t~lt.
, )  ?
z'(~,)= flP(l, ilti_e,ti_l) (4)i=1win, to  w,, d~,ti,,; P(z., I t - , ,  z.0) = I"(z:1), p(z. ,  I*,,, 1,1 ) =P(tuItl) to simplif3, the notation.If we consider morl)hoh)gi(:al analyses as a se-(t11011(:(2 o f  root  ;111(l \ ]Gs ,  each parse t,i can \])e rcp-?
res(;nted as (I i ,  IGi ,  , .
.
.
,  IG i , , , ) ,  where ni is thenuinber ()t" IG's in the, in, word.
:~ This rel)resental.ioilchanges the l)ro})lem as shown in Figure 1 wher(', the,chain rule has been used to factor out the individualcomt)oncnts.This f(irtlttll~ltioll stil l suffers from 1:,t1(!
data spat'so-ll(}SS 1)roblcll l .
To  allo, v ia tc  l;\]lis~ wc ill~ll,\[c thQ fo lk iw-ing siml)lifying assumlitions:1.
A root wor(1 del)ends only on l;he roots of the1)revious words, alld ix indet)cndent of the inflec-tional and derivationa\] productions on thein:l ' (n l (n -~,  IG~_.,4,..., 1G~_.,,,, .,),(r i -a,  I6 ' i _~,~, .
.
.
, /G~_~, , , _ , ) )  =P("~l"~-~,n-,) (~')The intention here is that this will be useflllin tile disambigua?ion of the root word when agiven form has mori)hological parses with dif-fiwent root words.
So, tbr instance, for disam-biguating the surface, form adam with the fol-lowing two parses:same morphological parses with the word forms gclirkcn andheretic, respectively but are i)ronounced (and writte.n) sl lghllydifferently.
These.
m'e rarely seen in written te.xts, and canthus l)e. ignored.aln our training and W.st data, the nmnbcr  of 1Gs in a wordform is on the average 1.6, the.refore, ni is usually 1 or 2.
We.have seen, occasionally, word tbrms with 5 or 6 inflectionalgroups.287I)(ti\[t1-1)zI )(t i It i -2,t i-1)P(  (ri, IGi,l .
.
.
IGi,n~ )\[(ri-2, IGi-2,1.
.
.
IGi-'2,ni_2 ), (ri-1, IG i - l ,1 .
.
.
IG i - i  , , i - ,  ))P(ri l(r i -2, \ [Gi -2,1. .
.
IGi-~,,,_2), (ri-1, IG i -La .
.
.
IGi-1,,zi_, )) xP ( IG i j  \[(ri-2, \[Gi-'e,1...IGi-'e,ni_2), (I'i-1, IOi-l,1 ...\[Gi_l,ni_, ), I'i) x. .
.
XP( IGi,m \[(ri- u, IG i- 2,1 ...\[Gi-2,,~,_=), (r i- l  , IG~-I,1 ...\[Gi-l,,,_~ ), ri, IGi,1, .,., i Gi ,m-l )Figure 1: Equation tbr morphological disambiguation(a) adam+Noun+A3sg+Pnon+Nom (man)(b) ada+Noun+A3sg+Plsg+Nom ( y island)in the iloun phrase k'trm~z~ kazakh adam (theman with a red sweater), only the roots (alongwith the part -of  speech of the root) of the pre-vious words will be used to select the right root.Note that tile selection of the root  has some im-pact on what the next IG in the word is, but weassuine that IGs are determined by the syntac-tic context and not by the root.2.
An interesting observation that we can makeabout q_hrkish is that when a word is consid-ere(l as a sequence of IGs, syntactic relationsare between the last IG of a (dependent) wordand with some (including the last) IG of the(head) word on the right (with nfinor eXCel)-tions) (Oflazer, 1999).Based on these assumptions and the equation in Fig-ure 1, we define three models, all of which are basedon word level trigrams:1.
Mode l  1: The presence of IGs in a word onlydepends on the final IGs of the previous words.This model ignores any morphotactical relationbetween an IG and any previous IG in the sameword.2.
Mode l  2: The presence of IGs in a word onlydepends on the final IGs of the previous wordsand the previous IG in tile same word.
Inthis model, we consider morphotactical rela-tions and assume that an IG (except the firstone) in a word form has some dependency ontile previous IG.
Given that on the average aword has about 1.6 IGs, IG bigrams should besufficient.3.
Mode l  3: This is the same as Model 2, exceptthat the dependence with the previous IG in aword is assumed to be indelmndent of the de-pendence on the final IGs of the previous words.This allows the formulation to separate the con-tributions of the morphotactics and syntax.The equations for these models are shown in Figure2.
We also have built a baseline model based onwhen tags are decomposed into inflectional groups.tile standard definition of the tagging problem inEquation 2.
For the baseline, we have assumed thatthe part of the morphological nalysis after the rootword is the tag in the conventional sense (and theassumption that P(wi\]ti) = 1 no longer holds).5 Experiments and ResultsTo evaluate our models, we f rst  trained our modelsand then tried to morphologically disambiguate ourtest data.
For statistical modeling we used SRILM- the SRI language modeling toolkit (Stolcke, 1999).Both the test data and training data werecollected from the web resources of a Turkishdaily newspN)er.
The tokens were analyzed usingthe morphological analyzer, developed by Oflazer(1994).
The mnbiguity of the training data wasthen reduced fl'om 1.75 to 1.55 using a preprocessor,that disambiguates lexicalized and non-lexicalizedcollocations and removes certain obviously impossi-ble parses, and trigs to analyze unknown words withall unkllown word processor.
The training data con-sists of the unambiguous sequences (US) consistingof about 650K tokens in a corpus of i million tokens,and two sets of manually dismnbiguated corpora of12,000 and 20,000 tokens.
Tile idea of using unam-biguous sequences is similar to Brill's work on un-supervised learning of disambiguation rules for POStagging (199517).The test data consists of 2763 tokens, 935 (~34?/0)of which have more than one morphological nalysisafter preprocessing.
The ambiguity of the test datawas reduced from 1.74 to 1.53 after prct)rocessing.As our evaluation metric, we used accuracy de-fined as follows:# of correct parsesx 100 accuracy = # o.f tokensThe accuracy results are given in Table 4.
For allcases, our models pertbrmed better than baseline tagmodel.
As expected, the tag model suffered consid-erably from data sparseness.
Using all of our train-ing data., we achieved an accuracy of 93.95%, wlfichis 2.57% points better titan tile tag model trained us-ing the same amount of data.
Models 2 and 3 gave288In all three models we assume that  roots and IGs are indel)cn(tenl.Mode l  1:  This  model  assumes that  un IG in ~ word depends on the last IGs of the two previous words.P(IGi,t:\[(ri-~, 1Gi-2,~ ... Gi-'~,,~_.2), ( r i -1 ,  IG i - l ,~ , .
.
.
,  IGi-~,,,~_~ ), ri, IGi ,~, .
.
.
,  IGi,t,-~)) I" l 1 ( G~,~.IIG~-~,,,,_~,-/Gi-l,ni_l )Ther(;fore,l ) ( t i \ [ t i -~ , t i -1 )II ik=.
l(0)Mode l  2: The  model  a ssmn(~s that  in addi t ion to th(~ dcl)(mdonci(;s in Model  1, an IG also (lot)ends on timprev ious  IG  in the s~mm word.P(1Gi,~.l(ri_.~, IGi-~,~ ...IGi-~,~,i .,), (ri-~Th(,r(',for(;,IG i _~, , , .
.
.
, IG i  1,,~__,),ri,lGi,1,...,lGi,~.--1) =?
\ [ I  ~'(~c~,~.l~(;~--~,,,,_~, ~(;~-~,,,,_,, IG~,k._, ) (7)k=lMode l  3: This  is same as Model  2, except the mort)hotact ic  and syntact ic  dt'4)t;ntlenci(;s arc considered tobc independent .\])(\]Gi,kl(l"i-2,lGi-2,1....\[Gi-2,,,i .
,),(ri- l , .
ld/i-.
i ,1,...,\]Gi-l,,,~_ 1) , r i , lG i , l , .
.
.
, l -Gi ,k- l )  =) ~\])(IGi l.\[IGi..2,,,~ .,, t ' (~ i_ l , , , ,  .l (~r i , t , .
- - l )r.l?lwr(;forc,P(ti\[ti-2,l.
i- j) = l)(~' i \ ] r i_ .~,r i_ l )  x \] (1(,,,11. ,.
IG,_u), .
.
.
.
, , , IO i - i , , , , _ , )  xIc--IIn order to simpli\[y the uotation, wc lmve dctlncd the follc, v:ing:) ,t ?I (IGi,t.
llGi,k_l= l'(1Gi,~.llGi ._,,,.
, .
IGi  j , , , ,_,) x P(IGi,~,.)I'(IGi,t.
IIGi,,~-I) )F(','~l','-,,','o) = IX',',)j~(,.:+.
(,,,.,) = r,(, .+.,)l(m,,~l?
(,_, .
.
.
.
, ,mo,,,o) = J (.rc~,~,,), , , = 17 , ,.
I (IG2,t\[IGo ...... ICt,,,, ) (IG2,11IG,,,,,)1 ( IGi, l \ [ IGi-2, ,~i_~, IGi-1, ,~i_ l ,  IG i ,o )P(IGI,I~I IG- I  .... 1, IGo,~o, IGx,k-1)P( IG2,t\[IGo,~o, IG1,~1, IG'~,t-l )= P( IG i , l l lG i _ .2 , , , i _ , , IG i_ l , , , i _~)= P(IGI,~IlGI,,,_I)= I ( IG2,t\[ IG1,,~I, IG2,t_I)P(IG2,~\[IGI, , .
, IG2,o) = P(IG2,,I IG~ .... )P( fG i , l l \ [a i ,o )  = I ; '  ( ~r (.T~i, 1 )for k = 1,2,.. .
, 'hi,  1 = 1,2, ...,n~, and i = 1,2, ..., 'n.F igure 2: Equut ions  for Modcls \], 2, and  3.289Training Data ~Lag Model Model 1 Model 1 Model 2 Model 3(Baseline) (Bigram)Unambiguous equences (US) 86 .75% 88 .21% 89 .06% 87 .01% 87.19%US + 12,000 words 91.34% 93 .52% 93 .34% 92 .d3% 92.72%US + 32,000 words 91.34% 93 .95% 93 .56% 92 .87% 92.94%Table 4: Accuracy results for difli;rent models.similar results, Model 2 suffered from data sparse-hess slightly more than Model 3, as expected.Surprisingly, the bigram version of Model I (i.e.,Equation (7), but with bigrams in root and IG mod-els), also performs quite well.
If we consider just thesyntactically relevant morl)hological features and ig-nore any senlantic features that we mark in ulorphol-ogy, the accuracy increases a bit flirt, her.
These stemti'om two properties of %lrkish: Most Turkish rootwords also have a proper noun reading, when writ-ten with the first letter cai)italized.
4 We (:ount it; asan error if the tagger does not get the correct 1)ropernoun marking, for a proper noun.
But this is usua\]lyimpossil)le especially at the begimfing of sentenceswhere the tagger can not exploit caI)italization andhas to back-off to a lower-order model.
In ahnost allof such cases, all syntactically relevant morl)hosyn-tactic features except the proper noun marking areactually correct.
Another imi)ortant ease is the pro-noun o, which has t)oth personal prollottll (s/he) anddemonstrative 1)ronoun readings (it) (in addition toa syntactically distinct determiner reading (that)).Resolution of this is always by semantic cousi(ler-atious.
When we count as (:orreet m~y errors in-volving such selnantic marker cases, we get an ac-curacy of 95.07% with the best (',as(; (cf.
93.91%of the Model 1).
This is slightly 1)etter than theprecision figures that is reported earlier on morpho-logical disambiguation of Turkish using constraint-based techniques (Oflazer and T/Jr, 1997).
Our re-suits are slightly better than the results on Czechof Haji~ and Hla(lkg (1998).
Megyesi (1999) reportsa 95.53% accuracy on Hungarian (a language whosefeatures relevant o this task are very close to thoseof Turkish), with just the POS tags 1)eing correct.
Inour model this corresponds to the root and the POStag of the last IG 1)eing correct and the accuracyof our best model with this assumi)tion is 96.07%.When POS tags and subtags are considered, the re-ported accuracy for Hungarian is 91.94% while thecorresl)onding accuracy in our case is 95.07%.
We.can also note that the results presented by Ezeizaet al (1998) for Basque are better titan ours.
Themain reason tbr this is that they eml)loy a muchmore sot)histicated (comt)ared to our t)reprocessor)din fact, any word form is a i)otential first name or a lastna I I10 .constraint-grammar based system which imI)rovest)recision without reducing recall.
Statistical tech-niques applied at'~er this disaml)iguation yield a bet-ter accuracy compared to starting from a more am-1)iguous initial state.Since our models assmned that we have indepen-dent models for disambiguating the root words, andthe IOs, we ran experiments to see the contributionof the individual models.
Table 5 summm'izes the ac-curacy results of the individual models for the bestcase (Model 1 in Table 4.
)Mode l  AccuracyIG Model 92.08%Root Model 80.36%Combined Model 93.95%Table 5: The contril)ution of the individual modelsibr the 1)est case.There are quite a number of classes of words whichare always ambiguous and the t)reprocessing thatwe have employed in creating the unambiguous se-quences ca.n never resolve these cases.
Tlms sta-tistical models trained using only the unambiguoussequences as the training data do not handle theseambiguous cases at all.
This is why the accuracyresults with only unalnbiguous equences are sig-nificantly lower (row 1 in Table 4).
The manuallydismnl)iguated training sets have such mnbiguitiesresolved, so those models perform much better.An analysis of the errors indicates the following:In 15% of the errors, the last IG of the word is in-correct )ut the root and the rest of the IOs, if any,are correct.
In 3% of the errors, the last IG of theword is correct but the either the root or SOlne ofthe previous IGs are incorrect.
In 82% of the errors,neither the last IG nor any of the previous IOs arecorrect.
Along a different dimension, in about 51%of the errors, the root and its part-of-speech are notdetermined correctly, while in 84% of the errors, theroot and the tirst IG combination is not correctlydetermined.2906 ConclusionsW(; have 1)resented an ai)l)roach t() slatisti(:al mod-eling fl)r agglutinativ(: lmlguages, esi)(;(:ially thosehaving l)roducl;ive d(;rivational 1)\]:(ulomena.
()ur ai)-l)roa('h essentia.lly involves l)re.al:ing u t) the full m(/r-t)hological ana.lysis across (l(~'rivational boundariesmid l;reai;ing the (:Onll)On(mt;s ;Is sul)tags, and l;heltdetermining the corre(:l; se(tuenc( ', of tags via sl;al;is-tical l;echniques.
This, to our knowl(~.
(lge, is th('.
firstdetailed attempt in statistical modeling of agghttina-rive langua.g('~s and (:an cerl;aJnly l)('.
al)plied to othersuch lmLguages like ltmlgari:m ;rod Fimfish with 1)re -duetive derivational morl)hology.7 AcknowledgmentsWe tlmnk Andreas Stoleke of SIH STAI{ lml) tbr pro-vi(ling us wil;h the \]anguag(; mod('.ling t;oolkit and ti)rvery helpflfl dis(:ussions on I;his work.
IAz Stlribergof SRI STAR Labs, and Bilge Say of Middle EastTe(:hnical University hfformal;i(:s \]nstitul;e, 1)rovidedhell)rid insights and (:ommenl;s.ReferencesF, ri(: Brill.
1995a.
Trat:slbrmal.i()n-1)as(~d err()r-(h:iv(m l('m'ning mid ha.rural angm~gc pro(:(:ssing:A case stu(ly in 1)art-ofsl)(~e(:h lagging.
Compul, a-tional Lin9'u, istics , 21 (4):543 566, l)ceeinl)er.Eric Brill.
19951).
Unsupervised learning of disam-Mguation rule's for l)art of st)(w~(:h (;agging.
In l)~v -eeedings of th, e Third l,Vorksh, op on Very \],a~:(I(:Corpora, Carat)ridge, MA, .hme.K(umeth W. Church.
1.988.
A sto(:hasti(: parts 1)ro-p~r}/lll all(\] ;* llOllll phrase  l)ars(}r for lllll(}sl;li(;\[;(}(1t( :t .
In l'roeeediv,9s of I,h,e ,5'eemM Co',@re'm:eon Applied Nat'm'al Lang'ua9e l)~vcessi'n9, Austin,~lZ'xas.1)oug Cutting, Julim: Kupiec, Jan Pedersen, andl)enelope Sibun.
1992.
A practical imri;-of-st)eechtagger, llt: Proceedings oJ" l,h,e Third Co'nfercnceon Applied Natural Language .l)~vees.sin9, Trenl;o,Italy.N.
Ezeiza, I. Alegria, J. M. Arriola, R. Urizar, andI.
Aduriz.
1998.
Colnbining stochastic and rule-I)ased m(;l, hods for dismnl)iguation i agglutina,rive bmgu;~ges.
In Proceedin9s of the 36 tl' An-nual Meeting of th, e Association for Computa-tional Linguistics and 17 th International Co~@r-enec on Computational Linguistics, pages 379384, Montreal, Quebec, Canada, August.Jan Haji~ and Barl)ora HladkS.
1998.
Tagginghfllective languages: Predictiol: of morphologi-cal categories fl)r a rich, smtcCured t,~gseI;.
InProceedings of COLING/ACL '98, pages 483-490,Montreal, Canada, Augusl;.Jorge Hankmner, 1989.
Lezieal Ilepresentation andProcess, chapter Mort)hological Parsing and theLexicon.
The MIT Press.l,?ed Karlsson, Atro Voutilainen, Juha \]teikkilii,and Arto Anl:tila.
1995.
Constraint Gramm, ar-ALanguage Inde.pcndent System for I>arsi'n9 Unre-.~trieted Tezt.
Mouton de Gruyter.Moshc I~oving;cr, Uzzi Ornan, and Aton Itai.
1995.Learning morpho-lexical probabilities fl'om anuntagged corpus with an application to He-tirew.
Comp'utational Lin9uisties, 21(3):383 404,Sei)t(nnb(~u'.Be:{l~ Megyesi.
1999.
Iml)roving Brill's POS tag-ger f(/r a.n agglutinative language.
In Pascal(',Fung and Joe Zhou, editors, \])roeeedin9s of th, e,\]oint ,5'IGDAT' Confere.
'nee on E'm, pirieal Methodsin Natural La'n9uage I'rocessin 9 and VeUl LmyleCo,7~ora , pages 275-284, College \]?ark, Maryland,USA, June.Kema.1 ()llazer and Ilker Km'uSz.
1994.
Tagging andmorphological disambiguation f ~lStrkish text.
Inl'roeeedinw of the 4 u' Applied Nal,'m'al LanguageProcessing Co~@renee, t)ages 144 149.
ACL, ()c-Iol)er.Iiemal ()flazer m~d GSktmn '.l'{ir.
1996.
Coml)in-ing hand-t:rafl;(~d rules a.nd unsul)('.rvis(~(l l ;arn-ing in (:onstraint-t)as('.d morphological disaml)igua-ti(m. In 1,3ri(: Brill and Kemw.th Chur(:h, editors,\]"lvceedin9.s of tll.e A(fL-SIGDAT Co'nil'fence on\],)mpirieal Meth.ods i'n Nal,'m'al Lan.qv, age P~veess-in 9.t(cmal Otlazer and GSklmn Tilt.
1997.
Mor-1)h()logi(:al disaml)iguation l)y vol;ing (:onstra.in(;s.In Proeeeding.s of the 35 u' A'n'n'ual Meeting ofthe A,s'sociatio'n for Computational Li'nguistie.s(A(H,'OT/lC~ACL'97), Madrid, Sl/ain, July.\](.cmal ()tlaz(!r, 1)iM: Z. Hal:kani-Tiir, mid G6khanT/Jr.
1999. l)(;sign ior a Turkish :;reebank.
In l',v-e('edin9 s of Workshop on Ling'uisl, ieally l'nt, e~Tn'el, r',dColTmra , al, I~,A CL '99, Bergen, Norway, .June..Kcmal Otlazer.
1994.
Two-level description of Turk-ish morphology.
Literary and l, inguistie Co'reput-ing, 9(2):137 148.K(.
'm~d ()tlazer.
1.999.
Del)endency pa.rsing with anextended iinite state al)proach.
It: Proceedings ofthe 371,h Ann'ual Meeting of the Association forCo'm,irutational Linguistics, College Park, Mary-land, .hme.Andreas Stolcke.
1999.
SRILM--the S1H languagemo(teling toolkit, h t tp  ://www.
speech, s r i .
corn/proj ec ts / s r i lm/ .291
