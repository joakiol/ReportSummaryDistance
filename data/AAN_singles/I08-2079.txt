Automatic rule acquisition for Chinese intra-chunk relationsQiang ZhouCenter for Speech and Language Technologies, Division of Technical Innovation and DevelopmentTsinghua National Laboratory for Information Science and TechnologyTsinghua University, Beijing 100084, P. R. Chinazq-lxd@mail.tsinghua.edu.cnAbstractMultiword chunking is defined as a task toautomatically analyze the external functionand internal structure of the multiwordchunk(MWC) in a sentence.
To deal withthis problem, we proposed a rule acquisitionalgorithm to automatically learn a chunkrule base, under the support of a large scaleannotated corpus and a lexical knowledgebase.
We also proposed an expectationprecision index to objectively evaluate thedescriptive capabilities of the refined rulebase.
Some experimental results indicatethat the algorithm can acquire about 9%useful expanded rules to cover 86%annotated positive examples, and improvethe expectation precision from 51% to 83%.These rules can be used to build an efficientrule-based Chinese MWC parser.1 IntroductionIn recent years, the chunking problem hasbecome a hot topic in the communities of naturallanguage processing.
From 2000 to 2005, severaldifferent chunking-related tasks, such as textchunking (Sang and Buchholz, 2000), clauseidentification (Sang and Dejean, 2001), semanticrole labeling (Carreras and Marquez, 2005), weredefined in the CoNLL conferences.
Much researchhas been devoted to the problem through differentpoints of view.Many computational linguists regard chunkingas a shallow parsing technique.
Due to itsefficiency and robustness on non-restricted texts,it has become an interesting alternative to fullparsing in many NLP applications.
On the base ofthe chunk scheme proposed by Abney (1991) andthe BIO tagging system proposed in Ramshaw andMarcus(1995), many machine learning techniquesare used to deal with the problem.
However,almost all the chunking systems focus on therecognition of non-overlapping cores of chunks tillnow, none of them care about the internalstructure analysis of chunks.In our opinion, the internal structure of a chunk,including its head and the dependency relationbetween head and other components, plays animportant role for semantic content understandingfor the chunk.
They are especially useful for thelanguages with few morphological inflections,such as the Chinese language.
Therefore, wedesign a multiword chunking task to recognizedifferent multiword chunks (MWCs) with thedetailed descriptions of external function andinternal structure in real texts.
Its main difficultylies in the preciously identification of differentlexical relationships among the MWC components.Some detailed lexical semantic knowledge isrequired in the task.To deal with this problem, we proposed a ruleacquisition algorithm to automatically learn aMWC rule base, under the support of a large scaleannotated corpus and a lexical knowledge base.We also proposed an expectation precision indexto evaluate the descriptive capabilities of therefined rule base.
Some experimental resultsindicate that our current algorithm can acquireabout 9% useful expanded rules to cover 86%annotated positive examples, and improve theexpectation precision from 51% to 83%.2 Multiword chunking taskInformally, a MWC is a chunk with two ormore words, where each word links to a semantichead through different dependency relations.
Foursyntactic dependency relationships are used in thepaper: (1) Modifier-Head relation, (2) Predicate-601Object relation,(3) Predicate-Compliment relation,(4) Coordinate relation.
They can determinate thefollowing functional position tags for each word ina MWC: (1) M--Modifier; (2) H--Head; (3)P--Predicate; (4) O--Object; (5) C--Compliment;(6) J--Coordinate constituent.
Based on them, wedefine three topological constructions as follows:(1) Left-Corner-Centre (LCC) constructionAll the words in a chunk link to the left-cornerhead and form a left-head dependency structure.Its basic pattern is: H C1 ?
Cn.
The typicaldependencies among them are Predicate-Object orPredicate-Compliment relations: C1?H, ?
,Cn?H.
They form the following functional tagserial : P [C|O].
(2) Right-Corner-Centre (RCC) constructionAll the words in a chunk link to theright-corner head and form a right-headdependency structure.
Its basic pattern is: A1 ?
AnH.
The typical dependencies among them areModifier-Head relations: A1?H, ?
, An?H.They form the following functional tag serial :{M}+ H.(3) Chain Hooking (CH) constructionEach word in a chunk links to its right-adjacent word.
All of them form a multi-headhooking chain.
Its basic pattern is: H0 H1 ?
Hn,where Hi, i?
[1,n-1] is the chain head in differntlevels, Hn is the semantic head of the overall chunk.The typical dependencies among them areModifier-Head or Coordinate relations : H0?H1, ?
, Hn-1?H n. They form the followingfunctional tag serial : {J}* or [M|J] {K|J}* H, whereK represents the internal chain head.We think the above three constructions cancover almost all important syntactic relations inreal text sentences.
Now, we can give a formaldefinition for a multiword chunk.Definition: two or more words can form amultiword chunk if and only if it has one of theabove three internal topological constructions.The MWC definition builds the one-to-onecorresponding between the word serials withdifferent function tags and their dependencystructure.
So we can easily describe some MWCswith complex nested structures.
In the paper, weadd a further restriction that each MWC can onlycomprise the content words, such as nouns, verbs,adjectives, etc.
This restriction can make us focuson the analysis of the basic content descriptionunits in a sentence.Each MWC is assigned two tags to describe itsexternal function and internal structure.
Forexample, a ?np-ZX?
MWC represents a nounchunk with internal modifier-head relationship.Table 1 lists all the function and relation tags usedin our MWC system.
The np, mp, tp, sp form asthe nominal chunk set.
Their typical relation tagsare ZX, LN and LH.
The vp and ap form as thepredicate chunk set.
Their typical relation tags areZX, PO, SB and LH.F-tags Descriptions R-tags Descriptionsnp noun chunk ZX modifier-headrelationshipvp verb chunk PO verb-objectrelationshipap adjectivechunkSB verb-complimentrelationshipmp quantitychunkLH Coordinaterelationshipsp space chunk LN chain hookingrelationshiptp time chunkTable 1 Function and relation tags of MWCsThe following is a MWC annotated sentence:[tp-ZX ?
?/t(long time) ?
?/f(since) ] ?/w?/r(he) ?/p(for) ?
?/v(safeguard) [np-ZX ?
?/n (world) ?
?/n(peace) ] ?/u [np-ZX ?
?/a(lofty) ??
/n(undertaking)] [vp-PO ??
/v(devote) ?
?/n (painstaking)] ?/w ??/v(make)?
/u ?
?
/a(outstanding) ?
/u ?
?
/v(contribution)  ?/w1 (For a long time past, he hasdevoted all his energy into the lofty undertaking tosafeguard world peace and made a outstandingcontribution.)
(1)There are four MWCs in the sentence.
Fromwhich, we can easily extract the positive andnegative examples for a MWC rule.
For example,in the sentence, we can extract a positive example:??
/v (devote) ??
/n (painstaking), and anegative example: ??
/v(safeguard) ??
/n(world) for the verb chunk rule : v+n?
vp-PO.3 Automatic rule acquisitionThe goal of the rule acquisition algorithm is to1 POS tags used in the sentence: t-time noun, f-direction,r-pronoun, p-preposition, v-verb, n-noun, u-auxilary,a-adjective, d-adverb, w-puntuation.602automatically acquire some syntactic structurerules to describe which words in which context ina sentence can be reduced to a reliable MWC, onthe base of a large scale annotated corpus and alexical knowledge base.Each rule will have the following format:<structure description string> ?
<reduced tag><confidence score>Two types of structural rules are used in ouralgorithm: (1) Basic rules, where only POS tagsare used in the components of a structure rule; (2)Expanded rules, where some lexical andcontextual constraint is added into the structurerule string to give more detailed descriptions.
Thereduced tag has two kinds of MWC tags that aresame as ones defined in Table 1.Each rule consists of all the positive andnegative examples covered by the rule in theannotated corpus.
For the word serial matchedwith the structure description string of a rule, if itcan be reduced as a MWC in the annotatedsentence, it can be regarded as a positive example.Otherwise, it is a negative example.
All of themform a special state space for each acquired rule.Therefore, the confidence score (?)
for the rule canbe easily computed to evaluate the accuracyexpectation to apply it in an automatic parser.
Itscomputation formula is: ?
= fP / ( fP + fN), where fPis the frequency of the positive examples, and fN isthe frequency of the negative examples.A two-step acquisition strategy is adopted inour algorithm.The first step is rule learning.
We firstly extractall basic rules with positive examples from theannotated corpus.
Then, we match the extractedstructure string of each basic rule in all the corpussentences to find all possible negative examplesand build state space for it.
Through rulereliability computation (see the following section),we can extract all high-reliability basic rules as thefinal result, and all other basic rules with higherfrequency for further rule refinement.The second step is rule refining.
We graduallyexpand each rule with suitable lexical andcontextual constraint based on an outside lexicalknowledge base, dynamically divide andautomatically allocate its positive and negativeexamples into the expanded rules and formdifferent state spaces for them.
From them, we canextract all the high and middle reliabilityexpanded rules as the final results.At last, by combining all the extracted basic andexpanded rules, we build a hierarchical acquiredrule base for parser application.Two key techniques are proposed in thealgorithm:(1) Rule reliability evaluationThe intuition assumption is that: if a rule has ahigher confidence score and can cover morepositive examples, then it can be regarded as areliable rule.Types Decision conditions1 z (fP>=10) && (?>=0.85)z ((fP>=5) && (fP<10)) && (?>=0.9)z ((fP>=2) && (fP<5)) && (?>=0.95)2 z (fP>=10) && (?>=0.5)z ((fP>=5) && (fP <10)) && (?>=0.55)z ((fP>=2) && (fP<5)) && (?>=0.6)z (fP >0) && (?>=0.6)3 z (fP >=10) && (?>=0.1)z ((fP>=5) && (fP<10)) && (?>=0.2)z ((fP>=2) && (fP<5)) && (?>=0.3)z (fP>0) && (?>=0.3)4 All othersTable 2 Four reliability types of the acquiredrulesBy setting different thresholds for ?
and fP, wecan classify all acquired rules into the followingfour types of rule sets: (1) high-reliability (HR)rules; (2) middle-reliability (MR) rules; (3)low-reliability rules; (4) Useless and noise rules.Table 2 shows different decision conditions forthem in our current algorithm.
Based on thisuniform evaluation standard, we can easily extracteffective rules from different acquired rule baseand quickly exclude useless noise rules.
(2) Rule expansion and refinementWhen a rule is not reliable enough, theexpansion step is set off: new knowledge is addedto the rule in order to constrain it.
The purpose isto dynamically divide the state space of the ruleand reduce the proportion of negative examplescovered by the current rule.
For every annotatedpositive or negative example, our expansionstrategy is as follows:Firstly, we expand a rule description throughlooking up different lexical knowledge base.
Forthe verb chunks with LCC constructions, we usethe following lexical constraint: (1) Lexical-syntactic relation pairs, (2) Subcategory frame of603head verb.
For the noun chunks with RCC and CHconstructions, we use the following lexicalconstraint: (1) Lexical-syntactic relation pairs, (2)Semantic class of head noun.Secondly, we expand a rule description examplewith or without lexical constraint through lookingup its left and right adjacent contexts.
For eachrule waiting for expansion, we add its left-adjacentPOS tag, right-adjacent POS tag, left and rightadjacent POS tag to form three expanded rule withcontextual constraint.For example, for the positive example ??
?/v(devote) ?
?/n (painstaking) ?
of ?v+n?
rule inthe above sentence (1), we can get the followingexpanded rules:z v(WC-L)+n(WC-R) // + v-n relationship pairz v(winl:VNPLIST)+n // + verb subcate framez n__v+n // + left POS constraintz v+n__w  // +right POS constraintz n__v+n__w  // +l and +r POS constraintThey can be put into the state space pool as theexpanded rules with positive example informationfor frequency calculation.Unlike the information-gain measure used inFOIL system (Quinlan, 1990), we do not imposeany criteria for selecting different knowledge.
Allthe suitable expanded rules are selected throughthe final confidence score evaluation indexes.4 Experimental resultsAll the news files with about 200,000 words inthe Chinese treebank TCT (Zhou, 2004) wereselected as the experimental data.
They wereseparated into two data sets: (1) training set, whichconsists of about 80% data and is used for ruleacquisition; (2) test set, which consists of about20% data and is used for parser evaluation.Then we automatically extracted all the MWCsfrom the annotated trees and built two MWCbanks.
Among them, 76% are noun chunks andverb chunks.
They are the key points for ruleacquisition and parsing application.
In the trainingset, about 94% verb chunks are two-word chunks.But for noun chunks, the percentage of two-wordchunks is only 76%.
More than 24% noun chunkscomprise three or more words.
The complexitiesof noun chunks bring more difficulties for ruleacquisition and automatic MWC parsing.We also used the following lexical knowledgebase for rule expansion and refinement: (1)Lexical relationship base.
It consists of 966953lexical pairs with different syntactic relationships.All the data are extracted from 4 differentlanguage resources.
(2) Verb subcategory data.
Itconsists of 5712 verbs with the ?v+np?
subcatframes and 1065 verbs with the ?v+vp?
subcatframes.
All the data are extracted from a Chinesegrammatical dictionary (Yu and al., 1998).
(3)Noun thesaura data.
It consists of 26906 nounsannotated with the different semantic types All thedata are extracted from Hownet-20002.4.1 Rule base acquisitionWe ran our algorithm on the above languageresources and obtained the following results.In the rule learning stage, we extracted 735basic rules from the training set.
After reliabilityevaluation, we obtained 61 HR rules and 150 lessreliable rules for further refinement.
Althoughthese 211 rules only make up 29% of all the 735acquired rules, they cover about 97% positiveexamples in the training set.
Thus, almost all theuseful information can be reserved for further ruleexpansion and refinement.In the rule refining stage, 47858 rules wereexpanded from the 150 basic rules.
Among them,all 2036 HR and 2362 MR rules were selected asthe final results.
They make up about 9% of all theexpanded rules, but cover 86% positive examples.It indicates the effectiveness of our current ruleacquisition algorithm.In order to evaluate the descriptive capability ofthe acquired rules objectively, we proposed anexpectation precision (EP) index to estimate theparsing accuracy when we apply the acquiredrules to all the positive examples in the training set.Its computation formula is as follows:?
?===NiPiiNiPi ffEP11/)*( ?where N is the total number of the rules in a rulebase, fPi and ?i are the positive example frequencyand confidence score of the ith rule in the rule base.An intuition assumption behind the EP definitionis that a rule base with higher EP index will implyits better descriptive capability for some speciallinguistic phenomena.
Therefore, its better parsingperformance in a rule-based parser can beexpected.
To prove this assumption, we designed a2 The data is available in http://www.keenage.com604simple comparison experiment to analyze theimprovement effects of different lexical andcontextual constraint used in our expanded rules.We divided all 150 basic rules into 4 subsets,according to their different internal structurecharacteristics: (1) Noun chunks with RCC andCH constructions; (2) Verb chunks with LCCconstructions; (3) Verb chunks with RCCconstructions; (4) All other MWCs.The rules in the subset 1 and 2 cover majority ofthe positive examples in the training set.
Theyhave complex internal structures and lexicalrelations.
So we applied the lexical knowledgebase and contextual constraint to expand them.Comparatively, the rules in subset 3 and 4 havesimpler structures, so we only used the contextualconstraint to expand them.Table 3 shows the EP indexes of these rulesubsets before and after rule refining.
For all 150basic rules, after rule expansion and refinement,the EP index was improved about 65%.
For thesimpler structure rules in subset 3 and 4, just theapplication of contextual constraint can bringdramatic improvement in the EP index.
Itindicates the importance of the local contextualinformation for multiword chunk recognition.SubsetRulesumCoveredpositiveexamplesEP beforeexpansion(%)EP afterexpansion(%)1 51 13689 52.70 81.402 20 8859 45.14 80.563 24 2342 28.12 93.274 55 3566 66.85 93.22Total 150 28456 50.56 83.36Table 3 Descriptive capability analysis ofdifferent kinds of expanded rule setsFor the major subset 1 and 2, EP index alsoshows great improvement.
It increased about 54%and 78% in the subset 1 and 2 respectively.
As wecan see, the applying effects of lexical andcontextual constraint on the verb chunks weresuperior to that on the noun chunks.
Two factorscontribute to this phenomenon.
First, the simplerinternal structures of most verb chunks guaranteethe availability of almost all corresponding lexicalrelationship pairs.
Second, most lexical pairs usedin verb chunks have stronger semantic relatednessthan that in noun chunks.4.2 Parsing performance evaluationBased on the rule base automatically acquiredthrough the above algorithm, we developed arule-based MWC parser to automaticallyrecognize different kinds of MWCs in theChinese sentences after word segmentation andPOS tagging.
Through ?-based disambiguationtechnique, the parser can output most reliableMWCs in the disambiguated region of a sentenceand keep some ambigous regions with lessreliable MWC structures to provide multipleselection possibilities for a full syntactic parser.Some detailed information of the parser can befound in (Zhou, 2007).We used three commonly-used indexes :precision, recall and F-measure to evaluate theperformance of the parser.
Two different criteriawere set to determinate the correctness of arecognized MWC.
(1) ?B+F+R?
criterion : Itmust have the same left and right boundaries,function tag and relation tag as that of the goldstandard.
(2) ?B+F?
criterion : It must have thesame left and right boundaries, function tag asthat of the gold standard.Table 4 shows the experimental results underthe disambigutated regions, which cover 95% ofthe test data.Type ?B+F+R?
criterion ?B+F?
criterionnp 75.25/75.76/75.50 83.68/84.25/83.97vp 83.23/81.46/82.34 87.35/85.49/86.41mp 94.89/95.26/95.08 94.89/95.26/95.08ap 93.99/97.33/95.63 93.99/97.33/95.63tp 92.75/88.18/90.40 93.52/88.92/91.16sp 78.76/86.41/82.41 79.65/87.38/83.33Total 81.76/81.44/81.60 87.01/86.67/86.84Table 4  Open test results (P/R/F-m, %)under the disambiguated regionsThe differences of F-measures among threeMWC subsets, i.e.
noun chunks, verb chunks andother chunks, show interesting positiveassociation with the differences of their EPindexes listed in the previous sections.
When weapply the acquired rule base with higher EPindex in the rule-based parser, we can get betterparsing performance.
It indicates that EP valuecan be used as an important objective index toevaluate the descriptive capability of the rulebase automatically acquired for large scaleannotated corpus.The lower F-measure of noun and verb chunk605under ?B+F+R?
criterion shows the difficulty forlexical relation recognition, especially for thecomplex noun chunks.
There are still muchimprovement room in future research.5 Related workIn the area of chunking rule acquisition andrefinement, several approaches have beenproposed.
Cardie and Pierce(1999) explored therole of lexicalization and pruning of grammars forbase noun phrase identification.
Their conclusionis that error-driven pruning is a remarkably robustmethod for improving the performance ofgrammar rules.
Dejean(2002) proposed atop-down inductive system, ALLis, for learningand refining linguistic structures on the base ofcontextual and lexicalization knowledge extractedfrom an annotated corpus.
Choi et al(2005)proposed a method for automatically extractingpartial parsing rules from a tree-annotated corpususing decision tree induction.
The acquiredgrammar is similar to a phrase structure grammar,with contextual and lexical information, but itallows building structures of depth one or more.All these researches prove the important roleof lexical and contextual information forimproving the rule descriptive capability.However, the lexical information used in thesesystems is still restricted in the lexical head of aconstituent.
None of the lexical relationshipknowledge extracted from the annotated corpus orother outside language resources has been applied.Therefore, the room for improvement of the ruledescriptive capability is restricted to a certainextent.6 ConclusionsThree main contributions of the paper aresummarized as follows.
(1) We design a newmultiword chunking task.
Based on thetopological structure definition, we establish thebuilt-in relations between multiword chunkexamples in annotated corpus and lexicalrelationship pairs in outside lexical knowledgebase.
(2) We propose an efficient algorithm toautomatically acquire hierarchical structure rulesfrom large-scale annotated corpus.
By introducingdifferent kinds of lexical knowledge coming fromseveral different language resources, we set up anopen learning environment for rule expansion andrefinement.
(3) We propose an expectationprecision index to evaluate the descriptivecapability of the refined rule base.
Experimentalresults show that it has stronger positiveassociation with the F-measure of parserperformance evaluation.Acknowledgements.
The research wassupported by NSFC (Grant No.
60573185,60520130299).
Thank the comments and advice ofthe anonymous reviewers.ReferencesSteven Abney.
1991.
Parsing by Chunks.
In R. Berwick,S.
Abney and C. Tenny (eds.)
Principle-BasedParsing, Kluwer Academic Publishers.Claire Cardie and D. Pierce.
1999.
The Role ofLexicalization and Pruning for Base Noun PhraseGrammars.
In Proceedings of the Sixteenth NationalConference on Artificial Intelligence (AAAI-99).X.
Carreras and L. M`arquez.
2005.
Introduction to theconll-2004 shared tasks: Semantic role labeling.
In Proc.
ofCoNLL-2005.Myung-Seok Choi, Chul Su Lim, and Key-Sun Choi.2005.
Automatic Partial Parsing Rule AcquisitionUsing Decision Tree Induction.
In R. Dale et al(Eds.).
Proc.
of IJCNLP 2005, Seoul, Korea .p143?154.Herve Dejean.
2002 Learning rules and their exceptions.Journal of Machine Learning Research, 2002:669?693.J R. Quinlan 1990.
Learning logical definitions fromrelations.
Machine Learning, 5:239?266.L Ramshaw and M Marcus.
1995.Text chunking usingtransformation-based learning.
In Proc.
of the ThirdWorkshop on Very Large Corpora, p82-94.Erik F. Tjong Kim Sang and S. Buchholz 2000Introduction to CoNLL-200 Shared Task: Chunking.In Proc.
of CoNLL-2000 and LLL-2000.
Lisbon.p127-132.Erik F. Tjong Kim Sang and H. D?jean 2001.Introduction to the CoNLL-2001 Shared Task:Clause Identification.
In Proc.
of CoNLL-2001,Toulouse, France, p53-57.Shiwen Yu, Xuefeng Zhu, et al 1998 A CompleteSpecification of the Grammatical Knowledge-base ofContemporary Chinese.
Tsinghua University Press.
(in Chinese)Qiang Zhou 2004.
Annotation scheme for ChineseTreebank.
Journal of Chinese InformationProcessing, 18(4): 1-8.
(in Chinese)Qiang Zhou.
2007.
A rule-based Chinese chunk parser.In Proc.
Of ICCC-2007, furthercoming.606
