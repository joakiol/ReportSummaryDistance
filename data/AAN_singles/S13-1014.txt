Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 103?108, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsDeepPurple: Lexical, String and Affective Feature Fusion for Sentence-LevelSemantic Similarity EstimationNikolaos Malandrakis1, Elias Iosif2, Vassiliki Prokopi2, Alexandros Potamianos2,Shrikanth Narayanan11Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA2Department of ECE, Technical University of Crete, 73100 Chania, Greecemalandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,shri@sipi.usc.eduAbstractThis paper describes our submission for the*SEM shared task of Semantic Textual Sim-ilarity.
We estimate the semantic similaritybetween two sentences using regression mod-els with features: 1) n-gram hit rates (lexicalmatches) between sentences, 2) lexical seman-tic similarity between non-matching words, 3)string similarity metrics, 4) affective contentsimilarity and 5) sentence length.
Domainadaptation is applied in the form of indepen-dent models and a model selection strategyachieving a mean correlation of 0.47.1 IntroductionText semantic similarity estimation has been an ac-tive research area, thanks to a variety of potential ap-plications and the wide availability of data affordedby the world wide web.
Semantic textual similar-ity (STS) estimates can be used for information ex-traction (Szpektor and Dagan, 2008), question an-swering (Harabagiu and Hickl, 2006) and machinetranslation (Mirkin et al 2009).
Term-level simi-larity has been successfully applied to problems likegrammar induction (Meng and Siu, 2002) and affec-tive text categorization (Malandrakis et al 2011).
Inthis work, we built on previous research and our sub-mission to SemEval?2012 (Malandrakis et al 2012)to create a sentence-level STS model for the sharedtask of *SEM 2013 (Agirre et al 2013).Semantic similarity between words has beenwell researched, with a variety of knowledge-based(Miller, 1990; Budanitsky and Hirst, 2006) andcorpus-based (Baroni and Lenci, 2010; Iosif andPotamianos, 2010) metrics proposed.
Moving tosentences increases the complexity exponentiallyand as a result has led to measurements of simi-larity at various levels: lexical (Malakasiotis andAndroutsopoulos, 2007), syntactic (Malakasiotis,2009; Zanzotto et al 2009), and semantic (Rinaldiet al 2003; Bos and Markert, 2005).
Machine trans-lation evaluation metrics can be used to estimate lex-ical level similarity (Finch et al 2005; Perez andAlfonseca, 2005), including BLEU (Papineni et al2002), a metric using word n-gram hit rates.
The pi-lot task of sentence STS in SemEval 2012 (Agirre etal., 2012) showed a similar trend towards multi-levelsimilarity, with the top performing systems utilizinglarge amounts of partial similarity metrics and do-main adaptation (the use of separate models for eachinput domain) (Ba?r et al 2012; S?aric?
et al 2012).Our approach is originally motivated by BLEUand primarily utilizes ?hard?
and ?soft?
n-gram hitrates to estimate similarity.
Compared to last year,we utilize different alignment strategies (to decidewhich n-grams should be compared with which).We also include string similarities (at the token andcharacter level) and similarity of affective content,expressed through the difference in sentence arousaland valence ratings.
Finally we added domain adap-tation: the creation of separate models per domainand a strategy to select the most appropriate model.2 ModelOur model is based upon that submitted for the sametask in 2012 (Malandrakis et al 2012).
To esti-mate semantic similarity metrics we use a super-vised model with features extracted using corpus-103based word-level similarity metrics.
To combinethese metrics into a sentence-level similarity scorewe use a modification of BLEU (Papineni et al2002) that utilizes word-level semantic similarities,string level comparisons and comparisons of affec-tive content, detailed below.2.1 Word level semantic similarityCo-occurrence-based.
The semantic similarity be-tween two words, wi and wj , is estimated as theirpointwise mutual information (Church and Hanks,1990): I(i, j) = log p?(i,j)p?(i)p?
(j) , where p?
(i) and p?
(j) arethe occurrence probabilities of wi and wj , respec-tively, while the probability of their co-occurrenceis denoted by p?
(i, j).
In our previous participationin SemEval12-STS task (Malandrakis et al 2012)we employed a modification of the pointwise mutualinformation based on the maximum sense similar-ity assumption (Resnik, 1995) and the minimizationof the respective error in similarity estimation.
Inparticular, exponential weights ?
were introduced inorder to reduce the overestimation of denominatorprobabilities.
The modified metric Ia(i, j), is de-fined as:Ia(i, j)=12[logp?
(i, j)p??(i)p?
(j) + logp?
(i, j)p?(i)p??(j)].
(1)The weight ?
was estimated on the corpus of (Iosifand Potamianos, 2012) in order to maximize wordsense coverage in the semantic neighborhood ofeach word.
The Ia(i, j) metric using the estimatedvalue of ?
= 0.8 was shown to significantlyoutperform I(i, j) and to achieve state-of-the-artresults on standard semantic similarity datasets(Rubenstein and Goodenough, 1965; Miller andCharles, 1998; Finkelstein et al 2002).Context-based: The fundamental assumptionbehind context-based metrics is that similarityof context implies similarity of meaning (Harris,1954).
A contextual window of size 2H + 1 wordsis centered on the word of interest wi and lexicalfeatures are extracted.
For every instance of wiin the corpus the H words left and right of wiformulate a feature vector vi.
For a given value ofH the context-based semantic similarity betweentwo words, wi and wj , is computed as the cosineof their feature vectors: QH(i, j) = vi.vj||vi|| ||vj || .The elements of feature vectors can be weightedaccording various schemes [(Iosif and Potamianos,2010)], while, here we use a binary scheme.Network-based: The aforementioned similaritymetrics were used for the definition of a semanticnetwork (Iosif and Potamianos, 2013; Iosif et al2013).
A number of similarity metrics were pro-posed under either the attributional similarity (Tur-ney, 2006) or the maximum sense similarity (Resnik,1995) assumptions of lexical semantics1.2.2 Sentence level similaritiesTo utilize word-level semantic similarities in thesentence-level task we use a modified version ofBLEU (Papineni et al 2002).
The model works intwo passes: the first pass identifies exact matches(similar to baseline BLEU), the second pass com-pares non-matched terms using semantic similarity.Non-matched terms from the hypothesis sentenceare compared with all terms of the reference sen-tence (regardless of whether they were matched dur-ing the first pass).
In the case of bigram and higherorder terms, the process is applied recursively: thebigrams are decomposed into two words and thesimilarity between them is estimated by applying thesame method to the words.
All word similarity met-rics used are peak-to-peak normalized in the [0,1]range, so they serve as a ?degree-of-match?.
The se-mantic similarity scores from term pairs are summed(just like n-gram hits) to obtain a BLEU-like hit-rate.Alignment is performed via maximum similarity:we iterate on the hypothesis n-grams, left-to-right,and compare each with the most similar n-gram inthe reference.
The features produced by this processare ?soft?
hit-rates (for 1-, 2-, 3-, 4-grams)2.
We alsouse the ?hard?
hit rates produced by baseline BLEUas features of the final model.2.3 String similaritiesWe use the following string-based similarity fea-tures: 1) Longest Common Subsequence Similarity(LCSS) (Lin and Och, 2004) based on the LongestCommon Subsequence (LCS) character-based dy-1The network-based metrics were applied only during thetraining phase of the shared task, due to time limitations.
Theyexhibited almost identical performance as the metric defined by(1), which was used in the test runs.2Note that the features are computed twice on each sentencepair and then averaged.104namic programming algorithm.
LCSS represents thelength of the longest string (or strings) that is a sub-string (or are substrings) of two or more strings.
2)Skip bigram co-occurrence measures the overlap ofskip-bigrams between two sentences or phrases.
Askip-bigram is defined as any pair of words in thesentence order, allowing for arbitrary gaps betweenwords (Lin and Och, 2004).
3) Containment is de-fined as the percentage of a sentence that is con-tained in another sentence.
It is a number between0 and 1, where 1 means the hypothesis sentence isfully contained in the reference sentence (Broder,1997).
We express containment as the amount of n-grams of a sentence contained in another.
The con-tainment metric is not symmetric and is calculatedas: c(X,Y ) = |S(X) ?
S(Y )|/S(X), where S(X)and S(Y ) are all the n-grams of sentences X and Yrespectively.2.4 Affective similarityWe used the method proposed in (Malandrakis et al2011) to estimate affective features.
Continuous (va-lence and arousal) ratings in [?1, 1] of any term arerepresented as a linear combination of a function ofits semantic similarities to a set of seed words andthe affective ratings of these words, as follows:v?
(wj) = a0 +N?i=1ai v(wi) dij , (2)where wj is the term we mean to characterize,w1...wN are the seed words, v(wi) is the valence rat-ing for seed word wi, ai is the weight correspondingto seed word wi (that is estimated as described next),dij is a measure of semantic similarity between wiandwj (for the purposes of this work, cosine similar-ity between context vectors is used).
The weights aiare estimated over the Affective norms for EnglishWords (ANEW) (Bradley and Lang, 1999) corpus.Using this model we generate affective ratings forevery content word (noun, verb, adjective or adverb)of every sentence.
We assume that these can ad-equately describe the affective content of the sen-tences.
To create an ?affective similarity metric?
weuse the difference of means of the word affective rat-ings between two sentences.d?affect = 2?
|?(v?(s1))?
?(v?
(s2))| (3)where ?(v?
(si)) the mean of content word ratings in-cluded in sentence i.2.5 FusionThe aforementioned features are combined usingone of two possible models.
The first model is aMultiple Linear Regression (MLR) modelD?L = a0 +k?n=1an fk, (4)where D?L is the estimated similarity, fk are the un-supervised semantic similarity metrics and an arethe trainable parameters of the model.The second model is motivated by an assumptionof cognitive scaling of similarity scores: we expectthat the perception of hit rates is non-linearly af-fected by the length of the sentences.
We call this thehierarchical fusion scheme.
It is a combination of(overlapping) MLR models, each matching a rangeof sentence lengths.
The first model DL1 is trainedwith sentences with length up to l1, i.e., l ?
l1, thesecond model DL2 up to length l2 etc.
During test-ing, sentences with length l ?
[1, l1] are decodedwith DL1, sentences with length l ?
(l1, l2] withmodel DL2 etc.
Each of these partial models is alinear fusion model as shown in (4).
In this work,we use four models with l1 = 10, l2 = 20, l3 = 30,l4 = ?.Domain adaptation is employed, by creating sep-arate models per domain (training data source).
Be-yond that, we also create a unified model, trainedon all data to be used as a fallback if an appropriatemodel can not be decided upon during evaluation.3 Experimental Procedure and ResultsInitially all sentences are pre-processed by theCoreNLP (Finkel et al 2005; Toutanova et al2003) suite of tools, a process that includes namedentity recognition, normalization, part of speech tag-ging, lemmatization and stemming.
We evaluatedmultiple types of preprocessing per unsupervisedmetric and chose different ones depending on themetric.
Word-level semantic similarities, used forsoft comparisons and affective feature extraction,were computed over a corpus of 116 million websnippets collected by posing one query for everyword in the Aspell spellchecker (asp, ) vocabulary tothe Yahoo!
search engine.
Word-level emotional rat-ings in continuous valence and arousal scales wereproduced by a model trained on the ANEW dataset105and using contextual similarities.
Finally, string sim-ilarities were calculated over the original unmodifiedsentences.Next, results are reported in terms of correla-tion between the generated scores and the groundtruth, for each corpus in the shared task, as well astheir weighted mean.
Feature selection is appliedto the large candidate feature set using a wrapper-based backward selection approach on the train-ing data.The final feature set contains 15 features:soft hit rates calculated over content word 1- to 4-grams (4 features), soft hit rates calculated over un-igrams per part-of-speech, for adjectives, nouns, ad-verbs, verbs (4 features), BLEU unigram hit ratesfor all words and content words (2 features), skipand containment similarities, containment normal-ized by sum of sentence lengths or product of sen-tence lengths (3 features) and affective similaritiesfor arousal and valence (2 features).Domain adaptation methods are the only dif-ference between the three submitted runs.
For allthree runs we train one linear model per training setand a fallback model.
For the first run, dubbed lin-ear, the fallback model is linear and model selectionduring evaluation is performed by file name, there-fore results for the OnWN set are produced by amodel trained with OnWN data, while the rest areproduced by the fallback model.
The second run,dubbed length, uses a hierarchical fallback modeland model selection is performed by file name.
Thethird run, dubbed adapt, uses the same models asthe first run and each test set is assigned to a model(i.e., the fallback model is never used).
The test set -model (training) mapping for this run is: OnWN ?OnWN, headlines ?
SMTnews, SMT ?
Europarland FNWN?
OnWN.Table 1: Correlation performance for the linear model us-ing lexical (L), string (S) and affect (A) featuresFeature headl.
OnWN FNWN SMT meanL 0.68 0.51 0.23 0.25 0.46L+S 0.69 0.49 0.23 0.26 0.46L+S+A 0.69 0.51 0.27 0.28 0.47Results are shown in Tables 1 and 2.
Results forthe linear run using subsets of the final feature setare shown in Table 1.
Lexical features (hit rates) areobviously the most valuable features.
String similar-ities provided us with an improvement in the train-Table 2: Correlation performance on the evaluation set.Run headl.
OnWN FNWN SMT meanlinear 0.69 0.51 0.27 0.28 0.47length 0.65 0.51 0.25 0.28 0.46adapt 0.62 0.51 0.33 0.30 0.46ing set which is not reflected in the test set.
Af-fect proved valuable, particularly in the most diffi-cult sets of FNWN and SMT.Results for the three submission runs are shownin Table 2.
Our best run was the simplest one, usinga purely linear model and effectively no adaptation.Adding a more aggressive adaptation strategy im-proved results in the FNWN and SMT sets, so thereis definitely some potential, however the improve-ment observed is nowhere near that observed in thetraining data or the same task of SemEval 2012.
Wehave to question whether this improvement is an ar-tifact of the rating distributions of these two sets(SMT contains virtually only high ratings, FNWNcontains virtually only low ratings): such wild mis-matches in priors among training and test sets canbe mitigated using more elaborate machine learningalgorithms (rather than employing better semanticsimilarity features or algorithms).
Overall the sys-tem performs well in the two sets containing largesimilarity rating ranges.4 ConclusionsWe have improved over our previous model of sen-tence semantic similarity.
The inclusion of string-based similarities and more so of affective contentmeasures proved significant, but domain adaptationprovided mixed results.
While expanding the modelto include more layers of similarity estimates isclearly a step in the right direction, further work isrequired to include even more layers.
Using syntac-tic information and more levels of abstraction (e.g.concepts) are obvious next steps.5 AcknowledgementsThe first four authors have been partially fundedby the PortDial project (Language Resources forPortable Multilingual Spoken Dialog Systems) sup-ported by the EU Seventh Framework Programme(FP7), grant number 296170.106ReferencesE.
Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.2012.
Semeval-2012 task 6: A pilot on semantic tex-tual similarity.
In Proc.
SemEval, pages 385?393.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In Proc.
*SEM.Gnu aspell.
http://www.aspell.net.D.
Ba?r, C. Biemann, I. Gurevych, and T. Zesch.
2012.Ukp: Computing semantic textual similarity by com-bining multiple content similarity measures.
In Proc.SemEval, pages 435?440.M.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based semantics.Computational Linguistics, 36(4):673?721.J.
Bos and K. Markert.
2005.
Recognising textual en-tailment with logical inference.
In Proceedings of theHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, page 628635.M.
Bradley and P. Lang.
1999.
Affective norms for En-glish words (ANEW): Stimuli, instruction manual andaffective ratings.
Technical report C-1.
The Center forResearch in Psychophysiology, University of Florida.Andrei Z. Broder.
1997.
On the resemblance and con-tainment of documents.
In In Compression and Com-plexity of Sequences (SEQUENCES97, pages 21?29.IEEE Computer Society.A.
Budanitsky and G. Hirst.
2006.
Evaluating WordNet-based measures of semantic distance.
ComputationalLinguistics, 32:13?47.K.
W. Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational Linguistics, 16(1):22?29.A.
Finch, S. Y. Hwang, and E. Sumita.
2005.
Using ma-chine translation evaluation techniques to determinesentence-level semantic equivalence.
In Proceedingsof the 3rd International Workshop on Paraphrasing,page 1724.J.
R. Finkel, T. Grenager, and C. D. Manning.
2005.
In-corporating non-local information into information ex-traction systems by gibbs sampling.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics, pages 363?370.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
ACMTransactions on Information Systems, 20(1):116?131.S.
Harabagiu and A. Hickl.
2006.
Methods for Us-ing Textual Entailment in Open-Domain Question An-swering.
In Proceedings of the 21st InternationalCon-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 905?912.Z.
Harris.
1954.
Distributional structure.
Word,10(23):146?162.E.
Iosif and A. Potamianos.
2010.
Unsupervised seman-tic similarity computation between terms using webdocuments.
IEEE Transactions on Knowledge andData Engineering, 22(11):1637?1647.E.
Iosif and A. Potamianos.
2012.
Semsim: Resourcesfor normalized semantic similarity computation usinglexical networks.
In Proc.
Eighth International Con-ference on Language Resources and Evaluation, pages3499?3504.Elias Iosif and Alexandros Potamianos.
2013.
SimilarityComputation Using Semantic Networks Created FromWeb-Harvested Data.
Natural Language Engineering,(submitted).E.
Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-vanou.
2013.
Semantic similarity computation for ab-stract and concrete nouns using network-based distri-butional semantic models.
In 10th International Con-ference on Computational Semantics (IWCS), pages328?334.Chin-Yew Lin and Franz Josef Och.
2004.
Automaticevaluation of machine translation quality using longestcommon subsequence and skip-bigram statistics.
InProceedings of the 42nd Annual Meeting on Associa-tion for Computational Linguistics, ACL ?04, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.P.
Malakasiotis and I. Androutsopoulos.
2007.
Learn-ing textual entailment using svms and string similar-ity measures.
In Proceedings of of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing,pages 42?47.P.
Malakasiotis.
2009.
Paraphrase recognition using ma-chine learning to combine similarity measures.
In Pro-ceedings of the 47th Annual Meeting of ACL and the4th Int.
Joint Conference on Natural Language Pro-cessing of AFNLP, pages 42?47.N.
Malandrakis, A. Potamianos, E. Iosif, andS.
Narayanan.
2011.
Kernel models for affec-tive lexicon creation.
In Proc.
Interspeech, pages2977?2980.N.
Malandrakis, E. Iosif, and A. Potamianos.
2012.DeepPurple: Estimating sentence semantic similarityusing n-gram regression models and web snippets.
InProc.
Sixth International Workshop on Semantic Eval-uation (SemEval) ?
The First Joint Conference onLexical and Computational Semantics (*SEM), pages565?570.H.
Meng and K.-C. Siu.
2002.
Semi-automatic acquisi-tion of semantic structures for understanding domain-107specific natural language queries.
IEEE Transactionson Knowledge and Data Engineering, 14(1):172?181.G.
Miller and W. Charles.
1998.
Contextual correlatesof semantic similarity.
Language and Cognitive Pro-cesses, 6(1):1?28.G.
Miller.
1990.
Wordnet: An on-line lexical database.International Journal of Lexicography, 3(4):235?312.S.
Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-man, and S. Idan.
2009.
Source-language entailmentmodeling for translating unknown terms.
In Proceed-ings of the 47th AnnualMeeting of ACL and the 4th Int.Joint Conference on Natural Language Processing ofAFNLP, pages 791?799.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 311?318.D.
Perez and E. Alfonseca.
2005.
Application of thebleu algorithm for recognizing textual entailments.
InProceedings of the PASCAL Challenges Worshop onRecognising Textual Entailment.P.
Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxanomy.
In Proc.
of In-ternational Joint Conference for Artificial Intelligence,pages 448?453.F.
Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, andD.
Molla.
2003.
Exploiting paraphrases in a questionanswering system.
In Proceedings of the 2nd Interna-tional Workshop on Paraphrasing, pages 25?32.H.
Rubenstein and J.
B. Goodenough.
1965.
Contextualcorrelates of synonymy.
Communications of the ACM,8(10):627?633.I.
Szpektor and I. Dagan.
2008.
Learning entailmentrules for unary templates.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics, pages 849?856.K.
Toutanova, D. Klein, C. D. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proceedings of Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology, pages 173?180.P.
Turney.
2006.
Similarity of semantic relations.
Com-putational Linguistics, 32(3):379?416.F.
S?aric?, G.
Glavas?, M. Karan, J.
S?najder, and B. Dal-belo Bas?ic?.
2012.
Takelab: Systems for measuringsemantic text similarity.
In Proc.
SemEval, pages 441?448.F.
Zanzotto, M. Pennacchiotti, and A. Moschitti.2009.
A machine-learning approach to textual en-tailment recognition.
Natural Language Engineering,15(4):551582.108
