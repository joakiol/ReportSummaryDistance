Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 446?456,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsGrammatical Relations in Chinese:GB-Ground Extraction and Data-Driven ParsingWeiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding, Xiaojun Wan?Institute of Computer Science and Technology, Peking UniversityThe MOE Key Laboratory of Computational Linguistics, Peking University{ws,duyantao,kouxin,wanxiaojun}@pku.edu.cn, dsy100@gmail.comAbstractThis paper is concerned with building linguistic re-sources and statistical parsers for deep grammaticalrelation (GR) analysis of Chinese texts.
A set oflinguistic rules is defined to explore implicit phrasestructural information and thus build high-qualityGR annotations that are represented as general di-rected dependency graphs.
The reliability of thislinguistically-motivated GR extraction procedure ishighlighted by manual evaluation.
Based on theconverted corpus, we study transition-based, data-driven models for GR parsing.
We present a noveltransition system which suits GR graphs better thanexisting systems.
The key idea is to introduce a newtype of transition that reorders top k elements in thememory module.
Evaluation gauges how successfulGR parsing for Chinese can be by applying data-driven models.1 IntroductionGrammatical relations (GRs) represent functionalrelationships between language units in a sen-tence.
They are exemplified in traditional gram-mars by the notions of subject, direct/indirectobject, etc.
GRs have assumed an importantrole in linguistic theorizing, within a variety ofapproaches ranging from generative grammar tofunctional theories.
For example, several com-putational grammar formalisms, such as Lexi-cal Function Grammar (LFG; Bresnan and Ka-plan, 1982; Dalrymple, 2001) and Head-drivenPhrase Structure Grammar (HPSG; Pollard andSag, 1994) encode grammatical functions directly.In particular, GRs can be viewed as the depen-dency backbone of an LFG analysis that providegeneral linguistic insights, and have great potentialadvantages for NLP applications, (Kaplan et al,2004; Briscoe and Carroll, 2006; Clark and Cur-ran, 2007a; Miyao et al, 2007).
?Email correspondence.In this paper, we address the question of an-alyzing Chinese sentences with deep GRs.
Toacquire high-quality GR corpus, we propose alinguistically-motivated algorithm to translate aGovernment and Binding (GB; Chomsky, 1981;Carnie, 2007) grounded phrase structure treebank,i.e.
Chinese Treebank (CTB; Xue et al, 2005)to a deep dependency bank where GRs are ex-plicitly represented.
Different from popular shal-low dependency parsing that focus on tree-shapedstructures, our GR annotations are represented asgeneral directed graphs that express not only lo-cal but also various long-distance dependencies,such as coordinations, control/raising construc-tions, topicalization, relative clauses and manyother complicated linguistic phenomena that goesbeyond shallow syntax (see Fig.
1 for example.
).Manual evaluation highlights the reliability of ourlinguistically-motivated GR extraction algorithm:The overall dependency-based precision and recallare 99.17 and 98.87.
The automatically-convertedcorpus would be of use for a wide variety of NLPtasks.Recent years have seen the introduction of anumber of treebank-guided statistical parsers ca-pable of generating considerably accurate parsesfor Chinese.
With the high-quality GR resourceat hand, we study data-driven GR parsing.
Previ-ous work on dependency parsing mainly focusedon structures that can be represented in terms ofdirected trees.
We notice two exceptions.
Sagaeand Tsujii (2008) and Titov et al (2009) individ-ually studied two transition systems that can gen-erate more general graphs rather than trees.
In-spired by their work, we study transition-basedmodels for building deep dependency structures.The existence of a large number of crossing arcs inGR graphs makes left-to-right, incremental graphspanning computationally hard.
Applied to ourdata, the two existing systems cover only 51.0%and 76.5% GR graphs respectively.
To better suit446??
??
?
??
??
?
??
??
??
?
???
?
?Pudong recently issue practice involve economic field regulatory documentroot rootcomptemptempsubjsubjprtprtobjobjcompsubj*lddobjnmodrelativenmodFigure 1: An example: Pudong recently enacted regulatory documents involving the economic field.The symbol ?*ldd?
indicates long-distance dependencies; ?subj*ldd?
between the word ??
?/involve?and the word ???/documents?
represents a long-range subject-predicate relation.
The arguments andadjuncts of the coordinated verbs, namely ???/issue?
and ???/practice,?
are separately yet distribu-tively linked the two heads.our problem, we extend Titov et al?s work andstudy what we call K-permutation transition sys-tem.
The key idea is to introduce a new type oftransition that reorders top k (2 ?
k ?
K) el-ements in the memory module of a stack-basedtransition system.
With the increase of K, the ex-pressiveness of the corresponding system strictlyincreases.
We propose an oracle deriving methodwhich is guaranteed to find a sound transition se-quence if one exits.
Moreover, we introduce aneffective approximation of that oracle, which de-creases decoding ambiguity but practically coversalmost exactly the same graphs for our data.Based on the stronger transition system, webuild a GR parser with a discriminative model fordisambiguation and a beam decoder for inference.We conduct experiments on CTB 6.0 to profile thisparser.
With the increase of the K, the parser isable to utilize more GR graphs for training andthe numeric performance is improved.
Evaluationgauges how successful GR parsing for Chinesecan be by applying data-driven models.
Detailedanalysis reveal some important factors that maypossibly boost the performance.
To our knowl-edge, this work provides the first result of exten-sive experiments of parsing Chinese with GRs.We release our GR processing kit and gold-standard annotations for research purposes.
Theseresources can be downloaded at http://www.icst.pku.edu.cn/lcwm/omg.2 GB-grounded GR ExtractionIn this section, we discuss the construction of theGR annotations.
Basically, the annotations are au-tomatically converted from a GB-grounded phrase-structure treebank, namely CTB.
Conceptually,this conversion is similar to the conversions fromCTB structures to representations in deep gram-mar formalisms (Tse and Curran, 2010; Yu et al,2010; Guo et al, 2007; Xia, 2001).
However, ourwork is grounded in GB, which is the linguistic ba-sis of the construction of CTB.
We argue that thistheoretical choice makes the conversion processmore compatible with the original annotations andtherefore more accurate.
We use directed graphs toexplicitly encode bi-lexical dependencies involvedin coordination, raising/control constructions, ex-traction, topicalization, and many other compli-cated phenomena.
Fig.
1 shows an example ofsuch a GR graph and its original CTB annotation.2.1 Linguistic BasisGRs are encoded in different ways in different lan-guages.
In some languages, e.g.
Turkish, gram-matical function is encoded by means of morpho-logical marking, while in highly configurationallanguages, e.g.
Chinese, the grammatical functionof a phrase is heavily determined by its constituentstructure position.
Dominant Chomskyan theo-ries, including GB, have defined GRs as configu-rations at phrase structures.
Following this princi-ple, CTB groups words into constituents throughthe use of a limited set of fundamental grammat-ical functions.
Transformational grammar utilizesempty categories (ECs) to represent long-distancedependencies.
In CTB, traces are provided byrelating displaced linguistic material to where itshould be interpreted semantically.
By exploitingconfigurational information, traces and functionaltag annotations, GR information can be hopefully447IPVP?=?VP?=?NP?=(?
OBJ)NP?=?NN?=???NN??(?
NMOD)???CP??(?
REL)DEC?=??IP?=(?
COMP)VP?=?NP?=(?
OBJ)NN?=???NP??(?
NMOD)NN?=???VV?=???NP?=(?
SBJ)-NONE-*T*AS?=(?
PRT)?VCD?=?VV?????VV?????LCP??(?
TMP)LC?=??NP?=(?
COMP)NT?=???NP?=(?
SBJ)NR?
?Figure 2: The original CTB annotation augmented with LFG-like f-structure annotations of the runningexample.derived from CTB trees with high accuracy.2.2 The Extraction AlgorithmOur treebank conversion algorithm borrowskey insights from Lexical Functional Grammar(LFG; Bresnan and Kaplan, 1982; Dalrymple,2001).
LFG posits two levels of representation:c(onstituent)-structure and f(unctional)-structureminimally.
C-structure is represented by phrase-structure trees, and captures surface syntactic con-figurations such as word order, while f-structureencodes grammatical functions.
It is easy to ex-tract a dependency backbone which approximatesbasic predicate-argument-adjunct structures fromf-structures.
The construction of the widely usedPARC DepBank (King et al, 2003) is a good ex-ample.LFG relates c-structure and f-structure throughf-structure annotations, which compositionallymap every constituent to a corresponding f-structure.
Borrowing this key idea, we translateCTB trees to dependency graphs by first augment-ing each constituency with f-structure annotations,then propagating the head words of the head orconjunct daughter(s) upwards to their parents, andfinally creating a dependency graph.
The follow-ing presents details step-by-step.Tapping implicit information.
Xue (2007) in-troduced a systematic study to tap the implicitfunctional information of CTB.
This gives us avery good start to extract GRs.
We slightly modifytheir method to enrich a CTB tree with f-structureannotations: Each node in a resulting tree is anno-tated with one and only one corresponding equa-tion.
See Fig.
2 for example.
Comparing the orig-inal annotation and enriched one, we can see thatthe functionality of this step is to explicitly repre-sent and regulate grammatical functions.Beyond CTB annotations: tracing more.
Nat-ural languages do not always interpret linguisticmaterial locally.
In order to obtain accurate andcomplete GR, predicate-argument, or logical formrepresentations, a hallmark of deep grammars isthat they usually involve a non-local dependencyresolution mechanism.
CTB trees utilize ECs andcoindexed materials to represent long-distance de-pendencies.
An EC is a nominal element that doesnot have any phonological content and is thereforeunpronounced.
Two kinds of anaphoric ECs, i.e.big PRO and trace, are annotated in CTB.
Theo-retically speaking, only trace is generated as theresult of movement and therefore annotated withantecedents in CTB.
We carefully check the anno-tation and find that a considerable amount of an-tecedents are not labeled, and hence a lot of impor-448VP{??,??}VP{??,??}NP{??}NP{??}CP{?}DEC{?}IP{??}VP{??}NP{??*ldd}AS{?}VP{??,??}LCP{?
}Figure 3: An example of lexicalized tree afterhead word upward passing.
Only partial result isshown.
The long-distance dependency between???/involve?
and ???/document?
is createdthrough copying the dependent to a coindexedanaphoric EC position.tant non-local information is missing.
In addition,since the big PRO is also anaphoric, it is possibleto find coindexed components sometimes.
Suchnon-local information is also very valuable.Beyond CTB annotations, we introduce a num-ber of phrase-structure patterns to extract morenon-local dependencies.
The method heavilyleverages linguistic rules to exploit structural in-formation.
We take into account both theoreti-cal assumptions and analyzing practices to enrichcoindexation information according to phrase-structure patterns.
In particular, we try to linkan anaphoric EC e with its c-commonders if nonon-empty antecedent has already been coindexedwith e. Because the CTB is influenced deeply bythe X-bar syntax, which regulates constituent anal-ysis much, the number of our linguistic rules isquite modest.
For the development of conversionrules, we used the first 9 files of CTB, which con-tains about 100 sentences.
Readers can refer tothe well-documented Perl script for details.
SeeFig.
2 for example.
The noun phrase ?????
?/regulatory documents?
is related to the trace?*T*.?
This coindexation is not labeled by theoriginal annotation.Passing head words and linking ECs.
Basedon an enriched tree, our algorithm propagates thehead word of the head daughter upwards to theirparents, linking coindexed units, and finally creat-ing a GR graph.
The partial result after head wordpassing of the running example is shown in Fig.
3.There are two differences of the head word passingbetween our GR extraction and a ?normal?
depen-dency tree extraction.
First, the GR extraction pro-cedure may pass multiple head words to its parent,especially in a coordination construction.
Second,Precision Recall F-scoreUnlabeled 99.48 99.17 99.32Labeled 99.17 98.87 99.02Table 1: Manual evaluation of 209 sentences.long-distance dependencies are created by linkingECs and their coindexed phrases.2.3 Manual EvaluationTo have a precise understanding of whether our ex-traction algorithm works well, we have selected 20files that contains 209 sentences in total for man-ual evaluation.
Linguistic experts carefully exam-ine the corresponding GR graphs derived by ourextraction algorithm and correct all errors.
In otherwords, a gold standard GR annotation set is cre-ated.
The measure for comparing two dependencygraphs is precision/recall of GR tokens which aredefined as ?wh, wd, l?
tuples, wherewhis the head,wdis the dependent and l is the relation.
Labeledprecision/recall (LP/LR) is the ratio of tuples cor-rectly identified by the automatic generator, whileunlabeled precision/recall (UP/UR) is the ratio re-gardless of l. F-score is a harmonic mean of pre-cision and recall.
These measures correspond toattachment scores (LAS/UAS) in dependency treeparsing.
To evaluate our GR parsing models thatwill be introduced later, we also report these met-rics.The overall performance is summarized in Tab.1.
We can see that the automatical GR extractionachieves relatively high performance.
There aretwo sources of errors in treebank conversion: (1)inadequate conversion rules and (2) wrong or in-consistent original annotations.
During the cre-ation of the gold standard corpus, we find thatthe former is mainly caused by complicated un-bounded dependencies and the lack of internalstructure for some kinds of phrases.
Such prob-lems are very hard to solve through rules only, ifnot possible, since original annotations do not pro-vide sufficient information.
The latter problem ismore scattered and unpredictable.2.4 StatisticsAllowing non-projective dependencies generallymakes parsing either by graph-based or transition-based dependency parsing harder.
Substantial re-search effort has been devoted in recent years tothe design of elegant solutions for this problem.There are much more crossing arcs in the GR449graphs than syntactic dependency trees.
In thetraining data (defined in Section 4.1), there are558132 arcs and 86534 crossing pairs, About halfof the sentences have crossing arcs (10930 out of22277).
The wide existence of crossing arcs posesan essential challenge for GR parsing, namely, tofind methods for handling crossing arcs without asignificant loss in accuracy and efficiency.3 Transition-based GR ParsingThe availability of large-scale treebanks has con-tributed to the blossoming of statistical approachesto build accurate shallow constituency and depen-dency parsers.
With high-quality GR resources athand, it is possible to study statistical approachesto automatically parse GR graphs.
In this section,we investigate the feasibility of applying a data-driven, grammar-free approach to build GRs di-rectly.
In particular, transition-based dependencyparsing method is studied.3.1 Data-Driven Dependency ParsingData-driven, grammar-free dependency parsinghas received an increasing amount of attention inthe past decade.
Such approaches, e.g.
transition-based (Yamada and Matsumoto, 2003; Nivre,2008) and graph-based (McDonald, 2006; Tor-res Martins et al, 2009) models have attractedthe most attention of dependency parsing in re-cent years.
Transition-based parsers utilize tran-sition systems to derive dependency trees togetherwith treebank-induced statistical models for pre-dicting transitions.
This approach was pioneeredby (Yamada and Matsumoto, 2003) and (Nivreet al, 2004).
Most research concentrated on sur-face syntactic structures, and the majority of ex-isting approaches are limited to producing onlytrees.
We notice two exceptions.
Sagae and Tsu-jii (2008) and Titov et al (2009) individually in-troduced two transition systems that can generatespecific graphs rather than trees.
Inspired by theirwork, we study transition-based approach to buildGR graphs.3.2 Transition SystemsFollowing (Nivre, 2008), we define a transitionsystem for dependency parsing as a quadruple S =(C, T, cs, Ct), where1.
C is a set of configurations, each of whichcontains a buffer ?
of (remaining) words anda set A of dependency arcs,TransitionsSHIFT (?, j|?,A)?
(?|j, ?,A)LEFT-ARCl(?|i, j|?,A)?
(?|i, j|?,A ?
{(j, l, i)})RIGHT-ARCl(?|i, j|?,A)?
(?|i, j|?,A ?
{(i, l, j)})POP (?|i, ?, A)?
(?, ?,A)ROTATEk(?|ik| .
.
.
|i2|i1, ?, A)?
(?|i1|ik| .
.
.
|i2, ?, A)Table 2: K-permutation System.2.
T is a set of transitions, each of which is a(partial) function t : C 7?
C,3.
csis an initialization function, mapping asentence x to a configuration, with ?
=[1, .
.
.
, n],4.
Ct?
C is a set of terminal configurations.Given a sentence x = w1, .
.
.
, wnand a graphG = (V,A) on it, if there is a sequence of tran-sitions t1, .
.
.
, tmand a sequence of configura-tions c0, .
.
.
, cmsuch that c0= cs(x), ti(ci?1) =ci(i = 1, .
.
.
,m), cm?
Ct, and Acm= A, we saythe sequence of transitions is an oracle sequence.And we define?Aci= A ?
Acifor the arcs to bebuilt in ci.
In a typical transition-based parsingprocess, the input words are put into a queue andpartially built structures are organized by a stack.A set of SHIFT/REDUCE actions are performed se-quentially to consume words from the queue andupdate the partial parsing results.3.3 Online ReorderingAmong existing systems, Sagae and Tsujii?s is de-signed for projective graphs (denoted by G1inDefinition 1), and Titov et al?s handles only aspecific subset of non-projective graphs as wellas projective graphs (G2).
Applied to our data,only 51.0% and 76.5% of the extracted graphs areparsable with their systems.
Obviously, it is nec-essary to investigate new transition systems for theparsing task in our study.
To deal with crossingarcs, Titov et al (2009) and Nivre (2009) designeda SWAP transition that switches the position of thetwo topmost nodes on the stack.
Inspired by theirwork, we extend this approach to parse more gen-eral graphs.
The basic idea is to provide our newsystem with an ability to reorder more nodes dur-ing decoding in an online fashion, which we referto as online reordering.3.4 K-Permutation SystemWe define a K-permutation transition systemSK= (C, T, cs, Ct), where a configuration c =450(?, ?,A) ?
C contains a stack ?
of nodes be-sides ?
and A.
We set the initial configurationfor a sentence x = w1, .
.
.
, wnto be cs(x) =([], [1, .
.
.
, n], {}), and take Ctto be the set of allconfigurations of the form ct= (?, [], A) (for anyarc set A).
The set of transitions T contains fivetypes of actions, as shown in Tab.
2:1.
SHIFT removes the front element from ?
andpushes it onto ?.2.
LEFT-ARCl/RIGHT-ARClupdates a configu-ration by adding (i, l, j)/(j, l, i) to A where iis the top of ?, and j is the front of ?.3.
POP deletes the top element of ?.4.
ROTATEkupdates a configuration with stack?|ik| .
.
.
|i2|i1by rotating the top k nodesin stack left by one index, obtaining?|i1|ik| .
.
.
|i2, with constraint 2 ?
k ?
K.We refer to this system as K-permutation becauseby rotating the top k (2 ?
k ?
K) nodes in thestack, we can obtain all the permutations of thetop K nodes.
Note that S2is identical to Titovet al?s; S?is complete with respect to the class ofall directed graphs without self-loop, since we canarbitrarily permute the nodes in the stack.
The K-permutation system exhibits a nice property: Thesets of corresponding graphs are strictly mono-tonic with respect to the ?
operation.Definition 1.
If a graphG can be parsed with tran-sition system SK, we say G is a K-perm graph.We use GKto denote the set of all k-perm graphs.Specially, G0= ?, G1is the set of all projectivegraphs, and G?=?
?k=0Gk.Theorem 1.
Gi( Gi+1,?i ?
0.Proof.
It is obvious that Gi?
Gi+1and G0( G1.Fig.
4 gives an example which is in Gi+1but notin Gifor all i > 0, indicating Gi6= Gi+1.Theorem 2.
G?is the set of all graphs withoutself-loop.Proof.
It follows immediately from the fact thatG ?
G|V |, ?G = ?V,E?.The transition systems introduced in (Sagae andTsujii, 2008) and (Titov et al, 2009) can be viewedas S11and S2.1Though Sagae and Tsujii (2008) introduced additionalconstraints to exclude cyclic path, the fundamental transitionmechanism of their system is the same to S1.w1?
?
?
wiwi+1?
?
?
w2iw2i+1w2i+2Figure 4: A graph which is in Gi+1, but not in Gi.3.5 Normal Form OracleThe K-permutation transition system may allowmultiple oracle transition sequences on one graph,but trying to sum all the possible oracles is usu-ally computational expensive.
Here we give a con-struction procedure which is guaranteed to find anoracle sequence if one exits.
We refer it as normalform oracle (NFO).Let L(j) be the ordered list of nodes connectedto j in?Aci?1for j ?
?ci?1, and let LK(?ci?1) =[L(j1), .
.
.
, L(jmax{l,K})].
If ?ci?1is empty, thenwe set tito SHIFT; if there is no arc linked toj1in?Aci?1, then we set tito POP; if there exitsa ?
?Aci?1linking j1and b, then we set tito LEFT-ARC or RIGHT-ARC correspondingly.
When thereare only SHIFT and ROTATE left, we first applya sequence of ROTATE?s to make LK(?)
com-plete ordered by lexicographical order, then applya SHIFT.
Let ci= ti(ci?1), we continue to com-pute ti+1, until ?ciis empty.Theorem 3.
If a graph is parsable with the transi-tion system SKthen the construction procedure isguaranteed to find an oracle transition sequence.Proof.
During the construction, all the arcs arebuilt by LEFT-ARC or RIGHT-ARC, which linksthe top of the stack and the front of the buffer.Therefore, we prefer L(?)
to be as orderly as pos-sible, to make the words to be linked sooner on thetop of the stack.
the construction procedure abovedoes best within the power of the system SK.3.6 An Approximation for NFOIn the construction of NFO transitions, we ex-haustively use the ROTATE?s to make L(?)
com-plete ordered.
We also observed that the tran-sition LEFT-ARC, RIGHT-ARC and SHIFT onlychange the relative order between the first elementof L(?)
and the rest elements.
Therefore we ex-plored an approximate procedure to determine theROTATE?s, based on the observation.
We call it ap-proximate NFO (ANFO).
Using notation definedin Section 3.5, the approximate procedure goes asfollows.
When it comes to the determination of451w1w2w3w4w5w6w7w8w9Figure 5: A graph that can be parsedwith S3with a transition sequenceSSSSR3SR3APAPR2R3SR3SR3APAPAPAPAP,where S stands for SHIFT, R for ROTATE, A forLEFT-ARC, and P for POP.
But the approximateprocedure fails to find the oracle, since R2R3inbold in the sequence are not to be applied.the ROTATE sequence, let k be the largest m suchthat 0 ?
m ?
min{K, l} and L(jm) strictly pre-cedes L(j1) by the lexicographical order (here weassume L(j0) strictly precedes any L(j), j ?
?
).If k > 0, we set tito ROTATEk; else we set titoSHIFT.
The approximation assumes L(?)
is com-pletely ordered except the first element, and insertthe first element to its proper place each time.Definition 2.
We define?GKas the graphs the ora-cle of which can be extracted by SKwith the ap-proximation procedure.It can be inferred similarly that Theorem 1 andTheorem 2 also hold for?G?s.
However, the?GKisnot equal to GKin non-trivial cases.Theorem 4.?Gi( Gi,?i ?
3.Proof.
It is trivial that?Gi?
Gi.
An example graphthat is in G3but not in?G3is shown in Figure 5,examples for arbitrary i > 3 can be constructedsimilarly.The above theorem indicates the inadequacy ofthe ANFO deriving procedure.
Nevertheless, em-pirical evaluation (Section 4.2) shows that the cov-erage of AFO and ANFO deriving procedures arealmost identical when applying to linguistic data.3.7 Statistical ParsingWhen we parse a sentence w1w2?
?
?wn, we startwith the initial configuration c0= cs(x), andchoose next transition ti= C(ci?1) iteratively ac-cording to a discriminative classifier trained on or-acle sequences.
To build a parser, we use a struc-tured classifier to approximate the oracle, and ap-ply the Passive-Aggressive (PA) algorithm (Cram-mer et al, 2006) for parameter estimation.
ThePA algorithm is similar to the Perceptron algo-rithm, the difference from which is the update ofweight vector.
We also use parameter averagingand early update to achieve better training.
Devel-oping features has been shown crucial to advanc-ing the state-of-the-art in dependency tree parsing(Koo and Collins, 2010; Zhang and Nivre, 2011).To build accurate deep dependency parsers, weutilize a large set of features for disambiguation.See the notes included in the supplymentary ma-terial for details.
To improve the performance, wealso apply the technique of beam search, whichkeep a beam of transition sequences with highestscores when parsing.4 Experiments4.1 Experimental setupCTB is a segmented, part-of-speech (POS) tagged,and fully bracketed corpus in the constituency for-malism, and very popular to evaluate fundamen-tal NLP tasks, including word segmentation (Sunand Xu, 2011), POS tagging (Sun and Uszkoreit,2012), and syntactic parsing (Zhang and Clark,2009; Sun and Wan, 2013).
We use CTB 6.0 anddefine the training, development and test sets ac-cording to the CoNLL 2009 shared task.
We usegold-standard word segmentation and POS tag-ing results as inputs.
All transition-based parsingmodels are trained with beam 16 and iteration 30.Overall precision/recall/f-score with respect to de-pendency tokens is reported.
To evaluate the abil-ity to recover non-local dependencies, the recall ofsuch dependencies are reported too.4.2 Coverage and AccuracyThere is a dual effect of the increase of the param-eter k to our transition-based dependency parser.On one hand, the higher k is, the more expres-sivity the corresponding transition system has.
Asystem with higher k covers more structures andallows to use more data for training.
On the otherhand, higher k brings more ambiguities to the cor-responding parser, and the parsing performancemay thus suffer.
Note that the ambiguity exists notonly in each step for transition decision, but alsoin selecting the training oracle.The left-most columns of Tab.
3 shows the cov-erage of K-permutation transition system with re-spect to different K and different oracle derivingalgorithms.
Readers may be surprised that thecoverage of NFO and ANFO deriving proceduresis the same.
Actually, all the covered graphs bythe two oracle deriving procedures are exactly the452System NFO ANFO UP UR UF LP LR LF URLLRLURNLLRNLS276.5 76.5 85.88 81.00 83.37 83.98 79.21 81.53 81.93 80.34 58.88 52.17S389.0 89.0 86.02 81.72 83.82 84.07 79.86 81.91 82.61 80.94 60.46 54.28S495.6 95.6 86.28 82.06 84.12 84.35 80.22 82.23 82.92 81.29 61.48 54.77S598.4 98.4 86.44 82.21 84.27 84.51 80.37 82.39 83.15 81.51 59.80 53.30Table 3: Coverage and accuracy of the GR parser on the development data.same, except for S3.
Only 1 from 22277 sen-tences can find a NFO but not an ANFO.
Thisnumber demonstrates the effectiveness of ANFO.In the following experiments, we use the ANFO?sto train our parser.Applied to our data, S2, i.e.
the exact system in-troduced by Titov et al (2009), only covers 76.5%GR graphs.
This is very different from the re-sult obtained on the CoNLL shared task data forEnglish semantic role labeling (SRL).
Accordingto (Titov et al, 2009), 99% semantic-role-labelledgraphs can be generated by S2.
We think there aretwo main reasons accounting for the differences,and highlight the importance of the expressivenessof transition systems to solve deep dependencyparsing problems.
First, the SRL task only focuseson finding arguments and adjuncts of verbal (andnominal) predicates, while dependencies headedby other words are not contained in its graph rep-resentation.
On contrast, a deep dependency struc-ture, like GR graph, approximates deep syntacticor semantic information of a sentence as a whole,and therefore is more dense.
As a result, permuta-tion system with a very low k is incapable to han-dle more cases.
Another reason is about the Chi-nese language.
Some language-specific propertiesresult in complex crossing arcs.
For example, se-rial verb constructions are widely used in Chineseto describe several separate events without con-junctions.
The verbal heads in such constructionsshare subjects and adjuncts, both of which are be-fore the heads.
The distributive dependencies be-tween verbal heads and subjects/adjuncts usuallyproduce crossing arcs (see Fig.
6).
To test our as-sumption, we evaluate the coverage of S2over thefunctor-argument dependency graphs provided bythe English and Chinese CCGBank (Hockenmaierand Steedman, 2007; Tse and Curran, 2010).
Theresult is 96.9% vs. 89.0%, which confirms ourlinguistic intuition under another grammar formal-ism.Tab.
3 summarizes the performance of thetransition-based parser with different configura-tions to reveal how well data-driven parsing cansubject adjunct verb1verb2Figure 6: A simplified example to illustrate cross-ing arcs in serial verbal constructions.be performed in realistic situations.
We can seethat with the increase of K, the overall parsing ac-curacy incrementally goes up.
The high complex-ity of Chinese deep dependency structures demon-strates the importance of the expressiveness of atransition system, while the improved numeric ac-curacies practically certify the benefits.
The twopoints merit further exploration to more expressivetransition systems for deep dependency parsing, atleast for Chinese.
The labeled evaluation scoreson the final test data are presented in Tab.
4.Test UP UR UF LRLLRNLS583.93 79.82 81.82 80.94 54.38Table 4: Performance on the test data.4.3 Precision vs. RecallA noteworthy thing about the overall performanceis that the precision is promising but the recall istoo low behind.
This difference is consistent withthe result obtained by a shift-reduce CCG parser(Zhang and Clark, 2011).
The functor-argumentdependencies generated by that parser also has arelatively high precision but considerably low re-call.
There are two similarities between our parserand theirs: 1) both parsers produce dependencygraphs rather trees; 2) both parser employ a beamdecoder that does not guarantee global optimality.To build NLP application, e.g.
information extrac-tion, systems upon GR parsing, such property mer-its attention.
A good trade-off between the preci-sion and the recall may have a great impact on finalresults.4534.4 Local vs. Non-localAlthough the micro accuracy of all dependenciesare considerably good, the ability of current state-of-the-art statistical parsers to find difficult non-local materials is far from satisfactory, even forEnglish (Rimell et al, 2009; Bender et al, 2011).We report the accuracy in terms of local and non-local dependencies respectively to show the diffi-culty of the recovery of non-local dependencies.The last four columns of Tab.
3 demonstrates thelabeled/unlabeled recall of local (URL/LRL) andnon-local dependencies (URNL/LRNL).
We canclearly see that non-local dependency recovery isextremely difficult for Chinese parsing.4.5 Deep vs. DeepCCG and HPSG parsers also favor the dependency-based metrics for evaluation (Clark and Curran,2007b; Miyao and Tsujii, 2008).
Previous workon Chinese CCG and HPSG parsing unanimouslyagrees that obtaining the deep analysis of Chineseis more challenging (Yu et al, 2011; Tse and Cur-ran, 2012).
The successful C&C and Enju parsersprovide very inaccurate results for Chinese texts.Though the numbers profiling the qualities of deepdependency structures under different formalismsare not directly comparable, all empirical eval-uation indicates that the state-of-the-art of deeplinguistic processing for Chinese lag behind verymuch.5 Related WorkWide-coverage in-depth and accurate linguisticprocessing is desirable for many practical NLP ap-plications, such as machine translation (Wu et al,2010) and information extraction (Miyao et al,2008).
Parsing in deep formalisms, e.g.
CCG,HPSG, LFG and TAG, provides valuable, richerlinguistic information, and researchers thus drawmore and more attention to it.
Very recently, studyon deep linguistic processing for Chinese has beeninitialized.
Our work is one of them.To quickly construct deep annotations, corpus-driven grammar engineering has been studied.Phrase structure trees in CTB have been semi-automatically converted to deep derivations in theCCG (Tse and Curran, 2010), LFG (Guo et al,2007), TAG (Xia, 2001) and HPSG (Yu et al,2010) formalisms.
Our GR extraction work is sim-ilar, but grounded in GB, which is more consistentwith the construction of the original annotations.Based on converted fine-grained linguistic an-notations, successful English deep parsers, such asC&C (Clark and Curran, 2007b) and Enju (Miyaoand Tsujii, 2008), have been evaluated (Yu et al,2011; Tse and Curran, 2012).
We also borrowmany ideas from recent advances in deep syntac-tic or semantic parsing for English.
In particular,Sagae and Tsujii (2008)?s and Titov et al (2009)?sstudies on transition-based deep dependency pars-ing motivated our work very much.
However, sim-ple adoption of their systems does not resolve Chi-nese GR parsing well because the GR graphs aremuch more complicated.
Our investigation on theK-permutation transition system advances the ca-pacity of existing methods.6 ConclusionRecent years witnessed rapid progress made ondeep linguistic processing for English, and ini-tial attempts for Chinese.
Our work stands inbetween traditional dependency tree parsing anddeep linguistic processing.
We introduced a sys-tem for automatically extracting grammatical rela-tions of Chinese sentences from GB phrase struc-ture trees.
The present work remedies the re-source gap by facilitating the accurate extractionof GR annotations from GB trees.
Manual evalua-tion demonstrate the effectiveness of our method.With the availability of high-quality GR resources,transition-based methods for GR parsing was stud-ied.
A new formal system, namely K-permutationsystem, is well theoretically discussed and prac-tically implemented as the core module of a deepdependency parser.
Empirical evaluation and anal-ysis were presented to give better understanding ofthe Chinese GR parsing problem.
Detailed anal-ysis reveals some important directions for futureinvestigation.AcknowledgementThe work was supported by NSFC (61300064,61170166 and 61331011) and National High-TechR&D Program (2012AA011101).ReferencesEmily M. Bender, Dan Flickinger, Stephan Oepen, andYi Zhang.
2011.
Parser evaluation over local and non-local deep dependencies in a large corpus.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 397?408.Association for Computational Linguistics, Edinburgh,Scotland, UK.
URL http://www.aclweb.org/anthology/D11-1037.454J.
Bresnan and R. M. Kaplan.
1982.
Introduction: Grammarsas mental representations of language.
In J. Bresnan, edi-tor, The Mental Representation of Grammatical Relations,pages xvii?lii.
MIT Press, Cambridge, MA.Ted Briscoe and John Carroll.
2006.
Evaluating the ac-curacy of an unlexicalized statistical parser on the parcdepbank.
In Proceedings of the COLING/ACL 2006Main Conference Poster Sessions, pages 41?48.
Associ-ation for Computational Linguistics, Sydney, Australia.URL http://www.aclweb.org/anthology/P/P06/P06-2006.Andrew Carnie.
2007.
Syntax: A Generative Introduction.Blackwell Publishing, Blackwell Publishing 350 MainStreet, Malden, MA 02148-5020, USA, second edition.Noam Chomsky.
1981.
Lectures on Government and Binding.Foris Publications, Dordecht.Stephen Clark and James Curran.
2007a.
Formalism-independent parser evaluation with ccg and depbank.
InProceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 248?255.
Associa-tion for Computational Linguistics, Prague, Czech Repub-lic.
URL http://www.aclweb.org/anthology/P07-1032.Stephen Clark and James R. Curran.
2007b.
Wide-coverageefficient statistical parsing with CCG and log-linear mod-els.
Comput.
Linguist., 33(4):493?552.
URL http://dx.doi.org/10.1162/coli.2007.33.4.493.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
JOURNAL OF MACHINELEARNING RESEARCH, 7:551?585.M.
Dalrymple.
2001.
Lexical-Functional Grammar, vol-ume 34 of Syntax and Semantics.
Academic Press, NewYork.Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2007.Treebank-based acquisition of lfg resources for Chinese.In Proceedings of the LFG07 Conference.
CSLI Publica-tions, California, USA.Julia Hockenmaier and Mark Steedman.
2007.
CCGbank: Acorpus of CCG derivations and dependency structures ex-tracted from the penn treebank.
Computational Linguis-tics, 33(3):355?396.Ron Kaplan, Stefan Riezler, Tracy H King, John TMaxwell III, Alex Vasserman, and Richard Crouch.
2004.Speed and accuracy in shallow and deep stochastic pars-ing.
In Daniel Marcu Susan Dumais and Salim Roukos,editors, HLT-NAACL 2004: Main Proceedings, pages 97?104.
Association for Computational Linguistics, Boston,Massachusetts, USA.Tracy Holloway King, Richard Crouch, Stefan Riezler, MaryDalrymple, and Ronald M. Kaplan.
2003.
The PARC 700dependency bank.
In In Proceedings of the 4th Inter-national Workshop on Linguistically Interpreted Corpora(LINC-03), pages 1?8.Terry Koo and Michael Collins.
2010.
Efficient third-orderdependency parsers.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguistics,pages 1?11.
Association for Computational Linguistics,Uppsala, Sweden.
URL http://www.aclweb.org/anthology/P10-1001.Ryan McDonald.
2006.
Discriminative learning and span-ning tree algorithms for dependency parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA, USA.Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-oriented evalu-ation of syntactic parsers and their representations.
InProceedings of ACL-08: HLT, pages 46?54.
Associ-ation for Computational Linguistics, Columbus, Ohio.URL http://www.aclweb.org/anthology/P/P08/P08-1006.Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii.
2007.
To-wards framework-independent evaluation of deep linguis-tic parsers.
In Ann Copestake, editor, Proceedings ofthe GEAF 2007 Workshop, CSLI Studies in Computa-tional Linguistics Online, page 21 pages.
CSLI Publica-tions.
URL http://www.cs.cmu.edu/?sagae/docs/geaf07miyaoetal.pdf.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature forestmodels for probabilistic hpsg parsing.
Comput.
Lin-guist., 34(1):35?80.
URL http://dx.doi.org/10.1162/coli.2008.34.1.35.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Comput.
Linguist., 34:513?553.
URL http://dx.doi.org/10.1162/coli.07-056-R1-07-027.Joakim Nivre.
2009.
Non-projective dependency parsingin expected linear time.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, pages 351?359.
Association for Computational Linguistics, Sun-tec, Singapore.
URL http://www.aclweb.org/anthology/P/P09/P09-1040.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.
Memory-based dependency parsing.
In Hwee Tou Ng and EllenRiloff, editors, HLT-NAACL 2004 Workshop: Eighth Con-ference on Computational Natural Language Learning(CoNLL-2004), pages 49?56.
Association for Computa-tional Linguistics, Boston, Massachusetts, USA.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
The University of Chicago Press,Chicago.Laura Rimell, Stephen Clark, and Mark Steedman.
2009.
Un-bounded dependency recovery for parser evaluation.
InProceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 813?821.Association for Computational Linguistics, Singapore.URL http://www.aclweb.org/anthology/D/D09/D09-1085.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-reduce de-pendency DAG parsing.
In Proceedings of the 22ndInternational Conference on Computational Linguistics,pages 753?760.
Coling 2008 Organizing Committee,Manchester, UK.
URL http://www.aclweb.org/anthology/C08-1095.Weiwei Sun and Hans Uszkoreit.
2012.
Capturing paradig-matic and syntagmatic lexical relations: Towards accu-rate Chinese part-of-speech tagging.
In Proceedings of the50th Annual Meeting of the Association for ComputationalLinguistics.
Association for Computational Linguistics.Weiwei Sun and Xiaojun Wan.
2013.
Data-driven, pcfg-basedand pseudo-pcfg-based models for Chinese dependencyparsing.
Transactions of the Association for Computa-tional Linguistics (TACL).Weiwei Sun and Jia Xu.
2011.
Enhancing Chineseword segmentation using unlabeled data.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 970?979.Association for Computational Linguistics, Edinburgh,455Scotland, UK.
URL http://www.aclweb.org/anthology/D11-1090.Ivan Titov, James Henderson, Paola Merlo, and GabrieleMusillo.
2009.
Online graph planarisation for syn-chronous parsing of semantic and syntactic dependen-cies.
In Proceedings of the 21st international jont con-ference on Artifical intelligence, pages 1562?1567.
Mor-gan Kaufmann Publishers Inc., San Francisco, CA, USA.URL http://dl.acm.org/citation.cfm?id=1661445.1661696.Andre Torres Martins, Noah Smith, and Eric Xing.
2009.Concise integer linear programming formulations for de-pendency parsing.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 342?350.
Asso-ciation for Computational Linguistics, Suntec, Singapore.URL http://www.aclweb.org/anthology/P/P09/P09-1039.Daniel Tse and James R. Curran.
2010.
Chinese CCG-bank: extracting CCG derivations from the penn Chi-nese treebank.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (Coling 2010),pages 1083?1091.
Coling 2010 Organizing Committee,Beijing, China.
URL http://www.aclweb.org/anthology/C10-1122.Daniel Tse and James R. Curran.
2012.
The challengesof parsing Chinese with combinatory categorial gram-mar.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technologies,pages 295?304.
Association for Computational Linguis-tics, Montr?eal, Canada.
URL http://www.aclweb.org/anthology/N12-1030.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.2010.
Fine-grained tree-to-string translation rule ex-traction.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics, pages325?334.
Association for Computational Linguistics, Up-psala, Sweden.
URL http://www.aclweb.org/anthology/P10-1034.Fei Xia.
2001.
Automatic grammar generation from two dif-ferent perspectives.
Ph.D. thesis, University of Pennsylva-nia.Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.2005.
The penn Chinese treebank: Phrase structure an-notation of a large corpus.
Natural Language Engineer-ing, 11:207?238.
URL http://portal.acm.org/citation.cfm?id=1064781.1064785.Nianwen Xue.
2007.
Tapping the implicit information for thePS to DS conversion of the Chinese treebank.
In Proceed-ings of the Sixth International Workshop on Treebanks andLinguistics Theories.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InThe 8th International Workshop of Parsing Technologies(IWPT2003), pages 195?206.Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli Wang,and Junichi Tsujii.
2011.
Analysis of the difficul-ties in Chinese deep parsing.
In Proceedings of the12th International Conference on Parsing Technologies,pages 48?57.
Association for Computational Linguistics,Dublin, Ireland.
URL http://www.aclweb.org/anthology/W11-2907.Kun Yu, Miyao Yusuke, Xiangli Wang, Takuya Matsuzaki,and Junichi Tsujii.
2010.
Semi-automatically devel-oping Chinese hpsg grammar from the penn Chinesetreebank for deep parsing.
In Coling 2010: Posters,pages 1417?1425.
Coling 2010 Organizing Committee,Beijing, China.
URL http://www.aclweb.org/anthology/C10-2162.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the Chinese treebank using a global discrim-inative model.
In Proceedings of the 11th Interna-tional Conference on Parsing Technologies (IWPT?09),pages 162?171.
Association for Computational Linguis-tics, Paris, France.
URL http://www.aclweb.org/anthology/W09-3825.Yue Zhang and Stephen Clark.
2011.
Shift-reduceCCG parsing.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 683?692.Association for Computational Linguistics, Portland,Oregon, USA.
URL http://www.aclweb.org/anthology/P11-1069.Yue Zhang and Joakim Nivre.
2011.
Transition-based depen-dency parsing with rich non-local features.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Technolo-gies, pages 188?193.
Association for Computational Lin-guistics, Portland, Oregon, USA.
URL http://www.aclweb.org/anthology/P11-2033.456
