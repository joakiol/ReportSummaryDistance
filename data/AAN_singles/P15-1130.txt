Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1343?1353,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsPredicting Polarities of Tweets by Composing Word Embeddings withLong Short-Term MemoryXin Wang1, Yuanchao Liu1, Chengjie Sun1, Baoxun Wang2and Xiaolong Wang11School of Computer Science and Technology,Harbin Institute of Technology, Harbin, China2Application and Service Group, Microsoft, Beijing, China1{xwang,lyc,cjsun,wangxl}@insun.hit.edu.cn2baoxwang@microsoft.comAbstractIn this paper, we introduce Long Short-Term Memory (LSTM) recurrent networkfor twitter sentiment prediction.
With thehelp of gates and constant error carouselsin the memory block structure, the modelcould handle interactions between wordsthrough a flexible compositional function.Experiments on a public noisy labelleddata show that our model outperforms sev-eral feature-engineering approaches, withthe result comparable to the current bestdata-driven technique.
According to theevaluation on a generated negation phrasetest set, the proposed architecture dou-bles the performance of non-neural modelbased on bag-of-word features.
Further-more, words with special functions (suchas negation and transition) are distin-guished and the dissimilarities of wordswith opposite sentiment are magnified.
Aninteresting case study on negation expres-sion processing shows a promising poten-tial of the architecture dealing with com-plex sentiment phrases.1 IntroductionTwitter and other similar microblogs are rich re-sources for opinions on various kinds of productsand events.
Detecting sentiment in microblogs isa challenging task that has attracted increasing re-search interest in recent years (Hu et al, 2013b;Volkova et al, 2013).Go et al (2009) carried out the pioneer workof predicting sentiment in tweets using machinelearning technology.
They conducted comprehen-sive experiments on multiple classifiers based onbag-of-words feature.
Such feature is widely usedbecause it?s simple and surprisingly efficient inmany tasks.
However, there are also disadvan-tages of bag-of-words features represented by one-hot vectors.
Firstly, it bears a data sparsity is-sue (Saif et al, 2012a).
In tweets, irregulari-ties and 140-character limitation exacerbate thesparseness.
Secondly, losing sequence informa-tion makes it difficult to figure out the polarityproperly (Pang et al, 2002).
A typical case is thatthe sentiment word in a negation phrase tends toexpress opposite sentiment to that of the context.Distributed representations of words can easethe sparseness, but there are limitations to theunsupervised-learned ones.
Words with specialfunctions in specific tasks are not distinguished.Such as negation words, which play a specialrole in polarity classification, are represented sim-ilarly with other adverbs.
Such similarities willlimit the compositional models?
abilities of de-scribing a sentiment-specific interaction betweenwords.
Moreover, word vectors trained by co-occurrence statistics in a small window of con-text effectively represent the syntactic and seman-tic similarity.
Thus, words like good and bad havevery similar representations (Socher et al, 2011).It?s problematic for sentiment classifiers.Sentiment is expressed by phrases rather thanby words (Socher et al, 2013).
Seizing such se-quence information would help to analyze com-plex sentiment expressions.
One possible methodto leverage context is connecting embeddings ofwords in a window and compose them to a fix-length vector (Collobert et al, 2011).
However,window-based methods may have difficulty reach-ing long-distance words and simply connectedvectors do not always represent the interactions ofcontext properly.Theoretically, a recurrent neural network couldprocess the whole sentence of arbitrary length byencoding the context cyclically.
However, thelength of reachable context is often limited whenusing stochastic gradient descent (Bengio et al,1994; Pascanu et al, 2013).
Besides that, a1343traditional recurrent architecture is not powerfulenough to deal with the complex sentiment expres-sions.
Fixed input limits the network?s ability oflearning task-specific representations and simpleadditive combination of hidden activations and in-put activations has difficulty capturing more com-plex linguistic phenomena.In this paper, we introduce the Long Short-Term Memory (LSTM) recurrent neural networkfor twitter sentiment classification by means ofsimulating the interactions of words during thecompositional process.
Multiplicative operationsbetween word embeddings through gate structuresprovide more flexibility and lead to better com-positional results compare to the additive onesin simple recurrent neural network.
Experimen-tally, the proposed architecture outperforms vari-ous classifiers and feature engineering approaches,matching the performance of the current best data-driven approach.
Vectors of task-distinctive words(such as not) are distinguished after tuning andrepresentations of opposite-polarity words are sep-arated.
Moreover, predicting result on negationtest set shows our model is effective in dealingwith negation phrases (a typical case of sentimentexpressed by sequence).
We study the process ofthe network handling the negation expressions andshow the promising potential of our model sim-ulating complex linguistic phenomena with gatesand constant error carousels in the LSTM blocks.2 Related Work2.1 Microblogs Sentiment AnalysisThere have been a large amount of works on sen-timent analysis over tweets.
Some research makesuse of social network information (Tan et al,2011; Calais Guerra et al, 2011).
These works re-veal that social network relations of opinion hold-ers could bring an influential bias to the textualmodels.
While some other works utilize the mi-croblogging features uncommon in the formal lit-erature, such as hashtags, emoticons (Hu et al,2013a; Liu et al, 2012).
Speriosu et al (2011) pro-pose a unified graph propagation model to lever-age textual features (such as emoticons) as well associal information.Semantic concept or entity based approacheslead another research direction.
Saif et al (2012a;2012b) make use of sentiment-topic features andentities extracted by a third-party service to easedata sparsity.
Aspect-based models are also ex-ploited to improve the tweet-level classifier (Lekand Poo, 2013).2.2 Representation Learning and DeepModelsBengio et al (2003) use distributed representa-tions for words to fight the curse of dimension-ality when training a neural probabilistic languagemodel.
Such word vectors ease the syntactic andsemantic sparsity of bag-of-words representations.Much recent research has explored such represen-tations (Turian et al, 2010; Huang et al, 2012).Recent works reveal that modifying word vec-tors during training could capture polarity infor-mation for the sentiment words effectively (Socheret al, 2011; Tang et al, 2014).
It would be alsoinsightful to analyze the embeddings that changedthe most during training.
We conduct a compar-ison between initial and tuned vectors and showhow the tuned vectors of task-distinctive functionwords cooperate with the proposed architecture tocapture sequence information.Distributed word vectors help in various NLPtasks when using in neural models (Collobert etal., 2011; Kalchbrenner et al, 2014).
Com-posing these representations to fix-length vectorsthat contain phrase or sentence level informationalso improves performance of sentiment analy-sis (Yessenalina and Cardie, 2011).
Recursiveneural networks model contextual interaction inbinary trees (Socher et al, 2011; Socher et al,2013).
Words in the complex utterances are con-sidered as leaf nodes and composed in a bottom-up fashion.
However, it?s difficult to get a binarytree structure from the irregular short commentslike tweets.
Not requiring structure informationor parser, long short-term memory models encodethe context in a chain and accommodate complexlinguistic phenomena with structure of gates andconstant error carousels.3 Recurrent Neural Networks forSentiment AnalysisRecurrent Neural Networks (RNN) have gainedattention in NLP field since Mikolov et al (2010)developed a statistical language model based ona simple form known as Elman network (El-man, 1990).
Recent works used RNNs to pre-dict words or characters in a sequence (Chrupa?a,2014; Zhang and Lapata, 2014).
Treating opin-ion expression extraction as a sequence labelling1344inputhiddenoutputt-1 t t+1Figure 1: Illustration of simple recurrent neuralnetwork.
The input of the hidden layer comesfrom both input layer and the hidden layer acti-vations of previous time step.problem, Irsoy and Cardie (2014) leverage deepRNN models and achieve new state-of-the-art re-sults for fine-grained extraction task.
The lastestwork propose a tree-structured LSTM and conducta comprehensive study on using LSTM in predict-ing the semantic relatedness of two sentences andsentiment classification (Tai et al, 2015).Fig.1 shows the illustration of a recurrent net-work.
By using self-connected layers, RNNs al-low information cyclically encoded inside the net-works.
Such structures make it possible to get afix-length representation of a whole tweet by tem-porally composing word vectors.The recurrent architecture we used in this workis shown in Fig.2.
Each word is mapped to a vec-tor through a Lookup-Table (LT) layer.
The in-put of the hidden layer comes from both the cur-rent lookup-table layer activations and the hiddenlayer?s activations one step back in time.
In thisway, hidden layer encodes the past and current in-formation.
The hidden activations of the last timestep could be considered as the representation ofthe whole sentence and used as input to classifica-tion layer.
By storing the word vectors in LT layer,the model has reading and tuning access to wordrepresentations.Based on such recurrent architecture, we cancapture sequence information in the context andidentify polarities of the tweets.3.1 Elman Network With FixedLookup-TableRNN-FLT:A simple implementation of the recur-rent sentiment classifier is an Elman network (alsoknown as simple RNN) with Fixed Lookup-Table(FLT).
In such model, unsupervised pre-trainedword vectors in LT layer are constant during thewhole training process.
The hidden layer activa-hhhh YFigure 2: Illustration of the general recurrent ar-chitecture unfolded as a deep feedforward net-work.tion of position h at time t is:bth= f(ath)(1)ath=E?iwiheti+H?h?wh?hbt?1h?
(2)where etrepresents the E-length embedding ofthe tth word of the sentence, which stored in LTlayer.
wihis the weight of connection between in-put and hidden layer, while wh?his the weightsof recurrent connection (self-connection of hiddenlayer).
f represents the sigmoid function.
Thebinary classification loss function O is computedvia cross entropy (CE) criterion and the network istrained by stochastic gradient descent using back-propagation through time (BPTT) (Werbos, 1990).Here, we introduce the notation:?ti=?O?ati(3)Firstly, the error propagate from output layer tohidden layer of last time step T .
The derivativeswith respect to the hidden activation of position iat the last time step T are computed as follow:?Ti= f?
(aTi)?O?yvi(4)where virepresents the weights of hidden-outputconnection and the activation of the output layer yis used to estimate probability of the tweet bearing1345a particular polarity.y = f(H?ibTivi)(5)Then the gradients of hidden layer of previoustime steps can be recursively computed as:?th= f?(ath)H?h??t+1h?whh?
(6)3.2 Elman Network with TrainableLookup-TableUnsupervised trained word embeddings representthe syntactic and semantic similarity.
However,in specific tasks, the importance and functions ofdifferent words vary.
Negation words have simi-lar unsupervised trained representations with otheradverbs, but they make distinctive contributionsin sentiment expressions.
Besides the functionwords, tuning word vectors of sentiment wordsinto polarity-representable ones turns out to be aneffective way to improve the performance of sen-timent classifiers.
(Maas et al, 2011; Labutov andLipson, 2013).
Such tuned vectors work togetherwith the deep models, gaining the ability to de-scribe complex linguistic phenomena.RNN-TLT: To this end, we modify the wordvectors in the Trainable Lookup-Table (TLT) viaback propagation to get a better embedding ofwords.
The gradient of lookup-table layer is:?ti= g?
(ati)H?h=1?thwih=H?h=1?thwih(7)where identity function g (x) = x is considered asthe activation function of lookup-table layer.3.3 Long Short-Term MemoryThe simple RNN has the ability to capture con-text information.
However, the length of reach-able context is often limited.
The gradient tendsto vanish or blow up during the back propaga-tion (Bengio et al, 1994; Pascanu et al, 2013).Moreover, Elman network simply combines pre-vious hidden activations with the current inputsthrough addictive function.
Such combination isnot powerful enough to describe a complex inter-actions of words.An effective solution for these problems isthe Long Short-Term Memory (LSTM) architec-ture (Hochreiter and Schmidhuber, 1997; Gers,&(&RXWSXWLQSXWRXWSXWJDWHIRUJHWJDWHLQSXWJDWHFigure 3: Illustration of LSTM memory blockwith one cell.
Constant Error Carousel (CEC)maintains the internal activation (called state) witha recurrent connection of fixed weight 1.0, whichmay be reset by the forget gate.
The input andoutput gates scale the input and output respec-tively.
All the gates are controlled by the main-tained state, network input and hidden activationof previous time step.2001).
Such architecture consists of a set of re-currently connected subnets, known as memoryblocks.
Each block contains one or more self-connected memory cells and the input, output andforget gates.
Fig.3 gives an illustration of anLSTM block.
Once an error signal arrives Con-stant Error Carousel (CEC), it remains constant,neither growing nor decaying unless the forgetgate squashes it.
In this way, it solves the vanish-ing gradient problem and learns more appropriateparameters during training.Moreover, based on this structure, the input,output and stored information can be partial ad-justed by the gates, which enhances the flexibil-ity of the model.
The activations of hidden layerrely on the current/previous state, previous hiddenactivation and current input.
These activations in-teract to make up the final hidden outputs throughnot only additive but also element-wise multiplica-tive functions.
Such structures are more capable tolearn a complex composition of word vectors thansimple RNNs.These gates are controlled by current input, pre-vious hidden activation and cell state in CEC unit:GtI= f(UIxt+ VIht?1+WIst?1)(8)1346GtF= f(UFxt+ VFht?1+WFst?1)(9)GtO= f(UOxt+ VOht?1+WOst)(10)where Gtindicates the gate activation at time t,xt, htand stis input, hidden activation and state inCEC unit at time t respectively, while U , V andWrepresent the corresponding weight matrices con-nect them to the gates.
Subscript I , F and O in-dicate input, forget and output respectively.
TheCEC state and block output are computed by thefunctions with element-wise multiplicative opera-tion:st= GtFst?1+GtIf(USxt+ VSht?1)(11)at= GtOst(12)where USindicates connection weight between in-put and state, while VSrepresents the weight ma-trix connecting hidden layer to state.LSTM-TLT: By replacing the conventionalneural units in RNN-TLT with LSTM blocks,we can get the LSTM network with TrainableLookup-Table.
Such model achieves a flexiblecompositional structure where the activations in-teract in a multiplicative function.
It providesthe capacity of describing diverse linguistic phe-nomenon by learning complex compositions ofword embeddings.4 Experiments4.1 Data SetWe conduct experiments on the Stanford Twit-ter Sentiment corpus (STS)1.
The noisy-labelleddataset is collected using emoticons as queries inTwitter API (Go et al, 2009).
800,000 tweets con-taining positive emoticons are extracted and la-belled as positive, while 800,000 negative tweetsare extracted based on negative emoticons.
Themanually labelled test set consists of 177 negativeand 182 positive tweets.4.2 Experimental SettingsRecurrent Neural Network: We implementthe recurrent architecture with trainable lookup-table layer by modifying RNNLIB (Graves, 2010)toolkit.Early Stopping: From the noisy labelled data,we randomly selected 20,000 negative and 20,0001http://twittersentiment.appspot.com/positive tweets as validation set for early stopping.The rest 1,560,000 tweets are used as training set.Parameter Setting: Tuned on the validation set,the size of the hidden layer is set to 60.Word Embeddings: We run word2vec on thetraining set of 1.56M tweets (without labels) to getdomain-specific representations and use them asinitial input of the model.
Limited to the input for-mat of the toolkit, we learned 25-dimensional (rel-atively small) vectors.
Skip-gram architecture andhierarchical softmax algorithm are chosen duringtraining.4.3 Comparison with Data DrivenApproachesClassifier Accuracy(%)SVM 81.6MNB 82.7MAXENT 83.0MAX-TDNN 78.8NBoW 80.9DCNN 87.4RAE 77.6RNN-FLT 80.2RNN-TLT 86.4LSTM-TLT 87.2Table 1: Accuracies of different classifiers.Naive Bayes, Maximum Entropy and SVM arewidely used classifiers.
Go et al (2009) presentedthe results of three non-neural models using uni-gram and bigram features.Dynamic Convolutional Neural Network(DCNN) (Kalchbrenner et al, 2014) is a general-ization of MAX-TDNN (Collobert et al, 2011).It has a clear hierarchy and is able to capturelong-range semantic relations.
While the NeuralBag-of-Words (NBoW) takes the summation ofword vectors as the input of a classification layer.Kalchbrenner et al (2014) reported performancesof the above three neural classifiers.Recursive Autoencoder (RAE) has proven to bean effective model to compose words vectors insentiment classification tasks (Socher et al, 2011).We run RAE with randomly initialized word em-beddings.
We do not compare with RNTN (Socheret al, 2013) for lack of phrase-level sentiment la-bels and accurate parsing results.Table 1 shows the accuracies of different clas-sifiers.
Notably, RNN-TLT and LSTM-TLT out-1347perform the three non-neural classifiers.
Trainedon the considerable data, these classifiers pro-vide strong baselines.
However, bag-of-words rep-resentations are not powerful enough.
Sparsityand losing sequence information hurt the perfor-mance of classifiers.
Neural models overcomethese problems by using distributed representa-tions and temporally encoding the contextual in-teraction.We notice a considerable increase in the perfor-mance of the RNN-TLTwith respect to the NBoW,whose embeddings are also tuned during super-vised training.
It suggests that recurrent modelscould generate better tweet-level representationsfor the task by composing the word embeddingsin a temporal manner and capturing the sequentialinformation of the context.Convolutional neural networks have outstand-ing abilities of feature extraction, while LSTM-TLT achieves a comparable performance.
It sug-gests that LSTM model is effective in learningsentence-level representations with a flexible com-positional structure.RAE provides more general representations ofphrases by learning to reconstruct the word vec-tors.
Recurrent models outperform RAE indi-cates that task-specific composing and representa-tion learning with less syntactic information leadto a better result.Comparing RNN-FLT with RNN-TLT, we caneasily figure out that the model with trainablelookup-table achieves better performance.
Thisis due to the fact that tuned embeddings capturethe sentiment information of text by distinguish-ing words with opposite sentiment polarities andproviding more flexibility for composing.
LSTM-TLT does not outperform RNN-TLT significantly.And the situations are almost the same on short-sentence (less than 25 words) and long-sentence(not less than 25 words) test set.
Such results in-dicate that the ability of LSTM getting access tolonger-distance context is not the determinant ofimprovement, while the capacity of LSTM han-dling complex expressions plays a more importantrole.
Such capacity will be further discussed insubsection 4.7.Since the training set is large enough, we havenot observed strong overfitting during the trainingprocess.
Therefore, no regularization technologyis employed in the experiments.4.4 Comparison with Feature EngineeringApproachesMethod Craft feature Accuracy(%)Speriosu etal.
(2011)emoticon 84.7hashtagSaif etal.
(2012a)sentiment-topic 86.3semantic 84.1Lek andPoo (2013)aspect-based 88.3This work 87.2Table 2: Comparison with different feature engi-neering methods.Table 2 shows the comparison with differentfeature engineering methods.
In Speriosu et al(2011)?s work, sentiment labels propagated in agraph constructed on the basis of contextual re-lations (e.g.
word presence in a tweet) as wellas social relations.
Saif et al (2012a) eased thedata sparsity by adding sentiment-topic featuresthat extracted using traditional lexicon.
While Lekand Poo (2013) extracted tuple of [aspect, word,sentiment] with hand-crafted templates.
With thehelp of opinion lexicon and POS tagger especiallydesigned for twitter data, their approach achieveda state-of-the-art result.Even though these methods rely on lexicons andextracted entities, our data-driven model outper-forms most of them, except the aspect-based onethat introduced twitter-specific resources.
Thisis due to the fact that traditional lexicons, evenemoticons added, are not able to cover the diver-sification of twitter sentiment expressions, whileLSTM learns appropriate representations of senti-ment information through compositional manner.4.5 Experiments on Manually Labelled DataDifferent from STS dataset deciding the polar-ity based on emoticons, the benchmark datasetin SemEval 2013 (Nakov et al, 2013) is labelledby human annotators.
In this work we focus onthe binary polarity classification and abandon theneutral tweets.
There are 4099/735/1742 avail-able tweets in the training/dev/test set respectively.Since the training set is relatively small, we don?tapply fine tuning on word vectors.
Namely weuse fixed lookup-table for both RNN and LSTM.300-dimensional vectors are learned on the 1.56Mtweets of STS dataset using word2vec.
Other set-tings stay the same as previous experiments.1348Method Accuracy(%)SVM 74.5RAE 75.4RNN-FLT 83.0LSTM-FLT 84.0Table 3: Accuracies of different methods on Se-mEval 2013Table 3 shows our work compared to SVMand Recursive Autoencoder.
From the result, wecan see that the recurrent models outperforms thebaselines by exploiting more context informationof word interactions.4.6 Representation LearningRecent works reveal that modifying word vec-tors during training could capture polarity infor-mation for the sentiment words effectively (Socheret al, 2011; Tang et al, 2014).
However, it wouldbe also helpful to analyse the embeddings thatchanged the most.Function words: We choose 1000 most fre-quent words.
For each word, we compute the dis-tance between unsupervised vector and tuned vec-tor.
20 words that change most are shown in Fig.4.It?s noteworthy that there are five negationwords (not, no, n?t, never and Not) in the notably-change group.
The representations of negationwords are quite similar with other adverbs in un-supervised learned embeddings, while the pro-posed model distinguishes them.
This indicate thatour polarity-supervised models identify negationwords as distinctive symbols in sentiment classifi-cation task, while unsupervised learned vectors donot contain such information.Besides the negation words and sentimentwords, there are also other prepositions, pronounsand conjunctions change dramatically (e.g.
andand but).
Such function words also play a specialrole in sentiment expressions (Socher et al, 2013)and the model in this paper distinguishes them.However, the contributions of these words to thetask are not that explainable as negation words (atleast without sentiment strength information).To further explain how the tuned vectors worktogether with the network and describe interac-tions between words, we study the process of themodel classifying negation phrases in the follow-ing subsection.Sentiment words: In order to study the em-0 0.1 0.2 0.3 0.4 0.5InotAndwaitthisforbefromnobadNown'tbetterButfinallyneverNotbutmegoodFigure 4: Word change scale to [0,1].
Distancesare computed by reversing cosine similarity.bedding change of sentiment words, we choosethe most frequent sentiment words in our train-ing data, 20 positive and 20 negative, and ob-serve the dissimilarity of the vectors in a two-dimensional space.
An alternative least-squarescaling is implemented based on Euclidean dis-tance between word vectors.
Figure 5 showssentiment-specific tuning reduces the overlap ofopposite polarities.
Polarities of words are identi-fied based on a widely-used sentiment lexicon (Huand Liu, 2004).To explicitly evaluate it, we selected embed-dings of 2000 most frequent sentiment words(1000 each polarity) and compute the centers ofboth classes.
If an embedding is closer to the op-posite polarity center, we consider it as an over-lap.
Experimentally, the proportion of overlap ofunsupervised learned vectors is 19.55%, while theone of tuned vectors is 11.4%.
Namely the over-lap ratio is reduced by 41.7%.
Experimentally,such polarity separating relies on tuning throughlookup-table layer rather than LSTM structure.With the decrease of overlap of polarities, senti-ment of word turns more distinguishable, which ishelpful for polarity prediction.4.7 Case Study: NegationNegation phrases are typical cases where senti-ment is expressed by sequence rather than words.To evaluate the ability of the model dealing withsuch cases, we select most frequent 1000 negativeand 1000 positive words in the training data andgenerate the corresponding negation phrases (such134931.3.554321.2.3??
!2321.2.3.4.5321.2.3.4Figure 5: Distance of word vectors shown in two-dimensional space.
The above figure shows thedistribution of unsupervised learning vectors andthe below figure indicates the tuned one.
The solidand hollow points represent the positive and nega-tive words respectively.as not good).Classifier Accuracy(%)MNB+unigram+bigram 32.98RNN-TLT 52.00LSTM-TLT 64.85Table 4: Accuracy on generated negation phrasestest set.Statistical result shows that only 37.6% of thenegation phrases appeared in the training text.
Itsets a theoretical upper bound to the classifiersbased on the unigram and bigram features.
Ex-perimental result shown in Table 4 indicates thatLSTM model effectively handles the sequentialexpressions of negation.
By composing word vec-tors, recurrent models ease the sparsity of bag-of-word features and achieve a significant improvethan MNB using unigram and bigram features.LSTM outperform RNN by 12.85%, such resultsuggests the element-wise multiplicative composi-tional function of LSTM provides more flexibilityto simulate interactions between word vectors.
Aclear process of LSTM handling negation phrasesis observed, which is described in the rest of thesubsection, while the one of RNN is not that obvi-ous.As mentioned in 4.6, the task-distinctive func-/36/11./36./61./86/11./21./31./41./51./61./71opu`cbehyperplaneopu`hppe=t?`cbe=t?`hppeopu=t???
!2Figure 6: Hidden activations of negation phrases.<s> represent the beginning of sentences.
not badand good lead to positive outputs, while not goodand bad result in negative values.
The dotted lineindicates the classification hyperplane.
The solidarrows represent the hidden vector changes whenthe network take the word good as input, while thedotted arrows indicate the changes when the wordbad is input.
The sentiment words are input in twosituations (as initial input or after negation word),while the changes of hidden vectors of same wordare opposite in the two situations.tion words are distinguished.
It would be insight-ful to show how it works together with the LSTMstructure.We train the network on STS dataset and test iton few words and phrases (good, bad, not goodand not bad).
For the convenience of analysis theactivation within the network, we set the size ofhidden layer to 2.
Such setting reduces the perfor-mance by about 7% on the public test set, but thetrained model still work effectively.
Fig.6 showsthe activations of LSTM hidden layers.
Both sen-timent words and negation phrases are classifiedinto correct categories.
Furthermore, when senti-ment words like good (i) input as the first wordof sentence and (ii) input after negation word, itcause opposite change in hidden layer.
These be-haviours simulate the change of sentiment in thenegation expressions.As mentioned in 3.3, gates?
activations are con-trolled by current input, state in CEC unit and out-put of hidden layer of previous time step.
Theyare many possible ways for the model to simulat-ing the sentiment change.
In the experiment, theobserved situation is shown in Fig.7:Negation word contains both polarities.
The1350QRWJRRGV!QHJDWLYHJRRGQRWQRWQHJSRVQHJSRVRSHQ FORVHFigure 7: Observed process of LSTM block han-dling negation phrase not good.
Some less impor-tant connections are omitted in this figure.positive-axle and negative-axle are almost orthog-onal.
Negation word has large components onboth axles.not make input gate close.
Experiments showrecurrent activations make the input gate close,namely previous word not squashes the input (bothcurrent and recurrent input) to a very small value.Choose a polarity to forget.
The combinationof the recurrent input not and current input goodmake the CEC unit forget the positive informa-tion, namely they make forget gate reduce state?scomponent on positive-axle while leaving a largeprojection on negative-axle.
A significant dissim-ilarity of forget gate activations between positiveand negative words is observed in the experiment,when they are input after not.In this way, the temporally-input phrase notgood shows a negative polarity.
Correspondingly,phrase not bad turns positive after reducing thenegative components of the negation word.
Suchcase shows the process of the gates and CEC unitcooperating in the LSTM structure.
Together withtuned vectors, the architecture has a promising po-tential of capture sequence information by simu-lating complex interactions between words.5 ConclusionIn this paper we have explored to capture twit-ter sentiment expressed by interactions of words.The contributions of this paper can be summarizedas follows: (i) We have described long short-termmemory based model to compose word represen-tations through a flexible compositional function.Tested on a public dataset, the proposed architec-ture achieves result comparable to the current bestdata-driven model.
The experiment on negationtest set shows the ability of the model capturingsequential information.
(ii) Beyond tuning vectorsof sentiment words, we put forward a perspectiveof distinguishing task-distinctive function wordsonly relying on the label of the whole sequence.
(iii) We conduct an interesting case study on theprocess of task-distinctive word vectors workingtogether with deep model, which is usually con-sidered as a black-box in other neural networks,indicating the promising potential of the architec-ture simulating complex linguistic phenomena.AcknowledgmentsWe thank Deyuan Zhang, Lei Cui, Feng Liu andMing Liu for their great help.
We thank theanonymous reviewers for their insightful feed-backs on this work.
This research is sup-ported by National Natural Science Foundationof China (No.613400114), Specialized ResearchFund for the Doctoral Program of Higher Educa-tion (No.20132302120047), the Special FinancialGrant from the China Postdoctoral Science Foun-dation (No.2014T70340), China Postdoctoral Sci-ence Foundation (No.2013M530156)ReferencesYoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gra-dient descent is difficult.
Neural Networks, IEEETransactions on, 5(2):157?166.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Pedro Henrique Calais Guerra, Adriano Veloso, Wag-ner Meira Jr, and Virg?
?lio Almeida.
2011.
Frombias to opinion: a transfer-learning approach to real-time sentiment analysis.
In Proceedings of the 17thACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 150?158.ACM.Grzegorz Chrupa?a.
2014.
Normalizing tweets withedit scripts and recurrent neural embeddings.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 680?686, Baltimore, Mary-land, June.
Association for Computational Linguis-tics.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.13512011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Felix Gers.
2001.
Long Short-Term Memory in Recur-rent Neural Networks.
Ph.D. thesis, Ph.
D. thesis,Ecole Polytechnique Federale de Lausanne.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.Alex Graves.
2010.
Rnnlib: A recurrent neuralnetwork library for sequence learning problems.http://sourceforge.net/projects/rnnl.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.2013a.
Unsupervised sentiment analysis with emo-tional signals.
In Proceedings of the 22nd interna-tional conference on World Wide Web, pages 607?618.
International World Wide Web ConferencesSteering Committee.Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu.
2013b.Exploiting social relations for sentiment analysis inmicroblogging.
In Proceedings of the sixth ACM in-ternational conference on Web search and data min-ing, pages 537?546.
ACM.Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th AnnualMeet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 873?882.
Asso-ciation for Computational Linguistics.Ozan Irsoy and Claire Cardie.
2014.
Opinion miningwith deep recurrent neural networks.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages720?728.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics, June.Igor Labutov and Hod Lipson.
2013.
Re-embeddingwords.
In Proceedings of the 51th Annual Meet-ing of the Association for Computational Linguis-tics, pages 489?493.
Association for ComputationalLinguistics.Hsiang Hui Lek and Danny CC Poo.
2013.
Aspect-based twitter sentiment classification.
In Tools withArtificial Intelligence (ICTAI), 2013 IEEE 25th In-ternational Conference on, pages 366?373.
IEEE.Kun-Lin Liu, Wu-Jun Li, and Minyi Guo.
2012.Emoticon smoothed language models for twittersentiment analysis.
In AAAI.Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
InProceedings of the 49th AnnualMeeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 142?150.Tomas Mikolov, Martin Karafia?t, Lukas Burget, JanCernocky`, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Preslav Nakov, Zornitsa Kozareva, Alan Ritter, SaraRosenthal, Veselin Stoyanov, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Proceedings of the International Work-shop on Semantic Evaluation, SemEval, volume 13.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, pages 79?86.
As-sociation for Computational Linguistics.Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-gio.
2013.
On the difficulty of training recurrentneural networks.
In Proceedings of The 30th In-ternational Conference on Machine Learning, pages1310?1318.Hassan Saif, Yulan He, and Harith Alani.
2012a.
Al-leviating data sparsity for twitter sentiment analysis.Making Sense of Microposts (# MSM2012).Hassan Saif, Yulan He, and Harith Alani.
2012b.
Se-mantic sentiment analysis of twitter.
In The Seman-tic Web?ISWC 2012, pages 508?524.
Springer.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.
Association forComputational Linguistics.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1631?1642.1352Michael Speriosu, Nikita Sudan, Sid Upadhyay, andJason Baldridge.
2011.
Twitter polarity classifica-tion with label propagation over lexical links and thefollower graph.
In Proceedings of the First work-shop on Unsupervised Learning in NLP, pages 53?63.
Association for Computational Linguistics.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved semantic representa-tions from tree-structured long short-term memorynetworks.
arXiv preprint arXiv:1503.00075.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentimentanalysis incorporating social networks.
In Proceed-ings of the 17th ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 1397?1405.
ACM.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014.
Learning sentiment-specific word embedding for twitter sentiment clas-sification.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics, pages 1555?1565.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Svitlana Volkova, Theresa Wilson, and DavidYarowsky.
2013.
Exploring sentiment in socialmedia: Bootstrapping subjectivity clues frommultilingual twitter streams.
In Association forComputational Linguistics (ACL).Paul J Werbos.
1990.
Backpropagation through time:what it does and how to do it.
Proceedings of theIEEE, 78(10):1550?1560.Ainur Yessenalina and Claire Cardie.
2011.
Com-positional matrix-space models for sentiment anal-ysis.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages172?182.
Association for Computational Linguis-tics.Xingxing Zhang and Mirella Lapata.
2014.
Chi-nese poetry generation with recurrent neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 670?680.1353
