TOWARDS A COMPUTATIONAL MODEL FORTHE SEMANTICS  OF WHY-QUEST IONSW.
WahlsterGermanisches SeminarUniversitaet HamburgVon-Melle-Park 6D-2000 Hamburg 13Federal Republic of GermanySummary.
This paper discusses aspects of acomputational model for the semantics of why-ques-tions which are relevant to the implementation ofan explanation component in a natural languagedialogue system.
After a brief survey of all ofthe explanation components which have been imple-mented to date, some of the distinguishing featuresof the explanation component designed and imple-mented by the author are listed.
In the firstpart of the paper the major types of signals which,like the word whV, can be used to set the expla-nation component into action are listed, and someways of recognizing them automatically are con-sidered.
In addition to these linguistic signals,communicative and cognitive conditions which canhave the same effect are discussed.
In the secondpart the various schemata.for argumentative dia-logue sequences which can be handled by the ex-planation component in question are examined, Par-ticular attention is paid to problems arising inconnection with the iteration of why-questionsand the verbalization of multiple justifications.Finally schemata for metacommunicative why-ques-tions and for why-questions asked by the user areinvestigated.IntroductionThe explanation component of a natural lan-guage AI system is that component whose job it isto generate, in response to a why-question an ex-planation which is both understandable to theuser and appropriate to the current state of thedialogue.Although there has been relatively littleresearch into the semantics and pragmatics of why-questions1,5,9, 17 and the cognitive processes un-derlying the answering of them, several AI systemsdo exist which are capable of handling certaintypes of why-questions.
The practical value ofthe incorporation of an explanation component liesessentially in the fact that, as Stallman andSussman have put it, '~such programs are more con-vincing when right and easier to debug whenwrong".~5Figure I provides an overview and compari-son of the explanation components which have beenimplemented to date: BLAH 22, DIGITALIS ADVISOR 16,EL Is, EXPOUND ~, HAM-RPMI~, 21, LUIG113, MYCIN12, ~,NOAH 11, PROSPECTOR 7, SHRDLU ~,  TKP2 I?
(The symbol"-" signifies that the attribute in question isnot applicable to the given system).This paper presents some results of my experiencein designing and implementing an explanation com-ponentS1; together, they represent a step towarda computational model for the semantics of why-questions.
The explanation component was designedas a module which could in principle be incorpora-ted into any natural language AI system.
It hasbeen tested within the natural language dialoguesystem HAM-RPM 6, which converses with a human part-ner in colloquial German about limited but inter-changeable scenes.In implementing HAM-RPM we have taken intoaccount the human ability to deduce useful infor-mation even in the case of fuzzy knowledge by ap-proximate reasoning.
The model of fuzzy reasoningused in HAM-RPM can be characterized by the fol-lowing four properties2?
:(a) A fuzzy inference rule represents a weakimplication; a particular 'implicationstrength' must thus be associated with eachsuch rule.
(b) The premises of a fuzzy inference rule areoften fulfilled only to a certain degree.
(c) The applicability of a fuzzy inference rulein the derivation of a particular conclusionis likewise a matter of degree.
(d) Several mutually independent fuzzy inferencerules can corroborate each other in the de-rivation of a particular conclusion.The explanation component which I have developeddiffers from BLAH 22, one of the most advancedexplanation components which have similar goals,in that on the one hand fuzzy inference rules andfacts can be modified by appropriate hedges (inaccordance with (a) through (c) above), and on theother hand the system is able in the course of adialogue to generate multiple justifications foran explanandum (in accordance with (d) above).
Afurther important difference between this expla-nation component and the other systems includedin Figure I is that the system is equipped witha fairly sophisticated natural language generator,which is ATN-based and includes algorithms forgenerating pronouns and definite descriptions 19.Only two aspects of this explanation compo-nent will be discussed in this paper: The signalson the basis of which the explanation componentgenerates an argumentative answer to a questionasked by the user and the speech act schemata forthe argumentative dialogue sequences which can be--144--SYSTEM' S GENERAL CHARACTERISTICS8 ~ ~~ s. ~-_ :~'-BLAHU.S.
in- AMORDAMORD come tax ruleslawsEXPLANATION COMPONENTLINGUISTIC \] COMMUNICATIVE AC~CAPABILITIES 'I CAPABIL!TI ES , C=~ "- 88 ;~ -~ 'Z  ~ =  ,- ~= .- o ~ "~ ~'~ ~ o= ~ : ~.~.~ ~ ?
ul c =COGNITIVEAPABILITIES8~JNo I schemata COLL MOD, DIA STR, DETasser t ions ,suggested a l -HYP ternat ives~Idecls ionsDIGITALISADVISORmedicine: OWL schemata,OWL digitalis procedures No canned TECH DIA DETtherapy textsystem'squestions,reasoningchainELelectrical ARS ARS circuit rules ana lys i s !No HYP system'sconclusionsEXPOUNDHAM-RPMLUIGIMYCINNOAHPROSPECTORLISPlogic: predicate simpleformal calculus No case TECH - STRproofs formulas grammart raf f icFUZZY scene, DEDUCE ATN-basedroom- procedures Yes generatorp COLL MOD, DIA DET, STR FUZbooking schemataSOL kitchen SOL I specific I!
Yes generation: COLLworld procedures 0roceduresmedicine:'iproductio nLISP bacter ia l  i Yes schemata TECH DIA DET FUZ rules ,rnfect ions Iirepair ofelectro- SOUP NoSOUP mechanical proceduresequipmentgeology: rules inLISP mineral inference No schemata TECHexplo-  netrat ionFUZtheoremssystem's ques-t ions and con-c|usJons, rea-soning, inc l .mu l t ip le  der-ivat ionssimulatedact ionssystem's ques-tions and con-ciusions, rea-soning chain,meta-infer-encessystem'sins t ruct ionstO usersystem'squestionsSHRDLUTKP2MICRO- blocks consequent specificPLANNER world theorems Yes generatiOnmrocedures COLLlogic: IpredicateLISP formal calculus No schemata TECHproofs formulas ISTRFigure I: Comparison of all explanation components implemented to date--145--systemlssimulatedactionstheoremsrealized in the system.A Formal Description of the Signals Suggesting anArgumentative AnswerThe purpose of the present section is tolist the major types of signals which are capableof setting an explanation component into action.The resulting classification of linguistic expres-sions does not, of course, imply that all of theexpressions in a given category are completelysynonymous.Signals for Argumentative Answers in the User'sUtterancesFrom the point of view of algorithmic recognition,the simplest case is that in which the user elic-its an argumentative answer from the system byasking a direct question.
The word why can oftenbe interpreted as a signal for an argumentativeanswer.
On the other hand, its exact meaning de-pends on the dialogue context and it can be usedwithin speech acts which have nothing to do withexplanation, such as making a suggestion or acomment 5.
In spite of its ambiguity, the wordwhy represents the only means of eliciting anargumentative answer in most AI systems which havean explanation component.Special idiomatic expressions such as thoselisted in (LI) can have the same function as theword why.
In the system HAM-RPM expressions like(LI) How come, what ... for, how do you knowthese are recognized through pattern matchingduring lexical analysis 6.Indirect questions such as those in (LI) re-quire that the system be able to divide the ut-terance into matrix sentence and embedded sen-tence syntactically; only then can it process thelatter using the same means as in the case of di-rect questions containing why or the questions in(L1).
(L2) Please tell me why A, I'd like to know why AFurther types of signals include direct (see LJ)and indirect (see L4) requests.
The problem of(LJ) Please explain why A, prove that A(L4) I 'd be interested in hearing why you thinkthat A, Are you prepared to justify yourconclusion that A?how indirect speech acts such as the requests in(L4) can be recognized automatically is one whichhas recently been attracting much attention fromnatural language AI researchersJ, 8The word why and the expressions in (LI)needn't accompany the proposition to be explainedwithin a single utterance, as they do in the ex-ample (El); they can also be used alone after thesystem has answered a question to elicit an expla-nation of the answer (cf.
E2).
(El) USER (U): Why is Glenbrook Drive closed?
(E2.1) USER (U) : Is Olenbrook Drive closed?
(E2.2) SYSTEM (S): Yes.
(E2.3) USER (U): Hew do you explain that?The expressions in (LJ) and (L4) can also be usedto achieve just the opposite: An argumentativeanswer is requested in advance, before the corres-ponding question has been asked of the system.
(EJ) PLease explain your answer: Do you thinkthat A?As the continuation of (E2.l) and (E2.2) represen-ted by (E2.4) and (E2.5) illustrates, a speakeroften explains a previously given answer whenthe listener - perhaps using an expression suchas the ones in (LS)  shows signs of doubt as to(L5) Really?
Are you sure?
That's strange.
(E2.4) U: Really?
(E2.5) S: Yeah, they're repaving it.the truth of the answer.A kind of signal which suggests an argumen-tative answer in a still more obvious manner isthe category of utterances by the user which indi-cate an opinion contrary to that expressed by thesystem (cf.
L6).
The idiomatic expressions in (L5)(L6) I doubt that, That doesn't follow, I can'tbelieve that..., Since when?and (L6) which always express doubt or a contraryopinion no matter what the current dialogue con-text may be, can be handled adequately if infor-mation concerning their implications is stored inthe system's 'idiom lexicon '6.A further way in which the user can indi-rectly ask a why-question is by himself suggestingan explanation of what the system has just asser-ted, while at the same time indicating a desireto have the explanation confirmed by the system.For example, after the system has given the an-swer (E2.2), the user should be able, by askingthe question (E2.6), to elicit an explanation like(E2.7) from the system.
If this kind of behavior(E2.6) U: Because of an accident?
(?2.7) S: No, because they're repaying it.is to be realized in a dialogue system, the pro-gram must be able to recognize (E2.6) as a pro-posed explanation.
Algorithms which recognizeexplanations in certain contexts have been de-veloped, e.g., for the ICAI system ACE TM and thetext-understanding system PAM 23.Leading and rhetorical questions whichsuggest an affirmative answer may be seen as con-taining an implicit request to justify the answerif it is negative.
If the system's answer to (EJ.I)(E3.1) U: You aren't going to restrict me to40k of core today again, are you?
(?3.2) S: Yes, in fact I am.
I've got 47 jobslogged-in in the moment.is not something like (E3.2), but rather simplyYes, in fact I am, the system isn't exhibitingthe sort of cooperative behavior which we wouldlike to have in a natural language dialogue sys-tem.These last two types of speech acts cannotat present be handled adequately by AI systems.The same is true of explanations within theschema reproach-justification (cf.
E4.1 and E4.2).-146-(E4.1) U: You erased my file COLING.
TMP#(E4.2) S: Yeah, your log-out quota was exceeded.Communicative and Cognitive Conditions as Signalsfor Arj'umenEatliv ~ AnswersTwo further kinds of signals which suggest argu-mentative answers deserve mention in this section.In contrast to the preceding types they can be in-corporated without difficulty into existing AIsystems, e.g.
HAM-RPM 21.Both kinds of signal lead to the question sbeing oucr-~we2..?d in that they suggest an argu-mentative answer in the absence of any explicitor implicit request for such an answer in theuser's question.On the one hand, the system may offer anunsolicited explanation for reasons of p(z.,utneAtae2./?~ if it has already noticed that the userseems to have a tendency to ask for explanationsof answers 6.On the other hand, over-answering may evenbe reasonably expected of the system in the casewhere the answer is based on uncertain beliefsand approximate or hypothetical reasoning.
Thiskind of behavior can be modelled to a limitedextent if the system is programmed so as to at-tempt to generate an explanation as soon as itsconfidence in its own answer sinks below a cer-tain threshold, e.g., because the implicationstrength (see (a) above) of one of the inferencerules it has used is low (cf.
E5.1, E5.2)?
The(E5?I) U: I wonder if the Mercedes is cheap.
(E5.2) S: I imagine so -- .it's pretty old andrusty.generation of an argumentative answer in such acontext falls outside the usual scope of lin-guistic analysis; it is a good example of an ap-plication of the AI paradigm in that the con-dition which gives rise to the generation of anargumentative answer is a certain property of acognitive process, namely the inference processby which the answer is derived.Figure 2 summarizes the various signals forargumentative answers which have been discussedin this section (types of signals which have beenimplemented in HAM-RPM's explanation componentare indicated by a *).,quest ion~_~_quest ion  word *idiomatic expression *__------direct *?
request~_____~ind i rec t  *?
evidence of doubt in user *?
evidence of a contrary opinion in user *?
inadequate explanation suggested by user?
unexpected answer to a leading or rhetoricalquestion?
evidence of reproach in user?
/par tner - tac t i cs  *"?ver-answer~ng"-~--uncertainty about own answer *Figure 2: Signals which can elicit an argumen-tative answerSpeech Act Sch.emata for Ar@umentative Dijloju @SequencesThis section deals with argumentative dia-logue sequences and their reconstruction in AIsystems.
The speech act sequence depicted inschema I will serve as a starting point.
($1.1) U: <yes-no-question>($1.2) S: <affirmat{ve answer> (with restric-ting hedge)($1.3) U: Why?
($1.4) S: <argumentative answer>Interpretation of $1.3 by S:What is the basis for the assertion (be-lief) in $1.2 that A?Schema I: A simple argumentative dialogue sequenceIn schema I, as in the schemata to follow, theword why represents the entire class of signalsin the user's utterances for argumentative answerswhich were discussed in the previous section.Here is an example of a simple argumentativedialogue sequence:(E6.1) U: Do you know if the Mercedes is cheap?
(?6.2) S: I think so.
(E6.3) U: What makes you think so?
(E6.4) S: It's in need of repairs.Iterated Why-questions and Ultimate ExplanationsA sequence such as (E6.1) through (E6.4) may becontinued by one or more repititions of schema 2,in which the user requests that the system's ar-gumentative answer itself be explained.
($2.1) U: Why?
($2.2) S: <argumentative answer>Schema 2: Iteration of a why-questionThe dialogue sequence (E6.5) through (E6.8) is acontinuation of (E6) in which two further why-questions occur.
The answer (E6.8) is an example(E6.5) U: Why?
(E6.6) S: It's in need of repairs because itsrear axle is bent.
(?6.7) U: How come?
(E6.8) S: That's just the way it is.of an u./_-t/mcc.tC cxpZ~noJCio~.
Though it is debatablewhether ultimate explanations in a philosophicalsense are in fact possible, it is clear that par-ticipants in everyday dialogues frequently offerexplanations which they are not in a position toexplain further.
Some typical formulations whichare used in such cases are listed in (L7).
(L7) It's obvious, That's the way it is,Can't you see it?The Ambiguity of Iterated Why-questionsA further problem in connection with iteratedwhy-questions is the ambiguity which they reg-ularly involve.
Each of the why-questions afterthe first one can refer either to (a) the asser-tion which constituted the explanans, or (b) the--147 -inferential relationship between the explanansand the explanandum.
($3.1) U: Why Q?
($3.2) S: Because P.J %Why P??
Why (P ~ Q) ?Schema 3: The ambiguity of an iterated why-questionIf the second sort of interpretation is appliedto the question (E6.7), an answer such as (E6.9)becomes appropriate.
(E6.9) S: A machine is in need of repairs whenone of its parts is in need of repairs.It is of course possible to eliminate this ambi-guity with a more precise formulation of the why-question, as when, for example, ($2.1) is re-placed with ($2.1').
(S2.1') U: I know that.
But why does that makeyou think that Q7Although interpretation (a) is far more commonthan (b) in nontechnical dialogues, the occur-rence of questions such as ($2.1') shows that itis nonetheless worthwhile to provide an AI systemwith the ability to answer in accordance witheither of the possible interpretations.
For inter-pretation (b), this means that the system must beable,'like HAM-RPM al, to verbalize the inferencerules it uses.Jf the system is requested, via a furtherwhy-question, to explain an inference rule thatit has verbalized in this way, the existence ofa third type of argument in addition to the pres-entation of factual evidence and the verbalisationof inference rules becomes evident: The systemmay supply a bacl./n9 Is for its inference rule.A backing usually refers to a convention, a theory,or observations.An explanation component which uses back-ings must have access to the corresponding meta-knowledge about its inference rules.The Elicitation of a Multiple dustific@tionA further variant of schema 2 can be used to ex-hibit the step-by-step elicitation of a multiplejustification.
Instead of simply asking anotherwhy-question, the user specifically requestsfurther corroborating evidence for the explanan-dum.
Some typical expressions are listed in (L8).
(L8) IS that all?
Any other reason?
Just becauseof that?
($4.1) U: <request for further evidence>($4.2) S: <corroborating evidence for SI.2>Schema 4: The elicitation of a muJtiple justifi-cationAs the example (E6.10) through (E6.13) shows,schema 4 can be instantiated several times insuccession.
(E6.10) U: Is that the only reason?
(?6.11) S: Well, it's pretty old and beat-up.
(E6.12) U: Anything else?
(E6.13) S: It's a bit rusty..Djalo@ue Schemata with Metacommunicative Why-qua-tionsin all of the dialogue schemata we have examinedso far, a why-question asked by the user follow-ed an answer by the system to a previous question.In this section we shall discuss dialogue se-quences in which why-questions refer to questionsor requests.
In fact, of course, any kind ofspeech act, e.g.
a threat or an insult, can giverise to a metacommunicative why-question; thetwo types to be discussed here are those mostrelevant to foreseeable applications of naturallanguage AI systems.Schema 5 will serve as a starting point.
Inclarification dialogues schema 6,a variant ofschema 5, can be instantiated.
($5.1) S: <question>,<request>(55.2) U: Why?
(55.3) S: <argumentative answer>($5.4) U: <response to S5.1>interpretation of $5.2 by S:What was the intention underlying thespeech act in $5.1?Schema 5: A dialogue sequence with a metacommuni-cative why-question($6.1) U: <question>($6.2) S: <clarification question concerningS6.1>,<request for a paraphrase of$6.1>($6.3) U: Why?
($6.4) S: <argumentative answer>(S6.5) U: <response to S6.2>(S6.6) S: <response to $6.1>Schema 6: A metacommunicative why-question with-in a clarification dialogueHere is a dialogue sequence containing a meta-communicative why-question asked by the user:(E7.1) U: Please list all articles since 1978on the subject of 'presposition'.
(E7.2) S: Do you really mean 'presposition'?
(E7.3) U: Why do you ask?
(E7.4) S: I don't know this word.
(E7.5) U: I meant 'presupposition'(E7.6) S: I have the fol lowing entries: ...Why-questions Asked by the SystemAlthough all of the why-questions considered sofar have been asked by the user, the system canalso ask why the user has made a particular input?This situation is described by schema 5 exceptthat the roles of USER (U) and SYSTEM (S) are re-versed?Providing an application-oriented AI systemwith the ability to ask such why-questions isworthwhile because there are many situations inwhich the system requires further informationabout the user's intention to guide its searchfor an answer or to help to formulate its answerin a communicatively adequate manner?
Of course,-148--the system can only make use of the user's an-swer to such a why-question if it is equippedwith the ability to analyse argumentative an-swers.
The example (E8) might occur in one ofHAM-RPM's dialogue situations, in which the sys-tem simulates a hotel manager who is anxious torent a particular room to a caller who is in-quiring about it.
It illustrates the way infor-mation about the dialogue partner's intentionscan influence the way a particular state of af-fairs is described.
(E8.1) U: Has the room got a big desk?
(E8.2) S: Why do you ask?
(E8.3) U: Because I've got a lot of work to do.
(E8.4) S: Yes, the desk is fairly large.
(E8.3') U: I hate big desks.
(E8.4') S: It isn't particularly big.The schemata we have investigated in this and theprevious sections can also be embedded in one an-other, as can be seen from schema 7.
In thisschema, (S7.4),but not ($7.3), is a metacommuni-cative why-question.
($7.1) U: <yes-no-question>($7.2) S: <affirmative answer> (with restric-ting hedge)(S7.3) U: Why?
($7.4) S: Why do you ask?
($7.5) U: <argumentative answer to $7.4>($7.6) S: <argumentative answer to $7.3>Schema 7: Successive why-questions of differenttypesIn mixed-Z~.).2J.o.~..i_u?
systems, in which either ofthe partners can initiate a dialogue sequence,the system must be able both to ask and to an-swer why-questions, including those of a meta-communicative nature.Summary and Integration of All Argumentative Dia-logue Schemata Relevant to AI SystemsFigure 3 summarizes and integrates the schematafor argumentative dialogue sequences discussedin the preceding sections.
The arrows joining therectangles indicate that one speech act followsanother in time.
If arrows join two rectanglesin both directions, loops such as those discussedin connection with iterated why-questions arepossible.
Double vertical lines on the left- orright-hand side of a rectangle indicate that thespeech act in question can be the first or thelast speech act in a sequence, respectively.
Thesystem's criteria for recognizing at each pointwhich of the possible speech acts the user hasperformed and for selecting its own speech actsare not included in the diagram.If one extends Figure 3 by permitting thereversal of the roles of system and user, allschemata for argumentative dialogue sequences 21are included which are relevant for foreseeableapplications in dialogue systems with mixed-ini-tiative.Technical DataA non-compiled version of HAM-RPM is run-ning on the DECsystem 1070 (PDP-10) of the Fach-bereich fuer Informatik of the University of Ham-burg under the TOPSI0 operating system.
Compri-sing approximately 600 LISP/FUZZY procedures, thecurrent version occupies 150K of 36-bit words andrequires from one to Fifteen seconds for a re-sponse.AcknowledgementsI wish to thank Anthony Jameson for care-ful reading of an earlier draft of this paper.'
1 ( I)  USER ' (2) SYSTEM , (,3) USERI II Ii II I, \]Llariflcationl 'I AI question/ Ik~ I/~  request I~I Iquest  ion  answer  why-quest  ionII/',; ~ rejection IF:i i :L illi I(4) SYSTEM~r ultlmate explanat Ionil/argumentat i ve \]'~I,3  onswor IL I II II\oference tollprevious IIexplanat Ion II ) t ,, I(5) USERi~ request forfurtherevidenceresponseto  (2) 111(6) SYSTEManswerto (1)Figure 3: Schemata for argumentative dialogue sequences in Al systems-149--References\[I\] Bromberger, S. (1966): Why-questions.
In:Colodny, R.
(ed.
): Mind and Cosmos.Pittsburgh: Univ.
Press, p. 86-111\[2\] Chester, D. (1976): The translation of for-mal proofs into English.
In: Artifi-cial Intelligence, 7, 3, p. 261-278\[3\] Cohen, P.R.
(1978): On knowing what to say:Planning speech acts.
Univ.
of Toron-to, Dept.
of Computer Science, Tech-nical Report No.
118\[4\] Davis, R. (1976): Applications of meta levelknowledge to the construction, main-tenance and use of large knowledgebases.
Stanford Univ., Technical Re-port STAN-CS-76-562\[5\] Freeman, C. (1976): A pragmatic analysis oftenseless why-questions.
In: Mufwene,S.S., Walker, C.A., Steeuer, S.B.(eds.
): Papers of the twelfth region-al meeting of the Chicago LinguisticSociety.
Chicago: Chicago LinguisticSociety, p. 208-219\[6\] v. Hahn, W., Hoeppner, W., Jameson, A.,Wahlster, W. (1980): The anatomy ofthe natural language dialogue systemHAM-RPM.
In: Bolc, L.
(ed.
): Naturallanguage based computer systems.Munich: Hanser/Macmillan\[7\] Hart, P.E., Duda, R.O.
(1977): PROSPECTOR -A computer-based consultation systemfor mineral exploration.
StanfordResearch International, AI Center,Technical Note 155\[8\] Hayes, P., Reddy, R. (1979): An anatomy ofgraceful interaction in spoken andwritten man-machine communication.Carnegie-Mellon-Univ., Dept.
of Com-puter Science, Technical Report CMU-CS-79-144\[9\] Heringer, H.J.
(1974): Praktische Semantik.Stuttgart: Klett\[I0\] Nakamishi, M., Nagata, M., Ueda, K. (1979):An automatic theorem prover genera-ting a proof in natural language.
In:IJCAI-79, Tokyo, p. 636-638\[11\] Sacerdoti, E.D.
(1977): A structure for plansand behavior.
N.Y.: Elsevier\[12\] Scott, C.A., Clancey, A., Davis, R.,Shortliffe, E.H. (1977): Explanationcapabilities of production-based con-sultation systems.
In: American Jour-nal of Computational Linguistics,Microfiche 62\[13\] Scragg, G.W.
(1974): Answering questionsabout processes.
Univ.
of California,San Diego, Ph.D. Thesis\[14\] Sleeman, D.H., Hendley, R.J. (1979): ACE:A system which analyses complex ex-planations.
In: International Journalof Man-Machine Studies, 11, p. 125-144\[15\] Stallman, R.M., Sussman, G.J.
(1977): For-ward reasoning and dependency-direc-ted backtracking in a system for com-puter-aided circuit analysis.
In:Artificial Intelligence 9, P. 135-196\[16\] Swartout, W.R. (1977): A Digitalis therapyadvisor with explanations.
MIT Lab.for Computer Science, Technical Re-port TR-176\[17\] Tondl, L. (1969): Semantics of the questionin a problem-solving situation.
CzechAcademy of Science, Prague\[18\] Tou\]min, S. (1969): The uses of argument,Cambridge: Univ.
Press (Ist ed.
1958)\[19\] Wahlster, W., Jameson, A., Hoeppner, W.(1978): Glancing, referring and ex-planing in the dialogue system HAM-RPM.In: American Journal of ComputationalLinguistics, Microfiche 77, P. 53-67\[20\] Wahlster, W. (1980): Implementing fuzzinessin dialogue systems.
In: Rieger, B.(ed.
): Empirical Semantics.
Bochum:Brockmeyer\[21\] Wahlster, W. (1980): Automatic generation ofnatural language explanations for con-clusions based on fuzzy inferences.
(in preparation)\[22\] Weiner, J.L.
(1979): The structure of naturalexplanation: Theory and application.System Development Corporation, SantaMonica, Technical Report SP-4035\[23\] Wilensky, R. (1978): Why John maried Mary:Understanding stories involving re-curring goals.
In: Cognitive Science,2, p. 235-266\[24\] Winograd, T. (1972): Understanding naturallanguage.
N.Y.: Academic--150--
