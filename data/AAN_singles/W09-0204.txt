Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 25?32,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsA Study of Convolution Tree Kernel with Local AlignmentLidan ZhangDepartment of Computer ScienceHKU, Hong Konglzhang@cs.hku.hkKwok-Ping ChanDepartment of Computer ScienceHKU, Hong Kongkpchan@cs.hku.hkAbstractThis paper discusses a new convolu-tion tree kernel by introducing localalignments.
The main idea of the newkernel is to allow some syntactic al-ternations during each match betweensubtrees.
In this paper, we give analgorithm to calculate the compositekernel.
The experiment results showpromising improvements on two tasks:semantic role labeling and questionclassification.1 IntroductionRecently kernel-based methods have becomea state-of-art technique and been widely usedin natural language processing applications.In this method, a key problem is how to de-sign a proper kernel function in terms of dif-ferent data representations.
So far, there aretwo kinds of data representations.
One is toencode an object with a flat vector whose ele-ment correspond to an extracted feature fromthe object.
However the feature vector is sen-sitive to the structural variations.
The ex-traction schema is heavily dependent on dif-ferent problems.
On the other hand, kernelfunction can be directly calculated on the ob-ject.
The advantages are that the originaltopological information is to a large extentpreserved and the introduction of additionalnoise may be avoided.
Thus structure-basedkernels can well model syntactic parse treein a variety of applications, such as relationextraction(Zelenko et al, 2003), named en-tity recognition(Culotta and Sorensen, 2004),semantic role labeling(Moschitti et al, 2008)and so on.To compute the structural kernel function,Haussler (1999) introduced a general type ofkernel function, called?
Convolution kernel?.Based on this work, Collins and Duffy (2002)proposed a tree kernel calculation by count-ing the common subtrees.
In other words,two trees are considered if and only if thesetwo trees are exactly same.
In real sentences,some structural alternations within a givenphrase are permitted without changing its us-age.
Therefore, Moschitti (2004) proposedpartial trees to partially match between sub-trees.
Kashima and Koyanagi (2002) general-ize the tree kernel to labeled order tree kernelwith more flexible match.
And from the ideaof introducing linguistical knowledge, Zhanget al (2007) proposed a grammar-driven treekernel, in which two subtrees are same if andonly if the corresponding two productions arein the same manually defined set.
In addi-tion, the problem of hard matching can be al-leviated by processing or mapping the trees.For example, Tai mapping (Kuboyama et al,2006) generalized the kernel from countingsubtrees to counting the function of mapping.Moreover multi-source knowledge can benefitkernel calculation, such as using dependencyinformation to dynamically determine the treespan (Qian et al, 2008).In this paper, we propose a tree kernel cal-culation algorithm by allowing variations inproductions.
The variation is measured withlocal alignment score between two derivativePOS sequences.
To reduce the computationcomplexity, we use the dynamic programmingalgorithm to compute the score of any align-ment.
And the top n alignments are consid-ered in the kernel.25Another problem in Collins and Duffy?stree kernel is context-free.
It does not con-sider any semantic information located at theleaf nodes of the parsing trees.
To lexicalizedtree kernel, Bloehdorn et al (2007) consid-ered the associated term similarity by virtueof WordNet.
Shen et al (2003) constructed aseparate lexical feature containing words on agiven path and merged into the kernel in linearcombination.The paper is organized as follows.
In sec-tion 2, we describe the commonly used treekernel.
In section 3, we propose our methodto make use of the local alignment informa-tion in kernel calculation.
Section 4 presentsthe results of our experiments for two differ-ent applications ( Semantic Role Labeling andQuestion Classification).
Finally section 5provides our conclusions.2 Convolution Tree KernelThe main idea of tree kernel is to countthe number of common subtrees betweentwo trees T1 and T2.
In convolutionaltree kernel (Collins and Duffy, 2002), atree(T ) is represented as a vector h(T ) =(h1(T ), ..., hi(T ), ..., hn(T )), where hi(T ) isthe number of occurrences of the ith tree frag-ment in the tree T .
Since the number of sub-trees is exponential with the parse tree size,it is infeasible to directly count the commonsubtrees.
To reduce the computation complex-ity, a recursive kernel calculation algorithmwas presented.
Given two trees T1 and T2,K(T1, T2) = < h(T1), h(T2) > (1)=?ihi(T1)hi(T2)=?i(?n1?NT1Ii(n1)?n2?NT2Ii(n2))=?n1?NT1?n2?NT24(n1, n2)where, NT1 and NT2 are the sets of all nodes intrees T1 and T2, respectively.
Ii(n) is the indi-cator function to be 1 if i-th subtree is rootedat node n and 0 otherwise.
And 4(n1, n2) isthe number of common subtrees rooted at n1and n2.
It can be computed efficiently accord-ing to the following rules:(1) If the productions at n1 and n2 are differ-ent, 4(n1, n2) = 0(2) If the productions at n1 and n2 are same,and n1 and n2 are pre-terminals, then4(n1, n2) = ?
(3) Else, 4(n1, n2) = ?
?nc(n1)j (1 +4(ch(n1, j), ch(n2, j)))where nc(n1) is the number of children ofn1 in the tree.
Note that n1 = n2 be-cause the productions at n1 and n2 are same.ch(n1, j) represents the jth child of noden1.
And 0 < ?
?
1 is the parameterto downweight the contribution of larger treefragments to the kernel.
It corresponds toK(T1, T2) =?i ?sizeihi(T1)hi(T2), wheresizei is the number of rules in the i?th frag-ment.
The time complexity of computing thiskernel is O(|NT1| ?
|NT2|).3 Tree Kernel with Local Alignment3.1 General FrameworkAs we referred, one of problems in the ba-sic tree kernel is its hard match between tworules.
In other words, at each tree level,the two subtrees are required to be perfectlyequal.
However, in real sentences, somemodifiers can be added into a phrase with-out changing the phrase?s function.
For ex-ample, two sentences are given in Figure 1.Considering ?A1?
role, the similarities be-tween two subtrees(in circle) are 0 in (Collinsand Duffy, 2002), because the productions?NP?DT ADJP NN?
and ?NP?DT NN?are not identical.
From linguistical point ofview, the adjective phrase is optional in realsentences, which does not change the corre-sponding semantic role.
Thus the modifiercomponents(like ?ADJP?
in the above exam-ple) should be neglected in similarity compar-isons.To make the hard match flexible, we canalign two string sequences derived from thesame node.
Considering the above example,26SNPNNPRichardVPVPAUXhas VBNtakenNPDTaNNapproachADJPRBRmoreJJaudience-friendlySNPNNPRichardVPVPAUXhas VBNtakenNPDTaNNapproachSNPNNPRichardVPVPAUXhas VBNtakenNPDTaNNapproachNULLvA0A1A1A1(a) (b) (c)Figure 1: Syntactic parse tree with ?A1?
semantic rolean alignment might be ?DT ADJP NN?
vs?DT - NN?, by inserting a symbol(-).
Thesymbol(-) corresponds to a ?NULL?
subtreein the parser tree.
And the ?NULL?
subtreecan be regarded as a null character in the sen-tence, see Figure 1(c).Convolution kernels, studied in (Haussler,1999) gave the framework to construct a com-plex kernel from its simple elements.
Supposex ?
X can be decomposed into x1, ..., xm ?~x.
Let R be a relation over X1?
...?Xm?Xsuch that R(~x) is true iff x1, ..., xm are partsof x. R?1(x) = {~x|R(~x, x)}, which returnsall components.
For example, x is any string,then ~x can be its characters.
The convolutionkernel K is defined as:K(x, y) =?~x?R?1(x),~y?R?1(y)m?d=1Kd(xd, yd)(2)Considering our problem, for example, aderived string sequence x by the rule ?n1 ?x?.
R(xi, x) is true iff xi appears in the righthand of x.
Given two POS sequences x andy derived from two nodes n1 and n2, respec-tively, A(x, y) denotes all the possible align-ments of the sequence.
The general form ofthe kernel with local alignment is defined as:K ?
(n1, n2) =?
(i,j)?A(x,y)K(ni1, nj2) (3)4?
(n1, n2) = ??
(i,j)?A(x,y)AS(i,j)nc(n1,i)?d=1(1 +4?
(ch(n1, i, d), ch(n2, j, d))where, (i, j) denotes the ith and jth variationfor x and y, AS(i,j) is the score for alignment iand j.
And ch(n1, i, d) selects the dth subtreefor the ith aligned schema of node n1.It is easily to prove the above kernel is pos-itive semi-definite, since the kernel K(ni1, nj2)is positive semi-definite.
The native computa-tion is impractical because the number of allpossible alignments(|A(x, y)|) is exponentialwith respect to |x| and |y|.
In the next sec-tion, we will discuss how to calculate AS(i,j)for each alignment.3.2 Local Alignment KernelThe local alignment(LA) kernel was usuallyused in bioinformatics, to compare the sim-ilarity between two protein sequences(x andy) by exploring their alignments(Saigo et al,2004).KLA(x, y) =?pi?A(x,y)exp?s(x,y,pi) (4)where ?
?
0 is a parameter, A(x, y) denotesall possible local alignments between x and y,and s(x, y, pi) is the local alignment score fora given alignment schema pi, which is equalto:s(x, y, pi) =|pi|?i=1S(xpii1 , ypii2)?|pi|?1?j=1[g(pii+11 ?
pii1) + g(pii+12 ?
pii2)](5)In equation( 5), S is a substitution matrix, andg is a gap penalty function.
The alignmentscore is the sum of the substitution score be-tween the correspondence at the aligned posi-tion, minus the sum of the gap penalty for the27case that ?-?
symbol is inserted.
In natural lan-guage processing, the substitution matrix canbe selected as identity matrix and no penaltyis accounted.Obviously, the direct computation of theoriginal KLA is not practical.
Saigo (2004)presented a dynamic programming algorithmwith time complexity O(|x|?|y|).
In this paper,this dynamic algorithm is used to compute thekernel matrix, whose element(i, j) is used asAS(i,j) measurement in equation(3).3.3 Local Alignment Tree KernelNow we embed the above local alignmentscore into the general tree kernel computation.Equation(3) can be re-written into following:4?
(n1, n2) = ?
?pi?A(x,y)(exp?s(x,y,pi)?nc(n1,i)?k=1(1 +4?
(ch(n1, i, k), ch(n2, j, k))))(6)To further reduce the computation com-plexity, a threshold (?)
is used to filter outalignments with low scores.
This can help toavoid over-generated subtrees and only selectthe significant alignments.
In other words,by using the threshold (?
), we can select thesalient subtree variations for kernels.
The fi-nal kernel calculation is shown below:4?
(n1, n2) = ?
?pi ?
A(x, y)s(x, y, pi) > ?(?
?s(x,y,pi)?nc(n1,i)?k=1(1 +4?
(ch(n1, i, k), ch(n2, j, k))))(7)After filtering, the kernel is still positivesemi-definite.
This can be easily proved usingthe theorem in (Shin and Kuboyama, 2008),since this subset selection is transitive.
Morespecifically, if s(x, y, pi) > ??
s(y, z, pi?)
>?, then s(x, z, pi + pi?)
> ?.The algorithm to compute the local align-ment tree kernel is given in algorithm 1.
Forany two nodes pair(xi and yj), the local align-ment score M(xi, yj) is assigned.
In the ker-nel matrix calculation, the worst case occurswhen the tree is balanced and most of thealignments are selected.Algorithm 1 algorithm for local alignmenttree kernelRequire: 2 nodes n1,n2 in parse trees;Theproductions are n1 ?
x1, ..., xm and n2 ?y1, ..., ynreturn 4?
(n1, n2)if n1 and n2 are not same then4?
(n1, n2) = 0elseif both n1 and n2 are pre-terminals then4?
(n1, n2) = 1elsecalculate kernel matrix by equation( 4)for each possible alignment docalculate 4?
(n1, n2) by equation(7)end forend ifend if4 Experiments4.1 Semantic Role Labeling4.1.1 Experiment SetupWe use the CoNLL-2005 SRL shared taskdata(Carreras and Marquez, 2005) as our ex-perimental data.
It is from the Wall StreetJournal part of the Penn Treebank, togetherwith predicate-arguments information fromthe PropBank.
According to the shared task,sections 02-21 are used for training, section24 for development and section 23 as well assome data from Brown corpus are left for test.The data sets are described in Table 1.Sentences ArgumentsTraining 39,832 239,858Dev 1,346 8,346Test WSJ 1,346 8,346Brown 450 2,350Table 1: Data sets statistics28Considering the two steps in semantic rolelabeling, i.e.
semantic role identification andrecognition.
We assume identification hasbeen done correctly, and only consider thesemantic role classification.
In our experi-ment, we focus on the semantic classes in-clude 6 core (A0-A5), 12 adjunct(AM-) and8 reference(R-) arguments.In our implementation, SVM-Light-TK1(Moschitti, 2004) is modified.
For SVMmulti-classifier, the ONE-vs-ALL (OVA)strategy is selected.
In all, we prepare the datafor each semantic role (r) as following:(1) Given a sentence and its correct full syn-tactic parse tree;(2) Let P be the predicate.
Its potential argu-ments A are extracted according to (Xueand Palmer, 2004)(3) For each pair < p, a >?
P ?
A: if acovers exactly the words of semantic roleof p, put minimal subtree < p, a > intopositive example set (T+r ); else put it inthe negative examples (T?r )In our experiments, we set ?
= 0.5.4.1.2 Experimental ResultsThe classification performance is evalu-ated with respect to accuracy, precision(p),recall(r) and F1 = 2pr/(p+ r).Accuracy(%)(Collins and Duffy, 2002) 84.35(Moschitti, 2004) 86.72(Zhang et al, 2007) 87.96Our Kernel 88.48Table 2: Performance comparison betweendifferent kernel performance on WSJ data1http://dit.unitn.it/ moschitt/Tree-Kernel.htmP(%) R(%) F?=1Development 81.03 68.91 74.48WSJ Test 84.97 79.45 82.11Brown Test 76.95 70.94 73.51WSJ+Brown 82.98 75.40 79.01WSJ P(%) R(%) FA0 81.28 83.90 82.56A1 84.22 66.39 74.25A2 77.27 62.36 69.02A3 93.33 21.21 34.57A4 82.61 51.35 63.33A5 100.00 40.00 57.41AM-ADV 74.21 56.21 63.92AM-CAU 75.00 46.09 57.09AM-DIR 57.14 16.00 25.00AM-DIS 77.78 70.00 73.68AM-EXT 75.00 53.10 62.18AM-LOC 89.66 74.83 81.57AM-MNR 84.62 48.20 61.41AM-MOD 96.64 92.00 94.26AM-NEG 99.30 95.30 97.26AM-PNC 48.20 28.31 35.67AM-PRD 50.00 30.00 37.50AM-TMP 87.87 73.43 80.00R-A0 81.08 67.80 73.85R-A1 77.50 49.60 60.49R-A2 58.00 42.67 49.17R-AM-CAU 100.00 25.00 40.00R-AM-EXT 100.00 100.00 100.00R-AM-LOC 100.00 55.00 70.97R-AM-MNR 50.00 25.00 33.33R-AM-TMP 85.71 52.94 65.46Table 3: top: overall performance result ondata sets ; bottom: detail result on WSJdataTable 2 compares the performance of ourmethod and other three famous kernels onWSJ test data.
We implemented these threemethods with the same settings describedin the papers.
It shows that our kernelachieves the best performance with 88.48%accuracy.
The advantages of our approachare: 1).
the alignments allow soft syntacticstructure match; 2).
threshold can avoid over-generation and selected salient alignments.Table 3 gives our performance on data setsand the detail result on WSJ test data.29Similarity DefinitionWu and Palmer simWUP (c1, c2) = 2dep(lso(c1,c2))d(c1,lso(c1,c2))+d(c2,lso(c1,c2))+2dep(lso(c1,c2))Resnik simRES(c1, c2) = ?
logP (lso(c1, c2))Lin simLIN(c1, c2) = 2 logP (lso(c1,c2))logP (c1)+logP (c2)Table 4: popular semantic similarity measurements4.2 Question Classification4.2.1 Semantic-enriched Tree KernelAnother problem in the tree kernel (Collinsand Duffy, 2002) is the lack of semantic in-formation, since the match stops at the pre-terminals.
All the lexical information is en-coded at the leaf nodes of parsing trees.
How-ever, the semantic knowledge is important insome text applications, like Question Classi-fication.
To introduce semantic similaritiesbetween words into our kernel, we use theframework in Bloehdorn et al (2007) andrewrite the rule (2) in the iterative tree kernelcalculation(in section 2).
(2) If the productions at n1 andn2 are same, and n1 and n2 arepre-terminals, then 4(n1, n2) =?
?kw(w1, w2)where w1 and w2 are two words derived frompre-terminals n1 and n2, respectively, and theparameter ?
is to control the contribution ofthe leaves.
Note that each preterminal hasone child or equally covers one word.
Sokw(w1, w2) actually calculate the similaritybetween two words w1 and w2.In general, there are two ways to mea-sure the semantic similarities.
One is to de-rive from semantic networks such as Word-Net (Mavroeidis et al, 2005; Bloehdorn etal., 2006).
The other way is to use statisti-cal methods of distributional or co-occurrence(O?
Se?aghdha and Copestake, 2008) behaviorof the words.WordNet2 can be regarded as direct graphssemantically linking concepts by means ofrelations.
Table 4 gives some similaritymeasures between two arbitrary concepts c12http://wordnet.princeton.edu/and c2.
For our application, the word-to-word similarity can be obtained by maximiz-ing the corresponding concept-based similar-ity scores.
In our implementation, we useWordNet::Similarity package3(Patwardhan etal., 2003) and the noun hierarchy of WordNet.In Table 4, dep is the length of path from anode to its global root, lso(c1, c2) representsthe lowest super-ordinate of c1 and c2.
Thedetail definitions can be found in (Budanitskyand Hirst, 2006) .As an alternative, Latent Semantic Anal-ysis(LSA) is a technique.
It calculates thewords similarities by means of occurrenceof terms in documents.
Given a term-by-document matrix X , its singular value decom-position is: X = U?V T , where ?
is a diago-nal matrix with singular values in decreasingarrangement.
The column of U are singularvectors corresponding to the individual singu-lar value.
Then the latent semantic similaritykernel of terms ti and tj is:simLSA =< U ik(U jk)T > (8)where Uk = IkU is to project U onto its firstk dimensions.
Ik is the identity matrix whosefirst k diagonal elements are 1 and all the otherelements are 0.
And U ik is the i-th row ofthe matrix Uk.
From equation (8), the LSA-based similarity between two terms is the in-ner product of the two projected vectors.
Thedetails of LSA can be found in (Cristianini etal., 2002; Choi et al, 2001).4.2.2 Experiment ResultsIn this set of experiment, we evaluate differ-ent types of kernels for Question Classifica-tion(QC) task.
The duty of QC is to cat-egorize questions into different classes.
In3http://search.cpan.org/dist/WordNet-Similarity30Accuracy(%) 1000 2000 3000 4000 5500BOW 77.1 83.3 87.2 87.3 89.2TK 80.2 86.2 87.4 88.6 91.2LATK 80.4 86.5 87.5 88.8 91.6?
= 1WUP 81.3 87.3 88.0 89.8 92.5RES 81.0 87.1 87.9 89.5 92.2LIN 81.1 87.0 88.0 89.3 92.4LSA(k = 50) 80.8 86.9 87.8 89.3 91.7Table 5: Classification accuracy of different kernels on different data setsthis paper we use the same dataset as intro-duced in(Li and Roth, 2002).
The dataset isdivided4 into 5500 questions for training and500 questions from TREC 20 for testing.
Thetotal training samples are randomly dividedinto 5 subsets with sizes 1,000, 2,000, 3,000,4,000 and 5,500 respectively.
All the ques-tions are labeled into 6 coarse grained cate-gories and 50 fine grained categories: Abbre-viations (abbreviation and expansion), Entity(animal, body, color, creation, currency, med-ical, event, food, instrument, language, let-ter, plant, product, religion, sport, substance,symbol, technique, term, vehicle, word), De-scription (definition, description, manner, rea-son), Human (description, group, individual,title), Location (city, country, mountain, state)and Numeric (code, count, date, distance,money, order, percent, period, speed, temper-ature, size, weight).In this paper, we compare the linear ker-nel based on bag-of-word (BOW), the originaltree kernel (TK), the local alignment tree ker-nel (section 3, LATK) and its correspondenceswith LSA similarity and a set of semantic-enriched LATK with different similarity met-rics.To obtain the parse tree, we use Charniakparser5 for every question.
Like the previ-ous experiment, SVM-Light-TK software andthe OVA strategy are implemented.
In all ex-periments, we use the default parameter inSVM(e.g.
margin parameter) and set ?
= 1.In LSA model, we set k = 50.
Finally, weuse multi-classification accuracy to evaluate4http://l2r.cs.uiuc.edu/ cogcomp/Data/QA/QC/5ftp://ftp.cs.brown.edu/pub/nlparser/the performance.Table 5 gives the results of the experiments.We can see that the local alignment tree ker-nel increase the multi-classification accuracyof the basic tree kernel by about 0.4%.
Theintroduction of semantic information furtherimproves accuracy.
Among WordNet-basedmetrics, ?Wu and Palmer?
metric achievesthe best result, i.e.
92.5%.
As a whole,the WordNet-based similarities perform betterthan LSA-based measurement.5 ConclusionIn this paper, we propose a tree kernel calcula-tion by allowing local alignments.
More flex-ible productions are considered in line withmodifiers in real sentences.
Considering textrelated applications, words similarities havebeen merged into the presented tree kernel.These similarities can be derived from dif-ferent WordNet-based metrics or documentstatistics.
Finally experiments are carried ontwo different applications (Semantic Role La-beling and Question Classification).For further work, we plan to study exploit-ing semantic knowledge in the kernel.
Apromising direction is to study the differenteffects of these semantic similarities.
We areinterested in some distributional similarities(Lee, 1999) given certain context.
Also theeffectivenss of the semantic-enriched tree ker-nel in SRL is another problem.ReferencesStephan Bloehdorn, Roberto Basili, Marco Cammisa,and Alessandro Moschitti.
2006.
Semantic kernelsfor text classification based on topological measures31of feature similarity.
In ICDM ?06: Proceedings ofthe Sixth International Conference on Data Mining,pages 808?812, Washington, DC, USA.
IEEE Com-puter Society.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.X.
Carreras and L. Marquez.
2005.
Introduction to theconll-2005 shared task: Semantic role labeling.
InCoNLL ?05: Proceedings of the 9th Conference onComputational Natural Language Learning.Freddy Y. Y. Choi, Peter Wiemer-hastings, and JohannaMoore.
2001.
Latent semantic analysis for textsegmentation.
In In Proceedings of EMNLP, pages109?117.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
InACL, pages 263?270.Nello Cristianini, John Shawe-Taylor, and HumaLodhi.
2002.
Latent semantic kernels.
J. Intell.Inf.
Syst., 18(2-3):127?152.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In ACL ?04:Proceedings of the 42nd Annual Meeting on Asso-ciation for Computational Linguistics, pages 423?429, Morristown, NJ, USA.
Association for Com-putational Linguistics.David Haussler.
1999.
Convolution kernels on discretestructures.
Technical report.Tetsuji Kuboyama, Kilho Shin, and Hisashi Kashima.2006.
Flexible tree kernels based on counting thenumber of tree mappings.
In ECML/PKDD Work-shop on Mining and Learning with Graphs.Lillian Lee.
1999.
Measures of distributional similar-ity.
In 37th Annual Meeting of the Association forComputational Linguistics, pages 25?32.Xin Li and Dan Roth.
2002.
Learning question clas-sifiers.
In Proceedings of the 19th internationalconference on Computational linguistics, pages 1?7, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Dimitrios Mavroeidis, George Tsatsaronis, MichalisVazirgiannis, Martin Theobald, and GerhardWeikum.
2005.
Word sense disambiguation forexploiting hierarchical thesauri in text classification.In Al?
?pio Jorge, Lu?
?s Torgo, Pavel Brazdil, RuiCamacho, and Gama Joao, editors, Knowledgediscovery in databases: PKDD 2005 : 9th Eu-ropean Conference on Principles and Practiceof Knowledge Discovery in Databases, volume3721 of Lecture Notes in Computer Science, pages181?192, Porto, Portugal.
Springer.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role label-ing.
Comput.
Linguist., 34(2):193?224.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow semantic parsing.
In ACL ?04:Proceedings of the 42nd Annual Meeting on Asso-ciation for Computational Linguistics, pages 335?342, Morristown, NJ, USA.
Association for Com-putational Linguistics.Diarmuid O?
Se?aghdha and Ann Copestake.
2008.
Se-mantic classification with distributional kernels.
InProceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages649?656, Manchester, UK, August.
Coling 2008 Or-ganizing Committee.Siddharth Patwardhan, Satanjeev Banerjee, and TedPedersen.
2003.
Using measures of semantic re-latedness for word sense disambiguation.
In In Pro-ceedings of the Fourth International Conference onIntelligent Text Processing and Computational Lin-guistics (CICLING-03), pages 241?257.Longhua Qian, Guodong Zhou, Fang Kong, QiaomingZhu, and Peide Qian.
2008.
Exploiting constituentdependencies for tree kernel-based semantic relationextraction.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling2008), pages 697?704, Manchester, UK, August.Coling 2008 Organizing Committee.Hiroto Saigo, Jean-Philippe Vert, Nobuhisa Ueda, andTatsuya Akutsu.
2004.
Protein homology detec-tion using string alignment kernels.
Bioinformatics,20(11):1682?1689.Kilho Shin and Tetsuji Kuboyama.
2008.
A gener-alization of haussler?s convolution kernel: mappingkernel.
In ICML, pages 944?951.Nianwen Xue and Martha Palmer.
2004.
Calibrat-ing features for semantic role labeling.
In DekangLin and Dekai Wu, editors, Proceedings of EMNLP2004, pages 88?94, Barcelona, Spain, July.
Associ-ation for Computational Linguistics.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relation ex-traction.
J. Mach.
Learn.
Res., 3:1083?1106.Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,Guodong Zhou, Ting Liu, and Sheng Li.
2007.A grammar-driven convolution tree kernel for se-mantic role classification.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 200?207, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.32
