Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30?38,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMaximizing Component Quality in Bilingual Word-Aligned SegmentationsSpyros Martzoukos Christophe Costa Flor?encio Christof MonzIntelligent Systems Lab Amsterdam, University of AmsterdamScience Park 904, 1098 XH Amsterdam, The Netherlands{S.Martzoukos, C.CostaFlorencio, C.Monz}@uva.nlAbstractGiven a pair of source and target languagesentences which are translations of each otherwith known word alignments between them,we extract bilingual phrase-level segmenta-tions of such a pair.
This is done by identi-fying two appropriate measures that assess thequality of phrase segments, one on the mono-lingual level for both language sides, and oneon the bilingual level.
The monolingual mea-sure is based on the notion of partition refine-ments and the bilingual measure is based onstructural properties of the graph that repre-sents phrase segments and word alignments.These two measures are incorporated in a ba-sic adaptation of the Cross-Entropy methodfor the purpose of extracting an N -best listof bilingual phrase-level segmentations.
Astraight-forward application of such lists inStatistical Machine Translation (SMT) yieldsa conservative phrase pair extraction methodthat reduces phrase-table sizes by 90% withinsignificant loss in translation quality.1 IntroductionGiven a pair of source and target language sen-tences which are translations of each other withknown word alignments between them, the problemof extracting high quality bilingual phrase segmen-tations is defined as follows: Maximize the qualityof phrase segments, i.e., groupings of consecutivewords, in both language sides, subject to constraintsimposed by the underlying word alignments.
Thepurpose of this work is to provide a solution to thismaximization problem and investigate the effect ofthe resulting high quality bilingual phrase segmentson SMT.
For brevity, ?phrase-level sentence segmen-tation?
and ?phrase segment?
will henceforth be sim-ply referred to as ?segmentation?
and ?segment?
re-spectively.The exact definition of segments?
quality dependson the application.
Our notion of a segmentation ofmaximum quality is defined as the set of consecutivewords of the sentence that captures maximum col-locational and/or grammatical characteristics.
Thisimplies that a sequence of tokens is identified as asegment if its fully compositional expressive poweris higher than the expressive power of any combina-tion of partial compositions.
Since this definition isfairly general it is thus suitable for most NLP tasks.In particular, it is tailored to the type of segmentsthat are suitable for the purposes of SMT and is inline with previous work (Blackwood et al., 2008;Paul et al., 2010).With this definition in mind, we introduce amonolingual segment quality measure that is basedon assessing the cost of converting one segmentationinto another by means of an elementary operation.This operation, namely the ?splitting?
of a segmentinto two segments, together with all possible seg-mentations of a sentence are known to form a par-tially ordered set (Guo, 1997).
Such a constructionis known as partition refinement and gives rise to thedesired monolingual surface quality measure.The presence of word alignments between thesentence pair provides additional structure whichshould not be ignored.
In the language of graph the-ory, a segment can also be viewed as a chain, i.e., agraph in which vertices are the segment?s words and30an edge between two words exists if and only if thesewords are consecutive.
Then, a bilingual segmenta-tion is represented by the graph that is formed by allits source and target language chains together withedges induced by word alignments.
Motivated bythe phrase pair extraction methods of SMT (Och etal., 1999; Koehn et al., 2003), we focus on the con-nected components, or simply components of such arepresentation.
We explain that the extent to whichwe can delete word alignments from a componentwithout violating its component status, gives rise toa bilingual, purely structural quality measure.The surface and structural measures are incorpo-rated in one algorithm that extracts an N -best listof bilingual word-aligned segmentations.
This algo-rithm, which is an adaptation of the Cross-Entropymethod (Rubinstein, 1997), performs joint maxi-mization of surface (in both languages) and struc-tural quality measures.
Components of graph repre-sentations of the resulting N -best lists give rise tohigh quality translation units.
These units, whichform a small subset of all possible (continuous) con-sistent phrase pairs, are used to construct SMT mod-els.
Results on Czech?English and German?Englishdatasets show a 90% reduction in phrase-table sizeswith insignificant loss in translation quality whichare in line with other pruning techniques in SMT(Johnson et al., 2007; Zens et al., 2012).2 Monolingual Surface Quality MeasureGiven a sentence s1s2...skthat consists of wordssi, 1 ?
i ?
k, we introduce an empirical count-based measure that assesses the quality of its seg-mentations.
By fixing a segmentation ?, we are in-terested in assessing the cost of perturbing ?
andgenerating another segmentation ??.
A perturbationof ?
is achieved by splitting a segment of ?
intotwo new segments, while keeping all other segmentsfixed.
For example, for a sentence with five words, if?
: (s1s2)(s3s4s5), where brackets are used to dis-tinguish the segments s1s2and s3s4s5, then ?
canbe perturbed in three different ways:?
??
: (s1)(s2)(s3s4s5), by splitting the first seg-ment of ?.?
???
: (s1s2)(s3)(s4s5), by splitting at the firstposition of the second segment of ?.?
????
: (s1s2)(s3s4)(s5), by splitting at the sec-ond position of the second segment of ?,so that ?
?, ??
?and ???
?are the perturbations of ?.Such perturbations are known as partition refine-ments in the literature (Stanley, 1997).
The set of allsegmentations of a sentence, equipped with the split-ting operation forms a partially ordered set (Guo,1997), and its visual representation is known as theHasse diagram.
Figure 1 shows such a partially or-dered set for a sentence with four words.
?s1 s2 s3 s4??s1?
?s2 s3 s4?
?
s1 s2??
s3 s4?
?s1 s2 s3??s4??s1??s2??
s3 s4?
?s1?
?s2 s3??
s4?
?s1 s2??s3??
s4??s1??s2??
s3?
?s4?Figure 1: Hasse diagram of segmentation refine-ments for a sentence with four words.The cost of perturbing a segmentation into an-other, i.e., the weight of a directed edge in the Hassediagram, is calculated from n-gram counts that areextracted from a monolingual training corpus.
Letn(s) be the empirical count of phrase s in the corpus.Given a segmentation ?
of a sentence, let seg(?)
de-note the set of ?
?s segments.
In the above examplewe have for instance seg(???)
= {s1s2, s3, s4s5}.The probability of s in ?
is given by relative fre-quenciesp?
(s) =n(s)?s??seg(?)n(s?).
(1)The cost of perturbing ?
into ?
?by splitting a seg-ment ss?
of ?
into segments s and s?
is defined bycost????
(s, s?)
= logp?(ss?)p??(s)p??(s?
), (2)and we say that s and s?
are co-responsible for theperturbation ?
?
??.
Intuitively, this cost functionyields the amount of energy (log of probability) thatis lost when performing a perturbation.
On a more31technical level, it is closely related to metric spaceson partially ordered sets (Monjardet, 1981; Orumand Joslyn, 2009), but we do not go into further de-tails here.The cost function admits a measure for the seg-ments that are co-responsible for perturbing ?
into?
?and we define the gain of s from the perturbation?
?
??asgain????
(s) = ?cost????
(s, s?).
(3)A segment smay be co-responsible for different per-turbations, and we have to consider all such pertur-bations.
LetR(s) = {?
?
??
: s /?
seg(?
), s ?
seg(??)}
(4)denote the set of perturbations for which s is co-responsible.
Then, the average gain of s in the sen-tence is given bygain(s) =1|R(s)|?{????}?R(s)gain????(s).
(5)Intuitively, gain(s) measures how difficult it is tobreak phrase s into sub-phrases.
Finally, the surfacequality measure of a segmentation ?
of a sentence isgiven byg(?)
=?s?seg(?)gain(s).
(6)Note that g is a real number.
The relation g(?)
>g(??)
implies that ?
is a better segmentation than ?
?.We conclude this section with two remarks: (i)The exact computation of gain(s) for each possi-ble segment s is computationally expensive sinceall perturbations need to be considered.
In prac-tice we can simply generate a random sample of nomore than 1500 segmentations and compute gain(?
)based on that sample only.
(ii) Each sentence ofthe monolingual training corpus (from which the n-gram counts are extracted) should have the begin-ning and end-of-sentence tokens.
The count for eachof them is equal to the number of sentences in thecorpus, and they are treated as regular words.
With-out going into further details they provide the pur-pose of normalization.3 Bilingual Structural Quality MeasureGiven a word-aligned sentence pair, we introduce apurely structural measure that assesses the quality ofits bilingual segmentations.
By ?purely structural?we mean that the focus is entirely on combinatorialaspects of the bilingual segmentations and the wordalignments.
For that reason we turn to a graph theo-retic framework.A segment can also be viewed as a chain, i.e., agraph in which vertices are the segment?s words andan edge between two words exists if and only if thesewords are consecutive.
Then, a source segmentation?
and a target segmentation ?
are graphs that con-sist of source chains and target chains respectively.The graph formed by ?, ?
and the translation edgesinduced by word alignments is thus a graph repre-sentation of a bilingual word-aligned segmentation.We focus on a particular type of subgraphs of thisrepresentation, namely its connected components, orsimply components.
A component is a graph suchthat (a) there exists a path between any two of itsvertices, and (b) there does not exist a path betweena vertex of the component and a vertex outside thecomponent.
Condition (a) means, both technicallyand intuitively, that a component is connected andCondition (b) requires connectivity to be maximal.Components play a key role in SMT.
The mostwidely used strategy for extracting high qualityphrase-level translations without linguistic informa-tion, namely the consistency method (Och et al.,1999; Koehn et al., 2003) is entirely based on com-ponents of word aligned unsegmented sentence pairs(Martzoukos et al., 2013).
In particular, each ex-tracted translation is either a component or the unionof components.
Since an unsegmented sentencepair is just one possible configuration of all possi-ble bilingual segmentations, we consequently haveno direct reason to investigate further than compo-nents.In order to get an intuition of the measure that willbe introduced in this section, we begin with an ex-ample.
Figure 2, shows two different configurationsof the pair (?, ?)
for the same sentence pair withknown and fixed word alignments.
Both configu-rations have the same number of edges that connectsource vertices (3) and the same number of edgesthat connect target vertices (2).
However, one would321 2 3 4 ?
?
?1 2 3 4 ?
?1 2 3 4 ?
?
?1 2 3 4 ?
?Figure 2: Graph representations of two bilingualsegmentations with fixed word alignments.
Sourceand target vertices are shown with circles andsquares respectively.expect the top configuration to represent a betterbilingual segmentation.
This is because it has morecomponents (4 opposed to 2 for the bottom config-uration) and because it consists of ?tighter?
clusters,i.e., ?tighter?
components.A general measure that would capture this obser-vation requires a balance between the number ofedges of source and target chains, the number ofcomponents and the number of translation edges, allcoupled with how these edges and vertices are con-nected.
This might seem as a daunting task that canbe tackled with a combination of heuristics, but thereis actually a graph-theoretic measure that can fullydescribe the sought structure.
We proceed with in-troducing this measure.Let C denote the set of components of the graphrepresentation of a bilingual word-aligned segmen-tation.
We are interested in measuring the extent towhich we can delete translation edges from c ?
C,while retaining its component status.
Let acdenotethe subset of translation edges that are restricted tothe component c. We define the positive integergain(c) = number of ways ofdeleting translation edges from ac,while keeping c connected, (7)where the option of deleting nothing is counted.
In-tuitively, by keeping the edges of the chains fixedthe quantity gain(c) measures how difficult it is toperturb a component from its connected state to adisconnected state.Figure 3 shows two components c and c?that sat-isfy gain(c) = gain(c?)
= 3.
Both componentsare equally difficult to be perturbed into a discon-nected state, but only superficially.
The actual struc-tural quality of c is revealed when it is ?compared?
tocomponent c?
that consists of the same source and tar-get vertices, the same translation edges but its sourcevertices form exactly one chain and similarly for itstarget vertices; c?
is essentially the ?upper bound?
ofc.
In general, the maximum value of gain(c), withcc '?cFigure 3: Superficially similar components c and c?.Comparing c with c?
yields c?s true structural quality.respect to a fixed set of source and target verticesand translation edges, is attained when it consistsof exactly one source chain and exactly one targetchain.
It is not difficult to see that the desired max-imum value is always 2|ac|?
1.
In the example ofFigure 3, the structural quality of c and c?is thus3/(25?
1) = 9.7% and 3/(22?
1) = 100% respec-tively.
Hence, the measure that evaluates the struc-tural quality of a bilingual word-aligned segmenta-tion (?, ?)
is given byf(?, ?)
=(?c?Cgain(c)2|ac|?
1)1|C|, (8)which takes values in (0, 1].
The relation f(?, ?)
>f(?
?, ??)
implies that (?, ?)
is a better bilingual seg-mentation than (?
?, ??
).We conclude this section with two remarks: (i) Acomponent with no translation edges, i.e., a sourceor target segment whose words are all unaligned, hasa contribution of 1/0 in (8).
In practice we excludesuch components from C. (ii) In graph theory thequantity gain(c) is known as the number of con-nected spanning subgraphs (CSSGs) of graph c andis the key quantity of network reliability (Valiant,1979; Coulbourn, 1987).
Finding the number ofCSSGs of a general graph is a known #P-hard prob-lem (Welsh, 1997).
In our setting, graphs have spe-cific formation (source and target chains connectedvia translation edges) and we are interested in thedeletion of translation edges only; it is possible to33compute gain(?)
in polynomial time, but we do notgo into further details here.4 Extracting Bilingual Segmentations withthe Cross-Entropy MethodEquipped with the measures of Sections 2 and 3 weturn to extracting anN -best list of bilingual segmen-tations for a given sentence pair.
The search space isexponential in the total number of words of the sen-tence pair.
We propose a new approach for this task,by noting a direct connection with the combinato-rial problems that can be solved efficiently and ef-fectively with the Cross-Entropy (CE) method (Ru-binstein, 1997).The CE method is an iterative self-tuning sam-pling method that has applications in various com-binatorial and continuous global optimization prob-lems as well as in rare event detection.
A detailedaccount on the CE method is beyond the scope ofthis work, and we thus simply describe its applica-tion to our problem.In particular, we first establish the connection be-tween the most basic form of the CE method and theproblem of finding the best monolingual segmen-tation of a sentence, with respect to some scoringfunction (not necessarily the one that was introducedin Section 2).
This connection yields a simple, ef-ficient and effective algorithm for the monolingualmaximization problem.
Then, the transition to thebilingual level is done by incorporating the measureof Section 3 in the algorithm, thus performing jointmaximization of surface and structural quality.
Fi-nally, the generation of theN -best list will be trivial.A segmentation of a given sentence has a bit-string representation in the following way: If twoconsecutive words in the sentence belong to thesame segment in the segmentation, then this pair ofwords is encoded by ?1?, otherwise by ?0?.
Such arepresentation is bijective and, thus, for the rest ofthis section, we do not distinguish between a seg-mentation and its bit-string representation.
In thissetting, the CE method takes its most basic form(De Boer et al., 2005).
In a nutshell, it is a re-peated application of (a) sampling bit-strings froma parametrized probability mass function, (b) scor-ing them and keeping only a small high-performingsubsample, and (c) updating the parameters of theprobability mass function based on that subsampleonly.We assume no prior knowledge on the qualityof bit-strings, so that they are all equally likely.
Inother words, each position of a randomly chosenbit-string can be either a ?0?
or a ?1?
with probability1/2.
The aim is to tune these position probabilitiestowards the best bit-string, with respect to somescoring function g. In particular, let the sentencehave n words and let ` = n ?
1 be the length ofbit-strings.
A bit-string labeled by an integer i isdenoted by biand its jth bit by bij.
The algorithm isas follows:0.
Initialize the bit-string position probabilitiesp0= (p01, ..., p0`) = (1/2, ..., 1/2) and set M = 20`(sample size), ?
= d1%Me (keep top 1% ofsamples), ?
= 0.7 (smoothing parameter) and t = 1(iteration).1.
Generate a sample b1, ..., bMof bit-strings, eachof length `, such that bij?Bernoulli(pt?1j), for alli = 1, ...,M and j = 1, ..., `.1.1 Compute scores g(b1), ..., g(bM).1.2 Order them descendingly as g(bpi(1)) > ... >g(bpi(M)).2.
Focus on the best performing ones: Compute?t= g(bpi(?
)); samples performing less than thisthreshold will be ignored.3.
Use the best performing sub-sample of b1, ..., bMto update position probabilities:ptj=?Mi=1Ii(?t)bij?Mi=1Ii(?t), j = 1, ..., `, (9)where the choice function Iiis given byIi(?t) ={1, if g(bi) > ?t0, otherwise.4.
Smooth the updated position probabilities asptj:= ?ptj+ (1 ?
?
)pt?1j, j = 1, ..., `.
(10)E. If for some t > 5we have ?t= ?t?1= ... = ?t?5then stop.
Else, t := t + 1 and go to Step 1.34The values for the parameters M , ?
and ?
re-ported here are in line with the ones suggested in theliterature (Rubinstein and Kroese, 2004) for combi-natorial problems such as this one.
After the execu-tion of the algorithm, the updated vector of positionprobabilities converges to sequence of ?0?s and ?1?s,which corresponds to the best segmentation under g.The extension to bilingual level is done by incor-porating the structural quality measure of Section 3.The setting is similar, i.e., samples are again bit-strings, but of length ` = n + m ?
2, where n andm are the number of words in the source and tar-get sentence respectively.
The first n ?
1 bits corre-spond to the source sentence and the rest to the targetsentence.
The surface quality score of such a bit-string is given by the harmonic mean of its sourceand target surface quality scores.1The bit-stringscoring function throughout Steps 1 ?
3 is given bythe harmonic mean of surface and structural qualityscores.
Finally, N -best lists are trivially generated,simply by collecting the top-N performing accumu-lated samples of a maximization process.5 ExperimentsGiven a sentence pair with known and fixed wordalignments, the result of the method described inSection 4 is an N -best list of bilingual segmenta-tions of such a pair.
The objective function providesa balance between compositional expressive powerof segments in both languages and synchronizationvia word alignments.
Thus, each (continuous) com-ponent of such a bilingual segmentation leads to theextraction of a high quality phrase pair.As was mentioned in Section 3, each extractedphrase pair of standard phrase-based SMT is con-structed from a component or from the union ofcomponents of an unsegmented word-aligned sen-tence pair.
For each sentence pair, all possible(continuous) components and (continuous) unionsof components give rise to the extracted (contin-uous) phrase pairs.
In this section we investigatethe impact to SMT models and translation quality,when extracting phrase pairs (from the N -best lists)1As it was mentioned in Section 2 the surface quality scorein (6) is a real number.
At each iteration of the algorithm thesurface score of a segmentation can be converted into a numberin [0, 1] via Min-Max normalization.
This holds for both sourceand target sides of a bit-string (independently).Cz?En De?EnEuroparl (v7) 642,505 1,889,791News Commentary (v8) 139,679 177,079Total 782,184 2,066,870Table 1: Number of filtered parallel sentences forCzech?English and German?English.that correspond to components only.
A reductionin phrase-table size is guaranteed because we areessentially extracting only a subset of all possiblecontinuous phrase pairs.
The challenge is to verifywhether this subset can provide a sufficient transla-tion model.Both the baseline and our system are standardphrase-basedMT systems.
Bidirectional word align-ments are generated with GIZA++ (Och and Ney,2003) and ?grow-diag-final-and?.
These are usedto construct a phrase-table with bidirectional phraseprobabilities, lexical weights and a reordering modelwith monotone, swap and discontinuous orienta-tions, conditioned on both the previous and the nextphrase.
4-gram interpolated language models withKneser-Ney smoothing are built with SRILM (Stol-cke, 2002).
A distortion limit of 6 and a phrase-penalty are also used.
All model parameters aretuned with MERT (Och, 2003).
Decoding duringtuning and testing is done with Moses (Koehn et.
al,2007).
Since our system only affects which phrasesare extracted, lexical weights and reordering orien-tations are the same for both systems.Datasets are from the WMT?13 translation task(Bojar et al., 2013): Translation and reorderingmodels are trained on Czech?English and German?English corpora (Table 1).
Language models andsegment measures gain , as defined in (5), are trainedon 35.3M Czech, 50.0M German and 94.5M En-glish sentences from the provided monolingual data.Tuning is done on newstest2010 and performanceis evaluated on newstest2008, newstest2009, new-stest2011 and newstest2012 with BLEU (Papineniet al., 2001).In our experiments the size of anN -best list variesaccording to the total number of words in the sen-tence pair, say w. For the purposes of phrase ex-traction in SMT we would ideally require all localmaxima to be part of an N -best list.
This would35MethodCzech?English English?Czech Czech?English?08 ?09 ?11 ?12 ?08 ?09 ?11 ?12 PT size (retain%)Baseline 19.6 20.6 22.6 20.6 14.8 15.6 16.6 14.9 44.6M (100%)N -best 19.7 20.4 22.4 20.3 14.4 15.2 16.3 14.3 4.4M (9.8%)N -best & unseg.
19.6 20.5 22.6 20.7 14.6 15.4 16.8 14.7 4.6M (10.4%)Table 2: BLEU scores and phrase-table (PT) sizes for Czech?English.
Phrase-table of ?Baseline?
is con-structed from all consistent phrase pairs.
Phrase-table of ?N -best?
is constructed from consistent phrasepairs that are components of the top-N bilingual word-aligned segmentations of each sentence pair.
Simi-larly for ?N -best & unseg.
?, but consistent phrase pairs that are components of each (unsegmented) sentencepair are also included.MethodGerman?English English?German German?English?08 ?09 ?11 ?12 ?08 ?09 ?11 ?12 PT size (retain%)Baseline 21.4 20.8 21.3 22.1 15.1 15.1 16.0 16.5 102.3M (100%)N -best 21.3 20.6 21.3 21.8 15.0 15.0 15.6 16.0 9.4M (9.2%)N -best & unseg.
21.5 20.8 21.5 22.0 15.4 15.2 15.7 16.2 9.9M (9.7%)Table 3: Similar to Table 2, but for German?English.guarantee the extraction of all high quality phrasepairs, with (empirically) desired variations, whilekeeping N small.
Since the CE method performsglobal optimization, the resulting members of an N -best list are in the vicinity of the global maximum.Consequently, we cannot guarantee the inclusion oflocal maxima.
We set N = d30%we so that atleast some variation from the global maximum is in-cluded, but is not large enough to contaminate thelists with noisy bilingual segmentations.
The result-ing lists have 22 bilingual segmentations on aver-age for both language pairs.
Figure 4 shows typicalGerman?English best performing bilingual segmen-tations.BLEU scores are reported in Tables 2 and 3 forCzech?English and German?English respectively.Methods ?Baseline?
and ?N -best?
are the ones de-scribed above.
Phrase-table sizes are reduced asexpected and performance when translating to En-glish is comparable.
The significant drops in new-stest2012 when translating from the morphologi-cally poorer language (English) prompts us to in-clude more ?basic?
phrase pairs in the phrase-tables.This leads to augmenting each N -best list by its un-segmented sentence pair.
Consequently, method ?N -best & unseg.?
extracts the same phrase pairs as ?N -best?, together with those from components of theunsegmented sentence pairs.
As a result, transla-tion quality is comparable to ?Baseline?
across alllanguage directions and small phrase-table sizes areretained.6 Discussion and Future WorkThis work can also be viewed as an attempt to un-derstand bilinguality as a generalization of mono-linguality.
There is conceptual common ground onwhat gain(x) for phrase x (Section 2) or componentx (Section 3) computes.
In both cases it measureshow ?stable?
a unit is.
The stability of a phrase x isdetermined by how difficult it is to split x into multi-ple phrases.
The partially ordered set framework ofpartition refinements is the natural setting for suchcomputations.
In order to determine the stabilityof a component we turn to empirical evidence fromSMT: ?good?
phrase pairs are extracted from com-ponents or unions of components of the graph thatrepresents word-aligned sentence pairs.
The stabil-ity of a component x is therefore determined by howdifficult it is to break x into multiple components.
Itis thus interesting to investigate whether there existsa general approach that unifies partition refinementsand network reliability for the purpose of identifyinghighly stable multilingual units.3612 34???
?
?
?
?
34 ?
?4 ??
??
??
?2???
?3?
?
?3??
?
?
?
?34?
?4 ??
?
???
?
?
2?4????
??
?
?4?41?14 ??
?
3??
?23??
?
?
?
?
?
?
?
?
?
?
?
?
?3?
3 ??
?
?
?
?3?
?
??4???
?
?34?14?
3???4?
??
?
?
?
?
?
??3?
??
?
?
?3?
??
?
?
?
?
?4 12 432?4 ??
?221??1?42?
??
?
?4 ?3?
??
?
?
?
?
?
?
?4 ??3??
?
?
?
?
?
4?1?
?221?14 ???34???4?
?4 ??1???
?4 ??
?
?
?
?
?1?
?4?
??
?
?
?
?
?
?1??
?
?4 ?41?44???
?13?1?4?
??
?
?
?
?4 ??1??
?
?
?34?
34?
??
?
?
?
?
?
?
?34 ?41?4 ????
?
?
?
?
14???
?
??
?1?421?1???1???
?32?1?3???
?
?
?
?
?4??21?1?3??32?1?3????
???
?4?3????
?
?
?4???
?
?
?
?
?
?
?
??
??
?
?
?
?
?
?
?
?
?4?4????
?1???
?4??1???
?
?
???
?
?
???
?3143??????
?2?4?3???34????
?
?
?2?4???
??4??44?4?34?3????
?
?4?1???
?
?
?
?
?
?4?1??1??3??421??1???3????
?
??
?3????
??1???
?
??
?
?2???
?
??
?
????
?3?3???11??????
?
?2???3?1????1?1??21????3412???34?2??1????2???3?1???
?
??3??3213??4???4?
?Figure 4: Typical fragments from best performingGerman?English segmentations.The focus has been on bilingual segmentations,but as was mentioned in Section 2, it is possibleto apply the CE method for generating monolingualsegmentations.
By using (6) as the objective func-tion, we observed that the resulting segmentationsyield promising applications in n-gram topic model-ing, named entity recognition and Chinese segmen-tation.
However, in the spirit of Ries et al.
(1996),attempts to minimize perplexity instead of maximiz-ing (6), resulted in larger segments and the segmentquality definition of Section 1 was not met.The sizes of the resulting phrase-tables togetherwith the type of phrase pairs that are extracted leadto applications involving discontinuous phrase pairs.In (Galley and Manning, 2010) there was evidencethat discontinuous phrase pairs that are extractedfrom discontinuous components of word-alignedsentence pairs can improve translation quality.1Asthe number of such components is much bigger thanthe continuous ones, (Gimpel and Smith, 2011) pro-pose a Bayesian nonparametric model for finding themost probable discontinuous phrase pairs.
This canalso be done from the N -best lists that are generatedin Section 4, and it would be interesting to see theeffect of such phrase pairs in our existing models.In a longer version of this work we intend tostudy the effect in translation quality when varyingsome of the parameters (size of N -best lists, samplesizes for training gain in Section 2 and for the CEmethod), as well as when extracting source-drivenbilingual segmentations as in (Sanchis-Trilles et al.,2011).7 ConclusionsIn this work, we have presented a solution to theproblem of extracting bilingual segmentations in thepresence of word alignments.
Two measures that as-sess the quality of bilingual segmentations based onthe expressive power of segments in both languagesand their synchronization via word alignments havebeen introduced.
We have established the link be-tween the CE method and finding the best monolin-gual and bilingual segmentations.
These measuresformed the objective function of the CE methodwhose maximization resulted in an N -best list ofbilingual segmentations for a given sentence pair.By extracting only phrase pairs that correspond tocomponents from bilingual segmentations of thoselists, we found that phrase table sizes can be reducedwith insignificant loss in translation quality.AcknowledgementsThis research was funded in part by the Euro-pean Commission through the CoSyne project FP7-ICT-4-248531 and the Netherlands Organisationfor Scientific Research (NWO) under project nr.639.022.213.1By ?discontinuous component?
we mean a componentwhose source or target words (vertices) form a discontinuoussubstring in the source or target sentence respectively.37ReferencesGraeme Blackwood, Adria de Gispert, and WilliamByrne.
2008.
Phrasal Segmentation Models for Sta-tistical Machine Translation.
In COLING.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, Philipp Koehn,Christof Monz, Matt Post, Radu Soricut, and LuciaSpecia.
2013.
Findings of the 2013 Workshop on Sta-tistical Machine Translation.
In WMT.Charlie J. Coulbourn.
1987.
The Combinatorics of Net-work Reliability.
Oxford University Press.Pieter-Tjerk De Boer, Dirk P. Kroese, Shie Mannor, andReuven Y. Rubinstein.
2005.
A Tutorial on the Cross-Entropy Method.
Annals of Operations Research,vol.
134, pages 19?67.Michel Galley and Christopher D. Manning.
2010.
Ac-curate Non-Hierarchical Phrase-Based Translation.
InNAACL.Kevin Gimpel and Noah A. Smith.
2011.
GenerativeModels of Monolingual and Bilingual Gappy Patterns.In WMT.Jin Guo.
1997.
Critical Tokenization and its Properties.Computational Linguistics, vol.
23(4), pages 569?596.Howard Johnson, Joel Martin, George Foster, and RolandKuhn.
2007.
Improving translation quality by discard-ing most of the phrase-table.
In EMNLP-CoNLL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ond?rej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In ACL,demonstration session.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In HLT-NAACL.Spyros Martzoukos, Christophe Costa Flor?encio, andChristof Monz.
2013.
Investigating Connectivity andConsistency Criteria for Phrase Pair Extraction in Sta-tistical Machine Translation.
In Meeting on Mathe-matics of Language.Bernard Monjardet.
1981.
Metrics on partially orderedsets ?
a survey.
Discrete Mathematics, vol.
35, pages173?184.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In ACL.Franz J. Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, vol.
29 (1), pages 19?51.Franz J. Och, Christoph Tillmann, and Hermann Ney.1999.
Improved Alignment Models for Statistical Ma-chine Translation.
In EMNLP-VLC.Chris Orum and Cliff A. Joslyn.
2009.
Valuations andMetrics on Partially Ordered Sets.
Computing Re-search Repository - CORR, vol.
abs/0903.2.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalua-tion of machine translation.
In ACL.Michael Paul, Andrew Finch, and Eiichiro Sumita.
2010.Integration of Multiple Bilingually-Learned Segmen-tation Schemes into Statistical Machine Translation.In WMT and MetricsMATR.Klaus Ries, Finn Dag Bu, and Alex Waibel.
1996.
Classphrase models for language modeling.
In ICSLP.Reuven Y. Rubinstein.
1997.
Optimization of ComputerSimulation Models with Rare Events.
European Jour-nal of Operations Research, vol.
99, pages 89?112.Reuven Y. Rubinstein and Dirk P. Kroese.
2004.
TheCross-Entropy Method: A Unified Approach to Com-binatorial Optimization, Monte-Carlo Simulation andMachine Learning.
Springer-Verlag, New York.Germ?an Sanchis-Trilles, Daniel Ortiz-Mart?
?nez, Jes?usGonz?alez-Rubio, Jorge Gonz?alez, and FranciscoCasacuberta.
2011.
Bilingual segmentation forphrasetable pruning in Statistical Machine Translation.In EAMT.Richard P. Stanley.
1997.
Enumerative Combinatorics,Volume 1.
Cambridge University Press.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In ICSLP.Leslie G. Valiant.
1979.
The complexity of enumerationand reliability problems.
SIAM Journal on Comput-ing, vol.
8, pages 410?421.Dominic J.
A. Welsh.
1997.
Approximate counting.Surveys in Combinatorics, London Math.
Soc.
LectureNotes Ser., 241, pages 287?324.Richard Zens, Daisy Stanton, and Peng Xu.
2012.
ASystematic Comparison of Phrase Table Pruning Tech-niques.
In EMNLP.38
