Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 159?165,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAutomatic Coupling of Answer Extraction and Information RetrievalXuchen Yao and Benjamin Van DurmeJohns Hopkins UniversityBaltimore, MD, USAPeter ClarkVulcan Inc.Seattle, WA, USAAbstractInformation Retrieval (IR) and AnswerExtraction are often designed as isolatedor loosely connected components in Ques-tion Answering (QA), with repeated over-engineering on IR, and not necessarily per-formance gain for QA.
We propose totightly integrate them by coupling auto-matically learned features for answer ex-traction to a shallow-structured IR model.Our method is very quick to implement,and significantly improves IR for QA(measured in Mean Average Precision andMean Reciprocal Rank) by 10%-20%against an uncoupled retrieval baselinein both document and passage retrieval,which further leads to a downstream 20%improvement in QA F1.1 IntroductionThe overall performance of a Question Answer-ing system is bounded by its Information Re-trieval (IR) front end, resulting in research specif-ically on Information Retrieval for Question An-swering (IR4QA) (Greenwood, 2008; Sakai et al,2010).
Common approaches such as query expan-sion, structured retrieval, and translation modelsshow patterns of complicated engineering on theIR side, or isolate the upstream passage retrievalfrom downstream answer extraction.
We arguethat: 1. an IR front end should deliver exactlywhat a QA1 back end needs; 2. many intuitionsemployed by QA should be and can be re-used inIR, rather than re-invented.
We propose a coupledretrieval method with prior knowledge of its down-stream QA component, that feeds QA with exactlythe information needed.1After this point in the paper we use the term QA in anarrow sense: QA without the IR component, i.e., answerextraction.As a motivating example, using the ques-tion When was Alaska purchased fromthe TREC 2002 QA track as the query to the In-dri search engine, the top sentence retrieved fromthe accompanying AQUAINT corpus is:Eventually Alaska Airlines willallow all travelers who havepurchased electronic ticketsthrough any means.While this relates Alaska and purchased, itis not a useful passage for the given question.2 Itis apparent that the question asks for a date.
Priorwork proposed predictive annotation (Prager et al,2000; Prager et al, 2006): text is first annotated ina predictive manner (of what types of questions itmight answer) with 20 answer types and then in-dexed.
A question analysis component (consistingof 400 question templates) maps the desired an-swer type to one of the 20 existing answer types.Retrieval is then performed with both the questionand predicated answer types in the query.However, predictive annotation has the limita-tion of being labor intensive and assuming the un-derlying NLP pipeline to be accurate.
We avoidthese limitations by directly asking the down-stream QA system for the information about whichentities answer which questions, via two steps:1. reusing the question analysis components fromQA; 2. forming a query based on the most relevantanswer features given a question from the learnedQA model.
There is no query-time overhead andno manual template creation.
Moreover, this ap-proach is more robust against, e.g., entity recog-nition errors, because answer typing knowledge islearned from how the data was actually labeled,not from how the data was assumed to be labeled(e.g., manual templates usually assume perfect la-beling of named entities, but often it is not the case2Based on a non-optimized IR configuration, none of thetop 1000 returned passages contained the correct answer:1867.159in practice).We use our statistically-trained QA system (Yaoet al, 2013) that recognizes the association be-tween question type and expected answer typesthrough various features.
The QA system employsa linear chain Conditional Random Field (CRF)(Lafferty et al, 2001) and tags each token as eitheran answer (ANS) or not (O).
This will be our off-the-shelf QA system, which recognizes the associ-ation between question type and expected answertypes through various features based on e.g., part-of-speech tagging (POS) and named entity recog-nition (NER).With weights optimized by CRF training (Ta-ble 1), we can learn how answer features are cor-related with question features.
These features,whose weights are optimized by the CRF train-ing, directly reflect what the most important an-swer types associated with each question type are.For instance, line 2 in Table 1 says that if there is awhen question, and the current token?s NER labelis DATE, then it is likely that this token is taggedas ANS.
IR can easily make use of this knowledge:for a when question, IR retrieves sentences withtokens labeled as DATE by NER, or POS tagged asCD.
The only extra processing is to pre-tag andindex the text with POS and NER labels.
The ana-lyzing power of discriminative answer features forIR comes for free from a trained QA system.
Un-like predictive annotation, statistical evidence de-termines the best answer features given the ques-tion, with no manual pattern or templates needed.To compare again predictive annotation withour approach: predictive annotation works in aforward mode, downstream QA is tailored for up-stream IR, i.e., QA works on whatever IR re-trieves.
Our method works in reverse (backward):downstream QA dictates upstream IR, i.e., IR re-trieves what QA wants.
Moreover, our approachextends easily beyond fixed answer types such asnamed entities: we are already using POS tags as ademonstration.
We can potentially use any helpfulanswer features in retrieval.
For instance, if theQA system learns that in order to is highlycorrelated with why question through lexicalizedfeatures, or some certain dependency relations arehelpful in answering questions with specific struc-tures, then it is natural and easy for the IR compo-nent to incorporate them.There is also a distinction between our methodand the technique of learning to rank applied infeature label weightqword=when|POS0=CD ANS 0.86qword=when|NER0=DATE ANS 0.79qword=when|POS0=CD O -0.74Table 1: Learned weights for sampled features with respectto the label of current token (indexed by [0]) in a CRF.
Thelarger the weight, the more ?important?
is this feature to helptag the current token with the corresponding label.
For in-stance, line 1 says when answering a when question, andthe POS of current token is CD (cardinal number), it is likely(large weight) that the token is tagged as ANS.QA (Bilotti et al, 2010; Agarwal et al, 2012).
Ourmethod is a QA-driven approach that provides su-pervision for IR from a learned QA model, whilelearning to rank is essentially an IR-driven ap-proach: the supervision for IR comes from a la-beled ranking list of retrieval results.Overall, we make the following contributions:?
Our proposed method tightly integrates QAwith IR and the reuse of analysis from QA doesnot put extra overhead on the IR queries.
ThisQA-driven approach provides a holistic solutionto the task of IR4QA.?
We learn statistical evidence about what theform of answers to different questions look like,rather than using manually authored templates.This provides great flexibility in using answerfeatures in IR queries.We give a full spectrum evaluation of all threestages of IR+QA: document retrieval, passage re-trieval and answer extraction, to examine thor-oughly the effectiveness of the method.3 All ofour code and datasets are publicly available.42 BackgroundBesides Predictive Annotation, our work is closestto structured retrieval, which covers techniques ofdependency path mapping (Lin and Pantel, 2001;Cui et al, 2005; Kaisser, 2012), graph matchingwith Semantic Role Labeling (Shen and Lapata,2007) and answer type checking (Pinchak et al,2009), etc.
Specifically, Bilotti et al (2007) pro-posed indexing text with their semantic roles andnamed entities.
Queries then include constraintsof semantic roles and named entities for the pred-icate and its arguments in the question.
Improve-ments in recall of answer-bearing sentences wereshown over the bag-of-words baseline.
Zhao and3Rarely are all three aspects presented in concert (see ?2).4http://code.google.com/p/jacana/160Callan (2008) extended this work with approx-imate matching and smoothing.
Most researchuses parsing to assign deep structures.
Com-pared to shallow (POS, NER) structured retrieval,deep structures need more processing power andsmoothing, but might also be more precise.
5Most of the above (except Kaisser (2012)) onlyreported on IR or QA, but not both, assuming thatimprovement in one naturally improves the other.Bilotti and Nyberg (2008) challenged this assump-tion and called for tighter coupling between IR andQA.
This paper is aimed at that challenge.3 MethodTable 1 already shows some examples of featuresassociating question types with answer types.
Westore the features and their learned weights fromthe trained model for IR usage.We let the trained QA system guide the queryformulation when performing coupled retrievalwith Indri (Strohman et al, 2005), given a corpusalready annotated with POS tags and NER labels.Then retrieval runs in four steps (Figure 1):1.
Question Analysis.
The question analysis com-ponent from QA is reused here.
In this imple-mentation, the only information we have cho-sen to use from the question is the questionword (e.g., how, who) and the lexical answertypes (LAT) in case of what/which questions.2.
Answer Feature Selection.
Given the questionword, we select the 5 highest weighted features(e.g., POS[0]=CD for a when question).3.
Query Formulation.
The original question iscombined with the top features as the query.4.
Coupled Retrieval.
Indri retrieves a ranked listof documents or passages.As motivated in the introduction, this frameworkis aimed at providing the following benefits:Reuse of QA components on the IR side.
IRreuses both code for question analysis and topweighted features from QA.Statistical selection of answer features.
For in-stance, the NER tagger we used divides locationinto two categories: GPE (geo locations) and LOC5Ogilvie (2010) showed in chapter 4.3 that keyword andnamed entities based retrieval actually outperformed SRL-based structured retrieval in MAP for the answer-bearing sen-tence retrieval task in their setting.
In this paper we do notintend to re-invent another parse-based structure matching al-gorithm, but only use shallow structures to show the idea ofcoupling QA with IR; in the future this might be extended toincorporate ?deeper?
structure.
(non-GPE ).
Both of them are learned to be impor-tant to where questions.Error tolerance along the NLP pipeline.
IRand QA share the same processing pipeline.
Sys-tematic errors made by the processing tools aretolerated, in the sense that if the same pre-processing error is made on both the questionand sentence, an answer may still be found.Take the previous where question, besidesNER[0]=GPE and NER[0]=LOC, we also foundoddly NER[0]=PERSON an important feature, dueto that the NER tool sometimes mistakes PERSONfor LOC.
For instance, the volcano name MaunaLoa is labeled as a PERSON instead of a LOC.
Butsince the importance of this feature is recognizedby downstream QA, the upstream IR is still moti-vated to retrieve it.Queries were lightly optimized using the fol-lowing strategies:Query Weighting In practice query words areweighted:#weight(1.0 When 1.0 was 1.0 Alaska 1.0 purchased?
#max(#any:CD #any:DATE))with a weight ?
for the answer types tuned viacross-validation.Since NER and POS tags are not lexicalizedthey accumulate many more counts (i.e.
term fre-quency) than individual words, thus we in gen-eral downweight by setting ?
< 1.0, giving theexpected answer types ?enough say?
but not ?toomuch say?
:NER Types First We found NER labels better in-dicators of expected answer types than POS tags.The reasons are two-fold: 1.
In general POS tagsare too coarse-grained in answer types than NERlabels.
E.g., NNP can answer who and wherequestions, but is not as precise as PERSON andGPE.
2.
POS tags accumulate even more countsthan NER labels, thus they need separate down-weighting.
Learning the interplay of these weightsin a joint IR/QA model, is an interesting path forfuture work.
If the top-weighted features are basedon NER, then we do not include POS tags for thatquestion.
Otherwise POS tags are useful, for in-stance, in answering how questions.Unigram QA Model The QA system uses up totrigram features (Table 1 shows examples of uni-gram and bigram features).
Thus it is able to learn,for instance, that a POS sequence of IN CD NNSis likely an answer to a when question (such as:in 5 years).
This requires that the IR queries161When was Alaska purchased?qword=whenqword=when|POS[0]=CD ?
ANS: 0.86qword=when|NER[0]=DATE ?
ANS: 0.79...#combine(Alaska purchased #max(#any:CD  #any:DATE))1.
Simple question analysis(reuse from QA)2.
Get top weighted features w.r.t qword(from trained QA model)3.
Query formulation4.
Coupled retrievalOn <DATE>March 30, <CD> 1867 </CD> </DATE>, U.S. ... reached agreement ... to purchase ... Alaska ...The islands were sold to the United States in <CD>1867</CD> with the purchase of Alaska.
?...Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets ...12...50Figure 1: Coupled retrieval with queries directly con-structed from highest weighted features of downstream QA.The retrieved and ranked list of sentences is POS and NERtagged, but only query-relevant tags are shown due to spacelimit.
A bag-of-words retrieval approach would have the sen-tence shown above at rank 50 at its top position instead.look for a consecutive IN CD NNS sequence.
Wedrop this strict constraint (which may need furthersmoothing) and only use unigram features, not bysimply extracting ?good?
unigram features fromthe trained model, but by re-training the modelwith only unigram features.
In answer extraction,we still use up to trigram features.
64 ExperimentsWe want to measure and compare the performanceof the following retrieval techniques:1. uncoupled retrieval with an off-the-shelf IR en-gine by using the question as query (baseline),2.
QA-driven coupled retrieval (proposed), and3.
answer-bearing retrieval by using both thequestion and known answer as query, only eval-uated for answer extraction (upper bound),at the three stages of question answering:1.
Document retrieval (for relevant docs from cor-pus), measured by Mean Average Precision(MAP) and Mean Reciprocal Rank (MRR).2.
Passage retrieval (finding relevant sentencesfrom the document), also by MAP and MRR.3.
Answer extraction, measured by F1.6This is because the weights of unigram to trigram fea-tures in a loglinear CRF model is a balanced consequence formaximization.
A unigram feature might end up with lowerweight because another trigram containing this unigram getsa higher weight.
Then we would have missed this featureif we only used top unigram features.
Thus we re-train themodel with only unigram features to make sure weights are?assigned properly?
among only unigram features.set questions sentences#all #pos.
#all #pos.TRAIN 2205 1756 (80%) 22043 7637 (35%)TESTgold 99 88 (89%) 990 368 (37%)Table 2: Statistics for AMT-collected data (total cost wasaround $800 for paying three Turkers per sentence).
Positivequestions are those with an answer found.
Positive sentencesare those bearing an answer.All coupled and uncoupled queries are performedwith Indri v5.3 (Strohman et al, 2005).4.1 DataTest Set for IR and QA The MIT109 test col-lection by Lin and Katz (2006) contains 109questions from TREC 2002 and provides a near-exhaustive judgment of relevant documents foreach question.
We removed 10 questions that donot have an answer by matching the TREC answerpatterns.
Then we call this test set MIT99.Training Set for QA We used Amazon Mechani-cal Turk to collect training data for the QA systemby issuing answer-bearing queries for TREC1999-2003 questions.
For the top 10 retrieved sen-tences for each question, three Turkers judgedwhether each sentence contained the answer.
Theinter-coder agreement rate was 0.81 (Krippen-dorff, 2004; Artstein and Poesio, 2008).The 99 questions of MIT99 were extracted fromthe Turk collection as our TESTgold with the re-maining as TRAIN, with statistics shown in Table2.
Note that only 88 questions out of MIT99 havean answer from the top 10 query results.Finally both the training and test data weresentence-segmented and word-tokenized byNLTK (Bird and Loper, 2004), dependency-parsed by the Stanford Parser (Klein andManning, 2003), and NER-tagged by the IllinoisNamed Entity Tagger (Ratinov and Roth, 2009)with an 18-label type set.Corpus Preprocessing for IR The AQUAINT(LDC2002T31) corpus, on which the MIT99questions are based, was processed in exactly thesame manner as was the QA training set.
Butonly sentence boundaries, POS tags and NER la-bels were kept as the annotation of the corpus.4.2 Document and Passage RetrievalWe issued uncoupled queries consisting of ques-tion words, and QA-driven coupled queries con-sisting of both the question and expected answertypes, then retrieved the top 1000 documents, and162type coupled uncoupledMAP MRR MAP MRRdocument 0.2524 0.4835 0.2110 0.4298sentence 0.1375 0.2987 0.1200 0.2544Table 3: Coupled vs. uncoupled document/sentence re-trieval in MAP and MRR on MIT99.
Significance level(Smucker et al, 2007) for both MAP: p < 0.001 and forboth MRR: p < 0.05.finally computed MAP and MRR against the gold-standard MIT99 per-document judgment.To find the best weighting ?
for coupled re-trieval, we used 5-fold cross-validation and final-ized at ?
= 0.1.
Table 3 shows the results.Coupled retrieval outperforms (20% by MAP withp < 0.001 and 12% by MRR with p < 0.01) un-coupled retrieval significantly according to pairedrandomization test (Smucker et al, 2007).For passage retrieval, we extracted relevant sin-gle sentences.
Recall that MIT99 only containsdocument-level judgment.
To generate a test setfor sentence retrieval, we matched each sentencefrom relevant documents provided by MIT99 foreach question against the TREC answer patterns.We found no significant difference between re-trieving sentences from the documents returnedby document retrieval or directly from the corpus.Numbers of the latter are shown in Table 3.
Still,coupled retrieval is significantly better by about10% in MAP and 17% in MRR.4.3 Answer ExtractionLastly we sent the sentences to the downstreamQA engine (trained on TRAIN) and computed F1per K for the top K retrieved sentences, 7 shownin Figure 2.
The best F1 with coupled sentence re-trieval is 0.231, 20% better than F1 of 0.192 withuncoupled retrieval, both at K = 1.The two descending lines at the bottom reflectthe fact that the majority-voting mechanism fromthe QA system was too simple: F1 drops as K in-creases.
Thus we also computed F1?s assumingperfect voting: a voting oracle that always selectsthe correct answer as long as the QA system pro-duces one, thus the two ascending lines in the cen-ter of Figure 2.
Still, F1 with coupled retrieval isalways better: reiterating the fact that coupled re-trieval covers more answer-bearing sentences.7Lin (2007), Zhang et al (2007), and Kaisser (2012) alsoevaluated on MIT109.
However their QA engines used web-based search engines, thus leading to results that are neitherreproducible nor directly comparable with ours.Finally, to find the upper bound for QA, wedrew the two upper lines, testing on TESTgold de-scribed in Table 2.
The test sentences were ob-tained with answer-bearing queries.
This is as-suming almost perfect IR.
The gap between thetop two and other lines signals more room for im-provements for IR in terms of better coverage andbetter rank for answer-bearing sentences.1 2 3 5 10 15 20 50 100 200 500 1000Top K Sentences Retrieved0.00.10.20.30.40.50.60.70.8F1Coupled (0.231)Uncoupled (0.192)Gold Oracle (0.755)Gold (0.596)Coupled Oracle (0.609)Uncoupled Oracle (0.569)Figure 2: F1 values for answer extraction on MIT99.
BestF1?s for each method are parenthesized in the legend.
?Or-acle?
methods assumed perfect voting of answer candidates(a question is answered correctly if the system ever producedone correct answer for it).
?Gold?
was tested on TESTgold.5 ConclusionWe described a method to perform coupled in-formation retrieval with a prior knowledge of thedownstream QA system.
Specifically, we coupledIR queries with automatically learned answer fea-tures from QA and observed significant improve-ments in document/passage retrieval and boostedF1 in answer extraction.
This method has the mer-its of not requiring hand-built question and answertemplates and being flexible in incorporating vari-ous answer features automatically learned and op-timized from the downstream QA system.AcknowledgementWe thank Vulcan Inc. for funding this work.
Wealso thank Paul Ogilvie, James Mayfield, Paul Mc-Namee, Jason Eisner and the three anonymous re-viewers for insightful comments.163ReferencesArvind Agarwal, Hema Raghavan, Karthik Subbian,Prem Melville, Richard D. Lawrence, David C.Gondek, and James Fan.
2012.
Learning to rankfor robust question answering.
In Proceedings ofthe 21st ACM international conference on Informa-tion and knowledge management, CIKM ?12, pages833?842, New York, NY, USA.
ACM.Ron Artstein and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Compu-tational Linguistics, 34(4):555?596.M.W.
Bilotti and E. Nyberg.
2008.
Improving textretrieval precision and answer accuracy in questionanswering systems.
In Coling 2008: Proceedingsof the 2nd workshop on Information Retrieval forQuestion Answering, pages 1?8.M.W.
Bilotti, P. Ogilvie, J. Callan, and E. Nyberg.2007.
Structured retrieval for question answer-ing.
In Proceedings of the 30th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 351?358.
ACM.M.W.
Bilotti, J. Elsas, J. Carbonell, and E. Nyberg.2010.
Rank learning for factoid question answer-ing with linguistic and semantic constraints.
In Pro-ceedings of the 19th ACM international conferenceon Information and knowledge management, pages459?468.
ACM.Steven Bird and Edward Loper.
2004.
Nltk: The nat-ural language toolkit.
In The Companion Volume tothe Proceedings of 42st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 214?217, Barcelona, Spain, July.Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, andTat-Seng Chua.
2005.
Question answering passageretrieval using dependency relations.
In Proceed-ings of the 28th annual international ACM SIGIRconference on Research and development in infor-mation retrieval, SIGIR ?05, pages 400?407, NewYork, NY, USA.
ACM.Mark A. Greenwood, editor.
2008.
Coling 2008: Pro-ceedings of the 2nd workshop on Information Re-trieval for Question Answering.
Coling 2008 Orga-nizing Committee, Manchester, UK, August.Michael Kaisser.
2012.
Answer Sentence Retrieval byMatching Dependency Paths acquired from Ques-tion/Answer Sentence Pairs.
In EACL, pages 88?98.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In In Proc.
the 41st An-nual Meeting of the Association for ComputationalLinguistics.Klaus H. Krippendorff.
2004.
Content Analysis: AnIntroduction to Its Methodology.
Sage Publications,Inc, 2nd edition.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML?01, pages 282?289, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.J.
Lin and B. Katz.
2006.
Building a reusable testcollection for question answering.
Journal of theAmerican Society for Information Science and Tech-nology, 57(7):851?861.D.
Lin and P. Pantel.
2001.
Discovery of inferencerules for question-answering.
Natural LanguageEngineering, 7(4):343?360.Jimmy Lin.
2007.
An exploration of the principles un-derlying redundancy-based factoid question answer-ing.
ACM Trans.
Inf.
Syst., 25(2), April.P.
Ogilvie.
2010.
Retrieval using Document Struc-ture and Annotations.
Ph.D. thesis, Carnegie MellonUniversity.Christopher Pinchak, Davood Rafiei, and Dekang Lin.2009.
Answer typing for information retrieval.
InProceedings of the 18th ACM conference on In-formation and knowledge management, CIKM ?09,pages 1955?1958, New York, NY, USA.
ACM.John Prager, Eric Brown, Anni Coden, and DragomirRadev.
2000.
Question-answering by predictive an-notation.
In Proceedings of the 23rd annual inter-national ACM SIGIR conference on Research anddevelopment in information retrieval, SIGIR ?00,pages 184?191, New York, NY, USA.
ACM.J.
Prager, J. Chu-Carroll, E. Brown, and K. Czuba.2006.
Question answering by predictive annota-tion.
Advances in Open Domain Question Answer-ing, pages 307?347.L.
Ratinov and D. Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL, 6.Tetsuya Sakai, Hideki Shima, Noriko Kando, Rui-hua Song, Chuan-Jie Lin, Teruko Mitamura, MihoSugimito, and Cheng-Wei Lee.
2010.
Overviewof the ntcir-7 aclia ir4qa task.
In Proceedings ofNTCIR-8 Workshop Meeting, Tokyo, Japan.D.
Shen and M. Lapata.
2007.
Using semantic rolesto improve question answering.
In Proceedings ofEMNLP-CoNLL, pages 12?21.M.D.
Smucker, J. Allan, and B. Carterette.
2007.
Acomparison of statistical significance tests for in-formation retrieval evaluation.
In Proceedings ofthe sixteenth ACM conference on Conference on in-formation and knowledge management, pages 623?632.
ACM.164T.
Strohman, D. Metzler, H. Turtle, and W.B.
Croft.2005.
Indri: A language model-based search enginefor complex queries.
In Proceedings of the Interna-tional Conference on Intelligent Analysis, volume 2,pages 2?6.
Citeseer.Xuchen Yao, Benjamin Van Durme, Peter Clark, andChris Callison-Burch.
2013.
Answer Extraction asSequence Tagging with Tree Edit Distance.
In Pro-ceedings of NAACL 2013.Xian Zhang, Yu Hao, Xiaoyan Zhu, Ming Li, andDavid R. Cheriton.
2007.
Information distancefrom a question to an answer.
In Proceedings ofthe 13th ACM SIGKDD international conference onKnowledge discovery and data mining, KDD ?07,pages 874?883, New York, NY, USA.
ACM.L.
Zhao and J. Callan.
2008.
A generative retrievalmodel for structured documents.
In Proceedings ofthe 17th ACM conference on Information and knowl-edge management, pages 1163?1172.
ACM.165
