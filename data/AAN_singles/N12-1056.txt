2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 513?517,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsSpace Efficiencies in Discourse Modeling via Conditional Random SamplingBrian KjerstenCLSPJohns Hopkins UniversityBenjamin Van DurmeHLTCOEJohns Hopkins UniversityAbstractRecent exploratory efforts in discourse-levellanguage modeling have relied heavily on cal-culating Pointwise Mutual Information (PMI),which involves significant computation whendone over large collections.
Prior work hasrequired aggressive pruning or independenceassumptions to compute scores on large col-lections.
We show the method of Condi-tional Random Sampling, thus far an underuti-lized technique, to be a space-efficient meansof representing the sufficient statistics in dis-course that underly recent PMI-based work.This is demonstrated in the context of induc-ing Shankian script-like structures over newsarticles.1 IntroductionIt has become common to model the distributionalaffinity between some word or phrase pair, (wi, wj),as a function of co-occurance within some con-text boundary.
Church and Hanks (1990) suggestedpointwise mutual information: PMI(wi, wj) =log Pr(wi,wj)Pr(wi) Pr(wj) , showing linguistically appealingresults using contexts defined by fixed width n-gramwindows, and syntactic dependencies derived fromautomatically parsed corpora.
Later work such asby Lin (1999) continued this tradition.
Here we con-sider document, or discourse-level contexts, such asexplored by Rosenfeld (1994) or Church (2000), andmore recently by those such as Chambers and Juraf-sky (2008) or Van Durme and Lall (2009b).In the spirit of recent work in randomized algo-rithms for large-scale HLT (such as by Ravichandranet al (2005), Talbot and Osborne (2007), Goyal etal.
(2010), Talbot and Brants (2008),Van Durme andLall (2009a), Levenberg and Osborne (2009), Goyalet al (2010), Petrovic et al (2010), Van Durme andLall (2010), or Goyal and Daume?
(2011)), we pro-pose the method of Conditional Random Sampling(CRS) by Li and Church (2007) as an efficient wayto store approximations of the statistics used to cal-culate PMI for applications in inducing rudimentaryscript-like structures.Efficiently storing such structures is an impor-tant step in integrating document-level statistics intodownstream tasks, such as characterizing complexscenarios (Chambers and Jurafsky, 2011), or storyunderstanding (Gordon et al, 2011).2 BackgroundConditional Random Sampling (CRS) Li andChurch (2007) proposed CRS to approximate thecontingency table between elements in a query, tobe used in distributional similarity measures suchas cosine similarity, correlation, and PMI.
Centralis the idea of the postings list, which is made upof the identifiers of each document that contains agiven word or phrase.
A set of such lists, one pertype in the underlying vocabulary, is known as aninverted index.
To reduce storage costs, a CRS trun-cates these lists, now called sketches, such that eachsketch is no larger than some length parameter k.Formally, assume an ordered list of documentidentifiers, ?
= (1, 2, ...), where each referenceddocument is a bag of words drawn from a vocabu-lary of size V .
Let Pi ?
?
be the postings list forsome element wi ?
V .
The function pi represents a513random permutation on the space of identifiers in ?.The sketch, Si, is defined as the first k elements ofthe permuted list: Si = mink(pi(Pi)).
1Let q be a two-element query, (wi, wj).
Giventhe postings lists for wi, wj , we can constructa four-cell contingency table containing the fre-quency of documents that contained only wi, onlywj , both together, or neither.
A CRS allows forapproximating this table in O(k) time by rely-ing on a sample of ?, specific to q: pi(?
)q =(1, 2, 3, ...,min(max(Si),max(Sj))).The PMI of q, given ?, can be estimated frompi(?
)q using the approximate word occurrence,Pr(wi) = |Si?pi(?)q|/|pi(?
)q|, and co-occurrence,Pr(wi ?
wj) = |Si ?
Sj ?
pi(?)q|/|pi(?
)q|.This scheme generalizes to longer queries oflength m, where storage costs remain O(V k), andquery time scales at O(mk).
Li and Church (2007)proved that CRS produces an unbiased estimate ofthe probabilities, and showed empirically that vari-ance is a function of k and m.Despite its simplicity and promise for large-scaledata mining in NLP, CRS has thus-far seen minimalapplication in the community.Trigger Language Models As here, Rosenfeld(1994)?s work on trigger language models was con-cerned with document level context.
He identifiedtrigger pairs: pairs of word sequences where thepresence of the first word sequence affects the prob-ability of the other, possibly at long distances.
Herecommended selecting a small list of trigger pairsbased on the highest average mutual information(often simply called mutual information), althoughintuitively PMI could also be used.
Computationalconstraints forced him to apply heavy pruning to thebigrams in his model.Scripts A script, proposed by Schank (1975), is aform of Minsky-style frame that captures common-sense knowledge regarding typical events.
For ex-ample, if a machine were to reason about eating at arestaurant, it should associate to this event: the ex-1For example, assume some word wi that appears in doc-uments d1, d4, d10 and d12.
The identifiers are then randomlypermuted via pi such that: d?3 = d1, d?2 = d4, d?7 = d10 andd?1 = d12.
Following permutation, the postings list for wi ismade up of identifiers that map to the same underlying docu-ments as before, but now in a different order.
If we let k = 3,then Si = (1, 2, 3), corresponding to documents: (d12, d4, d1).istence of a customer or patron that usually pays forthe meal that is ordered by the patron, then servedby the waiter, etc.Chambers and Jurafsky (2008) suggested induc-ing a similar structure called a narrative chain: fo-cus on the situational descriptions explicitly pertain-ing to a single protagonist, a series of referenceswithin a document that are automatically labeledas coreferent.
With a large corpus, one can thenfind those sets of verbs (as anchors of basic sit-uational descriptions) which tend to co-occur, andshare a protagonist, leading to an approximate sub-set of Schank?s original conception.2Underlying the co-occurrence framework ofChambers and Jurafsky was finding those verbs withhigh PMI.
Starting with some initial element, chainswere built greedily by adding the term, x, that max-imized the average of the pairwise PMI between xand every term already in the chain:Wn+1 = arg maxW1nn?j=1pmi(W,Wj)By relying on the average pairwise PMI, they aremaking independence assumptions that are not al-ways valid.
In order to consider more nuanced jointeffects between more than two terms, more efficientmethods would need to be considered.3 ExperimentsSetup Following Chambers and Jurafsky (2008),we extracted and lemmatized the verbs from theNew York Times section of the Gigaword Corpus us-ing the Stanford POS tagger (Toutanova et al, 2004)and the Morpha lemmatizer (Minnen et al, 2000).After filtering various POS tagger errors and settinga minimum document frequency (df) of 50, we wentfrom a vocabulary of 94,803 words to 8,051.3 Forvarious values of k we built sketches over 1,655,193documents, for each resulting word type.2Given a large collection of news articles, some on the topicof local crime, one might see a story such as: ?...
searched forMichaeli ... hei was arrested ... Mikei plead guilty ... convictedhimi ...?, helping to support an induced chain: (search, arrest,plead, acquit, convict, sentence).3Types containing punctuation other than hyphens and un-derscores were discarded as tagger-error.514Table 1: Top-n by approximate PMI, for varying k. Subscripts denote rank under true PMI, when less than 50.plead plead, admit plead, admit, convict1 sentence4 sentence4 sentence4 abuse?
sentence5 owe?
sentence22 commit?
defraud5 misbrand2 convict22 prosecute15 admitt11 prosecute33 indict10 indict10 defraud5 owe?
testify20 engage?
arrest84 prosecute33 arraign6 arraign6 investigate?
indict10 investigate28 testify55 abuse?
conspire11 manslaughter1 understand?
defraud7 prey?
acquit16 convict24 convict24 bilk8 defraud7 convict22 defraud?
indict4k = 100 1,000 10,000 1,000 10,000 1,000 10,000We use a generalized definition of PMI for threeor more items as the logarithm of the joint probabil-ity divided by the product of the marginals.Subjective Quality We first consider the lemma-tized version of the motivating example by Cham-bers and Jurafsky (2008): [plead, admit, convict],breaking it into 1-, 2-, and 3-element seeds.
Theyreported the top 6 elements that maximize averagepairwise PMI as: sentence, parole, fire, indict, fine,deny.
We see similar results in Table 1, while not-ing again the distinction in underlying statistics: wedid not restrict ourselves to cooccurrence based onshared coreferring arguments.These results show intuitive discourse-level rela-tionships with a sketch size as small as k = 100for the unary seed.
In addition, when examining thetrue PMI rank of each of these terms (reflected assubscripts), we see that highly ranked items in theapproximate lists come from the set of items highlyranked in the non-approximate version.4 A majorbenefit of the approach is that it allows for approxi-mate scoring of larger sets of elements jointly, with-out the traditionally assumed storage penalty.5Accuracy 1 We measured the trade-off betweenPMI approximation accuracy and sketch size.Triples of verb tokens were sampled at random fromthe narrative cloze test set of Chambers and Jurafsky(2008).
Seed terms were limited to verbs with df be-tween 1,000 and 100,000 to extract lists of the top-25 candidate verbs by joint, approximate PMI.
For4The word ?sentence?
is consistently higher ranked in theapproximate PMI list than it is in the true PMI list: results stemfrom a given shared permutation across the queries, and thusapproximation errors are more likely to be correlated.5For example, we report that PMI(plead, admit, convict)> PMI(plead, admit, owe), when k = 1, 000, as com-pared to: avg(PMI(plead, convict), PMI(admit, convict)) >avg(PMI(plead, owe), PMI(admit, owe)).a given rank r, we measured the overlap of the truetop-3 PMI and the approximate list, rank r or higher(see Figure 1(a)).
If query size is 2, k = 10, 000,the true top-3 true PMI items tend to rank well inthe approximate PMI list.
We observe that theserandomly assembled queries tax the sketch-basedapproximation, motivating the next experiment onnon-uniformly sampled queries.Accuracy 2 In a more realistic scenario, we mighthave more discretion in selecting terms of interest.Here we chose the first word of each seed uniformlyat random from each document, and selected subse-quent seed words to maximize the true PMI with theestablished words in the seed.
We constrained theseed terms to have df between 1,000 and 100,000.Then, for each seed of length 1, 2, and 3 words,we found the 25-best list of terms using approximatePMI, considering only terms that occur in more than50 documents.
Figure 1(b) shows the results of thisPMI approximation tradeoff.
With a sketch size of10,000, a rank of 5 is enough to contain two out ofthe top three items, and the number gradually con-tinues to grow as rank size increases.Memory Analysis Accuracy in a CRS is a func-tion of the aggressiveness in space savings: as k ap-proaches the true length of the posting list for wi,the resulting approximations are closer to truth, atthe cost of increased storage.
When k =?, CRS isthe same as using an inverted index: Fig.
2 shows thepercent memory required for our data, compared to astandard index, as the sketch size increases.
For ourdata, a full index involves storing 95 million docu-ment numbers.
For the k = 10, 000 results, we seethat 23% of a full index was needed.Figure 1(c) shows the quality of approximate bestPMI lists as memory usage is varied.
A 2-wordquery needs about 20% of the memory for 2.5 of the515RankMean.Observed0.00.51.01.52.02.53.0l l l l l ll ll l l l5 10 15 20 25k1000 10000ml 2 3 4(a)RankMean.Observed0.00.51.01.52.02.53.0l ll l l ll l l l l l5 10 15 20 25k1000 10000ml 2 3 4(b)Percent.MemoryMean.Observed0.00.51.01.52.02.53.0 llllllllll0 20 40 60 80 100ml 234(c)Figure 1: (a) Average number of true top-3 PMI items when seed terms have 1,000?
df?
100,000 and are chosen uni-formly at random from documents.
(b) Average number of true top-3 PMI items when seeds are moderate-frequencyhigh-PMI tuples.
(c) Average number of true top-3 PMI items in the top ten approximate PMI list, as a function ofmemory usage, when seeds are moderate-frequency high-PMI tuples.log10(k)Percent.Total204060801002 3 4 5 6Figure 2: % of inverted index stored, as function of k.top 3 true PMI items to appear in the top 10.
Over40% memory is needed for a 4-word query.
2.5 ofthe top 3 true PMI items appear in the top 50 whenthe memory is about 35%.
This suggests that CRSallows us to use a fraction of the memory of storinga full inverted index, but that memory requirementsgrow with query size.Discussion Storing exact PMIs of three or fourwords would be expensive to store in memory forany moderately sized vocabulary, because it wouldinvolve storing on the order of V m count statis-tics.
If we are approximating this with a CRS, westore sketches of length k or less for every wordin the vocabulary, which is O(kV ).
Table 1 andFig.
1(b) show that the two-word queries start toget good performance when k is near 10,000.
Thisrequires 22.7% of the memory of a complete in-verted index, or 21.5 million postings.
The threeand four word queries get good performance neark = 100, 000.
With this sketch size, 60.5 millionpostings are stored.4 ConclusionWe have proposed using Conditional Random Sam-pling for approximating PMI in the discourse under-standing community.
We have shown that the ap-proximate PMI rank list produces results that are in-tuitive and consistent with the exact PMI even withsignificant memory savings.
This enables us to ap-proximate PMI for tuples longer than pairs withoutundue independence assumptions.
One future av-enue is to explore the use of this structure in appli-cations such as machine translation, as potentiallyenabling greater use of long distance dependenciesthan in prior work, such as by Hasan et al (2008).5 AcknowledgementsWe acknowledge support from the National ScienceFoundation PIRE Grant No.
OISE-0530118.
Weacknowledge the Army Research Laboratory for itssupport to the first author under SCEP (the StudentCareer Experience Program).
Any opinions, find-ings, conclusions, or recommendations expressed inthis material are those of the authors and do not nec-essarily reflect the views of the supporting agencies.516ReferencesNathanael Chambers and Dan Jurafsky.
2008.
Unsuper-vised learning of narrative event chains.
In Proceed-ings of ACL.Nathanael Chambers and Dan Jurafsky.
2011.
Template-based information extraction without the templates.
InProceedings of ACL.Kenneth Church and Patrick Hanks.
1990.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 16(1):22?29.Kenneth W. Church.
2000.
Empirical estimates of adap-tation: The chance of two noriegas is closer to p/2 thanp2.
In Proceedings of COLING.Andrew Gordon, Cosmin Bejan, and Kenji Sagae.
2011.Commonsense causal reasoning using millions of per-sonal stories.
In Proceedings of AAAI.Amit Goyal and Hal Daume?.
2011.
Lossy conservativeupdate (lcu) sketch: Succinct approximate count stor-age.
In AAAI.Amit Goyal, Jagadeesh Jagarlamundi, Hal Daume?, andSuresh Venkatasubramanian.
2010.
Sketch techniquesfor scaling distributional similarity to the web.
In 6thWAC Workshop at NAACL-HLT.Sasa Hasan, Juri Ganitkevitch, Hermann Ney, andJ.
Andre?s-Ferrer.
2008.
Triplet lexicon models forstatistical machine translation.
In Proceedings ofEMNLP.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for smt.
In Pro-ceedings of EMNLP.Ping Li and Kenneth Church.
2007.
A sketch algo-rithm for estimating two-way and multi-way associ-ations.
Computational Linguistics, 33(2):305?354.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL.Guido Minnen, John Carroll, and Darren Pearce.
2000.Robust, applied morphological generation.
In Pro-ceedings of the 1st International Natural LanguageGeneration Conference.Sasa Petrovic, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applicationto twitter.
In Proceedings of NAACL.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized Algorithms and NLP: Using Lo-cality Sensitive Hash Functions for High Speed NounClustering.
In Proceedings of ACL.Ronald Rosenfeld.
1994.
Adaptive statistical languagemodeling: A maximum entropy approach.
Ph.D. the-sis, Carnegie Mellon University.Roger C. Schank.
1975.
Using knowledge to understand.In Theoretical Issues in Natural Language Processing.David Talbot and Thorsten Brants.
2008.
Randomizedlanguage models via perfect hash functions.
In Pro-ceeedings of ACL.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine translation.In Proceedings of ACL.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2004.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of NAACL.Benjamin Van Durme and Ashwin Lall.
2009a.
Proba-bilistic Counting with Randomized Storage.
In Pro-ceedings of IJCAI.Benjamin Van Durme and Ashwin Lall.
2009b.
Stream-ing pointwise mutual information.
In NIPS.Benjamin Van Durme and Ashwin Lall.
2010.
OnlineGeneration of Locality Sensitive Hash Signatures.
InProceedings of ACL.517
