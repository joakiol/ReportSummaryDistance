Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2144?2152,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNonparametric Bayesian Models for Spoken Language UnderstandingKei WakabayashiTsukuba University, 1-2 Kasuga,Tsukuba, Ibaraki 305-8550, Japankwakaba@slis.tsukuba.ac.jpJohane Takeuchi, Kotaro Funakoshiand Mikio NakanoHonda Research Institute Japan Co., Ltd.8-1 Honcho, Wako, Saitama 351-0188, Japan{johane.takeuchi,funakoshi,nakano}@jp.honda-ri.comAbstractIn this paper, we propose a new generative ap-proach for semantic slot filling task in spokenlanguage understanding using a nonparamet-ric Bayesian formalism.
Slot filling is typi-cally formulated as a sequential labeling prob-lem, which does not directly deal with the pos-terior distribution of possible slot values.
Wepresent a nonparametric Bayesian model in-volving the generation of arbitrary natural lan-guage phrases, which allows an explicit cal-culation of the distribution over an infinite setof slot values.
We demonstrate that this ap-proach significantly improves slot estimationaccuracy compared to the existing sequentiallabeling algorithm.1 IntroductionSpoken language understanding (SLU) refers to thechallenge of recognizing a speaker?s intent from anatural language utterance, which is typically de-fined as a slot filling task.
For example, in the ut-terance ?Remind me to call John at 9am tomorrow?,the specified information {?time?
: ?9am tomor-row?}
and {?subject?
: ?to call John?}
should beextracted.
The term slot refers to a variable such asthe time or subject that is expected to be filled witha value provided through the user?s utterance.The slot filling task is typically formulated as asequential labeling problem as shown in Figure 1.This labeling scheme naturally represents the recog-nition of arbitrary phrases that appear in the tran-scription of an utterance.
Formally speaking, whenwe assume a given set of slots {s1, ..., sM} and de-note the corresponding slot values by {vs1 , ..., vsM }where vsi ?
Vsi , the domain of each slot value Vsi isan infinite set of word sequences.
In this paper, weuse the term arbitrary slot filling task to refer to thisimplicit problem statement, which inherently under-lies the sequential labeling formulation.In contrast, a different line of work has exploredthe case where Vsi is provided as a finite set of possi-ble values that can be handled by a backend system(Henderson, 2015).
We refer to this type of task asa categorical slot filling task.
In this case, the slotfilling task is regarded as a classification problemthat explicitly considers a value-based prediction, asshown in Figure 2.
From this point of view, we cansay that a distribution of slot values is actually con-centrated in a small set of typical phrases, even inthe arbitrary slot filling task, because users basicallyknow what kind of function is offered by the system.To reflect this observation, in this paper we ex-plore the value-based formulation approach for arbi-trary slot filling tasks.
Unlike the sequential labelingformulation, which is basically position-based labelprediction, our method directly estimates the poste-rior distribution over an infinite set of possible val-ues for each slot Vsi .
The distribution is representedby using a Dirichlet process (Gershman and Blei,2012), which is a nonparametric Bayesian formal-ism that generates a categorical distribution for anyspace.
We demonstrate that this approach improvesestimation accuracy in the arbitrary slot filling taskcompared with conventional sequential labeling ap-proach.The rest of this paper is organized as follows.
InSection 2, we review the existing approaches forcategorical and arbitrary slot filling tasks and intro-2144!!
!!
!!
!!
"#$%&'!
!!
!!
"#()'(!
*#()'(!
!!+,-!
.//0+12!
3/)!
(!
)'4$(5)(1$!
+1!
$6'!
3'1!
7+8/1!
()'(!Figure 1: sequential labeling formulation for slot filling tasks.!"#!$%%&!'(!)%*!+!*,-.+/*+'.!!'!.0,!
),'!1!2%'!+*,+!P(restaurant | u) = 0.96P(pub | u) = 0.03P(None | u) = 0.01...P( fen_ ditton | u) = 0.98P(girton | u) = 0.005P(None | u) = 0.01...P(italian | u) = 0.005P(mexican | u) = 0.01P(None | u) = 0.85...!"#"!34.53,6!
34+*,+6!
34)%%16!Figure 2: Value-based formulation.
The posterior probabilitiesof values for each slot are explicitly computed.duce related work.
In Section 3, we present ournonparametric Bayesian formulation, the hierarchi-cal Dirichlet process slot model (HDPSM), whichdirectly models an infinite set of slot values.
On thebasis of the HDPSM, we develop a generative utter-ance model that allows us to compute the posteriorprobability of slot values in Section 4.
In Section 5,we introduce a two-stage slot filling algorithm thatconsists of a candidate generation step and a candi-date ranking step using the proposed model.
In Sec-tion 6, we show the experimental results for multipledatasets in different domains to demonstrate that theproposed algorithm performs better than the base-line sequential labeling method.
We conclude inSection 7 with a brief summary.2 Related WorkThe difference between the categorical and arbitraryslot filling approaches has not been explicitly dis-cussed in a comparative manner to date.
In this sec-tion, we review existing work for both approaches.For the categorical slot filling approach, variousalgorithms that directly model the distribution ofslot values have been proposed, including generativemodels (Williams, 2010), maximum entropy linearclassifiers (Metallinou et al, 2013), and neural net-works (Ren et al, 2014).
However, none of thesemodels are applicable for predicting a variable thatranges over an infinite set, and it is not straightfor-ward to extend them suitably.
In particular, a dis-criminative approach is not applicable for arbitraryslot filling tasks because it requires a fixed finite setof slot values to take statistics.The arbitrary slot filling approach is a naturalapplication of shallow semantic parsing (Gildea,2002), which is naturally formulated as a sequen-tial labeling problem.
Various sequential labelingalgorithms have been applied to this task, includingsupport vector machines, conditional random fields(CRF) (Lafferty et al, 2001; Hahn et al, 2011), anddeep neural networks (Mesnil et al, 2015; Xu andSarikaya, 2013).
Vukotic et al (2015) reported thatthe CRF is still the most accurate, rapid, and stablemethod among them.
Because the focus of this pa-per is arbitrary slot filling tasks, we use CRFs as ourbaseline method.In this paper, we apply nonparametric Bayesianmodels (Gershman and Blei, 2012) to representthe distribution over arbitrary phrases for each slot.The effectiveness of this phrase modeling approachhas been examined in various applications includingmorphological analysis (Goldwater et al, 2011) andinfinite vocabulary topic models (Zhai and Boyd-graber, 2013).
Our method can be regarded as anapplication of this idea, although it is not straight-forward to integrate it with the utterance generationprocess, as we explain later.Consequently, our proposed method is catego-rized as a generative approach.
There are many ad-vantages inherent in generative approaches that havebeen examined, including unsupervised SLU (Chenet al, 2015), automatic feature extraction (Tur etal., 2013), and integration with syntactic modeling(Lorenzo et al, 2013).
Another convenient prop-erty of generative models is that prior knowledgecan be integrated in an intuitive way (Raymond etal., 2006).
This often leads to better performancewith less training data compared with discriminativemodels trained completely from scratch (Komataniet al, 2010).3 Hierarchical Dirichlet Process SlotModelIn this section, we present a nonparametric Bayesianformulation that directly models the distributionover an infinite set of possible values for each slot.Let S = {s1, ..., sMS} be a given set of slots andMS be the number of slots.
We define each slot sias a random variable ranging over an infinite set of2145letter sequences V , which is represented as follows:V = {b1, ..., bL|b?
?
C,L ?
0}where C is a set of characters including the blankcharacter and any other character that potentially ap-pears in the transcription of an utterance.
Conse-quently, we regard the set of slots S as also being arandom variable that ranges over VMS .
The objec-tive of this section is to develop the formulation ofthe probabilistic distribution p(S).3.1 Dirichlet ProcessWe apply the Dirichlet process (DP) to model boththe distribution for an individual slot pi(si) and thejoint distribution p(S).
In this subsection, we reviewthe definition and key properties of DP with generalnotation for the target distribution G over the do-main X .
In the DP for the prior of pi(si) that isdescribed in Section 3.2, the domain X correspondsto a set of slot values V , e.g., ?fen ditton?, ?newchesterton?, and None.
In the DP for p(S) presentedin Section 3.3, X indicates a set of tuples of slot val-ues VMS , e.g., (?restaurant?, ?new chesterton?, ?fastfood?)
and (?restaurant?, ?fen ditton?, None).The DP is a probabilistic distribution over the dis-tribution G. DP is parameterized by ?0 and G0,where ?0 > 0 is a concentration parameter and G0is a base distribution over X .
If G is drawn fromDP (?0, G0) (i.e., G ?
DP (?0, G0)), then the fol-lowing Dirichlet distributed property holds for anypartition of X denoted by {A1, ..., AL}:(G(A1), ..., G(AL)) ?
Dir(?
(A1), ..., ?
(AL))where ?
(A) = ?0G0(A), which is known as thebase measure of DP.Ferguson (1973) proved an important propertyof a posterior distribution of repeated i.i.d.
sam-ples x1:N = {x1, ..., xN} drawn from G ?DP (?0, G0).
Consider a countably infinite set ofatoms ?
= {?1, ?2, ...} that are independentlydrawn from G0.
Let ci ?
N be the assignment of anatom for sample xi, which is generated by a sequen-tial draw with the following conditional probability:p(cN+1 = k|c1:N ) ={nkN+?0 k ?
K?0N+?0 k = K + 1where nk is the number of times that the kth atomappears in c1:N and K is the number of differentatoms in c1:N .
Given the assignment c1:N , the pre-dictive distribution of xN+1 ?
X is represented inthe following form:P (xN+1 = ?|c1:N ,?1:K , ?0, G0)=K?k=1nkN + ?0 ?
(?k, ?)
+?0N + ?0G0(?
)The base distribution possibly generates an iden-tical value for different atoms, such as (?1 = ?fenditton?, ?2 = ?new chesterton?, ?3 = ?fen ditton?
).The assignment ci is an auxiliary variable to indi-cate which of these atoms is assigned to the ith datapoint xi; when xi = ?fen ditton?, ci can be 1 or 3.The posterior distribution above depends on the fre-quency of atom nk, not on the frequency of ?
itself.The atoms ?
and the assignment c are latent vari-ables that should be determined at runtime.3.2 Individual Slot ModelFirst we formulate the distribution for an individ-ual slot as pi(si) ?
DP (?0i , G0i ) where G0i is abase distribution over the set of phrases V .
1 Wedefine G0i as a generative model that consists oftwo-step generation: generation of the phrase length0 ?
Li ?
Lmax using a categorical distribution andgeneration of a letter sequence s1:Li using an n-grammodel, as follows:Li ?
Categorical(?i)s?i ?
p(s?i|s??n+1:?
?1i ,?i)where ?i and ?i are parameters for the categoricaldistribution and the n-gram model for slot si, respec-tively.
This explicit modeling of the length helpsavoid the bias toward shorter phrases and leads toa better distribution, as reported by Zhai and Boyd-graber (2013).
We define G0i as a joint distributionof these models:G0i (s1:Lii ) = p(Li|?i)Li??=1p(s?i|s??n+1:?
?1i ,?i) (1)G0i potentially generates an empty phrase of Li = 0to express the case that the slot value vsi is not1Note that the subscript i for s, p, ?0 and G0 indicates theslot type such as ?type?, ?area?
and ?food?
in Figure 2.2146provided by an utterance.
Therefore, the distribu-tion pi(si) can naturally represent the probability ofNone , which is shown in Figure 2.We consider prior distributions of the parame-ters ?i and ?i to treat the n-gram characteristics ofeach slot in a fully Bayesian manner.
p(?)
is givenas a Lmax-dimensional symmetric Dirichlet distri-bution with parameter a.
We also define the |C|-dimensional symmetric Dirichlet distributions withparameter b for each n-gram context, since given thecontext p(s?i|s??n+1:?
?1i ,?i) is just a categorical dis-tribution that ranges over C. Consider we observeN phrases si for slot i.
Let nLi?
be the number ofphrases that have length ?
and n?ih be the numberof times that letter s?
= h appears after contexts??n+1:?
?1 = ?.
The predictive probability of aphrase is represented as follows:G0i (s1:Lii |si) =nLi?
+ bN + bCLi?
?=1n?is?i + a?c n?ic + a?Lmaxl=1 nLil3.3 Generative Model for a Set of Slot ValuesA naive definition of the joint distribution p(S) isa product of all slot probabilities ?MSi=1 pi(si) formaking an independence assumption.
However, theslot values are generally correlated with each other(Chen et al, 2015).
To obtain more accurate dis-tribution, we formulate p(S) using another DP thatrecognizes a frequent combination of slot values, asp(S) ?
DP (?1, G2) where G2 is a base distribu-tion over VMS .
We apply the naive independenceassumption to G2 as follows:G2(S) =MS?i=1pi(si)The whole generation process of S involves two-layered DPs that share atoms among them.
In thissense, this generative model is regarded as a hierar-chical Dirichlet process (Teh et al, 2005).Let G1i (si) = pi(si) and G3(S) = p(S) for con-sistent notations.
In summary, we define the hierar-chical Dirichlet process slot model (HDPSM) as agenerative model that has the following generationprocess.G1i ?
DP (?0i , G0i )G3 ?
DP (?1, G2)S ?
G33.4 Inference of HDPSMIn a slot filling task, observations of S1:T ={S1, ..., ST } are available as training data.
The in-ference of HDPSM refers to the estimation of ?, ?and the atom assignments for each DP.We formulate the HDPSM in a form of the Chi-nese restaurant franchise process, which is one of theexplicit representations of hierarchical DPs obtainedby marginalizing out the base distributions.
Teh etal.
(2005) presents a Gibbs sampler for this repre-sentation, which involves a repetitive resampling ofatoms and assignment.
In our method, we prefer toadopt a single pass inference, which samples the as-signment for each observation only once.
Our pre-liminary experiments showed that the quality of in-ference is not affected because S is observed unlikethe settings in Teh et al (2005).We denote the atoms and the atom assignment inthe first level DP DP (?1, G2) by ?1 and c11:N , re-spectively.
The posterior probability of atom assign-ment for a new observation SN+1 is represented asfollows:p(c1N+1 = k|c11:N ,?1, SN+1)?{n1k?
(?1k, SN+1) k ?
K?1G2(SN+1) k = K + 1where n1k is the number of times that the kth atomappears in c11:N and K is the number of differentatoms in c11:N .
?0i and c0i1:K denote the atoms and the assignmentin the second level DPs DP (?0i , G0i ).
The secondlevel DPs assign atoms to each first level atom ?1k,i.e.
the second level atom ?0it is generated only whena new atom is assigned for SN+1 at the first level.The posterior probability of atom assignment at thesecond level is:p(c0iK+1 = t|c0i1:K ,?0i , sN+1i)?{n0it?
(?0it, SN+1) t ?
Ti?0iG0(SN+1) t = Ti + 1where n0it is the number of times that the tth atomappears in c0i1:K and Ti is the number of differentatoms in c0i1:K .The single pass inference procedure is presentedin Algorithm 1.
Given the atoms ?
and the as-signments c, the predictive distribution of SN+1 =2147Algorithm 1 Single pass inference of HDPSMInput: A set of observations S1:N1: Set empty list to c1 and c0i2: for d = 1 to N do3: k ?
p(c1d = k|c11:d?1,?1, Sd)4: if k = K + 1 then5: for i = 1 to MS do6: ti ?
p(c0iK+1 = ti|c0i1:K ,?0i , sdi)7: if ti = Ti + 1 then8: Update nLi and n?i with sdi9: end if10: c0K+1 ?
ti and ?0iti ?
sdi11: end for12: end if13: c1d ?
k and ?1k ?
S14: end for{sN+11, ..., sN+1MS} is calculated as follows:P (SN+1|c,?)
=K?k=1n1kN + ?1 ?
(?1k, SN+1) (2)+ ?1N + ?1MS?i=1P (sN+1i|c0i ,?0i )P (sN+1i|c0i ,?0i ) =Ti?t=1n0itK + ?0i?
(?0it, sN+1i)+ ?0iK + ?0iG0i (sN+1i|?0i )4 Generative Model for an UtteranceWe present a generative utterance model to derive aslot estimation algorithm given utterance u.
Figure 3presents the basic concept of our generative model.In the proposed model, we formulate the distributionof slot values as well as the distribution of non-slotparts.
In Figure 3, the phrases ?hi we?re in um?
and?and we need a?
should be removed to identify theslot information.
We call these non-slot phrases asfunctional fillers because they more or less have afunction to convey information.
Identifying the setof non-slot phrases is equivalent to identifying theset of slot phrases.
Therefore, we define a generativemodel of functional fillers in the same way as the slotvalues.!"#$%&'%#"(#)*#+%(#,"-.(#/(,#$%#(%%,#/#'%01/)'/(1!!"#$2#'%01/)'/(1#%&$%2#+%(#,"-.(#'(()2#!"#$%3-%'/(4%2!5.(1%(1#67.12!*$+,--,-+2#!"#$%&'%#"(#)*#.,))/$2#/(,#$%#(%%,#/#$-),-+2#!"#$!8)(49.
(/7#8"77%'2!Figure 3: The proposed generative utterance model.
We at-tempt to find the best combination of the slot parts and the non-slot parts (i.e., functional filler parts) by using this model.4.1 Functional FillerWe assume an utterance u is a concatenation of slotvalues S and functional fillers F .
A functional filleris represented as a phrase that ranges over V .
Toderive the utterance model, we first formulate a gen-erative model for functional fillers.In our observation, the distribution of the func-tional filler depends on its position in an utterance.For example, utterances often begin with typicalphrases such as ?Hello I?m looking for ...?
or ?Hiplease find ...?, which can hardly ever appear at otherpositions.
To reflect this observation, we introducea filler slot to separately model the functional fillersbased on a position feature.
Specifically, we definethree filler slots: beginning filler f1, which precedesany slot value, ending filler f3, which appears at theend of an utterance, and middle filler f2, which isinserted between slot values.
We use the term con-tent slot to refer to S when we intend to explicitlydistinguish it from a filler slot.Let F = {f1, f2, f3} be a set of filler slots andMF = 3 be the number of filler slots.
Each slot fiis a random variable ranging over V and F is a ran-dom variable over VMF .
These notations for fillerslots indicate compatibility to a content slot, whichsuggests that we can formulate F using HDPSMs,as follows:H1i ?
DP (?0i , H0i )H3 ?
DP (?1, H2)F ?
H3where H0i is an n-gram-based distribution overV that is defined in an identical way to (1) andH2(F ) =?MFi=1 H1i (F ).2148G0 G1 SuH 0 H1 FMSMFG3H 3 DFigure 4: Graphical model of the utterance model.4.2 Utterance ModelFigure 4 presents the graphical model of our utter-ance model.
We assume that an utterance u is builtwith phrases provided by S and F .
Therefore, theconditional distribution p(u|S, F ) basically involvesa distribution over the permutation of these slot val-ues with two constraints: f1 is placed first and f3has to be placed last.
In our formulation, we simplyadopt a uniform distribution over all possible permu-tations.For training the utterance model, we assume that aset of annotated utterances is available.
Each train-ing instance consists of utterance u and annotatedslot values S. Given u and S, we assume that thefunctional fillers F can be uniquely identified.
Forthe example in Figure 3, we can identify the sub-sequence in u that corresponds to each content slotvalue of ?restaurant?
and ?fen ditton?.
This match-ing result leads to the identification of filler slot val-ues.
Consequently, a triple (u, S, F ) is regarded asan observation.
Because the HDPSMs of the contentslot and of the filler slot are conditionally indepen-dent given S and F , we can separately apply Algo-rithm 1 to train each HDPSM.For slot filling, we examine the posterior proba-bility of content slot values S given u, which can bereformed as follows:P (S|u) ?
?FP (u|S, F )P (S)P (F )In this equation, we can remove the summation of Fbecause filler slot values F are uniquely identifiedregarding u and S in our assumption.
Additionally,we approximately regard P (u|S, F ) as a constant ifu can be built with S and F .
By using these assump-tions, the posterior probability is reduced to the fol-lowing formula:P (S|u) ?
P (S)P (F ) (3)!
"# !
!
!
"#$%&$ '#$%&$ '#$%&$ !
!
!
!
"#()*&$%& !
!
!
!
"#$%&$ '#$%&$ !
!
!
!
"#()*&'(& !
!
!
!
"#$%&$ '#$%&$ !
!
"#+,,- '#+,,- !./ 0&1%& /2 34 +&2 -/((,2 $2- 0& 2&&- $ %&5($3%$2(!
"#$%&$ 346+&26-/((,2()*& %&5($3%$2(+7 ./60&1%&6/2+8 $2-60&62&&-6$$%&$%&$ +&26-/((,2()*& %&5($3%$2(+7 ./60&1%&6/2634+8 $2-60&62&&-6$'(&$%&$ +&26-/((,2+,,- 2&&-6$+7 ./60&1%&6/2634+8 $2-60&+9 %&5($3%$2(Figure 5: Candidate generation using sequential labeling algo-rithm.
The figure shows the case of N = 3.where F in this formula is fillers identified given uand S. Consequently, the proposed method attemptsto find the most likely combination of the slot val-ues and the non-slot phrases, since all words in anutterance have to belong to either of them.
By usingtrained HDPSM (i.e., the posterior given all trainingdata), P (S) and P (F ) can be computed by (2).5 Candidate GenerationFor estimating slot values given u, we adopt a can-didate generation approach (Williams, 2014) thatleverages another slot filling algorithm to enumer-ate likely candidates.
2 Specifically, we assume acandidate generation function g(u) that generates Ncandidates {S1, ..., SN} regarding u.
Our slot fill-ing algorithm computes the posterior probability by(3) for each candidate slot Sj and takes the candi-date that has the highest posterior probability.
In thisestimation process, our utterance model works as asecondary filter that covers the error of the primaryanalysis.Figure 5 provides an example of candidate gener-ation by using a sequential labeling algorithm withIOB tags.
The subsequences to which the O tag isassigned can be regarded as functional fillers.
Thevalues for each filler slot are identified depending onthe position of the subsequence, as the figure shows.6 ExperimentsWe evaluate the performance of the proposed gener-ative model with an experiment using the algorithm2The direct inference of the generative utterance model is atopic for near future work.
The MCMC method will circumventthe difficulty of searching the entire candidate space.2149name #utterances #slots max.
diversityDSTC 1,441 6 55Weather 1,442 3 191Table 1: Datasets in the experiment.
Max.
diversity refers tothe maximum number of value types that are taken by a slot.described in Section 5.
We adopt a conditional ran-dom field (CRF) as a candidate generation algorithmthat generates N -best estimation as candidates.
Forthe CRF, we apply commonly used features includ-ing unigram and bigram of the surface form and partof speech of the word.
We used CRF++3 as the CRFimplementation.6.1 DatasetThe performance of our method is evaluated usingtwo datasets from different languages, as summa-rized in Table 1.
The first dataset is provided bythe third Dialog State Tracking Challenge (Hender-son, 2015), hereafter referred to as the DSTC corpus.The DSTC corpus consists of dialogs in the touristinformation domain.
In our experiment, we use theuser?s first utterance in each dialog, which typicallydescribes the user?s query to the system.
Utteranceswithout any slot information are excluded.
We man-ually modified the annotated slot values into ?as-is form?
to allow a sequential labeling method toextract the ground-truth values.
This identificationprocess can be done in a semi-automatic manner thatinvolves no expert knowledge.
We apply the part ofspeech tagger in NLTK4 for the CRF application.The second dataset is a weather corpus consistingof user utterances in an in-house corpus of human-machine dialogues in the weather domain.
It con-tains 1,442 questions spoken in Japanese.
In thiscorpus, the number of value types for each slot ishigher than DSTC, which indicates a more challeng-ing task.
We applied the Japanese morphologicalanalyzer MeCab (Kudo et al, 2004) to segment theJapanese text into words before applying CRF.For both datasets, we examine the effectof the amount of available annotated utterancesby varying the number of training data in25, 50, 75, 100, 200, 400, 800, all.3https://taku910.github.io/crfpp/4http://www.nltk.org/#train CRF best HDP N = 5 HDP N = 30025 0.560 0.706* 0.684*50 0.709 0.791* 0.765*75 0.748 0.824* 0.817*100 0.791 0.845* 0.837*200 0.839 0.901* 0.876*400 0.904 0.938* 0.936*800 0.926 0.953* 0.947*1296 0.938 0.960* 0.951Table 2: Slot estimation accuracy for the DSTC corpus.
Theasterisk (*) indicates that the accuracy is statistically significantcompared against CRF best (p < 0.005).#train CRF best HDP N = 5 HDP N = 30025 0.327 0.452* 0.480*50 0.379 0.488* 0.499*75 0.397 0.504* 0.522*100 0.418 0.501* 0.512*200 0.493 0.526* 0.531*400 0.512 0.551* 0.549*800 0.533 0.555* 0.554*1297 0.546 0.560* 0.554Table 3: Slot estimation accuracy for the Japanese weather cor-pus.
An asterisk (*) indicates statistical significance againstCRF best (p < 0.01).6.2 Evaluation MetricsThe methods are compared in terms of slot estima-tion accuracy.
Let nc be the number of utterancesfor which the estimated slot S and the ground-truthslot S?
are perfectly matched, and let ne be the num-ber of the utterances including an estimation error.The slot estimation accuracy is simply calculated asncnc+ne .
All evaluation scores are calculated as theaverage of 10-fold cross validation.
We also conducta binomial test to examine the statistical significanceof the improvement in the proposed algorithm com-pared to the CRF baseline.6.3 ResultsTables 2 and 3 present the slot estimation accu-racy for the DSTC corpus and the Japanese weathercorpus, respectively.
The baseline (CRF best) is amethod that takes only one best output of CRF forslot estimation.
HDP with N = 5 and N = 300is the proposed method, where N is the number ofcandidates generated by the CRF candidate genera-2150utterance estimation by CRF best estimation by HDP N = 5im looking for a restaurant that type:restaurant (*) type:restaurant,food:fast foodserves fast foodi want a moderate restaurant in area:new chesterton, area:new chesterton,the new chesterton area type:restaurant, type:restaurant,food:moderate (*) pricerange:moderateim looking for a cheap chine pricerange:cheap,type:restaurant, pricerange:cheap,type:restaurant,chinese takeaway restaurant food:chinese takeaway food:chine chinese takeaway (*)Table 4: Examples of estimated slot values for the condition of #train is 800.
An asterisk (*) indicates misrecognition.tor.
The asterisks (*) beside the HDP accuracy in-dicate the statistical significance against CRF best,which is tested using the binomial test.Results show that our proposed method performssignificantly better than CRF.
Especially when theamount of training data is limited, the proposedmethod outperforms the baseline.
This property isattractive for practical speech recognition systemsthat offer many different functions.
Accurate recog-nition at an early stage of development allows apractitioner to launch a service that results in quicklycollecting hundreds of speech examples.Since we use the CRF as a candidate generator,we expect that the CRF N-best can rank the correctanswer higher in the candidate list.
In fact, the topfive candidates cover almost all of the correct an-swers.
Therefore, the result in the comparison ofN = 5 and N = 300 suggests the stability of theproposed method against the mostly noisy 295 can-didates.
Because the proposed algorithm makes nouse of the original ranking order, N = 300 is aharder condition in which to identify the correct an-swer.
Nevertheless, the result shows that the drop inthe performance is limited; the accuracy is still sig-nificantly better than the baseline.
This result sug-gests that the proposed method is less dependent onthe performance of the candidate generator.Table 4 presents some examples of the slot val-ues estimated by CRF best and HDP with N = 5for the condition where the number of training utter-ances is 800.
The first two are samples where CRFbest failed to predict the correct values.
These er-rors are attributed to infrequent sequential patternscaused by the less trained expressions ?that servesfast food?
and ?moderate restaurant?
because CRFis a position-based classifier.
The value-based for-mulation allows the model to learn that the phrase?fast food?
is more likely to be a food name than tobe a functional filler and to reject the candidate.The third example in Table 4 shows an errorusing HDP, which extracted ?chine chinese take-away?
which includes a reparandum of disfluency(Georgila et al, 2010).
This error can be attributedto the fact that this kind of disfluency resembles thetrue slot value, which leads to a higher probabilityof ?chine?
in the food slot model compared to inthe functional filler model.
Regarding this type oferror, preliminary application of a disfluency detec-tion method (Zayats et al, 2016) is promising forimproving accuracy.The execution time for training the proposedHDP utterance model with 1297 training data inthe Japanese weather corpus was about 0.3 seconds.This is a good performance since the CRF trainingtakes about 5.5 seconds.
Moreover, the training ofthe proposed HDP model is scalable and works in anonline manner because it is a single pass algorithm.When we have a very large number of training ex-amples, the bottleneck is the CRF training, whichrequires scanning the whole dataset repeatedly.7 ConclusionIn this paper, we proposed an arbitrary slot fill-ing method that directly deals with the posteriorprobability of slot values by using nonparametricBayesian models.
We presented a two-stage methodthat involves an N-best candidate generation step,which is typically done using a CRF.
Experimentalresults show that our method significantly improvesrecognition accuracy.
This empirical evidence sug-gests that the value-based formulation is a promis-ing approach for arbitrary slot filling tasks, which isworth exploring further in future work.2151ReferencesYun-Nung Chen, William Yang Wang, Anatole Gersh-man, and Alexander Rudnicky.
2015.
Matrix Fac-torization with Knowledge Graph Propagation for Un-supervised Spoken Language Understanding.
In Proc.Annual Meeting of the Association for ComputationalLinguistics.Thomas S. Ferguson.
1973.
A Bayesian Analysis ofSome Nonparametric Problems.
The Annual of Statis-tics, 1(2):209?230.Kallirroi Georgila, Ning Wang, and Jonathan Gratch.2010.
Cross-Domain Speech Disfluency Detection.
InProc.
Annual SIGDIAL Meeting on Discourse and Di-alogue.Samuel J. Gershman and David M. Blei.
2012.
A tu-torial on Bayesian nonparametric models.
Journal ofMathematical Psychology, 56(1):1?12.Daniel Gildea.
2002.
Automatic labeling of semanticroles.
Computational Linguistics, 28(3):245?288.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2011.
Producing Power-Law Distributionsand Damping Word Frequencies with Two-Stage Lan-guage Models.
Journal of Machine Learning Re-search, 12:2335?2382.Stefan Hahn, Marco Dinarelli, Christian Raymond,Fabrice Lefevre, Patrick Lehnen, Renato De Mori,Alessandro Moschitti, Hermann Ney, and GiuseppeRiccardi.
2011.
Comparing stochastic approaches tospoken language understanding in multiple languages.IEEE Transactions on Audio, Speech and LanguageProcessing, 19(6):1569?1583.Matthew Henderson.
2015.
Machine Learning for Dia-log State Tracking: A Review.
In Proc.
Workshop onMachine Learning in Spoken Language Processing.Kazunori Komatani, Masaki Katsumaru, Mikio Nakano,Kotaro Funakoshi, Tetsuya Ogata, and Hiroshi G.Okuno.
2010.
Automatic Allocation of Training Datafor Rapid Prototyping.
In Proc.
International Confer-ence on Computational Linguistics.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying Conditional Random Fields toJapanese Morphological Analysis.
In Proc.
EmpiricalMethods in Natural Language Processing.John Lafferty, Andrew McCallum, and Fernando C NPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proc.
International Conference on Machine Learn-ing.Alejandra Lorenzo, Lina M Rojas-barahona, andChristophe Cerisara.
2013.
Unsupervised structuredsemantic inference for spoken dialog reservation tasks.In Proc.
Annual SIGDIAL Meeting on Discourse andDialogue.Gregoire Mesnil, Yann Dauphin, Kaisheng Yao, YoshuaBengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He,Larry Heck, Gokhan Tur, Dong Yu, and GeoffreyZweig.
2015.
Using Recurrent Neural Networksfor Slot Filling in Spoken Language Understanding.IEEE/ACM Transactions on Audio, Speech, and Lan-guage Processing, 23(3):530?539.Angeliki Metallinou, Dan Bohus, and Jason Williams.2013.
Discriminative state tracking for spoken dialogsystems.
Proc.
Annual Meeting of the Association forComputational Linguistics.Christian Raymond, Fre?de?ric Be?chet, Renato De Mori,and Ge?raldine Damnati.
2006.
On the use of finitestate transducers for semantic interpretation.
SpeechCommunication, 48(3-4):288?304.Hang Ren, Weiqun Xu, and Yonghong Yan.
2014.Markovian discriminative modeling for dialog statetracking.
In Proc.
Annual SIGDIAL Meeting on Dis-course and Dialogue.Yee W. Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2005.
Hierarchical Dirichlet Pro-cesses.
Journal of the American Statistical Associa-tion, 101:1566?1581.Gokhan Tur, Asli Celikyilmaz, and Dilek Hakkani-Tur.2013.
Latent Semantic Modeling for Slot Filling inConversational Understanding.
In Proc.
InternationalConference on Acoustics, Speech and Signal Process-ing.Vedran Vukotic, Christian Raymond, and GuillaumeGravier.
2015.
Is it Time to Switch to Word Em-bedding and Recurrent Neural Networks for SpokenLanguage Understanding?
In Proc.
Interspeech.Jason D. Williams.
2010.
Incremental partition re-combination for efficient tracking of multiple dialogstates.
In Proc.
International Conference on Acous-tics, Speech and Signal Processing.Jason D Williams.
2014.
Web-style ranking and SLUcombination for dialog state tracking.
In Proc.
AnnualSIGDIAL Meeting on Discourse and Dialogue.Puyang Xu and Ruhi Sarikaya.
2013.
Convolutional neu-ral network based triangular CRF for joint intent de-tection and slot filling.
In Proc.
IEEE Workshop onAutomatic Speech Recognition and Understanding.Vicky Zayats, Mari Ostendorf, and Hannaneh Hajishirzi.2016.
Disfluency Detection using a BidirectionalLSTM.
arXiv preprint arXiv:1604.03209.Ke Zhai and Jordan Boyd-graber.
2013.
Online LatentDirichlet Allocation with Infinite Vocabulary.
In Proc.International Conference on Machine Learning.2152
