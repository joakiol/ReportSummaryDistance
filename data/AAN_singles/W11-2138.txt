Proceedings of the 6th Workshop on Statistical Machine Translation, pages 330?336,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsImproving Translation Model by Monolingual Data?Ondr?ej Bojar and Ales?
Tamchynabojar@ufal.mff.cuni.cz, a.tamchyna@gmail.comInstitute of Formal and Applied Linguistics,Faculty of Mathematics and Physics, Charles University in PragueAbstractWe use target-side monolingual data to ex-tend the vocabulary of the translation modelin statistical machine translation.
This methodcalled ?reverse self-training?
improves the de-coder?s ability to produce grammatically cor-rect translations into languages with morphol-ogy richer than the source language esp.
insmall-data setting.
We empirically evalu-ate the gains for several pairs of Europeanlanguages and discuss some approaches ofthe underlying back-off techniques needed totranslate unseen forms of known words.
Wealso provide a description of the systems wesubmitted to WMT11 Shared Task.1 IntroductionLike any other statistical NLP task, SMT relies onsizable language data for training.
However the par-allel data required for MT are a very scarce resource,making it difficult to train MT systems of decentquality.
On the other hand, it is usually possible toobtain large amounts of monolingual data.In this paper, we attempt to make use of themonolingual data to reduce the sparseness of surfaceforms, an issue typical for morphologically rich lan-guages.
When MT systems translate into such lan-guages, the limited size of parallel data often causesthe situation where the output should include a wordform never observed in the training data.
Eventhough the parallel data do contain the desired word?
This work has been supported by the grants EuroMatrix-Plus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of theCzech Republic), P406/10/P259, and MSM 0021620838.in other forms, a standard phrase-based decoder hasno way of using it to generate the correct translation.Reverse self-training addresses this problem byincorporating the available monolingual data in thetranslation model.
This paper builds upon the ideaoutlined in Bojar and Tamchyna (2011), describinghow this technique was incorporated in the WMTShared Task and extending the experimental evalu-ation of reverse self-training in several directions ?the examined language pairs (Section 4.2), data size(Section 4.3) and back-off techniques (Section 4.4).2 Related WorkThe idea of using monolingual data for improvingthe translation model has been explored in severalprevious works.
Bertoldi and Federico (2009) usedmonolingual data for adapting existing translationmodels to translation of data from different domains.In their experiments, the most effective approachwas to train a new translation model from ?fake?parallel data consisting of target-side monolingualdata and their machine translation into the sourcelanguage by a baseline system.Ueffing et al (2007) used a boot-strapping tech-nique to extend translation models using mono-lingual data.
They gradually translated additionalsource-side sentences and selectively incorporatedthem and their translations in the model.Our technique also bears a similarity to de Gis-pert et al (2005), in that we try to use a back-offfor surface forms to generalize our model and pro-duce translations with word forms never seen in theoriginal parallel data.
However, instead of a rule-based approach, we take advantage of the available330Source English Target Czech Czech LemmatizedParallel (small) a cat chased.
.
.
= koc?ka honila.
.
.
koc?ka honit.
.
.I saw a cat = vide?l jsem koc?ku vide?t by?t koc?kaI read about a dog = c?etl jsem o psovi c??
?st by?t o pesMonolingual (large) ?
c?etl jsem o koc?ce c??
?st by?t o koc?kaI read about a cat ?
Use reverse translation backed-off by lemmas.Figure 1: The essence of reverse self-training: a new phrase pair (?about a cat?
= ?o koc?ce?)
is learned based on asmall parallel corpus and large target-side monolingual texts.data and learn these forms statistically.
We are there-fore not limited to verbs, but our system is only ableto generate surface forms observed in the target-sidemonolingual data.3 Reverse Self-TrainingFigure 1 illustrates the core of the method.
Usingavailable parallel data, we first train an MT systemto translate from the target to the source language.Since we want to gather new word forms from themonolingual data, this reverse model needs the abil-ity to translate them.
For that purpose we use a fac-tored translation model (Koehn and Hoang, 2007)with two alternative decoding paths: form?formand back-off?form.
We experimented with severaloptions for the back-off (simple stemming by trun-cation or full lemmatization), see Section 4.4.
Thedecoder can thus use a less sparse representation ofwords if their exact forms are not available in theparallel data.We use this reverse model to translate (muchlarger) target-side monolingual data into the sourcelanguage.
We preserve the word alignments of thephrases as used in the decoding so we directly ob-tain the word alignment in the new ?parallel?
cor-pus.
This gives us enough information to proceedwith the standard MT system training ?
we extractand score the phrases consistent with the constructedword alignment and create the phrase table.We combine this enlarged translation model witha model trained on the true parallel data and useMinimum Error Rate Training (Och, 2003) to findthe balance between the two models.
The finalmodel has four separate components ?
two languagemodels (one trained on parallel and one on monolin-gual data) and the two translation models.We do not expect the translation quality to im-prove simply because more data is included in train-ing ?
by adding translations generated using knowndata, the model could gain only new combinationsof known words.
However, by using a back-offto less sparse units (e.g.
lemmas) in the factoredtarget?source translation, we enable the decoderto produce previously unseen surface forms.
Thesetranslations are then included in the model, reducingthe data sparseness of the target-side surface forms.4 ExperimentsWe used common tools for phrase-based translation?
Moses (Koehn et al, 2007) decoder and tools,SRILM (Stolcke, 2002) and KenLM (Heafield,2011) for language modelling and GIZA++ (Ochand Ney, 2000) for word alignments.For reverse self-training, we needed Moses to alsooutput word alignments between source sentencesand their translations.
As we were not able to makethe existing version of this feature work, we added anew option and re-implemented this funcionality.We rely on automatic translation quality eval-uation throughout our paper, namely the well-established BLEU metric (Papineni et al, 2002).
Weestimate 95% confidence bounds for the scores asdescribed in Koehn (2004).
We evaluated our trans-lations on lower-cased sentences.4.1 Data SourcesAside from the WMT 2011 Translation Task data,we also used several additional data sources for theexperiments aimed at evaluating various aspects ofreverse self-training.JRC-AcquisWe used the JRC-Acquis 3.0 corpus (Steinbergeret al, 2006) mainly because of the number of avail-able languages.
This corpus contains a large amount331Source Target Corpus Size (k sents) Vocabulary Size Ratio Baseline +Mono LM +Mono TMPara MonoEnglish Czech 94 662 1.67 40.9?1.9 43.5?2.0 *44.3?2.0English Finnish 123 863 2.81 27.0?1.9 27.6?1.8 28.3?1.7English German 127 889 1.83 34.8?1.8 36.4?1.8 37.6?1.8English Slovak 109 763 2.03 35.3?1.6 37.3?1.7 37.7?1.8French Czech 95 665 1.43 39.9?1.9 42.5?1.8 43.1?1.8French Finnish 125 875 2.45 26.7?1.8 27.8?1.7 28.3?1.8French German 128 896 1.58 38.5?1.8 40.2?1.8 *40.5?1.8German Czech 95 665 0.91 35.2?1.8 37.0?1.9 *37.3?1.9Table 1: BLEU scores of European language pairs on JRC data.
Asterisks in the last column mark experiments forwhich MERT had to be re-run.of legislative texts of the European Union.
The factthat all data in the corpus come from a single, verynarrow domain has two effects ?
models trained onthis corpus perform mostly very well in that domain(as documented e.g.
in Koehn et al (2009)), but failwhen translating ordinary texts such as news or fic-tion.
Sentences in this corpus also tend to be ratherlong (e.g.
30 words on average for English).CzEngCzEng 0.9 (Bojar and ?Zabokrtsky?, 2009) is a par-allel richly annotated Czech-English corpus.
It con-tains roughly 8 million parallel sentences from avariety of domains, including European regulations(about 34% of tokens), fiction (15%), news (3%),technical texts (10%) and unofficial movie subtitles(27%).
We do not make much use of the rich anno-tation in this paper, however we did experiment withusing Czech lemmas (included in the annotation) asthe back-off factor for reverse self-training.4.2 Comparison Across LanguagesIn order to determine how successful our approachis across languages, we experimented with Czech,Finnish, German and Slovak as target languages.
Allof them have a rich morphology in some sense.
Welimited our selection of source languages to English,French and German because our method focuses onthe target language anyway.
We did however com-bine the languages with respect to the richness oftheir vocabulary ?
the source language has less wordforms in almost all cases.Czech and Slovak are very close languages, shar-ing a large portion of vocabulary and having a verysimilar grammar.
There are many inflectional rulesfor verbs, nouns, adjectives, pronouns and numerals.Sentence structure is exhibited by various agreementrules which often apply over long distance.
Most ofthe issues commonly associated with rich morphol-ogy are clearly observable in these languages.German also has some inflection, albeit much lesscomplex.
The main source of German vocabularysize are the compound words.
Finnish serves as anexample of agglutinative languages well-known forthe abundance of word forms.Table 1 contains the summary of our experimen-tal results.
Here, only the JRC-Acquis corpus wasused for training, development and evaluation.
Forevery language pair, we extracted the first 10 per-cent of the parallel corpus and used them as the par-allel data.
The last 70 percent of the same corpuswere our ?monolingual?
data.
We used a separateset of 1000 sentences for the development and an-other 1000 for testing.Sentence counts of the corpora are shown in thecolumns Corpus Size Para and Mono.
The tablealso shows the ratio between observed vocabularysize of the target and source language.
Except forthe German?Czech language pair, the ratios arehigher than 1.
The Baseline column contains theBLEU score of a system trained solely on the paral-lel data (i.e.
the first 10 percent).
A 5-gram languagemodel was used.
The ?+Mono LM?
scores wereachieved by adding a 5-gram language model trainedon the monolingual data as a separate component(its weight was determined by MERT).
The last col-umn contains the scores after adding the translationmodel self-trained on target monolingual data.
Thismodel was also added as another component and theweights associated with it were found by MERT.332For the back-off in the reverse self-training, weused a simple suffix-trimming heuristic suitable forfusional languages: cut off the last three charactersof each word always keeping at least the first threecharacters.
This heuristic reduces the vocabularysize to a half for Czech and Slovak but it is muchless effective for Finish and German (Table 2), ascan be expected from their linguistic properties.Language Vocabulary reduced to (%)Czech 52Finnish 64German 73Slovak 51Table 2: Reduction of vocabulary size by suffix trimmingWe did not use any linguistic tools, such as mor-phological analyzers, in this set of experiments.
Wesee the main point of this section in illustrating theapplicability of our technique on a wide range of lan-guages, including languages for which such tools arenot available.We encountered problems when using MERT tobalance the weights of the four model components.Our model consisted of 14 features ?
one for eachlanguage model, five for each translation model(phrase probability and lexical weight for both di-rections and phrase penalty), word penalty and dis-tortion penalty.
The extra 5 weights of the reverselytrained translation model caused MERT to diverge insome cases.
Since we used the mert-moses.plscript for tuning and kept the default parameters,MERT ran for 25 iterations and stopped.
As a result,even though our method seemed to improve trans-lation performance in most language pairs, severalexperiments contradicted this observation.
We sim-ply reran the final tuning procedure in these casesand were able to achieve an improvement in BLEUas well.
These language pairs are marked with a ?
*?sign in Table 1.A possible explanation for this behaviour ofMERT is that the alternative decoding paths add alot of possible derivations that generate the samestring.
To validate our hypothesis we examined adiverging run of MERT for English?Czech transla-tion with two translation models.
Our n-best listscontained the best 100 derivations for each trans-Figure 2: Vocabulary ratio and BLEU score0.20.40.60.811.20.8  1  1.2 1.4 1.6 1.8  2  2.2 2.4 2.6 2.8  3GaininBLEU(absolute)Vocabulary size ratioen-csen-fien-deen-skfr-csfr-fifr-dede-cslated sentence from the development data.
On av-erage (over all 1000 sentences and over all runs), then-best list only contained 6.13 different translationsof a sentence.
The result of the same calculationapplied on the baseline run of MERT (which con-verged in 9 iterations) was 34.85 hypotheses.
Thisclear disproportion shows that MERT had much lessinformation when optimizing our model.Overall, reverse self-training seems helpful fortranslating into morphologically rich languages.
Weachieved promising gains in BLEU, even over thebaseline including a language model trained on themonolingual data.
The improvement ranges fromroughly 0.3 (e.g.
German?Czech) to over 1 point(English?German) absolute.
This result also indi-cates that suffix trimming is a quite robust heuristic,useful for a variety of language types.Figure 2 illustrates the relationship between vo-cabulary size ratio of the language pair and theimprovement in translation quality.
Although thepoints are distributed quite irregularly, a certain ten-dency towards higher gains with higher ratios is ob-servable.
We assume that reverse self-training ismost useful in cases where a single word form in thesource language can be translated as several forms inthe target language.
A higher ratio between vocab-ulary sizes suggests that these cases happen moreoften, thus providing more space for improvementusing our method.3334.3 Data SizesWe conducted a series of English-to-Czech experi-ments with fixed parallel data and a varying size ofmonolingual data.
We used the CzEng corpus, 500thousand parallel sentences and from 500 thousandup to 5 million monolingual sentences.
We usedtwo separate sets of 1000 sentences from CzEng fordevelopment and evaluation.
Our results are sum-marized in Figure 3.
The gains in BLEU becomemore significant as the size of included monolingualdata increases.
The highest improvement can be ob-served when the data are largest ?
over 3 points ab-solute.
Figure 4 shows an example of the impact ontranslation quality ?
the ?Mono?
data are 5 millionsentences.When evaluated from this point of view, ourmethod can also be seen as a way of considerablyimproving translation quality for languages with lit-tle available parallel data.We also experimented with varying size of paral-lel data (500 thousand to 5 million sentences) and itseffect on reverse self-training contribution.
The sizeof monolingual data was always 5 million sentences.We first measured the percentage of test data wordforms covered by the training data.
We calculatedthe value for parallel data and for the combination ofparallel and monolingual data.
For word forms thatappeared only in the monolingual data, a differentform of the word had to be contained in the paralleldata (so that the model can learn it through the back-off heuristic) in order to be counted in.
The differ-ence between the first and second value can simplybe thought of as the upper-bound estimation of re-verse self-training contribution.
Figure 5 shows theresults along with BLEU scores achieved in transla-tion experiments following this scenario.Our technique has much greater effect for smallparallel data sizes; the amount of newly learnedword forms declines rapidly as the size grows.Similarly, improvement in BLEU score decreasesquickly and becomes negligible around 2 millionparallel sentences.4.4 Back-off TechniquesWe experimented with several options for the back-off factor in English?Czech translation.
Data fromtraining section of CzEng were used, 1 million par-Figure 3: Relation between monolingual data size andgains in BLEU score26272829303132330  1  2  3  4  5BLEUMonolingual data size (millions of sentences)Mono LM and TMMono LMFigure 5: Varying parallel data size, surface form cov-erage (?Parallel?, ?Parallel and Mono?)
and BLEU score(?Mono LM?, ?Mono LM and TM?
)262830323436380  0.5  1  1.5  2  2.5  3  3.5  4  4.59092949698BLEUCoverageof surfaceforms(%)Parallel data size (millions of sentences)Mono LM and TMMono LMBLEUCoverageof surfaceforms(%)Parallel and MonoParallelallel sentences and another 5 million sentences astarget-side monolingual data.
As in the previoussection, the sizes of our development and evaluationsets were a thousand sentences.CzEng annotation contains lexically disam-biguated word lemmas, an appealing option for ourpurposes.
We also tried trimming the last 3 charac-ters of each word, keeping at least the first 3 charac-ters intact.
Stemming of each word to four charac-ters was also evaluated (Stem-4).Table 3 summarizes our results.
The last columnshows the vocabulary size compared to original vo-cabulary size, estimated on lower-cased words.We are not surprised by stemming performing the334System Translation GlossBaseline Jsi tak zrcadla?
Are youSG so mirrors?
(ungrammatical)+Mono LM Jsi neobjedna?vejte zrcadla?
Did youSG don?t orderPL mirrors?
(ungrammatical)+Mono TM Uz?
sis objednal zrcadla?
Have youSG orderedSG the mirrors (for yourself) yet?Figure 4: Translation of the sentence ?Did you order the mirrors??
by baseline systems and a reversely-trained system.Only the last one is able to generate the correct form of the word ?order?.worst ?
the equivalence classes generated by thissimple heuristic are too broad.
Using lemmas seemsoptimal from the linguistic point of view, howeversuffix trimming outperformed this approach in ourexperiments.
We feel that finding well-performingback-off techniques for other languages merits fur-ther research.Back-off BLEU Vocabulary Size (%)Baseline 31.82?3.24 100Stem-4 32.73?3.19 19Lemma 33.05?3.40 54Trimmed Suffix 33.28?3.32 47Table 3: Back-off BLEU scores comparison4.5 WMT SystemsWe submitted systems that used reverse self-training (cu-tamchyna) for English?Czech andEnglish?German language pairs.Our parallel data for German were constrained tothe provided set (1.9 million sentences).
For Czech,we used the training sections of CzEng and the sup-plied WMT11 News Commentary data (7.3 millionsentences in total).In case of German, we only used the suppliedmonolingual data, for Czech we used a large col-lection of texts for language modelling (i.e.
uncon-strained).
The reverse self-training used only theconstrained data ?
2.3 million sentences in Germanand 2.2 in Czech.
In case of Czech, we only usedthe News monolingual data from 2010 and 2011 forreverse self-training ?
we expected that recent datafrom the same domain as the test set would improvetranslation performance the most.We achieved mixed results with these systems ?for translation into German, reverse self-training didnot improve translation performance.
For Czech,we were able to achieve a small gain, even thoughthe reversely translated data contained less sentencesthan the parallel data.
Our BLEU scores were alsoaffected by submitting translation outputs withoutnormalized punctuation and with a slightly differenttokenization.In this scenario, a lot of parallel data were avail-able and we did not manage to prepare a reverselytrained model from larger monolingual data.
Bothof these factors contributed to the inconclusive re-sults.Table 4 shows case-insensitive BLEU scores ascalculated in the official evaluation.Target Language Mono LM +Mono TMGerman 14.8 14.8Czech 15.7 15.9Table 4: Case-insensitive BLEU of WMT systems5 ConclusionWe introduced a technique for exploiting monolin-gual data to improve the quality of translation intomorphologically rich languages.We carried out experiments showing improve-ments in BLEU when using our method for trans-lating into Czech, Finnish, German and Slovak withsmall parallel data.
We discussed the issues of in-cluding similar translation models as separate com-ponents in MERT.We showed that gains in BLEU score increasewith growing size of monolingual data.
On the otherhand, growing parallel data size diminishes the ef-fect of our method quite rapidly.
We also docu-mented our experiments with several back-off tech-niques for English to Czech translation.Finally, we described our primary submissions tothe WMT 2011 Shared Translation Task.335ReferencesNicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translation withmonolingual resources.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages182?189, Athens, Greece, March.
Association forComputational Linguistics.Ondr?ej Bojar and Ales?
Tamchyna.
2011.
Forms Wanted:Training SMT on Monolingual Data.
Abstract atMachine Translation and Morphologically-Rich Lan-guages.
Research Workshop of the Israel ScienceFoundation University of Haifa, Israel, January.Ondr?ej Bojar and Zdene?k ?Zabokrtsky?.
2009.
CzEng0.9: Large Parallel Treebank with Rich Annotation.Prague Bulletin of Mathematical Linguistics, 92:63?83.Adria` de Gispert, Jose?
B. Marin?o, and Josep M. Crego.2005.
Improving statistical machine translation byclassifying and generalizing inflected verb forms.
InEurospeech 2005, pages 3185?3188, Lisbon, Portugal.Kenneth Heafield.
2011.
Kenlm: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, Edin-burgh, UK, July.
Association for Computational Lin-guistics.Philipp Koehn and Hieu Hoang.
2007.
Factored Transla-tion Models.
In Proc.
of EMNLP.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL.
TheAssociation for Computer Linguistics.Philipp Koehn, Alexandra Birch, and Ralf Steinberger.2009.
462 machine translation systems for europe.
InMT Summit XII.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In EMNLP, pages388?395.
ACL.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
pages 440?447, Hongkong,China, October.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In ACL, pages 311?318.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz Erjavec, Dan Tufis, and DanielVarga.
2006.
The JRC-acquis: A multilingualaligned parallel corpus with 20+ languages.
CoRR,abs/cs/0609058.
informal publication.Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit, June 06.Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.2007.
Semi-supervised model adaptation for statisticalmachine translation.
Machine Translation, 21(2):77?94.336
