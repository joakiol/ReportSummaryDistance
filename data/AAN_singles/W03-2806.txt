Intrinsic versus Extrinsic Evaluations of Parsing SystemsDiego Molla?Centre for Language TechnologyDepartment of ComputingMacquarie UniversitySydney, NSW 2109, Australiadiego@ics.mq.edu.auBen HutchinsonDivision of InformaticsUniversity of EdinburghEdinburgh EH8 9LW, United KingdomB.Hutchinson@sms.ed.ac.ukAbstractA wide range of parser and/or grammarevaluation methods have been reportedin the literature.
However, in most casesthese evaluations take the parsers in-dependently (intrinsic evaluations), andonly in a few cases has the effectof different parsers in real applicationsbeen measured (extrinsic evaluations).This paper compares two evaluationsof the Link Grammar parser and theConexor Functional Dependency Gram-mar parser.
The parsing systems, de-spite both being dependency-based, re-turn different types of dependencies,making a direct comparison impossi-ble.
In the intrinsic evaluation, the accu-racy of the parsers is compared indepen-dently by converting the dependenciesinto grammatical relations and using themethodology of Carroll et al (1998) forparser comparison.
In the extrinsic eval-uation, the parsers?
impact in a practi-cal application is compared within thecontext of answer extraction.
The dif-ferences in the results are significant.1 IntroductionParsing is a principal stage in many natural lan-guage processing (NLP) systems.
A good parser isexpected to return an accurate syntactic structureof a sentence.
This structure is typically forwardedto other modules so that they can work with un-ambiguous and well-defined structures represent-ing the sentences.
It is to be expected that theperformance of an NLP system quickly degradesif the parsing system returns incorrect syntacticstructures, and therefore an evaluation of parsingcoverage and accuracy is important.According to Galliers and Sparck Jones (1993),there are two main criteria in performance evalua-tion: ?Intrinsic criteria are those relating to a sys-tem?s objective, extrinsic criteria those relating toits function i.e.
to its role in relation to its setup?spurpose.?
(Galliers and Sparck Jones, 1993, p22).Thus, an intrinsic evaluation of a parser wouldanalyse the accuracy of the results returned by theparser as a stand-alone system, whereas an ex-trinsic evaluation would analyse the impact of theparser within the context of a broader NLP appli-cation.There are currently several parsingsystems that attempt to achieve a widecoverage of the English language (suchas those developed by Collins (1996),Ja?rvinen and Tapanainen (1997), andSleator and Temperley (1993)).
There is alsosubstantial literature on parsing evaluation (see,for example, work by Sutcliffe et al (1996),Black (1996), Carroll et al (1998), andBangalore et al (1998)).
Recently there hasbeen a shift from constituency-based (e.g.
count-ing crossing brackets (Black et al, 1991)) todependency-based evaluation (Lin, 1995; Carrollet al, 1998).
Those evaluation methodologiestypically focus on comparisons of stand-aloneparsers (intrinsic evaluations).
In this paper wereport on the comparison between an intrinsicevaluation and an evaluation of the impact ofthe parser in a real application (an extrinsicevaluation).We have chosen answer extraction as an exam-ple of a practical application within which to testthe parsing systems.
In particular, the extrinsicevaluation uses ExtrAns, an answer extraction sys-tem that operates over Unix manual pages (Molla?et al, 2000).
The two grammar systems to com-pare are Link Grammar (Sleator and Temperley,1993) and the Conexor Functional DependencyGrammar parser (Tapanainen and Ja?rvinen, 1997)(henceforth referred to as Conexor FDG).
Theseparsing systems were chosen because both includea dependency-based parser and a comprehensivegrammar of English.
However, the structures re-turned are so different that a direct comparison be-tween them is not straightforward.
In Section 2 wereview the main differences between Link Gram-mar and Conexor FDG.
In Section 3 we presentthe intrinsic comparison of parsers, and in Sec-tion 4 we comment on the extrinsic comparisonwithin the context of answer extraction.
The re-sults of the evaluations are discussed in Section 5.2 Link Grammar and Conexor FDGLink Grammar (Sleator and Temperley, 1993) isa grammar theory that is strongly dependency-based.
A freely available parsing system that im-plements the Link Grammar theory has been de-veloped at Carnegie Mellon University.
The pars-ing system includes an extensive grammar and lex-icon and has a wide coverage of the English lan-guage.
Conexor FDG (Tapanainen and Ja?rvinen,1997) is a commercial parser and grammar, basedon the theory of Functional Dependency Gram-mar, and was originally developed at the Univer-sity of Helsinki.Despite both being dependency-based, there aresubstantial differences between the structures re-turned by the two parsers.
Figure 1 shows LinkGrammar?s output for a sample sentence, and Fig-ure 2 shows the dependency structure returnedby Conexor FDG for comparison.
Table 1 ex-plains the dependency types used in the depen-dency structures of the figures.The differences between the dependency struc-tures returned by Link Grammar 2.1 and ConexorFDG 3.6 can be summarised as follows.Direction of dependency: Link Grammar?s?links?, although similar to true dependencies, donot state which participant is the head and whichis the dependent.
However, Link Grammar usesdifferent link types for head-right links and head-left links, so this information can be recovered.Conexor FDG always indicates the direction of thedependence.Clausal heads: Link Grammar generallychooses the front-most element to be the headof a clause, rather than the main verb.
This istrue of both matrix and subordinate clauses, asexemplified by the Wd and R links in Figure 1.Conexor FDG follows the orthodox convention ofchoosing the main verb as the head of the clause.Graph structures: Link Grammar?s links com-bine dependencies at the surface-syntactic anddeep-syntactic levels (e.g., the link Bs, whichlinks a noun modified by a subject-type relativeclause to the relative clause?s head verb, in Fig-ure 1 indicates a deep-syntactic dependency).
Theresulting structures are graphs rather than trees.An example is shown in Figure 1, where the nounman modified by a relative clause is linked to boththe complementiser and the head verb of the rela-tive clause.Conjunctions: Our version of Link Grammaranalyses a coordinating conjunction as the head ofa coordinated phrase (Figure 1).
This is a modifi-cation of Link Grammar?s default behaviour whichreturns a list of parses, one parse per conjunct.However in Conexor FDG?s analyses the head willbe either the first or the last conjunct, dependingon whether the coordinated phrase?s head lies tothe left or to the right (Figure 2).Dependency types: Link Grammar uses a set ofabout 90 link types and many subtypes, which ad-dress very specific syntactic constructions (e.g.
thelink type EB connects adverbs to forms of be be-fore a noun phrase or prepositional phrase: Heis APPARENTLY a good programmer).
On theother hand, Conexor FDG uses a set of 32 de-///// the man.n that came.v ate.v bananas.n and apples.n with a fork.n1WdDsSsBsR RSMVpO^ JsDsFigure 1: Output of Link Grammar.///// the man that came ate bananas and apples with a forkmain <>det> subjmod<>subjins <obj< cc<cc <  pcomp<>detFigure 2: Dependency structure returned by Conexor FDG.pendency relations, ranging from traditional gram-matical functions (e.g.
subject, object), to specifictypes of modifiers (e.g.
frequency, duration, loca-tion).Both Conexor FDG and Link Grammar alsoreturn non-dependency information.
For LinkGrammar, this consists of some word class in-formation, shown as suffixes in Figure 1.
ForConexor FDG, the base form morphological in-formation of each word is returned, along with a?functional?
tag or morpho-syntactic function anda ?surface syntactic?
tag for each word.13 Intrinsic EvaluationsGiven that both parses are dependency-based, in-trinsic evaluations that are based on constituencystructures (e.g.
(Black et al, 1991)) are hardto perform.
Dependency-based evaluations arenot easy either: directly comparing dependencygraphs (as suggested by Lin (1995), for exam-ple) becomes difficult given the differences be-tween the structures returned by the Link Gram-mar parser and Conexor FDG.
We there-fore need an approach that is independent fromthe format of the parser output.
FollowingCarroll et al (1998) we use grammatical relationsto compare the accuracy of Link Grammar andConexor FDG.
Carroll et al (1998) propose a setof twenty parser-independent grammatical rela-tions arranged in a hierarchy representing differ-ent degrees of specificity.
Four relations from thehierarchy are shown in Table 2.
The arguments to1See (Ja?rvinen and Tapanainen, 1997) for more informa-tion on the output from Conexor FDG.each relation specify a head, a dependent, and pos-sibly an initial grammatical relation (in the caseof SUBJ in passive sentences, for example) or the?type?, which specifies the word introducing thedependent (in the case of XCOMP).For example, the grammatical relations of thesentence the man that came ate bananas and ap-ples with a fork without asking has the followingrelations:SUBJ(eat,man, ),OBJ(eat,banana),OBJ(eat,apple),MOD(fork,eat,with),SUBJ(come,man, ),MOD(that,man,come),XCOMP(without,eat,ask)The terms ?head?
and ?dependent?
usedby Carroll et al (1998) to refer to the argumentsof grammatical relations should not be con-fused with the similar terms in the theory ofdependency grammar.
Grammatical relationsand dependency arcs represent different phe-nomena.
An example should suffice to illustratethe difference; consider The man that came atebananas and apples with a fork.
In dependencygrammar a unique head is assigned to each word,for example the head of man is ate.
Howeverman is the dependent of more than one gram-matical relation, namely SUBJ(eat,man, )and SUBJ(come,man, ).
Furthermore, independency grammar a word can have at mostone dependent of each argument type, and so atecan have at most one object, for example.
ButLink Grammar Conexor FDGName Description Name DescriptionBs Singular external object of relative clause cc CoordinationDs Singular determiner det DeterminerJs Singular object of a preposition ins <not documented>MVp Verb-modifying preposition main Main elementO?
Object mod General post-modifierR Relative clause obj ObjectRS Part of subject-type relative clause pcomp Prepositional complementSs Singular subject subj SubjectWd Declarative sentenceTable 1: Some of the dependency types used by Link Grammar and Conexor FDG.Relation DescriptionSUBJ(head, dependent, initial gr) SubjectOBJ(head, dependent) ObjectXCOMP(type, head, dependent) Clausal complement without an overt subjectMOD(type, head, dependent) ModifierTable 2: Grammatical relations used in the intrinsic evaluation.the same is not true for grammatical relations,and we get both OBJ(eat,banana) andOBJ(eat,apple).3.1 AccuracyOur intrinsic evaluation began on the assumptionthat grammatical relations could be deduced fromthe dependency structures returned by the parsers.In practise, however, this deduction process is notalways straightforward; for example complexityarises when arguments are shared across clauses.In addition, Link Grammar?s analysis of the front-most elements as clausal heads complicates thegrammatical relation deduction when there aremodifying clauses.An existing corpus of 500 sentences/10,000words annotated with grammatical relations wasused for the evaluation (Carroll et al, 1999).
Werestricted the evaluation to just the four relationsshown in Table 2.
This decision had two motiva-tions.
Firstly, since the dependency parsers?
out-put did not recognise some distinctions made inthe hierarchy of relations, it did not make sense totest these distinctions.
Secondly, we wanted thededuction of grammatical relations to be as simplea process as possible, to minimise the chance ofintroducing errors.
This second consideration alsoled us to purposefully ignore the sharing of argu-ments induced by control verbs, as this could notalways be deduced reliably.
Since this was donefor both parsers the comparison remains meaning-ful.Algorithms for producing grammatical relationsfrom Link Grammar and Conexor FDG outputwere developed and implemented.
The results ofparsing the corpus are shown in Table 3.
SinceConexor FDG returns one parse per sentence onlyand Link Grammar returns all parses ranked, thefirst (i.e.
the best) parse returned by Link Gram-mar was used in the intrinsic evaluation.The table shows significantly lower values ofrecall and precision for Link Grammar.
This ispartly due to the fact that Link Grammar?s linksoften do not connect the head of the clause, as wehave seen with the Wd link in Figure 1.3.2 SpeedLink Grammar took 1,212 seconds to parse the10,000 word corpus, while Conexor FDG took20.5 seconds.
This difference is due partly to thefact that Link Grammar finds and returns multiple(and often many) alternative parses.
For example,With LinkGrammarWithConexorFDGPrecision SUBJ 50.3% 73.6%OBJ 48.5% 84.8%XCOMP 62.2% 76.2%MOD 57.2% 63.7%Average 54.6% 74.6%Recall SUBJ 39.1% 64.5%OBJ 50% 53.4%XCOMP 32.1% 64.7%MOD 53.7% 56.2%Average 43.7% 59.7%Table 3: Accuracy of identification of grammaticalrelations.Link Grammar found a total of 410,509 parses ofthe 505 corpus sentences.4 Extrinsic EvaluationsIt is important to know not only the accuracy ofa parser but how possible parsing errors affect thesuccess of an NLP application.
This is the goal ofan extrinsic evaluation, where the system is eval-uated in relation to the embedding setup.
Usinganswer extraction as an example of an NLP appli-cation, we compared the performance of the LinkGrammar system and Conexor FDG.4.1 Answer Extraction and ExtrAnsThe fundamental goal of Answer Extraction (AE)is to locate those exact phrases of unedited textdocuments that answer a query worded in nat-ural language.
AE has received much attentionrecently, as the increasingly active Question An-swering track in TREC demonstrates (Voorhees,2001b; Voorhees, 2001a).ExtrAns is an answer extraction system thatoperates over UNIX manual pages (Molla?
et al,2000).
A core process in ExtrAns is the produc-tion of semantic information in the shape of logi-cal forms for each sentence of each manual page,as well as the user query.
These logical forms aredesigned so that they can be derived from any sen-tence (using robust approaches to treat very com-plex or ungrammatical sentences), and they are op-timised for NLP tasks that involve the semanticcomparison of sentences, such as AE.ExtrAns?
logical forms are called minimal log-ical forms (MLFs) because they encode the mini-mum information required for effective answer ex-traction.
In particular, only the main dependenciesbetween the verb and arguments are expressed,plus modifier and adjunct relations.
Thus, com-plex quantification, tense and aspect, temporal re-lations, plurality, and modality are not expressed.The MLFs use reification to achieve flat expres-sions, very much in the line of Davidson (1967),Hobbs (1985), and Copestake et al (1997).
In thecurrent implementation only reification to objects,eventualities (events or states), and properties isapplied.
For example, the MLF of the sentence cpwill quickly copy files is:holds(e4),object(cp,o1,[x1]),object(s command,o2,[x1]),evt(s copy,e4,[x1,x6]),object(s file,o3,[x6]),prop(quickly,p3,[e4]).In other words, there is an entity x1 which rep-resents an object of type command;2 there is anentity x6 (a file); there is an entity e4, which rep-resents a copying event where the first argumentis x1 and the second argument is x6; there is anentity p3which states that e4 is done quickly, andthe event e4, that is, the copying, holds.ExtrAns finds the answers to the questions byconverting the MLFs of the questions into Prologqueries and then running Prolog?s default resolu-tion mechanism to find those MLFs that can provethe question.This default search procedure is called the syn-onym mode since ExtrAns uses a small WordNet-style thesaurus (Fellbaum, 1998) to convert all thesynonyms into a synonym representative.
Extr-Ans also has an approximate mode which, be-sides normalising all synonyms, scores all docu-ment sentences on the basis of the maximum num-ber of predicates that unify between the MLFs ofthe query and the answer candidate (Molla?
et al,2000).
If all query predicates can be matched then2ExtrAns uses additional domain knowledge to infer thatcp is a command.the approximate mode returns exactly the same an-swers as the synonym mode.4.2 The ComparisonIdeally, answer extraction systems should be eval-uated according to how successful they are in help-ing users to complete their tasks.
The use of thesystem will therefore depend on such factors ashow many potential answers the user is presentedwith at a time, the way these potential answers areranked, how many potential answers the user isprepared to read while searching for an actual an-swer, and so on.
These issues, though important,are beyond the scope of the present evaluation.
Inthis evaluation we focus solely on the relevance ofthe set of results returned by ExtrAns.4.2.1 MethodResources from a previous evaluation of Extr-Ans (Molla?
et al, 2000) were re-used for this eval-uation.
These resources were: a) a collection of500 man pages, and b) a test set of 26 queries andrelevant answers found in the 500 manual pages.The careful and labour-intensive construction ofthe test set gives us confidence that practically allrelevant answers to each query are present in thetest set.
The queries themselves were selected ac-cording to the following criteria:?
There must be at least one answer in the man-ual page collection.?
The query asks how to perform a particularaction, or how a particular command works.?
The query is simple, i.e.
it asks only onequestion.The manual pages were parsed using ConexorFDG and Link Grammar.
The latter has a param-eter for outputting either all parses found, or justthe best parse found, and both parameter settingswere used.
The queries were then parsed by bothparsers and their logical forms were used to searchthe respective databases.
The experiment was re-peated using both the synonym and approximatesearch modes.Parser Precision4 Recall F-scoreConexor FDG 55.8% 8.9% 0.074LG?best 49.7% 11.4% 0.099LG?all 50.9% 13.1% 0.120Table 4: Averages per query in synonym mode.Parser Precision4 Recall F-scoreConexor FDG 28.3% 21.9% 0.177LG?best 31.8% 15.8% 0.150LG?all 40.5% 20.5% 0.183Table 5: Averages per query in approximate mode.4.2.2 ResultsPrecision, Recall and the F-score (with Preci-sion and Recall equally weighted) for each querywere calculated.3 When no results were returnedfor a query the precision could not be calculated,but the F-score is equal to zero.
The results areshown in Tables 4 and 5.
The number of times theresults for a query contained no relevant answersare shown in Table 6.The tables show that the approximate modegives better results than the synonym mode.
Thisis to be expected, since the synonym mode returnsexact matches only and therefore some questionsmay not produce any results.
For those questions,recall and F would be zero.
In fact, the number ofquestions without answers in the synonym modeis so large that the comparison between ConexorFDG and Link Grammar becomes unreliable inthis mode.
In this discussion, therefore, we willfocus on the approximate mode.The results returned by Link Grammar when allparses are considered are significantly better thanwhen only the first (i.e.
the best) parse is consid-3F was calculated using the expressionF = 2 ?
|returned and relevant||returned| + |relevant|which is equivalent to the usual formulation (with ?
= 1):F = (?2 + 1) ?
Precision ?
Recall?2Precision + Recall4Average over queries for which precision is defined, i.e.when the number of returns is non-zero.Parser Search mode NoresultsreturnedNothingrelevantreturnedCon.
FDG Synonym 20 20Con.
FDG Approximate 0 8LG?best Synonym 16 18LG?best Approximate 1 11LG?all Synonym 15 18LG?all Approximate 4 12Table 6: Numbers of times no relevant answerswere found.ered.
This shows that, in the answer extractiontask, it is better to use the logical forms of allpossible sentence interpretations.
Recall increasesand, remarkably, precision increases as well.
Thismeans that the system is more likely to includenew relevant answers when all parses are consid-ered.In many applications it is more practical to con-sider one parse only.
Conexor FDG, for example,returns one parse only, and the parsing speed com-parison (Section 3.2) shows an important differ-ence in parsing time.
If we compare Conexor FDGwith Link Grammar set to return just the best parse?
since Conexor FDG returns one parse only, thisis the fairest comparison ?
we can see that recallof the system using Conexor FDG is higher thanthat of the system using Link Grammar, while re-taining similar precision.5 DiscussionThe fairest extrinsic comparison between ConexorFDG and Link Grammar is the one that uses thebest parse returned by Link Grammar, and the an-swer extraction method follows the approximatemode.
With these settings, Conexor FDG pro-duces better results than Link Grammar.
However,the results of the extrinsic comparison are far lessdramatic than those of the intrinsic comparison,specially in the precision figures.One reason for the difference in the results isthat the intrinsic evaluation compares grammaticalrelation accuracy, whereas the answer extractionsystem used in the extrinsic evaluation uses logi-cal forms.
A preliminary inspection of the gram-matical relations and logical forms of questionsand correct answers shows that high overlap ofgrammatical relations does not translate into highoverlap of logical forms.
A reason for this differ-ence is that the semantic interpreters used in theextrinsic evaluation explore exhaustively the de-pendency structures returned by both parsing sys-tems and they try to recover as much informationas possible.
In contrast with this, the generators ofgrammatical relations used in the intrinsic evalua-tion provide the most direct mapping from depen-dency structures to grammatical relations.
For ex-ample, typically a dependency structure would notshow a long dependency like the subject of comein the sentence John wanted Mary to come:John wanted.v Mary to.o come.vSsTOoOs IAs a result, the grammatical relations would notshow the subject of come.
However, the subjectof come can be traced by following several de-pendencies (I, TOo and Os above) and ExtrAns?semantic interpreters do follow these dependen-cies.
In other words, the semantic interpretersuse more information than what is directly en-coded in the dependency structures.
Therefore,the logical forms contain richer information thanthe grammatical relations.
We decided not to op-timise the grammatical relations used in our eval-uation because we wanted to test the expressivityof the inherent grammars.
It would be question-able whether we should recover more informationthan what is directly expressed.
After all, providedthat the parse contains all the words in the origi-nal order, we can theoretically ignore the sentencestructure and still recover all the information.6 Summary and Further WorkWe have performed intrinsic evaluations ofparsers and extrinsic evaluations within thecontext of answer extraction.
These evaluationsstrengthen Galliers and Sparck Jones (1993)?sclaim that intrinsic evaluations are of very limitedvalue.
In particular, our evaluations show thatintrinsic evaluations may provide results thatare distorted with respect to the most intuitivepurpose of a parsing system: to deliver syntacticstructures to subsequent modules of practical NLPsystems.
There is a clear need for frameworks forextrinsic evaluations of parsers for different NLPapplications.Further research to confirm this conclusion willbe to try and minimise the occurrence of vari-ables in the experiments by using the same corpusfor both the intrinsic and the extrinsic evaluationsand/or by using an answer extraction system thatoperates on the level of grammatical relations in-stead of MLFs.
Additional further research willbe the use of other intrinsic evaluation methodolo-gies and extrinsic evaluations within the context ofvarious other embedding setups.AcknowledgementThis research is supported by the Macquarie Uni-versity New Staff grant MUNS?9601/0069.ReferencesSrinivas Bangalore, Anoop Sarkar, Christine Doran,and Beth Ann Hockey.
1998.
Grammar & parserevaluation in the XTAG project.
In Proc.
Workshopon the Evaluation of Parsing Systems, LREC98.Ezra Black, S.P.
Abney, D. Flickinger, C. Gdaniec,R.
Grisham, P. Harrison, D. Hindle, R. Ingria,F.
Jelinek, J. Klavans, M. Liberman, M.P.
Mar-cus, S. Roukos, B. Santorini, and T. Strzalkowski.1991.
A procedure for quantitatively comparing thesyntactic coverage of English grammars.
In Proc.DARPA Speech and Natural Language Workshop,pages 306?311, Pacific Grove, CA.
Morgan Kauf-mann.Ezra Black.
1996.
Evaluation of broad-coveragenatural-language parsers.
In Ronald A. Cole, JosephMariani, Hans Uszkoreit, Annie Zaenen, and VictorZue, editors, Survey of the State of the Art in Hu-man Language Technology, pages 488?490.
CSLU,Oregon Graduate Institute.John Carroll, Ted Briscoe, and Antonio Sanfilippo.1998.
Parser evaluation: a survey and a new pro-posal.
In Proc.
LREC98.John Carroll, G. Minnen, and T. Briscoe.
1999.
Corpusannotation for parser evaluation.Michael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proc.ACL.
Santa Cruz.Ann Copestake, Dan Flickinger, and Ivan A. Sag.1997.
Minimal recursion semantics: an introduc-tion.
Technical report, CSLI, Stanford University,Stanford, CA.Donald Davidson.
1967.
The logical form of actionsentences.
In Nicholas Rescher, editor, The Logic ofDecision and Action, pages 81?120.
Univ.
of Pitts-burgh Press.Christiane Fellbaum.
1998.
Wordnet: Introduction.
InChristiane Fellbaum, editor, WordNet: an electroniclexical database, Language, Speech, and Communi-cation, pages 1?19.
MIT Press, Cambrige, MA.Julia R. Galliers and Karen Sparck Jones.
1993.
Evalu-ating natural language processing systems.
Techni-cal Report TR-291, Computer Laboratory, Univer-sity of Cambridge.Jerry R. Hobbs.
1985.
Ontological promiscuity.
InProc.
ACL?85, pages 61?69.
University of Chicago,Association for Computational Linguistics.Timo Ja?rvinen and Pasi Tapanainen.
1997.
A depen-dency parser for english.
Technical Report TR-1,Department of Linguistics, University of Helsinki,Helsinki.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proc.
IJCAI-95, pages 1420?1425, Montreal, Canada.Diego Molla?, Rolf Schwitter, Michael Hess, andRachel Fournier.
2000.
Extrans, an answer extrac-tion system.
T.A.L., 41(2):495?522.Daniel D. Sleator and Davy Temperley.
1993.
ParsingEnglish with a link grammar.
In Proc.
Third Inter-national Workshop on Parsing Technologies, pages277?292.Richard F. E. Sutcliffe, Heinz-Detlev Koch, and An-nette McElligott, editors.
1996.
Industrial Parsingof Software Manuals.
Rodopi, Amsterdam.Pasi Tapanainen and Timo Ja?rvinen.
1997.
A non-projective dependency parser.
In Procs.
ANLP-97.ACL.Ellen M. Voorhees.
2001a.
Overview of the TREC2001 question answering track.
In Ellen M.Voorhees and Donna K. Harman, editors, Proc.TREC-10, number 500-250 in NIST Special Publi-cation.
NIST.Ellen M. Voorhees.
2001b.
The TREC questionanswering track.
Natural Language Engineering,7(4):361?378.
