Proceedings of the 12th Conference of the European Chapter of the ACL, pages 193?201,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsCorrecting Dependency Annotation ErrorsMarkus DickinsonIndiana UniversityBloomington, IN, USAmd7@indiana.eduAbstractBuilding on work detecting errors in de-pendency annotation, we set out to correctlocal dependency errors.
To do this, weoutline the properties of annotation errorsthat make the task challenging and theirexistence problematic for learning.
Forthe task, we define a feature-based modelthat explicitly accounts for non-relationsbetween words, and then use ambiguitiesfrom one model to constrain a second,more relaxed model.
In this way, we aresuccessfully able to correct many errors,in a way which is potentially applicable todependency parsing more generally.1 Introduction and MotivationAnnotation error detection has been explored forpart-of-speech (POS), syntactic constituency, se-mantic role, and syntactic dependency annotation(see Boyd et al, 2008, and references therein).Such work is extremely useful, given the harm-fulness of annotation errors for training, includingthe learning of noise (e.g., Hogan, 2007; Habashet al, 2007), and for evaluation (e.g., Padro andMarquez, 1998).
But little work has been doneto show the full impact of errors, or what typesof cases are the most damaging, important sincenoise can sometimes be overcome (cf.
Osborne,2002).
Likewise, it is not clear how to learn fromconsistently misannotated data; studies often onlynote the presence of errors or eliminate them fromevaluation (e.g., Hogan, 2007), and a previous at-tempt at correction was limited to POS annotation(Dickinson, 2006).
By moving from annotationerror detection to error correction, we can morefully elucidate ways in which noise can be over-come and ways it cannot.We thus explore annotation error correction andits feasibility for dependency annotation, a formof annotation that provides argument relationsamong words and is useful for training and testingdependency parsers (e.g., Nivre, 2006; McDonaldand Pereira, 2006).
A recent innovation in depen-dency parsing, relevant here, is to use the predic-tions made by one model to refine another (Nivreand McDonald, 2008; Torres Martins et al, 2008).This general notion can be employed here, as dif-ferent models of the data have different predictionsabout whch parts are erroneous and can highlightthe contributions of different features.
Using dif-ferences that complement one another, we can be-gin to sort accurate from inaccurate patterns, byintegrating models in such a way as to learn thetrue patterns and not the errors.
Although we focuson dependency annotation, the methods are poten-tially applicable for different types of annotation,given that they are based on the similar data repre-sentations (see sections 2.1 and 3.2).In order to examine the effects of errors andto refine one model with another?s information,we need to isolate the problematic cases.
Thedata representation must therefore be such that itclearly allows for the specific identification of er-rors between words.
Thus, we explore relativelysimple models of the data, emphasizing small sub-structures (see section 3.2).
This simple model-ing is not always rich enough for full dependencyparsing, but different models can reveal conflict-ing information and are generally useful as part ofa larger system.
Graph-based models of depen-dency parsing (e.g., McDonald et al, 2006), forexample, rely on breaking parsing down into deci-sions about smaller substructures, and focusing onpairs of words has been used for domain adapta-tion (Chen et al, 2008) and in memory-based pars-ing (Canisius et al, 2006).
Exploring annotationerror correction in this way can provide insightsinto more general uses of the annotation, just asprevious work on correction for POS annotation(Dickinson, 2006) led to a way to improve POS193tagging (Dickinson, 2007).After describing previous work on error detec-tion and correction in section 2, we outline in sec-tion 3 how we model the data, focusing on individ-ual relations between pairs of words.
In section 4,we illustrate the difficulties of error correction andshow how simple combinations of local featuresperform poorly.
Based on the idea that ambigui-ties from strict, lexical models can constrain moregeneral POS models, we see improvement in errorcorrection in section 5.2 Background2.1 Error detectionWe base our method of error correction on aform of error detection for dependency annota-tion (Boyd et al, 2008).
The variation n-gram ap-proach was developed for constituency-based tree-banks (Dickinson and Meurers, 2003, 2005) andit detects strings which occur multiple times inthe corpus with varying annotation, the so-calledvariation nuclei.
For example, the variation nu-cleus next Tuesday occurs three times in the WallStreet Journal portion of the Penn Treebank (Tay-lor et al, 2003), twice labeled as NP and once asPP (Dickinson and Meurers, 2003).Every variation detected in the annotation of anucleus is classified as either an annotation erroror as a genuine ambiguity.
The basic heuristicfor detecting errors requires one word of recur-ring context on each side of the nucleus.
The nu-cleus with its repeated surrounding context is re-ferred to as a variation n-gram.
While the originalproposal expanded the context as far as possiblegiven the repeated n-gram, using only the immedi-ately surrounding words as context is sufficient fordetecting errors with high precision (Boyd et al,2008).
This ?shortest?
context heuristic receivessome support from research on first language ac-quisition (Mintz, 2006) and unsupervised gram-mar induction (Klein and Manning, 2002).The approach can detect both bracketing and la-beling errors in constituency annotation, and wealready saw a labeling error for next Tuesday.
Asan example of a bracketing error, the variation nu-cleus last month occurs within the NP its biggestjolt last month once with the label NP and once asa non-constituent, which in the algorithm is han-dled through a special label NIL.The method for detecting annotation errors canbe extended to discontinuous constituency annota-tion (Dickinson and Meurers, 2005), making it ap-plicable to dependency annotation, where wordsin a relation can be arbitrarily far apart.
Specifi-cally, Boyd et al (2008) adapt the method by treat-ing dependency pairs as variation nuclei, and theyinclude NIL elements for pairs of words not an-notated as a relation.
The method is successfulat detecting annotation errors in corpora for threedifferent languages, with precisions of 93% forSwedish, 60% for Czech, and 48% for German.12.2 Error correctionCorrecting POS annotation errors can be done byapplying a POS tagger and altering the input POStags (Dickinson, 2006).
Namely, ambiguity classinformation (e.g., IN/RB/RP) is added to each cor-pus position for training, creating complex ambi-guity tags, such as <IN/RB/RP,IN>.
While thisresults in successful correction, it is not clear howit applies to annotation which is not positional anduses NIL labels.
However, ambiguity class infor-mation is relevant when there is a choice betweenlabels; we return to this in section 5.3 Modeling the data3.1 The dataFor our data set, we use the written portion (sec-tions P and G) of the Swedish Talbanken05 tree-bank (Nivre et al, 2006), a reconstruction of theTalbanken76 corpus (Einarsson, 1976) The writtendata of Talbanken05 consists of 11,431 sentenceswith 197,123 tokens, annotated using 69 types ofdependency relations.This is a small sample, but it matches thedata used for error detection, which results in634 shortest non-fringe variation n-grams, corre-sponding to 2490 tokens.
From a subset of 210nuclei (917 tokens), hand-evaluation reveals errordetection precision to be 93% (195/210), with 274(of the 917) corpus positions in need of correction(Boyd et al, 2008).
This means that 643 positionsdo not need to be corrected, setting a baseline of70.1% (643/917) for error correction.2 FollowingDickinson (2006), we train our models on the en-tire corpus, explicitly including NIL relations (see1The German experiment uses a more relaxed heuristic;precision is likely higher with the shortest context heuristic.2Detection and correction precision are different measure-ments: for detection, it is the percentage of variation nucleitypes where at least one is incorrect; for correction, it is thepercentage of corpus tokens with the true (corrected) label.194section 3.2); we train on the original annotation,but not the corrections.3.2 Individual relationsAnnotation error correction involves overcomingnoise in the corpus, in order to learn the truepatterns underlying the data.
This is a slightlydifferent goal from that of general dependencyparsing methods, which often integrate a vari-ety of features in making decisions about depen-dency relations (cf., e.g., Nivre, 2006; McDon-ald and Pereira, 2006).
Instead of maximizing afeature model to improve parsing, we isolate in-dividual pieces of information (e.g., context POStags), thereby being able to pinpoint, for example,when non-local information is needed for particu-lar types of relations and pointing to cases wherepieces of information conflict (cf.
also McDonaldand Nivre, 2007).To support this isolation of information, we usedependency pairs as the basic unit of analysis andassign a dependency label to each word pair.
Fol-lowing Boyd et al (2008), we add L or R to thelabel to indicate which word is the head, the left(L) or the right (R).
This is tantamount to han-dling pairs of words as single entries in a ?lex-icon?
and provides a natural way to talk of am-biguities.
Breaking the representation down intostrings whch receive a label also makes the methodapplicable to other annotation types (e.g., Dickin-son and Meurers, 2005).A major issue in generating a lexicon is howto handle pairs of words which are not dependen-cies.
We follow Boyd et al (2008) and generateNIL labels for those pairs of words which alsooccur as a true labeled relation.
In other words,only word pairs which can be relations can also beNILs.
For every sentence, then, when we producefeature lists (see section 3.3), we produce them forall word pairs that are related or could potentiallybe related, but not those which have never beenobserved as a dependency pair.
This selection ofNIL items works because there are no unknownwords.
We use the method in Dickinson and Meur-ers (2005) to efficiently calculate the NIL tokens.Focusing on word pairs and not attempting tobuild a a whole dependency graph allows us to ex-plore the relations between different kinds of fea-tures, and it has the potential benefit of not rely-ing on possibly erroneous sister relations.
Fromthe perspective of error correction, we cannot as-sume that information from the other relations inthe sentence is reliable.3 This representation alsofits nicely with previous work, both in error de-tection (see section 2.1) and in dependency pars-ing (e.g., Canisius et al, 2006; Chen et al, 2008).Most directly, Canisius et al (2006) integrate sucha representation into a memory-based dependencyparser, treating each pair individually, with wordsand POS tags as features.3.3 Method of learningWe employ memory-based learning (MBL) forcorrection.
MBL stores all corpus instances asvectors of features, and given a new instance, thetask of the classifier is to find the most similarcases in memory to deduce the best class.
Giventhe previous discussion of the goals of correctingerrors, what seems to be needed is a way to findpatterns which do not fully generalize because ofnoise appearing in very similar cases in the cor-pus.
As Zavrel et al (1997, p. 137) state about theadvantages of MBL:Because language-processing tasks typ-ically can only be described as a com-plex interaction of regularities, sub-regularities and (families of) exceptions,storing all empirical data as potentiallyuseful in analogical extrapolation worksbetter than extracting the main regulari-ties and forgetting the individual exam-ples (Daelemans, 1996).By storing all corpus examples, as MBL does,both correct and incorrect data is maintained, al-lowing us to pinpoint the effect of errors on train-ing.
For our experiments, we use TiMBL, version6.1 (Daelemans et al, 2007), with the default set-tings.
We use the default overlap metric, as thismaintains a direct connection to majority-basedcorrection.
We could run TiMBL with differentvalues of k, as this should lead to better featureintegration.
However, this is difficult to explorewithout development data, and initial experimentswith higher k values were not promising (see sec-tion 4.2).To fully correct every error, one could also ex-periment with a real dependency parser in the fu-ture, in order to look beyond the immediate con-text and to account for interactions between rela-3We use POS information, which is also prone to errors,but on a different level of annotation.
Still, this has its prob-lems, as discussed in section 4.1.195tions.
The approach to correction pursued here,however, isolates problems for assigning depen-dency structures, highlighting the effectiveness ofdifferent features within the same local domain.Initial experiments with a dependency parser wereagain not promising (see section 4.2).3.4 Integrating featuresWhen using features for individual relations, wehave different options for integrating them.
Onthe one hand, one can simply additively combinefeatures into a larger vector for training, as de-scribed in section 4.2.
On the other hand, one canuse one set of features to constrain another set,as described in section 5.
Pulling apart the fea-tures commonly employed in dependency parsingcan help indicate the contributions each has on theclassification.This general idea is akin to the notion of clas-sifier stacking, and in the realm of dependencyparsing, Nivre and McDonald (2008) successfullystack classifiers to improve parsing by ?allow[ing]a model to learn relative to the predictions of theother?
(p. 951).
The output from one classifieris used as a feature in the next one (see also Tor-res Martins et al, 2008).
Nivre and McDonald(2008) use different kinds of learning paradigms,but the general idea can be carried over to a situ-ation using the same learning mechanism.
Insteadof focusing on what one learning algorithm in-forms another about, we ask what one set of moreor less informative features can inform another setabout, as described in section 5.1.4 Performing error correction4.1 ChallengesThe task of automatic error correction in somesense seems straightforward, in that there are nounknown words.
Furthermore, we are looking atidentical recurring words, which should for themost part have consistent annotation.
But it is pre-cisely this similarity of local contexts that makesthe correction task challenging.Given that variations contain sets of corpus po-sitions with differing labels, it is tempting to takethe error detection output and use a heuristic of?majority rules?
for the correction cases, i.e., cor-rect the cases to the majority label.
When us-ing only information from the word sequence, thisruns into problems quickly, however, in that thereare many non-majority labels which are correct.Some of these non-majority cases pattern in uni-form ways and are thus more correctable; oth-ers are less tractable in being corrected, as theybehave in non-uniform and often non-local ways.Exploring the differences will highlight what canand cannot be easily corrected, underscoring thedifficulties in training from erroneous annotation.Uniform non-majority cases The first problemwith correction to the majority label is an issueof coverage: a large number of variations are tiesbetween two different labels.
Out of 634 shortestnon-fringe variation nuclei, 342 (53.94%) have nomajority label; for the corresponding 2490 tokens,749 (30.08%) have no majority tag.The variation a?r va?g (?is way?
), for example, ap-pears twice with the same local context shown in(1),4 once incorrectly labeled as OO-L (other ob-ject [head on the left]) and once correctly as SP-L (subjective predicative complement).
To dis-tinguish these two, more information is necessarythan the exact sequence of words.
In this case, forexample, looking at the POS categories of the nu-clei could potentially lead to accurate correction:AV NN is SP-L 1032 times and OO-L 32 times(AV = the verb ?vara?
(be), NN = other noun).While some ties might require non-local informa-tion, we can see that local?but more general?information could accurately break this tie.
(1) ka?rlekenslove?sva?gwaya?r/AVisenala?nglongva?g/NNwayochand.
.
.. .
.Secondly, in a surprising number of cases wherethere is a majority tag (122 out of the 917 tokenswe have a correction for), a non-majority labelis actually correct.
For the example in (2), thestring institution kvarleva (?institution remnant?
)varies between CC-L (sister of first conjunct in bi-nary branching analysis of coordination) and AN-L (apposition).5 CC-L appears 5 times and AN-L3 times, but the CC-L cases are incorrect and needto be changed to AN-L.(2) enanfo?ra?ldradobsoleteinstitution/NNinstitution,/IK,en/ENakvarleva/NNremnantfra?nfrom1800-taletsthe 1800s4We put variation nuclei in bold and underline the imme-diately surrounding context.5Note that CC is a category introduced in the conversionfrom the 1976 to the 2005 corpus.196Other cases with a non-majority label haveother problems.
In example (3), for instance, thestring under ha?gnet (?under protection?)
varies inthis context between HD-L (other head, 3 cases)and PA-L (complement of preposition, 5 cases),where the PA-L cases need to be corrected to HD-L.
Both of these categories are new, so part of theissue here could be in the consistency of the con-version.
(3) friafreelivlifeunder/PRunderha?gnet/ID|NNthe protectionav/ID|PRofettaenonega?ngtimegivetgivenlo?ftepromiseThe additional problem is that there are other,correlated errors in the analysis, as shown in fig-ure 1.
In the case of the correct HD analysis, bothha?gnet and av are POS-annotated as ID (part of id-iom (multi-word unit)) and are HD dependents ofunder, indicating that the three words make up anidiom.
The PA analysis is a non-idiomatic analy-sis, with ha?gnet as NN.AT ET HD HDfria liv under ha?gnet av ...AJ NN PR ID IDAT ET PA PAfria liv under ha?gnet av ...AJ NN PR NN PRFigure 1: Erroneous POS & dependency variationSignificantly, ha?gnet only appears 10 times inthe corpus, all with under as its head, 5 times HD-L and 5 times PA-L. We will not focus explicitlyon correcting these types of cases, but the exampleserves to emphasize the necessity of correction atall levels of annotation.Non-uniform non-majority cases All of theabove cases have in common that whatever changeis needed, it needs to be done for all positions in avariation.
But this is not sound, as error detectionprecision is not 100%.
Thus, there are variationswhich clearly must not change.For example, in (4), there is legitimate varia-tion between PA-L (4a) and HD-L (4b), stemmingfrom the fact that one case is non-idiomatic, andthe other is idiomatic, despite having identical lo-cal context.
In these examples, at least the POSlabels are different.
Note, though, that in (4) weneed to trust the POS labels to overcome the simi-larity of text, and in (3) we need to distrust them.6(4) a. Med/PRwithandraotherord/NNwordsenana?ndama?lsenligappropriate...b. Med/ABwithandraotherord/IDwordsenaformformavofprostitutionprostitution.Without non-local information, some legitimatevariations are virtually irresolvable.
Consider (5),for instance: here, we find variation between SS-R(other subject), as in (5a), and FS-R (dummy sub-ject), as in (5b).
Crucially, the POS tags are thesame, and the context is the same.
What differen-tiates these cases is that ga?r has a different set ofdependents in the two sentences, as shown in fig-ure 2; to use this information would require us totrust the rest of the dependency structure or to usea dependency parser which accurately derives thestructural differences.
(5) a.
Det/POitga?r/VVgoesbarajustintenotihoptogether.
?It just doesn?t add up.?b.
Det/POitga?r/VVgoesbarajustintenotatttoha?llaholdihoptogether......4.2 Using local informationWhile some variations require non-local informa-tion, we have seen that some cases are correctablesimply with different kinds of local information(cf.
(1)).
In this paper, we will not attempt todirectly cover non-local cases or cases with POSannotation problems, instead trying to improve theintegration of different pieces of local information.In our experiments, we trained simple models ofthe original corpus using TiMBL (see section 3.3)and then tested on the same corpus.
The modelswe use include words (W) and/or tags (T) for nu-cleus and/or context positions, where context here6Rerunning the experiments in the paper by first runninga POS tagger showed slight degradations in precision.197SS MA NA PLDet ga?r bara inte ihopPO VV AB AB ABFS CA NA IM ESDet ga?r bara inte att ha?lla ...PO VV AB AB IM VVFigure 2: Correct dependency variationrefers only to the immediately surrounding words.These are outlined in table 1, for different mod-els of the nucleus (Nuc.)
and the context (Con.
).For instance, the model 6 representation of exam-ple (6) (=(1)) consists of all the underlined wordsand tags.
(6) ka?rlekens va?g/NN a?r/AV en/EN la?ng/AJva?g/NN och/++ man go?r oklokt ...In table 1, we report the precision figures fordifferent models on the 917 positions we havecorrections for.
We report the correction preci-sion for positions the classifier changed the labelof (Changed), and the overall correction precision(Overall).
We also report the precision TiMBL hasfor the whole corpus, with respect to the originaltags (instead of the corrected tags).# Nuc.
Con.
TiMBL Changed Overall1 W - 86.6% 34.0% 62.5%2 W, T - 88.1% 35.9% 64.8%3 W W 99.8% 50.3% 72.7%4 W W, T 99.9% 52.6% 73.5%5 W, T W 99.9% 50.8% 72.4%6 W, T W, T 99.9% 51.2% 72.6%7 T - 73.4% 20.1% 49.5%8 T T 92.7% 50.2% 73.2%Table 1: The models testedWe can draw a few conclusions from these re-sults.
First, all models using contexual informa-tion perform essentially the same?approximately50% on changed positions and 73% overall.
Whennot generalizing to new data, simply adding fea-tures (i.e., words or tags) to the model is less im-portant than the sheer presence of context.
Thisis true even for some higher values of k: model6, for example, has only 73.2% and 72.1% overallprecision for k = 2 and k = 3, respectively.Secondly, these results confirm that the task isdifficult, even for a corpus with relatively high er-ror detection precision (see section 2.1).
Despitehigh similarity of context (e.g., model 6), the bestresults are only around 73%, and this is given abaseline (no changes) of 70%.
While a more ex-pansive set of features would help, there are otherproblems here, as the method appears to be over-training.
There is no question that we are learningthe ?correct?
patterns, i.e., 99.9% similarity to thebenchmark in the best cases.
The problem is that,for error correction, we have to overcome noise inthe data.
Training and testing with the dependencyparser MaltParser (Nivre et al, 2007, default set-tings) is no better, with 72.1% overall precision(despite a labeled attachment score of 98.3%).Recall in this light that there are variations forwhich the non-majority label is the correct one;attempting to get a non-majority label correct us-ing a strict lexical model does not work.
To beable not to learn the erroneous patterns requiresa more general model.
Interestingly, a more gen-eral model?e.g., treating the corpus as a sequenceof tags (model 8)?results in equally good correc-tion, without being a good overall fit to the cor-pus data (only 92.7%).
This model, too, learnsnoise, as it misses cases that the lexical models getcorrect.
Simply combining the features does nothelp (cf.
model 6); what we need is to use infor-mation from both stricter and looser models in away that allows general patterns to emerge with-out overgeneralizing.5 Model combinationGiven the discussion in section 4.1 surroundingexamples (1)-(5), it is clear that the informationneeded for correction is sometimes within theimmediate context, although that information isneeded, however, is often different.
Consider themore general models, 7 and 8, which only use POStag information.
While sometimes this general in-formation is effective, at times it is dramaticallyincorrect.
For example, for (7), the original (incor-rect) relation between finna and erbjuda is CC-L;the model 7 classifier selects OO-L as the correcttag; model 8 selects NIL; and the correct label is+F-L (coordination at main clause level).198(7) fo?rso?kertryfinna/VVto findettala?mpligtsuitablearbetejobiino?ppnaopenmarknadenmarketellerorerbjuda/VVto offerandraotherarbetsmo?jligheterwork possibilities.The original variation for the nucleus finna erb-juda (?find offer?)
is between CC-L and +F-L, butwhen represented as the POS tags VV VV (otherverb), there are 42 possible labels, with OO-L be-ing the most frequent.
This allows for too muchconfusion.
If model 7 had more restrictions on theset of allowable tags, it could make a more sensi-ble choice and, in this case, select the correct label.5.1 Using ambiguity classesPrevious error correction work (Dickinson, 2006)used ambiguity classes for POS annotation, andthis is precisely the type of information we needto constrain the label to one which we know is rel-evant to the current case.
Here, we investigate am-biguity class information derived from one modelintegrated into another model.There are at least two main ways we can useambiguity classes in our models.
The first is whatwe have just been describing: an ambiguity classcan serve as a constraint on the set of possible out-comes for the system.
If the correct label is in theambiguity class (as it usually is for error correc-tion), this constraining can do no worse than theoriginal model.
The other way to use an ambigu-ity class is as a feature in the model.
The successof this approach depends on whether or not eachambiguity class patterns in its own way, i.e., de-fines a sub-regularity within a feature set.5.2 Experiment detailsWe consider two different feature models, thosecontaining only tags (models 7 and 8), and addto these ambiguity classes derived from two othermodels, those containing only words (models 1and 3).
To correct the labels, we need modelswhich do not strictly adhere to the corpus, and thetag-based models are best at this (see the TiMBLresults in table 1).
The ambiguity classes, how-ever, must be fairly constrained, and the word-based models do this best (cf.
example (7)).5.2.1 Ambiguity classes as constraintsAs described in section 5.1, we can use ambiguityclasses to constrain the output of a model.
Specif-ically, we take models 7 and 8 and constrain eachselected tag to be one which is within the ambi-guity class of a lexical model, either 1 or 3.
Thatis, if the TiMBL-determined label is not in the am-biguity class, we select the most likely tag of theones which are.
If no majority label can be de-cided from this restricted set, we fall back to theTiMBL-selected tag.
In (7), for instance, if we usemodel 7, the TiMBL tag is OO-L, but model 3?sambiguity class restricts this to either CC-L or +F-L. For the representation VV VV, the label CC-Lappears 315 times and +F-L 544 times, so +F-L iscorrectly selected.7The results are given in table 2, which can becompared to the the original models 7 and 8 in ta-ble 1, i.e., total precisions of 49.5% and 73.2%,respectively.
With these simple constraints, model8 now outperforms any other model (75.5%), andmodel 7 begins to approach all the models that usecontextual information (68.8%).# AC Changed Total7 1 28.5% (114/400) 57.4% (526/917)7 3 45.9% (138/301) 68.8% (631/917)8 1 54.0% (142/263) 74.8% (686/917)8 3 56.7% (144/254) 75.5% (692/917)Table 2: Constraining TiMBL with ACs5.2.2 Ambiguity classes as featuresAmbiguity classes from one model can also beused as features for another (see section 5.1); inthis case, ambiguity class information from lexicalmodels (1 and 3) is used as a feature for POS tagmodels (7 and 8).
The results are given in table 3,where we can see dramatically improved perfor-mance from the original models (cf.
table 1) andgenerally improved performance over using ambi-guity classes as constraints (cf.
table 2).# AC Changed Total7 1 33.2% (122/368) 61.9% (568/917)7 3 50.2% (131/261) 72.1% (661/917)8 1 59.0% (148/251) 76.4% (701/917)8 3 55.1% (130/236) 73.6% (675/917)Table 3: TiMBL with ACs as featuresIf we compare the two results for model 7(61.9% vs. 72.1%) and then the two results formodel 8 (76.4% vs. 73.6%), we observe that the7Even if CC-L had been selected here, the choice is sig-nificantly better than OO-L.199better use of ambiguity classes integrates contex-tual and non-contextual features.
Model 7 (POS,no context) with model 3 ambiguity classes (lex-ical, with context) is better than using ambiguityclasses derived from a non-contextual model.
Formodel 8, on the other hand, which uses contextualPOS features, using the ambiguity class withoutcontext (model 1) does better.
In some ways, thiscombination of model 8 with model 1 ambiguityclasses makes the most sense: ambiguity classesare derived from a lexicon, and for dependency an-notation, a lexicon can be treated as a set of pairsof words.
It is also noteworthy that model 7, de-spite not using context directly, achieves compara-ble results to all the previous models using context,once appropriate ambiguity classes are employed.5.2.3 Both methodsGiven that the results of ambiguity classes as fea-tures are better than that of constraining, we cannow easily combine both methodologies, by con-straining the output from section 5.2.2 with theambiguity class tags.
The results are given in ta-ble 4; as we can see, all results are a slight im-provement over using ambiguity classes as fea-tures without constraining the output (table 3).
Us-ing only local context, the best model here is 3.2%points better than the best original model, repre-senting an improvement in correction.# AC Changed Total7 1 33.5% (123/367) 62.2% (570/917)7 3 55.8% (139/249) 74.1% (679/917)8 1 59.6% (149/250) 76.7% (703/917)8 3 57.1% (133/233) 74.3% (681/917)Table 4: TiMBL w/ ACs as features & constraints6 Summary and OutlookAfter outlining the challenges of error correction,we have shown how to integrate information fromdifferent models of dependency annotation in or-der to perform annotation error correction.
By us-ing ambiguity classes from lexical models, both asfeatures and as constraints on the final output, wesaw improvements in POS models that were ableto overcome noise, without using non-local infor-mation.A first step in further validating these methodsis to correct other dependency corpora; this is lim-ited, of course, by the amount of corpora with cor-rected data available.
Secondly, because this workis based on features and using ambiguity classes, itcan in principle be applied to other types of anno-tation, e.g., syntactic constituency annotation andsemantic role annotation.
In this light, it is inter-esting to note the connection to annotation errordetection: the work here is in some sense an ex-tension of the variation n-gram method.
Whetherit can be employed as an error detection system onits own requires future work.Another way in which this work can be ex-tended is to explore how these representations andintegration of features can be used for dependencyparsing.
There are several issues to work out, how-ever, in making insights from this work more gen-eral.
First, it is not clear that pairs of words are suf-ficiently general to treat them as a lexicon, whenone is parsing new data.
Secondly, we have ex-plicit representations for word pairs not annotatedas a dependency relation (i.e., NILs), and these areconstrained by looking at those which are the samewords as real relations.
Again, one would have todetermine which pairs of words need NIL repre-sentations in new data.AcknowledgementsThanks to Yvonne Samuelsson for help with theSwedish examples; to Joakim Nivre, Mattias Nils-son, and Eva Pettersson for the evaluation data forTalbanken05; and to the three anonymous review-ers for their insightful comments.ReferencesBoyd, Adriane, Markus Dickinson and DetmarMeurers (2008).
On Detecting Errors in Depen-dency Treebanks.
Research on Language andComputation 6(2), 113?137.Canisius, Sander, Toine Bogers, Antal van denBosch, Jeroen Geertzen and Erik Tjong KimSang (2006).
Dependency parsing by infer-ence over high-recall dependency predictions.In Proceedings of CoNLL-X.
New York.Chen, Wenliang, Youzheng Wu and Hitoshi Isa-hara (2008).
Learning Reliable Information forDependency Parsing Adaptation.
In Proceed-ings of Coling 2008.
Manchester.Daelemans, Walter (1996).
Abstraction Consid-ered Harmful: Lazy Learning of Language Pro-cessing.
In Proceedings of the 6th Belgian-Dutch Conference on Machine Learning.
Maas-tricht, The Netherlands.200Daelemans, Walter, Jakub Zavrel, Ko Van derSloot and Antal Van den Bosch (2007).
TiMBL:Tilburg Memory Based Learner, version 6.1,Reference Guide.
Tech.
rep., ILK ResearchGroup.
ILK Research Group Technical ReportSeries no.
07-07.Dickinson, Markus (2006).
From Detecting Errorsto Automatically Correcting Them.
In Proceed-ings of EACL-06.
Trento, Italy.Dickinson, Markus (2007).
Determining Ambigu-ity Classes for Part-of-Speech Tagging.
In Pro-ceedings of RANLP-07.
Borovets, Bulgaria.Dickinson, Markus and W. Detmar Meurers(2003).
Detecting Inconsistencies in Treebanks.In Proceedings of TLT-03.
Va?xjo?, Sweden.Dickinson, Markus and W. Detmar Meurers(2005).
Detecting Errors in DiscontinuousStructural Annotation.
In Proceedings of ACL-05.Einarsson, Jan (1976).
Talbankens skrift-spr?akskonkordans.
Tech.
rep., Lund Univer-sity, Dept.
of Scandinavian Languages.Habash, Nizar, Ryan Gabbard, Owen Rambow,Seth Kulick and Mitch Marcus (2007).
Deter-mining Case in Arabic: Learning Complex Lin-guistic Behavior Requires Complex LinguisticFeatures.
In Proceedings of EMNLP-07.Hogan, Deirdre (2007).
Coordinate Noun PhraseDisambiguation in a Generative Parsing Model.In Proceedings of ACL-07.
Prague.Klein, Dan and Christopher D. Manning (2002).
AGenerative Constituent-Context Model for Im-proved Grammar Induction.
In Proceedings ofACL-02.
Philadelphia, PA.McDonald, Ryan, Kevin Lerman and FernandoPereira (2006).
Multilingual Dependency Anal-ysis with a Two-Stage Discriminative Parser.
InProceedings of CoNLL-X.
New York City.McDonald, Ryan and Joakim Nivre (2007).
Char-acterizing the Errors of Data-Driven Depen-dency Parsing Models.
In Proceedings ofEMNLP-CoNLL-07.
Prague, pp.
122?131.McDonald, Ryan and Fernando Pereira (2006).Online learning of approximate dependencyparsing algorithms.
In Proceedings of EACL-06.
Trento.Mintz, Toben H. (2006).
Finding the verbs: dis-tributional cues to categories available to younglearners.
In K. Hirsh-Pasek and R. M.
Golinkoff(eds.
), Action Meets Word: How Children LearnVerbs, New York: Oxford University Press, pp.31?63.Nivre, Joakim (2006).
Inductive DependencyParsing.
Berlin: Springer.Nivre, Joakim, Johan Hall, Jens Nilsson, AtanasChanev, Gulsen Eryigit, Sandra Kubler, Sve-toslav Marinov and Erwin Marsi (2007).
Malt-Parser: A language-independent system fordata-driven dependency parsing.
Natural Lan-guage Engineering 13(2), 95?135.Nivre, Joakim and Ryan McDonald (2008).
Inte-grating Graph-Based and Transition-Based De-pendency Parsers.
In Proceedings of ACL-08:HLT .
Columbus, OH.Nivre, Joakim, Jens Nilsson and Johan Hall(2006).
Talbanken05: A Swedish Treebankwith Phrase Structure and Dependency Annota-tion.
In Proceedings of LREC-06.
Genoa, Italy.Osborne, Miles (2002).
Shallow Parsing usingNoisy and Non-Stationary Training Material.
InJMLR Special Issue on Machine Learning Ap-proaches to Shallow Parsing, vol.
2, pp.
695?719.Padro, Lluis and Lluis Marquez (1998).
On theEvaluation and Comparison of Taggers: the Ef-fect of Noise in Testing Corpora.
In Proceed-ings of ACL-COLING-98.
San Francisco, CA.Taylor, Ann, Mitchell Marcus and Beatrice San-torini (2003).
The Penn Treebank: AnOverview.
In Anne Abeille?
(ed.
), Treebanks:Building and using syntactically annotated cor-pora, Dordrecht: Kluwer, chap.
1, pp.
5?22.Torres Martins, Andre?
Filipe, Dipanjan Das,Noah A. Smith and Eric P. Xing (2008).
Stack-ing Dependency Parsers.
In Proceedings ofEMNLP-08.
Honolulu, Hawaii, pp.
157?166.Zavrel, Jakub, Walter Daelemans and Jorn Veensta(1997).
Resolving PP attachment Ambiguitieswith Memory-Based Learning.
In Proceedingsof CoNLL-97.
Madrid.201
