Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 37?42,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsConcrete Assignments for Teaching NLP in an M.S.
ProgramReva FreedmanDepartment of Computer ScienceNorthern Illinois UniversityDeKalb, IL 60115freedman@cs.niu.eduAbstractThe professionally oriented computerscience M.S.
students at Northern IllinoisUniversity are intelligent, interested innew ideas, and have good programmingskills and a good math background.However, they have no linguisticsbackground, find traditional academicprose difficult and uninteresting, and havehad no exposure to research.
Given thispopulation, the assignments I have foundmost successful in teaching Introduction toNLP involve concrete projects wherestudents could see for themselves thephenomena discussed in class.
This paperdescribes three of my most successfulassignments: duplicating Kernighan etal.
?s Bayesian approach to spellingcorrection, a study of Greenberg?suniversals in the student?s native language,and a dialogue generation project.
Foreach assignment I discuss what thestudents learned and why the assignmentwas successful.1 IntroductionNorthern Illinois University is a large publicuniversity (25,000 students) located in the farm-oriented exurbs of Chicago, about 60 miles west ofthe city.
Most of the undergraduate computerscience majors and about a third of the  M.S.students come from the hi-tech corridor west ofChicago or small towns near the university.
Theremaining M.S.
students are international students,currently mostly from India.This paper discusses my experiences in twosemesters of teaching Introduction to NLP andthree semesters of teaching an NLP unit in anIntroduction to Artificial Intelligence course.Because the students have no background inlinguistics and are not used to reading the type ofacademic prose found in the textbook (Jurafskyand Martin, 2000), the most successful units I havetaught involved concrete assignments wherestudents could see for themselves the phenomenadiscussed in class.
Successful assignments also didnot assume any background in linguistics, evensuch basic notions as part of speech.To provide an overview of the field, each yearthe NLP course contains three segments: one on astatistical approach to NLP, one on syntax, andone on a logic-based approach.
The three segmentsare also chosen to include topics from phonologyand morphology, syntax, and pragmatics.
Thespecific content changes from year to year in aneffort to find topics that both represent currentissues in the field and capture the students?imagination.This paper describes three of the mostsuccessful assignments.
For each one, I describethe assignment, the topics the students learned, andwhy it was successful.
The three assignments are:duplicating Kernighan et al?s Bayesian approach37to spelling correction, a study of Greenberg?suniversals in a language other than English(usually the student?s native language), and adialogue generation project using my researchsoftware.2 Background2.1 Student demographicsMost of the students taking Introduction to NLPare graduate students, although undergraduates areeligible if they have had three semesters of C++programming.
Graduate students in cognitivescience-related fields, such as psychology orlinguistics, are eligible if they have taken onesemester of programming and are willing to teachthemselves about trees.
I actively recruit non-computer science students because it makes thecourse more interesting.
In addition to providing abroader spectrum of interests, they tend to be moreoutgoing.
They tend to be more willing to answerquestions in class, and also to ask questions inclass, which many of the computer sciencestudents will not do.The preferred career path among the students isto obtain a programming job in local industry,preferably in a hi-tech area.
However, among bothundergraduates and graduate students, a fewcontinue their education.
One minority studentwith no previous experience in research becameinterested and is now planning to apply to a PhDprogram.
In general, students take the course outof a desire to do something different from thenormal operating systems, networking anddatabase courses.
An occasional student also takesthe course because it fits in their schedule orbecause it doesn?t have onerous prerequisites.In general, the international students have goodto near-native competence in spoken English,although a few cannot not follow my lectures, andsome do not have sufficient writing skills for anessay exam.
All could read my lecture noteswithout difficulty.
Both among the internationalstudents and local students, many do not havesufficient experience with formal academic proseto understand the textbook (Jurafsky and Martin,2000).
Students?
first languages have includedTelugu, Hindi/Urdu, Nepali, Chinese (Mandarin),and Bulgarian.2.2 Student backgroundKoedinger (2001), in his research on tutoringsystems for high school mathematics, gives thefollowing as his fundamental principle: ?thestudent is not like me.?
In particular, studentbackground frequently did not include thefollowing items:1) Parts of speech2) Basic English grammar3) Relationships between languages and languagefamilies4) Practical issues, such as the importance oftransliteration and glossing5) Philosophical issues, such as the fact that thereis no single authoritative grammar of a naturallanguage or that one language is not moredifficult than another in an absolute senseHowever, students were talented at and enjoyedprogramming.
Most students also had a good mathbackground.
Finally, they were enthusiastic aboutlearning new things, as long as it involved concreteexamples that they could work out and a sampleproblem with a solution that they could use as amodel.3 Spelling correction3.1 BackgroundThe goal of the first section of the course was toshow the students the power of statistical methodsin NLP.
In this section, students were asked toduplicate the calculations used in Kernighan etal.
?s (1990) Bayesian approach to spellingcorrection, as explained in Section 5.5 of thetextbook.Kernighan et al choose as the preferredcorrection the one that maximizes P(t|c)P(c),where t is the typo and c is a candidate correction.Candidate corrections are generated by assumingthat errors involve only one letter or thetransposition of two adjacent letters.
To reproducethis calculation, students need the confusionmatrices provided in the original paper, a source ofunigram and bigram data, and a source for wordfrequencies.383.2 AssignmentStudents are given some misspelled words andpossible corrections, such as the followingexamples from Kernighan et almisspelled word possible correctionsambitios  ambitiousambitionsambitionFor each of these misspelled words, students areasked to do the following:a)  Use the method described by Kernighan et al,or equivalently in section 5.5 of the text, to findthe probability of each possible correction.b)  Use their preferred spell checker (MicrosoftWord, Unix ispell, etc.)
to generate possiblecorrections for the same misspelled words.The following questions are asked for eachmisspelled word:  Is the most probable correction according toKernighan?s algorithm the same as the onesuggested by your program?  Which additional possible corrections (i.e., non-single-error corrections or non-single wordcorrections) does your program generate?  Which of Kernighan?s possible corrections doesyour program omit?Since Kernighan?s original paper omits theunigram and bigram count matrices, I provide afile with this information.
Students are encouragedto find a source for word frequencies on the Web.As one option, I suggest they use any searchengine (e.g., Google), after class discussion aboutthe approximations involved in this approach.Students are also given two summary questionsto answer:  A former student, Mr.
I. M. Kluless, says: Idon?t see the point of using the frequency ofpotential corrections in the corpus (i.e., the priorprobability) as part of Kernighan?s algorithm.
Iwould just use the likelihood of a given error.
Howwould you answer Mr. Kluless?
(One way to thinkabout this question is: what would happen if youleft it out?
)  Another former student, Ms. U. R. Useless, says:I don?t see the point of using the likelihood of agiven error as part of Kernighan?s algorithm.
Iwould just use the prior probability.
How wouldyou answer Ms. Useless?3.3 ResultsStudents enjoyed this assignment because it wasstraightforward and used mathematics they werefamiliar with.
They were uniformly surprised todiscover that spelling correction is generally donetoday using Bayesian concepts rather than bydictionary lookup alone.
They were also surprisedto learn that learn that results were largelyindependent of the corpus chosen.
Students whoalready knew Bayes?
theorem learned about anapplication completely different from the onesthey had used in other courses.The majority of students used my suggestion toapproximate word frequencies in a corpus by pagecounts in Google.
They were surprised to learnthat in spite of the number of ways in which theweb differs from an ideal corpus, the volume ofdata available ensures that accurate results are stillobtained.
The better students searched the web forcorpora they preferred, including the works ofShakespeare and an online interface to the BritishNational Corpus(http://sara.natcorp.ox.ac.uk/lookup.html).4 Syntax and language universals4.1 BackgroundThe second section of the course had as its goal toteach the students some basic aspects of syntax.
Istarted with parts of speech and basic concepts ofcontext-free grammars.
I then introducedunification grammars as a way of obtaining morepower with fewer rules.As a way of showing the syntactic variationamong languages, I also introduced some ofGreenberg?s word order universals (Greenberg,1966), following the exposition in Baker (2001).Although identifying the most probable underlyingword order (SVO, etc.)
of an unknown languagecan involve significant linguistic intuition, I didnot expect students to achieve that goal.
Rather, Iused Greenberg?s ideas to make students think39about the rules they were generating instead ofgenerating S --> NP VP by rote.
Additionally, theuse of multiple languages contributed to theuniversity?s goal of introducing ideas ofinternationalization and diversity in classes wherefeasible.4.2 AssignmentThe students were asked to prepare a 15-minuteclass presentation showing two or three interestingphenomena of one of the languages of the world.Most students used their native language.They were asked to include the followinginformation: Where the language fits in Greenberg?sclassification (SVO, etc.
) One or more syntactic phenomena that makethe language interesting A grammar fragment (a set of CFG rules,possibly with unification-based features)illustrating one of the chosen phenomenaThey could show several interesting phenomenawith a short implementation of one, a complexphenomenon and a longer fragment of grammar, orone interesting phenomenon and multiple ways toimplement it.For each example they used, they were requiredto show the original transliterated into the Romanalphabet, a morpheme-level analysis, and atranslation into English.As a template, I gave a presentation using alanguage none of them had been exposed to,modern Hebrew.
The four sample phenomena Ipresented were: a) there is no indefinite article,b) nouns and adjectives must agree in gender andnumber, c) adjectives follow the noun, and d) thedefinite article is attached to every adjective in anNP as well as to the noun.In addition to providing an example of the scoperequired, the presentation also introduced thestudents to conventions of linguistic presentation,including interlinear display of transliteration,morpheme analysis, and translation.
One slidefrom my presentation is shown below:he- khatul   ha- gadolDET cat-M-S  DET big-M-S?the big cat?he- khatulim   ha- g?dolimDET cat-M-PL   DET big-M-PL?the big cats?4.3 ResultsThis assignment was useful for ensuring thatstudents had a basic grasp of many elements ofsyntax covered in Section II of the textbook,including parts of speech, context-free grammars,and unification grammars.
Second, the classpresentations provided students concrete examplesof some major syntactic concepts that alllanguages share, as well as some of thedifferences.
Finally, this assignment enabledstudents to learn about and present some of thecore linguistic features of their native language.5 Dialogue generation5.1 BackgroundThe third segment of the course had as its goal toshow how a logic-based approach is useful inNLP.
Since some of my previous work involvesimplementing dialogue software using a logic-based approach, dialogue systems was a naturalchoice for this segment.Phenomena discussed in lecture included theconcepts of speech act and discourse intention, therelationship between syntactic form and intention,direct and indirect speech acts, and a shortintroduction to dialogue act classification.As a counterbalance to the more theoreticalmaterial from Greenberg, this section includedsome information about current commercial usesof NLP.
Students were asked to read an articlefrom the popular press (Mount, 2005) describingexperiences with currently available commercialsystems.I used my own software, APE (Freedman,2000), a domain-independent dialogue planinterpreter based on reactive planning concepts.APE uses a rule-based macro languageimplemented in Common Lisp.
It is a hierarchicaltask network (HTN) style planner, achieving eachgoal via a series of subgoals.
APE?s high-levelplanning loop alternates between waiting for userinput and planning responses.
It executes planoperators until a primitive, non-decomposable one40is obtained.
In addition to elicit and inform, planscan also include primitives to query and updateAPE?s internal knowledge base, thus giving thesystem a ?mind.?
Primitives are added to a bufferuntil a primitive requiring a response from the useris received.
At that point the operators in thebuffer are used to build the output text.
Goals arerepresented using first-order logic withoutquantifiers, with full unification used formatching.APE provides two ways to change a plan inprogress.
The author can instruct the system eitherto switch from one method of satisfying a goal toanother or to add new goals at the top of theagenda, possibly replacing existing goals.
Thelatter facility is particularly useful in dialoguegeneration, since it allows the system to promptthe user after an error.
This feature makes APEmore powerful than the pushdown automaton onemight use to implement a context-free grammar.In addition, APE is obviously more powerful thanthe finite-state machines often used in dialoguegeneration.Use of APE allows students to generate realistichierarchically structured conversations with areasonable number of rules.5.2 AssignmentSample code presented in class involved lookingup data in a database of presidents?
names.
Thesample system prompted the user for input, thenprovided answers, error messages, and re-promptsas appropriate.
As an illustration of the power ofthe approach, I also demonstrated some of myresearch software, which showed conversationsembedded in a variety of front-end GUIs.For the assignment, students were asked tochoose their own topic.
They were asked to choosea problem, then provide a database layout anddraw a graph showing the possible conversationstheir system could generate.
Finally, they wereasked to implement the code.
At the end of thesemester, students made a five-minute presentationto the class showing their application.5.3 ResultsStudents greatly enjoyed this assignment becauseit involved the activity they enjoyed most, namelyprogramming.
Even though it was qualitativelydifferent from other algorithms they had learned,they had no trouble learning the unificationalgorithm, both iterative and recursive versions,because they were experienced in learningalgorithms.
For most students in our program, thisproject will be their only experience with a non-imperative programming language.Students were not bothered by the fact that thesample software provided included some featuresnot discussed in class.
In fact, some of the betterstudents studied these features and used them intheir own programs.Every student mastered the basics of logicprogramming, including how to choose betweenalternatives, establish a default, implement multi-step and hierarchical procedures, interact with theuser, and access an external database.
They alsolearned how to use unification along with multiplefirst-order relations to access and update adatabase.
The weaker students simply used thesample software as a guide, while the strongerones mastered the underlying concepts and wrotemore creative code.Student projects ranged the gamut, including asystem for running a car dealership, a gameidentifying movie directors, and an interactivesystem providing health information.6 ConclusionsTeaching NLP to students for whom this will bethe only exposure to the topic, and possibly theonly exposure to a research-oriented topic, can bea successful and enjoyable experience for bothstudents and teacher.
With good organization,students can do useful projects even in onesemester.One factor that has increased studentsatisfaction as well as their mastery of the materialis the use of concrete assignments where studentscan see for themselves concepts described in class.Three such assignments I have successfully usedinvolve duplicating Kernighan et al?s Bayesianapproach to spelling correction, a study ofGreenberg?s universals in the student?s nativelanguage, and a dialogue generation project usingmy research software.
Each of these assignments isused in one of the three segments of the course:statistical approaches to language, introduction tosyntax, and logic-based approaches to NLP.41AcknowledgmentsMichael Glass of Valparaiso University graciouslysupplied the unigram and bigram counts needed toimplement Kernighan et al?s (1990) spellingcorrection algorithm.ReferencesBaker, M. (2001).
Atoms of Language: The Mind?sHidden Rules of Grammar.
New York: Basic Books.Freedman, R. (2000).
Using a Reactive Planner as theBasis for a Dialogue Agent.
In Proceedings of theThirteenth Florida Artificial Intelligence ResearchSymposium (FLAIRS 2000), Orlando.Greenberg, J.
(1966).
Some universals of grammar withparticular reference to the order of meaningfulelements.
In Universals of language, ed.J.
Greenberg, pp.
73?113.
Cambridge, MA: MITPress.
2nd ed.Kernighan, M., Church, K., and Gale, W. (1990).
Aspelling correction program based on a noisy channelmodel.
In COLING ?90 (Helsinki), v. 2, pp.
205?211.Available online from the ACL archive athttp://acl.ldc.upenn.edu/C/C90/C90-2036.pdf.Koedinger, K. (2001).
The Student is Not Like Me.
InTenth International Conference on ArtificialIntelligence in Education (AI-ED 2001).
San Antonio,TX.
Keynote address.
Slides available online athttp://www.itsconference.org/content/seminars.htm.Mount, I.
(2005).
Cranky Consumer: Testing OnlineService Reps. Wall Street Journal, Feb. 1, 2005.42
