Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1786?1791,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsComparing Representations of Semantic Roles forString-To-Tree DecodingMarzieh Bazrafshan and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractWe introduce new features for incorpo-rating semantic predicate-argument struc-tures in machine translation (MT).
Themethods focus on the completeness of thesemantic structures of the translations, aswell as the order of the translated seman-tic roles.
We experiment with translationrules which contain the core argumentsfor the predicates in the source side of aMT system, and observe that using theserules significantly improves the translationquality.
We also present a new semanticfeature that resembles a language model.Our results show that the language modelfeature can also significantly improve MTresults.1 IntroductionIn recent years, there have been increasing ef-forts to incorporate semantics in statistical ma-chine translation (SMT), and the use of predicate-argument structures has provided promising im-provements in translation quality.
Wu and Fung(2009) showed that shallow semantic parsing canimprove the translation quality in a machine trans-lation system.
They introduced a two step model,in which they used a semantic parser to rerankthe translation hypotheses of a phrase-based sys-tem.
Liu and Gildea (2010) used semantic fea-tures for a tree-to-string syntax based SMT sys-tem.
Their features modeled deletion and reorder-ing for source side semantic roles, and they im-proved the translation quality.
Xiong et al.
(2012)incorporated the semantic structures into phrase-based SMT by adding syntactic and semantic fea-tures to their translation model.
They proposedtwo discriminative models which included fea-tures for predicate translation and argument re-ordering from source to target side.
Bazrafshanand Gildea (2013) used semantic structures ina string-to-tree translation system by extractingtranslation rules enriched with semantic informa-tion, and showed that this can improve the trans-lation quality.
Li et al.
(2013) used predicate-argument structure reordering models for hierar-chical phrase-based translation, and they used lin-guistically motivated constraints for phrase trans-lation.In this paper, we experiment with methods forincorporating semantics in a string-to-tree MTsystem.
These methods are designed to model theorder of translation, as well as the completenessof the semantic structures.
We extract translationrules that include the complete semantic structurein the source side, and compare that with usingsemantic rules for the target side predicates.
Wepresent a method for modeling the order of seman-tic role sequences that appear spread across multi-ple syntax-based translation rules, in order to over-come the problem that a rule representing the en-tire semantic structure of a predicate is often toolarge and too specific to apply to new sentencesduring decoding.
For this method, we compare theverb-specific roles of PropBank and the more gen-eral thematic roles of VerbNet.These essential arguments of a verbal predicateare called the core arguments.
Standard syntax-based MT is incapable of ensuring that the tar-get translation includes all of the core argumentsof a predicate that appear in the source sentence.To encourage the translation of the likely core ar-guments, we follow the work of Bazrafshan andGildea (2013), who use special translation ruleswith complete semantic structures of the predi-cates in the target side of their MT system.
Eachof these rules includes a predicate and all of itscore arguments.
Instead of incorporating only thetarget side semantic rules, we extract the specialrules for both the source and the target sides, andcompare the effectiveness of adding these rules to1786S-8NP-7-ARG11victimized byNP-7-ARG02NP-7-ARG11 ?
NP-7-ARG0 2Figure 1: A complete semantic rule (Bazrafshanand Gildea (2013)).the system separately and simultaneously.Besides the completeness of the arguments, it isalso important for the arguments to appear in thecorrect order.
Our second method is designed toencourage correct order of translation for both thecore and the non-core roles in the target sentence.We designed a new feature that resembles the lan-guage model feature in a standard MT system.
Wetrain a n-gram language model on sequences of se-mantic roles, by treating the semantic roles as thewords in what we call the semantic language.
Ourexperimental results show that the language modelfeature significantly improves translation quality.Semantic Role Labeling (SRL): We use se-mantic role labelers to annotate the training datathat we use to extract the translation rules.
For tar-get side SRL, the role labels are attached to thenonterminal nodes in the syntactic parse of eachsentence.
For source side SRL, the labels annotatethe spans from the source sentence that they cover.We train our semantic role labeler using two differ-ent standards: Propbank (Palmer et al., 2005) andVerbNet (Kipper Schuler, 2005).PropBank annotates the Penn Treebank withpredicate-argument structures.It use generic labels(such as Arg0, Arg1, etc.)
which are definedspecifically for each verb.
We trained a semanticrole labeler on the annotated Penn Treebank dataand used the classifier to tag our training data.VerbNet is a verb lexicon that categorizes En-glish verbs into hierarchical classes, and annotatesthem with thematic roles for the arguments thatthey accept.
Since the thematic roles use moremeaningful labels (e.g.
Agent, Patient, etc.
), a lan-guage model trained on VerbNet labels may bemore likely to generalize across verbs than onetrained on PropBank labels.
It may also providemore information, since VerbNet has a larger setof labels than PropBank.
To train the semanticrole labeler on VerbNet, we used the mappingsA?
BC c0[B, i, j] c1[C, j, k] c2[A, i, k] c0+ c1+ c2Figure 2: A deduction step in our baseline decoderprovided by the SemLink project (Palmer, 2009)to annotate the Penn Treebank with the VerbNetroles.
These mappings map the roles in PropBankto the thematic roles of VerbNet.
When there is nomapping for a role, we keep the role from Prop-bank.2 Using Semantics in MachineTranslationIn this section, we present our techniques for in-corporating semantics inMT: source side semanticrules, and the semantic language model.2.1 Source Side Semantic RulesBazrafshan and Gildea (2013) extracted transla-tion rules that included a predicate and all of itsarguments from the target side, and added thoserules to the baseline rules of their string-to-treeMT system.
Figure 1 shows an example of suchrules, which we refer to as complete semanticrules.
The new rules encourage the decoder togenerate translations that include all of the seman-tic roles that appear in the source sentence.In this paper, we use the same idea to extractrules from the semantic structures of the sourceside.
The complete semantic rules consist of thesmallest fragments of the combination of GHKM(Galley et al., 2004) rules that include one pred-icate and all of its core arguments that appear inthe sentence.
Rather than keeping the predicateand argument labels attached to the non-terminals,we remove those labels from our extracted seman-tic rules, to keep the non-terminals in the semanticrules consistent with the non-terminals of the base-line GHKM rules.
This is also important when us-ing both the source and the target semantic rules(i.e.
Chinese and English rules), as it has beenshown that there are cross lingual mismatches be-tween Chinese and English semantic roles in bilin-gual sentences (Fung et al., 2007).We extract a complete semantic rule for eachverbal predicate of each sentence pair in the train-ing data.
To extract the target side complete se-mantic rules, using the target side SRL anno-1787A?
BC to space c0(x1 x2 Destination)[B, i, j, (Agent, )] c1[C, j, k, (PRED bring, Theme, )] c2[A, i, k, (Agent, PRED bring,-*-, Theme, Destination)] c0+ c1+ c++ LMcost(Agent, PRED bring,-*-, Theme, Destination)Figure 3: A deduction step in the semantic language model method.tated training data, we follow the general GHKMmethod, and modify it to ensure that each fron-tier node (Galley et al., 2004) in a rule includes ei-ther all or none of the semantic role labels (i.e.
thepredicate and all of its present core arguments) inits descendants in the target side tree.
The result-ing rule then includes the predicate and all of itsarguments.
We use the source side SRL annotatedtraining data to extract the source side semanticrules.
Since the annotations specify the spans ofthe semantic roles, we extract the semantic rulesby ensuring that the span of the root (in the targetside) of the extracted rule covers all of the spansof the roles in the predicate-argument structure.The semantic rules are then used together withthe original GHKM rules.
We add a binary featureto distinguish the semantic rules from the rest.
Weexperiment with adding the semantic rules fromthe source side, and compare that with adding se-mantic rules of both the source and the target side.In all of the experiments in this paper, we usea string-to-tree decoder which uses a CYK styleparser (Yamada and Knight, 2002).
Figure 2 de-picts a deduction step in the baseline decoder.
TheCFG rule in the first line is used to generate anew item A with span (i, k) using items B andC, which have spans (i, j) and (j, k) respectively.The cost of each item is shown on the right.
Forexperimenting with complete semantic rules, inaddition having more rules, the only other modi-fication made to the baseline system is extendingthe feature vector to include the new feature.
Wedo not modify the decoder in any significant way.2.2 Semantic Language ModelThe semantic language model is designed to en-courage the correct order of translation for the se-mantic roles.
While the complete translation rulesof Section 2.1 contain the order of the translationfor core semantic roles, they do not include thenon-core semantic roles, that is, semantic roleswhich are not essential for the verbal predicates,but do contribute to the meaning of the predicate.In addition, the semantic LM can help in caseswhere no specific complete semantic rule can ap-ply, which makes the system more flexible.The semantic language model resembles a reg-ular language model, but instead of words, it de-fines a probability distribution over sequences ofsemantic roles.
For this method we also use a se-mantic role labeler on our training data, and usethe labeled data to train a tri-gram semantic lan-guage model.The rules are extracted using the baseline ruleextraction method.
As opposed to the previousmethod, the rules for this method are not derivedby combining GHKM rules, but rather are reg-ular GHKM rules which are annotated with se-mantic roles.
We make a new field in each ruleto keep the ordered list of the semantic roles inthat rule.
We also include the nonterminals of theright-hand-side of the rule in that ordered list, tobe able to substitute the semantic roles from theinput translation items in the correct order.
Thedecoder uses this new field to save the semanticroles in the translation items, and propagates thesemantic LM states in the same way that the reg-ular language model states are propagated by thedecoder.We define a new feature for the semantic lan-guage model, and score the semantic states in eachtranslation item, again analogously to a regularlanguage model.
Figure 3 depicts how the de-duction for this method is different from our base-line.
In this example, the semantic roles ?Agent?,?PRED bring?
and ?Theme?
come from the inputitems, and the role ?Destination?
(which tags theterminals ?to space?)
comes from the translationrule.We stemmed the verbs for training this feature,and also annotated our rules with stemmed verbalpredicates.
The stemming helps the training sincethe argument types of a verb are normally inde-pendent of its inflected variants.1788avg.
BLEU Scoredev test p-valueBaseline 26.01 25.00 -Source 26.44 25.17 0.048Source and target 26.39 25.63 < 10?10Propbank LM 26.38 25.08 0.108VerbNet LM 26.58 25.23 0.025Table 1: Comparisons of the methods with thebaseline.
The BLEU scores are calculated on thetop 3 results from 15 runs MERT for each experi-ments.
The p-values are calculated by comparingeach method against the baseline system.3 Experiments3.1 Experimental SetupThe data that we used for training the MT sys-tem was a Chinese-English corpus derived fromnewswire text from LDC.1The data consists of250K sentences, which is 6.3M words in the En-glish side.
Our language model was trained onthe English side of the entire data, which consistedof 1.65M sentences (39.3M words).
Our develop-ment and test sets are from the newswire portionof NIST evaluations (2004, 2005, 2006).
We used392 sentences for the development set and 428sentences for the test set.
These sentences havelengths smaller than 30, and they each have 4 ref-erence translations.
We used our in-house string-to-tree decoder that uses Earley parsing.
Otherthan the features that we presented for our newmethods, we used a set of nine standard features.The rules for the baseline system were extractedusing the GHKM method.
Our baseline GHKMrules also include composed rules, where largerrules are constructed by combining two levels ofthe regular GHKM rules.
We exclude any unaryrules (Chung et al., 2011), and only keep rulesthat have scope up to 3 (Hopkins and Langmead,2010).
For the semantic language model, we usedthe SRILM package (Stolcke, 2002) and traineda tri-gram language model with the default Good-Turing smoothing.Our target side semantic role labeler uses a max-imum entropy classifier to label parsed sentences.We used Sections 02-22 of the Penn TreeBank to1The data was randomly selected from the follow-ing sources: LDC2006E86, LDC2006E93, LDC2002E18,LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,LDC2006E24, LDC2006E92, LDC2006E24train the labeler, and sections 24 and 23 for devel-opment set and training set respectively.
The la-beler has a precision of 90% and a recall of 88%.We used the Chinese semantic role labeler of Wuand Palmer (2011) for source side SRL, whichuses the LIBLINEAR (Fan et al., 2008) as a classi-fier.
Minimum Error Rate Training (MERT) (Och,2003) was used for tuning the feature weights.For all of our experiments, we ran 15 instancesof MERT with random initial weight vectors, andused the weights of the top 3 results on the de-velopment set to test the systems on the test set.We chose to use the top 3 runs (rather than thebest run) of each system to account for the insta-bility of MERT (Clark et al., 2011).
This methodis designed to reflect the average performance ofthe MT system when trained with random restartsof MERT: we wish to discount runs in which theoptimizer is stuck in a poor region of the weightspace, but also to average across several good runsin order not to be mislead by the high variance ofthe single best run.
For each of our MT systems,we merged the results of the top 3 runs on the testset into one file, and ran a statistical significancetest, comparing it to the merged top 3 results fromour baseline system.
The 3 runs were merged byduplicating each run 3 times, and arranging themin the file so that the significance testing compareseach run with all the runs of the baseline.
We per-formed significance testing using paired bootstrapresampling (Koehn, 2004).
The difference is con-sidered statistically significant if p < 0.05 using1000 iterations of paired bootstrap resampling.3.2 ResultsOur results are shown in Table 1.
The secondand the third columns contain the average BLEUscore (Papineni et al., 2002) on the top three re-sults on the development and test sets.
The fourthcolumn is the p-value for statistical significancetesting against the baseline.
The first row showsthe results for our baseline.
The second row con-tains the results for using the source (Chinese)side complete semantic rules of Section 2.1, andthe third row is the results for combining boththe source and the target side complete semanticrules.
As noted before, in both of these experi-ments we also use the regular GHKM rules.
Theresult show that the source side complete seman-tic rules improve the system (p = 0.048), and aswe expected, combining the source and the tar-1789Source Sentence ??
,???????????????????????
.Reference therefore , it is the international community ?s responsibility to protect the children from harms resultedfrom armed conflicts .Baseline the armed conflicts will harm the importance of the international community the responsibilities .
there-fore , from child protectionVerbet LM therefore , the importance of the international community is to protect children from the harm affectedby the armed conflicts .Source Sentence ????????
,???????????
,????????
.Reference compared with last year ?s meeting , the smell of gunpowder has disappeared in this year ?s meeting andthe two sides ?
standpoints are getting closer .Baseline disappears on gunpowder , near the stance of the two sides compared with last year ?s meeting , themeeting of this year .Verbet LM the smells of gunpowder has disappeared , the position in the two sides approach .
compared with lastyear ?s meeting , this meeting(a) Comparison of the language model method (using VerbNet) and the baseline system.Source Sentence ????????
,???????????????
.Reference scientists have boldly predicted that the british spacecraft might have been stuck in a hole .Baseline scientists boldly expected , this vessel uk may have in the space ship in hang tung .Semantic Rules scientists have boldly expected this vessel and the possible settlement of the space ship in hang tung .Source Sentence ?????????????????
.Reference the us government should show goodwills to north korea ?s stand .Baseline this position of the government of the united states to goodwill toward the dprk .Semantic Rules this position that the us government should use goodwill toward the dprk .
(b) Comparison of the experiments with source and target side semantic rules and the baseline system.Figure 4: Comparison of example translations from our semantic methods and the baseline system.get side rules improves the system even more sig-nificantly (p < 10?10).
To measure the effectof combining the rules, in a separate experimentwe replicated the complete semantic rules exper-iments of Bazrafshan and Gildea (2013), and ranstatistical significance tests comparing the combi-nation of the source and target rules with usingonly the source or the target semantic rules sep-arately.
The results showed that combining the se-mantic rules outperforms both of the experimentsthat used rules from only one side (with p < 0.05in both cases).The results for the language model feature areshown in the last two rows of the table.
Us-ing Propbank for language model training did notchange the system in any significant way (p =0.108), but using VerbNet significantly improvedthe results (p = 0.025).
Figure 4(a) contains anexample comparing the baseline system with theVerbNet language model.
We can see how theVerbNet language model helps the decoder trans-late the argument in the correct order.
The baselinesystem has also generated the correct arguments,but the output is in the wrong order.
Figure 4(b)compares the experiment with semantic rules ofboth target and source side and the baseline sys-tem.
Translation of the word ?use?
by our seman-tic rules is a good example showing how the de-coder uses these semantic rules to generate a morecomplete predicate-argument structure.4 ConclusionsWe experimented with two techniques for incor-porating semantics in machine translation.
Themodels were designed to help the decoder trans-late semantic roles in the correct order, as wellas generating complete predicate-argument struc-tures.
We observed that using a semantic lan-guage model can significantly improve the trans-lations, and help the decoder to generate the se-mantic roles in the correct order.
Adding transla-tion rules with complete semantic structures alsoimproved our MT system.
We experimented withusing source side complete semantic rules, as wellas using rules for both the source and the targetsides.
Both of our experiments showed improve-ments over the baseline, and as expected, the sec-ond one had a higher improvement.AcknowledgmentsPartially funded by NSF grant IIS-0910611.1790ReferencesMarzieh Bazrafshan and Daniel Gildea.
2013.
Seman-tic roles for string to tree machine translation.
InAssociation for Computational Linguistics (ACL-13)short paper.Tagyoung Chung, Licheng Fang, and Daniel Gildea.2011.
Issues concerning decoding with synchronouscontext-free grammar.
In Proceedings of the ACL2011 Conference Short Papers, Portland, Oregon.Association for Computational Linguistics.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for opti-mizer instability.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: Short Pa-pers - Volume 2, HLT ?11, pages 176?181, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: A li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.Pascale Fung, Zhaojun Wu, Yongsheng Yang, andDekai Wu.
2007.
Learning Bilingual Seman-tic Frames: Shallow Semantic Parsing vs. Seman-tic Role Projection.
In TMI-2007: Proceedings ofthe 11 th International Conference on Theoreticaland Methodological Issues in Machine Translation,Sk?ovde, Sweden.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translationrule?
In Proceedings of NAACL-04, pages 273?280,Boston, MA.Mark Hopkins and Greg Langmead.
2010.
SCFG de-coding without binarization.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 646?655, Cambridge,MA, October.Karin Kipper Schuler.
2005.
VerbNet: A broad-coverage, comprehensive verb lexicon.
Ph.D. thesis,University of Pennsylvania.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, pages 388?395, Barcelona, Spain, July.Junhui Li, Philip Resnik, and Hal Daum?e III.
2013.Modeling syntactic and semantic structures in hier-archical phrase-based translation.
In HLT-NAACL,pages 540?549.Ding Liu and Daniel Gildea.
2010.
Semantic role fea-tures for machine translation.
In COLING-10, Bei-jing.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedingsof ACL-03, pages 160?167, Sapporo, Japan.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Martha.
Palmer.
2009.
SemLink: Linking PropBank,VerbNet and FrameNet.
In Proceedings of the Gen-erative Lexicon ConferenceGenLex-09, Pisa, Italy,Sept.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof ACL-02, pages 311?318, Philadelphia, PA.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In International Confer-ence on Spoken Language Processing, volume 2,pages 901?904.Dekai Wu and Pascale Fung.
2009.
Semantic roles forsmt: A hybrid two-pass model.
In Proceedings ofthe HLT-NAACL 2009: Short Papers, Boulder, Col-orado.Shumin Wu and Martha Palmer.
2011.
Semantic map-ping using automatic word alignment and seman-tic role labeling.
In Proceedings of the Fifth Work-shop on Syntax, Semantics and Structure in Statisti-cal Translation, SSST-5, pages 21?30, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Deyi Xiong, Min Zhang, and Haizhou Li.
2012.
Mod-eling the translation of predicate-argument structurefor smt.
In ACL (1), pages 902?911.Kenji Yamada and Kevin Knight.
2002.
A decoderfor syntax-based statistical MT.
In Proceedings ofACL-02, pages 303?310, Philadelphia, PA.1791
