Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288?297,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAssessing the relative reading level of sentence pairs for text simplificationSowmya Vajjala and Detmar MeurersLEAD Graduate School, Seminar f?ur SprachwissenschaftUniversit?at T?ubingen{sowmya,dm}@sfs.uni-tuebingen.deAbstractWhile the automatic analysis of the read-ability of texts has a long history, the useof readability assessment for text simplifi-cation has received only little attention sofar.
In this paper, we explore readabilitymodels for identifying differences in thereading levels of simplified and unsimpli-fied versions of sentences.Our experiments show that a relative rank-ing is preferable to an absolute binary oneand that the accuracy of identifying rel-ative simplification depends on the ini-tial reading level of the unsimplified ver-sion.
The approach is particularly success-ful in classifying the relative reading levelof harder sentences.In terms of practical relevance, the ap-proach promises to be useful for identi-fying particularly relevant targets for sim-plification and to evaluate simplificationsgiven specific readability constraints.1 IntroductionText simplification essentially is the process ofrewriting a given text to make it easier to processfor a given audience.
The target audience can ei-ther be human users trying to understand a text ormachine applications, such as a parser analyzingtext.
Text simplification has been used in a vari-ety of application scenarios, from providing sim-plified newspaper texts for aphasic readers (Can-ning and Tait, 1999) to supporting the extraction ofprotein-protein interactions in the biomedical do-main (Jonnalagadda and Gonzalez, 2009).A related field of research is automatic readabil-ity assessment, which can be useful for evaluatingtext simplification.
It can also be relevant for in-termediate simplification steps, such as the identi-fication of target sentences for simplification.
Yet,so far there has only been little research connect-ing the two subfields, possibly because readabilityresearch typically analyzes documents, whereassimplification approaches generally targeted lex-ical and syntactic aspects at the sentence level.
Inthis paper, we attempt to bridge this gap betweenreadability and simplification by studying read-ability at a sentence level and exploring how wellcan a readability model identify the differences be-tween unsimplified and simplified sentences.Our main research questions in this paper are:1.
Can the readability features that worked at thedocument level successfully be used at the sen-tence level?
2.
How accurately can we identify thedifferences in the sentential reading level beforeand after simplification?
To pursue these ques-tions, we started with constructing a document-level readability model.
We then applied it to nor-mal and simplified versions of sentences drawnfrom Wikipedia and Simple Wikipedia.As context of our work, we first discuss rel-evant related research.
Section 2 then describesthe corpora and the features we used to constructour readability model.
Section 3 discusses theperformance of our readability model in compari-son with other existing systems.
Sections 4 and 5present our experiments with sentence level read-ability analysis and the results.
In Section 6 wepresent our conclusions and plans for future work.1.1 Related WorkResearch into automatic text simplification essen-tially started with the idea of splitting long sen-tences into multiple shorter sentences to improveparsing efficiency (Chandrasekar et al., 1996;Chandrasekar and Srinivas, 1996).
This wasfollowed by rule-based approaches targeting hu-man and machine uses (Carroll et al., 1999; Sid-dharthan, 2002, 2004).With the availability of a sentence-aligned cor-pus based on Wikipedia and SimpleWikipedia288texts, data-driven approaches, partly inspired bystatistical machine translation, appeared (Specia,2010; Zhu et al., 2010; Bach et al., 2011; Costerand Kauchak, 2011; Woodsend and Lapata, 2011).While simplification methods have evolved, un-derstanding which parts of a text need to be sim-plified and methods for evaluating the simplifiedtext so far received only little attention.
The useof readability assessment for simplification hasmostly been restricted to using traditional read-ability formulae for evaluating or generating sim-plified text (Zhu et al., 2010; Wubben et al.,2012; Klerke and S?gaard, 2013; Stymne et al.,2013).
Some recent work briefly addresses issuessuch as classifying sentences by their reading level(Napoles and Dredze, 2010) and identifying sen-tential transformations needed for text simplifica-tion using text complexity features (Medero andOstendorf, 2011).
Some simplification approachesfor non-English languages (Aluisio et al., 2010;Gasperin et al., 2009;?Stajner et al., 2013) alsotouch on the use of readability assessment.In the present paper, we focus on the neglectedconnection between readability analysis and sim-plification.
We show through a cross-corpus eval-uation that a document level, regression-basedreadability model successfully identifies the dif-ferences between simplified vs. unsimplified sen-tences.
This approach can be useful in variousstages of simplification ranging from identifyingsimplification targets to the evaluation of simplifi-cation outcomes.2 Corpora and Features2.1 CorporaWe built and tested our document and sentencelevel readability models using three publicly avail-able text corpora with reading level annotations.WeeBit Corpus: The WeeBit corpus (Vajjalaand Meurers, 2012) consists of 3,125 articles be-longing to five reading levels, with 625 articlesper reading level.
The texts compiled from theWeeklyReader and BBC Bitesize target Englishlanguage learners from 7 to 16 years of age.
Weused this corpus to build our primary readabilitymodel by mapping the five reading levels in thecorpus to a scale of 1?5 and considered readabil-ity assessment as a regression problem.Common Core Standards Corpus: This cor-pus consists of 168 English texts available fromthe Appendix B of the Common Core Standardsreading initiative of the U.S. education system(CCSSO, 2010).
They are annotated by expertswith grade bands that cover the grades 1 to 12.These texts serve as exemplars for the level ofreading ability at a given grade level.
This corpuswas introduced as an evaluation corpus for read-ability models in the recent past (Sheehan et al.,2010; Nelson et al., 2012; Flor et al., 2013), so weused it to compare our model with other systems.Wiki-SimpleWiki Sentence Aligned Corpus:This corpus was created by Zhu et al.
(2010) andconsists of ?100k aligned sentence pairs drawnfrom Wikipedia and Simple English Wikipedia.We removed all pairs of identical sentences, i.e.,where the Wiki and the SimpleWiki versions arethe same.
We used this corpus to study readinglevel assessment at the sentence level.2.2 FeaturesWe started with the feature set described in Vajjalaand Meurers (2012) and added new features fo-cusing on the morphological and psycholinguisticproperties of words.
The features can be broadlyclassified into four groups.Lexical richness and POS features: Weadapted the lexical features from Vajjala andMeurers (2012).
This includes measures of lexicalrichness from Second Language Acquisition(SLA) research and measures of lexical variation(noun, verb, adjective, adverb and modifier vari-ation).
In addition, this feature set also includespart-of-speech densities (e.g., the average # ofnouns per sentence).
The information needed tocalculate these features was extracted using theStanford Tagger (Toutanova et al., 2003).
Noneof the lexical richness and POS features we usedrefer to specific words or lemmas.Syntactic Complexity features: Parse treebased features and some syntactic complexitymeasures derived from SLA research proveduseful for readability classification in the past, sowe made use of all the syntactic features fromVajjala and Meurers (2012): mean lengths ofvarious production units (sentence, clause, t-unit),measures of coordination and subordination(e.g., # of coordinate clauses per clause), thepresence of particular syntactic structures (e.g.,VPs per t-unit), the number of phrases of variouscategories (e.g., NP, VP, PP), the average lengths289of phrases, the parse tree height, and the numberof constituents per subtree.
None of the syntacticfeatures refer to specific words or lemmas.
Weused the BerkeleyParser (Petrov and Klein, 2007)for generating the parse trees and the Tregex tool(Levy and Andrew, 2006) to count the occurrencesof the syntactic patterns.While the first two feature sets are based on ourprevious work, as far as we know the next two areused in readability assessment for the first time.Features from the Celex Lexical Database:The Celex Lexical Database (Baayen et al., 1995)is a database consisting of information about mor-phological, syntactic, orthographic and phonolog-ical properties of words along with word frequen-cies in various corpora.
Celex for English containsthis information for more than 50,000 lemmas.
Anoverview of the fields in the Celex database is pro-vided online1and the Celex user manual2.We used the morphological and syntactic prop-erties of lemmas as features.
We excluded wordfrequency statistics and properties which consistedof word strings.
In all, we used 35 morphologi-cal and 49 syntactic properties that were expressedusing either character or numeric codes in thisdatabase as features for our task.The morphological properties in Celex includeinformation about the derivational, inflectionaland compositional features of the words, theirmorphological origins and complexity.
The syn-tactic properties of the words in Celex describethe attributes of a word depending on its parts ofspeech.
For the morphological and syntactic prop-erties from this database, we used the proportionof occurrences per text as features.
For example,the ratio of transitive verbs, complex morphologi-cal words, and vocative nouns to number of words.Lemmas from the text that do not have entries inthe Celex database were ignored.Word frequency statistics from Celex have beenused before to analyze text difficulty in the past(Crossley et al., 2007).
However, to our knowl-edge, this is the first time morphological and syn-tactic information from the Celex database is usedfor readability assessment.Psycholinguistic features: The MRC Psy-cholinguistic Database (Wilson, 1988) is a freelyavailable, machine readable dictionary annotated1http://celex.mpi.nl/help/elemmas.html2http://catalog.ldc.upenn.edu/docs/LDC96L14with 26 linguistic and psychological attributes ofabout 1.5 million words.3We used the measuresof word familiarity, concreteness, imageability,meaningfulness, and age of acquisition fromthis database as our features, by encoding theiraverage values per text.Kuperman et al.
(2012) compiled a freely avail-able database that includes Age of Acquisition(AoA) ratings for over 50,000 English words.4This database was created through crowd sourcingand was compared with several other AoA norms,which are also included in the database.
For eachof the five AoA norms, we computed the averageAoA of words per text.Turning to the final resource used, we includedthe average number of senses per word as calcu-lated using the MIT Java WordNet Interface as afeature.5We excluded auxiliary verbs for this cal-culation as they tend to have multiple senses thatdo not necessarily contribute to reading difficulty.Combining the four feature groups, we encode151 features for each text.3 Document-Level Readability ModelIn our first experiment, we tested the document-level readability model based on the 151 featuresusing the WeeBit corpus.
Under a regression per-spective on readability, we evaluated the approachusing Pearson Correlation and Root Mean SquareError (RMSE) in a 10-fold cross-validation set-ting.
We used the SMO Regression implementa-tion from WEKA (Hall et al., 2009) and achieved aPearson correlation of 0.92 and an RMSE of 0.53.The document-level performance of our 151feature model is virtually identical to that of the re-gression model we presented in Vajjala and Meur-ers (2013).
But compared to our previous work,the Celex and psycholinguistic features we in-cluded here provide more lexical information thatis meaningful to compute even for the sentence-level analysis we turn to in the next section.To be able to compare our document-levelresults with other contemporary readability ap-proaches, we need a common test corpus.
Nel-son et al.
(2012) compared several state of the artreadability assessment systems using five test setsand showed that the systems that went beyond tra-ditional formulae and wordlists performed better3http://www.psych.rl.ac.uk4http://crr.ugent.be/archives/8065http://projects.csail.mit.edu/jwi290on these real-life test sets.
We tested our modelon one of the publicly accessible test corpora fromthis study, the Common Core Standards Corpus.Flor et al.
(2013) used the same test set to studya measure of lexical tightness, providing a furtherperformance reference.Table 1 compares the performance of our modelto that reported for several commercial (indicatedin italics) and research systems on this test set.Nelson et al.
(2012) used Spearman?s Rank Cor-relation and Flor et al.
(2013) used Pearson Corre-lation as evaluation metrics.
To facilitate compar-ison, for our approach we provide both measures.System Spearman PearsonOur System 0.69 0.61Nelson et al.
(2012):REAP60.54 ?ATOS70.59 ?DRP80.53 ?Lexile90.50 ?Reading Maturity100.69 ?SourceRater110.75 ?Flor et al.
(2013):Lexical Tightness ?
-0.44Flesch-Kincaid ?
0.49Text length ?
0.36Table 1: Performance on CommonCore dataAs the table shows, our model is the best non-commercial system and overall second (tied withthe Reading Maturity system) to SourceRater asthe best performing commercial system on thistest set.
These results on an independent test setconfirm the validity of our document-level read-ability model.
With this baseline, we turned to asentence-level readability analysis.4 Sentence-Level Binary ClassificationFor each of the pairs in the Wiki-SimpleWiki Sen-tence Aligned Corpus introduced above, we la-beled the sentence from Wikipedia as hard andthat from Simple English Wikipedia as simple.The corpus thus consisted of single sentences,each labeled either simple or hard.
On this basis,we constructed a binary classification model.6http://reap.cs.cmu.edu7http://renlearn.com/atos8http://questarai.com/Products/DRPProgram9http://lexile.com10http://readingmaturity.com11http://naeptba.ets.org/SourceRater3Our document-level readability model does notinclude discourse features, so all 151 features canalso be computed for individual sentences.
Webuilt a binary sentence-level classification modelusing WEKA?s Sequential Minimal Optimization(SMO) for training an SVM in WEKA on theWiki-SimpleWiki sentence aligned corpus.
Thechoice of algorithm was primarily motivated bythe fact that it was shown to be efficient in previ-ous work on readability classification (Feng, 2010;Hancke et al., 2012; Falkenjack et al., 2013).The accuracy of the resulting classifier deter-mining whether a given sentence is simple orhard was disappointing, reaching only 66% accu-racy in a 10-fold cross-validation setting.
Exper-iments with different classification algorithms didnot yield any more promising results.
To studyhow the classification performance is impacted bythe size of the training data, we experimented withdifferent sizes, using SMO as the classification al-gorithm.
Figure 1 shows the classification accu-racy with different training set sizes.6565.56666.56767.56868.50  10  20  30  40  50  60  70  80  90  100classification accuracy (in %)% of training data usedRelation between Binary Sentence Classification Accuracy and Training Data sizeFigure 1: Training size vs. classification accuracyThe graph shows that beyond 10% of the trainingdata, more training data did not result in signifi-cant differences in classification accuracy.
Evenat 10%, the training set contains around 10k in-stances per category, so the variability of any ofthe patterns distinguished by our features is suffi-ciently represented.We also explored whether feature selectioncould be useful.
A subset of features chosen by re-moving correlated features using the CfsSubsetE-val method in WEKA did not improve the results,yielding an accuracy of 65.8%.
A simple base-line based on the sentence length as single featureresults in an accuracy of 60.5%, underscoring the291limited value of the rich feature set in this binaryclassification setup.For the sake of a direct comparison with thedocument-level model, we also explored modelingthe task as a regression on a 1?2 scale.
In compar-ison to the document-level model, which as dis-cussed in section 3 had a correlation of 0.92, thesentence-level model achieves only a correlationof 0.4.
A direct comparison is also possible whenwe train the document-level model as a five-classclassifier with SMO.
This model achieved a clas-sification accuracy of ?90% on the documents,compared to the 66% accuracy of the sentence-level model classifying sentences.
So under eachof these perspectives, the sentence-level models onthe sentence task are much less successful than thedocument-level models on the document task.But does this indicate that it is not possible toaccurately identify the reading level distinctionsbetween simplified and unsimplified versions atthe sentence level?
Is there not enough informa-tion available when considering a single sentence?We hypothesized that the drop in the classi-fication accuracy instead results from the rela-tive nature of simplification.
For each pair ofthe Wiki-SimpleWiki sentence aligned corpus weused, the Wiki sentence was harder than the Sim-pleWikipedia sentence.
But this does not neces-sarily mean that each of the Wikipedia sentencesis harder than each of the SimpleWikipedia sen-tences.
The low accuracy of the binary classi-fier may thus simply result from the inappropriateassumption of an absolute, binary classificationviewing each of the sentences originating fromSimpleWikipedia as simple and each from the reg-ular Wiki as hard.The confusion matrices of the binary classifi-cation suggests some support for this hypothesis,as more simple sentences were classified as hardcompared to the other way around.
This can resultwhen a simple sentence is simpler than its hardversion, but could actually be simplified further ?and as such may still be harder than another un-simplified sentence.
The hypothesis thus amountsto saying that the two-class classification modelmistakenly turned the relative difference betweenthe sentence pairs into a global classification of in-dividual sentences, independent of the pairs theyoccur in.How can we verify this hypothesis?
The sen-tence corpus only provides the relative ranking ofthe pairs, but we can try to identify more fine-grained readability levels for sentences by apply-ing the five class readability model for documentsthat was introduced in section 3.5 Relative Reading Levels of SentencesWe applied the document-level readability modelto the individual sentences from the Wiki-SimpleWiki corpus to study which reading levelsare identified by our model.
As we are using a re-gression model, the values sometimes go beyondthe training corpus?
scale of 1?5.
For ease of com-parison, we rounded off the reading levels to thefive level scale, i.e., 1 means 1 or below, and 5means 5 or above.
Figure 2 shows the distributionof Wikipedia and SimpleWikipedia sentences ac-cording to the predictions of our document-levelreadability model trained on the WeeBit corpus.51015202530354045501  1.5  2  2.5  3  3.5  4  4.5  5Percentageof the totalsentences at that levelReading levelDistribution of reading levels of Normal and Simplified SentencesWikiSimple WikiFigure 2: Reading level distribution of theWikipedia and SimpleWikipedia sentencesThe model determines that a high percentage ofthe SimpleWiki sentences belong to lower readinglevels, with over 45% at the lowest reading level;yet there also are some SimpleWikipedia sen-tences which are aligned even to the highest read-ability level.
In contrast, the regular Wikipediasentences are evenly distributed across all readinglevels.The distributions identified by the model sup-port our hypothesis that some Wiki sentences aresimpler than some SimpleWikipedia sentences.Note that this is fully compatible with the fact thatfor each pair of (SimpleWiki,Wiki) sentences in-cluded in the corpus, the former is higher in read-ing level than the latter; e.g., just consider two sen-tence pairs with the levels (1, 2) and (3, 5).2925.1 On the discriminating power of the modelZooming in on the relative reading levels of thepaired unsimplified and simplified sentences, wewanted to determine for how many sentence pairsthe sentence reading levels determined by ourmodel are compatible with the pair?s ranking.
Inother words, we calculated the percentage of pairs(S,N) in which the reading level of a simplifiedsentence (S) is identified as less than, equal to, orgreater than the unsimplified (normal) version ofthe sentence (N ), i.e., S<N , S=N , and S>N .Where simplification split a sentence into multiplesentences, we computed S as the average readinglevel of the split sentences.Given the regression model setup, we can con-sider how big the difference between two readinglevels determined by the model should be in or-der for us to interpret it as a categorical differencein reading level.
Let us call this discriminatingreading-level difference the d-level.
For example,with d = 0.3, a sentence pair determined to beat levels (3.4, 3.2) would be considered a case ofS=N , whereas (3.4, 3.7) would be an instance ofS <N .
The d-value can be understood as a mea-sure of how fine-grained the model is in identify-ing reading-level differences between sentences.If we consider the percentage of samples identi-fied as S <=N as an accuracy measure, Figure 3shows the accuracy for different d-values.1020304050607080901000  0.2  0.4  0.6  0.8  1Percentageof the totalsamplesd-valueComparison of Normal and Simplified SentencesS<=NFigure 3: Accurately identified S<=NWe can observe that the percentage of instancesthat the model correctly identifies as S <= Nsteadily increases from 70% to 90% as d increases.While the value of d in theory can be anything,values beyond 1 are uninteresting in the context ofthis study.
At d = 1, most of the sentence pairsalready belong to S=N , so increasing this furtherwould defeat the purpose of identifying reading-level differences.
The higher the d-value, the moreof the simplified and unsimplified pairs are lumpedtogether as indistinguishable.Spelling out the different cases from Figure 3,the number of pairs identified correctly, equated,and misclassified as a function of the d-value isshown in Figure 4.10152025303540455055600  0.2  0.4  0.6  0.8  1Percentageof the totalsamplesd-valueComparison of Normal and Simplified SentencesS<NS=NS>NFigure 4: Correctly (S < N ), equated (S = N ),and incorrectly (S>N ) identified sentence pairsAt d = 0.4, around 50% of the pairs are cor-rectly classified, 20% are misclassified, and 30%equated.
At d=0.7, the rate of pairs for which nodistinction can be determined already rises above50%.
For d-values between 0.3 and 0.6, the per-centage of correctly identified pairs exceeds thepercentage of equated pairs, which in turn exceedsthe percentage of misclassified pairs.5.2 Influence of reading-level on accuracyWe saw in Figure 2 that the Wikipedia sentencesare uniformly distributed across the reading lev-els, and for each of these sentences, a human sim-plified version is included in the corpus.
Evensentences identified by our readability model asbelonging to the lower reading levels thus werefurther simplified.
This leads us to investigatewhether the reading level of the unsimplified sen-tence influences the ability of our model to cor-rectly identify the simplification relationship.To investigate this, we separately analyzed pairswhere the unsimplified sentences had a higherreading level and those where it had a lower read-ing level, taking the middle of the scale (2.5) as the293cut-off point.
Figure 5 shows the accuracies ob-tained when distinguishing unsimplified sentencesof two readability levels.5560657075808590951000  0.2  0.4  0.6  0.8  1Percentageof the totalsampleshavingS<=Nd-valueS<=N vs d, when N >=2.5 and N<2.5N>=2.5N<2.5Figure 5: Accuracy (S<=N) for different N typesFor the pairs where the reading level of the unsim-plified version is high, the accuracy of the read-ability model is high (80?95%).
In the other case,the accuracy drops to 65?75% (for 0.3<= d <=0.6).
Presumably the complex sentences for whichthe model performs best offer more syntactic andlexical material informing the features used.When we split the graph into the three casesagain (S < N , S = N , S > N ), the pairs with ahigh-level unsimplified sentence in Figure 6 fol-low the overall picture of Figure 4.010203040506070800  0.2  0.4  0.6  0.8  1Percentageof the totalsamplesd-valueComparison at Higher Values of NS<NS=NS>NFigure 6: Results for N>=2.5On the other hand, the results in Figure 7 for thepairs with an unsimplified sentence at a low read-ability level establish that the model essentially isincapable to identify readability differences.102030405060700  0.2  0.4  0.6  0.8  1Percentageof the totalsamplesd-valueComparison at Lower Values of NS<NS=NS>NFigure 7: Results for N<2.5The correctly identified S<N and the incorrectlyidentified S >N cases mostly overlap, indicatingchance-level performance.
Increasing the d-levelonly increases the number of equated pairs, with-out much impact on the number of correctly dis-tinguished pairs.In real-world terms, this means that it is diffi-cult to identify simplifications of an already sim-ple sentence.
While some of this difficulty maystem from the fact that simple sentences are likelyto be shorter and thus offer less linguistic materialon which an analysis can be based, it also pointsto a need for more research on features that canreliably distinguish lower levels of readability.Summing up, the experiments discussed in thissection show that a document-level readabilitymodel trained on the WeeBit corpus can provideinsightful perspectives on the nature of simplifica-tion at the sentence level.
The results emphasizethe relative nature of readability and the need formore features capable of identifying characteris-tics distinguishing sentences at lower levels.6 ConclusionsWe started with constructing a document-levelreadability model and compared its performancewith other readability systems on a standard testset.
Having established the state-of-the-art perfor-mance of our document-level model, we moved onto investigate the use of the features and the modelat the sentence level.In the sentence-level research, we first used thesame feature set to construct a two-class readabil-ity model on the sentences from the Wikipedia-SimpleWikipedia sentence aligned corpus.
The294model only achieved a classification accuracy of66%.
Exploring the causes for this low perfor-mance, we studied the sentences in the alignedpairs through the lens of our document-level read-ability model, the regression model based on thefive level data of the WeeBit corpus.
Our ex-periment identifies most of the Simple Wikipediasentences as belonging to the lower levels, withsome sentences also showing up at higher lev-els.
The sentences from the normal Wikipedia,on the other hand, display a uniform distributionacross all reading levels.
A simplified sentence(S) can thus be at a lower reading level than itspaired unsimplified sentence (N) while also beingat a higher reading level than another unsimplifiedsentence.
Given this distribution of reading lev-els, the low performance of the binary classifieris expected.
Instead of an absolute, binary differ-ence in reading levels that counts each Wikipediasentence from the corpus as hard and each SimpleWikipedia sentence as simple, a relative rankingof reading levels seems to better suit the data.Inspecting the relative difference in the read-ing levels of the aligned unsimplified-simplifiedsentence pairs, we characterized the accuracy ofpredicting the relative reading level ranking in apair correctly depending on the reading-level dif-ference d required to required to identify a cate-gorical difference.
While the experiments wereperformed to verify the hypothesis that simpli-fication is relative, they also confirm that thedocument-level readability model trained on theWeeBit corpus generalized well to Wikipedia-SimpleWikipedia as a different, sentence-levelcorpus.The analysis revealed that the accuracy dependson the initial reading level of the unsimplifiedsentence.
The model performs very well whenthe reading level of the unsimplified sentence ishigher, but the features seem limited in their abil-ity to pick up on the differences between sentencesat the lowest levels.
In future work, we thus in-tend to add more features identifying differencesbetween lower levels of readability.Taking the focus on the relative ranking ofthe readability of sentences one step further, weare currently studying if modeling the readabilityproblem as preference learning or ordinal regres-sion will improve the accuracy in predicting therelation between simplified and unsimplified sen-tence versions.Overall, the paper contributes to the state of theart by providing a methodology to quantitativelyevaluate the degree of simplification performedby an automatic system.
The results can also bepotentially useful in providing assistive feedbackfor human writers preparing simplified texts givenspecific target user constraints.
We plan to explorethe idea of generating simplified text with read-ability constraints as suggested in Stymne et al.
(2013) for Machine Translation.AcknowledgementsWe thank the anonymous reviewers for their de-tailed comments.
Our research was funded bythe LEAD Graduate School (GSC 1028, http://purl.org/lead), a project of the ExcellenceInitiative of the German federal and state gov-ernments, and the European Commission?s 7thFramework Program under grant agreement num-ber 238405 (CLARA).ReferencesSandra Aluisio, Lucia Specia, Caroline Gasperin, andCarolina Scarton.
2010.
Readability assessment fortext simplification.
In Proceedings of the NAACLHLT 2010 Fifth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 1?9.R.
H. Baayen, R. Piepenbrock, and L. Gulikers.1995.
The CELEX lexical databases.
CDROM,http://www.ldc.upenn.edu/Catalog/readme_files/celex.readme.html.Nguyen Bach, Qin Gao, Stephan Vogel, and AlexWaibel.
2011.
Tris: A statistical sentence simplifierwith log-linear models and margin-based discrimi-native training.
In Proceedings of 5th InternationalJoint Conference on Natural Language Processing,pages 474?482.
Asian Federation of Natural Lan-guage Processing.Yvonne Canning and John Tait.
1999.
Syntactic sim-plification of newspaper text for aphasic readers.
InProceedings of SIGIR-99 Workshop on CustomisedInformation Delivery, pages 6?11.John Carroll, Guido Minnen, Darren Pearce, YvonneCanning, Siobhan Devlin, and John Tait.
1999.Simplifying text for language-impaired readers.
InProceedings of the 9th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL), pages 269?270.CCSSO.
2010.
Common core state standards for en-glish language arts & literacy in history/social stud-ies, science, and technical subjects.
appendix B: Textexemplars and sample performance tasks.
Technicalreport, National Governors Association Center for295Best Practices, Council of Chief State School Of-ficers.
http://www.corestandards.org/assets/Appendix_B.pdf.R.
Chandrasekar and B. Srinivas.
1996.
Automatic in-duction of rules for text simplification.
TechnicalReport IRCS Report 96?30, Upenn, NSF Scienceand Technology Center for Research in CognitiveScience.R.
Chandrasekar, Christine Doran, and B. Srinivas.1996.
Motivations and methods for text simplifica-tion.
In Proceedings of the 16th International Con-ference on Computational Linguistics (COLING),pages 1041?1044.William Coster and David Kauchak.
2011.
Simple en-glish wikipedia: A new text simplification task.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 665?669, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Scott A. Crossley, David F. Dufty, Philip M. McCarthy,and Danielle S. McNamara.
2007.
Toward a newreadability: A mixed model approach.
In Danielle S.McNamara and Greg Trafton, editors, Proceedingsof the 29th annual conference of the Cognitive Sci-ence Society.
Cognitive Science Society.Johan Falkenjack, Katarina Heimann M?uhlenbock, andArne J?onsson.
2013.
Features indicating readabilityin swedish text.
In Proceedings of the 19th NordicConference of Computational Linguistics (NODAL-IDA).Lijun Feng.
2010.
Automatic Readability Assessment.Ph.D.
thesis, City University of New York (CUNY).Michael Flor, Beata Beigman Klebanov, and Kath-leen M. Sheehan.
2013.
Lexical tightness and textcomplexity.
In Proceedings of the Second Workshopon Natural Language Processing for Improving Tex-tual Accessibility.Caroline Gasperin, Lucia Specia, Tiago F. Pereira, andSandra M. Aluisio.
2009.
Learning when to sim-plify sentences for natural text simplification.
InEncontro Nacional de Intelig?encia Artificial (ENIA-2009).Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.In The SIGKDD Explorations, volume 11, pages 10?18.Julia Hancke, Detmar Meurers, and Sowmya Vajjala.2012.
Readability classification for german usinglexical, syntactic, and morphological features.
InProceedings of the 24th International Conference onComputational Linguistics (COLING), pages 1063?1080, Mumbay, India.Siddhartha Jonnalagadda and Graciela Gonzalez.2009.
Sentence simplification aids protein-proteininteraction extraction.
In Proceedings of The 3rdInternational Symposium on Languages in Biologyand Medicine, Jeju Island, South Korea, November8-10, 2009.Sigrid Klerke and Anders S?gaard.
2013.
Simple,readable sub-sentences.
In Proceedings of the ACLStudent Research Workshop.Victor Kuperman, Hans Stadthagen-Gonzalez, andMarc Brysbaert.
2012.
Age-of-acquisition ratingsfor 30,000 english words.
Behavior Research Meth-ods, 44(4):978?990.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In 5th International Conference on Lan-guage Resources and Evaluation, Genoa, Italy.Julie Medero and Marie Ostendorf.
2011.
Identifyingtargets for syntactic simplification.
In ISCA Interna-tional Workshop on Speech and Language Technol-ogy in Education (SLaTE 2011).Courtney Napoles and Mark Dredze.
2010.
Learn-ing simple wikipedia: a cogitation in ascertainingabecedarian language.
In Proceedings of the NAACLHLT 2010 Workshop on Computational Linguisticsand Writing: Writing Processes and Authoring Aids,CL&W ?10, pages 42?50, Stroudsburg, PA, USA.Association for Computational Linguistics.J.
Nelson, C. Perfetti, D. Liben, and M. Liben.
2012.Measures of text difficulty: Testing their predic-tive value for grade levels and student performance.Technical report, The Council of Chief State SchoolOfficers.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Kathleen M. Sheehan, Irene Kostin, Yoko Futagi, andMichael Flor.
2010.
Generating automated textcomplexity classifications that are aligned with tar-geted text complexity standards.
Technical ReportRR-10-28, ETS, December.Advaith Siddharthan.
2002.
An architecture for a textsimplification system.
In In Proceedings of the Lan-guage Engineering Conference 2002 (LEC 2002).Advaith Siddharthan.
2004.
Syntactic simplificationand text cohesion.
Technical Report UCAM-CL-TR-597, University of Cambridge Computer Labo-ratory.Lucia Specia.
2010.
Translating from complex to sim-plified sentences.
In Proceedings of the 9th interna-tional conference on Computational Processing ofthe Portuguese Language (PROPOR?10).296Sara Stymne, J?org Tiedemann, Christian Hardmeier,and Joakim Nivre.
2013.
Statistical machine trans-lation with readability constraints.
In Proceedings ofthe 19th Nordic Conference of Computational Lin-guistics (NODALIDA 2013).K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In HLT-NAACL, pages252?259, Edmonton, Canada.Sowmya Vajjala and Detmar Meurers.
2012.
On im-proving the accuracy of readability classification us-ing insights from second language acquisition.
InIn Proceedings of the 7th Workshop on InnovativeUse of NLP for Building Educational Applications,pages 163?-173.Sowmya Vajjala and Detmar Meurers.
2013.
On theapplicability of readability models to web texts.
InProceedings of the Second Workshop on Predictingand Improving Text Readability for Target ReaderPopulations.M.D.
Wilson.
1988.
The MRC psycholinguisticdatabase: Machine readable dictionary, version 2.Behavioural Research Methods, Instruments andComputers, 20(1):6?11.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Sander Wubben, Antal van den Bosch, and EmielKrahmer.
2012.
Sentence simplification by mono-lingual machine translation.
In Proceedings of ACL2012.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of The23rd International Conference on ComputationalLinguistics (COLING), August 2010.
Beijing, China.Sanja?Stajner, Biljana Drndarevic, and Horaccio Sag-gion.
2013.
Corpus-based sentence deletion andsplit decisions for spanish text simplification.
In CI-CLing 2013: The 14th International Conference onIntelligent Text Processing and Computational Lin-guistics.297
