First Joint Conference on Lexical and Computational Semantics (*SEM), pages 673?678,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUOW: Semantically Informed Text SimilarityMiguel Rios and Wilker AzizResearch Group in Computational LinguisticsUniversity of WolverhamptonStafford Street, Wolverhampton,WV1 1SB, UK{M.Rios, W.Aziz}@wlv.ac.ukLucia SpeciaDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 Portobello,Sheffield, S1 4DP, UKL.Specia@sheffield.ac.ukAbstractThe UOW submissions to the Semantic Tex-tual Similarity task at SemEval-2012 use asupervised machine learning algorithm alongwith features based on lexical, syntactic andsemantic similarity metrics to predict the se-mantic equivalence between a pair of sen-tences.
The lexical metrics are based on word-overlap.
A shallow syntactic metric is basedon the overlap of base-phrase labels.
Thesemantically informed metrics are based onthe preservation of named entities and on thealignment of verb predicates and the overlapof argument roles using inexact matching.
Oursubmissions outperformed the official base-line, with our best system ranked above aver-age, but the contribution of the semantic met-rics was not conclusive.1 IntroductionWe describe the UOW submissions to the SemanticTextual Similarity (STS) task at SemEval-2012.
Oursystems are based on combining similarity scores asfeatures using a regression algorithm to predict thedegree of semantic equivalence between a pair ofsentences.
We train the regression algorithm withdifferent classes of similarity metrics: i) lexical,ii) syntactic and iii) semantic.
The lexical similar-ity metrics are: i) cosine similarity using a bag-of-words representation, and ii) precision, recall andF-measure of content words.
The syntactic metriccomputes BLEU (Papineni et al, 2002), a machinetranslation evaluation metric, over a labels of base-phrases (chunks).
Two semantic metrics are used: ametric based on the preservation of Named Entitiesand TINE (Rios et al, 2011).
Named entities arematched by type and content: while the type has tomatch exactly, the content is compared with the as-sistance of a distributional thesaurus.
TINE is a met-ric proposed to measure adequacy in machine trans-lation and favors similar semantic frames.
TINEattempts to align verb predicates, assuming a one-to-one correspondence between semantic roles, andconsidering ontologies for inexact alignment.
Thesurface realization of the arguments is compared us-ing a distributional thesaurus and the cosine similar-ity metric.
Finally, we use METEOR (Denkowskiand Lavie, 2010), also a common metric for ma-chine translation evaluation, that also computes in-exact word overlap as at way of measuring the im-pact of our semantic metrics.The lexical and syntactic metrics complement thesemantic metrics in dealing with the phenomena ob-served in the task?s dataset.
For instance, from theMSRvid dataset:S1 Two men are playing football.S2 Two men are practicing football.In this case, as typical of paraphrasing, the situa-tion and participants are the same while the surfacerealization differs, but playing can be consideredsimilar to practicing.
From the SMT-eur dataset:S3 The Council of Europe, along with the Court ofHuman Rights, has a wealth of experience ofsuch forms of supervision, and we can build onthese.673S4 Just as the European Court of Human Rights, theCouncil of Europe has also considerable expe-rience with regard to these forms of control; wecan take as a basis.Similarly, here although with different realiza-tions, the Court of Human Rights and the EuropeanCourt of Human Rights represent the same entity.Semantic metrics based on predicate-argumentstructure can play a role in cases when different re-alization have similar semantic roles:S5 The right of a government arbitrarily to set asideits own constitution is the defining characteris-tic of a tyranny.S6 The right for a government to draw aside its con-stitution arbitrarily is the definition character-istic of a tyranny.In this work we attempt to exploit the fact that su-perficial variations such the ones in these examplesshould still render very similarity scores.In Section 2 we describe the similarity metrics inmore detail.
In Section 3 we show the results of ourthree systems.
In Section 4 we discuss these resultsand in Section 5 we present some conclusions.2 Similarity MetricsThe metrics used in this work are as follows:2.1 Lexical metricsAll our lexical metrics use the same surface repre-sentation: words.
However, the cosine metric usesbag-of-words, while all the other metrics use onlycontent words.
We thus first represent the sentencesas bag-of-words.
For example, given the pair of sen-tences S7 and S8:S7 A man is riding a bicycle.S8 A man is riding a bike.the bag-of-words are S7 = {A, man, is, riding, a,bicycle,.}
and S8 = {A, man, is, riding, a, bike, .
},and the bag-of-content-words are S7 = {man, riding,bicycle} and S8 = {man, riding, bike}.We compute similarity scores using the followingmetrics between a pair of sentencesA andB: cosinedistance (Equation 1), precision (Equation 2), recall(Equation 3) and F-measure (Equation 4).cosine(A,B) =|A?B|?|A| ?
|B|(1)precision(A,B) =|A?B||B|(2)recall(A,B) =|A?B||A|(3)F (A,B) = 2 ?precision(A,B) ?
recall(A,B)precision(A,B) + recall(A,B)(4)2.2 BLEU over base-phrasesThe BLEU metric is used for the automatic evalua-tion of Machine Translation.
The metric computesthe precision of exact matching of n-grams betweena hypothesis and reference translations.
This sim-ple procedure has limitations such as: the matchingof non-content words mixed with the counts of con-tent words affects in a perfect matching that can hap-pen even if the order of sequences of n-grams in thehypothesis and reference translation are very differ-ent, changing completely the meaning of the trans-lation.
To account for similarity in word order weuse BLEU over base-phrase labels instead of words,leaving the lexical matching for other lexical and se-mantic metrics.
We compute the matchings of 1-4-grams of base-phrase labels.
This metric favorssimilar syntactic order.2.3 Named Entities metricThe goal of the metric is to deal with synonym enti-ties.
First, named entities are grouped by class (e.g.Organization), and then the content of the named en-tities within the same classes is compared throughcosine similarity.
If the surface realization is differ-ent, we retrieve words that share the same contextwith the named entity using Dekang Lin?s distribu-tional thesaurus (Lin, 1998).
Therefore, the cosinesimilarity will have more information than just thenamed entities themselves.
For example, from thesentence pair S9 and S10:S9 Companies include IBM Corp. ...674S10 Companies include International Business Ma-chines ...The entity from S9: IBM Corp. and the entityfrom S10: International Business Machines havethe same tag Organization.
The metric groupsthem and adds words from the thesaurus result-ing in the following bag-of-words.
S9: {IBMCorp.,...
Microsoft, Intel, Sun Microsystems, Mo-torola/Motorola, Hewlett-Packard/Hewlett-Packard,Novell, Apple Computer...} and S10: {InternationalBusiness Machines,... Apple Computer, Yahoo, Mi-crosoft, Alcoa...}.
The metric then computes the co-sine similarity between this expanded pair of bag-of-words.2.4 METEORThis metric is also a lexical metric based on uni-gram matching between two sentences.
However,matches can be exact, using stems, synonyms, orparaphrases of unigrams.
The synonym matching iscomputed using WordNet (Fellbaum, 1998) and theparaphrase matching is computed using paraphrasetables (Callison-Burch et al, 2010).
The structure ofthe sentences is not not directly considered, but sim-ilar word orders are rewarded through higher scoresfor the matching of longer fragments.2.5 Semantic Role Label metricRios et al (2011) propose TINE, an automatic met-ric based on the use semantic roles to align predi-cates and their respective arguments in a pair of sen-tences.
The metric complements lexical matchingwith a shallow semantic component to better addressadequacy in machine translation evaluation.
Themain contribution of such a metric is to provide amore flexible way of measuring the overlap betweenshallow semantic representations (semantic role la-bels) that considers both the semantic structure ofthe sentence and the content of the semantic compo-nents.This metric allows to match synonym predicatesby using verb ontologies such as VerbNet (Schuler,2006) and VerbOcean (Chklovski and Pantel, 2004)and distributional semantics similarity metrics, suchas Dekang Lin?s thesaurus (Lin, 1998), where pre-vious semantic metrics only perform exact match ofpredicate structures and arguments.
For example, inVerbNet the verbs spook and terrify share the sameclass amuse-31.1, and in VerbOcean the verb dressis related to the verb wear, so these are consideredmatches in TINE.The main sources of errors in this metric are thematching of unrelated verbs and the lack of coverageof the ontologies.
For example, for S11 and S12,remain and say are (incorrectly) related as given byVerbOcean.S11 If snow falls on the slopes this week, Christmaswill sell out too, says Schiefert.S12 If the roads remain snowfall during the week,the dates of Christmas will dry up, saidSchiefert.For this work the matching of unrelated verbs isa particularly crucial issue, since the sentences to becompared are not necessarily similar, as it is the gen-eral case in machine translation.
We have thus mod-ified the metric with a preliminary optimization stepwhich aligns the verb predicates by measuring twodegrees of similarity: i) how similar their argumentsare, and ii) how related the predicates?
realizationsare.
Both scores are combined as shown in Equation5 to score the similarity between the two predicates(Av, Bv) from a pair of sentences (A,B).sim(Av,Bv) = (wlex ?
lexScore(Av, Bv))+(warg ?
argScore(Aarg, Barg))(5)where wlex and warg are the weights for eachcomponent, argScore(Aarg, Barg) is the similarity,which is computed as in Equation 7, of the argu-ments between the predicates being compared andlexScore(Av, Bv) is the similarity score extractedfrom the Dekang Lin?s thesaurus between the predi-cates being compared.
The Dekang Lin?s thesaurusis an automatically built thesaurus, and for eachword it has an entry with the most similar words andtheir similarity scores.
If the verbs are related in thethesaurus we use their similarity score as lexScoreotherwise lexScore = 0.
The pair of predicateswith the maximum sim score is aligned.
The align-ment is an optimization problem where predicatesare aligned 1-1: we search for all 1-1 alignments thatlead to the maximum average sim for the pair of sen-tences.
For example, S13 and S14 have the follow-ing list of predicates: S13 = {loaded, rose, ending}675and S14 = {laced, climbed}.
The metric compareseach pair of predicates and it aligns the predicatesrose and climbed because they are related in the the-saurus with a similarity score lexScore = 0.796and a argScore = 0.185 given that the weights areset to 0.5 and sum up to 1 the predicates reach themaximum sim = 0.429 score.
The output of thisstep results in a set of aligned verbs between a pairof sentences.S13 The tech - loaded Nasdaq composite rose 0points to 0 , ending at its highest level for 0months.S14 The technology - laced Nasdaq Composite In-dex IXIC climbed 0 points , or 0 percent , to0.The SRL similarity metric semanticRole be-tween two sentences A and B is then defined as:semanticRole(A,B) =?v?V verbScore(Av, Bv)|VB |(6)The verbScore in Equation 6 is computed overthe set of aligned predicates from the previous opti-mization step and for each aligned predicate the ar-gument similarity is computed by Equation 7.verbScore(Av, Bv) =?arg?ArgA?ArgBargScore(Aarg, Barg)|ArgB |(7)In Equation 6, V is the set of verbs aligned betweenthe two sentences A and B, and |VB| is the num-ber of verbs in one of the sentences.1 The similar-ity between the arguments of a verb pair (Av, Bv)in V is measured as defined in Equation 7, whereArgA and ArgB are the sets of labeled argumentsof the first and the second sentences and |ArgB| isthe number of arguments of the verb in B.2 TheargScore(Aarg, Barg) computation is based on thecosine similarity as in Equation 1.
We treat the to-kens in the argument as a bag-of-words.1This is inherited from the use of the metric focusing on re-call in machine translation, where the B is the reference trans-lation.
In this work a better approach could be to compute thismetric twice, in both directions.2Again, from the analogy of a recall metric for machinetranslation.3 Experiments and ResultsWe use the following state-of-the-art tools to pre-process the data for feature extraction: i) Tree-Tagger3 for lemmas and ii) SENNA (Collobert etal., 2011)4 for Part-of-Speech tagging, Chunking,Named Entity Recognition and Semantic Role La-beling.
SENNA has been reported to achieve an F-measure of 75.79% for tagging semantic roles on theCoNLL-2005 2 benchmark.
The final feature set in-cludes:?
Lexical metrics?
Cosine metric over bag-of-words?
Precision over content words?
Recall over content words?
F-measure over content words?
BLEU metric over chunks?
METEOR metric over words (with stems, syn-onyms and paraphrases)?
Named Entity metric?
Semantic Role Labeling metricThe Machine Learning algorithm used for re-gression is the LIBSVM5 Support Vector Machine(SVM) implementation using the radial basis kernelfunction.
We used a simple genetic algorithm (Backet al, 1999) to tune the parameters of the SVM.
Theconfiguration of the genetic algorithm is as follows:?
Fitness function: minimize the mean squarederror found by cross-validation?
Chromosome: real numbers for SVM parame-ters ?, cost and ?
Number of individuals: 80?
Number of generations: 100?
Selection method: roulette?
Crossover probability: 0.93http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/4http://ml.nec-labs.com/senna/2http://www.lsi.upc.edu/ srlconll/5http://www.csie.ntu.edu.tw/ cjlin/libsvm/676?
Mutation probability: 0.01We submitted three system runs, each is a varia-tion of the above feature set.
For the official submis-sion we used the systems with optimized SVM pa-rameters.
We trained SVM models with each of thefollowing task datasets: MSRpar, MSRvid, SMT-eur and the combination of MSRpar+MSRvid.
Foreach test dataset we applied their respective trainingmodels, except for the new test sets, not covered byany training set: for On-WN we used the combina-tion MSRpar+MSRvid, and for SMT-news we usedSMT-eur.Tables 1 to 3 focus on the Pearson correlationof our three systems/runs for individual datasets ofthe predicted scores against human annotation, com-pared against the official baseline, which uses a sim-ple word overlap metric.
Table 4 shows the aver-age results over all five datasets, where ALL standsfor the Pearson correlation with the gold standardfor the five dataset, Rank is the absolute rank amongall submissions, ALLnrm is the Pearson correlationwhen each dataset is fitted to the gold standard us-ing least squares, RankNrm is the correspondingrank and Mean is the weighted mean across the fivedatasets, where the weight depends on the numberof sentence pairs in the dataset.3.1 Run 1: All except SRL featuresOur first run uses the lexical, BLEU, METEOR andNamed Entities features, without the SRL feature.Table 1 shows the results over the test set, whereRun 1-A is the version without SVM parameter op-timization and Run 1-B are the official results withoptimized parameters for SVM.Task Run 1-A Run 1-B BaselineMSRpar 0.455 0.455 0.433MSRvid 0.706 0.362 0.300SMT-eur 0.461 0.307 0.454On-WN 0.514 0.281 0.586SMT-news 0.386 0.208 0.390Table 1: Results for Run 1 using lexical, chunking,named entities and METEOR as features.
A is the non-optimized version, B are the official results3.2 Run 2: SRL featureIn this run we use only the SRL feature in order toanalyze whether this feature on its own could be suf-ficient or lexical and other simpler features are im-portant.
Table 2 shows the results over the test setwithout parameter optimization (Run 2-A) and theofficial results with optimized parameters for SVM(Run 2-B).Task Run 2-A Run 2-B BaselineMSRpar 0.335 0.300 0.433MSRvid 0.264 0.291 0.300SMT-eur 0.264 0.161 0.454On-WN 0.281 0.257 0.586SMT-news 0.189 0.221 0.390Table 2: Results for Run 2 using the SRL feature only.
Ais the non-optimized version, B are the official results3.3 Run 3: All featuresIn the last run we use all features.
Table 3 showsthe results over the test set without parameter opti-mization (Run 3-A) and the official results with op-timized parameters for SVM (Run 3-B).Task Run 3-A Run 3-B BaselineMSRpar 0.472 0.353 0.433MSRvid 0.705 0.572 0.300SMT-eur 0.471 0.307 0.454On-WN 0.511 0.264 0.586SMT-news 0.410 0.116 0.390Table 3: Results for Run 3 using all features.
A is thenon-optimized version, B are the official results4 DiscussionTable 4 shows the ranking and normalized offi-cial scores of our submissions compared against thebaseline.
Our submissions outperform the officialbaseline but significantly underperform the top sys-tems in the shared task.
The best system (Run 1)achieved an above average ranking, but disappoint-ingly the performance of our most complete system(Run 3) using the semantic metric is poorer.
Sur-prisingly, the results of the non-optimized versionsoutperform the optimized versions used in our offi-cial submission.
One possible reason for that is theoverfitting of the optimized models to the trainingsets.Run 1 and Run 3 have very similar results: theoverall correlation between all datasets of these twosystems is 0.98.
One of the reasons for these resultsis that the SRL metric is compromised by the length677System ALL Rank ALLnrm RankNrm Mean RankMeanRun 1 0.640 36 0.719 71 0.382 80Run 2 0.536 59 0.629 88 0.257 88Run 3 0.598 49 0.696 82 0.347 84Baseline 0.311 87 0.673 85 0.436 70Table 4: Official results and ranking over the test set for Runs 1-3 with SVM parameters optimizedof the sentences.
In the MSRvid dataset, where thesentences are simple such as ?Someone is drawing?,resulting in a good semantic parsing, a high per-formance for this metric is achieved.
However, inthe SMT datasets, sentences are much longer (andoften ungrammatical, since they are produced by amachine translation system) and the performance ofthe metric drops.
In addition, the SRL metric makesmistakes such as judging as highly similar sentencessuch as ?A man is peeling a potato?
and ?A man isslicing a potato?, where the arguments are the samebut the situations are different.5 ConclusionsWe have presented our systems based on similar-ity scores as features to train a regression algorithmto predict the semantic similarity between a pairof sentences.
Our official submissions outperformthe baseline method, but have lower performancethan most participants, and a simpler version of thesystems without any parameter optimization provedmore robust.
Disappointingly, our main contribu-tion, the addition of a metric based on Semantic RoleLabels shows no improvement as compared to sim-pler metrics.AcknowledgmentsThis work was supported by the Mexican NationalCouncil for Science and Technology (CONACYT),scholarship reference 309261.ReferencesThomas Back, David B. Fogel, and ZbigniewMichalewicz, editors.
1999.
Evolutionary Com-putation 1, Basic Algorithms and Operators.
IOPPublishing Ltd., Bristol, UK, 1st edition.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden, July.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained Semantic VerbRelations.
In Dekang Lin and Dekai Wu, editors, Pro-ceedings of EMNLP 2004, pages 33?40, Barcelona,Spain, July.Ronan Collobert, Jason Weston, Leon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural Language Processing (almost) from Scratch.Michael Denkowski and Alon Lavie.
2010.
Meteor-nextand the meteor paraphrase tables: Improved evaluationsupport for five target languages.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Transla-tion and MetricsMATR, pages 339?342, July.Christiane Fellbaum, editor.
1998.
WordNet An Elec-tronic Lexical Database.
Cambridge, MA ; London,May.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics - Volume 2, ACL ?98, pages 768?774, Stroudsburg, PA, USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.Miguel Rios, Wilker Aziz, and Lucia Specia.
2011.
Tine:A metric to assess mt adequacy.
Proceedings of theSixth Workshop on Statistical Machine Translation.Karin Kipper Schuler.
2006.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. thesis,University of Pennsylvania.678
