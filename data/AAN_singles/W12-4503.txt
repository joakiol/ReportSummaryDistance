Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 49?55,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsData-driven Multilingual Coreference Resolution using Resolver StackingAnders Bjo?rkelund and Richa?rd FarkasInstitute for Natural Language ProcessingUniversity of Stuttgart{anders,farkas}@ims.uni-stuttgart.deAbstractThis paper describes our contribution to theCoNLL 2012 Shared Task.1 We present anovel decoding algorithm for coreference res-olution which is combined with a standardpair-wise coreference resolver in a stackingapproach.
The stacked decoders are evaluatedon the three languages of the Shared Task.
Weobtain an official overall score of 58.25 whichis the second highest in the Shared Task.1 IntroductionIn this paper we present our contribution to theCoNLL 2012 Shared Task (Pradhan et al, 2012).We follow the standard architecture where mentionsare extracted in the first step, then they are clusteredusing a pair-wise classifier (see e.g., (Ng, 2010)).For English, the set of extracted mentions is filteredby removing non-referential occurrences of certainpronouns.
Our coreference resolver at its core re-lies on a pair-wise classifier.
To overcome the prob-lems associated with the isolated pair-wise deci-sions, we devised a novel decoding algorithm whichcompares a mention to partially built clusters.
Forour Shared Task contribution we combined this al-gorithm with conventional pair-wise decoding algo-rithms in a stacking approach.In the Shared Task evaluation, our system re-ceived an overall official score of 58.25, which isthe second highest among the sixteen participants.21The system is available for download on http://www.ims.uni-stuttgart.de/?anders/2The overall score is the average of MUC, B3, and CEAFE,averaged over all three languages2 Mention ExtractionSince all mentions are not annotated in Shared Taskdata, but only mentions that take part in coreferencechains, training a general-purpose anaphoricity clas-sifier is non-trivial.
We thus implemented a high-recall, low-precision mention extraction module thatallows the coreference resolver to see most of thepossible mentions, but has to learn to sort out thenon-referential mentions.The mention extraction module relies mainly onthe syntactic parse tree, but also on named entities(which were only provided for English in the pre-dicted versions of the Shared Task data).Since the part-of-speech tags vary a bit across thelanguages, so do our extraction rules: For Arabic,we extract all NP?s, and all terminals with part-of-speech tags PRP and PRP$; for Chinese, we extractall NP?s, and all terminals with part-of-speech tagsPN and NR; for English, we extract all NP?s, all ter-minals with part-of-speech tags PRP and PRP$, andall named entities.Early experiments indicated that the Englishcoreference resolver frequently makes mistakes re-lated to non-referential instances of the pronouns it(often referred to as expletive or pleonastic in the lit-erature), we, and you (generic mentions, which arenot annotated according to the OntoNotes annota-tion guidelines).
To address this issue, we developeda referential/non-referential mention classifier inorder to identify these mentions.
The classifier actsas a filter after the mention extraction module andremoves clear cases of non-referential mentions.Our basic assumption was that when these pro-49th = 0.5 th = 0.95Precision Recall F1 Precision Recall F1 # occurrencesit 75.41 61.92 68 86.78 38.65 53.48 10,307we 65.93 41.61 51.02 75.41 24.20 36.64 5,323you 79.10 74.26 76.60 88.36 51.59 65.15 11,297Average 75.73 63.05 68.81 86.17 41.04 55.60 26,927Table 1: Performance of the non-referential classifier used for English.
Precision, recall, and F-measure are brokendown by pronoun (top three rows), and the micro-average over all three (bottom row).
The left side uses a probabilitythreshold of 0.5, and the right one a threshold of 0.95.
The last column denotes the number of occurrences of thecorresponding token.
All numbers are computed on the development set.nouns do not participate in any coreference chain,they are examples of non-referential mentions.Based on this assumption, we extracted referentialand non-referential examples from the training setand trained binary MaxEnt classifiers using the Mal-let toolkit (McCallum, 2002).Since the mentions filtered by these classifiersare permanently removed, they are never presentedas potential mentions to the coreference resolver.Hence, we aim for a classifier that yields few falsepositives (i.e., mentions classified as non-referentialalthough they were not).
False negatives, on theother hand, may be passed on to the resolver, which,ideally, does not assign them to a cluster.
The pre-cision/recall tradeoff can easily be controlled by ad-justing the threshold of the posterior probability ofthese classifiers, requiring a very high probabilitythat a mention is non-referential.
Preliminary ex-periments indicated that a threshold of 0.95 workedbest when the coreference resolver was trained andevaluated on these filtered mentions.We also found that the target pronouns should behandled separately, i.e., instead of training one sin-gle classifier we trained independent classifiers foreach of the target pronouns.
The individual per-formance of the classifiers, as well as the micro-average over all three pronouns are shown in Ta-ble 1, both using the default probability thresholdof 0.5, and the higher 0.95.
In the final, fine-tunedEnglish coreference system, we found that the useof the classifiers with the higher threshold improvedin all coreference metrics, and gave an increase ofabout 0.5 in the official CoNLL score.The feature set used by the classifiers describesthe (in-sentence) context of the pronoun.
It consistsof the uni-, bi-, and trigrams of word forms and POStags in a window of ?5; the position inside the sen-tence; the preceding and following verb and adjec-tive; the distance to the following named entity; thegenre of the document; and whether the mention isbetween quotes.
For English, we additionally ex-tended this general feature set by re-implementingthe features of Boyd et al (2005).We investigated similar classifiers for Arabic andChinese as well.
We selected targets based on thefrequency statistics of tokens being referential andnon-referential on the training set and used the gen-eral feature set described above.
However, theseclassifiers did not contribute to the more complexcoreference system, hence the non-referential clas-sifiers are included only in the English system.3 Training Instance generationTo generate training instances for the pair-wise clas-sifier, we employed the approach described by Soonet al (2001).
In this approach, for every extractedanaphoric mention mj , we create a positive train-ing instance with its closest preceding antecedentmi: P = {(mi,mj)}.
Negative examples are con-structed by considering all the pairs of mj and the(non-coreferent) mentions mk between mi and mj :N = {(mk,mj)|i < k < j}.
We extract the train-ing examples on the version of the training set thatuses predicted information, and restrict the mentionsconsidered to the ones extracted by our mention ex-traction module.
Using these training examples, wetrain a linear logistic regression classifier using theLIBLINEAR package (Fan et al, 2008).To create training examples for the English clas-sifier, which uses the non-referential classifier forpronouns, we made a 10-fold cross-annotation onthe training set with this classifier.
I.e., the docu-ments were partitioned into 10 sets D1, D2, ..., D10,and when extracting training examples for docu-50ments in Dp, the non-referential classifier trained onDtp =?i 6=pDi was applied.4 DecodingWe implemented several decoding algorithms forour resolver.
The two most common decoding al-gorithms often found in literature are the so-calledBestFirst (henceforth BF) and ClosestFirst (CF) al-gorithms (Ng, 2010).
Both work in a similar man-ner and consider mentions linearly ordered as theyoccur in the document.
They proceed left-to-rightand for every mention mj , they consider all pairs(mi,mj), where mi precedes mj , and queries theclassifier whether they are coreferent or not.
Themain difference between the two algorithms is thatthe CF algorithm selects the closest preceding men-tion deemed coreferent with mj by the classifier,while the BF algorithm selects the most probablepreceding mention.
Most probable is determinedby some sort of confidence measure of how likelytwo mentions are to corefer according to the classi-fier.
For both algorithms, the threshold can also betuned separately, e.g., requiring a probability largerthan a certain threshold thcoref in order to establisha link between two mentions.
Since the logistic clas-sifiers we use directly model a probability distribu-tion, we simply use the posterior probability of thecoref class as our confidence score.Following Bjo?rkelund and Nugues (2011) we alsoimplemented a decoder that works differently de-pending on whether mj is a pronoun or not.
Specifi-cally, for pronouns, the CF algorithm is used, other-wise the BF algorithm is used.
In the remainder, weshall refer to this decoder as PronounsClosestFirst,or simply PCF.4.1 Disallowing transitive nestingA specific kind of mistake we frequently saw in ouroutput is that two clearly disreferent nested mentionsare put in the same cluster.
Although nestednesscan be used as a feature for the classifier, and thisappeared to improve performance, two nested men-tions can still be put into the same cluster becausethey are both classified as coreferent with a different,preceding mention.
The end result is that the twonested mentions are inadvertently clustered throughtransitivity.For example, consider the two occurrences of thephrase her mother in (1) below.
The spans in the ex-ample are labeled alphabetically according to theirlinear order in the document.3 Before the resolverconsiders the last mention d, it has already success-fully placed (a, c) in the same cluster.
The first pairinvolving d is (c, d), which is correctly classified asdisreferent (here, the feature set informs the classi-fier that (c, d) are nested).
However, the pair (a, d)is easily classified as coreferent since the head nounof a agrees in gender and number with d (and theyare not nested).A different problem is related to named entitiesin possessive constructions.
Consider (2), where ourmention extractor extracted e, because it was an NP,and f , because it was tagged as a GPE by the namedentity recognizer.
Again, the pair (e, f) is correctlyclassified as disreferent, but both e and f are likelyto be classified as coreferent with preceding men-tions of Taiwan, since our string matching featureignores possessive markers.
(1) ... she seemed to have such a good relation-ship with [[her]b mother]a.
Like [[her]d mother]ctreated her like a human being ...(2) [[Taiwan]f ?s]eTo circumvent this problem, we let the decodersbuild the clusters incrementally as they work theirway through a document and disallow this type oftransitive nesting.
For instance, when the decoder istrying to find an antecedent for d in (1), a and c havealready been clustered together, and when the pair(c, d) is classified as disreferent, the decoder is con-strained to skip over other members of c?s cluster asit moves backwards in the document.
This modifi-cation gave an increase of about 0.6 in the CoNLLscore for English, and about 0.4 for Arabic and Chi-nese, and we used this constraint whenever we usethe above-mentioned decoders.4.2 A Cluster-Mention Decoding AlgorithmThe pair-wise classifier architecture has, justifiably,received much criticism as it makes decisions basedon single pairs of mentions only.
We therefore de-3We impose a total order on the mentions by sorting themby starting point.
For multiple mentions with the same startingpoint, the longer is considered to precede the shorter.51vised a decoding algorithm that has a better perspec-tive on entire clusters.The algorithm works by incrementally mergingclusters as mentions are processed.
Initially, everymention forms its own cluster.
When the next men-tion mj is processed, it is compared to all the pre-ceding mentions, M = {mi|i < j}.
The score oflinking mj with mi is defined according to:score(mi,mj) = (?mc?CP (coref |(mc,mj)))1/|C|where P (coref |(mi,mj)) is the posterior probabil-ity that mi and mj are coreferent according to thepair-wise classifier, and C denotes the cluster thatmi belongs to.After considering all preceding mentions, thecluster of mj is merged with the cluster of the men-tion with which it had the highest score, assumingthis score is higher than a given threshold thcoref .Otherwise it remains in its own cluster.The task of the score function is to capturecluster-level information.
When mj is compared toa mention mi, the score is computed as the geo-metric mean of the product of the probabilities oflinking mj to all mentions in the cluster that mibelongs to.
Also note that for two preceding men-tions mi1 and mi2 that already belong to the samecluster, score(mi1 ,mj) = score(mi2 ,mj).
I.e., thescore is the same when mj is compared to all men-tions belonging to the same cluster.
Since this algo-rithm works by maximizing the average probabilityfor linking a mention, we dub this algorithm Aver-ageMaxProb, or AMP for short.It should also be noted that other definitionsof the cluster score function score are conceiv-able.4 However, initial experiments with other clus-ter score functions performed worse than the defi-nition above, and time prevented us from exploringthis conclusively.Contrary to the pair-wise decoding algorithmswhere pair-wise decisions are made in isolation, theorder in which mentions are processed make a dif-ference to the AMP decoder.
It is generally ac-cepted that named entities are more informative and4In the extreme case, one could take the maximum of thelink probabilities over the mentions that belong to the clusterC, in which case the algorithm collapses into the BF algorithm.easier to resolve than common noun phrases andpronouns.
To leverage this, we follow Sapena etal.
(2010) who reorder mentions based on mentiontype.
Specifically, we first process proper nounphrases, then common noun phrases, and finally pro-nouns.
This implies that common noun phraseshave to have a reasonable agreement not only withpreceding proper noun phrases of a cluster, but allproper noun phrases in a document (where reason-able means that the geometric average of all poste-rior probabilities stay reasonably high).
Similarly,pronouns are forced agree reasonably with all properand common nouns phrases in a given cluster, andnot only the preceding ones.
Early experimentsshowed an increase in performance using reorder-ing, and we consequently used reordering for all lan-guages in the experiments.5 FeaturesAn advantage of the pair-wise model and of the lin-ear classifiers we use is that they can easily accom-modate very large feature spaces, while still remain-ing reasonably fast.
We exploited this by building alarge number of parametrized feature templates, thatallowed us to experiment easily and quickly withdifferent feature sets.
Additionally, since our clas-sifiers are linear, we also evaluated a large numberof feature conjunctions, which proved to be crucialto gain reasonable performance.Due to space restrictions we can not list the com-plete set of features used in this paper but mentionbriefly what type of features we used.
Most of themare taken from previous work on coreference reso-lution (Soon et al, 2001; Luo and Zitouni, 2005;Sapena et al, 2010; Bjo?rkelund and Nugues, 2011).For a complete list of features the reader can referto the download of the resolver, which includes thefeature sets and parameters used for every language.One set of feature templates we use is based onsurface forms and part-of-speech tags of the first andlast, previous and following, and head tokens of thespans that make up mentions.
Another set of tem-plates are based on the syntax trees, including bothsubcategorization frames as well as paths in the syn-tax tree.
To extract head words of mentions, weused the head percolation rules of Choi and Palmer(2010) for Arabic and English, and those of Zhang52and Clark (2011) for Chinese.While Chinese and English display no or rela-tively small variety in morphological inflection, Ara-bic has a very complex morphology.
This meansthat Arabic suffers from greater data sparseness withrespect to lexical features.
This is exaggerated bythe fact that the Arabic training set is considerablysmaller than the Chinese and English ones.
Hence,we used the lemmas and unvocalised Buckwalterforms that were provided in the Arabic dataset.We also tried to extract number and gender in-formation based on affixes of Arabic surface forms.These features did, however, not help much.
Wedid however see a considerable increase in perfor-mance when we added features that correspond tothe Shortest Edit Script (Myers, 1986) between sur-face forms and unvocalised Buckwalter forms, re-spectively.
We believe that edit scripts are better atcapturing the differences in gender and number sig-naled by certain morphemes than our hand-craftedrules.6 Resolver StackingIn Table 2 we present a comparison of the BF, PCF,and AMP resolvers.
We omit the results of the CFdecoder, since it always did worse and the corre-sponding numbers would not add more to the pic-ture.
The table shows F-measures of mention de-tection (MD), the MUC metric, the B3 metric, andthe entity-based CEAF metric.
The CoNLL score,which is computed as the arithmetic mean of MUC,B3, and CEAFE, is shown in the last row.Comparing the AMP decoder to the pair-wise de-coders, we find that it generally ?
i.e., with respectto the CoNLL average ?
performs worse though italways obtains higher scores with the CEAFE met-ric.
When we looked at the precision and recall formention detection, we also found that the AMP de-coder suffers from lower recall, but higher precision.This led us to conclude that this decoder is more con-servative in terms of clustering mentions, and buildssmaller, but more consistent clusters.
We could alsoverify this when we computed average cluster sizeson the output of the different decoders.In order to combine the strengths of the AMPdecoder and the pair-wise decoders we employedstacking, i.e., we feed the output of one resolverArabic BF PCF AMP StackedMD 58.63 58.49 58.21 60.51MUC 45.8 45.4 43.2 46.66B3 66.65 66.56 66.39 66.3CEAFE 41.52 41.58 43.1 42.57CoNLL 51.32 51.18 50.9 51.84Chinese BF PCF AMP StackedMD 67.22 67.19 66.79 67.61MUC 59.58 59.43 57.23 59.84B3 72.9 72.82 72.7 73.35CEAFE 46.99 46.98 48.25 47.7CoNLL 59.82 59.74 59.39 60.30English BF PCF AMP StackedMD 74.33 74.42 73.75 74.96MUC 66.76 66.93 62.74 67.12B3 70.96 71.11 68.05 71.18CEAFE 45.46 45.83 46.49 46.84CoNLL 61.06 61.29 59.09 61.71Table 2: Performance of different decoders on the devel-opment set for each language.
The configuration of theStacked systems is described in detail in Section 7.as input to a second.
The second resolver is in-formed about the decision of the first one by intro-ducing an additional feature that encodes the deci-sion of the first resolver.
This feature can take fivevalues, depending on how the first resolver treatedthe two mentions in question: NEITHER, when noneof the mentions were placed in a cluster; IONLY,when only the first (antecedent) mention was placedin a cluster; JONLY, when only the second (anaphor)mention was placed in a cluster; COREF, when bothmentions were placed in the same cluster; and DIS-REF, when both mentions were clustered, but in dif-ferent clusters.In addition to the stacking feature, the second re-solver uses the exact same feature set as the first re-solver.
To generate the information for the stack fea-ture for training, we made a 10-fold cross-annotationon the training set, in the same way that we cross-annotated the non-referential classifier for English.In early stacking experiments, we experimentedwith several combinations of the different decoders.We found that stacking different pair-wise decodersdid not give any improvement.
We believe the rea-son for this is that these decoders are too similar andhence can not really benefit from each other.
How-ever, when we used the AMP decoder as the first53step, and a pair-wise decoder as the second, we sawan increase in performance, particularly with respectto the CEAFE metric.7 Feature and Parameter TuningFor every language we tuned decoder parametersand feature sets individually.
The feature sets weretuned semi-automatically by evaluating the additionof a new feature template (or template conjunction)to a baseline set.
Ideally, we would add featuretemplates to the baseline set incrementally one at atime, following a cross-validation on the training set.However, to reduce computational effort and timeconsumption, we resorted to doing only one or twofolds out of a 4-fold cross-validation, and adding thetwo to three most contributing templates in every it-eration to the baseline set.
The feature sets were op-timized to maximize the official CoNLL score usingthe standard BF decoder.For the final submission we tuned the thresholdsfor each decoder, and the choice of pair-wise de-coder to use as the second decoder for each lan-guage.
Modifying the threshold of the AMP decodergave very small differences in overall score and wekept the threshold for this decoder at 0.5.
How-ever, when we increased the probability thresholdfor the second resolver, we found that performanceincreased across all languages.The choice of decoder for the second resolver, andthe probability threshold for this, was determined bya 4-fold cross-validation on the training set.
For ourfinal submission, as well as in the column Stackedin Table 2, we used the following combinations: ForArabic, the threshold was set to 0.60, and the PCFdecoder was used; for Chinese, the threshold was setto 0.65, and the BF decoder was used; for English,the threshold was set to 0.65, and the PCF decoderwas used.8 Official ResultsThe final scores of our system are presented in Ta-ble 3.
The table also includes the results on the sup-plementary tracks: gold mention boundaries (GB),when the perfect boundaries of mentions were given;and gold mentions (GM), when only the mentions inthe gold standard were given (with gold boundaries).For all three settings we used the same model, whichArabic PM GB GMMD 60.55 60.61 76.43MUC 47.82 47.90 60.81B3 68.54 68.61 67.29CEAFE 44.3 44 49.32CoNLL 53.55 53.50 59.14Chinese PM GB GMMD 66.37 71.02 83.47MUC 58.61 63.56 76.85B3 73.10 74.52 76.30CEAFE 48.19 50.20 56.61CoNLL 59.97 62.76 69.92English PM GB GMMD 75.38 75.3 86.16MUC 67.58 67.29 78.70B3 70.26 69.70 72.67CEAFE 45.87 45.27 53.23CoNLL 61.24 60.75 68.20Table 3: Performance on the shared task test set.
Us-ing predicted mentions (PM; i.e., the official evalua-tion), gold mentions boundaries (GB), and gold mentions(GM).was trained on the concatenation of the training andthe development sets.Compared to the results on the development set(cf.
Table 2), we see a slight drop for Chinese andEnglish, but a fairly big increase for Arabic.
Giventhat Chinese and English have the biggest trainingsets, we speculate that the increase in Arabic mightstem from the increased lexical coverage providedby training on both the training and the developmentsets.9 ConclusionWe have presented a novel cluster-based coreferenceresolution algorithm.
This algorithm was combinedwith conventional pair-wise resolution algorithms ina stacking approach.
We applied our system to allthree languages in the Shared Task, and obtained anofficial overall final score of 58.25 which was thesecond highest in the Shared Task.AcknowledgmentsThis work was supported by the DeutscheForschungsgemeinschaft (DFG) via the SFB 732?Incremental Specification in Context?, projects D4(PI Helmut Schmid) and D8 (PI Jonas Kuhn).54ReferencesAnders Bjo?rkelund and Pierre Nugues.
2011.
Explor-ing lexicalized features for coreference resolution.
InProceedings of the Fifteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 45?50, June.Adriane Boyd, Whitney Gegg-Harrison, and Donna By-ron.
2005.
Identifying non-referential it: A machinelearning approach incorporating linguistically moti-vated patterns.
In Proceedings of the ACL Workshopon Feature Engineering for Machine Learning in Nat-ural Language Processing, pages 40?47, June.Jinho D. Choi and Martha Palmer.
2010.
RobustConstituent-to-Dependency Conversion for English.In Proceedings of 9th Treebanks and Linguistic The-ories Workshop (TLT), pages 55?66.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Xiaoqiang Luo and Imed Zitouni.
2005.
Multi-lingualcoreference resolution with syntactic features.
In Pro-ceedings of Human Language Technology Conferenceand Conference on Empirical Methods in Natural Lan-guage Processing, pages 660?667, October.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Eugene W. Myers.
1986.
An O(ND) difference algo-rithm and its variations.
Algorithmica, 1:251?266.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, pages 1396?1411, July.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unrestrictedcoreference in OntoNotes.
In Proceedings of theSixteenth Conference on Computational Natural Lan-guage Learning (CoNLL 2012).Emili Sapena, Llu?
?s Padro?, and Jordi Turmo.
2010.
Aglobal relaxation labeling approach to coreference res-olution.
In Coling 2010: Posters, pages 1086?1094,August.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Yue Zhang and Stephen Clark.
2011.
Syntactic process-ing using the generalized perceptron and beam search.Computational Linguistics, 37(1):105?151.55
