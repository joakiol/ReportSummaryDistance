NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 56?65,Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational LinguisticsGenerating Situated Assisting Utterances to Facilitate Tactile-MapUnderstanding: A Prototype SystemKris Lohmann, Ole Eichhorn, and Timo BaumannDepartment of Informatics, University of HamburgVogt-Ko?lln-Stra?e 3022527 Hamburg, Germany{lohmann,9eichhor,baumann}@informatik.uni-hamburg.deAbstractTactile maps are important substitutes for vi-sual maps for blind and visually impaired peo-ple and the efficiency of tactile-map readingcan largely be improved by giving assisting ut-terances that make use of spatial language.
Inthis paper, we elaborate earlier ideas for a sys-tem that generates such utterances and presenta prototype implementation based on a seman-tic conceptualization of the movements that themap user performs.
A worked example showsthe plausibility of the solution and the outputthat the prototype generates given input derivedfrom experimental data.1 IntroductionHumans use maps in everyday scenarios.
Especiallyfor blind and visually impaired people, tactile mapsare helpful accessible substitutes for visual maps(Espinosa, Ungar, Ochaita, Blades, & Spencer, 1998;Ungar, 2000).
However, tactile maps are less efficientthan visual maps, as they have to be read sequen-tially.
A further problem of physical tactile maps isrestricted availability.
While physical tactile maps arerarely available and costly to produce, modern haptichuman-computer interfaces can be used to presentvirtual variants of tactile maps (virtual tactile maps)providing a similar functionality.
For example, theSensable Phantom Omni device used in our researchenables a user to feel virtual three-dimensional ob-jects (see Figure 1).
It can be thought of as a reverserobotic arm that makes virtual haptic perception pos-sible by generating force feedback.
In the context ofthe research discussed, these objects are virtual tac-tile maps.
These consist of a virtual plane on whichstreets and potential landmarks (such as buildings)are presented as cavities.In recent work, Habel, Kerzel, and Lohmann(2010) have suggested a multi-modal map calledVerbally Assisting Virtual-Environment Tactile Map(VAVETaM) with the goal to enable more efficientacquisition of spatial survey (overview) knowledgefor blind and visually impaired people.VAVETaM extends the approaches towards multi-modal maps (see Section 2) by generating situatedspatial language.
The prototype described reacts tothe user?s exploration movements more like a humanverbally assisting a tactile map reader would do, e.g.,by describing spatial relations between objects onthe map.
The users may explore the map freely, i.e.,they choose which map objects are of interest andin which order they explore them.
This demands forsituated natural language generation (Roy & Reiter,2005), which produces timely appropriate assistingutterances.
Previously, the suggested system has notbeen implemented.The goal of this paper is to show that the ideas ofLohmann, Kerzel, and Habel (2010) and Lohmann,Eschenbach, and Habel (2011) can be implementedin a prototype which is able to generate helpful as-sisting utterances; that is, to show that the language-generation components of VAVETaM are technicallypossible.
The remainder of the paper is structured asfollows: We first briefly survey some related workin Section 2, and then describe the overall structureof VAVETaM in Section 3.
We then present a de-scription of our system in Section 4 paying specialattention to the input to natural language generation(Subsection 4.1) and the generation component itself(Subsection 4.2).
We show the appropriateness of theapproach by discussing an example input, the pro-56Figure 1: The Sensable Omni Haptic Device and a Visual-ization of a Virtual Tactile Map.cesses performed, and the automatically generatedoutput in Section 5 before we close with concludingremarks in Section 6.2 Related WorkTo make maps more accessible for visually impairedpeople by overcoming drawbacks of uni-modal tac-tile maps, a number of multi-modal systems that com-bine haptics and sound have been developed.
Anearly system is the NOMAD system.
It is based ona traditional physical tactile map, which is placedon a touch pad.
The system allows for the associa-tion of sound to objects on the map (Parkes, 1988,1994).
The approach to use traditional physical tac-tile maps as overlays on touch pads has been usedin various systems that were developed subsequently(e.g., Miele, Landau, & Gilden, 2006; Wang, Li,Hedgpeth, & Haven, 2009).
Overviews of researchon accessible maps for blind and visually impairedpeople can be found in Buzzi, Buzzi, Leporini, andMartusciello (2011) and in De Almeida (Vasconcel-los) and Tsuji (2005).
Other researchers have ad-vanced the way haptic perception is realized by usingmore flexible human-computer-interaction systemsthat do not need physical tactile map overlays.
Forexample, Zeng and Weber (2010) have proposed anaudio-tactile system which is based on a large-scalebraille display and De Felice, Renna, Attolico, andDistante (2007) presented the Omero system, whichmakes use of a virtual haptic interface similar to theinterface used in our research.Existing systems work on the basis of sounds orcanned texts that are associated to objects or areason the map.
Sound playback starts when the usertouches a map object or, in some systems, by click-ing or tapping on it.
Yet, when humans are asked toverbally assist a virtual tactile map explorer, theyproduce assisting utterances in which they makemuch more use of spatial language and give briefaugmenting descriptions of the objects that are cur-rently explored and their surroundings (Lohmann etal., 2011).
Based on this, Lohmann and colleaguessuggest which informational content should be in-cluded in assisting utterances for a tactile-map read-ing task.
Among the types of information that aresuggested for verbal assisting utterances is informa-tion allowing for identification of objects, e.g., bystating its name (e.g., ?This is 42nd Avenue?
); in-formation about the spatial relation of objects (?Thechurch is above the museum?
); and talking aboutthe ends of streets that are explored (?This street isrestricted to the left by the map frame?
).Empirical (Wizard-of-Oz-like) research with 24blindfolded sighted participants has concerned anaudio-tactile system that makes use of assisting ut-terances containing the information discussed aboveand shown its potential.
Different outcome measures,among them sketch maps and a verbal task, showedan improved knowledge acquisition with verbal as-sisting utterances compared to a baseline condition inwhich participants verbally only received informationabout the names of objects (Lohmann & Habel, forth-coming).
Empirical research with blind and visuallyimpaired people is ongoing.
Data from the ongoingexperiment with blind and visually impaired partic-ipants is used to show the function of the system inSection 5.3 The Structure of VAVETaMIn this section we will recap the overall structure ofVAVETaM as presented by Habel et al (2010) andLohmann et al (2010).
Figure 2 depicts the relevantparts of the structure.The Virtual-Environment Tactile Map (VETM)57AudoOuputVrtual-EnvronmentTactle Map (VETM)Generaton ofVerbal Assstance(GVA)MEP ObserverFormulaton &ArtculatonMap-KnowledgeReasonng (MKR) ComponentsInformaton FlowPoston Dataof Haptc DevceFigure 2: The Interaction of the Generation Components with Other Components of VAVETaM (modified versionfollowing Habel et al, 2010).knowledge base forms the basis for rendering thetactile map, for analyzing movements, and for verbal-izing assistive utterances forming the central knowl-edge component in the architecture.Knowledge needed for natural-language genera-tion is represented in a propositional format whichis linked to knowledge needed for movement classi-fication and for the haptic presentation of the map.The latter is stored in a spatial-geometric, coordinate-based format.
The knowledge for assistance gener-ation is represented using the Referential-Nets for-malism developed by Habel (1986) and successfullyused by Guhe, Habel, and Tschander (2004) for nat-ural language generation.
Knowledge for verbaliza-tion is organized by interrelated Referential Objects(RefOs), which are the potential objects of discourse.A referential object consists of an identifier for theobject (an arbitrary string, for example pt3), addi-tional associated information such as the sort of theobject, and associated propositional information thatcan be verbalized (such as the name of the objectand relations to other objects, e.g., that the object is?left of?
another object).
Important sorts of objectsin the map domain are potential landmarks, regions,the frame of the map, and tracks and track segments1.See Lohmann et al (2011) for a discussion of thepropositional layer of the VETM knowledge base.The Haptic Device provides a stream of positiondata.
This stream of data is the input to the Map-Exploratory-Procedures Observer (MEP Observer)component and its subcomponents which analyzesthe movements the map user performs.
By categoriz-ing the movements and specifying them with identi-1A track is a structure enabling locomotion, such as a street.The meaning of the term is similar to the meaning of the term?path?
introduced by Lynch (1960).fiers of the objects currently explored by the user, aconceptualization of the user?s movements is createdthat is suitable as input to the component dealingwith assisting-utterance generation.
For the case oftactile-map explorations, different circumstances af-fect which information shall be given via naturallanguage in an exploration situation: (a) what kindof information is the user is trying to get (explorationcategory), (b) about which object the user is tryingto get information, and (c) what has happened before(history).The Map-Knowledge Reasoning (MKR) compo-nent serves as memory for both the MEP Observerand the GVA component by keeping track of ver-bal and haptic information that has been presentedto the user.
This component hence helps to avoidunnecessary verbal repetitions.The Generation of Verbal Assistance (GVA) com-ponent, which is at the core of the prototype thatwe will present in Section 4, solves the central taskof natural language generation.
This component se-lects the knowledge that is suitable for verbalizationin an exploration situation from the VETM knowl-edge base and prepares it in a way appropriate forfurther output.
It sends preverbal messages (PVMs,see Levelt, 1989), propositional representations ofthe semantics of the planned utterance, to the Formu-lation & Articulation components for the generationof a surface structure and final utterance.4 Description of the PrototypeIn order to show how an artificial system is able togenerate situated assistance in a well-formed fashion,we present a prototype implementation of the corecomponents for natural language generation in theVAVETaM system.58We implemented dummy components in place forthe Map-Knowledge Reasoning (MKR) and MEPObserver components to allow us to test the naturallanguage output.
The MKR Simulator provides basicfunctions sufficient to avoid unnecessary repetitionsof utterances by preventing production of the samemessage for a defined time period.
An exceptionto this rule are those messages that are needed toidentify an object on the map, such as ?This is Dorfs-tra?e?, which are given every time the user touchesan object.2 The MEP Simulator generates input tothe component as the MEP Observer is planned todo (see Kerzel & Habel, 2011, for a discussion of apossible technical realization).In the following subsection, we will discuss Map-Exploratory Procedures (MEPs), which are outputby the MEP Observer and form the basic input tothe generation component (GVA), which we thendiscuss in Subsection 4.2.
Finally, we present theinner workings of the Formulation & Articulationcomponents in Subsection 4.3.4.1 Conceptualization of the User?s MovementsOne of the core challenges for situated natural lan-guage generation is to timely connect the user?s per-cepts (in the case of virtual-tactile-map explorationindicated by movements that the user performs withthe device) to symbolic natural language (Roy &Reiter, 2005).
The task to be solved is to have awell-specified conceptualization of exploration situa-tions.
An exploration situation is constituted by thekind of movements the user performs, the map ob-jects the user wants to gain knowledge about (whichconstitutes the haptic focus (Lohmann et al, 2011)),and the haptic exploration and verbalization history.In the structure of the VAVETaM system, the MEPObserver fulfills the task of categorizing the user?smovements and detecting objects in the haptic focus.Lohmann et al (2011) discuss how Map-ExploratoryProcedures (MEPs), a specialization of ExploratoryProcedures, introduced as categories of general hap-tic interaction by Lederman and Klatzky (2009), canbe used to categorize the map user?s movements.MEP types are shown in Table 1.For example, a trackMEP is, straightforwardly,2User studies showed that the verbal identification is neces-sary to recognize the haptic objects.MEP Type IndicationtrackMEP Exploration of a trackor track segment objectlandmarkMEP Exploration of a potentiallandmark objectregionMEP Exploration of a region objectframeMEP Exploration of a frame objectstopMEP No explorationTable 1: Map Exploratory Procedures (MEPs).Dorfstra?ePotential landmarksTracks (ptX) and overlappingTrack Segments (ptsX)ptXptsXpts55Blumen-stra?eAmselweg pt5The user?s movementAldi LidlFigure 3: Visualization of a Part of a Virtual Tactile Map.characterized by a track-following movement indi-cating that the user wants to know something abouta track object.
MEPs are (optionally) specified withidentifier(s) that link objects on the propositionallayer of the VETM knowledge base as belonging tothe haptic focus of the MEP.In this work, we extend the concept to be able tocope with multiple objects or parts of objects thatcan simultaneously be in the user?s haptic focus.
Thefollowing example illustrates overlapping haptic foci(see Figure 3).
Consider the track with the name?Dorfstra?e?
being represented as track object pt5 onthe propositional layer of the VETM knowledge base.If the track pt5 forms a dead end, this dead end canadditionally be represented as a unique track segmentobject (pts55).
When the user explores the track pt5from the left to the right, at a certain point, both pt5and pts55 are in the haptic focus.Since the user is exploring a track, the movementis characterized by a trackMEP which is specifiedby the objects pt5 and pts55 and either will be in theprimary haptic focus.3 Thus, in this case, pts55 is in3Notice that the decision whether in fact pt5 or pts55 are59GVA AgendaPVM ConstructionGVA Controller ...Utterance Plans &Agenda OperationsFormulation &ArticulationMKR (Map-Knowledge Reasoner) / MKR SimulatorMEPObserver/MEPSimulator Passive KnowledgeComponentsActive ProcessingComponentsInformation FlowVETMFigure 4: The Architecture of the Generation of Verbal Assistance Component.the secondary haptic focus.
It is reasonable to talkabout both, the track and the dead end itself.
As anotational convention, we denote MEPs by their type,the object in primary focus (if available), and a (possi-bly empty) list of objects in secondary focus.
For theexample above, we write trackMEP(pt5, [pts55]).4.2 Structure of the Generation ComponentThe focus of our prototype is on the GVA componentof the VAVETaM system that solves the ?What tosay??
task, the task of determining the content appro-priate for utterance in an exploration situation (Reiter& Dale, 2000; De Smedt, Horacek, & Zock, 1996).This component interacts with different componentsintroduced above (see Section 3): (1) it receives theconceptualization of the user?s movements (MEPsand specifications) from the MEP Observer; (2) itaccesses the propositional layer of the VETM knowl-edge base in order to retrieve information about theobjects that is suitable for verbalization; (3) it inter-acts with the MKR component, which keeps track ofthe exploration and verbalization history; and (4) itthen sends semantic representations in the form ofpreverbal messages (PVMs) to the Formulation &Articulation components.The GVA component consists of several subcom-ponents which are visualized in Figure 4.
The GVAController controls the execution of other processesthrough controlling the Agenda, which is an orderedlist of preverbal-message representations of utter-ances.4 Once the GVA component receives a (spec-ified) MEP describing the user?s movements fromthe MEP Observer, it looks up Utterance Plans &focussed primarily upon is up to the MEP Observer component.4The term ?Agenda?
is used in a similar context in the Colla-gen system (Rich, Sidner, & Lesh, 2001).Agenda Operations that specify which information toexpress is suitable in the given exploration situationand where it should be placed on the Agenda.
ThePVM Construction component searches an utteranceplan that allows to construct a preverbal messagethat contains this information.
The top element ofthe Agenda is passed on to the Formulation & Ar-ticulation component as soon as that component hasfinished uttering the previous element.In the current implementation of the GVA, utter-ance plans are stored as lists of potential messagesand construction rules.
For example, with a track-MEP, associated knowledge is stored that the objectshall first be identified (by either stating the nameassociated to that object, e.g., ?Dorfstra?e?
or choos-ing a referring expression that allows for definiteidentification).
Then, if available, information aboutgeometric relations such as parallelism with otherlinear objects on the map is selected from the VETMknowledge base, followed by information about spa-tial relations with other map objects.
Subsequently,the construction of a preverbal message that informsthe user about the extent of the track in the hapticfocus is tried, followed by information about cross-ings the track has.
For each of these constructionrules is tested whether the VETM knowledge basecontains suitable information.
If it does, a preverbalmessage is generated and added to the Agenda unlessthe MKR component rejects the message becausethis utterance is inappropriate given the explorationand verbalization history, which prevents unneces-sary repetitions of information.
For example, if theuser has previously explored the track pt5 and alreadyreceived the information that the buildings ?Lidl?
and?Aldi?
(cf.
Figure 3) are above the track a short timebefore, the articulation of this information is pre-60Figure 5: Literal Translation of the Template for a GermanIdentification Message.vented and the user is given other information (ornone, if no more suitable information is available).4.3 Formulation and ArticulationIn the prototype system presented, formulation isimplemented in a template-based approach (Reiter& Dale, 2000).
The Formulation component usesa set of sentence templates which consist of partiallexicalizations and gaps to fill with information forthe exploration situations.
Additionally, a lexiconstores knowledge about natural language expressionsthat can be used to express spatial situations.
Fig-ure 5 shows a simple template used for the genera-tion of identification messages.5 Of the four utter-ance parts depicted in the left box, one is chosenrandomly enabling some variation in the utterances.If the MKR component has marked the preverbalmessage as a repetition of a previously articulatedutterance, a marker word is placed in the sentence(here: ?again?).
Then, the sentence is completed byeither selecting the name of the object in focus fromthe VETM knowledge base or by selecting a referringexpression.
The former results in utterances such as?This is Dorfstra?e?.
This text is then synthesizedusing text-to-speech (TTS) software.5 A Worked ExampleAs described, the development of the component thatconceptualizes the user?s movement is not yet fin-ished.
Therefore, to show the function of the imple-mentation, we used example inputs that were derivedby manually annotating screen-records from experi-mental data that was previously collected in Wizard-of-Oz-like experiments with blindfolded sighted,blind, and visually impaired people.
In these ex-5Note that the system is implemented in German; the order-ing of elements indead leads to grammatically correct Germansentences.periments, participants received pre-recorded verbalassisting utterances that were selected by the experi-menter using a custom-built software tool based on avisualization of the user?s movement on a computerscreen (Lohmann & Habel, forthcoming, and Sec-tion 2).
Using video records of the visualizationsof the user?s movements, the first author manuallyannotated the relevant MEPs and their specificationsthat, in the VAVETaM structure, the MEP Observercomponent should output.
These manually annotatedMEPs form the input to test the prototype system.6In order to exemplify the function of the gener-ation system, a small part of one of the annotatedinputs is detailed in this section.7 Figure 6 visual-izes a part of the movement of a visually impairedmap explorer and the corresponding names and iden-tifiers of the objects used for the specification of theMEPs in the VETM knowledge base.
As the figureshows, the map explorer touches the track pt3, com-ing from the left.
The track is explored for a whilewith small movements.
(This position is remainedfor a relatively long time, maybe listening to the on-going utterances.)
Then, the map explorer proceedsto the bottom end of the track before following thetrack upwards.
Figure 6 shows that the bottom end ofthe track is conceptualized as distinct track segment,track segment pts33, which is part of the track pt3.pt3Amselwegpts33Kartenrand [map frame]Figure 6: Example Movement a Visually Impaired MapExplorer Performed in an Ongoing Experiment.The annotated MEPs and their specification of thissmall exploration movement are shown in Table 2.The GVA component and the Formulation & Articu-lation components generate detailed log files that in-6Detecting MEPs is an instance of event detection in virtualhaptic environments (Kerzel & Habel, 2011), which showedits applicability for the task in an early prototype (M. Kerzel,personal communication).7We also tested other annotated inputs; this example is repre-sentative of the behavior of the prototype.61Time in Seconds Input to the GVA.
.
.
.
.
.33.0?54.0 trackMEP(pt3)54.0?57.0 trackMEP(pt3, [pts33])57.0?57.8 trackMEP(pt3).
.
.
.
.
.Table 2: Manually Categorized MEPs and Specificationsfor the Exploration Depicted in Figure 6.dicate which information has been selected from theVETM knowledge base, which preverbal messages(PVMs) are put onto the Agenda, and how utterancesare articulated.
Based on the log files, we detail theprocesses performed by the GVA component and theresulting verbal output in Table 3.During the user?s long first exploration movementof the track pt3 from seconds 33 to 54, which is con-ceptualized by trackMEP(pt3), the GVA componentexpresses all the information that is associated withthe track pt3 in the VETM.
The first message informsthe user about the identity of the track by stating theidentifying utterance ?This is Amselweg?.
Then, theuser is informed about geometric relations of thistrack to other tracks.
In the present case, informationabout parallelism with the track pt4 is available in theVETM and a corresponding utterance is produced.Subsequently, the user is informed about the extentof the track, i.e., where it ends.
Then, informationabout the intersections the track has is uttered.
Theseare all assisting utterances that are possible given thecurrent MEP and the knowledge base.8Next, the user moves downwards resulting in thedistinct track segment pts33 coming into secondaryfocus.
All PVMs about the object in primary focus(pt3) are blocked by the MKR component, as theyhave just been uttered.
Thus, a message that informsthe user about his or her position on the track segmentis formulated, resulting in a message such as ?Here,Amselweg is restricted by the map frame?.
When theuser leaves the track segment pt33, no further assist-ing utterances are given as all information associatedwith the track pt3 has been expressed recently.8Note that the order in which information is given is fixedin the current system as explained in Subsection 4.2.
Whethergiving the messages in another order, which is potentially moreflexible, is more helpful, has to be further evaluated.. .
.33.0?54.0 sMEP Simulator fires trackMEP(pt3)GVA receives: trackMEP(pt3)GVA clears agenda due to MEP changePVM Construction is able to generate PVMs of class:Identification, Geometric Relation, Extension, Junc-tionsPVMs Identification, Geometric Relation, Extension,Junctions, are put on the Agenda (0 prohibited by Map-Knowledge Reasoner)Formulation getting Identification PVM for the RefOpt3: the following aspects have been chosen by PVMConstruction: name ?Amselweg?Speechout: ?Dies ist der Amselweg.?
[?This is Amsel-weg.?
]Formulation getting Geometric Relation PVM for theRefO pt3: the following aspects have been chosen bythe PVM Construction: IS PARALLEL TO with thearguments [pt3, pt4]Speechout: ?Parallel zu ihm verla?uft die Blumenstra?e.?[?.
.
.
which is parallel to Blumenstra?e?
]Formulation getting Extent PVM for the RefO pt3: thefollowing aspects have been chosen by the PVM Con-struction: predicate HAS UPPER LIMIT with the ar-guments [pt3, ptco1]; predicate HAS LOWER LIMITwith the arguments [pt3, pfr3]Speechout: ?.
.
.
er muendet nach oben in die Dorfstra?eund endet unten am Kartenrand.?
[?.
.
.
it forms a cornerwith Dorfstra?e at the top and at the bottom is restrictedby the map frame.?
]Formulation getting Junctions PVM for the RefO pt3:the following aspects have been chosen by the PVMConstruction: predicate IS IN TRACK CONFIG withthe arguments [pt3, ptco4]Speechout: ?Au?erdem hat er eine Kreuzung mitder Hochstra?e.?
[?Furthermore, the street crossesHochstra?e.?
]54.0?57.0 sMEP Simulator changes MEP specification to track-MEP(pt3, [pts33])GVA receives: trackMEP(pt3, [pts33])GVA detects secondary focus changePVM Construction is able to generate PVMs of class:IdentificationIdentification-class PVM is put at the front of theAgenda (0 prohibited by Map-Knowledge Reasoner)Formulation getting Identification PVM for the RefOpts33.
.
.62. .
.Speechout: ?Hier endet der Amselweg am Kartenrand.?
[?Here, Amselweg is restricted by the map frame.?
]57.0?57.8 sMEP Simulator changes MEP specification to track-MEP(pt3)GVA receives: trackMEP(pt3)Nothing happens, primary focus not new.
.
.Table 3: The Processes and Output (German and Trans-lated) of the GVA and the Formulator.6 ConclusionWe presented a prototype system that generates situ-ated assisting utterances for tactile-map explorationsto ease tactile map learning.
The prototype is basedon an earlier concept.
We focussed on the GVAcomponent in the system, which solves the ?Whatto say??
task of natural language generation, takinginto account the situated context.
We exemplifiedthe working of the component in a testing environ-ment based on a conceptualization of a part of areal tactile-map exploration, for which it generatesplausible and timely output that is comparable toassisting utterances that were in previous researchtested in Wizard-of-Oz-like experiments with blind-folded sighted people and in ongoing experimentswith blind and visually impaired people.
Therefore,we conclude that a generation system working in themanner described is technically possible.
We alsoexplained in detail the structure and implementationof MEPs, which are the basis for categorization of theuser?s movements and, with additional specification,the input to the GVA component.More fine-grained analysis is needed to gainknowledge (1) about how much information shouldbe given via the verbal channel to maximize effi-ciency, and (2) whether the system can be improvedby using more flexible Utterance Plans.7 Discussion and OutlookOne problem which became apparent in the experi-ments and also in preliminary tests of the fully inte-grated prototype system is the fact that the user?s ex-ploration movements on the map may be very quick.In these cases, the information to be delivered mayalready be outdated when the assistive utterance con-veys this information.
This is partly due to the Ger-man word order, as can be seen in Figure 5, whichshows the template for identification messages.Problems can occur in cases where an utteranceis verbalized shortly before the user starts exploringanother map object.
In this case, the exploration situ-ation changes during articulation.
Currently, the com-ponents concerned with language generation work ina modularized sequential manner without feedback.If an utterance was sent to formulation, it cannot notbe changed anymore.
Hence, it can happen that as-sisting utterances and the user?s exploration are notin all cases timely.One possible remedy to this problem is to extendthe formulation to work in an incremental fashionsuch that it explicitly handles situations in which acurrently articulated utterance is outdated (e.g., anidentification utterance that is no longer valid becausethe object to be identified has gone out of focus) andby altering it to a new utterance of similar structure(i.e., an identification utterance for a different ob-ject which just came into the haptic focus).
In thiscase, it could adapt the ongoing utterance (if it isstill in an early stage of production) to replace theprevious identifying word (e.g., ?Amselweg?)
withthe new word (i.e., ?Dorfstra?e?).
Of course, this isonly possible if the articulation (text-to-speech syn-thesis) works in an incremental fashion (i.e., it is ableto change yet unspoken parts of an ongoing utter-ance).
Such work is currently ongoing and we planto integrate this functionality in our future work.AcknowledgmentsThe research reported in this paper has been partiallysupported by DFG (German Science Foundation) inIRTG 1247 ?Cross-modal Interaction in Natural andArtificial Cognitive Systems?
(CINACS).
We thankthe anonymous reviewers for their highly useful com-mentaries.ReferencesBuzzi, M. C., Buzzi, M., Leporini, B., & Martu-sciello, L. (2011).
Making visual maps acces-sible to the blind.
Universal Access in Human-Computer Interaction.
Users Diversity, 271?280.63De Almeida (Vasconcellos), R. A., & Tsuji, B.(2005).
Interactive mapping for people who areblind or visually impaired.
In Modern cartog-raphy series (Vol.
4, pp.
411?431).
Elsevier.De Felice, F., Renna, F., Attolico, G., & Distante,A.
(2007).
A haptic/acoustic application toallow blind the access to spatial information.In World haptics conference (pp.
310 ?
315).De Smedt, K., Horacek, H., & Zock, M. (1996).Architectures for natural language generation:Problems and perspectives.
Trends in NaturalLanguage Generation An Artificial IntelligencePerspective, 17?46.Espinosa, M. A., Ungar, S., Ochaita, E., Blades, M.,& Spencer, C. (1998).
Comparing methods forintroducing blind and visually impaired peopleto unfamiliar urban environments.
Journal ofEnvironmental Psychology, 18, 277 ?
287.Guhe, M., Habel, C., & Tschander, L. (2004).
Incre-mental generation of interconnected preverbalmessages.
In T. Pechmann & C. Habel (Eds.
),Multidisciplinary approaches to language pro-duction (pp.
7?52).
Berlin, New York: DeGruyter.Habel, C. (1986).
Prinzipien der Referentialita?t.Berlin, Heidelberg, New York: Springer.Habel, C., Kerzel, M., & Lohmann, K. (2010).
Ver-bal assistance in Tactile-Map explorations: Acase for visual representations and reasoning.In Proceedings of AAAI workshop on visualrepresentations and reasoning 2010.Kerzel, M., & Habel, C. (2011).
Monitoring and de-scribing events for virtual-environment tactile-map exploration.
In M. F. W. A. Galton &M. Duckham (Eds.
), Proceedings of workshopon ?identifying objects, processes and events?,10th international conference on spatial infor-mation theory.
Belfast, ME.Lederman, S., & Klatzky, R. (2009).
Haptic per-ception: A tutorial.
Attention, Perception, &Psychophysics, 71(7), 1439?1459.Levelt, W. J. M. (1989).
Speaking: From intention toarticulation.
Cambridge, MA: The MIT Press.Lohmann, K., Eschenbach, C., & Habel, C. (2011).Linking spatial haptic perception to linguis-tic representations: Assisting utterances forTactile-Map explorations.
In M. Egenhofer,N.
Giudice, R. Moratz, & M. Worboys (Eds.
),Spatial information theory (pp.
328?349).Berlin, Heidelberg: Springer.Lohmann, K., & Habel, C. (forthcoming).
Extendedverbal assistance facilitates knowledge acqui-sition of virtual tactile maps.
Accepted forpresentation at Spatial Cognition 2012.Lohmann, K., Kerzel, M., & Habel, C. (2010).
Gen-erating verbal assistance for Tactile-Map ex-plorations.
In I. van der Sluis, K. Bergmann,C.
van Hooijdonk, & M. Theune (Eds.
), Pro-ceedings of the 3rd workshop on multimodaloutput generation 2010.
Dublin.Lynch, K. (1960).
The image of the city.
Cambridge,MA; London: MIT Press.Miele, J.
A., Landau, S., & Gilden, D. (2006).
Talk-ing TMAP: automated generation of audio-tactile maps using Smith-Kettlewell?s TMAPsoftware.
British Journal of Visual Impairment,24(2), 93?100.Parkes, D. (1988).
?NOMAD?
: An audio-tactiletool for the acquisition, use and managementof spatially distributed information by partiallysighted and blind people.
In Proceedings ofthe 2nd international conference on maps andgraphics for visually disabled people.
Notting-ham, UK.Parkes, D. (1994).
Audio tactile systems for de-signing and learning complex environments asa vision impaired person: static and dynamicspatial information access.
Learning Environ-ment Technology: Selected Papers from LETA,94, 219?223.Reiter, E., & Dale, R. (2000).
Building natural lan-guage generation systems.
Cambridge: Cam-bridge University Press.Rich, C., Sidner, C. L., & Lesh, N. (2001).
Colla-gen: applying collaborative discourse theoryto human-computer interaction.
AI magazine,22(4), 15?26.Roy, D., & Reiter, E. (2005).
Connecting languageto the world.
Artificial Intelligence, 167(1-2),1?12.Ungar, S. (2000).
Cognitive mapping without visualexperience.
In R. Kitchin & S.
Freundschuh(Eds.
), Cognitive mapping: Past, present andfuture (pp.
221?248).
London: Routledge.Wang, Z., Li, B., Hedgpeth, T., & Haven, T. (2009).Instant tactile-audio map: enabling access to64digital maps for people with visual impairment.In Proceeding of the 11th international ACMSIGACCESS conference on computers and ac-cessibility (pp.
43?50).
Pittsburg, PA.Zeng, L., & Weber, G. (2010).
Audio-haptic browserfor a geographical information system.
InK.
Miesenberger, W. Zagler, & A.
Karschmer(Eds.
), Computers helping people with specialneeds, part II (pp.
466?473).65
