Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553?560,Sydney, July 2006. c?2006 Association for Computational LinguisticsExploiting Comparable Corpora and Bilingual Dictionariesfor Cross-Language Text CategorizationAlfio Gliozzo and Carlo StrapparavaITC-Irstvia Sommarive, I-38050, Trento, ITALY{gliozzo,strappa}@itc.itAbstractCross-language Text Categorization is thetask of assigning semantic classes to docu-ments written in a target language (e.g.
En-glish) while the system is trained using la-beled documents in a source language (e.g.Italian).In this work we present many solutions ac-cording to the availability of bilingual re-sources, and we show that it is possibleto deal with the problem even when nosuch resources are accessible.
The coretechnique relies on the automatic acquisi-tion of Multilingual Domain Models fromcomparable corpora.Experiments show the effectiveness of ourapproach, providing a low cost solution forthe Cross Language Text Categorizationtask.
In particular, when bilingual dictio-naries are available the performance of thecategorization gets close to that of mono-lingual text categorization.1 IntroductionIn the worldwide scenario of the Web age, mul-tilinguality is a crucial issue to deal with andto investigate, leading us to reformulate most ofthe classical Natural Language Processing (NLP)problems into a multilingual setting.
For in-stance the classical monolingual Text Categoriza-tion (TC) problem can be reformulated as a CrossLanguage Text Categorization (CLTC) task, inwhich the system is trained using labeled exam-ples in a source language (e.g.
English), and itclassifies documents in a different target language(e.g.
Italian).The applicative interest for the CLTC is im-mediately clear in the globalized Web scenario.For example, in the community based trade (e.g.eBay) it is often necessary to archive texts in dif-ferent languages by adopting common merceolog-ical categories, very often defined by collectionsof documents in a source language (e.g.
English).Another application along this direction is CrossLingual Question Answering, in which it wouldbe very useful to filter out the candidate answersaccording to their topics.In the literature, this task has been proposedquite recently (Bel et al, 2003; Gliozzo and Strap-parava, 2005).
In those works, authors exploitedcomparable corpora showing promising results.
Amore recent work (Rigutini et al, 2005) proposedthe use of Machine Translation techniques to ap-proach the same task.Classical approaches for multilingual problemshave been conceived by following two main direc-tions: (i) knowledge based approaches, mostly im-plemented by rule based systems and (ii) empiricalapproaches, in general relying on statistical learn-ing from parallel corpora.
Knowledge based ap-proaches are often affected by low accuracy.
Suchlimitation is mainly due to the problem of tun-ing large scale multilingual lexical resources (e.g.MultiWordNet, EuroWordNet) for the specific ap-plication task (e.g.
discarding irrelevant senses,extending the lexicon with domain specific termsand their translations).
On the other hand, em-pirical approaches are in general more accurate,because they can be trained from domain specificcollections of parallel text to represent the appli-cation needs.
There exist many interesting worksabout using parallel corpora for multilingual appli-cations (Melamed, 2001), such as Machine Trans-lation (Callison-Burch et al, 2004), Cross Lingual553Information Retrieval (Littman et al, 1998), andso on.However it is not always easy to find or buildparallel corpora.
This is the main reason whythe ?weaker?
notion of comparable corpora is amatter of recent interest in the field of Computa-tional Linguistics (Gaussier et al, 2004).
In fact,comparable corpora are easier to collect for mostlanguages (e.g.
collections of international newsagencies), providing a low cost knowledge sourcefor multilingual applications.The main problem of adopting comparable cor-pora for multilingual knowledge acquisition is thatonly weaker statistical evidence can be captured.In fact, while parallel corpora provide stronger(text-based) statistical evidence to detect transla-tion pairs by analyzing term co-occurrences intranslated documents, comparable corpora pro-vides weaker (term-based) evidence, because textalignments are not available.In this paper we present some solutions to dealwith CLTC according to the availability of bilin-gual resources, and we show that it is possibleto deal with the problem even when no such re-sources are accessible.
The core technique relieson the automatic acquisition of Multilingual Do-main Models (MDMs) from comparable corpora.This allows us to define a kernel function (i.e.
asimilarity function among documents in differentlanguages) that is then exploited inside a SupportVector Machines classification framework.
Wealso investigate this problem exploiting synset-aligned multilingual WordNets and standard bilin-gual dictionaries (e.g.
Collins).Experiments show the effectiveness of our ap-proach, providing a simple and low cost solu-tion for the Cross-Language Text Categorizationtask.
In particular, when bilingual dictionar-ies/repositories are available, the performance ofthe categorization gets close to that of monolin-gual TC.The paper is structured as follows.
Section 2briefly discusses the notion of comparable cor-pora.
Section 3 shows how to perform cross-lingual TC when no bilingual dictionaries areavailable and it is possible to rely on a compa-rability assumption.
Section 4 present a moreelaborated technique to acquire MDMs exploitingbilingual resources, such as MultiWordNet (i.e.a synset-aligned WordNet) and Collins bilingualdictionary.
Section 5 evaluates our methodolo-gies and Section 6 concludes the paper suggestingsome future developments.2 Comparable CorporaComparable corpora are collections of texts in dif-ferent languages regarding similar topics (e.g.
acollection of news published by agencies in thesame period).
More restrictive requirements areexpected for parallel corpora (i.e.
corpora com-posed of texts which are mutual translations),while the class of the multilingual corpora (i.e.collection of texts expressed in different languageswithout any additional requirement) is the moregeneral.
Obviously parallel corpora are also com-parable, while comparable corpora are also multi-lingual.In a more precise way, let L ={L1, L2, .
.
.
, Ll} be a set of languages, letT i = {ti1, ti2, .
.
.
, tin} be a collection of texts ex-pressed in the language Li ?
L, and let ?
(tjh, tiz)be a function that returns 1 if tiz is the translationof tjh and 0 otherwise.
A multilingual corpus isthe collection of texts defined by T ?
= ?i T i. Ifthe function ?
exists for every text tiz ?
T ?
andfor every language Lj , and is known, then thecorpus is parallel and aligned at document level.For the purpose of this paper it is enough to as-sume that two corpora are comparable, i.e.
theyare composed of documents about the same top-ics and produced in the same period (e.g.
possiblyfrom different news agencies), and it is not knownif a function ?
exists, even if in principle it couldexist and return 1 for a strict subset of documentpairs.The texts inside comparable corpora, beingabout the same topics, should refer to the sameconcepts by using various expressions in differentlanguages.
On the other hand, most of the propernouns, relevant entities and words that are not yetlexicalized in the language, are expressed by usingtheir original terms.
As a consequence the sameentities will be denoted with the same words indifferent languages, allowing us to automaticallydetect couples of translation pairs just by look-ing at the word shape (Koehn and Knight, 2002).Our hypothesis is that comparable corpora containa large amount of such words, just because texts,referring to the same topics in different languages,will often adopt the same terms to denote the sameentities1 .1According to our assumption, a possible additional cri-554However, the simple presence of these sharedwords is not enough to get significant results inCLTC tasks.
As we will see, we need to exploitthese common words to induce a second-ordersimilarity for the other words in the lexicons.2.1 The Multilingual Vector Space ModelLet T = {t1, t2, .
.
.
, tn} be a corpus, and V ={w1, w2, .
.
.
, wk} be its vocabulary.
In the mono-lingual settings, the Vector Space Model (VSM)is a k-dimensional space Rk, in which the texttj ?
T is represented by means of the vector ~tjsuch that the zth component of ~tj is the frequencyof wz in tj .
The similarity among two texts in theVSM is then estimated by computing the cosine oftheir vectors in the VSM.Unfortunately, such a model cannot be adoptedin the multilingual settings, because the VSMs ofdifferent languages are mainly disjoint, and thesimilarity between two texts in different languageswould always turn out to be zero.
This situationis represented in Figure 1, in which both the left-bottom and the rigth-upper regions of the matrixare totally filled by zeros.On the other hand, the assumption of corporacomparability seen in Section 2, implies the pres-ence of a number of common words, representedby the central rows of the matrix in Figure 1.As we will show in Section 5, this model israther poor because of its sparseness.
In the nextsection, we will show how to use such words asseeds to induce a Multilingual Domain VSM, inwhich second order relations among terms anddocuments in different languages are consideredto improve the similarity estimation.3 Exploiting Comparable CorporaLooking at the multilingual term-by-documentmatrix in Figure 1, a first attempt to merge thesubspaces associated to each language is to exploitthe information provided by external knowledgesources, such as bilingual dictionaries, e.g.
col-lapsing all the rows representing translation pairs.In this setting, the similarity among texts in dif-ferent languages could be estimated by exploit-ing the classical VSM just described.
However,the main disadvantage of this approach to esti-mate inter-lingual text similarity is that it stronglyterion to decide whether two corpora are comparable is toestimate the percentage of terms in the intersection of theirvocabularies.relies on the availability of a multilingual lexicalresource.
For languages with scarce resources abilingual dictionary could be not easily available.Secondly, an important requirement of such a re-source is its coverage (i.e.
the amount of possibletranslation pairs that are actually contained in it).Finally, another problem is that ambiguous termscould be translated in different ways, leading us tocollapse together rows describing terms with verydifferent meanings.
In Section 4 we will see howthe availability of bilingual dictionaries influencesthe techniques and the performance.
In the presentSection we want to explore the case in which suchresources are supposed not available.3.1 Multilingual Domain ModelA MDM is a multilingual extension of the conceptof Domain Model.
In the literature, Domain Mod-els have been introduced to represent ambiguityand variability (Gliozzo et al, 2004) and success-fully exploited in many NLP applications, such asWord Sense Disambiguation (Strapparava et al,2004), Text Categorization and Term Categoriza-tion.A Domain Model is composed of soft clustersof terms.
Each cluster represents a semantic do-main, i.e.
a set of terms that often co-occur intexts having similar topics.
Such clusters iden-tify groups of words belonging to the same seman-tic field, and thus highly paradigmatically related.MDMs are Domain Models containing terms inmore than one language.A MDM is represented by a matrix D, contain-ing the degree of association among terms in allthe languages and domains, as illustrated in Table1.
For example the term virus is associated to bothMEDICINE COMPUTER SCIENCEHIV e/i 1 0AIDSe/i 1 0viruse/i 0.5 0.5hospitale 1 0laptope 0 1Microsofte/i 0 1clinicai 1 0Table 1: Example of Domain Matrix.
we denotesEnglish terms, wi Italian terms and we/i the com-mon terms to both languages.the domain COMPUTER SCIENCE and the domainMEDICINE while the domain MEDICINE is associ-ated to both the terms AIDS and HIV.
Inter-lingual555????????????????????????????????
?English texts Italian textste1 te2 ?
?
?
ten?1 ten ti1 ti2 ?
?
?
tim?1 timwe1 0 1 ?
?
?
0 1 0 0 ?
?
?EnglishLexiconwe2 1 1 ?
?
?
1 0 0. .
.... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
... 0 ...wep?1 0 1 ?
?
?
0 0. .
.
0wep 0 1 ?
?
?
0 0 ?
?
?
0 0common wi we/i1 0 1 ?
?
?
0 0 0 0 ?
?
?
1 0... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.wi1 0 0 ?
?
?
0 1 ?
?
?
1 1ItalianLexiconwi2 0. .
.
1 1 ?
?
?
0 1... ... 0 ... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.wiq?1.
.
.
0 0 1 ?
?
?
0 1wiq ?
?
?
0 0 0 1 ?
?
?
1 0????????????????????????????????
?Figure 1: Multilingual term-by-document matrixdomain relations are captured by placing differ-ent terms of different languages in the same se-mantic field (as for example HIV e/i, AIDSe/i,hospitale, and clinicai).
Most of the named enti-ties, such as Microsoft and HIV are expressed us-ing the same string in both languages.Formally, let V i = {wi1, wi2, .
.
.
, wiki} be thevocabulary of the corpus T i composed of doc-ument expressed in the language Li, let V ?
=?i V i be the set of all the terms in all the lan-guages, and let k?
= |V ?| be the cardinality ofthis set.
Let D = {D1, D2, ..., Dd} be a set of do-mains.
A DM is fully defined by a k?
?
d domainmatrix D representing in each cell di,z the domainrelevance of the ith term of V ?
with respect to thedomain Dz .
The domain matrix D is used to de-fine a function D : Rk?
?
Rd, that maps the doc-ument vectors ~tj expressed into the multilingualclassical VSM (see Section 2.1), into the vectors~t?j in the multilingual domain VSM.
The functionD is defined by2D(~tj) = ~tj(IIDFD) = ~t?j (1)where IIDF is a diagonal matrix such that iIDFi,l =IDF (wli), ~tj is represented as a row vector, andIDF (wli) is the Inverse Document Frequency of2In (Wong et al, 1985) the formula 1 is used to define aGeneralized Vector Space Model, of which the Domain VSMis a particular instance.wli evaluated in the corpus T l.In this work we exploit Latent Semantic Anal-ysis (LSA) (Deerwester et al, 1990) to automat-ically acquire a MDM from comparable corpora.LSA is an unsupervised technique for estimatingthe similarity among texts and terms in a largecorpus.
In the monolingual settings LSA is per-formed by means of a Singular Value Decom-position (SVD) of the term-by-document matrixT describing the corpus.
SVD decomposes theterm-by-document matrix T into three matrixesT ' V?k?UT where ?k?
is the diagonal k ?
kmatrix containing the highest k?
k eigenval-ues of T, and all the remaining elements are setto 0.
The parameter k?
is the dimensionality ofthe Domain VSM and can be fixed in advance (i.e.k?
= d).In the literature (Littman et al, 1998) LSAhas been used in multilingual settings to definea multilingual space in which texts in differentlanguages can be represented and compared.
Inthat work LSA strongly relied on the availabilityof aligned parallel corpora: documents in all thelanguages are represented in a term-by-documentmatrix (see Figure 1) and then the columns corre-sponding to sets of translated documents are col-lapsed (i.e.
they are substituted by their sum) be-fore starting the LSA process.
The effect of thisstep is to merge the subspaces (i.e.
the right andthe left sectors of the matrix in Figure 1) in which556the documents have been originally represented.In this paper we propose a variation of this strat-egy, performing a multilingual LSA in the case inwhich an aligned parallel corpus is not available.It exploits the presence of common words amongdifferent languages in the term-by-document ma-trix.
The SVD process has the effect of creating aLSA space in which documents in both languagesare represented.
Of course, the higher the numberof common words, the more information will beprovided to the SVD algorithm to find commonLSA dimension for the two languages.
The re-sulting LSA dimensions can be perceived as mul-tilingual clusters of terms and document.
LSA canthen be used to define a Multilingual Domain Ma-trix DLSA.
For further details see (Gliozzo andStrapparava, 2005).As Kernel Methods are the state-of-the-art su-pervised framework for learning and they havebeen successfully adopted to approach the TC task(Joachims, 2002), we chose this framework to per-form all our experiments, in particular SupportVector Machines3 .
Taking into account the exter-nal knowledge provided by a MDM it is possibleestimate the topic similarity among two texts ex-pressed in different languages, with the followingkernel:KD(ti, tj) =?D(ti),D(tj)???D(tj),D(tj)??D(ti),D(ti)?
(2)where D is defined as in equation 1.Note that when we want to estimate the similar-ity in the standard Multilingual VSM, as describedin Section 2.1, we can use a simple bag of wordskernel.
The BoW kernel is a particular case of theDomain Kernel, in which D = I, and I is the iden-tity matrix.
In the evaluation typically we considerthe BoW Kernel as a baseline.4 Exploiting Bilingual DictionariesWhen bilingual resources are available it is possi-ble to augment the the ?common?
portion of thematrix in Figure 1.
In our experiments we ex-ploit two alternative multilingual resources: Mul-tiWordNet and the Collins English-Italian bilin-gual dictionary.3We adopted the efficient implementation freely availableat http://svmlight.joachims.org/.MultiWordNet4.
It is a multilingual computa-tional lexicon, conceived to be strictly alignedwith the Princeton WordNet.
The available lan-guages are Italian, Spanish, Hebrew and Roma-nian.
In our experiment we used the English andthe Italian components.
The last version of theItalian WordNet contains around 58,000 Italianword senses and 41,500 lemmas organized into32,700 synsets aligned whenever possible withWordNet English synsets.
The Italian synsetsare created in correspondence with the PrincetonWordNet synsets, whenever possible, and seman-tic relations are imported from the correspondingEnglish synsets.
This implies that the synset indexstructure is the same for the two languages.Thus for the all the monosemic words, we aug-ment each text in the dataset with the correspond-ing synset-id, which act as an expansion of the?common?
terms of the matrix in Figure 1.
Adopt-ing the methodology described in Section 3.1, weexploit these common sense-indexing to inducea second-order similarity for the other terms inthe lexicons.
We evaluate the performance of thecross-lingual text categorization, using both theBoW Kernel and the Multilingual Domain Kernel,observing that also in this case the leverage of theexternal knowledge brought by the MDM is effec-tive.It is also possible to augment each text with allthe synset-ids of all the words (i.e.
monosemic andpolysemic) present in the dataset, hoping that theSVM machine learning device cut off the noisedue to the inevitable spurious senses introduced inthe training examples.
Obviously in this case, dif-ferently from the ?monosemic?
enrichment seenabove, it does not make sense to apply any dimen-sionality reduction supplied by the MultilingualDomain Model (i.e.
the resulting second-order re-lations among terms and documents produced ona such ?extended?
corpus should not be meaning-ful)5.Collins.
The Collins machine-readable bilingualdictionary is a medium size dictionary includ-ing 37,727 headwords in the English Section and32,602 headwords in the Italian Section.This is a traditional dictionary, without sense in-dexing like the WordNet repository.
In this case4Available at http://multiwordnet.itc.it.5The use of a WSD system would help in this issue.
How-ever the rationale of this paper is to see how far it is possibleto go with very few resources.
And we suppose that a multi-lingual all-words WSD system is not easily available.557English ItalianCategories Training Test Total Training Test TotalQuality of Life 5759 1989 7748 5781 1901 7682Made in Italy 5711 1864 7575 6111 2068 8179Tourism 5731 1857 7588 6090 2015 8105Culture and School 3665 1245 4910 6284 2104 8388Total 20866 6955 27821 24266 8088 32354Table 2: Number of documents in the data set partitionswe follow the way, for each text of one language,to augment all the present words with the transla-tion words found in the dictionary.
For the samereason, we chose not to exploit the MDM, whileexperimenting along this way.5 EvaluationThe CLTC task has been rarely attempted in theliterature, and standard evaluation benchmark arenot available.
For this reason, we developedan evaluation task by adopting a news corpuskindly put at our disposal by AdnKronos, an im-portant Italian news provider.
The corpus con-sists of 32,354 Italian and 27,821 English newspartitioned by AdnKronos into four fixed cat-egories: QUALITY OF LIFE, MADE IN ITALY,TOURISM, CULTURE AND SCHOOL.
The En-glish and the Italian corpora are comparable, inthe sense stated in Section 2, i.e.
they cover thesame topics and the same period of time.
Somenews stories are translated in the other language(but no alignment indication is given), some oth-ers are present only in the English set, and someothers only in the Italian.
The average length ofthe news stories is about 300 words.
We randomlysplit both the English and Italian part into 75%training and 25% test (see Table 2).
We processedthe corpus with PoS taggers, keeping only nouns,verbs, adjectives and adverbs.Table 3 reports the vocabulary dimensions ofthe English and Italian training partitions, the vo-cabulary of the merged training, and how manycommon lemmata are present (about 14% of thetotal).
Among the common lemmata, 97% arenouns and most of them are proper nouns.
Thusthe initial term-by-document matrix is a 43,384 ?45,132 matrix, while the DLSA was acquired us-ing 400 dimensions.As far as the CLTC task is concerned, we triedthe many possible options.
In all the cases wetrained on the English part and we classified theItalian part, and we trained on the Italian and clas-# lemmataEnglish training 22,704Italian training 26,404English + Italian 43,384common lemmata 5,724Table 3: Number of lemmata in the training partsof the corpussified on the English part.
When used, the MDMwas acquired running the SVD only on the joint(English and Italian) training parts.Using only comparable corpora.
Figure 2 re-ports the performance without any use of bilingualdictionaries.
Each graph show the learning curvesrespectively using a BoW kernel (that is consid-ered here as a baseline) and the multilingual do-main kernel.
We can observe that the latter largelyoutperform a standard BoW approach.
Analyzingthe learning curves, it is worth noting that whenthe quantity of training increases, the performancebecomes better and better for the Multilingual Do-main Kernel, suggesting that with more availabletraining it could be possible to improve the results.Using bilingual dictionaries.
Figure 3 reportsthe learning curves exploiting the addition of thesynset-ids of the monosemic words in the corpus.As expected the use of a multilingual repositoryimproves the classification results.
Note that theMDM outperforms the BoW kernel.Figure 4 shows the results adding in the Englishand Italian parts of the corpus all the synset-ids(i.e.
monosemic and polisemic) and all the transla-tions found in the Collins dictionary respectively.These are the best results we get in our experi-ments.
In these figures we report also the perfor-mance of the corresponding monolingual TC (weused the SVM with the BoW kernel), which canbe considered as an upper bound.
We can observethat the CLTC results are quite close to the perfor-mance obtained in the monolingual classificationtasks.5580.20.30.40.50.60.70 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training data (train on English, test on Italian)Multilingual Domain KernelBow Kernel0.20.30.40.50.60.70 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training data (train on Italian, test on English)Multilingual Domain KernelBow KernelFigure 2: Cross-language learning curves: no use of bilingual dictionaries0.20.30.40.50.60.70 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training data (train on English, test on Italian)Multilingual Domain KernelBow Kernel0.20.30.40.50.60.70 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training data (train on Italian, test on English)Multilingual Domain KernelBow KernelFigure 3: Cross-language learning curves: monosemic synsets from MultiWordNet0.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training data (train on English, test on Italian)Monolingual (Italian) TCCollinsMultiWordNet0.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of training data (train on Italian, test on English)Monolingual (English) TCCollinsMultiWordNetFigure 4: Cross-language learning curves: all synsets from MultiWordNet // All translations from Collins5596 Conclusion and Future WorkIn this paper we have shown that the problem ofcross-language text categorization on comparablecorpora is a feasible task.
In particular, it is pos-sible to deal with it even when no bilingual re-sources are available.
On the other hand when it ispossible to exploit bilingual repositories, such as asynset-aligned WordNet or a bilingual dictionary,the obtained performance is close to that achievedfor the monolingual task.
In any case we thinkthat our methodology is low-cost and simple, andit can represent a technologically viable solutionfor multilingual problems.
For the future we try toexplore also the use of a word sense disambigua-tion all-words system.
We are confident that evenwith the actual state-of-the-art WSD performance,we can improve the actual results.AcknowledgmentsThis work has been partially supported by the ON-TOTEXT (From Text to Knowledge for the Se-mantic Web) project, funded by the AutonomousProvince of Trento under the FUP-2004 program.ReferencesN.
Bel, C. Koster, and M. Villegas.
2003.
Cross-lingual text categorization.
In Proceedings of Eu-ropean Conference on Digital Libraries (ECDL),Trondheim, August.C.
Callison-Burch, D. Talbot, and M. Osborne.2004.
Statistical machine translation with word-andsentence-aligned parallel corpora.
In Proceedings ofACL-04, Barcelona, Spain, July.S.
Deerwester, S. T. Dumais, G. W. Furnas, T.K.
Lan-dauer, and R. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Societyfor Information Science, 41(6):391?407.E.
Gaussier, J. M. Renders, I. Matveeva, C. Goutte, andH.
Dejean.
2004.
A geometric view on bilinguallexicon extraction from comparable corpora.
In Pro-ceedings of ACL-04, Barcelona, Spain, July.A.
Gliozzo and C. Strapparava.
2005.
Cross languagetext categorization by acquiring multilingual domainmodels from comparable corpora.
In Proc.
of theACL Workshop on Building and Using Parallel Texts(in conjunction of ACL-05), University of Michigan,Ann Arbor, June.A.
Gliozzo, C. Strapparava, and I. Dagan.
2004.
Unsu-pervised and supervised exploitation of semantic do-mains in lexical disambiguation.
Computer Speechand Language, 18:275?299.T.
Joachims.
2002.
Learning to Classify Text usingSupport Vector Machines.
Kluwer Academic Pub-lishers.P.
Koehn and K. Knight.
2002.
Learning a translationlexicon from monolingual corpora.
In Proceedingsof ACL Workshop on Unsupervised Lexical Acquisi-tion, Philadelphia, July.M.
Littman, S. Dumais, and T. Landauer.
1998.
Auto-matic cross-language information retrieval using la-tent semantic indexing.
In G. Grefenstette, editor,Cross Language Information Retrieval, pages 51?62.
Kluwer Academic Publishers.D.
Melamed.
2001.
Empirical Methods for ExploitingParallel Texts.
The MIT Press.L.
Rigutini, M. Maggini, and B. Liu.
2005.
An EMbased training algorithm for cross-language text cat-egorizaton.
In Proceedings of Web Intelligence Con-ference (WI-2005), Compie`gne, France, September.C.
Strapparava, A. Gliozzo, and C. Giuliano.2004.
Pattern abstraction and term similarity forword sense disambiguation.
In Proceedings ofSENSEVAL-3, Barcelona, Spain, July.S.K.M.
Wong, W. Ziarko, and P.C.N.
Wong.
1985.Generalized vector space model in information re-trieval.
In Proceedings of the 8th ACM SIGIR Con-ference.560
