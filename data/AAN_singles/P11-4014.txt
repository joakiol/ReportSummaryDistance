Proceedings of the ACL-HLT 2011 System Demonstrations, pages 80?85,Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational LinguisticsA Speech-based Just-in-Time Retrieval System using Semantic SearchAndrei Popescu-Belis, Majid Yazdani, Alexandre Nanchen, and Philip N. GarnerIdiap Research InstituteRue Marconi 19, CP 5921920 Martigny, Switzerland{apbelis,myazdani,ananchen,pgarner}@idiap.chAbstractThe Automatic Content Linking Device is ajust-in-time document retrieval system whichmonitors an ongoing conversation or a mono-logue and enriches it with potentially relateddocuments, including multimedia ones, fromlocal repositories or from the Internet.
Thedocuments are found using keyword-basedsearch or using a semantic similarity measurebetween documents and the words obtainedfrom automatic speech recognition.
Resultsare displayed in real time to meeting partici-pants, or to users watching a recorded lectureor conversation.1 IntroductionEnriching a monologue or a conversation with re-lated content, such as textual or audio-visual docu-ments on the same topic, is a task with multiple ap-plications in the field of computer-mediated human-human communication.
In this paper, we describethe Automatic Content Linking Device (ACLD), asystem that analyzes spoken input from one or morespeakers using automatic speech recognition (ASR),in order to retrieve related content, in real-time, froma variety of repositories.
These include local doc-ument databases or archives of multimedia record-ings, as well as websites.
Local repositories arequeried using a keyword-based search engine, or us-ing a semantic similarity measure, while websitesare queried using commercial search engines.We will first describe the scenarios of use of theACLD in Section 2, and review previous systems forjust-in-time retrieval in Section 3.
The ACLD com-ponents will be outlined in Sections 4.1 to 4.5.
Fourtypes of evaluation results obtained with our systemwill finally be summarized in Sections 5.1 to 5.4.2 Content Linking: Scenarios of UseJust-in-time information retrieval, i.e.
finding usefuldocuments without the need for a user to initiate a di-rect search for them, is one of the ways in which thelarge quantity of knowledge that is available in net-worked environments can be efficiently put to use.To perform this task, a system must consider ex-plicit and implicit input from users, mainly speechor typed input, and attempt to model their context,in order to provide recommendations, which usersare free to consult if they feel the need for additionalinformation.One of the main scenarios of use for the ACLDinvolves people taking part in meetings, who oftenmention documents containing facts under discus-sion, but do not have the time to search for themwithout interrupting the discussion flow.
The ACLDperforms this search for them.
Moreover, as theACLD was developed on meetings from the AMICorpus, it can also perform the same operations ona replayed meeting, as a complement to a meet-ing browser, for development or demonstration pur-poses.In a second scenario, content linking is performedover live or recorded lectures, for instance in acomputer-assisted learning environment for individ-ual students.
The ACLD enriches the lectures withrelated material drawn from various repositories,through a search process that can be guided in real80time by its user.
The advantage of real-time con-tent linking over a more static enrichment, such asthe Feynman lectures at Microsoft Research,1 is thatusers can tune search parameters at will while view-ing the lecture.3 Just-in-Time Retrieval SystemsThe first precursors to the ACLD were the Fixitquery-free search system (Hart and Graham, 1997),the Remembrance Agent for just-in-time retrieval(Rhodes and Maes, 2000), and the Implicit Queries(IQ) system (Dumais et al, 2004).
Fixit monitoredthe state of a user?s interaction with a diagnosticsystem, and excerpts from maintenance manuals de-pending on the interaction state.
The RemembranceAgent was integrated to the Emacs text editor, andran searches over emails or notes at regular time in-tervals (every few seconds) using the latest 20?500words typed by the user.
The IQ system generatedcontext-sensitive searches based on a user?s ongoingactivities on their computer, such as writing email.A version of the Remembrance Agent called Jim-miny was conceived as a wearable assistant for tak-ing notes, but ASR was only simulated for evalua-tion (Rhodes, 1997).The Watson system (Budzik and Hammond,2000) monitored the user?s operations in a text ed-itor, but proposed a more complex mechanism thanthe Remembrance Agent for selecting terms forqueries, which were directed to a web search engine.Another assistant for an authoring environment wasdeveloped in the A-Propos project (Puerta Melguizoet al, 2008).
A query-free system was designed forenriching television news with articles from the Web(Henziker et al, 2005).The FAME interactive space (Metze and al.,2006), which provides multi-modal access to record-ings of lectures via a table top interface, bears manysimilarities to the ACLD.
However, it requires theuse of specific voice commands by one user only,and does not spontaneously follow a conversation.More recently, several speech-based search en-gines have become available, including as smartphone applications.
Conversely, many systems al-low searching of spoken document archives.2 Inspi-1See http://research.microsoft.com/apps/tools/tuva/.2See workshops at http://www.searchingspeech.org.ration from these approaches, which are not query-free, can nevertheless be useful to just-in-time re-trieval.
Other related systems are the Speech Spot-ter (Goto et al, 2004) and a personal assistant usingdual-purpose speech (Lyons et al, 2004), which en-able users to search for information using commandsthat are identified in the speech flow.The ACLD improves over numerous past ones bygiving access to indexed multimedia recordings aswell as websites, with fully operational ASR and se-mantic search, as we now explain.4 Description of the ACLDThe architecture of the ACLD comprises the follow-ing functions: document preparation, text extractionand indexing; input sensing and query preparation;search and integration of results; user interface todisplay the results.4.1 Document Preparation and IndexingThe preparation of the local database of documentsfor content linking involves mainly the extraction oftext, and then the indexing of the documents, whichis done using Apache Lucene software.
Text can beextracted from a large variety of formats (includ-ing MS Office, PDF, and HTML) and hierarchiesof directories are recursively scanned.
The docu-ment repository is generally prepared before usingthe ACLD, but users can also add files at will.
Be-cause past discussions are relevant to subsequentones, they are passed through offline ASR and thenchunked into smaller units (e.g.
of fixed length, orbased on a homogeneous topic).
The resulting textsare indexed along with the other documents.The ACLD uses external search engines to searchin external repositories, for instance the Google Websearch API or the Google Desktop application tosearch the user?s local drives.4.2 Sensing the User?s Information NeedsWe believe that the most useful cues about the in-formation needs of participants in a conversation,or of people viewing a lecture, are the words thatare spoken during the conversation or the lecture.For the ACLD, we use the AMI real-time ASR sys-tem (Garner et al, 2009).
One of its main featuresis the use of a pre-compiled grammar, which al-lows it to retain accuracy even when running in real-81time on a low resource machine.
Of course, whencontent linking is done over past meetings, or fortext extraction from past recordings, the ASR sys-tem runs slower than real-time to maximize accuracyof recognition.
However, the accuracy of real-timeASR is only about 1% lower than the unconstrainedmode which takes several times real-time.For the RT07 meeting data, when using signalsfrom individual headset microphones, the AMI ASRsystem reaches about 38% word error rate.
Witha microphone array, this increases to about 41%.These values indicate that enough correct words aresensed by the real-time ASR to make it applicableto the ACLD, and that a robust search mechanismcould help avoiding retrieval errors due to spuriouswords.The words obtained from the ASR are filtered forstopwords, so that only content words are used forsearch; our list has about 80 words.
Furthermore,we believe that existing knowledge about the impor-tant terminology of a domain or project can be usedto increase the impact of specific words on search.
Alist of pre-specified keywords can be defined basedon such knowledge and can be modified while run-ning the ACLD.
For instance, for remote control de-sign as in the AMI Corpus scenario, this list includesabout 30 words such as ?chip?, ?button?, or ?mate-rial?.
If any of them is detected in the ASR output,then their importance is increased for searching, butotherwise all the other words from the ASR (minusthe stopwords) are used for constructing the query.4.3 Querying the Document DatabaseThe Query Aggregator (QA) uses the ASR wordsto retrieve the most relevant documents from one ormore databases.
The current version of the ACLDmakes use of semantic search (see next subsection),while previous versions used word-based searchfrom Apache Lucene for local documents, or fromthe Google Web or Google Desktop APIs.
ASRwords from the latest time frame are put together(minus the stopwords) to form queries, and recog-nized keywords are boosted in the Lucene query.Queries are formulated at regular time intervals, typ-ically every 15-30 seconds, or on demand.
This du-ration is a compromise between the need to gatherenough words for search, and the need to refresh thesearch results reasonably often.The results are integrated with those from theprevious time frame, using a persistence model tosmooth variations over time.
The model keeps trackof the salience of each result, initialized from theirranking among the search results, then decreasingin time unless the document is again retrieved.
Therate of decrease (or its inverse, persistence) can betuned by the user, but in any case, all past results aresaved by the user interface and can be consulted atany time.4.4 Semantic Search over WikipediaThe goal of our method for semantic search is toimprove the relevance of the retrieved documents,and to make the mechanism more robust to noisefrom the ASR.
We have applied to document re-trieval the graph-based model of semantic rela-tedness that we recently developed (Yazdani andPopescu-Belis, 2010), which is also related to otherproposals (Strube and Ponzetto, 2006; Gabrilovichand Markovitch, 2007; Yeh et al, 2009).The model is grounded in a measure of seman-tic relatedness between text fragments, which iscomputed using random walk over the network ofWikipedia articles ?
about 1.2 million articles fromthe WEX data set (Metaweb Technologies, 2010).The articles are linked through hyperlinks, and alsothrough lexical similarity links that are constructedupon initialization.
The random walk model allowsthe computation of a visiting probability (VP) fromone article to another, and then a VP between sets ofarticles, which has been shown to function as a mea-sure of semantic relatedness, and has been appliedto various NLP problems.
To compute relatednessbetween two text fragments, these are first projectedrepresented into the network by the ten closest arti-cles in terms of lexical similarity.For the ACLD, the use of semantic relatedness fordocument retrieval amounts to searching, in a verylarge collection, the documents that are the mostclosely related to the words from the ASR in a giventimeframe.
Here, the document collection is (again)the set of Wikipedia articles from WEX, and the goalis to return the eight most related articles.
Such asearch is hard to perform in real time; hence, the so-lution that was found makes use of several approx-imations to compute average VP between the ASRfragment and all articles in the Wikipedia network.82Figure 1: Unobtrusive UI displaying document results.Hovering the mouse over a result (here, the most relevantone) displays a pop-up window with more informationabout it.4.5 The User Interface (UI)The main goal of the UI is to make available allinformation produced by the system, in a config-urable way, allowing users to see a larger or smalleramount of information according to their needs.
Amodular architecture with a flexible layout has beenimplemented, maximizing the accessibility but alsothe understandability of the results, and displayingalso intermediary data such as ASR words and foundkeywords.
The UI displays up to five widgets, whichcan be arranged at will:1.
ASR results with highlighted keywords.2.
Tag-cloud of keywords, coding for recency andfrequency of keywords.3.
Names of documents and past meeting snippetsfound by the QA.4.
Names of web pages found via the Google API.5.
Names of local files found via the GoogleDesktop API.Two main arrangements are intended, thoughmany others are possible: an informative full-screenUI, shown in Figure 2 with widgets 1?4; and an un-obtrusive widget UI, with superposed tabs, shown inFigure 1 with widget 3.The document names displayed in widgets 3?5function as hyperlinks to the documents, launchingappropriate external viewers when the user clicks onthem.
Moreover, when hovering over a documentname, a pop-up window displays metadata and doc-ument excerpts that match words from the query, asan explanation of why the document was retrieved.5 Evaluation ExperimentsFour types of evidence for the relevance and utilityof the ACLD are summarized in this section.5.1 Feedback from Potential UsersThe ACLD was demonstrated to about 50 potentialusers (industrial partners, focus groups, etc.)
in aseries of sessions of about 30 minutes, starting witha presentation of the ACLD and continuing with adiscussion and elicitation of feedback.
The overallconcept was generally found useful, with positiveverbal evaluations.
Feedback for smaller and largerimprovements was collected: e.g.
the importance ofmatching context, linking on demand, and the UI un-obtrusive mode.5.2 Pilot Task-based ExperimentsA pilot experiment was conducted by a team at theUniversity of Edinburgh with an earlier version ofthe unobtrusive UI.
Four subjects had to complete atask that was started in previous meetings (ES2008a-b-c from the AMI Corpus).
The goal was to comparetwo conditions, with vs. without the ACLD, in termsof satisfied constraints, overall efficiency, and satis-faction.
Two pilot runs have shown that the ACLDwas being consulted about five times per meeting.Therefore, many more runs are required to reach sta-tistical significance of observations, and remain tobe executed depending on future resources.5.3 Usability Evaluation of the UIThe UI was submitted to a usability evaluation ex-periment with nine non-technical subjects.
Thesubjects used the ACLD over a replayed meetingrecording, and were asked to perform several taskswith it, such as adding a keyword to monitor, search-ing for a word, or changing the layout.
The subjectsthen rated usability-related statements, leading to anassessment on the System Usability Scale (Brooke,1996).The overall usability score was 68% (SD: 10),which is considered as ?acceptable usability?
for theSUS.
The average task-completion time was 45?75 seconds.
In free-form feedback, subjects foundthe system helpful to review meetings but also lec-tures, appreciated the availability of documents, butalso noted that search results (with keyword-based83Figure 2: Full screen UI with four widgets: ASR, keywords, document and website results.search) were often irrelevant.
They also suggestedsimplifying the UI (menus, layout) and embeddinga media player for use in the meeting or lecture re-play scenario.5.4 Comparing the Relevance ofKeyword-based vs. Semantic SearchWe compared the output of semantic search withthat of keyword-based search.
The ASR transcriptof one AMI meeting (ES2008d) was passed to bothsearch methods, and ?evaluation snippets?
contain-ing the manual transcript for one-minute excerpts,accompanied by the 8-best Wikipedia articles foundby each method were produced.
Overall, 36 snip-pets were generated.
The manual transcript shown tosubjects was enriched with punctuation and speak-ers?
names, and the names of the Wikipedia pageswere placed on each side of the transcript frame.Subjects were then asked to read each snippet,and decide which of the two document sets was themost relevant to the discussion taking place, i.e.
themost useful as a suggestion to the participants.
Theycould also answer ?none?, and could consult the re-sult if necessary.Results were obtained from 8 subjects, each see-ing 9 snippets out of 36.
Every snippet was thusseen by two subjects.
The subjects agreed on 23(64%) snippets and disagreed on 13 (36%).
In fact,the number of true disagreements not including theanswer ?none?
was only 7 out of 36.Over the 23 snippets on which subjects agreed,the result of semantic search was judged more rel-evant than that of keyword search for 19 snippets(53% of the total), and the reverse for 4 snippetsonly (11%).
Alternatively, if one counts the votescast by subjects in favor of each system, regardlessof agreement, then semantic search received 72%of the votes and keyword-based only 28%.
Thesenumbers show that semantic search quite clearly im-proves relevance in comparison to keyword-basedone, but there is still room for improvement.6 ConclusionThe ACLD is, to the best of our knowledge, thefirst just-in-time retrieval system to use spontaneousspeech and to support access to multimedia docu-ments and web pages, using a robust semantic searchmethod.
Future work will aim at improving the rel-evance of semantic search, at modeling context to84improve timing of results, and at inferring relevancefeedback from users.
The ACLD should also be ap-plied to specific use cases, and an experiment withgroup work in a learning environment is under way.AcknowledgmentsThe authors gratefully acknowledge the supportof the EU AMI and AMIDA Integrated Projects(http://www.amiproject.org) and of the Swiss IM2NCCR on Interactive Multimodal Information Man-agement (http://www.im2.ch).ReferencesJohn Brooke.
1996.
SUS: A ?quick and dirty?
us-ability scale.
In Patrick W. Jordan, Bruce Thomas,Bernard A. Weerdmeester, and Ian L. McClelland, ed-itors, Usability evaluation in industry, pages 189?194.Taylor and Francis, London, UK.Jay Budzik and Kristian J. Hammond.
2000.
User inter-actions with everyday applications as context for just-in-time information access.
In IUI 2000 (5th Interna-tional Conference on Intelligent User Interfaces), NewOrleans, LA.Susan Dumais, Edward Cutrell, Raman Sarin, and EricHorvitz.
2004.
Implicit Queries (IQ) for contextual-ized search.
In SIGIR 2004 (27th ACM SIGIR Confer-ence) Demonstrations, page 534, Sheffield, UK.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using Wikipedia-basedexplicit semantic analysis.
In IJCAI 2007 (20th Inter-national Joint Conference on Artificial Intelligence),pages 6?12, Hyderabad, India.Philip N. Garner, John Dines, Thomas Hain, AsmaaEl Hannani, Martin Karafiat, Danil Korchagin, MikeLincoln, Vincent Wan, and Le Zhang.
2009.
Real-time ASR from meetings.
In Interspeech 2009 (10thAnnual Conference of the Intl.
Speech CommunicationAssociation), pages 2119?2122, Brighton, UK.Masataka Goto, Koji Kitayama, Katsunobu Itou, and Tet-sunori Kobayashi.
2004.
Speech Spotter: On-demandspeech recognition in human-human conversation onthe telephone or in face-to-face situations.
In ICSLP2004 (8th International Conference on Spoken Lan-guage Processing), pages 1533?1536, Jeju Island.Peter E. Hart and Jamey Graham.
1997.
Query-free in-formation retrieval.
IEEE Expert: Intelligent Systemsand Their Applications, 12(5):32?37.Monika Henziker, Bay-Wei Chang, Brian Milch, andSergey Brin.
2005.
Query-free news search.
WorldWide Web: Internet and Web Information Systems,8:101?126.Kent Lyons, Christopher Skeels, Thad Starner, Cor-nelis M. Snoeck, Benjamin A. Wong, and Daniel Ash-brook.
2004.
Augmenting conversations using dual-purpose speech.
In UIST 2004 (17th Annual ACMSymposium on User Interface Software and Technol-ogy), pages 237?246, Santa Fe, NM.Metaweb Technologies.
2010.
Freebase Wikipedia Ex-traction (WEX).
http://download.freebase.com/wex/.Florian Metze and al.
2006.
The ?Fame?
interactivespace.
In Machine Learning for Multimodal Interac-tion II, LNCS 3869, pages 126?137.
Springer, Berlin.Maria Carmen Puerta Melguizo, Olga Monoz Ramos,Lou Boves, Toine Bogers, and Antal van den Bosch.2008.
A personalized recommender system for writ-ing in the Internet age.
In LREC 2008 Workshop onNLP Resources, Algorithms, and Tools for AuthoringAids, pages 21?26, Marrakech, Morocco.Bradley J. Rhodes and Pattie Maes.
2000.
Just-in-timeinformation retrieval agents.
IBM Systems Journal,39(3-4):685?704.Bradley J. Rhodes.
1997.
The Wearable RemembranceAgent: A system for augmented memory.
PersonalTechnologies: Special Issue on Wearable Computing,1:218?224.Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
Computing semantic relatedness usingWikipedia.
In AAAI 2006 (21st National Conferenceon Artificial Intelligence), pages 1419?1424, Boston,MA.Majid Yazdani and Andrei Popescu-Belis.
2010.
A ran-dom walk framework to compute textual semantic sim-ilarity: A unified model for three benchmark tasks.
InICSC 2010 (4th IEEE International Conference on Se-mantic Computing), pages 424?429, Pittsburgh, PA.Eric Yeh, Daniel Ramage, Christopher D. Manning,Eneko Agirre, and Aitor Soroa.
2009.
WikiWalk: ran-dom walks on Wikipedia for semantic relatedness.
InTextGraphs-4 (4th Workshop on Graph-based Methodsfor NLP), pages 41?49, Singapore.85
