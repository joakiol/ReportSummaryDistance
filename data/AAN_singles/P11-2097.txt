Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 552?557,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsIdentification of Domain-Specific Senses in a Machine-Readable DictionaryFumiyo FukumotoInterdisciplinary Graduate School ofMedicine and Engineering,Univ.
of Yamanashifukumoto@yamanashi.ac.jpYoshimi SuzukiInterdisciplinary Graduate School ofMedicine and Engineering,Univ.
of Yamanashiysuzuki@yamanashi.ac.jpAbstractThis paper focuses on domain-specific sensesand presents a method for assigning cate-gory/domain label to each sense of words ina dictionary.
The method first identifies eachsense of a word in the dictionary to its cor-responding category.
We used a text classifi-cation technique to select appropriate sensesfor each domain.
Then, senses were scored bycomputing the rank scores.
We used MarkovRandom Walk (MRW) model.
The methodwas tested on English and Japanese resources,WordNet 3.0 and EDR Japanese dictionary.For evaluation of the method, we comparedEnglish results with the Subject Field Codes(SFC) resources.
We also compared each En-glish and Japanese results to the first senseheuristics in the WSD task.
These resultssuggest that identification of domain-specificsenses (IDSS) may actually be of benefit.1 IntroductionDomain-specific sense of a word is crucial informa-tion for many NLP tasks and their applications, suchasWord Sense Disambiguation (WSD) and Informa-tion Retrieval (IR).
For example, in the WSD task,McCarthy et al presented a method to find predom-inant noun senses automatically using a thesaurusacquired from raw textual corpora and the Word-Net similarity package (McCarthy et al, 2004; Mc-Carthy et al, 2007).
They used parsed data to findwords with a similar distribution to the target word.Unlike Buitelaar et al approach (Buitelaar andSacaleanu, 2001), they evaluated their method us-ing publically available resources, namely SemCor(Miller et al, 1998) and the SENSEVAL-2 Englishall-words task.
The major motivation for their workwas similar to ours, i.e., to try to capture changes inranking of senses for documents from different do-mains.Domain adaptation is also an approach for fo-cussing on domain-specific senses and used in theWSD task (Chand and Ng, 2007; Zhong et al, 2008;Agirre and Lacalle, 2009).
Chan et.
al.
proposeda supervised domain adaptation on a manually se-lected subset of 21 nouns from the DSO corpus hav-ing examples from the Brown corpus and Wall StreetJournal corpus.
They used active learning, count-merging, and predominant sense estimation in orderto save target annotation effort.
They showed thatfor the set of nouns which have different predomi-nant senses between the training and target domains,the annotation effort was reduced up to 29%.
Agirreet.
al.
presented a method of supervised domainadaptation (Agirre and Lacalle, 2009).
They madeuse of unlabeled data with SVM (Vapnik, 1995),a combination of kernels and SVM, and showedthat domain adaptation is an important technique forWSD systems.
The major motivation for domainadaptation is that the sense distribution depends onthe domain in which a word is used.
Most of themadapted textual corpus which is used for training onWSD.In the context of dictionary-based approach, thefirst sense heuristic applied to WordNet is often usedas a baseline for supervised WSD systems (Cotton etal., 1998), as the senses in WordNet are ordered ac-cording to the frequency data in the manually taggedresource SemCor (Miller et al, 1998).
The usual552drawback in the first sense heuristic applied to theWordNet is the small size of the SemCor corpus.Therefore, senses that do not occur in SemCor areoften ordered arbitrarily.
More seriously, the deci-sion is not based on the domain but on the frequencyof SemCor data.
Magnini et al presented a lexi-cal resource where WordNet 2.0 synsets were anno-tated with Subject Field Codes (SFC) by a procedurethat exploits the WordNet structure (Magnini andCavaglia, 2000; Bentivogli et al, 2004).
The resultsshowed that 96% of the WordNet synsets of the nounhierarchy could have been annotated using 115 dif-ferent SFC, while identification of the domain labelsfor word senses was required a considerable amountof hand-labeling.In this paper, we focus on domain-specificsenses and propose a method for assigning cate-gory/domain label to each sense of words in a dictio-nary.
Our approach is automated, and requires onlydocuments assigned to domains/categories, such asReuters corpus, and a dictionary with gloss text,such as WordNet.
Therefore, it can be applied easilyto a new domain, sense inventory or different lan-guages, given sufficient documents.2 Identification of Domain-Specific SensesOur approach, IDSS consists of two steps: selectionof senses and computation of rank scores.2.1 Selection of sensesThe first step to find domain-specific senses is to se-lect appropriate senses for each domain.
We useda corpus where each document is classified into do-mains.
The selection is done by using a text classi-fication technique.
We divided documents into twosets, i.e., training and test sets.
The training set isused to train SVM classifiers, and the test set is totest SVM classifiers.
For each domain, we collectednoun words.
Let D be a domain set, and S be a setof senses that the word w ?
W has.
Here, W is a setof noun words.
The senses are obtained as follows:1.
For each sense s ?
S, and for each d ?
D, weapplied word replacement, i.e., we replaced win the training documents assigning to the do-main d with its gloss text in a dictionary.2.
All the training and test documents are taggedby a part-of-speech tagger, and represented asterm vectors with frequency.3.
The SVM was applied to the two types of train-ing documents, i.e., with and without word re-placement, and classifiers for each category aregenerated.4.
SVM classifiers are applied to the test data.
Ifthe classification accuracy of the domain d isequal or higher than that without word replace-ment, the sense s of a word w is judged to be acandidate sense in the domain d.The procedure is applied to all w ?
W .2.2 Computation of rank scoresWe note that text classification accuracy used in se-lection of senses depends on the number of wordsconsisting gloss in a dictionary.
However, it is notso large.
As a result, many of the classification ac-curacy with word replacement were equal to thosewithout word replacement1.
Then in the second pro-cedure, we scored senses by using MRW model.Given a set of senses Sd in the domain d, Gd =(Sd, E) is a graph reflecting the relationships be-tween senses in the set.
Each sense si in Sd is agloss text assigned from a dictionary.
E is a set ofedges, which is a subset of Sd ?
Sd.
Each edge eijin E is associated with an affinity weight f(i ?
j)between senses si and sj (i 6= j).
The weight is com-puted using the standard cosine measure betweentwo senses.
The transition probability from si tosj is then defined by normalizing the correspondingaffinity weight p(i ?
j) = f(i?j)P|Sd|k=1 f(i?k), if ?f 6= 0,otherwise, 0.We used the row-normalized matrix Uij =(Uij)|Sd|?|Sd| to describe G with each entry corre-sponding to the transition probability, where Uij =p(i ?
j).
To make U a stochastic matrix, the rowswith all zero elements are replaced by a smooth-ing vector with all elements set to 1|Sd| .
The matrixform of the saliency score Score(si) can be formu-lated in a recursive form as in the MRW model: ~?= ?UT~?
+ (1??
)|Sd| ~e, where~?
= [Score(si)]|Sd|?1is a vector of saliency scores for the senses.
~e is acolumn vector with all elements equal to 1. ?
is a1In the experiment, the classification accuracy of more than50% of words has not changed.553damping factor.
We set ?
to 0.85, as in the PageR-ank (Brin and Page, 1998).
The final transition ma-trix is given by the formula (1), and each score of thesense in a specific domain is obtained by the princi-pal eigenvector of the new transition matrix M .M = ?UT + (1 ?
?
)| Sd |~e~eT (1)We applied the algorithm for each domain.
Wenote that the matrix M is a high-dimensional space.Therefore, we used a ScaLAPACK, a library ofhigh-performance linear algebra routines for dis-tributed memory MIMD parallel computing (Netlib,2007)2.
We selected the topmost K% senses accord-ing to rank score for each domain and make a sense-domain list.
For each word w in a document, findthe sense s that has the highest score within the list.If a domain with the highest score of the sense s anda domain in a document appearing w match, s is re-garded as a domain-specific sense of the word w.3 Experiments3.1 WordNet 3.0We assigned Reuters categories to each sense ofwords in WordNet 3.0 3.
The Reuters documentsare organized into 126 categories (Rose et al, 2002).We selected 20 categories consisting a variety ofgenres.
We used one month of documents, from20th Aug to 19th Sept 1996 to train the SVM model.Similarly, we classified the following one month ofdocuments into these 20 categories.
All documentswere tagged by Tree Tagger (Schmid, 1995).Table 1 shows 20 categories, the number of train-ing and test documents, and F-score (Baseline)by SVM.
For each category, we collected nounwords with more than five frequencies from one-year Reuters corpus.
We randomly divided theseinto two: 10% for training and the remaining 90%for test data.
The training data is used to estimate Kaccording to rank score, and test data is used to testthe method using the estimated value K. We man-ually evaluated a sense-domain list.
As a result, weset K to 50%.
Table 2 shows the result using the2For implementation, we used a supercomputer, SPARCEn-terprise M9000, 64CPU, 1TB memory.3http://wordnet/princeton.edu/test data, i.e., the total number of words and senses,and the number of selected senses (Select S) that theclassification accuracy of each domain was equal orhigher than the result without word replacement.
Weused these senses as an input of MRW.There are no existing sense-tagged data for these20 categories that could be used for evaluation.Therefore, we selected a limited number of wordsand evaluated these words qualitatively.
To dothis, we used SFC resources (Magnini and Cavaglia,2000), which annotate WordNet 2.0 synsets with do-main labels.
We manually corresponded Reutersand SFC categories.
Table 3 shows the results of12 Reuters categories that could be corresponded toSFC labels.
In Table 3, ?Reuters?
shows categories,and ?IDSS?
shows the number of senses assigned byour approach.
?SFC?
refers to the number of sensesappearing in the SFC resource.
?S & R?
denotes thenumber of senses appearing in both SFC and Reuterscorpus.
?Prec?
is a ratio of correct assignments by?IDSS?
divided by the total number of ?IDSS?
as-signments.
We manually evaluated senses not ap-pearing in SFC resource.
We note that the corpusused in our approach is different from SFC.
There-fore, recall denotes a ratio of the number of sensesmatched in our approach and SFC divided by thetotal number of senses appearing in both SFC andReuters.As shown in Table 3, the best performance was?weather?
and recall was 0.986, while the resultfor ?war?
was only 0.149.
Examining the resultof text classification by word replacement, the for-mer was 0.07 F-score improvement by word replace-ment, while that of the later was only 0.02.
One rea-son is related to the length of the gloss in WordNet:the average number of words consisting the gloss as-signed to ?weather?
was 8.62, while that for ?war?was 5.75.
IDSS depends on the size of gloss text inWordNet.
Efficacy can be improved if we can assigngloss sentences to WordNet based on corpus statis-tics.
This is a rich space for further exploration.In the WSD task, a first sense heuristic is oftenapplied because of its powerful and needless of ex-pensive hand-annotated data sets.
We thus comparedthe results obtained by our method to those obtainedby the first sense heuristic.
For each of the 12 cat-egories, we randomly picked up 10 words from thesenses assigned by our approach.
For each word, we554Cat Train Test F-score Cat Train Test F-scoreLegal/judicial 897 808 .499 Funding 3,245 3,588 .709Production 2,179 2,267 .463 Research 204 180 .345Advertising 113 170 .477 Management 923 812 .753Employment 1,224 1,305 .703 Disasters 757 522 .726Arts/entertainments 326 295 .536 Environment 532 420 .476Fashion 13 50 .333 Health 524 447 .513Labour issues 1,278 1,343 .741 Religion 257 251 .665Science 158 128 .528 Sports 2,311 2,682 .967Travel 47 64 .517 War 3,126 2,674 .678Elections 1,107 1,208 .689 Weather 409 247 .688Table 1: Classification performance (Baseline)Cat Words Senses S senses Cat Words Senses S sensesLegal/judicial 10,920 62,008 25,891 Funding 11,383 28,299 26,209Production 13,967 31,398 30,541 Research 7,047 19,423 18,600Advertising 7,960 23,154 20,414 Management 9,386 24,374 22,961Employment 11,056 28,413 25,915 Disasters 10,176 28,420 24,266Arts 12,587 29,303 28,410 Environment 10,737 26,226 25,413Fashion 4,039 15,001 12,319 Health 10,408 25,065 24,630Labour issues 11,043 28,410 25,845 Religion 8,547 21,845 21,468Science 8,643 23,121 21,861 Sports 12,946 31,209 29,049Travel 5,366 16,216 15,032 War 13,864 32,476 30,476Elections 11,602 29,310 26,978 Weather 6,059 18,239 16,402Table 2: The # of candidate senses (WordNet)Reuters IDSS SFC S&R Rec PrecLegal/judicial 25,715 3,187 809 .904 .893Funding 2,254 2,944 747 .632 .650Arts 3,978 3,482 576 .791 .812Environment 3,725 56 7 .857 .763Fashion 12,108 2,112 241 .892 .793Sports 935 1,394 338 .800 .820Health 10,347 79 79 .329 .302Science 21,635 62,513 2,736 .810 .783Religion 1,766 3,408 213 .359 .365Travel 14,925 506 86 .662 .673War 2,999 1,668 301 .149 .102Weather 16,244 253 72 .986 .970Average 9,719 6,800 517 .686 .661Table 3: The results against SFC resourceselected 10 sentences from the documents belongingto each corresponding category.
Thus, we tested 100sentences for each category.
Table 4 shows the re-sults.
?Sense?
refers to the number of average sensespar a word.
Table 4 shows that the average preci-sion by our method was 0.648, while the result ob-tained by the first sense heuristic was 0.581.
Table4 also shows that overall performance obtained byour method was better than that with the first senseheuristic in all categories.3.2 EDR dictionaryWe assigned categories from Japanese Mainichinewspapers to each sense of words in EDR Japanesedictionary 4.
The Mainichi documents are organizedinto 15 categories.
We selected 4 categories, eachof which has sufficient number of documents.
Alldocuments were tagged by a morphological analyzerChasen (Matsumoto et al, 2000), and nouns are ex-tracted.
We used 10,000 documents for each cate-gory from 1991 to 2000 year to train SVM model.We classified other 600 documents from the sameperiod into one of these four categories.
Table 5shows categories and F-score (Baseline) by SVM.We used the same ratio used in English data to es-timate K .
As a result, we set K to 30%.
Table 6shows the result of IDSS.
?Prec?
refers to the preci-sion of IDSS, i.e., we randomly selected 300 senses4http://www2.nict.go.jp/r/r312/EDR/index.html555Cat Sense IDSS First senseCorrect Wrong Prec Correct Wrong PrecLegal/judicial 5.3 69 31 .69 63 37 .63Funding 5.6 60 40 .60 43 57 .43Arts/entertainments 4.5 62 38 .62 48 52 .48Environment 6.5 72 28 .72 70 30 .70Fashion 4.7 74 26 .74 73 27 .73Sports 4.3 72 28 .72 70 30 .70Health 4.5 68 32 .68 62 38 .62Science 5.0 69 31 .69 65 35 .65Religion 4.1 54 46 .54 52 48 .52Travel 4.8 75 25 .75 68 32 .68War 4.9 53 47 .53 30 70 .30Weather 5.3 60 40 .60 53 47 .53Average 4.95 64.8 35.1 0.648 58.0 41.9 0.581Table 4: IDSS against the first sense heuristic (WordNet)Cat Precision Recall F-scoreInternational .650 .853 .778Economy .703 .804 .750Science .867 .952 .908Sport .808 .995 .892Table 5: Text classification performance (Baseline)Cat Words Senses S senses PrecInternational 3,607 11,292 10,647 .642Economy 3,180 9,921 9,537 .571Science 4,759 17,061 13,711 .673Sport 3,724 12,568 11,074 .681Average 3,818 12,711 11,242 .642Table 6: The # of selected senses (EDR)for each category and evaluated these senses qualita-tively.
The average precision for four categories was0.642.In the WSD task, we randomly picked up 30words from the senses assigned by our method.
Foreach word, we selected 10 sentences from the doc-uments belonging to each corresponding category.Table 7 shows the results.
As we can see fromTable 7 that IDSS was also better than the firstsense heuristics in Japanese data.
For the first senseheuristics, there was no significant difference be-tween English and Japanese, while the number ofsenses par a word in Japanese resource was 3.191,and it was smaller than that with WordNet (4.950).One reason is the same as SemCor data, i.e., theCat Sense IDSS First senseInternational 2.873 .630 .587Economy 2.793 .677 .637Science 4.223 .723 .610Sports 2.873 .620 .477Average 3.191 .662 .593Table 7: IDSS against the first sense heuristic (EDR)small size of the EDR corpus.
Therefore, there aremany senses that do not occur in the corpus.
In fact,there are 62,460 nouns which appeared in both EDRand Mainichi newspapers (from 1991 to 2000 year),164,761 senses in all.
Of these, there are 114,267senses not appearing in the EDR corpus.
This alsodemonstrates that automatic IDSS is more effectivethan the frequency-based first sense heuristics.4 ConclusionWe presented a method for assigning categories toeach sense of words in a machine-readable dictio-nary.
For evaluation of the method using Word-Net 3.0, the average precision was 0.661, and recallagainst the SFC was 0.686.
Moreover, the result ofWSD obtained by our method outperformed againstthe first sense heuristic in both English and Japanese.Future work will include: (i) applying the methodto other part-of-speech words, (ii) comparing themethod with existing other automated method, and(iii) extending the method to find domain-specificsenses with unknown words.556ReferencesE.
Agirre and O. L. Lacalle.
2009.
Supervised domainadaption for wsd.
In Proc.
of the 12th Conference ofthe European Chapter of the ACL, pages 42?50.L.
Bentivogli, P. Forner, B. Magnini, and E. Pianta.
2004.Revising the WORDNET DOMAINS Hierarchy: Se-mantics, Coverage and Balancing.
In In Proc.
of COL-ING 2004 Workshop on Multilingual Linguistic Re-sources, pages 101?108.S.
Brin and L. Page.
1998.
The Anatomy of a Large-scale Hypertextual Web Search Engine.
In ComputerNetworks and ISDN Systems, volume 30, pages 1?7.P.
Buitelaar and B. Sacaleanu.
2001.
Ranking and Se-lecting Synsets by Domain Relevance.
In Proc.
ofWordNet and Other Lexical Resources: Applications,Extensions and Customization, pages 119?124.Y.
S. Chand and H. T. Ng.
2007.
Domain adaptationwith active learning for word sense disambiguation.
InProc.
of the 45th Annual Meeting of the Association ofComputational Linguistics, pages 49?56.S.
Cotton, P. Edmonds, A. Kilgarriff, andM.
Palmer.
1998.
SENSEVAL-2,http://www.sle.sharp.co.uk/senseval2/.B.
Magnini and G. Cavaglia.
2000.
Integrating SubjectField Codes into WordNet.
In In Proc.
of LREC-2000.Y.
Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano,Y.
Matsuda, K. Takaoka, and M. Asahara.
2000.Japanese Morphological Analysis System ChaSenVersion 2.2.1.
In NAIST Technical Report NAIST.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004.Finding Predominant Senses in Untagged Text.
InProc.
of the 42nd Annual Meeting of the Associationfor Computational Linguistics, pages 280?287.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.2007.
Unsupervised Acquisition of PredominantWord Senses.
Computational Linguistics, 33(4):553?590.G.
A. Miller, C. Leacock, R. Tengi, and R. T. Bunker.1998.
A Semantic Concordance.
In Proc.
of the ARPAWorkshop on Human Language Technology, pages303?308.Netlib.
2007. http://www.netlib.org/scalapack/index.html.In Netlib Repository at UTK and ORNL.T.
G. Rose, M. Stevenson, and M. Whitehead.
2002.The Reuters Corpus Volume 1 - from yesterday?s newsto tomorrow?s language resources.
In Proc.
of ThirdInternational Conference on Language Resources andEvaluation.H.
Schmid.
1995.
Improvements in Part-of-Speech Tag-ging with an Application to German.
In Proc.
of theEACL SIGDAT Workshop.V.
Vapnik.
1995.
The Nature of Statistical Learning The-ory.
Springer.Z.
Zhong, H. T. Ng, and Y. S. Chan.
2008.
Word sensedisambiguation using ontonotes: An empirical study.In Proc.
of the 2008 Conference on Empirical Methodsin Natural Language Processing, pages 1002?1010.557
