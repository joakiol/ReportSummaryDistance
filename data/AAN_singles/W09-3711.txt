Proceedings of the 8th International Conference on Computational Semantics, pages 104?115,Tilburg, January 2009. c?2009 International Conference on Computational SemanticsSupporting inferences in semantic space:representing words as regionsKatrin ErkUniversity of Texas at Austinkatrin.erk@mail.utexas.eduAbstractSemantic space models represent the meaning of a word as a vectorin high-dimensional space.
They offer a framework in which the mean-ing representation of a word can be computed from its context, but thequestion remains how they support inferences.
While there has beensome work on paraphrase-based inferences in semantic space, it is notclear how semantic space models would support inferences involvinghyponymy, like horse ran ?
animal moved.
In this paper, we first dis-cuss what a point in semantic space stands for, contrasting semanticspace with Ga?rdenforsian conceptual space.
Building on this, we pro-pose an extension of the semantic space representation from a pointto a region.
We present a model for learning a region representationfor word meaning in semantic space, based on the fact that points atclose distance tend to represent similar meanings.
We show that thismodel can be used to predict, with high precision, when a hyponymy-based inference rule is applicable.
Moving beyond paraphrase-basedand hyponymy-based inference rules, we last discuss in what way se-mantic space models can support inferences.1 IntroductionSemantic space models represent the meaning of a word as a vector in a high-dimensional space, where the dimensions stand for contexts in which theword occurs [14, 10, 21, 20].
They have been used successfully in NLP [15],as well as in psychology [10, 13, 16].
Semantic space models, which areinduced automatically from corpus data, can be used to characterize themeaning of an occurrence of a word in a specific sentence [17, 3] withoutrecourse to dictionary senses.
This is interesting especially in the light ofthe recent debate about the problems of dictionary senses [9, 7].
However,104????horsedim129dim20.
.
.????v????animaldim11003dim25.
.
.????
horseanimalFigure 1: Modeling hyponymy in semantic space: as subsumption betweenfeature structures (left) or as subregion inclusion (right)it makes sense to characterize the meaning of words through semantic spacerepresentations only if these representations allow for inferences.
(1) Google acquired YouTube =?
Google bought YouTube(2) A horse ran =?
An animal movedEx.
(1) is an example of an inference involving a paraphrase: acquire can besubstituted for buy in some contexts, but not all, for example not in con-texts involving acquiring skills.
Ex.
(2) is an inference based on hyponymy :run implies move in some contexts, but not all, for example not in the con-text computer runs.
In this paper, we concentrate on these two importanttypes of inferences, but return to the broader question of how inferences aresupported by semantic space models towards the end.Semantic space models support paraphrase inferences: Lists of potentialparaphrases (for example buy and gain for acquire) and the applicabilityof a paraphrase rule in context [17] can be read off semantic space repre-sentations.
The same cannot be said for hyponymy-based inferences.
Themost obvious conceptualization of hyponymy in semantic space, illustratedin Fig.
1 (left), is to view the vectors as feature structures, and hyponymy assubsumption.
However, it seems unlikely that horse would occur in a subsetof the contexts in which animal is found (though see Cimiano et al[1]).There is another possible conceptualization of hyponymy in semantic space,illustrated in Fig.
1 (right): If the representation of a word?s meaning insemantic space were a region rather than a point, hyponymy could be mod-eled as the sub-region relation.
This is also the model that Ga?rdenfors [5]proposes within his framework of conceptual spaces, however it is notclear that the notion of a point in space is the same in conceptual space asin semantic space.
To better contrast the two frameworks, we will refer tosemantic space as co-occurrence space in the rest of this paper.This paper makes two contributions.
First, it discusses the notion ofa point in space in both conceptual and co-occurrence space, arguing thatthey are fundamentally different, with points in co-occurrence space notrepresenting potential entities but mixtures of uses.
Second, it introducesa computational model for extending the representation of word meaning105in co-occurrence space from a point to a region.
In doing so, it makes useof the property that points in co-occurrence space that are close togetherrepresent similar meanings.We do not assume that the subregion relation will hold between inducedhyponym and hypernym representations, no more than that the subsump-tion relation would hold between them.
Instead, we will argue that theregion representations make it possible to encode hyponymy informationcollected from another source, for example WordNet [4] or a hyponymy in-duction scheme [8, 25].Plan of the paper.
Sec.
2 gives a short overview of existing geometric mod-els of meaning.
In Sec.
3 we discuss the significance of a point in conceptualspace and in co-occurrence space, finding that the two frameworks differfundamentally in this respect, but that we can still represent word mean-ings as regions in co-occurrence space.
Building on this, Sec.
4 introducesa region model of word meaning in co-occurrence space that can be learnedautomatically from corpus data.
Sec.
5 reports on experiments testing themodel on the task of predicting hyponymy relations between occurrences ofwords.
Sec.
6 looks at both paraphrase-based and hyponymy-based infer-ences to see how their applicability can be tested in co-occurrence space andhow this generalizes to other types of inference rules.
Sec.
7 concludes.2 Related workIn this section we give a short overview of three types of geometric models ofmeaning: co-occurrence space models, conceptual space models, and modelsof human concept representation.Co-occurrence space models.
Co-occurrence space models (vector spacemodels) represent the meaning of a word as a vector in high-dimensionalspace [14, 10, 21, 20].
In the simplest case, the vector for a target wordw is constructed by counting how often each other word co-occurs with win a given corpus, in a context window of n words around the occurrencesof w. Each potentially co-occurring word d then becomes a dimension, andthe co-occurrence counts of w with d become the value of w?s vector ondimension d. As an example, Table 1 shows some co-occurrence counts forthe target words letter and surprise in Austen?s Pride and Prejudice.
Thereare many variations on co-occurrence space representations, for exampleusing syntactic context rather than word co-occurrence [20].
The most im-portant property of co-occurrence space models is that similarity betweentarget words can be estimated as distance in space, using measures such as106admirer all allow almost am and angry .
.
.letter 1 8 1 2 2 56 1 .
.
.surprise 0 7 0 0 4 22 0 .
.
.Table 1: Some co-occurrence counts for letter, surprise in Austen?s Prideand PrejudiceEuclidean distance or cosine similarity.
Co-occurrence space models havebeen used both in NLP [15, 25, 22] and in psychology [10, 13, 16].
Theyhave mostly been used to represent the meaning of a word by summingover all its occurrences.
We will call these vectors summation vectors.A few studies have developed models that represent the meaning of an oc-currence of a word in a specific sentence [10, 22, 17, 3].
The occurrencevector for a word in a specific sentence is typically computed by combiningits summation vector with that of a single context word in the same sen-tence, for example the direct object of a target verb.
For Ex.
(1) this wouldmean computing the meaning representation of this occurrence of acquireby combining the summation vectors of acquire and YouTube.
The simplestmechanism that has been used for the vector combination is vector addi-tion.
Models for occurrence meaning have typically been tested on a taskof judging paraphrase appropriateness: A model is given an occurrence, forexample acquire in Ex.
(1), and a potential paraphrase, for example buy.The model then estimates the appropriateness of the paraphrase in the cur-rent context as the similarity of the occurrence vector of acquire with thesummation vector of buy.Conceptual space.
Ga?rdenfors [5] proposes representing concepts as re-gions in a conceptual space, whose quality dimensions correspond to inter-pretable features.
In the simplest case, those are types of sensory perception(color, temperature).
Ga?rdenfors defines natural properties to be propertiesthat occupy convex regions in conceptual space, proposing that all proper-ties used in human cognition are natural.
Natural properties offer a solutionto the problem of induction if ?undesirable?
properties such as grue [6] donot form convex regions.Human concept representation.
In psychology, feature vector basedmodels of human concept representation (e.g.
[24, 19]) are used to modelcategorization.
Since many experiments on human concept representationhave been performed using verbal cues, these models represent aspects ofword meaning [18], possibly along with other types of knowledge.
Nosofsky?sGeneralized Context Model (GCM) [19] models the probability of catego-107rizing an exemplar ~e as a member of a concept C asP (C|~e) =?~?
?Cw~?sim(~?,~e)?concept C??~?
?C?w~?sim(~?,~e)where the concept C is a set of remembered exemplars, w~?is an exemplarweight, and the similarity sim(~?,~e) between ~?
and ~e is defined as sim(~?,~e) =exp(z ?
?dimension iwi(?i?
ei)2).
z is a general sensitivity parameter, wiis aweight for dimension i, and ?i, eiare the values of ~?
and ~e on dimension i.3 Points in co-occurrence spaceSince we need to be clear about the entities about which we perform in-ferences, it is important to understand what a point in conceptual and co-occurrence space stands for.
This is the topic of this section.In conceptual space, a point is a potential entity, quality, or event.
Inthe region occupied by the concept yellow, each point denotes a hue ofyellow.
In co-occurrence space, on the other hand, the representation ofyellow is a point, the summation vector.
corpus occurrences of yellow, yellowdoor as well as yellow pages.
The summation vector is thus not a potentialpercept, but a sum or mixture of uses.
As vectors are computed entirelyfrom observed contexts, summation vectors can be computed for words likeyellow just as well as tomorrow or idea.
Furthermore, the summation vectoris a representation of the word?s meaning, rather than a meaning.An occurrence vector is also a point in co-occurrence space.
It, too, doesnot represent a potential entity.
It is computed from two summation vec-tors: Summation vectors are primary, and occurrence vectors are derived,in all current co-occurrence space approaches.
Computing the meaning rep-resentation of acquire in the context of YouTube by combining~acquire and~Y ouTube amounts to constructing a pseudo-summation vector for a wordacquire-in-the-context-of-YouTube, making pseudo-counts for the dimensionsbased on the context counts of acquire and YouTube.
If the occurrence vectoris computed through addition, as~acquire+~Y ouTube, we are basically takingthe contexts in which acquire-in-the-context-of-YouTube has been observedto be the union of the contexts of acquire and YouTube.
So both summationand occurrence vectors are, in fact, summation vectors representing mixturesof uses.
They do not describe potential entities, like points in conceptualspace, but are representations of potential meanings of words.The regions in co-occurrence space we want to identify will thus not beregions of similar entities, but regions of similar mixtures of uses.
Encoding108external hyponymy information in a co-occurrence space, as we will do be-low, thus means stating that any mixture of uses in which the hyponym canoccur is also a mixture of uses where the hypernym could be found.
This isplausible for pairs like horse and animal, though it stretches corpus realitysomewhat for other hypernyms of horse like vertebrate.4 A model for regions in co-occurrence spaceIn this section, we develop a model for automatically inducing region repre-sentations for word meaning in co-occurrence space.
Our aim is to induce aregion representation from existing summation vectors and occurrence vec-tors.
There is existing work on inducing regions from points in a geometricmodel, in psychological models of human concept representation (Sec.
2).These models use either a single point (prototype models)1or a set of points(exemplar models), and induce regions of points that are sufficiently closeto the prototype or exemplars.
They share two central properties, both ofwhich can be observed in the GCM similarity formula (Sec.
2): (P1) Di-mensions differ in how strongly they influence classification.
(P2) Similaritydecreases exponentially with distance (Shepard?s law, [23]).
We adopt (P1)and (P2) for our model in co-occurrence space.
As co-occurrence space canmodel conceptual phenomena like lexical priming [13], it is reasonable toassume that its notion of similarity matches that of conceptual models.
Weconstruct a prototype-style model, with the summation vector as the proto-type, using the following additional assumptions: (P3) The representationof a word?s meaning in co-occurrence space is a contiguous region surround-ing the word?s summation vector.
(P4) The region includes the occurrencevectors of the word.
Property (P4) builds on the argument from Sec.
3 thatoccurrence vectors are pseudo-summation vectors.
It also matches previouswork on judging paraphrase appropriateness (Sec.
2), since those studiessuccessfully rely on the assumption that occurrence vectors will be close tosummation vectors that represent similar meanings.We define a model for region representations of word meaning that isbased on distance from the summation vector, and that uses the occurrencevectors to determine the distance from the summation vector at which pointsshould still be considered as part of the region.
For a given target word w,we construct a log-linear model that estimates the probability P (in|~x) thata point ~x is inside the meaning region of w, as follows:1Some prototype models use a prototype that has a weighted list of possible values foreach feature.109P (in|~x) =1Zexp(?i?inifi(~x)) (1)where the fiare features that characterize the point ~x, and the ?iniareweights identifying the importance of the different features for the classin.
Z is a normalizing factor ensuring that the result is a probability.
Let~w = ?w1, .
.
.
, wn?
be the summation vector for the word w, in a space ofn dimensions.
Then we define the features fi, for 1 ?
i ?
n, to encodedistance from ~w in each dimension, withfi(~x) = (wi?
xi)2(2)This log-linear model has property (P1) through the weights ?ini.
It hasproperty (P2) through the exponential relation between the estimated prob-ability and the distances fi.
We will use occurrence vectors of w (as positivedata) and occurrence vectors of other words (as negative data) to estimatethe ?iduring training, thus calibrating the weights by the distances betweenthe summation vector and known members of the region.
In the current pa-per, we will consider only regions with sharp boundaries, which we obtain byplacing a threshold of 0.5 on the probability P (in|~x).
However, we considerit important that this model can also be used to represent regions with softboundaries by using P (in|~x) without a threshold.
It may thus be able tomodel borderline uses of a word, and unclear boundaries between senses [2].5 Experiments on hyponymyIn this section, we report on experiments on hyponymy in co-occurrencespace.
We test whether different co-occurrence space models can predict,given meaning representations (summation vectors, occurrence vectors, orregions) of two words, whether one of the two words is a hypernym of theother.
In all tests, the models do not see the words, just the co-occurrencespace representations.Experimental setting.
We used a Minipar [11] dependency parse of theBritish National Corpus (BNC) as the source of data for all experiments be-low.
The written portion of the BNC was split at random into two halves:a training half and a test half.
We used WordNet 3.0 as the ?ground truth?against which to evaluate models.
We work with two main sets of lemmas:first, the set of monosemous verbs according to WordNet (we refer to thisset as Mon), and second, the set of hypernyms of the verbs in Mon (wecall this set Hyp).
We concentrate on monosemous words in the current110paper since they will allow us to evaluate property (P3) most directly.
Sincethe model from Sec.
4 needs substantive amounts of occurrence vectors fortraining, we restricted both sets Mon and Hyp to verbs that occur with atleast 50 different direct objects in the training half of the BNC.
The directobjects, in turn, were restricted to those that occurred no more than 6,500and no less than 270 times with verbs in the BNC, to remove both uninfor-mative and sparse objects.
(The boundaries were determined heuristicallyby inspection of the direct objects for this pilot study.)
This resulted in a setMon consisting of 120 verbs, and Hyp consisting of 430 verbs.
Summationvectors for all words were computed with the dv package2from the traininghalf of the BNC, using vectors of 500 dimensions with raw co-occurrencecounts as dimension values.Experiment 1: Subsumption.
Above, we have hypothesized that co-occurrence space representations of hyponyms and hypernyms, in the formin which they are induced from corpus data, cannot in general be assumed tobe in either a subsumption or a subregion relation.
We test this hypothesis,starting with subsumption.
We define subsumption as ~x v ~y ??
?i(yi>0?
xi> 0).
Now, any given verb in Mon will be the hyponym of some verbsin Hyp and unrelated to others.
So we test, for each summation vector ~v1ofa verb in Mon and summation vector ~v2of a verb in Hyp, whether ~v1v ~v2.The result is that Mon verbs subsume 5% of the Hyp verbs of whichthey are hyponyms, and 1% of the Hyp verbs that are unrelated.We conclude that subsumption between summation vectors in co-occurrencespace is not a reliable indicator of the hyponymy relation between words.Experiment 2: Subregion relation.
Next, we test whether, when werepresent a Hyp-verb as a region in co-occurrence space, occurrences of itsMon-hyponyms fall inside that region, and occurrences of non-hypernymsare outside.
First, we compute occurrence vectors for each Hyp or Mon verbv as described in Sec.
2: Given an occurrence of a verb v, we compute itsoccurrence vector by combining the summation vector of v with the sum-mation vector of the direct object of v in the given sentence3.
We combinetwo summation vectors by computing their average.
In this experiment, weuse occurrences from both halves of the BNC.
With those summation andoccurrence vectors in hand, we then learn a region representation for eachHyp verb using the model from Sec.
4.
We implemented the region modelusing the OpenNLP maxent package4.
Last, we test, for each Mon verb2http://www.nlpado.de/~sebastian/dv.html3Occurrences without a direct object were not used in the experiments.4http://maxent.sourceforge.net/111occurrence vector and each Hyp region, whether the occurrence vector isclassified as being inside the region.
The result is that the region modelsclassified zero hyponym occurrences as being inside, resulting in precisionand recall of 0.0.
These results show clearly that our earlier hypothe-sis was correct: The co-occurrence representations that we have inducedfrom corpus data do not lend themselves to reading off hyponymy relationsthrough either subsumption or the subregion relation.Experiment 3: Encoding hyponymy.
These findings do not mean thatit is impossible to test the applicability of hyponymy-based inferences inco-occurrence space.
If we cannot induce hyponymy relations from existingvector representations, we may still be able to encode hyponymy informationfrom a separate source such as WordNet.
Note that this would be difficultin a words-as-points representation: The only possibility there would beto modify summation vectors.
With a words-as-regions representation, wecan keep the summation vectors constant and modify the regions.
Our aimin this experiment is to produce a region representation for a Hyp verb vsuch that occurrence vectors of v?s hyponyms will fall into the region.
Weuse only direct hypernyms of Mon verbs in this experiment, a 273-verbsubset of Hyp we call DHyp.
For each DHyp verb v, we learn a regionrepresentation centered on v?s summation vector, using as positive trainingdata all occurrences of v and v?s direct hyponyms in the training half ofthe BNC.
(Negative training data are occurrences of other DHyp verbs andtheir children.)
We then test, for each occurrence of a Mon verb in thetest half of the BNC that does not occur in the training half with the samedirect object, whether it is classified as being inside v?s region.
The resultof this experiment is a precision of 95.2, recall of 43.4, and F-score of59.6 (against a random baseline of prec=11.0, rec=50.2, and F=18.0).
Thisshows that it is possible to encode hyponymy information in a co-occurrencespace representation: The region model identifies hyponym occurrences withvery high precision.
If anything, the region is too narrow, classifying manyactual hyponyms as negatives.6 Inference in co-occurrence spaceIn this section we take a step back to ask what it means for co-occurrencespace to support inferences, taking the inferences in Ex.
(1) and (2) as anexample.
The inference in Ex.
(1), which involves a paraphrase, is supportedin two ways: (I1) Paraphrase candidates ?
words that may be substitutedfor acquire in some contexts ?
can be read off a co-occurrence space represen-112tation [12].
They are the words whose summation vectors are closest to thesummation vector of acquire in space.
In this way, co-occurrence space canbe used for the construction of context-dependent paraphrase rules.
(I2)Given an occurrence ~o of acquire, the appropriateness of applying the para-phrase rule substituting buy for acquire is estimated based on the distancebetween ~o and the summation vector~buy of buy [17, 3].
This can be usedto select the single best paraphrase candidate for the given occurrence ofacquire, or to produce a ranking of all paraphrase candidates [3].Concerning the hyponymy-based inference in Ex.
(2), we have establishedin Experiments 1 and 2 (Sec.
5) that it is at least not straightforward toconstruct hyponymy-based rules from co-occurrence space representationsin analogy to (I1).
However, (H2) Experiment 3 has shown that, given aset of attested occurrences of hyponyms of move, we can construct a regionrepresentation for move in co-occurrence space that can be used to testapplicability of hyponymy-based rules: The appropriateness of applying thehyponymy-based rule substituting move for this specific occurrence of runcan be estimated based on whether the occurrence vector of run is locatedinside the move region.In both (I2) and (H2), an inference rule is ?attached?
to a point or aregion in co-occurrence space: the summation vector of buy in (I2), theregion representation of move in (H2).
The inference rule is considered ap-plicable to an occurrence if its occurrence vector is close enough in space tothe attachment point or inside the attachment region.
Co-occurrence spacethus offers a natural way of determining the applicability of a (paraphraseor hyponymy-based) inference rule to a particular occurrence, via distanceto the attachment point or inclusion in the attachment region.
Applicabil-ity can be treated as a yes/no decision, or it can be expressed through agraded degree of confidence.
In the case of attachment point, this degreeof confidence would simply be the similarity between occurrence vector andattachment point.
Concerning attachment regions, note that the model ofSec.
4 actually estimates a probability of region inclusion for a given pointin space.
In this paper, we have placed a threshold of 0.5 on the probabilityto derive hard judgments, but the probability can also be used directly as adegree of confidence.The general principle of using co-occurrence space representation to rateinference rule applicability, and to do this by linking rules to attachmentpoints or regions, could maybe be used for other kinds of inference rules aswell.
The prerequisite is, of course, that it must make sense to judge ruleapplicability through a single attachment point or region.1137 Conclusion and outlookIn this paper, we have studied how semantic space representations supportinferences, focusing on hyponymy.
To encode hyponymy through the sub-region relation, we have considered word meaning representations throughregions in semantic space.
We have argued that a point in semantic spacerepresents a mixture of uses, not a potential entity, and that the regionsin semantic space we want to identify are those that represent the same orsimilar meanings.
We have introduced a computational model that learnsregion representations, and we have shown that this model can predict hy-ponymy with high precision.
Finally, we have suggested that semantic spacesupports inferences by attaching inference rules to points or regions in spaceand licensing rule application depending on distance in space.
It is an openquestion how far the idea of attachment points and attachment regions canbe extended beyond the paraphrase and hyponymy rules we have consideredhere; this is the question we will consider next.Acknowledgements.
Many thanks to Manfred Pinkal, Jason Baldridge,David Beaver, Graham Katz, Alexander Koller, and Ray Mooney for veryhelpful discussions.
(All errors are, of course, my own.
)References[1] P. Cimiano, A. Hotho, and S. Staab.
Learning concept hierarchies from textcorpora using formal concept anaylsis.
Journal of Artificial Intelligence Re-search, 24:305?339, 2005.
[2] K. Erk and S. Pado?.
Towards a computational model of gradience in wordsense.
In Proceedings of IWCS-7, Tilburg, The Netherlands, 2007.
[3] K. Erk and S. Pado.
A structured vector space model for word meaning incontext.
In Proceedings of EMNLP-08, Hawaii, 2008.
[4] C. Fellbaum, editor.
WordNet: An electronic lexical database.
MIT Press,Cambridge, MA, 1998.
[5] P. Ga?rdenfors.
Conceptual spaces.
MIT press, Cambridge, MA, 2004.
[6] N. Goodman.
Fact, Fiction, and Forecast.
Harvard University Press, Camb-dridge, MA, 1955.
[7] P. Hanks.
Do word meanings exist?
Computers and the Humanities, 34(1-2):205?215(11), 2000.
[8] M. Hearst.
Automatic acquisition of hyponyms from large text corpora.
InProceedings of COLING 1992, Nantes, France, 1992.114[9] A. Kilgarriff.
I don?t believe in word senses.
Computers and the Humanities,31(2):91?113, 1997.
[10] T. Landauer and S. Dumais.
A solution to Platos problem: the latent seman-tic analysis theory of acquisition, induction, and representation of knowledge.Psychological Review, 104(2):211?240, 1997.
[11] D. Lin.
Principle-based parsing without overgeneration.
In Proceedings ofACL?93, Columbus, Ohio, USA, 1993.
[12] D. Lin.
Automatic retrieval and clustering of similar words.
In COLING-ACL98, Montreal, Canada, 1998.
[13] W. Lowe and S. McDonald.
The direct route: Mediated priming in semanticspace.
In Proceedings of the Cognitive Science Society, 2000.
[14] K. Lund and C. Burgess.
Producing high-dimensional semantic spaces fromlexical co-occurrence.
Behavior Research Methods, Instruments, and Comput-ers, 28:203?208, 1996.
[15] C. D. Manning, P. Raghavan, and H. Schu?tze.
Introduction to InformationRetrieval.
Cambridge University Press, 2008.
[16] S. McDonald and M. Ramscar.
Testing the distributional hypothesis: Theinfluence of context on judgements of semantic similarity.
In Proceedings ofthe Cognitive Science Society, 2001.
[17] J. Mitchell and M. Lapata.
Vector-based models of semantic composition.
InProceedings of ACL-08, Columbus, OH, 2008.
[18] G. L. Murphy.
The Big Book of Concepts.
MIT Press, 2002.
[19] R. M. Nosofsky.
Attention, similarity, and the identification-categorizationrelationship.
Journal of Experimental Psychology: General, 115:39?57, 1986.
[20] S. Pado?
and M. Lapata.
Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199, 2007.
[21] M. Sahlgren and J. Karlgren.
Automatic bilingual lexicon acquisition usingrandom indexing of parallel corpora.
Journal of Natural Language Engineering,Special Issue on Parallel Texts, 11(3), 2005.
[22] H. Schu?tze.
Automatic word sense discrimination.
Computational Linguistics,24(1), 1998.
[23] R. Shepard.
Towards a universal law of generalization for psychological science.Science, 237(4820):1317?1323, 1987.
[24] E. E. Smith, D. Osherson, L. J. Rips, and M. Keane.
Combining prototypes:A selective modification model.
Cognitive Science, 12(4):485?527, 1988.
[25] R. Snow, D. Jurafsky, and A. Y. Ng.
Semantic taxonomy induction fromheterogenous evidence.
In Proceedings of COLING/ACL?06, 2006.115
