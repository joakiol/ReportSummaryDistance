AnThe N-Best Algorithm:Efficient Procedure for Finding Top NSentence HypothesesYen-Lu Chow and R ichard  Schwar tzBBN Systems and Techno log ies  Corporat ionCambr idge .
MA 02138ABSTRACTIn this paper we introduce a new search algorithm thatprovides a simple, clean, and efficient interface betweenthe speech and natural anguage components of a spo-ken language system.
The N-Best algorithm is a time-synchronous Viterbi-style beam search algorithm thatcan be made to find the most likely N whole sentencealternatives that are within a given a "beam" of the mostlikely sentence.
The algorithm can be shown to be exactunder some reasonable constraints.
That is, it guaran-tees that the answers it finds are, in fact, the most  likelysentence hypotheses.
The computation is linear withthe length of the utterance, and faster than linear in N.When used together with a first-order statistical gram-mar, the correct sentence is usually within the first fewsentence choices.
The output of the algorithm, which isan ordered set of sentence hypotheses with acoustic andlanguage model scores can easily be processed by naturallanguage knowledge sources.
Thus, this method of inte-grating speech recognition and natural anguage avoidsthe huge expansion of the search space that would beneeded to include all possible knowledge sources in atop-down search.
The algorithm has also been used togenerate alternative sentence hypotheses for discrimina-tive training.
Finally, the alternative sentences generatedare useful for testing overgeneration f syntax and se-mantics.1 IntroductionIn a spoken language system (SLS) we have a largesearch problem.
We must find the most likely word se-quence consistent with all knowledges ources (speech,statistical N-gram, natural anguage).
The natural an-guage (NL) knowledge sources are many and varied, andmight include syntax, semantics, discourse, pragmatics,and prosodics.
One way to use all of these constraintsis to perform a top-down search that, at each point, usesall of the knowledge sources (KSs) to determine whichwords can come next, and with what probabilities.
As-suming an exhaustive search in this space, we can findthe most likely sentence.
However, since many of theseKSs contain "long-distance" ffects (for example, agree-ment between words that are far apart in the input), thesearch space can be quite large, even when pruned usingvarious beam search or best-first search techniques.
Fur-thermore, a top-down search strategy requires that all ofthe KSs be formulated in a predictive, left-to-right man-ner.
This may place an unnecessary restriction on thetype of knowledge that can be used.The general solution that we have adopted is to applythe KSs in the proper order to constrain the search pro-gressively.
Thus, we trade off the entropy reduction thata KS provides against he cost of applying that KS.
Nat-urally, we can also use a pruning strategy to reduce thesearch space further.
By ordering the various KSs, weattempt to minimize the computational costs and com-plexity for a given level of search error rate.
To do thiswe apply the most powerful and cheapest KSs first togenerate the top N hypotheses.
Then, these hypothesesare evaluated using the remaining KSs.
In the remain-der of this paper we present the N-best search paradigm,followed by the N-best search algorithm.
Finally, wepresent statistics of the rank of the correct sentence in alist of the top N sentences using acoustic-phonetic mod-els and a statistical language model.2 The  N-best  Parad igmFigure I illustrates the general N-best search paradigm.We order the various KSs in terms of their relative powerand cost.
Those that provide more constraint, at a lessercost, are used first in the N-best search.
The output ofthis search is a list of the most likely whole sentencehypotheses, along with their scores.
These hypotheses199are then rescored (or filtered) by the remaining KSs.Depending on the amount of computation required,we might include more or less KSs in the first N-bestsearch.
For example, it is quite inexpensive to searchusing a first-order statistical language model, since weneed only one instance of each word in our network.
Fre-quently, a syntactic model of NL will be quite large, soit might be reserved until after the list generation.
Giventhe list, each alternative can usually be considered in turnin a fraction of a second.
If the syntax is small enough,it can be included in the initial N-best search, to furtherreduce the list that would be presented to the remainderof the KSs.
Another example of this progressive fil-tering could be the use of high-order statistical languagemodels.
While the high-order model ~equently providesadded power (over a first-order model), the added poweris usually not commensurate with the large amount of ex-tra computation and storage needed for the search.
Inthis case, a first-order language model can be used to re-duce the choice to a small number of alternatives whichcan then be reordered using the higher-order model.Besides the obvious computational dvantages, thereare several other practical advantages of this paradigm.Since the output of the first stage is a small amount oftext, and there is no further processing required fromthe acoustic recognition component, he interface be-tween the speech recognition and the other KSs is triv-ially simple, while still optimal.
As such this paradigmprovides a most convenient mechanism for integratingwork among several research sites.
In addition, the highdegree of modularity means that the different componentsubsystems can be optimized and even implemented sep-arately (both hardware and software).
For example, thespeech recogmtion might run on a special-purpose arrayprocessor-like machine, while the NL might run on ageneral purpose host.3 The  N-Best  A lgor i thmThe optimal N-Best algorithm is quite similar to thetime-synchronous Viterbi decoder that is used quite com-monly, with a few small changes.
It must compute prob-abilities of word-sequences rather than state-sequences,and it must find all such sequences within the specifiedbeam.At each state:?
Keep separate records for theories with differentword sequence histories.?
Add probabilities for the same theory.?
Keep up to a specified maximum number N of the-ories whose probabilities are within a threshold ofthe probability of most likely word sequence at thatstate.
Note that this state-dependent-threshold isdistinct from the global beam search threshold.This algorithm requires (at least) N times the memoryfor each state of the hidden Markov model.
However,this memory is typically much smaller than the amountof memory needed to represent all the different acousticmodels.
We assume here, that the overall "beam" of thesearch is much larger than the "beam at each state" toavoid pruning errors.
In fact, for the first-order grammar,it is even reasonable to have an infinite beam, since thenumber of states is determined only by the vocabularysize.At first glance, one might expect hat the cost of com-bining several sets of N theories into one set of N the-odes at a state might require computation on the orderof N 2.
However, we have devised a "grow ~.md prune"strategy that avoids this problem.
At each state, we sim-ply gather all of the incoming theories.
At any instant,we know the best scoring theory coming to this state atthis time.
From this, we compute a pruning thresholdfor the state.
This is used to discard any theories thatare below the threshold.
At the end of the frame (orif the number of theories gets too large), we reduce thenumber of theories using aprune and count strategy thatrequires no sorting.
Since this computation is small, wefind, empirically that the overall computation i creaseswith x/N, or slower than linear.
This makes it practicalto use somewhat high values of N in the search.4 Rank  of  the Cor rec t  AnswerWhether the N-best search is practical depends directlyon whether we can assure that the correct answer is re-liably within the list that is created by the first stage.
(Actually, it is sufficient if there is any answer that willbe accepted by all the NL KSs, since no amount ofsearch would make the system choose the lower scoringcorrect answer m this case.)
It is possible that when thecorrect answer is not the top choice, it might be quite fardown the list, since there could be exponentially manyother alternatives that score between the highest scor-ing answer and the correct answer.
Whether this is truedepends on the power of the acoustic-phonetic modelsand the statistical language model used in the N-best200search.
Therefore we have accumulated statistics of therank of the correct sentence in the list of N answers fortwo different language models: the statistical class gram-mar(perplexity 100), and no grammar(perplexity 1000).Figure 2 plots the cumulative distribution of the rankfor the two different language models.
The distributionis plotted for lists up to 100 long.
We have also markedthe average rank on the distribution.
As can be seen,for the case of no language model, the average rankis higher than that for the statistical grammar.
In fact,about 20% of the time, the correct answer is not onthe list at all.
However, when we use the statisticalclass grammar, which is a fairly weak grammar for thisdomain, we find that the average rank is 1.8, since mostof the time, the correct answer is within the first fewchoices.
In fact, for this test of 215 sentences.
99 percentof the sentences were round within the 24 top choices.Furthermore, the acoustic model used in this experimentis an earlier version that results in twice the word errorrate of the most recent models reported elsewhere inthese proceedings.
This means that when redone withbetter acoustic models, the rank will be considerablylower.To illustrate the types of lists that we see we show be-low a sample N-best output.
In this example, the correctanswer is the fifth one on the list.Example of N-best OutputAnswer:Set chart switch resolution to high.Top N Choices:Set charts which resolution to five.Set charts which resolution to high.Set charts which resolution to on.Set chart switch resolution to five.Set chart switch resolution to high.
(***)Set chart switch resolution to on.Set charts which resolution to the high.Set the charts which resolution to five.procedure, or by using overall statistics of typical errors.Instead, we can generate all the actual alternatives thatare appropriate to each particular sentence.A second application for the N-best algorithm is togenerate aitemative sentences that can be used to testovergenerafion i  the design of natural language sys-tems.
Typically, if overgeneration is tested at all, it is bygenerating random sentences using the NL model, andseeing whether they make sense.
One problem with thisis that many of the word sequences generated this waywould never, in fact, be presented to a NL system byany reasonable acoustic recognition component.
Thus,much of the tuning is being done on unimportant prob-lems.
A second problem is that the work of exanfiningthe generated sentences i  a very tedious manual process.If, instead, we generate N-best lists from a real acous-tic recognition system, then we can ask the NL systemto parse all the sentences that are known to be wrong.Hopefully the NL system will reject most of these, andwe only need to look at those that were accepted, to seewhether they should have been.6 ConclusionWe have presented a new algorithm for computing thetop N sentence hypotheses for a hidden Markov model.Unlike previous algorithms, this one is guaranteed tofind the most likely scoring hypotheses with essentiallyconstant computation time.
This new algorithm makespossible a simple and efficient approach to integration ofseveral knowledge sources, in particular the integrationof arbitrary natural anguage knowledge sources in spo-ken language systems.
In addition there are other usefulapplications of the algorithm.AcknowledgementThis work was .supported by the Defense AdvancedResearch Projects Agency and monitored by the Officeof Naval Research under Contract No.
N00014-85-C-0279.5 Other Applications for N-Best AlgorithmWe have, so far, found two additional application forthe N-Best algorithm.
The first is to generate alternativehypotheses for discriminative training algorithms.
Typi-cally, alternatives must be generated using a fast match201Sp,.+h__l JOr+.r,+ Reorder 'L_.
To p Input ~ N-Best Sentence'Lm List |ChoiceKS 1 KS 2Stat~icalGrammarSyntaxstatistical Grammar+ Syntu1st order statisticalFull NLPSemantics~ etc.Semantics, etc.Higher-order statisticalFigure 1: N-best Search Paradigm1 O09080706050403020'10~ C  L AS S GRAMP"IAR(Perplexity- 100)~ ~  GRMIMARerplextty - 1000)III'1I,I!11.8!
20 30 ~ ~ 60 70 8O090BO7060SO403020I0I 90Figure 2: Cumulative Distribution of Rank of Correct Answer202
