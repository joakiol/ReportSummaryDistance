Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1047?1055,Beijing, August 2010Discriminative Induction of Sub-Tree Alignmentusing Limited Labeled DataJun Sun1,2                             Min Zhang1                      Chew Lim Tan21Institute for Infocomm Research      2School of Computing, National University of Singaporesunjun@comp.nus.edu.sg     mzhang@i2r.a-star.edu.sg     tancl@comp.nus.edu.sgAbstractWe employ Maximum Entropy model to con-duct sub-tree alignment between bilingualphrasal structure trees.
Various lexical andstructural knowledge is explored to measure thesyntactic similarity across Chinese-English bi-lingual tree pairs.
In the experiment, we evalu-ate the sub-tree alignment using both goldstandard tree bank and the automatically parsedcorpus with manually annotated sub-tree align-ment.
Compared with a heuristic similaritybased method, the proposed method significant-ly improves the performance with only limitedsub-tree aligned data.
To examine its effective-ness for multilingual applications, we further at-tempt different approaches to apply the sub-treealignment in both phrase and syntax based SMTsystems.
We then compare the performancewith that of the widely used word alignment.Experimental results on benchmark data showthat sub-tree alignment benefits both systems byrelaxing the constraint of the word alignment.1 IntroductionRecent research in Statistical Machine Translation(SMT) tends to incorporate more linguisticallygrammatical information into the translation mod-el known as linguistically motivated syntax-basedmodels.
To develop such models, the phrasalstructure parse tree is usually adopted as the repre-sentation of bilingual sentence pairs either on thesource side (Huang et al, 2006; Liu et al, 2006)or on the target side (Galley et al, 2006; Marcu etal., 2006), or even on both sides (Graehl andKnight, 2004; Zhang et al, 2007).
Most of theabove models either construct a pipeline to trans-form from/to tree structure, or synchronously gen-erate two trees in parallel (i.e., synchronous pars-ing).
Both cases require syntactically rich transla-tional equivalences to handle non-local reordering.However, most current works obtain the syntactictranslational equivalences by initially conductingalignment on the word level.
To employ wordalignment as a hard constraint for rule extractionhas difficulty in capturing such non-local phenom-ena and will fully propagate the word alignmenterror to the later stage of rule extraction.Alternatively, some initial attempts have beenmade to directly conduct syntactic structurealignment.
As mentioned in Tinsley et al (2007),the early work usually constructs the structurealignment by hand, which is time-consuming.
Re-cent research tries to automatically align the bilin-gual syntactic sub-trees.
However, most of theseworks suffer from the following problems.
Firstly,the alignment is conducted based on heuristicrules, which may lose extensibility and generalityin spite of accommodating some common cases(Groves et al, 2004).
Secondly, various similaritycomputation methods are used based merely onlexical translation probabilities (Tinsley et al,2007; Imamura, 2001) regardless of structural fea-tures.
We believe the structure information is animportant issue to capture the non-local structuraldivergence of languages by modeling beyond theplain text.To address the above issues, we present a statis-tical framework based on Maximum Entropy(MaxEnt) model.
Specifically, we consider sub-tree alignment as a binary classification problemand use Maximum Entropy model to classify eachinstance as aligned or unaligned.
Then, we per-form a greedy search within the reduced searchspace to conduct sub-tree alignment links based onthe alignment probabilities obtained from the clas-sifier.Unlike the previous approaches that can onlymeasure the structural divergence via lexical fea-tures, our approach can incorporate both lexicaland structural features.
Additionally, instead ofexplicitly describing the instances of sub-tree pairsas factorized sub-structures, we frame most of ourfeatures as score based feature functions, whichhelps solve the problem using limited sub-treealignment annotated data.
To train the model andevaluate the alignment performance, we adopt1047HIT Chinese-English parallel tree bank for goldstandard evaluation.
To explore its effectiveness inSMT systems, we also manually annotate sub-treealignment on automatically parsed tree pairs andperform the noisy data evaluation.
Experimentalresults show that by only using limited sub-treealigned data of both corpora, the proposed ap-proach significantly outperforms the baselinemethod (Tinsley et al, 2007).
The proposed fea-tures are very effective in modeling the bilingualstructural similarity.
We further apply the sub-treealignment to relax the constraint of word align-ment for both phrase and syntax based SMT sys-tems and gain an improvement in BLEU.2 Problem definitionA sub-tree alignment process pairs up the sub-trees across bilingual parse trees, whose lexicalleaf nodes covered are translational equivalent, i.e.,sharing the same semantics.
Grammatically, thetask conducts links between syntactic constituentswith the maximum tree structures generated overtheir word sequences in bilingual tree pairs.In general, sub-tree alignment can also be inter-preted as conducting multiple links across internalnodes between sentence-aligned tree pairs asshown in Fig.
1.
The aligned sub-tree pairs usuallymaintain a non-isomorphic relation with each oth-er especially for higher layers.
We adapt the samecriteria as Tinsley et al (2007) in our study:(i) a node can only be linked once;(ii) descendants of a source linked node mayonly link to descendants of its targetlinked counterpart;(iii) ancestors of a source linked node may on-ly link to ancestors of its target linkedcounterpart.where the term ?node?
refers to root of a sub-tree, which can be used to represent the sub-tree.3 ModelWe solve the problem as binary classification andemploy MaxEnt model with a greedy search.Given a bilingual tree pair    and   ,{            } is the source tree consisting ofsub-trees?where   is also the number of nodes inthe source tree        {            } is the tar-get tree consisting of   sub-trees, where   is alsothe number of nodes in the target tree   .For each sub-tree pair         in the given bilin-gual parse trees         , the sub-tree alignmentprobability is given by:( |       )[?
(        )]?
[?
()](1)where{(     )(2)Feature functions are defined in a quadruple(         ).
is an additional variable to incorpo-rate new dependencies other than the sub-treepairs.
For each feature function   (         ), aweight    is applied to tailor the distribution.After classifying the candidate sub-tree pairs asaligned or unaligned, we perform a greedy searchwithin the reduced search space to conduct surelinks based on the conditional probability( |       )  obtained from the classifier.
Thealignment probability is independently normalizedfor each sub-tree pair and hence suitable as asearching metric.The greedy search algorithm can be describedas an automaton.
A state in the search space is apartial alignment with respect to the given bilin-gual tree pair.
A transition is to add one more linkof node pairs to the current state.
The initial statehas no link.
The terminal state is a state where nomore links can be added according to the defini-tion in Section 2.
We use greedy search to gener-ate the best-links at the early stage.
There are cas-es that the correctly-aligned tree pairs have veryfew links, while we have a bunch of candidateswith lower alignment probabilities.
However, thesum of the lower probabilities is larger than that ofthe correct links?, since the number of correctlinks is much fewer.
This makes the alignmentresults biased to be with more links.
The greedysearch helps avoid this asymmetric problem.4 Feature FunctionsIn this section, we introduce a variety of featurefunctions to capture the semantically equivalentSVBA?(NULL)?(me)?(give)??(pen)?(.
)P WJRVGNGVOGive topenthe me .VBP DT NN TO PRP PUNC.NP PPVPSTs:Tt:Figure 1: Sub-tree alignment as referred toNode alignment1048counterparts and structural divergence across lan-guages.
For the semantic equivalence, we definelexical and word alignment feature functions.Since those feature functions are directional, wedescribe most of these functions as conditionalfeature functions based on the conditional lexicalprobabilities.
We also introduce the tree structuralfeatures to deal with the structural divergence ofbilingual parse trees.
Inspired by Burkett andKlein (2008), we introduce the feature functions inan internal-external manner based on the fact thatthe feature scores for an aligned sub-tree pair tendto be high inside both sub-trees, while they tend tobe low inside one sub-tree and outside the other.4.1 Internal Lexical FeaturesWe use this feature to measure the degree of se-mantic equivalence of the sub-tree pair.
Accordingto the definition of sub-tree alignment in Section 2,the word sequence covered by the sub-tree pairshould be translational equivalence.
Therefore, thelexicons within the two corresponding sub-spansshould be highly related in semantics.
We definethe internal lexical features as follows:(  |  )  (?
?
)(  )(  |  )  (?
?
)(  )where        refers to the lexical translationprobability from the source word   to the targetword   within the sub-tree spans, whilerefers to that from target to source;        refers tothe word set for the internal span of the sourcesub-tree   , while   (  ) refers to that of the targetsub-tree   .4.2 Internal-External Lexical FeaturesIntuitively, lexical translation probabilities tend tobe high within the translational equivalence, whilelow within the non-equivalent counterparts.
Ac-cording to this, we define the internal-external lex-ical feature functions as follows:(  |  )?
(  ){(             )}(  )|  (  )|(  |   )?
(  ){(             )}    (  )where         refers to the word set for the ex-ternal span of the source sub-tree   , whilerefers to that of the target sub-tree   .
We choose arepresentation different from the internal lexicalfeature scores, since for cases with small innerspan and large outer span, the sum of internal-external scores may be overestimated.
As a result,we change the sum operation into max, which iseasy to be normalized.4.3 Internal Word Alignment FeaturesAlthough the word alignment information withinbilingual sentence pairs is to some extent not reli-able, the links of word alignment account muchfor the co-occurrence of the aligned terms.
Wedefine the internal word alignment features as fol-lows:(     )?
?
(             )(  )    (  )(         |  (  )|)where{The binary function        is introduced totrigger the computation only when a word alignedlink exists for the two words       within the sub-tree span.4.4 Internal-External Word Alignment Fea-turesSimilar to lexical features, we also introduce in-ternal-external word alignment features as follows:(     )?
?
(             )(  )(          |  (  )|)(     )?
?
(             )(  )(         |   (  )|)where{4.5 Tree Structural FeaturesIn addition to the lexical correspondence, we alsocapture the structural divergence by introducingthe tree structural features as follows:Span difference: Translational equivalent sub-tree pairs tend to share similar length of spans.Thus the model will penalize the candidate sub-tree pairs with largely different length of spans.
(     )  |(  )|Number of Descendants: Similarly, the num-ber of the root?s descendants of the aligned sub-trees should also correspond.
(     )  |(  )|1049where      refers to the descendant set of theroot to an individual sub-tree.Tree Depth difference: Intuitively, translation-al equivalent sub-tree pairs tend to have similardepth from the root node of the parse tree.
We canfurther allow the model to penalize the candidatesub-tree pairs with different distance from the rootnode.
(     )  |(  )(  )(  )|4.6 Binary Grammatical FeaturesIn the previous sections, we design some scorebased feature functions to describe syntactic treestructural similarities, rather than directly usingthe substructures.
This is because for limited anno-tated tree alignment data, features like tokens andgrammar rules are rather sparse.
In spite of this,we still have a closed set of grammatical tagswhich can be covered by a small amount of data.Therefore, we use the combination of root gram-mar tags of the sub-tree pairs as binary features.5 TrainingWe train the sub-tree alignment model in twosteps:Firstly, we learn the various feature functions.On one hand, GIZA++ is offline trained on a largeamount of bilingual sentences to compute the lexi-cal and word alignment features.
On the otherhand, the tree structural features, similar to wordand phrase penalty features in phrase based SMTmodels, are computed online for both training andtesting.Secondly, we train the MaxEnt model in Eq.
1,using the training corpus which consists of thebilingual parse tree pairs with manually annotatedsub-tree alignment.
We apply the widely used GIS(Generalized Iterative Scaling) algorithm (Darrochand Ratcliff, 1972) to optimize.
In practice, wemodify Och?s implementation YASMET.Since we consider each sub-tree pair as an indi-vidual instance, it is easy to see that the negativesamples heavily overwhelm the positive ones.
ForGIS training, such a skewed distribution easilydrives the parameters to facilitate the negative in-stances.
We address this problem by giving moreweight to the positive training instances.6 Experiments on Sub-Tree AlignmentsWe utilize two different corpora to evaluate theproposed sub-tree alignment method and its capa-bility to plug in the related applications respective-ly.
One is HIT English Chinese parallel tree bankwith both tree structure and sub-tree alignmentmanually annotated.
The other is the automaticallyparsed bilingual tree pairs (allowing minor parsingerrors) with manually annotated sub-tree align-ment.
The latter benefits MT task, since most lin-guistically motivated syntax SMT systems requirea held-out automatic parser to achieve rule induc-tion.6.1 Data preparationFor the gold standard corpus based experiment, weuse HIT 1  Chinese-English parallel tree bank,which is collected from English learning textbooks in China as well as example sentences indictionaries.
It consists of 16131 gold standardparse tree pairs with manually annotated sub-treealignments.
The annotation strictly preserves thesemantic equivalence, i.e., it only conducts surelinks in the internal node level, while ignoringpossible links adopted in word alignment.
In con-trast, in the POS level, n-to-n links are allowed inannotation.
In order to be consistent with the defi-nition in Section 2, we delete those n-to-n links inPOS level.
The word segmentation, tokenizationand parse-tree in the corpus are manually con-structed or checked.
The Chinese parse tree in HITtree bank adopts a different annotation criterionfrom the Penn TreeBank annotation, which is de-signed by the HIT research team.
The new criteri-on can better facilitate the description of some rarestructural phenomena in Chinese.
The Englishparse tree still uses Penn TreeBank annotation.The statistics of HIT corpus is shown in Table 1.Chinese English# of Sentence pair 16131Avg.
Sentence Length 13.06 13.00Avg.
# of sub-tree 21.60 23.74Avg.
# of alignment 11.71Table 1.
Statistics for HIT gold standard Tree bankSince the induction of sub-tree alignment is de-signed to benefit the machine translation modeling,it is preferable to conduct the sub-tree alignmentexperiment on the corpus for MT evaluation.However, most syntax based SMT systems use anautomatic parser to facilitate training and decoding,which introduces parsing errors.
Additionally, thegold standard HIT corpus is not applicable for MT1  HIT corpus is designed and constructed by HIT mitlab.http://mitlab.hit.edu.cn/index.php/resources.html .
We li-censed the corpus from them for research usage.1050experiment due to problems of domain divergence,annotation discrepancy (Chinese parse tree adoptsa different grammar from Penn Treebank annota-tions) and degree of tolerance for parsing errors.Due to the above issues, we annotate a new dataset to apply the sub-tree alignment in machinetranslation.
We randomly select 300 bilingual sen-tence pairs from the Chinese-English FBIS corpuswith the length     in both the source and targetsides.
The selected plain sentence pairs are furtherparsed by Stanford parser (Klein and Manning,2003) on both the English and Chinese sides.
Wemanually annotate the sub-tree alignment for theautomatically parsed tree pairs according to thedefinition in Section 2.
To be fully consistent withthe definition, we strictly preserve the semanticequivalence for the aligned sub-trees to keep ahigh precision.
In other words, we do not conductany doubtful links.
The corpus is further dividedinto 200 aligned tree pairs for training and 100 fortesting.
Some initial statistic of the automaticallyparsed corpus is shown in Table 2.6.2 Baseline approachWe implement the work in Tinsley et al (2007) asour baseline methodology.Given a tree pair        , the baseline ap-proach first takes all the links between the sub-treepairs as alignment hypotheses, i.e., the Cartesianproduct of the two sub-tree sets:{            }  {            }By using the lexical translation probabilities,each hypothesis is assigned an alignment score.All hypotheses with zero score are pruned out.Then the algorithm iteratively selects the link ofthe sub-tree pairs with the maximum score as asure link, and blocks all hypotheses that contradictwith this link and itself, until no non-blocked hy-potheses remain.The baseline system uses many heuristics insearching the optimal solutions with alternativescore functions.
Heuristic skip1 skips the tied hy-potheses with the same score, until it finds thehighest-scoring hypothesis with no competitors ofthe same score.
Heuristic skip2 deals with thesame problem.
Initially, it skips over the tied hy-potheses.
When a hypothesis sub-tree pairwithout any competitor of the same score is found,where neither    nor    has been skipped over, thehypothesis is chosen as a sure link.
Heuristicspan1 postpones the selection of the hypotheseson the POS level.
Since the highest-scoring hy-potheses tend to appear on the leaf nodes, it mayintroduce ambiguity when conducting the align-ment for a POS node whose child word appearstwice in a sentence.The baseline method proposes two score func-tions based on the lexical translation probability.They also compute the score function by splittingthe tree into the internal and external components.Tinsley et al (2007) adopt the lexical transla-tion probabilities dumped by GIZA++ (Och andNey, 2003) to compute the span based scores foreach pair of sub-trees.
Although all of their heuris-tics combinations are re-implemented in our study,we only present the best result among them withthe highest Recall and F-value as our baseline,denoted as skip2_s1_span12.6.3 Experimental settings?
To examine the effectiveness of the proposedfeatures, we(1) learn the word alignment using the combina-tion of the 14k of HIT tree bank and FBIS (240k)corpus for both our approach and the baselinemethod, and divide the remaining HIT corpus as1k for training and 1k for testing.
(2) learn the word alignment on the entire FBIStraining corpus (240k) for both our approach andthe baseline method.
We then train and test onFBIS corpus of 200 and 100 respectively as statedin Table 2.?
In our task, annotating large amount of sub-treealignment corpus is time consuming and more dif-ficult compared with the tasks like sequence label-ing.
One of the important issues we are concernedabout is whether we can achieve an acceptableperformance with limited training data.
We(3) adopt the entire FBIS data (240k) to learnthe word alignment and various amount of HITgold standard corpus to train the MaxEnt model.Then we test the alignment performance on thesame HIT test set (1k) as (1).2 s1 denotes score function 1 in Tinsley et al (2007)Chinese English# of Sentence pair 200Train Avg.
Sentence Length 17 20.84Avg.
# of sub-tree 28.87 34.54Avg.
# of alignment 17.07Test # of Sentence pair 100Avg.
Sentence Length 16.84 20.75Avg.
# of sub-tree 29.18 34.1Avg.
# of alignment 17.75Table 2.
FBIS selected Corpus Statistics1051?
We further test the robustness of our methodunder different amount of data to learn the lexicaland word alignment feature functions.
We gradu-ally change the amount of FBIS corpus to train theword alignment.
Then we(4) use the same training (1k) and testing data(1k) with (1);(5) use FBIS corpus 200 to train MaxEnt modeland 100 for testing similar to (2).6.4 Experimental resultsWe use Precision, Recall and F-score to measurethe alignment performance and obtain the resultsas follows:?
In Table 3 and 4 for Exp (1) and (2) respectively,we show that by incrementally adding new fea-tures in a certain order, the F-value consistentlyincreases and both outperform the baseline method.From both tables, we find that the Binary fea-tures, with the combination of root grammar tagsof the sub-tree pairs, significantly improve thealignment performance.
We also try the differentcombinations of the parent, child or even siblingsto the root nodes.
However, all these derivativeconfigurations decrease the performance.
We at-tribute the ineffectiveness to data sparseness.
Fur-ther exploration suggests that the binary feature inHIT gold standard corpus exhibits a substantiallylarger improvement against other features thanFBIS corpus (Table 3 against Table 4).
The reasoncould be that the grammar tags in the gold stand-ard corpus are accurate, while FBIS corpus suffersfrom parsing errors.
Apart from that, the lexi-cal/word-alignment features in Table 3 do not per-form well, since the word alignment is trainedmainly on the cross domain FBIS corpus.
This isalso an important reason why there is a large gapin performance between Table 3 and 4, where theautomatic parsed FBIS corpus performs betterthan HIT gold standard tree bank in all configura-tions as well as the baseline.?
In Fig.
2(a) for Exp (3), we examine perfor-mance under different amount of training datafrom 1k to 15k.
The results change very little withover the amount of 1k.
Even with only 0.25k train-ing data, we are able to gain a result close to thebest performance.
This suggests that by utilizingonly a small amount of sub-tree aligned corpus,we can still achieve a satisfactory alignment result.The benefits come from the usage of the scorebased feature functions by avoiding using sub-structures as binary features, which suffers fromthe data sparseness problem.?
In Fig.
2(b-e) for Exp (4&5), we find that in-creasing the amount of corpus to train GIZA++does not improve much for the proposed methodon both HIT gold standard corpus (Fig.
2: b, c)and the automatic parsed data (Fig.
2: d, e).
This isdue to the various kinds of features utilized by theMaxEnt model, which does not bet on the lexicaland word alignment feature too much.
As for thebaseline method, we can only detect a relativelylarge improvement in the initial increment of cor-pus, while later additions do not help.
This resultsuggests that the baseline method is relatively lessextensible since it works completely on the lexicalsimilarities which can be only learned from theword alignment corpus.7 Experiments on Machine TranslationIn addition to the alignment evaluation, we con-duct MT evaluation as well.
We explore the effec-tiveness of sub-tree alignment for both phrase andlinguistically motivated syntax based systems.7.1 Experimental configurationIn the experiments, we train the translation modelon FBIS corpus (7.2M (Chinese) + 9.2M (English)words in 240,000 sentence pairs) and train a 4-gram language model on the Xinhua portion of theEnglish Gigaword corpus (181M words) using theSRILM Toolkits (Stolcke, 2002).
We use theseFeatures Precision Recall F-valueIn Lexical 50.96 48.11 49.49+ InOut Lexical 55.26 53.84 54.54+ In word align 56.16 60.59 58.29+ InOut word align 55.80 62.25 58.85+ Tree Structure  57.64 63.11 60.25+ Binary Feature 73.14 85.11 78.67Baseline [Tinsley 2007] 64.14 66.99 65.53Table 3.
Sub-tree alignment of different featurecombination for HIT gold standard test setFeatures Precision Recall F-valueIn Lexical 63.53 54.87 58.88+ InOut Lexical 66.00 63.66 64.81+ In word align 70.89 75.88 73.30+ InOut word align 72.05 80.16 75.89+ Tree Structure  72.03 80.95 76.23+ Binary Feature 76.08 85.29 80.42Baseline [Tinsley 2007] 70.48 78.70 74.36Table 4.
Sub-tree alignment of differentfeature combination for FBIS test set1052sentences with less than 50 characters from theNIST MT-2002 test set as the development set (tospeed up tuning for syntax based system) and theNIST MT-2005 test set as our test set.
We use theStanford parser (Klein and Manning, 2003) toparse bilingual sentences on the training set andChinese sentences on the development and test set.The evaluation metric is case-sensitive BLEU-4.For the phrase based system, we use Moses(Koehn et al 2007) with its default settings.
Forthe syntax based system, since sub-tree alignmentcan directly benefit Tree-2-Tree based systems,we apply the sub-tree alignment in an SMT systembased on Synchronous Tree Substitution Grammar(STSG) (Zhang et al, 2007).
The STSG baseddecoder uses a pair of elementary tree as a basictranslation unit.
Recent research on tree based sys-tems shows that relaxing the restriction from treestructure to tree sequence structure (SynchronousTree Sequence Substitution Grammar: STSSG)significantly improves the translation performance(Zhang et al, 2008).
We implement theSTSG/STSSG based model in Pisces decoder withthe same features and settings in Sun et al (2009).The STSSG based decoder translates each spaniteratively in a bottom up manner which guaran-tees that when translating a source span, any of itssub-spans has already been translated.
The STSGbased experiment can be easily achieved by re-stricting the translation rule set in the STSSG de-coder to be elementary tree pairs only.For the alignment setting of the baselines, weuse the word alignment trained on the entireFBIS(240k) corpus by GIZA++ with heuristicgrow-diag-final for Moses and the syntax systemsand perform rule extraction constrained on theword alignment.
As for the experiments adoptingsub-tree alignment, we use the above word align-ment to learn lexical/word alignment features, andtrain the sub-tree alignment model with FBIStraining data (200).7.2 Experimental resultsUtilizing the syntactic rules only has been arguedto be ineffective (Koehn et al, 2003).
Therefore,instead of using the sub-tree aligned rules only, wetry to improve the word alignment constrainedrule set by sub-tree alignment as shown in Table 5.Firstly, we try to Directly Concatenate (DirC)the sub-tree alignment constraint rule set3 to theoriginal syntax/phrase rule set based on wordalignment.
Then we re-train the MT model based3 For syntax based system, it?s just the sub-tree pairs deductedfrom the sub-tree alignment; for phrase based system, it's thephrases with context equivalent to the aligned sub-tree pairs.a                                                                                        b                                                                                     cd         eFigure 2: a. Precision/Recall/F-score for various amount of training data (k).b~e.
Various amount of data to train word alignmentb.
Precision/Recall for HIT test set.
c. F-score for HIT test set.d.
Precision/Recall for FBIS test set.
e. F-score for FBIS test set.0.25 1 3 5 10 150.50.60.70.80.9PrecisionRecallF-value0 50 100 150 200 2500.40.50.60.70.80.91PrecisionRecallBaseline-PrecisionBaseline-Recall0 50 100 150 200 2500.50.60.70.80.91F-valuleBaseline-F-value0 50 100 150 200 2500.50.60.70.80.91PrecisionRecallBaseline-PrecisionBaseline-Recall0 50 100 150 200 2500.50.60.70.80.91F-valuleBaseline-F-value1053on the obtained rule set.
Tinsley et al (2009) at-tempts different duplication of sub-tree alignmentconstraint rule set to append to the original phraserule set and reports positive results.
However, asshown in Table 5, we only achieve very minorimprovement (in STSSG based model the scoreeven drops) by direct introducing the new rules.Secondly, we propose a new approach to utilizesub-tree alignment by modifying the rule extrac-tion process.
We allow the bilingual phrases whichare consistent with Either Word alignment or Sub-tree alignment (EWoS) instead of to be consistentwith word alignment only.
The results in Table 5show that EWoS achieves consistently better per-formance than the baseline and DirC method.
Wealso find that sub-tree alignment benefits theSTSSG based model less compared with othersystems.
This is probably due to the fact that theSTSSG based system relies much on the tree se-quence rules.To benefit intuitive understanding, we providetwo alignment snippets in the MT training corpusin Fig.
3, where the red lines across the non-terminal nodes are the sub-tree aligned links con-ducted by our model, while the purple lines acrossthe terminal nodes are the word alignment linkstrained by GIZA++.
In the first example, the wordIsrael is wrongly aligned to two ????
?s byGIZA++, where the wrong link is denoted by thedash line.
This is common, since in a compoundsentence in English, the entities appeared morethan once are often replaced by pronouns at itslater appearances.
Therefore, the syntactic rulesconstraint by NR1-NNP1, IP2-VP2 and PP3-VP3respectively cannot be extracted for syntax sys-tems; while for phrase systems, context around thefirst ?????
cannot be fully explored.
In thesecond example, the empty word ???
is wronglyaligned, which usually occurs in Chinese-Englishword alignment.
As shown in Fig.
3, both casescan be resolved by sub-tree alignment conductedby our model, indicating that sub-tree alignment isa decent supplement to the word alignment rule set.8 ConclusionIn this paper, we propose a framework for bilin-gual sub-tree alignment using Maximum Entropymodel.
We explore various lexical and structuralfeatures to improve the alignment performance.We also manually annotated the automatic parsedtree pairs for both alignment evaluation and MTexperiment.
Experimental results show that ouralignment framework significantly outperformsthe baseline method and the proposed features arevery effective to capture the bilingual structuralsimilarity.
Additionally, we find that our approachcan perform well using only a small amount ofsub-tree aligned training corpus.
Further experi-ment shows that our approach benefits both phraseand syntax based MT systems.System Rules BLEUMoses BP* 23.86DirC  24.12EWoS  24.45SyntaxSTSGSTSG 24.71DirC  24.91EWoS  25.21Syntax STSSG 25.92STSSG DirC  25.88EWoS  26.12Table 5.
MT evaluation on various systemsBP* denotes bilingual phrases.BP, STSG, STSSG are baseline rule sets using wordalignment to constrain rule extraction.S1:T1:VP2PP3TOVP3P IP2CP VPVP DEC AD VVVV NR1?(to)??(oppose)???(Israel)?(`s)??(illegal)??
(occupation)NPNNNPVBJJNNP1POSTo oppose Israel `s illegal occupation???
(Israel)...NP VPNR NN VV AS???(Barak)??(government)??(choose)?
(NULL)NPNNPDT NNPthe Barak GovernmentNNPchoseS2:T2:Figure 3: Comparison between Sub-tree alignment results and Word alignment results1054ReferencesDavid Burkett and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In Pro-ceedings of EMNLP-08.
877-886.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang and IgnacioThayer.
2006.
Scalable Inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of COLING-ACL-06.
961-968.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In Proceedings of HLT-NAACL-2004.105-112.Declan Groves, Mary Hearne, and Andy Way.
2004.Robust sub-sentential alignment of phrase-structuretrees.
In Proceedings of COLING-04, pages 1072-1078.Liang Huang, Kevin Knight and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of Locality.
In Proceedings of AMTA-06.Kenji Imamura.
2001.
Hierarchical Phrase AlignmentHarmonized with Parsing.
In Proceedings of NLPRS.377-384.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of ACL-03.
423-430.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, Rich-ard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of ACL-07.
177-180.Philipp Koehn, Franz Josef Och and Daniel Marcu.2003.
Statistical Phrase-based Translation.
In Pro-ceedings of HLT-NAACL-2003.
48-54.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String alignment template for statistical machinetranslation.
In Proceedings of ACL-06, 609-616.Daniel Marcu, Wei Wang, Abdessamad Echihabi andKevin Knight.
2006.
SPMT: statistical machinetranslation with syntactified target languagephrases.
In Proceedings of EMNLP-06.
44-52.Franz Josef Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19-51, March.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP-02.
901-904.Jun Sun, Min Zhang and Chew Lim Tan.
2009.
A non-contiguous Tree Sequence Alignment-based Modelfor Statistical Machine Translation.
In Proceedingsof ACL-IJCNLP-09.
914-922.John Tinsley, Ventsislav Zhechev, Mary Hearne, andAndy Way.
2007.
Robust language pair-independentsub-tree alignment.
In Proceedings of MachineTranslation Summit-XI-07.John Tinsley, Mary Hearne, and Andy Way.
2009.Parallel treebanks in phrase-based statistical ma-chine translation.
In Proceedings of CICLING-09.Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, ShengLi and Chew Lim Tan.
2007.
A tree-to-tree align-ment-based model for statistical machine translation.In Proceedings of MT Summit-XI -07.
535-542.Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li,Chew Lim Tan and Sheng Li.
2008.
A tree sequencealignment-based tree-to-tree translation model.
InProceedings of ACL-08.
559-567.1055
