How to Encode Semantic Knowledge: AMethod for Meaning Representation andComputer-Aided AcquisitionPaola Velardi*Universita of AnconaMichela Fasolo~CervedomaniMaria Teresa Pazienza tUniversity of Roma IINatural language processing will not be able to compete with traditional information retrievalunless high-coverage t chniques are developed.
It is commonly agreed that a poor encoding ofthe semantic lexicon is the bottleneck of many existing systems.
A hand encoding of semanticknowledge on an extensive basis is not realistic; hence, it is important o devise methods bywhich such knowledge can be acquired in part or entirely by a computer.
But what type ofsemantic knowledge could be automatically learned, from which sources, and by what methods?This paper explores the above issues and proposes an algorithm to learn syncategorematicconcepts from text exemplars.
What is learned about a concept is not its defining features,such as kinship, but rather its patterns of use.The knowledge acquisition method is based on learning by observations; observations areexamples of word co-occurrences (collocations) in a large corpus, detected by a morphosyntacticanalyzer.
A semantic bias is used to associate collocations with the appropriate meaning relation,if one exists.
Based upon single or multiple xamples, the acquired knowledge is then generalizedto create semantic rules on concept uses.Interactive human intervention is required in the training phase, when the bias is definedand refined.
The duration of this phase depends upon the semantic losure of the sublanguageon which the experiment is carried out.
After training, final approval by a linguist is stillneeded for the acquired semantic rules.
At the current stage of experimentation f this system,it is unclear whether and when human supervision could be further reduced.1.
Four Important Questions in Lexical SemanticsIn the past few years there has been a growing interest in the field of lexical semantics.It is now commonly agreed that the the acquisition of the semantic lexicon is a centralissue in computational linguistics.
The major contributions on this topic are collectedin Computational Linguistics (1987), Evens (1988), and Zernik (in press).Research in lexical semantics i , however, rather heterogeneous in scope, methods,and results.
Paraphrasing the famous "What, when and how?"
questions introduced byKathleen McKeown (1985) in her studies on language generation, we pose four similarquestions that we believe important to understand the consequences of conceptual nd?
Instituto di Informatica, via Brecce Bianche, Anconat Dipartimento di Elettronica, Roma~: corso Stati Uniti, Padova(~) 1991 Association for Computational LinguisticsComputational Linguistics Volume 17, Number 2linguistic decisions in lexical semantics:..Why?
Why is lexical semantics useful in the first place?
Linguists andpsychologists are interested in the study of word senses to shed light onimportant aspects of human communication, such as concept formationand language use.
Lexicographers need computational ids to analyze ina more compact and extensive way word definitions in dictionaries.Computer scientists need semantics for the purpose of natural languageprocessing.?
The option and choices in lexical semantics are deeply related to theultimate objective of research.
1What?
What is encoded in a semantic lexicon?
Each word is a world.Despite the interest that semantics has received from the scholars ofdifferent disciplines ince the early history of humanity, a unifyingtheory of meaning does not exist.
In practice, the type and quality of theexpressed phenomena again depend upon the end user: a psychologist, alexicographer, or a computer.
For example, much work on lexicalacquisition from dictionaries produces lexical entries in a format hat isclearly useful for lexicographers ( ee the many examples in Byrd (1987)and Boguraev (1989)), but their utility for the purpose of automaticlanguage processing is less evident.
In fact, the output of a lexicalanalyzer is an entry in which several semantic and syntactic fields areidentified, but many of these fields are raw text.In psychology and linguistics, semantic knowledge is modeled with very deep,more or less formal, expressions (see Figure 1 in Section 2).
Often semantic mod-els focus on some very specific aspect of language communication, according to thescientific interest of a scholar.In natural anguage processing (NLP hereafter), lexical entries typically expresslinguistic knowledge as commonsensically understood and used by humans.
The en-tries are entirely formatted in some knowledge representation language and can bemanipulated by a computer.
Within the field of NLP, many options are still possible,as summarized in Section 2.. Where?
What are the sources of lexical semantic knowledge??
Introspection is often a source of data, no matter what is thebackground of the scientist.
However, introspection posestheoretical nd implementation problems.
Theoretical, because"different researchers with different theories would observedifferent things about their internal thoughts..." (Anderson1989).
Implementation, because consistency becomes a majorproblem when the size of the lexicon exceeds a few hundredentries.?
Psycholinguistic experiments, uch as verbal protocols, are moreappropriate, because they produce observable and measurable1 This is not to say that here is a clear cut among the interests oflinguists, psychologists, and computerscientists.
We believe that, just o take one example, a poor knowledge ofthe results and methods oflexicography, linguistics, and cognitive science inpart motivates the nonstriking success of NLP indeveloping large-scale systems (Velardi 1990).154Velardi et al Encoding Semantic Knowledgeexample 1 from (Leech 1981):boy = +animate -adult  +maleexample 2 from (Mel~uk 1987):help =Y carrying out Z, X uses his resources W in order for W to helpY to carry out Z; the use of resources by X and the carrying out of Zby Y are simultaneousexample 3 from (Schank 1972):throw =actor PROPELs and object from a source LOCation to adestination LOCationFigure 1Examples of conceptual meaning representation in the literature.data.
One of the largest hand-encoded semantic lexicons(Dahlgren 1989 and Dahlgren, communication to AAAI90Stanford seminars) was built by asking subjects to freelistfeatures common to objects.
Despite the scientific interest ofsuch experiments, they cannot be extensively repeated for thepurpose of acquiring several thousand word sense definitions.On-line corpora and dictionaries are widely available today andprovide experimental evidence of word uses and worddefinitions.
The major advantage of on-line resources i that inprinciple they provide the basis for very large experiments, eventhough at present he methods of analysis are not fullydeveloped.
This paper describes an experiment in lexicalacquisition from corpora.How?
The final issue is implementation.
Implementation may be thoughtof at various levels.
It is the hard work of implementing a system in areal domain, or the more conceptual task of defining a mathematicalframework to manipulate the objects defined within a linguistic model.Quite obviously the "hows" in the literature are much more than the "wheres"and "whats'.
No classification of lexical semantic methods is attempted here for sakeof brevity.
In Section 5 we examine the research that is more closely related to theproblem under examination i this paper, i.e.
lexical acquisition for natural languageprocessing.2.
"What?
": Meaning Representation in a Semantic LexiconThis section further considers the issue: what knowledge can be encoded in a seman-tic lexicon?
We will not attempt an overall survey of the field of semantics, whichhas provided material for many fascinating books; rather, we will focus on the verypractical problem of representing language xpressions on a computer, in a way thatcan be useful for natural anguage processing applications, e.g.
machine translation,information retrieval, user-friendly interfaces.155Computational Linguistics Volume 17, Number 2In the field of NLP, several approaches have been adopted to represent semanticknowledge.
We are not concerned here with semantic: languages, which are relativelywell developed, but with meaning representation principles.A thorough classification of meaning types was attempted by Leech in his bookon semantics (Leech 1981).
In surveying the meaning representation styles adoptedin the computational linguistic literature, we found that many implemented naturallanguage processors adopt one of the following two meaning types for the lexicon:conceptual (or deep) and collocative (or superficial)...Conceptual meaning.
Conceptual meaning is the cognitive content ofwords; it can be expressed by features or by primitives: conceptualmeaning is "deep" in that it expresses phenomena that are deeplyembedded in language.Collocative meaning.
What is communicated through associations betweenwords or word classes.
Collocative meaning is "superficial" in that itdoes not seek "the real sense" of a word, but rather "describes" its usesin everyday language, or in some subworld language (economy,computers, etc.).
It provides more than a simple analysis ofco-occurrences, because it attempts an explanation of word associationsin terms of meaning relations between a lexical item and other items orclasses.The conceptual vs. collocative distinction in computational linguistics closely cor-responds to the defining vs. syncategorematic features distinction in psychology.Syncategorematic 2 concepts (Keil 1989) are those "almost entirely defined by theirpattern of use, and (the) others by almost pure belief."
The use of syncategorematicconcepts is supported by evidence from psycholinguistic studies.
Humans more nat-urally describe word senses with their characteristics and relations with other wordsthan with kindship and other internal features.
Not surprisingly, a similar "natural-ness" is observed in the examples of collocative meaning descriptions.
Examples howan evident similarity and are easily readable, whereas concept definitions in the con-ceptual framework are very different in different papers, and more obscure.
3Both conceptual (defining) and collocative (syncategorematic) features are formallyrepresented in the NLP literature using some subjective, human-produced set of prim-itives (conceptual dependencies, semantic relations, conceptual categories) on whichthere is no shared agreement at the current ime.
As far as conceptual meaning isconcerned, the quality and quantity of phenomena to be shown in a representationis subjective as well: the linguist relies mostly on his/her introspection.
Collocativemeaning can rely on the solid evidence represented by word associations; the inter-pretation of an association is subjective, but valid associations are an observable, venthough vast, phenomenon.In principle, the inferential power of collocative, or surface, meaning represen-tation is lower than for conceptual meaning, because it does not account for many2 Since the terms urface semantics, ollocative meaning, and syncategorematic features all refer to thevery fact hat concepts (in humans and computers) are frequently described by lists of characteristicsand usage types, we will use all the above terms interchangeably.3 Notice in the examples ofcollocative meaning descriptions that he "is-a" relation isconceptual, notcollocative, innature.
Kindship relations are very important in NLP because they provide the basis forinferences in semantic interpretation.
However, the classification f word senses intype hierarchies is aconceptually very complex, almost unsolvable problem.156Velardi et al Encoding Semantic Knowledgeimportant aspects of human communication, such as beliefs, preconditions, and knowl-edge of cause-effect relations.
These phenomena cannot be captured by an analysis ofmeaning relations between uttered words in a sentence.Despite this, collocative meaning has been shown to be a useful knowledge schemefor a number of computer applications in language processing.
In Niremburg (1987),the validity of this approach is demonstrated for TRANSLATOR, a system used formachine translation in the computer subworld.
A semantic knowledge framework inthe style of collocative meaning is also adopted in Ace (Jacobs 1987), which has beenused by the TRUMP language analyzer in a variety of applications.In our previous work on semantic knowledge representation (Pazienza 1988, Ve-lardi 1988, Antonacci 1989) we showed that a semantic dictionary in the style of col-locative meaning is a powerful basis for semantic interpretation.The knowledge power provided by the semantic lexicon (about 1000 manually en-tered definitions) was measured by the capability of the language processor DANTE toanswer a variety of questions concerning previously analyzed sentences (press agencyreleases on economics).
It was found that, even though the system was unable to per-form complex inferences, it could successfully answer more than 90% of the questions(Pazienza 1988).
4Representing word senses and sentences with surface semantics is hence useful(though not entirely sufficient) for many NLP applications.An additional and very important advantage of surface semantics i that it makes itfeasible to acquire large lexicons, as discussed in the following sections.
"Acquirability"in our view is extremely important to evaluate a knowledge representation framework.3.
"Where?
": Sources for Acquiring Lexical Semantic KnowledgeAcquiring semantic knowledge on a systematic basis is quite a complex task.
Oneneeds not to look at metaphors or idioms to find this; even the interpretation ofapparently simple sentences i riddled with such difficulties that it is difficult to isolatea piece of the problem.
A manual codification of the lexicon is a prohibitive task,regardless of the framework adopted for semantic knowledge representation; evenwhen a large team of knowledge nterers is available, consistency and completenessare a major problem.
We believe that automatic or semi-automatic a quisition of thelexicon is a critical factor in determining how widespread the use of natural anguageprocessors will be in the next few years.Recently a few methods were presented for computer-aided semantic knowledgeacquisition.
The majority of these methods use standard on-line dictionaries as a sourceof data.The information presented in a dictionary has in our view some intrinsic limita-tions:?
definitions are often circular; e.g., the definition of a term A may refer toa term B that in turn points to A;?
definitions are not homogeneous a far as the quality and quantity ofinformation provided: they can be very sketchy, or give detailed4 The test was performed over a six-month period on about 50 occasional visitors and staff members ofthe IBM Rome scientific enter, unaware of the system capabilities and structure.
The user would lookat 60 different releases, previously analyzed by the system (or re-analyzed during the demo), and freelyask questions about the content of these texts.
See the referenced papers for examples of sentences andof (answered and not answered) query types.157Computational Linguistics Volume 17, Number 2structural information, or list examples of use-types, or describe moreinternal features;a dictionary is the result of a conceptualization effort performed by somehuman specialist(s); this effort may not be consistent with, or suitable for,the objectives of an application for which a language processor is built.A second approach is using corpora rather than human-oriented dictionary entries.Corpora provide experimental evidence of word uses, word associations, and suchlanguage phenomena as metaphors, idioms, and metonyms.
Corpora are a genuine,"naive" example of language use, whereas dictionaries result from an effort of intro-spection performed by language xperts, i.e.
lexicographers.
Corpora are more inter-esting than dictionaries as a source of linguistic knowledge, just as tribes are moreinteresting than "civilized" communities in anthropology.The problem and at the same time the advantage of corpora is that they are rawtexts; dictionary entries use some formal notation that facilitates the task of linguisticdata processing.No computer program may ever be able to derive formatted ata from a com-pletely unformatted source.
Hence the ability to extract lexical semantic informationfrom a corpus depends upon a powerful set of mapping rules between phrasal patternsand human-produced semantic primitives and relations.
In machine learning, this isreferred to as the semantic bias.There is no evidence of innate conceptual primitives, apart from some very gen-eral ones (time, animacy, place, etc.
), and even on these there is no shared agreement.We must hence accept he intrinsic limitation of using a bias whose source is the intro-spection of a single, or of a community of scientists.
But even though the symbols wechoose are arbitrary, their role is the prediction of basic statements, i.e.
the processingof NL sentences in a way that is useful to some computational purpose, and shouldbe evaluated on this ground.4.
A Method for the Acquisition and Interpretation of a Semantic LexiconOur research on lexical acquisition from corpora started in 1988, when a first versionof the system was built as utility for the DANTE natural language processor (Velardi1989), a system that analyzes press agency releases on finance and economics.
Thecurrent version, described hereafter, is a self-contained tool on which we are runninga large experiment using a nationwide corpus of enterprise descriptions (overall, morethan one million descriptions).
The objective is to derive a domain-dependent semanticlexicon of 10,000 entries to be used for information retrieval in the sub-domain ofagricultural enterprises (Fasolo 1990).
The project is a cooperative ffort among theUniversities of Ancona and Roma II, and the CERVED, the company that owns andmanages the database of all commercial enterprises registered at the Chambers ofCommerce in Italy.In the current version, the system is able to acquire syncategorematic concepts,learning and interpreting patterns of use from text exemplars.
Generated lexical entriesare of the type shown in Figure 2 of Section 2.
We do not exclude the possibility ofacquiring defining features from dictionary entries and query collections at a laterstage of this project.
At present, however, we are interested in fully exploring thepower of collocative semantics for NLP.
We are also interested in the analysis of thelinguistic material that is being produced by the system.In what follows the methodology is described in detail.158Velardi et al Encoding Semantic Knowledgeexample 1 from (Velardi 1988)agreement =is_a decision_actparticipant person, organizationtheme transactioncause communication_exchangemanner interesting important effective ...example 2 from (Niremburg 1987):person =isa creatureagent_of take put find speech-action mental-actionconsist_of hand foot...source_of speech-actiondestination_of speech-actionpower humanspeed slowmass humanFigure 2Examples of collocative meaning representation i  the literature4.1 What Is GivenThe input to the system is:.
a list of syntactic ollocates, e.g.
subject-verb, verb-object,noun-preposition-noun, noun-adjective, tc.
extracted throughmorphologic (Russo 1987) and syntactic analysis of the selected corpus.The level at which syntactic analysis should be performed to derivecollocates is a matter of debate.
In Smadja (1989) it is suggested thatparsing can be avoided by simply examining the neighborhood of aword w at a distance of +5.
Our experience demonstrates that thisalgorithm produces too many collocations, of which a minority areactually semantically related.At the other extreme is full syntactic parsing, as performed in Velardi(1989).
This is computationally too expensive for large corpora and failsto produce useful collocations when sentences are not fully grammatical,as for example in the agricultural businesses database.
A typical text inthis corpus is:vendita al minuto di legno da costruzione  manufatti in abeteproduzione mobili di legno e di metallo(*) retail sale of wood for construction and hand-manufactured infir-tree production furniture of wood and of metalA better trade-off between speed and accuracy is to enable syntacticparsing of sentence parts and use some context-dependent heuristics tocut sentences into clauses (Fasolo 1990).
However, the +5 collocations arealso collected for a reason that will be clarified later on.159Computational Linguistics Volume 17, Number 2.
A semantic bias.
The semantic bias is the kernel of any learning algorithm,as no system can learn much more than what it already knows (Micalski1983).
This consists of:(a) a domain-dependent concept hierarchy.
This is a many-to-manymapping from words to word sense names and an ordered list ofconceptual categories.The hypothesis of hand-entering a type hierarchy would notrequire an unreasonable amount of time, because the task iscomparable to entering a morphologic lexicon (Russo 1987).
Theproblem is rather a conceptual one.
The way humans categorizeconcepts in classes is far from being understood.
Mere propertyinheritance seems to be inadequate at fully modelingcategorization in humans (Lakoff 1987; Rosch 1975), and thisvery fact discouraged us from attempting some automatichierarchy acquisition in the framework of machine learning(Gennari 1989).
The limitations of current machine learningapproaches when applied to language learning are discussed inSection 5.We consider the problem of acquiring type hierachies as anopen issue, to which more thought and more research are beingdevoted in our current work.
(b) a set of domain-dependent conceptual relations, and amany-to-many mapping (synt-sem) between syntactic relationsand the corresponding conceptual relations (see Velardi 1988and Antonacci 1989 for extensive xamples ofsyntax-to-semantics mapping);(c) a set of coarse-grained selectional restrictions on the use ofconceptual relations, represented by concept-relation-concept(CRC) triples.
CRCs are expressed in Conceptual Graph notation(Sowa 1984).4.2 The OutputThe system produces two types of output:1. a set of fine-grained CRCs, that are clustered around concepts or aroundconceptual relations;2. an average-grained semantic knowledge base, organized in CRC triples.The semantic knowledge base is acquired from a source sub-corpus and is used, beforea final approval, for the semantic interpretation f sentences in a test-bed sub-corpus.The semantic interpreter is basically that described in Velardi (1988) and in otherpapers, to which the interested reader may refer.Such terms as 'fine,' average,' and 'coarse' are obviously fuzzy.
To the extent hismakes sense, we ranged the above terms as follows:fine-grained CRC are those in which concepts directly map into contentwords (e.g.
\[COW\] ~- (PATIENT) , -  \[BREED\]).
These CRCs are truebecause they are observed in the domain subworld.160Velardi et al Encoding Semantic Knowledgeii) average-grained CRC are those in which concepts are fathers orgrand-fathers of content-word concepts.
These CRC are 'typically' true,as they may have a limited number of exceptions observed in thedomain sublanguage ( .g.
\[ANIMAL\] ~-- (PATIENT) ~-- \[BREED\] istypically true, even though breeding mosquitoes i quite odd).iii) coarse-grained CRC are those in which concepts are at a higher level inthe taxonomy (e.g.
\[ACTION\] --~ (BENEFICIARY) -~\[ANIMATE_ENTITY\]).
They state necessary, but not sufficient, conditionson the use of conceptual relations.The notion of "high-level" and "low-level" in a taxonomy is also relative to the ap-plication domain.
For example, in a computer world, the concept COMPUTER_SOFT-WARE may be rather high-level.4.3 Learning Syncategorematic Concepts from Text ExemplarsTo acquire syncategorematic knowledge on concepts, the algorithm proceeds as fol-lows: For any syntactic ollocate sc(wl,w2):1.
Restrict he set of conceptual relations that could correspond to thesyntactic ollocate using the synt-sem table.2.
Use coarse-grained knowledge and taxonomic knowledge to furtherrestrict he hypotheses.3.
If no interpretation is found, reject the collocate.
If one or moreinterpretations is found, put the resulting CRC(s) on a temporaryknowledge base of fine-grained knowledge.4.
Generalize the result by replacing the concepts in the CRC with theirclosest supertypes, using the structural overcommitment principle(Webster 1989).
Add the result to a temporary knowledge base ofaverage-grained knowledge.5.
Repeat steps 1-4 for all the collocates of the same syntactic type, or (userchoice) those including the same word W. Further generalize one step upin the hierarchy, based on at least three examples.6.
Present he results to the linguist for a final approval, then add to thepermanent knowledge base.In a first, training phase, the linguist is requested to inspect he system output instep 3, to verify and refine the semantic bias.4.4 A Complete ExampleFigure 3 shows the output of step 3.
On the left-hand side of window 1 (ok-sema)and window 2 (no-sema) flow the syntactic ollocates acquired during the syntacticanalysis of the corpus.
Syntactic ollocates are couples, like adj-noun, noun-verb, verb-noun, or triples, like verb-prep-noun (G_V__Pd'q), noun-prep-noun (G_N_P_N) etc.
InFigure 3 the triples are shown with the preposition "in".For each collocate, the learner accesses the knowledge provided by the semanticbias and generates a CRC triple, corresponding to a plausible semantic interpretation161Computational Linguistics Volume 17, Number 2= File Edit Search Windows Fonts Eval Hermes KB DRL LearnOk_semaG N)N - eo l~ i~re~z ione  in $er r~ = co l~ iw~e luo9o  s t r r~ ed i f le ioG H_~_H - e .
l~ iwre+Ziont  in  ser r~ : eo lz iwore  l~ogo  s t r r~_ed i f ie ioG N~H - ?
io~e in s?~r~ : f i o re  luo9o  s t* r .
?4 i f l c loG._H..~_H -- ~llew~e41~e~to in ~e<lu~ = ~l le~re  luogo ~e~u~G H_~_H - f~gi~lo in se~ol~ = f~giolo inelusione ~e~ol~G.,_H_~_N - m~nuf~t, in ~be~e = tn~nuf~:~o ~m~e~i~ ebez?G pp_)  H - Sur?
i I~re  in  scate l~ = lu rge la re  luogo  seazo laG V~H - uxil izz~re in ~ziend~ = .~ i l i xz~e luogo ~zi~nJ~_?~ifieioO V_~N - ut i l l zz~re in ~zlen~ = ut i l i zz~r?
luogo .zicnd~ ~dlf icio?Y JN  - $urgel~e in $c~zol~ = $urgel~?
luogo $c~zol.O V_~_H - surgel~re in se~ol~ = surgel.re ~zo J in~le  sc~Zol~No_sema K8 FiltroG N_I_N : f~angi~u~to in fo r~ = f l~ng i tu t to_b~iv i~a I fo rm ( I )G N_~ H : f*~ngi%u~zo i n  form~ = f r~ngi~utto_ .~cchin~ I form~ ( 1 )G R~_H : eommzre i~r t  in  fozm~ = ?ommerc i~x .
/ f .
r ,~  ( 1 )G N_~_H : ?
r~no in  negQx lo  = 9r~no / n tgoz lo  ~ iv i~ ( 1 )O H_~N : 9 r~no in  n?9 .z io  = 9r~no / negox io  ed l f i~ io  ( 1 )G R I_H : l~ro  in  fe~yo = l~voro_~t~iv i~ / ?err6 ( 1 }G_V_I_H : p ro<lur re  in  9ene ,e  = pro~urre  / 9. ,e re  ( 3 \]G V I _N  : uz i l l zz~re  in  ~z icnd~ = u~i l t zxore  / mx ien~ ~iy i~ ( 1 )G Y ~ H : .~ i l i zz~r?
i~  ~z ien~a = u~i l i zz~re  / ~z iend~_org~nizz tz ione  ( 1 )G-V~}-H u%i l t+ i  .
.
.
.
.
in ~zlen& : u t i l e  /" ~zlen~ ~t iv i~ ( 1 )O V l _H  : u~i le+izx~re  in  ~z ien~ = u~i le  / ~z lenda  ed i f le io  ( 1 )G_V~N : u~i le+izz~re  in  ~z iend~ = ut i l t  / ~z iend~ ox~nizz~=i~ne ( 1 )I~ modo "gestire a ~\ ]noleggio" ~I I If modo (' a' , IIC svolgere attivita: : , IIc oggettO economico::-:: ).1 I- 1" modo "commercio-al-% modot6b_s_s_o('G_V_P_N',A,'a',B, liiiilmodo,A,B).
~ I% destinazione_beneficiarioI~ jFigure 3Production of CRC triples from syntactic collocatesof the syntactic collocate (shown to the right of the '=' in the window ok-sema).
Ifan interpretation is not found, because the syntactic attachment is not semanticallyvalidated, the collocate is shown in the window no-sema, i.e., it is rejected.
5An example may clarify the steps:..acquired syntactic collocate: "coltivazione in serra" (farming ingreenhouse)possible subsumed conceptual relations (in Italian and in the specificdomain) by the noun-"in'-noun collocation:luogo (location) (ex.
: farming in greenhouse)stato_finale (final state) (ex.
: trasformazione in vino = transformation inwine)materia (matter) (ex.
: lavoro in ferro = craft in iron)luogo_figurato (figurative location) (ex.
: intervento in settore = interventionin field)etc.5 Window "KB Filtro" shows an excerpt of coarse-grained knowledge of conceptual relations and thewindow "sint_sema Filtro" is an excerpt of the synt-sem table.162Velardi et al Encoding Semantic Knowledge...coarse-grained knowledge used to select he most plausibleinterpretation(s), expressed in Conceptual Graph notation:1.
\[ACTIVITY\] ~ (LOCATION) ~ \[PLACE\]2.
\[CHANGE\] ~ (FINAL_STATE)~\[PRODUCT\]3.
\[ARTIFACT\]--~ (MATTER)---+ \[MATTER\]4.
\[ACTIVITY\]--+(FIGURATIVE_LOCATION)-* \[AMBIT\]generated fine-grained CRC (applying rule 1):\[FARMING\] ~ (LOCATION)--*\[GREENHOUSE\]generated average-grained CRC:\[AGRICULTURAL_ACTIVITY\]--+ (LOCATION) ~ \[BUILDING _FOR_CULTIVA-TION\]When the algorithm runs the first several times, the linguist user inspects the fine-grained output, as shown in Figure 3, to verify a correct partition of the collocatesamong the two windows, and a correct interpretation of the semantically plausiblesyntactic ollocates.
Errors are used to refine the bias.
The bias is now stable, at leastfor what concerns the 'known' words (about 5000 root-form words).
In our domain,this took two or three round steps through the algorithm (i.e.
run the algorithm,verify, and correct he bias), for each type of syntactic ollocation.
We believe that the(relatively) low semantic ambiguity of the domain sub-language and the availabilityof a well-defined set of conceptual relations contributed to the result.After this first system training phase, the linguist only overviews the averagegrained CRCs, which must be tested before final acquisition in the knowledge base.Whether and how human intervention can still be reduced is unclear at the presentstage of the experiment.
Figure 4 shows a generalization session.
Given one or more ex-amples, such as allevamento di pesce (fish breeding) that are interpreted by the PATIENT(=animate direct object) relation, the system proposes to acquire the rule (shown witha reverse video in Figure 4):Rule 1\[BREED\]-+ (PTNT)-,\[ANIMAL_FOR_FEEDING\]The user can acquire the rule by clicking on the "Acquisisce" button, or test the rulebefore approval.
The test is performed by showing the possible implications of thatrule.
These are obtained by listing all low-level CRCs with non-zero probability ofoccurrence in the corpus, 6shown in the lower window of Figure 4.In case of exceptions, it is the choice of the linguist to reject the rule (button "Ri-fiuta"), or to explicitly account for the exception(s) in a negative-example knowledgebase, by marking with an "n" the triples that are not semantically correct.
The ruleshown in Figure 4 above generates only correct associations.
The subtypes of BREED,e.g.
DRAIN and DRILL, would in principle produce odd, if not totally unreasonable,associations, uch as "fish training."
Such associations, however, are not listed in thelower window of Figure 4 because the words "fish" and "drill" never occur in thecorpus at a distance of +5.6 For very large corpora, as the one used here, producing the complete list of possible implicationswould generate insome case hundreds of examples, especially when both concepts ina CRC triple arenot terminal nodes in the hierarchy.
The linguist is therefore presented only with the list of collocatesthat have a non-zero probability of occurrence.
This list is the list of all the collocations found in the(sub-)corpus ing the -4-5 algorithm of Smadja (1989).163Computational Linguistics Volume 17, Number 2~_ File Edit Search Windows Fonts Eval Hermes KB DRL LearnGruppo sintatticoG_N_P_N = allevare+mento di pesceRelazione da acquisire(al levare)->\[paziente\]->(c_animale_alimentazione)GeneralizzazioniGeneralizzay - a l le~re  paz iente  pesce.y - a l levare  paz iente  suino.y - a l levare  pa~iente  capr ino.y - a l levare  paz iente  bovino.y - a l le~re  paz iente  ovino.y - a l levare  paz iente  caval lo .y - a l levare  paz iente  conig l io .y - a l levare  paz iente  maia le .y - a l levare  paz iente  ga l l ina .y - a l levare  paz iente  pol lo.y - a l levare  paz iente  magrone.Figure 4Example rule generalization session with the computer system4.5 DiscussionThis section describes the major problems and results of the acquisition algorithm.4.5.1 Definition of the Semantic Bias.
The most difficult task, both in the domaindescribed in this paper and in the press agency releases domain described in Velardi(1989), is to define at the appropriate l vel of generality the selectional restrictions onconceptual relations.
If restrictions are very coarse, they give rise to errors or (morefrequently) to multiple interpretations at the end of step 2 in Section 4.3.
If they arevery selective, we get back to the case of a hand-encoded semantic lexicon.In the first application (economy and finance) we used about 50 conceptual rela-tions.
The language domain was quite rich, and often it was not possible to express aselectional restriction with a single CRC triple; on the average, 1.5 CRCs per relationwere necessary.For example, the following are the selectional restrictions for the relation PARTIC-IPANT:\[HUMAN _ENTITY\],-- (PARTICIPANT),-- \[FLYI,\[SAILI,\[MEETINGI,\[AGREEMENT\]Examples of phrasal patterns interpreted by this relation are: John flies (to New York);an agreement between Fiat and Nissan; a contract among the companies, the assemblyof the administrators, etc.
(In the third example, notice that the word "contract" wasclassified in the economy domain both as an AGREEMENT and as a DOCUMENT.
)164Velardi et al Encoding Semantic KnowledgeAs for the agricultural enterprises, about 20 conceptual relations could provideenough expressive power to the semantic representation language.
Most relations weredefined by a single, coarse-grained CRC.
Only a few required more detailed selectionalrestrictions, as for example the PART_OF relation:\[ANIMATE\]--* (PART_OF)--* \[BODY_PART\]\[VEGETABLE \]--* (PART_OF) ~ \[VEGETABLE _PART\]\[CONCRETE _OBJ\] --* (PART _OF)--* \[CONCRETE _OBJ\]The major difficulty here is that a conceptual category is not always available to expressa selectional restriction in a compact form.
For example, the third restriction above isintended to capture patterns uch as: "the pages of the book; the engine of the car; thetop of the hill, etc."
To express the relations among word couples such as (top, hill),(engine, car), (page,book) etc., a better CRC is:\[CONCRETE_OBJ\]--~(PART_OF)--*\[OBJECT_PART\]However, the type hierarchy defined for the agricultural domain does not have a classnamed OBJECT_PART, simply because this was not found useful, given the subworldlexicon.
Hence, the third restriction looks very coarse, but turned out to be selectiveenough for the sub-language.
Errors are of course possible, but remember that: first,the output of the acquisition process is supervised by a linguist for final approval (step6 of Section 4.3); second, coarse-grained knowledge is used only for acquisition, notduring the semantic interpretation of sentences.
In other words, coarse knowledge isonly a bias from which a more refined semantic knowledge is acquired.4.5.2 Semantic Ambiguity.
A second issue is semantic ambiguity.
The system inter-prets word associations outside a context.
This may give rise to several interpreta-tions for a single syntactic ollocate, even though the domain has very little lexicalambiguity.
7A good example of what could happen in a more general domain is the fol-lowing: Consider the phrase " .
.
.
run  towards the bank.
.
.
,"  that gives rise to theV_towards_N triple "run, towards,bank."
Without a context, two CRCs are created:\[RUN\]~(DIR)-*\[RIVER_BANK\] and \[RUN\]-*(DIR)--*\[BANK_BUILDING\].
The otherinterpretations, such as BANK_ORGANIZATION and BANK_ACTIVITY are rejectedbecause their association in a "V_towards_N" position gives no plausible interpreta-tion, if the semantic bias is "smart" enough.
Now, the very fact that in the sentencefrom which the collocate was extracted "bank" was a river bank rather than a build-ing simply does not matter.
We are learning more than what the sentence was sug-gesting.
In fact, we acquire two use types of the concepts RUN, RIVER_BANK andBANK_ACTIVITY (namely, that it is possible to run in the DIRection where a "*bank"is located), that are perfectly correct and can be used for semantic disambiguation.When a syntactic ollocate is interpreted in a context, as during the semantic analysisof a sentence, the interpretation algorithm makes it possible to consider simultaneously7 In fact, the word-to-concept table that maps words into concept names has fewer entries than themorphologic lexicon.
For example, words that end with "zione" (tion) have the same concept type asthe correspondent verb, e.g.
production and produce.
Other more complex examples of word toconcept mappings are not mentioned for brevity.
Ambiguous words are mostly those designating bothan activity and a building where the activity takes place, as detailed in an example later on.
But this istaken care of with a single metonymic rule (Lakoff 1987), rather than replicating the entries.165Computational Linguistics Volume 17, Number 2several restrictions (Velardi 1988); for example, the direct-indirect object relations, etc.It is in sentence interpretation that a full disambiguation is necessary, though notalways possible.4.5.3 Syntactic Disambiguation.
The semantic knowledge base (SKB) acquired by oursystem is a large set of selectional restrictions on word uses, expressed by CRCs, thatcan be ordered either around concepts or around conceptual relations.
The represen-tation language is Conceptual Graphs, because we believe that this formalism hasseveral advantages (Velardi 1988).
However, standard logic could also be adopted.The majority of implemented NLP systems use selectional restrictions for syntacticand semantic disambiguation i  a more or less standard way.
We claim hence that theapplicability of the algorithm presented in this paper does not depend upon the specificalgorithm used for semantic interpretation, and the method can be adapted with minorchanges to many NLP systems.For the sake of completeness, we provide hereafter a brief summary of the semanticalgorithm used in the DANTE system and in a system for information retrieval ofagricultural businesses descriptions, focusing on the important problem of syntacticambiguity.
Details are given in the referenced papers.The semantic interpreter proceeds bottom-up, in parallel with syntactic analysis.
Atthe lower level nodes of the tree, it verifies whether it is possible to find the appropriateconcepts and relations that interpret a given syntactic relation between words.
Whileprogressing up toward the root of the tree, it replaces yntactic relations betweenphrases by conceptual relations between partial Conceptual Graphs.
It does this byfollowing steps 1 and 2 of the acquisition algorithm (Section 4.3), but in step 2 ratherthan using selectional restrictions on conceptual relations (coarse-grained knowledge)it accesses the SKB.
If at some point no interpretation is found, the system backtracksand selects a different syntactic attachment.
For example, consider the following threesentences:1. produrre vino in bottiglia (*to produce wine in bottle)2. vendere uva alHngrosso (*to sell grapes at wholesale)3. produrre vino per i soci (*to supply wine for the shareholders).All the above three phrases give rise to syntactic ambiguity, and precisely: I:((V-N1)-prep-N2) or 2:(V-(Nl-prep-N2)).
For sentence I and tree 1, the interpreter first generatesthe graph: \[PRODUCE\]~(OBJ)~\[WINE\].
Then, a join is attempted between the headof the graph, \[PRODUCE\], with the rest of the sentence (in bottle).
The preposition"in" corresponds to several conceptual relations (see the example in Section 4.4), butthe selectional restrictions on concept uses available in the SKB do not suggest anyvalid interpretation.
As no complete Conceptual Graph can be produced for tree 1,tree 2 is explored.
Tree 2 generates first the graph:\[WINE\]--* (LOCATION) ~ \[BOTTLE\]where the head concept is \[WINE\].
Then, a join is attempted between \[PRODUCE\]and \[WINE\].
This produces the final graph:\[PRODUCE\]--,(OBJ)~\[WINE\]--,(LOCATION)--*\[BOTTLE\].166Velardi et al Encoding Semantic KnowledgeThrough a similar process, tree 1 is selected for sentence 2, that gives the graph:\[SALE\] --* (OBJ)-,\[GRAPE\]--* (MANNER)--*\[WHOLESALE\]In sentence 3, both trees are indeed plausible: the shareholders are the DESTI-NATION both of the wine and of the supply.
In this case, for information retrievalpurposes, it really does not matter which solution is preferred.4.5.4 A First Evaluation.
After some training and changes due to refinements in bias,the system was used to acquire the SKB for a prototype system for semantic odifi-cation of a test-bed set of agricultural texts, different from the one used for definingand testing the bias.
The NLP system is described in Fasolo (1990) and is similar tothe one described in Velardi (1988) and Antonacci (1989) and in the other papers onthe DANTE system, except for the use of shallow methods in syntax and the abilityto produce partial interpretations if parts of a text are not understood.Many parts of this NLP system are still under development, but for what con-cerns the adequacy of the semantic knowledge base, the results are very encouraging.Currently, the test runs on about 3000 collocates, but these numbers keep changing.We plan to produce more accurate statistics after one full-year experimentation.
Thefollowing is a very partial discussion of the results, useful for pinpointing the majorproblems.1.
About 10% of the syntactic ollocates are rejected because one or boththe words were unknown (no morphologic entry).
Some of these aretypos or abbreviations, and the others are really unknown.
A majoreffort is being devoted toward an extension of the morphologic lexiconand the introduction of error correction algorithms.
Recovery proceduressuch as the use of semantic information attached to word endings (e.g.
"zione"="tion" always indicate an action, "x-ficio" is always a buildingfor making some product x, etc.)
can also be introduced.2.
Only about 1-2% correct syntactic ollocates produce two interpretationswhere one is incorrect in whatever context.
This is to some extentunavoidable, because the selectional restrictions on conceptual relationshave obviously many exceptions, even though the probability of actuallyencountering such exceptions i low.
It is unclear to what extent hepresence of errors in the semantic knowledge base can induce errors insentence interpretation, when multiple constraints are analyzed together.Currently, errors are purged by the linguist before final acquisition.3.
No incorrect syntactic ollocates are erroneously given an interpretation.4.
Some collocates were rejected because they included pronouns.Anaphoric references are not handled in the current version of the NLPsystem.5.
One correct collocate was erroneously rejected showing the need for anew conceptual relation, FIGURATIVE_LOCATION (see Section 4.4).There is no doubt that in order to fully validate this experiment we need to analyzethousands of texts, not hundreds.167Computational Linguistics Volume 17, Number 2This requires: first, that the morphologic lexicon be extended; second, that theNLP system that uses the acquired semantic knowledge be completed in several parts,primarily the question-answering module; third, and more importantly, that severallinguistic issues be given more thorough solutions, in particular the definition andacquisition of conceptual categories.5.
Related ResearchThe method presented in this paper is at the frontier between machine learning andcomputational linguistics.
It is closely related to two research areas: concept formationand lexical acquisition.The similarities and differences of the problem discussed in this paper with that ofconcept formation are better understood by comparing our objectives with the formaltasks of concept formation (Fisher 1985; Gennari 1989):1.
Given: a sequential presentation of instances and their associateddescriptions;2.
Find: a clustering of those instances in categories;3.
Find: an intensional definition for each category;4.
Find: a hierarchical organization for those categories.Unlike the work on concept formation, instances (words in contexts) are not associatedwith descriptions (CRC triples), hence (1) is only in part a given.
Conversely, (2) and(4) are a "given" rather than a "find."
In summary:1.
Given: a sequential presentation of word associations;2.
Given: a many to many mapping from words to concept ypes;3.
Given: a hierarchical ordering of concept ypes;4.
Find: for each observation (two associated words), the concept ypes andthe conceptual relation that interpret hat observation (CRC);5.
Find: a definition for each concept hat summarizes its instances (derivedCRCs).The more substantial difference is that the typical application dealt with in conceptformation is clustering natural kinds (animals, biological species, etc.
), whereas most ofthe terms we deal with are nominal kinds (verbs, etc.)
and artifacts.
Natural kinds aremore easily described in terms of defining (internal) features, and their origin seemmore basic to their "kindhood" (Keil 1989) than does the origin of nominal kinds.The latter are better described by their external relations with other objects.
In otherwords, finding a type hierarchy is less basic but much more difficult for nominal kindsand artifacts than for natural kinds.
This justifies the focus given to the derivation ofconcept descriptions rather than to conceptual clustering, even though we are awarethat the hand-entering of the type hierarchy is the major limitation of our algorithm.A second research area related to our work is lexical acquisition.
An up-to-datesurvey of the most recent papers in this area is found in Computational Linguistics(1987) and Zernik (forthcoming).
Many of the papers collected in the above two issuesare more relevant o the fields of lexicography and cognition than to NLP.
One of the168Velardi et al Encoding Semantic Knowledgefew lexical knowledge acquisition systems for NLP is described in Jacobs (1988) andZernik (1989).
When an unknown word is encountered, the system uses pre-existingknowledge of the context in which the word occurred to derive its conceptual category.The context is provided by on-line texts in the economic domain.
For example, theunknown word merger in "another merger offer" is categorized as merger-transactionusing semantic knowledge of the word offer and of pre-analyzed sentences referring toa previous offer event, as suggested by the word another.
This method is interesting butallows a conceptual typing of unknown words only when everything else is known.
Itdoes not attack the problem of extensive lexical acquisition, but rather that of robustlanguage processing.In Binot (1987) prepositional attachments found in dictionary definitions are in-terpreted in terms of conceptual relations (e.g.
WITH=INSTRUMENT) and used tosolve syntactic ambiguity in parsing.
The problem is that the information ecessaryto disambiguate is often not found, or requires everal complex searches through thedictionary because of circularity and cross-references.In Smadja (1989) the system EXTRACT, which uses shallow methods to extensivelyderive collocates from corpora, is described.
The system produces a list of tuples(wl,w2,f), where wl and w2 are two co-occurring words and f is the frequency ofappearance in the corpus.
No semantic interpretation is attempted, but it is claimedthat mere co-occurrence knowledge can help language generators to correctly handlecollocationally restricted sentences.6.
Concluding RemarksThe algorithm presented in this paper automatically detects co-occurrences in contextsand provides a semantic interpretation of the meaning relation between co-occurringwords.
Interpreted co-occurrences are used to build a semantic lexicon based on col-locative meaning descriptions.
The acquired concepts are syncategorematic; e.g., arecompletely defined by their pattern of use.
It is assumed that such knowledge is suffi-cient to produce a surface semantic interpretation of raw text, i.e.
a Conceptual Graphwhere content words and syntactic relations are replaced by the appropriate concepttypes and conceptual relations.
This assumption i deed proved reasonable in our pre-vious work on semantic interpretation, where the same type of semantic knowledgewas hand-entered.The observation of co-occurrences is language-dependent, context-dependent; theinterpretation algorithm is (to some extent) language-independent, but relies uponhuman-derived primitives and relations that are in general context-dependent.
Nolanguage model can prove to be objective, or even plausible.
In principle, languagerules and primitives do not exist.
But even though symbols are arbitrary, their role isnot to mimic human comprehension, but rather to produce some formal description ofraw textual input, in a form that is ultimately useful for some relevant NLP application.We feel that no human-invented semantic language will ever provide a full inter-pretation of language phenomena.
We also strongly believe that more shallow methodssuch as the one discussed in this paper must be devised to give current NLP systemsmore breadth, as this will ultimately determine how widespread the use of NLP tech-nology will be in the near future.AcknowledgmentsThis work has been in part supported bythe European Community, under grantPRO-ART 1989 and 1990, and in part by theCERVED.169Computational Linguistics Volume 17, Number 2ReferencesAnderson, J. R. (1989).
A Theory of theOrigins of Human Knowledge.
ArtificialIntelligence.
vol.
31-40, September.Antonacci, Pazienza, and Russo, Velardi(1989).
A System for Text Analysis andLexical Knowledge Acquisition.
Data andKnowledge Engineering.
n 4.Binot, J. L., and Jensen, K. (1987).
ASemantic Expert Using an On-lineStandard Dictionary.
Proceedings oftheIJCAI.
Milano.Boguraev, B.; Byrd, R.; Klavans, J.; and Neff,M.
(1989).
From Machine ReadableDictionaries to Lexical Databases.
FirstLexical Acquisition Workshop, Zernik ed.,Detroit.Byrd, R.; Calzolari, N.; Chodorow, M.;Klavans, J.; Neff, M.; Rizk, O.
(1987).Tools and Methods for ComputationalLexicography.
Computational Linguistics.Computational Linguistics pecial issue on theLexicon, Walker, D., Zampolli, A., andCalzolari, N., eds.
July-December, 1987.Dahlgren, K., and McDowell, J.
(1989).Knowledge Representation forCommonsense Reasoning with Texts.Computational Linguistics, vol.
15, n. 3,September.Evens, M.
(ed.)
(1988).
Relational Models ofthe Lexicon.
Cambridge University Press.Fasolo, M.; Garbuio, L.; and Guarino, N.(1990).
Comprensione di descrizioni diattivita' economico-produttive espresse inlinguaggio naturale.
GULP Conferenceon Logic Programming.
Padova.Fisher, D., and Langley, P. (1986).Conceptual Clustering and Its Relation toNumerical Taxonomy.
Artificial Intelligenceand Statistics.
Addison-Wesley.Gennari, J.; Langley, P.; and Fisher, D.(1989).
Model of Incremental ConceptFormation.
Artificial Intelligence,vol.
31-40, September.Jacobs, P. (1987).
A Knowledge Frameworkfor Natural Language Analysis, InProceedings of IJCAI87.
Milano, August.Jacobs, P. (1988).
Making Sense of LexicalAcquisition, In Proceedings ofAAAI88.St.
Paul, August.Keil, F. (1989).
Concepts, Kinds andCognitive Development, In Data andKnowledge Engineering.
The MIT Press.Lakoff, G. (1987).
Woman, Fire and DangerousThings.
The University of Chicago Press.Leech, Geoffrey.
(1981).
Semantics: The Studyof Meaning, Second edition, PenguinBooks.McKeown, K. (1985).
Text Generation.Cambridge University Press.Michalski, R. S.; Carbonell, J. C.; andMitchell, T. M. (1983).
Machine LearningVol I. Tioga Publishing Company, PaloAlto.Mel~uk, I., and Polguere, A.
(1987).
AFormal Lexicon in Meaning-Text Theory(or How To Do Lexica with Words).Computational Linguistics 13(3--4): 261-275.Niremburg, S., and Raskin, V. (1987).
TheSubworld Concept Lexicon and theLexicon Management System.Computational Linguistics.Pazienza, M. T., and Velardi, P. (1988).Using a Semantic Knowledge Base toSupport A Natural Language Interface toa Text Database.
7th InternationalConference on Entity-Relationship A proach,Rome, November 16-18.Rosch, E. (1975).
Cognitive Representationof Semantic Categories.
Journal ofExperimental Psychology 104, 192-233.Russo, M. (1987).
A Generative GrammarApproach for the Morphologic andMorphosyntactic Analysis of Italian, InThird Conference ofthe European Chapter ofthe ACL, Copenhagen, April 1-3.Schank, R. C. (1972).
ConceptualDependency: A Theory of NaturalLanguage Understanding, In CognitivePsychology, vol.
3.Sowa, J. E (1984).
Conceptual Structures inMind and Machine.
Addison-Wesley.Smadja, E (1989).
Lexical Co-occurrence:The Missing Link, In Literary andLinguistic Computing, Vol.
4, n. 3.Velardi, P.; Pazienza, M. T.; and DeGiovanetti, M. (1988).
Conceptual Graphsfor the Analysis and Generation ofSentences, In IBM Journal of R&D, specialissue on language processing, March.Velardi, P., and Pazienza, M. T. (1989).Computer-Aided Acquisition of LexicalCooccurrences, In Proceedings ofthe ACL1989.
Vancouver.Velardi, P. (1990).
Why Human TranslatorsStill Sleep in Peace?
(Four Engineeringand Linguistic Gaps in NLP), In Proc.
ofCOLING 90.
Helsinki, August.Webster, M., and Marcus, M. (1989).Automatic Acquisition of the LexicalSemantics of Verbs from Sentence Frames.In Proceedings ofthe ACL 1989.
Vancouver.Zernik, U.
(1989).
Lexicon Acquisition:Learning from Corpus by Capitalizing onLexical Categories, In IJCAI 1989, Detroit.Zernik, U., ed.
(in press) Lexical Acquisition:Using On-line Resources to Build a Lexicon,Lawrence Erlbaum.170
