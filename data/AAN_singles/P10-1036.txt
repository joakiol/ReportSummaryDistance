Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345?355,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsFaster Parsing by Supertagger AdaptationJonathan K. Kummerfelda Jessika Roesner b Tim Dawborna James HaggertyaJames R. Currana?
Stephen Clark c?School of Information Technologiesa Department of Computer Scienceb Computer LaboratorycUniversity of Sydney University of Texas at Austin University of CambridgeNSW 2006, Australia Austin, TX, USA Cambridge CB3 0FD, UKjames@it.usyd.edu.aua?
stephen.clark@cl.cam.ac.uk c?AbstractWe propose a novel self-training methodfor a parser which uses a lexicalised gram-mar and supertagger, focusing on increas-ing the speed of the parser rather thanits accuracy.
The idea is to train the su-pertagger on large amounts of parser out-put, so that the supertagger can learn tosupply the supertags that the parser willeventually choose as part of the highest-scoring derivation.
Since the supertag-ger supplies fewer supertags overall, theparsing speed is increased.
We demon-strate the effectiveness of the method us-ing a CCG supertagger and parser, obtain-ing significant speed increases on newspa-per text with no loss in accuracy.
We alsoshow that the method can be used to adaptthe CCG parser to new domains, obtain-ing accuracy and speed improvements forWikipedia and biomedical text.1 IntroductionIn many NLP tasks and applications, e.g.
distribu-tional similarity (Curran, 2004) and question an-swering (Dumais et al, 2002), large volumes oftext and detailed syntactic information are bothcritical for high performance.
To avoid a trade-off between these two, we need to increase parsingspeed, but without losing accuracy.Parsing with lexicalised grammar formalisms,such as Lexicalised Tree Adjoining Grammar andCombinatory Categorial Grammar (CCG; Steed-man, 2000), can be made more efficient using asupertagger.
Bangalore and Joshi (1999) call su-pertagging almost parsing because of the signifi-cant reduction in ambiguity which occurs once thesupertags have been assigned.In this paper, we focus on the CCG parser andsupertagger described in Clark and Curran (2007).Since the CCG lexical category set used by the su-pertagger is much larger than the Penn TreebankPOS tag set, the accuracy of supertagging is muchlower than POS tagging; hence the CCG supertag-ger assigns multiple supertags1 to a word, whenthe local context does not provide enough infor-mation to decide on the correct supertag.The supertagger feeds lexical categories to theparser, and the two interact, sometimes using mul-tiple passes over a sentence.
If a spanning analy-sis cannot be found by the parser, the number oflexical categories supplied by the supertagger isincreased.
The supertagger-parser interaction in-fluences speed in two ways: first, the larger thelexical ambiguity, the more derivations the parsermust consider; second, each further pass is ascostly as parsing a whole extra sentence.Our goal is to increase parsing speed withoutloss of accuracy.
The technique we use is a formof self-training, in which the output of the parser isused to train the supertagger component.
The ex-isting literature on self-training reports mixed re-sults.
Clark et al (2003) were unable to improvethe accuracy of POS tagging using self-training.In contrast, McClosky et al (2006a) report im-proved accuracy through self-training for a two-stage parser and re-ranker.Here our goal is not to improve accuracy, onlyto maintain it, which we achieve through an adap-tive supertagger.
The adaptive supertagger pro-duces lexical categories that the parser would haveused in the final derivation when using the base-line model.
However, it does so with much lowerambiguity levels, and potentially during an ear-lier pass, which means sentences are parsed faster.By increasing the ambiguity level of the adaptivemodels to match the baseline system, we can alsoslightly increase supertagging accuracy, which canlead to higher parsing accuracy.1We use supertag and lexical category interchangeably.345Using the parser to generate training data alsohas the advantage that it is not a domain specificprocess.
Previous work has shown that parserstypically perform poorly outside of their train-ing domain (Gildea, 2001).
Using a newspaper-trained parser, we constructed new training sets forWikipedia and biomedical text.
These were usedto create new supertagging models adapted to thedifferent domains.The self-training method of adapting the su-pertagger to suit the parser increased parsing speedby more than 50% across all three domains, with-out loss of accuracy.
Using an adapted supertaggerwith ambiguity levels tuned to match the baselinesystem, we were also able to increase F-score onlabelled grammatical relations by 0.75%.2 BackgroundMany statistical parsers use two stages: a tag-ging stage that labels each word with its gram-matical role, and a parsing stage that uses the tagsto form a parse tree.
Lexicalised grammars typ-ically contain a much smaller set of rules thanphrase-structure grammars, relying on tags (su-pertags) that contain a more detailed descriptionof each word?s role in the sentence.
This leads tomuch larger tag sets, and shifts a large proportionof the search for an optimal derivation to the tag-ging component of the parser.Figure 1 gives two sentences and their CCGderivations, showing how some of the syntacticambiguity is transferred to the supertagging com-ponent in a lexicalised grammar.
Note that thelexical category assigned to with is different ineach case, reflecting the fact that the prepositionalphrase attaches differently.
Either we need a tag-ging model that can resolve this ambiguity, or bothlexical categories must be supplied to the parserwhich can then attempt to resolve the ambiguityby eventually selecting between them.2.1 SupertaggingSupertaggers typically use standard linear-timetagging algorithms, and only consider words in thelocal context when assigning a supertag.
The C&Csupertagger is similar to the Ratnaparkhi (1996)tagger, using features based on words and POStags in a five-word window surrounding the targetword, and defining a local probability distributionover supertags for each word in the sentence, giventhe previous two supertags.
The Viterbi algorithmI ate pizza with cutleryNP (S\NP)/NP NP ((S\NP)\(S\NP))/NP NP> >S\NP (S\NP)\(S\NP)<S\NP<SI ate pizza with anchoviesNP (S\NP)/NP NP (NP\NP)/NP NP>NP\NP<NP>S\NP<SFigure 1: Two CCG derivations with PP ambiguity.can be used to find the most probable supertag se-quence.
Alternatively the Forward-Backward al-gorithm can be used to efficiently sum over all se-quences, giving a probability distribution over su-pertags for each word which is conditional only onthe input sentence.Supertaggers can be made accurate enough forwide coverage parsing using multi-tagging (Chenet al, 1999), in which more than one supertagcan be assigned to a word; however, as more su-pertags are supplied by the supertagger, parsingefficiency decreases (Chen et al, 2002), demon-strating the influence of lexical ambiguity on pars-ing complexity (Sarkar et al, 2000).Clark and Curran (2004) applied supertaggingto CCG, using a flexible multi-tagging approach.The supertagger assigns to a word all lexical cate-gories whose probabilities are within some factor,?, of the most probable category for that word.When the supertagger is integrated with the C&Cparser, several progressively lower ?
values areconsidered.
If a sentence is not parsed on onepass then the parser attempts to parse the sentenceagain with a lower ?
value, using a larger set ofcategories from the supertagger.
Since most sen-tences are parsed at the first level (in which the av-erage number of supertags assigned to each wordis only slightly greater than one), this providessome of the speed benefit of single tagging, butwithout loss of coverage (Clark and Curran, 2004).Supertagging has since been effectively appliedto other formalisms, such as HPSG (Blunsom andBaldwin, 2006; Zhang et al, 2009), and as an in-formation source for tasks such as Statistical Ma-chine Translation (Hassan et al, 2007).
The useof parser output for supertagger training has beenexplored for LTAG by Sarkar (2007).
However, thefocus of that work was on improving parser andsupertagger accuracy rather than speed.346Previously , watch imports were denied such duty-free treatmentS/S , N /N N (S [dcl ]\NP)/(S [pss]\NP) (S [pss]\NP)/NP NP/NP N/N NN N (S[dcl]\NP)/NP S [pss]\NP (N /N )/(N /N )S [adj ]\NP (S [dcl ]\NP)/(S [adj ]\NP) (S [pss]\NP)/NP N /N(S [pt ]\NP)/NP(S[dcl]\NP)/NPFigure 2: An example sentence and the sets of categories assigned by the supertagger.
The first categoryin each column is correct and the categories used by the parser are marked in bold.
The correct categoryfor watch is included here, for expository purposes, but in fact was not provided by the supertagger.2.2 Semi-supervised trainingPrevious exploration of semi-supervised trainingin NLP has focused on improving accuracy, oftenfor the case where only small amounts of manuallylabelled training data are available.
One approachis co-training, in which two models with indepen-dent views of the data iteratively inform each otherby labelling extra training data.
Sarkar (2001) ap-plied co-training to LTAG parsing, in which the su-pertagger and parser provide the two views.
Steed-man et al (2003) extended the method to a varietyof parser pairs.Another method is to use a re-ranker (Collinsand Koo, 2002) on the output of a system to gener-ate new training data.
Like co-training, this takesadvantage of a different view of the data, but thetwo views are not independent as the re-ranker islimited to the set of options produced by the sys-tem.
This method has been used effectively toimprove parsing performance on newspaper text(McClosky et al, 2006a), as well as adapting aPenn Treebank parser to a new domain (McCloskyet al, 2006b).As well as using independent views of data togenerate extra training data, multiple views can beused to provide constraints at test time.
Holling-shead and Roark (2007) improved the accuracyof a parsing pipeline by using the output of laterstages to constrain earlier stages.The only work we are aware of that uses self-training to improve the efficiency of parsers is vanNoord (2009), who adopts a similar idea to theone in this paper for improving the efficiency ofa Dutch parser based on a manually constructedHPSG grammar.3 Adaptive SupertaggingThe purpose of the supertagger is to cut down thesearch space for the parser by reducing the set ofcategories that must be considered for each word.A perfect supertagger would assign the correct cat-egory to every word.
CCG supertaggers are about92% accurate when assigning a single lexical cate-gory to each word (Clark and Curran, 2004).
Thisis not accurate enough for wide coverage parsingand so a multi-tagging approach is used instead.In the final derivation, the parser uses one categoryfrom each set, and it is important to note that hav-ing the correct category in the set does not guaran-tee that the parser will use it.Figure 2 gives an example sentence and the setsof lexical categories supplied by the supertagger,for a particular value of ?.2 The usual target ofthe supertagging task is to produce the top row ofcategories in Figure 2, the correct categories.
Wepropose a new task that instead aims for the cat-egories the parser will use, which are marked inbold for this case.
The purpose of this new task isto improve speed.The reason speed will be improved is that wecan construct models that will constrain the set ofpossible derivations more than the baseline model.We can construct these models because we canobtain much more of our target output, parser-annotated sentences, than we could for the gold-standard supertagging task.The new target data will contain tagging errors,and so supertagging accuracy measured againstthe correct categories may decrease.
If we ob-tained perfect accuracy on our new task then wewould be removing all of the categories not cho-sen by the parser.
However, parsing accuracy willnot decrease since the parser will still receive thecategories it would have used, and will thereforebe able to form the same highest-scoring deriva-tion (and hence will choose it).To test this idea we parsed millions of sentences2Two of the categories for such have been left out forreasons of space, and the correct category for watch has beenincluded for expository reasons.
The fact that the supertaggerdoes not supply this category is the reason that the parser doesnot analyse the sentence correctly.347in three domains, producing new data annotatedwith the categories that the parser used with thebaseline model.
We constructed new supertaggingmodels that are adapted to suit the parser by train-ing on the combination of these sets and the stan-dard training corpora.
We applied standard evalu-ation metrics for speed and accuracy, and exploredthe source of the changes in parsing performance.4 DataIn this work, we consider three domains: news-wire, Wikipedia text and biomedical text.4.1 Training and accuracy evaluationWe have used Sections 02-21 of CCGbank (Hock-enmaier and Steedman, 2007), the CCG version ofthe Penn Treebank (Marcus et al, 1993), as train-ing data for the newspaper domain.
Sections 00and 23 were used for development and test eval-uation.
A further 113,346,430 tokens (4,566,241sentences) of raw data from the Wall Street Jour-nal section of the North American News Corpus(Graff, 1995) were parsed to produce the trainingdata for adaptation.
This text was tokenised us-ing the C&C tools tokeniser and parsed using ourbaseline models.
For the smaller training sets, sen-tences from 1988 were used as they would be mostsimilar in style to the evaluation corpus.
In all ex-periments the sentences from 1989 were excludedto ensure no overlap occurred with CCGbank.As Wikipedia text we have used 794,024,397tokens (51,673,069 sentences) from Wikipedia ar-ticles.
This text was processed in the same way asthe NANC data to produce parser-annotated train-ing data.
For supertagger evaluation, one thousandsentences were manually annotated with CCG lex-ical categories and POS tags.
For parser evalua-tion, three hundred of these sentences were man-ually annotated with DepBank grammatical rela-tions (King et al, 2003) in the style of Briscoeand Carroll (2006).
Both sets of annotations wereproduced by manually correcting the output of thebaseline system.
The annotation was performedby Stephen Clark and Laura Rimell.For the biomedical domain we have used sev-eral different resources.
As gold standard data forsupertagger evaluation we have used supertaggedGENIA data (Kim et al, 2003), annotated byRimell and Clark (2008).
For parsing evalua-tion, grammatical relations from the BioInfer cor-pus were used (Pyysalo et al, 2007), with theSource Sentence Length Corpus %Range Average Variance0-4 3.26 0.64 1.25-20 14.04 17.41 39.2News 21-40 28.76 29.27 49.441-250 49.73 86.73 10.2All 24.83 152.15 100.00-4 2.81 0.60 22.45-20 11.64 21.56 48.9Wiki 21-40 28.02 28.48 24.341-250 49.69 77.70 4.5All 15.33 154.57 100.00-4 2.98 0.75 0.95-20 14.54 15.14 41.3Bio 21-40 28.49 29.34 48.041-250 49.17 68.34 9.8All 24.53 139.35 100.0Table 1: Statistics for sentences in the supertaggertraining data.
Sentences containing more than 250tokens were not included in our data sets.same post-processing process as Rimell and Clark(2009) to convert the C&C parser output to Stan-ford format grammatical relations (de Marneffeet al, 2006).
For adaptive training we haveused 1,900,618,859 tokens (76,739,723 sentences)from the MEDLINE abstracts tokenised by McIn-tosh and Curran (2008).
These sentences werePOS-tagged and parsed twice, once as for thenewswire and Wikipedia data, and then again, us-ing the bio-specific models developed by Rimelland Clark (2009).
Statistics for the sentences inthe training sets are given in Table 1.4.2 Speed evaluation dataFor speed evaluation we held out three sets of sen-tences from each domain-specific corpus.
Specif-ically, we used 30,000, 4,000 and 2,000 uniquesentences of length 5-20, 21-40 and 41-250 tokensrespectively.
Speeds on these length controlledsets were combined to calculate an overall pars-ing speed for the text in each domain.
Note thatmore than 20% of the Wikipedia sentences wereless than five words in length and the overall dis-tribution is skewed towards shorter sentences com-pared to the other corpora.5 EvaluationWe used the hybrid parsing model described inClark and Curran (2007), and the Viterbi decoderto find the highest-scoring derivation.
The multi-pass supertagger-parser interaction was also used.The test data was excluded from training datafor the supertagger for all of the newswire andWikipedia models.
For the biomedical models ten-348fold cross validation was used.
The accuracy ofsupertagging is measured by multi-tagging at thefirst ?
level and considering a word correct if thecorrect tag is amongst any of the assigned tags.For the biomedical parser evaluation we haveused the parsing model and grammatical relationconversion script from Rimell and Clark (2009).Our timing measurements are calculated in twoways.
Overall times were measured using the C&Cparser?s timers.
Individual sentence measurementswere made using the Intel timing registers, sincestandard methods are not accurate enough for theshort time it takes to parse a single sentence.To check whether changes were statistically sig-nificant we applied the test described by Chinchor(1995).
This measures the probability that two setsof responses are drawn from the same distribution,where a score below 0.05 is considered significant.Models were trained on an Intel Core2Duo3GHz with 4GB of RAM.
The evaluation was per-formed on a dual quad-core Intel Xeon 2.27GHzwith 16GB of RAM.5.1 Tagging ambiguity optimisationThe number of lexical categories assigned to aword by the CCG supertagger depends on the prob-abilities calculated for each category and the ?level being used.
Each lexical category with aprobability within a factor of ?
of the most prob-able category is included.
This means that thechoice of ?
level determines the tagging ambigu-ity, and so has great influence on parsing speed, ac-curacy and coverage.
Also, the tagging ambiguityproduced by a ?
level will vary between models.A more confident model will have a more peakeddistribution of category probabilities for a word,and therefore need a smaller ?
value to assign thesame number of categories.Additionally, the C&C parser uses multiple ?levels.
The first pass over a sentence is at a high ?level, resulting in a low tagging ambiguity.
If thecategories assigned are too restrictive to enable aspanning analysis, the system makes another passwith a lower ?
level, resulting in a higher taggingambiguity.
A maximum of five passes are made,with the ?
levels varying from 0.075 to 0.001.We have taken two approaches to choosing ?levels.
When the aim of an experiment is to im-prove speed, we use the system?s default ?
levels.While this choice means a more confident modelwill assign fewer tags, this simply reflects the factthat the model is more confident.
It should pro-duce similar accuracy results, but with lower am-biguity, which will lead to higher speed.For accuracy optimisation experiments we tunethe ?
levels to produce the same average taggingambiguity as the baseline model on Section 00 ofCCGbank.
Accuracy depends heavily on the num-ber of categories supplied, so the new models areat an accuracy disadvantage if they propose fewercategories.
By matching the ambiguity of the de-fault model, we can increase accuracy at the costof some of the speed improvements the new mod-els obtain.6 ResultsWe have performed four primary sets of exper-iments to explore the ability of an adaptive su-pertagger to improve parsing speed or accuracy.
Inthe first two experiments, we explore performanceon the newswire domain, which is the source oftraining data for the parsing model and the base-line supertagging model.
In the second set of ex-periments, we train on a mixture of gold standardnewswire data and parser-annotated data from thetarget domain.In both cases we perform two experiments.
Thefirst aimed to improve speed, keeping the ?
levelsthe same.
This should lead to an increase in speedas the extra training data means the models aremore confident and so have lower ambiguity thanthe baseline model for a given ?
value.
The secondexperiment aimed to improve accuracy, tuning the?
levels as described in the previous section.6.1 Newswire speed improvementIn our first experiment, we trained supertaggermodels using Generalised Iterative Scaling (GIS)(Darroch and Ratcliff, 1972), the limited mem-ory BFGS method (BFGS) (Nocedal and Wright,1999), the averaged perceptron (Collins, 2002),and the margin infused relaxed algorithm (MIRA)(Crammer and Singer, 2003).
Note that theseare all alternative methods for estimating the lo-cal log-linear probability distributions used by theRatnaparkhi-style tagger.
We do not use globaltagging models as in Lafferty et al (2001) orCollins (2002).
The training data consisted of Sec-tions 02?21 of CCGbank and progressively largerquantities of parser-annotated NANC data ?
fromzero to four million extra sentences.
The results ofthese tests are presented in Table 2.349Ambiguity (%) Tagging Accuracy (%) F-score Speed (sents / sec)Data 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4mBaseline 1.27 96.34 85.46 39.6BFGS 1.27 1.23 1.19 1.18 96.33 96.18 95.95 95.93 85.45 85.51 85.57 85.68 39.8 49.6 71.8 60.0GIS 1.28 1.24 1.21 1.20 96.44 96.27 96.09 96.11 85.44 85.46 85.58 85.62 37.4 44.1 51.3 54.1MIRA 1.30 1.24 1.17 1.13 96.44 96.14 95.56 95.18 85.44 85.40 85.38 85.42 34.1 44.8 60.2 73.3Table 2: Speed improvements on newswire, using various amounts of parser-annotated NANC data.Sentences Av.
Time Change (ms) Total Time Change (s)Sentence length 5-20 21-40 41-250 5-20 21-40 41-250 5-20 21-40 41-250Lower tag amb.
1166 333 281 -7.54 -71.42 -183.23 -1.1 -29 -26Earlier pass Same tag amb.
248 38 8 -2.94 -27.08 -108.28 -0.095 -1.3 -0.44Higher tag amb.
530 33 14 -5.84 -32.25 -44.10 -0.40 -1.3 -0.31Lower tag amb.
19288 3120 1533 -1.13 -5.18 -38.05 -2.8 -20 -30Same pass Same tag amb.
7285 259 35 -0.29 0.94 24.57 -0.28 0.30 0.44Higher tag amb.
1133 101 24 -0.25 2.70 8.09 -0.037 0.34 0.099Lower tag amb.
334 114 104 0.90 7.60 -46.34 0.039 1.1 -2.5Later pass Same tag amb.
14 1 0 1.06 4.26 n/a 0.0019 0.0053 0.0Higher tag amb.
2 1 1 -0.13 26.43 308.03 -3.4e-05 0.033 0.16Table 3: Breakdown of the source of changes in speed.
The test sentences are divided into nine setsbased on the change in parsing behaviour between the baseline model and a model trained using MIRA,Sections 02-21 of CCGbank and 4,000,000 NANC sentences.Using the default ?
levels we found that theperceptron-trained models lost accuracy, disqual-ifying them from this test.
The BFGS, GIS andMIRA models produced mixed results, but nostatistically significant decrease in accuracy, andas the amount of parser-annotated data was in-creased, parsing speed increased by up to 85%.To determine the source of the speed improve-ment we considered the times recorded by the tim-ing registers.
In Table 3, we have aggregated thesemeasurements based on the change in the pass atwhich the sentence is parsed, and how the tag-ging ambiguity changes on that pass.
For sen-tences parsed on two different passes the ambigu-ity comparison is at the earlier pass.
The ?TotalTime Change?
section of the table is the change inparsing time for sentences of that type when pars-ing ten thousand sentences from the corpus.
Thistakes into consideration the actual distribution ofsentence lengths in the corpus.Several effects can be observed in these re-sults.
72% of sentences are parsed on the samepass, but with lower tag ambiguity (5th row in Ta-ble 3).
This provides 44% of the speed improve-ment.
Three to six times as many sentences areparsed on an earlier pass than are parsed on a laterpass.
This means the sentences parsed later havevery little effect on the overall speed.
At the sametime, the average gain for sentences parsed earlieris almost always larger than the average cost forsentences parsed later.
These effects combine toproduce a particularly large improvement for thesentences parsed at an earlier pass.
In fact, despitemaking up only 7% of sentences in the set, thoseparsed earlier with lower ambiguity provide 50%of the speed improvement.It is also interesting to note the changes for sen-tences parsed on the same pass, with the sameambiguity.
We may expect these sentences to beparsed in approximately the same amount of time,and this is the case for the short set, but not for thetwo larger sets, where we see an increase in pars-ing time.
This suggests that the categories beingsupplied are more productive, leading to a largerset of possible derivations.6.2 Newswire accuracy optimisedAny decrease in tagging ambiguity will generallylead to a decrease in accuracy.
The parser uses amore sophisticated algorithm with global knowl-edge of the sentence and so we would expect itto be better at choosing categories than the su-pertagger.
Unlike the supertagger it will excludecategories that cannot be used in a derivation.
Inthe previous section, we saw that training the su-pertagger on parser output allowed us to developmodels that produced the same categories, despitelower tagging ambiguity.
Since they were trainedon the categories the parser was able to use inderivations, these models should also now be pro-viding categories that are more likely to be useful.This leads us to our second experiment, opti-350Tagging Accuracy (%) F-score Speed (sents / sec)NANC sents 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4mBaseline 96.34 85.46 39.6BFGS 96.33 96.42 96.42 96.66 85.45 85.55 85.64 85.98 39.5 43.7 43.9 42.7GIS 96.34 96.43 96.53 96.62 85.36 85.47 85.84 85.87 39.1 41.4 41.7 42.6Perceptron 95.82 95.99 96.30 - 85.28 85.39 85.64 - 45.9 48.0 45.2 -MIRA 96.23 96.29 96.46 96.63 85.47 85.45 85.55 85.84 37.7 41.4 41.4 42.9Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.Train Corpus Ambiguity Tag.
Acc.
F-score Speed (sents / sec)News Wiki Bio News Wiki Bio News Wiki Bio News Wiki BioBaseline 1.267 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1News 1.126 1.151 1.130 95.18 93.56 90.07 85.42 81.2 75.2 73.3 83.9 60.3Wiki 1.147 1.154 1.129 95.06 93.52 90.03 84.70 81.4 75.5 62.4 73.9 58.7Bio 1.134 1.146 1.114 94.66 93.15 89.88 84.23 80.7 75.9 66.2 90.4 59.3Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences.
Thehighlighted values are the top speed for each evaluation set and results that are statistically indistinguish-able from it.mising accuracy on newswire.
We used the samemodels as in the previous experiment, but tunedthe ?
levels as described in Section 5.1.Comparing Tables 2 and 4 we can see the in-fluence of ?
level choice, and therefore taggingambiguity.
When the default ?
values were usedambiguity dropped consistently as more parser-annotated data was used, and category accuracydropped in the same way.
Tuning the ?
levels tomatch ambiguity produces the opposite trend.Interestingly, while the decrease in supertag ac-curacy in the previous experiment did not translateinto a decrease in F-score, the increase in tag accu-racy here does translate into an increase in F-score.This indicates that the supertagger is adapting tosuit the parser.
In the previous experiment, thesupertagger was still providing the categories theparser would have used with the baseline supertag-ging model, but it provided fewer other categories.Since the parser is not a perfect supertagger theseother categories may in fact have been incorrect,and so supertagger accuracy goes down, withoutchanging parsing results.
Here we have allowedthe supertagger to assign extra categories, whichwill only increase its accuracy.The increase in F-score has two sources.
First,our supertagger is more accurate, and so the parseris more likely to receive category sets that can becombined into the correct derivation.
Also, the su-pertagger has been trained on categories that theparser is able to use in derivations, which meansthey are more productive.As Table 6 shows, this change translates into animprovement of up to 0.75% in F-score on SectionModel Tag.
Acc.
F-score Speed(%) (%) (sents/sec)Baseline 96.51 85.20 39.6GIS, 4,000k NANC 96.83 85.95 42.6BFGS, 4,000k NANC 96.91 85.90 42.7MIRA, 4,000k NANC 96.84 85.79 42.9Table 6: Evaluation of top models on Section 23 ofCCGbank.
All changes in F-score are statisticallysignificant.23 of CCGbank.
All of the new models in the tablemake a statistically significant improvement overthe baseline.It is also interesting to note that the results inTables 2, 4 and 6, are similar for all of the train-ing algorithms.
However, the training times differconsiderably.
For all four algorithms the trainingtime is proportional to the amount of data, but theGIS and BFGS models trained on only CCGbanktook 4,500 and 4,200 seconds to train, while theequivalent perceptron and MIRA models took 90and 95 seconds to train.6.3 Annotation method comparisonTo determine whether these improvements weredependent on the annotations being producedby the parser we performed a set of tests withsupertagger, rather than parser, annotated data.Three extra training sets were created by annotat-ing newswire sentences with supertags using thebaseline supertagging model.
One set used theone-best tagger, and two were produced using themost probable tag for each word out of the set sup-plied by the multi-tagger, with variations in the ?value and dictionary cutoff for the two sets.351Train Corpus Ambiguity Tag.
Acc.
F-score Speed (sents / sec)Wiki Bio News Wiki Bio News Wiki Bio News Wiki BioBaseline 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1News 1.331 1.322 96.53 94.86 91.32 85.84 80.1 75.2 41.8 32.6 31.4Wiki 1.293 1.251 96.28 94.79 91.08 85.02 81.7 75.8 40.4 37.2 37.2Bio 1.287 1.195 96.15 94.28 91.03 84.95 80.6 76.1 39.2 52.9 26.2Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.Annotation method Tag.
Acc.
F-scoreBaseline 96.34 85.46Parser 96.46 85.55One-best super 95.94 85.24Multi-tagger a 95.91 84.98Multi-tagger b 96.00 84.99Table 8: Comparison of annotation methods forextra data.
The multi-taggers used ?
values 0.075and 0.001, and dictionary cutoffs 20 and 150, fortaggers a and b respectively.Corpus Speed (sents / sec)Sent length 5-20 21-40 41-250News 242 44.8 8.24Wiki 224 42.0 6.10Bio 268 41.5 6.48Table 9: Cross-corpus speed for the baselinemodel on data sets balanced on sentence length.As Table 8 shows, in all cases the use ofsupertagger-annotated data led to poorer perfor-mance than the baseline system, while the use ofparser-annotated data led to an improvement in F-score.
The parser has access to a range of infor-mation that the supertagger does not, producing adifferent view of the data that the supertagger canproductively learn from.6.4 Cross-domain speed improvementWhen applying parsers out of domain they are typ-ically slower and less accurate (Gildea, 2001).
Inthis experiment, we attempt to increase speed onout-of-domain data.
Note that for some of the re-sults presented here it may appear that the C&Cparser does not lose speed when out of domain,since the Wikipedia and biomedical corpora con-tain shorter sentences on average than the newscorpus.
However, by testing on balanced sets itis clear that speed does decrease, particularly forlonger sentences, as shown in Table 9.For our domain adaptation development ex-periments, we considered a collection of differ-ent models; here we only present results for thebest set of models.
For speed improvement thesewere MIRA models trained on 4,000,000 parser-annotated sentences from the target domain.As Table 5 shows, this training method pro-duces models adapted to the new domain.
In par-ticular, note that models trained on Wikipedia orthe biomedical data produce lower F-scores3 thanthe baseline on newswire.
Meanwhile, on thetarget domain they are adapted to, these modelsachieve a higher F-score and parse sentences atleast 45% faster than the baseline.The changes in tagging ambiguity and accuracyalso show that adaptation has occurred.
In allcases, the new models have lower tagging ambi-guity, and lower supertag accuracy.
However, onthe corpus of the extra data, the performance ofthe adapted models is comparable to the baselinemodel, which means the parser is probably still bereceiving the same categories that it used from thesets provided by the baseline system.6.5 Cross-domain accuracy optimisedThe ambiguity tuning method used to improve ac-curacy on the newspaper domain can also be ap-plied to the models trained on other domains.
InTable 7, we have tested models trained using GISand 400,000 sentences of parsed target-domaintext, with ?
levels tuned to match ambiguity withthe baseline.As for the newspaper domain, we observe in-creased supertag accuracy and F-score.
Also, inalmost every case the new models perform worsethan the baseline on domains other than the onethey were trained on.In some cases the models in Table 7 are less ac-curate than those in Table 5.
This is because aswell as optimising the ?
levels we have changedtraining methods.
All of the training methods weretried, but only the method with the best results innewswire is included here, which for F-score whentrained on 400,000 sentences was GIS.The accuracy presented so far for the biomedi-3Note that the F-scores for Wikipedia and biomedical textare reported to only three significant figures as only 300 and500 sentences respectively were available for parser evalua-tion.352Train Corpus F-scoreRimell and Clark (2009) 81.5Baseline 80.7CCGbank + Genia 81.5+ Newswire 81.9+ Wikipedia 82.2+ Biomedical 81.7+ R&C annotated Bio 82.3Table 10: Performance comparison for models us-ing extra gold standard biomedical data.
Modelswere trained with GIS and 4,000,000 extra sen-tences, and are tested using a POS-tagger trainedon biomedical data.cal model is considerably lower than that reportedby Rimell and Clark (2009).
This is because nogold standard biomedical training data was usedin our experiments.
Table 10 shows the results ofadding Rimell and Clark?s gold standard biomedi-cal supertag data and using their biomedical POS-tagger.
The table also shows how accuracy can befurther improved by adding our parser-annotateddata from the biomedical domain as well as theadditional gold standard data.7 ConclusionThis work has demonstrated that an adapted su-pertagger can improve parsing speed and accu-racy.
The purpose of the supertagger is to re-duce the search space for the parser.
By train-ing the supertagger on parser output, we allow theparser to reach the derivation it would have found,sooner.
This approach also enables domain adap-tation, improving speed and accuracy outside theoriginal domain of the parser.The perceptron-based algorithms used in thiswork are also able to function online, modifyingthe model weights after each sentence is parsed.This could be used to construct a system that con-tinuously adapts to the domain it is parsing.By training on parser-annotated NANC datawe constructed models that were adapted to thenewspaper-trained parser.
The fastest modelparsed sentences 1.85 times as fast and was asaccurate as the baseline system.
Adaptive train-ing is also an effective method of improving per-formance on other domains.
Models trained onparser-annotated Wikipedia text and MEDLINEtext had improved performance on these target do-mains, in terms of both speed and accuracy.
Op-timising for speed or accuracy can be achieved bymodifying the ?
levels used by the supertagger,which controls the lexical category ambiguity ateach level used by the parser.The result is an accurate and efficient wide-coverage CCG parser that can be easily adapted forNLP applications in new domains without manu-ally annotating data.AcknowledgementsWe thank the reviewers for their helpful feed-back.
This work was supported by Australian Re-search Council Discovery grants DP0665973 andDP1097291, the Capital Markets Cooperative Re-search Centre, and a University of Sydney MeritScholarship.
Part of the work was completed at theJohns Hopkins University Summer Workshop and(partially) supported by National Science Founda-tion Grant Number IIS-0833652.ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25(2):237?265.Phil Blunsom and Timothy Baldwin.
2006.
Multi-lingual deep lexical acquisition for HPSGs via su-pertagging.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 164?171, Sydney, Australia.Ted Briscoe and John Carroll.
2006.
Evaluating theaccuracy of an unlexicalized statistical parser on thePARC DepBank.
In Proceedings of the Poster Ses-sion of the 21st International Conference on Com-putational Linguistics, Sydney, Australia.John Chen, Srinivas Bangalore, and Vijay K. Shanker.1999.
New models for improving supertag disam-biguation.
In Proceedings of the Ninth Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 188?195, Bergen, Nor-way.John Chen, Srinivas Bangalore, Michael Collins, andOwen Rambow.
2002.
Reranking an n-gram su-pertagger.
In Proceedings of the 6th InternationalWorkshop on Tree Adjoining Grammars and RelatedFrameworks, pages 259?268, Venice, Italy.Nancy Chinchor.
1995.
Statistical significanceof MUC-6 results.
In Proceedings of the SixthMessage Understanding Conference, pages 39?43,Columbia, MD, USA.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In Proceedings of the 20th International Con-ference on Computational Linguistics, pages 282?288, Geneva, Switzerland.353Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark, James R. Curran, and Miles Osborne.2003.
Bootstrapping POS-taggers using unlabelleddata.
In Proceedings of the seventh Conference onNatural Language Learning, pages 49?55, Edmon-ton, Canada.Michael Collins and Terry Koo.
2002.
Discriminativereranking for natural language parsing.
Computa-tional Linguistics, 31(1):25?69.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing, pages 1?8, Philadel-phia, PA, USA.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.James R. Curran.
2004.
From Distributional to Seman-tic Similarity.
Ph.D. thesis, University of Edinburgh.John N. Darroch and David Ratcliff.
1972.
General-ized iterative scaling for log-linear models.
The An-nals of Mathematical Statistics, 43(5):1470?1480.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation, pages 449?54,Genoa, Italy.Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,and Andrew Ng.
2002.
Web question answering: Ismore always better?
In Proceedings of the 25th In-ternational ACMSIGIR Conference on Research andDevelopment, Tampere, Finland.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the 2001 Conferenceon Empirical Methods in Natural Language Pro-cessing, Pittsburgh, PA, USA.David Graff.
1995.
North American News Text Cor-pus.
LDC95T21.
Linguistic Data Consortium.Philadelphia, PA, USA.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.Supertagged phrase-based statistical machine trans-lation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages288?295, Prague, Czech Republic.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Kristy Hollingshead and Brian Roark.
2007.
Pipelineiteration.
In Proceedings of the 45th Meeting of theAssociation for Computational Linguistics, pages952?959, Prague, Czech Republic.Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, andJun?ichi Tsujii.
2003.
GENIA corpus - a seman-tically annotated corpus for bio-textmining.
Bioin-formatics, 19(1):180?182.Tracy H. King, Richard Crouch, Stefan Riezler, MaryDalrymple, and Ronald M. Kaplan.
2003.
ThePARC 700 Dependency Bank.
In Proceedings ofthe 4th International Workshop on Linguistically In-terpreted Corpora, Budapest, Hungary.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning, pages282?289, San Francisco, CA, USA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006a.
Effective self-training for parsing.
InProceedings of the Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics, Brook-lyn, NY, USA.David McClosky, Eugene Charniak, and Mark John-son.
2006b.
Reranking and self-training for parseradaptation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics, pages 337?344, Sydney, Aus-tralia.Tara McIntosh and James R. Curran.
2008.
Weightedmutual exclusion bootstrapping for domain inde-pendent lexicon and template acquisition.
In Pro-ceedings of the Australasian Language TechnologyWorkshop, Hobart, Australia.Jorge Nocedal and Stephen J. Wright.
1999.
Numeri-cal Optimization.
Springer.Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-tri Haverinen, Juho Heimonen, and Tapio Salakoski.2007.
On the unification of syntactic annotationsunder the Stanford dependency scheme: a case studyon bioinfer and GENIA.
In Proceedings of the ACLworkshop on biological, translational, and clinicallanguage processing, pages 25?32, Prague, CzechRepublic.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the 1996 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 133?142, Philadelphia, PA, USA.354Laura Rimell and Stephen Clark.
2008.
Adapting alexicalized-grammar parser to contrasting domains.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages475?484, Honolulu, HI, USA.Laura Rimell and Stephen Clark.
2009.
Port-ing a lexicalized-grammar parser to the biomedi-cal domain.
Journal of Biomedical Informatics,42(5):852?865.Anoop Sarkar, Fel Xia, and Aravind K. Joshi.
2000.Some experiments on indicators of parsing com-plexity for lexicalized grammars.
In Proceedings ofthe COLING Workshop on Efficiency in Large-scaleParsing Systems, pages 37?42, Luxembourg.Anoop Sarkar.
2001.
Applying co-training methodsto statistical parsing.
In Proceedings of the SecondMeeting of the North American Chapter of the As-sociation for Computational Linguistics, pages 1?8,Pittsburgh, PA, USA.Anoop Sarkar.
2007.
Combining supertagging andlexicalized tree-adjoining grammar parsing.
InSrinivas Bangalore and Aravind Joshi, editors, Com-plexity of Lexical Descriptions and its Relevance toNatural Language Processing: A Supertagging Ap-proach.
MIT Press, Boston, MA, USA.Mark Steedman, Miles Osborne, Anoop Sarkar,Stephen Clark, Rebecca Hwa, Julia Hockenmaier,Paul Ruhlen, Stephen Baker, and Jeremiah Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proceedings of the 10th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 331?338, Budapest, Hun-gary.Geertjan van Noord.
2009.
Learning efficient parsing.In Proceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 817?825.
Association for Com-putational Linguistics.Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichiTsujii.
2009.
HPSG supertagging: A sequence la-beling view.
In Proceedings of the 11th Interna-tional Conference on Parsing Technologies, pages210?213, Paris, France.355
