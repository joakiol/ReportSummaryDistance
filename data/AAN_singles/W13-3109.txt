Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 64?71,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsUsing a Keyness Metric for Single and Multi Document SummarisationMahmoud El-HajSchool of Computing andCommunicationsLancaster UniversityUnited Kingdomm.el-haj@lancaster.ac.ukPaul RaysonSchool of Computing andCommunicationsLancaster UniversityUnited Kingdomp.rayson@lancaster.ac.ukAbstractIn this paper we show the results ofour participation in the MultiLing 2013summarisation tasks.
We participatedwith single-document and multi-documentcorpus-based summarisers for both Ara-bic and English languages.
The sum-marisers used word frequency lists andlog likelihood calculations to generate sin-gle and multi document summaries.
Thesingle and multi summaries generated byour systems were evaluated by Arabicand English native speaker participantsand by different automatic evaluation met-rics, ROUGE, AutoSummENG, MeMoGand NPowER.
We compare our results toother systems that participated in the sametracks on both Arabic and English lan-guages.
Our single-document summaris-ers performed particularly well in the auto-matic evaluation with our English single-document summariser performing betteron average than the results of the otherparticipants.
Our Arabic multi-documentsummariser performed well in the humanevaluation ranking second.1 IntroductionSystems that can automatically summarise docu-ments are becoming ever more desirable with theincreasing volume of information available on theWeb.
Automatic text summarisation is the processof producing a shortened version of a text by theuse of computers.
For example, reducing a textdocument or a group of related documents into ashorter version of sentences or paragraphs usingautomated tools and techniques.The summary should convey the key contri-butions of the text.
In other words, only keysentences should appear in the summary and theprocess of defining those sentences is highly de-pendent on the summarisation method used.
Inautomatic summarisation there are two main ap-proaches that are broadly used, extractive and ab-stractive.
The first method, the extractive sum-marisation, extracts, up to a certain limit, thekey sentences or paragraphs from the text and or-ders them in a way that will produce a coherentsummary.
The extracted units differ from onesummariser to another.
Most summarisers usesentences rather than larger units such as para-graphs.
Extractive summarisation methods arethe focus method on automatic text summarisa-tion.
The other method, abstractive summarisa-tion, involves more language dependent tools andNatural Language Generation (NLG) technology.In our work we used extractive single and multi-document Arabic and English summarisers.A successful summarisation approach needs agood guide to find the most important sentencesthat are relevant to a certain criterion.
Therefore,the proposed methods should work on extractingthe most important sentences from a set of relatedarticles.In this paper we present the results of our par-ticipation to the MultiLing 2013 summarisationtasks.
MultiLing 2013 was built upon the TextAnalysis Conference (TAC) MultiLing Pilot taskof 2011 (Giannakopoulos et al 2011).
MultiL-ing 2013 this year asked for participants to runtheir summarisers on different languages having acorpus and gold standard summaries in the sameseven languages (Arabic, Czech, English, French,Greek, Hebrew or Hindi) of TAC 2011 with a50% increase to the corpora size.
It also intro-duced three new languages (Chinese, Romanianand Spanish).
MultiLing 2013 this year intro-duced a new single-document summarisation pilotfor 40 languages including the above mentionedlanguages (in our case Arabic and English).In this paper we introduce the results of our64single-document and multi-document summaris-ers at the MultiLing 2013 summarisation tasks.We used a language independent corpus-basedword frequency technique and the log-likelihoodstatistic to extract sentences with the maximumsum of log likelihood.
The output summary is ex-pected to be no more than 250 words.2 Related Work2.1 Automatic SummarisationWork on automatic summarisation dates backmore than 50 years, with a focus on the Englishlanguage (Luhn, 1958).
The work on Arabic au-tomatic summarisation is more recent and still noton par with the research on English and other Eu-ropean languages.
Early work on Arabic summari-sation started less than 10 years ago (Conroy et al2006; Douzidia and Lapalme, 2004).Over time, there have been various approachesto automatic text summarisation.
These ap-proaches include single-document and multi-document summarisation.
Both single-documentand multi-document summarisation use the sum-marisation methods mentioned earlier, i.e.
ex-tractive or abstractive.
Summarising a text couldbe dependent on input information such as a userquery or it could be generic where no user queryis used.The approach of single-document summarisa-tion relies on the idea of producing a summaryfor a single document.
The main factor in single-document summarisation is to identify the mostimportant (informative) parts of a document.
Earlywork on single-document summarisation was thework by Luhn (1958).
In his work he lookedfor sentences containing keywords that are mostfrequent in a text.
The sentences with highlyweighted keywords were selected.
The work byLuhn highlighted the need for features that reflectthe importance of a certain sentence in a text.
Bax-endale (1958) showed the importance of sentence-position in a text, which is understood to be oneof the earliest extracted features in automatic textsummarisation.
They took a sample of 200 para-graphs and found that in 80% of the paragraphsthe most important sentence was the first one.Multi-document summarisation produces a sin-gle summary of a set of documents.
The docu-ments are assumed to be about the same genre andtopic.
The analysis in this area is performed typi-cally at either the sentence or document level.2.2 Corpus-based and Word Frequency inSummarisationCorpus-based techniques are mainly used to com-pare corpora for linguistic analysis (Rayson andGarside, 2000; Rayson et al 2004).
There aretwo main types of corpora comparisons, 1) com-paring a sample corpus with a larger standardcorpus (Scott, 2000).
2) comparing two corporaof equal size (Granger, 1998).
In our work weadopted the first approach, where we used a muchlarger reference corpus.
The first word list is thefrequency list of all the words in the document (orgroup of documents) to be summarised which iscompared to the word frequency list of a muchlarger standard corpus.
We do that for both Ara-bic and English texts.
Word frequency has beenproven as an important feature when determininga sentence?s importance (Li et al 2006).
Nenkovaand Vanderwende (2005) studies the impact of fre-quency on summarisation.
In their work they in-vestigated the association between words that ap-pear frequently in a document (group of relateddocuments), and the likelihood that they will beselected by a human summariser to be included ina summary.
Taking the top performing summaris-ers at the DUC 20031 they computed how many ofthe top frequency words from the input documentsappeared in the system summaries.
They found thefollowing: 1) Words with high frequency in theinput documents are very likely to appear in thehuman summaries.
2) The automatic summaris-ers include less of these high frequency words.These two findings by Nenkova and Vanderwende(2005) tell us two important facts.
Firstly, it con-firms that word frequency is an important factorthat impacts humans?
decisions on which contentto include in the summary.
Secondly, the overlapbetween human and system summaries can be im-proved by including more of the high frequencywords in the generated system summaries.
Basedon Nenkova?s study we expand the work on wordfrequency by comparing word frequency lists ofdifferent corpora in a way to select sentences withthe maximum sum of log likelihood ratio.
The log-likelihood calculation favours words whose fre-quencies are unexpectedly high in a document.2.3 Statistical SummarisationThe use of statistical approaches (e.g.
log-likelihood) in text summarisation is a common1http://duc.nist.gov/duc2003/tasks.html65technique, especially when building a language in-dependent text summariser.Morita et al(2011) introduced what they called?query-snowball?, a method for query-orientedextractive multi-document summarisation.
Theyworked on closing the gap between the query andthe relevant sentences.
They formulated the sum-marisation problem based on word pairs as a max-imum cover problem with Knapsack Constraints(MCKP), which is an optimisation problem thatmaximises the total score of words covered by asummary within a certain length limit.Knight and Marcu (2000) used the ExpectationMaximisation (EM) algorithm to compress sen-tences for an abstractive text summarisation sys-tem.
EM is an iterative method for finding Maxi-mum Likelihood (ML) or Maximum A Posteriori(MAP) estimates of parameters in statistical mod-els.
In their summariser, EM was used in the sen-tences compression process to shorten many sen-tences into one by compressing a syntactic parsetree of a sentence in order to produce a shorter butmaximally grammatical version.
Similarly, Mad-nani et al(2007) performed multi-document sum-marisation by generating compressed versions ofsource sentences as summary candidates and usedweighted features of these candidates to constructsummaries.Hennig (2009) introduced a query-based la-tent Semantic Analysis (LSA) automatic text sum-mariser.
It finds statistical semantic relationshipsbetween the extracted sentences rather than wordby word matching relations (Hofmann, 1999).The summariser selects sentences with the highestlikelihood score.In our work we used log-likelihood to selectsentences with the maximum sum of log likeli-hood scores, unlike the traditional method of mea-suring cosine similarity overlap between articlesor sentences to indicate importance (Luhn, 1958;Barzilay et al 2001; Radev et al 2004).
Themain advantage of our approach is that the auto-matic summariser does not need to compare sen-tences in a document with an initial one (e.g.
firstsentence or a query).
Our approach works by cal-culating the keyness (or log-likelihood) score foreach token (word) in a sentence, then picks, to alimit of 250 words, the sentences with the highestsum of the tokens?
log-likelihood scores.To the best of our knowledge the use of corpus-based frequency list to calculate the log-likelihoodscore for text summarisation has not been reportedfor the Arabic language.3 Dataset and Evaluation Metrics3.1 Test CollectionThe test collection for the MultiLing 2013 is avail-able in the previously mentioned languages.2 Thedataset is based on WikiNews texts.3 The sourcedocuments contain no meta-data or tags and arerepresented as UTF8 plain text les.
The multi-document dataset of each language contains (100-150) articles divided into 10 or 15 reference sets,each contains 10 related articles discussing thesame topic.
The original language of the datasetis English.
The organisers of the tasks were re-sponsible for translating the corpus into differ-ent languages by having native speaker partici-pants for each of the 10 languages.
In addi-tion to the news articles the dataset al provideshuman-generated multi-document gold standardsummaries.
The single-document dataset containssingle documents for 40 language (30 documentseach) discussing various topics and collected fromWikipedia.43.2 EvaluationEvaluating the quality and consistency of a gen-erated summary has proven to be a difficult prob-lem (Fiszman et al 2009).
This is mainly becausethere is no obvious ideal, objective summary.
Twoclasses of metrics have been developed: form met-rics and content metrics.
Form metrics focus ongrammaticality, overall text coherence, and organ-isation.
They are usually measured on a pointscale (Brandow et al 1995).
Content metrics aremore difficult to measure.
Typically, system out-put is compared sentence by sentence or unit byunit to one or more human-generated ideal sum-maries.
As with information retrieval, the per-centage of information presented in the system?ssummary (precision) and the percentage of impor-tant information omitted from the summary (re-call) can be assessed.
There are various mod-els for system evaluation that may help in solvingthis problem.
This include automatic evaluations(e.g.
ROUGE and AutoSummENG), and human-performed evaluations.
For the MultiLing 2013task, the summaries generated by the participants2http://multiling.iit.demokritos.gr/file/all3http://www.wikinews.org/4http://www.wikipedia.org/66were evaluated automatically based on human-generated model summaries provided by fluentspeakers of each corresponding language (nativespeakers in the general case).
The models usedwere, ROUGE variations (ROUGE1, ROUGE2,ROUGE-SU4) (Lin, 2004), the MeMoG varia-tion (Giannakopoulos and Karkaletsis, 2011) ofAutoSummENG (Giannakopoulos et al 2008)and NPowER (Giannakopoulos and Karkaletsis,2013).
ROUGE was not used to evaluate thesingle-document summaries.The summaries were also evaluated manuallyby human participants.
For the manual evalua-tion the human evaluators were provided with thefollowing guidelines: Each summary is to be as-signed an integer grade from 1 to 5, related to theoverall responsiveness of the summary.
We con-sider a text to be worth a 5, if it appears to coverall the important aspects of the corresponding doc-ument set using fluent, readable language.
A textshould be assigned a 1, if it is either unreadable,nonsensical, or contains only trivial informationfrom the document set.
We consider the contentand the quality of the language to be equally im-portant in the grading.Note, the human evaluation results for the En-glish language are not included in this paper as bythe time of writing the results were not yet pub-lished.
We only report the human evaluation re-sults of the Arabic multi-document summaries.4 Corpus-based SummarisationOur summarisation approach is a corpus-basedwhere we use word frequency lists to compare cor-pora and calculate the log likelihood score for eachword in the list.
The compared corpora includestandard Arabic and English corpora in additionto the Arabic and English summarisation datasetsprovided by MultiLing 2013 for the single andmulti-document summarisation tasks.
The subsec-tions below describe the creation of the word listsand the standard corpora we used for the compar-ison process.4.1 Word FrequenciesWe used a simple methodology to generate theword frequency lists for the Arabic and Englishsummarisation datasets provided by MultiLing2013.
The datasets used in our experiments weresingle-document and multi-document documentsin English and Arabic.
For the multi-document(a) Arabic Sample (b) English SampleFigure 1: Arabic and English Word Frequency ListSampledataset we counted the word frequency for all thedocuments in a reference set (group of related arti-cles), each set contains on average 10 related arti-cles.
The single-document dataset was straightfor-ward, we calculated word frequencies for all thewords in each document.
Figure 1 shows a sam-ple of random words and their frequencies for bothArabic and English languages.
The sample was se-lected from the MultiLing dataset word frequencylists.
As shown in the figure we did not eliminatethe stop-words, we treat them as normal words.4.2 Standard CorporaIn our work we compared the word frequency listof the summarisation dataset against the largerArabic and English standard corpora.
For eachof the standard corpora we had a list of word fre-quencies (up to 5, 000 words) for both Arabic andEnglish using the frequency dictionary of Ara-bic (Buckwalter and Parkinson, 2011) and the Cor-pus of Contemporary American English (COCA)top 5,000 words (Davies, 2010).The frequency dictionary of Arabic provides alist of the 5,000 most frequently used words inModern Standard Arabic (MSA) in addition toseveral of the most widely spoken Arabic dialects.The list was created based on a 30-million-wordcorpus of Arabic including written and spoken ma-terial from all around the Arab world.
The Ara-bic summarisation dataset provided by MultiL-ing 2013 was also written using MSA.
The cor-pus of contemporary American English COCA isa freely searchable 450-million-word corpus con-taining text in American English of different num-ber of genres.
To be consistent with the Arabic67word frequency list, we used the top 5000 wordsfrom the 450 million word COCA corpus.5 Summarisation MethodologyIn our experiments we used generic single-document and multi-document extractive sum-marisers that have been implemented for bothArabic and English (using identical processingpipelines for both languages).
Summaries werecreated by selecting sentences from a single doc-ument or set of related documents.
The followingsubsections show the methods used in our exper-iments, the actual summarisation process and theexperimental setup.5.1 Calculating Log-LikelihoodWe begin the summarisation process by calculat-ing the log likelihood score for each word in theword frequency lists (see Section 4.1) using thesame methodology described in (Rayson and Gar-side, 2000).
This was performed by constructing acontingency table as in Table 1.CorpusOneCorpusTwoTotalFrequencyof Worda b a+bFrequencyof otherwordsc-a d-b c+d-a-bTotal c d c+dTable 1: Contingency TableThe values c and d correspond to the number ofwords in corpus one and corpus two respectively.Where a and b are the observed values (O).
Foreach corpus we calculated the expected value Eusing the following formula:Ei =Ni?iOi?iNiNi is the total frequency in corpus i (i in ourcase takes the values 1 (c) and 2 (d) for the Multi-Ling Arabic Summaries dataset and the frequencydictionary of Arabic (or MultiLing English Sum-maries dataset and COCA corpus) respectively.The log-likelihood can be calculated as follows:LL = 2 ?
((a ?
ln(aE1)) + (b ?
ln(bE2)))5.2 Summarisation ProcessWe used the same processing pipeline for both thesingle-document and multi-document summaris-ers.
For each word in the MultiLing summari-sation dataset (Arabic and English) we calculatedthe log likelihood scores using the calculations de-scribed in Section 5.1.
We summed up the loglikelihood scores for each sentence in the datasetand we picked the sentences (up to 250 word limit)with the highest sum of log likelihood scores.
Themain difference between the single-document andmulti-document summarisers is that we treat theset of related documents in the multiling datasetas one document.6 Single-Document Summarisation TaskMultiLing 2013 this year introduced a new single-document summarisation pilot for 40 languagesincluding (Arabic, Czech, English, French, Greek,Hebrew, Hindi, Spanish, Chinese, Romanian...etc).
In our case we participated in two lan-guages only, English and Arabic.The pilot aim was to measure the ability of au-tomated systems to apply single document sum-marisation, in the context of Wikipedia texts.Given a single encyclopedic entry, with severalsections/subsections, describing a specific subject,the pilot guidelines asked the participating sys-tems to provide a summary covering the mainpoints of the entry (similarly to the lead section ofa Wikipedia page).
The MultiLing 2013 single-document summaries dataset consisted of (non-parallel) documents in the above mentioned lan-guages.For the English language, there were 7 partici-pants (peers) including a baseline system (ID5).The Arabic language had 6 participants includingthe same baseline system.7 Multi-Document Summarisation TaskThe Multi-document summarisation task requiredthe participants to generate a single, fluent, rep-resentative summary from a set of documents de-scribing an event sequence.
The language of thedocument set was within a given range of lan-guages and all documents in a set shared the samelanguage.
The task guidelines required the outputsummary to be of the same language as its sourcedocuments.
The output summary should be 250words at most.68The set of documents were available in 10 lan-guages (Arabic, Czech, English, French, Greek,Hebrew, Hindi, Spanish, Chinese and Romanian).In our case we participated using the Arabic andEnglish set of documents only.For the English language, there were 10 partic-ipants (peers) including a baseline (ID6) and atopline (ID61) systems.
The Arabic language had10 participants as well, including the same base-line and topline systems.The baseline summariser sorted sentences basedon their cosine similarity to the centroid of a clus-ter.
Then starts adding sentences to the summary,until it either reaches 250 words, or it hits the endof the document.
In the second case, it continueswith the next document in the sorted list.The topline summariser used information fromthe model summaries (i.e.
cheats).
First, it split allsource documents into sentences.
Then it used agenetic algorithm to generate summaries that havea vector with maximal cosine similarity to the cen-troid vector of the model summary texts.8 Results and DiscussionOur single-document summarisers, both Englishand Arabic, performed particularly well in the au-tomatic evaluation.
Ranking first and second re-spectively.Tables 2 and 3 illustrate the AutoSummEng(AutoSumm), MeMoG and NPowER results andthe ranking of our English and Arabic single-document summarisers (System ID2).System AutoSumm MeMoG NPowERID2 0.136 0.136 1.685ID41 0.129 0.129 1.661ID42 0.127 0.127 1.656ID3 0.127 0.127 1.654ID1 0.124 0.124 1.647ID4 0.123 0.123 1.641ID5 0.040 0.040 1.367Table 2: English Automatic Evaluation Scores(single-document)The evaluation scores of our single-documentsummarisers confirm with (Li et al 2006) and(Nenkova and Vanderwende, 2005) findings, werethey found that word frequency is an importantfeature when determining sentences importanceand that words with high frequency in the inputSystem AutoSumm MeMoG NPowERID3 0.092 0.092 1.538ID2 0.087 0.087 1.524ID41 0.055 0.055 1.418ID42 0.055 0.055 1.416ID4 0.053 0.053 1.411ID5 0.025 0.025 1.317Table 3: Arabic Automatic Evaluation Scores(single-document)System ScoreID6 3.711ID3 3.578ID2 3.578ID4 3.489ID1 3.467ID11 3.333ID21 3.111ID51 2.778ID5 2.711ID61 2.489Table 4: Arabic Manual Evaluation Scores (multi-document)documents are very likely to appear in the hu-man summaries, which explains the high correla-tion between our single-document and the human(model) summaries as illustrated in the evalua-tion scores (Tables 2 and 3).
The single-documentsummaries were evaluated automatically only.Our Arabic multi-document summariser per-formed well in the human evaluation ranking sec-ond jointly with System ID2.
Table 4 shows theaverage scores of the human evaluation process,our system is referred to as ID3.
On the otherhand, we did not perform well in the automaticevaluation of the multi-document summarisationtask for both English and Arabic.
Our systems didnot perform better than the baseline.
The auto-matic evaluation results placed our Arabic and En-glish summariser further down in the ranked listsof systems compared to the human assessment.This is an area for future work as this seems tosuggest that the automatic evaluation metrics arenot necessarily in line with human judgements.The low automatic evaluation scores are dueto two main reasons.
First, we treated the setof related documents (multi-documents) as a sin-gle big document (See Section 5.2), this penalised69our summaries as selecting the sentences with themaximum sum of log likelihood score lead tomany important sentences being overlooked.
Thiscan be solved by running the summariser on eachdocument to suggest candidate sentences and thenselecting the top sentence(s) of each document togenerate the final summary.
Second, we did notwork on eliminating redundancies.
Finally, thelog-likelihood score might be improved by the in-clusion of a dispersion score or weighting to exam-ine the evenness of the spread of each word acrossall the documents.9 ConclusionIn this paper we presented the results of our par-ticipation in the MultiLing 2013 summarisationtask.
We submitted results for single-documentand multi-document summarisation in two lan-guages, English and Arabic.
We applied a corpus-based summariser that used corpus-based wordfrequency lists.
We used a list of the 5,000 mostfrequently used words in Modern Standard Ara-bic (MSA) and English.
Using the frequency dic-tionary of Arabic and the corpus of contemporaryAmerican English (COCA).Based on the automatic evaluation scores, wefound that our approach appears to work very wellfor Arabic and English single-document summari-sation.
According to the human evaluation scoresthe approach could potentially work for Arabicmulti-document summarisation as well.
We be-lieve that the approach could still work well formulti-document summarisation following the sug-gested solutions in Section 8.ReferencesR.
Barzilay, N. Elhadad, and K. McKeown.
2001.
Sen-tence Ordering in Multidocument Summarization.In Proceedings of the First International Conferenceon Human Language Technology Research, HLT?01,pages 1?7, Stroudsburg, PA, USA.
Association forComputational Linguistics.P.
Baxendale.
1958.
Machine-made index for technicalliterature: an experiment.
IBM Journal of Researchand Development, 2(4):354?361.R.
Brandow, K. Mitze, and Lisa F. Rau.
1995.Automatic Condensation of Electronic Publicationsby Sentence Selection.
Inf.
Process.
Manage.,31(5):675?685.T.
Buckwalter and D. Parkinson.
2011.
A FrequencyDictionary of Arabic: Core Vocabulary for Learn-ers.
Routledge, London, United Kingdom.J.
Conroy, J. Schlesinger, D. O?Leary, and J. Goldstein.2006.
Back to Basics: CLASSY 2006.
In Pro-ceedings of the 6th Document Understanding Con-ferences.
DUC.M.
Davies.
2010.
The Corpus of Contemporary Amer-ican English as the First Reliable Monitor Corpusof English.
Literary and Linguistic Computing,25:447?464.F.
Douzidia and G. Lapalme.
2004.
Lakhas, an Ara-bic Summarising System.
In Proceedings of the 4thDocument Understanding Conferences , pages 128?135.
DUC.M.
Fiszman, D. Demner-Fushman, H. Kilicoglu, andT.
Rindflesch.
2009.
Automatic Summarizationof MEDLINE Citations for Evidence-based MedicalTreatment: A Topic-oriented Evaluation.
Jouranl ofBiomedical Informatics, 42(5):801?813.G.
Giannakopoulos and V. Karkaletsis.
2011.
Au-toSummENG and MeMoG in Evaluating GuidedSummaries.
In The Proceedings of the Text AnalysisConference, MD, USA.
TAC.G.
Giannakopoulos and V. Karkaletsis.
2013.
Sum-mary evaluation: Together we stand npower-ed.
InAlexander Gelbukh, editor, Computational Linguis-tics and Intelligent Text Processing, volume 7817 ofLecture Notes in Computer Science, pages 436?450.Springer Berlin Heidelberg.G.
Giannakopoulos, V. Karkaletsis, G. Vouros, andP.
Stamatopoulos.
2008.
Summarization Sys-tem Evaluation Revisited: N?Gram Graphs.
ACMTransactions on Speech and Language Processing(TSLP), 5(3):1?39.G.
Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,J.
Steinberger, and V. Varma.
2011.
TAC 2011 Mul-tiLing Pilot Overview.
In Text Analysis Conference(TAC) 2011, MultiLing Summarisation Pilot, Mary-land, USA.
TAC.S.
Granger.
1998.
The computer learner corpus: Aversatile new source of data for SLA research.
pages3?18.L.
Hennig.
2009.
Topic-based multi-document sum-marization with probabilistic latent semantic analy-sis.
In Proceedings of the International ConferenceRANLP-2009, pages 144?149, Borovets, Bulgaria,September.
Association for Computational Linguis-tics.T.
Hofmann.
1999.
Probabilistic latent semantic in-dexing.
In Proceedings of the 22nd annual inter-national ACM SIGIR conference on Research anddevelopment in information retrieval, SIGIR ?99,pages 50?57, New York, NY, USA.
ACM.K.
Knight and D. Marcu.
2000.
Statistics-Based Sum-marization ?
Step One: Sentence Compression.
InProceedings of the Seventeenth National Conferenceon Artificial Intelligence and Twelfth Conference70on Innovative Applications of Artificial Intelligence,pages 703?710, Menlo Park, CA.
AAAI Press.W.
Li, B. Li, and M. Wu.
2006.
Query Focus GuidedSentence Selection Strategy.C.
Lin.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
In Proceedings of theWorkshop on Text Summarization Branches Out(WAS 2004), pages 25?26.
WAS 2004).H.
Luhn.
1958.
The Automatic Creation of LiteratureAbstracts.
IBM Journal of Research and Develop-ment, 2(2):159?165.N.
Madnani, D. Zajic, B. Dorr, N. Ayan, and J. Lin.2007.
Multiple Alternative Sentence Compressionsfor Automatic Text Summarization.
In Proceedingsof the 7th Document Understanding Conference atNLT/NAACL, page 26.
DUC.H.
Morita, T. Sakai, and M. Okumura.
2011.
QuerySnowball: A Co-occurrence-based Approach toMulti-document Summarization for Question An-swering.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies: short papers- Volume 2, HLT?11, pages 223?229, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.A.
Nenkova and L. Vanderwende.
2005.
The impact offrequency on summarization.
Microsoft Research,Redmond, Washington, Tech.
Rep. MSR-TR-2005-101.D.
Radev, H. Jing, M. Sty, and D. Tam.
2004.Centroid-based Summarization of Multiple Docu-ments.
Information Processing and Management,40:919?938.P.
Rayson and R. Garside.
2000.
Comparing corporausing frequency profiling.
In Proceedings of theworkshop on Comparing corpora - Volume 9, WCC?00, pages 1?6, Stroudsburg, PA, USA.P.
Rayson, D. Berridge, and B. Francis.
2004.
Ex-tending the cochran rule for the comparison of wordfrequencies between corpora.
In Proceedings of the7th International Conference on Statistical analysisof textual data (JADT 2004, pages 926?936.M.
Scott.
2000.
Focusing on the text and its keywords.
In Burnard, L. and McEnery, T.
(eds.)
Re-thinking language pedagogy from a corpus perspec-tive: papers from the third international conferenceon teaching and language corpora, pages 103?121.71
