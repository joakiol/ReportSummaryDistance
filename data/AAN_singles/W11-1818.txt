Proceedings of BioNLP Shared Task 2011 Workshop, pages 130?137,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsComplex Biological Event Extraction from Full Text using Signatures ofLinguistic and Semantic FeaturesLiam R. McGrath and Kelly Domico and Courtney D. Corley and Bobbie-Jo Webb-RobertsonPacific Northwest National Laboratory902 Battelle BLVD, PO BOX 999Richland, WA 99352{liam | kelly.domico | court | bj}@pnl.govAbstractBuilding on technical advances from theBioNLP 2009 Shared Task Challenge, the2011 challenge sets forth to generalize tech-niques to other complex biological event ex-traction tasks.
In this paper, we present theimplementation and evaluation of a signature-based machine-learning technique to predictevents from full texts of infectious diseasedocuments.
Specifically, our approach usesnovel signatures composed of traditional lin-guistic features and semantic knowledge topredict event triggers and their candidate argu-ments.
Using a leave-one out analysis, we re-port the contribution of linguistic and shallowsemantic features in the trigger prediction andcandidate argument extraction.
Lastly, we ex-amine evaluations and posit causes for errorsin our complex biological event extraction.1 IntroductionThe BioNLP 2009 Shared Task (Kim et al, 2009)was the first shared task to address fine-grained in-formation extraction for the bio-molecular domain,by defining a task involving extraction of eventtypes from the GENIA ontology.
The BioNLP 2011Shared Task ( (Kim et al, 2011)) series generalizedthis defining a series of tasks involving more texttypes, domains and target event types.
Among thetasks for the new series is the Infection Disease task,proposed and investigated by (Pyysalo et al, 2011;Pyysalo et al, 2010; Bjorne et al, 2010).Like the other tasks for the BioNLP Shared Taskseries, the goal is to extract mentions of relevantevents from biomedical publications.
To extractan event, the event trigger and all arguments mustbe identified in the text by exact offset and typedaccording to a given set of event and argumentclasses (Miwa et al, 2010).
Entity annotations aregiven for a set of entity types that fill many of thearguments.Here we describe Pacific Northwest National Lab-oratory?s (PNNL) submission to the BioNLP 2011Infectious Disease shared task.
We describe the ap-proach and then discuss results, including an analy-sis of errors and contribution of various features.2 ApproachOur system uses a signature-based machine-learningapproach.
The system is domain-independent,using a primary task description vocabulary andtraining data to learn the task, but domain re-sources can be incorporated as additional featureswhen available, as described here.
The approachcan be broken down into 4 components: an au-tomated annotation pipeline to provide the basisfor features, classification-based trigger identifica-tion and argument identification components, and apost-processing component to apply semantic con-straints.
The UIMA framework1 is used to integratethe components into a pipeline architecture.2.1 Primary TasksA definition of the events to be extracted is used todefine candidates for classification and post-processthe results of the classification.
First a list ofdomain-specific entity classes is given.
Entities of1http://uima.apache.org/130Event Class ArgumentsGene expression Theme(Protein|Regulon-operon)Transcription Theme(Protein|Regulon-operon)Protein catabolism Theme(Protein)Phosphorylation Theme(Protein), Site(entity)?Localization Theme(core entity), AtLoc(entity)?, ToLoc(entity)?Binding Theme(core entity)+, Site(entity)*Regulation Theme(core entity|event), Cause(core entity|event)?, Site(entity)?, CSite(entity)?Positive regulation Theme(core entity|event), Cause(core entity|event)?, Site(entity)?, CSite(entity)?Negative regulation Theme(core entity|event), Cause(core entity|event)?, Site(entity)?, CSite(entity)?Process Participant(core entity)?Table 1: Summary of the target events.
Type restrictions on fillers of each argument type are shown in parenthesis.Multiplicity of each argument type is also marked (+ = one-to-many, ?
= zero-to-one, * = zero-to-many, otherwise =one).these classes are assumed to be annotated in the data,as is the case for the Infectious Disease task.
Then,each event class is given, with a list of argumenttypes for each.
Each argument is marked with itsmultiplicity, indicating how many of this argumenttype is valid for each event, either: one ?
exactly oneis required, one-to-many ?
one or more is required,zero-to-one ?
one is optional, and zero-to-many ?one or many are optional.
Also, restrictions on theclasses of entities that can fill each argument aregiven, by listing: one or more class names ?
indicat-ing the valid domain-specific entity classes from thedefinition, core entity ?
indicating that any domain-specific entity in the definition is valid, event ?
indi-cating that any event in the definition is valid, or en-tity ?
indicating that any span from the text is valid.Table 1 shows the summary of the event extractiontasks for the Infectious Disease track.2.2 AnnotationLinguistic and domain annotations are automaticallyapplied to the document to be used for trigger andargument identification in framing the tasks for clas-sification and generating features for each instance.Linguistic annotations include sentence splits, to-kens, parts of speech, tree parses, typed dependen-cies (deMarneffe et al, 2006; MacKinlay et al,2009), and stems.
For the Infectious Disease task,the parses from the Stanford Parser (Klein and Man-ning, 2003) provided by the Supporting Analysis(Stenetorp et al, 2011) was used to obtain all ofthese linguistic annotations, except for the stems,which were obtained from the Porter Stemmer (vanRijsbergen et al, 1980).For the Infectious Disease task, two sets of do-main specific annotations are included: knowntrigger words for each event class and semantictags from the Unified Medical Language System(UMLS) (Bodenreider, 2004).
Annotations forknown trigger words are created using a dictionaryof word stem-event class pairs created from anno-tated training data.
An entry is created in the dictio-nary every time a new stem is seen as a trigger foran event class.
When a word with one of these stemsis seen during processing, it is annotated as a typicaltrigger word for that event class.Semantic tags are calculating using MetaMap2010 (Aronson and Lang, 2010).
MetaMap providessemantic tags for terms in a document with up tothree levels of specificity, from most to least spe-cific: concept, type and group (Torii et al, 2011).Word sense disambiguation is used to identify thebest tags for each term.
For example, consider thetags identified by MetaMap for the phrase Humanperipheral B cells:Humanconcept: Homo sapienstype: Humangroup: Living BeingsPeripheraltype: Spatial Conceptgroup: Concepts & IdeasB-Cellsconcept: B-Lymphocytestype: Cell131group: AnatomyIn this example, semantic mappings were found forthree terms: Human, Peripheral and B-Cells.
Hu-man and B-Cells were mapped to specific concepts,but Peripheral was mapped to a more general group.Entities are also annotated at this point.
For theInfectious Disease task, annotations for five entitytypes are given: Protein, Two-component system,Chemical, Organism, or Regulon/Operon.2.3 Trigger IdentificationTriggers are identified using an SVM classifier (Vap-nik, 1995; Joachims, 1999).
Candidate triggers arechosen from the words in the text by part-of-speech.Based on known triggers seen in the training data, allnouns, verbs, adjectives, prepositions and adverbsare selected as candidates.
A binary model is trainedfor each event type, and candidate triggers are testedagainst each classifier.The following features are used to classify candi-date event triggers:?
term ?
the candidate trigger?
stem ?
the stem of the term?
part of speech ?
the part of speech of the term?
capitalization ?
capitalization of the term?
punctuation ?
individual features for the pres-ence of different punctuation types?
numerics ?
the presence of a number in theterm?
ngrams ?
4-grams of characters from the term?
known trigger types ?
tags from list of knowntrigger terms for each event type?
lexical context ?
terms in the same sentence?
syntactic dependencies ?
the type and role(governor or dependent) of typed dependenciesinvolving the trigger?
semantic type ?
type mapping from MetaMap?
semantic group ?
group mapping fromMetaMapFor training data, both the Infectious Diseasetraining set and the GENIA training set were used.Although the GENIA training set represents a dif-ferent genre and is annotated with a slightly differ-ent vocabulary than the Infectious Disease task data,it is similar enough to provide some beneficial su-pervision.
The Infectious Disease training data isrelatively small at 154 documents so including thelarger GENIA training set at 910 documents resultsin a much more larger training set.
Testing on theInfectious Disease development data, a 1 point im-provement in fscore in overall results is seen withthe additional training data.2.4 Argument IdentificationArguments are also identified using an SVM classi-fier.
For each predicted trigger, candidate argumentsare selected based on the argument types.
For ar-guments that are restricted to being filled by someset of specific entity and event types, each anno-tated entity and predicted event is selected as a can-didate.
For arguments that can be filled by any spanof text, each span corresponding to a constituent ofthe tree parse is selected as a candidate.
Each pairof an event trigger and a candidate argument servesas an instance for the classification.
A binary modelis trained for each event type, and each pair is testedagainst each classifier.Many of the features used are inspired by thoseused in semantic role labeling systems (Gildea andJurafsky, 2002).
Given an event trigger and a can-didate argument, the following features are used toclassify event arguments:?
trigger type ?
the predicted event type of thetrigger?
argument terms ?
the text of the argument?
argument type ?
entity or event type annota-tion on the argument?
argument super-type ?
core entity or core ar-gument?
trigger and argument stems ?
the stems ofeach?
trigger and argument parts of speech ?
thepart of speech of each?
parse tree path ?
from the trigger to argumentvia least common ancestor in tree parse, as alist of phrase types?
voice of sentence ?
active or passive?
trigger and argument partial paths ?
fromthe trigger or argument to the least common an-cestor in tree parse, as a list of phrase types132?
relative position of argument to trigger ?
be-fore or after?
trigger sub-categorization ?
representation ofthe phrase structure rule that describes the rela-tionship between the trigger, its parent and itssiblings.The training data used is the same as for trig-ger identification: the Infectious Disease training setplus the Genia training set.2.5 Post-processingA post-processing component is used to turn outputfrom the various classifiers into semantically validoutput according to the target task.
For each pre-dicted trigger, the positive predictions for each argu-ment model are collected, and the set is compared tothe argument restrictions in the target task descrip-tion.For example, the types on argument predictionsare compared to the argument restrictions in thetarget task, and non-conforming ones are dropped.Then the multiplicity of the arguments for each pre-dicted event is checked against the task vocabulary.Where there were not sufficient positive argumentpredictions to make a full event, the best negativepredictions from the model are tried.
When a com-pliant set of arguments can not be created for a pre-dicted event, it is dropped.3 Results and DiscussionResults for the system on both the development dataand the official test data for the task are shown inTable 2 and Table 5, respectively.
For the develop-ment data, a system using gold-standard event trig-gers is included, to isolate the performance of argu-ment identification.
In all cases, the total fscore fornon-regulation events were much higher than regula-tion events.
On the official test data, the system per-formed the best in predicting Phosphorylation (fs-core = 71.43), Gene Expression (fscore = 53.33) andProcess events (fscore = 51.04), but was unable tofind any Transcription and Regulation events.
Thisis also evident in the results on the development datausing predicted triggers; additionally, no matcheswere found for localization and binding events.
Thetotal fscore on the development data using gold trig-gers was 55.33, more than 13 points higher thanwhen using predicted triggers.
In the discussion thatfollows, we detail the importance of individual fea-tures and their contribution to evaluation fscores.3.1 Feature ImportanceThe effect of each argument and trigger feature typeon the Infectious Disease development data was de-termined using a leave-one-out approach.
The ar-gument and trigger feature effect results are shownin Table 3 and Table 4, respectively.
In a series ofexperiments, each feature type is left out of the fullfeature set one-by-one.
The difference in fscore be-tween each of these systems and the full feature setsystem is the effect of the feature type; a high nega-tive effect indicates a significant contribution to thesystem since the removal of the feature resulted in alower fscore.Features fscore effectall features 41.66w/o argument terms 36.16 -5.50w/o argument type 39.50 -2.16w/o trigger partial path 40.65 -1.01w/o argument part of speech 40.98 -0.68w/o argument partial path 41.16 -0.50w/o trigger sub-categorization 41.45 -0.21w/o argument stem 41.48 -0.18w/o argument super-type 41.63 -0.03w/o trigger type 41.63 -0.03w/o trigger part of speech 41.81 0.15w/o trigger stem 41.81 0.15w/o voice of sentence 41.85 0.19w/o relative position 42.21 0.55w/o parse tree path 42.67 1.01Table 3: Effect of each argument feature type on Infec-tious Disease development data.Within the argument feature set system, the parsetree path feature had a notable positive effect of1.01.
The features providing the greatest contribu-tion were argument terms and argument type witheffects of -5.50 and -2.16, respectively.
Within thetrigger feature set system, the lexical context andsyntactic dependencies features showed the highestnegative effect signifying positive contribution to thesystem.
The text and known trigger types featuresshowed a negative contribution to the system.133Using Gold Triggers Using Predicted TriggersEvent Class gold/ans./match recall prec.
fscore gold/ans./match recall prec.
fscoreGene expression 134 / 110 / 100 74.63 90.00 81.60 134 / 132 / 85 64.18 64.39 64.29Transcription 35 / 26 / 23 65.71 88.46 75.41 25 / 0 / 0 0.00 0.00 0.00Protein catabolism 0 / 0 / 0 0.00 0.00 0.00 0 / 0 / 0 0.00 0.00 0.00Phosphorylation 13 / 13 / 13 100.00 100.00 100.00 13 / 14 / 13 100.00 92.86 96.30Localization 1 / 1 / 0 0.00 0.00 0.00 1 / 10 / 0 0.00 0.00 0.00Binding 17 / 6 / 0 0.00 0.00 0.00 17 / 3 / 0 0.00 0.00 0.00Process 206 / 180 / 122 59.22 67.78 63.21 207 / 184 / 108 52.17 58.70 55.24Regulation 81 / 61 / 20 24.69 32.79 28.17 80 / 0 / 0 0.00 0.00 0.00Positive regulation 113 / 91 / 36 31.86 39.56 35.29 113 / 42 / 13 11.50 30.95 16.77Negative regulation 90 / 71 / 32 35.56 45.07 39.75 90 / 42 / 11 12.22 26.19 16.67TOTAL 690 / 559 / 346 50.14 61.72 55.33 680 / 427 / 230 33.97 53.86 41.66Table 2: Results on Infectious Disease development data.
The system is compared to a system using gold standardtriggers to isolate performance of argument identification.Features fscore effectall features 41.66w/o lexical context 40.14 -1.52w/o syntactic dependencies 40.28 -1.38w/o ngrams 40.88 -0.78w/o part of speech 41.48 -0.18w/o capitalization 41.51 -0.15w/o numerics 41.51 -0.15w/o semantic group 41.55 -0.11w/o punctuation 41.59 -0.07w/o stem 41.74 0.08w/o semantic type 41.82 0.16w/o known trigger types 42.11 0.45w/o text 42.31 0.65Table 4: Effect of each trigger feature type on InfectiousDisease development data.3.2 Transcription and Regulation eventsLastly, we present representative examples of errors(e.g., false positive, false negative, poor recall) pro-duced by our system in the Infectious Disease trackcore tasks.
The discussion herein will cover eval-uations where our system did not correctly predict(transcription and regulation) any events or partiallypredicted (binding and +/- regulation) event triggersand arguments.
In the text examples that follow, trig-gers are underlined and arguments are italicized.The following are transcription events from thedocument PMC1804205-02-Results-03 in the devel-opment data.?
In contrast to the phenotype of the pta ackAdouble mutant, pbgP transcription was reducedin the pmrD mutant (Fig.
3).?
Growth at pH 5.8 resulted in pmrDtranscript levels that were approximately3.5-fold higher than in organisms grown at pH 7.7(Fig.
4A).In both the development and test data evaluations,our system did not predict any transcription events,resulting in a 0.0 fscore; however, the systemachieved 75.41 fscore when the gold-standard trig-gers were provided to the evaluation.
Because ar-gument prediction performed well, the system willbenefit most by improving transcription event trig-ger prediction.The following are regulation events from the doc-ument PMC1804205-02-Results-01in the develop-ment data.?
.
.
.
we grew Salmonella cells harbouring chro-mosomal lacZYA transcriptional fusions to thePmrA-regulated genes pbgP, pmrC and ugd(Wosten and Groisman, 1999) in N-minimalmedia buffered at pH 5.8 or 7.7.?
We determined that Chelex 100 was effective atchelating iron because expression of the pmrA-independent iron-repressed iroA gene .
.
.Similar to the transcription task, our system did notpredict any regulation events, resulting in a 0.0 fs-core.
Unlike transcription events though, our sys-tem performed poorly on both argument identifica-tion and trigger prediction.
The system achieved a28.17 fscore when gold-standard triggers were used134Event Class gold (match) answer (match) recall prec.
fscoreGene expression 152 80 148 80 52.63 54.05 53.33Transcription 50 0 0 0 0.00 0.00 0.00Protein catabolism 5 1 12 1 20.00 8.33 11.76Phosphorylation 16 10 12 10 62.50 83.33 71.43Localization 7 4 22 4 57.14 18.18 27.59Binding 56 7 14 7 12.50 50.00 20.00Regulation 193 0 0 0 0.00 0.00 0.00Positive regulation 193 34 87 34 17.62 39.08 24.29Negative regulation 181 32 68 32 17.68 47.06 25.70Process 516 234 401 234 45.35 58.35 51.04TOTAL 1369 402 764 402 29.36 52.62 37.69Table 5: Official results on Infectious Disease test datain the evaluation.
Hypotheses for poor performanceon candidate argument prediction are addressed inthe following sections.We posit that false negative trigger identificationsare due to the limited full text training data (i.e.
tran-scription events) and the inability of our system topredict non-verb triggers (i.e.
second transcriptionexample above).
The SVM classifier was unableto distinguish between true transcription event trig-gers and transcription-related terms and ultimately,did not predict any transcription event in the devel-opment or test evaluations.
To improve transcrip-tion event prediction, immediate effort should fo-cus on 1) providing additional training data (e.g.,BioCreativec?iteBioCreative) and 2) introduce a trig-ger word filter that defines a subset of event triggersthat have the best hit rate in the corpus.
The hit rateis the number of occurrences of the word in a sen-tence per event type, divided by the total count in thegold standard (Nguyen et al, 2010).3.3 +/-Regulation and BindingThe following positive regulation event is from doc-ument PMC1874608-03-RESULTS-03 in the devel-opment data.?
Invasiveness for HEp-2 cells was reduced to39.1% of the wild-type level by mlc mutation,whereas it was increased by 1.57-fold by hilEmutation (Figure 3B).In the preceding example, our system correctlypredicted the +regulation trigger and the theme hilE;however, the correct argument was a gene expres-sion event, not the entity.
Many errors in the positiveand negative regulation events were of this type; thepredicted argument was a theme and not an event.Evaluation of our system?s binding event predic-tions resulted in low recall (12.50 or 0.0) in thetest and development evaluations.
The proceedingbinding events are from document PMC1874608-03-RESULTS-05 in the development data.
In bothof the examples, our system correctly predicted thetrigger binding; however, no arguments were pre-dicted.
Evaluation on the development data withgold standard triggers also resulted in an fscore of0.0; thus, further algorithm refinement is needed toimprove binding scores.?
Mlc directly represses hilE by binding to the P3promoter?
These results clearly demonstrate that Mlccan regulate directly the hilE P3 promoter bybinding to the promoter.The following binding event is from documentPMC1874608-01-INTRODUCTION in the devel-opment data and is representative of errors acrossmany of the tasks.
Here, the trigger is correctly pre-dicted; however, the candidate arguments did notmatch with the reference data.
Upon closer look,the arguments were drawn from the entire sentence,rather than an independent clause.
The syntacticparse feature was not sufficient to prevent over-predicting arguments for the trigger, a potential so-lution is to add the arguments syntactic dependency135to the trigger as a feature to the candidate argumentselection.?
Using two-hybrid analysis, it has been shownthat HilE interacts with HilD, which suggeststhat HilE represses hilA expression by inhibit-ing the activity of HilD through a protein-protein interaction (19,20).4 SummaryThis article reports Pacific Northwest National Lab-oratory?s entry to the BioNLP Shared Task 2011 In-fectious Disease track competition.
Our system usesa signature-based machine-learning approach incor-porating traditional linguistic features and shallowsemantic concepts from NIH?s METAMAP The-saurus.
We examine the contribution of each ofthe linguistic and semantic features to the over-all fscore for our system.
This approach performswell on gene expression, process and phosphoryla-tion event prediction.
Transcription, regulation andbinding events each achieve low fscores and war-rant further research to improve their effectiveness.Lastly, we present a performance analysis of thetranscription, regulation and binding tasks.
Futurework to improve our system?s performance could in-clude pre-processing using simple patterns (Nguyenet al, 2010), information extraction from figure cap-tions (Kim and Yu, 2011) and text-to-text event ex-traction.
The last suggested improvement is to addsemantic features to the candidate argument predic-tion algorithm in addition to using rich features, suchas semantic roles (Torii et al, 2011).AcknowledgementsThe authors thank the Signature Discovery Initia-tive, part of the Laboratory Directed Research andDevelopment Program at Pacific Northwest NationalLaboratory (PNNL).
PNNL is operated by Battellefor the U.S. Department of Energy under contractDE-ACO5-76RLO 1830.ReferencesAlan R Aronson and Franc?ois-Michel Lang.
2010.
Anoverview of metamap: historical perspective and re-cent advances.
J AmMed Inform Assoc, 17(3):229?36,May.J Bjorne, F Ginter, S Pyysalo, J Tsujii, and T Salakoski.2010.
Complex event extraction at pubmed scale.Bioinformatics, 26(12):i382?i390, Jun.O.
Bodenreider.
2004.
The unified medical languagesystem (UMLS): integrating biomedical terminology.Nucleic acids research, 32(suppl 1):D267.M.C.
deMarneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of LREC 2006.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.T.
Joachims.
1999.
Making large scale SVM learningpractical.
Advances in Kernel Methods ?
Support Vec-tor Learning.Daehyun Kim and Hong Yu.
2011.
Figure text extractionin biomedical literature.
PLoS ONE, 6(1):e15338, Jan.JD Kim, T Ohta, S Pyysalo, Y Kano, and J Tsujii.
2009.Overview of bionlp?09 shared task on event extraction.Proceedings of the Workshop on BioNLP: Shared Task,pages 1?9.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, and Jun?ichi Tsujii.
2011.
Overview ofBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.D.
Klein and C.D.
Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of the 41st Annual Meet-ing on Association for Computational Linguistics-Volume 1, pages 423?430.
Association for Computa-tional Linguistics.A MacKinlay, D Martinez, and T Baldwin.
2009.Biomedical event annotation with crfs and precisiongrammars.
Proceedings of the Workshop on BioNLP:Shared Task, pages 77?85.Makoto Miwa, Rune Saetre, Jin-Dong Kim, and Jun?ichiTsujii.
2010.
Event extraction with complex eventclassification using rich features.
J. Bioinform.
Com-put.
Biol., 8(1):131?46, Feb.Quang Long Nguyen, Domonkos Tikk, and Ulf Leser.2010.
Simple tricks for improving pattern-based in-formation extraction from the biomedical literature.
JBiomed Semantics, 1(1):9, Jan.S.
Pyysalo, T. Ohta, H.C. Cho, D. Sullivan, C. Mao,B.
Sobral, J. Tsujii, and S. Ananiadou.
2010.
Towardsevent extraction from full texts on infectious diseases.In Proceedings of the 2010 Workshop on BiomedicalNatural Language Processing, pages 132?140.
Asso-ciation for Computational Linguistics.Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-livan, Chunhong Mao, Chunxia Wang, Bruno So-bral, Jun?ichi Tsujii, and Sophia Ananiadou.
2011.136Overview of the Infectious Diseases (ID) task ofBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, TomokoOhta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011.BioNLP Shared Task 2011: Supporting Resources.
InProceedings of the BioNLP 2011 Workshop Compan-ion Volume for Shared Task, Portland, Oregon, June.Association for Computational Linguistics.Manabu Torii, Lanlan Yin, Thang Nguyen, Chand TMazumdar, Hongfang Liu, David M Hartley, andNoele P Nelson.
2011.
An exploratory study of a textclassification framework for internet-based surveil-lance of emerging epidemics.
International Journalof Medical Informatics, 80(1):56?66, Jan.C.J.
van Rijsbergen, S.E.
Robertson, and M.F.
Porter.1980.
New models in probabilistic information re-trieval.V.
Vapnik.
1995.
The Nature of Statistical Learning The-ory.
Springer, New York.137
