Proceedings of NAACL HLT 2007, pages 300?307,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsMultiple Aspect Ranking using the Good Grief AlgorithmBenjamin Snyder and Regina BarzilayComputer Science and Articial Intelligence LaboratoryMassachusetts Institute of Technology{bsnyder,regina}@csail.mit.eduAbstractWe address the problem of analyzing mul-tiple related opinions in a text.
For in-stance, in a restaurant review such opin-ions may include food, ambience and ser-vice.
We formulate this task as a multipleaspect ranking problem, where the goal isto produce a set of numerical scores, onefor each aspect.
We present an algorithmthat jointly learns ranking models for in-dividual aspects by modeling the depen-dencies between assigned ranks.
This al-gorithm guides the prediction of individ-ual rankers by analyzing meta-relationsbetween opinions, such as agreement andcontrast.
We prove that our agreement-based joint model is more expressive thanindividual ranking models.
Our empiricalresults further confirm the strength of themodel: the algorithm provides significantimprovement over both individual rankersand a state-of-the-art joint ranking model.1 IntroductionPrevious work on sentiment categorization makes animplicit assumption that a single score can expressthe polarity of an opinion text (Pang et al, 2002;Turney, 2002; Yu and Hatzivassiloglou, 2003).However, multiple opinions on related matters areoften intertwined throughout a text.
For example,a restaurant review may express judgment on foodquality as well as the service and ambience of therestaurant.
Rather than lumping these aspects into asingle score, we would like to capture each aspect ofthe writer?s opinion separately, thereby providing amore fine-grained view of opinions in the review.To this end, we aim to predict a set of numericranks that reflects the user?s satisfaction for each as-pect.
In the example above, we would assign a nu-meric rank from 1-5 for each of: food quality, ser-vice, and ambience.A straightforward approach to this task would beto rank1 the text independently for each aspect, us-ing standard ranking techniques such as regressionor classification.
However, this approach fails to ex-ploit meaningful dependencies between users?
judg-ments across different aspects.
Knowledge of thesedependencies can be crucial in predicting accurateranks, as a user?s opinions on one aspect can influ-ence his or her opinions on others.The algorithm presented in this paper modelsthe dependencies between different labels via theagreement relation.
The agreement relation captureswhether the user equally likes all aspects of the itemor whether he or she expresses different degrees ofsatisfaction.
Since this relation can often be deter-mined automatically for a given text (Marcu andEchihabi, 2002), we can readily use it to improverank prediction.The Good Grief model consists of a rankingmodel for each aspect as well as an agreement modelwhich predicts whether or not all rank aspects are1In this paper, ranking refers to the task of assigning an inte-ger from 1 to k to each instance.
This task is sometimes referredto as ?ordinal regression?
(Crammer and Singer, 2001) and ?rat-ing prediction?
(Pang and Lee, 2005).300equal.
The Good Grief decoding algorithm pre-dicts a set of ranks ?
one for each aspect ?
whichmaximally satisfy the preferences of the individualrankers and the agreement model.
For example, ifthe agreement model predicts consensus but the in-dividual rankers select ranks ?5, 5, 4?, then the de-coder decides whether to trust the the third ranker,or alter its prediction and output ?5, 5, 5?
to be con-sistent with the agreement prediction.
To obtain amodel well-suited for this decoding, we also developa joint training method that conjoins the training ofmultiple aspect models.We demonstrate that the agreement-based jointmodel is more expressive than individual rankingmodels.
That is, every training set that can be per-fectly ranked by individual ranking models for eachaspect can also be perfectly ranked with our jointmodel.
In addition, we give a simple example of atraining set which cannot be perfectly ranked with-out agreement-based joint inference.
Our experi-mental results further confirm the strength of theGood Grief model.
Our model significantly outper-forms individual ranking models as well as a state-of-the-art joint ranking model.2 Related WorkSentiment Classification Traditionally, categoriza-tion of opinion texts has been cast as a binary classi-fication task (Pang et al, 2002; Turney, 2002; Yu andHatzivassiloglou, 2003; Dave et al, 2003).
Morerecent work (Pang and Lee, 2005; Goldberg andZhu, 2006) has expanded this analysis to the rank-ing framework where the goal is to assess reviewpolarity on a multi-point scale.
While this approachprovides a richer representation of a single opinion,it still operates on the assumption of one opinion pertext.
Our work generalizes this setting to the prob-lem of analyzing multiple opinions ?
or multiple as-pects of an opinion.
Since multiple opinions in a sin-gle text are related, it is insufficient to treat them asseparate single-aspect ranking tasks.
This motivatesour exploration of a new method for joint multipleaspect ranking.Ranking The ranking, or ordinal regression,problem has been extensivly studied in the MachineLearning and Information Retrieval communities.
Inthis section we focus on two online ranking methodswhich form the basis of our approach.
The first isa model proposed by Crammer and Singer (2001).The task is to predict a rank y ?
{1, ..., k} for ev-ery input x ?
Rn.
Their model stores a weightvector w ?
Rn and a vector of increasing bound-aries b0 = ??
?
b1 ?
... ?
bk?1 ?
bk = ?which divide the real line into k segments, one foreach possible rank.
The model first scores each inputwith the weight vector: score(x) = w ?
x. Finally,the model locates score(x) on the real line and re-turns the appropriate rank as indicated by the bound-aries.
Formally, the model returns the rank r suchthat br?1 ?
score(x) < br.
The model is trainedwith the Perceptron Ranking algorithm (or ?PRankalgorithm?
), which reacts to incorrect predictions onthe training set by updating the weight and boundaryvectors.
The PRanking model and algorithm weretested on the EachMovie dataset with a separateranking model learned for each user in the database.An extension of this model is provided by Basil-ico and Hofmann (2004) in the context of collabora-tive filtering.
Instead of training a separate model foreach user, Basilico and Hofmann train a joint rank-ing model which shares a set of boundaries across allusers.
In addition to these shared boundaries, user-specific weight vectors are stored.
To compute thescore for input x and user i, the weight vectors forall users are employed:scorei(x) = w[i] ?x +?jsim(i, j)(w[j] ?x) (1)where 0 ?
sim(i, j) ?
1 is the cosine similarity be-tween users i and j, computed on the entire trainingset.
Once the score has been computed, the predic-tion rule follows that of the PRanking model.
Themodel is trained using the PRank algorithm, with theexception of the new definition for the scoring func-tion.2 While this model shares information betweenthe different ranking problems, it fails to explicitlymodel relations between the rank predictions.
Incontrast, our algorithm uses an agreement model tolearn such relations and inform joint predictions.2In the notation of Basilico and Hofmann (2004), this def-inition of scorei(x) corresponds to the kernel K = (KidU +KcoU )?KatX .3013 The AlgorithmThe goal of our algorithm is to find a rank assign-ment that is consistent with predictions of individ-ual rankers and the agreement model.
To this end,we develop the Good Grief decoding procedure thatminimizes the dissatisfaction (grief ) of individualcomponents with a joint prediction.
In this section,we formally define the grief of each component, anda mechanism for its minimization.
We then describeour method for joint training of individual rankersthat takes into account the Good Grief decoding pro-cedure.3.1 Problem FormulationIn an m-aspect ranking problem, we are givena training sequence of instance-label pairs(x1,y1), ..., (xt,yt), .... Each instance xt is afeature vector in Rn and the label yt is a vector ofm ranks in Ym, where Y = {1, .., k} is the set ofpossible ranks.
The ith component of yt is the rankfor the ith aspect, and will be denoted by y[i]t. Thegoal is to learn a mapping from instances to ranksets, H : X ?
Ym, which minimizes the distancebetween predicted ranks and true ranks.3.2 The ModelOur m-aspect ranking model containsm+1 compo-nents: (?w[1],b[1]?, ..., ?w[m],b[m]?,a).
The firstm components are individual ranking models, onefor each aspect, and the final component is the agree-ment model.
For each aspect i ?
1...m, w[i] ?
Rnis a vector of weights on the input features, andb[i] ?
Rk?1 is a vector of boundaries which dividethe real line into k intervals, corresponding to thek possible ranks.
The default prediction of the as-pect ranking model simply uses the ranking rule ofthe PRank algorithm.
This rule predicts the rank rsuch that b[i]r?1 ?
scorei(x) < b[i]r.3 The valuescorei(x) can be defined simply as the dot productw[i]?x, or it can take into account the weight vectorsfor other aspects weighted by a measure of inter-aspect similarity.
We adopt the definition given inequation 1, replacing the user-specific weight vec-tors with our aspect-specific weight vectors.3More precisely (taking into account the possibility of ties):y?
[i] = minr?
{1,..,k}{r : scorei(x)?
b[i]r < 0}The agreement model is a vector of weights a ?Rn.
A value of a ?
x > 0 predicts that the ranks ofall m aspects are equal, and a value of a ?
x ?
0indicates disagreement.
The absolute value |a ?
x|indicates the confidence in the agreement prediction.The goal of the decoding procedure is to predict ajoint rank for the m aspects which satisfies the in-dividual ranking models as well as the agreementmodel.
For a given input x, the individual modelfor aspect i predicts a default rank y?
[i] based on itsfeature weight and boundary vectors ?w[i],b[i]?.
Inaddition, the agreement model makes a predictionregarding rank consensus based on a ?
x. However,the default aspect predictions y?
[1] .
.
.
y?
[m] may notaccord with the agreement model.
For example, ifa ?x > 0, but y?
[i] 6= y?
[j] for some i, j ?
1...m, thenthe agreement model predicts complete consensus,whereas the individual aspect models do not.We therefore adopt a joint prediction criterionwhich simultaneously takes into account all modelcomponents ?
individual aspect models as well asthe agreement model.
For each possible predic-tion r = (r[1], ..., r[m]) this criterion assesses thelevel of grief associated with the ith-aspect rankingmodel, gi(x, r[i]).
Similarly, we compute the griefof the agreement model with the joint prediction,ga(x, r) (both gi and ga are defined formally below).The decoder then predicts the m ranks which mini-mize the overall grief:H(x) = arg minr?Ym[ga(x, r) +m?i=1gi(x, r[i])](2)If the default rank predictions for the aspect models,y?
= (y?
[1], ..., y?
[m]), are in accord with the agree-ment model (both indicating consensus or both in-dicating contrast), then the grief of all model com-ponents will be zero, and we simply output y?.
Onthe other hand, if y?
indicates disagreement but theagreement model predicts consensus, then we havethe option of predicting y?
and bearing the grief ofthe agreement model.
Alternatively, we can predictsome consensus y?
(i.e.
with y?
[i] = y?
[j], ?i, j) andbear the grief of the component ranking models.
Thedecoder H chooses the option with lowest overallgrief.44This decoding criterion assumes that the griefs of the com-302Now we formally define the measures of griefused in this criterion.Aspect Model Grief We define the grief of the ith-aspect ranking model with respect to a rank r to bethe smallest magnitude correction term which placesthe input?s score into the rth segment of the real line:gi(x, r) = min |c|s.t.b[i]r?1 ?
scorei(x) + c < b[i]rAgreement Model Grief Similarly, we define thegrief of the agreement model with respect to a jointrank r = (r[1], .
.
.
, r[m]) as the smallest correctionneeded to bring the agreement score into accord withthe agreement relation between the individual ranksr[1], .
.
.
, r[m]:ga(x, r) = min |c|s.t.a ?
x + c > 0 ?
?i, j ?
1...m : r[i] = r[j]?a ?
x + c ?
0 ?
?i, j ?
1...m : r[i] 6= r[j]3.3 TrainingRanking models Pseudo-code for Good Grief train-ing is shown in Figure 1.
This training algorithmis based on PRanking (Crammer and Singer, 2001),an online perceptron algorithm.
The training is per-formed by iteratively ranking each training input xand updating the model.
If the predicted rank y?
isequal to the true rank y, the weight and boundariesvectors remain unchanged.
On the other hand, ify?
6= y, then the weights and boundaries are updatedto improve the prediction for x (step 4.c in Figure 1).See (Crammer and Singer, 2001) for explanationand analysis of this update rule.Our algorithm departs from PRanking by con-joining the updates for the m ranking models.
Weachieve this by using Good Grief decoding at eachstep throughout training.
Our decoder H(x) (fromequation 2) uses all the aspect component modelsponent models are comparable.
In practice, we take an uncali-brated agreement model a?
and reweight it with a tuning param-eter: a = ?a?.
The value of ?
is estimated using the develop-ment set.
We assume that the griefs of the ranking models arecomparable since they are jointly trained.as well as the (previously trained) agreement modelto determine the predicted rank for each aspect.
Inconcrete terms, for every training instance x, we pre-dict the ranks of all aspects simultaneously (step 2 inFigure 1).
Then, for each aspect we make a separateupdate based on this joint prediction (step 4 in Fig-ure 1), instead of using the individual models?
pre-dictions.Agreement model The agreement model a is as-sumed to have been previously trained on the sametraining data.
An instance is labeled with a positivelabel if all the ranks associated with this instance areequal.
The rest of the instances are labeled as nega-tive.
This model can use any standard training algo-rithm for binary classification such as Perceptron orSVM optimization.3.4 Feature RepresentationRanking Models Following previous work on senti-ment classification (Pang et al, 2002), we representeach review as a vector of lexical features.
Morespecifically, we extract all unigrams and bigrams,discarding those that appear fewer than three times.This process yields about 30,000 features.Agreement Model The agreement model also op-erates over lexicalized features.
The effectivenessof these features for recognition of discourse rela-tions has been previously shown by Marcu and Echi-habi (2002).
In addition to unigrams and bigrams,we also introduce a feature that measures the maxi-mum contrastive distance between pairs of words ina review.
For example, the presence of ?delicious?and ?dirty?
indicate high contrast, whereas the pair?expensive?
and ?slow?
indicate low contrast.
Thecontrastive distance for a pair of words is computedby considering the difference in relative weight as-signed to the words in individually trained PRankingmodels.4 AnalysisIn this section, we prove that our model is able toperfectly rank a strict superset of the training cor-pora perfectly rankable by m ranking models indi-vidually.
We first show that if the independent rank-ing models can individually rank a training set per-fectly, then our model can do so as well.
Next, weshow that our model is more expressive by providing303Input : (x1,y1), ..., (xT ,yT ), Agreement model a, Decoder defintion H(x) (from equation 2).Initialize : Set w[i]1 = 0, b[i]11, ..., b[i]1k?1 = 0, b[i]1k =?, ?i ?
1...m.Loop : For t = 1, 2, ..., T :1.
Get a new instance xt ?
Rn.2.
Predict y?t = H(x; wt,bt,a) (Equation 2).3.
Get a new label yt.4.
For aspect i = 1, ...,m:If y?
[i]t 6= y[i]t update model (otherwise set w[i]t+1 = w[i]t, b[i]t+1r = b[i]tr, ?r):4.a For r = 1, ..., k ?
1 : If y[i]t ?
r then y[i]tr = ?1else y[i]tr = 1.4.b For r = 1, ..., k ?
1 : If (y?
[i]t ?
r)y[i]tr ?
0 then ?
[i]tr = y[i]trelse ?
[i]tr = 0.4.c Update w[i]t+1 ?
w[i]t + (?r ?
[i]tr)xt.For r = 1, ..., k ?
1 update : b[i]t+1r ?
b[i]tr ?
?
[i]tr.Output : H(x; wT+1,bT+1,a).Figure 1: Good Grief Training.
The algorithm is based on PRanking training algorithm.
Our algorithmdiffers in the joint computation of all aspect predictions y?t based on the Good Grief Criterion (step 2) andthe calculation of updates for each aspect based on the joint prediction (step 4).a simple illustrative example of a training set whichcan only be perfectly ranked with the inclusion of anagreement model.First we introduce some notation.
For each train-ing instance (xt,yt), each aspect i ?
1...m, andeach rank r ?
1...k, define an auxiliary variabley[i]tr with y[i]tr = ?1 if y[i]t ?
r and y[i]tr = 1if y[i]t > r. In words, y[i]tr indicates whether thetrue rank y[i]t is to the right or left of a potentialrank r.Now suppose that a training set(x1,y1), ..., (xT ,yT ) is perfectly rankable foreach aspect independently.
That is, for eachaspect i ?
1...m, there exists some ideal modelv[i]?
= (w[i]?, b[i]?)
such that the signed dis-tance from the prediction to the rth boundary:w[i]?
?
xt ?
b[i]?r has the same sign as the auxil-iary variable y[i]tr.
In other words, the minimummargin over all training instances and ranks,?
= minr,t{(w[i]?
?xt?
b[i]?r)y[i]tr}, is no less thanzero.Now for the tth training instance, define an agree-ment auxiliary variable at, where at = 1 when allaspects agree in rank and at = ?1 when at leasttwo aspects disagree in rank.
First consider the casewhere the agreement model a perfectly classifies alltraining instances: (a ?
xt)at > 0, ?t.
It is clearthat Good Grief decoding with the ideal joint model(?w[1]?,b[1]?
?, ..., ?w[m]?,b[m]?
?,a) will producethe same output as the component ranking modelsrun separately (since the grief will always be zero forthe default rank predictions).
Now consider the casewhere the training data is not linearly separable withregard to agreement classification.
Define the mar-gin of the worst case error to be ?
= maxt{|(a?xt)| :(a?xt)at < 0}.
If ?
< ?, then again Good Grief de-coding will always produce the default results (sincethe grief of the agreement model will be at most ?
incases of error, whereas the grief of the ranking mod-els for any deviation from their default predictionswill be at least ?).
On the other hand, if ?
?
?, thenthe agreement model errors could potentially disruptthe perfect ranking.
However, we need only rescalew?
:= w?(??
+ ?)
and b?
:= b?(??
+ ?)
to ensure thatthe grief of the ranking models will always exceedthe grief of the agreement model in cases where thelatter is in error.
Thus whenever independent rank-ing models can perfectly rank a training set, a jointranking model with Good Grief decoding can do soas well.Now we give a simple example of a training setwhich can only be perfectly ranked with the addi-tion of an agreement model.
Consider a training setof four instances with two rank aspects:304?x1,y1?
= ?
(1, 0, 1), (2, 1)??x2,y2?
= ?
(1, 0, 0), (2, 2)??x3,y3?
= ?
(0, 1, 1), (1, 2)??x4,y4?
= ?
(0, 1, 0), (1, 1)?We can interpret these inputs as feature vectors cor-responding to the presence of ?good?, ?bad?, and?but not?
in the following four sentences:The food was good, but not the ambience.The food was good, and so was the ambience.The food was bad, but not the ambience.The food was bad, and so was the ambience.We can further interpret the first rank aspect as thequality of food, and the second as the quality of theambience, both on a scale of 1-2.A simple ranking model which only considers thewords ?good?
and ?bad?
perfectly ranks the food as-pect.
However, it is easy to see that no single modelperfectly ranks the ambience aspect.
Consider anymodel ?w,b = (b)?.
Note that w ?
x1 < b andw ?
x2 ?
b together imply that w3 < 0, whereasw ?
x3 ?
b and w ?
x4 < b together imply thatw3 > 0.
Thus independent ranking models cannotperfectly rank this corpus.The addition of an agreement model, however,can easily yield a perfect ranking.
With a =(0, 0,?5) (which predicts contrast with the presenceof the words ?but not?)
and a ranking model for theambience aspect such as w = (1,?1, 0),b = (0),the Good Grief decoder will produce a perfect rank.5 Experimental Set-UpWe evaluate our multi-aspect ranking algorithm on acorpus5 of restaurant reviews available on the web-site http://www.we8there.com.
Reviewsfrom this website have been previously used in othersentiment analysis tasks (Higashinaka et al, 2006).Each review is accompanied by a set of five ranks,each on a scale of 1-5, covering food, ambience, ser-vice, value, and overall experience.
These ranks areprovided by consumers who wrote original reviews.Our corpus does not contain incomplete data pointssince all the reviews available on this website con-tain both a review text and the values for all the fiveaspects.Training and Testing Division Our corpus con-5Data and code used in this paper are available athttp://people.csail.mit.edu/bsnyder/naacl07tains 4,488 reviews, averaging 115 words.
We ran-domly select 3,488 reviews for training, 500 for de-velopment and 500 for testing.Parameter Tuning We used the development setto determine optimal numbers of training iterationsfor our model and for the baseline models.
Also,given an initial uncalibrated agreement model a?, wedefine our agreement model to be a = ?a?
for anappropriate scaling factor ?.
We tune the value of ?on the development set.Corpus Statistics Our training corpus contains528 among 55 = 3025 possible rank sets.
The mostfrequent rank set ?5, 5, 5, 5, 5?
accounts for 30.5%of the training set.
However, no other rank set com-prises more than 5% of the data.
To cover 90% ofoccurrences in the training set, 227 rank sets are re-quired.
Therefore, treating a rank tuple as a singlelabel is not a viable option for this task.
We alsofind that reviews with full agreement across rank as-pects are quite common in our corpus, accountingfor 38% of the training data.
Thus an agreement-based approach is natural and relevant.A rank of 5 is the most common rank for all as-pects and thus a prediction of all 5?s gives a MAJOR-ITY baseline and a natural indication of task diffi-culty.Evaluation Measures We evaluate our algorithmand the baseline using ranking loss (Crammer andSinger, 2001; Basilico and Hofmann, 2004).
Rank-ing loss measures the average distance betweenthe true rank and the predicted rank.
Formally,given N test instances (x1,y1), ..., (xN ,yN ) of anm-aspect ranking problem and the correspondingpredictions y?1, ..., y?N , ranking loss is defined as?t,i|y[i]t?y?
[i]t|mN .
Lower values of this measure cor-respond to a better performance of the algorithm.6 ResultsComparison with Baselines Table 1 shows the per-formance of the Good Grief training algorithm GGTRAIN+DECODE along with various baselines, in-cluding the simple MAJORITY baseline mentionedin section 5.
The first competitive baseline, PRANK,learns a separate ranker for each aspect using thePRank algorithm.
The second competitive baseline,SIM, shares the weight vectors across aspects usinga similarity measure (Basilico and Hofmann, 2004).305Food Service Value Atmosphere Experience TotalMAJORITY 0.848 1.056 1.030 1.044 1.028 1.001PRANK 0.606 0.676 0.700 0.776 0.618 0.675SIM 0.562 0.648 0.706 0.798 0.600 0.663GG DECODE 0.544 0.648 0.704 0.798 0.584 0.656GG TRAIN+DECODE 0.534 0.622 0.644 0.774 0.584 0.632GG ORACLE 0.510 0.578 0.674 0.694 0.518 0.595Table 1: Ranking loss on the test set for variants of Good Grief and various baselines.Figure 2: Rank loss for our algorithm and baselinesas a function of training round.Both of these methods are described in detail in Sec-tion 2.
In addition, we consider two variants of ouralgorithm: GG DECODE employs the PRank train-ing algorithm to independently train all componentranking models and only applies Good Grief decod-ing at test time.
GG ORACLE uses Good Grief train-ing and decoding but in both cases is given perfectknowledge of whether or not the true ranks all agree(instead of using the trained agreement model).Our model achieves a rank error of 0.632, com-pared to 0.675 for PRANK and 0.663 for SIM.
Bothof these differences are statistically significant atp < 0.002 by a Fisher Sign Test.
The gain in perfor-mance is observed across all five aspects.
Our modelalso yields significant improvement (p < 0.05) overthe decoding-only variant GG DECODE, confirm-ing the importance of joint training.
As shown inFigure 2, our model demonstrates consistent im-provement over the baselines across all the trainingrounds.Model Analysis We separately analyze our per-Consensus Non-consensusPRANK 0.414 0.864GG TRAIN+DECODE 0.324 0.854GG ORACLE 0.281 0.830Table 2: Ranking loss for our model and PRANKcomputed separately on cases of actual consensusand actual disagreement.formance on the 210 test instances where all thetarget ranks agree and the remaining 290 instanceswhere there is some contrast.
As Table 2 shows, weoutperform the PRANK baseline in both cases.
How-ever on the consensus instances we achieve a relativereduction in error of 21.8% compared to only a 1.1%reduction for the other set.
In cases of consensus,the agreement model can guide the ranking modelsby reducing the decision space to five rank sets.
Incases of disagreement, however, our model does notprovide sufficient constraints as the vast majority ofranking sets remain viable.
This explains the perfor-mance of GG ORACLE, the variant of our algorithmwith perfect knowledge of agreement/disagreementfacts.
As shown in Table 1, GG ORACLE yields sub-stantial improvement over our algorithm, but mostof this gain comes from consensus instances (see Ta-ble 2).We also examine the impact of the agreementmodel accuracy on our algorithm.
The agreementmodel, when considered on its own, achieves clas-sification accuracy of 67% on the test set, comparedto a majority baseline of 58%.
However, those in-stances with high confidence |a ?
x| exhibit substan-tially higher classification accuracy.
Figure 3 showsthe performance of the agreement model as a func-tion of the confidence value.
The 10% of the datawith highest confidence values can be classified by306Figure 3: Accuracy of the agreement model on sub-sets of test instances with highest confidence |a ?
x|.the agreement model with 90% accuracy, and thethird of the data with highest confidence can be clas-sified at 80% accuracy.This property explains why the agreement modelhelps in joint ranking even though its overall accu-racy may seem low.
Under the Good Grief criterion,the agreement model?s prediction will only be en-forced when its grief outweighs that of the rankingmodels.
Thus in cases where the prediction confi-dence (|a?x|) is relatively low,6 the agreement modelwill essentially be ignored.7 Conclusion and Future WorkWe considered the problem of analyzing multiple re-lated aspects of user reviews.
The algorithm pre-sented jointly learns ranking models for individualaspects by modeling the dependencies between as-signed ranks.
The strength of our algorithm liesin its ability to guide the prediction of individualrankers using rhetorical relations between aspectssuch as agreement and contrast.
Our method yieldssignificant empirical improvements over individualrankers as well as a state-of-the-art joint rankingmodel.Our current model employs a single rhetorical re-lation ?
agreement vs. contrast ?
to model depen-dencies between different opinions.
As our analy-6What counts as ?relatively low?
will depend on both thevalue of the tuning parameter ?
and the confidence of the com-ponent ranking models for a particular input x.sis shows, this relation does not provide sufficientconstraints for non-consensus instances.
An avenuefor future research is to consider the impact of addi-tional rhetorical relations between aspects.
We alsoplan to theoretically analyze the convergence prop-erties of this and other joint perceptron algorithms.AcknowledgmentsThe authors acknowledge the support of the National Sci-ence Foundation (CAREER grant IIS-0448168 and grant IIS-0415865) and the Microsoft Research Faculty Fellowship.Thanks to Michael Collins, Pawan Deshpande, Jacob Eisen-stein, Igor Malioutov, Luke Zettlemoyer, and the anonymousreviewers for helpful comments and suggestions.
Thanks alsoto Vasumathi Raman for programming assistance.
Any opin-ions, findings, and conclusions or recommendations expressedabove are those of the authors and do not necessarily reflect theviews of the NSF.ReferencesJ.
Basilico, T. Hofmann.
2004.
Unifying collabora-tive and content-based filtering.
In Proceedings of theICML, 65?72.K.
Crammer, Y.
Singer.
2001.
Pranking with ranking.
InNIPS, 641?647.K.
Dave, S. Lawrence, D. Pennock.
2003.
Miningthe peanut gallery: Opinion extraction and semanticclassification of product reviews.
In Proceedings ofWWW, 519?528.A.
B. Goldberg, X. Zhu.
2006.
Seeing stars when therearen?t many stars: Graph-based semi-supervised learn-ing for sentiment categorization.
In Proceedings ofHLT/NAACL workshop on TextGraphs, 45?52.R.
Higashinaka, R. Prasad, M. Walker.
2006.
Learn-ing to generate naturalistic utterances using reviewsin spoken dialogue systems.
In Proceedings of COL-ING/ACL, 265?272.D.
Marcu, A. Echihabi.
2002.
An unsupervised approachto recognizing discourse relations.
In Proceedings ofACL, 368?375.B.
Pang, L. Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with respectto rating scales.
In Proceedings of the ACL, 115?124.B.
Pang, L. Lee, S. Vaithyanathan.
2002.
Thumbs up?sentiment classification using machine learning tech-niques.
In Proceedings of EMNLP, 79?86.P.
Turney.
2002.
Thumbs up or thumbs down?
semanticorientation applied to unsupervised classsification ofreviews.
In Proceedings of the ACL, 417?424.H.
Yu, V. Hatzivassiloglou.
2003.
Towards answeringopinion questions: Separating facts from opinions andidentifying the polarity of opinion sentences.
In Pro-ceedings of EMNLP, 129?136.307
