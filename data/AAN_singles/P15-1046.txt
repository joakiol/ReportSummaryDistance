Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 473?482,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsNew Transfer Learning Techniques for Disparate Label SetsYoung-Bum Kim?Karl Stratos?Ruhi Sarikaya?Minwoo Jeong?
?Microsoft Corporation, Redmond, WA?Columbia University, New York, NY{ybkim, ruhi.sarikaya, minwoo.jeong}@microsoft.comstratos@cs.columbia.eduAbstractIn natural language understanding (NLU),a user utterance can be labeled differentlydepending on the domain or application(e.g., weather vs. calendar).
Standarddomain adaptation techniques are not di-rectly applicable to take advantage of theexisting annotations because they assumethat the label set is invariant.
We proposea solution based on label embeddings in-duced from canonical correlation analysis(CCA) that reduces the problem to a stan-dard domain adaptation task and allowsuse of a number of transfer learning tech-niques.
We also introduce a new trans-fer learning technique based on pretrain-ing of hidden-unit CRFs (HUCRFs).
Weperform extensive experiments on slot tag-ging on eight personal digital assistant do-mains and demonstrate that the proposedmethods are superior to strong baselines.1 IntroductionThe main goal of NLU is to automatically extractthe meaning of spoken or typed queries.
In recentyears, this task has become increasingly impor-tant as more and more speech-based applicationshave emerged.
Recent releases of personal dig-ital assistants such as Siri, Google Now, DragonGo and Cortana in smart phones provide natu-ral language based interface for a variety of do-mains (e.g.
places, weather, communications, re-minders).
The NLU in these domains are basedon statistical machine learned models which re-quire annotated training data.
Typically each do-main has its own schema to annotate the words andqueries.
However the meaning of words and utter-ances could be different in each domain.
For ex-ample, ?sunny?
is considered a weather conditionin the weather domain but it may be a song title ina music domain.
Thus every time a new applica-tion is developed or a new domain is built, a sig-nificant amount of resources is invested in creatingannotations specific to that application or domain.One might attempt to apply existing techniques(Blitzer et al, 2006; Daum?e III, 2007) in domainadaption to this problem, but a straightforward ap-plication is not possible because these techniquesassume that the label set is invariant.In this work, we provide a simple and effec-tive solution to this problem by abstracting the la-bel types using the canonical correlation analysis(CCA) by Hotelling (Hotelling, 1936) a powerfuland flexible statistical technique for dimensional-ity reduction.
We derive a low dimensional rep-resentation for each label type that is maximallycorrelated to the average context of that label viaCCA.
These shared label representations, or labelembeddings, allow us to map label types acrossdifferent domains and reduce the setting to a stan-dard domain adaptation problem.
After the map-ping, we can apply the standard transfer learningtechniques to solve the problem.Additionally, we introduce a novel pretrainingtechnique for hidden-unit CRFs (HUCRFs) to ef-fectively transfer knowledge from one domain toanother.
In our experiments, we find that ourpretraining method is almost always superior tostrong baselines such as the popular domain adap-tation method of Daum?e III (2007).2 Problem description and related workLet D be the number of distinct domains.
Let Xibe the space of observed samples for the i-th do-main.
Let Yibe the space of possible labels for thei-th domain.
In most previous works in domainadaptation (Blitzer et al, 2006; Daum?e III, 2007),observed data samples may vary but label space is473invariant1.
That is,Yi= Yj?i, j ?
{1 .
.
.D}butXi6= Xjfor some domains i and j.
For exam-ple, in part-of-speech (POS) tagging on newswireand biomedical domains, the observed data sam-ple may be radically different but the POS tag setremains the same.In practice, there are cases, where the samequery is labeled differently depending on the do-main or application and the context.
For example,Fred Myer can be tagged differently; ?send a textmessage to Fred Myer?
and ?get me driving direc-tion to Fred Myer ?.
In the first case, Fred Myer isperson in user?s contact list but it is a grocery storein the second one.So, we relax the constraint that label spacesmust be the same.
Instead, we assume that sur-face forms (i.e words) are similar.
This is a natu-ral setting in developing multiple applications onspeech utterances; input spaces (service requestutterances) do not change drastically but outputspaces (slot tags) might.Multi-task learning differs from our task.
Ingeneral multi-task learning aims to improve per-formance across all domains while our domainadaptation objective is to optimize the perfor-mance of semantic slot tagger on the target do-main.Below, we review related work in domain adap-tion and natural language understanding (NLU).2.1 Related WorkDomain adaptation has been widely used in manynatural language processing (NLP) applicationsincluding part-of-speech tagging (Schnabel andSch?utze, 2014), parsing (McClosky et al, 2010),and machine translation (Foster et al, 2010).Most of the work can be classified either su-pervised domain adaptation (Chelba and Acero,2006; Blitzer et al, 2006; Daume III and Marcu,2006; Daum?e III, 2007; Finkel and Manning,2009; Chen et al, 2011) or semi-supervised adap-tation (Ando and Zhang, 2005; Jiang and Zhai,2007; Kumar et al, 2010; Huang and Yates, 2010).Our problem setting falls into the former.Multi-task learning has become popular in NLP.Sutton and McCallum (2005) showed that joint1Multilingual learning (Kim et al, 2011; Kim and Snyder,2012; Kim and Snyder, 2013) has same setting.learning and/or decoding of sub-tasks helps to im-prove performance.
Collobert and Weston (2008)proved the similar claim in a deep learning archi-tecture.
While our problem resembles their set-tings, there are two clear distinctions.
First, weaim to optimize performance on the target domainby minimizing the gap between source and targetdomain while multi-task learning jointly learns theshared tasks.
Second, in our problem the domainsare different, but they are closely related.
On theother hand, prior work focuses on multiple sub-tasks of the same data.Despite the increasing interest in NLU (De Moriet al, 2008; Xu and Sarikaya, 2013; Sarikaya etal., 2014; Xu and Sarikaya, 2014; Anastasakos etal., 2014; El-Kahky et al, 2014; Liu and Sarikaya,2014; Marin et al, 2014; Celikyilmaz et al, 2015;Ma et al, 2015; Kim et al, 2015), transfer learn-ing in the context of NLU has not been much ex-plored.
The most relevant previous work is Tur(2006) and Li et al (2011), which described boththe effectiveness of multi-task learning in the con-text of NLU.
For multi-task learning, they usedshared slots by associating each slot type with ag-gregate active feature weight vector based on anexisting domain specific slot tagger.
Our empiri-cal results shows that these vector representationmight be helpful to find shared slots across do-main, but cannot find bijective mapping betweendomains.Also, Jeong and Lee (2009) presented a transferlearning approach in multi-domain NLU, wherethe model jointly learns slot taggers in multipledomains and simultaneously predicts domain de-tection and slot tagging results.2To share parame-ters across domains, they added an additional nodefor domain prediction on top of the slot sequence.However, this framework also limited to a settingin which the label set remains invariant.
In con-trast, our method is restricted to this setting with-out any modification of models.3 Sequence Modeling TechniqueThe proposed techniques in Section 4 and 5 aregeneric methodologies and not tied to any partic-ular models such as any sequence models and in-stanced based models.
However, because of supe-rior performance over CRF, we use a hidden unitCRF (HUCRF) of Maaten et al (2011).2Jeong and Lee (2009) pointed out that if the domain isgiven, their method is the same as that of Daum?e III (2007).474Figure 1: Graphical representation of hidden unitCRFs.While popular and effective, a CRF is still a lin-ear model.
In contrast, a HUCRF benefits fromnonlinearity, leading to superior performance overCRF (Maaten et al, 2011).
Thus we will focus onHUCRFs to demonstrate our techniques in experi-ments.3.1 Hidden Unit CRF (HUCRF)A HUCRF introduces a layer of binary-valued hid-den units z = z1.
.
.
zn?
{0, 1} for each pair oflabel sequence y = y1.
.
.
ynand observation se-quence x = x1.
.
.
xn.
A HUCRF parametrized by?
?
Rdand ?
?
Rd?defines a joint probability ofy and z conditioned on x as follows:p?,?
(y, z|x) =exp(?>?
(x, z) + ?>?
(z, y))?z??{0,1}ny??Y(x,z?)exp(?>?
(x, z?)
+ ?>?
(z?, y?
))(1)where Y(x, z) is the set of all possible label se-quences for x and z, and ?
(x, z) ?
Rdand?
(z, y) ?
Rd?are global feature functions that de-compose into local feature functions:?
(x, z) =n?j=1?
(x, j, zj)?
(z, y) =n?j=1?
(zj, yj?1, yj)HUCRF forces the interaction between the obser-vations and the labels at each position j to gothrough a latent variable zj: see Figure 1 for illus-tration.
Then the probability of labels y is givenby marginalizing over the hidden units,p?,?
(y|x) =?z?{0,1}np?,?
(y, z|x)As in restricted Boltzmann machines (Larochelleand Bengio, 2008), hidden units are conditionallyindependent given observations and labels.
Thisallows for efficient inference with HUCRFs de-spite their richness (see Maaten et al (2011) fordetails).
We use a perceptron-style algorithm ofMaaten et al (2011) for training HUCRFs.4 Transfer learning between domainswith different label setsIn this section, we describe three methods for uti-lizing annotations in domains with different la-bel types.
First two methods are about transfer-ring features and last method is about transfer-ring model parameters.
Each of these methods re-quires some sort of mapping for label types.
Afine-grained label type needs to be mapped to acoarse one; a label type in one domain needs to bemapped to the corresponding label type in anotherdomain.
We will provide a solution to obtainingthese label mappings automatically in Section 5.4.1 Coarse-to-fine predictionThis approach has some similarities to the methodof Li et al (2011) in that shared slots are usedto transfer information between domains.
In thistwo-stage approach, we train a model on thesource domain, make predictions on the target do-main, and then use the predicted labels as addi-tional features to train a final model on the targetdomain.
This can be helpful if there is some cor-relation between the label types in the source do-main and the label types in the target domain.However, it is not desirable to directly use thelabel types in the source domain since they canbe highly specific to that particular domain.
Aneffective way to combat this problem is to re-duce the original label types such start-time,contract-info, and restaurant as to aset of coarse label types such as name, date,time, and location that are universally sharedacross all domains.
By doing so, we can usethe first model to predict generic labels such astime and then use the second model to use thisinformation to predict fine-grained labels such asstart-time and end-time.4.2 Method of Daum?e III (2007)In this popular technique for domain adapta-tion, we train a model on the union of thesource domain data and the target domain data475but with the following preprocessing step: eachfeature is duplicated and the copy is conjoinedwith a domain indicator.
For example, in aWEATHER domain dataset, a feature that indi-cates the identity of the string ?Sunny?
willgenerate both w(0) = Sunny and (w(0) =Sunny) ?
(domain = WEATHER) as fea-ture types.
This preprocessing allows the modelto utilize all data through the common featuresand at the same time specialize to specific do-mains through the domain specific features.
Thisis especially helpful when there is label ambigu-ity on particular features (e.g., ?Sunny?
might be aweather-condition in a WEATHER domaindataset but a music-song-name in a MUSICdomain dataset).Note that a straightforward application of thistechnique is in general not feasible in our situation.This is because we have features conjoined withlabel types and our domains do not share labeltypes.
This breaks the sharing of features acrossdomains: many feature types in the source domainare disjoint from those in the target domain due todifferent labeling.Thus it is necessary to first map source domainlabel types to target domain label type.
After themapping, features are shared across domains andwe can apply this technique.4.3 Transferring model parameterIn this approach, we train HUCRF on the sourcedomain and transfer the learned parameters to ini-tialize the training process on the target domain.This can be helpful for at least two reasons:1.
The resulting model will have parameters forfeature types observed in the source domainas well as the target domain.
Thus it has bet-ter feature coverage.2.
If the training objective is non-convex, thisinitialization can be helpful in avoiding badlocal optima.Since the training objective of HUCRFs is non-convex, both benefits can apply.
We show in ourexperiments that this is indeed the case: the modelbenefits from both better feature coverage and bet-ter initialization.Note that in order to use this approach, we needto map source domain label types to target domainlabel type so that we know which parameter inFigure 2: Illustration of a pretraining scheme forHUCRFs.the source domain corresponds to which param-eter in the target domain.
This can be a many-to-one, one-to-many, one-to-one mapping dependingon the label sets.4.3.1 Pretraining with HUCRFsIn fact, pretraining HUCRFs in the source domaincan be done in various ways.
Recall that there aretwo parameter types: ?
?
Rdfor scoring obser-vations and hidden states and ?
?
Rd?for scoringhidden states and labels (Eq.
(1)).
In pretraining,we first train a model (?1, ?1) on the source data{(x(i)src, y(i)src)}nsrci=1:(?1, ?1) ?
arg max?,?nsrc?i=1log p?,?
(y(i)src|x(i)src)Then we train a model (?2, ?2) on the targetdata {(x(i)trg, y(i)trg)}ntrgi=1by initializing (?2, ?2) ?
(?1, ?1):(?2, ?2) ?
arg max?,?ntrg?i=1log p?,?
(y(i)trg|x(i)trg)Here, we can choose to initialize only ?2?
?1anddiscard the parameters for hidden states and labelssince they may not be the same.
The ?1parame-ters model the hidden structures in the source do-main data and serve as a good initialization pointfor learning the ?2parameters in the target domain.This can be helpful if the mapping between the la-bel types in the source data and the label types inthe target data is unreliable.
This process is illus-trated in Figure 2.5 Automatic generation of labelmappingsAll methods described in Section 4 requirea way to propagate the information in labeltypes across different domains.
A straightfor-ward solution would be to manually construct476such mappings by inspection.
For instance, wecan specify that start-time and end-timeare grouped as the same label time, or thatthe label public-transportation-routein the PLACES domain maps to the labelimplicit-location in the CALENDAR do-main.Instead, we propose a technique that automat-ically generates the label mappings.
We inducevector representations for all label types throughcanonical correlation analysis (CCA) ?
a pow-erful and flexible technique for deriving low-dimensional representation.
We give a review ofCCA in Section 5.1 and describe how we usethe technique to construct label mappings in Sec-tion 5.2.5.1 Canonical Correlation Analysis (CCA)CCA is a general technique that operates on apair of multi-dimensional variables.
CCA finds kdimensions (k is a parameter to be specified) inwhich these variables are maximally correlated.Let x1.
.
.
xn?
Rdand y1.
.
.
yn?
Rd?be nsamples of the two variables.
For simplicity, as-sume that these variables have zero mean.
ThenCCA computes the following for i = 1 .
.
.
k:arg maxui?Rd, vi?Rd?
:u>iui?=0 ?i?<iv>ivi?=0 ?i?<i?nl=1(u>ixl)(v>iyl)??nl=1(u>ixl)2?
?nl=1(v>iyl)2In other words, each (ui, vi) is a pair of projec-tion vectors such that the correlation between theprojected variables u>ixland v>iyl(now scalars) ismaximized, under the constraint that this projec-tion is uncorrelated with the previous i ?
1 pro-jections.This is a non-convex problem due to the inter-action between uiand vi.
Fortunately, a methodbased on singular value decomposition (SVD) pro-vides an efficient and exact solution to this prob-lem (Hotelling, 1936).
The resulting solutionu1.
.
.
uk?
Rdand v1.
.
.
vk?
Rd?can be usedto project the variables from the original d- andd?-dimensional spaces to a k-dimensional space:x ?
Rd??
x?
?
Rk: x?i= u>ixy ?
Rd???
y?
?
Rk: y?i= v>iyThe new k-dimensional representation of eachvariable now contains information about the othervariable.
The value of k is usually selected to bemuch smaller than d or d?, so the representation istypically also low-dimensional.5.2 Inducing label embeddingsWe now describe how to use CCA to induce vec-tor representations for label types.
Using the samenotation, let n be the number of instances of la-bels in the entire data.
Let x1.
.
.
xnbe the originalrepresentations of the label samples and y1.
.
.
ynbe the original representations of the associatedwords set contained in the labels.We employ the following definition for the orig-inal representations for reasons we explain below.Let d be the number of distinct label types and d?be the number of distinct word types.?
xl?
Rdis a zero vector in which the entrycorresponding to the label type of the l-th in-stance is set to 1.?
yl?
Rd?is a zero vector in which the entriescorresponding to words spanned by the labelare set to 1.The motivation for this definition is that similarlabel types often have similar or same word.For instance, consider two label typesstart-time, (start time of a calendar event)and end-time, meaning (the end time of a cal-endar event).
Each type is frequently associatedwith phrases about time.
The phrases {?9 pm?,?7?, ?8 am?}
might be labeled as start-time;the phrases {?9 am?, ?7 pm?}
might be labeledas end-time.
In these examples, both labeltypes share words ?am?, ?pm?, ?9?, and ?7?
eventhough phrases may not match exactly.Figure 3 gives the CCA algorithm for inducinglabel embeddings.
It produces a k-dimensionalvector for each label type corresponding to theCCA projection of the one-hot encoding of thatlabel.5.3 Discussion on alternative labelrepresentationsWe point out that there are other options for in-ducing label representations besides CCA.
Forinstance, one could simply use the sparse fea-ture vector representation of each label.
How-ever, CCA?s low-dimensional projection is com-putationally more convenient and arguably moregeneralizable.
One can also consider training apredictive model similar to word2vec (Mikolov477Figure 4: Bijective mapping: labels in REMINDER domain (orange box) are mapped into those inPLACES and ALARM domains.CCA-LABELInput: labeled sequences {(x(i), y(i))}ni=1, dimension kOutput: label vector v ?
Rkfor each label type1.
For each label type l ?
{1 .
.
.
d} and word type w ?
{1 .
.
.
d} present in the sequences, calculate?
count(l) = number of times label l occurs?
count(w) = number of times word w occurs?
count(l, w) = number of times word w occursunder label l2.
Define a matrix ?
?
Rd?d?where:?l,w=count(l, w)?count(l)count(w)3.
Perform rank-k SVD on ?.
Let U ?
Rd?kbe a matrixwhere the i-th column is the left singular vector of ?corresponding to the i-th largest singular value.4.
For each label l, set the l-th normalized row of U to beits vector representation.Figure 3: CCA algorithm for inducing label em-beddings.et al, 2013).
But this requires significant efforts inimplementation and also very long training time.In contrast, CCA is simple, efficient, and effec-tive and can be readily implemented.
Also, CCAis theoretically well understood while methods in-spired by neural networks are not.5.4 Constructing label mappingsVector representations of label types allow for nat-ural solutions to the task of constructing labelmappings.5.4.1 Mapping to a coarse label setGiven a domain and the label types that occurin the domain, we can reduce the number of la-bel types by simply clustering their vector repre-sentations.
For instance, if the embeddings forstart-time and end-time are close together,they will be grouped as a single label type.
We runthe k-means algorithm on the label embeddings toobtain this coarse label set.Table 1 shows examples of this clustering.
Itdemonstrates that the CCA representations ob-tained by the procedure described in Section 5.2are indeed informative of the labels?
properties.Cluster Labels Cluster LabelsTimestart timePersoncontact infoend time artistoriginal start time from contact nametravel time relationship nameLocabsolute locLoc ATTRprefer routeleaving loc public trans routefrom loc nearbyposition ref distanceTable 1: Some of cluster examples5.4.2 Bijective mapping between label setsGiven a pair of domains and their label sets, wecan create a bijective label mapping by findingthe nearest neighbor of each label type.
Figure 4shows some actual examples of CCA-based bijec-tive maps, where the label set in the REMINDERdomain is mapped to the PLACES and ALARMdomains.
One particularly interesting example isthat move earlier time in REMINDER do-main is mapped to Travel time in PLACESand Duration in ALARM domain.
This is a tagused in a user utterance requesting to move an478Domains # of label Source Training Test DescriptionAlarm 7 27865 3334 Set alrmsCalendar 20 50255 7017 Set appointments & meetings in the calendarCommunication 18 104881 14484 Make calls, send texts, and communication related user requestNote 4 17445 2342 Note takingOndevice 7 60847 9704 Phone settingsPlaces 32 150348 20798 Find places & get directionReminder 16 62664 8235 Setting time, person & place based reminderWeather 9 53096 9114 Weather forecasts & historical information about weather patternsTable 2: Size of number of label, labeled data set size and description for Alarm, Calendar, Communica-tion, Note, Ondevice, Places, Reminder and Weather domains partitioned into training and test set.appointment to an earlier time.
For example, inthe query ?move the dentist?s appointment up by30 minutes.
?, the phrase ?30 minutes?
is taggedwith move earlier time.
The role of this tagis very similar to the role of Travel time inPLACES (not Time) and Duration in ALARMS(not Start date), and CCA is able to recoverthis relation.6 ExperimentsIn this section, we turn to experimental findings toprovide empirical support for our proposed meth-ods.6.1 SetupTo test the effectiveness of our approach, we applyit to a suite of eight Cortana personal assistant do-mains for slot sequence tagging tasks, where thegoal is to find the correct semantic tagging of thewords in a given user utterance.The data statistics and short descriptions areshown in Table 2.
As the table indicates, the do-mains have very different granularity and diversesemantics.6.2 BaselinesIn all our experiments, we trained HUCRF andonly used n-gram features, including unigram, bi-gram, and trigram within a window of five words(?2 words) around the current word as binary fea-ture functions.
With these features, we comparethe following methods for slot tagging:?
NoAdapt: train only on target training data.?
Union: train on the union of source and targettraining data.?
Daume: train with the feature duplicationmethod described in 4.2.?
C2F: train with the coarse-to-fine predictionmethod described in 4.1.?
Pretrain: train with the pretraining methoddescribed in 4.3.1.To apply these methods except for Target, wetreat each of the eight domains in turn as the testdomain, with one of remaining seven domain asthe source domain.
As in general domain adap-tation setting, we assume that the source domainhas a sufficient amount of labeled data but the tar-get domain has an insufficient amount of labeleddata.
Specifically, For each test or target domain,we only use 10% of the training examples to sim-ulate data scarcity.
In the following experiments,we report the slot F-measure, using the standardCoNLL evaluation script36.3 Results on mappingsMapping techniqueAdaptationtechniqueManual Li et al (2011) CCAUnion 68.16 64.7 70.51Daume 73.42 67.32 75.85C2F 75.47 75.69 76.29Pretrain 77.72 76.99 78.76NoAdapt 75.13Table 3: Comparison of slot F1 scores usingthe proposed CCA-derived mapping versus othermapping methods combined with different adap-tation techniques.To assess the quality of our automatic mappingmethods via CCA described in Section 5, we com-pared against manually established mappings andalso the mapping method of Li et al (2011).
Themethod of Li et al (2011) is to associate eachslot type with the aggregate active feature weightvectors based on an existing domain specific slottagger (a CRF).
Manual mapping were performed3http://www.cnts.ua.ac.be/conll2000/chunking/output.html479Target Source Minimum distance domain performanceDomain Nearest Domain NoAdapt Union Daume C2F PretrainAlarm Calendar 74.82 84.46 84.97 81.54 84.88Calendar Reminder 70.51 73.94 73.07 72.82 77.08Note Reminder 65.38 56.39 69.89 66.6 69.55Ondevice Weather 70.86 66.66 71.17 71.49 73.5Reminder Calendar 77.3 83.38 82.19 81.29 83.22Communication Reminder 79.31 74.28 80.33 79.66 82.96Places Weather 73.93 73.74 75.86 73.73 80.11Weather Places 92.78 92.88 94.43 93.75 97.18Average - 75.61 75.72 78.99 77.61 81.06Table 4: Slot F1 scores on each target domain using adapted models from the nearest source domain.hhhhhhhhhhhhSourceTargetAlarm Calendar Note Ondevice Reminder Communication Places Weather AverageNoAdapt 74.82 70.51 65.38 70.86 77.3 79.31 73.93 92.78 75.61AlarmUnion - 72.26 59.92 67.32 79.45 77.91 73.78 92.67 74.76Daume - 72.77 66.28 70.94 81.12 80.38 75.62 93.12 77.18C2F - 70.59 64.06 71 78.8 79.5 74.29 92.75 75.86Pretrain - 76.68 68.12 71.8 81.25 81.5 77.1 95.03 78.78CalendarUnion 84.46 - 50.64 64.7 83.38 75.02 71.13 93.2 74.65Daume 84.97 - 65.43 70.12 82.19 79.78 75.21 93.1 78.69C2F 81.54 - 66.08 71.22 81.29 80.11 73.75 93.18 78.17Pretrain 84.88 - 69.21 72.3 83.22 82.75 77.89 95.8 80.86NoteUnion 60.26 60.42 - 65.79 69.81 76.85 70.56 90.02 70.53Daume 66.03 67.38 - 69.54 76.65 77.83 73.49 92.09 74.72C2F 74.68 70.51 - 71.34 77.49 79.48 74.17 92.89 77.22Pretrain 75.52 72.4 - 71.4 80.1 82.06 76.53 94.22 78.89OndeviceUnion 63.72 66.28 55.67 - 75.16 74.85 70.59 90.7 71.00Daume 71.01 69.39 64.02 - 75.75 77.92 74.41 92.62 75.02C2F 74.02 70.33 64.99 - 77.43 79.53 73.84 92.71 76.12Pretrain 76.27 71.59 67.21 - 78.67 82.34 77.45 95.04 78.37ReminderUnion 84.74 73.94 56.39 61.27 - 74.28 68.14 92.22 73.00Daume 84.66 73.07 69.89 67.94 - 80.33 73.36 93.19 77.49C2F 80.42 72.82 66.6 71.36 - 79.66 74.35 92.38 76.80Pretrain 84.75 77.08 69.55 71.9 - 82.96 78.57 95.37 80.03CommunicationUnion 58.25 54.69 65.28 62.95 63.98 - 68.16 87.13 65.78Daume 70.4 67.41 69.14 69.26 77.67 - 73.33 92.82 74.29C2F 74.54 70.84 65.48 70.81 77.68 - 74.15 92.79 75.18Pretrain 76.04 74.01 68.76 73.2 80.74 - 76.83 94.58 77.74PlacesUnion 71.7 67.56 45.37 53.93 67.78 63.67 - 92.88 66.13Daume 75.69 69.01 66.11 65.46 79.01 78.42 - 94.43 75.45C2F 78.9 71.64 66.93 71.26 79.2 79.19 - 93.75 77.27Pretrain 76.8 74.12 67.5 72.7 81 81.89 - 97.18 78.74WeatherUnion 69.43 58.53 56.76 66.66 74.98 77.53 73.74 - 68.23Daume 75 71.73 66.54 71.17 79.36 80.57 75.86 - 74.32C2F 77.61 71.47 63.24 71.49 78.44 79.43 73.73 - 73.63Pretrain 77.37 74.5 68.23 73.5 80.96 82.05 80.11 - 76.67AverageUnion 70.37 64.81 55.72 63.23 73.51 74.3 70.87 91.26 70.51Daume 75.4 70.23 66.77 69.2 78.32 79.32 74.47 93.05 75.85C2F 77.39 71.17 65.4 71.21 78.62 79.56 74.04 92.92 76.29Pretrain 78.80 74.34 68.37 72.40 80.85 82.22 77.78 95.32 78.76Table 5: Slot F1 scores of using Union, Daume, Coarse-to-Fine and pretraining on all pairs of source andtarget data.
The numbers in boldface are the best performing adaptation technique in each pair.by two experienced annotators who have PhD inlinguistics and machine learning.
Each annotatorfirst assigned mapping slot labels independentlyand then both annotators collaborated to reducedisagreement of their mapping results.
Initially,the disagreement of their mapping rate betweentwo annotators was about 30% because labels ofslot tagging are very diverse; furthermore, in somecases it is not clear for human annotators if thereexists a valid mapping.The results are shown at Table 3.
Vector repre-sentation of Li et al (2011) increases the F1 scoreslightly from 75.13 to 75.69 in C2F, but it does nothelp as much in cases that require bijective map-ping: Daume, Union and Pretrain.In contrast, the proposed CCA based techniqueconsistently outperforms the NoAdapt baselinesby significant margins.
More importantly, it alsooutperforms manual results under all conditions.It is perhaps not so surprising ?
the CCA derivedmapping is completely data driven, while humanannotators have nothing but the prior linguistic480knowledge about the slot tags and the domain.6.4 Main ResultsThe full results are shown in Table 5, where allpairs of source and target languages are consid-ered for domain adaptation.
It is clear from the ta-ble that we can always achieve better results usingadaptation techniques than the non-adapted mod-els trained only on the target data.
Also, our pro-posed pretraining method outperforms other typesof adaptation in most cases.The overall result of our experiments are shownin Table 4.
In this experiment, we compare dif-ferent adaptation techniques using our suggestedCCA-based mapping.
Here, except for NoAdapt,we use both the target and the nearest source do-main data.
To find the nearest domain, we firstmap fine grained label set to coarse label set byusing the method described in Section 5.4.1 andthen count how many coarse labels are used in adomain.
And then we can find the nearest sourcedomain by calculating the l2distance between themultinomial distributions of the source domainand the target domain over the set of coarse labels.For example, for CALENDAR, we identifyREMINDER as the nearest domain and vice versabecause most of their labels are attributes relatedto time.
In all experiments, the domain adaptedmodels perform better than using only target do-main data which achieves 75.1% F1 score.
Sim-ply combining source and target domain using ourautomatically mapped slot labels performs slightlybetter than baseline.
C2F boosts the performanceup to 77.61% and Daume is able to reach 78.99%.4Finally, our proposed method, pretrain achievesnearly 81.02% F1 score.7 ConclusionWe presented an approach to take advantage of ex-isting annotations when the data are similar butthe label sets are different.
This approach wasbased on label embeddings from CCA, which re-duces the setting to a standard domain adapta-tion problem.
Combined with a novel pretrain-ing scheme applied to hidden-unit CRFs, our ap-proach is shown to be superior to strong baselinesin extensive experiments for slot tagging on eightdistinct personal assistant domains.4It is known that Daume is less beneficial when the sourceand target domains are similar due to the increased number offeatures.ReferencesTasos Anastasakos, Young-Bum Kim, and Anoop Deo-ras.
2014.
Task specific continuous word represen-tations for mono and multi-lingual spoken languageunderstanding.
In Proceeding of the ICASSP, pages3246?3250.
IEEE.Rie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multipletasks and unlabeled data.
The Journal of MachineLearning Research, 6:1817?1853.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the EMNLP,pages 120?128.
Association for Computational Lin-guistics.Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasu-pat, and Ruhi Sarikaya.
2015.
Enriching word em-beddings using knowledge graph for semantic tag-ging in conversational dialog systems.
AAAI - As-sociation for the Advancement of Artificial Intelli-gence.Ciprian Chelba and Alex Acero.
2006.
Adaptation ofmaximum entropy capitalizer: Little data can help alot.
Computer Speech & Language, 20(4):382?399.Minmin Chen, Kilian Q Weinberger, and John Blitzer.2011.
Co-training for domain adaptation.
In Ad-vances in neural information processing systems,pages 2456?2464.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the ICML, pages 160?167.
ACM.Hal Daume III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, pages 101?126.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
proceedings of the ACL, page 256.Renato De Mori, Fr?ed?eric Bechet, Dilek Hakkani-Tur,Michael McTear, Giuseppe Riccardi, and GokhanTur.
2008.
Spoken language understanding.
Sig-nal Processing Magazine, IEEE, 25(3):50?58.Ali El-Kahky, Derek Liu, Ruhi Sarikaya, Gokhan Tur,Dilek Hakkani-Tur, and Larry Heck.
2014.
Ex-tending domain coverage of language understand-ing systems via intent transfer between domains us-ing knowledge graphs and search query click logs.IEEE, Proceedings of the ICASSP.Jenny Rose Finkel and Christopher D Manning.
2009.Hierarchical bayesian domain adaptation.
In Pro-ceedings of the ACL, pages 602?610.
Association forComputational Linguistics.481George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proceed-ings of the EMNLP, pages 451?459.
Association forComputational Linguistics.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377.Fei Huang and Alexander Yates.
2010.
Exploringrepresentation-learning approaches to domain adap-tation.
In Proceedings of the 2010 Workshop on Do-main Adaptation for Natural Language Processing,pages 23?30.
Association for Computational Lin-guistics.Minwoo Jeong and Gary Geunbae Lee.
2009.
Multi-domain spoken language understanding with trans-fer learning.
Speech Communication, 51(5):412?424.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in nlp.
In Proceed-ings of the ACL, volume 7, pages 264?271.
Associ-ation for Computational Linguistics.Young-Bum Kim and Benjamin Snyder.
2012.
Univer-sal grapheme-to-phoneme prediction over latin al-phabets.
In Proceedings of the EMNLP, pages 332?343, Jeju Island, South Korea, July.
Association forComputational Linguistics.Young-Bum Kim and Benjamin Snyder.
2013.
Unsu-pervised consonant-vowel prediction over hundredsof languages.
In Proceedings of the ACL, pages1527?1536.
Association for Computational Linguis-tics.Young-Bum Kim, Jo?ao V Grac?a, and Benjamin Sny-der.
2011.
Universal morphological analysis usingstructured nearest neighbor prediction.
In Proceed-ings of the EMNLP, pages 322?332.
Association forComputational Linguistics.Young-Bum Kim, Minwoo Jeong, Karl Stratos, andRuhi Sarikaya.
2015.
Weakly supervised slottagging with partially labeled sequences from websearch click logs.
In Proceedings of the NAACL.
As-sociation for Computational Linguistics.Abhishek Kumar, Avishek Saha, and Hal Daume.2010.
Co-regularization based semi-supervised do-main adaptation.
In Advances in Neural InformationProcessing Systems, pages 478?486.Hugo Larochelle and Yoshua Bengio.
2008.
Classifi-cation using discriminative restricted boltzmann ma-chines.
In Proceedings of the ICML.Xiao Li, Ye-Yi Wang, and G?okhan T?ur.
2011.
Multi-task learning for spoken language understandingwith shared slots.
In Proceeding of the INTER-SPEECH, pages 701?704.
IEEE.Xiaohu Liu and Ruhi Sarikaya.
2014.
A discriminativemodel based entity dictionary weighting approachfor spoken language understanding.
IEEE Instituteof Electrical and Electronics Engineers.Yi Ma, Paul A. Crook, Ruhi Sarikaya, and Eric Fosler-Lussier.
2015.
Knowledge graph inference for spo-ken dialog systems.
In Proceedings of the ICASSP.IEEE.Laurens Maaten, Max Welling, and Lawrence K Saul.2011.
Hidden-unit conditional random fields.
In In-ternational Conference on Artificial Intelligence andStatistics.Alex Marin, Roman Holenstein, Ruhi Sarikaya, andMari Ostendorf.
2014.
Learning phrase patterns fortext classification using a knowledge graph and un-labeled data.
ISCA - International Speech Commu-nication Association.David McClosky, Eugene Charniak, and Mark John-son.
2010.
Automatic domain adaptation for pars-ing.
In Proceedings of the NAACL, pages 28?36.Association for Computational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Ruhi Sarikaya, Asli C, Anoop Deoras, and MinwooJeong.
2014.
Shrinkage based features for slot tag-ging with conditional random fields.
Proceeding ofISCA - International Speech Communication Asso-ciation, September.Tobias Schnabel and Hinrich Sch?utze.
2014.
Flors:Fast and simple domain adaptation for part-of-speech tagging.
Transactions of the Association forComputational Linguistics, 2:15?26.Charles Sutton and Andrew McCallum.
2005.
Compo-sition of conditional random fields for transfer learn-ing.
In Proceedings of the EMNLP, pages 748?754.Association for Computational Linguistics.Gokhan Tur.
2006.
Multitask learning for spokenlanguage understanding.
In Proceedings of theICASSP, Toulouse, France.
IEEE.Puyang Xu and Ruhi Sarikaya.
2013.
Convolutionalneural network based triangular crf for joint in-tent detection and slot filling.
In Automatic SpeechRecognition and Understanding (ASRU), pages 78?83.
IEEE.Puyang Xu and Ruhi Sarikaya.
2014.
Targeted featuredropout for robust slot filling in natural language un-derstanding.
ISCA - International Speech Commu-nication Association.482
