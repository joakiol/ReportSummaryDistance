First Joint Conference on Lexical and Computational Semantics (*SEM), pages 493?496,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUNT-SIMPRANK: Systems for Lexical Simplification RankingRavi SinhaUniversity of North Texas1155 Union Circle #311277Denton, Texas76203-5017RaviSinha@my.unt.eduAbstractThis paper presents three systems that tookpart in the lexical simplification task at SE-MEVAL 2012.
Speculating on what the con-cept of simplicity might mean for a word,the systems apply different approaches to rankthe given candidate lists.
One of the systemsperforms second-best (statistically significant)and another one performs third-best out of 9systems and 3 baselines.
Notably, the third-best system is very close to the second-best,and at the same time much more resource-lightin comparison.1 IntroductionLexical simplification (described in (Specia et al,2012)) is a newer problem that has arisen follow-ing a recent surge in interest in the related task oflexical substitution (McCarthy et al, 2007).
Whilelexical substitution aims at making systems generatesuitable paraphrases for a target word in an instance,which do not necessarily have to be simpler versionsof the original, it has been speculated that one pos-sible use of the task could be lexical simplification,in particular in the realm of making educational textmore readable for non-native speakers.The task of lexical simplification, which thusderives from lexical substitution, uses the samedata set, and has been introduced at the 6thInternational Workshop on Semantic Evaluation(SEMEVAL 2012), in conjunction with the First JointConference on Lexical and Computational Seman-tics (*SEM 2012).
Instead of asking systems to pro-vide substitutes, the task provides the systems withall substitutes and asks them to be ranked.The task provides several instances of triplets of acontext C, a target word T , and a set of gold stan-dard substitutes S. The systems are supposed torank the substitutes si ?
S from the simplest to themost difficult, and match their predictions againstthe provided human annotations.
The organizers de-fine simple loosely as words that can be understoodby a wide variety of people, regardless of their lit-eracy and cognitive levels, age, and regional back-grounds.The task is novel in that so far most work has beendone on syntactic simplification and not on lexicalsimplification.
Carroll et.
al.
(Carroll et al, 1998)seem to have pioneered some methodology and eval-uation metrics in this field.
Yatskar et.
al.
(Yatskar etal., 2010) use an unsupervised learning method andmetadata from the Simple English Wikipedia.2 DataThe data (trial and test, no training) have beenadopted from the original lexical substitutiontask (McCarthy et al, 2007).
The trial set has 300examples, each with a context, a target word, anda set of substitutions.
The test set has 1710 exam-ples.
The organizers provide a scorer for the task,the trial gold standard rankings, and three baselines.The data is provided in XML format, with tags iden-tifying the lemmas, parts of speech, instances, con-texts and head words.
The substitutions and goldrankings are in plain text format.4933 ResourcesIntuitively, a simple word is likely to have a highfrequency in a resource that is supposed to containsimple words.
Other factors that could intuitively in-fluence simplicity would be the frequency in spokenconversation, and whether the word is polysemousor not.
As such, the following resources have beenselected to contribute to the metric used in rankingthe substitutes.3.1 Simple English WikipediaSimple English Wikipedia has been used before insimplicity analysis, as described in (Yatskar et al,2010).
It is a publicly available, smaller Wikipedia(298MB decompressed), which claims to only con-sist of words that are somehow simple.
For all thesubstitute candidates, I count their frequencies of oc-currence in this resource, and these counts serve asa factor in computing the corresponding simplicityscores (refer to Equation 1.
)3.2 Transcribed Spoken English CorpusA set of spoken dialogues is also utilized in thisproject to measure simplicity.
Spoken language in-tuitively contains more conversational words, andhas the same kind of resolution power as the Sim-ple English Wikipedia when it comes to the relativesimplicity of a word.
Frequency counts of all thesubstitute candidates in a set of dialogue corpora iscomputed, and used as another factor in the Equa-tions 1 and 3.3.3 WordNetWordNet, as described in (Fellbaum, 1998), is a lex-ical knowledge base that combines the properties ofa thesaurus with that of a semantic network.
The ba-sic entry in WordNet is a synset, which is defined asa set of synonyms.
I use WordNet 3.0, which hasover 150,000 unique words, over 110,000 synsets,and over 200,000 word-sense pairs.
For each substi-tute, I extract the raw number of senses (for all partsof speech possible) for that word present in Word-Net.
This count serves as yet another factor in theproposed simplicity measure, under the hypothesisthat a simple word is used very frequently, and istherefore polysemous.3.4 Web1T Google N-gram CorpusThe Google Web 1T corpus (Brants and Franz,2006) is a collection of English N-grams, rangingfrom one to five N-grams, and their respective fre-quency counts observed on the Web.
The corpus wasgenerated from approximately 1 trillion tokens ofwords from the Web, predominantly English.
Thiscorpus is also used in both SIMPRANK and SALSAsystems, with the intuition that simpler words willhave higher counts on the Web taken as a whole.3.5 SaLSASALSA (Stand-alone Lexical Substitution Ana-lyzer) is an in-house application which accepts as in-puts sentences with target words marked distinctly,and then builds all possible 3-grams by substitut-ing the target word with synonyms (and inflectionsthereof).
It then queries the Web1T corpus using anin-house quick lookup application and gathers thecounts for all 3-grams.
Finally, it sums the counts,and assigns the aggregated scores to each corre-sponding synonym and outputs a reverse-ranked listof the synonyms.
More detail about this method-ology can be found in (Sinha and Mihalcea, 2009).SALSA uses the exact same methodology describedin the paper, except that it is a stand-alone tool.4 Experimental SetupFigure 1 shows the general higher-level picture ofhow the experiments have been performed.
SIM-PRANK uses five resources, including the unigramfrequency data, while SIMPRANKLIGHT does notuse the unigram frequencies.I hypothesize that the simplicity of a word couldbe represented as the Equation 1 (here cword() rep-resents the frequency count of the word in a givenresource).simplicity(word) =1len(word) + cword(SimpleWiki)+ cword(Discourse) + cword(WordNet)+ cword(Unigrams) (1)This formula is very empirical in nature, in that ithas been found based on extensive experimentation494Figure 1: High-level schematic diagram of the experi-ments(Table 1).
It intuitively makes sense that a simpleword is supposed to have high frequency counts inlexical resources that are meant to be simple by de-sign.
Formally,simplicity(word)?
frequency(SimpleResource)?1length(2)Here, SimpleResource could be any resourcethat contains simple words.
Apart from frequencycounts, we could possibly also leverage morphologyfor finding simplicity.
Intuitively, a 3-letter word ora 4-letter word would most likely be simpler than aword that has a longer length.
This accounts for thelength factor in the equations.As Table 1 depicts, a lot of experiments were per-formed where the components (counts) were mul-tiplied instead of being added, normalized insteadof adding without normalization1, and also experi-ments where subsets of the resources were selected.The scores obtained using the gold standard and thetrial data are also shown in the table.
The best com-1The normalization is done by dividing by the maximumvalue obtained for that particular resourcebination found (experiment 8 in the table) is outlinedin Equation 1.Note however, that the Google Web1T corpus isexpensive in terms of money, computation time andstorage space.
Thus, another set of experiments wasperformed (listed as experiments 1a in Table 1 leav-ing the unigram counts out, and it was found to workalmost just as well.
This system has been labeledSIMPRANKLIGHT and uses the formula in Equa-tion 3.simplicity(word) =1len(word) + cword(SimpleWiki)+ cword(Discourse) + cword(WordNet)(3)The substitutes can then be sorted in the decreas-ing order of simplicity scores.
The substitute withthe highest simplicity score is hypothesized to be thesimplest.Table 1: Variants of the experiments performedSN System components Method Remarks Scorebaseline no-change 0.05baseline random 0.01baseline unigram count (Web1T) 0.391 len, simplewiki, discourse, wordnet add normalize 0.201a len, simplewiki, discourse, wordnet add don?t normalize 0.372 len, simplewiki, discourse, wordnet add normalize, inc sort -0.203 len, simplewiki, discourse, wordnet multiply don?t normalize 0.254 simplewiki, discourse, wordnet add don?t normalize 0.364a simplewiki, discourse, wordnet add normalize 0.224b simplewiki, discourse, wordnet multiply don?t normalize 0.265 len, simplewiki, wordnet add don?t normalize 0.365a len, simplewiki, wordnet add normalize 0.195b len, simplewiki, wordnet multiply don?t normalize 0.266 len, discourse, wordnet add don?t normalize 0.316a len, discourse, wordnet add normalize 0.206b len, discourse, wordnet multiply don?t normalize 0.257 len, simplewiki, discourse add don?t normalize 0.377a len, simplewiki, discourse add normalize 0.227b len, simplewiki, discourse multiply don?t normalize 0.328 len, simplewiki, discourse, word-net, unigramsadd don?t normalize 0.398a len, simplewiki, discourse, word-net, unigramsadd normalize 0.228b len, simplewiki, discourse, word-net, unigramsmultiply don?t normalize 0.269 SaLSA 0.36Experiment 2 in Table 1 shows what happenswhen an increasing-order ranking of the simplicityscores is used.
A negative score here underscoresthe correctness of both the simplicity score as wellas that of the reverse-ranking.The third system, SALSA (Stand-alone LexicalSubstitution Analyzer) is the only system out of the495three that takes advantage of the context providedwith the data set.
It builds all possible 3-grams fromthe context, replacing the target word one-by-one bya substitute candidate (and inflections of the substi-tute candidates).
It then sums their frequency countsin the Web1T corpus and assigns the sum to the sim-plicity score of a particular synonym.
The synonymscan then be reverse-ranked.5 System Standings and DiscussionFor the test data, Table 2 depicts the system stand-ings, separated by statistical significance.Table 2: Test data system scoresRank Team ID System ID Score1 WLV-SHEF SimpLex 0.4962 baseline Sim Freq 0.4712 UNT SimpRank 0.4712 annlor simple 0.4653 UNT SimpRankL 0.4494 EMNLPCPH ORD1 0.4055 EMNLPCPH ORD2 0.3936 SB mmSystem 0.2897 annlor lmbing 0.1998 baseline No Change 0.1069 baseline Rand 0.01310 UNT SaLSA -0.082Surprisingly, the systems SIMPRANK and SIM-PRANKLIGHT, which do not use the contexts pro-vided, score much better than SALSA, which doesuse the contexts.
Apparently simplicity is rather astatistical concept even for humans (the annotatorsfor the gold standard) and not a contextual one.
Alsosurprisingly, SIMPRANKLIGHT, which does not useGoogle Web1T data, performs extremely well andwithin 0.02 of the raw scores.What is also surprising is the inability of all-but-one systems to beat the baseline of using simple fre-quency counts from Web1T, which is in turn basedentirely on statistical counts and does not take thecontext into account.A major contribution of this paper is the discoverythat other, lighter, free resources work just as wellas the expensive (in money, time and space) Web1Tdata when it comes to identifying which word is sim-ple and which one is not.6 Future WorkI plan to extend this experiment by performing ab-lation studies of all the individual features, play-ing with new features, and also performing machinelearning experiments to see if supervised experi-ments are a better way of solving the problem oflexical simplicity ranking.ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.John Carroll, Guido Minnen, Yvonne Canning, SiobhanDevlin, and John Tait.
1998.
Practical simplificationof english newspaper text to assist aphasic readers.
InIn Proc.
of AAAI-98 Workshop on Integrating ArtificialIntelligence and Assistive Technology, pages 7?10.Christiane Fellbaum, editor.
1998.
WordNet An Elec-tronic Lexical Database.
The MIT Press, Cambridge,MA ; London, May.Diana McCarthy, Falmer East Sussex, and Roberto Nav-igli.
2007.
Semeval-2007 task 10: English lexicalsubstitution task.
In In Proceedings of the 4th work-shop on Semantic Evaluations (SemEval-2007), pages48?53.Ravi Sinha and Rada Mihalcea.
2009.
Combining lex-ical resources for contextual synonym expansion.
InProceedings of the International Conference RANLP-2009, pages 404?410, Borovets, Bulgaria, September.Association for Computational Linguistics.Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea.2012.
Semeval-2012 task 1: English lexical simplifi-cation.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), Mon-treal, Canada.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of simplic-ity: Unsupervised extraction of lexical simplificationsfrom Wikipedia.
In Proceedings of the NAACL, pages365?368.496
