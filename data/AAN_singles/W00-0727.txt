In: Proceedings of CoNLL-2000 and LLL-2000, pages 133-135, Lisbon, Portugal, 2000.Learning Syntactic Structures with XMLHerv6  D6 jeanSeminar fiir SprachwissenschaftUniversit~it Ti ibingende j  eanOsf s.  nph?l ,  un?- tueb ingen,  de1 I n t roduct ionWe present the result of a symbolic machinelearning system for the CoNLL-2000 sharedtask.
This system, ALLiS, is based on theoryrefinement.
In this paper we want to show thatXML format not only offers a good frameworkto annotate texts, but also provides a good for-malism and tools in order to learn (syntactic)structures.2 ALLiSALLiS (Architecture for Learning LinguisticStructure) (D6jean, 2000a), (D6jean, 2000b) is asymbolic machine learning system.
The learn-ing system is based on theory refinement.
Ittries to refine (to improve) an existing imperfectgrammar using operators uch as contextualisa-tion and lexicalisation (Section 4).
ALLiS sepa-rates the task of the generation of rules and thetask of the use of these rules (task of parsing).First symbolic rules are learned and saved usingan own formalism, and in a second time, theserules are converted into a proper formalism usedby a specific rule-based parser.
The rules gen-erated by ALLiS contain enough contextual in-formation so that their conversions into otherformalisms is possible (D6jean, 2000b).We present here the three tools we tested inorder to parse texts.CASS : the CASS system (Abney, 1996) pro-vides a very fast parser which uses Regular Ex-pression Grammar.
The inconvenient of thissystem is twofold: first CASS only handles tags.Some pre- and post-processings are needed.Second, it is impossible to use contextual infor-mation.
Using CASS, it is thus difficult to im-plement rules generated by the two refinementoperators used in ALLiS: contextualisation (i -troduction of contexts) and lexicalisation (useof the word level).XFST  : the Xerox finite State Tool (Kart-tunen et al, 1997) offers a rich finite state for-malism (operator of contextualisation, replaceoperator).
If the formalism is powerful enough,the main problem with XFST is the numberof transducers generated by ALLiS.
For eachcontext and each word occurring in the ruleslearned by ALLiS corresponds a transducer.The size of the final Regular Expression Gram-mar is quite big and the compilation and pro-cessing time keeps low.
A phase of optimisationwould be required.LT TTT  : the last tool tried, LT TTT(Grover et al, 1999), is a text tokenisation sys-tem and toolset.
One of the tools, #gmatchreads a grammar (also written in XML format)which is used to annotate/parse XML texts.
Us-ing XML properties, the grammar has easily ac-cess to all the levels of the document (word, tag,phrase, and higher structures).
The manipula-t ion/annotation of several structures (as in theCoNLL-2000 shared task) is very easy.
LT TTTis a good trade-off between the rapidity of CASSand the rich formalism of XFST.We now explain how the LT TTT  is used dur-ing the learning and the parsing task.3 Data and Learning Algor i thmThe training corpus is an XML document whereeach structure is marked up (Table 1).
Train-ing corpus can contain other structures or in-formation.
Using XML tools, we can choosewhich information we want to use.
The DTDused by ALLiS (Table 2) describes a documentcomposed of words (W) which compose phrases(PHR), and a sequence of phrases composes asentence (S).
This DTD should be updated inorder to take into account intermediate l vels133<S><PHR C='NP'><W C='NNP'>Mr.</W><W C='NNP'>Percival</W></PHR><PHR C='VP><W C='VBD'>declined</W><W C='T0'>to</W><W C='VB'>comment</W></PHR><W C='.
'>.</W></S>Table 1: Example of training corpus.<!ELEMENT DOCS (#PCDATAITEXT)* ><!ELEMENT TEXT (SIPHRIW)+ ><!ELEMENT S (PHRIW)+ ><!ELEMENT PHR (W)+ ><!ATTLIST PHR C CDATA "" ><!ELEMENT W (#PCDATA)><!ATTLIST W C CDATA ""S CDATA " "BK CDATA ""CAT CDATA "" >Table 2: DTD used for the training corpus.between the phrase and the sentence.The learning method consists of finding con-texts in which an element (tag or word) canbe associated to a specific category with highconfidence.
Some elements do not need con-texts and are themselves confident.
In the casean element requires contexts, these contextsare computed by the use of queries which ex-amine training corpus.
XML Path Languageprovides an easy way of addressing nodes ofan XML document.
For example, the query/TEXT/S/PHR \[C= ' NP ' \]/W determines elementsW directly under an entity PHR with theattribute C='NP' ,  under the entities S andTEXT.
A query returns the indicated items oneby one until the set denoted by the query is ex-hausted.
The above query returns all the ele-ments occurring in a phrase marked NP.The next section provides some examples ofqueries used during the learning task.
(Groveret al, 1999) describes the syntax of the LT XMLquery language.4 Examples  o f  LT  XML Quer iesThe default category of a tag is given by theration between its number of occurrences in thestructure we want to recognise and the numberof occurrences in the training corpus.
The twofollowing queries compute this ration for the tagVBG and the NP structure:?
* / (PHR \ [C='NP' \ ] /W \[C='VBG' \] )?
*/W \[C='VBG'\]The first query can be read as follows: elementW with the attribute C='VBG' occurring in anentity PHR with the attr ibute C=' NP ', this laststructure occurring anywhere in the document.The second query counts all the occurrences ofthe entity W with the attr ibute C='VBG' any-where in the document.
If the ration is higherthan a threshold 0, then the tag is consideredas belonging to the structure.
If not, we haveto use operators of specialisation.The principle consists of enriching the queryso that the ratio 0 is higher than a given thresh-old.
Contextualisation i troduces left and/orright elements in the query.
Lexicalisation usesthe word value and not only the tag.
Sincethe tag VBG is not reliable (it mainly occursin an VP structure), we have to look for largercontexts.
The following query returns the listof the elements W occurring on the left of anelement W \[C= ~ VBG' \] in the same phrase NP(PHR \[C=' NP' \] ).PHR \[C='NP'\]/(W!
,W \[C:'VBG'\] )The tag DT  is one the elements of this list.
Wenow try to find negative examples.
The secondquery looks for all the occurrences of the entityW \[C=' VBG' \] which occur outside the NP  struc-ture in the same context: a tag DT occurringbefore the tag VBG.PHR\[C='NP'\]/W \[C='DT'\] ,W\[C='VBG'\] )Using the result of these two queries, a new ratiois computed to determine whether the categoryof VBG is reliable in this context (the answeris affirmative).
For some tags, the ratio stillremains below the threshold.The last query shows that we can also haveaccess to the word itself: it looks for the wordoperating tagged VBG inside a phrase cate-gorised as NP:134PHR \[C=' NP' \]/W \[C=' VBG' \ ] /#='  operat ing 'Contexts in which the word occurs outside thestructure are also built, and the ratio is com-puted in the same was as above.5 Pars ing  w i th  f sgmatchOnce rules are learned, they are converted intothe formalism of the parser.
Table 3 providesan example of rules used by fsgmatch (Groveret al, 1999).
The rule consists of adding theattributes CAT=' AL' S='NP' (left adjunct of aNoun Phrase(NP)) to each word with the at-tribute C='VBG' which occurs after a word withthe attribute C= 'DT'.<RULE name="tL" targ_sg="~ \[CAT=' AL'S='NP'\] "><REL mat ch=" W \[C = ' DT 'm_mod= ' TEST 'S='NP'\] "> </REL><REL mat ch="W \[C= ' VBG ' \] "></REL></RULE>Table 3: An example of fsgmatch rule.<PHR C='PP'><W CAT='N' C='IN'>Under</W></PHR><PHR C='NP'><W S='NP' BK='L' CAT='AL' C='DT'>the</W><W S='NP' CAT='AL' C='VBG'>existing</W><W S= 'NP' CAT='N' C='NN'>contract</W></PHR><W C=','>,</W>Table 4: Example of output.For each structure, words are marked up withtheir categories, and then a rule inserts an en-tity PHR with an attribute C which correspondsto the name of the structure.
The entity PHRis inserted by a rule which recognises sequencesof entities W corresponding to a structure.
Ta-ble 4 shows an output where two phrases wereadded: a prepositional one (<PHR C='PP'>),and a noun phrase (<PHR C='NP' >).6 Order ing  St ructuresThe different structures are learned sequen-tially.
If, in principle, the order used for learningtest dataADJPADVPCONJPINTJLSTNPPPPRTSBARVPprecision74.26%72.10%100.00%100.00%0.00%92.38%95.57%81.43%90.47%92.48%all 91.87% 92.31%recall F~=i68.49% 71.2680.25% 75.9655.56% 71.4350.00% 66.670.00% 0.0092.71% 92.5497.86% 96.7053.77% 64.7776.26% 82.7692.92% 92.7092.09Table 5: ALLiS resultsstructures i not important, the practice showsthat it is better to begin with structures whichcontain other structures.
Once a structure islearned, it is not taken into account during thelearning of the next structures.
This avoidsgenerating of complementary contexts alreadylearned at the preceding level.
For instance,ALLiS first learns NP structure.
Once categori-sation rules for the tag VBG are learned, ALLiSdoes not take into account VBG occurring in aNP when it learns following structures.
At theVP level, it is thus useless of learn contexts inwhich VBG does not occur in a VP (cases whichmainly correspond to occurrences of VBG inNP).
The parsing phase has of course to use thesame learning order.
The (partial) order usedis: NP, VP, ADJP, ADVP, PP, PRT, CONJP,SBAR, INTJ, LST.
Table 5 shows evaluation ofALLiS for the CoNLL-2000 shared task.Re ferencesSteven Abney.
1996.
Partial parsing via finite-statecascades.
In Proceedings of the ESSLLI '96 Ro-bust Parsing Workshop.Herv4 D4jean.
2000a.
Theory refinement and natu-ral language l arning.
In COLING'2000.Herv4 D4jean.
2000b.
A Symbolic system for natu-ral language l arning.
In CONLL'2000.Claire Grover, Andrei Mikheev, andColin Matheson, 1999.
LT TTT  ver-sion 1.
O: Text Tokenisation Software.http://www.ltg.ed.ac.uk/software/ttt/.Lauri Karttunen, Tam~s Ga~il, and Andr4 Kempe.1997.
Xerox finite-state tool.
Technical report,Xerox Research Centre Europe, Grenoble.135
