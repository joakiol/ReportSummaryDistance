Coling 2010: Poster Volume, pages 1086?1094,Beijing, August 2010A Global Relaxation Labeling Approach to Coreference ResolutionEmili Sapena, Llu?
?s Padro?
and Jordi Turmo?TALP Research CenterUniversitat Polite`cnica de Catalunya{esapena, padro, turmo}@lsi.upc.eduAbstractThis paper presents a constraint-basedgraph partitioning approach to corefer-ence resolution solved by relaxation label-ing.
The approach combines the strengthsof groupwise classifiers and chain forma-tion methods in one global method.
Ex-periments show that our approach signifi-cantly outperforms systems based on sep-arate classification and chain formationsteps, and that it achieves the best resultsin the state of the art for the same datasetand metrics.1 IntroductionCoreference resolution is a natural language pro-cessing task which consists of determining thementions that refer to the same entity in a textor discourse.
A mention is a noun phrase refer-ring to an entity and includes named entities, def-inite noun phrases, and pronouns.
For instance,?Michael Jackson?
and ?the youngest of Jackson5?
are two mentions referring to the same entity.A typical machine learning-based coreferenceresolution system usually consists of two steps:(i) classification, where the system evaluates thecoreferentiality of each pair or group of mentions,and (ii) formation of chains, where given the con-fidence values of the previous classifications thesystem forms the coreference chains.
?Research supported by the Spanish Science and In-novation Ministry, via the KNOW2 project (TIN2009-14715-C04-04) and from the European Community?s Sev-enth Framework Programme (FP7/2007-2013) under GrantAgreement number 247762 (FAUST)Regarding the classification step, pioneer sys-tems developed were based on pairwise classi-fiers.
Given a pair of mentions, the process gen-erates a feature vector and feeds it to a classi-fier.
The resolution is done by considering eachmention of the document as anaphor1 and look-ing backward until the antecedent is found orthe beginning of the document is reached (Aoneand Bennett, 1995; McCarthy and Lehnert, 1995;Soon et al, 2001).A first approach towards groupwise classifiersis the twin-candidate model (Yang et al, 2003).The model faces the problem as a competition be-tween two candidates to be the antecedent of theanaphor into account.
Each candidate mention iscompared with all the others in a round robin con-test.
Following the groupwise approach, rankersconsider all the possible antecedent mentions atonce (Denis and Baldridge, 2008).
Rankers canobtain more accurate results due to a more in-formed context where all candidate mentions areconsidered at the same time.Coreference chains are formed after classifi-cation.
Many systems form the chains by join-ing each positively-classified pair (i.e.
single-link) or with simple improvements such as linkingan anaphor only to its antecedent with maximumconfidence value (Ng and Cardie, 2002).Some works propose more elaborated methodsthan single-link for chain formation.
The ap-proaches used are Integer Linear Programming1Typically a pair of coreferential mentions mi and mj(i < j) are called antecedent and anaphor respectively,though mj may not be anaphoric.1086(ILP) (Denis and Baldridge, 2007; Klenner andAilloud, 2009; Finkel and Manning, 2008), graphpartitioning (Nicolae and Nicolae, 2006), andclustering (Klenner and Ailloud, 2008).
The mainadvantage of these types of post-processes is theenforcement of transitivity sorting out the con-tradictions that the previous classification processmay introduce.Although chain formation processes search forglobal consistency, the lack of contextual infor-mation in the classification step is propagated for-ward.
Few works try to overcome the limita-tions of keeping classification and chain formationapart.
Luo et al (2004) search the most proba-ble path comparing each mention with the partial-entities formed so far using a Bell tree struc-ture.
McCallum and Wellner (2005) propose agraph partitioning cutting by distances, with thepeculiarity that distances are learned consideringcoreferential chains of the labeled data instead ofpairs.
Culotta et al (2007) combine a groupwiseclassifier with a clustering process in a First-Orderprobabilistic model.The approach presented in this paper followsthe same research line of joining group classifi-cation and chain formation in the same step.
Con-cretely, we propose a graph representation of theproblem solved by a relaxation labeling process,reducing coreference resolution to a graph par-titioning problem given a set of constraints.
Inthis manner, decisions are taken considering thewhole set of mentions, ensuring consistency andavoiding that classification decisions are indepen-dently taken.
Our experimental results on theACE dataset show that our approach outperformssystems based on separate classification and chainformation steps, and that it achieves the best re-sults in the state of the art for the same dataset andmetrics.The paper is organized as follows.
Section 2 de-scribes the graph representation of the task.
Sec-tion 3 explains the use of relaxation labeling algo-rithm and the machine learning process.
Finally,experiments and results are explained in Section 4before paper is concluded.2 Graph RepresentationLet G = G(V,E) be an undirected graph whereV is a set of vertices and E a set of edges.
Letm = (m1, ...,mn) be the set of mentions of adocument with n mentions to resolve.
Each men-tion mi in the document is represented as a vertexvi ?
V .
An edge eij ?
E is added to the graph forpairs of vertices (vi, vj) representing the possibil-ity that both mentions corefer.
The list of adjacentvertices of a vertex vi is A(vi).Let C be our set of constraints.
Given a pair ofmentions (mi, mj), a subset of constraints Cij ?C restrict the compatibility of both mentions.
Cijis used to compute the weight value of the edgeconnecting vi and vj .
Let wij ?
W be the weightof the edge eij :wij =?k?Cij?kfk(mi,mj) (1)where fk(?)
is a function that evaluates the con-straint k. And ?k is the weight associated to theconstraint k (?k and wij can be negative).In our approach, each vertex (vi) in the graphis a variable (vi) for the algorithm.
Let Li be thenumber of different values (labels) that are pos-sible for vi.
The possible labels of each variableare the partitions that the vertex can be assigned.Note that the number of partitions (entities) in adocument is unknown, but it is at most the num-ber of vertices (mentions), because in a extremecase, each mention in a document could be refer-ring to a different entity.
A vertex with index i canbe in the first i partitions (i.e.
Li = i).Each combination of labelings for the graphvertices is a partitioning (?).
The resolution pro-cess searches the partitioning ??
which optimizesthe goodness function F (?,W ), which dependson the edge weights W. In this manner, ??
is opti-mal if:F (?
?,W ) ?
F (?,W ),??
(2)The next section describes the algorithm usedin the resolution process.3 Relaxation LabelingRelaxation labeling (Relax) is a generic name fora family of iterative algorithms which perform1087function optimization, based on local information.The algorithm has been widely used to solve NLPproblems such as PoS-tagging (Ma`rquez et al,2000), chunking, knowledge integration, and Se-mantic Parsing (Atserias, 2006).Relaxation labeling solves our weighted con-straint satisfaction problem dealing with the edgeweights.
In this manner, each vertex is assigned toa partition satisfying as many constraints as pos-sible.
To do that, the algorithm assigns a pro-bability for each possible label of each variable.Let H = (h1,h2, .
.
.
,hn) be the weighted label-ing to optimize, where each hi is a vector con-taining the probability distribution of vi, that is:hi = (hi1, hi2, .
.
.
, hiLi).
Given that the resolutionprocess is iterative, the probability for label l ofvariable vi at time step t is hil(t), or simply hilwhen the time step is not relevant.The support for a pair variable-label (Sil) ex-presses how compatible is the assignment of labell to variable vi considering the labels of adjacentvariables and the edge weights.
Although severalsupport functions may be used (Torras, 1989), wechose the following one, which defines the sup-port as the sum of the edge weights that relatevariable vi with each adjacent variable vj multi-plied by the weight for the same label l of vj :Sil =?j?A(vi)wij ?
hjl (3)where wij is the edge weight obtained in Equa-tion 1.
In our version of the algorithm, A(vi) isthe list of adjacent vertices of vi but only includ-ing the ones with an index k < i. Consequently,the weights only have influence in one directionwhich is equivalent to using a directed graph.
Al-though the proposed representation is based ona general undirected graph, preliminary experi-ments showed that using directed edges yieldshigher perfomance in this particular problem.The aim of the algorithm is to find a weightedlabeling such that global consistency is maxi-mized.
Maximizing global consistency is definedas maximizing the average support for each vari-able.
Formally, H?
is a consistent labeling if:Initialize:H := H0,Main loop:repeatFor each variable viFor each possible label l for viSil =?j?A(vi) wij ?
hjlEnd forFor each possible label l for vihil(t + 1) =hil(t)?(1+Sil)?Lik=1hik(t)?
(1+Sik)End forEnd forUntil no more significant changesFigure 1: Relaxation labeling algorithmLi?l=1h?il ?
Sil ?Li?l=1hil ?
Sil ?h,?i (4)A partitioning ?
is directly obtained from theweighted labeling H assigning to each variablethe label with maximum probability.
The sup-ports and the weighted labeling depend on theedge weights (Equation 3).
To satisfy Equation4 is equivalent to satisfy Equation 2.
Many stud-ies have been done towards the demonstration ofthe consistency, convergence and cost reductionadvantages of the relaxation algorithm (Rosenfeldet al, 1976; Hummel and Zucker, 1987; Pelillo,1997).
Although some of the conditions requiredby the formal demonstrations are not fulfilled inour case, the presented algorithm ?that forces astop after a number of iterations?
has proven use-ful for practical purposes.Figure 1 shows the pseudo-code of the relax-ation algorithm.
The process updates the weightsof the labels in each step until convergence.
Theconvergence is met when no more significantchanges are done in an iteration.
Specifically,when the maximum change in an update step(maxi,l(|hil(t+1)?hil(t)|)) is lower than a param-eter , a small value (0.001 in our experiments),or a fixed number of iterations is reached (2000 inour experiments).
Finally, the assigned label for avariable is the one with the highest weight.
Figure2 shows a representation.1088Figure 2: Representation of Relax.
The vertices represent-ing mentions are connected by weighted edges eij .
Each ver-tex has a vector hi of probabilities to belong to different par-titions.
The figure shows h2, h3 and h4.3.1 ConstraintsThe performance of the resolution process de-pends on the edge weights obtained by a set ofweighted constraints (Equation 1).
Any methodor combination of methods to generate constraintscan be used.
For example, a set of constraintshandwritten by linguist experts can be added toanother automatically obtained set.This section explains the automatic constraintgeneration process carried out in this work, usinga set of feature functions and a training corpus.Ma`rquez et al (2000) have successfully used sim-ilar processes to acquire constraints for constraintsatisfaction algorithms.Each pair of mentions (mi, mj) in a trainingdocument is evaluated by a set of feature functions(Figure 3).
The values returned by these functionsform a positive example when the pair of men-tions corefer, and a negative one otherwise.
Threespecialized models are constructed depending onthe type of anaphor mention (mj) of the pair: pro-noun, named entity or nominal.For each specialized model, a decision tree(DT) is generated and a set of rules is ex-tracted with C4.5 rule-learning algorithm (Quin-lan, 1993).
These rules are our set of constraints.The C4.5rules algorithm generates a set of rulesfor each path from the learnt tree.
It then general-izes the rules by dropping conditions.The weight assigned to a constraint (?k) is itsDIST: Distance betweenmi andmj in sentences: numberDIST MEN: Distance betweenmi andmj in mentions: numberAPPOSITIVE: One mention is in apposition with the other: y,nI/J IN QUOTES:mi/j is in quotes or inside a NP or a sentencein quotes: y,nI/J FIRST:mi/j is the first mention in the sentence: y,nI/J DEF NP:mi/j is a definitive NP: y,nI/J DEM NP:mi/j is a demonstrative NP: y,nI/J INDEF NP:mi/j is an indefinite NP: y,nSTR MATCH: String matching ofmi andmj : y,nPRO STR: Both are pronouns and their strings match: y,nPN STR: Both are proper names and their strings match: y,nNONPRO STR: String matching like in Soon et al (2001)and mentions are not pronouns: y,nHEAD MATCH: String matching of NP heads: y,nNUMBER: The number of both mentions match: y,n,uGENDER: The gender of both mentions match: y,n,uAGREEMENT: Gender and number of bothmentions match: y,n,uI/J THIRD PERSON:mi/j is 3rd person: y,nPROPER NAME: Both mentions are proper names: y,n,uI/J PERSON:mi/j is a person (pronoun orproper name in a list): y,nANIMACY: Animacy of both mentions match(persons, objects): y,nI/J REFLEXIVE:mi/j is a reflexive pronoun: y,nI/J TYPE:mi/j is a pronoun (p), entity (e) or nominal (n)NESTED: One mention is included in the other: y,nMAXIMALNP: Both mentions have the same NP parentor they are nested: y,nI/J MAXIMALNP:mi/j is not included in anyother mention: y,nI/J EMBEDDED:mi/j is a noun and is not a maximal NP: y,nBINDING: Conditions B and C of binding theory: y,nSEMCLASS: Semantic class of both mentions match: y,n,u(the same as Soon et al (2001))ALIAS: One mention is an alias of the other: y,n,u(only entities, else unknown)Figure 3: Feature functions usedprecision over the training data (Pk), but shiftedto be zero-centered: ?k = Pk ?
0.5.3.2 PruningAnalyzing the errors of development experiments,we have found two main error patterns that can besolved by a pruning process.
First, the contribu-tion of the edge weights for the resolution dependson the size of the document.
And second, manyweak edge weights may sum up to produce a biasin the wrong direction.The weight of an edge depends on the weightsassigned for the constraints which apply to a pairof mentions according to Equation 1.
Each ver-tex is adjacent to all the other vertices.
This pro-duces that the larger the number of adjacencies,the smaller the influence of a constraint is.
A con-sequence is that resolution for large and short do-cuments has different results.Many works have to deal with similar prob-lems, specially the ones looking backward for an-tecedents.
The larger the document, the more pos-1089sible antecedents the system has to classify.
Thisproblem is usually solved looking for antecedentsin a window of few sentences, which entails anevident limitation of recall.Regarding the weak edge weights, it is notablethat some kind of mention pairs are very weaklyinformative.
For example, the pairs (pronoun,pronoun).
Many stories have a few main charac-ters which monopolize the pronouns of the doc-ument.
This produces many positive training ex-amples for pairs of pronouns matching in genderand person, which may lead the algorithm to pro-duce large coreferential chains joining all thesementions even for stories where there are manydifferent characters.
For example, we have foundin the results of some documents a huge corefer-ence chain including every pronoun ?he?.
Thisis because a pair of mentions (?he?, ?he?)
is usu-ally linked with a small positive weight.
Althoughthe highest adjacent edge weight of a ?he?
men-tion may link with the correct antecedent, the sumof several edge weights linking the mention withother ?he?
causes the problem.A pruning process is perfomed solving bothproblems and reducing computational costs fromO(n3) to O(n2).
For each vertex?s adjacency listA(vi), only a maximum of N edges remain and theothers are pruned.
Concretely, the N/2 edges withlargest positive weight and the N/2 with largestnegative weight.
The value of N is empiricallychosen by maximizing performances over trainingdata.
On the one hand, the pruning forces the max-imum adjacency to be constant and the contribu-tion of the edge weights does not depend on thesize of the document.
On the other hand, mostedges of the less informative pairs are discardedavoiding further confusion.
There are no limita-tions in distance or other restrictions which maycause a loss of recall.3.3 Initial StateThe initial state of the vertices define the a pri-ori probabilities for each vertex to be in each par-tition.
There are several possible initial states.In the case where no prior information is avail-able, a random or uniformly distributed state iscommonly used.
However, a well-informed initialstate should drive faster the relaxation process toa better solution.
This section describes the well-informed initial state chosen in our approach andthe random one.
Both are compared in the exper-iments (Section 4.2).The well-informed initial state favors the cre-ation of new chains.
Variable vi has Li = i pos-sible values while variable vi+1 has Li + 1.
Theprobability distribution of vi+1 is equiprobable forvalues from 1 to Li but it is the double for the pro-bability to start a new chain Li + 1.hil = 1Li+1 , ?l = 1..Li ?
1hiLi =2Li+1Pronouns do not follow this distribution but atotally equiprobable one, given that they are usu-ally anaphoric.hil = 1Li , ?l = 1..LiThis configuration enables the resolution pro-cess to determine as singletons the mentions forwhich little evidence is available.
This small dif-ference between initial probability weights is alsointroduced in order to avoid exceptional caseswhere all support values contribute with the samevalue.The random initial state is also used in ourexperiments to test that our proposed configura-tion is better-informed than random.
Given theequiprobability state, we add a random value toeach probability to be in a partition:hil = 1Li + il, ?l = 0..Liwhere il is a random value ?12Li ?
il ?
12Li .These little random differences may help the algo-rithm to avoid local minima.3.4 ReorderingThe vertices of the graph would usually be placedin the same order as the mentions are found in thedocument (chronological).
In this manner, vi cor-responds to mi.
However, as suggested by Luo(2007), there is no need to generate the modelfollowing that order.
In our approach, the firstvariables have a lower number of possible labels.Moreover, an error in the first variables has moreinfluence on the performance than an error in thelater ones.
Placing named entities at the beginningis reasonably to expect that is helpful for the al-gorithm, given that named entities are usually themost informative mentions.1090Tokens Mentions Entitiesbnews train 66627 9937 4408bnews test 17463 2579 1040npaper train 68970 11283 4163npaper test 17404 2483 942nwire train 70832 10693 4297nwire test 16772 2608 1137Figure 4: Statistics about ACE-phase02Suppose we have three mentions appearing inthis order somewhere in a document: ?A.
Smith?,?he?, ?Alice Smith?.
For proximity, mention ?he?may tend to link with ?A.
Smith?.
Then, the thirdmention ?Alice Smith?
clearly is the whole nameof ?A.
Smith?
but the gender with ?he?
does notagree.
Given that our implementation acts like adirected graph only looking backward (see Sec-tion 3), mention ?he?
won?t change its tendencyand it may cause a split in the ?Alice Smith?
coref-erence chain.
However, having named entities infirst place and pronouns at the end, enables themention ?he?
to determine that ?A.
Smith?
and?Alice Smith?
having the same label are not goodantecedents.Reordering only affects on the number of pos-sible labels of the variables and the list of adjacen-cies A(vi).
The chronological order of the docu-ment is taken into account by the constraints re-gardless of the graph representation.
Our experi-ments confirm (Section 4) that placing first namedentity mentions, then nominal mentions and fi-nally the pronouns, the precision increases consid-erably.
Inside of each of these groups, the order isthe same order of the document.4 Experiments and ResultsWe evaluate our approach to coreference res-olution using ACE-phase02 corpus, which iscomposed of three sections: Broadcast News(BNEWS), Newswire (NWIRE) and Newspaper(NPAPER).
Each section is in turn composed of atraining set and a test set.
Figure 4 shows somestatistics about this corpus.In our experiments, we consider the true men-tions of ACE.
This is because our focus is onevaluating pairwise approach versus the graphpartitioning approach and also comparing themto some state-of-the-art approaches which alsouse true mentions.
Moreover, details on men-tion identifier systems and their performances arerarely published by the systems based on auto-matic identification of mentions and it difficultsthe comparison.To evaluate our system we use CEAF (Luo,2005) and B3 (Bagga and Baldwin, 1998).
CEAFis computed based on the best one-to-one map be-tween key coreference chains and response ones.We use the mention-based similarity metric whichcounts the number of common mentions sharedby key coreference chains and response ones.
Aswe are using true mentions for the experiments,precision, recall and F1 are the same value andonly F1 is shown.
B3 scorer is used for com-parison reasons.
B3 algorithm looks at the pres-ence/absence of mentions for each entity in thesystem output.
Precision and recall numbers arecomputed for each mention, and the average givesthe final precision and recall numbers.MUC scorer (Vilain et al, 1995) is not usedin our experiments.
Although it has been widelyused in the state of the art, we consider the newermetrics have overcome some MUC limitations(Bagga and Baldwin, 1998; Luo, 2005; Klennerand Ailloud, 2008; Denis and Baldridge, 2008).Our preprocessing pipeline consists ofFreeLing (Atserias et al, 2006) for sentencesplitting and tokenization, SVMTool (Gimenezand Marquez, 2004) for part of speech taggingand BIO (Surdeanu et al, 2005) for named entityrecognition and classification.
No lemmatizationneither syntactic analysis are used.4.1 Baselines4.1.1 DT with automatic feature selectionThe baseline developed in our work is based onSoon et al (2001) with the improvements of Ngand Cardie (2002), which uses a Decision Tree(DT).
Many research works use the same refe-rences in order to evaluate possible improvementsdone by their new models or by the incorporationof new features.The features used in the baseline are the samethan those used in our proposed system (Figure3).
However, some features are noisy and manyothers have redundancy which causes low perfor-mances using DTs.
In order to select the best set1091bnews npaper nwire GlobalMetric: CEAF CEAF B3Model F1 F1 F1 F1 P R F1DT 60.6 57.8 60.5 59.7 61.0 74.1 66.9DT Hill 67.8 61.6 65.0 64.8 74.7 69.8 72.2Table 1: Results ACE-phase02.
Comparing baselines based on Decision Trees.bnews npaper nwire GlobalMetric: CEAF CEAF B3Model F1 F1 F1 F1 P R F1DT 60.6 59.5 64.7 61.7 63.3 74.7 68.5DT + ILP 62.8 60.3 63.7 62.5 72.4 69.2 70.7DT Hill 67.8 63.2 67.2 66.5 76.8 71.0 73.8DT Hill + ILP 67.6 63.5 66.7 66.3 80.0 68.3 73.7Relax 69.5 68.3 73.0 70.4 86.5 67.9 76.1Table 2: Results on documents shorter than 200 mentions of ACE-phase02of features a Hill Climbing process has been per-formed doing a five-fold cross-validation over thetraining corpus.
A similar feature selection pro-cess has been done by Hoste (2005).The Hill Climbing process starts using thewhole set of features.
A cross-validation is done(un)masking each feature.
The (un)masked fea-ture with more improvement is (added to) re-moved from the set.
The process is repeated untilan iteration without improvements is reached.Note that this optimization process is biased bythe metric used to evaluate each feature combi-nation.
We use CEAF in our experiments, whichencourages precision and consistency.4.1.2 Integer Linear ProgrammingThe second baseline developed forms the coref-erence chains given the output of the pair classi-fication of the first baseline.
A set of binary vari-ables (xij) symbolize whether pairs of mentions(mi,mj) corefer (xij = 1) or not (xij = 0).
Anobjective function is defined as follows:min?i<j ?log(Pcij)xij ?
log(1?
Pcij)(1?
xij)where Pcij is the confidence value of mentionsmi and mj to corefer obtained by the pair clas-sifier.
The minimization of the objective func-tion is done by Integer Linear Programming (ILP)in a similar way to (Klenner, 2007; Denis andBaldridge, 2007; Finkel and Manning, 2008).
Inorder to keep consistency in the results, which isthe goal of this post-process, a set of triangularconstraints is required.
For each three mentionswith indexes i < j < k the corresponding vari-ables have to satisfy three constraints:?
xik ?
xij + xjk ?
1?
xij ?
xik + xjk ?
1?
xjk ?
xij + xik ?
1This implies that this model needs, for a doc-ument with n mentions, 12n(n ?
1) variables and12n(n ?
1)(n ?
2) constraints to assure consis-tency2.
This is an important limitation with a viewto scalability.
In our experiments only documentsshorter than 200 mentions can be solved by thisbaseline due to its computational cost.4.2 ExperimentsFour experiments have been done in order to eval-uate our proposed approach.
This section de-scribes and analyzes the results of each experi-ment.
Finally, our performances are comparedwith the state of the art.The first experiment compares the perfor-mances of our baselines (Table 1).
?DT?
is thesystem based on Decision Tree using all the fea-tures of Figure 3 and ?DT+Hill?
is a DT usingthe features selected by the Hill Climbing process(Section 4.1.1).
There is a significant improve-ment in the performances (5.1 points with CEAF,5.3 with B3) after the automatic feature selectionprocess is done.2 16n(n ?
1)(n ?
2) for each one of the three triangularconstraints1092bnews npaper nwire GlobalMetric: CEAF CEAF B3Model F1 F1 F1 F1 P R F1Relax 67.3 64.4 69.5 67.2 88.4 62.7 73.3Relax pruning 68.6 65.2 70.1 68.0 82.3 66.9 73.8Relax pruning & reorder 69.5 67.3 72.1 69.7 85.3 66.8 74.9Relax random IS 68.2 66.1 71.0 68.5 83.5 66.7 74.2MaxEnt+ILP (Denis, 2007) - - - 66.2 81.4 65.6 72.7Rankers (Denis, 2007) 65.7 65.3 68.1 67.0 79.8 66.8 72.7Table 3: Results ACE-phase02.In the second experiment the ILP chain forma-tion process is applied using the output of bothDTs.
Results are shown in Table 2.
Note that ILPonly applies to documents shorter than 200 men-tions due to its excessive computational cost (Sec-tion 4.1.2).
Results for Relax applied to the samedocuments are also included for comparison.
ILPforces consistency of the results producing an in-crease in precision score with B3 metric in bothcases.
However, ?DT+Hill?
has been optimizedfor CEAF metric which encourages precision andconsistency.
For this, a post-process forcing con-sistency seems unnecessary for a classifier alreadyoptimized.
Relax significantly outperforms all thebaselines.The third experiment shows the improvementsachieved by the use of pruning and reorderingtechniques (Sections 3.2 and 3.4).
Table 3 showsthe results.
Pruning improves performances withboth metrics.
B3 precision is decreased but theglobal F1 is increased due to a considerably im-provement of recall.
Reordering recovers the pre-cision lost by the pruning without loosing recall,which achieves the best performances of 69.7 withCEAF and 74.9 with B3.The fourth experiment evaluates the influenceof the initial state.
A comparison is done withthe proposed initial state (Section 3.3) and therandom one.
The results shown in Table 3for random initial state are the average of 3executions.
The system called ?Relax randomIS?
is using the same values for pruning andreordering techniques than the best result ofprevious experiment: ?Relax pruning & reorder?.As expected, results with a well-informed initialstate outperform the random ones.Finally, Relax performances are compared withthe best scores we have found using the same cor-pora and metrics.
We compare our approach withspecialized Rankers ?groupwise classifier?, anda system using ILP not only forcing consistencybut also using information about anaphoricity andnamed entities.
Relax outperforms both systemswith both metrics (Table 3).5 ConclusionThe approach for coreference resolution presentedin this paper is a constraint-based graph partition-ing solved by relaxation labeling.The decision to join or not a set of mentionsin the same entity is taken considering always thewhole set of previous mentions like in groupwiseclassifiers.
Contrarily to the approaches wherevariables are the linkage of each pair of mentions,in this model consistency is implicitly forced.Moreover, the influence of the partial results ofthe other mentions at the same time avoids thatdecisions are independently taken.The capacity to easily incorporate constraintsfrom different sources and using different know-ledge is also remarkable.
This flexibility givesa great potencial to the approach.
Anaphoricityfiltering is not needed given that the necessaryknowledge can be also introduced by constraints.In addition, three tecniques to improve resultshave been presented: reordering, pruning and fea-ture selection by Hill Climbing.
The experimentsconfirm their utility.The experimental results clearly outperform thebaselines with separate classification and chainformaiton.
The approach also outperforms oth-ers in the state of the art using same corpora andmetrics.1093ReferencesAone, C. and S.W.
Bennett.
1995.
Evaluating automatedand manual acquisition of anaphora resolution strategies.In Proceedings of the 33rd annual meeting on ACL, pages122?129.Atserias, J., B. Casas, E. Comelles, M. Gonza?lez, L. Padro?,and M. Padro?.
2006.
Freeling 1.3: Syntactic and semanticservices in an open-source nlp library.
In Proceedings ofthe fifth international conference on Language Resourcesand Evaluation (LREC 2006), ELRA.
Genoa, Italy.Atserias, J.
2006.
Towards Robustness in Natural Lan-guage Understanding.
Ph.D. Thesis, Dept.
Lenguajesy Sistemas Informa?ticos.
Euskal Herriko Unibertsitatea.Donosti.
Spain.Bagga, A. and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
Proceedings of the Linguistic Coref-erence Workshop at LREC, pages 563?566.Culotta, A., M. Wick, and A. McCallum.
2007.
First-OrderProbabilistic Models for Coreference Resolution.
Pro-ceedings of NAACL HLT, pages 81?88.Denis, P. and J. Baldridge.
2007.
Joint Determination ofAnaphoricity and Coreference Resolution using IntegerProgramming.
Proceedings of NAACL HLT, pages 236?243.Denis, P. and J. Baldridge.
2008.
Specialized models andranking for coreference resolution.
Proceedings of theEMNLP, Hawaii, USA.Denis, P. 2007.
New Learning Models for Robust Refer-ence Resolution.
Ph.D. dissertation, University of Texasat Austin.Finkel, J.R. and C.D.
Manning.
2008.
Enforcing transitivityin coreference resolution.
In Proceedings of the 46th An-nual Meeting of the ACL HLT: Short Papers, pages 45?48.Association for Computational Linguistics.Gimenez, J. and L. Marquez.
2004.
Svmtool: A general postagger generator based on support vector machines.
InProceedings of the 4th International Conference on Lan-guage Resources and Evaluation, pages 43?46.Hoste, V. 2005.
Optimization Issues in Machine Learning ofCoreference Resolution.
PhD thesis.Hummel, R. A. and S. W. Zucker.
1987.
On the foundationsof relaxation labeling processes.
pages 585?605.Klenner, M. and E?.
Ailloud.
2008.
Enhancing CoreferenceClustering.
In Proceedings of the Second Workshop onAnaphora Resolution.
WAR II.Klenner, M. and E. Ailloud.
2009.
Optimization in Corefer-ence Resolution Is Not Needed: A Nearly-Optimal Algo-rithm with Intensional Constraints.
In Proceedings of the12th Conference of the EACL.Klenner, M. 2007.
Enforcing consistency on coreferencesets.
In Recent Advances in Natural Language Processing(RANLP), pages 323?328.Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla, andS.
Roukos.
2004.
A mention-synchronous coreferenceresolution algorithm based on the bell tree.
In Proceed-ings of 42nd ACL, page 135.Luo, X.
2005.
On coreference resolution performance met-rics.
Proc.
of HLT-EMNLP, pages 25?32.Luo, X.
2007.
Coreference or not: A twin model for coref-erence resolution.
In Proceedings of NAACL HLT, pages73?80.Ma`rquez, L., L.
Padro?, and H.
Rodr??guez.
2000.
A ma-chine learning approach for pos tagging.
Machine Learn-ing Journal, 39(1):59?91.McCallum, A. and B. Wellner.
2005.
Conditional modelsof identity uncertainty with application to noun corefer-ence.
Advances in Neural Information Processing Sys-tems, 17:905?912.McCarthy, J.F.
and W.G.
Lehnert.
1995.
Using decisiontrees for coreference resolution.
Proceedings of the Four-teenth International Conference on Artificial Intelligence,pages 1050?1055.Ng, V. and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
Proceedings of the40th Annual Meeting on Association for ComputationalLinguistics, pages 104?111.Nicolae, C. and G. Nicolae.
2006.
Best Cut: A Graph Al-gorithm for Coreference Resolution.
Proceedings of the2006 Conference on EMNLP, pages 275?283.Pelillo, M. 1997.
The dynamics of nonlinear relaxation la-beling processes.
Journal of Mathematical Imaging andVision, 7(4):309?323.Quinlan, J.R. 1993.
C4.5: Programs for Machine Learning.Morgan Kaufmann.Rosenfeld, R., R. A. Hummel, and S. W. Zucker.
1976.Scene labelling by relaxation operations.
IEEE Transac-tions on Systems, Man and Cybernetics, 6(6):420?433.Soon, W.M., H.T.
Ng, and D.C.Y.
Lim.
2001.
A MachineLearning Approach to Coreference Resolution of NounPhrases.
Computational Linguistics, 27(4):521?544.Surdeanu, M., J. Turmo, and E. Comelles.
2005.
NamedEntity Recognition from Spontaneous Open-DomainSpeech.
In Ninth European Conference on Speech Com-munication and Technology.
ISCA.Torras, C. 1989.
Relaxation and neural learning: Pointsof convergence and divergence.
Journal of Parallel andDistributed Computing, 6:217?244.Vilain, M., J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreference scor-ing scheme.
Proceedings of the 6th conference on Mes-sage understanding, pages 45?52.Yang, X., G. Zhou, J. Su, and C.L.
Tan.
2003.
Coreferenceresolution using competition learning approach.
In ACL?03: Proceedings of the 41st Annual Meeting on Associa-tion for Computational Linguistics, pages 176?183.1094
