Paraphrase Generation as Monolingual Translation: Data and EvaluationSander Wubben, Antal van den Bosch, Emiel KrahmerTilburg centre for Cognition and CommunicationTilburg UniversityTilburg, The Netherlands{s.wubben,antal.vdnbosch,e.j.krahmer}@uvt.nlAbstractIn this paper we investigate the auto-matic generation and evaluation of senten-tial paraphrases.
We describe a methodfor generating sentential paraphrases byusing a large aligned monolingual cor-pus of news headlines acquired automat-ically from Google News and a stan-dard Phrase-Based Machine Translation(PBMT) framework.
The output of thissystem is compared to a word substitu-tion baseline.
Human judges prefer thePBMT paraphrasing system over the wordsubstitution system.
We demonstrate thatBLEU correlates well with human judge-ments provided that the generated para-phrased sentence is sufficiently differentfrom the source sentence.1 IntroductionText-to-text generation is an increasingly studiedsubfield in natural language processing.
In con-trast with the typical natural language generationparadigm of converting concepts to text, in text-to-text generation a source text is converted into atarget text that approximates the meaning of thesource text.
Text-to-text generation extends tosuch varied tasks as summarization (Knight andMarcu, 2002), question-answering (Lin and Pan-tel, 2001), machine translation, and paraphrasegeneration.Sentential paraphrase generation (SPG) is theprocess of transforming a source sentence into atarget sentence in the same language which dif-fers in form from the source sentence, but approx-imates its meaning.
Paraphrasing is often used asa subtask in more complex NLP applications toallow for more variation in text strings presentedas input, for example to generate paraphrases ofquestions that in their original form cannot be an-swered (Lin and Pantel, 2001; Riezler et al, 2007),or to generate paraphrases of sentences that failedto translate (Callison-Burch et al, 2006).
Para-phrasing has also been used in the evaluation ofmachine translation system output (Russo-Lassneret al, 2006; Kauchak and Barzilay, 2006; Zhouet al, 2006).
Adding certain constraints to para-phrasing allows for additional useful applications.When a constraint is specified that a paraphraseshould be shorter than the input text, paraphras-ing can be used for sentence compression (Knightand Marcu, 2002; Barzilay and Lee, 2003) as wellas for text simplification for question answering orsubtitle generation (Daelemans et al, 2004).We regard SPG as a monolingual machine trans-lation task, where the source and target languagesare the same (Quirk et al, 2004).
However, thereare two problems that have to be dealt with tomake this approach work, namely obtaining a suf-ficient amount of examples, and a proper eval-uation methodology.
As Callison-Burch et al(2008) argue, automatic evaluation of paraphras-ing is problematic.
The essence of SPG is to gen-erate a sentence that is structurally different fromthe source.
Automatic evaluation metrics in re-lated fields such as machine translation operate ona notion of similarity, while paraphrasing centersaround achieving dissimilarity.
Besides the eval-uation issue, another problem is that for an data-driven MT account of paraphrasing to work, alarge collection of data is required.
In this case,this would have to be pairs of sentences that areparaphrases of each other.
So far, paraphrasingdata sets of sufficient size have been mostly lack-ing.
We argue that the headlines aggregated byGoogle News offer an attractive avenue.2 Data CollectionCurrently not many resources are available forparaphrasing; one example is the Microsoft Para-phrase Corpus (MSR) (Dolan et al, 2004; Nelkenand Shieber, 2006), which with its 139,000 alignedPolice investigate Doherty drug picsDoherty under police investigationPolice to probe Pete picsPete Doherty arrested in drug-photo probeRocker photographed injecting unconscious fanDoherty ?injected unconscious fan with drug?Photos may show Pete Doherty injecting passed-out fanDoherty ?injected female fan?Figure 1: Part of a sample headline cluster, withaligned paraphrasesparaphrases can be considered relatively small.
Inthis study we explore the use of a large, automat-ically acquired aligned paraphrase corpus.
Ourmethod consists of crawling the headlines aggre-gated and clustered by Google News and thenaligning paraphrases within each of these clusters.An example of such a cluster is given in Figure 1.For each pair of headlines in a cluster, we calcu-late the Cosine similarity over the word vectors ofthe two headlines.
If the similarity exceeds a de-fined upper threshold it is accepted; if it is belowa defined lower threshold it is rejected.
In the casethat it lies between the thresholds, the process isrepeated but then with word vectors taken from asnippet from the corresponding news article.
Thismethod, described in earlier work Wubben et al(2009), was reported to yield a precision of 0.76and a recall of 0.41 on clustering actual Dutchparaphrases in a headline corpus.
We adapted thismethod to English.
Our data consists of Englishheadlines that appeared in Google News over theperiod of April to September 2006.
Using thismethod we end up with a corpus of 7,400,144 pair-wise alignments of 1,025,605 unique headlines1.3 Paraphrasing methodsIn our approach we use the collection of au-tomatically obtained aligned headlines to traina paraphrase generation model using a Phrase-Based MT framework.
We compare this ap-proach to a word substitution baseline.
The gen-erated paraphrases along with their source head-1This list of aligned pairs is available athttp://ilk.uvt.nl/?swubben/resources.htmllines are presented to human judges, whose rat-ings are compared to the BLEU (Papineni et al,2002), METEOR (Banerjee and Lavie, 2005) andROUGE (Lin, 2004) automatic evaluation metrics.3.1 Phrase-Based MTWe use the MOSES package to train aPhrase-Based Machine Translation model(PBMT) (Koehn et al, 2007).
Such a modelnormally finds a best translation e?
of a text inlanguage f to a text in language e by combininga translation model p(f |e) with a language modelp(e):e?
= argmaxe?e?p(f |e)p(e)GIZA++ is used to perform the word align-ments (Och and Ney, 2003) which are then used inthe Moses pipeline to generate phrase alignmentsin order to build the paraphrase model.
We first to-kenize our data before training a recaser.
We thenlowercase all data and use all unique headlines inthe training data to train a language model with theSRILM toolkit (Stolcke, 2002).
Then we invokethe GIZA++ aligner using the 7M training para-phrase pairs.
We run GIZA++ with standard set-tings and we perform no optimization.
Finally, weuse the MOSES decoder to generate paraphrasesfor our test data.Instead of assigning equal weights to languageand translation model, we assign a larger weightof 0.7 to the language model to generate betterformed (but more conservative) paraphrases.
Be-cause dissimilarity is a factor that is very impor-tant for paraphrasing but not implemented in aPBMT model, we perform post-hoc reranking ofthe different candidate outputs based on dissimi-larity.
For each headline in the testset we generatethe ten best paraphrases as scored by the decoderand then rerank them according to dissimilarity tothe source using the Levenshtein distance measureat the word level.
The resulting headlines are re-cased using the previously trained recaser.3.2 Word SubstitutionWe compare the PBMT results with a simple wordsubstitution baseline.
For each noun, adjective andverb in the sentence this model takes that word andits Part of Speech tag and retrieves from Word-Net its most frequent synonym from the most fre-quent synset containing the input word.
We use theMemory Based Tagger (Daelemans et al, 1996)System HeadlineSource Florida executes notorious serial killerPBMT Serial killer executed in FloridaWord Sub.
Florida executes ill-famed series slayerSource Dublin evacuates airport due to bomb scarePBMT Dublin airport evacuated after bomb threatWord Sub.
Dublin evacuates airdrome due to bomb panicSource N. Korea blasts nuclear sanctionsPBMT N. Korea nuclear blast of sanctionsWord Sub.
N. Korea blasts atomic sanctionsTable 1: Examples of generated paraphrased head-linestrained on the Brown corpus to generate the POS-tags.
The WordNet::QueryData2 Perl module isused to query WordNet (Fellbaum, 1998).
Gener-ated headlines and their source for both systemsare given in Table 1.4 EvaluationFor the evaluation of the generated paraphraseswe set up a human judgement study, and comparethe human judges?
ratings to automatic evaluationmeasures in order to gain more insight in the auto-matic evaluation of paraphrasing.4.1 MethodWe randomly select 160 headlines that meet thefollowing criteria: the headline has to be compre-hensible without reading the corresponding newsarticle, both systems have to be able to produce aparaphrase for each headline, and there have to bea minimum of eight paraphrases for each headline.We need these paraphrases as multiple referencesfor our automatic evaluation measures to accountfor the diversity in real-world paraphrases, as thealigned paraphrased headlines in Figure 1 witness.The judges are presented with the 160 head-lines, along with the paraphrases generated byboth systems.
The order of the headlines is ran-domized, and the order of the two paraphrases foreach headline is also randomized to prevent a biastowards one of the paraphrases.
The judges areasked to rate the paraphrases on a 1 to 7 scale,where 1 means that the paraphrase is very bad and7 means that the paraphrase is very good.
Thejudges were instructed to base their overall qualityjudgement on whether the meaning was retained,the paraphrase was grammatical and fluent, andwhether the paraphrase was in fact different from2http://search.cpan.org/dist/WordNet-QueryData/QueryData.pmsystem mean stdev.PBMT 4.60 0.44Word Substitution 3.59 0.64Table 2: Results of human judgements (N = 10)the source sentence.
Ten judges rated two para-phrases per headline, resulting in a total of 3,200scores.
All judges were blind to the purpose of theevaluation and had no background in paraphrasingresearch.4.2 ResultsThe average scores assigned by the human judgesto the output of the two systems are displayed inTable 2.
These results show that the judges ratedthe quality of the PBMT paraphrases significantlyhigher than those generated by the word substitu-tion system (t(18) = 4.11, p < .001).Results from the automatic measures as wellas the Levenshtein distance are listed in Table 3.We use a Levenshtein distance over tokens.
First,we observe that both systems perform roughly thesame amount of edit operations on a sentence, re-sulting in a Levenshtein distance over words of2.76 for the PBMT system and 2.67 for the WordSubstitution system.
BLEU, METEOR and threetypical ROUGE metrics3 all rate the PBMT sys-tem higher than the Word Substitution system.Notice also that the all metrics assign the high-est scores to the original sentences, as is to be ex-pected: because every operation we perform is inthe same language, the source sentence is also aparaphrase of the reference sentences that we usefor scoring our generated headline.
If we pick arandom sentence from the reference set and scoreit against the rest of the set, we obtain similarscores.
This means that this score can be regardedas an upper bound score for paraphrasing: we cannot expect our paraphrases to be better than thoseproduced by humans.
However, this also showsthat these measures cannot be used directly as anautomatic evaluation method of paraphrasing, asthey assign the highest score to the ?paraphrase?
inwhich nothing has changed.
The scores observedin Table 3 do indicate that the paraphrases gener-3ROUGE-1, ROUGE-2 and ROUGE-SU4 are alsoadopted for the DUC 2007 evaluation campaign,http://www-nlpir.nist.gov/projects/duc/duc2007/tasks.htmlSystem BLEU ROUGE-1 ROUGE-2 ROUGE-SU4 METEOR Lev.dist.
Lev.
stdev.PBMT 50.88 0.76 0.36 0.42 0.71 2.76 1.35Wordsub.
24.80 0.59 0.22 0.26 0.54 2.67 1.50Source 60.58 0.80 0.45 0.47 0.77 0 0Table 3: Automatic evaluation and sentence Levenshtein scores0 1 2 3 4 5 6Levenshtein distance00.20.40.60.8correlationBLEUROUGE-1ROUGE-2ROUGE-SU4METEORFigure 2: Correlations between human judge-ments and automatic evaluation metrics for vari-ous edit distancesated by PBMT are less well formed than the orig-inal source sentence.There is an overall medium correlation betweenthe BLEU measure and human judgements (r =0.41, p < 0.001).
We see a lower correlationbetween the various ROUGE scores and humanjudgements, with ROUGE-1 showing the highestcorrelation (r = 0.29, p < 0.001).
Between thetwo lies the METEOR correlation (r = 0.35, p <0.001).
However, if we split the data according toLevenshtein distance, we observe that we gener-ally get a higher correlation for all the tested met-rics when the Levenshtein distance is higher, asvisualized in Figure 2.
At Levenshtein distance 5,the BLEU score achieves a correlation of 0.78 withhuman judgements, while ROUGE-1 manages toachieve a 0.74 correlation.
Beyond edit distance5, data sparsity occurs.5 DiscussionIn this paper we have shown that with an automat-ically obtained parallel monolingual corpus withseveral millions of paired examples, it is possi-ble to develop an SPG system based on a PBMTframework.
Human judges preferred the outputof our PBMT system over the output of a wordsubstitution system.
We have also addressed theproblem of automatic paraphrase evaluation.
Wemeasured BLEU, METEOR and ROUGE scores,and observed that these automatic scores corre-late with human judgements to some degree, butthat the correlation is highly dependent on editdistance.
At low edit distances automatic metricsfail to properly assess the quality of paraphrases,whereas at edit distance 5 the correlation of BLEUwith human judgements is 0.78, indicating that athigher edit distances these automatic measures canbe utilized to rate the quality of the generated para-phrases.
From edit distance 2, BLEU correlatesbest with human judgements, indicating that MTevaluation metrics might be best for SPG evalua-tion.The data we used for paraphrasing consists ofheadlines.
Paraphrase patterns we learn are thoseused in headlines and therefore different fromstandard language.
The advantage of our approachis that it paraphrases those parts of sentences thatit can paraphrase, and leaves the unknown partsintact.
It is straightforward to train a languagemodel on in-domain text and use the translationmodel acquired from the headlines to generateparaphrases for other domains.
We are also inter-ested in capturing paraphrase patterns from otherdomains, but acquiring parallel corpora for thesedomains is not trivial.Instead of post-hoc dissimilarity reranking ofthe candidate paraphrase sentences we intend todevelop a proper paraphrasing model that takesdissimilarity into account in the decoding pro-cess.
In addition, we plan to investigate if ourparaphrase generation approach is applicable tosentence compression and simplification.
On thetopic of automatic evaluation, we aim to definean automatic paraphrase generation assessmentscore.
A paraphrase evaluation measure should beable to recognize that a good paraphrase is a well-formed sentence in the source language, yet it isclearly dissimilar to the source.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, June.Regina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: an unsupervised approach usingmultiple-sequence alignment.
In NAACL ?03: Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology, pages16?23.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine transla-tion using paraphrases.
In Proceedings of the mainconference on Human Language Technology Con-ference of the North American Chapter of the Asso-ciation of Computational Linguistics, pages 17?24.Chris Callison-Burch, Trevor Cohn, and Mirella Lap-ata.
2008.
Parametric: an automatic evaluation met-ric for paraphrasing.
In COLING ?08: Proceedingsof the 22nd International Conference on Computa-tional Linguistics, pages 97?104.Walter Daelemans, Jakub Zavrel, Peter Berck, andSteven Gillis.
1996.
Mbt: A memory-based part ofspeech tagger-generator.
In Proc.
of Fourth Work-shop on Very Large Corpora, pages 14?27.Walter Daelemans, Anja Hothker, and Erik TjongKim Sang.
2004.
Automatic sentence simplificationfor subtitling in dutch and english.
In Proceedings ofthe 4th International Conference on Language Re-sources and Evaluation, pages 1045?1048.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: exploiting massively parallel news sources.
InCOLING ?04: Proceedings of the 20th internationalconference on Computational Linguistics, page 350.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database, May.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedingsof the Human Language Technology Conference ofthe NAACL, Main Conference, pages 455?462, June.Kevin Knight and Daniel Marcu.
2002.
Summa-rization beyond sentence extraction: a probabilis-tic approach to sentence compression.
Artif.
Intell.,139(1):91?107.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens,Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In ACL.Dekang Lin and Patrick Pantel.
2001.
Dirt: Discov-ery of inference rules from text.
In KDD ?01: Pro-ceedings of the seventh ACM SIGKDD internationalconference on Knowledge discovery and data min-ing, pages 323?328.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Proc.
ACL workshop onText Summarization Branches Out, page 10.Rani Nelken and Stuart M. Shieber.
2006.
Towards ro-bust context-sensitive sentence alignment for mono-lingual corpora.
In Proceedings of the 11th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL-06), 3?7 April.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Comput.
Linguist., 29(1):19?51, March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In ACL ?02: Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics, pages 311?318.Chris Quirk, Chris Brockett, and William Dolan.2004.
Monolingual machine translation for para-phrase generation.
In Dekang Lin and Dekai Wu,editors, Proceedings of EMNLP 2004, pages 142?149, July.Stefan Riezler, Alexander Vasserman, IoannisTsochantaridis, Vibhu O. Mittal, and Yi Liu.
2007.Statistical machine translation for query expansionin answer retrieval.
In ACL.Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.2006.
A paraphrase-based approach to machinetranslation evaluation.
Technical report, Universityof Maryland, College Park.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In In Proc.
Int.
Conf.
on SpokenLanguage Processing, pages 901?904.Sander Wubben, Antal van den Bosch, Emiel Krahmer,and Erwin Marsi.
2009.
Clustering and matchingheadlines for automatic paraphrase acquisition.
InENLG ?09: Proceedings of the 12th European Work-shop on Natural Language Generation, pages 122?125.Liang Zhou, Chin-Yew Lin, and Eduard Hovy.
2006.Re-evaluating machine translation results with para-phrase support.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 77?84, July.
