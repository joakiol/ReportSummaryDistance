Proceedings of the 6th Workshop on Statistical Machine Translation, pages 457?463,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe Universitat d?Alacant hybrid machine translation system for WMT 2011V?
?ctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-Mart?
?nez, Juan Antonio Pe?rez-OrtizTransducens Research GroupDepartament de Llenguatges i Sistemes Informa`ticsUniversitat d?Alacant, E-03071, Alacant, Spain{vmsanchez,fsanchez,japerez}@dlsi.ua.esAbstractThis paper describes the machine translation(MT) system developed by the TransducensResearch Group, from Universitat d?Alacant,Spain, for the WMT 2011 shared transla-tion task.
We submitted a hybrid system forthe Spanish?English language pair consist-ing of a phrase-based statistical MT systemwhose phrase table was enriched with bilin-gual phrase pairs matching transfer rules anddictionary entries from the Apertium shallow-transfer rule-based MT platform.
Our hybridsystem outperforms, in terms of BLEU, GTMand METEOR, a standard phrase-based statis-tical MT system trained on the same corpus,and received the second best BLEU score inthe automatic evaluation.1 IntroductionThis paper describes the system submitted bythe Transducens Research Group (Universitatd?Alacant, Spain) to the shared translation task ofthe EMNLP 2011 Sixth Workshop on StatisticalMachine Translation (WMT 2011).
We partici-pated in the Spanish?English task with a hybrid sys-tem that combines, in a phrase-based statistical ma-chine translation (PBSMT) system, bilingual infor-mation obtained from parallel corpora in the usualway (Koehn, 2010, ch.
5), and bilingual informa-tion from the Spanish?English language pair in theApertium (Forcada et al, 2011) rule-based machinetranslation (RMBT) platform.A wide range of hybrid approaches (Thurmair,2009) may be taken in order to build a machinetranslation system which takes advantage of a par-allel corpus and explicit linguistic information fromRBMT.
In particular, our hybridisation approach di-rectly enriches the phrase table of a PBSMT systemwith phrase pairs generated from the explicit lin-guistic resources from an Apertium-based shallow-transfer RBMT system.
Apertium, which is de-scribed in detail below, does not perform a completesyntactic analysis of the input sentences, but ratherworks with simpler linear intermediate representa-tions.The rest of the paper is organised as follows.
Nextsection overviews the two MT systems we combinein our submission.
Section 3 outlines related hybridapproaches, whereas our approach is described inSection 4.
Sections 5 and 6 describe, respectively,the resources we used to build our submission andthe results achieved for the Spanish?English lan-guage pair.
The paper ends with some concludingremarks.2 Translation approachesWe briefly describe the rationale behind the PBSMT(section 2.1) and the shallow-transfer RBMT (sec-tion 2.2) systems we have used in our hybridisationapproach.2.1 Phrase-based statistical machinetranslationPhrase-based statistical machine translation sys-tems (Koehn et al, 2003) translate sentences bymaximising the translation probability as definedby the log-linear combination of a number of fea-ture functions, whose weights are chosen to opti-457mise translation quality (Och, 2003).
A core com-ponent of every PBSMT system is the phrase ta-ble, which contains bilingual phrase pairs extractedfrom a bilingual corpus after word alignment (Ochand Ney, 2003).
The set of translations from whichthe most probable one is chosen is built by segment-ing the source-language (SL) sentence in all possi-ble ways and then combining the translation of thedifferent source segments according to the phrase ta-ble.
Common feature functions are: source-to-targetand target-to-source phrase translation probabilities,source-to-target and target-to-source lexical weight-ings (calculated by using a probabilistic bilingualdictionary), reordering costs, number of words inthe output (word penalty), number of phrase pairsused (phrase penalty), and likelihood of the outputas given by a target-language (TL) model.2.2 Shallow-transfer rule-based machinetranslationThe RBMT process (Hutchins and Somers, 1992)can be split into three different steps: i) analysis ofthe SL text to build a SL intermediate representation,ii) transfer from that SL intermediate representationto a TL intermediate representation, and iii) genera-tion of the final translation from the TL intermediaterepresentation.Shallow-transfer RBMT systems use relativelysimple intermediate representations, which arebased on lexical forms consisting of lemma, partof speech and morphological inflection informationof the words in the input sentence, and apply sim-ple shallow-transfer rules that operate on sequencesof lexical forms: this kind of systems do not per-form a full parsing.
Apertium (Forcada et al, 2011),the shallow-transfer RBMT platform we have used,splits the transfer step into structural and lexicaltransfer.
The lexical transfer is done by using a bilin-gual dictionary which, for each SL lexical form, al-ways provides the same TL lexical form; thus, nolexical selection is performed.
Multi-word expres-sions (such as on the other hand, which acts as asingle adverb) may be analysed by Apertium to (orgenerated from) a single lexical form.Structural transfer in Apertium is done by apply-ing a set of rules in a left-to-right, longest-matchfashion to prevent the translation from being per-formed word for word in those cases in which thiswould result in an incorrect translation.
Structuraltransfer rules process sequences of lexical forms byperforming operations such as reorderings or gen-der and number agreements.
For the translation be-tween non-related language pairs, such as Spanish?English, the structural transfer may be split intothree levels in order to facilitate the writing of rulesby linguists.
The first level performs short-distanceoperations, such as gender and number agreementbetween nouns and adjectives, and groups sequencesof lexical forms into chunks; second-level rules per-form inter chunk operations, such as agreements be-tween more distant constituents (i.e.
subject andmain verb); and third-level ones de-encapsulate thechunks and generate a sequence of TL lexical formsfrom each chunk.
Note that, although the multi-level shallow transfer allows performing operationsbetween words which are distant in the source sen-tence, shallow-transfer RBMT systems are less pow-erful that the ones which perform full parsing.
In ad-dition, each lexical form is processed at most by onerule in the same level.The following example illustrates how lexical andstructural transfer are performed in Apertium.
Sup-pose that the Spanish sentence Por otra parte misamigos americanos han decidido venir is to be trans-lated into English.
First, it is analysed as:por otra parte<adv>m?
?o<det><pos><mf><pl>amigo<n><m><pl>americano<adj><m><pl>haber<vbhaver><pri><p3><pl>decidir<vblex><pp><m><sg>venir<vblex><inf>which splits the sentence in seven lexical forms: amulti-word adverb (por otra parte), a plural pos-sessive determiner (m?
?o), a noun and an adjectivein masculine plural (amigo and americano, respec-tively), the third-person plural form of the presenttense of the verb to be (haber), the masculine sin-gular past participle of the verb decidir and the verbvenir in infinitive mood.
Then, the transfer step isexecuted.
It starts by performing the lexical trans-fer and applying the first-level rules of the structuraltransfer in parallel.
The lexical transfer of each SLlexical form gives as a result:on the other hand<adv>my<det><pos><pl>friend<n><pl>american<adj>458have<vbhaver><pres>decide<vblex><pp>come<vblex><inf>Four first-level structural transfer rules are trig-gered: the first one matches a single adverb (thefirst lexical form in the example); the second onematches a determiner followed by an adjective anda noun (the next three lexical forms); the third onematches a form of the verb haber plus the past par-ticiple form of another verb (the next two lexicalforms); and the last one matches a verb in infini-tive mood (last lexical form).
Each of these first-level rules group the matched lexical forms in thesame chunk and perform local operations within thechunk; for instance, the second rule reorders the ad-jective and the noun:ADV{ on the other hand<adv> }NOUN_PHRASE{ my<det><pos><pl>american<adj> friend<n><pl> }HABER_PP{ have<vbhaver><pres>decide<vblex><pp> }INF{ come<vblex><inf> }After that, inter chunk operations are performed.The chunk sequence HABER PP (verb in presentperfect tense) INF (verb in infinitive mood) matchesa second-level rule which adds the preposition to be-tween them:ADV{ on the other hand<adv> }NOUN_PHRASE{ my<det><pos><pl>friend<n><pl> american<adj> }HABER_PP{ have<vbhaver><pres>decide<vblex><pp> }TO{ to<pr> }INF{ come<vblex><inf> }Third-level structural transfer removes chunk en-capsulations so that a plain sequence of lexical formsis generated:on the other hand<adv>my<det><pos><pl>american<adj>friend<n><pl>have<vbhaver><pres>decide<vblex><pp>to<pr> come<vblex><inf>Finally, the translation into TL is generated fromthe TL lexical forms: On the other hand my Ameri-can friends have decided to come.3 Related workLinguistic data from RBMT have already been usedto enrich SMT systems in different ways.
Bilingualdictionaries have been added to SMT systems sinceits early days (Brown et al, 1993); one of the sim-plest strategies involves adding the dictionary entriesdirectly to the training parallel corpus (Tyers, 2009;Schwenk et al, 2009).
Other approaches go beyondthat.
Eisele et al (2008) first translate the sentencesin the test set with an RBMT system, then apply theusual phrase-extraction algorithm over the resultingsmall parallel corpus, and finally add the obtainedphrase pairs to the original phrase table.
It is worthnoting that neither of these two strategies guaranteethat the multi-word expressions in the RBMT bilin-gual dictionary appearing in the sentences to trans-late will be translated as such because they may besplit into smaller units by the phrase-extraction algo-rithm.
Our approach overcomes this issue by addingthe data obtained from the RBMT system directlyto the phrase table.
Preliminary experiments withApertium data shows that our hybrid approach out-performs the one by Eisele et al (2008) when trans-lating Spanish texts into English.4 Enhancing phrase-based SMT withshallow-transfer linguistic resourcesAs already mentioned, the Apertium structural trans-fer detects sequences of lexical forms which needto be translated together to prevent them from be-ing translated word for word, which would result inan incorrect translation.
Therefore, adding to thephrase table of a PBSMT system all the bilingualphrase pairs which either match one of these se-quences of lexical forms in the structural transfer oran entry in the bilingual dictionary suffices to encodeall the linguistic information provided by Apertium.We add these bilingual phrase pairs directly to thephrase table, instead of adding them to the trainingcorpus and rely on the phrase extraction algorithm(Koehn, 2010, sec.
5.2.3), to avoid splitting themulti-word expressions provided by Apertium intosmaller phrases (Schwenk et al, 2009, sec.
2).4.1 Phrase pair generationGenerating the set of bilingual phrase pairs whichmatch bilingual dictionary entries is straightforward.First, all the SL surface forms that are recognised byApertium and their corresponding lexical forms aregenerated.
Then, these SL lexical forms are trans-459lated using the bilingual dictionary, and finally theirTL surface forms are generated.Bilingual phrase pairs which match structuraltransfer rules are generated in a similar way.
First,the SL sentences to be translated are analysed to gettheir SL lexical forms, and then the sequences of lex-ical forms that either match a first-level or a second-level structural transfer rule are passed through theApertium pipeline to get their translations.
If a se-quence of SL lexical forms is matched by more thanone structural transfer rule in the same level, it willbe used to generate as many bilingual phrase pairsas different rules it matches.
This differs from theway in which Apertium translates, since in thosecase only the longest rule would be applied.The following example illustrates this procedure.Let the Spanish sentence Por otra parte mis amigosamericanos han decidido venir, from the examplein the previous section, be one of the sentences tobe translated.
The SL sequences por otra parte, misamigos americanos, amigos americanos, han deci-dido, venir and han decidido venir would be used togenerate bilingual phrase pairs because they match afirst-level rule, a second-level rule, or both.
The SLwords amigos americanos are used twice becausethey are covered by two first-level rules: one thatmatches a determiner followed by a noun and an ad-jective, and another that matches a noun followed byan adjective.
Note that when using Apertium in theregular way, outside this hybrid approach, only thefirst rule is applied as a consequence of the left-to-right, longest match policy.
The SL words han de-cidido and venir are used because they match first-level rules, whereas han decidido venir matches asecond-level rule.It is worth noting that the generation of bilin-gual phrase pairs from the shallow-transfer rules isguided by the test corpus.
We decided to do it inthis way in order to avoid meaningless phrases andalso to make our approach computationally feasible.Consider, for instance, the rule which is triggeredevery time a determiner followed by a noun and anadjective is detected.
Generating all the possiblephrase pairs matching this rule would involve com-bining all the determiners in the dictionary with allthe nouns and all the adjectives, causing the genera-tion of many meaningless phrases, such as el nin?oinala?mbrico ?
the wireless boy.
In addition, thenumber of combinations to deal with becomes un-manageable as the length of the rule grows.4.2 Scoring the new phrase pairsState-of-the-art PBSMT systems usually attach 5scores to every phrase pair in the translation table:source-to-target and target-to-source phrase trans-lation probabilities, source-to-target and target-to-source lexical weightings, and phrase penalty.To calculate the phrase translation probabilities ofthe phrase pairs obtained from the shallow-transferRBMT resources we simply add them once to thelist of corpus-extracted phrase pairs, and then com-pute the probabilities by relative frequency as it isusually done (Koehn, 2010, sec.
5.2.5).
In this re-gard, it is worth noting that, as RBMT-generatedphrase pairs are added only once, if one of them hap-pens to share its source side with many other corpus-extracted phrase pairs, or even with a single, veryfrequent one, the RBMT-generated phrase pair willreceive lower scores, which penalises its use.
Toalleviate this without adding the same phrase pairan arbitrary amount of times, we introduce an ad-ditional boolean score to flag phrase pairs obtainedfrom the RBMT resources.The fact that the generation of bilingual phrasepairs from shallow transfer rules is guided by the testcorpus may cause the translation of a sentence to beinfluenced by other sentences in the test set.
Thishappens when the translation provided by Apertiumfor a subsegment of a test sentence matching anApertium structural transfer rule is shared with oneor more subsegments in the test corpus.
In that case,the phrase translation probability p(source|target)of the resulting bilingual phrase pair is lower thanif no subsegments with the same translation werefound.To calculate the lexical weightings (Koehn, 2010,sec.
5.3.3) of the RBMT-generated phrase pairs,the alignments between the words in the source sideand those in the target side are needed.
These wordalignments are obtained by tracing back the opera-tions carried out in the different steps of the shallow-transfer RBMT system.
Only those words whichare neither split nor joint with other words by theRBMT engine are included in the alignments; thus,multi-word expressions are left unaligned.
This isdone for convenience, since in this way multi-word460Figure 1: Example of word alignment obtained by tracing back the operations done by Apertium when translatingfrom Spanish to English the sentence Por otra parte mis amigos americanos han decidido venir.
Note that porotra parte is analysed by Apertium as a multi-word expression whose words are left unaligned for convenience (seesection 4.2).expressions are assigned a lexical weighting of 1.0.Figure 1 shows the alignment between the words inthe running example.5 System trainingWe submitted a hybrid system for the Spanish?English language pair built by following the strat-egy described above.
The initial phrase table wasbuilt from all the parallel corpora distributed as partof the WMT 2011 shared translation task, namelyEuroparl (Koehn, 2005), News Commentary andUnited Nations.
In a similar way, the languagemodel was built from the the Europarl (Koehn,2005) and the News Crawl monolingual English cor-pora.
The weights of the different feature functionswere optimised by means of minimum error ratetraining (Och, 2003) on the 2008 test set.1 Table 1summarises the data about the corpora used to buildour submission.
We also built a baseline PBSMTsystem trained on the same corpora and a reducedversion of our system whose phrase table was en-riched only with dictionary entries.The Apertium (Forcada et al, 2011) engine andthe linguistic resources for Spanish?English weredownloaded from the Apertium Subversion repos-itory.The linguistic data contains 326 228 entriesin the bilingual dictionary, 106 first-level structuraltransfer rules, and 31 second-level rules.
As en-tries in the bilingual dictionary contain mappings be-tween SL and TL lemmas, when phrase pairs match-ing the bilingual dictionary are generated all the pos-sible inflections of these lemmas are produced.We used the free/open-source PBSMT systemMoses (Koehn et al, 2007), together with theIRSTLM language modelling toolkit (Federico etal., 2008), which was used to train a 5-gram lan-1The corpora can be downloaded from http://www.statmt.org/wmt11/translation-task.html.Task Corpus SentencesLanguage modelEuroparl 2 015 440News Crawl 112 905 708Total 114 921 148TrainingEuroparl 1 786 594News Commentary 132 571United Nations 10 662 993Total 12 582 158Total clean 8 992 751Tuning newstest2008 2 051Test newstest2011 3 003Table 1: Size of the corpora used in the experiments.The bilingual training corpora has been cleaned to re-move empty parallel sentences and those which containmore than 40 tokens.guage model using interpolated Kneser-Ney dis-counting (Goodman and Chen, 1998).
Word align-ments from the training parallel corpus were com-puted by means of GIZA++ (Och and Ney, 2003).The cube pruning (Huang and Chiang, 2007) decod-ing algorithm was chosen in order to speed-up thetuning step and the translation of the test set.6 Results and discussionTable 2 reports the translation performanceas measured by BLEU (Papineni et al,2002), GTM (Melamed et al, 2003) and ME-TEOR2 (Banerjee and Lavie, 2005) for Apertiumand the three systems presented in the previoussection, as well as the size of the phrase table andthe amount of unknown words in the test set.
Thehybrid approach outperforms the baseline PBSMTsystem in terms of the three evaluation metrics.The confidence interval of the difference betweenthem, computed by doing 1 000 iterations of paired2Modules exact, stem, synonym and paraphrase (Denkowskiand Lavie, 2010) were used.461system BLEU GTM METEOR # of unknown words phrase table sizebaseline 28.06 52.40 47.27 1 447 254 693 494UA-dict 28.58 52.55 47.41 1 274 255 860 346UA 28.73 52.66 47.51 1 274 255 872 094Apertium 23.89 50.71 45.65 4 064 -Table 2: Case-insensitive BLEU, GTM, and METEOR scores obtained by the hybrid approach submitted to theWMT 2011 shared translation task (UA), a reduced version of it whose phrase table is enriched using only bilingualdictionary entries (UA-dict), a baseline PBSMT system trained with the same corpus (baseline), and Apertium on thenewstest2011 test set.
The number of unknown words and the phrase table size are also reported when applicable.bootstrap resampling (Zhang et al, 2004) witha p-level of 0.05, does not overlap with zero forany evaluation metric,3 which confirms that it isstatistically significant.
Our hybrid approach alsooutperforms Apertium in terms of the three eval-uation metrics.4 However, the difference betweenour complete hybrid system and the version whichonly takes advantage of bilingual dictionary is notstatistically significant for any metric.5The results show how the addition of RBMT-generated data leads to an improvement over thebaseline PBMST system, even though it was trainedwith a very large parallel corpus and the propor-tion of entries from the Apertium data in the phrasetable is very small (0.46%).
5.94% of the phrasepairs chosen by the decoder were generated from theApertium data.
The improvement may be explainedby the fact that the sentences in the test set belong tothe news domain and Apertium data has been devel-oped bearing in mind the translation of general texts(mainly news), whereas most of the bilingual train-ing corpus comes from specialised domains.
In addi-tion, the morphology of Spanish is quite rich, whichmakes it very difficult to find all possible inflectionsof the same lemma in a parallel corpus.
Therefore,Apertium-generated phrases, which contain hand-crafted knowledge from a general domain, cover3The confidence interval of the difference between our sys-tem and the baseline PBSMT system for BLEU, GTM andMETEOR is [0.38, 0.93], [0.06, 0.45], and [0.06, 0.42], respec-tively.4The confidence interval of the difference between ourapproach and Apertium for BLEU, GTM and METEOR is[4.35, 5.35], [1.55, 2.32], and [1.50, 2.21], respectively.5The confidence interval of the difference between our ap-proach and the reduced version which does not use structuraltransfer rules for BLEU, GTM and METEOR is [?0.07, 0.37],[?0.06, 0.27], and [?0.06, 0.26], respectively.some sequences of words in the input text which arenot covered, or are sparsely found, in the originaltraining corpora, as shown by the reduction in theamount of unknown words (1 447 unknown wordsversus 1 274).
In other words, Apertium linguisticinformation does not completely overlap with thedata learned from the parallel corpus.
Regarding thesmall difference between the hybrid system enrichedwith all the Apertium resources and the one that onlyincludes the bilingual dictionary, preliminary exper-iments shows that the impact of the shallow-transferrules is higher when the TL is highly inflected andthe SL is not, which is exactly the scenario oppositeto the one described in this paper.7 Concluding remarksWe have presented the MT system submitted bythe Transducens Research Group from Universitatd?Alacant to the WMT2011 shared translation task.This is the first submission of our team to this sharedtask.
We developed a hybrid system for the Spanish?English language pair which enriches the phrase ta-ble of a standard PBSMT system with phrase pairsgenerated from the RBMT linguistic resources pro-vided by Apertium.
Our system outperforms a base-line PBSMT in terms of BLEU, GTM and METEORscores by a statistically significant margin.AcknowledgementsWork funded by the Spanish Ministry of Scienceand Innovation through project TIN2009-14009-C02-01 and by Generalitat Valenciana through grantACIF/2010/174 (VALi+d programme).462ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for MT evaluation with improved corre-lation with human judgments.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summariza-tion, pages 65?72, Ann Arbor, Michigan.P.
F. Brown, S. A. D. Pietra, V. J. D. Pietra, M. J.Goldsmith, J. Hajic, R. L. Mercer, and S. Mohanty.1993.
But dictionaries are data too.
In Proceedings ofthe workshop on Human Language Technology, pages202?205, Princeton, New Jersey.M.
Denkowski and A. Lavie.
2010.
METEOR-NEXTand the METEOR paraphrase tables: Improved evalu-ation support for five target languages.
In Proceedingsof the ACL 2010 Joint Workshop on Statistical Ma-chine Translation and Metrics MATR, pages 339?342,Uppsala, Sweden.A.
Eisele, C. Federmann, H. Saint-Amand, M. Jelling-haus, T. Herrmann, and Y. Chen.
2008.
Using Mosesto integrate multiple rule-based machine translationengines into a hybrid system.
In Proceedings of theThird Workshop on Statistical Machine Translation,pages 179?182, Columbus, Ohio.M.
Federico, N. Bertoldi, and M. Cettolo.
2008.IRSTLM: an open source toolkit for handling largescale language models.
In INTERSPEECH-2008,pages 1618?1621, Brisbane, Australia.M.L.
Forcada, M.
Ginest?
?-Rosell, J. Nordfalk,J.
O?Regan, S. Ortiz-Rojas, J.A.
Pe?rez-Ortiz,F.
Sa?nchez-Mart?
?nez, G.
Ram?
?rez-Sa?nchez, and F.M.Tyers.
2011.
Apertium: a free/open-source platformfor rule-based machine translation.
Machine Trans-lation.
Special Issue on Free/Open-Source MachineTranslation, In press.J.
Goodman and S. F. Chen.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal Report TR-10-98, Harvard University, August.L.
Huang and D. Chiang.
2007.
Forest rescoring: Fasterdecoding with integrated language models.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 144?151, Prague,Czech Republic.W.
J. Hutchins and H. L. Somers.
1992.
An introductionto machine translation, volume 362.
Academic PressNew York.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of the Hu-man Language Technology and North American As-sociation for Computational Linguistics Conference,pages 48?54, Edmonton, Canada.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, C. Shen,W.and Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the ACL onInteractive Poster and Demonstration Sessions, pages177?180, Prague, Czech Republic.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
MT summit, 5:12?16.P.
Koehn.
2010.
Statistical Machine Translation.
Cam-bridge University Press.I.
D. Melamed, R. Green, and J. P. Turian.
2003.
Preci-sion and recall of machine translation.
In Proceedingsof the 2003 Conference of the North American Chap-ter of the Association for Computational Linguistics onHuman Language Technology, pages 61?63, Edmon-ton, Canada.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29:19?51, March.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, pages 160?167, Sapporo, Japan.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 311?318, Philadelphia, Pennsylvania.H.
Schwenk, S. Abdul-Rauf, L. Barrault, and J. Senel-lart.
2009.
SMT and SPE machine translation systemsfor WMT?09.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, pages 130?134, Athens, Greece.G.
Thurmair.
2009.
Comparing different architectures ofhybrid Machine Translation systems.
In ProceedingsMT Summit XII, Ottawa, Ontario, Canada.F.
M. Tyers.
2009.
Rule-based augmentation of trainingdata in Breton-French statistical machine translation.In Proceedings of the 13th Annual Conference of theEuropean Association for Machine Translation, pages213?217, Barcelona, Spain.Y.
Zhang, S. Vogel, and A. Waibel.
2004.
Interpret-ing BLEU/NIST scores: How much improvement dowe need to have a better system.
In Proceedings ofthe Fourth International Conference on Language Re-sources and Evaluation, pages 2051?2054, Lisbon,Portugal.463
