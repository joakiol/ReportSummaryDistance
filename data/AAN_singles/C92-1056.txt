Lexical Disambiguation using Simulated AnnealingJim Cowie, Joe Guthrie* and Louise GuthrieComputing Research LaboratoryBox 30001New Mexico State UniversityLas Cruces, NM 88003-0(301ABSTRACTThe resolution of lexical ambiguity isimportant for most natural language process-ing tasks, and a range of computationaltechniques have been proposed for its solu-tion.
None of these has yet proven effectiveon a large scale.
In this paper, we describea method for lexical disambiguation of textusing the definitions in a machine-readabledictionm~j together with the technique ofsimulated annealing.
The method operateson complete sentences and attempts to selectthe optimal combinations of word senses forall the words in the sentence simultaneously.The words in the sentences may be any ofthe 28,000 headwords in Longman's Dic-tionary of Contemporary English (LDOCE)and are disambiguated relative to the sensesgiven in LDOCE.
Our initial results on asample set of 50 sentences are comparableto those of other researchers, and the fullyautomatic method requires no hand-codingof lexical entries, or hand-tagging of text.L IntroductionThe problem of word-sense disambi-guation is central to text processing.Recently, promising computational methodshave been suggested \[Lesk, 1986; McDonald* Present address: Mathematics DepaJtment.University of Texas at El Paso, El Paso, Tx79968et al, 1990; Veronis and Ide, 1990; Wilks etal., 1990; Zernik and ,lacobs, 1990; Guthrieet al, 1991; Hearst, 1991\] which attempt ouse the local context of the word to bedisambiguated together with informationabout each of its word senses to solve thisproblem.
Lesk \[1986\] described a techniquewhich measured the amount of overlapbetween a dictionary sense definition andthe definitions of the words in the local con-text of the word to be disambiguated.
Heillustrated his method by successfullydisambiguating the word "cone" in thephrases "pine cone" and "ice cream cone".Later researchers have extended this basicidea in various ways.
Wilks et al, \[1990\]identified neighborhoods of the 2,187 con-trol vocabulary words in Longman's Dic-tionary of Contemporaay English (LDOCE)\[Procter, 1978\] based on the co-occurrenceof words in LDOCE dictionary definitions.These neighborhoods were then used toexpand the word sense definitions of theword to be disambiguated, and the overlapbetween the expanded definitions and thelocal context was used to select the correctsense of a word.
A similar method reportedby Guthrie et al, \[1991\] who defined subjectspecific neighborhoods of words, using thesubject area markings in the machine read-able version of LDOCE.
Hearst \[1991\] sug-gests using syntactic information and part-of-speech tagging to aid in the disambigua-tton.
She gathers co-occurrence informationAclxs DE COLING-92.
NANTES, 23-28 AOt\]T 1992 3 5 9 PROC, OV COLING-92, NAN'rES, AUG. 23-28, 1992from manually senseutagged text.
Zemikand Jacobs \[1990\] also derive their neigh-borhoods from a training text which hasI~en sense-tagged by hand.
Their methodincorporates other clues as to the sense ofthe word in question found in the morphol-ogy or by first tagging the text as to part ofspeech.Although each of these techniques looksomewhat promising for disambiguation, thetechniques have only been applied to severalwords, and the results have been based onexperiments which repeatedly disambiguatea single word (or in \[Zernik and Jacobs.1990\], one of three words) in a largenumber of sentences.
In the cases where asuccess rate for the technique is reported,the results vary from 35% to 80%, depend-ing on whether the correct sense is desired,or some coarser grained distraction is con-sidered acceptable.Since only one sense is computed at atime, the question arises as to whether andhow to incolporate the fact that a sense hasbeen chosen for a word when attempting todisambiguate the next.
Should this firstchoice be changed in light of how otherword senses are selected?
Although theseproblems were pointed out in Lesk's origi-nal paper, they have not yet been addressed.A method of word sense disambigua-tion which is designed to operate on a largescale and simultaneously for several wordswas suggested by Veronis and Ide \[1990\].The basis of this method is the constructionof large neural networks which have wordsand word senses (chosen from the machinereadable version of the Collins Dictionary)as nodes.
Links are established from a wordto each of its senses, and from each sense toevery word in its definition.
Inhibiting linksare constructed between senses of the sameword.
In order to disambiguate a sentence,the words in the sentence are activated inthe network, and this activation is allowedto spread with feedback.
This cycle isrepeated a presetected number of times, e.g.,100.
At the end of this process, each wordin the sentence is disambiguated by choos-ing its sense which is most highly activated.The authors report encouraging resultson word pairs such as "page" and "pen" and"pen" and "goat".
The only complete sen-tence reported on was "The young page putthe goat in the pen" in which "page" and"pen" might be expected to work together tocause the wrong sense of each to be chosen.The inclusion of the word "young" over-comes this problem, and both "page" and"pen" are correctly disambiguated.The authors report that problems arepresented by such factol~ as maintaining abalance between the activation of a wordand its senses and the fact that a word withmany senses tends to have more connectionsthan one with fewer senses.
They indicatethat matters uch as setting thresholds andrates of decay also present some difficulties.In contrast o the somewhat numericaltechniques described above, more principledmethods based on linguistic informationsuch as selection restrictions or semanticpreferences \[Wilks, 1975a; 1975b; Wilksand Fass, 1991\] have also been used for lex-ical disambiguation.
These methods requireextensive hand crafting by specialists of lex-ical items: assigning semantic ategories tonouns, preferences to verbs and adjectives,etc..
Maintaining consistency in thesecategories and preferences i  a problem, andthese methods are also susceptible to thecombinatorial explosion described above.In this paper we suggest the applicationof a computational method called simulatedannealing to this general class of methods(including some of the numerical methodsreferenced above) to allow all senses to bedetermined at once in a computationallyeffective way.
We describe the applicationof simulated annealing to a basic methodsimilar to that of Lesk \[1986\] whieh alsouses the subject area markings in LDOCE,but which doesn't make use of otherfeatures uch as part of speech tagging.
Thesimplicity of the technique makes it fullyautomatic, and it requires no hand-taggingAcrEs DE COLING-92, NANTES, 23-28 AOUT 1992 3 6 0 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992of text or hand-crafting of neighborhoods.When this basic method operates under theguidance of the simulated annealing algo-rithm, sense selections are made con-currently for all ambiguous words in thesentence in a way designed to optimize theirchoice.
The system's performance on a setof test sentences was encouraging and canbe expected to improve when some of therefinements mentioned above are incor-porated.2.
Simulated AnnealingThe method of simulated annealing\[Metropolis et al, 1953; Kirkpatrick et al,1983\] is a technique for solving large scaleproblems of combinatorial minimization.
Ithas been successfully applied to the famoustraveling salesman problem of finding theshortest route for a salesman who must visita number of cities in turn, and is now astandard method for optimizing the place-ment of circuit elements on large scaleintegrated circuits.
Simulated annealing wasapplied to parsing by Sampson \[1986\], butsince the method has not yet been widelyapplied to Computational Linguistics orNatural Language Processing, we describe itbriefly.The name of the algorithm is an anal-ogy to the process by which metals cool andanneal.
A feature of this phenomenon isthat slow cooling usually allows the metal toreach a uniform composition and aminimum energy state, while fast coolingleads to an amorphous tate with higherenergy.
In simulated annealing, a parameterT which corresponds to temperature isdecreased slowly enough to allow the sys-tem to find its minimum.The process requires a function E ofconfigurations of the system whichcorresponds to the energy.
It is E that weseek to minimize.
From a stinting point, anew configuration is randomly chosen, and anew value of E is computed.
If the new E isless than the old one, the new configurationis chosen to replace the older.
An essentialfeature of simulated annealing is that even ifthe new E is larger than the old (indicatingthat this configuration is farther away fromthe desired minimum than tile last choice),the new configuration may be chosen.
Thedecision of whether or not to replace the oldconfiguration with the new infelior one ismade probabilistically.
This feature ofallowing the algorithm to "go up hill" helpsit to avoid setting on a local minimumwhich is not the actual minimum.
Insucceeding trials, it becomes more difficultfor configurations which increase E to bechosen, and finally, when the method hasretained the same configuration for longenough, that configuration is chosen as thesolution.
In the travelnig salesman example,the configurations are the different pathsthrough the cities, and E is the total lengthof his trip.
The final configmation is anapproximation to the shortest path throughthe cities.
The next section describes howthe algorithm may be applied to word-sensedisambiguation.3.
Word-Sense DisambiguationGiven a sentence with N words, wemay represent the senses of the ith word ass i l ,  si2, ' sik,, where k~ is the number ofsenses of the ith word which appear inLDOCE.
A configuration of the system isobtained by choosing a sense for each wordin the sentence.
Our goal is to choose thatconfiguration which a human disambiguatorwould choose.
To that end, we must definea function E whose minimum we may rea-sonable expect to correspond to the correctchoice of the word senses.The value of E for a givenconfiguration is calculated in terms of thedefinitions of the N senses which make itup.
All words in these definitions arestemmed, and the results stored in a list.
Ifa subject code is given for a sense, the codeis treated as a stemmed word.
The redun-dancy R is computed by giving a stemmedword form which appears n times a score ofn-1  and adding up the scores.
Finally, E isdefined to be - 1I+R 'ACRES DI,; COLING-92, NANTES, 23-28 Ate '  1992 3 6 1 PROC.
OF COLING-92, NANTEs, Ant;.
23-28, 1992The rationale behind this choice of E isthat word senses which belong together in asentence will have more words and subjectcodes in common in their definitions (largervalues of R) than senses which do notbelong together.
Minimizing E will maxim-ize R and determine our choice of wordsenses,The starting configuration C is chosento be that in which sense number one ofeach word is chosen.
Since the senses inLDOCE are generally listed with the mostfrequently used sense first, this is a likelystarting point.
The value of E is computedfor this configuration.
The next step is tochoose at random a word number i and asense S~j of that ith word.
The configurationC' is is constnacted by replacing the oldsense of the ith word by the sense S o. LetL~E be the change fTom E to the value com-puted for C'.
If ~E < 0, then C' replaces C,and we make a new random change in C'.If A~.
> 0, we change to C' with probability~EP = e r .
In this expression, T is a constantwhose initial value is 1, and thedecision ofwhether or not to adopt C' is made by cal-ling a random number generator.
If thenumber generated is less than P, C isreplaced by C'.
Otherwise, C is retained.This process of generating newconfigurations and checking to see whetheror not to choose them is repeated on theorder of 1000 times, T is replaced by 0.9T,and the loop entered again.
Once the loop isexecuted with no change in theconfiguration, the routine ends, and this finalconfiguration tells which word seflses are tobe selected.4.
ExperimentsTo evaluate a method of word sensedtsambiguation it is necessary to check theresults by hand or have text which hasalready been disambiguated by hand to useas test data.
Since there is no general agree-ment on word senses, each system musthave its own test data.
Thus even thoughthe algorithm we have described isautomatic and has coverage of the 28, 000words in LDOCE, the evaluation is the tedi-ous hand work the system is meant o easeor eliminate.In our first experiment, he algorithmdescribed above was used to disambiguate50 example sentences from LDOCE.
A stoplist of very common words such as "the","as", and "of" was removed from each sen-tence.
The sentences then contained fromtwo to fifteen words, with an average of 5.5ambiguous words per sentence.
Definitionsin LDOCE are broken down first into broadsenses which we call "homographs", andthen into individual senses which distinguishamong the various meanings.
For example,one homograph of "bank" means roughly"something piled up."
There are five sensesin this homograph which distinguishwhether the thing piled up is snow, clouds,earth by a river, etc.Results of the algorithm were evaluatedby having a Iterate human disambiguate hesentences and comparing these choices ofword senses with the output of the program.Using the human choices as the standard,the algorithm correctly disambiguated 47%of the words to the sense level, and 72 % tothe homograph level.More recently we have developed asoftware tool to improve the process ofmanual disambiguation of test sentences.Slight modifications to the software allow itto be used in conjunction with the algorithmas a computer aided disambiguation system.The software displays the text to be disam-biguated in a window, and when the userchooses a word, all its definitions aredisplayed in another window.
The userthen selects the appropriate sense, and thisselection is added to a file corresponding tothe original text.
This file is called the keyand the results of the algorithm are scoredagainst it.Using this tool, 17 sentences for theWall Street Journal were disambiguated byhand relative to LDOCE.
The same stop listAcI'~ DE COLING-92, NANTES, 23-28 Ao(rr 1992 3 6 2 Pgoc.
OF COLING-92, NANTES, AUG. 23-28, 1992of common words was used as in the firstexperiment.
The algorithm was used todisambiguate the 17 sentences, and theresults automatically scored against he key.Results for the Wall Street Journal sentenceswere similar to those for the first experi-ment.One difficulty with the present algo-rithm is that long definitions tend to begiven preference over shorter ones.
Wordsdefined succinctly by a synonym are greatlypenalized.
The function E must be made tobetter model the problem to improve perfor-mance.
On the other hand, the simulatedannealing itself seems to be doing very wellat finding the minimum.
In those caseswhere the configuration selected is not thecorrect disambiguation of the sentence, thecorrect disambiguation ever had a lowervalue of E than the configuration selected.Experiments in which we varied the begin-ning temperature and the rate of coolingdidn't change tile configuration ultimatelyselected and seemed to show that thoseparameters are not very delicate.Direct comparisons of these successrates with those of other methods is difficult.Veronis and Ide \[1990\] propose a large scalemethod, but results are reported for only onesentence, and no success rate is given.None of the other methods was used todisambiguate very ambiguous word in asentence.
They were applied to one, or atmost a few, highly ambiguous words.
Itappears that in some cases the fact that oursuccess rates include not only highly ambi-guous words, but some words w~th only afew senses is offset by the fact that otherresearchers have used a broader definition ofword sense.
For example, the four senses of"interest" used by Zernlk and Jacobs \[1990\]may correspond more closely to our twohomographs and not our ten senses of"interest."
Their success rate in tagging thethree words "interest", "stock", and "bond"was 70%.
Thus it appears that the methodwe propose is comparable in effectiveness tothe other computational methods of word-sense disambiguation, and has the advan-tages of being automatically applicable to allthe 28,000 words in LDOCE and of beingcomputationally pructical.Below we give two examples of theresults of the technique.
The words follow-ing the arrow are the stemmed wordsselected from the definitions and used to cal-culate the redundancy.
The headword andsense numbers are those used in themachine readable version of LDOCE.EXAMPLE SENTENCE 1The fish floundered on the river bank,struggling to breatheDISAMBIGUATION1) fish hw 1 sense !
: DEF -> fishcreature whose blood change tempera-ture according around live water use itsFIN tail swim2) river hw 0 sense 1 : DEF -> river widenature stream water flow between banklake another sea3) bank hw 1 sense 1 : DEF -> bank landalong side river lake4) sta-uggle hw 1 sense 0 : DEF -> s~ug-gle violent move fight against hing5) breathe hw 0 sense 2 : DEF -> breathelight liveEXAMPLE SENTENCE 2The interest on myaccrued over the yearsDISAMBIGUATIONbank account1) interest hw 1 sense 6 : DEF -> interestmoney paid use2) bank hw 4 sense 1 : DEF -> bank placemoney keep paid demand where relatedactivity go3) account hw 1 sense 5 : DEF -> accountrecord state money receive paid bankbusy particular period dateACRES DE COLING-92, NAI'CI'ES, 23-28 Ao~zr 1992 3 6 3 Prtoc.
oi: COI,1NG-92, NANTES, AUG. 23-28, 19924) accrue hw 0 sense 0 : DEF -> accruebecome big more addition5) year hw 0 sense 3 : DEF -> year period365 day measure any pointFinally, we show two graphs whichillustrate the convergence of the simulatedannealing technique to the minimum energy? '
(E) level.
The second graph is a close-up ofthe final cycles of the complete processshown in the first graph.I~QI--tm~-,Imm-tilal--l ira--dam-,~umamlinll.
.
.
.
.
44f .
.
.
.
o .
.
,J L .
.
.
.
.
.
.
)5--- ._-St--"- - -  , ;  , - - -  I - .
.
~-.
:::*~113"", ,.~lu-tl "Y,.'I3-?~111""?
,)" " ' I " !
~/ I~uf  ~ x  14 "1Ama~llRg'y,=5.
ConclusionThis paper describes a method forword-sense disambiguation based on theample technique of choosing senses of thewords in a sentence so that their definitionsin LDOCE have the most words and subjectcodes in common.
The amount of computa-tion necessary to find this optimal choiceexactly quickly becomes prohibitive as thenumber of ambiguous words and the numberof senses increase.
The computational tech-nique of simulated annealing allows a goodapproxamation to be computed quickly.Thus all the words m a sentence are disam-biguated simultaneously, m a reasonablerune, and automatically (with no handdisambiguation of training text).
Resultsusing this technique are comparable to othercomputational techniques and enhancementsincorporating co-occurrence and part-of-speech information, which have beenexploited in one-word-at-a time techniques,may be expected to improve the perfor-mance.ReferencesGuthrie, I., Guthrie, L., Wilks, Y., and Aidi-nejad, H. (1991).
Subject-DependentCo-Occurrence and Word SenseDisambiguafion, Proceedings of the29th Annual Meeting of the Associationfor Computational Linguistics, Berke-ley, CA.
June 1991. pp.
146-152.Also Memoranda in Computer andCognitive Science MCCS-91-206,Computing Research Laboratory, NewMexico Smm University.Hearst, M. (1991).
Toward Noun HomonymDisambiguation - Using Local Contextin Large Text Corpora Proceedings ofthe Seventh Annual Conference of theUW Centre for the New OED and TextResearch, Using Corpora pp.
1-22.Kirkpatrick, S., Gelatt, C. D., and Vecchi,M.
P. (1983).
Optimization by Simu-tared Annealing, Science vol.
220, pp.671-680.ACTES DE COL ING-92 ,  NANTES, 23-28 AOtrr 1992 3 6 4 PROC.
OF COL ING-92 ,  NANTES, AUG. 23-28, 1992McDonald, J. E., Plate, T, andSchvaneveldt, R. W. (1990).
UsingPathfinder to Extract Semantic Infor-mation from Text.
In Schvaneveldt, R.W.
(ed.)
Pathfinder Associative Net-works: Studies in Knowledge Organi-sation, Norwood, NJ: Ablex.?
Metaopolis, N., Rosenbluth, A., Rosenbluth,M., Teller, A., and Teller, E. (1953) J.Chem.
Phys.
vol.
21, p.1087.Procter, P., R. llson, J. Ayto, et al (1978)Longman Dictionary of ContemporaryEnglish.
Harlow, UK: Longman GroupLimited.Sampson, G. (1986).
A Stochastic Approachto Parsing.
l lth International Confer-ence on Computational Linguistics(COL1NG-86).
pp.
151-155.Veronis, J. and N.
Ide.
(1990).Word SenseDisambiguation with Very LargeNeural Networks Extracted fromMachine Readable Dictionaries.Proceedings of the 13th Conference onComputational Linguistics (COLING-90), Helsinki, Finland, 2, pp.
389-394.Wilks, Yorick A.
(1975a).
An IntelligentAnalyzer and Understander of English.Communications of the ACM, 18, 5, pp.264-274.
Reprinted in "Readings inNatural Language Processing," Editedby Barbara J. Grosz, Karen Sparck-Jones and Bonnie Lynn Webber, LosAltos: Morgan Kaufmann, 1986, pp.193-203.Wilks, Yorick A.
(1975b).
A PreferentialPattern-Seeking Semantics for NaturalLanguage Inference.
Artificial Intelli-gence, 6, pp.
53-74.Wilks, Y. and Fass.
D. (1991).
PreferenceSemantics: a family history, To appearm Computing and Mathematics withApplications (in press).
A shorter ver-sion in the second edition of the Ency-clopedia of Artificial Intelligence.Zernik, Lift and Paul Jacobs (1990).
Tag-ging for Learning: Collecting ThematicRelations from Corpus.
Proceedings ofthe 13th International Conference onComputational Linguistics (COL1NG?90), Helsinki, Finland, 1, pp.34-37.ACRES DE COLING-92, NAtCrEs, 23-28 AO~l' 1992 3 6 5 PROC.
OF COLING-92.
N^NTES.
AUG. 23-28, 1992
