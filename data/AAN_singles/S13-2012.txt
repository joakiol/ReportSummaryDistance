Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 73?77, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsNavyTime: Event and Time Ordering from Raw TextNathanael ChambersUnited States Naval AcademyAnnapolis, MD 21401, USAnchamber@usna.eduAbstractThis paper describes a complete event/timeordering system that annotates raw text withevents, times, and the ordering relations be-tween them at the SemEval-2013 Task 1.
Task1 is a unique challenge because it starts fromraw text, rather than pre-annotated text withknown events and times.
A working systemfirst identifies events and times, then identifieswhich events and times should be ordered, andfinally labels the ordering relation betweenthem.
We present a split classifier approachthat breaks the ordering tasks into smaller de-cision points.
Experiments show that morespecialized classifiers perform better than fewjoint classifiers.
The NavyTime system rankedsecond both overall and in most subtasks likeevent extraction and relation labeling.1 IntroductionThe SemEval-2013 Task 1 (TempEval-3) contest isthe third instantiation of an event ordering challenge.However, it is the first to start from raw text withthe challenge to create an end-to-end algorithm forevent ordering.
Previous challenges included the in-dividual aspects of such a system, including eventextraction, timex extraction, and event/time ordering(Verhagen et al 2007; Verhagen et al 2010).
How-ever, neither task was dependent on the other.
Thispaper presents NavyTime, a system inspired partlyby this previous breakup of the tasks.
We focus onbreaking up the event/time ordering task further, andshow that 5 classifiers yield better performance thanthe traditional 3 (or even 1).The first required steps to annotate a document areto extract its events and time expressions.
This pa-per describes a new event extractor with a rich set ofcontextual features that is a top performer for eventattributes at Tempeval-3.
We then explore additionsto SUTime, a top rule-based extractor for time ex-pressions (Chang and Manning, 2012).
However,the core challenge is to link these extracted eventsand times together.
We describe new models forthese difficult tasks: (1) identifying ordered pairs,and (2) labeling the ordering relations.Relation identification is rarely addressed in theliterature.
Given a set of events, which pairs ofevents are temporally related?
Almost all previouswork assumes we are given the pairs, and the taskis to label the relation (before, after, etc.).
Rawtext presents a new challenge: extract the relevantpairs before labeling them.
We present some of thefirst results that compare rule-based approaches totrained probabilistic classifiers.
These are the firstsuch comparisons to our knowledge.Finally, after relation identification, we label re-lations between the pairs.
This is the traditionalevent ordering task, although we now start fromnoisy pairs.
Our main contribution is to build in-dependent classifiers for intra-sentence event/timepairs.
We show improved performance when train-ing these split classifiers.
NavyTime?s approach ishighly competitive, achieving 2nd place in relationlabeling (and overall).2 DatasetAll models are developed on the TimeBank (Puste-jovsky et al 2003) and AQUAINT corpora (Mani73et al 2007).
These labeled newspaper articles havefueled many years of event ordering research.
Time-Bank includes 183 documents and AQUAINT in-cludes 73.
The annotators of each were given dif-ferent guidance, so they provide unique distributionsof relations.
Development of the algorithms in thispaper were solely on 10-fold cross validation on theunion of the two corpora.The SemEval-2013 Task 1 (TempEval-3) providesunseen raw text to then evaluate the final systems.Final results are from this set of unseen newspaperarticles.
They were annotated by a different set ofpeople who annotated TimeBank and AQUAINT.3 Event ExtractionThe first stage to processing raw text is to extractthe event mentions.
We treat this as a binary classi-fication task, classifying each token as either eventor not-event.
Events are always single tokens in theTimeBank/AQUAINT corpora, so a document withn tokens requires n classifications.
Further, eachevent is marked up with its tense, aspect, and class.We used a maximum entropy classificationframework based on the lexical and syntactic con-text of the target word.
The same features are usedto first identify events (binary decision), and thenthree classifiers are trained for the tense, aspect, andclass.
The following features were used:Token N-grams: Standard n-gram context thatincludes the target token (1,2,3grams), as well asthe unigrams and bigrams that occur directly beforeand after the target token.Part of Speech n-grams: The POS tag of the target,and the bigram and trigram ending with the target.Lemma: The lemmatized token in WordNet.WordNet-Event: A binary feature, true if the tokenis a descendent of the Event synset in WordNet.Parse Path: The tree path from the token?s leafnode to the root of the syntactic parse tree.Typed Dependencies: The typed dependency tripleof any edge that begins or ends with the target.We used 10-fold cross validation on the combinedcorpora of TimeBank and AQUAINT to develop theabove features, and then trained one classifier on theentire dataset.
Our approach was the 2nd best eventextraction system out of 8 submission sites on theunseen test set from TempEval-3.
Detailed resultsare given in Figure 1.Results on event attribute extraction were alsogood (Figure 1).
We again ranked 2nd best in bothTense and Aspect.
Only with the Class attribute didwe fare worse (4th of 8).
We look forward to com-paring approaches to see why this particular attributewas not as successful.4 Temporal Expression ExtractionAs with event extraction, time expressions need tobe identified from the raw text.
Recent work on timeextraction has suggested that rule-based approachesoutperform others (Chang and Manning, 2012), sowe adopted the proven SUTime system for this task.SUTime is a rule-based system that extracts phrasesand normalizes them to a TimeML time.
However,we improved it with some TimeBank specific rules.We observed that the phrases ?a year ago?
and ?thelatest quarter?
were often inconsistent with standardTimeBank annotations.
These tend to involve fiscalquarters, largely due to TimeBank?s heavy weight onthe financial genre.
For these phrases, we first deter-mine the current fiscal quarter, and adjust the nor-malized time to include the quarter, not just the year(e.g., 2nd quarter of 2012, rather than just 2012).Further, the generic phrase ?last year?
should nor-malize to just a year, and not include a more specificmonth or quarter.
We added rules to strip off months.SUTime was the best system for time extraction,and our usage matched its performance as one wouldhope.
Full credit goes to SUTime, and its extractionis not a contribution of this paper.
However, Navy-Time outperformed SUTime by over 3.5 F1 pointson time normalization.
Our additional rulebank ap-pears to have helped significantly, allowing Navy-Time to be the 2nd best in this category behind Hei-delTime.
We recommend users to use either Heidel-Time or SUTime with the NavyTime rulebank.5 Temporal Relation ExtractionAfter events and time expressions are identified, itremains to create temporal links between them.
Atemporal link is an ordering relation that occurs infour possible entity pairings: event-event, event-time, time-time, and event-DCT (DCT is the doc-ument creation time).74Event Extraction F1ATT-1 81.05NavyTime 80.30KUL 79.32cleartk-4 & cleartk-3 78.81ATT-3 78.63JU-CSE 78.62KUL-TE3RunABC 77.11Temp:ESAfeature 68.97FSS-TimEx 65.06Temp:WordNetfeature 63.90Class AttributeSystem Class F1ATT 71.88KUL 70.17cleartk 67.87NavyTime 67.48Temp:ESA 54.55JU-CSE 52.69Temp:WNet 50.00FSS-TimEx 42.94Tense and Aspect AttributesSystem Tense F1 Aspect F1cleartk 62.18 70.40NavyTime 61.67 72.43ATT 59.47 73.50JU-CSE 58.62 72.14KUL 49.70 63.20not all systems participatedFigure 1: Complete event rankings on all subtasks scored by F1.
Extraction is token span matching.It is unrealistic to label all possible pairs in a doc-ument.
Many event/time pairs have ambiguous or-derings, and others are simply not labeled by the an-notators.
We propose a two-stage approach wherewe first identify likely pairs (relation identification),and then independently decide what specific order-ing relation holds between them (relation labeling).5.1 Relation IdentificationTempEval-3 defined the set of possible relations toexist in particular configurations: (1) any pairs inthe same sentence, (2) event-event pairs of mainevents in adjacent sentences, and (3) event-DCTpairs.
However, the training and test corpora do notfollow these rules.
Many pairs are skipped to savehuman effort.
This task is thus a difficult balance be-tween labeling all true relations, but also matchingthe human annotators.
We tried two approaches toidentifying pairs: rule-based, and data-driven learn-ing.Rule-Based: We extract all event-event and event-time pairs in the same sentence if they are adjacentto each other (no intervening events or times).
Wealso extract main event pairs of adjacent sentences.We identify main events by finding the highest VPin the parse tree.Data-Driven: This approach treats it as a bi-nary classification task.
Given a pair of enti-ties, determine if they are ordered or not-ordered.We condense the training corpora?s TLINK rela-tions into ordered, and label all non-labeled pairsas not-ordered.
We tried a variety of classifiersfor each event/time pair type: (1) intra-sentenceevent-event, (2) intra-sentence event-time, (3) inter-Event-Event FeaturesToken, lemma, wordnet synsetPOS tag n-grams surrounding eventsSyntactic tree dominanceLinear order in textDoes another event appear in between?Parse path from e1 to e2Typed dependency path from e1 to e2Event-Time FeaturesEvent POS, token, lemma, wordnet synsetEvent tense, aspect, and classIs time a day of the week?Entire time phraseLast token in time phraseDoes time end the sentence?Bigram of event token and time tokenSyntactic tree dominanceParse path from event to timeTyped dependency path from event to timeEvent-DCT FeatureEvent POS, token, lemma, wordnet synsetEvent tense, aspect, and classBag-of-words unigrams surrounding the eventFigure 2: Features in the 3 types of classifiers.sentence event-event, and (4) event-DCT.The data-driven features are shown in Figure 2.After labeling pairs of entities, the ordered pairs arethen labeled with specific relations, described next.5.2 Relation LabelingThis is the traditional ordering task.
Given a setof entity pairs, label each with a temporal relation.TempEval-3 uses the full set of 12 relations.Traditionally, ordering research trains a singleclassifier for all event-event links, and a second forall event-time links.
We experimented with more75UTTime Best 56.45NavyTime (TimeBank+AQUAINT) 46.83NavyTime (TimeBank) 43.92JU-CSE Best 34.77Table 1: Task Crel, F1 scores of relation labeling.specific classifiers, observing that two events in thesame sentence share a syntactic context that does notexist between two events in different sentences.
Wemust instead rely on discourse cues and word seman-tics for the latter.
We thus propose using differentclassifiers to learn better feature weights for theseunique contexts.
Splitting into separate classifiers islargely unexplored on TimeBank, and just recentlyapplied to a medical domain (Xu et al 2013).We train two MaxEnt classifiers for event-eventlinks (inter and intra-sentence), and two for event-time links.
The event-DCT links also have their ownclassifier for a total of 5 classifiers.
We use the samefeatures (Figure 2) as in relation identification.5.3 Experiments and ResultsAll models were created by using 10-fold cross val-idation on TimeBank+AQUAINT.
The best modelwas then trained on the entire set.
Features seenonly once were trimmed from training.
The relationlabeling confidence threshold was set to 0.3.
Finalresults are reported on the held out test set providedby SemEval-2013 Task 1 (TempEval-3).Our first experiments focus on relation labeling.This is a simpler task than identification in that westart with known pairs of entities, and the task is toassign a label to them (Task C-relation at SemEval-2013 Task 1).
Table 1 gives the results.
Our systeminitially ranked second with 46.83.The next task is both relation identification andrelation labeling combined (Task C).
This is unfor-tunately a task that is difficult to define.
Without acompletely labeled graph of events and times, it isnot about true extraction, but matching human la-beling decisions that were constrained by time andeffort.
We experimented with rule-based vs data-driven extractors.
We held our relation labelingmodel constant, and swapped different identificationmodels in and out.
Our best configuration was eval-uated on test.
Results are shown in Table 2.
Navy-Time is the third best performer.Finally, the full task from raw text requires allcleartk Best 36.26UTTime-5 34.90NavyTime (TimeBank+AQUAINT) 31.06JU-CSE Best 26.41NavyTime (TimeBank) 25.84KUL 24.83Table 2: Task C, F1 scores of relation ID and labeling.cleartk Best 30.98NavyTime (TimeBank+AQUAINT) 27.28JU-CSE 24.61NavyTime (TimeBank) 21.99KUL 19.01Table 3: Task ABC, Extraction and labeling raw text.stages of this paper, starting from event and tem-poral extraction, then applying relation ID and la-beling.
Results are shown in Table 3.
Our systemranked 2nd of 4 systems.Our best performing setup uses trained classi-fiers for relation identification of event-event andevent-DCT links, but deterministic rules for event-time links (Sec 5.1).
It then uses trained classi-fiers for relation labeling of all pair types.
Train-ing with TimeBank+AQUAINT outperformed justTimeBank.
The split classifier approach for intraand inter-sentence event-event relations also outper-formed a single event-event classifier.
We cannotgive more specific results due to space constraints.6 DiscussionOur system was 2nd in most of the subtasks andoverall (Task ABC).
Split-classifiers for inter andintra-sentence pairs are beneficial.
Syntactic fea-tures help event extraction.
Compared to cleartk,NavyTime was better in event and time extractionindividually, but worse overall.
Our approach to re-lation identification is likely the culprit.We urge future work to focus on relation identifi-cation.
Event and time performance is high, and re-lation labeling is covered in the literature.
For iden-tification, it is not clear that TimeBank-style corporaare appropriate for evaluation.
Human annotators donot create connected graphs.
How can we evaluatesystems that do?
Do we want systems that mimicimperfect, but testable human effort?
Accurate eval-uation on raw text requires fully labeled test sets.76ReferencesAngel Chang and Christopher D. Manning.
2012.
Su-time: a library for recognizing and normalizing timeexpressions.
In Proceedings of the Language Re-sources and Evaluation Conference.Inderjeet Mani, Ben Wellner, Marc Verhagen, and JamesPustejovsky.
2007.
Three approaches to learningtlinks in timeml.
Technical Report CS-07-268, Bran-deis University.James Pustejovsky, Patrick Hanks, Roser Sauri, AndrewSee, Robert Gaizauskas, Andrea Setzer, DragomirRadev, Beth Sundheim, David Day, Lisa Ferro, et al2003.
The timebank corpus.
In Corpus linguistics,volume 2003, page 40.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporal re-lation identification.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, pages 75?80.
Association for Computational Linguistics.Marc Verhagen, Roser Sauri, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 task 13:Tempeval-2.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 57?62.
As-sociation for Computational Linguistics.Yan Xu, Yining Wang, Tianren Liu, Junichi Tsujii, andEric I-Chao Chang.
2013.
An end-to-end systemto identify temporal relation in discharge summaries:2012 i2b2 challenge.
Journal of the American Medi-cal Informatics Association.77
