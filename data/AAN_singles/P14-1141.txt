Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1501?1511,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPredicting Instructor?s Intervention in MOOC forumsSnigdha Chaturvedi Dan Goldwasser Hal Daum?e IIIDepartment of Computer Science,University of Maryland, College Park, Maryland{snigdhac, goldwas1, hal}@umiacs.umd.eduAbstractInstructor intervention in student discus-sion forums is a vital component inMassive Open Online Courses (MOOCs),where personalized interaction is limited.This paper introduces the problem of pre-dicting instructor interventions in MOOCforums.
We propose several predictionmodels designed to capture unique aspectsof MOOCs, combining course informa-tion, forum structure and posts content.Our models abstract contents of individ-ual posts of threads using latent categories,learned jointly with the binary interven-tion prediction problem.
Experiments overdata from two Coursera MOOCs demon-strate that incorporating the structure ofthreads into the learning problem leads tobetter predictive performance.1 IntroductionUbiquitous computing and easy access to highbandwidth internet have reshaped the modusoperandi in distance education towards MassiveOpen Online Courses (MOOCs).
Courses offeredby ventures such as Coursera and Udacity now im-part inexpensive and high-quality education fromfield-experts to thousands of learners across geo-graphic and cultural barriers.Even as the MOOC model shows exciting pos-sibilities, it presents a multitude of challenges thatmust first be negotiated to completely realize itspotential.
MOOCs platforms have been especiallycriticized on grounds of lacking a personalizededucational experience (Edmundson, 2012).
Un-like traditional classrooms, the predominant modeof interaction between students and instructors inMOOCs is via online discussion forums.
Ideally,forum discussions can help make up for the lackof direct interaction, by enabling students to askquestions and clarify doubts.
However, due tohuge class sizes, even during the short durationof a course, MOOCs witness a very large numberof threads on these forums.
Owing to extremelyskewed ratios of students to instructional staff, itcan be prohibitively time-consuming for the in-structional staff to manually follow all threads of aforum.
Hence there is a pressing need for automat-ically curating the discussions for the instructors.In this paper, we focus on identifying situa-tions in which instructor (used interchangeablywith ?instructional staff?
in this paper) interven-tion is warranted.
Using existing forum posts andinteractions, we frame this as a binary predictionproblem of identifying instructor?s intervention inforum threads.
Our initial analysis revealed thatinstructors usually intervene on threads discussingstudents?
issues close to a quiz or exam.
Theyalso take interest in grading issues and logisticsproblems.
There are multiple cues specific to theMOOC setting, which when combined with therich lexical information present in the forums, canyield useful predictive models.Analyzing forum-postings contents and bring-ing the most pertinent content to the instructor?sattention would help instructors receive timelyfeedback and design interventions as needed.From the students?
perspective, the problem is ev-ident from an examination of existing forum con-tent, indicating that if students want instructor?sinput on some issues, the only way for them toget his/her attention is by ?up-voting?
their votes.Fig.
1 provides some examples of this behavior.This is clearly an inefficient solution.Our main technical contribution is introducingthree different models addressing the task of pre-dicting instructor interventions.
The first uses a lo-gistic regression model that primarily incorporateshigh level information about threads and posts.However, forum threads have structure which isnot leveraged our initial model.
We present two1501?The problem summary: Anyone else having problems viewing the video lecture...very choppy.
If you are also experi-encing this issue; please upvote this post.?
?I read that by up-voting threads and posts you can get the instructors?
attention faster.?
?Its is very bad to me that I achieved 10 marks in my 1st assignment and now 9 marks in my 2nd assignment, now I won?tget certificate, please Course staff it is my appeal to change the passing scheme or please be lenient.
Please upvote mypost so that staff take this problem under consideration.
?Figure 1: Sample posts that showing students desiring instructor?s attention have to resolve to the ineffi-cient method of getting their posts upvoted.additional structured models.
Both models assumethat posts of a thread structure it in form of a storyor a ?chain of events.?
For example, an openingpost of a thread might pose a question and the fol-lowing posts can then answer or comment on thequestion.
Our second and third models tap thislinear ?chain of events?
behavior by assuming thatindividual posts belong to latent categories whichrepresent their textual content at an abstract leveland that an instructor?s decision to reply to a postis based on this chain of events (represented by thelatent categories).
We present two different waysof utilizing this ?chain of events?
behavior for pre-dicting instructor?s intervention which can be ei-ther simply modeled as the ?next step?
is this chainof events (Linear Chain Markov Model) or as adecision globally depending on the entire chain(Global Chain Model).
Our experiments on twodifferent datasets reveal that using the latent postcategories helps in better prediction.Our contributions can be summarized as:?
We motivate and introduce the importantproblem of predicting instructor interventionin MOOC forums?
We present two chain based models that in-corporate thread structure.?
We show the utility of modeling thread struc-ture, and the value of lexical and domain spe-cific knowledge for the prediction task2 Related WorkTo the best of our knowledge, the problem of pre-dicting instructor?s intervention in MOOC forumshas not been addressed yet.
Prior work deals withanalyzing general online discussion forums of so-cial media sites (Kleinberg, 2013): such as pre-dicting comment volume (Backstrom et al, 2013;De Choudhury et al, 2009; Wang et al, 2012;Tsagkias et al, 2009; Yano and Smith, 2010; Artziet al, 2012) and rate of content diffusion (Kwak etal., 2010; Lerman and Ghosh, 2010; Bakshy et al,2011; Romero et al, 2011; Artzi et al, 2012) andalso question answering (Chaturvedi et al, 2014).Wang et al (2007) incorporate thread structureof conversations using features in email threadswhile Goldwasser and Daum?e III (2014) use la-tent structure, aimed to identify relevant dialogsegments, for predicting objections during court-room deliberations.
Other related work includespeech act recognition in emails and forums butat a sentence level (Jeong et al, 2009), and us-ing social network analysis to improve messageclassification into pre-determined types (Fortunaet al, 2007).
Discussion forums data has also beenused to address other interesting challenges suchas extracting chatbox knowledge for use in gen-eral online forums (Huang et al, 2007) and auto-matically extracting answers from discussion fo-rums (Catherine et al, 2013), subjectivity analy-sis of online forums (Biyani et al, 2013).
Mostof these methods use ideas similar to ours: identi-fying that threads (or discussions) have an under-lying structure and that messages belong to cate-gories.
However, they operate in a different do-main, which makes their goals and methods dif-ferent from ours.Our work is most closely related to that of Back-strom et al (2013) which introduced the re-entryprediction task ?predicting whether a user whohas participated in a thread will later contributeanother comment to it.
While seemingly related,their prediction task, focusing on users who havealready commented on a thread, and their algorith-mic approach are different than ours.
Our workis also very closely related to that of Wang et al(2013) who predict solvedness ?which predictsif there is a solution to the original problem postedin the thread.
Like us, they believe that categoryof posts can assist in the prediction task, however,possibly owing to the complexity of general dis-cussion forums, they had to manually create andannotate data with a sophisticated taxonomy.
Wedo not make such assumptions.The work presented in (G?omez et al, 2008;1502Liben-Nowell and Kleinberg, 2008; Kumar et al,2010; Golub and Jackson, 2010; Wang et al,2011; Aumayr et al, 2011) discuss characteriz-ing threads using reply-graphs (often trees) andlearning this structure.
However, this representa-tion is not natural for the MOOC domain wherediscussions are relatively more focused on thethread topic and are better organized using sec-tions within the forums.Although most prior work focuses on discus-sion forums of social media sites such as Twitteror Facebook, where the dynamics of interaction isvery different from MOOCs, a small number ofrecent work address the unique MOOC setting.Stump et al (2013) propose a framework forcategorizing forum posts by designing a taxonomyand annotating posts manually to assist general fo-rum analysis.
Our model learns categories in adata-driven manner guided by the binary super-vision (intervention decision) and serves a differ-ent purpose.
Nevertheless, in Sec.
4.3 we comparethe categories learnt by our models with those pro-posed by Stump et al (2013).Apart from this, recent works have looked intointeresting challenges in this domain such as bet-ter peer grading models (Piech et al, 2013), codereview (Huang et al, 2013; Nguyen et al, 2014),improving student engagement (Anderson et al,2014) and understanding how students learn andcode (Piech et al, 2012; Kizilcec et al, 2013;Ramesh et al, 2013).3 Intervention Prediction ModelsIn this section, we explain our models in detail.3.1 Problem SettingIn our description it is assumed that a discus-sion board is organized into multiple forums (rep-resenting topics such as ?Assignment?, ?StudyGroup?
etc.).
A forum consists of multiplethreads.
Each thread (t) has a title and consists ofmultiple posts (pi).
Individual posts do not havea title and the number of posts varies dramaticallyfrom one thread to another.
We address the prob-lem of predicting if the course instructor would in-tervene on a thread, t. The instructor?s decision tointervene, r, equals 0 when the instructor doesn?treply to the thread and 1 otherwise.
The individualposts are not assumed to be labeled with any cat-egory and the only supervision given to the modelduring training is in form of intervention decision.3.2 Logistic Regression (LR)Our first attempt at solving this problem involvedtraining a logistic regression for the binary predic-tion task which models P (r|t).3.2.1 Feature EngineeringOur logistic regression model uses the follow-ing two types of features: Thread only featuresand Aggregated post features.
?Thread only fea-tures?
capture information about the thread suchas when, where, by who was the thread posted andlexical features based on the title of the thread.While these features provide a high-level infor-mation about the thread, it is also important toanalyze the contents of the posts of the thread.In order to maintain a manageable feature space,we compress the features from posts and representthem using our ?Aggregated post features?.Thread only features:1. a binary feature indicating if the thread wasstarted by an anonymous user2.
three binary features indicating whether thethread was marked as approved, unresolvedor deleted (respectively)3. forum id in which the thread was posted4.
time when the thread was started5.
time of last posting on the thread6.
total number of posts in the thread7.
a binary feature indicating if the thread titlecontains the words lecture or lectures8.
a binary feature indicating if the thread titlecontains the words assignment, quiz, grade,project, exam (and their plural forms)Aggregated post features:9. sum of number of votes received by the indi-vidual posts10.
mean and variance of the posting times of in-dividual posts in the thread11.
mean of time difference between the post-ing times of individual posts and the closestcourse landmark.
A course landmark is thedeadline of an assignment, exam or project.12.
sum of count of occurrences of assessmentrelated words e.g.
grade, exam, assignment,quiz, reading, project etc.
in the posts13.
sum of count of occurrences of words indicat-ing technical problems e.g.
problem, error14.
sum of count of occurrences of thread con-clusive words like thank you and thank15.
sum of count of occurrences of request, sub-mit, suggest1503h1h2hnr ?
(t)p1p2pnT(a) Linear Chain Markov Model (LCMM)h1h2hnr ?
(t)p1p2pnT(b) Global Chain Model (GCM)Figure 2: Diagrams of the Linear Chain MarkovModel (LCMM) and the Global Chain Model(GCM).
pi, r and ?
(t) are observed and hiare thelatent variables.
piand hirepresent the posts ofthe thread and their latent categories respectively;r represents the instructor?s intervention and ?
(t)represent the non-structural features used by thelogistic regression model.We had also considered and dropped (becauseof no performance gain) other features about iden-tity of the user who started the thread, numberof distinct participants in the thread (an impor-tant feature used by Backstrom et al (2013)), bi-nary feature indicating if the first and the last postswere by the same user, average number of wordsin the thread?s posts, lexical features capturing ref-erences to the instructors in the posts etc.3.3 Linear Chain Markov Model (LCMM)The logistic regression model is good at exploit-ing the thread level features but not the content ofindividual posts.
The ?Aggregated post features?attempt to capture this information but since thenumber of posts in a thread is variable, these fea-tures relied on aggregated values.
We believe thatconsidering aggregate values is not sufficient forthe task in hand.
As noted before, posts of a threadare not independent of each other.
Instead, theyare arranged chronologically such that a post ispublished in reply to the preceding posts and thisFor every thread, t, in the dataset:1.
Choose a start state, h1, and emit the firstpost, p1.2.
For every subsequent post, pi?
i ?
{2 .
.
.
n} :(a) Transition from hi?1to hi.
(b) Emit post pi.3.
Generate the instructor?s interventiondecision, r, using the last state hnandnon-structural features, ?
(t).Figure 3: Instructor?s intervention decision pro-cess for the Linear Chain Markov Model.might effect an instructor?s decision to reply.
Forexample, consider a thread that starts with a ques-tion.
The following posts will be students?
attemptto answer the question or raise further concerns orcomment on previous posts.
The instructor?s post,though a future event, will be a part of this process.We, therefore, propose to model this completeprocess using a linear chain markov model shownin Fig.
2a.
The model abstractly represents the in-formation from individual posts (pi) using latentcategories (hi).
The intervention decision, r, isthe last step in the chain and thus incorporates in-formation from the individual posts.
It also de-pends on the thread level features: ?Thread onlyfeatures?
and the ?Aggregated post features?
jointlyrepresented by ?
(t) (also referred to as the non-structural features).
This process is explained inFig.
3.We use hand-crafted features to model the dy-namics of the generative process.
Whenever a la-tent state emits a post or transits to another latentstate (or to the final intervention decision state),emission and transition features get fired which arethen multiplied by respective weights to computea thread?s ?score?
:fw(t, p) = maxh[w ?
?
(p, r, h, t)] (1)Note that the non-structural features, ?
(t), alsocontribute to the final score.3.3.1 Learning and InferenceDuring training we maximize the combined scoresof all threads in the dataset using a generic EMstyle algorithm.
The supervision in this model isprovided only in form of the observed interven-tion decision, r and the post categories, hiare hid-1504den.
The model uses the pseudocode shown in Al-gorithm 1 to iteratively refine the weight vectors.In each iteration, the model first uses viterbi algo-rithm to decode thread sequences with the currentweights wtto find optimal highest scoring latentstate sequences that agree with the observed in-tervention state (r = r?).
In the next step, giventhe latent state assignments from the previous step,a structured perceptron algorithm (Collins, 2002)is used to update the weights wt+1using weightsfrom the previous step, wt, initialization.Algorithm 1 Training algorithm for LCMM1: Input: Labeled data D = {(t, p, r)i}2: Output: Weights w3: Initialization: Set wjrandomly, ?j4: for t : 1 to N do5:?hi= argmaxh[wt?
?
(p, r, h, t)] suchthat r = ri?i6: wt+1= StructuredPerceptron(t, p,?h, r)7: end for8: return wWhile testing, we use the learned weights andviterbi decoding to compute the intervention stateand the best scoring latent category sequence.3.3.2 Feature EngineeringIn addition to the ?Thread Only Features?
and the?Aggregated post features?, ?
(t) (Sec.
3.2.1, thismodel uses the following emission and transitionfeatures:Post Emission Features:1.
?
(pi, hi) = count of occurrences of questionwords or question marks in piif the state ishi; 0 otherwise.2.
?
(pi, hi) = count of occurrences of thankwords (thank you or thanks) in piif the stateis hi; 0 otherwise.3.
?
(pi, hi) = count of occurrences of greetingwords (e.g.
hi, hello, good morning, welcomeetc ) in piif the state is hi; 0 otherwise.4.
?
(pi, hi) = count of occurrences of assess-ment related words (e.g.
grade, exam, assign-ment, quiz, reading, project etc.)
in piif thestate is hi; 0 otherwise.5.
?
(pi, hi) = count of occurrences of request,submit or suggest in piif the state is hi; 0otherwise.6.
?
(pi, hi) = log(course duration/t(pi)) if thestate is hi; 0 otherwise.
Here t(pi) is the dif-ference between the posting time of piandthe closest course landmark (assignment orproject deadline or exam).7.
?
(pi, pi?1, hi) = difference between postingtimes of piand pi?1normalized by courseduration if the state is hi; 0 otherwise.Transition Features:1.
?
(hi?1, hi) = 1 if previous state is hi?1andcurrent state is hi; 0 otherwise.2.
?
(hi?1, hi, pi, pi?1) = cosine similarity be-tween pi?1and piif previous state is hi?1and current state is hi; 0 otherwise.3.
?
(hi?1, hi, pi, pi?1) = length of piif previ-ous state is hi?1, pi?1has non-zero questionwords and current state is hi; 0 otherwise.4.
?
(hn, r) = 1 if last post?s state is hnand in-tervention decision is r; 0 otherwise.5.
?
(hn, r, pn) = 1 if last post?s state is hn, pnhas non-zero question words and interventiondecision is r; 0 otherwise.6.
?
(hn, r, pn) = log(course duration/t(pn)) iflast post?s state is hnand intervention deci-sion is r; 0 otherwise.
Here t(pn) is the dif-ference between the posting time of pnandthe closest course landmark (assignment orproject deadline or exam).3.4 Global Chain Model (GCM)In this model we propose another way of incorpo-rating the chain structure of a thread.
Like the pre-vious model, this model also assumes that postsbelong to latent categories.
It, however, doesn?tmodel the instructor?s intervention decision as astep in the thread generation process.
Instead, itassumes that instructor?s decision to intervene isdependent on all the posts in the threads, mod-eled using the latent post categories.
This modelis shown in Fig.
2b.
Assuming that p representsposts of thread t, h represents the latent categoryassignments, r represents the intervention deci-sion; feature vector, ?
(p, r, h, t), is extracted foreach thread and using the weight vector, w, thismodel defines a decision function, similar to whatis shown in Equation 1.3.4.1 Learning and InferenceSimilar to the traditional maximum margin basedSupport Vector Machine (SVM) formulation, ourmodel?s objective function is defined as:minw?2||w||2+T?jl(?rjfw(tj, pj)) (2)1505where ?
is the regularization coefficient, tjis thejththread with intervention decision rjand pjarethe posts of this thread.
w is the weight vector, l(?
)is the squared hinge loss function and fw(tj, pj) isdefined in Equation 1.Replacing the term fw(tj, pj) with the con-tents of Equation 1 in the minimization objectiveabove, reveals the key difference from the tradi-tional SVM formulation - the objective functionhas a maximum term inside the global minimiza-tion problem making it non-convex.We, therefore, employ the optimization algo-rithm presented in (Chang et al, 2010) to solvethis problem.
Exploiting the semi-convexity prop-erty (Felzenszwalb et al, 2010), the algorithmworks in two steps, each executed iteratively.
Inthe first step, it determines the latent variable as-signments for positive examples.
The algorithmthen performs two step iteratively - first it deter-mines the structural assignments for the negativeexamples, and then optimizes the fixed objectivefunction using a cutting plane algorithm.
Oncethis process converges for negative examples, thealgorithm reassigns values to the latent variablesfor positive examples, and proceeds to the secondstep.
The algorithm stops once a local minimumis reached.
A somewhat similar approach, whichuses the Convex-Concave Procedure (CCCP) ispresented by (Yu and Joachims, 2009).At test time, given a thread, t, and it posts, p,we use the learned weights to compute fw(t, p)and classify it as belonging to the positive class(instructor intervenes) if fw(t, p) ?
0.3.4.2 Feature EngineeringThe feature set used by this model is very sim-ilar to the features used by the previous model.In addition to the non-structural features usedby the logistic regression model (Sec.
3.2.1), ituses all the Post Emission features and the threetransition features represented by ?
(hi?1, hi) and?
(hi?1, hi, pi, pi?1) as described in Sec.
3.3.2.4 Empirical EvaluationThis section describes our experiments.4.1 Datasets and Evaluation MeasureFor our experiments, we have used the forumcontent of two MOOCs from different domains(science and humanities), offered by Coursera1,1https://www.coursera.org/a leading education technology company.
Bothcourses were taught by professors from the Uni-versity of Maryland, College Park.Genes and the Human Condition (From Behav-ior to Biotechnology) (GHC) dataset.2Thiscourse was attended by 30,000 students and theinstructional staff comprised of 2 instructors, 3Teaching Assistants and 56 technical support staff.The discussion forum of this course consisted of980 threads composed of about 3,800 posts.Women and the Civil Rights Movement (WCR)dataset.3The course consisted of a classroomof about 14,600 students, 1 instructor, 6 TeachingAssistants and 49 support staff.
Its discussion fo-rum consisted of 800 threads and 3,900 posts.We evaluate our models on held-out test sets.For the GHC dataset, the test set consisted of 186threads out of which the instructor intervened on24 while, for the WCR dataset, the instructor in-tervened on 21 out of 155 threads.Also, it was commonly observed that after aninstructor intervenes on a thread, its posting and/orviewing behavior increases.
We, therefore, onlyconsider the student posts until the instructor?s firstintervention.
Care was also taken to not use fea-tures that increased/decreased disproportionatelybecause of the instructor?s intervention such asnumber of views or votes of a thread.In our evaluation we approximate instructor?s?should reply?
instances with those where the in-structor indeed replied.
Unlike general forumusers, we believe that the correlation between thetwo scenarios is quite high for instructors.
It istheir responsibility to reply, and by choosing to aMOOC, they have ?bought in?
to the idea of forumparticipation.
The relatively smaller class sizes ofthese two MOOCs also ensured that most threadswere manually reviewed, thus reducing instancesof ?missed?
threads while retaining the posting be-havior and content of a typical MOOC.4.2 Experimental ResultsSince the purpose of solving this problem is toidentify the threads which should be brought tothe notice of the instructors, we measure the per-formance of our models using F-measure of thepositive class.
The values of various parameterswere selected using 10-fold Cross Validation on2https://www.coursera.org/course/genes3https://www.coursera.org/course/womencivilrights1506ModelGenes and the Human Condition (GHC) Women and the Civil Rights (WCR)P R F P R FLR 44.44 16.67 24.24 66.67 15.38 25.00J48 45.50 20.80 28.55 25.00 23.10 24.01LCMM 33.33 29.17 31.11 42.86 23.08 30.00GCM 60.00 25.00 35.29 50.00 18.52 27.03Table 1: Held-out test set performances of chain models, LCMM and GCM, are better than that of theunstructured models, LR and J48.Figure 4: Visualization of lexical contents of thecategories learnt by our model from the GHCdataset.
Each row is a category and each columnrepresents a feature vector.
Bright cream colorrepresents high values while lower values are rep-resented by darker shades.
Dark beige columnsare used to better separate the five feature clusters,F1-F5, which represent words that are common inthanking, logistics-related, introductory, syllabusrelated and miscellaneous posts respectively.
Cat-egories 1,2,3 and 4 are dominated by F2, F4, F1and F3 respectively indicating a semantic segrega-tion of posts by our model?s categories.the training set.
Table 1 presents the performancesof the proposed models on the held-out test sets.We also report performance of a decision tree(J48) on the test sets for sake of comparison.We can see that the chain based models, LinearChain Markov Model (LCMM) and Global ChainModel (GCM), outperform the unstructured mod-els, namely Logistic regression (LR) and DecisionTrees (J48).
This validates our hypothesis that us-ing the post structure results in better modeling ofinstructor?s intervention.The table also reveals that GCM yields high pre-cision and low recall values, which is possibly dueto the model being more conservative owing to in-formation from all posts of the thread.4.3 Visual Exploration of CategoriesOur chain based models assume that posts belongto different (latent) categories and use these cate-gories to make intervention predictions.
Since thisprocess of discovering categories is data driven, itwould be interesting to examine the contents ofthese categories.
Fig.
4 presents a heat map oflexical content of categories identified by LCMMfrom the GHC dataset.
The value of H (num-ber of categories) was set to be 4 and was pre-determined during the model selection procedure.Each row of the heat map represents a categoryand the columns represent values of individual fea-tures, f(w, c), defined as: f(w, c) =C(w,c)<C(w,c)>where, C(w, c) is total count of occurrences of aword, w, in all posts assigned to category, c and< C(w, c) > represents its expected count basedon its frequency in the dataset.
While the actualsize of vocabulary is huge, we use only a smallsubset of words in our feature vector for this visu-alization.
These feature values, after normaliza-tion, are represented in the heat map using col-ors ranging from bright cream (high value) to darkblack (low value).
The darker the shade of a cell,the lower is the value represented by it.For visual convenience, the features are man-ually clustered into five groups (F1 to F5) eachseparated by a dark beige colored column in theheat map.
The first column of the heat map rep-resents the F1 group which consists of words likethank you, thanks etc.
These words are character-istic of posts that mark either the conclusion of aresolved thread or are posted towards the end ofthe course.
Rows corresponding to the category 3in Table 2 show two examples of such posts.
Simi-larly, F2 represents the features related to logisticsof the course and F3 captures introductory postsby new students.
Finally, F4 contains words thatare closely related to the subfield of gene and hu-man conditions and would appear in posts that dis-cuss specific aspects or chapters of the course con-1507tents, while F5 contains general buzz words thatwould appear frequently in any biology course.Analyzing individual rows of the heat map, wecan see that out of F1 to F4, Categories 1, 2, 3 and4 are dominated by logistics (F2), course contentrelated (F4), thank you (F1) and introductory posts(F3) respectively, represented by bright colors intheir respective rows.
We also observe similar cor-relations while examining the columns of the heatmap.
Also, F5, which contains words common tothe gene and human health domain, is scatteredacross multiple categories.
For example, dna/rnaand breeding are sufficiently frequent in category1 as well as 2.Table 2 gives examples of representative postsfrom the four clusters.
Due to space constraints,we show only part of the complete post.
We cansee that these examples agree with our observa-tions from the heat map.Furthermore, as noted in Sec.
2, we comparethe semantics of clusters learnt by our models withthose proposed by Stump et al (2013) even thoughthe two categorizations are not directly compara-ble.
Nevertheless, generally speaking, our cate-gory 1 corresponds to Stump et al (2013)?s Coursestructure/policies and category 2 corresponds toContent.
Interestingly, categories 3 and 4, whichrepresent valedictory and introductory posts, cor-respond to a single Social/affective from the previ-ous work.We can, therefore, conclude that the model, in-deed splits the posts into categories that look se-mantically coherent to the human eyes.4.4 Choice of Number of CategoriesOur chain based models, assigning forum posts tolatent categories, are parameterized with H , thenumber of categories.
We therefore, study the sen-sitivity of our models to this parameter.
Fig.
5,plots the 10-fold cross validation performance ofthe models with increasing values ofH for the twodatasets.
Interestingly, the sensitivity of the twomodels to the value of H is very different.The LCMM model?s performance fluctuates asthe value of H increases.
The initial performanceimprovement might be due to an increase in the ex-pressive power of the model.
Performance peaksat H = 4 and then decreases, perhaps owing toover-fitting of the data.In contrast, GCM performance remains steadyfor various values of H which might be attributed(a) Genes and the Human Condition dataset(b) Women and the Civil Rights Movement datasetFigure 5: Cross validation performances of thetwo models with increasing number of categories.to the explicit regularization coefficient whichhelps combat over-fitting, by encouraging zeroweights for unnecessary categories.4.5 How important are linguistic features?We now focus on the structure independent fea-tures and experiment with their predictive value,according to types.
We divide the features used bythe LR into the following categories:4?
Full: set of all features (feature no.
1 to 15)?
lexical: based on content of thread titles andposts (feature no.
7 to 8 and 12 to 13)?
landmark: based on course landmarks (e.g,exams, quizzes) information (feature no.
11)?
MOOCs-specific: features specific to theMOOCs domain (lexical + landmark fea-tures)?
post: based only on aggregated posts infor-mation (feature no.
9 to 15)?
temporal: based on posting time patterns(feature no.
4, 5 and 10)Fig.
6 shows 10-fold cross validation F-measureof the positive class for LR when different types offeatures are excluded from the full set.The figure reveals that the MOOCs-specificfeatures (purple bar) are important for both thedatasets indicating a need for designing special-ized models for forums analysis in this domain.4Please refer to Sec 3.2.1 for description of the feature id.1508Category Example posts1 ?I?m having some issues with video playback.
I have downloaded the videos to my laptop...?1 ?There was no mention of the nuclear envelope in the Week One lecture, yet it was in the quiz.
Is this a mistake?
?2 ?DNA methylation is a crucial part of normal development of organisms and cell differentiation in higher organisms...?2 ?In the lecture, she said there are...I don?t see how tumor-suppressor genes are a cancer group mutation.
?3 ?Thank you very much for a most enjoyable and informative course.
?3 ?Great glossary!
Thank you!
?4 ?Hello everyone, I?m ... from the Netherlands.
I?m a life science student.4 ?Hi, my name is ... this is my third class with coursera?Table 2: Representative posts from the four categories learnt by our model.
Due to space and privacyconcerns we omit some parts of the text, indicated by ?.
.
.
?.
(a) Genes and the Human Condition dataset(b) Women and the Civil Rights Movement datasetFigure 6: Cross validation performances of thevarious feature types for the two datasets.Also, lexical features (red bar) and post features(blue bar) have pretty dramatic effects in GHC andWCR data respectively.Interestingly, removing the landmark feature set(green bar) causes a considerable drop in predic-tive performance, even though it consists of onlyone feature.
Other temporal features (orange bar)also turn out to be important for the prediction.From a separate instructor activity vs time graph(not shown due to space constraints), we observedthat instructors tend to get more active as thecourse progresses and their activity level also in-creases around quizzes/exams deadlines.We can, therefore, conclude that all featuretypes are important and that lexical as well asMOOC specific analysis is necessary for model-ing instructor?s intervention.5 ConclusionOne of the main challenges in MOOCs is man-aging student-instructor interaction.
The massivescale of these courses rules out any form of per-sonalized interaction, leaving instructors with theneed to go over the forum discussions, gauge stu-dent reactions and selectively respond when ap-propriate.
This time consuming and error pronetask stresses the need for methods and tools sup-plying this actionable information automatically.This paper takes a first step in that direction,and formulates the novel problem of predicting in-structor intervention in MOOC discussion forums.Our main technical contribution is to constructpredictive models combining information aboutforum post content and posting behavior with in-formation about the course and its landmarks.We propose three models for addressing thetask.
The first, a logistic regression model istrained on thread level and aggregated post fea-tures.
The other two models take thread structureinto account when making the prediction.
Thesemodels assume that posts can be represented bycategories which characterize post content at anabstract level, and treat category assignments aslatent variables organized according to, and influ-enced by, the forum thread structure.Our experiments on forum data from two differ-ent Coursera MOOCs show that utilizing threadstructure is important for predicting instructor?sbehavior.
Furthermore, our qualitative analysisshows that our latent categories are semanticallycoherent to human eye.1509ReferencesAshton Anderson, Daniel P. Huttenlocher, Jon M.Kleinberg, and Jure Leskovec.
2014.
Engaging withmassive online courses.
In WWW, pages 687?698.Yoav Artzi, Patrick Pantel, and Michael Gamon.
2012.Predicting responses to microblog posts.
In Pro-ceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 602?606, Stroudsburg, PA,USA.
Association for Computational Linguistics.Erik Aumayr, Jeffrey Chan, and Conor Hayes.
2011.Reconstruction of threaded conversations in onlinediscussion forums.
In Lada A. Adamic, Ricardo A.Baeza-Yates, and Scott Counts, editors, ICWSM.The AAAI Press.Lars Backstrom, Jon Kleinberg, Lillian Lee, and Cris-tian Danescu-Niculescu-Mizil.
2013.
Characteriz-ing and curating conversation threads: Expansion,focus, volume, re-entry.
In Proceedings of the SixthACM International Conference on Web Search andData Mining, WSDM ?13, pages 13?22, New York,NY, USA.
ACM.Eytan Bakshy, Jake M. Hofman, Winter A. Mason, andDuncan J. Watts.
2011.
Everyone?s an influencer:Quantifying influence on twitter.
In Proceedings ofthe Fourth ACM International Conference on WebSearch and Data Mining, WSDM ?11, pages 65?74,New York, NY, USA.
ACM.Prakhar Biyani, Cornelia Caragea, and Prasenjit Mitra.2013.
Predicting subjectivity orientation of onlineforum threads.
In CICLing (2), pages 109?120.Rose Catherine, Rashmi Gangadharaiah, KarthikVisweswariah, and Dinesh Raghu.
2013.
Semi-supervised answer extraction from discussion fo-rums.
In Proceedings of the Sixth International JointConference on Natural Language Processing, pages1?9, Nagoya, Japan, October.
Asian Federation ofNatural Language Processing.Ming-Wei Chang, Dan Goldwasser, Dan Roth, andVivek Srikumar.
2010.
Discriminative learning overconstrained latent representations.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, HLT ?10, pages 429?437, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Snigdha Chaturvedi, Vittorio Castelli, Radu Florian,Ramesh M. Nallapati, and Hema Raghavan.
2014.Joint question clustering and relevance predictionfor open domain non-factoid question answering.
InProceedings of the 23rd International Conference onWorld Wide Web, WWW ?14, pages 503?514, Re-public and Canton of Geneva, Switzerland.
Interna-tional World Wide Web Conferences Steering Com-mittee.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 Conference on Empirical Methods inNatural Language Processing - Volume 10, EMNLP?02, pages 1?8, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Munmun De Choudhury, Hari Sundaram, Ajita John,and Dor?ee Duncan Seligmann.
2009.
What makesconversations interesting?
themes, participants andconsequences of conversations in online social me-dia.
In 18th International World Wide Web Confer-ence (WWW), pages 331?331, April.Mark Edmundson.
2012.
The trouble with online edu-cation, July 19. http://www.nytimes.com/2012/07/20/opinion/the-trouble-with-online-education.html.Pedro F. Felzenszwalb, Ross B. Girshick, DavidMcAllester, and Deva Ramanan.
2010.
Objectdetection with discriminatively trained part-basedmodels.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 32(9):1627?1645.Blaz Fortuna, Eduarda Mendes Rodrigues, and NatasaMilic-Frayling.
2007.
Improving the classifica-tion of newsgroup messages through social networkanalysis.
In Proceedings of the Sixteenth ACM Con-ference on Conference on Information and Knowl-edge Management, CIKM ?07, pages 877?880, NewYork, NY, USA.
ACM.Dan Goldwasser and Hal Daum?e III.
2014.
?I object!
?modeling latent pragmatic effects in courtroom di-alogues.
European Chapter of the Association forComputational Linguistics (EACL), April.
To ap-pear.Benjamin Golub and Matthew O. Jackson.
2010.
See-ing only the successes: The power of selection biasin explaining the structure of observed internet dif-fusions.Vicenc?
G?omez, Andreas Kaltenbrunner, and VicenteL?opez.
2008.
Statistical analysis of the social net-work and discussion threads in slashdot.
In Proceed-ings of the 17th International Conference on WorldWide Web, WWW ?08, pages 645?654, New York,NY, USA.
ACM.Jizhou Huang, Ming Zhou, and Dan Yang.
2007.
Ex-tracting chatbox knowledge from online discussionforums.
In Proceedings of the 20th InternationalJoint Conference on Artifical Intelligence, IJCAI?07,pages 423?428, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Jonathan Huang, Chris Piech, Andy Nguyen, andLeonidas J. Guibas.
2013.
Syntactic and functionalvariability of a million code submissions in a ma-chine learning mooc.
In AIED Workshops.1510Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.2009.
Semi-supervised speech act recognition inemails and forums.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 3 - Volume 3, EMNLP?09, pages 1250?1259, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Ren?e F. Kizilcec, Chris Piech, and Emily Schnei-der.
2013.
Deconstructing disengagement: analyz-ing learner subpopulations in massive open onlinecourses.
In LAK, pages 170?179.Jon M. Kleinberg.
2013.
Computational perspectiveson social phenomena at global scales.
In FrancescaRossi, editor, IJCAI.
IJCAI/AAAI.Ravi Kumar, Mohammad Mahdian, and Mary McGlo-hon.
2010.
Dynamics of conversations.
In Pro-ceedings of the 16th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, KDD ?10, pages 553?562, New York, NY, USA.ACM.Haewoon Kwak, Changhyun Lee, Hosung Park, andSue Moon.
2010.
What is twitter, a social networkor a news media?
In Proceedings of the 19th In-ternational Conference on World Wide Web, WWW?10, pages 591?600, New York, NY, USA.
ACM.K.
Lerman and R. Ghosh.
2010.
Information conta-gion: An empirical study of the spread of news ondigg and twitter social networks.
In Proceedings of4th International Conference on Weblogs and SocialMedia (ICWSM).David Liben-Nowell and Jon Kleinberg.
2008.
Trac-ing the flow of information on a global scale usingInternet chain-letter data.
Proceedings of the Na-tional Academy of Sciences, 105(12):4633?4638, 25March.Andy Nguyen, Christopher Piech, Jonathan Huang,and Leonidas J. Guibas.
2014.
Codewebs: scalablehomework search for massive open online program-ming courses.
In WWW, pages 491?502.Chris Piech, Mehran Sahami, Daphne Koller, SteveCooper, and Paulo Blikstein.
2012.
Modeling howstudents learn to program.
In SIGCSE, pages 153?160.Chris Piech, Jonathan Huang, Zhenghao Chen, ChuongDo, Andrew Ng, and Daphne Koller.
2013.
Tunedmodels of peer assessment in MOOCs.
In Proceed-ings of The 6th International Conference on Educa-tional Data Mining (EDM 2013).Arti Ramesh, Dan Goldwasser, Bert Huang, HalDaum?e III, and Lise Getoor.
2013.
Modelinglearner engagement in moocs using probabilistic softlogic.
In NIPS Workshop on Data Driven Education.Daniel M. Romero, Brendan Meeder, and Jon Klein-berg.
2011.
Differences in the mechanics of in-formation diffusion across topics: Idioms, politicalhashtags, and complex contagion on twitter.
In Pro-ceedings of the 20th International Conference onWorld Wide Web, WWW ?11, pages 695?704, NewYork, NY, USA.
ACM.Glenda S. Stump, Jennifer DeBoer, Jonathan Whit-tinghill, and Lori Breslow.
2013.
Development of aframework to classify mooc discussion forum posts:Methodology and challenges.Manos Tsagkias, Wouter Weerkamp, and Maartende Rijke.
2009.
Predicting the volume of com-ments on online news stories.
In Proceedings of the18th ACM Conference on Information and Knowl-edge Management, CIKM ?09, pages 1765?1768,New York, NY, USA.
ACM.Yi-Chia Wang, Mahesh Joshi, and Carolyn PensteinRos.
2007.
A feature based approach to leveragingcontext for classifying newsgroup style discussionsegments.
In John A. Carroll, Antal van den Bosch,and Annie Zaenen, editors, ACL.
The Associationfor Computational Linguistics.Hongning Wang, Chi Wang, ChengXiang Zhai, and Ji-awei Han.
2011.
Learning online discussion struc-tures by conditional random fields.
In Proceedingsof the 34th International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, SIGIR ?11, pages 435?444, New York, NY,USA.
ACM.Chunyan Wang, Mao Ye, and Bernardo A. Huberman.2012.
From user comments to on-line conversa-tions.
In Proceedings of the 18th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, KDD ?12, pages 244?252, NewYork, NY, USA.
ACM.Li Wang, Su Nam Kim, and Timothy Baldwin.
2013.The utility of discourse structure in forum thread re-trieval.
In AIRS, pages 284?295.Tae Yano and Noah A. Smith.
2010.
What?s worthy ofcomment?
content and comment volume in politicalblogs.
In William W. Cohen and Samuel Gosling,editors, ICWSM.
The AAAI Press.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural svms with latent variables.
InProceedings of the 26th Annual International Con-ference on Machine Learning, ICML ?09, pages1169?1176, New York, NY, USA.
ACM.1511
