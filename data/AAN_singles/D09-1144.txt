Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1388?1397,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAutomatic Acquisition of the Argument-Predicate Relationsfrom a Frame-Annotated CorpusEkaterina OvchinnikovaUniversity of Osnabru?ckeovchinn@uos.deTheodore AlexandrovUniversity of Brementheodore@math.uni-bremen.deTonio WandmacherUniversity of Osnabru?cktwandmac@uos.deAbstractThis paper presents an approach to au-tomatic acquisition of the argument-predicate relations from a semanticallyannotated corpus.
We use SALSA, aGerman newspaper corpus manually an-notated with role-semantic informationbased on frame semantics.
Since the rel-atively small size of SALSA does not al-low to estimate the semantic relatednessin the extracted argument-predicate pairs,we use a larger corpus for ranking.
Twoexperiments have been performed in or-der to evaluate the proposed approach.In the first experiment we compare au-tomatically extracted argument-predicaterelations with the gold standard formedfrom associations provided by human sub-jects.
In the second experiment we cal-culate correlation between automatic relat-edness measure and human ranking of theextracted relations.1 IntroductionThere are many debates in lexical semantics aboutwhat kind of world knowledge actually belongsto the meaning of a lexeme.
Nowadays, it iswidely accepted that predicates impose selectionalrestrictions on their arguments.
For example, sincewe know that the predicate to be hungry mainlytakes expressions describing animate beings as ar-guments, we can correctly resolve the anaphorain the following sentence: We gave the bananasto the monkeys because they were hungry.
Thereexists also multiple linguistic evidence showingthat the semantics of arguments can help to pre-dict implicit predicates.
For example, the sentenceJohn finished the cigarette usually means John fin-ished smoking the cigarette because the meaningof the noun cigarette is strongly associated withthe smoking activity.It has been claimed that information about pred-icates associated with nouns can be helpful fora variety of tasks in natural language processing(NLP), see for example (Pustejovsky et al, 1993;Voorhees, 1994).
However, at present there existsno corresponding lexical semantic resource.
Sev-eral approaches have been presented that aim atcreating a knowledge base containing noun-verbrelations.
There are two main research paradigmsfor developing such knowledge bases.
The firstparadigm assumes manual development of the re-source (Pustejovsky et al, 2006), while the sec-ond one relies on automatic acquisition methods,see for example (Cimiano and Wenderoth, 2007).In this paper we propose a procedure for auto-matic acquisition of argument-predicate relationsfrom a semantically annotated corpus.
In line with(Lapata and Lascarides, 2003) our approach isbased on the assumption that predicates are omit-ted in a discourse when they are highly predictablefrom the semantics of their arguments.
We exploitSALSA (Burchardt et al, 2006), a German news-paper corpus manually annotated with FrameNetframes based on frame semantics.
Using a man-ually annotated corpus for relation extraction hasone particular advantage compared to extractionfrom plain text: the type of an argument-predicaterelation is already annotated; there is no need todetermine it by automatic means which are usu-ally error-prone.
However, the relatively smallsize of SALSA does not allow to make relevantpredictions about the degree of semantic related-ness in the extracted argument-predicate pairs, seesection 4.
We therefore employ a considerablylarger unannotated corpus for weighting.
The re-sults are evaluated quantitatively against humanjudgments obtained experimentally.
The proposedevaluation procedure is similar to that presented in(Cimiano and Wenderoth, 2007).
First, we createa gold standard for 30 words from the argumentlist and evaluate our approach with respect to this1388gold standard.
Second, we provide results froman evaluation in which test subjects are asked torate automatically extracted relations using a four-point scale.The paper is structured as follows: Section 2describes some linguistic phenomena requiring in-ferences of an implicit predicate from the seman-tics of an explicitly given argument.
In section 3we give a short overview of the related work.
Sec-tions 4 discusses the SALSA corpus.
Section 5 in-troduces our approach.
Finally, section 6 describesan experimental evaluation of the presented ap-proach and section 7 concludes the paper.2 Implicit PredicatesIn this section we discuss some linguistic phenom-ena requiring inferences of an implicit predicatefrom the semantics of an explicitly given argumentfor their resolution.
One of the most studied phe-nomena that Pustejovsky (1991) has called logicalmetonymy is illustrated by the examples (1a) and(1b) below.
In the case of logical metonymy an im-plicit predicate is inferable from particular verb-noun and adjective-noun pairs in a systematic way.The verb anfangen ?to start?
and the adjective kom-pliziert ?complicated?
in the mentioned examplessemantically select for an event, while the nouns(Buch ?book?
and Frage ?question?
respectively)have a different semantic type.
However, the setof the most probable implicit predicates is pre-dictable from the semantics of the nouns.
Thus,(1a) plausibly means Als ich angefangen habe,dieses Buch zu lesen/schreiben... ?When I havestarted to read/write this book...?
and (2a) plau-sibly means eine Frage die kompliziert zu beant-worten ist ?a question which is complicated to an-swer?.Example 1(a) Als ich mit diesem Buch angefangen habe...?When I have started this book...?
(b) eine komplizierte Frage?a complicated question?
(c) Studentenfutter?student food?
(d) Nachrichtenagentur Xinhua u?ber Beziehun-gen beider Seiten der Taiwan-Strasse?News agency Xinhua about relations of bothsides of the Taiwan Strait?
(e) Hans ist beredt?Hans is eloquent?As we can see from Example 1, besides logi-cal metonymy there are other linguistic phenom-ena requiring knowledge about predicates associ-ated with an argument for their resolution.
Exam-ple (1c) contains a noun compound which can beinterpreted on basis of the meaning of the nounFutter ?food?.
In general, noun compounds can beinterpreted in many different ways depending onthe semantics of the constituencies: morning cof-fee is a coffee which is drunk in the morning, brickhouse is a house which is made of bricks etc.
Incase of (1c) the relation via the predicate essen ?toeat?
taking Studenten ?students?
as a subject andFutter ?food?
as an object seems to be the mostplausible one.The phrase (1d) is a title of a newspaper ar-ticle.
As in the previous examples, a predicateis left out in (1d).
The meaning of the prepo-sition u?ber ?about?
can help to narrow down theset of possible predicates, but still allows an in-adequately large range of interpretations.
How-ever, the semantics of the noun Nachrichtenagen-tur ?news agency?
supports such interpretations asberichten ?to report?
or informieren ?to inform?.Most of the literature discusses predicates infer-able from nouns.
However, other parts of speechcan support similar inferences.
In example (1e) apredicate is predictable on the basis of the mean-ing of the adjective beredt ?eloquent?.
The sen-tence (1e) most plausibly means that Hans speakseloquently.Example 1 shows that knowledge about pred-icates associated with explicitly given argumentscan help to deal with several linguistic phenom-ena.
The cases when a predictable predicate is leftout are not rare in natural language.
For example,for logical metonymy a corpus study has shownthat the constructions like begin V NP occur rarelyif the verb V corresponds to a highly plausible in-terpretation of begin NP (Briscoe et al, 1990).3 Related WorkThe most influential account of logical metonymyis provided by Pustejovsky?s theory of the Gen-erative Lexicon, GL (Pustejovsky, 1991).
Ac-cording to Pustejovsky the meaning of a noun in-cludes a qualia structure representing ?the essen-tial attributes of an object as defined by the lexi-cal item?.
Thus, the lexical meaning of the nounbook includes read and write as qualia roles.
Inthe framework of GL, Pustejovsky et al (2006)1389are manually developing the Brandeis SemanticOntology which is a large generative lexicon on-tology and dictionary.
There also exist several ap-proaches to automatic acquisition of qualia struc-tures from text corpora which aim at supportingthe time-consuming manual work.
For example,Pustejovsky et al (1993) use generalized syntac-tic patterns for extracting qualia structures from apartially parsed corpus.
Cimiano and Wenderoth(2007) suggest a pattern-based method for auto-matic extraction of qualia structures from the Web.The results of the human judgment experiment re-ported in (Cimiano and Wenderoth, 2007) suggestthat the automatic acquisition of qualia structuresis a difficult task.
Human test subjects have showna very low agreement (11,8% average agreement)in providing qualia structures for given nouns.Another line of research on inferring implicitpredicates concerns using information about col-locations derived from corpora.
For example,Lapata and Lascarides (2003) resolve logicalmetonymy on the basis of the distribution of para-phrases like finish the cigarette ?
finish smok-ing the cigarette and easy problem ?
problemwhich is easy to solve in a corpus.
This approachshows promising results, but it is limited to logi-cal metonymy.
Similarly, Nastase et al (2006) usegrammatical collocations for defining semantic re-lations between constituents in noun compounds.In our study we aim at extracting intuitivelyplausible argument-predicate relations from a se-mantically annotated corpus.
Using an annotatedcorpus we avoid problems of defining types ofthese relations by automatic means which are usu-ally error-prone.
We represent argument-predicaterelations in terms of FrameNet frames which al-low for a fine-grained and grounded representationsupporting paraphrasing, see next sections.
Ourapproach is not restricted to nouns.
We also con-cern relations where argument positions are filledby adjectives, adverbs or even verbs.4 The SALSA CorpusFor relation extraction we have chosen the SALSAcorpus (Burchardt et al, 2006) developed at Saar-land University.
SALSA is a German corpusmanually annotated with role-semantic informa-tion, based on the syntactically annotated TIGERnewspaper corpus (Brants et al, 2002).
The2006 SALSA release which we have used con-tains about 20 000 annotated predicate instances.The corpus is annotated with the set of FrameNetframes.The FrameNet, FN (Ruppenhofer et al, 2006),lexical resource is based on frame semantics (Fill-more, 1976), see http://framenet.icsi.berkeley.edu.The lexical meaning of predicates in FN is ex-pressed in terms of frames (approx.
800 frames)which are supposed to describe prototypical sit-uations spoken about in natural language.
Everyframe contains a set of roles (or frame elements,FEs) corresponding to the participants of the de-scribed situation.
Predicates with similar seman-tics are assigned to the same frame, e.g.
to giveand to hand over refer to the GIVING frame.
Con-sider a FN annotation for the sentence (2a) below.In this annotation DONOR, RECIPIENT and THEMEare roles in the frame GIVING and John, Mary anda book are fillers of these roles.
The FN anno-tation generalizes across near meaning-preservingtransformations, see (2b).Example 2(a) [John]DONOR [gave]GIVING[Mary]RECIPIENT [a book]THEME.
(b) [John]DONOR [gave]GIVING [abook]THEME [to Mary]RECIPIENT.In FN information about syntactic realizationpatterns of frame elements as well as informationabout frequency of occurrences of these patterns incorpora is provided.
For example, the role DONORin the frame GIVING is most frequently filled by anoun phrase in the subject position or by a prepo-sitional phrase with the preposition by as the headin the complement position.The FN project originally aimed at developing aframe-semantic lexicon for English.
Later on FNframes turned out to be to a large extent languageindependent (Burchardt et al, 2006).
In most ofthe cases German predicates could be successfullydescribed by the FN frames.
However, some ofthe frames required adaptation to the German data,e.g.
new FEs were introduced.
Since FN does notcover all possible word senses, new frames neededto be added for some of the predicates.We have chosen the SALSA corpus for ourexperiments because to our knowledge it is theonly freely available corpus which contains bothsyntactic and role-semantic annotation.
However,we are aware that SALSA (approx.
700 000tokens) is too small to compute a reliable co-occurrence model for measuring plausibility of theextracted argument-predicate relations, though it1390is relatively large for a manually annotated cor-pus.
As it was shown in (Bullinaria and Levy,2007), co-occurrence-based approaches need verylarge training corpora in order to reliably computesemantic relatedness.
The SALSA corpus, com-prising less than 1 million tokens, is too small forthis purpose.
Moreover, a considerable number ofpredicates in SALSA appeared to be unannotated.Some of the high frequency pairs, as for exam-ple Bombe, explodieren ?bomb, to explode?, occurin SALSA only once, just as occasional pairs likeDeutsche, entdecken ?German, to discover?.
Wehave tried to overcome the size problems by usinga larger unannotated corpus for recomputing therating of our resulting relations, see next section.5 Automatic Acquisition of theArgument-Predicate RelationsIn line with (Lapata and Lascarides, 2003), our ap-proach to extraction of argument-predicate (AP)relations is based on two assumptions:A1: If predicates are highly predictable from thesemantics of their arguments then they can beomitted in a discourse;A2: If a predicate frequently takes a word as anargument then it is highly predictable from the se-mantics of this word.In the proposed experimental setting argument-predicate relations are defined in terms of theFrameNet frames.
Thus, we aim at extractingfrom SALSA tuples of the form ?Argument, ROLE,FRAME, Predicate?
such that the Argument plau-sibly fills the ROLE in the FRAME evoked by thePredicate.
As already mentioned in section 3,our approach is not restricted to nouns.
We alsotreat arguments expressed by other content partsof speech.
The proposed relation extraction pro-cedure consists in?
finding for every content word which occursin the corpus a set of predicates taking thisword as an argument with a high probability;?
defining a relation between the word and ev-ery predicate from this set by finding whichroles the noun fills in frames evoked by thepredicate;?
estimating the degree of the semantic relat-edness in the extracted argument-predicatepairs.For example, analyzing the following sentence[Fu?nf Oppositionelle]SUSPECT sind in Ebe-biyin [von der Polizei]AUTHORITIES [festgenom-men]ARREST worden.
?Five members of the opposition have beenarrested by the police in Ebebiyin.
?we aim at extracting the following tuples:Argument Role Frame PredicateOppositionell SUSPECT ARREST festnehmenPolizei AUTHORITIES ARREST festnehmenRelation ExtractionIn SALSA, every sentence is annotated with a setof frames in such a way that for every frame itsFEs refer to some syntactic constituents in the sen-tence.
In order to extract argument-predicate rela-tions from SALSA we need 1) to find a contenthead for every constituent corresponding to a FE;2) to resolve possibly existing anaphora.
SinceSALSA is syntactically annotated, the first taskproved to be relatively easy.1 On the contrary,anaphora resolution is well-known to be one ofmost challenging NLP tasks.
In our study, wedo not focus on it, and we treat only pronominalanaphora using the following straightforward res-olution algorithm: given a pronoun the first nounwhich agrees in number and gender with the pro-noun is supposed to be its antecedent.
In orderto evaluate this resolution procedure we have in-spected 100 anaphoric cases.
In approximatelythree fourths of the cases the anaphora were re-solved correctly.
Therefore, we have assigned aconfidence rate of 0,75 to the FE fillers resultingfrom a resolved anaphora.
In non-anaphoric casesa confidence rate of 1 was assigned.For every extracted tuple of the form?Argument, ROLE, FRAME, Predicate?
wehave summed up the corresponding confidencerates.
Finally, we have obtained around 30 000tuples with confidence rates ranging from 0,75to 88.
It is not surprising that most of the argu-ments appeared to be nouns, while most of thepredicates are expressed by verbs.
Since SALSAhas been annotated manually, there are almostno mistakes in defining types of the semantic1We have excluded from the consideration foreign-language expressions, while proper nouns were treated in theusual way.
For verb phrases with auxiliary or modal verbs asheads the main verb was taken as a corresponding role filler.1391relations between arguments and predicates.2 Forseveral pairs, the semantic relation between anargument and a predicate is ambiguous.
Considerthe tuples extracted for the word pair Buch,schreiben ?book, to write?
which are given below.While the first tuple corresponds to phrases likeein Buch schreiben ?to write a book?, the secondone abstracts from the expressions like in einemBuch schreiben ?to write in a book?.Argument Role Frame PredicateBuch TEXT TEXT CREATION schreibenBuch MEDIUM STATEMENT schreibenAdditionally, ambiguity can arise because of theannotation disagreements in SALSA.
For exam-ple, the pair (Haft, sitzen) ?imprisonment?, ?to sit?in Table 1 was annotated in SALSA both with theBEING LOCATED and with the POSTURE frames.As mentioned in section 4, a considerable num-ber of predicates in SALSA is not annotated se-mantically.
In order to find out how many relevantAP-relations get lost if we consider only seman-tically annotated predicates, we have additionallyextracted AP-pairs on the basis of the syntactic an-notation only.
The anaphora resolution procedureas described above was again applied to the syn-tactic argument heads.
We have obtained around56 500 pairs with confidence rates ranging from0,75 to 71,50.3As one could expect, being a newspaper corpusSALSA appeared to be thematically unbalanced.The most frequent argument-predicate relationsoccurring in SALSA reflect common topics dis-cussed in newspapers: economics (e.g.
(Prozent,steigen), ?percent?, ?to increase?
), criminality (e.g.
(Haft, verurteilen) ?imprisonment?, ?to sentence?
),catastrophes (e.g.
(Mensch, to?ten) ?human?, ?tokill?)
etc.RankingAs mentioned in section 4, the size of SALSAdoes not allow to make relevant predictions aboutthe distribution of frames and role fillers.
Only2% of the relations occur in SALSA more then3 times.
In order to overcome this problem wehave developed a measure of semantic relatednessbetween the extracted arguments and predicates2Mistakes can arise only because of the annotation errorsand errors in the anaphora resolution procedure.3The comparison of the results obtained by the extractionprocedure based on the semantic annotation with the resultsof the procedure based on the syntactic annotation only isprovided in the next section.which takes into account their co-occurrence in alarger and more representative corpus.
For com-puting semantic relatedness we have used a lem-matized newspaper corpus (Su?ddeutsche Zeitung,SZ) of 145 million words.
Given a tuple t with aconfidence rate c containing an argument a and apredicate p, the relatedness measure rm of t wascomputed as follows:rm(t) = lsa(a, p) + c/max(c),where the lsa(a, p) is based on Latent SemanticAnalysis, LSA (Deerwester et al, 1990).
LSA isa vector-based technique that has been shown togive reliable estimates on semantic relatedness.
Itmakes use of distributional similarities of wordsin text and constructs a semantic space (or wordspace) in which every word of a given vocabularyis represented as a vector.
Such vectors can thenbe compared to one another by the usual vectorsimilarity measures (e.g.
cosine).
We calculatedthe LSA word space using the Infomap toolkit10v.
0.8.6 (http://infomap-nlp.sourceforge.net).
Theco-occurrence matrix (window size: 5 words)comprised 80 000?3 000 terms and was reducedby SVD to 300 dimensions.
For the vector com-parisons the cosine measure was applied.
To thosewords which did not occur in the analyzed SZ cor-pus (approx.
3500 words) a lsa measure of 0 wasassigned.
To provide a comparable contribution torm, the confidence rates c extracted from SALSAare divided by the maximal confidence rate.
Therm function is a linear interpolation of the lsa andthe normalized c measure.
As mentioned above,the c measure is a discriminative factor for only2% of the relations.
For the remaining 98% thenormalized c values are small (0,003 or 0,002 or0,001).
Therefore, calculating the rm measure wemainly rely on lsa, while normalized c actuallyplays a role only for the relations frequently oc-curring in SALSA.
Table 1 contains the 5 most se-mantically related predicates for an example argu-ment.6 EvaluationSince the extracted argument-predicate relationsare intended to be used for inferring intuitively ob-vious predicates,we evaluate to which extend theycorrespond to human intuition.1392Table 1: Examples of the extracted argument-predicate relationsArgument Role Frame Predicate rmHaft FINDING VERDICT verurteilen ?to sentence?
0,939?imprisonment?
LOCATION BEING LOCATED sitzen ?to sit?
0,237LOCATION POSTURE sitzen ?to sit?
0,226MESSAGE REQUEST fordern ?to demand?
0,153BAD OUTCOME RUN RISK-FNSALSA drohen ?to threaten?
0,144Gold StandardSimilar to (Cimiano and Wenderoth, 2007) weprovide a gold standard for 30 test arguments oc-curring in the SALSA corpus.
The test argu-ments were selected randomly from the set ofthose arguments that have more than one pred-icate associated with them such that a value ofargument-predicate relatedness exceeds the aver-age one.
These words were nearly uniformly dis-tributed among 20 participants of the experiment,who were all non-linguists.
We also ensured thateach word was treated by three different subjects.For every word we asked our subjects to write be-tween 5 and 10 short phrases that contain a pred-icate taking the given word as an argument, e.g.book ?
to read a book.
The participants were askedto provide phrases instead of single predicates, be-cause we wanted to control the syntactic and se-mantic position of the arguments.
The participantsreceived an instruction informally describing thenotion of predicate and what kind of phrases theyare supposed to come up with.
Besides the taskdescription they were shown examples containingappropriate and inappropriate phrases.
Some ofthe examples are given below.Example 3(a) Aktie ?stock?
: Kauf der Aktien ?buying ofstocks?, Aktien kaufen ?to buy stocks?, Aktien ander Bo?rse ?stocks on the bourse?
(is inappropriatebecause the word ?bourse?
describes a place andnot an event)(b) beredt ?eloquent?
: beredt sprechen ?tospeak eloquently?, ein beredter Sprecher ?an elo-quent speaker?
(is inappropriate because the word?speaker?
describes a person and not an event)The test was conducted via e-mail.
In or-der to compare the human associations with theextracted AP-relations, we have manually anno-tated the obtained phrases with SALSA frames.The agreement for the described task for everycue word was calculated as the averaged pairwiseagreement between the AP-relations delivered bythe three subjects, S1, S2and S3, as follows:Agr =|S1?S2||S1?S2|+|S2?S3||S2?S3|+|S2?S3||S2?S3|3.Agreement results for every cue word are re-ported in table 2.
Second column of the tablecontains gold standard predicates which were pro-vided by all 3 participants treating the same word.4Averaging over all words, we got a mean agree-ment of 13%.
Though this value seems to be low,it is consistent with a mean agreement of 11,8%for a similar task reported in (Cimiano and Wen-deroth, 2007), see section 3.
Cimiano and Wen-deroth (2007) show that the lowest agreement isyielded for more abstract words, while the agree-ment for very concrete words is reasonable.
Wecould not make a similar observation, see table 2.Comparison with the Gold StandardIn the first experiment we checked whether pred-icates which people associate with the test argu-ments can be automatically extracted by our pro-cedure.
For this aim we compared the gold stan-dard with all automatically extracted argument-predicate relations5 containing some of the 30 cuewords as follows.
These relations were ranked ac-cording to the relatedness measure described inprevious section.
In line with (Cimiano and Wen-deroth, 2007) we exploited an approach commonin information retrieval for estimating the qual-ity of correspondence of a ranked output to agold standard, see (Baeza-Yates and Ribeiro-Neto,1999).Given some n automatically extracted relationswith the highest ranking we calculated a precision-recall curve expressing precision and recall of ourprocedure compared to the gold standard.
The pre-cision characterizes the procedure exactness, i.e.how many redundant relations are retrieved.
The4The overall gold standard consists of 33 tuples.5In order to evaluate the procedure extracting AP-relations on the basis of the semantic annotation we com-pared automatically extracted tuples to the gold standard tu-ples.
For the procedure using the syntactic annotation onlythe AP-pairs were considered without regarding frames andFEs.1393recall measures the completeness, i.e.
how manyrelations of the gold standard are extracted auto-matically.
For each point of the curve (which isa pair (p, r) of values of precision p and recall r)we calculated the F -measure as F = 2pr/(p+ r)which is the harmonic mean between recall andprecision.
The precision-recall curve is a set ofprecision values for the prespecified recall levelsvarying from 0 to 1 with a step 0,1.
Then, to pro-duce only one value evaluating the quality of theranked output compared to the gold standard, foreach precision-recall curve we calculated Fmax,the maximal value of the F -measure achievedfor the points of this curve.
Fmaxexpresses thebest trade-off between precision and recall for thegiven ranked output.
Finally, among all possiblen (numbers of the considered relations with thehighest ranking) we selected that one which pro-vides the maximal Fmaxvalue.The resulting maximal Fmaxvalues are 0,47 forthe procedure extracting AP-relations on the basisof the semantic annotation and 0,41 for the pro-cedure using the syntactic annotation only.
Wecompared these results with the baseline resultsof maximal Fmaxvalues produced for the outputwith random ranking.
The calculation of the base-line was repeated 100 times, each time a new ran-dom ranking was generated.
The lowest baselineresults are 0,08/0,06 (semantic/syntactic annota-tion), the highest are 0,18/0,14 and the mediansare 0,1/0,07.
One can see that the results producedusing the relatedness measure (0,47/0,41) greatlyexceed the baseline.
Based on this comparison weconclude that the ranking done using the related-ness measure brings a significant advantage.
Thevalues of precision and recall for the reported max-imal Fmaxvalues are 0,5/0,33 (semantic/syntacticannotation) and 0,45/0,54 respectively.
This re-sults show that half of the AP-relations from thegold standard appeared to be in the list of the top-ranked tuples extracted by the ?semantic?
proce-dure, while the size of this list (n = 28) was al-most equal to the size of the gold standard (33).The differences in performance between the ?se-mantic?
and ?syntactic?
procedures could be ex-plained by the fact that the ?syntactic?
procedurefinds in the corpus more related predicates for ev-ery argument than the ?semantic?
one.
Neverthe-less, the ?semantic?
procedure shows better per-formance.Next we investigated the results for each argu-ment used in the gold standard separately in thesame way as described above.
For each argumentthe Fmaxmeasure has been computed.
Becauseof the low agreement between the subjects ques-tioned for the gold standard (see above), in thesecalculations we considered all predicates reportedby our subjects.
The calculated Fmaxvalues arereported in table 2 which shows a correlation be-tween Fmaxvalues calculated for the ?semantic?and ?syntactic?
procedures.
However, there isno correlation with human agreement.
This issueneeds a further investigation, see section 7.Human Judgments of the RelatednessFollowing (Cimiano and Wenderoth, 2007), in or-der to check whether the calculated relatednessis reasonable according to human intuition, wehave performed another experiment.
For each ofthe 30 words selected for the gold standard weselected the 5 top ranked predicates.
Since forsome of the cue arguments only 3 predicates werefound in the corpus, the final test set contains only138 argument-predicate tuples.
From these tupleswe generated short grammatically correct phrasesstructurally similar to those in example 3.
Thesephrases were uniformly distributed among 10 sub-jects so that every phrase was evaluated by onesubject.
The participants were asked to rate thephrases with respect to their naturalness using ascale from 0 to 3, whereby 0 means ?unnatural?,1 ?possible?, 2 ?natural?
and 3 ?totally natural andself-evident?.Further on we investigated the relationship be-tween the human estimates and the relatednessvalues obtained automatically.
For this aim weused the Spearman rank correlation coefficient.Because of four-points scale used, the humanrankings are equal for many tuples which lead tothe so-called effect of ties.
For this reason wecomputed the correlation coefficient with a cor-rection for ties.
The coefficient value is 0,30 andthis correlation is statistically significant with p-value 0,0006.
Based on these results we concludethat our relatedness measure is correlated with hu-man judgments.
Taking into account the subjec-tive character of human ranking in terms of nat-uralness, the achieved correlation values can beconsidered as high.1394Table 2: Evaluation results for 30 gold standard cue words.Cue word Shared predicates Agr Sem.
FmaxSyn.
FmaxName ?name?
haben ?to have?
14% 0,2 0,48Urlaub ?vacation?
fahren ?to go?
8% 0,13 0,16Sprache ?language?
sprechen ?to speak?, lernen ?to learn?
14% 0,4 0,3Strafe ?fine?
verurteilen ?to sentence?
11% 0,21 0,3Stuhl ?chair?
sitzen ?to sit?
14% 0,1 0,2Bombe ?bomb?
hochgehen ?to blow up?
14% 0,11 0,22Blatt ?gazette?, ?page?, ?leaf?
?
2% 0 0Flughafen ?airport?
ankommen ?to arrive?, fahren ?to go?
21% 0,17 0,17Gesetz ?low?
?
8% 0,17 0,38Polizei ?police?
rufen ?to call?
11% 0,22 0,23Kompromiss ?compromise?
schliessen ?to make?
15% 0,07 0,29Fluggesellschaft ?airline?
?
3% 0,11 0,38Antrag ?proposal?, ?application?
stellen ?to introduce?, ablehnen ?to decline?
24% 0,43 0,42Zeitung ?newspaper?
lesen ?to read?
13% 0,17 0,09Brief ?letter?
verschicken ?to send?, schreiben ?to write?
19% 0,23 0,12Flu?chtling ?refugee?
aufnehmen ?to accept?
13% 0 0,07Buch ?book?
schreiben ?to write?, lesen ?to read?
15% 0,44 0,39Za?hler ?counter?
ablesen ?
to read?
11% 0 0Anzahl ?number?
?
3% 0,23 0,19Prozent ?percent?
?
3% 0,48 0,21Ziel ?goal?
verfehlen ?to miss?, erreichen ?to reach?
20% 0,3 0,48Schule ?school?
schwa?nzen ?to miss?, gehen ?to go?
22% 0,13 0,23Amt ?position?, ?department?
bekleiden, innehaben ?to hold?, gehen ?to go?
20% 0 0,17Frage ?question?
beantworten ?to answer?, stellen ?to ask?
20% 0,15 0,37Mensch ?human?
sein ?to be?
16% 0,09 0,03Zeuge ?witness?
aussagen ?to testify?, sein ?to be?
22% 0,13 0,19Thema ?theme?
?
7% 0,14 0,26Preistra?ger ?prize winner?
?
5% 0,08 0,08Initiative ?initiative?
ergreifen ?to take?
17% 0,1 0,13Wohnung ?flat?
?
7% 0,09 0,177 Conclusion and DiscussionIn this paper we presented an approach to auto-matic extraction of argument-predicate relationsfrom a frame-annotated corpus.6 In our approachwe aimed to combine the advantages offered byannotated and unannotated lexical resources.
Be-sides extracting AP-pairs the proposed method al-lows us to define types of semantic relations interms of FrameNet frames.
The proposed proce-dure is not restricted to arguments expressed bynouns and treats also other content parts of speech.The main goal of this paper was to show thatthough manually annotated corpora usually havea relatively small size, they can be successfullyexploited for the relation extraction.
An obviouslimitation of the presented approach is that it isbounded to manual annotations which are hardto obtain.
However, since semantic annotationsare useful for many different goals in linguisticsand NLP, the number of reliable annotated cor-pora constantly grows.7 Moreover, recently sev-6The complete list of the extracted AP-relations as wellas the results of the experiment will be available online athttp://www.ikw.uni-osnabrueck.de/?eovchinn/APrels/.7At present FrameNet annotated corpora areeral tools have been developed which perform roleannotation automatically, for example see (Erkand Pado, 2006).
Therefore we believe that ap-proaches using semantic annotation are valid andpromising.
In the future we plan to experimentwith large role-annotated corpora for English suchas PropBank (approx.
300 000 words, (Palmeret al, 2005)) and the FrameNet-annotated corpusprovided by the FN project (more than 135 000annotated sentences, (Ruppenhofer et al, 2006)).Since these corpora do not contain syntactic anno-tation, for extracting argument-predicate relationswe will need to parse annotated sentences.There are several ways to improve the proposedprocedure.
First, an implementation of a moreadvanced anaphora resolution algorithm treatingpronominal as well as nominal anaphora shouldsignificantly raise the precision/recall characteris-tics.
Second, splitting German compounds occur-ring in the corpus should provide additional ev-idence.
We have treated such words as Kunde?client?
and Privatkunde ?private client?
as differ-ent lexemes, while they are strongly related se-available for English, German and Spanish, seehttp://framenet.icsi.berkeley.edu.1395mantically and information about predicates co-occurring with the second word could probablybe used for describing the semantics of the firstone.
Concerning relatedness measure, additionalcorpus-based measures such as Web-based mea-sures (Cimiano and Wenderoth, 2007) or measuresbased on syntactic relations (Pustejovsky et al,1993) could appear to be useful for improving theranking of the extracted relations.The presented procedure was evaluated quanti-tatively against human judgments obtained experi-mentally.
The participants of the experiment wereasked to provide short phrases containing givencue words and predicates associated with thesewords as well as to rate phrases generated from theautomatically extracted AP-relations.
Concerningthe first experiment, the low human agreement hasshown that the proposed association task appearedto be difficult for the subjects.
Nevertheless, thedescribed learning procedure proved to extract in-tuitively reasonable relations.The evaluation strategy presented in this pa-per on relies on the underlying assumptions (A1and A2 in section 5) and is compatible with theother approaches to relation extraction, cf.
(Cimi-ano and Wenderoth, 2007).
However, it is plau-sible that human responses in the context of pro-viding associated predicates for target words willdiffer from the responses in the experimental set-tings where subjects are asked to infer implicitpredicates, e.g.
to extend phrases containing im-plicit predicates.
In the future we plan to im-plement a procedure making use of the extractedAP-relations which would automatically extendphrases containing implicit predicates.
Then weintend to compare output results of the procedurewith the human responses.
Additionally, a study ofa possible correspondence between human agree-ment on associated predicates and a semantic typeof an argument (e.g.
concrete/abstract, naturalkind/artifact) should be performed on more test ar-guments.Potential ApplicationsAs already mentioned in the literature, see for ex-ample (Lapata and Lascarides, 2003), knowledgeabout implicit predicates could be potentially use-ful for a variety of NLP tasks such as languagegeneration, information extraction, question an-swering or machine translation.
Many applica-tions of semantic relations in NLP are connectedto paraphrasing or query expansion, see for ex-ample (Voorhees, 1994).
Suppose that a searchengine or a question answering system receivesthe query schnelle Bombe ?quick bomb?.
Prob-ably, in this case the user is interested in find-ing information about bombs that explode quicklyrather then about bombs in general.
Knowledgeabout predicates associated with the noun Bombe?bomb?
could be used for predicting a set of prob-able implicit predicates.
For generation of the se-mantically and syntactically correct paraphrases itis sometimes not enough to guess the most prob-able argument-predicate pairs.
Information abouttypes of an argument-predicate relation could behelpful, i.e.
which semantic and syntactic posi-tion does the argument fill in the argument struc-ture of the predicate.
For example, compareeine Bombe explodiert schnell ?a bomb explodesquickly?
for schnelle Bombe with ein Buch schnelllesen/schreiben ?to read/write a book quickly?
forschnelles Buch ?quick book?.
In the first case theargument Bombe fills the subject position, whilein the second case Buch fills the object posi-tion.
Since FrameNet contains information aboutsyntactic realization patterns for frame elements,representation of argument-predicate relations interms of frames directly supports generation of se-mantically and syntactically correct paraphrases.The described procedure could also supportmanual development of a lexical resource, provid-ing evidence from corpora as well as the distribu-tional information.ReferencesRicardo Baeza-Yates and Berthier Ribeiro-Neto.
1999.Modern Information Retrieval.
Addison Wesley,Harlow, 1. aufl.
edition.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories.Ted Briscoe, Ann Copestake, and Bran Boguraev.1990.
Enjoy the paper: Lexical semantics via lex-icology.
In Proceedings of the 13th InternationalConference on Computational Linguistics, pages42?47.John Bullinaria and Joseph Levy.
2007.
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39(3):510?526.Aljoscha Burchardt, Katrin Erk, Anette Frank, An-drea Kowalski, Sebastian Pado, and Manfred Pinkal.13962006.
The SALSA corpus: A German corpus re-source for lexical semantics.
In Proceedings ofLREC 2006, pages 969?974.Philipp Cimiano and Johanna Wenderoth.
2007.
Auto-matic Acquisition of Ranked Qualia Structures fromthe Web.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics,pages 888?895.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
AmericanSociety of Information Science, 41(6):391?407.Katrin Erk and Sebastian Pado.
2006.
Shalmaneser -a flexible toolbox for semantic role assignment.
InProceedings of LREC 2006, Genoa, Italy.Charles J. Fillmore.
1976.
Frame semantics and thenature of language.
In Annals of the New YorkAcademy of Sciences: Conference on the Origin andDevelopment of Language and Speech, volume 280,pages 20?32.Mirella Lapata and Alex Lascarides.
2003.
A Prob-abilistic Account of Logical Metonymy.
Computa-tional Linguistics, 29(2):261?316.Vivi Nastase, Jelber Sayyad-Shirabad, MarinaSokolova, and Stan Szpakowicz.
2006.
Learningnoun-modifier semantic relations with corpus-basedand wordnet-based features.
In Proceedings of theAAAI 2006.Martha Palmer, Dan Gildea, and Paul Kingsbury.2005.
The Proposition Bank: A Corpus Annotatedwith Semantic Roles.
Computational Linguistics,31(1):71?106.James Pustejovsky, Peter Anick, and Sabine Bergler.1993.
Lexical semantic techniques for corpus anal-ysis.
Computational Linguistics, 19(2):331?358.James Pustejovsky, Catherine Havasi, Roser Saur,Patrick Hanks, Anna Rumshisky, Jessica Littman,Jos Castao, and Marc Verhagen.
2006.
Towards agenerative lexical resource: The Brandeis SemanticOntology.
In Proceedings of the Fifth Language Re-source and Evaluation Conference.James Pustejovsky.
1991.
The Generative Lexicon.Computational Linguistics, 17(4):409?441.Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, Christopher R. Johnson, and Jan Schef-fczyk.
2006.
FrameNet II: Extended Theory andPractice.
International Computer Science Institute.Ellen M. Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings of the17th annual international ACM SIGIR conferenceon Research and development in information re-trieval, pages 61?69.1397
