Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 117?120,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPCo-Feedback Ranking for Query-Focused SummarizationFuru Wei1,2,3  Wenjie Li1 and Yanxiang He21 The Hong Kong Polytechnic University, Hong Kong{csfwei,cswjli}@comp.polyu.edu.hk2 Wuhan University, China{frwei,yxhe}@whu.edu.cn3 IBM China Research Laboratory, Beijing, ChinaAbstractIn this paper, we propose a novel rankingframework ?
Co-Feedback Ranking (Co-FRank), which allows two base rankers tosupervise each other during the rankingprocess by providing their own ranking resultsas feedback to the other parties so as to boostthe ranking performance.
The mutual rankingrefinement process continues until the twobase rankers cannot learn from each other anymore.
The overall performance is improved bythe enhancement of the base rankers throughthe mutual learning mechanism.
We apply thisframework to the sentence ranking problem inquery-focused summarization and evaluate itseffectiveness on the DUC 2005 data set.
Theresults are promising.1 Introduction and BackgroundSentence ranking is the issue of most concern inextractive summarization.
Feature-basedapproaches rank the sentences based on thefeatures elaborately designed to characterize thedifferent aspects of the sentences.
They havebeen extensively investigated in the past due totheir easy implementation and the ability toachieve promising results.
The use of feature-based ranking has led to many successful (e.g.top five) systems in DUC 2005-2007 query-focused summarization (Over et al, 2007).
Avariety of statistical and linguistic features, suchas term distribution, sentence length, sentenceposition, and named entity, etc., can be found inliterature.
Among them, query relevance,centroid (Radev et al, 2004) and signature term(Lin and Hovy, 2000) are most remarkable.There are two alternative approaches tointegrate the features.
One is to combine featuresinto a unified representation first, and then use itto rank the sentences.
The other is to utilize rankfusion or rank aggregation techniques to combinethe ranking results (orders, ranks or scores)produced by the multiple ranking functions into aunified rank.
The most popular implementationof the latter approaches is to linearly combine thefeatures to obtain an overall score which is thenused as the ranking criterion.
The weights of thefeatures are either experimentally tuned orautomatically derived by applying learning-basedmechanisms.
However, both of the above-mentioned ?combine-then-rank?
and ?rank-then-combine?
approaches have a common drawback.They do not make full use of the informationprovided by the different ranking functions andneglect the interaction among them beforecombination.
We believe that each individualranking function (we call it base ranker) is ableto provide valuable information to the other baserankers such that they learn from each other bymeans of mutual ranking refinement, which inturn results in overall improvement in ranking.To the best of our knowledge, this is a researcharea that has not been well addressed in the past.The inspiration for the work presented in thispaper comes from the idea of Co-Training (Blumand Mitchell, 1998), which is a very successfulparadigm in the semi-supervised learningframework for classification.
In essence, co-training employs two weak classifiers that helpaugment each other to boost the performance ofthe learning algorithms.
Two classifiers mutuallycooperate with each other by providing their ownlabeling results to enrich the training data for theother parties during the supervised learningprocess.
Analogously, in the context of ranking,although each base ranker cannot decide theoverall ranking well on itself, its ranking resultsindeed reflect its opinion towards the rankingfrom its point of view.
The two base rankers canthen share their own opinions by providing theranking results to each other as feedback.
Foreach ranker, the feedback from the other rankercontains additional information to guide therefinement of its ranking results if the feedbackis defined and used appropriately.
This processcontinues until the two base rankers can not learnfrom each other any more.
We call this rankingparadigm Co-Feedback Ranking (Co-FRank).The way how to use the feedback information117varies depending on the nature of a ranking task.In this paper, we particularly consider the task ofquery-focused summarization.
We design a newsentence ranking algorithm which allows aquery-dependent ranker and a query-independentranker mutually learn from each other under theCo-FRank framework.2 Co-Feedback Ranking for Query-Focused Summarization2.1 Co-Feedback Ranking FrameworkGiven a set of objects O, one can define two baseranker f1 and f2:     Ooofof ??o?o ,, 21 .
Theranking results produced by f1 and f2 individuallyare by no means perfect but the two rankers canprovide relatively reasonable rankinginformation to supervise each other so as tojointly improve themselves.
One way to do Co-Feedback ranking is to take the most confidentranking results (e.g.
highly ranked instancesbased on orders, ranks or scores) from one baseranker as feedback to update the other?s rankingresults, and vice versa.
This process continuesiteratively until the termination condition isreached, as depicted in Procedure 1.
While thestandard Co-Training algorithm requires twosufficient and redundant views, we suggest f1 andf2 be two independent rankers which emphasizetwo different aspects of the objects in O.Procedure 1.
Co-FRank(f1, f2, O)1:  Rank O with f1 and obtain the ranking results r1;2:  Rank O with f2 and obtain the ranking results r2;3:  Repeat4:  Select the top N ranked objects 1W  from r1 asfeedback to supervise f2, and re-rank O using f2and 1W ; Update r2;5:  Select the top N ranked objects 2W  from r2 asfeedback to supervise f1, and re-rank O using f1and 2W ; Update r1;5:  Until I(O).The termination condition I(O) can be definedaccording to different application scenarios.
Forexample, I(O) may require the top K rankedobjects in r1 and r2 to be identical if one isparticularly interested in the top ranked objects.It is also very likely that r1 and r2 do not changeany more after several iterations (or the top Kobjects do not change).
In this case, the two baserankers can not learn from each other any more,and the Co-Feedback ranking process shouldterminate either.
The final ranking results can beeasily determined by combining the two baserankers without any parameter, because theyhave already learnt from each other and can beequally treated.2.2 Query-Focused Summarization basedon Co-FRankThe task of query-focused summarization is toproduce a short summary (250 words in length)for a set of related documents D with respect tothe query q that reflects a user?s informationneed.
We follow the traditional extractivesummarization framework in this study, wherethe two critical processes involved are sentenceranking and sentence selection, yet we focusmore on the sentence ranking algorithm based onCo-FRank.
As for sentence selection, weincrementally add into the summary the highestranked sentence if it doesn?t significantly repeat1the information already included in the summaryuntil the word limitation is reached.In the context of query-focused summarization,two kinds of features, i.e.
query-dependent andquery-independent features are necessary andthey are supposed to complement each other.
Wethen use these two kinds of features to developthe two base rankers.
The query-dependentfeature (i.e.
the relevance of the sentence s to thequery q) is defined as the cosine similaritybetween s and q.qsqsqsqsrelf ?x  ?
/,cos,1  (1)The words in s and q vectors are weighted bytf*isf.
Meanwhile, the query-independent feature(i.e.
the sentence significance based on wordcentroid) is defined asswcscfsw/2 ?
?
?
(2)where c(w) is the centroid weight of the word win s and     DSDs ws Nisftfwc w?
?
?
.
DSN  is the totalnumber of the sentences in D, swtf  is thefrequency of w in s, and  wDw sfNisf Slog  is theinverse sentence frequency (ISF) of w, where sfwis the sentence frequency of w in D. The sentenceranking algorithm based on Co-FRank is detailedin the following Algorithm 1.Algorithm 1.
Co-FRank(f1, f2, D, q)1:  Extract sentences S={s1, ?
sm} from D;2:  Rank S with f1 and obtain the ranking results r1;3:  Rank S with f2 and obtain the ranking results r2;4:  Normalize r1,            11111 minmaxmin rrrsrsr ii  ;5:  Normalize r2,            22222 minmaxmin rrrsrsr ii  ;6:  Repeat1 A sentence is discarded if the cosine similarity of it to anysentence already selected into the summary is greater than0.9.1187:  Select the top N ranked sentences at round n n1Wfrom r1 as feedback for f2, and re-rank S using f2and n1W ,nssims nkkii /,12 1?
m WS ,     22 222 minmaxminSSSSS iii ssfsr 222 1 SKK ??m                         (3)8: Select the top N ranked sentences at round n n2Wfrom r2 as feedback for f1, and re-rank S using f1and n2W ;nssims nkkii /,11 2?
m WS ,     11 111 minmaxminSSSSS iii ssfsr 111 1 SKK ??m                              (4)9: Until the top K sentences in r1 and r2 are the same,both r1 and r2 do not change any more, ormaximum iteration round is achieved.10: Calculate the final ranking results,221 iii srsrsr  .
(5)The update strategies used in Algorithm 1, asformulated in Formulas (3) and (4), are designedbased on the intuition that the new ranking of thesentence s from one base ranker (say f1) consistsof two parts.
The first part is the initial rankingproduced by f1.
The second part is the similaritybetween s and the top N feedback provided bythe other ranker (say f2), and vice versa.
The topK ranked sentences by f2 are supposed to behighly supported by f2.
As a result, a sentencethat is similar to those top ranked sentencesshould deserve a high rank as well.
nissim 2,Wcaptures the effect of such feedback at round nand the definition of it may vary with regard tothe application background.
For example, it canbe defined as the maximum, the minimum or theaverage similarity value between si and a set offeedback sentences in 2W .
Through this mutualinteraction, the two base rankers supervise eachother and are expected as a whole to producemore reliable ranking results.We assume each base ranker is most confidentwith its first ranked sentence and set N to 1.Accordingly,  nissim 2,W is defined as the similaritybetween si and the one sentence in n2W .
K  is abalance factor which can be viewed as theproportion of the dependence of the new rankingresults on its initial ranking results.
K is set to 10as 10 sentences are basically sufficient for thesummarization task we work on.
We carry out atmost 5 iterations in the current implementation.3 Experimental StudyWe take the DUC 2005 data set as the evaluationcorpus in this preliminary study.
ROUGE (Linand Hovy, 2003), which has been officiallyadopted in the DUC for years is used as theevaluation criterion.
For the purpose ofcomparison, we implement the following twobasic ranking functions and the linearcombination of them for reference, i.e.
the queryrelevance based ranker (denoted by QRR, sameas f1) and the word centroid based ranker(denoted by WCR, same as f2), and the linearcombined ranker, LCR= O QRR+(1- O )WCR,where O  is a combination parameter.
QRR andWCR are normalized by    minmaxmin x ,where x, max and min denote the original rankingscore, the maximum ranking score and minimumranking score produced by a ranker, respectively.Table 1 shows the results of the average recallscores of ROUGE-1, ROUGE-2 and ROUGE-SU4 along with their 95% confidence intervalsincluded within square brackets.
Among them,ROUGE-2 is the primary DUC evaluationcriterion.ROUGE-1 ROUGE-2 ROUGE-SU4QRR 0.3597 [0.3540, 0.3654]0.0664[0.0630, 0.0697]0.1229[0.1196, 0.1261]WCR 0.3504 [0.3436, 0.3565]0.0644[0.0614, 0.0675]0.1171[0.1138, 0.1202]LCR* 0.3513 [0.3449, 0.3572]0.0645[0.0613, 0.0676]0.1177[0.1145, 0.1209]Co-FRank+0.3769[0.3712, 0.3829]0.0762[0.0724, 0.0799]0.1317[0.1282, 0.1351]LCR**0.3753[0.3692, 0.3813]0.0757[0.0719, 0.0796]0.1302[0.1265, 0.1340]Co-FRank++0.3783[0.3719, 0.3852]0.0775[0.0733, 0.0810]0.1323[0.1293, 0.1360]* The worst results produced by LCR when O  = 0.1+ The worst results produced by Co-FRank when K  = 0.6** The best results produced by LCR when O  = 0.4++ The best results produced by Co-FRank when K  = 0.8Table 1 Compare different ranking strategiesNote that the improvement of LCR over QRRand WCR is rather significant if the combinationparameter O  is selected appropriately.
Besides,Co-FRank is always superior to LCR regardlessof the best or the worst ouput, and theimprovement is visible.
The reason is that bothQRR and WCR are enhanced step by step in Co-FRank, which in turn results in the increasedoverall performance.
The trend of theimprovement has been clearly observed in theexperiments.
This observation validates ourmotivation and the rationality of the algorithmproposed in this paper and motivates our furtherinvestigation on this topic.We continue to examine the parameter settingsin LCR and Co-FRank.
Table 2 shows the resultsof LCR when the value of O  changes from 0.1 to1191.0, and Table 3 shows the results of Co-FRankwith K  ranging from 0.5 to 0.9.
Notice that K  isnot a combination parameter.
We believe that abase ranker should have at least half belief in itsinitial ranking results and thus the value of the Kshould be greater than 0.5.
We find that LCRheavily depends on O .
LCR produces relativelygood and stable results with O  varying from 0.4to 0.6.
However, the ROUGE scores dropapparently when O  heading towards its two endvalues, i.e.
0.1 and 1.0.O  ROUGE-1 ROUGE-2 ROUGE-SU40.1 0.3513 [0.3449, 0.3572]0.0645[0.0613, 0.0676]0.1177[0.1145, 0.1209]0.2 0.3623 [0.3559, 0.3685]0.0699[0.0662, 0.0736]0.1235[0.1197, 0.1271]0.3 0.3721 [0.3660, 0.3778]0.0741[0.0706, 0.0778]0.1281[0.1246, 0.1318]0.4 0.3753 [0.3692, 0.3813]0.0757[0.0719, 0.0796]0.1302[0.1265, 0.1340]0.5 0.3756 [0.3698, 0.3814]0.0755[0.0717, 0.0793]0.1307[0.1272, 0.1342]0.6 0.3770 [0.3710, 0.3826]0.0754[0.0716, 0.0791]0.1323[0.1286, 0.1357]0.7 0.3698 [0.3636, 0.3759]0.0718[0.0680, 0.0756]0.1284[0.1246, 0.1318]0.8 0.3672 [0.3613, 0.3730]0.0706[0.0669, 0.0743]0.1271[0.1234, 0.1305]0.9 0.3651 [0.3591, 0.3708]0.0689[0.0652, 0.0726]0.1258[0.1220, 0.1293]Table 2 LCR with different O  valuesAs shown in Table 3, the Co-FRank canalways produce stable and promising resultsregardless of the change of K .
More important,even the worst result produced by Co-FRank stilloutperforms the best result produced by LCR.K  ROUGE-1 ROUGE-2 ROUGE-SU40.5 0.3750 [0.3687, 0.3810]0.0766[0.0727, 0.0804]0.1308[0.1270, 0.1344]0.6 0.3769 [0.3712, 0.3829]0.0762[0.0724, 0.0799]0.1317[0.1282, 0.1351]0.7 0.3775 [0.3713, 0.3835]0.0763[0.0724, 0.0801]0.1319[0.1282, 0.1354]0.8 0.3783 [0.3719, 0.3852]0.0775[0.0733, 0.0810]0.1323[0.1293, 0.1360]0.9 0.3779 [0.3722, 0.3835]0.0765[0.0728, 0.0803]0.1319[0.1285, 0.1354Table 3 Co-FRank with different K  valuesWe then compare our results to the DUCparticipating systems.
We present the followingrepresentative ROUGE results of (1) the topthree DUC participating systems according toROUGE-2 scores (S15, S17 and S10); and (2)the NIST baseline which simply selects the firstsentences from the documents.ROUGE-1 ROUGE-2 ROUGE-SU4Co-FRank 0.3783 0.0775 0.1323S15 - 0.0725 0.1316S17 - 0.0717 0.1297S10 - 0.0698 0.1253Baseline   0.0403 0.0872Table 4 Compare with DUC participating systemsIt is clearly shown in Table 4 that Co-FRankcan produce a very competitive result, whichsignificantly outperforms the NIST baseline andmeanwhile it is superior to the best participatingsystem in the DUC 2005.4 Conclusion and Future WorkIn this paper, we propose a novel rankingframework, namely Co-Feedback Ranking (Co-FRank), and examine its effectiveness in query-focused summarization.
There is still a lot ofwork to be done on this topic.
Although we showthe promising achievements of Co-Frank fromthe perspective of experimental studies, weexpect a more theoretical analysis on Co-FRank.Meanwhile, we would like to investigate moreappropriate techniques to use feedback, and weare interested in applying Co-FRank to the otherapplications, such as opinion summarizationwhere the integration of opinion-biased anddocument-biased ranking is necessary.AcknowledgmentsThe work described in this paper was supportedby the Hong Kong Polytechnic Universityinternal the grants (G-YG80 and G-YH53) andthe China NSF grant (60703008).ReferencesAvrim Blum and Tom Mitchell.
1998.
CombiningLabeled and Unlabeled Data with Co-Training.
InProceedings of the Eleventh Annual Conference onComputational Learning Theory, pp92-100.Chin-Yew Lin and Eduard Hovy.
2000.
TheAutomated Acquisition of Topic Signature for TextSummarization.
In Proceedings of COLING,pp495-501.Chin-Yew Lin and Eduard Hovy.
2003.
AutomaticEvaluation of Summaries Using N-gram Co-occurrence Statistics.
In Proceedings of HLT-NAACL, pp71-78.Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,and Daniel Tam.
2004.
Centroid-basedSummarization of Multiple Documents.Information Processing and Management, 40:919-938.Paul Over, Hoa Dang and Donna Harman.
2007.
DUCin Context.
Information Processing andManagement, 43(6):1506-1520.120
