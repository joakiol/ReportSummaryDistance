Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 372?381,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsMulti-view Response Selection for Human-Computer ConversationXiangyang Zhou1?, Daxiang Dong1?, Hua Wu1, Shiqi Zhao1,Dianhai Yu1,2, Hao Tian1,2, Xuan Liu1 and Rui Yan11Baidu Inc., Beijing, China2School of Information Science and Technology,University of Science and Technology of China{zhouxiangyang, dongdaxiang, wu hua, zhaoshiqi,yudianhai, tianhao, liuxuan, yanrui}@baidu.comAbstractIn this paper, we study the task of responseselection for multi-turn human-computer con-versation.
Previous approaches take word asa unit and view context and response as se-quences of words.
This kind of approachesdo not explicitly take each utterance as aunit, therefore it is difficult to catch utterance-level discourse information and dependencies.In this paper, we propose a multi-view re-sponse selection model that integrates infor-mation from two different views, i.e., wordsequence view and utterance sequence view.We jointly model the two views via deep neu-ral networks.
Experimental results on a publiccorpus for context-sensitive response selectiondemonstrate the effectiveness of the proposedmulti-view model, which significantly outper-forms other single-view baselines.1 IntroductionSelecting a potential response from a set of can-didates is an important and challenging task foropen-domain human-computer conversation, espe-cially for the retrieval-based human-computer con-versation.
In general, a set of candidate responsesfrom the indexed conversation corpus are retrieved,and then the best one is selected from the candidatesas the system?s response (Ji et al, 2014).Previous Deep Neural Network (DNN) based ap-proaches to response selection represent context andresponse as two embeddings.
The response is se-lected based on the similarity of these two embed-dings (Lowe et al, 2015; Kadlec et al, 2015).
In?These two authors contributed equallythese work, context and response are taken as twoseparate word sequences without considering the re-lationship among utterances in the context and re-sponse.
The response selection in these models islargely influenced by word-level information.
Wecalled this kind of models as word sequence modelin this paper.
Besides word-level dependencies,utterance-level semantic and discourse informationare also very important to catch the conversation top-ics to ensure coherence (Grosz and Sidner, 1986).For example an utterance can be an affirmation,negation or deduction to the previous utterances,or starts a new topic for discussion.
This kind ofutterance-level information is generally ignored inword sequence model, which may be helpful for se-lecting the next response.
Therefore, it is necessaryto take each utterance as a unit and model the contextand response from the view of utterance sequence.This paper proposes a multi-view response selec-tion model, which integrates information from bothword sequence view and utterance sequence view.Our assumption is that each view can represent rela-tionships between context and response from a par-ticular aspect, and features extracted from the wordsequence and the utterance sequence provide com-plementary information for response selection.
Aneffective integration of these two views is expectedto improve the model performance.
To the best ofour knowledge, this is the first work to improve theresponse selection for multi-turn human-computerconversation in a multi-view manner.We evaluate the performance of the multi-view re-sponse selection model on a public corpus contain-ing about one million context-response-label triples.372This corpus was extracted from an online chattingroom for Ubuntu troubleshooting, which is calledthe Ubuntu Corpus in this paper (Lowe et al, 2015).Experimental results show that the proposed multi-view response selection model significantly outper-forms the current best single-view models for multi-turn human-computer conversation.The rest of this paper is organized as follows.
InSection 2, we briefly introduce related works.
Thenwe move on to a detailed description of our modelin Section 3.
Experimental results are described inSection 4.
Analysis of our models is shown in Sec-tion 5.
We conclude the paper in Section 6.2 Related Work2.1 Conversation SystemEstablishing a machine that can interact with hu-man beings via natural language is one of the mostchallenging problems in Artificial Intelligent (AI).Early studies of conversation models are generallydesigned for specific domain, like booking restau-rant, and require numerous domain knowledge aswell as human efforts in model design and featureengineering (Walker et al, 2001).
Hence it is toocostly to adapt those models to other domains.
Re-cently leveraging ?big dialogs?
for open domainconversation draws increasing research attentions.One critical issue for open domain conversation isto produce a reasonable response.
Responding tothis challenge, two promising solutions have beenproposed: 1) retrieval-based model which selects aresponse from a large corpus (Ji et al, 2014; Yan etal., 2016; Yan et al, ).
2) generation-based modelwhich directly generates the next utterance (Wen etal., 2015a; Wen et al, 2015b).2.2 Response SelectionResearch on response selection for human-computerconversation can be classified into two branches,i.e., single-turn and multi-turn response selection.Single-turn models only leverage the last utterancein the context for selecting resposne and most ofthem take the word sequence view.
Lu and Li(2013) proposed a DNN-based matching model forresponse selection.
Hu et al, (2014) improved theperformance using Convolutional Neural Networks(CNN) (LeCun et al, 1989).
In 2015, a furtherstudy conducted by Wang et al (2015a) achievedbetter results using tree structures as the input of aDNN model.
Nevertheless, those models built forsingle-turn response selection ignore the whole con-text information, which makes it difficult to be im-plemented in the multi-turn response selection tasks.On the other hand, research on multi-turn re-sponse selection usually takes the whole context intoconsideration and views the context and responseas word sequences.
Lowe et al, (2015) proposed aLong Short-Term Memory (LSTM) (Hochreiter andSchmidhuber, 1997) based response selection modelfor multi-turn conversation, where words from con-text and response are modeled with LSTM.
The se-lection of a response is based on the similarity ofembeddings between the context and response.
Sim-ilar to the work of Lowe et al, Kadlec et al, (2015)replaced LSTM with Temporal Convolutional Neu-ral Networks (TCNN) (Kim, 2014) and Bidirect-LSTM.
Their experimental results show that mod-els with LSTM perform better than other neural net-works.
However, the utterance-level discourse infor-mation and dependencies have been left out in thesestudies since they view the context and response asword sequences.2.3 Response GenerationAnother line of related research focuses on gener-ating responses for human-computer conversation.Ritter et al, (2011) trained a phrase-based statisti-cal machine translation model on a corpus of ut-terance pairs extracted from Twitter human-humanconversation and used it as a response generator forsingle-turn conversation.
Vinyals and Le (2015) re-garded single-turn conversation as a sequence-to-sequence problem and proposed an encoder-decoderbased response generation model, where the post re-sponse is first encoded using LSTM and its embed-ding used as the initialization state of another LSTMto generate the response.
Shang et al, (2015) im-proved the encoder-decoder based model using at-tention signals.
Sordoni et al, (2015) proposed acontext-sensitive response generation model, wherethe context is represented by bag-of-words and fedinto a recurrent language model to generate the nextresponse.In this paper, we focused on the task of responseselection.373?/?/&" )"01 02 0341 015 01 02 06 07 0803!
!  '&    	!&!$!
 !$$!&$ "#$!
!  '& 	!&!$!
 !$$!&"!
! 91 "!
! 92 "!
! 96    )("?
?;!"
< = 1 &, ) = $	(+" + ).
.
.
.
.
.A1 A2 A15 A1 A8!%!
&?341 ?3"!&Figure 1: Word sequence model for response selection3 Response Selection ModelIn the task of response selection, a conventionalDNN-based architecture represents the context andresponse as low dimensional embeddings with deeplearning models.
The response is selected based onthe similarity of these two embeddings.
We formu-late it asp(y = 1|c, r) = ?(?
?c TW?
?r + b) (1)where c and r denote the context and response,?
?c and ?
?r are their embeddings constructed withDNNs.
?
(x) is a sigmoid function defined as?
(x) = 11+e?x .
p(y = 1|c, r) is the confidence ofselecting response r for context c. The matrix Wand the scalar b are metric parameters to be learnedto measure the similarity between the context andresponse.We extend this architecture in a multi-view man-ner, which jointly models the context and responsein two views.
In this section, we first briefly describethe word sequence model.
Then we introduce theutterance sequence model and multi-view responseselection model in details.3.1 Word Sequence ModelThe word sequence model in this paper is similarto the LSTM-based model proposed in Lowe et al(2015).
As shown in Figure 1, three utterances ofcontext c, written as u1, u2 and u3, are connectedas a sequence of words.
A special word sosis inserted between every two adjacent utterances,denoting the boundary between utterances.
Giventhe word sequences of context and response, wordsare mapped into word embeddings through a sharedlookup table.
A Gated Recurrent Unit neural net-work (GRU) (Chung et al, 2014) is employed toconstruct the context embedding and response em-bedding.
It operates recurrently on the two wordembedding sequences as Equation 2 to Equation 5,where ht?1 is the hidden state of GRU when it readsa word embedding et?1 of word wt?1, h0 is a zerovector as the initiation state, zt is an update gate andrt is a reset gate.
The new hidden state ht for em-bedding et is a combination of the previous hiddenstate ht?1 and the input embedding et, controlledby the update gate zt and reset gate rt.
U , Uz , Ur,W , Wz and Wr are model parameters of GRU to belearned.
?
denotes element-wise multiplication.374ssossotemporal convolutional layer with padding size 1max pooling layerword embedding layerword-level gated recurrent unit layerutterance-level gated recurrent unit layerword sequence viewutterance sequence view??
= ?(???????
+ ??)??
= ?(???????
+ ??)??
????
?
?utterance ??
utterance ??
utterance ??
response ?Utteranceembedding ???
???
??
?Responseembedding ?
?Figure 2: Multi-view response selection modelht = (1?
zt)?
ht?1 + zt ?
h?t (2)zt = ?
(Wzet + Uzht?1) (3)h?t = tanh(Wet + U(rt ?
ht?1)) (4)rt = ?
(Wret + Urht?1) (5)After reading the whole word embedding sequence,word-level semantic and dependencies in the wholesequence are encoded in the hidden state of GRU,which represents the meaning of the whole sequence(Karpathy et al, 2015).
Therefore we use thelast hidden state of GRU as the context embeddingand response embedding in word sequence model,named ?
?cw and ?
?rw respectively1.
The confidence ofselecting response in word sequence model is thencalculated as in Equation 6:pw(y = 1|c, r) = ?(??cwTWw?
?rw + bw) (6)where Ww and bw are metric parameters to betrained in word sequence model.
?
?cw and ?
?rw are con-structed by a same GRU in word sequence model.1We use two subscripts, i.e., w and u, to distinguish notationin the two views.3.2 Utterance Sequence ModelUtterance sequence model regards the context as ahierarchical structure, where the response and eachutterance are first represented based on word embed-dings, then the context embedding is constructed forthe confidence calculation of response selection.
Asthe lower part of Figure 2 illustrates, the construc-tion of the utterance embedding and response em-bedding is in a convolutional manner, which con-tains the following layers:Padding Layer: Given a word embeddingsequence belonging to a certain utter-ance (response), namely [e1, ..., em], thepadding layer makes its outer border withbn/2c zero vectors, the padded sequence is[01, .., 0bn/2c, e1, ..., em, 01, .., 0bn/2c], wheren is the size of convolution window used intemporal convolutional layer.Temporal Convolutional Layer: Temporal convo-lutional layer reads the padded word embed-ding sequence through a sliding convolutionwindow with size n. For every step that thesliding window moves, a region vector is pro-duced by concatenating the word embeddingswithin the sliding window, denoted as [ei?...
?375ei+n?1] ?
Rn|e|, where ?
denotes the concate-nation of embeddings, |e| is the size of wordembedding.
The temporal convolutional layerconsists of k kernels, each of which impliesa certain dimension and maps the region vec-tor to a value in its dimension by convolutionoperation.
The convolution result of each ker-nel, termed convi, is further activated with theRELU non-linear activation function (Xu et al,2015), which is formulated as:frelu(convi) = max(convi, 0) (7)Pooling Layer: Because utterance and response arenaturally variable-sized, we put a max-over-time pooling layer on the top of temporal con-volutional layer (Kim, 2014), which extractsthe max value for each kernel, and gets a fix-sized representation of length k for utteranceand response.In particular, representations constructed by CNNwith max-pooling reflect the core meanings of ut-terance and response.
The embeddings of utter-ance ui and response r in utterance sequence vieware referred to as ?
?uiu and ??ru.
Utterance embed-dings are connected in the sequence and fed into aGRU, which captures utterance-level semantic anddiscourse information in the whole context and en-codes those information as context embedding, writ-ten as ?
?cu .
The confidence of selecting response rfor context c in utterance sequence model, namedpu(y = 1|c, r), is calculated using Equation 8:pu(y = 1|c, r) = ?(??cuTWu?
?ru + bu) (8)It is worth noticing that the TCNN used here isshared in constructing the utterance embedding andresponse embedding.
The word embeddings arealso shared for both the context and response.
Thesos tag in word sequence view is not used in theutterance sequence model.3.3 Multi-view ModelOrganic integration of different views has beenproven to be very effective in the field of recommen-dation, representation learning and other researchareas (Elkahky et al, 2015; Wang et al, 2015b).Most existing multi-view models integrate differ-ent views via a linear/nonlinear combination.
Re-searchers have demonstrated that jointly minimizingtwo factors, i.e., 1) the training error of each viewand 2) the disagreement between complementaryviews can significantly improve the performance ofthe combination of multi-views (Xu et al, 2013).Our multi-view response selection model is de-signed as shown in Figure 2.
As we can see, thecontext and response are jointly represented as se-mantic embeddings in these two views.
The under-lying word embeddings are shared across the con-text and response in these two views.
The com-plementary information of these two views is ex-changed via the shared word embeddings.
The ut-terance embeddings are modeled through a TCNNin the utterance sequence view.
Two independentGated Recurrent Units are used to model the wordembeddings and utterance embeddings separately onword sequence view and utterance sequence view,the former of which captures dependencies in wordlevel and the latter captures utterance-level semanticand discourse information.
Confidences for select-ing the response in these two views are calculatedseparately.
We optimize the multi-view model byminimizing the following loss:L = LD + LL +?2 ???
(9)LD =?i(pw(li)p?u(li) + pu(li)p?w(li)) (10)LL =?i(1?
pw(li)) +?i(1?
pu(li)) (11)where the object function of the multi-view model Lis comprised of the disagreement loss LD, the like-lihood loss LL and the regular term ?2???.
pw(li) =pw(y = li|c, r) and pu(li) = pu(y = li|c, r) de-note the likelihood of the i-th instance with label lifrom training set in these two views.
Only two la-bels, {0, 1}, denote the correctness of the responseduring training.
p?w(li) and p?u(li) denote the proba-bility pw(y 6= li) and pu(y 6= li) respectively.
Themulti-view model is trained to jointly minimize thedisagreement loss and the likelihood loss.
?
denotesall the parameters of the multi-view model.The unweighted summation of confidences fromthese two views is used during prediction, defined as376Model/Metrics 1 in 10 R@1 1 in 10 R@2 1in 10 R@5 1 in 2 R@1Random-guess 10% 20% 50% 50%TF-IDF 41.0% 54.5% 70.8% 65.9%Word-seq-LSTM (Lowe et al, 2015) 60.40% 74.50% 92.60% 87.80%Word-seq-GRU 60.85% 75.71% 93.13% 88.55%Utter-seq-GRU 62.19% 76.56% 93.42% 88.83%Multi-view 66.15% 80.12% 95.09% 90.80%Table 1: Performance comparison between our models and baseline models.
In the table, Word-seq-LSTM is the experimentresult of the LSTM-based word sequence model reported by Lowe et at (2015).
Word-seq GRU is the word sequence model thatwe implement with GRU.
Utter-seq-GRU is the proposed utterance-sequence model.
The Multi-view is our multi-view responseselection model.
In addition, we list the performance of Random-guess and TF-IDFin Equation 12:smtv(y = 1|c, r) =pw(y = 1|c, r) + pu(y = 1|c, r)(12)The response with larger smtv(y = 1|c, r) is morelikely to be selected.
We will investigate other com-bination models in our future work.4 Experiment4.1 DatasetOur model is evaluated on the public Ubuntu Cor-pus (Lowe et al, 2015), designed for response selec-tion study of multi-turn human-computer conversa-tion (Serban et al, 2015).
The dataset contains 0.93million human-human dialogues crawled from anInternet chatting room for Ubuntu trouble shooting.Around 1 million context-response-labeled triples,namely < c, r, l >, are generated for training af-ter preprocessing2, where the original context andthe corresponding response are taken as the positiveinstances while the random utterances in the dataset taken as the negative instances, and the numberof positive instance and negative instance in train-ing set is balanced.
The validation set and testingset are constructed in a similar way to the trainingset, with one notable difference that for each contextand the corresponding positive response, 9 negativeresponses are randomly selected for further evalua-tion.4.2 Experiment SetupFollowing the work of Lowe et al, (2015), the eval-uation metric is 1 in m Recall@k (denoted 1 in m2Preprocessing includes tokenization, recognition of namedentity, urls and numbers.R@k), where a response selection model is designedto select k most likely responses among m candi-dates, and it gets the score ?1?
if the correct responseis in the k selected ones.
This metric can be seenas an adaptation of the precision and recall metricspreviously applied to dialogue datasets (Schatzmannet al, 2005).
It is worth noticing that 1 in 2 R@1equals to precision and recall in binary classifica-tion.4.3 Model Training and Hyper-parametersWe initialize word embeddings with a pre-trainedembedding matrix through GloVe (Pennington et al,2014) 3.
We use Stochastic Gradient Descent (SGD)for optimizing.
Hidden size for a gated recurrentunit is set to 200 in both word sequence model andutterance sequence model.
The number of convo-lutional kernels is set to 200.
Our initial learningrate is 0.01 with mini-batch size of 32.
Other hyper-parameters are set exactly the same as the baseline.We train our models with a single machine using12 threads and each model will converge after 4-5epochs of training data.
The best model is selectedwith a holdout validation dataset.4.4 Comparison ApproachesWe consider the word sequence model implementedby Lowe et at., (2015) with LSTM as our base-line, the best model in context-sensitive responseselection so far.
Moreover, we also implement theword sequence model and the utterance sequencemodel with GRU for further analysis.
Two simpleapproaches are also implemented, i.e., the Random-3Initialization of word embedding can be obtained onhttps://github.com/npow/ubottu377(a) 1 in 2 R@1 (b) 1 in 10 R@1(c) 1 in 10 R@2 (d) 1 in 10 R@5Figure 3: Performance comparison between word sequence model (with/without sos tags) and utterance sequence model.
Wechoose the number of utterances in range of [2,6], since most samples in testset fall in this intervalguess and the TF-IDF, as the bottom line for perfor-mance comparison.
The performance of Random-guess is calculated by mathematics with an assump-tion that each response in candidates has the equalprobability to be selected.
The TF-IDF is imple-mented in the same way in Lowe et al, (2015).
TFfor a word is calculated as the count of times itappears in a certain context or response.
IDF foreach word w is log( N|d?D:w?d|), where D denotesthe whole training set, N is the size of D, d is aconversation in D. The context and the response intestset are represented as a bag-of-words accordingto TF-IDF.
The selection confidence is estimated asthe cosine score between context and response.4.5 Experimental ResultWe summarize the experiment result in Table 1.
Asshown in Table 1, all DNN-based models achievesignificant improvements compared to Random-guess and TF-IDF, which implies the effectivenessof DNN models in the task of response selection.The word sequence models implemented with GRUand LSTM achieve similar performance.
The ut-terance sequence model significantly outperformsword sequence models for 1 in 10 R@1.
Multi-view model significantly outperforms all the othermodels, especially for 1 in 10 R@1, which is moredifficult and closer to the real world scenario thanother metrics.
The experimental result demonstratesthe effectiveness of multi-view model and provesthat word sequence view and utterance sequenceview can bring complementary information for eachother.5 AnalysisWe examine the complementarity between word se-quence model and utterance sequence model in twofolds, i.e., via statistic analysis and case study.5.1 Statistical AnalysisWe compare the performance of word sequencemodel4 and utterance sequence model for differentnumber of utterances in the contexts.
In addition,we also examine what the contribution sos tagmakes in word sequence view.
The performance4The GRU-based word sequence model that we imple-mented is used for comparison.378User(utterance) Word Sequence View Utterance Sequence ViewWildintellect:(Utterance-1)anyone know where to find a list of all language codes a locales with each ?itaylor57:(Utterance-2)  __url__  __url__Wildintellect:(Utterance-3)thanks but that list seems incomplete thanks but that list seems incompleteitaylor57:(Utterance-4) __url__ __url__Selected Responsei already looked at that one , also incomplete , lacks the locales within a language groupdoes it work ?User(utterance) Word Sequence View Utterance Sequence Viewastra-x:(Utterance-1)alright so has anyone solved an error with __path__ ext4 leaking ?alright so has anyone solved an error with __path__ ext4 leaking ?sipior:(Utterance-2) what sort of error ?
what sort of error ?astra-x:(Utterance-3)my reported free disk space says full , yet last week it was 60g free on __path__ , and i cannot find anymore than 29g of files.
yet __path__ and __path__ are reported correctlymy reported free disk space says full , yet last week it was 60g free on __path__ , and i cannot find anymore than 29g of files.
yet __path__ and __path__ are reported correctlysipior:(Utterance-4)how are you getting the disk space information ?how are you getting the disk space information ?Selected Response__path__ should be 10g and __path__ should be 19g want me to pastebin all my debugging ?anyone know where to find a list of all language codes a locales with each ?Figure 4: Case studies for analysis of word sequence model and utterance sequence model.
The context and the selected responsesare collected from testset.
Response with a green checkmark means it is a correct one, otherwise it is incorrect.
Words (Utterances)in bold are the important elements recognized by our importance analysis approach.
The yellow start denotes the selection ofmulti-view model.is shown in Figure 3.
We can see that as thenumber of turns increases, the utterance sequencemodel outperforms word sequence model more sig-nificantly, which implies that utterance sequencemodel can provide complementary information toword sequence model for a long context.
Further-more, word sequence model without sos tag hasan obvious fall in performance compared with wordsequence model with sos , which implies its cru-cial role in distinguishing utterances for modelingcontext.5.2 Case StudyWe analyze samples from testset to examine thecomplementarity between these two views.
The keywords for word sequence model and core utterancesfor utterance sequence model are extracted for anal-ysis.
These important elements are recognized basedon the work of Li et al (2015), where the gradi-ents of their embeddings are used for importanceanalysis.
After studying the testset, we find that theword sequence model selects responses according tothe matching of key words while the utterance se-quence model selects responses based on the match-ing of core utterances.
We list two cases in Figure4 as examples.As it shows, the word sequence model prefers toselect the response that shares similar key words tothe context, such as the words ?incomplete?
and?locales?
in example 1 or ?60g?
and ?19g?
in ex-ample 2.
Although key word matching is a use-ful feature in selecting response for cases such asexample 1, it fails in cases like example 2, whereincorrect response happens to share similar wordswith the context.
Utterance sequence model, on theother side, leverages core utterances for selecting re-sponse.
As shown in example 2, utterance-1 andutterance-2 are recognized as the core utterances, themain topic of the two utterance is ?solved?
and ?er-ror?, which is close to the topic of the correct re-379sponse.
However, for cases like example 1, wherethe core meaning of correct response is jointly com-bined with different words in different utterances,the utterance sequence model does not perform well.The multi-view model can successfully select thecorrect responses in both two examples, which im-plies its ability to jointly leverage information fromthese two views.6 ConclusionIn this paper, we propose a multi-view response se-lection model for multi-turn human-computer con-versation.
We integrate the existing word sequenceview and a new view, i.e., utterance sequence view,into a unified multi-view model.
In the view of utter-ance sequence, discourse information can be learntthrough utterance-level recurrent neural network,different from word sequence view.
The represen-tations learnt from the two views provide comple-mentary information for each other in the task of re-sponse selection.
Experiments show that our multi-view model significantly outperforms the state-of-the-art word sequence view models.
We will extendour framework to response generation approaches inour future work.
We believe it will help constructa better representation of context in the encodingphrase of DNN-based generation model and thus im-prove the performance.AcknowledgementThis paper is supported by National Basic Re-search Program of China (973 program No.2014CB340505).
We gratefully thank the anony-mous reviewers for their insightful comments.ReferencesJunyoung Chung, C?aglar Gu?lc?ehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.
CoRR, abs/1412.3555.Ali Mamdouh Elkahky, Yang Song, and Xiaodong He.2015.
A multi-view deep learning approach for crossdomain user modeling in recommendation systems.
InProceedings of the 24th International Conference onWorld Wide Web, pages 278?288.
International WorldWide Web Conferences Steering Committee.Barbara J Grosz and Candace L Sidner.
1986.
Attention,intentions, and the structure of discourse.
Computa-tional linguistics, 12(3):175?204.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.2014.
Convolutional neural network architectures formatching natural language sentences.
In Advances inNeural Information Processing Systems, pages 2042?2050.Zongcheng Ji, Zhengdong Lu, and Hang Li.
2014.
Aninformation retrieval approach to short text conversa-tion.
arXiv preprint arXiv:1408.6988.Rudolf Kadlec, Martin Schmid, and Jan Kleindienst.2015.
Improved deep learning baselines for ubuntucorpus dialogs.
arXiv preprint arXiv:1510.03753.Andrej Karpathy, Justin Johnson, and Fei-Fei Li.
2015.Visualizing and understanding recurrent networks.arXiv preprint arXiv:1506.02078.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
arXiv preprint arXiv:1408.5882.Yann LeCun, Bernhard Boser, John S Denker, DonnieHenderson, Richard E Howard, Wayne Hubbard, andLawrence D Jackel.
1989.
Backpropagation appliedto handwritten zip code recognition.
Neural computa-tion, 1(4):541?551.Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.2015.
Visualizing and understanding neural models innlp.
arXiv preprint arXiv:1506.01066.Ryan Lowe, Nissan Pow, Iulian Serban, and JoellePineau.
2015.
The ubuntu dialogue corpus: A largedataset for research in unstructured multi-turn dia-logue systems.
arXiv preprint arXiv:1506.08909.Zhengdong Lu and Hang Li.
2013.
A deep architecturefor matching short texts.
In Advances in Neural Infor-mation Processing Systems, pages 1367?1375.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for word rep-resentation.
In In Proc.
EMNLP, pages 1532?1543.Alan Ritter, Colin Cherry, and William B Dolan.
2011.Data-driven response generation in social media.
In InProc.
EMNLP, pages 583?593.
Association for Com-putational Linguistics.Jost Schatzmann, Kallirroi Georgila, and Steve Young.2005.
Quantitative evaluation of user simulation tech-niques for spoken dialogue systems.
In 6th SIGdialWorkshop on DISCOURSE and DIALOGUE.Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, andJoelle Pineau.
2015.
A survey of available corpora forbuilding data-driven dialogue systems.
arXiv preprintarXiv:1512.05742.380Lifeng Shang, Zhengdong Lu, and Hang Li.
2015.
Neu-ral responding machine for short-text conversation.arXiv preprint arXiv:1503.02364.Alessandro Sordoni, Michel Galley, Michael Auli, ChrisBrockett, Yangfeng Ji, Margaret Mitchell, Jian-YunNie, Jianfeng Gao, and Bill Dolan.
2015.
Aneural network approach to context-sensitive gener-ation of conversational responses.
arXiv preprintarXiv:1506.06714.Oriol Vinyals and Quoc Le.
2015.
A neural conversa-tional model.
arXiv preprint arXiv:1506.05869.Marilyn A Walker, Rebecca Passonneau, and Julie EBoland.
2001.
Quantitative and qualitative evalua-tion of darpa communicator spoken dialogue systems.In Proceedings of the 39th Annual Meeting on Associ-ation for Computational Linguistics, pages 515?522.Association for Computational Linguistics.Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu.2015a.
Syntax-based deep matching of short texts.arXiv preprint arXiv:1503.02427.Weiran Wang, Raman Arora, Karen Livescu, and JeffBilmes.
2015b.
On deep multi-view representationlearning.
In Proceedings of the 32nd InternationalConference on Machine Learning (ICML-15), pages1083?1092.Tsung-Hsien Wen, Milica Gas?ic?, Dongho Kim, NikolaMrks?ic?, Pei-Hao Su, David Vandyke, and SteveYoung.
2015a.
Stochastic Language Generation inDialogue using Recurrent Neural Networks with Con-volutional Sentence Reranking.
In Proceedings of the16th Annual Meeting of the Special Interest Group onDiscourse and Dialogue (SIGDIAL).
Association forComputational Linguistics, September.Tsung-Hsien Wen, Milica Gas?ic?, Nikola Mrks?ic?, Pei-HaoSu, David Vandyke, and Steve Young.
2015b.
Seman-tically conditioned lstm-based natural language gen-eration for spoken dialogue systems.
In Proceedingsof the 2015 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP).
Association forComputational Linguistics, September.Jason D Williams and Steve Young.
2007.
Partially ob-servable markov decision processes for spoken dialogsystems.
Computer Speech & Language, 21(2):393?422.Chang Xu, Dacheng Tao, and Chao Xu.
2013.A survey on multi-view learning.
arXiv preprintarXiv:1304.5634.Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li.
2015.Empirical evaluation of rectified activations in convo-lutional network.
arXiv preprint arXiv:1505.00853.Zhao Yan, Nan Duan, Junwei Bao, Peng Chen, MingZhou, Zhoujun Li, and Jianshe Zhou.
Docchat: Aninformation retrieval approach for chatbot engines us-ing unstructured documents.Rui Yan, Yiping Song, and Hua Wu.
2016.
Learning torespond with deep neural networks for retrieval-basedhuman-computer conversation system.
In Proceedingsof the 39th International ACM SIGIR conference onResearch and Development in Information Retrieval,pages 55?64.
ACM.381
