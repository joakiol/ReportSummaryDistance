Proceedings of the 7th Workshop on Statistical Machine Translation, pages 64?70,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsTerrorCat: a Translation Error Categorization-based MT Quality MetricMark Fishel,?
Rico Sennrich,?
Maja Popovic?,?
Ondr?ej Bojar??
Institute of Computational Linguistics, University of Zurich{fishel,sennrich}@cl.uzh.ch?
German Research Center for Artificial Intelligence (DFKI), Berlinmaja.popovic@dfki.de?
Charles University in Prague, Faculty of Mathematics and Physics,Institute of Formal and Applied Linguisticsbojar@ufal.mff.cuni.czAbstractWe present TerrorCat, a submission to theWMT?12 metrics shared task.
TerrorCat usesfrequencies of automatically obtained transla-tion error categories as base for pairwise com-parison of translation hypotheses, which is inturn used to generate a score for every trans-lation.
The metric shows high overall corre-lation with human judgements on the systemlevel and more modest results on the level ofindividual sentences.1 The IdeaRecently a couple of methods of automatic trans-lation error analysis have emerged (Zeman et al,2011; Popovic?
and Ney, 2011).
Initial experimentshave shown that while agreement with human erroranalysis is low, these methods show better perfor-mance on tasks with a lower granularity, e.g.
rankingerror categories by frequency (Fishel et al, 2012).In this work we apply translation error analysis to atask with an even lower granularity: ranking transla-tions, one of the shared tasks of WMT?12.The aim of translation error analysis is to identifythe errors that translation systems make and catego-rize them into different types: e.g.
lexical, reorder-ing, punctuation errors, etc.
The two tools that wewill use ?
Hjerson and Addicter ?
both rely on a ref-erence translation.
The hypothesis translation that isbeing analyzed is first aligned to the reference on theword level, and then mistranslated, misplaced, mis-inflected, missing or superfluous words and other er-rors are identified.The main idea of our work is to quantify trans-lation quality based on the frequencies of differenterror categories.
The basic assumption is that differ-ent error categories have different importance fromthe point of view of overall translation quality: forinstance, it would be natural to assume that punc-tuation errors influence translation quality less thanmissing words or lexical choice errors.
Furthermore,an error category can be more important for one out-put language than the other: for example, word or-der can influence the meaning in an English sentencemore than in a Czech or German one, whereas in-flection errors are probably more frequent in the lat-ter two and can thus cause more damage.In the context of the ranking task, the absolutevalue of a numeric score has no importance, apartfrom being greater than, smaller than or equal to theother systems?
scores.
We therefore start by per-forming pairwise comparison of the translations ?the basic task is to compare two translations and re-port which one is better.
To conform with the WMTsubmission format we need to generate a numericscore as the output ?
which is obtained by compar-ing every possible pair of translations and then usingthe (normalized) total number of wins per translationas its final score.The general architecture of the metric is thus this:?
automatic error analysis is applied to the sys-tem outputs, yielding the frequencies of everyerror category for each sentence?
every possible pair of all system outputs is rep-resented as a vector of features, based on theerror category frequencies64?
a binary classifier takes these feature vectors asinput and assigns a win to one of the sentencesin every pair (apart from ties)?
the final score of a system equals to the normal-ized total number of wins per sentence?
the system-level score is averaged out over theindividual sentence scoresAn illustrative example is given in Figure 1.We call the result TerrorCat, the translation errorcategorization-based metric.2 The DetailsIn this section we will describe the specifics ofthe current implementation of the TerrorCat met-ric: translation error analysis, lemmatization, binaryclassifier and training data for the binary classifier.2.1 Translation Error AnalysisAddicter (Zeman et al, 2011) and Hjerson (Popovic?and Ney, 2011) use different methods for automaticerror analysis.
Addicter explicitly aligns the hy-pothesis and reference translations and induces errorcategories based on the alignment coverage whileHjerson compares words encompassed in the WER(word error rate) and PER (position-independentword error rate) scores to the same end.Previous evaluation of Addicter shows thathypothesis-reference alignment coverage (in termsof discovered word pairs) directly influences er-ror analysis quality; to increase alignment cover-age we used Berkeley aligner (Liang et al, 2006)and trained it on and applied it to the whole set ofreference-hypothesis pairs for every language pair.Both tools use word lemmas for their analysis;we used TreeTagger (Schmid, 1995) for analyzingEnglish, Spanish, German and French and Morc?e(Spoustova?
et al, 2007) to analyze Czech.
The sametools are used for PoS-tagging in some experiments.2.2 Binary ClassificationPairwise comparison of sentence pairs is achievedwith a binary SVM classifier, trained via sequentialminimal optimization (Platt, 1998), implemented inWeka (Hall et al, 2009).The input feature vectors are composed of fre-quency differences of every error category; since theSource: Wir sind Meister!Translations:Reference: We are the champions!HYP-1: Us champions!HYP-2: The champions we are .HYP-3: We are the champignons!Error Frequencies:HYP-1: 1?inflection, 2?missingHYP-2: 2?order, 1?punctuationHYP-3: 1?lex.choiceClassifier Output: (or manually createdinput in the training phase)HYP-1 < HYP-2HYP-1 < HYP-3HYP-2 > HYP-3Scores:HYP-1: 0HYP-2: 1HYP-3: 0.5Figure 1: Illustration of TerrorCat?s process for a singlesentence: translation errors in the hypothesis translationsare discovered by comparing them to the reference, errorfrequencies are extracted, pairwise comparisons are doneby the classifier and then converted to scores.
The showntranslation errors correspond to Hjerson?s output.maximum (normalized) frequency of any error rateis 1, the feature value range is [?1, 1].
To includeerror analysis from both Addicter and Hjerson theirrespective features are used side-by-side.2.3 Data ExtractionTraining data for the SVM classifier is taken fromthe WMT shared task manual ranking evaluationsof previous years (2007?2011), which consist of tu-ples of 2 to 5 ranked sentences for every languagepair.
Equal ranks are allowed, and translations ofthe same sentence by the same pair of systems canbe present in several tuples, possibly having conflict-ing comparison results.To convert the WMT manual ranking data intothe training data for the SVM classifier, we collectall rankings for each pair of translation hypothe-652007-2010 2007-2011fr-en 34 152 46 070de-en 36 792 53 790es-en 30 374 41 966cs-en 19 268 26 418en-fr 22 734 35 854en-de 36 076 56 054en-es 19 352 35 700en-cs 31 728 52 954Table 1: Dataset sizes for every language pair, basedon manual rankings from WMT shared tasks of previ-ous years: the number of pairs with non-conflicting, non-equivalent ranks.ses.
Pairs with equal ranks are discarded, conflictingranks for the same pairs are resolved with voting.
Ifthe voting is tied, the pair is also discarded.The kept translation pairs are mirrored (i.e.
bothdirections of every pair are added to the training setas independent entries) to ensure no bias towards thefirst or second translation in a pair.
We will laterpresent analysis of how well that works.2.4 TerrorCat+YouTerrorCat is distributed via GitHub; information ondownloading and using it can be found online.1 Ad-ditionally we are planning to provide more recentevaluations with new datasets, as well as pre-trainedmodels for various languages and language pairs.3 The ExperimentsIn the experimental part of our work, we search forthe best performing model variant, the aim of whichis to evaluate different input features, score calcula-tion strategies and other alternations.
The search isdone empirically: we evaluate one alternation at atime, and if it successful, it is added to the systembefore proceeding to test further alternations.Performance of the models is estimated on a held-out development set, taken from the WMT?11 data;the training data during the optimization phase iscomposed of ranking data from WMT 2007?2010.In the end we re-trained our system on the wholedata set (WMT 2007?2011) and applied it to the un-1http://terra.cl.uzh.ch/terrorcat.htmllabeled data from this year?s shared task.
The result-ing dataset sizes are given in Table 1.All of the resulting scores obtained by differentvariants of our metric are presented in Tables 2 (forsystem-level correlations) and 3 (for sentence-levelcorrelations), compared to BLEU and other selectedentries in the WMT?11 evaluation shared task.
Cor-relations are computed in the same way as in theWMT evaluations.3.1 Model OptimizationThe following is a brief description of successfulmodifications to the baseline system.Weighted WinsIn the baseline model, the score of the winningsystem in each pairwise comparison is increased by1.
To reduce the impact of low-confidence decisionsof the classifier on the final score we tested replac-ing the constant rewards to the winning system withvariable ones, proportional to the classifier?s confi-dence ?
a measure of which was obtained by fittinga logistic regression model to the SVM output.As the results show, this leads to minor improve-ments in sentence-level correlation and more notice-able improvements in system-level correlation (es-pecially English-French and Czech-English).
A pos-sible explanation for this difference in performanceon different levels is that low classification confi-dence on the sentence-level does not necessarily af-fect our ranking for that sentence, but reduces theimpact of that sentence on the system-level ranking.PoS-Split FeaturesThe original model only makes a difference be-tween individual error categories as produced byHjerson and Addicter.
It seems reasonable to assumethat errors may be more or less important, dependingon the part-of-speech of the words they occur in.
Wetherefore tested using the number of errors per er-ror category per PoS-tag as input features.
In otherwords, unlike the baseline, which relied on countsof missing, misplaced and other erroneous words,this alternation makes a difference between miss-ing nouns/verbs/etc., misplaced nouns, misinflectednouns/adjectives, and so on.The downside of this approach is that the numberof features is multiplied by the size of the PoS tag66Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*TerrorCat:Baseline 0.73 0.74 0.82 0.76 0.76 0.70 0.81 0.69 0.84 0.76Weighted wins 0.73 0.74 0.82 0.79 0.77 0.75 0.81 0.69 0.84 0.77PoS-features 0.87 0.76 0.80 0.86 0.82 0.76 0.86 0.74 0.87 0.81GenPoS-features 0.86 0.77 0.84 0.88 0.84 0.80 0.85 0.75 0.90 0.83No 2007 data (GenPoS) 0.89 0.80 0.80 0.95 0.86 0.85 0.84 0.81 0.90 0.85Other:BLEU 0.85 0.48 0.90 0.88 0.78 0.86 0.44 0.87 0.65 0.70mp4ibm1 0.08 0.56 0.12 0.91 0.42 0.61 0.91 0.71 0.76 0.75MTeRater-Plus 0.93 0.90 0.91 0.95 0.92 ?
?
?
?
?AMBER ti 0.94 0.63 0.85 0.88 0.83 0.84 0.54 0.88 0.56 0.70meteor-1.3-rank 0.93 0.71 0.88 0.91 0.86 0.85 0.30 0.74 0.65 0.63Table 2: System-level Spearman?s rank correlation coefficients (?)
between different variants of TerrorCat and hu-man judgements, based on WMT?11 data.
Other metric submissions are shown for comparison.
Highest scores perlanguage pair are highlighted in bold separately for TerrorCat variants and for other metrics.set.
Additionally, too specific distinctions can causedata sparsity, especially on the sentence level.As shown by the results, PoS-tag splitting of thefeatures is successful on the system level, but quitehurtful to the sentence-level correlations.
The poorperformance on the sentence level can be attributedto the aforementioned data sparsity: the number ofdifferent features is higher than the number of words(and hence, the biggest possible number of errors)in the sentences.
However, we cannot quite ex-plain, how a sum of these less reliable sentence-levelscores leads to more reliable system-level scores.To somewhat relieve data sparsity we defined sub-sets of the original PoS tag sets, mostly leaving outmorphological information and keeping just the gen-eral word types (nouns, verbs, adjectives, etc.).
Thisreduced the number of PoS-tags (and thus, the num-ber of input features) from 2 to 4 times and producedfurther increase in system-level and a smaller de-crease in sentence-level scores, see GenPoS results.To avoid splitting the metric into different ver-sions for system-level and sentence-level, we gavepriority to system-level correlations and adopted thegeneralized PoS-splitting of the features.Out-of-Domain DataThe human ranking data from WMT of previ-ous years do not constitute a completely homo-geneous dataset.
For starters, the test sets aretaken from different domains (News/News Com-mentary/Europarl), whereas the 2012 test set is fromthe News domain only.
Added to this, there might bea difference in the manual data, coming from differ-ent organization of the competition ?
e.g.
WMT?07was the only year when manual scoring of the trans-lations with adequacy/fluency was performed, andranking had just been introduced into the competi-tion.
Therefore we tested whether some subsets ofthe training data can result in better overall scores.Interestingly enough, leaving out News Commen-tary and Europarl test sets caused decreased correla-tions, although these account for just around 10%of the training data.
On the other hand, leaving outthe data from WMT?07 led to a significant gain inoverall performance.3.2 Error Meta-AnalysisTo better understand why sentence-level correlationsare low, we analyzed the core of TerrorCat ?
its pair-wise classifier.
Here, we focus on the most success-ful variant of the metric, which uses general PoS-tags and was trained on the WMT manual rankingsfrom 2008 to 2010.
Table 4 presents the confusionmatrices of the classifier (one for precision and onefor recall), taking into consideration the confidenceestimate.Evaluation is based on the data from 2011; theprediction data was mirrored in the same way as for67Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*TerrorCat:Baseline 0.20 0.22 0.33 0.25 0.25 0.30 0.19 0.24 0.20 0.23Weighted wins 0.20 0.23 0.33 0.25 0.25 0.31 0.20 0.24 0.20 0.24PoS-features 0.13 0.18 0.24 0.15 0.18 0.27 0.15 0.15 0.17 0.19GenPoS-features 0.16 0.24 0.31 0.22 0.23 0.27 0.18 0.22 0.19 0.22No 2007 data (GenPoS) 0.21 0.30 0.33 0.23 0.27 0.29 0.20 0.23 0.20 0.23Other:mp4ibm1 0.15 0.16 0.18 0.12 0.15 0.21 0.13 0.13 0.06 0.13MTeRater-Plus 0.30 0.36 0.45 0.36 0.37 ?
?
?
?
?AMBER ti 0.24 0.26 0.33 0.27 0.28 0.32 0.22 0.31 0.21 0.27meteor-1.3-rank 0.23 0.25 0.38 0.28 0.29 0.31 0.14 0.26 0.19 0.23Table 3: Sentence-level Kendall?s rank correlation coefficients (? )
between different variants of TerrorCat and hu-man judgements, based on WMT?11 data.
Other metric submissions are shown for comparison.
Highest scores perlanguage pair are highlighted in bold separately for TerrorCat variants and for other metrics.the training set.
Our aim was to measure the biasof the classifier towards first or second translationsin a pair (which is obviously an undesired effect).It can be seen that the confusion matrices are com-pletely symmetrical, indicating no position bias ofthe classifier ?
even lower-confidence decisions areabsolutely consistent.To make sure that this can be attributed to the mir-roring of the training set, we re-trained the classifieron non-mirrored training sets.
As a result, 9% of theinstances were labelled inconsistently, with the av-erage confidence of such inconsistent decisions be-ing extremely low (2.1%, compared to the overallaverage of 28.4%).
The resulting correlations haveslightly dropped as well ?
all indicating that mirror-ing the training sets does indeed remove the posi-tional bias and leads to slightly better performance.Looking at the confusion matrices overall, mostdecisions fall within the main diagonals (i.e.
thecells indicating correct decisions of the classifier).Looking strictly at the classifier?s decisions, the re-calls and precisions of the non-tied comparison out-puts (?<?
and ?>?)
are 57% precision, 69% recall.However, such strict estimates are too pessimistic inour case, since the effect of the classifier?s decisionsis proportional to the confidence estimate.
On thesentence level it means that low-confidence decisionerrors have less effect on the total score of a system.A definite source of error is the instability of the in-dividual translation errors on the sentence level, aneffect both Addicter and Hjerson are known to sufferfrom (Fishel et al, 2012).The precision of the classifier predictably dropstogether with the confidence, and almost half of themisclassifications come from unrecognized equiva-lent translations ?
as a result the recall of such pairsof equivalent translations is only 20%.
This can beexplained by the fact that the binary classifier wastrained on instances with just these two labels andwith no ties allowed.On the other hand the classifier?s 0-confidence de-cisions have a high precision (84%) on detecting theequivalent translations; after re-examining the datait turned out that 96% of the 0-confidence decisionswere made on input feature vectors containing onlyzero frequency differences.
Such vectors representpairs of sentences with identical translation erroranalyses, which are very often simply identical sen-tences ?
in which case the classifier cannot (and infact, should not) make an informed decision of onebeing better than the other.4 Related WorkTraditional MT metrics such as BLEU (Papineni etal., 2002) are based on a comparison of the trans-lation hypothesis to one or more human references.TerrorCat still uses a human reference to extract fea-tures from the error analysis with Addicter and Hjer-son, but at the core, TerrorCat compares hypothesesnot to a reference, but to each other.68Manual Classifier Output and Confidence: Precisionlabel < < or > >0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0< 81% 60% 45% 8% 32% 23% 10%= 9% 17% 23% 84% 23% 17% 9%> 10% 23% 32% 8% 45% 60% 81%Manual Classifier Output and Confidence: Recalllabel < < or > >0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0< 23% 18% 28% 1% 20% 7% 3%= 5% 9% 26% 20% 26% 9% 5%> 3% 7% 20% 1% 28% 18% 23%Table 4: The precision and recall confusion matrices of the classifier ?
judgements on whether one hypothesis is worsethan, equivalent to or better than another hypothesis are compared to the classifier?s output and confidence.It is thus most similar to SVM-RANK and Teslametrics, submissions to the WMT?10 shared met-rics task (Callison-Burch et al, 2010) which alsoused SVMs for ranking translations.
However, bothmetrics used SVMrank (Joachims, 2006) directly forranking (unlike TerrorCat, which uses a binary clas-sifier for pairwise comparisons).
Their features in-cluded some of the metric outputs (BLEU, ROUGE,etc.)
for SVM-RANK and similarity scores betweenbags of n-grams for Tesla (Dahlmeier et al, 2011).5 ConclusionsWe introduced the TerrorCat metric, which performspairwise comparison of translation hypotheses basedon frequencies of automatically obtained error cate-gories using a binary classifier, trained on manuallyranked data.
The comparison outcome is then con-verted to a numeric score for every sentence or doc-ument translation by averaging out the number ofwins per translation system.Our submitted system achieved an averagesystem-level correlation with human judgements inthe WMT?11 development set of 0.86 for transla-tion into English and 0.85 for translations from En-glish into other languages.
Particularly good per-formance was achieved on translations from Englishinto Czech (0.90) and back (0.95).
Sentence-levelscores are more modest: average 0.27 for transla-tion into English and 0.23 for those out of English.The scores remain to be checked against the humanjudgments from WMT?12.The introduced TerrorCat metric has certain de-pendencies.
For one thing, in order to apply it tonew languages, a training set of manual rankings isrequired ?
although this can be viewed as an advan-tage, since it enables the user to tune the metric tohis/her own preference.
Additionally, the metric de-pends on lemmatization and PoS-tagging.There is a number of directions to explore in thefuture.
For one, both Addicter and Hjerson reportMT errors related more to adequacy than fluency, al-though it was shown last year (Parton et al, 2011)that fluency is an important component in ratingtranslation quality.
It is also important to test howwell the metric performs if lemmatization and PoS-tagging are not available.For this year?s competition, training data wastaken separately for every language pair; it remainsto be tested whether combining human judgementswith the same target language and different sourcelanguages leads to better or worse performance.To conclude, we have described TerrorCat, oneof the submissions to the metrics shared task ofWMT?12.
TerrorCat is rather demanding to apply onone hand, having more requirements than the com-mon reference-hypothesis translation pair, but at thesame time correlates rather well with human judge-ments on the system level.69ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden.Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.
2011.Tesla at wmt 2011: Translation evaluation and tunablemetric.
In Proceedings of the Sixth Workshop on Sta-tistical Machine Translation, pages 78?84, Edinburgh,Scotland.Mark Fishel, Ondr?ej Bojar, and Maja Popovic?.
2012.Terra: a collection of translation error-annotated cor-pora.
In Proceedings of the 8th LREC, page in print,Istanbul, Turkey.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11.Thorsten Joachims.
2006.
Training linear SVMs inlinear time.
In Proceedings of the ACM Conferenceon Knowledge Discovery and Data Mining (KDD),Philadelphia, USA.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HLT-NAACL Conference, pages 104?111, New York, NY.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic evalu-ation of machine translation.
In ACL ?02: Proceedingsof the 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318, Morristown, NJ,USA.
Association for Computational Linguistics.Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-tin Chodorow.
2011.
E-rating machine translation.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 108?115, Edinburgh, Scot-land.John C. Platt.
1998.
Using analytic qp and sparsenessto speed training of support vector machines.
In Pro-ceedings of Neural Information Processing Systems11, pages 557?564, Denver, CO.Maja Popovic?
and Hermann Ney.
2011.
Towards au-tomatic error analysis of machine translation output.Computational Linguistics, 37(4):657?688.Helmut Schmid.
1995.
Improvements in part-of-speechtagging with an application to german.
In Proceedingsof the ACL SIGDAT-Workshop, Dublin, Ireland.Drahom?
?ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-bec, and Pavel Kve?ton?.
2007.
The best of two worlds:Cooperation of statistical and rule-based taggers forCzech.
In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL 2007,pages 67?74, Praha.Daniel Zeman, Mark Fishel, Jan Berka, and Ondr?ej Bo-jar.
2011.
Addicter: What is wrong with my transla-tions?
The Prague Bulletin of Mathematical Linguis-tics, 96:79?88.70
