Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 822?831,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPParser Adaptation and Projectionwith Quasi-Synchronous Grammar Features?David A. SmithDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA 01003, USAdasmith@cs.umass.eduJason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218, USAjason@cs.jhu.eduAbstractWe connect two scenarios in structuredlearning: adapting a parser trained onone corpus to another annotation style, andprojecting syntactic annotations from onelanguage to another.
We propose quasi-synchronous grammar (QG) features forthese structured learning tasks.
That is, wescore a aligned pair of source and targettrees based on local features of the treesand the alignment.
Our quasi-synchronousmodel assigns positive probability to anyalignment of any trees, in contrast to a syn-chronous grammar, which would insist onsome form of structural parallelism.In monolingual dependency parser adap-tation, we achieve high accuracy in trans-lating among multiple annotation stylesfor the same sentence.
On the moredifficult problem of cross-lingual parserprojection, we learn a dependency parserfor a target language by using bilin-gual text, an English parser, and auto-matic word alignments.
Our experimentsshow that unsupervised QG projection im-proves on parses trained using only high-precision projected annotations and faroutperforms, by more than 35% absolutedependency accuracy, learning an unsu-pervised parser from raw target-languagetext alone.
When a few target-languageparse trees are available, projection givesa boost equivalent to doubling the numberof target-language trees.
?The first author would like to thank the Center for Intel-ligent Information Retrieval at UMass Amherst.
We wouldalso like to thank Noah Smith and Rebecca Hwa for helpfuldiscussions and the anonymous reviewers for their sugges-tions for improving the paper.1 Introduction1.1 Parser AdaptationConsider the problem of learning a dependencyparser, which must produce a directed tree whosevertices are the words of a given sentence.
Thereare many differing conventions for representingsyntactic relations in dependency trees.
Say thatwe wish to output parses in the Prague style andso have annotated a small target corpus?e.g.,100 sentences?with those conventions.
A parsertrained on those hundred sentences will achievemediocre dependency accuracy (the proportion ofwords that attach to their correct parent).But what if we also had a large number of treesin the CoNLL style (the source corpus)?
Ide-ally they should help train our parser.
But unfor-tunately, a parser that learned to produce perfectCoNLL-style trees would, for example, get bothlinks ?wrong?
when its coordination constructionswere evaluated against a Prague-style gold stan-dard (Figure 1).If it were just a matter of this one construction,the obvious solution would be to write a few rulesby hand to transform the large source training cor-pus into the target style.
Suppose, however, thatthere were many more ways that our corpora dif-fered.
Then we would like to learn a statisticalmodel to transform one style of tree into another.We may not possess hand-annotated trainingdata for this tree-to-tree transformation task.
Thatwould require the two corpora to annotate some ofthe same sentences in different styles.But fortunately, we can automatically obtain anoisy form of the necessary paired-tree trainingdata.
A parser trained on the source corpus canparse the sentences in our target corpus, yieldingtrees (or more generally, probability distributionsover trees) in the source style.
We will then learna tree transformation model relating these noisysource trees to our known trees in the target style.822now ornever noworneverPrague Mel?
?cuknowor never now or neverCoNLL MALTFigure 1: Four of the five logically possible schemes forannotating coordination show up in human-produced depen-dency treebanks.
(The other possibility is a reverse Mel??cukscheme.)
These treebanks also differ on other conventions.This model should enable us to convert the orig-inal large source corpus to target style, giving usadditional training data in the target style.1.2 Parser ProjectionFor many target languages, however, we do nothave the luxury of a large parsed ?source cor-pus?
in the language, even one in a different styleor domain as above.
Thus, we may seek otherforms of data to augment our small target corpus.One option would be to leverage unannotated text(McClosky et al, 2006; Smith and Eisner, 2007).But we can also try to transfer syntactic informa-tion from a parsed source corpus in another lan-guage.
This is an extreme case of out-of-domaindata.
This leads to the second task of this paper:learning a statistical model to transform a syntac-tic analysis of a sentence in one language into ananalysis of its translation.Tree transformations are often modeled withsynchronous grammars.
Suppose we are given asentence w?in the ?source?
language and its trans-lation w into the ?target?
language.
Their syn-tactic parses t?and t are presumably not indepen-dent, but will tend to have some parallel or at leastcorrelated structure.
So we could jointly modelthe parses t?, t and the alignment a between them,with a model of the form p(t, a, t?| w,w?
).Such a joint model captures how t, a, t?mu-tually constrain each other, so that even partialknowledge of some of these three variables canhelp us to recover the others when training or de-coding on bilingual text.
This idea underlies anumber of recent papers on syntax-based align-ment (using t and t?to better recover a), grammarinduction from bitext (using a to better recover tand t?
), parser projection (using t?and a to betterFigure 2: With the English tree and alignment provided bya parser and aligner at test time, the Chinese parser finds thecorrect dependencies (see ?6).
A monolingual parser?s incor-rect edges are shown with dashed lines.recover t), as well as full joint parsing (Smith andSmith, 2004; Burkett and Klein, 2008).In this paper, we condition on the 1-best sourcetree t?.
As for the alignment a, our models ei-ther condition on the 1-best alignment or integratethe alignment out.
Our models are thus of theform p(t | w,w?, t?, a) or, in the generative case,p(w, t, a | w?, t?).
We intend to consider other for-mulations in future work.So far, this is very similar to the monolingualparser adaptation scenario, but there are a few keydifferences.
Since the source and target sentencesin the bitext are in different languages, there isno longer a trivial alignment between the wordsof the source and target trees.
Given word align-ments, we could simply try to project dependencylinks in the source tree onto the target text.
Alink-by-link projection, however, could result ininvalid trees on the target side, with cycles or dis-connected words.
Instead, our models learn thenecessary transformations that align and transforma source tree into a target tree by means of quasi-synchronous grammar (QG) features.Figure 2 shows an example of bitext helpingdisambiguation when a parser is trained with onlya small number of Chinese trees.
With the helpof the English tree and alignment, the parser isable to recover the correct Chinese dependen-cies using QG features.
Incorrect edges fromthe monolingual parser are shown with dashedlines.
(The bilingual parser corrects additional er-rors in the second half of this sentence, which hasbeen removed to improve legibility.)
The parseris able to recover the long-distance dependencyfrom the first Chinese word (China) to the last(begun), while skipping over the intervening noun823phrase that confused the undertrained monolin-gual parser.
Although, due to the auxiliary verb,?China?
and ?begun?
are siblings in English andnot in direct dependency, the QG features stillleverage this indirect projection.1.3 Plan of the PaperWe start by describing the features we use toaugment conditional and generative parsers whenscoring pairs of trees (?2).
Then we discuss in turnmonolingual (?3) and cross-lingual (?4) parseradaptation.
Finally, we present experiments oncross-lingual parser projection in conditions whenno target language trees are available for training(?5) and when some trees are available (?6).2 Form of the ModelWhat should our model of source and target treeslook like?
In our view, traditional approachesbased on synchronous grammar are problematicboth computationally and linguistically.
Full in-ference takes O(n6) time or worse (depending onthe grammar formalism).
Yet synchronous mod-els only consider a limited hypothesis space: e.g.,parses must be projective, and alignments must de-compose according to the recursive parse struc-ture.
(For example, two nodes can be alignedonly if their respective parents are also aligned.
)The synchronous model?s probability mass func-tion is also restricted to decompose in this way,so it makes certain conditional independence as-sumptions; put another way, it can evaluate onlycertain properties of the triple (t, a, t?
).We instead model (t, a, t?)
as an arbitrary graphthat includes dependency links among the wordsof each sentence as well as arbitrary alignmentlinks between the words of the two sentences.This permits non-synchronous and many-to-manyalignments.
The only hard constraint we imposeis that the dependency links within each sentencemust constitute a valid monolingual parse?a di-rected projective spanning tree.1Given the two sentences w,w?, our probabil-ity distribution over possible graphs considers lo-cal features of the parses, the alignment, and bothjointly.
Thus, we learn what local syntactic con-figurations tend to occur in each language and howthey correspond across languages.
As a result, wemight learn that parses are ?mostly synchronous,?but that there are some systematic cross-linguistic1Non-projective parsing would also be possible.divergences and some instances of sloppy (non-parallel or inexact) translation.
Our model is thus aform of quasi-synchronous grammar (QG) (Smithand Eisner, 2006a).
In that paper, QG was appliedto word alignment and has since found applica-tions in question answering (Wang et al, 2007),paraphrase detection (Das and Smith, 2009), andmachine translation (Gimpel and Smith, 2009).All the models in this paper are conditioned onthe source tree t?.
Conditionally-trained modelsof adaptation and projection also condition on thetarget string w and its alignment a to w?and thushave the form p(t | w,w?, t?, a); the unsupervised,generative projection models in ?5 have the formp(w, t, a | w?, t?
).The score s of a given tuple of trees, words, andalignment can thus be written as a dot product ofweights w with features f and g:s(t, t?, a, w,w?)
=?iwifi(t, w)+?jwjgj(t, t?, a, w,w?
)The features f look only at target words and de-pendencies.
In the conditional models of ?3 and?6, these features are those of an edge-factoreddependency parser (McDonald et al, 2005).
Inthe generative models of ?5, f has the form of adependency model with valence (Klein and Man-ning, 2004).
All models, for instance, have a fea-ture template that considers the parts of speech ofa potential parent-child relation.In order to benefit from the source language, wealso need to include bilingual features g. Whenscoring a candidate target dependency link fromword x ?
y, these features consider the relation-ship of their corresponding source words x?andy?.
(The correspondences are determined by thealignment a.)
For instance, the source tree t?maycontain the link x??
y?, which would cause a fea-ture for monotonic projection to fire for the x ?
yedge.
If, on the other hand, y??
x??
t?, ahead-swapping feature fires.
If x?= y?, i.e.
xand y align to the same word, the same-word fea-ture fires.
Similar features fire when x?and y?arein grandparent-grandchild, sibling, c-command, ornone-of-the above relationships, or when y alignsto NULL.
These alignment classes are called con-figurations (Smith and Eisner, 2006a, and follow-ing).
When training is conditioned on the targetwords (see ?3 and ?6 below), we conjoin these824configuration features with the part of speech andcoarse part of speech of one or both of the sourceand target words, i.e.
the feature template has fromone to four tags.In conditional training, the exponentiatedscores s are normalized by a constant: Z =?texp[s(t, t?, a, w,w?)].
For the generativemodel, the locally normalized generative processis explained in ?5.3.4.Previous researchers have written fix-up rulesto massage the projected links after the fact andlearned a parser from the resulting trees (Hwa etal., 2005).
Instead, our models learn the necessarytransformations that align and transform a sourcetree into a target tree.
Other researchers have tack-led the interesting task of learning parsers fromunparsed bitext alone (Kuhn, 2004; Snyder et al,2009); our methods take advantage of investmentsin high-resource languages such as English.
Inwork most closely related to this paper, Ganchev etal.
(2009) constrain the posterior distribution overtarget-language dependencies to align to sourcedependencies some ?reasonable?
proportion of thetime (?
70%, cf.
Table 2 in this paper).
Thisapproach performs well but cannot directly learnregular cross-language non-isomorphisms; for in-stance, some fixup rules for auxiliary verbs needto be introduced.
Finally, Huang et al (2009)use features, somewhat like QG configurations, onthe shift-reduce actions in a monolingual, target-language parser.3 AdaptationAs discussed in ?1, the adaptation scenario is aspecial case of parser projection where the wordalignments are one-to-one and observed.
To testour handling of QG features, we performed ex-periments in which training saw the correct parsetrees in both source and target domains, and themapping between them was simple and regular.We also performed experiments where the sourcetrees were replaced by the noisy output of a trainedparser, making the mapping more complex andharder to learn.We used the subset of the Penn Treebank fromthe CoNLL 2007 shared task and converted it todependency representation while varying two pa-rameters: (1) CoNLL vs. Prague coordinationstyle (Figure 1), and (2) preposition the head vs.the child of its nominal object.We trained an edge-factored dependency parser(McDonald et al, 2005) on ?source?
domain datathat followed one set of dependency conventions.We then trained an edge-factored parser with QGfeatures on a small amount of ?target?
domaindata.
The source parser outputs were produced forall target data, both training and test, so that fea-tures for the target parser could refer to them.In this task, we know what the gold-standardsource language parses are for any given text,since we can produce them from the original PennTreebank.
We can thus measure the contributionof adaptation loss alone, and the combined lossof imperfect source-domain parsing with adapta-tion (Table 1).
When no target domain trees areavailable, we simply have the performance of thesource domain parser on this out-of-domain data.Training a target-domain parser on as few as 10sentences shows substantial improvements in ac-curacy.
In the ?gold?
conditions, where the targetparser starts with perfect source trees, accuracyapproaches 100%; in the realistic ?parse?
condi-tions, where the target-domain parser gets noisysource-domain parses, the improvements are quitesignificant but approach a lower ceiling imposedby the performance of the source parser.2The adaptation problem in this section is a sim-ple proof of concept of the QG approach; however,more complex and realistic adaptation problemsexist.
Monolingual adaptation is perhaps most ob-viously useful when the source parser is a black-box or rule-based system or is trained on unavail-able data.
One might still want to use such a parserin some new context, which might require newdata or a new annotation standard.We are also interested in scenarios where wewant to avoid expensive retraining on large rean-notated treebanks.
We would like a linguist to beable to annotate a few trees according to a hy-pothesized theory and then quickly use QG adap-tation to get a parser for that theory.
One examplewould be adapting a constituency parser to pro-duce dependency parses.
We have concentratedhere on adapting between two dependency parsestyles, in order to line up with the cross-lingualtasks to which we now turn.2In the diagonal cells, source and target styles match, sotraining the QG parser amounts to a ?stacking?
technique(Martins et al, 2008).
The small training size and overreg-ularization of the QG parser mildly hurts in-domain parsingperformance.825% Dependency Accuracy on TargetCoNLL-PrepHead CoNLL-PrepChild Prague-PrepHead Prague-PrepChildSource 0 10 100 0 10 100 0 10 100 0 10 100Gold CoNLL-PrepHead 100 99.6 99.6 79.5 96.9 97.8 90.5 95.0 98.1 71.0 92.7 95.4Parse CoNLL-PrepHead 89.5 88.9 89.0 71.4 85.9 87.9 82.5 84.3 87.8 65.2 82.2 86.1Gold CoNLL-PrepChild 79.5 96.6 97.3 100 99.6 99.6 71.0 91.3 95.5 89.9 94.5 97.9Parse CoNLL-PrepChild 71.0 84.2 86.8 88.1 87.5 88.0 64.9 80.7 84.9 80.9 83.5 86.1Gold Prague-PrepHead 90.5 95.5 96.7 71.0 92.0 94.2 100 99.6 99.6 79.6 97.4 98.1Parse Prague-PrepHead 83.0 87.1 87.4 65.6 84.2 85.9 88.5 88.3 88.0 70.7 86.4 86.8Gold Prague-PrepChild 71.0 91.6 93.8 89.9 95.6 96.4 79.6 96.0 97.1 100 99.6 99.6Parse Prague-PrepChild 65.3 81.7 84.6 81.2 84.5 86.1 70.4 83.2 85.3 86.9 86.1 86.8Table 1: Adapting a parser to a new annotation style.
We learn to parse in a ?target?
style (wide column label) given somenumber (narrow column label) of supervised target-style training sentences.
As a font of additional features, all training andtest sentences have already been augmented with parses in some ?source?
style (row label): either gold-standard parses (anoracle experiment) or else the output of a parser trained on 18k source trees (more realistic).
If we have 0 training sentences, wesimply output the source-style parse.
But with 10 or 100 target-style training sentences, each off-diagonal block learns to adapt,mostly closing the gap with the diagonal block in the same column.
In the diagonal blocks, source and target styles match, andthe QG parser degrades performance when acting as a ?stacked?
parser.4 Cross-Lingual Projection: BackgroundAs in the adaptation scenario above, many syn-tactic structures can be transferred from one lan-guage to another.
In this section, we evaluate theextent of this direct projection on a small hand-annotated corpus.
In ?5, we will use a QG genera-tive model to learn dependency parsers from bitextwhen there are no annotations in the target lan-guage.
Finally, in ?6,we show how QG featurescan augment a target-language parser trained on asmall set of labeled trees.For syntactic annotation projection to work atall, we must hypothesize, or observe, that at leastsome syntactic structures are preserved in transla-tion.
Hwa et al (2005) have called this intuitionthe Direct Correspondence Assumption (DCA,with slight notational changes):Given a pair of sentences w and w?thatare translations of each other with syn-tactic structure t and t?, if nodes x?andy?of t?are aligned with nodes x and y oft, respectively, and if syntactic relation-ship R(x?, y?)
holds in t?, then R(x, y)holds in t.The validity of this assumption clearly dependson the node-to-node alignment of the two trees.We again work in a dependency framework, wheresyntactic nodes are simply lexical items.
This al-lows us to use existing work on word alignment.Hwa et al (2005) tested the DCA under ide-alized conditions by obtaining hand-corrected de-pendency parse trees of a few hundred sentencesof Spanish-English and Chinese-English bitext.They also used human-produced word alignments.Corpus Prec.
[%] Rec.
[%]Spanish 64.3 28.4(no punc.)
72.0 30.8Chinese 65.1 11.1(no punc.)
68.2 11.5Table 2: Precision and recall of direct dependency projectionvia one-to-one links alone.Since their word alignments could be many-to-many, they gave a heuristic Direct Projection Al-gorithm (DPA) for resolving them into componentdependency relations.
It should be noted that thisprocess introduced empty words into the projectedtarget language tree and left words that are un-aligned to English detached from the tree; as a re-sult, they measured performance in dependency F-score rather than accuracy.
With manual Englishparses and word alignments, this DPA achieved36.8% F-score in Spanish and 38.1% in Chinese.With Collins-model English parses and GIZA++word alignments, F-score was 33.9% for Spanishand 26.3% for Chinese.
Compare this to the Span-ish attach-left baseline of 31.0% and the Chineseattach-right baselines of 35.9%.
These discour-agingly low numbers led them to write language-specific transformation rules to fix up the projectedtrees.
After these rules were applied to the pro-jections of automatic English parses, F-score was65.7% for English and 52.4% for Chinese.While these F-scores were low, it is useful tolook at a subset of the alignment: dependenciesprojected across one-to-one alignments before theheuristic fix-ups had a much higher precision, iflower recall, than Hwa et al?s final results.
Us-826ing Hwa et al?s data, we calculated that the preci-sion of projection to Spanish and Chinese via theseone-to-one links was ?
65% (Table 2).
There isclearly more information in these direct links thanone would think from the F-scores.
To exploit thisinformation, however, we need to overcome theproblems of (1) learning from partial trees, whennot all target words are attached, and (2) learningin the presence of the still considerable noise in theprojected one-to-one dependencies?e.g., at least28% error for Spanish non-punctuation dependen-cies.What does this noise consist of?
Some errorsreflect fairly arbitrary annotation conventions intreebanks, e.g.
should the auxiliary verb gov-ern the main verb or vice versa.
(Examples likethis suggest that the projection problem containsthe adaptation problem above.)
Other errors arisefrom divergences in the complements required ofcertain head words.
In the German-English trans-lation pair, with co-indexed words aligned,[an [den Libanon1]] denken2?
remember2Lebanon1we would prefer that the preposition an attachto denken, even though the preposition?s objectLibanon aligns to a direct child of remember.In other words, we would like the grandparent-parent-child chain of denken ?
an ?
Libanonto align to the parent-child pair of remember ?Lebanon.
Finally, naturally occurring bitexts con-tain some number of free or erroneous transla-tions.
Machine translation researchers often seekto strike these examples from their training cor-pora; ?free?
translations are not usually welcomefrom an MT system.5 Unsupervised Cross-Lingual ProjectionFirst, we consider the problem of parser projectionwhen there are zero target-language trees avail-able.
As in much other work on unsupervisedparsing, we try to learn a generative model thatcan predict target-language sentences.
Our novelcontribution is to condition the probabilities of thegenerative actions on the dependency parse of asource-language translation.
Thus, our generativemodel is a quasi-synchronous grammar, exactly asin (Smith and Eisner, 2006a).3When training on target sentences w, there-fore, we tune the model parameters to maxi-mize not?tp(t, w) as in ordinary EM, but rather3Our task here is new; they used it for alignment.
?tp(t, w, a | t?, w?).
We hope that this condi-tional EM training will drive the model to posit ap-propriate syntactic relationships in the latent vari-able t, because?thanks to the structure of the QGmodel?that is the easiest way for it to exploit theextra information in t?, w?to help predict w.4Attest time, t?, w?are not made available, so we justuse the trained model to find argmaxtp(t | w),backing off from the conditioning on t?, w?andsumming over a.Below, we present the specific generative model(?5.1) and some details of training (?5.2).
We willthen compare three approaches (?5.3):?5.3.2 a straight EM baseline (which does notcondition on t?, w?at all)?5.3.3 a ?hard?
projection baseline (which naivelyprojects t?, w?to derive direct supervision inthe target language)?5.3.4 our conditional EM approach above (whichmakes t?, w?available to the learner for ?soft?indirect supervision via QG)5.1 Generative ModelsOur base models of target-language syntax aregenerative dependency models that have achievedstate-of-the art results in unsupervised dependencystructure induction.
The simplest version, calledDependency Model with Valence (DMV), has beenused in isolation and in combination with othermodels (Klein and Manning, 2004; Smith and Eis-ner, 2006b).
The DMV generates the right chil-dren, and then independently the left children, foreach node in the dependency tree.
Nodes corre-spond to words, which are represented by theirpart-of-speech tags.
At each step of generation,the DMV stochastically chooses whether to stopgenerating, conditioned on the currently generat-ing head; whether it is generating to the right orleft; and whether it has yet generated any chil-dren on that side.
If it chooses to continue, it then4The contrastive estimation of Smith and Eisner (2005)also used a form of conditional EM, with similar motiva-tion.
They suggested that EM grammar induction, whichlearns to predictw, unfortunately learns mostly to predict lex-ical topic or other properties of the training sentences that donot strongly require syntactic latent variables.
To focus EMon modeling the syntactic relationships, they conditioned theprediction of w on almost complete knowledge of the lexi-cal items.
Similarly, we condition on a source translation ofw.
Furthermore, our QG model structure makes it easy forEM to learn to exploit the (explicitly represented) syntacticproperties of that translation when predicting w.827stochastically generates the tag of a new child,conditioned on the head.
The parameters of themodel are thus of the formp(stop | head, dir, adj) (1)p(child | head, dir) (2)where head and child are part-of-speech tags,dir ?
{left, right}, and adj, stop ?
{true, false}.ROOT is stipulated to generate a single right child.Bilingual configurations that condition on t?, w?
(?2) are incorporated into the generative processas in Smith and Eisner (2006a).
When the modelis generating a new child for word x, aligned to x?,it first chooses a configuration and then chooses asource word y?in that configuration.
The child y isthen generated, conditioned on its parent x, mostrecent sibling a, and its source analogue y?.5.2 Details of EM TrainingAs in previous work on grammar induction, welearn the DMV from part-of-speech-tagged target-language text.
We use expectation maximization(EM) to maximize the likelihood of the data.
Sincethe likelihood function is nonconvex in the unsu-pervised case, our choice of initial parameters canhave a significant effect on the outcome.
Althoughwe could also try many random starting points, theinitializer in Klein and Manning (2004) performsquite well.The base dependency parser generates the rightdependents of a head separately from the left de-pendents, which allows O(n3) dynamic program-ming for an n-word target sentence.
Since the QGannotates nonterminals of the grammar with sin-gle nodes of t?, and we consider two nodes of t?when evaluating the above dependency configura-tions, QG parsing runs inO(n3m2) for anm-wordsource sentence.
If, however, we restrict candidatesenses for a target child c to come from links inan IBM Model 4 Viterbi alignment, we achieveO(n3k2), where k is the maximum number ofpossible words aligned to a given target languageword.
In practice, k  m, and parsing is not ap-preciably slower than in the monolingual setting.If all configurations were equiprobable, thesource sentence would provide no information tothe target.
In our QG experiments, therefore,we started with a bias towards direct parent?childlinks and a very small probability for breakagesof locality.
The values of other configuration pa-rameters seem, experimentally, less important forinsuring accurate learning.5.3 ExperimentsOur experiments compare learning on target lan-guage text to learning on parallel text.
In the lat-ter case, we compare learning from high-precisionone-to-one alignments alone, to learning from allalignments using a QG.5.3.1 CorporaOur development and test data were drawn fromthe German TIGER and Spanish Cast3LB tree-banks as converted to projective dependencies forthe CoNLL 2007 Shared Task (Brants et al, 2002;Civit Torruella and Mart??
Anton?
?n, 2002).5Our training data were subsets of the 2006Statistical Machine Translation Workshop SharedTask, in particular from the German-Englishand Spanish-English Europarl parallel corpora(Koehn, 2002).
The Shared Task provided pre-built automatic GIZA++ word alignments, whichwe used to facilitate replicability.
Since theseword alignments do not contain posterior proba-bilities or null links, nor do they distinguish whichlinks are in the IBMModel intersection, we treatedall links as equally likely when learning the QG.Target language words unaligned to any sourcelanguage words were the only nodes allowed toalign to NULL in QG derivations.We parsed the English side of the bitext with theprojective dependency parser described by Mc-Donald et al (2005) trained on the Penn Treebank??2?20.
Much previous work on unsupervisedgrammar induction has used gold-standard part-of-speech tags (Smith and Eisner, 2006b; Kleinand Manning, 2004; Klein and Manning, 2002).While there are no gold-standard tags for the Eu-roparl bitext, we did train a conditional Markov5We made one change to the annotation conventions inGerman: in the dependencies provided, words in a nounphrase governed by a preposition were all attached to thatpreposition.
This meant that in the phrase das Kind (?thechild?)
in, say, subject position, das was the child of Kind;but, in f?ur das Kind (?for the child?
), das was the child off?ur.
This seems to be a strange choice in converting from theTIGER constituency format, which does in fact annotate NPsinside PPs; we have standardized prepositions to govern onlythe head of the noun phrase.
We did not change any otherannotation conventions to make them more like English.
Inthe Spanish treebank, for instance, control verbs are the chil-dren of their verbal complements: in quiero decir (?I want tosay?=?I mean?
), quiero is the child of decir.
In German co-ordinations, the coordinands all attach to the first, but in En-glish, they all attach to the last.
These particular divergencesin annotation style hurt all of our models equally (since noneof them have access to labeled trees).
These annotation diver-gences are one motivation for experiments below that includesome target trees.828Dependency accuracy [%]Baselines German SpanishModify prev.
18.2 28.5Modify next 27.5 21.4EM 30.2 25.6Hard proj.
66.2 59.1Hard proj.
w/EM 58.6 53.0QG w/EM 68.5 64.8Table 3: Test accuracy with unsupervised training methodsmodel tagger on a few thousand tagged sentences.This is the only supervised data we used in the tar-get.
We created versions of each training corpuswith the first thousand, ten thousand, and hundredthousand sentence pairs, each a prefix of the next.Since the target-language-only baseline convergedmuch more slowly, we used a version of the cor-pora with sentences 15 target words or fewer.5.3.2 Fully Unsupervised EMUsing the target side of the bitext as training data,we initialized our model parameters as describedin ?5.2 and ran EM.
We checked convergence ona development set and measured unlabeled depen-dency accuracy on held-out test data.
We com-pare performance to simple attach-right and at-tach left baselines (Table 3).
For mostly head-final German, the ?modify next?
baseline is bet-ter; for mostly head-initial Spanish, ?modify pre-vious?
wins.
Even after several hundred iterations,performance was slightly, but not significantly bet-ter than the baseline for German.
EM training didnot beat the baseline for Spanish.65.3.3 Hard Projection, Semi-Supervised EMThe simplest approach to using the high-precisionone-to-one word alignments is labeled ?hard pro-jection?
in the table.
We filtered the training cor-pus to find sentences where enough links wereprojected to completely determine a target lan-guage tree.
Of course, we needed to filter morethan 1000 sentences of bitext to output 1000training sentences in this way.
We simply per-form supervised training with this subset, whichis still quite noisy (?4), and performance quickly6While these results are worse than those obtained previ-ously for this model, the experiments in Klein and Manning(2004) and only used sentences of 10 words or fewer, withoutpunctuation, and with gold-standard tags.
Punctuation in par-ticular seems to trip up the initializer: since a sentence-finalperiods appear in most sentences, EM often decides to makeit the head.plateaus.
Still, this method substantially improvesover the baselines and unsupervised EM.Restricting ourselves to fully projected treesseems a waste of information.
We can also sim-ply take all one-to-one projected links, impute ex-pected counts for the remaining dependencies withEM, and update our models.
This approach (?hardprojection with EM?
), however, performed worsethan using only the fully projected trees.
In fact,only the first iteration of EM with this methodmade any improvement; afterwards, EM degradedaccuracy further from the numbers in Table 3.5.3.4 Soft Projection: QG & Conditional EMThe quasi-synchronous model used all of thealignments in re-estimating its parameters and per-formed significantly better than hard projection.Unlike EM on the target language alone, the QG?sperformance does not depend on a clever initial-izer for initial model weights?all parameters ofthe generative model except for the QG configura-tion features were initialized to zero.
Setting theprior to prefer direct correspondence provides thenecessary bias to initialize learning.Error analysis showed that certain types of de-pendencies eluded the QG?s ability to learn frombitext.
The Spanish treebank treats some verbalcomplements as the heads of main verbs and aux-iliary verbs as the children of participles; the QG,following the English, learned the opposite de-pendency direction.
Spanish treebank conventionsfor punctuation were also a common source of er-rors.
In both German and Spanish, coordinations(a common bugbear for dependency grammars)were often mishandled: both treebanks attach thelater coordinands and any conjunctions to the firstcoordinand; the reverse is true in English.
Finally,in both German and Spanish, preposition attach-ments often led to errors, which is not surprisinggiven the unlexicalized target-language grammars.Rather than trying to adjudicate which dependen-cies are ?mere?
annotation conventions, it wouldbe useful to test learned dependency models onsome extrinsic task such as relation extraction ormachine translation.6 Supervised Cross-Lingual ProjectionFinally, we consider the problem of parser projec-tion when some target language trees are available.As in the adaptation case (?3), we train a condi-tional model (not a generative DMV) of the target829tree given the target sentence, using the monolin-gual and bilingual QG features, including config-urations conjoined with tags, outlined above (?2).For these experiments, we used the LDC?sEnglish-Chinese Parallel Treebank (ECTB).
Sincemanual word alignments also exist for a part ofthis corpus, we were able to measure the loss inaccuracy (if any) from the use of an automaticEnglish parser and word aligner.
The source-language English dependency parser was trainedon the Wall Street Journal, where it achieved 91%dependency accuracy on development data.
How-ever, it was only 80.3% accurate when applied toour task, the English side of the ECTB.7After parsing the source side of the bitext, wetrain a parser on the annotated target side, usingQG features described above (?2).
Both the mono-lingual target-language parser and the projectedparsers are trained to optimize conditional likeli-hood of the target trees t?with ten iterations ofstochastic gradient ascent.In Figure 3, we plot the performance of thetarget-language parser on held-out bitext.
Al-though projection performance is, not surprisingly,better if we know the true source trees at trainingand test time, even with the 1-best output of thesource parser, QG features help produce a parseras accurate asq one trained on twice the amountof monolingual data.
In ablation experiments, weincluded bilingual features only for directly pro-jected links, with no features for head-swapping,grandparents, etc.
When using 1-best Englishparses, parsers trained only with direct-projectionand monolingual features performed worse; whenusing gold English parses, parsers with direct-projection-only features performed better whentrained with more Chinese trees.7 DiscussionThe two related problems of parser adaptation andprojection are often approached in different ways.Many adaptation methods operate by simple aug-mentations of the target feature space, as we havedone here (Daume III, 2007).
Parser projection, onthe other hand, often uses a multi-stage pipeline7It would be useful to explore whether the techniques of?3 above could be used to improve English accuracy by do-main adaptation.
In theory a model with QG features trainedto perform well on Chinese should not suffer from an inaccu-rate, but consistent, English parser, but the results in Figure 3indicate a significant benefit to be had from better Englishparsing or from joint Chinese-English inference.10 20 50 100 200 500 1000 20000.600.650.700.750.800.85Training examplesUnlabeled accuracyTarget only+Gold alignments+Source text+Gold parses, alignments+Gold parsesFigure 3: Parser projection with target trees.
Using the trueor 1-best parse trees in the source language is equivalent tohaving twice as much data in the target language.
Note thatthe penalty for using automatic alignments instead of goldalignments is negligible; in fact, using Source text alone isoften higher than +Gold alignments.
Using gold source trees,however, significantly outperforms using 1-best source trees.
(Hwa et al, 2005).
The methods presented heremove parser projection much closer in efficiencyand simplicity to monolingual parsing.We showed that augmenting a target parser withquasi-synchronous features can lead to significantimprovements?first in experiments with adapt-ing to different dependency representations in En-glish, and then in cross-language parser projec-tion.
As with many domain adaptation problems,it is quite helpful to have some annotated tar-get data, especially when annotation styles vary(Dredze et al, 2007).
Our experiments show thatunsupervised QG projection improves on parserstrained using only high-precision projected anno-tations and far outperforms, by more than 35%absolute dependency accuracy, unsupervised EM.When a small number of target-language parsetrees is available, projection gives a boost equiv-alent to doubling the number of target trees.The loss in performance from conditioning onlyon noisy 1-best source parses points to some nat-ural avenues for improvement.
We are explor-ing methods that incorporate a packed parse for-est on the source side and similar representationsof uncertainty about alignments.
Building on ourrecent belief propagation work (Smith and Eis-ner, 2008), we can jointly infer two dependencytrees and their alignment, under a joint distribu-tion p(t, a, t?| w,w?)
that evaluates the full graphof dependency and alignment edges.830ReferencesS.
Brants, S. Dipper, S. Hansen, W. Lezius, andG.
Smith.
2002.
The TIGER treebank.
In TLT.David Burkett and Dan Klein.
2008.
Two lan-guages are better than one (for syntactic parsing).
InEMNLP.M.
Civit Torruella and M. A.
Mart??
Anton??n.
2002.Design principles for a Spanish treebank.
In TLT.Dipanjan Das and Noah A. Smith.
2009.
Paraphraseidentification as probabilistic quasi-synchronousrecognition.
In ACL-IJCNLP.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In ACL, pages 256?263.Mark Dredze, John Blitzer, Partha Pratim Taluk-dar, Kuzman Ganchev, Jo?ao Graca, and FernandoPereira.
2007.
Frustratingly hard domain adap-tation for dependency parsing.
In Proceedings ofthe CoNLL Shared Task Session of EMNLP-CoNLL2007, pages 1051?1055.Kuzman Ganchev, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar induction viabitext projection constraints.
In ACL-IJCNLP.Kevin Gimpel and Noah A. Smith.
2009.
Feature-richtranslation by quasi-synchronous lattice parsing.
InEMNLP.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In EMNLP.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11:311?325.Dan Klein and Christopher D. Manning.
2002.
Agenerative constituent-context model for improvedgrammar induction.
In ACL, pages 128?135.Dan Klein and Christopher D. Manning.
2004.Corpus-based induction of syntactic structure: Mod-els of dependency and constituency.
In ACL, pages479?486.Philipp Koehn.
2002.
Europarl: A multilingualcorpus for evaluation of machine translation.http://www.iccs.informatics.ed.ac.uk/?pkoehn/-publications/europarl.ps.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In ACL, pages 470?477.Andr?e F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.In EMNLP.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Reranking and self-training for parseradaptation.
In ACL, pages 337?344.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In ACL, pages 91?98.Noah A. Smith and Jason Eisner.
2005.
Guiding unsu-pervised grammar induction using contrastive esti-mation.
In International Joint Conference on Artifi-cial Intelligence (IJCAI) Workshop on GrammaticalInference Applications, Edinburgh, July.David A. Smith and Jason Eisner.
2006a.
Quasi-synchronous grammars: Alignment by soft projec-tion of syntactic dependencies.
In Proceedings ofthe HLT-NAACL Workshop on Statistical MachineTranslation, pages 23?30.Noah A. Smith and Jason Eisner.
2006b.
Annealingstructural bias in multilingual weighted grammar in-duction.
In ACL-COLING, pages 569?576.David A. Smith and Jason Eisner.
2007.
Bootstrap-ping feature-rich dependency parsers with entropicpriors.
In EMNLP-CoNLL, pages 667?677.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In EMNLP, pages145?156.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using English toparse Korean.
In EMNLP, pages 49?56.Benjamin Snyder, Tahira Naseem, and Regina Barzi-lay.
2009.
Unsupervised multilingual grammar in-duction.
In ACL-IJCNLP.Mengqiu Wang, Noah A. Smith, and Teruko Mita-mura.
2007.
What is the Jeopardy model?
a quasi-synchronous grammar for QA.
In EMNLP-CoNLL,pages 22?32.831
