Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 684?693,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsImproved Fully Unsupervised Parsing with Zoomed LearningRoi ReichartICNCThe Hebrew Universityroiri@cs.huji.ac.ilAri RappoportInstitute of computer scienceThe Hebrew Universityarir@cs.huji.ac.ilAbstractWe introduce a novel training algorithmfor unsupervised grammar induction, calledZoomed Learning.
Given a training set T anda test set S, the goal of our algorithm is toidentify subset pairs Ti, Si of T and S suchthat when the unsupervised parser is trainedon a training subset Ti its results on its pairedtest subset Si are better than when it is trainedon the entire training set T .
A successful ap-plication of zoomed learning improves overallperformance on the full test set S.We study our algorithm?s effect on the leadingalgorithm for the task of fully unsupervisedparsing (Seginer, 2007) in three different En-glish domains, WSJ, BROWN and GENIA, andshow that it improves the parser F-score by upto 4.47%.1 IntroductionGrammar induction is the task of learning grammati-cal structure from plain text without human supervi-sion.
The task is of great importance both for theunderstanding of human language acquisition andsince its output can be used by NLP applications,avoiding the costly and error prone creation of man-ually annotated corpora.
Many recent works haveaddressed the task (e.g.
(Klein and Manning, 2004;Seginer, 2007; Cohen and Smith, 2009; Headden etal., 2009)) and its importance has increased due tothe recent availability of huge corpora.A basic challenge to this research direction ishow to utilize training data in the best possibleway.
Klein and Manning (2004) report results fortheir dependency model with valence (DMV) forunsupervised dependency parsing when it is trainedand tested on the same corpus (both when sentencelength restriction is imposed, such as for WSJ10,and when it is not, such as for the entire WSJ).
To-day?s best unsupervised dependency parsers, whichare rooted in this model, train on short sentencesonly: both Headen et al, (2009) and Cohen andSmith (2009) train on WSJ10 even when the test setincludes longer sentences.Recently, Spitkovsky et al, (2010) demonstratedthat training the DMV model on sentences of up to15 words length yields better results on the entiresection 23 of WSJ (with no sentence length restric-tion) than training with the entire WSJ corpus.In contrast to these dependency models, theSeginer constituency parser achieves its best perfor-mance when trained on the entire WSJ corpus ei-ther if sentence length restriction is imposed on thetest corpus or not.
The sentence length restrictiontraining protocol of (Spitkovsky et al, 2010), harmsthis parser.
When the parser is trained with theentire WSJ corpus its F-score performance on theWSJ10, WSJ20 and the entire WSJ corpora are 76,64.8 and 56.7 respectively.
When training is donewith WSJ10 (WSJ20) performance degrades to 60(72.2), 37.4 (61.9) and 29.7 (48) respectively.In this paper we introduce the Zoomed Learn-ing (ZL) technique for unsupervised parser training:given a training set T and a test set S, it identifiessubset pairs Ti, Si of T and S such that when theunsupervised parser is trained on a training subsetTi its results on its paired test subset Si are betterthan when it is trained on the entire training set T .
A684successful application of zoomed learning improvesperformance on the full test set S.We describe ZL algorithms of increasing sophis-tication.
In the simplest algorithm the subsets arerandomly selected while in the more sophisticatedversions subset selection is done using a fully unsu-pervised measure of constituency parse tree quality.We apply ZL to the Seginer parser, the best al-gorithm for fully unsupervised constituency parsing.The input is a plain text corpus without any annota-tion, not even POS tagging1, and the output is anunlabeled bracketing for each sentence.We experiment in three different English do-mains: WSJ (economic newspaper), GENIA (biolog-ical articles) and BROWN (heterogeneous domains),and show that ZL improves the parser F-score by asmuch as 4.47%.2 Related WorkUnsupervised parsing has attracted researchers forover a quarter of a century (see (Clark, 2001; Klein,2005) for reviews).
In recent years efforts have beenmade to evaluate the algorithms on manually anno-tated corpora such as the WSJ PennTreebank.
Re-cent works on unlabeled bracketing or dependenciesinduction include (Klein and Manning, 2002; Kleinand Manning, 2004; Dennis, 2005; Bod, 2006a;Bod, 2006b; Bod, 2007; Smith and Eisner, 2006;Seginer, 2007; Cohen et al, 2008; Cohen and Smith,2009; Headden et al, 2009).
Most of the worksabove use POS tag sequences, created either manu-ally or by a supervised algorithm, as input.
The onlyexception is Seginer?s parser, which induces brack-eting from plain text.Our confidence-based ZL algorithms use thePUPA unsupervised parsing quality score (Reichartand Rappoport, 2009b).
As far as we know, PUPA isthe only unsupervised quality assessment algorithmfor syntactic parsers that has been proposed.
Com-bining PUPA with Seginer?s parser thus preserves thefully unsupervised nature of the task.Quality assessment of a learning algorithm?s out-put has been addressed for supervised algorithms1For clarity of exposition, we still refer to this corpus as ourtraining corpus.
In the algorithms presented in this paper, thetest set is included in the training set which is a common prac-tice in unsupervised parsing.
(see (Caruana and Niculescu-Mizil, 2006) for a sur-vey) and specifically for supervised syntactic pars-ing (Yates et al, 2006; Reichart and Rappoport,2007; Ravi et al, 2008; Kawahara and Uchimoto,2008).
All these algorithms are based on manuallyannotated data and thus do not preserve the unsuper-vised nature of the task addressed in this paper.We experiment with the Seginer parser for tworeasons.
First, this is the best algorithm for the taskof fully unsupervised parsing which motivates us toimprove its performance.
Second, this is the onlypublicly available unsupervised parser that inducesconstituency trees.
The PUPA score we use in ourconfidence-based algorithms is applicable for con-stituency trees only.
When additional constituencyparsers will be made available, we will test ZL withthem as well.
Interestingly, the results reported forother constituency models (the CCM model (Kleinand Manning, 2002) and the U-DOP model (Bod,2006a; Bod, 2006b)) are reported when the parser istrained on its test corpus even if the sentences is thatcorpus are of bounded length (e.g.
WSJ10).
Thisraises the question if using more training data (e.g.the entire WSJ) wisely can enhance these models.Recently, Spitkovsky et al, (2010) proposed threeapproaches for improvement of unsupervised gram-mar induction by considering the complexity of thetraining data.
The approaches have been appliedto the DMV unsupervised dependency parser (Kleinand Manning, 2004) and improved its performance.One of these approaches is to train the model withsentences whose length is up to 15 words.
As notedabove, such a training protocol fails to improve theperformance of the Seginer parser.The other approaches in that paper, bootstrappingvia iterated learning of increasingly longer sentencesand a combination of the bootstrapping and the shortsentences approaches, are not directly applicable tothe Seginer parser since its training method cannotbe trivially bootstrapped with parses created in for-mer steps (Seginer, 2007).Related machine learning methods.
ZL is re-lated to ensemble methods.
Both ZL and such meth-ods produce multiple learners, each of them trainedon a different subset of the training data, and decidewhich learner to use for a particular test instance.Bagging (Breiman, 1996) and boosting (Freund andSchapire, 1996), where the experts utilize the same685learning algorithm and differ in the sample of thetraining data they use for its training, were appliedto supervised parsing (Henderson and Brill, 2000;Becker and Osborne, 2005).
In Section 3 we discussthe connection of ZL to boosting.Owing to the fact that ZL produces differentlearners, it is natural to use it in conjunction withan ensemble method, which is what we do in thispaper with our EZL model (Section 3).ZL is also related to active learning (AL) (Cohnand Ladner, 1994).
AL also uses training subset se-lection, with the goal of obtaining a faster learningcurve for an algorithm.
AL is done in supervisedsettings, usually in order to minimize human anno-tation costs.
AL algorithms providing faster learningthan random subset selection for parsing have beenproposed (Reichart and Rappoport, 2009a; Hwa,2004).
However, we are not aware of AL applica-tions in which the overall performance on the testset has been improved.
In addition, our applicationhere is to an unsupervised problem.Algorithms that utilize unsupervised clusteringfor class decomposition in order to improve classi-fiers?
performance (e.g.
(Vilalta and Rish, 2003)) arerelated to ZL.
In such methods, examples that be-long to the same class are clustered, and the inducedclusters are considered as separate classes.
Thesemethods, however, have been applied only to super-vised classification in contrast to our work that ad-dresses unsupervised structured learning.
Moreover,after class decomposition a classifier is trained withthe entire training data while the subsets identifiedby a ZL algorithm are parsed by a parser trained onlywith the sentences they contain.3 Zoomed Learning AlgorithmsZoomed Learning proposes that performance on aparticular test instance might improve if training isdone on a proper subset of the training set.
TheZL view is clearly applicable when the training datais comprised of subsets originating from differentsources having different natures.
If the test data isalso similarly composed, performance on any partic-ular test instance might improve if training is doneon a training subset coming from the same source.However, even when the training and test data arefrom the same source, a ZL algorithm may capturefine differences between subsets.The ZL idea is therefore related to the notions ofin-domain and out-of-domain (domain adaptation).In the former, the training and test data are assumedto originate from the same domain.
In the latter, thetest data comes from a different domain, and there-fore has different statistics from the training data.Indeed, the performance of NLP algorithms in do-main adaptation scenarios is markedly lower than inin-domain ones (McClosky et al, 2006).ZL takes this observation to the extreme, assum-ing that a similar situation might exist even in in-domain scenarios.
After all, a ?domain?
is only acoarse qualification of the nature of a data set.
InNLP, a domain is usually specified as the genre ofthe text involved (e.g., ?newspapers?).
However,there are additional axes that might influence thestatistics obtained from training data, e.g., the syn-tactic nature of sentences.This section presents our ZL algorithms.
We startwith the simplest possible ZL algorithm where thesubsets are randomly selected.
We then describe ZLalgorithms based on quality-based parse selection.We first detail a basic version and then an extendedversion consisting of another level of parse selec-tion.
Finally, we briefly discuss the PUPA qualitymeasure that we use to evaluate the quality of a parsetree.In all versions of the algorithm the input consistsof a set T of N training sentences, a set S ?
T oftest sentences, and an integer number NH ?
N .Zoomed Learning with Random Selection(RZL).
The simplest ZL algorithm randomly assignseach of the training sentences to one of n sets (n = 2in this paper).
More explicitly, the set number isdrawn from a uniform distribution on {1, 2, .
.
.
n}.Each set is then parsed by a parser that is trainedonly with the sentences contained in that set.The intuition behind this algorithm is that differ-ent sets of sentences are likely to manifest differ-ent syntactic patterns.
Consequently, the best way tolearn the syntactic patterns of any given set of sen-tences might be to train the parser on the sentencescontained in the set.While simple, in Section 5 it is shown to improvethe performance of the Seginer parser.The Basic Quality-Based Algorithm (BZL).
Theidea of the basic ZL algorithm is that sentences for686which the parser provides low quality parses man-ifest different syntactic patterns than the sentencesfor which the parser provides high quality parses.The main challenge is therefore to estimate the qual-ity of the produced parses without supervision.The algorithm has three stages.
In the first, wecreate the fully-trained model by training the parserusing all of the N sentences of T .
We then parsethese N sentences using the fully-trained model.In the second, we compute a parse confidencescore for each of the N sentences, based on the Nparses produced in the first stage.
We divide thetraining sentences to two subsets: a high quality sub-set H consisting of the top scored NH sentences,and a lower quality subset L consisting of the otherNL = N ?
NH sentences.As is common practice for this problem (Kleinand Manning, 2004; Seginer, 2007), the test set iscontained in the training set.
This methodology isa valid one because the training set is unannotated.Our test set is thus naturally divided into two sub-sets, a high quality subset HT consisting of the testset sentences contained in H and a lower qualitysubset LT consisting of the test set sentences con-tained in L.In the third stage, each of the test subsets is parsedby a model trained only on its corresponding train-ing subset.
This stage is motivated by our assump-tion that the high and low quality subsets manifestdissimilar syntactic patterns, and consequently thestatistics of the parser?s parameters suitable for onesubset differ from those suitable for another.We compute the confidence score in the secondstage using the unsupervised PUPA algorithm (Re-ichart and Rappoport, 2009b).
POS tags for it areinduced using the fully unsupervised algorithm ofClark (2003).
The parser we experiment with is theincremental parser of Seginer (2007), whose inputconsists of raw sentences and does not include anykind of supervised POS tags (created either manu-ally or by a supervised algorithm).
Consequently,our algorithm is fully unsupervised.
The only pa-rameter it has is NH but ZL improves parser perfor-mance for most NH values.BZL is related to boosting.
In boosting after train-ing one member of the ensemble, examples are re-weighted such that examples that are classified cor-rectly are down-weighted.
BZL does something sim-ilar: it uses PUPA to estimate which sentences aregiven high quality parse trees, and down-weights ex-amples with high (low) PUPA score to 0 when train-ing the L-trained (H-trained) model.
However, inboosting the entire test set is annotated by the samelearning model, while ZL parses each test subsetwith a model trained on its corresponding trainingsubset.The Extended Quality-Based Algorithm (EZL).The basic algorithm produces an ensemble of twoparsing experts: the one trained on H and the onetrained on L. It uses the ensemble to parse the testset by applying the H-trained expert to HT and theL-trained expert to LT .
Naturally, there are otherways to utilize the ensemble to parse the test set.
Inaddition, even if parse trees generated by the expertsare better with high probability than those of thefully trained parser, they are not guaranteed to be so.The fully trained parser is therefore also a valuablemember in the ensemble.
Consequently, we intro-duce an extended zoomed learning algorithm (EZL).The extended version is implemented as a finalfourth stage of the previously described basic algo-rithm.
In this stage, the two test subsets are parsedby the fully trained parsing model, in addition to be-ing parsed by the zooming parsing models.
We nowhave two parses for each test sentence s: PZ(s), theparse created by a parser trained with the sentencescontained in its corresponding training subset, andPF (s), created by the fully trained parser.For each of the two parses of each test sentence,a confidence score is computed by PUPA.
As willbe reviewed below, PUPA uses a set of parsed sen-tences to compute the statistics on which its scoresare based.
Therefore, there are two sources for a dif-ference between the scores of the two parse trees of agiven test sentence: the difference between the treesthemselves, and the difference between the parses ofthe other sentences in the set.The PUPA score for PZ(s) is computed using theparses created for the sentences contained in the testsubset of s by a parser trained with the correspond-ing training subset.
The PUPA score for PF (s) iscomputed using the parses created for the entire testset by the fully trained parser.The algorithm now outputs a final parse by select-ing for each sentence the parse tree having the higherPUPA score.687The PUPA Confidence Score.
In the second andfourth stages of the confidence-based algorithms, anunsupervised confidence score is computed for eachof the induced parse trees.
The confidence scorealgorithm we use is the POS-based UnsupervisedParse Assessment (PUPA) algorithm (Reichart andRappoport, 2009b).
We provide here a brief descrip-tion of this algorithm.The input to PUPA is a set I of parsed sentences,and its output consists of a confidence score in [0, 1]assigned to each sentence in I .The PUPA algorithm collects statistics of the syn-tactic structures (parse tree constituents) containedin the set I of parsed sentences.
The constituent rep-resentation is based on the POS tags of the words inthe yield of the constituent and of the words in theyields of neighboring constituents.
We follow Re-ichart and Rappoport (2009b) and induce the POStags using the fully unsupervised POS induction al-gorithm of Clark (2003).The algorithm then goes over each individual treein the set I and scores it according to the collectedstatistics The PUPA algorithm is guided by the ideathat syntactic structures that are frequently createdby the parser are more likely to be correct than struc-tures the parser produces less frequently.
Therefore,constituents that are more frequent in the set I re-ceive higher scores after proper regularization is ap-plied to prevent potential biases.
The tree score is acombination of the scores of its constituents.Full details of the PUPA algorithm are given in(Reichart and Rappoport, 2009b).
The resultingscore was shown to be strongly correlated with theextrinsic quality of the parse tree, defined to be its F-score similarity to the manually created (gold stan-dard) parse tree of the sentence.4 Experimental SetupWe experimented with three English corpora: theWSJ Penn Treebank (Marcus et al, 1993) consist-ing of economic newspaper texts, the BROWN cor-pus (Francis and Kucera, 1979) consisting of textsof various English genres (e.g.
fiction, humor, ro-mance, mystery and adventure) and the GENIA cor-pus (Kim et al, 2003) consisting of abstracts of sci-entific articles from the biological domain.
All cor-pora were stripped of all annotation (bracketing andPOS tags).For all corpora we report the parser perfor-mance on the entire corpus (WSJ: 49206 sentences,BROWN: 24243 sentences, GENIA: 4661 sentences).For WSJ we also provide an analysis of the per-formance of the parser when applied to sentencesof bounded length.
These sub-corpora are WSJ10(7422 sentences), WSJ20 (25522 sentences) andWSJ40 (47513 sentences) where WSJY denotesthe subset of WSJ containing sentences of length atmost Y (excluding punctuation).Seginer?s parser achieves its best reported resultswhen trained on the full WSJ corpus.
Consequently,for all corpora, we compare the performance of theparser when trained with the ZL algorithms to itsperformance when trained with the full corpus.The POS tags required as input by the PUPA al-gorithm are induced by the fully unsupervised POSinduction algorithm of Clark (2003)2.
Reichart andRappoport (2009b) demonstrated an unsupervisedtechnique for the estimation of the number of in-duced POS tags with which the correlation betweenPUPA?s score and the parse F-score is maximized.When exploring an experimental setup identical toour WSJ setup, they set the number of induced tagsto be 5.
We therefore induced 5 POS tags for eachcorpus, using all its sentences as input for Clark?s al-gorithm.
Our implementation of the PUPA algorithmwill be made available on line.For each corpus we performed K experimentswith each of the three ZL algorithms, where Kequals to the number of sentences in the corpus di-vided by 1000 (rounded upwards).
In each experi-ment the size of the high quality H and lower qualityL training subsets is different.
H consists of the NHtop ranked sentences according to PUPA (or NH ran-domly selected sentences for RZL), with NH chang-ing from 1000 upwards in steps of 1000.
L consistsof the rest of the sentences in the training corpus(WSJ).
The results reported for RZL are averagedover 10 runs.We report the parser performance on the test cor-pus for each training protocol.
Following the un-supervised parsing literature multiple brackets andbrackets covering a single word are not counted, butthe sentence level bracket is.
We exclude punctua-2www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html688WSJ10, F(Full) = 76 WSJ20, F(Full) = 64.82 WSJ40, F(Full) = 57.54 WSJ, F(Full) = 56.7NH 25% 50% 75% 25% 50% 75% 25% 50% 75% 25% 50% 75%EZL 76.38 76.80 76.14 65.75 66.14 65.66 58.32 58.75 58.56 57.47 57.90 57.73+0.38 +0.80 +0.14 +0.93 +1.30 +0.82 +0.78 +1.21 +1.02 +0.77 +1.20 + 1.13BZL 75.07 75.78 75.02 65.08 65.74 64.79 58.13 58.70 58.21 57.30 57.88 57.66-0.93 -0.22 -0.98 +0.26 +0.92 -0.03 +0.59 +1.16 +0.67 +0.60 +1.18 +1.06RZL 75.41 75.00 75.32 64.43 64.66 65.32 57.27 57.63 58.39 56.44 56.84 57.59-0.59 -1.00 -0.68 -0.39 -0.16 +0.50 -0.27 +0.09 +0.85 -0.26 +0.14 +0.89WSJ10 WSJ20 WSJ40 WSJ|LT | 10% 20% 30% 10% 20% 30% 10% 20% 30% 10% 20% 30%EZL 1.32 0.95 0.61 2.98 3.13 1.76 2.60 2.80 2.62 2.44 2.40 2.50BZL 0.37 0.80 0.53 2.38 3.12 1.23 2.34 3.20 3.35 2.28 2.50 3.23RZL -2.10 -1.88 -1.20 -0.91 -0.50 0.72 0.30 0.35 1.50 0.34 0.50 1.60Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIAare shown in Table 2).
Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ.
Toptable: Results for various values of NH (the number of sentences in the high quality training subset).
Evaluationis performed for all sentences in the test corpora.
For each algorithm, the top line is its F-score performance andthe bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)).
The EZLalgorithm is superior.
Bottom table: Results for various lower quality test subsets.
Presented are the differences fromthe F-score of the fully-trained Seginer parser.
The test subsets selected by different algorithms for a specific NHvalue are not necessarily identical and for the sub-corpora they are not necessarily of identical size.
Reported are theimprovements for the LT ?s of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reportsresults for the entire test set, which is why we can report F-scores there).
The LT set size is denoted with |LT |.tion and null elements as in (Klein, 2005).
To evalu-ate the quality of a parse tree with respect to its goldstandard, the unlabeled parsing F-score is used.5 ResultsEntire Corpus Results.
We start by discussingthe effect of ZL on the performance of the Seginerparser when no length restriction is imposed on thetest corpus sentences (WSJ, BROWN and GENIA).Table 1 (top, right section, for WSJ), Figure 1 (topline, right graph, for WSJ), and Table 2 (the left sec-tion of each table, top table for BROWN and bot-tom table for GENIA) present the difference betweenthe F-score performance of the Seginer parser whentrained with the ZL algorithms and the parser?s per-formance when trained with the entire corpus.For all test corpora and sizes of the high qualitytraining subset (NH ), zoomed learning improves theparser performance.
ZL improves the parser perfor-mance by 1.13% (WSJ), 1.46% (BROWN, the numberdoes not appear in the table) and 4.47% (GENIA).For WSJ, the most substantial improvement is pro-vided by EZL, while for BROWN and GENIA the bestresults for some NH values are achieved by BZL andfor others by EZL (and for GENIA with small NHvalues even by RZL).Note, that for all three corpora zoomed learningwith random selection (RZL) improves the parserperformance on the entire test corpus, although to alesser extent than confidence-based ZL.
This is truefor almost all NH values, including those that do notappear in the tables.
See Figure 1 (top line, right-most graph) for WSJ.We follow the unsupervised parsing literature andprovide performance analysis for WSJ sentences ofbounded length (WSJ10, WSJ20 and WSJ40).
Toprevent clutter, for BROWN and GENIA we reportonly entire corpus results.Table 1 (top, left three sections) and Figure 1(top line, three leftmost graphs) present results forWSJ10, WSJ20 and WSJ40.The result patterns for the sub-corpora are similarto those reported for the entire WSJ corpus.
EZL andBZL both improve over the fully-trained parser, and689BROWN ENTIRE CORPUS (F = 57.19) LT HTNH 10% 30% 50% 70% 10% 30% 50% 70% 10% 30% 50% 70%EZL 0.55 0.69 0.64 0.66 0.65 0.82 1.15 1.31 -1.44 -0.03 0.04 0.30BZL 1.11 0.80 0.02 -0.10 1.42 1.20 0.76 0.51 -4.80 -1.30 -0.79 -0.37RZL 0.257 0.755 0.49 0.24 0.23 0.75 0.60 0.53 0.44 0.76 0.42 0.12GENIA ENTIRE CORPUS (F = 42.71) LT HTNH 10% 30% 50% 70% 10% 30% 50% 70% 10% 30% 50% 70%EZL 0.01 0.83 1.10 1.66 -0.01 0.76 0.80 3.37 0.34 1.00 1.40 1.55BZL -0.46 1.40 2.74 4.47 -0.54 0.40 0.96 4.09 0.42 4.29 4.60 5.49RZL 0.61 1.70 2.09 1.99 0.28 2.08 3.30 3.86 3.04 3.22 2.80 1.85Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora.
Results are presented for the entirecorpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset(right column section, HT ) of each corpus, as a function of the high quality training set size (NH).
Since the tablespresent entire corpus results, the training and test subsets are identical.the improvement of the former is more substantial.Baselines.
A key principle of ZL is the selectionof subsets that are better parsed by a parser trainedonly with the sentences they contain than with aparser trained with the entire training corpus.
Toverify the importance of this principle we consid-ered two alternative training protocols.In the first, the entire test corpus is parsed witha parser that was trained with a subset of randomlyselected sentences from the training set.
We run thisprotocol for all three corpora (and for the WSJ sub-corpora) with various training set sizes and obtainedsubstantial degradation in the parser performance.The performance monotonically increases with thetraining set size and reached its maximum when theentire corpus is used.
We conclude that using lesstraining material harms the parser performance if atest subset is not carefully selected.The second protocol is the ?less is more proto-col of Spitkovsky et al, (2010) in which we parsedeach test corpus using a parser that was trained withall training sentences of a bounded length.
Unlikein their paper, in which this protocol improves theperofrmance of the DMV unsupervised dependencyparser (Klein and Manning, 2004), for the Seginerparser the protocol harms the results.
When pars-ing the entire WSJ with a WSJ10-trained parser orwith a WSJ20-trained parser, the F-score results are59.99% and 72.22% compared to 76.00% of thefully-trained parser.
For GENIA the numbers are15.61 and 35.87 compared to 42.71 and for BROWNthey are 36.05 and 50.02 compared to 57.19 3.It is also interesting that sentence length is gen-erally not a good subset selection criterion for ZL.When parsing WSJ10 with a WSJ10-trained parser,F-score results are 59.29 while the F-score of thefully-trained parser on this corpus is 76.00.
Thesame phenomenon is observed with WSJ20 (F-scoreof 61.90 with WSJ20 training and of 64.82 withthe entire WSJ training), and for the BROWN corpus(65.01 vs. 69.43 for BROWN10 and 61.90 vs 62.92for BROWN20).
For GENIA, however, while parsingGENIA10 with a GENIA10-trained parser harms theperformance (45.28 vs. 60.23), parsing GENIA20with a GENIA20-trained parser enhances the perfor-mance (53.23 vs. 50.00).These results emphasize the power of random se-lection for ZL as random selection does provide agood selection criterion.LT vs. HT.
In what follows we analyze the ZLalgorithms aiming to characterize their strengths andweaknesses.Table 1 (bottom), the middle and right sectionsof Table 2 (both tables) and Figure 1 (second andthird lines) present the performance of the ZL algo-rithms on the lower quality and higher quality testsubsets (LT and HT ).
The results patterns for WSJand BROWN are different than those of GENIA.For WSJ (and its sub-corpora) and BROWN,3We repeated this protocol multiple times for each corpus,training the parser with sentences of length 5 to 45 in steps of 5.In all cases we observed performance degradation compared tothe fully-trained parser.6901 2 3 4x 104?2?101WSJ10, Whole CorpusXFScoreDifference0 1 2 3 4 5x 104?0.500.511.5WSJ20, Whole CorpusXFScoreDifference1 2 3 4x 104?0.500.511.5WSJ40, Whole CorpusXFScoreDifference0 1 2 3 4 5x 104?0.500.511.5WSJ, Whole CorpusXFScoreDifference1 2 3 4x 104?3?2?101WSJ10, Low Quality setXFScoreDifference1 2 3 4x 104?4?2024WSJ20, Low Quality setXFScoreDifference1 2 3 4x 104?4?2024WSJ40, Low Quality setXFScoreDifference1 2 3 4x 104?4?2024WSJ, Low Quality setXFScoreDifference1 2 3 4x 104?10?8?6?4?20WSJ10, High Quality setXFScoreDifference1 2 3 4x 104?6?4?201WSJ20, High Quality setXFScoreDifference0 1 2 3 4 5x 104?6?4?201WSJ40, High Quality setXFScoreDifference0 1 2 3 4 5x 104?6?4?201XFScoreDifferenceWSJ, High Quality set1 2 3 4x 104?50510WSJ, Low Quality setXFScore Difference0 1 2 3 4 5x 104?30?20?100WSJ, High Quality setXFScore Difference0 1 2 3 4 5x 104?10123WSJ, Whole CorpusXFScore DifferenceFigure 1: WSJ results.
Top Three Lines: Difference in F-score performance of the Seginer parser between trainingwith ZL and training with the entire WSJ corpus.
Results are presented for the entire corpus (top line), the lowerquality test subset (LT , middle line) and the higher quality test subset (HT , bottom line) as a function of the size ofthe high quality training subset X = NH , measured in sentences.
The curve with triangles is for the extended zoomedlearning algorithm (EZL), the solid curve is for the basic zoomed learning algorithm (BZL) and the dashed curve isfor zoomed learning with random selection (RZL).
Bottom line: Comparison between the performance of the Seginerparser with the EZL algorithm (curves with triangles) and when subset selection is performed using the oracle F-scoreof the trees (solid curves).
F-score differences from the performance of the fully trained parser are presented for theWSJ test corpus as a function of NH , the high quality training subset size.
Oracle selection is superior for the lowerquality subset but inferior for the high quality subset.confidence-based ZL (BZL and EZL) provides a sub-stantial improvement for LT .
For WSJ, F-score im-provement is up to 1.32% (WSJ10), 3.13% (WSJ20),3.35% (WSJ40) and 3.23% (the entire WSJ).
ForBROWN the improvement is up to 1.42%.For HT , confidence-based ZL is less effectivewhen these corpora are considered.
As indicatedin the third line of Figure 1, for WSJ and its sub-corpora, EZL leads to a small improvement on HT ,while BZL generally leads to a performance degra-dation on this test subset.
For BROWN (the right sec-tion of Table 2 (top)), confidence-based ZL gener-ally leads to a performance degradation on HT .For GENIA, EZL and BZL improve the parser per-formance on both LT and HT for most NH values.Understanding this difference is a subject for futureresearch.
Our initial hypothesis is that due to therelative small size of the GENIA corpus (4661 sen-tences compared 24243 and 49206 sentences of WSJand BROWN respectively), there is more room forimprovement in the parser performance on this cor-pus, and consequently ZL improves on both sets.Oracle Analysis.
Confidence-based ZL is basedon the idea that sentences for which the fully-trained691parser provides parses of similar quality manifestsimilar syntactic patterns.
Consequently, the parserperformance on a set of such sentences can be im-proved if it is trained only with the sentences con-tained in the set.
An oracle experiment, where se-lection is based on the F-score computed using thegold standard tree instead of on the PUPA score, canshed light on the validity of this idea.Figure 1 (bottom line) compares the performanceof EZL with that of the oracle-based zoomed learn-ing algorithm when the test corpus is the entire WSJ.For the low quality test subset, oracle selection isdramatically better than confidence-based selection.For the high quality test subset the opposite patternholds, that is, EZL is superior.
These differences leadto the entire corpus pattern where EZL is superior formost NH values.Oracle-based and confidence-based zoomedlearning demonstrate the same trend: they improveover the baseline for LT much more than for HT .For HT , oracle-based ZL even harms results andso does BZL, which does not benefit from theaveraging effect of EZL.
The magnitude of theeffect of oracle-based zoomed learning is muchstronger.
These results support our idea that trainingthe parser on a set selected by a well-designedconfidence test leads to improvement of the parserperformance for the selected sentences when thefully-trained parser produces parses of mediocrequality for them.Integration of the experimental results for zoomedlearning with the three selection methods: random,confidence-based and oracle-based leads to an im-portant conclusion that should guide future research.The more accurate the confidence score used by thezoomed learning algorithm, the more substantial isthe performance improvement for the low qualitytest subset, at the cost of more substantial degrada-tion in the performance on the high quality subset(but recall the different GENIA pattern which shouldbe further explored).EZL Variants.
For confidence-based ZL we ex-plored two methods for utilizing the ensemble mem-bers for generating a final parse tree for each of thetest sentences.
In BZL, the L-trained parser and theH-trained parser generate parse trees for LT andHT sentences respectively.
In EZL, for each sen-tence the final parse is selected between the parsecreated by a parser trained with the sentences con-tained in its corresponding training subset, and theparse created by the fully trained parser.There are other ways to use the ensemble mem-bers.
While for all corpora it is beneficial to usethe L-trained parser for the low quality test subset(LT ), the results for WSJ and BROWN imply that itmight be better to use the fully-trained parser or theEZL algorithm to parse the high quality test subset(HT ).
We have experimented with these methodsand got only a minor improvement over the resultsreported here (improvement is more substantial forBROWN than for WSJ but does not exceed 0.5% forboth).
This can also be inferred from the relativeminor performance degradation of BZL and EZL onHT .We also explored a ZL scenario in which the en-tire test set is parsed either by the H-trained parseror by the L-trained parser.
These protocols result insubstantial degradation in parser performance (com-pared to the fully-trained parser) since the perfor-mance of the H-trained parser on LT and the per-formance of the L-trained parser on HT are poor.6 ConclusionsWe introduced zoomed learning ?
a training algo-rithm for unsupervised parsers.
We applied threevariants of ZL to the best fully unsupervised pars-ing algorithm (Seginer, 2007) and show an improve-ment of up to 4.47% in three English domains: WSJ,BROWN and GENIA.Future research should focus on the developmentof more accurate estimators of parser output qual-ity, and experimentation with different corpora, lan-guages and parsers.Developing a quality assessment algorithm for de-pendency trees will allow us to apply confidence-based ZL to unsupervised dependency parsing.
Par-ticularly, it will enable us to explore the combina-tion of the methods proposed in (Spitkovsky et al,2010) with ZL for the DMV model and to integratethe PUPA score into their bootstrapping algorithm.Another direction is to apply ZL to other NLPtasks and ML areas, supervised and unsupervised.692ReferencesMarkus Becker and Miles Osborne, 2005.
A two-stagemethod for active learning of statistical grammars.
IJ-CAI ?05.Rens Bod, 2006a.
An all-subtrees approach to unsuper-vised parsing.
ACL-COLING ?06.Rens Bod, 2006b.
Unsupervised parsing with U-DOP.CoNLL ?06.Rens Bod, 2007.
Is the end of supervised parsing insight?
ACL ?07.Leo Breiman, 1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Rich Caruana and Alexandru Niculescu-Mizil, 2006.An empirical comparison of supervised learning algo-rithms.
ICML ?06.Alexander Clark, 2001.
Unsupervised language acquisi-tion: theory and practice.
Ph.D. thesis, University ofSussex.Alexander Clark, 2003.
Combining distributional andmorphological information for part of speech induc-tion.
EACL ?03.Shay Cohen, Kevin Gimpel and Noah Smith, 2008.Logistic normal priors for unsupervised probabilisticgrammar induction.
NIPS ?08.Shay Cohen and Noah Smith, 2009.
Shared logistic nor-mal distributions for soft parameter tying in unsuper-vised grammar induction.
NAACL ?09.David Cohn, Les Atlas and Richard Ladner.
1994.
Im-proving generalization with active learning.
MachineLearning, 15(2):201?221.Simon Dennis, 2005.
An exemplar-based approach tounsupervised parsing.
CogSci ?05.W.
N. Francis and H. Kucera 1979.
Manual of infor-mation to accompany a standard corpus of present-dayedited American English, for use with digital com-puters.
Department of Linguistics, Brown UniversityPress, Providence, RI.Yoav Freund and Robert E. Schapire, 1996.
Experimentswith a new boosting algorithm.
ICML ?96.William Headden III, Mark Johnson and David Mc-Closky, 2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
NAACL?09.John Henderson and Eric Brill, 2000.
Bagging andboosting a treebank parser.
NAACL ?00.Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30(3):253?276.Daisuke Kawahara and Kiyotaka Uchimoto 2008.Learning reliability of parses for domain adaptation ofdependency parsing.
IJCNLP ?08.Dan Klein and Christopher Manning, 2002.
A genera-tive constituent-context model for improved grammarinduction.
ACL ?02.Dan Klein and Christopher Manning, 2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
ACL ?04.Dan Klein, 2005.
The unsupervised learning of naturallanguage structure.
Ph.D. thesis, Stanford University.Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and Jun?ichiTsujii, 2003.
GENIA corpus ?
a semantically anno-tated corpus for bio-textmining.
Bioinformatics, (sup-plement: 11th ISMB) 19:i180?i182, Oxford Univer-sity Press, 2003.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark Johnson,2006.
Reranking and self-training for parser adapta-tion.
ACL-COLING ?06.Sujith Ravi, Kevin Knight and Radu Soricut, 2008.
Au-tomatic prediction of parser accuracy.
EMNLP ?08.Roi Reichart and Ari Rappoport, 2007.
An ensemblemethod for selection of high quality parses.
ACL ?07.Roi Reichart and Ari Rappoport, 2009a.
Sample se-lection for statistical parsers: cognitively driven algo-rithms and evaluation measures.
CoNLL ?09.Roi Reichart and Ari Rappoport, 2009b.
Automatic se-lection of high quality parses created by a fully unsu-pervised parser.
CoNLL ?09.Yoav Seginer, 2007.
Fast unsupervised incremental pars-ing.
ACL ?07.Noah Smith and Jason Eisner, 2006.
Annealing struc-tural bias in multilingual weighted grammar induction.ACL-COLING ?06.Valentin Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky, 2010.
From baby steps to leapfrog: how ?less ismore?
in unsupervised dependency parsing.
NAACL?10.Ricardo Vilalta and Irina Rish, 2003.
A decompositionof classes via clustering to explain and improve naivebayes.
ECML ?03.Alexander Yates, Stefan Schoenmackers and Oren Et-zioni, 2006.
Detecting parser errors using web-basedsemantic filters .
EMNLP ?06.693
