IIBi//////B|//BBBAutomat ion  o f  T reebank  Annotat ionThorsten Brants and Wojciech SkutUniversit?t des SaarlandesComputational LinguisticsD-66041 Saarbrficken, Germany{brant s, skut }Qcoli.
uni-sb, de?..
AbstractThis paper describes applications of stochasticand symbolic NLP methods to treebank anno-tation.
In paxticular we focus on (1) the au-tomation of treebank annotation, (2) the com-parison of conflicting annotations for the samesentence and (3) the automatic detection of in-consistencies.
These techniques are currentlyemployed for building a German treebank.1 IntroductionThe emergence of new statistical NLP methods in-creases the demand for corpora nnotated with syn-tactic structures.
The construction of such a cor-pus (a treebank) is a time-consuming task that canhardly be carried out unless ome annotation work isautomated.
Purely automatic annotation, however,is not reliable enough to be employed without someform of human supervision and hand-correction.This interactive annotation strategy requires toolsfor error detection and consistency checking.The present paper eviews our experience with thedevelopment of automatic annotation tools whichare currently used for building a corpus of Germannewspaper text.The next section gives an overview of the annota-tion format.
Section 3 describes three applicationsof statistical NLP methods to treebank annotation.Finally, section 4 discusses mechanisms for compar-ing structures assigned by different annotators.2 Annotating Argument Structure2.1 Annotat ion SchemeUnlike most treebanks of English, our corpus is an-notated with predicate-argumenl s~ructures and notphrase-structure tr es.
The reason is the free wordorder in German, a feature seriously affecting thetransparency of traditional phrase structures.
Thuslocal and non-local dependencies are represented inDar'uberPROAVrnu's nachgedacht werdenVMFIN VVPP VAINFmust thought-over be'it has to be thought over'about-it$.Figure h Sample structure from the Treebankthe same way, at the cost of allowing crossing treebranches, as shown in figure 11 .Such a direct representation of the predicate-argument relation makes annotation easier than itwould be if additional trace-filler co-references wereused for encoding discontinuous constituents.
Fur-thermore, our scheme facilitates automatic extrac-tion of valence frames and the construction of se-mantic representations.On the other hand, the predicate-argument struc-tures used for annotating our corpus can still be con-verted automatically into phrase-structure trees ifnecessary, cf.
(Skut et al, 1997a).
For more detailson the annotation scheme v. (Skut et al, 1997b).2.2 The Annotat ion ModeIn order to make annotation more reliable, each sen-tence is annotated independently b  two annotators.Afterwards, the results are compared, and both an-notators have to agree on a unique structure.
In1 The nodes and edges are labeled with category andfunction symbols, respectively (see appendix A).Brants and Skut 49 Automation of Treebank AnnotationThorsten Brants and Wojciech Skut (1998) Automation of Treebank Annotation.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 49-57.case of persistent disagreement or uncertainty, thegrammarian supervising the annotation work is con-sulted.It has turned out that comparing annotationsinvolves significantly more effort than annotationproper.
As we do not want to abandon the annotate-and-compare strategy, additional effort has been putinto the development of tools supporting the com-parison of annotated structures (see section 4).3 Automat ionThe efficiency of annotation can be significantly in-creased by using automatic annotation tools.
Never-theless, some form of human supervision and hand-correction is necessary to ensure sufficient reliabil-ity.
As pointed out by (Marcus, Santorini, andMarcinkiewicz, 1994), such a semi-automatic anno-tation strategy turns out to be superior to purelymanual annotation in terms of accuracy and effi-ciency.
Thus in most treebank projects, the task ofthe annotators consists in correcting the output of aparser, cf.
(Marcus, Santorini, and Marcinkiewicz,1994), (Black et al, 1996).As for our project, the unavailability ofbroad-coverage argument-structure and dependencyparsers made us adopt a bootstrapping strategy.Having started with completely manual annotation,we axe gradually increasing the degree of automa-tion.
The corpus annotated so far serves as train-ing material for annotation tools based on statis-tical NLP methods, and the degree of automationincreases with the amount of annotated sentences.Automatic processing and manual input are com-bined interactively: the annotator specifies ome in-formation, another piece of information is added au-tomatically, the annotator adds new information orcorrects parts of the structure, new parts are addedautomatically, and so on.
The size and type of suchannotation increments depends on the size of thetraining corpus.
Currently, manual annotation con-sists in specifying the hierarchical structure, whereascategory and function labels as well as simple sub-structures are assigned automatically.
These au-tomation steps are described in the following sec-tions.3.1 Tagging Grammatical FunctionsAssigning grammatical functions to a given hierar-chical structure is based on a generalization f stan-dard paxt-of-speech tagging techniques.In contrast o a standard probabilistic POS tag-ger (e.g.
(Cutting et al, 1992; Feldweg, 1995)), thetagger for grammatical functions works with lexical(1)Selbst besuchtADV VVPPhimself visitedhat Peter SabineVAFIN NE NEhas Peter Sabine'Peter never visited Sabine himself'lhieADVneverFigure 2: Example sentenceand contextual probability measures PO.
(') depend-ing on the category of a mother node (Q).
This ad-ditional parameter is necessary since the sequence ofgrammatical functions depends heavily on the typeof phrase in which it occurs.
Thus each category (S,VP, NP, PP etc.)
defines a separate Markov model.Under this perspective, categories of daughternodes correspond to the outputs of a Markov model(i.e., like words in POS tagging).
Grammatical func-tions can be viewed as states of the model, analo-gously to tags in a standard part-of-speech tagger.Given a sequence of word and phrase categoriesT = T1...Tk and a parent category Q, we cal-culate the sequence of grammatical functions G =G1.. .
Gk that link T and Q asaxgmax PQ( a\[T) (1)GPQ(G).
Po(TIG) = axgmax a PQ(T)= axgmaxPo(a ) ?
Pq(TIG)GAssuming the Markov property we havekPQ(TIG) = I"I PQ(T~IG,) (2)i----1and (using a trigram model)kpo(G) : IX Po(V, IGi-2, G,-1) (a)i=1The contexts are smoothed by linear interpolationof unigrams, bigrams, and trigrams.
Their weightsare calculated by deleted interpolation (Brown et al,1992).Brants and Skut 50 Automation of Treebank.4nnotationIIIiIIIIIII!IIIIIIIII!IIIIIIIIIIIII!IIIIThe structure of a sample sentence is shown jnfigure 2.
Here, the probability of the S node havingthis particular sequence of children is calculated asPs(G,T) = Ps(OCl$,$)-Ps(VPlOC)?
Ps(HDI$, OC).
Ps(VAFINIHD)?
Ps(SBIOC, HD)-Ps(NEISB )-Ps(NGIHD, SB)- Ps(ADVING )($ indicates the start of the sequence).The predictions of the tagger are correct in ap-prox.
94% of all cases.
During the annotation pro-cess this is further increased by exploiting a preci-sion/recall trade-off (cf.
section 3.5).3.2 Tagging Phrasa l  CategoriesThe second level of automation is the recognition ofphrasal categories, which frees the annotator fromtyping phrase labels.
The task is performed by anextension of the grammatical function tagger pre-sented in the previous ection.Recall that each phrasal category defines a dif-ferent Markov model.
Given the categories of thechildren nodes in a phrase, we can run these modelsin parallel.
The model that assigns the most prob-able sequence of grammatical functions determinesthe category label to be assigned to the parent node.Formally, we calculate the phrase category Q (andat the same time the sequence of grammatical func-tions G = G1 .. .
Gk) on the basis of the sequence ofdaughters 7" = T1 ... Tk withargmax maxPQ(GIT).
Q 6This procedure can also be performed using onelarge {combined) Markov model that enables a veryefficient calculation of the maximum.The overall accuracy of this approach is 95%.3.3 Tagging Hierarchical  StructureThe next automation step is the recognition of syn-tactic structures.
In general, this task is much moredifficult than assigning category and function labels,and requires a significantly larger training corpusthan the one currently available.
What can be doneat the present stage is the recognition of relativelysimple structures uch as NPs and PPs.
(Church, 1988) used a simple mechanism to markthe boundaries of NPs.
He used part-of-speech tag-ging and added two flags to the part-of-speech tagsto mark the beginning and the end of an NP.Our goal is more ambitious in that we mark notonly the phrase boundaries of NPs but also the com-plete structure of a wider class of phrases, startingwith APs, NPs and PPs.em DichterART NN1 +a poetE~in Tel Aviv lebenderAPPIq NE NE ADJA- - - -  - -  0 4--1.in Tel Aviv living'a poet living in Tel Aviv'Figure 3: Structural tags(Ratnaparkhi, 1997) uses an iterative procedure toassign two types of tags (start X and join X, whereX denotes the type of the phrase) combined with aprocess to build trees.We go one step further and assign simple struc-tures in one pass.
Furthermore, the nodes andbranches of these tree chunks have to be assignedcategory and function labels.The basic idea is to encode structures of lim-ited depth using a finite number of tags.
Given asequence of words (w0, wl .... wn/, we consider thestructural relation ri holding between wi and wi-1for 1 < i < n. For the recognition of NPs and PPs,it is sufficient o distinguish the following seven val-ues of rl which uniquely identify sub-structures oflimited depth.ri --0 if parent(wi) =parent(wi_l)+ if parent(wi) =parent2(wi_l)++ if parent(wi) = parentZ(wi_a)- if parent2(wi) =parent(wi_l)- -  if parentZ(wi) = parent(wi_l)= if parent2(wi) = parentg-(wi_l)1 elseIf more than one of the conditions above are met,the first of the corresponding tags in the list is as-signed.
A structure tagged with these symbols isshown in figure 3.In addition, we encode the POS tag ti assigned tow~.
On the basis of these two pieces of informationwe define structural tags as pairs Si = (r i ,  ti).
SuchBrants and Skut 51 Automation of Treebank Annotationtags constitute a finite alphabet of symbols describ-ing the structure and syntactic ategory of phrasesof depth < 3.The task is to assign the most probable sequenceof structural tags ((So, $1, ..., Sn)) to a sequence ofpart-of-speech tags (To, T1, ..., Tn).Given a sequence of part-of-speech tags T =T1 .. .
T~, we calculate the sequence of structural tagsS = $1 ... Sk such thatargmax P( S\]T) (4)sP(S) - P(TIS)= argmax s P(T)= argmaxP(S) .
P(TIS)SThe part-of-speech tags are encoded in the struc-tural tag (t), so S uniquely determines T. Therefore,we have P(T\[S) = 1 ifTi = ii and 0 otherwise, whichsimplifies calculations:argmax P(S).
P(T\[S) (5)s= argmax H P(SiISi-2, Si-1)P(TdSOS i=1As in the previous models, the contexts aresmoothed by linear interpolation of unigrams, bi-grams, and trigrams.
Their weights are calculatedby deleted interpolation.This chunk tagging technique can be applied totreebank annotation in two ways.
Firstly, we coulduse it as a preprocessor; the annotator would thencomplete and correct he output of the chunk tagger.The second alternative isto combine this chunkingwith manual input in an interactive way.
Then theannotator has to determine the boundaries of thesub-structure that is to be build by the program.Obviously, the second solution is favorable sincethe user supplies information about chunk bound-aries, while in the preprocessing mode the tagger hasto find both the boundaries and the internal struc-ture of the chunks.The assignment of structural tags is correct inmore than 94% of the cases.
For detailed resultssee section 3.6.3.3.4 Interaction and AlternationTo illustrate the interaction of manual input and theautomatic annotation techniques described above,we show the way in which the structure in figure3 is constructed.
The current version of the annota-tion tool supports automatic assignment of categoryand phrase labels, so the user has to specify the hi-erarchical structure step by step 2.The starting point is the plain string of words to-gether with their part-of-speech tags.
The annota-tor first selects the words Tel Aviv and executes thecommand "group" (this is all done with the mouse).Then the program inserts the category label MPN(multi-lexeme proper noun) and assigns the gram-matical function PNC (proper noun component) oboth words (cf.
sections 3.2 and 3.1).Having completed the first sub-structure, the an-notator selects the newly created MPN and thepreposition in, and creates a new phrase.
Thetool automatically inserts the phrase label PP andthe grammatical functions AC (adpositional casemarker) and NK (noun kernel component).
The fol-lowing two steps are to determine the componentsof the AP and, finally, those of the NP.At any time, the annotator has the opportunityto change and correct entries made by the program.This interactive annotation mode is favorablefrom the point of view of consistency checking.
Thefirst reason is that the annotation increments arerather small, so the annotator corrects not an entireparse tree, but a fairly simple local structure.
Theautomatic assignment of phrase and function labelsis generally more reliable than manual input becauseit is free of typically human errors (see the precisionresults in (Brants, Skut, and Krenn, 1997)).
Thusthe annotator can concentrate on the more difficulttask, i.e., building complex syntactic structures.The second reason is that errors corrected atlower levels in the structure facilitate the recogni-tion of structures at higher levels, thus many wrongreadings are excluded by confirming or correcting achoice at a lower level.The partial automation of the annotation process(automatic regocnition of phrase labels and gram-matical functions) has reduced the average anno-tation time from about 10 to 1.5 - 2 minutes persentence, i.e.
600 - 800 tokens per minute, whichis comparable to the figures published by the cre-ators of the Penn Treebank in (Marcus, Santorini,and Marcinkiewicz, 1994).The test version of the annotation tool using thestatistical chunking technique described in section3.3 permits even larger annotation increments andwe expect a further increase in annotation speed.The user just has to select he words constituting an~The chunk tagger has not yet been fully integratedinto the annotation tool.Brants and Skut 52 Automation of Treebank AnnotationII!iIIIIIIIIIIIIIIilIIIIIIIIIIIIIImmmmmmmmmm|mmmmmmmNP or PP.
The program assigns a sequence of struc-tural tags to them; these tags are then converted toa tree structure and all labels are inserted.3.5 ttel iabi l i tyTo make automatic annotation more reliable, theprogram assigning labels performs an additional re-liability check.
We do not only calculate the bestassignment, but also the second-best alternative andits probability.
If the probability of the alternativecomes very close to that of the best sequence of la-bels, we regard the choice as unreliable, and the an-notator is asked for confirmation.Currently, we employ three reliability levels, ex-pressed by quotients of probabilities Pbest/Psecond-If this quotient is close to one (i.e., smaller thansome threshold 01), the decision counts as unreli-able, and annotation is left to the annotator.
If thequotient is very large (i.e., greater than some thresh-old 02 > 91), the decision is regarded as reliable andthe respective annotation is made by the program.If the quotient fails between 91 and 02, the decisionis tagged as "almost reliable".
The annotation isinserted by the program, but has to be confirmed bythe annotator.This method enables the detection of a number oferrors that are likely to be missed if the annotatoris not asked for confirmation.The results of using these reliability levels are re-ported in the experiments section below.3.6 ExperimentsThis section reports on the accuracy achieved by themethods described in the previous ections.At present, our corpus contains approx.
6300 sen-tences (115,000 tokens) of German newspaper text(Frankfurter Rundschan).
Results of tagging gram-matical functions and phrase categories have im-proved slightly compared to those reported for asmaller corpus of approx.
1200 sentences (Brants,Skut, and Krenn, 1997).
Accuracy figures for tag-ging the hierarchical structure are published for thefirst time.For each experiment, the corpus was divided intotwo disjoint parts: 90% training data and 10% testdata.
This procedure was repeated ten times, andthe results were averaged.The thresholds 01 and 02 determining the reliabil-ity levels were set to 91 = 5 and 02 = 100.3.6.1 Grammat ica l  FunctionsWe employ the technique described in section 3.1to assign grammatical functions to a structure de-fined by an annotator.
Grammatical functions areTable 1: Levels of reliability and the percentage ofcases in which the tagger assigned a correct gram-matical function (or would have assigned ifa decisionhad been forced).grammaticalfunctionreliablemarkedunreliableoverallcases correct88% 97.0%8% 85.0%4% 59.5%100% 94.6%Table 2: Levels of reliability and the percentage ofcases in which the tagger assigned a correct phrasecategory (or would have assigned it if a decision hadbeen forced).phrasecategoryreliablemarkedunreliableoverallcases correct76% 99.0%19% 91.5%5% 56.7%100% 95.4%represented by edge labels.
Additionally, we exploitthe recall/accuracy tradeoff as described in section3.5.
The tagset of grammatical functions consists of45 tags.Tagging results are shown in table 1.
Overall ac-curacy is 94.6%.
88% of all predictions are classifiedas reliable, which is the most important class forthe actual annotation task.
Accuracy in this classis 97.0%.
It depends on the category of the phrase,e.g.
accuracy for reliable cases reaches 99% for 51Psand PPs.3.6.2 Phrasal CategoriesNow the task is to assign phrasal categories to astructure specified by the annotator, i.e., only thehierarchical structure is given.
We employ the tech-nique of competing Markov models as described insection 3.2 to assign phrase categories to the struc-ture.
Additionally, we compute alternatives to as-sign one of the three reliability levels to each decisionas described in section 3.5.
The tagset for phrasalcategories consists of 25 tags.As can be seen from table 2, the results of assign-ing phrasal categories are even better than those ofassigning grammatical functions.
Overall accuracyis 95.4%.
Tags that are regarded as reliable (76% ofall cases) have an accuracy of 99.0%, which resultsBrants and Skut 53 Automation of Treebank AnnotationTable 3: Chunk tagger accuracy with respect o hi-erarchical structure.structuraltagsreliablemarkedunreliableoverallcases correct86% 95.8%11% 93.2%3% 67.0%100% 94.4%in very reliable annotations.3.6.3 Chunk  TaggerThe chunk tagger described in section 3.3 assignstags encoding structural information to a sequenceof words and tags.
The accuracy figures presentedhere refer to the correct assignments of these tags(see table 3).The assignment ofstructural tags allows us to con-struct a tree; the labels are afterwards assigned ina bottom-up fashion by the function/category labeltagger described in earlier sections.Overall accuracy is 94.4% and reaches 95.8% inthe reliable cases.A different measure of the chunker's correctness ithe percentage of complete phrases recognized cor-rectly.
In order to determine this percentage, weextracted all chunks of the maximal depth recogniz-able by the chunker.
In a cross evaluation, 87.3% ofthese chunks were recognized correctly as far as thehierarchical structure is concerned.4 Compar ing  TreesAnnotations produced by different annotators arecompared automatically and differences are marked.The output of the comparison is given to the anno-tators.
First, each of the annotators goes throughthe differences on his own and corrects obvious er-rors.
Then remaining differences are resolved in adiscussion of the annotators.Additionally, the program calculates the proba-bilities of the two different annotations.
This is in-tended to be a first step towards resolving conflictingannotations automatically.
Both parts, tree match-ing and the calculation of probabilities for completetrees are described in the following sections.4.1 Tree Match ingThe problem addressed here is the comparison oftwo syntactic structures that share identical termi-nal nodes (the words of the annotated sentence).proc compare(A, B)for each non-terminal node X in A:search node Y in Bsuch that yield(X) = yield(Y)i f  Y exists:emit different labels if anyif Y does not exist:emit X and its yieldendendFigure 4: Basic asymmetric algorithm to compareannotation A with annotation B of the same sen-tence(Calder, 1997) presents a method of comparingthe structure of context free trees found in differ-ent annotations.
This section presents an extensionof this algorithm that compares predicate-argumentstructures possibly containing crossing branches (cf.figure 2).
Node and edge labels, representing phrasalcategories and grammatical functions, are also takeninto account.Phrasal (non-terminal) nodes are compared onthe basis of their yields: the yield of a nontermi-nal node X in an annotation A is the ordered setof terminals that are (directly or indirectly) dom-inated by X.
The yield need not be contiguoussince predicate-argument structures allow discontin-uous constituents.If both annotations contain nonterminal nodesthat cover the same terminal nodes, the labels of thenonterminal nodes and their edges are compared.This results in a combined measure of structuraland labeling differences, which is very useful incleaning the corpus and keeping track of the devel-opment of the treebank.We use the basic algorithm shown in figure 4 todetermine the differences in two annotations A andB.
The basic form is asymmetric.
Therefore, a com-plete comparison consists of two runs, one for eachdirection, and the outputs of both runs are com-bined.Figures 5 and 6 show examples of the output ofthe algorithm.
These outputs can be directly usedto mark the corresponding nodes and edges.The yield is sufficient o uniquely determine cor-responding nodes since the annotations used here donot contain unary branching nodes.
If unary branch-ing occurs, both the parent and the child have thesame terminal yield and further mechanism to de-termine corresponding nodes are needed.
(Calder,1997) points out possible solutions to this problem.Brants and Skut 54 Automation of Treebank AnnotationIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIImmmmmmmmmmmmmmmmmmm/2 t- tSelbst besucht hat Peter0 1 2 3ADV VVPP VAFIN NEhimself visited has Peter Sabine'Peter never visited Sabine himself'++Sabine nie4 5NE ADVneversentence I errors I(1) structure: 500 VP lOCI 0 1 4(Selbst besucht Sabine)(2) structure: 500 VP lOCI 0 I 4 S(Selbst besucht Sabine hie)Figure 5: Erroneous annotation (2) of the examplesentence in figure 2 (hie should be attached to Sinstead of VP), together with the output of the treecomparison algorithm?
All nodes are numbered toenable identification.
Additionally, this output canbe used to highlight the corresponding nodes andedges.
(3)Selbst0ADVhimselfbesucht hat Peter Sabine1 2 3 4VVPP VAFIN NE NEvisited has Peter Sabine'Peter never visited Sabine himself'?nie5ADVneversentence 1 errors 1(1) edge: 5 (ADV) \[NG\] nie(3) edge: 5 (ADV) \[MO\] hieFigure 6: Erroneous annotation (3) of the examplesentence in figure 2 (nie should have grammaticalfunction NG instead of MO), together with the out-put of the tree comparison algorithm.4.2 Probabi l i t iesThe probabilities of each sub-structure ofdepth oneare calculated separately according to the model de-scribed in sections 3.1 and 3.2.
Subsequently, theproduct of these probabilities i used as a scoringfunction for the complete structure?
This method isbased on the assumption that productions at differ-ent levels in a structure are independent, which isinherent o context free rules.Using the formulas from sections 3.1 and 3.2, theprobability P(A) of an annotation A is evaluated asP(A) = HP(Qi)i= lnnt= H PQ,(T,, G,)i--1rtnt ki= 1\] 1\] Poi(gi,ylg,,~-2, g/,~-l)i= l j= l?
PQ(ti,y I gij)A annotation (structure) for a sentencennt number of nonterminal nodes in Ant number of terminal nodes in An number of nodes = nnt + ntQi ith phrase in AT/ sequence of tags in QiGi sequence of gramm, func.
in Qiki number of elements in Qitij tag of j th child in Qigl,i grammatical function of jthchild in QiProbabilities computed in this way cannot be useddirectly to compare two annotations since they favorannotations with fewer nodes.
Each new nontermi-nal node introduces a new element in the productand makes it smaller.Therefore, we normalize the probabilities w.r.t.the number of nodes in the annotation, which yieldsthe perplexity PP(A) of an annotation A:PP(A)=~'p~A) (6)4.3 Appl icat ion to a CorpusThe procedures of tree matching and probability cal-culation were applied to our corpus, which currentlyconsists of approx.
6300 sentences (115,000 tokens)of German newspaper text, each sentence annotatedat least twice.We measured the agreement of independent anno-tations after first annotation but before correctionBrants and Skut 55 Automation of Treebank AnnotationTable 4: Comparison of independent semi-automaticannotations (1) after first, independent annotationand (2) after comparison but before the final discus-sion (current stage).word level:(1) ident, parent node(2) ident, gram.
func.node level:(3) identical nodes(4) identical nodes/labels(5) ident, node/gram, func.sentence  level:(6) identical structure(7) identical annotation<1> (2>92.3% 98.7%93.8% 99.1%87.6% 98.1%84.2% 97.4%76.6% 96.3%48.6% 90.8%34.6% 87.9%(1), and after correction but before the final discus-sion (2), which is the current stage of the corpus.The results are shown in table 4.As for measuring differences, we can count themat word, node and sentence l vel.At the word level, we are interested in (1) thenumber of correctly assigned parent categories (doesa word belong to a PP, NP, etc.?
), and (2) the num-ber of correctly assigned grammatical functions (isa word a head, modifier, subject, etc.?
).At the node level (non-terminals, phrases) wemeasure (3) the number of identical nodes, i.e., ifthere is a node in one annotation, we check whetherit corresponds to a node in the other annotation hav-ing the same yield.
Additionally, we count (4) thenumber of identical nodes having the same phrasalcategory, and (5) the number of identical nodes hav-ing the same phrasal category and the same gram-matical function within its parent phrase.At the sentence l vel, we measure (6) the numberof annotated sentences having the same structure,and, which is the strictest measure, (7) the numberof sentences having the same structure and the samelabels (i.e., exactly the same annotation).At the node level, we find 87.6% agreement in in-dependent annotations.
A large amount of the dif-ferences come from misinterpretation f the annota-tion guidelines by the annotators and are eliminatedafter comparison, which results in 98.1% agreement.This kind of comparison is the one most frequentlyused in the statistical parsing community for com-paring parser output.The sentence level is the strictest measure, andthe agreement is low (34.6% identical annotationsafter first annotation, 87.9% after comparison).
Butat this level, one error (e.g.
a wrong label) rendersTable 5: Using model perplexities to compare dif-ferent annotations: Accuracy of using the hypothe-sis that a correct annotation has a lower perplexitythan a wrong annotation.recall precision30% 95.3%45% 92.2%60% 88.6%85% 81.4%100% 65.8%the whole annotation to be wrong and the sentencecounts as an error.If we make the assumption that a correct anno-tation always has a lower perplexity than a wrongannotation for the same sentence, the system wouldmake a correct decision for 65.8% of the sentences(see table 5, last row).For approx.
70% of all sentences, at least oneof the initial annotations was completely correct.This means that the two initial annotations and theautomatic omparison yield a corpus with approx.65.8% ?
70% = 46% completely correct annotations(complete structure and all tags).One can further increase precision at the cost ofrecall by requiring the difference in perplexity to ex-ceed some minimum distance.
This precision/recalltradeoff is also shown in table 5.5 Conc lus ionThe techniques and automatic tools described in thispaper are designed to support annotation proper,online/offline consistency checking and the compar-ison of independent annotations of the same sen-tences.
Most of the techniques employ stochasticprocessing methods, which guarantee high accuracyand robustness.The bootstrapping approach adopted in ourproject makes the degree of automation a function ofavailable training data.
Easier processing tasks areautomated first.
Experience gained and data anno-tated at a lower level allow to increase the level ofautomation step by step.
The current size of ourcorpus (approx.
6300 sentences) enables reliable au-tomatic assignment of category and function labelsas well as simple structures.Future work will be concerned with developingautomatic annotation methods handling complexstructures, which should ultimately lead to the de-velopment of a parser for predicate-argument treescontaining crossing branches.Brants and Skut 56 Automation of Treebank AnnotationII!1IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII|I|||IIIIIIIII6 AcknowledgementsThis work is part of the DFG Sonderforschungs-bereich 378 Resource-Adaptive Cognitive Processes,Project C3 Concurrent'Grammar Processing.We wish to thank the universities of Stuttgartand Tiibingen for kindly providing us with a hand-corrected part-of-speech tagged corpus.
We alsowish to thank Oliver Plaehn, who did a great jobin implementing the annotation tool, and PeterSch~ifer, who built the tree comparison tool.
Specialthanks go to the five annotators continually increas-ing the size and the quality of our corpus.
And fi-nally, we thank Sabine Kramp for proof-reading thispaper.ReferencesBlack, Ezra, Stephen Eubank, Hideki Kashioka,David Magerman, Roger Garside, and GeoffreyLeech.
1996.
Beyond skeleton parsing: Producinga comprehensive large-scale general-English tree-bank with full grammatical nalysis.
In Proc.
ofCOLING-96, pages 107-113, Kopenhagen, Den-mark.Brants, Thorsten, Wojciech Skut, and BrigitteKrenn.
1997.
Tagging rammatical functions.
InProceedings of EMNLP-97, Providence, RI, USA.Brown, P. F., V. J. Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Com-putational Linguistics, 18(4):467-479.Calder, Jo.
1997.
On aligning trees.
In Proc.
ofEMNLP-97, Providence, RI, USA.Church, Kenneth Ward.
1988.
A stochastic partsprogram and noun phrase parser for unrestrictedtext.
In Proc.
Second Conference on Applied Nat-ural Language Processing, pages 136-143, Austin,Texas, USA.Cutting, Doug, Julian Kupiec, Jan Pedersen, andPenelope Sibun.
1992.
A practical part-of-speechtagger.
In Proceedings of the 3rd Conferenceon Applied Natural Language Processing (ACL),pages 133-140.Feldweg, Helmut.
1995.
Implementation a d eval-uation of a german hmm for pos disambiguation.In Proceedings of EACL-SIGDAT-95 Workshop,Dublin, Ireland.Marcus, Mitchell, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of English: the Penn Treebank.
In Su-san Armstrong, editor, Using Large Corpora.
MITPress.Ratnaparkhi, Adwait.
1997.
A linear observedtime statistical parser based on maximum entropymodels."
In Proceedings of EMNLP.97, Provi-dence, RI, USA.Skut, Wojciech, Thorsten Brants, Brigitte Krenn,and Hans Uszkoreit.
1997a.
Annotating unre-stricted German text.
In FacMagung der SektionComputerlinguistik der Deutschen Gesellschaftfffr Sprachwissenschafl, Heidelberg, Germany.Skut, Wojciech, Brigitte Krenn, Thorsten Brants,and Hans Uszkoreit.
1997b.
An annotationscheme for free word order languages.
In Proceed-ings of ANLP-97, Washington, DC.Thielen, Christine and Anne Schiller.
1995.
Einkleines und erweitertes Tagset f/its Deutsche.In Tagungsberichte des Arbeitstreffens Lezikon+ Text 17./18.
Februar 1994, Schlofl Hohen-t(~bingen.
Lezicographica Series Maior, Tfibingen.Niemeyer.Appendix A: TagsetsThis section contains descriptions of tags used in thispaper.
These are not complete lists.A.1 Part-of-Speech TagsWe use the Stuttgart-T/ibingen-Tagset.
The com-plete set is described in (Thielen and Schiller, 1995).ADJA attributive adjectiveADV adverb "APPR prepositionART articleNE proper nounNN common nounPROAV pronominal adverbVAFIN finite auxiliaryVAINF infinite auxiliaryVMFIN finite modal verbVVPP  past participle of main verbA.2 Phrasal CategoriesAP adjective phraseMPN multi-word proper nounNP noun phrasePP prepositional phraseS sentenceVP verb phraseA.3 Grammatical FunctionsAC adpositional c se markerHD headMO modifierNG negationNK noun kernelOA accusative objectOC dausal objectPNC proper noun componentSB subjectBran ts and Skut 5 7 Automation of Treebank ,4 nnotationmmmmmmmmmmmmmmmmmm
