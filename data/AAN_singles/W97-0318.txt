Learning Methods for Combining Linguistic Indicatorsto Classify VerbsEric V. SiegelDepar tment  of Computer  Sc ienceCo lumbia  Un ivers i tyNew York,  NY  10027evs@cs .co lumbia .eduAbst ractFourteen linguistically-motivated numeri-cal indicators are evaluated for their abil-ity to categorize verbs as either states orevents.
The values for each indicator arecomputed automatically across a corpusof text.
To improve classification perfor-mance, machine learning techniques areemployed to combine multiple indicators.Three machine learning methods are com-pared for this task: decision tree induction,a genetic algorithm, and log-linear egres-sion.1 Introduct ionThe ability to distinguish states, e.g., "Mark seemshappy," from events, e.g., "Rende ran down thestreet," is a necessary prerequisite for interpretingcertain adverbial adjuncts, as well as identifyingtemporal constraints between sentences in a dis-course (Moens and Steedman, 1988; Doff, 1992; Kla-vans, 1994).
Furthermore, stativity is the first ofthree fundamental temporal distinctions that com-pose the aspectual class of a clause.
Aspectual clas-sification is a necessary component for a systemthat analyzes temporal constraints, or performs lex-ical choice and tense selection in machine transla-tion (Moens and Steedman, 1988; Passonneau, 1988;Doff, 1992; Klavans, 1994).Researchers have used empirical analysis of cor-pora to develop linguistically-based numerical indi-cators that aid in aspectual classification (Klavansand Chodorow, 1992; Siegel and McKeown, 1996).Specifically, this technique takes advantage of lin-guistic constraints that pertain to aspect, e.g., onlyclauses that describe an event can appear in the pro-gressive.
Therefore, a verb that appears more fre-quently in the progressive is more likely to describean event.In this paper, we evaluate fourteen quantitativelinguistic indicators for their ability to classify verbsaccording to stativity.
The values of these indicatorsare computed automatically across a corpus of text.Classification performance is then measured over anunrestricted set of verbs.
Our analysis reveals a pre-dictive value for several indicators that have not tra-ditionally been linked to stativity in the linguisticsliterature.
Then, in order to improve classificationperformance, we apply machine learning methods tocombine multiple indicators.
Three machine learn-ing techniques are compared for this task: decisiontree induction, a genetic algorithm, and log-linearregression.In the following sections, we further detail andmotivate the distinction between states and events.Next, we describe our approach, detailing the set oflinguistic indicators, the corpus and tools used, andthe machine learning methods.
Finally, we presentexperimental results and discuss conclusions and fu-ture work.2 S ta t ive  and  Event  VerbsStativity must be identified to detect temporal con-straints between clauses attached with when.
For ex-ample, in interpreting, "She had good strength whenobjectively tested, ,1 the have-state began before orat the beginning of the test-event, and ended afteror at the end of the test-event.
However, in inter-preting, "Phototherapy was discontinued when thebilirubin came down to 13," the discontinue-eventbegan at the end of the come-event.
As another ex-ample, the simple present reading of an event, e.g.,"He jogs," denotes the habitual reading, i.e., "everyday," whereas the simple present reading of a state,e.g., "He appears healthy," implies "at the moment.
"Identifying stativity is the first step toward aspec-tually classifying a clause.
Events are further distin-guished by two additional features: 1) telic eventshave an explicit culminating point in time, whilenon-telic events do not, and 2) extended events havea time duration, while atomic events do not.
De-tecting the telicity and atomicity of a clause is neces-sary to identify temporal constraints between clausesand to interpret certain adverbial adjuncts (Moens1These examples of when come from the corpus ofmedical discharge summaries used for this work.156If a verb can occur: ...then it must be:in the progressive Extended Eventwith a temporal adverb(e.g., then)with a duration in-PP(e.g., in an hour)in the perfect enseEventTelic EventTelic Event  or StateTable 1: Example linguistic constraints excerptedfrom Klavans (1994).and Steedman, 1988; Passonneau, 1988; Dorr, 1992;Klavans, 1994).
However, since these features applyonly to events and not to states, a clause first mustbe classified according to stativity.Certain features of a clause, such as adjuncts andtense, are constrained by and contribute to the as-pectual class of the clause (Vendler, 1967; Dowty,1979; Pustejovsky, 1991; Passonneau, 1988; Klavans,1994).
Examples of such constraints are listed inTable 1.
Each entry in this table describes a syn-tactic aspectual marker and the constraints on theaspectual class of any clause that appears with thatmarker.
For example, a telic event can be modifiedby a duration in-PP, as in "You found us there inten minutes ,  "but a state cannot, e.g., "*You lovedhim in ten  minutes .
"In general, the presence of these linguistic markersin a particular clause indicates a constraint on theaspectual class of the clause, but the absence thereofdoes not place any constraint.
This makes it difficultfor a system to aspectually classify a clause basedon the presence or absence of a marker.
Therefore,these linguistic constraints are best exploited by asystem that measures their frequencies across verbs.Klavans and Chodorow (1992) pioneered the ap-plication of statistical corpus analysis to aspectualclassification by placing verbs on a "stativity scale"according to the frequency with which they occurin the progressive.
This way, verbs are automati-cally ranked according to their propensity towardsstativity.
We have previously applied this princi-ple towards distinguishing relic events from non-telicevents (Siegel and McKeown, 1996).
Classificationperformance was increased by combining multipleaspectual markers with a genetic algorithm.3 ApproachOur goal is to exploit linguistic constraints uch asthose listed in Table 1 by counting their frequenciesin a corpus.
For example, it is likely that event verbswill occur more frequently in the progressive thanstate verbs, since the progressive is constrained tooccur with event verbs.
Therefore, the frequencywith which a verb occurs in the progressive indicateswhether it is an event or stative verb.We have evaluated 14 such linguistic indicatorsover clauses selected uniformly from a text corpus.In this way, we are measuring classification perfor-mance over an unrestricted set of verbs.
First, theability for each indicator to individually distinguishbetween stative and event verbs is evaluated.
Then,in order in increase classification performance, ma-chine learning techniques are employed to combinemultiple indicators.In this section, we first describe the set of lin-guistic indicators used to discriminate vents andstates.
Then, we show how machine learning is usedto combine multiple indicators to improve classifica-tion performance.
Three learning methods are com-pared for this task.
Finally, we describe the corpusand evaluation set used for these experiments.3.1 L inguist ic  Ind icatorsThe first column of Table 2 lists the 14 linguistic in-dicators evaluated in this paper for classifying verbs.The second and third columns show the averagevalue for each indicator over stative and event verbs,respectively, as computed over a corpus of parsedclauses, described below in Section 3.3.
These val-ues, as well as the third column, are further detailedin Section 4.Each verb has a unique value for each indicator.The first indicator, f requency, is simply the the fre-quency with which each verb occurs.
As shown inTable 2, stative verbs occur more frequently thanevent verbs in our corpus.The remaining 13 indicators measure how fre-quently each verb occurs in a clause with the lin-guistic marker indicated.
This list includes the fourmarkers listed in Table 1, as well as 9 additionalmarkers that have not previously been linked to sta-tivity.
For example, the next three indicators listedin Table 2 measure the frequency with which verbs1) are modified by not or never, 2) are modified bya temporal adverb such as then or frequently, and3) have no deep subject (passivized phrases oftenhave no deep subject, e.g., "She was admitted to thehospital").
As shown, stative verbs are modified bynot or never more frequently than event verbs, butevent verbs are modified by temporal adverbs morefrequently than stative verbs.
For further detail re-garding the set of 14 indicators, see Siegel (1997).An individual indicator can be used to classifyverbs by simply establishing a threshold; if a verb'sindicator value is below the threshold, it is assignedone class, otherwise it is assigned the alternativeclass.
For example, in Table 3, which shows thepredominant class and four indicator values corre-sponding to each of four verbs, a threshold of 1.00%would allow events to be distinguished from statesbased on the values of the not/never indicator.
Thenext subsection describes how all 14 indicators canbe used together to classify verbs.157"nOt" orVerb class :freq "never"show state 2 ,131  1.55%admit event 1 ,895 0.05%discharge event 1 ,608 0.50%feel state 1 ,177 4.61%temporal no deepadverb subject0.52%1.11%1.87%1.20%18.07%91.13%96.64%52.52%Table 3: Example verbs and their indicator values.Linguistic Stative Event T-testIndicator Mean Mean P-value:frequency"not" or "never"temporal adverbno deep subjectpast/pres participleduration in-PPperfectpresent enseprogressivemanner adverbevaluation adverbpast tenseduration for-PPcontinuous adverb932.894.44%1.00%36.05%20.98%0.16%2.27%11.19%1.79%0.00%0.69%62.85%0.59%0.04%667.571.56%2.70%57.56%15.37%O.60%3.44%8.94%2.69%0.03%1.19%65.69%0.61%0.03%0.00000.00000.00000.00000.00050.00180.00540.09010.09030.16810.17660.23\]40.84020.8.i38Table 2: Indicators discriminate between twoclasses.3.2 Combining Indicators with LearningGiven a verb and its 14 indicator values, our goalis to use all 14 values in combination to classify theverb as a state or an event.
Once a function for com-bining indicator values has been established, previ-ously unobserved verbs can be automatically classi-fied according to their indicator values.
This sectiondescribes three machine learning methods employedto this end.Log-linear regression.
As suggested by Klavansand Chodorow (1992), a weighted sum of multi-ple indicators that results in one "overall" indica-tor may provide an increase in classification perfor-mance.
This method embodies the intuition thateach indicator correlates with the probability thata verb describes an event or state, but that eachindicator has its own unique scale, and so must beweighted accordingly.
One way to determine theseweights is log-linear egression (Santner and Duffy,1989), a popular technique for binary classification.This technique, which is more extensive than a sim-ple weighted sum, applies an inverse logit function,and employs the iterative reweighted least squaresalgorithm (Baker and Nelder, 1989).Genetic programming.
An alternative to avoidthe limitations of a linear combination is to gener-ate a non-linear function tree that combines multipleindicators.
A popular method for generating suchfunction trees is a genetic algorithm (Holland, 1975;Goldberg, 1989).
The use of genetic algorithms togenerate function trees (Cramer, 1985; Koza, 1992)is frequently called genetic programming.
The func-tion trees are generated from a set of 17 primi-tives: the binary functions ADD, MULTIPLY andDIVIDE, and 14 terminals corresponding to the 14indicators listed in Table 2.
This set of primitiveswas established empirically; conditional functions,subtraction, and random constants failed to changeperformance significantly.
The polarities for severalindicators were reversed according to the polaritiesof the weights established by log-linear egression.Because the genetic algorithm isstochastic, each runmay produce a different function tree.
Runs of thegenetic algorithm have a population size of 500, andend after 50,000 new individuals have been evalu-ated.A threshold must be selected for both linear andfunction tree combinations of indicators.
This way,overall outputs can be discriminated such that classi-fication performance is maximized.
For both meth-ods, this threshold is established over the trainingset and frozen for evaluation over the test set.Decision trees.
Another method capable of mod-eling non-linear relationships between indicators i adecision tree.
Each internal node of a decision tree isa choice point, dividing an individual indicator intoranges of possible values.
Each leaf node is labeledwith a classification (state or event).
Given the set ofindicator values corresponding to a verb, that verb'sclass is established by deterministically traversingthe tree from the root to a leaf.
The most popularmethod of decision tree induction, employed here,is recursive partitioning (Quinlan, 1986; Breiman etal., 1984), which expands the tree from top to bot-tom.
The Splus statistical package was used for theinduction process, with parameters set to their de-fault values.Previous efforts in corpus-based natural lan-guage processing have incorporated machine learn-ing methods to coordinate multiple linguistic indica-tors, e.g., to classify adjectives according to marked-ness (Hatzivassiloglou and McKeown, 1995), to per-form accent restoration (Yarowsky, 1994), for dis-ambiguation problems (Yarowsky, 1994; Luk, 1995),158n States Eventsbe 23,409 100.0% 0.0%have 7,882 69.9% 30.1%all other verbs 66,682 16.2% 83.8%Table 4: Breakdown of verb occurrences.and for the automatic identification of semanticallyrelated groups of words (Pereira, Tishby, and Lee,1993; Hatzivassiloglou and McKeown, 1993).
Formore detail on the machine learning experiments de-scribed here, see Siegel (1997).3.3 A Parsed  CorpusThe automatic identification of individual con-stituents within a clause is necessary to compute thevalues of the linguistic indicators in Table 2.
TheEnglish Slot Grammar (ESG) (McCord, 1990) haspreviously been used on corpora to accumulate as-pectual data (Klavans and Chodorow, 1992).
ESGis particularly attractive for this task since its out-put describes a clause's deep roles, detecting, for ex-ample, the deep subject and object of a passiviz~dphrase.Our experiments are performed across a 1,159,891word corpus of medical discharge summaries fromwhich 97,973 clauses were parsed fully by ESG, withno self-diagnostic errors (ESG produced error mes-sages on some of this corpus' complex sentences).The values of each indicator in Table 2 are com-puted, for each verb, across these 97,973 clauses.In this paper, we evaluate our approach over verbsother than be and have, the two most frequent verbsin this corpus.
Table 4 shows the distribution ofclauses with be, have, and remaining verbs as theirmain verb.
Clauses with be as their main verb al-ways denote states.
Have is highly ambiguous, sothe aspectual classification of clauses headed by havemust incorporate additional constituents.
For ex-ample, "The patient had Medicaid" denotes a state,while, "The patient had an enema" denotes an event.In separate work, we have shown that the semanticcategory of the direct object of have informs classi-fication according to stativity (Siegel, 1997).
Sincethe remaining problem is to increase the classifica-tion accuracy over the 68.1% of clauses that havemain verbs other than be and have, all results aremeasured only across that portion of the corpus.
Asshown in Table 4, 83.8% of clauses with verbs otherthan be and have are events.A portion of the parsed clauses must be manu-ally classified to provide supervised training data forthe three learning methods mentioned above, and toprovide a separate set of test data with which to eval-uate the classification performance ofour system.
Tothis end, we manually marked 1,851 clauses electcduniformly from the set of parsed clauses not headedby be or have.
As a linguistic test to mark accordingto stativity, each clause was tested for readabilitywith "What happened was... "~ Of these, 373 wererejected because of parsing problems (verb or directobject incorrectly identified).
This left 1,478 parsedclauses, which were divided equally into 739 trainingand 739 testing cases.Some verbs can denote both states and events,depending on other constituents of the clause.
Forexample, show denotes a state in "His lumbar punc-ture showed evidence of white cells," but denotes anevent in "He showed me the photographs."
However,in this corpus, most verbs other than have are highlydominated by one sense.
Of the 739 clauses includedin the training set, 235 verbs occurred.
Only 11 ofthese verbs were observed as both states and events.Among these, there was a strong tendency towardsone sense.
For example, show appears primarily asa state.
Only five verbs - say, state, supplement,describe, and lie, were not dominated by one classover 80% of the time.
Further, each of these wereobserved less than 6 times a piece, which makes theestimation of sense dominance inaccurate.The limited presence of verbal ambiguity in thetest set does, however, place an upper bound of97.4% on classification accuracy, since linguistic in-dicators are computed over the main verb only.4 Resu l tsSince we are evaluating our approach over verbsother than be and have, the test set is only 16.2%states, as shown in Table 4.
Therefore, simply clas-sifying every verb as an event achieves an accuracyof 83.8% over the 739 test cases, since 619 are events.However, this approach classifies all stative clausesincorrectly, achieving a stative recall of 0.0%.
Thismethod serves as a baseline for comparison sincewc are attempting to improve over an uninformedapproach.
34.1 Ind iv idua l  Ind icatorsThe second and third columns of Table 2 show theaverage value for each indicator over stative andevent clauses, as measured over the 739 training ex-amples.
As described above, these examples excludebe and have.
For example, 4.44% of stative clausesare modified by either not or never, but only 1.56%of event clauses were modified by these adverbs.
Thefourth column shows the results of T-tests that com-pare the indicator values over stative verbs to thoseover event verbs.
For example, there is less thana 0.05% chance that the difference between stativeand event means for the first four indicators listed2This test was suggested by Judith Klavans (personalcommunication).3Similar baselines for comparison have been used formany classification problems (Duda and Hart, 1973),e.g., part-of-speech tagging (Church, 1988; Allen, 1995).159is due to chance.
Overall, this shows that the differ-ences in stative and event averages are statisticallysignificant for the first seven indicators listed (p <.01).This analysis has revealed correlations betweenverb class and five indicators that have not beenlinked to stativity in the linguistics literature.
Of thetop seven indicators hown to have positive correla-tions with stativity, three have been linguisticallymotivated, as shown in Table 1.
The other fourwere not previously hypothesized to correlate withaspectual class: (1) verb :frequency, (2) occurrencesmodified by "not" or "never", (3) occurrences withno deep subject, and (4) occurrences in the past orpresent participle.
Furthermore, the last of theseseven, occurrences in the perfect ense, was not pre-viously hypothesized to correlate with stativity inparticular.However, a positive correlation between indicatorvalue and verb class does not necessarily mean anindicator can be used to increase classification ac-curacy.
Each indicator was tested individually forits ability to improve classification accuracy over thebaseline by selecting the best classification thresho',dover the training data.
Only two indicators, verb:frequency, and occurrences with not and never,were able to improve classification accuracy overthat obtained by classifying all clauses as events.To validate that this improved accuracy, the thresh-olds established over the training set were used overthe test set, with resulting accuracies of 88.0% and84.0%, respectively.
Binomial tests showed the firstof these to be a significant improvement over thebaseline of 83.8%, but not the second.4.2 Combin ing  Ind icatorsAll three machine learning methods successfullycombined indicator values, improving classificationaccuracy over the baseline measure.
As shown in Ta-ble 5, the decision tree's accuracy was 93.9%, geneticprogramming's function trees had an average accu-racy of 91.2% over seven runs, and the log-linear e-gression achieved an 86.7% accuracy.
Binomial testsshowed that both the decision tree and genetic pro-gramming achieved a significant improvement overthe 88.0% accuracy achieved by the :frequency indi-cator alone.
Therefore, we have shown that machinelearning methods can successfully combine multi-ple numerical indicators to improve the accuracy bywhich verbs are classified.The differences in accuracy between the threemethods are each significant (p < .01).
Therefore,these results highlight the importance of how linearand non-linear interactions between numerical lin-guistic indicators are modeled.4.3 Improved Recal l  T radeof fThe increase in the number of stative clauses cor-rectly classified, i.e.
stative recall, illustrates a moredramatic improvement over the baseline.
As shownin Table 5, stative recalls of 74.2%, 47.4% and 34.2%were achieved by the three learning methods, ascompared to the 0.0% stative recall achieved by thebaseline, while only a small loss in recall over eventclauses was suffered.
The baseline does not classifyany stative clauses correctly because it classifies allclauses as events.
This difference in recall is moredramatic than the accuracy improvement because ofthe dominance of event clauses in the test set.This favorable tradeoff between recall valuespresents an advantage for applications that weighthe identification of stative clauses more heavilythan that of event clauses.
For example, a preposi-tional phrase denoting a duration with/or,  e.g., ".fora minute," describes the duration of a state, e.g.,"She felt sick for two weeks," or the duration of thestate that results from a telic event, e.g., "She left theroom for a minute."
That is, correctly identifyingthe use of for depends on identifying the stativityof the clause it modifies.
A language understand-ing system that incorrectly classifies "She felt sickfor two weeks" as a non-telie event will not detectthat "for two weeks" describes the duration of thefeel-state.
If this system, for example, summarizesdurations, it is important to correctly identify states.In this case, our approach is advantageous.5 Conclus ions and Future WorkWe have compiled a set of fourteen quantitative lin-guistic indicators that, when used together, signifi-cantly improve the classification of verbs accordingto stativity.
The values of these indicators are mea-sured automatically across a corpus of text.Each of three machine learning techniques success-fully combined the indicators to improve classifica-tion performance.
The best of the three, decisiontree induction, achieved a classification accuracy of93.9%, as compared to the uninformed baseline's ac-curacy of 83.8%.
Furthermore, genetic programmingand log-linear egression also achieved improvementsover the baseline.
These results were measured overan unrestricted set of verbs.The improvement in classification performance ismore dramatically illustrated by the favorable trade-off between stative and event recall achieved by allthree of these methods, which is profitable for tasksthat weigh the identification of states more heavilythan events.This analysis has revealed correlations betweenstativity and five indicators that are not tradition-ally linked to stativity in the linguistic literature.Furthermore, one of these four, verb frequency, in-dividually increased classification accuracy from thebaseline method to 88.0%.To classify a clause, the current system uses onlythe indicator values corresponding to the clause'smain verb.
This procedure could be expanded to160overallaccuracydecision tree 93.9%genetic programming 91.2%log-linear 86.7%baseline 83.8%States II Eventsrecall precision recall precision74.2% 86.4% 97.7% 95.1%47.4% 97.3% 99.7% 90.7%34.2% 68.3% 96.9% 88.4%0.0% 100.0% II 100.0% 83.8%Table 5: Comparison of three learning methods and a performance baseline.incorporate rules that classify a clause directly fromclausal features (e.g., Is the main verb show, is theclause in the progressive?
), or by calculating indi-cator values over other clausal constituents in addi-tion to the verb (Siegel and McKeown, 1996; Siegel,1997).Classification performance may also improve byincorporating additional inguistic indicators, suchas co-occurrence with rate adverbs, e.g., quickly, oroccurrences as a complement offorce or persuade, assuggested by Klavans and Chodorow (1992).AcknowledgmentsKathleen R. McKeown was extremely helpful regard-ing the formulation of our work and Judith Klavansregarding linguistic techniques.
Alexander D. Char-fee, Vasileios Hatzivassiloglou, Dragomir Radev andDekai Wu provided many helpful insights regardingthe evaluation and presentation of our results.This research is supported in part by theColumbia University Center for Advanced Technol-ogy in High Performance Computing and Commu-nications in Healthcare (funded by the New YorkState Science and Technology Foundation), the Of-fice of Naval Research under contract N00014-95-1-0745 and by the National Science Foundation undercontract GER-90-24069.Finally, we would like to thank Andy Singleton forthe use of his GPQuick software.Re ferencesAllen, J.
1995.
Natural Language Understanding.Benjamin/Cummings, Redwood City, CA.Baker, R.J. and J.A.
Nelder.
1989.
The GLIMSystem, Release 3: Generalized Linear InteractiveModeling.
Numerical Algorithms Group, Oxford.Breiman, L., J.H.
Friedman, R.A. Olshen, and C.J.Stone.
1984.
Classification and Regression Trees.Wadsworth, Belmont.Church, K. 1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
In Pro-ceedings of the 2nd Conference for Applied NaturalLanguage Processing, pages 136-143.Cramer, N. 1985.
A representation for the adap-tive generation of simple sequential programs.
InJ.
Grefenstette, ditor, Proceedings of the \[First\]International Conference on Genetic Algorithms.Lawrence Erlbaum.Dorr, B.J.
1992.
A two-level knowledge representa-tion for machine translation: lexical semantics andtense/aspect.
In James Pustejovsky and SabineBergler, editors, Lezical Semantics and KnowledgeRepresentation.
Springer Verlag, Berlin.Dowty, D. 1979.
Word Meaning and MontagueGrammar.
D. Reidel, Dordrecht, W. Germany.Duda, R. O. and P.E.
Hart.
1973.
Pattern Classifi-cation and Scene Analysis.
Wiley, New York.Goldberg, D. 1989.
Genetic Algorithms in Search,Optimization, and Machine Learning.
Addison-Wesley Publishing Company, Inc., Reading, MA.Hatzivassiloglou, V. and K.R.
McKeown.
1993.
To-wards the automatic identification of adjectivalscales: Clustering adjectives according to mean-ing.
In Proceedings of the 31st Annual Meeting ofthe ACL, pages 172-182, Columbus, Ohio, June.Association for Computational Linguistics.Hatzivassiloglou, V. and K.R.
McKeown.
1995.A quantitative valuation of linguistic tests forthe automatic prediction of semantic markedness.In Proceedings of the 33rd Annual Meeting ofthe ACL, pages 197-204, Boston, Massachusetts,June.
Association for Computational Linguistics.Holland, J.
1975.
Adaptation in Natural and Arti-ficial Systems.
The University of Michigan Press,Ann Arbor, MI.Klavans, J.L.
1994.
Linguistic tests over large cor-pora: aspectual classes in the lexicon.
Technicalreport, Columbia University Dept.
of ComputerScience.
unpublished manuscript.Klavans, J.L.
and M. Chodorow.
1992.
Degrees ofstativity: the lexical representation f verb aspect.In Proceedings of the 14th International Confer-ence on Computation Linguistics.Koza, J.R. 1992.
Genetic Programming: On theprogramming ofcomputers by means of natural se-lection.
MIT Press, Cambridge, MA.161Luk, A.K.
1995.
Statistical sense disambiguationwith relatively small corpora using dictionary def-initions.
In Proceedings of the 33rd Annual Meet-ing of the ACL, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.McCord, M.C.
1990.
Slot grammar: A systemfor simpler construction of practical natural an-guage grammars.
In R. Studer, editor, Interna-tional Symposium on Natural Language and Logic.Springer Verlag.Moens, M. and M. Steedman.
1988.
Temporalontology and temporal reference.
ComputationalLinguistics, 14(2).Passonneau, R.J. 1988.
A computational model ofthe semantics of tense and aspect.
ComputationalLinguistics, 14(2).Pereira, F., N. Tishby, and L. Lee.
1993.
Distri-butional clustering of English words.
In Proceed-ings off the 31st Annual Meeting of the A CL, pages183-190, Columbus, Ohio, June.
Association forComputational Linguistics.Pustejovsky, J.
1991.
The syntax of event structure.Cognition; 41(103):47-92.Quinlan, J.R. 1986.
Induction of decision trees.
Ma-chine Learning, 1(1):81-106.Santner, T.J. and D.E.
Duffy.
1989.
The StatisticalAnalysis off Discrete Data.
Springer-Verlag, NewYork.Siegel, E.V.
1997.
Classifying Natural LanguagePhrases with Corpus-Based Linguistic Indicators.Ph.D.
thesis, Columbia University.Siegel, E.V.
and K.R.
McKeown.
1996.
Gatheringstatistics to aspectually classify sentences with agenetic algorithm.
In K. Oflazer and H. Somers,editors, Proceedings of the Second InternationalConference on New Methods in Language Process-ing, Ankara, Turkey, Sept. Bilkent University.Vendler, Z.
1967.
Verbs and times.
In Linguistics inPhilosophy.
Cornell University Press, Ithaca, NY.Yarowsky, D. 1994.
Decision lists for lexical ambi-guity resolution: Application to accent restorationin spanish and french.
In Proceedings of the 32ndAnnual Meeting of the ACL, San Francisco, CA,June.
Morgan Kaufmann.162
