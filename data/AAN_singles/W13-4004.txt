Proceedings of the SIGDIAL 2013 Conference, pages 21?30,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsToward a Better Understanding of Causality between Verbal Events:Extraction and Analysis of the Causal Power of Verb-Verb AssociationsMehwish Riaz and Roxana GirjuDepartment of Computer Science and Beckman InstituteUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801, USA{mriaz2, girju}@illinois.eduAbstractThe identification of causal relations be-tween verbal events is important forachieving natural language understanding.However, the problem has proven notori-ously difficult since it is not clear whichtypes of knowledge are necessary to solvethis challenging problem close to humanlevel performance.
Instead of employing alarge set of features proved useful in otherNLP tasks, we split the problem in smallersub problems.
Since verbs play a very im-portant role in causal relations, in this pa-per we harness, explore, and evaluate thepredictive power of causal associations ofverb-verb pairs.
More specifically, we pro-pose a set of knowledge-rich metrics tolearn the likelihood of causal relations be-tween verbs.
Employing these metrics, weautomatically generate a knowledge base(KBc) which identifies three categoriesof verb pairs: Strongly Causal, Ambigu-ous, and Strongly Non-causal.
The knowl-edge base is evaluated empirically.
The re-sults show that our metrics perform signif-icantly better than the state-of-the-art onthe task of detecting causal verbal events.1 IntroductionThe identification of semantic relations betweenevents is a mandatory component of natural lan-guage understanding.
In this paper, we focuson the identification of causal relations betweenevents represented by verbs.
Following Riaz andGirju (2010), we define a verbal event evi as?
[subjectvi] vi [objectvi]?, where the subject andobject of the verb may or may not be explicitlypresent in an instance.
Consider the following ex-amples:1.
Yoga builds stamina because you maintain your posesfor a certain period of time.
(CAUSE (emaintain, ebuild))2.
The monster storm Katrina raged ashore along theGulf Coast Monday morning.
There were early re-ports of buildings collapsing along the coast.
(CAUSE(erage, ecollapse))In example 1, the two bold events are causallyconnected by an explicit and unambiguous dis-course marker (because).
However, in English,not all discourse markers unambiguously iden-tify causality (Prasad et al 2008) - for exam-ple, Bethard and Martin (2008) proposed a cor-pus of 1000 causal and non-causal event pairs con-joined by the marker and.
Even more, causal re-lations can be encoded by implicit contexts - i.e.,those where no discourse marker is present (ex-ample 2).
Despite the recent achievements ob-tained in discourse processing, it is still unclearwhat types of knowledge can contribute most to-wards detecting causality in both explicit and im-plicit contexts (Sporleder and Lascarides, 2008).The complexity of the task of detecting causalitybetween events stems from the fact that there aremany factors involved, such as contextual featuresof an instance (e.g., lexical items, tenses of verbs,arguments of verbs, etc.
), semantic and pragmaticfeatures of events, background knowledge, worldknowledge, common sense, etc.
Prior approacheshave employed contextual features of an instanceto identify causality between events or discoursesegments (Bethard and Martin, 2008; Pitler andNenkova, 2009; Pitler et al 2009).
Althoughcontextual features provide important knowledgeabout sentence(s) in which events appear, humansalso make use of other information such as back-ground knowledge to comprehend causality.
Forinstance, in example 2 we use knowledge aboutthe causal association between verbal entities rageand collapse to label it with causality.This research is motivated by the need to extractand analyze other type of knowledge necessary forthe identification of causal relations between ver-bal events.
We start from the fact that verbs are the21main components of language to express eventsand semantic relations between events.
Thus, inorder to identify and extract causal relations be-tween events (denoted by (evi , evj )), it is criticalfor a model to employ knowledge about the ten-dency of a verb pair (vi, vj) to encode causation.For example, the pair (kill, arrest) has a high ten-dency to encode a cause relation irrespective of thecontext in which it is used, thereby a good indica-tor of causality.
The state-of-the-art resources onverb semantics, such as WordNet, VerbNet, Prop-Bank, FrameNet, etc.
(Miller, 1990; Kipper et al2000; Kingsbury et al 2002; Baker et al 1998),provide information about the semantic classes,thematic roles and selectional restrictions of verbs.Among these, WordNet is the only resource whichprovides information about the cause relation be-tween verbs, but it has very limited coverage.For VERBOCEAN, a semi-automatically generatedresource, Chklovski and Pantel (2004) have usedexplicit lexical patterns (e.g., ?verb * by verb?)
asmeans of mining enablement (cause-effect) rela-tions between verbs.
Such approaches help detect-ing causality with high precision but suffer fromlimited coverage due to the highly implicit na-ture of language.
Moreover, such resources donot provide any information about the likelihoodof a causal relation in verb pairs - e.g., (kill, ar-rest) has a high tendency to encode cause rela-tion as compared with the pair (build, maintain).The pair (build, maintain) seems ambiguous be-cause it can encode both cause and non-cause re-lations depending on the context, as shown by ex-amples 1 and 3.
Thus, causality detection modelsshould employ knowledge about which verb pairsare strongly causal (non-causal) in nature and forwhich pairs the context plays an important role tosignal causality.3.
Republicans had not cut the funds for maintaining thelevee and building up the ecological protections.
(NON-CAUSE)We propose a fully automated procedure to learnthe likelihood of causal relations in verb pairs.
Inthis process, we create three categories of verbpairs: Strongly Causal (Sc), Ambiguous (Ac) andStrongly Non-causal (S?c).
The result is a knowl-edge base (KBc) of causal associations of verbs.In KBc, the category Sc (S?c) contains the verbpairs which have the greatest (least) likelihood toencode a causal relation, respectively.
However,the category Ac contains ambiguous verb pairswhich have the likelihood to encode both causaland non-causal relations.
The information aboutsuch causal associations provides a rich knowl-edge source to causality detection models.The main contributions of our research are asfollows:?
We propose a set of novel metrics (i.e., ExplicitCausal Association (ECA), Implicit Causal As-sociation (ICA) and Boosted Causal Associa-tion (BCA)) to identify the likelihood of verbpairs to encode causality.
Our metrics exploitthe information available from a large numberof unlabeled explicit and implicit instances ofverb pairs for this purpose.?
We introduce an automated procedure to builda training corpus of causal and non-causalevent pairs.
This prevents us from the trou-ble of annotating a large number of event pairsfor cause and non-cause relations.
Our metricsmake use of supervision from the training cor-pus to identify causality in verb pairs.
We alsoprovide a mechanism to determine causal verbpairs which remain undiscovered due to the is-sue of training data sparseness.?
We revisit recent approaches employing distri-butional similarity methods to predict causal-ity between events (Riaz and Girju, 2010;Do et al 2011).
The state-of-the-art met-ric Cause-Effect Association (CEA) (Do etal., 2011) identifies causality mainly based onprobabilities of verb-verb, verb-argument, andargument-argument pairs.
In comparison withCEA, our metrics perform significantly betterby improving the prior knowledge about thecausal associations from CEA?s components.After a brief review of related work in next sec-tion, we describe our approach for acquisition oftraining corpus in section 3.
The model for the ex-traction of causal associations is presented in sec-tion 4, followed by the evaluation and discussionin section 5 and conclusion in section 6.2 Related WorkCausality has long been studied from variousperspectives by philosophers, data-mining re-searchers and computer scientists (Menzies, 2008;Woodward, 2008; Suppes, 1970; Silverstein et al2000; Pearl, 2000).In NLP, the problem of detecting causality be-tween events is a very challenging but less re-searched topic.
Previously, researchers have stud-22ied this task by focusing on supervised classifi-cation models for both verbal and nominal events(Girju, 2003; Bethard and Martin, 2008).
Bethardand Martin (2008), for example, have focusedmainly on the contextual features available in testinstances of verbal event pairs to predict causality.They have relied on a small scale dataset of 1000instances (697 training and 303 test) for this task.Unlike above models, recently some researchershave employed unsupervised causality detectionmetrics and minimal supervision for this task.
Forexample, Riaz and Girju (2010) have proposed anunsupervised metric Effect-Control Dependency(ECD) to determine causality between events innews scenarios.
Following their model, Do et al(2011) introduced an improved metric CEA whichuses PMI and some components of ECD to pre-dict the causal relation in verbal and nominal eventpairs in a text document.
They also proposed aminimally supervised method using explicit dis-course markers.
For example, they used ILPframework to assign a non-causal relation to all theevent pairs appearing in two discourse segmentsconnected by a non-causal marker.
They evalu-ated their model on a set of 20 documents, a highlyskewed evaluation set with around 2-3% causalinstances and 58% human inter-annotator agree-ment on cause-effect relations.
On verbal events,they reported 38.3% F-score with CEA and 1-2%improvement using minimally supervised method.As compared with above mentioned metrics, weintroduce knowledge rich association measureswhich employ supervision from the automaticallygenerated training corpus to learn causality.Several other NLP researchers have studiedrelated topics e.g., identifying events, buildingof temporal chain of events sharing a commonprotagonist (participant), predicting future eventsand identifying hidden links in news articles tobuild a coherent chain (Chambers and Jurafsky,2008; Chambers and Jurafsky, 2009; Radinskyand Horvitz, 2013; Shahaf and Guestrin, 2010).Unlike these tasks, our focus is on identifyingcausality between events.3 Acquisition of Training CorpusIn this section, we propose a fully automated pro-cedure to build a training corpus of event pairswhich encode cause and non-cause relations.
Thistraining corpus is used in our model to identify thelikelihood of cause relations in verb pairs.
As dis-cussed earlier, previous researchers have workedwith a small scale dataset of annotated event pairs.The current task requires us to use a large train-ing corpus to learn the pervasive relation of causal-ity and the manual generation of such corpus is alaborious task.
Therefore, we decided to dependon the unambiguous discourse markers becauseand but to automatically collect training instancesof cause and non-cause event pairs, respectively.For example, the marker because in the instance1 of section 1 encodes a cause relation betweenthe events ebuild and emaintain.
Some researchershave utilized unambiguous discourse markers toacquire training instances of semantic relations be-tween discourse segments (Marcu and Echihabi,2001; Sporleder and Lascarides, 2008).
However,the process is not simple for the current problemsince it is not always clear how to create a causalinstance of an event pair.
Consider the followingmeta instance I:I : <s>/m1 .
.
.
v1 .
.
.
v2 .
.
.
vk .
.
.
because .
.
.
vk+1.
.
.
vk+2, .
.
., vr, .
.
.m2/</s>.It is composed of main verbs (v1, v2, .
.
.,vr), discourse markers (m1, m2), and sentenceboundaries (<s>, </s>).
Here, we assume thatthe discourse markers or the sentence boundarieswhichever appear first in I represent the bound-aries of discourse segments for the marker because(appendix A contains a table of notations used inthis paper).
In I , there are k and r ?
k main verbsappearing before and after because, respectively.The problem here is to determine the event pair en-coding causality out of k?
(r?
k) choices.
Here,we consider that the most dependent pair amongall choices in I is the best candidate to encodecausality.In this work, we propose the following functionf(I) to pick the most dependent pair:f(I) = argmax(vi?mc ,vjmc )CD(vi, vj)?
PSI(vi, vj) (1)Here, i (j) refers to all verbs that appear be-fore (after) the causal marker (i.e., mc) because inI .
CD (equation 2) is a component of predicate-predicate association of CEA (Do et al 2011)to determine causal dependency of a pair (vi, vj).Do et al(2011) used the score CD to determinecausality in an unsupervised fashion but here weemploy this to build a training corpus of causalevent pairs.CD(vi, vj) = PMI(vi, vj)?max(vi, vj)?
IDF (vi, vj) (2)23The functions PMI, max and IDF depend on co-occurrence probabilities and idf scores to deter-mine causal dependency.
Due to space limitations,for details we refer the reader to Do et al(2011).Next, we define a novel penalization factor PSIfor the verbs of a pair appearing at greater distancefrom the causal marker because.
For example, thisassumes the verbs in the pair (v2, vk+2) are lesslikely to be in a cause relation as compared with(vk, vk+1) in I .
We came up with this idea becauseour initial experiments revealed that the causal in-stances obtained by penalizing CD with PSI pro-vide better training for our model as compared tousing only CD for this purpose.
The similar be-havior of reduction in the likelihood of causalitywith respect to increase in distance between twoevents was observed by Riaz and Girju (2010).PSI(vi, vj) = ?
logpos(vi) + pos(vj)2.0?
(C(vp) + C(vq))(3)Here, C(vp) (C(vq)) is the count of the mainverbs appearing before (after) because, respec-tively.
The distance of the verb is measured interms of its position (i.e., pos(vi)) with respect tobecause.
The position is 1 for the verb closest tobecause and 2 for the verb next to the closest verb.PSI has maximum value for (vk, vk+1) and it re-duces for other pairs with verbs at greater distancefrom because in instance I .In order to extract non-causal event pairs, weutilized instances with two discourse segmentsconjoined by the marker but which representscomparison (non-causal) relation.
Any event paircollected from the two discourse segments in non-causal relation encodes non-causality.
Therefore,we depend on selecting the closest verb pair fromthe instances of form I with marker but instead ofbecause.In this paper, we present the results producedusing a training corpus of 240K instances (50%for each class) from the English Gigaword Cor-pus.
In order to prepare this corpus, we identifieddiscourse markers (i.e., m1, m2), if available, be-fore and after because/but in each instance I andassumed that only those markers which have dis-course usage in I define boundaries of discoursesegments of because/but.
We used the list of 100explicit discourse markers provided by Prasad etal.
(2008) and the supervised approach of Pitlerand Nenkova (2009) to detect markers and the dis-course versus non-discourse usage of these mark-ers.
We use this training corpus to identify cau-sation for both explicit and implicit instances ofevent pairs.
Using this training corpus, a modeltends to give higher causal weights to those in-stances in which events are connected by the ex-plicit causal marker because as compared to im-plicit instances of causation.
Thus, to provide fairsupervision to both explicit and implicit instancesof event pairs, we remove the cue words becauseand but which were used to automatically label thetraining instances.4 Causal Associations of Verb PairsIn this section, we explain our approach to learnthe likelihood of causal relations in verb pairs byexploiting information available from both explicitand implicit instances of these pairs.
We extractedaround 12, 000 documents from the English Gi-gaword corpus to collect instances of verb pairsfrom single sentences (intra-sentential) and adja-cent sentences (inter-sentential) of text.
In this set,we added instances from 3, 000 articles on newsstories ?Hurricane Katrina?
and the ?Iraq war?.These articles were collected and used to iden-tify causal relations in news scenarios by Riaz andGirju (2010).
We used these collections becausenatural disaster and war-related news articles arerich in causal events and chains of such events.In order to identify the causal associations withhigh confidence, we decided to apply our model onthose verb pairs which have at least 30 instancesin the above mentioned documents.
We acquired10, 455 such verb pairs.
The set of intra- and inter-sentential instances of these verb pairs is referredto as the development set for our model.4.1 Explicit Causal Association (ECA)In order to find the likelihood of a verb pair to en-code causal relations, we define our novel metricExplicit Causal Association (ECA) as follows:ECA(vi, vj) =1| V P |?I(vi,vj)?V P(CD(vi, vj)?
CI) (4)where V P is the set of intra- and inter-sententialinstances (denoted by I(vi, vj)) of the verb pair(vi, vj), CD determines the causal dependency ofthe verb pair in unsupervised fashion (equation 2),and CI finds the tendency of instance I of (vi, vj)to belong to the cause class as compared to thenon-cause class using training corpus of eventpairs.
The goal of ECA is to combine the unsu-pervised causal dependency (i.e., CD) with the su-pervised score of instance I of belonging to cause24class than the non-cause one (i.e., CI ).
Here, CDrepresents the prior knowledge about the causalassociation based on co-occurrence probabilitiesand idf scores (equation 2).
It can discover lotsof false positives because the co-occurrence prob-abilities can fail to differentiate causality from anyother type of correlation.
Therefore, we improvethis prior knowledge with the help of supervisionfrom the training corpus containing instances ofboth cause and non-cause relations.
The globaldecision of the causal association is made by tak-ing the average of scores on all the instances con-taining that verb pair.
Notice that CD can also bemoved out from the summation function in equa-tion 4.We define the function CI as follows:CI =n?k=1log( P (fk | c)P (fk | ?c)) (5)Here, the notations c and ?
c represent causeand non-cause class, respectively.
The notationfk represents the feature of an instance I .
In thiswork, we use some language features of eventsand context of an instance I which are definedlater in this section.
P(fk | c) and P(fk | ?c) arethe smoothed probabilities of feature fk given thecause and non-cause training instances.
The valueof CI is positive only when the instance I has moretendency to encode a cause relation than a non-cause one.
To avoid negative values, we map CIscores to the range [0, 1] using CI?CminCmax?Cmin whereCmin (Cmax) is the minimum (maximum) value ofCI obtained on our development set, respectively.Also, we add a small value  to CI to avoid 0 value.Similarly, to avoid negative scores of PMI in equa-tion 2 we can map it to the range [0,1].We present below the features for the calcula-tion of CI .
We use lexical, syntatic and semanticfeatures on verbs and verb phrases of both eventsof a pair.
These features include words, lemmas,part-of-speech tags, all senses from WordNet forthe verbs and the lexical items of verb phrases.These features were introduced by Bethard andMartin (2008) (for an in-depth description of thesefeatures see Bethard and Martin (2008)).
Next, wedescribe the set of features which are the contribu-tions of this research.1.
Verbs Arguments: Words, lemma, part-of-speech tags and all senses from WordNet forsubject and object of verbs of both events.2.
Verbs and Arguments Pairs: For this fea-ture, we take the cross product of bothevents of a pair (evi ,evj ) where evi =[subjectvi] vi [objectvi] and evj = [subjectvj ]vj [objectvj ].
Some examples of this fea-ture are (subjectvi ,subjectvj ), (subjectvi ,vj),(subjectvi ,objectvj ), etc.
In this work, we useunordered pairs as features (i.e., (vi,vj)) issame as (vj ,vi)) because the temporal order ofevents is unknown for the unlabeled develop-ment set instances.
In future, this feature canbe improved by adding temporal information.The next three features are taken from the min-imum relevant context (mincontext) of a verbpair which we define as follows.
mincontext ofa pair (vi, vj) in an intra-sentential instance is<s>/m1 .
.
.
vi .
.
.
vj .
.
.m2/</s> ?
i.e., words be-tween the discourse markers (i.e., m1, m2) or sen-tence boundaries (i.e., <s>, </s>) whichever ap-pear first in the sentence.
The mincontext for thepair (vi, vj) in an inter-sentential is given below:<s> / m1 .
.
.
vi .
.
.m2 / </s><s> / m1 .
.
.
vj .
.
.m2 / </s>3.
Context Words: Lemmas of all words frommincontext.
This feature captures words otherthan two events.4.
Context Main Verbs: All main verbs and theirlemmas from mincontext.
It collects informa-tion about all verbs that appear with the causaland non-causal event pair.5.
Context Main Verb Pairs: The pairs of mainverbs from mincontext.
The lemmas are takenfrom the feature ?Context Main Verbs?
andthen the pairs on these lemmas are used as thisfeature.
For example, for lemmas of verbs (i.e.,v1, v2, .
.
.
, vk), pairs (i.e., (v1, v2), (v1, vk),etc.)
are used for this feature.
This featureis used to get information about the interest-ing causal chains of verbs that may appear incausal instances.We propose next a novel metric ICA to avoidthe problem of training data sparsity.4.2 Implicit Causal Association (ICA)In order to determine the causal associations us-ing ECA, we depend on explicit cause and non-cause training instances for supervision.
However,it is possible that some strongly causal verb pairsmay frequently appear in implicit causal contexts.Therefore, the causality of such pairs can remainuncaptured by ECA which merely relies on ex-plicit training instances.
For example, a pair (fall,25break) seems strongly causal, but it does not ap-pear often in our explicit training corpus due totraining data sparsity.
Thus, in order to handlethis problem, we propose a new metric called ICA.This metric makes use of functions for the identi-fication of roles of events in a cause relation.
Afterbriefly describing the roles of events in causal re-lations below, we continue with the description ofICA.4.2.1 Roles of Events in Cause RelationEach of the two events in a cause relation can beassigned either cause or effect role.
For examplefor the following training instance, the verb ap-pearing after because represents cause event andthe verb before because represents effect event.1.
Yoga builds stamina because you maintain your posesfor a certain period of time.
(Role: rC )2.
Yoga builds stamina because you maintain your poses fora certain period of time.
(Role: rE)The notation rC and rE represents the classes ofcause and effect role of events, respectively.
Weuse core features of events to determine the like-lihood of their roles in causation.
These featuresinclude lemma, part-of-speech tag, all senses fromWordNet of both verbs and their arguments (i.e.,subject and object).
Next, we use these features tohandle training data sparseness.4.2.2 Handling of Training Data SparsityTo deal with the problem of training data sparsity,we define the metric ICA as follows:ICA(vi, vj) =1| V P |?I(vi,vj)?V P(CD(vi, vj)?
CI?ERM(evi ,evj )) (6)where CD and CI are defined earlier and ERMdetermines the likelihood of roles of the events inthe cause relation.
We remind the reader that CDis the unsupervised causal dependency of verb pairand CI is the likelihood of instance I of the verbpair to belong to the cause class than the non-causeone using full set of features from section 4.1.Events Roles Matching (ERM(evi ,evj )) (equa-tions 7 and 8) is the negative log-likelihood ofevents evi and evj appearing as cause or effect roledetermined using the explicit causal instances ofthe training corpus and the core features of eventsdefined in section 4.2.1.ERM(evi ,evj ) = ?1.0?max(S(evi , rC) + S(evj , rE),S(evi , rE) + S(evj , rC)) (7)S(evi , rC) =n?k=1log(P (fk | rC)) (8)S(evj , rE) =n?k=1log(P (fk | rE))Here, S(evi , rC) is the score of evi being thecause event and S(evj , rE) is the score of evj be-ing the effect event.
These scores are computedusing smoothed probabilities ?
i.e., P(fk | rC) andP(fk | rE).
Similarly, S(evi , rE) and S(evj , rC)are calculated and max is taken.
The high valueof ERM represents low matching of an event pair(verbs and their arguments) in the explicit causalinstances of the training corpus.
The high valueof ERM of an event pair can have one of the fol-lowing two interpretations: (A) it is a non-causalevent pair, or (B) it is a causal event pair but thispair and the pairs which are semantically closer toit hardly appear in explicit causal contexts.
In themetric ICA, CI?
CD(vi, vj) is used as a guidingscore to interpret ERM as follows:1.
If CI?
CD(vi, vj) has high score then the valueof ERM is not penalized by this guiding scorebecause ERM?s value can be interpreted using(B) above.2.
If CI?
CD(vi, vj) has low score then the valueof ERM is penalized by this guiding score be-cause (evi , evj ) can be a non-causal pair ac-cording to the interpretation (A) above.ICA is a boosting factor to determine causalverb pairs which remain undiscovered because oftraining data sparseness.
We also define a BoostedCausal Association (BCA) metric by adding ICAto original ECA metric as follows:BCA(vi, vj) =1| V P |?I(vi,vj)?V P(CD(vi, vj)?
CI +CD(vi, vj)?
CI ?
ERM(evi ,evj )) (9)To build the knowledge base of causal asso-ciations (KBc), we generate a ranked list of allverb pairs based on the likelihood of causality en-coded by these pairs.
Here, we assume that verbpairs are uniformally distributed across three cat-egories - i.e., top one-third and bottom one-thirdranked verb pairs belong to Strongly Causal (Sc)and Strongly Non-Causal (S?c) categories and restof the pairs are considered Ambiguous (Ac).
Fol-lowing our assumption, we evaluate this catego-rization in next section, but in future researcherscan perform empirical study of how to automat-ically cluster verb pairs into three or more cate-gories with respect to causation.265 Evaluation and DiscussionIn this section, we present our evaluation ofknowledge base to identify causality between ver-bal events.
Specifically we performed experimentsto evaluate (1) the ranking of verb pairs based ontheir likelihood of encoding causality, and (2) thequality of the three categories of verb pairs inKBc(i.e., Sc, Ac and S?c).
For this purpose, we col-lected two test sets.
For each test set, we randomlyselected 50 verb pairs from the list of 10, 455 verbpairs in KBc.
For each verb pair, we selectedrandomly 3 intra- and 3 inter-sentential instancesfrom the English Gigaword corpus and the ?Hur-ricane Katrina?
and ?Iraq war?
articles.
In orderto keep the development set different from the testsets, we automatically traversed the developmentset to determine if any test instance is available init.
In case of finding any such test instance, weremoved it from the development set to performevaluation on unseen test instances.
Two annota-tors were asked to provide Cause or Non-Causelabels for each instance.
They were provided withannotation guidelines from the manipulation the-ory of causality (Woodward, 2008).
Given theseguidelines have been successfully used by Riazand Girju (2010), we use them here as well.
Forease of annotation, we randomly selected inter-sentential instances such that the length of eachsentence is at most 40 words.The human inter-annotator agreement achievedon Test-set1 (Test-set2) is 90% (88.3%) and theagreement on the cause class is 70% (62.7%), re-spectively.
The kappa score on Test-set1 (Test-set2) is 0.75 (0.69), respectively.
The Test-set1(Test-set2) contains 25% (22%) causal instances,respectively.We employed Spearman?s rank correlation co-efficient (equation 10) to compare the ranked listof verb pairs based on the scores of our metricsand the rank given by the human annotators.
Thescore P ranges from +1 to ?1 where +1 and ?1show strong and negative correlation, respectively.P = n(?xiyi)?
(?xi)(?yi)?n(?x2i )?
(?xi)2?n(?y2i )?
(?yi)2(10)Here, n is the total number of verb pairs in thetest set, xi is the human annotation rank and yi isthe metric?s rank of verb pair i of the test set.
Thevalues of xi and yi are determined as follows.
Foreach verb pair, Ch is calculated which is the num-ber of cause labels given by both human annota-Metric CEA ECA ICA BCATest-set1 -0.077 0.144 0.427 0.435Test-set2 0.167 0.217 0.353 0.338Table 1: The Spearman?s rank correlation coeffi-cient for the metrics CEA, ECA, ICA and BCA.Figure 1: The percentage of causal (%c) and non-causal (%?c) test instances in Sc,Ac and S?c gen-erated by the metrics CEA, ECA, ICA and BCA.tors out of 6 instances of a verb pair.
The pairs areranked in descending order according to the scoreCh s.t.
the top scored pair(s) gets rank 50 and thenext to the top pair(s) gets rank 49 and so on.
Sim-ilarly, ranks are given to the verb pairs accordingto the metric?s scores.
This way of evaluation wasalso used by Beamer and Girju (2009) for tempo-rally ordered adjacent verb pairs.
But here, we areworking with verb pairs appearing in any temporalorder in both intra- and inter-sentential instances.We used ECA, ICA and BCA scores to gener-ate the ranked list of all verb pairs.
In this work,we also used the state-of-the-art causality iden-tifier CEA (Do et al 2011) as baseline metric.For each verb pair, we computed the likelihood ofcausality by taking the average of CEA scores onall instances of that pair in the development set.The results with Spearman?s rank correlationcoefficient in Table 1 show that CEA is not verycapable of matching the human ranked list of pairsas compared with our metrics (i.e., ECA, ICA andBCA).
Specifically, the difference is significantfor Test-set1 where the correlation coefficient withCEA goes below 0.
This behavior of CEA makessense because it is unsupervised and requires moreknowledge to perform well.
As compared withECA, both ICA and BCA perform significantlybetter to match human ranking.
The Spearman?sscore gain by BCA on Test-set1 is of about 30(52) points over ECA (CEA) and the gain by ICAon Test-set2 is of about 13 (18) points over ECA(CEA), respectively.In order to explain the behavior of our metrics27more clearly, we performed an evaluation of threecategories of verb pairs as follows.
We generatedthree categories of verb pairs using our metrics andCEA.
We combined two test sets to show the per-centage of total causal and non-causal instances ofverb pairs that lie in Sc, Ac and S?c using follow-ing procedure.
If a verb pair belongs to Sc and has3 causal and 2 non-causal instances after humanagreement, then these 5 instances are consideredmembers of Sc.
This step is performed for all verbpairs in the test set.
After this the percentage oftotal causal and non-causal test instances are cal-culated for each category (see Figure 1).Figure 1 reveals that ICA, BCA and CEA aresuccessful in pulling more causal instances in Scas compared to ECA.
But, CEA has a hard timedistinguishing cause from non-cause instances be-cause it also brings the highest percentage of non-causal instances in Sc.
The reason is the depen-dence of CEA on PMI scores of pairs of verbs andarguments to make decision for causality wherePMI is not good enough to distinguish a simplecorrelation from an asymmetric relation of causal-ity.
However, ICA and BCA work better by plac-ing less non-causal instances in Sc as comparedwith CEA.
ICA and BCA also work better be-cause by pulling more causal instances in Sc andAc, these metrics are keeping least percentage ofcausal instances in S?c.
Also, ICA and BCAbring more causal instances in Sc as comparedwith ECA by handling training data sparseness.Another important line of research is the con-struction of a classifier on top of the componentof knowledge base for the classes of cause andnon-cause relations.
This allows us to evaluate ourmodel in terms of standard evaluation measures -i.e., precision, recall and F-score.
These measurescan also be used to compare our model with su-pervised classifier depending merely on shallowcontextual features with no information from theknowledge base.
Due to space limitations, we planto present such classifiers and evaluation in the fu-ture.5.1 AnalysisIn this work, we have focused on determining thepredictive power of knowledge of causal associ-ations of verb pairs to identify causality betweenevents.
Our results reveal that our best metrics(i.e., ICA and BCA) bring desired behavior ofkeeping least percentage of total causal instancesin category S?c.
However, there is need to build aclassifier on top of knowledge base which can helpdetection of non-causal instances for verb pairs liein Sc and Ac.
Here, we state some brief detailsof our test set which can help building such clas-sifier in future.
An important aspect to consideris the highly skewed nature of real distribution oftest set.
There are only 23.69% causal instancesin the test set and majority of these instances (i.e.,56.7%) are intra-sentential instances.
Therefore, aclassifier should have mechanism to decide whyinter-sentential instances of event pair are non-causal most of the time.
For example, some inter-sentential events may not even be directly relevantat first place because they appear in different sen-tences.
Another critical point to consider is the en-coding of non-causal instances by strongly causalverb pairs.
For example, we asked one of the an-notators to identify strongly causal verb pairs outof 100 verb pairs of the test set.
There are 22such verb pairs determined by our annotator andeach of these pairs contain 43% causal instanceson the average.
There are many factors (e.g., tem-poral information, arguments of verbs) which canmake an instance of strongly causal verb pair non-causal.
For example, (call, respond) may encodecausality only if ecall temporally precedes erespondas demonstrated by the following instances.1.
Deputies spotted the truck parked at the home of the sus-pect?s father and called for assistance.
The Border Patrolagents and others responded.
(CAUSE)2.
Prime Minister of Israel promptly responded to thewidespread unrest in the West Bank and Gaza, saying thathe would call a timeout to rethink Israel?s commitment topeace talks.
(NON-CAUSE)In future, the above issues need to be addressedto improve performance for the current task.6 ConclusionIn this research, we have developed a knowledgebase (KBc1) of causal associations of verb pairsto detect causality.
This resource provides thecausal associations in terms of three categories ofverb pairs (i.e., Strongly Causal, Ambiguous andStrongly Non-Causal).
We have proposed a set ofknowledge rich metrics to learn these associations.Our analysis of results reveals the biases of differ-ent metrics and brings important insights into thefuture research directions to address the challengeof detecting causality between verbal events.1We will make the resource available.28ReferencesCollin F. Baker, Charles J. Fillmore and John B. Lowe.1998.
The Berkeley FrameNet project.
In proceed-ings of COLING-ACL.
Montreal, Canada.Brandon Beamer and Roxana Girju.
2009.
Using a Bi-gram Event Model to Predict Causal Potential.
Inproceedings of Computational Linguistics and intel-ligent Text Processing (CICLING), 2009.Steven Bethard and James H. Martin.
2008.
LearningSemantic Links from a Corpus of Parallel Temporaland Causal Relations.
In proceedings of ACL-08:HLT.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised learning of narrative event chains.
In pro-ceedings of ACL-HLT 2008.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised learning of narrative schemas and their par-ticipants.
In proceedings of ACL 2009.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained SemanticVerb Relations.
In proceedings of Conference onEmpirical Methods in Natural Language Processing(EMNLP-04).
Barcelona, Spain.Quang X.
Do, Yee S. Chen and Dan Roth.
2011.
Min-imally Supervised Event Causality Identication.
Inproceedings of EMNLP-2011.Roxana Girju.
2003.
Automatic detection of causalrelations for Question Answering.
Association forComputational Linguistics ACL, Workshop on Mul-tilingual Summarization and Question AnsweringMachine Learning and Beyond 2003.Paul Kingsbury, Martha Palmer and Mitch Marcus.2002.
Adding semantic annotation to the Penn Tree-Bank.
In proceedings of HLT-2002.
San Diego, Cal-ifornia.Karin Kipper, Hoa T. Dang, and Martha Palmer.
2000.Class-based construction of a verb lexicon.
In pro-ceedings of AAAI-2000.
Austin, TX.Daniel Marcu and Abdessamad Echihabi.
2001.
Anunsupervised approach to recognizing discourse re-lations.
In proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics(ACL).Peter Menzies.
2008.
Counterfactual theories of cau-sation.
Online Encyclopedia of Philosophy, 2008.George A. Miller.
1990.
WordNet: An online lexi-cal database.
International Journal of Lexicography,3(4).Judea Pearl.
2000.
Causality.
Cambridge UniversityPress.Emily Pitler, Annie Louis and Ani Nenkova.
2009.Automatic Sense Prediction for Implicit DiscourseRelations in Text.
In proceedings of ACL-IJCNLP,2009.Emily Pitler and Ani Nenkova.
2009.
Using Syntaxto Disambiguate Explicit Discourse Connectives inText.
In proceedings of ACL-IJCNLP, 2009.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, Bonnie Web-ber.
2010.
The penn discourse treebank 2.0.
Inproceedings of LREC 2008.Kira Radinsky and Eric Horvitz.
2013.
Mining theWeb to Predict Future Events.
In proceedings ofsixth ACM international conference on Web searchand data mining, WSDM ?13.Mehwish Riaz and Roxana Girju.
2010.
Another Lookat Causality: Discovering Scenario-Specific Contin-gency Relationships with No Supervision.
In pro-ceedings of the IEEE 4th International Conferenceon Semantic Computing (ICSC).Dafna Shahaf and Carlos Guestrin.
2010.
Connectingthe Dots Between News Articles.
In proceedings ofKnowledge Discovery and Data Mining KDD 2010.Craig Silverstein, Sergey Brin, Rajeev Motwani andJeff Ullman.
2000.
Scalable Techniques for Min-ing Causal Structures.
Data Mining and KnowledgeDiscovery, 2000, 4(2?3):163?192.Caroline Sporleder and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetor-ical relations: An assessment.
Journal of NaturalLanguage Engineering Volume 14 Issue 3, July 2008Pages 369?416.Patrick Suppes.
1970.
A Probabilistic Theory ofCausality.
Amsterdam: North-Holland PublishingCompany, 1970.James Woodward.
2008.
Causation and Manipulation.Online Encyclopedia of Philosophy, 2008.29Appendix A. NotationsThis appendix presents the details of important notations used in this paper.Notation Equation(s) Explanationevi 6, 7, 8, 9 Verbal event represented by the verb viKBc ?
Knowledge base of causal associations of verb pairsSc ?
Strongly Causal category of verb pairsAc ?
Ambiguous category of verb pairsS?c ?
Strongly Non-Causal category of verb pairsmi ?
Discourse markermc 1 Causal marker (e.g., because)f(I) 1 Function to select the most dependent pair from two dis-course segments conjoined with causal markerCD(vi, vj) 1, 2, 4, 6, 9 Causal dependency of the verb pair (vi, vj)PSI(vi, vj) 1, 3 Penalization factor for the verbs of the pair (vi, vj) withrespect to their distance from the causal markerpos(vi) 3 Distance of verb in terms of its position with respect tocausal markerC(vp) 3 Count of main verbs appearing before causal markerC(vq) 3 Count of main verbs appearing after causal markerECA(vi, vj) 4 Explicit Causal Association of the verb pair (vi, vj)V P 4, 6, 9 Set of intra- and inter-sentential instances of a verb pairI(vi, vj) 4, 6, 9 Instance of the verb pair (vi, vj)CI 4, 5, 6, 9 Tendency of the instance I to belong to cause class thanthe non-cause onec 5 Cause class?c 5 Non-cause classCmin ?
Minimum value of CI obtained on the development setCmax ?
Maximum value of CI obtained on the development setrC 7, 8 Class of cause rolerE 7, 8 Class of effect roleICA(vi, vj) 6 Implicit Causal Association of the verb pair (vi, vj)ERM(evi , evj ) 6, 7 Events Roles Matching (ERM) determines the negativelog-likelihood of events to belong to class of cause oreffect roleS(evi , rC) 8 Score of evi to belong to the class of cause roleS(evj , rE) 8 Score of evj to belong to the class of effect roleP (fk|.)
5, 8 Probability of feature fk given some classBCA(vi, vj) 9 Boosted Causal Association of the verb pair (vi, vj)Table 2: Details of notations.30
