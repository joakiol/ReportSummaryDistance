Proceedings of the Second Workshop on Statistical Machine Translation, pages 9?16,Prague, June 2007. c?2007 Association for Computational LinguisticsCCG Supertags in Factored Statistical Machine TranslationAlexandra Birch Miles Osborne Philipp Koehna.c.birch-mayne@sms.ed.ac.uk miles@inf.ed.ac.uk pkoehn@inf.ed.ac.ukSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LW, UKAbstractCombinatorial Categorial Grammar (CCG)supertags present phrase-based machinetranslation with an opportunity to accessrich syntactic information at a word level.The challenge is incorporating this informa-tion into the translation process.
Factoredtranslation models allow the inclusion of su-pertags as a factor in the source or target lan-guage.
We show that this results in an im-provement in the quality of translation andthat the value of syntactic supertags in flatstructured phrase-based models is largelydue to better local reorderings.1 IntroductionIn large-scale machine translation evaluations,phrase-based models generally outperform syntax-based models1.
Phrase-based models are effectivebecause they capture the lexical dependencies be-tween languages.
However, these models, whichare equivalent to finite-state machines (Kumar andByrne, 2003), are unable to model long range wordorder differences.
Phrase-based models also lack theability to incorporate the generalisations implicit insyntactic knowledge and they do not respect linguis-tic phrase boundaries.
This makes it difficult to im-prove reordering in phrase-based models.Syntax-based models can overcome some of theproblems associated with phrase-based models be-cause they are able to capture the long range struc-tural mappings that occur in translation.
Recently1www.nist.gov/speech/tests/mt/mt06eval official results.htmlthere have been a few syntax-based models thatshow performance comparable to the phrase-basedmodels (Chiang, 2005; Marcu et al, 2006).
How-ever, reliably learning powerful rules from paralleldata is very difficult and prone to problems withsparsity and noise in the data.
These models alsosuffer from a large search space when decoding withan integrated language model, which can lead tosearch errors (Chiang, 2005).In this paper we investigate the idea of incorporat-ing syntax into phrase-based models, thereby lever-aging the strengths of both the phrase-based modelsand syntactic structures.
This is done using CCGsupertags, which provide a rich source of syntacticinformation.
CCG contains most of the structure ofthe grammar in the lexicon, which makes it possi-ble to introduce CCG supertags as a factor in a fac-tored translation model (Koehn et al, 2006).
Fac-tored models allow words to be vectors of features:one factor could be the surface form and other fac-tors could contain linguistic information.Factored models allow for the easy inclusion ofsupertags in different ways.
The first approach is togenerate CCG supertags as a factor in the target andthen apply an n-gram model over them, increasingthe probability of more frequently seen sequencesof supertags.
This is a simple way of including syn-tactic information in a phrase-based model, and hasalso been suggested by Hassan et al (2007).
Forboth Arabic-English (Hassan et al, 2007) and ourexperiments in Dutch-English, n-gram models overCCG supertags improve the quality of translation.By preferring more likely sequences of supertags,it is conceivable that the output of the decoder is9more grammatical.
However, its not clear exactlyhow syntactic information can benefit a flat struc-tured model: the constraints contained within su-pertags are not enforced and relationships betweensupertags are not linear.
We perform experiments toexplore the nature and limits of the contribution ofsupertags, using different orders of n-gram models,reordering models and focussed manual evaluation.It seems that the benefit of using n-gram supertagsequence models is largely from improving reorder-ing, as much of the gain is eroded by using a lexi-calised reordering model.
This is supported by themanual evaluation which shows a 44% improvementin reordering Dutch-English verb final sentences.The second and novel way we use supertags isto direct the translation process.
Supertags on thesource sentence allows the decoder to make deci-sions based on the structure of the input.
The sub-categorisation of a verb, for instance, might help se-lect the correct translation.
Using multiple depen-dencies on factors in the source, we need a strat-egy for dealing with sparse data.
We propose usinga logarithmic opinion pool (Smith et al, 2005) tocombine the more specific models (which depend onboth words and supertags) with more general mod-els (which only depends on words).
This paper is thefirst to suggest this approach for combining multipleinformation sources in machine translation.Although the addition of supertags to phrase-based translation does show some improvement,their overall impact is limited.
Sequence modelsover supertags clearly result in some improvementsin local reordering but syntactic information con-tains long distance dependencies which are simplynot utilised in phrase-based models.2 Factored ModelsInspired by work on factored language models,Koehn et al (2006) extend phrase-based models toincorporate multiple levels of linguistic knowledgeas factors.
Phrase-based models are limited to se-quences of words as their units with no access toadditional linguistic knowledge.
Factors allow forricher translation models, for example, the gender ortense of a word can be expressed.
Factors also allowthe model to generalise, for example, the lemma of aword could be used to generalise to unseen inflectedforms.The factored translation model combines featuresin a log-linear fashion (Och, 2003).
The most likelytarget sentence t?
is calculated using the decision rulein Equation 1:t?
= argmaxt{M?m=1?mhm(sFs1 , tFt1 )}(1)t?
?M?m=1?mhm(sFs1 , tFt1 ) (2)where M is the number of features, hm(sFs1 , tFt1 )are the feature functions over the factors, and ?
arethe weights which combine the features which areoptimised using minimum error rate training (Venu-gopal and Vogel, 2005).
Each function depends on avector sFs1 of source factors and a vector tFt1 of tar-get factors.
An example of a factored model used inupcoming experiments is:t?
?M?m=1?mhm(sw, twc) (3)where sw means the model depends on (s)ource(w)ords, and twc means the model generates (t)arget(w)ords and (c)cg supertags.
The model is showngraphically in Figure 1.WordWordCCGSOURCE TARGETFigure 1.
Factored translation with source words deter-mining target words and CCG supertagsFor our experiments we used the following fea-tures: the translation probabilities Pr(sFs1 |tFt1 ) andPr(tFt1 |sFs1 ), the lexical weights (Koehn et al, 2003)lex(sFs1 |tFt1 ) and lex(tFt1 |sFs1 ), and a phrase penaltye, which allows the model to learn a preference forlonger or shorter phrases.
Added to these features10is the word penalty e?1 which allows the model tolearn a preference for longer or shorter sentences,the distortion model d that prefers monotone wordorder, and the language model probability Pr(t).All these features are logged when combined in thelog-linear model in order to retain the impact of veryunlikely translations or sequences.One of the strengths of the factored model is itallows for n-gram distributions over factors on thetarget.
We call these distributions sequence models.By analogy with language models, for example, wecan construct a bigram sequence model as follows:p(f1, f2, .
.
.
fn) = p(f1)n?i=2p(fi|f(i?1))where f is a factor (eg.
CCG supertags) and n isthe length of the string.
Sequence models over POStags or supertags are smaller than language modelsbecause they have restricted lexicons.
Higher or-der, more powerful sequence models can thereforebe used.Applying multiple factors in the source can lead tosparse data problems.
One solution is to break downthe translation into smaller steps and translate eachfactor separately like in the following model wheresource words are translated separately to the sourcesupertags:t?
?M?m=1?mhm(sw, tw) +N?n=1?nhn(sc, tw)However, in many cases multiple dependenciesare desirable.
For instance translating CCG su-pertags independently of words could introduce er-rors.
Multiple dependencies require some form ofbacking off to simpler models in order to cover thecases where, for instance, the word has been seen intraining, but not with that particular supertag.
Dif-ferent backoff paths are possible, and it would beinteresting but prohibitively slow to apply a strat-egy similar to generalised parallel backoff (Bilmesand Kirchhoff, 2003) which is used in factored lan-guage models.
Backoff in factored language mod-els is made more difficult because there is no ob-vious backoff path.
This is compounded for fac-tored phrase-based translation models where one hasto consider backoff in terms of factors and n-gramlengths in both source and target languages.
Fur-thermore, the surface form of a word is probably themost valuable factor and so its contribution must al-ways be taken into account.
We therefore did not usebackoff and chose to use a log-linear combination offeatures and models instead.Our solution is to extract two translation models:t?
?M?m=1?mhm(swc, tw) +N?n=1?nhn(sw, tw) (4)One model consists of more specific features mand would return log probabilities, for examplelog2Pr(tw|swc), if the particular word and supertaghad been seen before in training.
Otherwise it re-turns ?C, a negative constant emulating log2(0).The other model consist of more general featuresn and always returns log probabilities, for examplelog2Pr(tw|sw).3 CCG and SupertagsCCGs have syntactically rich lexicons and a smallset of combinatory operators which assemble theparse-trees.
Each word in the sentence is assigned acategory from the lexicon.
A category may either beatomic (S, NP etc.)
or complex (S\S, (S\NP)/NPetc.).
Complex categories have the general form?/?
or ?\?
where ?
and ?
are themselves cate-gories.
An example of a CCG parse is given:Peter eats applesNP (S\NP)/NP NP>S\NP<Swhere the derivation proceeds as follows: ?eats?is combined with ?apples?
under the operation offorward application.
?eats?
can be thought of as afunction that takes a NP to the right and returns aS\NP.
Similarly the phrase ?eats apples?
can bethought of as a function which takes a noun phraseNP to the left and returns a sentence S. This opera-tion is called backward application.A sentence together with its CCG categories al-ready contains most of the information present in afull parse.
Because these categories are lexicalised,11they can easily be included into factored phrase-based translation.
CCG supertags are categories thathave been provided by a supertagger.
Supertagswere introduced by Bangalore (1999) as a way of in-creasing parsing efficiency by reducing the numberof structures assigned to each word.
Clark (2002)developed a suppertagger for CCG which uses aconditional maximum entropy model to estimate theprobability of words being assigned particular cat-egories.
Here is an example of a sentence that hasbeen supertagged in the training corpus:We all agree on that .NP NP\NP (S[dcl]\NP)/PP PP/NP NP .The verb ?agree?
has been assigned a complex su-pertag (S[dcl]\NP)/PP which determines the typeand direction of its arguments.
This information canbe used to improve the quality of translation.4 ExperimentsThe first set of experiments explores the effect ofCCG supertags on the target, translating from Dutchinto English.
The last experiment shows the effectof CCG supertags on the source, translating fromGerman into English.
These language pairs presenta considerable reordering challenge.
For example,Dutch and German have SOVword order in subordi-nate clauses.
This means that the verb often appearsat the end of the clause, far from the position of theEnglish verb.4.1 Experimental SetupThe experiments were run using Moses2, an opensource factored statistical machine translation sys-tem.
The SRILM language modelling toolkit (Stol-cke, 2002) was used with modified Kneser-Ney dis-counting and interpolation.
The CCG supertag-ger (Clark, 2002; Clark and Curran, 2004) was pro-vided with the C&C Language Processing Tools3.The supertagger was trained on the CCGBank inEnglish (Hockenmaier and Steedman, 2005) and inGerman (Hockenmaier, 2006).The Dutch-English parallel training data comesfrom the Europarl corpus (Koehn, 2005) and ex-cludes the proceedings from the last quarter of 2000.2see http://www.statmt.org/moses/3see http://svn.ask.it.usyd.edu.au/trac/candc/wikiThis consists of 855,677 sentences with a maximumof 50 words per sentence.
500 sentences of tuningdata and the 2000 sentences of test data are takenfrom the ACLWorkshop on Building and Using Par-allel Texts4.The German-English experiments use data fromthe NAACL 2006 Workshop on Statistical MachineTranslation5.
The data consists of 751,088 sentencesof training data, 500 sentences of tuning data and3064 sentences of test data.
The English and Ger-man training sets were POS tagged and supertaggedbefore lowercasing.
The language models and thesequence models were trained on the Europarl train-ing data.
Where not otherwise specified, the POStag and supertag sequence models are 5-gram mod-els and the language model is a 3-gram model.4.2 Sequence Models Over SupertagsOur first Dutch-English experiment seeks to estab-lish what effect sequence models have on machinetranslation.
We show that supertags improve trans-lation quality.
Together with Shen et al (2006) it isone of the first results to confirm the potential of thefactored model.Model BLEUsw, tw 23.97sw, twp 24.11sw, twc 24.42sw, twpc 24.43Table 1.
The effect of sequence models on Dutch-EnglishBLEU score.
Factors are (w)ords, (p)os tags, (c)cg su-pertags on the source s or the target tTable 1 shows that sequence models over CCG su-pertags in the target (model sw, twc) improves overthe baseline (model sw, tw) which has no supertags.Supertag sequence models also outperform modelswhich apply POS tag sequence models (sw, twp)and, interestingly do just as well as models whichapply both POS tag and supertag sequence mod-els (sw, twps).
Supertags are more informative thanPOS tags as they contain the syntactic context of aword.These experiments were run with the distortionlimit set to 6.
This means that at most 6 words in4see http://www.statmt.org/wpt05/5see http://www.statmt.org/wpt06/12the source sentence can be skipped.
We tried settingthe distortion limit to 15 to see if allowing longerdistance reorderings with CCG supertag sequencemodels could further improve performance, howeverit resulted in a decrease in performance to a BLEUscore of 23.84.4.3 Manual AnalysisThe BLEU score improvement in Table 1 does notexplain how the supertag sequence models affect thetranslation process.
As suggested by Callison-Burchet al(2006) we perform a focussed manual analysisof the output to see what changes have occurred.From the test set, we randomly selected 100sentences which required reordering of verbs: theDutch sentences ended with a verb which had to bemoved forward in the English translation.
We recordwhether or not the verb was correctly translated andwhether it was reordered to the correct position inthe target sentence.Model Translated Reorderedsw, tw 81 36sw, twc 87 52Table 2.
Analysis of % correct translation and reorderingof verbs for Dutch-English translationIn Table 2 we can see that the addition of the CCGsupertag sequence model improved both the transla-tion of the verbs and their reordering.
However, theimprovement is much more pronounced for reorder-ing.
The difference in the reordering results is signif-icant at p < 0.05 using the ?2 significance test.
Thisshows that the syntactic information in the CCG su-pertags is used by the model to prefer better wordorder for the target sentence.In Figure 2 we can see two examples of Dutch-English translations that have improved with the ap-plication of CCG supertag sequence models.
In thefirst example the verb ?heeft?
occurs at the end of thesource sentence.
The baseline model (sw, tw) doesnot manage to translate ?heeft?.
The model with theCCG supertag sequence model (sw, twc) translates itcorrectly as ?has?
and reorders it correctly 4 placesto the left.
The second example also shows the se-quence model correctly translating the Dutch verb atthe end of the sentence ?nodig?.
One can see that itis still not entirely grammatical.The improvements in reordering shown here arereorderings over a relatively short distance, two orthree positions.
This is well within the 5-gram orderof the CCG supertag sequence model and we there-fore consider this to be local reordering.4.4 Order of the Sequence ModelThe CCG supertags describe the syntactic contextof the word they are attached to.
Therefore theyhave an influence that is greater in scope than sur-face words or POS tags.
Increasing the order ofthe CCG supertag sequence model should also in-crease the ability to perform longer distance reorder-ing.
However, at some point the reliability of thepredictions of the sequence models is impaired dueto sparse counts.Model None 1gram 3gram 5gram 7gramsw, twc 24.18 23.96 24.19 24.42 24.32sw, twpc 24.34 23.86 24.09 24.43 24.14Table 3.
BLUE scores for Dutch-English models whichapply CCG supertag sequence models of varying ordersIn Table 3 we can see that the optimal order forthe CCG supertag sequence models is 5.4.5 Language Model vs. SupertagsThe language model makes a great contribution tothe correct order of the words in the target sentence.In this experiment we investigate whether by using astronger language model the contribution of the se-quence model will no longer be relevant.
The rel-ative contribution of the language mode and differ-ent sequence models is investigated for different lan-guage model n-gram lengths.Model None 1gram 3gram 5gram 7gramsw, tw - 21.22 23.97 24.05 24.13sw, twp 21.87 21.83 24.11 24.25 24.06sw, twc 21.75 21.70 24.42 24.67 24.60sw, twpc 21.99 22.07 24.43 24.48 24.42Table 4.
BLEU scores for Dutch-English models which uselanguage models of increasing n-gram length.
ColumnNone does not apply any language model.
Model sw, twdoes not apply any sequence models, and model sw, twpcapplies both POS tag and supertag sequence models.In Table 4 we can see that if no language modelis present(None), the system benefits slightly from13source:hij kan toch niet beweren dat hij daar geen exacte informatie over heeft !reference: how can he say he does not have any precise information ?sw, tw:he cannot say that he is not an exact information about .sw, twc: he cannot say that he has no precise information on this !source: wij moeten hun verwachtingen niet beschamen .
meer dan ooit hebben al die landen thans onze bijstand nodigreference: we must not disappoint them in their expectations , and now more than ever these countries need our helpsw, tw:we must not fail to their expectations , more than ever to have all these countries now our assistance necessarysw, twc: we must not fail to their expectations , more than ever , those countries now need our assistanceFigure 2.
Examples where the CCG supertag sequence model improves Dutch-English translationhaving access to all the other sequence models.However, the language model contribution is verystrong and in isolation contributes more to transla-tion performance than any other sequence model.Even with a high order language model, applyingthe CCG supertag sequence model still seems to im-prove performance.
This means that even if we usea more powerful language model, the structural in-formation contained in the supertags continues to bebeneficial.4.6 Lexicalised Reordering vs. SupertagsIn this experiment we investigate using a strongerreordering model to see how it compares to the con-tribution that CCG supertag sequence models make.Moses implements the lexicalised reordering modeldescribed by Tillman (2004), which learns whetherphrases prefer monotone, inverse or disjoint orienta-tions with regard to adjacent phrases.
We apply thisreordering models to the following experiments.Model None Lex.
Reord.sw, tw 23.97 24.72sw, twc 24.42 24.78Table 5.
Dutch-English models with and without a lexi-calised reordering model.In Table 5 we can see that lexicalised reorder-ing improves translation performance for both mod-els.
However, the improvement that was seen us-ing CCG supertags without lexicalised reordering,almost disappears when using a stronger reorderingmodel.
This suggests that CCG supertags?
contribu-tion is similar to that of a reordering model.
The lex-icalised reordering model only learns the orientationof a phrase with relation to its adjacent phrase, so itsinfluence is very limited in range.
If it can replaceCCG supertags, it suggests that supertags?
influenceis also within a local range.4.7 CCG Supertags on SourceSequence models over supertags improve the perfor-mance of phrase-based machine translation.
How-ever, this is a limited way of leveraging the rich syn-tactic information available in the CCG categories.We explore the potential of letting supertags directtranslation by including them as a factor on thesource.
This is similar to syntax-directed translationoriginally proposed for compiling (Aho and Ullman,1969), and also used in machine translation (Quirk etal., 2005; Huang et al, 2006).
Information about thesource words?
syntactic function and subcategori-sation can directly influence the hypotheses beingsearched in decoding.
These experiments were per-formed on the German to English translation task,in contrast to the Dutch to English results given inprevious experiments.We use a model which combines more specificdependencies on source words and source CCG su-pertags, with a more general model which only hasdependancies on the source word, see Equation 4.We explore two different ways of balancing the sta-tistical evidence from these multiple sources.
Thefirst way to combine the general and specific sourcesof information is by considering features from bothmodels as part of one large log-linear model.
How-ever, by including more and less informative fea-tures in one model, we may transfer too much ex-planatory power to the more specific features.
Toovercome this problem, Smith et al (2006) demon-strated that using ensembles of separately trainedmodels and combining them in a logarithmic opin-ion pool (LOP) leads to better parameter values.This approach was used as the second way in which14we combined our models.
An ensemble of log-linearmodels was combined using a multiplicative con-stant ?
which we train manually using held out data.t?
?M?m=1?mhm(swc, tw) + ?
(N?n=1?nhn(sw, tw))Typically, the two models would need to be nor-malised before being combined, but here the multi-plicative constant fulfils this ro?le by balancing theirseparate contributions.
This is the first work sug-gesting the application of LOPs to decoding in ma-chine translation.
In the future more sophisticatedtranslation models and ensembles of models willneed methods such as LOPs in order to balance sta-tistical evidence from multiple sources.Model BLEUsw, tw 23.30swc, tw 19.73single 23.29LOP 23.46Table 6.
German-English: CCG supertags are used as afactor on the source.
The simple models are combined intwo ways: either as a single log-linear model or as a LOPof log-linear modelsTable 6 shows that the simple, general model(model sw, tw) performs considerably better thanthe simple specific model, where there are multi-ple dependencies on both words and CCG supertags(model swc, tw).
This is because there are words inthe test sentence that have been seen before but notwith the CCG supertag.
Statistical evidence frommultiple sources must be combined.
The first wayto combine them is to join them in one single log-linear model, which is trained over many features.This makes finding good weights difficult as the in-fluence of the general model is greater, and its dif-ficult for the more specific model to discover goodweights.
The second method for combining the in-formation is to use the weights from the separatelytrained simple models and then combine them in aLOP.
Held out data is used to set the multiplicativeconstant needed to balance the contribution of thetwo models.
We can see that this second approach ismore successful and this suggests that it is importantto carefully consider the best ways of combining dif-ferent sources of information when using ensemblesof models.
However, the results of this experimentare not very conclusive.
There is no uncertainty inthe source sentence and the value of modelling it us-ing CCG supertags is still to be demonstrated.5 ConclusionThe factored translation model allows for the inclu-sion of valuable sources of information in many dif-ferent ways.
We have shown that the syntacticallyrich CCG supertags do improve the translation pro-cess and we investigate the best way of includingthem in the factored model.
Using CCG supertagsover the target shows the most improvement, espe-cially when using targeted manual evaluation.
How-ever, this effect seems to be largely due to improvedlocal reordering.
Reordering improvements can per-haps be more reliably made using better reorderingmodels or larger, more powerful language models.A further consideration is that supertags will alwaysbe limited to the few languages for which there aretreebanks.Syntactic information represents embeddedstructures which are naturally incorporated intogrammar-based models.
The ability of a flat struc-tured model to leverage this information seems to belimited.
CCG supertags?
ability to guide translationwould be enhanced if the constraints encoded inthe tags were to be enforced using combinatoryoperators.6 AcknowledgementsWe thank Hieu Hoang for assistance with Moses, Ju-lia Hockenmaier for access to CCGbank lexicons inGerman and English, and Stephen Clark and JamesCurran for providing the supertagger.
This work wassupported in part under the GALE program of theDefense Advanced Research Projects Agency, Con-tract No.
HR0011-06-C-0022 and in part under theEuroMatrix project funded by the European Com-mission (6th Framework Programme).15ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1969.
Properties of syn-tax directed translations.
Journal of Computer and SystemSciences, 3(3):319?334.Srinivas Bangalore and Aravind Joshi.
1999.
Supertagging:An approach to almost parsing.
Computational Linguistics,25(2):237?265.Jeff Bilmes and Katrin Kirchhoff.
2003.
Factored languagemodels and generalized parallel backoff.
In Proceedings ofthe North American Association for Computational Linguis-tics Conference, Edmonton, Canada.Chris Callison-Burch, Miles Osborne, and Philipp Koehn.2006.
Re-evaluating the role of Bleu in machine transla-tion research.
In Proceedings of the European Chapter ofthe Association for Computational Linguistics, Trento, Italy.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the Asso-ciation for Computational Linguistics, pages 263?270, AnnArbor, Michigan.Stephen Clark and James R. Curran.
2004.
Parsing the wsjusing ccg and log-linear models.
In Proceedings of theAssociation for Computational Linguistics, pages 103?110,Barcelona, Spain.Stephen Clark.
2002.
Supertagging for combinatory categorialgrammar.
In Proceedings of the International Workshop onTree Adjoining Grammars, pages 19?24, Venice, Italy.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.
Su-pertagged phrase-based statistical machine translation.
InProceedings of the Association for Computational Linguis-tics, Prague, Czech Republic.
(to appear).Julia Hockenmaier and Mark Steedman.
2005.
Ccgbank man-ual.
Technical Report MS-CIS-05-09, Department of Com-puter and Information Science, University of Pennsylvania.Julia Hockenmaier.
2006.
Creating a ccgbank and a wide-coverage ccg lexicon for german.
In Proceedings of the In-ternational Conference on Computational Linguistics and ofthe Association for Computational Linguistics, Sydney, Aus-tralia.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
Asyntax-directed translator with extended domain of locality.In Proceedings of the Workshop on Computationally HardProblems and Joint Inference in Speech and Language Pro-cessing, pages 1?8, New York City, New York.
Associationfor Computational Linguistics.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.
Statisti-cal phrase-based translation.
In Proceedings of the HumanLanguage Technology and North American Association forComputational Linguistics Conference, pages 127?133, Ed-monton, Canada.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Chris Callison-Burch, MarcelloFederico, Nicola Bertoldi, Richard Zens, Chris Dyer, BrookeCowan, Wade Shen, Christine Moran, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2006.
Open source toolkitfor statistical machine translation.
In Summer Workshop onLanguage Engineering, John Hopkins University Center forLanguage and Speech Processing.Philipp Koehn.
2005.
Europarl: A parallel corpus for statisticalmachine translation.
In MT Summit.Shankar Kumar and William Byrne.
2003.
A weighted finitestate transducer implementation of the alignment templatemodel for statistical machine translation.
In Proceedings ofthe Human Language Technology and North American As-sociation for Computational Linguistics Conference, pages63?70, Edmonton, Canada.Daniel Marcu, Wei Wang, Abdessamad Echihabi, and KevinKnight.
2006.
SPMT: Statistical machine translation withsyntactified target language phrases.
In Proceedings of theConference on Empirical Methods in Natural Language Pro-cessing, pages 44?52, Sydney, Australia.Franz Josef Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of the Associ-ation for Computational Linguistics, pages 160?167, Sap-poro, Japan.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informed phrasalSMT.
In Proceedings of the Association for ComputationalLinguistics, pages 271?279, Ann Arbor, Michigan.Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello Fed-erico.
2006.
The JHU workshop 2006 IWSLT system.
InProceedings of the International Workshop on Spoken Lan-guage Translation (IWSLT), pages 59?63, Kyoto, Japan.Andrew Smith and Miles Osborne.
2006.
Using gazetteers indiscriminative information extraction.
In The Conference onNatural Language Learning, New York City, USA.Andrew Smith, Trevor Cohn, and Miles Osborne.
2005.
Loga-rithmic opinion pools for conditional random fields.
In Pro-ceedings of the Association for Computational Linguistics,pages 18?25, Ann Arbor, Michigan.Andreas Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proceedings of Spoken Language Process-ing, pages 901?904.Christoph Tillman.
2004.
A unigram orientation model forstatistical machine translation.
In Proceedings of the Hu-man Language Technology and North American Associationfor Computational Linguistics Conference, pages 101?104,Boston, USA.
Association for Computational Linguistics.Ashish Venugopal and Stephan Vogel.
2005.
Considerationsin MCE and MMI training for statistical machine transla-tion.
In Proceedings of the European Association for Ma-chine Translation, Budapest, Hungary.16
