Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 160?165,Dublin, Ireland, August 23-24, 2014.CECL: a New Baseline and a Non-Compositional Approach for the SickBenchmarkYves BestgenCentre for English Corpus LinguisticsUniversit?e catholique de Louvainyves.bestgen@uclouvain.beAbstractThis paper describes the two proceduresfor determining the semantic similaritiesbetween sentences submitted for the Se-mEval 2014 Task 1.
MeanMaxSim, anunsupervised procedure, is proposed as anew baseline to assess the efficiency gainprovided by compositional models.
It out-performs a number of other baselines bya wide margin.
Compared to the word-overlap baseline, it has the advantage oftaking into account the distributional simi-larity between words that are also involvedin compositional models.
The secondprocedure aims at building a predictivemodel using as predictors MeanMaxSimand (transformed) lexical features describ-ing the differences between each sentenceof a pair.
It finished sixth out of 17 teamsin the textual similarity sub-task and sixthout of 19 in the textual entailment sub-task.1 IntroductionThe SemEval-2014 Task 1 (Marelli et al., 2014a)was designed to allow a rigorous evaluationof compositional distributional semantic models(CDSMs).
CDSMs aim to represent the meaningof phrases and sentences by composing the dis-tributional representations of the words they con-tain (Baroni et al., 2013; Bestgen and Cabiaux,2002; Erk and Pado, 2008; Grefenstette, 2013;Kintsch, 2001; Mitchell and Lapata, 2010); theyare thus an extension of Distributional SemanticModels (DSMs), which approximate the meaningof words with vectors summarizing their patternsof co-occurrence in a corpus (Baroni and Lenci,This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2010; Bestgen et al., 2006; Kintsch, 1998; Lan-dauer and Dumais, 1997).
The dataset for thistask, called SICK (Sentences Involving Composi-tional Knowledge), consists of almost 10,000 En-glish sentence pairs annotated for relatedness inmeaning and entailment relation by ten annotators(Marelli et al., 2014b).The rationale behind this dataset is that ?un-derstanding when two sentences have close mean-ings or entail each other crucially requires a com-positional semantics step?
(Marelli et al., 2014b),and thus that annotators judge the similarity be-tween the two sentences of a pair by first build-ing a mental representation of the meaning of eachsentence and then comparing these two represen-tations.
However, another option was availableto the annotators.
They could have paid atten-tion only to the differences between the sentences,and assessed the significance of these differences.Such an approach could have been favored by thedataset built on the basis of a thousand sentencesmodified by a limited number of (often) veryspecific transformations, producing sentence pairsthat might seem quite repetitive.
An analysis con-ducted during the training phase of the challengebrought some support for this hypothesis.
Theanalysis focused on pairs of sentences in which theonly difference between the two sentences was thereplacement of one content word by another, as inA man is singing to a girl vs. A man is singing toa woman, but also in A man is sitting in a fieldvs.
A man is running in a field.
The materialwas divided into two parts, 3500 sentence pairsin the training set and the remaining 1500 in thetest set.
First, the average similarity score for eachpair of interchanged words was calculated on thetraining set (e.g., in this sample, there were 16 sen-tence pairs in which woman and man were inter-changed, and their mean similarity score was 3.6).Then, these mean scores were used as the similar-ity scores of the sentence pairs of the test sample160in which the same words were interchanged.
Thecorrelation between the actual scores and the pre-dicted score was 0.83 (N=92), a value that can beconsidered as very high, given the restrictions onthe range in which the predicted similarity scoresvary (min=3.5 and max=5.0; Howell, 2008, pp.272-273).
It is important to note that this observa-tion does not prove that the participants have notbuilt a compositional representation, especially asit only deals with a very specific type of trans-formation.
It nevertheless suggests that analyz-ing only the differences between the sentences ofa pair could allow the similarity between them tobe effectively estimated.Following these observations, I opted to tryto determine the degree of efficacy that can beachieved by two non-compositional approaches.The first approach, totally unsupervised, is pro-posed as a new baseline to evaluate the efficacygains brought by compositional systems.
The sec-ond, a supervised approach, aims to capitalize onthe properties of the SICK benchmark.
Whilethese approaches have been developed specificallyfor the semantic relatedness sub-task, the secondhas also been applied to the textual entailment sub-task.
This paper describes the two proposed ap-proaches, their implementation in the context ofSemEval 2014 Task 1, and the results obtained.2 Proposed Approaches2.1 A New Baseline for CDSMAn evident baseline in the field of CDSM is basedon the proportion of common words in two sen-tences after the removal (or retaining) of stopwords (Cheung and Penn, 2012).
Its main weak-ness is that it does not take into account the seman-tic similarities between the words that are com-bined in the CDSM models.
It follows that a com-positional approach may seem significantly betterthan this baseline, even if it is not compositionalitythat matters but only the distributional part.
At firstglance, this problem can be circumvented by usingas baseline a simple compositional model like theadditive model.
The analyses below show that thismodel is much less effective for the SILK datasetthan the distributional baseline proposed here.MeanMaxSim, the proposed baseline, is an ex-tension of the classic measure based on the pro-portion of common words, taking advantage of thedistributional similarity but not of compositional-ity.
It corresponds to the mean, calculated using allthe words of the two sentences, of the maximumsemantic similarity between each word in a sen-tence and all the words of the other sentence.
Moreformaly, given two sentences a = (a1, .., an) andb = (b1, ..bm),MMS =(?imaxjsim(ai,bj)+?jmaxisim(ai,bj))n+mIn this study, the cosine between the word distri-butional representations was used as the measureof semantic similarity, but other measures may beused.
The common words of the two sentenceshave an important impact on MeanMaxSim, sincetheir similarity with themselves is equal to themaximum similarity possible.
Their impact wouldbe much lower if the average similarity betweena word and all the words in the other sentencewere employed instead of the maximum similar-ity.
Several variants of this measure can be used,for example not taking into account every instancewhere a word is repeated in a sentence or not al-lowing any single word to be the ?most similar?
toseveral other words.2.2 A Non-Compositional Approach Basedon the Differences Between the SentencesThe main limitation of the first approach in thecontext of this challenge is that it is completelyunsupervised and therefore does not take advan-tage of the training set provided by the task orga-nizers.
The second approach addresses this limi-tation.
It aims to build a predictive model, usingas predictors MeanMaxSim but also lexical fea-tures describing the differences between each sen-tence of a pair.
For the extraction of these fea-tures, each pair of sentences of the whole dataset(training and testing sets) is analyzed to iden-tify all the lemmas that are not present with thesame frequency in both sentences.
Each of thesedifferences is encoded as a feature whose valuecorresponds to the unsigned frequency difference.This step leads to a two-way contingency tablewith sentence pairs as rows and lexical featuresas columns.
Correspondence Analysis (Blasiusand Greenacre, 1994; Lebart et al., 2000), a sta-tistical procedure available in many off-the-shelfsoftware like R (Nenadic and Greenacre, 2006), isthen used to decompose this table into orthogonaldimensions ordered according to the correspond-ing part of associations between rows and columnsthey explain.
Each row receives a coordinate onthese dimensions and these coordinates are used aspredictors of the relatedness scores of the sentence161pairs.
In this way, not only are the frequencies oflexical features transformed into continuous pre-dictors, but these predictors also take into accountthe redundancy between the lexical features.
Fi-nally, a predictive model is built on the basis ofthe training set by means of multiple linear regres-sion with stepwise selection of the best predictors.For the textual entailment sub-task, the same pro-cedure was used except that the linear regressionwas replaced by a linear discriminant analysis.3 Implementation DetailsThis section describes the steps and additionalresources used to implement the proposed ap-proaches for the SICK challenge.3.1 Preprocessing of the DatasetAll sentences were tokenized and lemmatized bythe Stanford Parser (de Marneffe et al., 2006;Toutanova et al., 2003).3.2 Distributional SemanticsLatent Semantic Analysis (LSA), a classical DSM(Deerwester et al., 1991; Landauer et al., 1998),was used to gather the semantic similarity betweenwords from corpora.
The starting point of the anal-ysis is a lexical table containing the frequencies ofevery word in each of the text segments includedin the corpus.
This table is submitted to a singu-lar value decomposition, which extracts the mostsignificant orthogonal dimensions.
In this seman-tic space, the meaning of a word is represented bya vector and the semantic similarity between twowords is estimated by the cosine between their cor-responding vectors.Three corpora were used to estimate these simi-larities.
The first one, the TASA corpus, is com-posed of excerpts, with an approximate averagelength of 250 words, obtained by a random sam-pling of texts that American students read (Lan-dauer et al., 1998).
The version to which T.K.Landauer (Institute of Cognitive Science, Univer-sity of Colorado, Boulder) provided access con-tains approximately 12 million words.The second corpus, the BNC (British NationalCorpus; Aston and Burnard, 1998) is composedof approximately 100 million words and coversmany different genres.
As the documents includedin this corpus can be of up to 45,000 words, theywere divided into segments of 250 words, the lastsegment of a text being deleted if it containedfewer than 250 words.The third corpus (WIKI, approximately 600million words after preprocessing) is derived fromthe Wikipedia Foundation database, downloadedin April 2011.
It was built using WikiExtractor.pyby A. Fuschetto.
As for the BNC, the texts werecut into 250-word segments, and any segment offewer than 250 words was deleted.All these corpora were lemmatized by meansof the TreeTagger (Schmid, 1994).
In addition, aseries of functional words were removed as wellas all the words whose total frequency in the cor-pus was lower than 10.
The resulting (log-entropyweighted) matrices of co-occurrences were sub-mitted to a singular value decomposition (SVD-PACKC, Berry et al., 1993) and the first 300 eigen-vectors were retained.3.3 Unsupervised Approach DetailsBefore estimating the semantic similarity betweena pair of sentences using MeanMaxSim, words (intheir lemmatized forms) considered as stop wordswere filtered out.
This stop word list (n=82), wasbuilt specifically for the occasion on the basis ofthe list of the most frequent words in the trainingdataset.3.4 Supervised Approach DetailsTo identify words not present with the same fre-quency in both sentences, all the lemmas (includ-ing those belonging to the stop word list) weretaken into account.
The optimization of the param-eters of the predictive model was performed usinga three-fold cross-validation procedure, with twothirds of the 5000 sentence pairs for training andthe remaining third for testing.
The values testedby means of an exhaustive search were:?
Minimum threshold frequency of the lexicalfeatures in the complete dataset: from 10 to70 by step of 10.?
Number of dimensions retained from the CA:from 10 to the total number of dimensionsavailable by step of 10.?
P-value threshold to enter or remove predic-tors from the model: 0.01 and from 0.05 to0.45 by step of 0.05.This cross-validation procedure was repeatedfive times, each time changing the random distri-bution of sentence pairs in the samples.
The fi-nal values of the three parameters were selected162on the basis of the average correlation calculatedover all replications.
For the relatedness sub-task,the selected values were a minimum threshold fre-quency of 40, 140 dimensions and a p-value of0.20.
For the entailment sub-task, they were aminimum threshold frequency of 60, 100 dimen-sions and a p-value of 0.25.4 Results4.1 Semantic Relatedness Sub-TaskThe main measure of performance selected by thetask organizers was the Pearson correlation, calcu-lated on the test set (4927 sentence pairs), betweenthe mean values of similarity according to the an-notators and the values predicted by the automaticprocedures.Unsupervised Approach: MeanMaxSim.
Ta-ble 1 shows the results obtained by MeanMaxSim,based on the three corpora, and by three otherbaselines:?
WO: The word-overlap baseline proposed bythe organizers of the task, computed as thenumber of distinct tokens in both sentencesdivided by the number of distinct tokens inthe longer sentence, optimizing the numberof the most frequent words stripped off thesentences on the test set.?
SWL: The word-overlap baseline computedas in WO but using lemmas instead of wordsand the stop words list.?
ADD: The simple additive compositionalmodel, in which each sentence is representedby the sum of the vectors of the lemmas thatcompose it (stripping off stop words and us-ing the best performing corpus) and the simi-larity is the cosine between these two vectors(Bestgen et al., 2010; Guevara, 2011) .MeanMaxSim r Baseline rTASA 0.696 WO 0.627BNC 0.698 SWL 0.613WIKI 0.696 ADD 0.500Table 1: Pearson?s correlation for MeanMaxSimand several other baselines on the test set.MeanMaxSim produces almost identical resultsregardless of the corpus used.
The lack of differ-ence between the three corpora was unexpected.It could be related to the type of vocabulary usedin the SICK materials, seemingly mostly frequentand concrete words whose use could be relativelysimilar in the three corpora.
MeanMaxSim per-formance is clearly superior to all other baselines;among these, the additive model is the worst.
Thisresult is important because it shows that this com-positional model is not, for the SICK benchmark,the most interesting baseline to assess composi-tional approaches.
In the context of the best per-formance of the other teams, MeanMaxSim is(hopefully) well below the most effective proce-dures, which reached correlations above 0.80.Supervised Approach.
The supervised ap-proach resulted in a correlation of 0.78044, a valuewell above all baselines reported above.
This cor-relation ranked the procedure sixth out of 17, tiedwith another team (0.78019).
The three best teamsscored significantly higher, with correlations be-tween 0.826 and 0.828.4.2 Textual Entailment Sub-TaskOnly the supervised approach was used for thissub-task.
The proposed procedure achieved an ac-curacy of 79.998%, which ranks it sixth again, butout of 19 teams, still at a respectable distance fromthe best performance (84.575%).5 ConclusionThe main contribution of this research seems to bethe proposal of MeanMaxSim as baseline for eval-uating CDSM.
It outperforms a number of otherbaselines by a wide margin and is very easy tocalculate.
Compared to the word-overlap base-line, it has the advantage of taking into accountthe distributional similarity between words that arealso involved in compositional models.
The su-pervised approach proposed achieved an accept-able result (sixth out of 17) and it could easily beimproved, for example by replacing standard lin-ear regression by a procedure less sensitive to therisk of overfit due to the large number of predictorssuch as Partial Least Squares regression (Guevara,2011).
However, since this approach is not com-positional and its efficacy (compared to others) islimited, it is not obvious that trying to improve itwould be very useful.AcknowledgementsYves Bestgen is Research Associate of the BelgianFund for Scientific Research (F.R.S-FNRS).163ReferencesAston Guy, and Burnard Lou (1998).
The BNC Hand-book: Exploring the British National Corpus withSARA.
Edinburgh: Edinburgh University Press.Baroni, Marco, and Lenci Alessandro (2010) Distri-butional memory: A general framework for corpus-based semantics, Computational Linguistics, 36,673-721.Baroni, Marco, Bernardi, Raffaella, and Zamparelli,Roberto (2013).
Frege in space: a program for com-positional distributional semantics.
In Annie Zae-nen, Bonnie Webber and Martha Palmer.
LinguisticIssues in Language Technologies (LiLT), CSLI Pub-lications.Berry, Michael, Do, Theresa, O?Brien, Gavin, Krishna,Vijay, and Varadhan, Sowmini (1993).
Svdpackc:Version 1.0 user?s guide, Technical Report Num-ber CS-93-194, University of Tennessee, Knoxville,TN.Bestgen, Yves, and Cabiaux, Anne-Franoise (2002).L?analyse s?emantique latente et l?identification desm?etaphores.
In Actes de la 9me Conf?erence annuellesur le traitement automatique des langues naturelles(pp.
331-337).
Nancy : INRIA.Bestgen, Yves, Degand, Liesbeth, and Spooren,Wilbert (2006).
Towards automatic determinationof the semantics of connectives in large newspapercorpora.
Discourse Processes, 41, 175-193.Bestgen, Yves, Lories, Guy, and Thewissen, Jennifer(2010).
Using latent semantic analysis to measurecoherence in essays by foreign language learners?In Sergio Bolasco, Isabella Chiari and Luca Giu-liano (Eds.
), Proceedings of 10th International Con-ference on Statistical Analysis of Textual Data, 385-395.
Roma: LED.Blasius, Jorg, and Greenacre, Michael (1994).
Com-putation of Correspondence Analysis.
In MichaelGreenacre and Jorg Blasius (eds.
), CorrespondenceAnalysis in the Social Sciences, pp.
53-75.
AcademicPress, London.Cheung, Jackie, and Penn, Gerald (2012).
Evaluatingdistributional models of semantics for syntacticallyinvariant inference.
In Conference of the EuropeanChapter of the Association for Computational Lin-guistics, 33-43, Avignon, France.de Marneffe, Marie-Catherine, MacCartney, Bill, andManning, Christopher (2006).
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the 5th Edition of the Language Re-sources and Evaluation Conference.
Genoa, Italy.Deerwester, Scott, Dumais, Susan, Furnas, George,Landauer, Thomas and Harshman, Richard (1990).Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41, 391-407.Erk, Katrin, and Pado, Sebastian (2008).
A structuredvector space model for word meaning in context.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, 897-906,Honolulu, Hawaii.Grefenstette, Edward (2013).
Category-theoreticquantitative compositional distributional models ofnatural language semantics.
PhD Thesis, Univer-sity of Oxford, UK.Guevara, Emiliano (2011).
Computing semantic com-positionality in distributional semantics.
In Pro-ceedings of the Ninth International Conference onComputational Semantics, 135-144, Oxford, UK.Howell, David (2008).
M?ethodes statistiques en sci-ences humaines.
Bruxelles, Belgique: De BoeckUniversit?e.Kintsch, Walter (1998).
Comprehension: AParadigme for Cognition.
New York: CambridgeUniversity Press.Kintsch, Walter (2001).
Predication.
Cognitive Sci-ence, 25(2), 173-202.Landauer, Thomas, and Dumais, Susan, (1997).
A so-lution to Plato?s problem: The latent semantic anal-ysis theory of acquisition, induction and representa-tion of knowledge.
Psychological Review, 104(2),211-240.Landauer, Thomas, Foltz, Peter, and Laham, Darrell(1998).
An introduction to latent semantic analysis,Discourse Processes, 25, 259-284.Lebart, Ludovic, Piron Marie, et Morineau Alain(2000).
Statistique exploratoire multidimension-nelle (3e ?edition), Paris: Dunod.Marelli, Marco, Bentivogli, Luisa, Baroni, Marco,Bernardi, Raffaella, Menini, Stefano, and Zampar-elli, Roberto (2014a).
Semeval-2014 task 1: Evalu-ation of compositional distributional semantic mod-els on full sentences through semantic relatednessand textual entailment.
In Proceedings of SemEval-2014: Semantic Evaluation Exercises.
Dublin, Ire-land.Marelli, Marco, Menini, Stefano, Baroni, Marco, Ben-tivogli, Luisa, Bernardi, Raffaella, and Zamparelli,Roberto (2014b).
A SICK cure for the evaluationof compositional distributional semantic models.
InProceedings of the 9th Edition of the Language Re-sources and Evaluation Conference, Reykjavik, Ice-land.Mitchell, Jeff, and Lapata, Mirella (2010).
Composi-tion in distributional models of semantics.
CognitiveScience, 34, 1388-1429.Nenadic, Oleg, and Greenacre, Michael (2007).
Cor-respondence analysis in R, with two- and three-dimensional graphics: the CA package, Journal ofStatistical Software, 20(3), 1-13.164Schmid, Helmut (1994).
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of the1994 International Conference on New Methods inLanguage Processing, 44-49, Manchester, UK.Toutanova, Kristina, Klein, Dan, Manning, Christo-pher, and Singer, Yoram (2003).
Feature-rich part-of-speech tagging with a cyclic dependency net-work.
In Proceddings of the Human Language Tech-nology Conference of the North American Chap-ter of the Association for Computational Linguistic2003, 252-259, Edmonton, Canada.165
