Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 387?396,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsIt Depends: Dependency Parser ComparisonUsing A Web-based Evaluation ToolJinho D. ChoiEmory University400 Dowman Dr.Atlanta, GA 30322, USAjchoi31@emory.eduJoel TetreaultYahoo Labs229 West 43rd St.New York, NY 10036, USAtetreaul@yahoo-inc.comAmanda StentYahoo Labs229 West 43rd St.New York, NY 10036, USAstent@yahoo-inc.comAbstractThe last few years have seen a surge in thenumber of accurate, fast, publicly avail-able dependency parsers.
At the sametime, the use of dependency parsing inNLP applications has increased.
It can bedifficult for a non-expert to select a good?off-the-shelf?
parser.
We present a com-parative analysis of ten leading statisticaldependency parsers on a multi-genre cor-pus of English.
For our analysis, we de-veloped a new web-based tool that givesa convenient way of comparing depen-dency parser outputs.
Our analysis willhelp practitioners choose a parser to op-timize their desired speed/accuracy trade-off, and our tool will help practitioners ex-amine and compare parser output.1 IntroductionDependency parsing is a valuable form of syn-tactic processing for NLP applications due to itstransparent lexicalized representation and robust-ness with respect to flexible word order languages.Thanks to over a decade of research on statisti-cal dependency parsing, many dependency parsersare now publicly available.
In this paper, we re-port on a comparative analysis of leading statis-tical dependency parsers using a multi-genre cor-pus.
Our purpose is not to introduce a new pars-ing algorithm but to assess the performance of ex-isting systems across different genres of languageuse and to provide tools and recommendationsthat practitioners can use to choose a dependencyparser.
The contributions of this work include:?
A comparison of the accuracy and speed often state-of-the-art dependency parsers, cov-ering a range of approaches, on a large multi-genre corpus of English.?
A new web-based tool, DEPENDABLE, forside-by-side comparison and visualization ofthe output from multiple dependency parsers.?
A detailed error analysis for these parsersusing DEPENDABLE, with recommendationsfor parser choice for different factors.?
The release of the set of dependencies usedin our experiments, the test outputs from allparsers, and the parser-specific models.2 Related WorkThere have been several shared tasks on de-pendency parsing conducted by CoNLL (Buch-holz and Marsi, 2006; Nivre and others, 2007;Surdeanu and others, 2008; Haji?c and others,2009), SANCL (Petrov and McDonald, 2012),SPMRL (Seddah and others, 2013), and Se-mEval (Oepen and others, 2014).
These sharedtasks have led to the public release of numerousstatistical parsers.
The primary metrics reportedin these shared tasks are: labeled attachment score(LAS) ?
the percentage of predicted dependencieswhere the arc and the label are assigned correctly;unlabeled attachment score (UAS) ?
where the arcis assigned correctly; label accuracy score (LS) ?where the label is assigned correctly; and exactmatch (EM) ?
the percentage of sentences whosepredicted trees are entirely correct.Although shared tasks have been tremendouslyuseful for advancing the state of the art in depen-dency parsing, most English evaluation has em-ployed a single-genre corpus, the WSJ portion ofthe Penn Treebank (Marcus et al, 1993), so itis not immediately clear how these results gen-387BC BN MZ NW PT TC WB ALLTraining 171,120 206,057 163,627 876,399 296,437 85,466 284,975 2,084,081Development 29,962 25,274 15,422 147,958 25,206 11,467 36,351 291,640Test 35,952 26,424 17,875 60,757 25,883 10,976 38,490 216,357Training 10,826 10,349 6,672 34,492 21,419 8,969 12,452 105,179Development 2,117 1,295 642 5,896 1,780 1,634 1,797 15,161Test 2,211 1,357 780 2,327 1,869 1,366 1,787 11,697Table 1: Distribution of data used for our experiments.
The first three/last three rows show the number oftokens/trees in each genre.
BC: broadcasting conversation, BN: broadcasting news, MZ: news magazine,NW: newswire, PT: pivot text, TC: telephone conversation, WB: web text, ALL: all genres combined.eralize.1Furthermore, a detailed comparative er-ror analysis is typically lacking.
The most de-tailed comparison of dependency parsers to datewas performed by McDonald and Nivre (2007;2011); they analyzed accuracy as a function ofsentence length, dependency distance, valency,non-projectivity, part-of-speech tags and depen-dency labels.2Since then, additional analyses ofdependency parsers have been performed, but ei-ther with respect to specific linguistic phenom-ena (e.g.
(Nivre et al, 2010; Bender et al, 2011))or to downstream tasks (e.g.
(Miwa and others,2010; Petrov et al, 2010; Yuret et al, 2013)).3 Data3.1 OntoNotes 5We used the English portion of the OntoNotes 5corpus, a large multi-lingual, multi-genre cor-pus annotated with syntactic structure, predicate-argument structure, word senses, named entities,and coreference (Weischedel and others, 2011;Pradhan and others, 2013).
We chose this corpusrather than the Penn Treebank used in most pre-vious work because it is larger (2.9M vs. 1M to-kens) and more diverse (7 vs. 1 genres).
We usedthe standard data split used in CoNLL?123, but re-moved sentences containing only one token so asnot to artificially inflate accuracy.Table 1 shows the distribution across genresof training, development, and test data.
For themost strict and realistic comparison, we trained allten parsers using automatically assigned POS tagsfrom the tagger in ClearNLP (Choi and Palmer,2012a), which achieved accuracies of 97.34 and97.52 on the development and test data, respec-tively.
We also excluded any ?morphological?
fea-1The SANCL shared task used OntoNotes and the WebTreebanks instead for better generalization.2A detailed error analysis of constituency parsing was per-formed by (Kummerfeld and others, 2012).3conll.cemantix.org/2012/download/ids/ture from the input, as these are often not availablein non-annotated data.3.2 Dependency ConversionOntoNotes provides annotation of constituencytrees only.
Several programs are available for con-verting constituency trees into dependency trees.Table 2 shows a comparison between three ofthe most widely used: the LTH (Johansson andNugues, 2007),4, Stanford (de Marneffe and Man-ning, 2008),5and ClearNLP (Choi and Palmer,2012b)6dependency converters.
Compared to theStanford converter, the ClearNLP converter pro-duces a similar set of dependency labels but gen-erates fewer unclassified dependencies (0.23% vs.3.62%), which makes the training data less noisy.Both the LTH and ClearNLP converters pro-duce long-distance dependencies and use functiontags for the generation of dependency relations,which allows one to generate rich dependencystructures including non-projective dependencies.However, only the ClearNLP converter adaptedthe new Treebank guidelines used in OntoNotes.It can also produce secondary dependencies (e.g.right-node raising, referent), which can be used forfurther analysis.
We used the ClearNLP converterto produce dependencies for our experiments.LTH Stanford ClearNLPLong-distance X XSecondary 1 2 4Function tags X XNew TB format XTable 2: Dependency converters.
The ?secondary?row shows how many types of secondary depen-dencies that can be produced by each converter.4http://nlp.cs.lth.se/software5http://nlp.stanford.edu/software6http://www.clearnlp.com388Parser Approach Language LicenseClearNLP v2,37Transition-based, selectional branching (Choi and McCallum, 2013) Java ApacheGN138Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2LTDP v2.0.39Transition-based, beam-search + dynamic prog.
(Huang et al, 2012) Python n/aMate v3.6.110Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2RBG11Tensor decomposition, randomized hill-climb (Lei et al, 2014) Java MITRedshift12Transition-based, non-monotonic (Honnibal et al, 2013) Cython FOSSspaCy13Transition-based, greedy, dynamic oracle, Brown clusters Cython DualSNN14Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2Turbo v2.215Dual decomposition, 3rd-order features (Martins et al, 2013) C++ GPL v2Yara16Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java ApacheTable 3: Dependency parsers used in our experiments.4 ParsersWe compared ten state of the art parsers repre-senting a wide range of contemporary approachesto statistical dependency parsing (Table 3).
Wetrained each parser using the training data fromOntoNotes.
For all parsers we trained using theautomatic POS tags generated during data prepro-cessing, as described above.Training settings For most parsers, we used thedefault settings for training.
For the SNN parser,following the recommendation of the developers,we used the word embeddings from (Collobert andothers, 2011).Development data ClearNLP, LTDP, SNN andYara make use of the development data (for pa-rameter tuning).
Mate and Turbo self-tune param-eter settings using the training data.
The otherswere trained using their default/?standard?
param-eter settings.Beam search ClearNLP, LTDP, Redshift andYara have the option of different beam settings.The higher the beam size, the more accurate theparser usually becomes, but typically at the ex-pense of speed.
For LTDP and Redshift, we ex-perimented with beams of 1, 8, 16 and 64 andfound that the highest accuracy was achieved atbeam 8.17For ClearNLP and Yara, a beam size of7www.clearnlp.com8cs.bgu.ac.il/?yoavg/software/sdparser9acl.cs.qc.edu/?lhuang10code.google.com/p/mate-tools11github.com/taolei87/RBGParser12github.com/syllog1sm/Redshift13honnibal.github.io/spaCy14nlp.stanford.edu/software/nndep.shtml15www.ark.cs.cmu.edu/TurboParser16https://github.com/yahoo/YaraParser17Due to memory limitations we were unable to train Red-shift on a beam size greater than 8.64 produced the best accuracy, while a beam sizeof 1 for LTDT, ClearNLP, and Yara produced thebest speed performance.
Given this trend, we alsoinclude how those three parsers perform at beam 1in our analyses.Feature Sets RBG, Turbo and Yara have the op-tions of different feature sets.
A more complex orlarger feature set has the advantage of accuracy,but often at the expense of speed.
For RBG andTurbo, we use the ?Standard?
setting and for Yara,we use the default (?not basic?)
feature setting.Output All the parsers other than LTDP outputlabeled dependencies.
The ClearNLP, Mate, RBG,and Turbo parsers can generate non-projective de-pendencies.5 DEPENDABLE: Web-based Evaluationand Visualization ToolThere are several very useful tools for evaluatingthe output of dependency parsers, including thevenerable eval.pl18script used in the CoNLLshared tasks, and newer Java-based tools that sup-port visualization of and search over parse treessuch as TedEval (Tsarfaty et al, 2011),19Mal-tEval (Nilsson and Nivre, 2008)20and ?What?swrong with my NLP?
?.21Recently, there is mo-mentum towards web-based tools for annotationand visualization of NLP pipelines (Stenetorp andothers, 2012).
For this work, we used a new web-based tool, DEPENDABLE, developed by the firstauthor of this paper.
It requires no installation andso provides a convenient way to evaluate and com-pare dependency parsers.
The following are keyfeatures of DEPENDABLE:18ilk.uvt.nl/conll/software.html19www.tsarfaty.com/unipar/20www.maltparser.org/malteval.html21whatswrong.googlecode.com389Figure 1: Screenshot of our evaluation tool.?
It reads any type of Tab Separated Value(TSV) format, including the CoNLL formats.?
It computes LAS, UAS and LS for parse out-puts from multiple parsers against gold (man-ual) parses.?
It computes exact match scores for multipleparsers, and ?oracle ensemble?
output, theupper bound performance obtainable by com-bining all parser outputs.?
It allows the user to exclude symbol tokens,projective trees, or non-projective trees.?
It produces detailed analyses by POS tags, de-pendency labels, sentence lengths, and de-pendency distances.?
It reports statistical significance values for allparse outputs (using McNemar?s test).DEPENDABLE can be also used for visualizingand comparing multiple dependency trees together(Figure 2).
A key feature is that the user mayselect parse trees by specifying a range of accu-racy scores; this enabled us to perform the er-ror analyses in Section 6.5.
DEPENDABLE al-lows one to filter trees by sentence length andhighlights arc and label errors.
The evalua-tion and comparison tools are publicly avail-able at http://nlp.mathcs.emory.edu/clearnlp/dependable.Figure 2: Screenshot of our visualization tool.6 Results and Error AnalysisIn this section, we report overall parser accu-racy and speed.
We analyze parser accuracyby sentence length, dependency distance, non-projectivity, POS tags and dependency labels, andgenre.
We report detailed manual error analy-ses focusing on sentences that multiple parsersparsed incorrectly.22All analyses, other than pars-ing speed, were conducted using the DEPEND-ABLE tool.23The full set of outputs from allparsers, as well as the trained models for eachparser, available at http://amandastent.com/dependable/.We also include the greedy parsing results ofClearNLP, LTDP, and Yara in two of our anal-yses to better illustrate the differences betweenthe greedy and non-greedy settings.
The greedyparsing results are denoted by the subscript ?g?.These two analyses are the overall accuracy re-sults, presented in Section 6.1 (Table 4), and theoverall speed results, presented in Section 6.2 ((Table 5 and Figure ).
All other analyses excludethe ClearNLPg, LTDPgand Yarag.22For one sentence in the NW data, the LTDP parser failedto produce a complete parse containing all tokens, so weremoved this sentence for all parsers, leaving 11,696 trees(216,313 tokens) in the test data.23We compared the results produced by DEPENDABLEwith those produced by eval07.pl, and verified that LAS,UAS, LA, and EM were the same when punctuation wasincluded.
Our tool uses a slightly different symbol set thaneval07.pl: !"#$%&?()*+,-./:;<=>?@[\]?
?
{|}?390With Punctuation Without PunctuationOverall Exact Match Overall Exact MatchLAS UAS LS LAS UAS LS LAS UAS LS LAS UAS LSClearNLPg89.19 90.63 94.94 47.65 53.00 61.17 90.09 91.72 94.29 49.12 55.01 61.31GN13 87.59 89.17 93.99 43.78 48.89 56.71 88.75 90.54 93.32 45.44 51.20 56.88LTDPgn/a 85.75 n/a n/a 46.38 n/a n/a 87.16 n/a n/a 48.01 n/aSNN 86.42 88.15 93.54 42.98 48.53 55.87 87.63 89.59 92.70 43.96 49.83 55.91spaCy 87.92 89.61 94.08 43.36 48.79 55.67 88.95 90.86 93.32 44.97 51.28 55.70Yarag85.93 87.64 92.99 42.94 47.77 54.79 87.39 89.32 92.24 44.25 49.44 54.96ClearNLP 89.87 91.30 95.28 49.38 55.18 63.18 90.64 92.26 94.67 50.61 56.88 63.24LTDP n/a 88.18 n/a n/a 51.62 n/a n/a 89.17 n/a n/a 53.54 n/aMate 90.03 91.62 95.29 49.66 56.44 62.71 90.70 92.50 94.67 50.83 58.36 62.72RBG 89.57 91.45 94.71 46.49 55.49 58.45 90.23 92.35 94.01 47.64 56.54 58.07Redshift 89.48 91.01 95.04 49.71 55.82 62.70 90.27 92.00 94.42 50.88 57.28 62.78Turbo 89.81 91.50 95.00 48.08 55.33 60.49 90.49 92.40 94.34 49.29 57.09 60.52Yara 89.80 91.36 95.19 50.07 56.18 63.36 90.47 92.24 94.57 51.02 57.53 63.42Table 4: Overall parsing accuracy.
The top 6 rows and the bottom 7 rows show accuracies for greedy andnon-greedy parsers, respectively.6.1 Overall AccuracyIn Table 4, we report overall accuracy for eachparser.
For clarity, we report results separatelyfor greedy and non-greedy versions of the parsers.Over all the different metrics, MATE is a clearwinner, though ClearNLP, RBG, Redshift, Turboand Yara are very close in performance.
Look-ing at only the greedy parsers, ClearNLPgshows asignificant advantage over the others.We conducted a statistical significance test forthe the parsers (greedy versions excluded).
AllLAS differences are statistically significant at p <.01 (using McNemar?s test), except for: RBG vs.Redshift, Turbo vs. Yara, Turbo vs. ClearNLP andYara vs. ClearNLP.
All UAS differences are sta-tistically significant at p < .01 (using McNemar?stest), except for: SNN vs. LTDP, Turbo vs. Red-shift, Yara vs. RBG and ClearNLP vs. Yara.6.2 Overall SpeedWe ran timing experiments on a 64 core machinewith 16 Intel Xeon E5620 2.40 GHz processorsand 24G RAM, and used the unix time com-mand to time each run.
Some parsers are multi-threaded; for these, we ran in single-thread mode(since any parser can be externally parallelized).Most parsers do not report model load time, so wefirst ran each parser five times with a test set of10 sentences, and then averaged the middle threetimes to get the average model load time.24Next,we ran each parser five times with the entire testset and derived the overall parse time by averag-ing the middle three parse times.
We then sub-tracted the average model time from the average24Recall we exclude single-token sentences from our tests.parse time and averaged over the number of sen-tences and tokens.Sent/Sec Tokens/Sec LanguageClearNLPg555 10,271 JavaGN13 95 1,757 PythonLTDPg232 4,287 PythonSNN 465 8,602 JavaspaCy 755 13,963 CythonYarag532 9,838 JavaClearNLP 72 1,324 JavaLTDP 26 488 PythonMate 30 550 JavaRBG 57 1,056 JavaRedshift 188 3,470 CythonTurbo 19 349 C++Yara 18 340 JavaTable 5: Overall parsing speed.Figure 3: Number of sentences parsed per secondby each parser with respect to sentence length.Table 5 shows overall parsing speed for eachparser.
spaCy is the fastest greedy parser and Red-shift is the fastest non-greedy parser.
Figure 3391shows an analysis of parsing speed by sentencelength in bins of length 10.
As expected, as sen-tence length increases, parsing speed decreases re-markably.6.3 Detailed Accuracy AnalysesFor the following more detailed analyses, we usedall tokens (including punctuation).
As mentionedearlier, we exclude ClearNLPg, LTDPgand Yaragfrom these analyses and instead use their respec-tive non-greedy modes yielding higher accuracy.Sentence Length We analyzed parser accuracyby sentence length in bins of length 10 (Figure 4).As expected, all parsers perform better on shortersentences.
For sentences under length 10, UASranges from 93.49 to 95.5; however, UAS de-clines to a range of 81.66 and 86.61 for sen-tence lengths greater than 50.
The most accurateparsers (ClearNLP, Mate, RBG, Redshift, Turbo,and Yara) separate from the remaining when sen-tence length is more than 20 tokens.Figure 4: UAS by sentence length.Dependency Distance We analyzed parser ac-curacy by dependency distance (depth from eachdependent to its head; Figure 5).
Accuracy fallsoff more slowly as dependency distance increasesfor the top 6 parsers vs. the rest.Projectivity Some of our parsers only produceprojective parses.
Table 6 shows parsing accuracyfor trees containing only projective arcs (11,231trees, 202,521 tokens) and for trees containingnon-projective arcs (465 trees, 13,792 tokens).
Asbefore, all differences are statistically significantat p < .01 except for: Redshift vs. RBG for over-all LAS; LTDP vs. SNN for overall UAS; andTurbo vs. SpaCy for overall UAS.
For strictly pro-jective trees, the LTDP parser is 5th from the top inUAS.
Apart from this, the grouping between ?verygood?
and ?good?
parsers does not change.Figure 5: UAS by dependency distance.Projective only Non-proj.
onlyLAS UAS LAS UASClearNLP 90.20 91.62 85.10 86.72GN13 88.00 89.57 81.56 83.37LTDP n/a 90.24 n/a 57.83Mate 90.34 91.91 85.51 87.40RBG 89.86 91.72 84.83 86.94Redshift 89.90 91.41 83.30 85.12SNN 86.83 88.55 80.37 82.32spaCy 88.31 89.99 82.15 84.08Turbo 88.36 89.90 83.50 85.30Yara 90.20 91.74 83.92 85.74Table 6: Accuracy for proj.
and non-proj.
trees.Dependency Relations We were interested inwhich dependency relations were computed withhigh/low overall accuracy, and for which accuracyvaried between parsers.
The dependency relationswith the highest average LAS scores (> 97%)were possessive, hyph, expl, hmod, aux,det and poss.
These relations have strong lexi-cal clues (e.g.
possessive) or occur very often(e.g.
det).
Those with the lowest LAS scores(< 50%) were csubjpass, meta, dep, nmodand parataxis.
These either occur rarely or arevery general (dep).The most ?confusing?
dependency relations(those with the biggest range of accuracies acrossparsers) were csubj, preconj, csubjpass,parataxis, meta and oprd (all with a spreadof > 20%).
The Mate and Yara parsers each hadthe highest accuracy for 3 out of the top 10 ?con-fusing?
dependency relations.
The RBG parser392had the highest accuracy for 4 out of the top 10?most accurate?
dependency relations.
SNN hadthe lowest accuracy for 5 out of the top 10 ?leastaccurate?
dependency relations, while the RBGhad the lowest accuracy for another 4.POS Tags We also examined error types by partof speech tag of the dependent.
The POS tags withthe highest average LAS scores (> 97%) werethe highly unambiguous tags POS, WP$, MD, TO,HYPH, EX, PRP and PRP$.
With the exception ofWP$, these tags occur frequently.
Those with thelowest average LAS scores (< 75%) were punctu-ation markers ((, ) and :, and the rare tags AFX,FW, NFP and LS.Genres Table 7 shows parsing accuracy for eachparser for each of the seven genres comprisingthe English portion of OntoNotes 5.
Mate andClearNLP are responsible for the highest accuracyfor some genres, although accuracy differencesamong the top four parsers are generally small.Accuracy is highest for PT (pivot text, the Bible)and lowest for TC (telephone conversation) andWB (web data).
The web data is itself multi-genreand includes translations from Arabic and Chi-nese, while telephone conversation data includesdisfluencies and informal language.6.4 Oracle Ensemble PerformanceOne popular method for achieving higher accuracyon a classification task is to use system combina-tion (Bj?orkelund and others, 2014; Le Roux andothers, 2012; Le Roux et al, 2013; Sagae andLavie, 2006; Sagae and Tsujii, 2010; Haffari etal., 2011).
DEPENDABLE reports ensemble upperbound performance assuming that the best tree canbe identified by an oracle (macro), or that the bestarc can be identified by an oracle (micro).
Ta-ble 8 provides an upper bound on ensemble per-formance for future work.LAS UAS LSMacro 94.66 96.00 97.82Micro 96.52 97.61 98.40Table 8: Oracle ensemble performance.The highest match was achieved between the RBGand Mate parser (62.22 UAS).
ClearNLP, GN13and LTDP all matched with Redshift the best, andRBG, Redshift and Turbo matched with Mate thebest.
SNN, spaCy and Turbo did not match wellwith other parsers; their respective ?best match?score was never higher than 55.6.5 Error AnalysisFrom the test data, we pulled out parses whereonly one parser achieved very high accuracy, andparses where only one parser had low accuracy(Table 9).
As with the detailed performance anal-yses, we used the most accurate version of eachparser for this analysis.
Mate has the highest num-ber of ?generally good?
parses, while the SNNparser has the highest number of ?uniquely bad?parses.
The SNN parser tended to choose thewrong root, but this did not appear to be tied to thenumber of verbs in the sentence - rather, the SNNparser just makes the earliest ?reasonable?
choiceof root.Parser UAS ?
90 = 100 < 90 < 90All others UAS < 90 < 90 ?
90 = 100ClearNLP 42 11 45 15LTDP 29 12 182 36GN13 26 8 148 65Mate 75 19 44 10RBG 49 21 49 15Redshift 38 17 28 8SNN 70 23 417 142spaCy 48 17 218 73Turbo 54 15 28 14Yara 33 15 27 7Table 9: Differential parsing accuracies.To further analyze these results, we first looked atthe parse trees for ?errorful?
sentences where theparsers agreed.
From the test data, we extractedparses for sentences where at least two parsers gotUAS of < 50%.
This gave us 253 sentences.
Thedistribution of these errors across genres varied:PT - 2.8%, MZ - 3.5%, BN - 9.8%, NW - 10.3%,WB - 17.4%, BC - 25.3%, TC - 30.8%.By manual comparison using the DEPEND-ABLE tool, we identified frequently occurring po-tential sources of error.
We then manually anno-tated all sentences for these error types.
Figure 6shows the number of ?errorful?
sentences of eachtype.
Punctuation attachment ?errors?
are preva-lent.
For genres with ?noisy?
text (e.g.
broadcastconversation, telephone conversation) a significantproportion of errors come from fragmented sen-tences or those containing backchannels or disflu-encies.
There are also a number of sentences withwhat appeared to be manual dependency labelingerrors in the gold annotation.393BC BN MZ NW PT TC WBLAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UASClearNLP 88.95 90.36 89.59 91.01 89.56 91.24 89.79 91.08 95.88 96.68 87.17 88.93 87.93 89.83GN13 86.75 88.40 87.38 88.87 87.31 89.10 87.36 88.84 94.06 95.00 85.68 87.60 85.20 87.19LTDP n/a 86.81 n/a 87.43 n/a 88.87 n/a 88.40 n/a 93.52 n/a 85.85 n/a 86.37Mate 89.03 90.73 89.30 90.82 90.09 91.92 90.28 91.68 95.71 96.64 87.86 89.87 87.86 89.89RBG 88.64 90.58 88.99 90.86 89.28 91.45 89.85 91.47 95.27 96.41 87.36 89.65 87.12 89.61Redshift 88.60 90.19 88.96 90.46 89.11 90.90 89.63 90.99 95.36 96.22 87.14 88.99 87.27 89.31SNN 85.35 87.08 86.13 87.78 86.00 87.92 86.17 87.74 93.47 94.64 83.50 85.74 84.29 86.50spaCy 87.27 89.05 87.70 89.31 87.37 89.29 88.00 89.52 94.28 95.27 85.67 87.65 85.16 87.40Turbo 87.05 88.70 87.58 89.04 88.34 90.02 87.95 89.33 94.39 95.36 85.91 87.93 85.66 87.70Yara 88.90 90.53 89.40 90.89 89.72 91.42 90.00 91.41 95.41 96.32 87.35 89.19 87.55 89.61Total 2211 1357 780 2326 1869 1366 1787Table 7: Parsing accuracy by genre.Figure 6: Common error types in erroneous trees.6.6 RecommendationsEach of the transition-based parsers that was in-cluded in this evaluation can use varying beamwidths to trade off speed vs. accuracy, and eachparser has numerous other parameters that can betuned.
Notwithstanding all these variables, wecan make some recommendations.
Figure 7 illus-trates the speed vs. accuracy tradeoff across theparsers.
For highest accuracy (e.g.
in dialog sys-tems), Mate, RBG, Turbo, ClearNLP and Yara aregood choices.
For highest speed (e.g.
in web-scaleNLP), spaCy and ClearNLPgare good choices;SNN and Yaragare also good choices when ac-curacy is relatively not as important.7 Conclusions and Future WorkIn this paper we have: (a) provided a detailed com-parative analysis of several state-of-the-art statis-tical dependency parsers, focusing on accuracyFigure 7: Speed with respect to accuracy.and speed; and (b) presented DEPENDABLE, anew web-based evaluation and visualization toolfor analyzing dependency parsers.
DEPENDABLEsupports a wide range of useful functionalities.In the future, we plan to add regular expressionsearch over parses, and sorting within results ta-bles.
Our hope is that the results from the eval-uation as well as the tool will give non-expertsin parsing better insight into which parsing toolworks well under differing conditions.
We alsohope that the tool can be used to facilitate evalua-tion and be used as a teaching aid in NLP courses.Supplements to this paper include the tool,the parse outputs, the statistical models for eachparser, and the new set of dependency trees forOntoNotes 5 created using the ClearNLP depen-dency converter.
We do recommend examiningone?s data and task before choosing and/or train-ing a parser.
Are non-projective parses likely ordesirable?
Does the data contain disfluencies, sen-tence fragments, and other ?noisy text?
phenom-ena?
What is the average and standard deviationfor sentence length and dependency length?
Theanalyses in this paper can be used to select a parserif one has the answers to these questions.394In this work we did not implement an ensembleof parsers, partly because an ensemble necessarilyentails complexity and/or speed delays that renderit unusable by all but experts.
However, our anal-yses indicate that it may be possible to achievesmall but significant increases in accuracy of de-pendency parsing through ensemble methods.
Agood place to start would be with ClearNLP, Mate,or Redshift in combination with LTDP and Turbo,SNN or spaCy.
In addition, it may be possible toachieve good performance in particular genres bydoing ?mini-ensembles?
trained on general pur-pose data (e.g.
WB) and genre-specific data.
Weleave this for future work.
We also leave for fu-ture work the comparison of these parsers acrosslanguages.It remains to be seen what downstream impactdifferences in parsing accuracy of 2-5% have onthe goal task.
If the impact is small, then speedand ease of use are the criteria to optimize, andhere spaCy, ClearNLPg, Yaragand SNN are goodchoices.AcknowledgmentsWe would like to thank the researchers whohave made available data (especially OntoNotes),parsers (especially those compared in this work),and evaluation and visualization tools.
Specialthanks go to Boris Abramzon, Matthew Honnibal,Tao Lei, Danqi Li and Mohammad Sadegh Rasoolifor assistance in installation, trouble-shooting andgeneral discussion.
Additional thanks goes to thekind folks from the SANCL-SPMRL communityfor an informative discussion of evaluation and vi-sualization tools.
Finally, we would like to thankthe three reviewers, as well as Martin Chodorow,Dean Foster, Joseph Le Roux and Robert Stine, forfeedback on this paper.ReferencesEmily M. Bender, Dan Flickinger, Stephan Oepen, andYi Zhang.
2011.
Parser evaluation over local andnon-local deep dependencies in a large corpus.
InProceedings of EMNLP.Anders Bj?orkelund et al 2014.
Introducing the IMS-Wroc?aw-Szeged-CIS entry at the SPMRL 2014shared task: Reranking and morpho-syntax meetunlabeled data.
In Proceedings of the First JointWorkshop on Statistical Parsing of MorphologicallyRich Languages and Syntactic Analysis of Non-Canonical Languages.Bernd Bohnet.
2010.
Very high accuracy and fast de-pendency parsing is not a contradiction.
In Proceed-ings of COLING.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL.Danqi Chen and Christopher Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of EMNLP.Jinho D. Choi and Andrew McCallum.
2013.Transition-based Dependency Parsing with Selec-tional Branching.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, ACL?13, pages 1052?1062.Jinho D. Choi and Martha Palmer.
2012a.
Fast and Ro-bust Part-of-Speech Tagging Using Dynamic ModelSelection.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics, ACL?12, pages 363?367.Jinho D. Choi and Martha Palmer.
2012b.
Guidelinesfor the Clear Style Constituent to Dependency Con-version.
Technical Report 01-12, Institute of Cogni-tive Science, University of Colorado Boulder, Boul-der, CO, USA.Ronan Collobert et al 2011.
Natural language pro-cessing (almost) from scratch.
Journal of MachineLearning Research, 12:2493?2537.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependenciesrepresentation.
In Proceedings of the COLINGworkshop on Cross-Framework and Cross-DomainParser Evaluation.Yoav Goldberg and Joakim Nivre.
2013.
Training de-terministic parser with non-deterministic oracles.
InProceedings of TACL.Gholamreza Haffari, Marzieh Razavi, and AnoopSarkar.
2011.
An ensemble model that combinessyntactic and semantic clustering for discriminativedependency parsing.
In Proceedings of ACL-HLT.Jan Haji?c et al 2009.
The CoNLL-2009 sharedtask: Syntactic and semantic dependencies in mul-tiple languages.
In Proceedings of CoNLL.Matthew Honnibal, Yoav Goldberg, and Mark Johnson.2013.
A non-monotonic arc-eager transition systemfor dependency parsing.
In Proceedings of CoNLL.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Pro-ceedings of the NAACL.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In Proceedings of NODALIDA.395Jonathan K. Kummerfeld et al 2012.
Parser show-down at the wall street corral: An empirical inves-tigation of error types in parser output.
In Proceed-ings of EMNLP.Joseph Le Roux et al 2012.
DCU-Paris13 systemsfor the SANCL 2012 shared task.
In Proceedingsof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Joseph Le Roux, Antoine Rozenknop, and JenniferFoster.
2013.
Combining PCFG-LA models withdual decomposition: A case study with function la-bels and binarization.
In Proceedings of EMNLP.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1381?1391, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: the Penn treebank.
Compu-tational Linguistics, 19(2):313?330.Andr?e F. T. Martins, Miguel B. Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In Proceedings of theACL.Ryan McDonald and Joakim Nivre.
2007.
Character-izing the errors of data-driven dependency parsingmodels.
In Proceedings of EMNLP-CoNLL.Ryan McDonald and Joakim Nivre.
2011.
Analyzingand integrating dependency parsers.
ComputationalLinguistics, 37(1):197?230.Makoto Miwa et al 2010.
A comparative study of syn-tactic parsers for event extraction.
In Proceedings ofBioNLP.Jens Nilsson and Joakim Nivre.
2008.
MaltEval:An evaluation and visualization tool for dependencyparsing.
In Proceedings of LREC.Joakim Nivre et al 2007.
The CoNLL 2007 sharedtask on dependency parsing.
In Proceedings ofCoNLL.Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-los G?omez Rodr??guez.
2010.
Evaluation of de-pendency parsers on unbounded dependencies.
InProceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages833?841, Beijing, China, August.
Coling 2010 Or-ganizing Committee.Stephan Oepen et al 2014.
SemEval 2014 Task 8:Broad-Coverage Semantic Dependency Parsing.
InProceedings of the 8th International Workshop onSemantic Evaluation (SemEval 2014), pages 63?72.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Pro-ceedings of the First Workshop on Syntactic Analysisof Non-Canonical Language (SANCL) Shared Task.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for accurate de-terministic question parsing.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 705?713, Cambridge,MA, October.
Association for Computational Lin-guistics.Sameer Pradhan et al 2013.
Towards robust linguis-tic analysis using OntoNotes.
In Proceedings ofCoNLL.Mohammad Sadegh Rasooli and Joel R. Tetreault.2015.
Yara parser: A fast and accurate dependencyparser.
CoRR, abs/1503.06733.Kenji Sagae and Alon Lavie.
2006.
Parser combina-tion by reparsing.
In Proceedings HLT-NAACL.Kenji Sagae and Jun?ichi Tsujii.
2010.
Dependencyparsing and domain adaptation with data-driven LRmodels and parser ensembles.
In Trends in ParsingTechnology: Dependency Parsing, Domain Adapta-tion, and Deep Parsing, pages 57?68.
Springer.Djam?e Seddah et al 2013.
Overview of the SPMRL2013 shared task: A cross-framework evaluation ofparsing morphologically rich languages.
In Pro-ceedings of the 4th Workshop on Statistical Parsingof Morphologically Rich Languages.Pontus Stenetorp et al 2012.
BRAT: A web-based toolfor NLP-assisted text annotation.
In Proceedings ofthe EACL.Mihai Surdeanu et al 2008.
The CoNLL-2008 sharedtask on joint parsing of syntactic and semantic de-pendencies.
In Proceedings of CoNLL.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2011.
Evaluating dependency parsing: Robust andheuristics-free cross-annotation evaluation.
In Pro-ceedings of EMNLP.Ralph Weischedel et al 2011.
OntoNotes: A largetraining corpus for enhanced processing.
In JosephOlive, Caitlin Christianson, and John McCary, ed-itors, Handbook of Natural Language Processingand Machine Translation.
Springer.Deniz Yuret, Laura Rimell, and Aydin Han.
2013.Parser evaluation using textual entailments.
Lan-guage Resources and Evaluation, 47(3).396
