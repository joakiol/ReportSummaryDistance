Automatic Verb ClassificationBased on Statistical Distributions ofArgument StructurePaola Merlo*University of GenevaSuzanne Stevenson  tUniversity of TorontoAutomatic acquisition of lexical knowledge is critical to a wide range of natural language pro-cessing tasks.
Especially important is knowledge about verbs, which are the primary source ofrelational information in a sentence--the predicate-argument structure that relates an actionor state to its participants (i.e., who did what to whom).
In this work, we report on super-vised learning experiments o automatically classify three major types of English verbs, basedon their argument structure--specifically, the thematic roles they assign to participants.
We uselinguistically-motivated statistical indicators extracted from large annotated corpora to train theclassifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-basedupper bound we calculate at 86.5%.
A detailed analysis of the performance ofthe algorithm andof its errors con~'rms that the proposed features capture properties related to the argument struc-ture of the verbs.
Our results validate our hypotheses that knowledge about thematic relationsis crucial for verb classification, and that it can be gleaned from a corpus by automatic means.We thus demonstrate aneffective combination of deeper linguistic knowledge with the robustnessand scalability of statistical techniques.1.
IntroductionAutomatic acquisition of lexical knowledge is critical to a wide range of natural an-guage processing (NLP) tasks (Boguraev and Pustejovsky 1996).
Especially importantis knowledge about verbs, which are the primary source of relational information ina sentence--the predicate-argument structure that relates an action or state to its par-ticipants (i.e., who did what to whom).
In facing the task of automatic acquisition ofknowledge about verbs, two basic questions must be addressed:What information about verbs and their relational properties needs to belearned?What information can in practice be learned through automatic means?In answering these questions, some approaches to lexical acquisition have focused onlearning syntactic information about verbs, by automatically extracting subcategoriza-tion frames from a corpus or machine-readable dictionary (Brent 1993; Briscoe andCarroll 1997; Dorr 1997; Lapata 1999; Manning 1993; McCarthy and Korhonen 1998).
* Linguistics Department; University of Geneva; 2 rue de Candolle; 1211 Geneva 4, Switzerland; merlo@lettres.unige.cht Department of Computer Science; University of Toronto; 6 King's College Road; Toronto, ON M5S 3H5Canada; suzanne@cs.toronto.eduQ 2001 Association for Computational LinguisticsComputational Linguistics Volume 27, Number 3Table 1Examples of verbs from the three optionally intransitive classes.Unergative The horse raced past the barn.The jockey raced the horse past the barn.Unaccusative The butter melted in the pan.The cook melted the butter in the pan.Object-Drop The boy played.The boy played soccer.Other work has attempted to learn deeper semantic properties uch as selectional re-strictions (Resnik 1996; Riloff and Schmelzenbach 1998), verbal aspect (Klavans andChodorow 1992; Siegel 1999), or lexical-semantic verb classes uch as those proposedby Levin (1993) (Aone and McKee 1996; McCarthy 2000; Lapata and Brew 1999; Schulteim Walde 2000).
In this paper, we focus on argument structure--the thematic roles as-signed by a verb to its arguments--as the way in which the relational semantics ofthe verb is represented at the syntactic level.Specifically, our proposal is to automatically classify verbs based on argumentstructure properties, using statistical corpus-based methods.
We address the prob-lem of classification because it provides a means for lexical organization which caneffectively capture generalizations over verbs (Palmer 2000).
Within the context ofclassification, the use of argument structure provides a finer discrimination amongverbs than that induced by subcategorization frames (as we see below in our exampleclasses, which allow the same subcategorizations but differ in thematic assigmnent),but a coarser classification than that proposed by Levin (in which classes such asours are further subdivided according to more detailed semantic properties).
Thislevel of classification granularity appears to be appropriate for numerous languageengineering tasks.
Because knowledge of argument structure captures fundamentalparticipant/event relations, it is crucial in parsing and generation (e.g., Srinivas andJoshi \[1999\]; Stede \[1998\]), in machine translation (Dorr 1997), and in information re-trieval (Klavans and Kan 1998) and extraction (Riloff and Schmelzenbach 1998).
Ouruse of statistical corpus-based methods to achieve this level of classification is moti-vated by our hypothesis that class-based differences in argument structure are reflectedin statistics over the usages of the component verbs, and that those statistics can beautomatically extracted from a large annotated corpus.The particular classification problem within which we investigate this hypothesisis the task of learning the three major classes of optionally intransitive verbs in English:unergative, unaccusative, and object-drop verbs.
(For the unergative/unaccusative dis-tinction, see Perlmutter \[1978\]; Burzio \[1986\]; Levin and Rappaport Hovav \[1995\]).Table 1 shows an example of a verb from each class in its transitive and intransitiveusages.
These three classes are motivated by theoretical linguistic properties (see dis-cussion and references below, and in Stevenson and Merlo \[1997b\]; Merlo and Steven-son \[2000b\]).
Furthermore, it appears that the classes capture typological distinctionsthat are useful for machine translation (for example, causative unergatives are un-grammatical in many languages), as well as processing distinctions that are useful forgenerating naturally occurring language (for example, reduced relatives with unerga-tive verbs are awkward, but they are acceptable, and in fact often preferred to fullrelatives for unaccusative and object-drop verbs) (Stevenson and Merlo 1997b; Merloand Stevenson 1998).374Merlo and Stevenson Statistical Verb ClassificationTable 2Summary of thematic role assignments by class.Transitive IntransitiveClasses Subject Object SubjectUnergative Agent (of Causation) Agent AgentUnaccusative Agent (of Causation) Theme ThemeObject-Drop Agent Theme AgentThe question then is what underlies these distinctions.
We identify the propertythat precisely distinguishes among these three classes as that of argument structure--i.e., the thematic roles assigned by the verbs.
The thematic roles for each class, andtheir mapping to subject and object positions, are summarized in Table 2.
Note thatverbs across these three classes allow the same subcategorization frames (taking an NPobject or occurring intransitively); thus, classification based on subcategorization alonewould not distinguish them.
On the other hand, each of the three classes is comprisedof multiple Levin classes, because the latter eflect more detailed semantic distinctionsamong the verbs (Levin 1993); thus, classification based on Levin's labeling wouldmiss generalizations across the three broader classes.
By contrast, as shown in Table 2,each class has a unique pattern of thematic assignments, which categorize the verbsprecisely into the three classes of interest.Although the granularity of our classification differs from Levin's, we draw on herhypothesis that semantic properties of verbs are reflected in their syntactic behavior.The behavior that Levin focuses on is the notion of diathesis alternation--an alter-nation in the expression of the arguments of a verb, such as the different mappingsbetween transitive and intransitive that our verbs undergo.
Whether a verb partici-pates in a particular diathesis alternation or not is a key factor in Levin's approach toclassification.
We, like others in a computational framework, have extended this ideaby showing that statistics over the alternants of a verb effectively capture informationabout its class (Lapata 1999; McCarthy 2000; Lapata and Brew 1999).In our specific task, we analyze the pattern of thematic assignments given inTable 2 to develop statistical indicators that are able to determine the class of an op-tionally intransitive verb by capturing information across its transitive and intransitivealternants.
These indicators erve as input to a machine learning algorithm, under asupervised training methodology, which produces an automatic lassification systemfor our three verb classes.
Since we rely on patterns of behavior across multiple occur-rences of a verb, we begin with the problem of assigning a single class to the entireset of usages of a verb within the corpus.
For example, we measure properties acrossall occurrences of a word, such as raced, in order to assign a single classification tothe lexical entry for the verb race.
This contrasts with work classifying individual oc-currences of a verb in each local context, which have typically relied on training thatincludes instances of the verbs to be classified--essentially developing a bias that isused in conjunction with the local context o determine the best classification for newinstances of previously seen verbs.
By contrast, our method assigns a classificationto verbs that have not previously been seen in the training data.
Thus, while we donot as yet assign different classes to the instances of a verb, we can assign a singlepredominant class to new verbs that have never been encountered.To preview our results, we demonstrate hat combining just five numerical indi-cators, automatically extracted from large text corpora, is sufficient to reduce the error375Computational Linguistics Volume 27, Number 3rate in this classification task by more than 50% over chance.
Specifically, we achievealmost 70% accuracy in a task whose baseline (chance) performance is 34%, and whoseexpert-based upper bound is calculated at 86.5%.Beyond the interest for the particular classification task at hand, this work ad-dresses more general issues concerning verb class distinctions based in argumentstructure.
We evaluate our hypothesis that such distinctions are reflected in statis-tics over corpora through a computational experimental methodology in which weinvestigate as indicated each of the subhypotheses below, in the context of the threeverb classes under study:?
Lexical features capture argument structure differences between verbclasses.
1?
The linguistically distinctive features exhibit distributional differencesacross the verb classes that are apparent within linguistic experience (i.e.,they can be collected from text).?
The statistical distributions of (some of) the features contribute tolearning the classifications of the verbs.In the following sections, we show that all three hypotheses above are borne out.
InSection 2, we describe the argument structure distinctions of our three verb classesin more detail.
In support of the first hypothesis above, we discuss lexical correlatesof the underlying differences in thematic assignments hat distinguish the three verbclasses under investigation.
In Section 3, we show how to approximate these featuresby simple syntactic ounts, and how to perform these counts on available corpora.
Weconfirm the second hypothesis above, by showing that the differences in distributionpredicted by the underlying argument structures are largely found in the data.
InSection 4, in a series of machine learning experiments and a detailed analysis of errors,we confirm the third hypothesis by showing that the differences in the distribution ofthe extracted features are successfully used for verb classification.
Section 5 evaluatesthe significance of these results by comparing the program's accuracy to an expert-based upper bound.
We conclude the paper with a discussion of its contributions,comparison to related work, and suggestions for future extensions.2.
Deriving Classification Features from Argument StructureOur task is to automatically build a classifier that can distinguish the three majorclasses of optionally intransitive verbs in English.
As described above, these classesare differentiated by their argument structures.
In the first subsection below, we elab-orate on our description of the thematic role assignments for each of the verb classesunder investigation--unergative, unaccusative, and object-drop.
This analysis yields adistinctive pattern of thematic assignment for each class.
(For more detailed iscussionconcerning the linguistic properties of these classes, and the behavior of their compo-nent verbs, please see Stevenson and Merlo \[1997b\]; Merlo and Stevenson \[2000b\].
)Of course, the key to any automatic classification task is to determine a set of usefulfeatures for discriminating the items to be classified.
In the second subsection below,1 By lexical we mean features that we think are likely stored in the lexicon, because they are propertiesof words and not of phrases or sentences.
Note, however, that some lexical features may notnecessarily be stored with individual words--indeed, the motivation for classifying verbs to capturegeneralizations within each class uggests otherwise.376Merlo and Stevenson Statistical Verb Classificationwe show how the analysis of thematic distinctions enables us to determine lexicalproperties that we hypothesize will exhibit useful, detectable frequency differencesin our corpora, and thus serve as the machine learning features for our classificationexperiments.2.1 The Argument Structure DistinctionsThe verb classes are exemplified below, in sentences repeated from Table 1 for ease ofexposition.Unergative: (la) The horse raced past the barn.
(lb) The jockey raced the horse past the barn.Unaccusative: (2a) The butter melted in the pan.
(2b) The cook melted the butter in the pan.Object-Drop: (3a) The boy played.
(3b) The boy played soccer.The example sentences illustrate that all three classes participate in a diathesis alter-nation that relates a transitive and intransitive form of the verb.
However, accordingto Levin (1993), each class exhibits a different ype of diathesis alternation, which isdetermined by the particular semantic relations of the arguments othe verb.
We makethese distinctions explicit by drawing on a standard notion of thematic role, as eachclass has a distinct pattern of thematic assignments (i.e., different argument structures).We assume here that a thematic role is a label taken from a fixed inventory ofgrammaticalized semantic relations; for example, an Agent is the doer of an action,and a Theme is the entity undergoing an event (Gruber 1965).
While admitting thatsuch notions as Agent and Theme lack formal definitions (in our work and in theliterature more widely), the distinctions are clear enough to discriminate our threeverb classes.
For our purposes, these roles can simply be thought of as semantic labelswhich are non-decomposable, but there is nothing in our approach that rests on thisassumption.
Thus, our approach would also be compatible with a feature-based deft-nition of participant roles, as long as the features capture such general distinctions as,for example, the doer of an action and the entity acted upon (Dowty 1991).Note that in our focus on verb class distinctions we have not considered finer-grained features that rely on more specific semantic features, such as, for example,that the subject of the intransitive melt must be something that can change from solidto liquid.
While this type of feature may be important for semantic distinctions amongindividual verbs, it thus far seems irrelevant o the level of verb classification thatwe adopt, which groups verbs more broadly according to syntactic and (somewhatcoarser-grained) semantic properties.Our analysis of thematic assignment--which was summarized in Table 2, repeatedhere as Table 3--is elaborated here for each verb class.
The sentences in (1) aboveillustrate the relevant alternants ofan unergative verb, race.
Unergatives are intransitiveaction verbs whose transitive form, as in (lb), can be the causative counterpart of theintransitive form (la).
The type of causative alternation that unergatives participate inis the "induced action alternation" according to Levin (1993).
For our thematic analysis,we note that the subject of an intransitive activity verb is specified to be an Agent.The subject of the transitive form is indicated by the label Agent of Causation, whichindicates that the thematic role assigned to the subject is marked as the role which is377Computational Linguistics Volume 27, Number 3Table 3Summary of thematic assignments.Transitive IntransitiveClasses Subject Object SubjectUnergative Agent (of Causation) Agent AgentUnaccusative Agent (of Causation) Theme ThemeObject-Drop Agent Theme Agentintroduced with the causing event.
In a causative alternation, the semantic argumentof the subject of the intransitive surfaces as the object of the transitive (Brousseau andRitter 1991; Hale and Keyser 1993; Levin 1993; Levin and Rappaport Hovav 1995).
Forunergatives, this argument is an Agent and thus the alternation yields an object inthe transitive form that receives an Agent thematic role (Cruse 1972).
These thematicassignments are shown in the first row of Table 3.The sentences in (2) illustrate the corresponding forms of an unaccusative verb,melt.
Unaccusatives are intransitive change-of-state v rbs, as in (2a); the transitivecounterpart for these verbs also exhibits a causative alternation, as in (2b).
This isthe "causative/inchoative alternation" (Levin, 1993).
Like unergatives, the subject ofa transitive unaccusative is marked as the Agent of Causation.
Unlike unergatives,though, the alternating argument of an unaccusative (the subject of the intransitiveform that becomes the object of the transitive) is an entity undergoing a change ofstate, without active participation, and is therefore a Theme.
The resulting pattern ofthematic assignments i indicated in the second row of Table 3.The sentences in (3) use an object-drop verb, play.
These are activity verbs thatexhibit a non-causative diathesis alternation, in which the object is simply optional.This is dubbed "the unexpressed object alternation" (Levin 1993), and has severalsubtypes that we do not distinguish ere.
The thematic assignment for these verbs issimply Agent for the subject (in both transitive and intransitive forms), and Themefor the optional object; see the last row of Table 3.For further details and support of this analysis, please see the discussion in Steven-son and Merlo (1997b) and Merlo and Stevenson (2000b).
For our purposes here, theimportant fact to note is that each of the three classes can be uniquely identified bythe pattern of thematic assignments across the two alternants of the verbs.2.2 Features for Automatic ClassificationOur next task then is to derive, from these thematic patterns, useful features for au-tomatically classifying the verbs.
In what follows, we refer to the columns of Table 3to explain how we expect he thematic distinctions to give rise to distributional prop-erties, which, when appropriately approximated through corpus counts, will discrim-inate across the three classes.Transitivity Consider the first two columns of thematic roles in Table 3, which illus-trate the role assignment in the transitive construction.
The Prague school's notionof linguistic markedness (Jakobson 1971; Trubetzkoy 1939) enables us to establish ascale of markedness of these thematic assignments and make a principled predictionabout their frequency of occurrence.
Typical tests to determine the unmarked elementof a pair or scale are simplicity--the unmarked element is simpler, distr ibution--theunmarked member is more widely attested across languages, and f requency--the un-378Merlo and Stevenson Statistical Verb Classificationmarked member is more frequent (Greenberg 1966; Moravcsik and Wirth 1983).
Theclaim of markedness theory is that, once an element has been identified by one testas the unmarked element of a scale, then all other tests will be correlated.
The threethematic assignments appear to be ranked on a scale by the simplicity and distribu-tion tests, as we describe below.
From this, we can conclude that frequency, as a thirdcorrelated test, should also be ranked by the same scale, and we can therefore makepredictions about the expected frequencies of the three thematic assignments.First, we note that the specification ofan Agent of Causation for transitive unerga-tives (such as race) and unaccusatives (such as melt) indicates a causative construction.Causative constructions relate two events, the causing event and the core event de-scribed by the intransitive verb; the Agent of Causation is the Agent of the causingevent.
This double event structure can be considered as more complex than the sin-gle event hat is found in a transitive object-drop verb (such as play) (Stevenson andMerlo 1997b).
The simplicity test thus indicates that the causative unergatives andunaccusatives are marked in comparison to the transitive object-drop verbs.We further observe that the causative transitive of an unergative verb has an Agentthematic role in object position which is subordinated to the Agent of Causation insubject position, yielding an unusual "double agentive" thematic structure.
This lex-ical causativization f unergatives (in contrast to analytic ausativization) is a distri-butionally rarer phenomenon--found i  fewer languages--than lexical causatives ofunaccusatives.
In asking native speakers about our verbs, we have found that lexicalcausatives of unergative verbs are not attested in Italian, French, German, Portuguese,Gungbe (Kwa family), and Czech.
On the other hand, the lexical causatives are pos-sible for unaccusative rbs (i.e., where the object is a Theme) in all these languages.Vietnamese appears to allow a very restricted form of causativization f unergativeslimited to only those cases that have a comitative reading.
The typological distribu-tion test thus indicates that unergatives are more marked than unaccusatives in thetransitive form.From these observations, we can conclude that unergatives ( uch as race) havethe most marked transitive argument structure, unaccusatives ( uch as melt) havean intermediately marked transitive argument structure, and object-drops (such asplay) have the least marked transitive argument structure of the three.
Under theassumptions of markedness theory outlined above, we then predict hat unergativesare the least frequent in the transitive, that unaccusatives have intermediate frequencyin the transitive, and that object-drop verbs are the most frequent in the transitive.Causativity Due to the causative alternation of unergatives and unaccusatives, thethematic role of the subject of the intransitive is identical to that of the object of thetransitive, as shown in the second and third columns of thematic roles in Table 3.Given the identity of thematic role mapped to subject and object positions across thetwo alternants, we expect o observe the same noun occurring at times as subject ofthe verb, and at other times as object of the verb.
In contrast, for object-drop verbs,the thematic role of the subject of the intransitive is identical to that of the subject ofthe transitive, not the object of the transitive.
We therefore xpect hat it will be lesscommon for the same noun to occur in subject and object position across instances ofthe same object-drop verb.Thus, we hypothesize that this pattern of thematic role assignments will be re-flected in a differential amount of usage across the classes of the same nouns as sub-jects and objects for a given verb.
Generally, we would expect hat causative verbs(in our case, the unergative and unaccusative v rbs) would have a greater degree ofoverlap of nouns in subject and object position than non-causative transitive verbs (in379Computational Linguistics Volume 27, Number 3our case, the object-drop verbs).
However, since the causative is a transitive use, andthe transitive use of unergatives i expected to be rare (see above), we do not expectunergatives to exhibit a high degree of detectable overlap in a corpus.
Thus, this over-lap of subjects and objects should primarily distinguish unaccusatives (predicted tohave high overlap of subjects and objects) from the other two classes (each of whichis predicted to have low \[detectable\] overlap of subjects and objects).Animacy Finally, considering the roles in the first and last columns of thematic assign-ments in Table 3, we observe that unergative and object-drop verbs assign an agentiverole to their subject in both the transitive and intransitive, while unaccusatives a signan agentive role to their subject only in the transitive.
Under the assumption that theintransitive use of unaccusatives i  not rare, we then expect that unaccusatives willoccur less often overall with an agentive subject han will the other two verb classes.
(The assumption that unaccusatives are not rare in the intransitive is based on thelinguistic complexity of the causative transitive alternant, and is borne out in our cor-pus analysis.)
On the further assumption that Agents tend to be animate ntities moreso than Themes are, we expect hat unaccusatives will occur less frequently with ananimate subject compared to unergative and object-drop verbs.
Note the importanceof our use of frequency distributions: the claim is not that only Agents can be animate,but rather that nouns that receive an Agent role will more often be animate than nounsthat receive a Theme role.Additional Features The above interactions between thematic roles and the syntacticexpressions of arguments thus lead to three features whose distributional propertiesappear promising for distinguishing unergative, unaccusative and object-drop verbs:transitivity, causativity, and animacy of subject.
We also investigate two additional syn-tactic features: the use of the passive or active voice, and the use of the past participle orsimple past part-of-speech (POS) tag (VBN or VBD, in the Penn Treebank style).
Thesefeatures are related to the transitive/intransitive alternation, since a passive use impliesa transitive use of the verb, as well as to the use of a past participle form of the verb .
2Table 4 summarizes the features we derive from the thematic properties, and ourexpectations concerning their frequency of use.
We hypothesize that these five featureswill exhibit distributional differences in the observed usages of the verbs that can beused for classification.
In the next section, we describe the actual corpus counts that wedevelop to approximate the features we have identified.
(Notice that the counts willbe imperfect approximations to the thematic knowledge, beyond the inevitable rrorsdue to automatic extraction from large automatically annotated corpora.
Even whenthe counts are precise, they only constitute an approximation to the actual thematicnotions, since the features we are using are not logically implied by the knowledgewe want to capture, but only statistically correlated.)3.
Data Collection and AnalysisClearly, some of the features we've proposed are difficult (e.g., the passive use) or im-possible (e.g., animate subject use) to automatically extract with high accuracy from a2 For our sample verbs, the statistical correlation between the transitive and passive features i highlysignificant (N ----- 59, R = .44, p =- .001), as is the correlation between the transitive and past participlefeatures (N = 59, R = .36, p = .005).
(Since, as explained in the next section, our features are expressedas proportions---e.g., percent transitive use out of detected transitive and intransitive use----correlationsof intransitivity with passive or past participle use have the same magnitude but are negative.
)380Merlo and Stevenson Statistical Verb ClassificationTable 4The features and expected behavior.Expected FrequencyFeature Pattern ExplanationTransitivity Unerg < Unacc < ObjDrop Unaccusatives and unergatives have a causativetransitive, hence lower transitive use.
Further-more, unergatives have an agentive object, hencevery low transitive use.Causativity Unerg, ObjDrop < Unacc Object-drop verbs do not have a causal agent,hence low "causative" use.
Unergatives are rarein the transitive, hence low causative use.Animacy Unacc < Unerg, ObjDrop Unaccusatives have a Theme subject in the in-transitive, hence lower use of animate subjects.Passive Voice Unerg K Unacc K ObjDrop Passive implies transitive use, hence correlatedwith transitive feature.VBN Tag Unerg < Unacc < ObjDrop Passive implies past participle use (VBN), hencecorrelated with transitive (and passive).large corpus, given the current state of annotation.
However, we do assume that cur-rently available corpora, such as the Wall Street Journal (WSJ), provide a representative,and large enough, sample of language from which to gather corpus counts that can ap-proximate the distributional patterns of the verb class alternations.
Our work draws ontwo text corpora--one an automatically tagged combined corpus of 65 million words(primarily WSJ), the second an automatically parsed corpus of 29 million words (a sub-set of the WSJ text from the first corpus).
Using these corpora, we develop countingprocedures that yield relative frequency distributions for approximations to the fivelinguistic features we have determined, over a sample of verbs from our three classes.3.1 Materials and MethodWe chose a set of 20 verbs from each class based primarily on the classification i  Levin(1993).
3 The complete list of verbs appears in Table 5; the group 1/group 2 designationis explained below in the section on counting.
As indicated in the table, unergativesare manner-of-motion verbs (from the "run" class in Levin), unaccusatives are change-of-state verbs (from several of the classes in Levin's change-of-state super-class), whileobject-drop verbs were taken from a variety of classes in Levin's classification, all ofwhich undergo the unexpressed object alternation.
The most frequently used classesare verbs of change of possession, image-creation verbs, and verbs of creation andtransformation.
The selection of verbs was based partly on our intuitive judgmentthat the verbs were likely to be used with sufficient frequency in the WSJ.
Also, each3 We used an equal number of verbs from each class in order to have a balanced group of items.
Onepotential disadvantage of this decision is that each verb class is represented qually, even though theymay not be equally frequent in the corpora.
Although we lose the relative frequency informationamong the classes that could provide a better bias for assigning a default classification (i.e., the mostfrequent one), we have the advantage that our classifier will be equally informed (in terms of numberof exemplars) about each class.Note that there are only 19 unaccusative rbs because ripped, which was initially counted in theunaccusatives, was then excluded from the analysis as it occurred mostly in a very different usage inthe corpus (as verb+particle, in ripped off) from the intended optionally intransitive usage.381Computational Linguistics Volume 27, Number 3Table 5Verbs used in the experiments.Class Name Description Selected VerbsUnergative manner of motion jumped, rushed, marched, leaped, floated, raced, hurried, wan-dered, vaulted, paraded (group 1); galloped, glided, hiked,hopped, jogged, scooted, scurried, skipped, tiptoed, trotted(group 2).Unaccusative change of state opened, exploded, flooded, dissolved, cracked, hardened, boiled,melted, fractured, solidified (group 1); collapsed, cooled,folded, widened, changed, cleared, divided, simmered, stabi-lized (group 2).Object-Drop unexpressedobject alternationplayed, painted, kicked, carved, reaped, washed, danced,yelled, typed, knitted (group 1); borrowed, inherited, orga-nized, rented, sketched, cleaned, packed, studied, swallowed,called (group 2).verb presents the same form in the simple past and in the past participle (the reg-ular "-ed" form).
In order to simplify the counting procedure, we included only the"-ed" form of the verb, on the assumption that counts on this single verb form wouldapproximate the distribution of the features across all forms of the verb.
Additionally,as far as we were able given the preceding constraints, we selected verbs that couldoccur in the transitive and in the passive.
Finally, we aimed for a frequency cut-offof 10 occurrences or more for each verb, although for unergatives we had to use oneverb (jogged) that only occurred 8 times in order to have 20 verbs that satisfied theother criteria above.In performing this kind of corpus analysis, one has to recognize the fact thatcurrent corpus annotations do not distinguish verb senses.
In these counts, we didnot distinguish a core sense of the verb from an extended use of the verb.
So, forinstance, the sentence Consumer spending jumped 1.7% in February after a sharp drop themonth before (WSJ 1987) is counted as an occurrence of the manner-of-motion verb jumpin its intransitive form.
This particular sense extension has a transitive alternant, butnot a causative transitive (i.e., Consumer spending jumped the barrier .
.
.
.
but not Low taxesjumped consumer spending... ).
Thus, while the possible subcategorizations remain thesame, rates of transitivity and causativity may be different han for the literal manner-of-motion sense.
This is an unavoidable result of using simple, automatic extractionmethods given the current state of annotation of corpora.For each occurrence of each verb, we counted whether it was in a transitive orintransitive use (TRANS), in a passive or active use (PASS), in a past participle or simplepast use (VBN), in a causative or non-causative use (CAUS), and with an animate subjector not (ANIM).
4 Note that, except for the VBN feature, for which we simply extract hePOS tag from the corpus, all other counts are approximations to the actual linguisticbehaviour of the verb, as we describe in detail below.4 One additional feature was recorded--the log frequency ofthe verb in the 65 million wordcorpus--motivated by the conjecture that the frequency ofa verb may help in predicting its class.
Inour machine l arning experiments, however, this conjecture was not borne out, as the frequency featuredid not improve performance.
This is the case for experiments onall of the verbs, as well as forseparate experiments onthe group 1 verbs (which were matched across the classes for frequency) andthe group 2 verbs (which were not).
We therefore limit discussion here to the thematically-motivatedfeatures.382Merlo and Stevenson Statistical Verb ClassificationThe first three counts (TRANS, PASS, VBN) were performed on the tagged ACL/DCIcorpus available from the Linguistic Data Consortium, which includes the Brown Cor-pus (of one million words) and years 1987-1989 of the Wall Street Journal, a combinedcorpus in excess of 65 million words.
The counts for these features proceeded as fol-lows:?
TRANS: A number, a pronoun, a determiner, an adjective, or a noun wereconsidered to be indication of a potential object of the verb.
A verboccurrence preceded by forms of the verb be, or immediately followed bya potential object was counted as transitive; otherwise, the occurrencewas counted as intransitive (specifically, if the verb was followed by apunctuation s ign- -commas,  colons, full s tops- -or  by a conjunction, aparticle, a date, or a preposition.
)* PASS: A main verb (i.e., tagged VBD) was counted as active.
A tokenwith tag VBN was also counted as active if the closest precedingauxiliary was have, while it was counted as passive if the closestpreceding auxil iary was be.?
VBN: The counts for VBN/VBD were simply done based on the POSlabel within the tagged corpus.Each of the above three counts was normalized over all occurrences of the "-ed" formof the verb, yielding a single relative frequency measure for each verb for that feature;i.e., percent ransitive (versus intransitive) use, percent active (versus passive) use, andpercent VBN (versus VBD) use, respectively.The last two counts (CAUS and ANIM) were performed on a parsed version of the1988 year of the Wall Street Journal, so that we could extract subjects and objects ofthe verbs more accurately.
This corpus of 29 million words was provided to us byMichael Collins, and was automatically parsed with the parser described in Collins(1997).
5The counts, and their justification, are described here:CAUS: As discussed above, the object of a causative transitive is the samesemantic argument of the verb as the subject of the intransitive.
Thecausative feature was approximated by the following steps, intended tocapture the degree to which the subject of a verb can also occur as itsobject.
Specifically, for each verb occurrence, the subject and object (ifthere was one) were extracted from the parsed corpus.
The observedsubjects across all occurrences of the verb were placed into one multisetof nouns, and the observed objects into a second multiset of nouns.
(Amultiset, or bag, was used so that our representation i dicated thenumber  of times each noun was used as either subject or object.)
Then,the proport ion of overlap between the two multisets was calculated.
Wedefine overlap as the largest multiset of elements belonging to both the5 Readers might be concerned about he portability of this method to languages for which no largeparsed corpus is available.
It is possible that using a fully parsed corpus is not necessary.
Our resultswere replicated in English without he need for a fully parsed corpus (Anoop Sarkar, p.c., citing aproject report by Wootiporn Tripasai).
Our method was applied to 23 million words of the WSJ thatwere automatically tagged with Ratnaparkhi's maximum entropy tagger (Ratnaparkhi 1996) andchunked with the partial parser CASS (Abney 1996).
The results are very similar to ours (best accuracy66.6%), suggesting that a more accurate tagger than the one used on our corpus might in fact besufficient to overcome the fact that no full parse is available.383Computational Linguistics Volume 27, Number 3subject and the object multisets; e.g., the overlap between (a, a, a, b} and{a} is {a,a,a}.
The proportion is the ratio between the cardinality of theoverlap multiset, and the sum of the cardinality of the subject and objectmultisets.
For example, for the simple sets of characters above, the ratiowould be 3/5, yielding a value of .60 for the CAUS feature.ANIM: A problem with a feature like animacy is that it requires eithermanual determination f the animacy of extracted subjects, or referenceto an on-line resource such as WordNet for determining animacy.
Toapproximate animacy with a feature that can be extracted automatically,and without reference to a resource xternal to the corpus, we takeadvantage of the well-attested animacy hierarchy, according to whichpronouns are the most animate (Silverstein 1976; Dixon 1994).
Thehypothesis that the words I, we, you, she, he, and they most often referto animate ntities.
This hypothesis was confirmed by extracting100 occurrences of the pronoun they, which can be either animate orinanimate, from our 65 million word corpus.
The occurrencesimmediately preceded a verb.
After eliminating repetitions,94 occurrences were left, which were classified by hand, yielding 71animate pronouns, 11 inanimate pronouns and 12 unclassifiedoccurrences (for lack of sufficient context o recover the antecedent of thepronoun with certainty).
Thus, at least 76% of usages of they wereanimate; we assume the percentage of animate usages of the otherpronouns to be even higher.
Since the hypothesis was confirmed, wecount pronouns (other than it) in subject position (Kariaeva \[1999\]; cf.Aone and McKee \[1996\]).
The values for the feature were determined byautomatically extracting all subject/verb tuples including our 59 exampleverbs from the parsed corpus, and computing the ratio of occurrences ofpronoun subjects to all subjects for each verb.Finally, as indicated in Table 5, the verbs are designated as belonging to "group 1"or "group 2".
All the verbs are treated equally in our data analysis and in the machinelearning experiments, but this designation does indicate a difference in details of thecounting procedures described above.
The verbs in group I had been used in an earlierstudy in which it was important to minimize noisy data (Stevenson and Merlo 1997a),so they generally underwent greater manual intervention i  the counts.
In addinggroup 2 for the classification experiment, we chose to minimize the intervention iorder to demonstrate hat the classification process is robust enough to withstand theresulting noise in the data.For group 2, the transitivity, voice, and VBN counts were done automatically with-out any manual intervention.
For group 1, these three counts were done automaticallyby regular expression patterns, and then subjected to correction, partly by hand andpartly automatically, by one of the authors.
For transitivity, the adjustments vary forthe individual verbs.
Most of the reassignments from a transitive to an intransitivelabelling occurred when the following noun was not the direct object but rather ameasure phrase or a date.
Most of the reassignments from intransitive to transitiveoccurred when a particle or a preposition following the verb did not introduce a prepo-sitional phrase, but instead indicated apassive form (by) or was part of a phrasal verb.Some verbs were mostly used adjectivally, in which case they were excluded from thetransitivity counts.
For voice, the required adjustments included cases of coordinationof the past participle when the verb was preceded by a conjunction, or a comma.384Merlo and Stevenson Statistical Verb ClassificationTable 6Aggregated relative frequency data for the five features.
E = unergatives, A = unaccusatives,O = object-drops.TRANS PASS VBN CAUS ANIMClass N Mean SD Mean SD Mean SD Mean SD Mean SDE 20 0.23 0.23 0.07 0.12 0.21 0.26 0.00 0.00 0.25 0.24A 19 0.40 0.24 0.33 0.27 0.65 0.27 0.12 0.14 0.07 0.09O 20 0.62 0.25 0.31 0.26 0.65 0.23 0.04 0.07 0.15 0.14These were collected and classified by hand as passive or active based on intuition.Similarly, partial adjustments to the VBN counts were made by hand.For the causativity feature, subjects and objects were determined by manual  in-spection of the corpus for verbs belonging to group 1, while they were extractedautomatically from the parsed corpus for group 2.
The group 1 verbs were sampledin three ways, depending on total frequency.
For verbs with less than 150 occurrences,all instances of the verbs were used for subject/object extraction.
For verbs whosetotal frequency was greater than 150, but whose VBD frequency was in the range100-200, we extracted subjects and objects of the VBD occurrences only.
For higherfrequency verbs, we used only the first 100 VBD occurrences.
6 The same script forcomputing the overlap of the extracted subjects and objects was then used on theresulting subject/verb and verb/object  tuples for both group 1 and group 2 verbs.The animacy feature was calculated over subject/verb tuples extracted automati-cally for both groups of verbs from the parsed corpus.3.2 Data AnalysisThe data collection described above yields the following data points in total: TRANS:27403; PASS: 20481; VBN: 36297; CAt;S: 11307; ANIM: 7542.
(Different features yield differ-ent totals because they were sampled independently, and the search patterns to extractsome features are more imprecise than others.)
The aggregate means by class of thenormalized frequencies for all verbs are shown in Table 6; item by item distributionsare provided in Appendix A, and raw counts are available from the authors.
Notethat aggregate means are shown for illustration purposes only--al l  machine learningexperiments are performed on the individual normalized frequencies for each verb, asgiven in Appendix A.The observed istributions of each feature are indeed roughly as expected accord-ing to the description in Section 2.
Unergatives how a very low relative frequency ofthe TRANS feature, fol lowed by unaccusatives, then object-drop verbs.
Unaccusativeverbs show a high frequency of the CAUS feature and a low frequency of the ANIM fea-ture compared to the other classes.
Somewhat unexpectedly, object-drop verbs exhibita non-zero mean CAUS value (almost half the verbs have a CAUS value greater thanzero), leading to a three-way causative distinction among the verb classes.
We suspectthat the approximation that we used for causative use- - the overlap between subjects6 For this last set of high-frequency verbs (exploded, jumped, opened, played, rushed), we used the first100 occurrences a the simplest way to collect he sample.
In response to an anonymous reviewer'sconcern, we later verified that these counts were not different from counts obtained by randomsampling of 100 VBD occurrences.
A paired t-test of the two sets of counts (first 100 sampling andrandom sampling) indicates that the two sets of counts are not statistically different (t = 1.283, DF = 4,p = 0.2687).385Computational Linguistics Volume 27, Number 3Table 7Manually (Man) and automatically (Aut) calculated features for a random sample of verbs.T -~- TRANS,  P = PASS,  V ---- VBN,  C -~ CAUS,  A = ANIM.Unergative Unaccusative Object-Drophopped scurried folded stabilized inherited swallowedMan Aut Man Aut Man Aut Man Aut Man Aut Man AutT 0.21 0 .21  0.00 0.00 0 .71  0.23 0.24 0.18 1.00 0.64 0.96 0.35P 0.00 0.00 0.00 0.00 0.44 0.33 0.19 0.13 0.39 0.13 0.54 0.44V 0.03 0.00 0.10 0.00 0.56 0.73 0 .71  0.92 0.56 0.60 0.64 0.79C 0.00 0.00 0.00 0.00 0.54 0.00 0.24 0.35 0.00 0.06 0.00 0.04A 0.93 1.00 0.90 0.14 0.23 0.00 0.02 0.00 0.58 0.32 0.35 0.22and objects for a verb--also captures a "reciprocity" effect for some object-drop verbs(such as call), in which subjects and objects can be similar types of entities.
Finally,although expected to be a redundant indicator of transitivity, PASS and VBN, unlikeTRANS, have very similar values for unaccusative and object-drop verbs, indicatingthat their distributions are sensitive to factors we have not yet investigated.One issue we must address is how precisely the automatic ounts reflect he actuallinguistic behaviour of the verbs.
That is, we must be assured that the patterns we notein the data in Table 6 are accurate reflections of the differential behaviour of the verbclasses, and not an artifact of the way in which we estimate the features, or a result ofinaccuracies in the counts.
In order to evaluate the accuracy of our feature counts, weselected two verbs from each class, and determined the "true" value of each featurefor each of those six verbs through manual counting.
The six verbs were randomlyselected from the group 2 subset of the verbs, since counts for group 2 verbs (asexplained above) had not undergone manual correction.
This allows us to determinethe accuracy of the fully automatic ounting procedures.
The selected verbs (and theirfrequencies) are: hopped (29), scurried (21), folded (189), stabilized (286), inherited (357),swallowed (152).
For verbs that had a frequency of over 100 in the "-ed" form, weperformed the manual counts on the first 100 occurrences.Table 7 shows the results of the manual counts, reported as proportions to facil-itate comparison to the normalized automatic ounts, shown in adjoining columns.We observe first that, overall, most errors in the automatic ounts occur in the unac-cusative and object-drop verbs.
While tagging errors affect the VBN feature for all ofthe verbs somewhat, we note that TP~ANS and PaSS are consistently underestimated forunaccusative and object-drop verbs.
These errors make the unaccusative and object-drop feature values more similar to each other, and therefore potentially harder todistinguish.
Furthermore, because the TRANS and PASS values are underestimated bythe automatic ounts, and therefore lower in value, they are also closer to the valuesfor the unergative verbs.
For the CAUS feature, we predict the highest values for theunaccusative verbs, and while that prediction is confirmed, the automatic ounts forthat class also show the most errors.
Finally, although the general pattern of highervalues for the ANIM feature of unergatives and object-drop verbs is preserved in theautomatic ounts, the feature is underestimated for almost all the verbs, again makingthe values for that feature closer across the classes than they are in reality.We conclude that, although there are inaccuracies in all the counts, the generalpatterns expected based on our analysis of the verb classes hold in both the manualand automatic ounts.
Errors in the estimating and counting procedures are therefore386Merlo and Stevenson Statistical Verb Classificationnot likely to be responsible for the pattern of data in Table 6 above, which generallymatches our predictions.
Furthermore, the errors, at least for this random sample ofverbs, occur in a direction that makes our task of distinguishing the classes moredifficult, and indicates that developing more accurate search patterns may possiblysharpen the class distinctions, and improve the classification performance.4.
Experiments in ClassificationIn this section, we turn to our computational experiments that investigate whether thestatistical indicators of thematic properties that we have developed can in fact be usedto classify verbs.
Recall that the task we have set ourselves is that of automaticallylearning the best class for a set of usages of a verb, as opposed to classifying individualoccurrences of the verb.
The frequency distributions of our features yield a vector foreach verb that represents the estimated values for the verb on each dimension acrossthe entire corpus:Vector template: \[verb-name, TRANS, PASS, VBN, CAUS, ANIM, class\]Example: \[opened, .69, .09, .21, .16, .36, unacc\]The resulting set of 59 vectors constitutes the data for our machine learning experi-ments.
We use this data to train an automatic lassifier to determine, given the featurevalues for a new verb (not from the training set), which of the three major classes ofEnglish optionally intransitive verbs it belongs to.4.1 Experimental MethodologyIn pilot experiments on a subset of the features, we investigated a number  of su-pervised machine learning methods that produce automatic lassifiers (decision treeinduction, rule learning, and two types of neural networks), as well as hierarchi-cal clustering; see Stevenson et al (1999) for more detail.
Because we achieved ap-proximately the same level of performance in all cases, we narrowed our furtherexperimentation to the publicly available version of the C5.0 machine learning system(http: / /www.rulequest.com),  a newer version of C4.5 (Quinlan 1992), due to its easeof use and wide availability.
The C5.0 system generates both decision trees and cor-responding rule sets from a training set of known classifications.
In our experiments,we found little to no difference in performance between the trees and rule sets, andreport only the rule set results.In the experiments below, we follow two methodologies in training and testing,each of which tests a subset of cases held out from the training data.
Thus, in all cases,the results we report are on test data that was never seen in training.
7The first training and testing methodology we follow is 10-fold cross-validation.
Inthis approach, the system randomly divides the data into ten parts, and runs ten timeson a different 90%-training-data/10%-test-data split,yielding an average accuracy andstandard error across the ten test sets.
This training methodology is very useful for7 One anonymous reviewer aised the concern that we do not test on verbs that were unseen by theauthors prior to finalizing the specific features to count.
However, this does not reduce the generalityof our results.
The features we use are motivated by linguistic theory, and derived from the set ofthematic properties that discriminate the verb classes.
It is therefore very unlikely that they are skewedto the particular verbs we have chosen.
Furthermore, our cross-validation experiments, described in thenext subsection, show that our results hold across a very large number of randomly selected subsets ofthis sample of verbs.387Computational Linguistics Volume 27, Number 3our application, as it yields performance measures across a large number of trainingdata/test data sets, avoiding the problems of outliers in a single random selectionfrom a relatively small data set such as ours.The second methodology is a single hold-out raining and testing approach.
Here,the system is run N times, where N is the size of the data set (i.e., the 59 verbs inour case), each time holding out a single data vector as the test case and using theremaining N-1 vectors as the training set.
The single hold-out methodology yields anoverall accuracy rate (when the results are averaged across all N trials), but also--unlike cross-validation--gives us classification results on each individual data vector.This property enables us to analyze differential performance on the individual verbsand across the different verb classes.Under both training and testing methodologies, the baseline (chance) performancein this task--a three-way classification--is 33.9%.
In the single hold-out methodology,there are 59 test cases, with 20, 19, and 20 verbs each from the unergative, unaccusative,and object-drop classes, respectively.
Chance performance ofpicking a single class labelas a default and assigning it to all cases would yield at most 20 out of the 59 casescorrect, or 33.9%.
For the cross-validation methodology, the determination f a baselineis slightly more complex, as we are testing on a random selection of 10% of the fulldata set in each run.
The 33.9% figure represents he expected relative proportion of atest set that would be labelled correctly by assignment of a default class label to theentire test set.
Although the precise make-up of the test cases vary, on average the testset will represent the class membership proportions of the entire set of verbs.
Thus,as with the single hold-out approach, chance accuracy corresponds to a maximum of20/59, or 33.9%, of the test set being labelled correctly.The theoretical maximum accuracy for the task is, of course, 100%, although inSection 5 we discuss ome classification results from human experts that indicate thata more realistic expectation is much lower (around 87%).4.2 Results Using 10-Fold Cross-ValidationWe first report he results of experiments u ing a training methodology of 10-fold cross-validation repeated 50 times.
This means that the 10-fold cross-validation procedure isrepeated for 50 different random divisions of the data.
The numbers reported are theaverages of the results over all the trials.
That is, the average accuracy and standarderror from each random division of the data (a single cross-validation run including10 training and test sets) are averaged across the 50 different random divisions.
Thislarge number of experimental trials gives us a very tight bound on the mean accuracyreported, enabling us to determine with high confidence the statistical significance ofdifferences in results.Table 8 shows that performance of classification using individual features variesgreatly, from little above the baseline to almost 22% above the baseline, or a reductionof a third of the error rate, a very good result for a single feature.
(All reportedaccuracies in Table 8 are statistically distinct, at the p < .01 level, using an ANOVA\[dr = 249, F = 334.72\], with a Tukey-Kramer post test.
)The first line of Table 9 shows that the combination of all features achieves anaccuracy of 69.8%, which is 35.9% over the baseline, for a reduction in the error rate of54%.
This is a rather considerable r sult, given the very low baseline (33.9%).
Moreover,recall that our training and testing sets are always disjoint (cf., Lapata and Brew \[1999\];Siegel \[1999\]); in other words, we are predicting the classification of verbs that werenever seen in the training corpus, the hardest situation for a classification algorithm.The second through sixth lines of Table 9 show the accuracy achieved on eachsubset of features that results from removing asingle feature.
This allows us to evaluate388Merlo and Stevenson Statistical Verb ClassificationTable 8Percent accuracy and standard error of the verb classification task using each featureindividually, under a training methodology of 10-fold cross-validation repeated 50 times.Feature %Accuracy %SECAUS 55.7 .1VBN 52.5 .5PASS 50.2 .5TRANS 47.1 .4ANIM 35.3 .5Table 9Percent accuracy and standard error of the verb classification task using features incombination, under a training methodology of 10-fold cross-validation repeated 50 times.FeatureFeatures Used Not Used %Accuracy %SE1.
TRANS PASS VBN CAUS ANIM 69.8 .52.
TRANS VBN CAUS ANIM PASS 69.8 .53.
TRANS PASS VBN ANIM CAUS 67.3 .64.
TRANS PASS CAUS ANIM VBN 66.5 .55.
TRANS PASS VBN CAUS ANIM 63.2 .66.
PASS VBN CAUS ANIM TRANS 61 .6  .6the contribution of each feature to the performance of the classification process, bycomparing the performance of the subset without it, to the performance using the fullset of features.
We see that the removal of PASS (second line) has no effect on the results,while removal of the remaining features yields a 2-8% decrease in performance.
(InTable 9, the differences between all reported accuracies are statistically significant, atthe p < .05 level, except for between lines 1 and 2, lines 3 and 4, and lines 5 and 6,using an ANOVA \[dr = 299, F = 37.52\], with a Tukey-Kramer post test.)
We observethat the behavior of the features in combination cannot be predicted by the individualfeature behavior.
For example, CAUS, which is the best individually, does not greatlyaffect accuracy when combined with the other features (compare line 3 to line 1).Conversely, ANIM and TRANS, which do not classify verbs accurately when used alone,are the most relevant in a combination of features (compare lines 5 and 6 to line 1).
Weconclude that experimentation with combinations of features is required to determinethe relevance of individual features to the classification task.The general behaviour in classification based on individual features and on size4 and size 5 subsets of features is confirmed for all subsets.
Appendix B reports theresults for all subsets of feature combinations, in order of decreasing performance.Table 10 summarizes this information.
In the first data column, the table illustratesthe average accuracy across all subsets of each size.
The second through sixth datacolumns report the average accuracy of all the size n subsets in which each featureoccurs.
For example, the second data cell in the second row (54.9) indicates the averageaccuracy of all subsets of size 2 that contain the feature VBN.
The last row of thetable indicates the average accuracy for each feature of all subsets containing thatfeature.389Computational Linguistics Volume 27, Number 3Table 10Average percent accuracy of feature subsets, by subset size and by sets of each size includingeach feature.Mean Accuracy of SubsetsSubset Mean Accuracy that Include Each FeatureSize by Subset Size VBN PASS TRANS ANIM CAUS1 48.2 52.5 50.2 47.1 35.3 55.72 55.1 54.9 52.8 56.4 58.0 57.63 60.5 60.1 58.5 62.3 61.1 60.54 65.7 65.5 64.7 66.7 66.3 65.35 69.8 69.8 69.8 69.8 69.8 69.8Mean Acc/Feature: 60.6 59.2 60.5 58.1 61.8The first observation--that more features perform better-- is confirmed overall,in all subsets.
Looking at the first data column of Table 10, we can observe that, onaverage, larger sets of features perform better than smaller sets.
Furthermore, as can beseen in the following individual feature columns, individual features perform better ina bigger set than in a smaller set, without exception.
The second observation--that theperformance of individual features is not always a predictor of their performance incombination-- is confirmed by comparing the average performance of each feature insubsets of different sizes to the average across all subsets of each size.
We can observe,for instance, that the feature CAUS, which performs very well alone, is average infeature combinations of size 3 or 4.
By contrast, the feature ANIM, which is the worstif used alone, is very effective in combination, with above average performance for allsubsets of size 2 or greater.4.3 Results Using Single Hold-Out MethodologyOne of the disadvantages of the cross-validation training methodology, which aver-ages performance across a large number of random test sets, is that we do not haveperformance data for each verb, nor for each class of verbs.
In another set of experi-ments, we used the same C5.0 system, but employed a single hold-out training andtesting methodology.
In this approach, we hold out a single verb vector as the test case,and train the system on the remaining 58 cases.
We then test the resulting classifier onthe single hold-out case, and record the assigned class for that verb.
This procedureis repeated for each of the 59 verbs.
As noted above, the single hold-out methodologyhas the benefit of yielding both classification results on each individual verb, and anoverall accuracy rate (the average results across all 59 trials).
Moreover, the results onindividual verbs provide the data necessary for determining accuracy for each verbclass.
This allows us to determine the contribution of individual features as above, butwith reference to their effect on the performance of individual classes.
This is impor-tant, as it enables us to evaluate our hypotheses concerning the relation between thethematic features and verb class distinctions, which we turn to in Section 4.4.We performed single hold-out experiments on the full set of features, as well as oneach subset of features with a single feature removed.
The first line of Table 11 showsthat the overall accuracy for all five features is almost exactly the same as that achievedwith the 10-fold cross-validation methodology (69.5% versus 69.8%).
As with the cross-validation results, the removal of PASS does not degrade performance-- in fact, here itsremoval appears to improve performance (see line 2 of Table 11).
However, it shouldbe noted that this increase in performance results from one additional verb being390Merlo and Stevenson Statistical Verb ClassificationTable 11Percent accuracy of the verb classification task using features in combination, under a singlehold-out raining methodology.Feature %AccuracyFeatures Used Not Used on All Verbs1.
TRANS PASS VBN CAUS ANIM 69.52.
TRANS VBN CAUS ANIM PASS 71.23.
TRANS PASS VBN ANIM CAUS 62.74.
TRANS PASS CAUS AN1M VBN 61.05.
TRANS PASS VBN CAUS ANIM 61.06.
PASS VBN CAUS ANIM TRANS 64.4Table 12F score of classification within each class, under a single hold-out raining methodology.Feature F score (%) F score (%) F score (%)Features Used Not Used for Unergs for Unaccs for Objdrops1.
TRANS PASS VBN CAUS ANIM 73.9 68.62.
TRANS VBN CAUS ANIM PASS 76.2 75.73.
TRANS PASS VBN ANIM CAUS 65.1 60 .04.
TRANS PASS CAUS ANIM VBN 66 .7  65 .05.
TRANS PASS VBN CAUS AN1M 72.7 47.06.
PASS VBN CAUS ANIM TRANS 78.1 51.564.961.662.851.360.061.9classified correctly.
The remaining lines of Table 11 show that the removal of any otherfeature has a 5-8% negative ffect on performance, again similar to the cross-validationresults.
(Although note that the precise accuracy achieved is not the same in each caseas with 10-fold cross-validation, indicating that there is some sensitivity to the precisemake-up of the training set when using a subset of the features.
)Table 12 presents the results of the single hold-out experiments in terms of per-formance within each class, using an F measure with balanced precision and recall.
8The first line of the table shows clearly that, using all five features, the unergativesare classified with greater accuracy (F = 73.9%) than the unaccusative and object-dropverbs (F scores of 68.6% and 64.9%, respectively).
The features appear to be betterat distinguishing unergatives than the other two verb classes.
The remaining lines ofTable 12 show that this pattern holds for all of the subsets of features as well.
Clearly,future work on our verb classification task will need to focus on determining featuresthat better discriminate unaccusative and object-drop verbs.One potential explanation that we can exclude is that the pattern of results is duesimply to the frequencies of the verbs--that is, that more frequent verbs are more ac-curately classified.
We examined the relation between classification accuracy and log8 For all previous results, we reported an accuracy measure (the percentage of correct classifications outof all classifications).
Using the terminology oftrue or false positives/negatives, thisis the same astruePositives/(truePositives + fal eNegafives).
In the earlier esults, there are no falsePositives ortrueNegatives, since we are only considering for each verb whether it is correctly classified(truePositive) ornot (falseNegative).
However, when we turn to analyzing the data for each class, thepossibility arises of having falsePositives and trueNegatives for that class.
Hence, here we use thebalanced F score, which calculates an overall measure of performance as2PR/(P + R), in which P(precision) is truePositives/(truePositives + fal ePositives), and R (recall) istruePositives/(truePositives + fal eNegatives).391Computational Linguistics Volume 27, Number 3frequencies of the verbs, both by class and individually.
By class, unergatives havethe lowest average log frequency (1.8), but are the best classified, while unaccusativesand object-drops are comparable (average log frequency = 2.4).
If we group individ-ual verbs by frequency, the proportion of errors to the total number of verbs is notlinearly related to frequency (log frequency K 2:7  errors/24 verbs, or 29% error; logfrequency between 2 and 3 :7  errors/25 verbs, or 28% error; log frequency > 3 :4errors/10 verbs, or 40% error).
Moreover, it seems that the highest-frequency verbspose the most problems to the program.
In addition, the only verb of log frequencyK 1 is correctly classified, while the only one with log frequency > 4 is not.
In con-clusion, we do not find that there is a simple mapping from frequency to accuracy.
Inparticular, it is not the case that more frequent classes or verbs are more accuratelyclassified.One factor possibly contributing to the poorer performance on unaccusatives andobject-drops i  the greater degree of error in the automatic ounting procedures forthese verbs, which we discussed in Section 3.2.
In addition to exploration of otherlinguistic features, another area of future work is to develop better search patterns, fortransitivity and passive in particular.
Unfortunately, one limiting factor in automaticcounting is that we inherit the inevitable rrors in POS tags in an automatically taggedcorpus.
For example, while the unergative verbs are classified highly accurately, wenote that two of the three errors in misclassifying unergatives (galloped and paraded) aredue to a high degree of error in tagging.
9 The verb galloped is incorrectly tagged VBNinstead of VBD in all 12 of its uses in the corpus, and the verb paraded is incorrectlytagged VBN instead of VBD in 13 of its 33 uses in the corpus.
After correcting onlythe VBN feature of these two verbs to reflect he actual part of speech, overall accuracyin classification increases by almost 10%, illustrating the importance of both accuratecounts and accurate annotation of the corpora.4.4 Contribution of the Features to ClassificationWe can further use the single hold-out results to determine the contribution of eachfeature to accuracy within each class.
We do this by comparing the class labels as-signed using the full set of five features (TRANS, PASS, VBN, CAUS, ANIM) with the classlabels assigned using each size 4 subset of features.
The difference in classificationsbetween each four-feature subset and the full set of features indicates the changes inclass labels that we can attribute to the added feature in going from the four-featureto five-feature set.
Thus, we can see whether the features indeed contribute to dis-criminating the classes in the manner predicted in Section 2.2, and summarized herein Table 13.We illustrate the data with a set of confusion matrices, in Tables 14 and 15, whichshow the pattern of errors according to class label for each set of features.
In eachconfusion matrix, the rows indicate the actual class of incorrectly classified verbs, andthe columns indicate the assigned class.
For example, the first row of the first panelof Table 14 shows that one unergative was incorrectly labelled as unaccusative, andtwo unergatives as object-drop.
To determine the confusability of any two classes (the9 The third error in classification f unergatives is the verb floated, which we conjecture is due not tocounting errors, but to the linguistic properties of the verb itself.
The verb is unusual for amanner-of-motion verb in that the action is inherently "uncontrolled", and thus the subject of theintransitive/object of he transitive isa more passive ntity than with the other unergatives (perhapsindicating that the inventory of thematic roles should be refined to distinguish activity verbs with lessagentive subjects).
We think that this property relates to the notion of internal and external causationthat is an important factor in distinguishing unergative and unaccusative rbs.
We refer the interestedreader to Stevenson and Merlo (1997b), which discusses the latter issue in more detail.392Merlo and Stevenson Statistical Verb ClassificationTable 13Expected class discriminations for each feature.Feature Expected Frequency PatternTransitivity Unerg < Unacc < ObjDropCausativity Unerg, ObjDrop < UnaccAnimacy Unacc < Unerg, ObjDropPassive Voice Unerg < Unacc < ObjDropVBN Tag Unerg < Unacc < ObjDropTable 14Confusion matrix indicating number of errors in classification by verb class, for the full set offive features, compared to two of the four-feature sets.
E = unergatives, A = unaccusatives,O = object-drops.Assigned ClassAll features w/o CAUS W/O ANIME A O E A O E A OActual E 1 2 4 2 2 2Class A 4 3 5 2 5 6O 5 3 4 5 3 5Table 15Confusion matrix indicating number of errors in classification by verb class, for the full set offive features and for three of the four-feature sets.
E = unergatives, A = unaccusatives,O = object-drops.Assigned ClassAll features w/o TRANS W/O PASS w/o  VBNE A O E A O E A O E A OActual E 1 2 2 2 1 3 1 5Class A 4 3 3 7 1 4 2 4O 5 3 2 5 5 3 4 6opposite of discriminability), we look at two cells in the matrix: the one in whichverbs of the first class were assigned the label of the second class, and the one inwhich verbs of the second class were assigned the label of the first class.
(These pairsof cells are those opposite the diagonal of the confusion matrix.)
By examining thedecrease (or increase) in confusability of each pair of classes in going from a four-feature experiment to the five-feature xperiment, we gain insight into how well (orhow poorly) the added feature helps to discriminate ach pair of classes.An analysis of the confusion matrices reveals that the behavior of the featureslargely conforms to our linguistic predictions, leading us to conclude that the features393Computational Linguistics Volume 27, Number 3we counted worked largely for the reasons we had hypothesized.
We expected CAUSand ANIM to be particularly helpful in identifying unaccusatives, and these predictionsare confirmed.
Compare the second to the first panel of Table 14 (the errors withoutthe CAUS feature compared to the errors with the ?AUS feature added to the set).We see that, without the CAUS feature, the confusability between unaecusatives andunergatives, and between unaccusatives and object-drops, is 9 and 7 errors, respec-tively; but when CAUS is added to the set of features, the confusability between thesepairs of classes drops substantially, to5 and 6 errors, respectively.
On the other hand,the confusability between unergatives and object-drops becomes lightly worse (errorsincreasing from 6 to 7).
The latter indicates that the improvement in unaccusatives isnot simply due to an across-the-board improvement in accuracy as a result of havingmore features.
We see a similar pattern with the ANIM feature.
Comparing the thirdto the first panel of Table 14 (the errors without the ANIM feature compared to theerrors with the ANIM feature added to the set), we see an even larger improvement indiscriminability of unaccusatives when the ANIM feature is added.
The confusabilityof unaccusatives and unergatives drops from 7 errors to 5 errors, and of unaccusativesand object-drops from 11 errors to 6 errors.
Again, confusability of unergatives andobject-drops i worse, with an increase in errors of 5 to 7.We had predicted that the TRANS feature would make a three-way distinctionamong the verb classes, based on its predicted linear relationship between the classes(see the inequalities in Table 13).
We had further expected that PASS and VBN wouldbehave similarly, since these features are correlated to TRANS.
To make a three-way dis-tinction among the verb classes, we would expect confusability between all three pairsof verb classes to decrease (i.e., discriminability would improve) with the addition ofTRANS, PASS, or VBN.
We find that these predictions are confirmed in part.First consider the TRANS feature.
Comparing the second to the first panel of Ta-ble 15, we find that unergatives are already accurately classified, and the addition ofTRANS to the set does indeed greatly reduce the confusability of unaccusatives andobject-drops, with the number of errors dropping from 12 to 6.
However, we alsoobserve that the confusability of unergatives and unaccusatives is not improved, andthe confusability of unergatives and object-drops i worsened with the addition ofthe TRANS feature, with errors in the latter case increasing from 4 to 7.
We concludethat the expected three-way discriminability of TRANS is most apparent in the reducedconfusion of unaccusative and object-drop verbs.Our initial prediction was that PASS and VBN would behave similarly to TRANS--that is, also making a three-way distinction among the classes--although the aggregatedata revealed little difference in these feature values between unaccusatives and object-drops.
Comparing the third to the first panel of Table 15, we observe that the additionof the PAss feature hinders the discriminability of unergatives and unaccusatives (in-creasing errors from 2 to 5); it does help in discriminating the other pairs of classes,but only slightly (reducing the number of errors by 1 in each case).
The VBN fea-ture shows a similar pattern, but is much more helpful at distinguishing unergativesfrom object-drops, and object-drops from unaccusatives.
In comparing the fourth tothe first panel of Table 15, we find that the confusability of unergatives and object-drops is reduced from 9 errors to 7, and of unaccusatives and object-drops from 10errors to 6.
The latter result is somewhat surprising, since the aggregate VBN datafor the unaccusative and object-drop classes are virtually identical.
We conclude thatcontribution of a feature to classification is not predictable from the apparent dis-criminability of its numeric values across the classes.
This observation emphasizes theimportance of an experimental method to evaluating our approach to verb classifica-tion.394Merlo and Stevenson Statistical Verb ClassificationTable 16Percent agreement (%Agr) and pair-wise agreement (K) of three experts (El, E2, E3) and theprogram compared to each other and to a gold standard (Levin).PROGRAM E1 E2 E3%Agr K %Agr K %Agr K %Agr KE1 59% .36E2 68% .50 75% .59E3 66% .49 70% .53 77% .66LEVIN 69.5% .54 71% .56 86.5% .80 83% .745.
Establishing the Upper Bound for the TaskIn order to evaluate the performance of the algorithm in practice, we need to compareit to the accuracy of classification performed by an expert, which gives a realistic upperbound for the task.
The lively theoretical debate on class membership of verbs, and thecomplex nature of the linguistic information ecessary to accomplish this task, led us tobelieve that the task is difficult and not likely to be performed at 100% accuracy evenby experts, and is also likely to show differences in classification between experts.We report here the results of two experiments which measure expert accuracy inclassifying our verbs (compared to Levin's classification as the gold standard), as wellas inter-expert agreement.
(See also Merlo and Stevenson \[2000a\] for more details.
)To enable comparison of responses, we performed a closed-form questionnaire study,where the number  and types of the target classes are defined in advance, for whichwe prepared a forced-choice and a non-forced-choice variant.
The forced-choice studyprovides data for a maximal ly restricted experimental situation, which correspondsmost closely to the automatic verb classification task.
However,  we are also interestedin slightly more natural results- -provided by the non-forced-choice task- -where theexperts can assign the verbs to an "others" category.We asked three experts in lexical semantics (all native speakers of English) tocomplete the forced-choice lectronic questionnaire study.
Neither author was amongthe three experts, who were all professionals in computational or theoretical linguisticswith a specialty in lexical semantics.
Materials consisted of individually randomizedlists of the same 59 verbs used for the machine learning experiments, using Levin's(1993) electronic index, available from Chicago University Press.
The verbs were tobe classified into the three target classes--unergative, unaccusative, and object-drop--which were described in the instructions.
1?
(All materials and instructions are availableat URL http: / /www.lat l .unige.ch/ lat l /personal /paola.html.
)Table 16 shows an analysis of the results, reporting both percent agreement andpairwise agreement (according to the Kappa statistic) among the experts and theprogram.
~1 Assessing the percentage of verbs on which the experts agree gives us10 The definitions of the classes were as follows.
Unergative: A verb that assigns an agent heta role to thesubject in the intransitive.
If it is able to occur transitively, it can have a causative meaning.Unaccusative: A verb that assigns apatient/theme theta role to the subject in the intransitive.
When itoccurs transitively, it has a causative meaning.
Object-Drop: A verb that assigns an agent role to thesubject and patient/theme role to the object, which is optional.
When it occurs transitively, it does nothave a causative meaning.11 In the comparison of the program to the experts, we use the results of the classifier under singlehold-out raining--which yields an accuracy of 69.5%--because those results provide the classificationfor each of the individual verbs.395Computational Linguistics Volume 27, Number 3an intuitive measure.
However, this measure does not take into account how muchthe experts agree over  the expected agreement by chance.
The latter is provided bythe Kappa statistic, which we calculated following Klauer (1987, 55-57) (using the zdistribution to determine significance; p ~ 0.001 for all reported results).
The Kappavalue measures the experts', and our classifier's, degree of agreement over chance,with the gold standard and with each other.
Expected chance agreement varies withthe number and the relative proportions of categories used by the experts.
This meansthat two given pairs of experts might reach the same percent agreement on a giventask, but not have the same expected chance agreement, if they assigned verbs toclasses in different proportions.
The Kappa statistic ranges from 0, for no agreementabove chance, to 1, for perfect agreement.
The interpretation of the scale of agreementdepends on the domain, like all correlations.
Carletta (1996) cites the convention fromthe domain of content analysis indicating that .67 K K < .8 indicates marginal agree-ment, while K > .8 is an indication of good agreement.
We can observe that only oneof our agreement figures comes close to reaching what would be considered "good"under this interpretation.
Given the very high level of expertise of our human experts,we suspect hen that this is too stringent a scale for our task, which is qualitativelyquite different from content analysis.Evaluating the experts' performance summarized in Table 16, we can remark twothings, which confirm our expectations.
First, the task is difficult--i.e., not performedat 100% (or close) even by trained experts, when compared to the gold standard, withthe highest percent agreement with Levin at 86.5%.
Second, with respect to comparisonof the experts among themselves, the rate of agreement is never very high, and thevariability in agreement is considerable, ranging from .53 to .66.
This evaluation isalso supported by a 3-way agreement measure (Siegel and Castellan 1988).
Applyingthis calculation, we find that the percentage of verbs to which the three experts gavethe same classification (60%, K = 0.6) is smaller than any of the pairwise agreements,indicating that the experts do not all agree on the same subset of verbs.The observation that the experts often disagree on this difficult task suggests thata combination of expert judgments might increase the upper bound.
We tried thesimplest combination, by creating a new classification using a majority vote: eachverb was assigned the label given by at least two experts.
Only three cases did nothave any majority label; in these cases we used the classification of the most accurateexpert.
This new classification does not improve the upper bound, reaching only 86.4%(K = .80) compared to the gold standard.The evaluation is also informative with respect to the performance of the program.On the one hand, we observe that if we take the best performance achieved by anexpert in this task---86.5%--as the maximum achievable accuracy in classification,our algorithm then reduces the error rate over chance by approximately 68%, a veryrespectable r sult.
In fact, the accuracy of 69.5% achieved by the program is only 1.5%less than one of the human experts in comparison to the gold standard.
On the otherhand, the algorithm still does not perform at expert level, as indicated by the fact that,for all experts, the lowest agreement score is with the program.One interesting question is whether experts and program disagree on the sameverbs, and show similar patterns of errors.
The program makes 18 errors, in total, com-pared to the gold standard.
However, in 9 cases, at least one expert agrees with theclassification given by the program.
The program makes fewer errors on unergatives(3) and comparably many on unaccusatives and object-drops (7 and 8 respectively), in-dicating that members of the latter two classes are quite difficult to classify.
This differsfrom the pattern of average agreement between the experts and Levin, who agree on17.7 (of 20) unergatives, 16.7 (of 19) unaccusatives, and 11.3 (of 20) object-drops.
This396Merlo and Stevenson Statistical Verb Classificationclearly indicates that the object-drop class is the most difficult for the human expertsto define.
This class is the most heterogeneous in our verb list, consisting of verbsfrom several subclasses of the "unexpressed object alternation" class in (Levin, 1993).We conclude that the verb classification task is likely easier for very homogeneousclasses, and more difficult for more broadly defined classes, even when the exemplarsshare the critical syntactic behaviors.On the other hand, frequency does not appear to be a simple factor in explainingpatterns of agreement between experts, or increases in accuracy.
As in Section 4.3,we again analyze the relation between log frequency of the verbs and classificationperformance, here considering the performance of the experts.
We grouped verbs inthree log frequency classes: verbs with log frequency less than 2 (i.e., frequency lessthan 100), those with log frequency between 2 and 3 (i.e., frequency between 100and 1000), and those with log frequency over 3 (i.e., frequency over 1000).
The low-frequency group had 24 verbs (14 unergatives, 5 unaccusatives, and 5 object-drop),the intermediate-frequency group had 25 verbs (5 unergatives, 9 unaccusatives, and11 object-drops), and the high-frequency group had 10 verbs (1 unergative, 5 unac-cusatives, and 4 object-drops).
We found that verbs with high and low frequency ieldbetter accuracy and agreement among the experts than the verbs with mid frequency.Neither the accuracy of the majority classification, or the accuracy of the expert hathad the best agreement with Levin, were linearly affected by frequency.
For the ma-jority vote, verbs with frequency less than 100 yield an accuracy of 92%, K = .84;verbs with frequency between 100 and 1000, accuracy 80%, K = .69; and for verbswith frequency over 1000, accuracy 90%, K = .82.
For the "best" expert, the patternis similar: verbs with frequency less than 100 yield an accuracy of 87.5%, K = .74;verbs with frequency between 100 and 1000, accuracy 84%, K = .76; and verbs withfrequency over 1000, accuracy 90%, K = .82.We can see here that different frequency groups yield different classification be-havior.
However, the relation is not simple, and it is clearly affected by the compositionof the frequency group: the middle group contains mostly unaccusative and object-drop verbs, which are the verbs with which our experts have the most difficulty.
Thisconfirms that the class of the verb is the predominant factor in their pattern of errors.Note also that the pattern of accuracy across frequency groupings is not the same asthat of the program (see Section 4.3, which revealed the most errors by the program onthe highest frequency verbs), again indicating qualitative differences in performancebetween the program and the experts.Finally, one possible shortcoming of the above analysis is that the forced-choicetask, while maximally comparable to our computational experiments, may not be anatural one for human experts.
To explore this issue, we asked two different expertsin lexical semantics (one native speaker of English and one bilingual) to complete thenon-forced-choice electronic questionnaire study; again, neither author served as oneof the experts.
In this task, in addition to the three verb classes of interest, an answerof "other" was allowed.
Materials consisted of individually randomized lists of 119target and filler verbs taken from Levin's (1993) electronic index, as above.
The targetswere again the same 59 verbs used for the machine learning experiments.
To avoidunwanted priming of target items, the 60 fillers were automatically selected from theset of verbs that do not share any class with any of the senses of the 59 target verbsin Levin's index.
In this task, if we take only the target items into account, the expertsagreed 74.6% of the time (K = 0.64) with each other, and 86% (K = 0.80) and 69%(K = 0.57) with the gold standard.
(If we take all the verbs into consideration, theyagreed in 67% of the cases \[K = 0.56\] with each other, and 68% \[K = 0.55\] and 60.5%\[K = 0.46\] with the gold standard, respectively.)
These results show that the forced-397Computational Linguistics Volume 27, Number 3choice and non-forced-choice task are comparable in accuracy of classification andinter-judge agreement on the target classes, giving us confidence that the forced-choiceresults provide a reasonably stable upper bound for computational experiments.6.
DiscussionThe work presented here contributes to some central issues in computational linguis-tics, by providing novel insights, data, and methodology in some cases, and by rein-forcing some previously established results in others.
Our research stems from threemain hypotheses:...Argument structure is the relevant level of representation for verbclassification.Argument structure is manifested distributionally in syntacticalternations, giving rise to differences in subcategorization frames or thedistributions of their usage, or in the properties of the NP arguments toa verb.This information is detectable in a corpus and can be learnedautomatically.We discuss the relevant debate on each of these hypotheses, and the contribution ofour results to each, in the following subsections.6.1 Argument Structure and Verb ClassificationArgument structure has previously been recognized as one of the most promisingcandidates for accurate classification.
For example, Basili, Pazienza, and Velardi (1996)argue that relational properties of verbs--their argument structure--are more infor-mative for classification than their definitional properties (e.g., the fact that a verbdescribes a manner of motion or a way of cooking).
Their arguments rest on linguisticand psycholinguistic results on classification and language acquisition (in particular,Pinker, \[1989\]; Rosch \[1978\]).Our results confirm the primary role of argument structure in verb classification.Our experimental focus is particularly clear in this regard because we deal with verbsthat are "minimal pairs" with respect o argument structure.
By classifying verbs thatshow the same subcategorizations (transitive and intransitive) into different classes,we are able to eliminate one of the confounds in classification work created by thefact that subcategorization and argument structure are often co-variant.
We can inferthat the accuracy in our classification is due to argument structure information, assubcategorization is the same for all verbs.
Thus, we observe that the content of thethematic roles assigned by a verb is crucial for classification.6.2 Argument Structure and Distributional StatisticsOur results further support the assumption that thematic differences across verbclasses are apparent not only in differences in subcategorization frames, but also indifferences in their frequencies.
This connection relies heavily on the hypothesis thatlexical semantics and lexical syntax are correlated, following Levin (1985; 1993).
How-ever, this position has been challenged by Basili, Pazienza, and Velardi (1996) andBoguraev and Briscoe (1989), among others.
For example, in an attempt o assessthe actual completeness and usefulness of the Longman Dictionary of ContemporaryEnglish (LDOCE) entries, Boguraev and Briscoe (1989) found that people assigned a398Merlo and Stevenson Statistical Verb Classification"change of possession" meaning both to verbs that had dative-related subcategoriza-tion frames (as indicated in the LDOCE) and to verbs that did not.
Conversely, theyalso found that both verbs that have a change-of-possession c mponent in their mean-ing and those that do not could have a dative code.
They conclude that the thesis putforth by Levin (1985) is only partially supported.
Basili, Pazienza, and Velardi (1996)show further isolated examples meant to illustrate that lexical syntax and semanticsare not in a one-to-one relation.Many recent results, however, seem to converge in supporting the view that therelation between lexical syntax and semantics can be usefully exploited (Aone andMcKee 1996; Dorr 1997; Dorr, Garman, and Weinberg 1995; Dorr and Jones 1996; La-pata and Brew 1999; Schulte im Walde 2000; Siegel 1998; Siegel 1999).
Our work inparticular underscores the relation between the syntactic manifestations of argumentstructure, and lexical semantic lass.
In light of these recent successes, the conclusionsin Boguraev and Briscoe (1989) are clearly too pessimistic.
In fact, their results do notcontradict he more recent ones.
First of all, it is not the case that if an implicationholds from argument structure to subcategorization (change of possession implies da-tive shift), the converse also holds.
It comes as no surprise that verbs that do nothave any change-of-possession component in their meaning may also show dativeshift syntactically.
Secondly, as Boguraev and Briscoe themselves note, Levin's state-ment should be interpreted as a statistical trend, and as such, Boguraev and Briscoe'sresults also confirm it.
They claim however, that in adopting a statistical point of view,predictive power is lost.
Our work shows that this conclusion is not appropriate ither:the correlation is strong enough to be useful to predict semantic lassification, at leastfor the argument structures that have been investigated.6.3 Detection of Argument Structure in CorporaGiven the manifestation of argument structure in statistical distributions, we view cor-pora, especially if annotated with currently available tools, as repositories of implicitgrammars, which can be exploited in automatic verb-classification tasks.
Besides es-tablishing a relationship between syntactic alternations and underlying semantic prop-erties of verbs, our approach extends existing corpus-based learning techniques to thedetection and automatic acquisition of argument structure.
To date, most work in thisarea has focused on learning of subcategorization from unannotated or syntacticallyannotated text (e.g., Brent \[1993\]; Sanfilippo and Poznanski \[1992\]; Manning \[1993\];Collins \[1997\]).
Others have tackled the problem of lexical semantic lassification, butusing only subcategorization frequencies as input data (Lapata and Brew 1999; Schulteim Walde 2000).
Specifically, these researchers have not explicitly addressed the def-inition of features to tap directly into thematic role differences that are not reflectedin subcategorization distinctions.
On the other hand, when learning of thematic roleassignment has been the explicit goal, the text has been semantically annotated (Web-ster and Marcus 1989), or external semantic resources have been consulted (Aone andMcKee 1996; McCarthy 2000).
We extend these results by showing that thematic in-formation can be induced from linguistically-guided counts in a corpus, without theuse of thematic role tagging or external resources uch as WordNet.Finally, our results converge with the increasing agreement that corpus-based tech-niques are fruitful in the automatic onstruction of computational lexicons, providingmachine readable dictionaries with complementary, reusable resources, such as fre-quencies of argument structures.
Moreover, these techniques produce data that is eas-ily updated, as the information contained in corpora changes all the time, allowingfor adaptability to new domains or usage patterns.
This dynamic aspect could be ex-ploited if techniques uch as the one presented here are developed, which can work399Computational Linguistics Volume 27, Number 3on a rough collection of texts, and do not require a carefully balanced corpus or time-consuming semantic tagging.7.
Related WorkWe conclude from the discussion above that our own work and work of others upportour hypotheses concerning the importance of the relation between classes of verbs andthe syntactic expression of argument structure in corpora.
In light of this, it is instruc-tive to evaluate our results in the context of other work that shares this view.
Somerelated work requires either exact exemplars for acquisition, or external pre-compiledresources.
For example, Dorr (1997) summarizes a number of automatic lassificationexperiments based on encoding Levin's alternations directly, as symbolic propertiesof a verb (Dorr, Garman, and Weinberg 1995; Dorr and Jones 1996).
Each verb is rep-resented as the binary settings of a vector of possible alternations, acquired througha large corpus analysis yielding exemplars of the alternation.
To cope with sparsedata, the corpus information is supplemented by syntactic information obtained fromthe LDOCE and semantic information obtained from WordNet.
This procedure clas-sifies 95 unknown verbs with 61% accuracy.
Dorr also remarks that this result couldbe improved to 83% if missing LDOCE codes were added.
While Dorr's work re-quires finding exact exemplars of the alternation, Oishi and Matsumoto (1997) presenta method that, like ours, uses surface indicators to approximate underlying proper-ties.
From a dictionary of dependency relations, they extract case-marking particles asindicators of the grammatical function properties of the verbs (which they call the-matic properties), such as subject and object.
Adverbials indicate aspectual properties.The combination of these two orthogonal dimensions gives rise to a classification ofJapanese verbs.Other work has sought o combine corpus-based extraction of verbal propertieswith statistical methods for classifying verbs.
Siegel's work on automatic aspectualclassification (1998, 1999) also reveals a close relationship between verb-related syn-tactic and semantic information.
In this work, experiments o learn aspectual classifi-cation from linguistically-based numerical indicators are reported.
Using combinationsof seven statistical indicators (some morphological nd some reflecting syntactic o-occurrences), it is possible to learn the distinction between events and states for 739verb tokens with an improvement of 10% over the baseline (error rate reduction of74%), and to learn the distinction between culminated and non-culminated vents for308 verb tokens with an improvement of 11% (error rate reduction of 29%) (Siegel1999).In work on lexical semantic verb classification, Lapata and Brew (1999) furthersupport he thesis of a predictive correlation between syntax and semantics in a statis-tical framework, showing that the frequency distributions of subcategorization frameswithin and across classes can disambiguate he usages of a verb with more than oneknown lexical semantic lass.
On 306 verbs that are disambiguated by subcategoriza-tion frame, they achieve 91.8% accuracy on a task with a 65.7% baseline, for a 76%reduction in error rate.
On 31 verbs that can take the same subcategorization(s) indifferent classes--more similar to our situation in that subcategorization alone cannotdistinguish the classes--they achieve 83.9% accuracy compared to a 61.3% baseline,for a 58% reduction in error.
Aone and McKee (1996), working with a much coarser-grained classification of verbs, present a technique for predicate-argument xtractionfrom multi-lingual texts.
Like ours, their work goes beyond statistics over subcate-gorizations to include counts over the more directly semantic feature of animacy.
Nonumerical evaluation of their results is provided.400Merlo and Stevenson Statistical Verb ClassificationSchulte im Walde (2000) applies two clustering methods to two types of frequencydata for 153 verbs from 30 Levin (1993) classes.
One set of experiments uses verbsubcategorization frequencies, and the other uses subcategorization frequencies plusselectional preferences (a numerical measure based on an adaptation of the relativeentropy method of Resnik \[1996\]).
The best results achieved are a correct classificationof 58 verbs out of 153, with a precision of 61% and recall of 36%, obtained usingonly subcategorization frequencies.
We calculate that this corresponds to an F-score of45% with balanced precision and recall, n The use of selectional preference informationdecreases classification performance under either clustering algorithm.
The results aresomewhat difficult o evaluate further, as there is no description of the classes included.Also, the method of counting correctness entails that some "correct" classes may besplit across distant clusters (this level of detail is not reported), so it is unclear howcoherent the class behaviour actually is.McCarthy (2000) proposes a method to identify diathesis alternations.
After learn-ing subcategorization frames, based on a parsed corpus, selectional preferences areacquired for slots of the subcategorization frames, using probability distributions overWordnet classes.
Alternations are detected by testing the hypothesis that, given anyverb, the selectional preferences for arguments occurring in alternating slots will bemore similar to each other than those for slots that do not alternate.
For instance, givena verb participating in the causative alternation, its selectional preferences for the sub-ject in an intransitive use, and for the object in a transitive use, will be more similarto each other than the selectional preferences for these two slots of a verb that doesnot participate in the causative alternation.
This method achieves the best accuracyfor the causative and the conative alternations (73% and 83%, respectively), despitesparseness of data.
McCarthy reports that a simpler measure of selectional preferencesbased simply on head words yields a lower 63% accuracy.
Since this latter measureis very similar to our CAUS feature, we think that our results would also improve byadopting a similar method of abstracting from head words to classes.Our work extends each of these approaches in some dimension, thereby provid-ing additional support for the hypothesis that syntax and semantics are correlated ina systematic and predictive way.
We extend Dorr's alternation-based automatic lassi-fication to a statistical setting.
By using distributional pproximations of indicators ofalternations, we solve the sparse data problem without recourse to external sources ofknowledge, such as the LDOCE, and in addition, we are able to learn argument struc-ture alternations using exclusively positive examples.
We improve on the approach ofOishi and Matsumoto (1997) by learning argument structure properties, which, unlikegrammatical functions, are not marked morphologically, and by not relying on exter-nal sources of knowledge.
Furthermore, in contrast o Siegel (1998) and Lapata andBrew (1999) our method applies successfully to previously unseen words--i.e., testcases that were not represented in the training set .
13 This is a very important propertyof lexical acquisition algorithms to be used for lexicon organization, as their maininterest lies in being applied to unknown words.On the other hand, our approach is similar to the approaches of Siegel, and La-pata and Brew (1999), in attempting to learn semantic notions from distributions of12 A baseline of 5% is reported, based on a closest-neighbor pairing of verbs, but it is not straightforwardto compare this task to the proposed clustering algorithm.
Determining a meaningful baseline forunsupervised clustering isclearly achallenge, but this gives an indication that the clustering task isindeed ifficult.13 Siegel (1998) reports two experiments over verb types with disjoint raining and test sets, but theresults were not significantly different from the baseline.401Computational Linguistics Volume 27, Number 3indicators that can be gleaned from a text.
In our case, we are trying to learn argu-ment structure, a finer-grained classification than the dichotomic distinctions studiedby Siegel.
Like Lapata and Brew, three of our indicators--TRANS, VBN, PASS--are basedon the assumption that distributional differences in subcategorization frames are re-lated to underlying verb class distinctions.
However, we also show that other syntacticindicators--cAUS and ANIM--can be devised that tap directly into the argument struc-ture of a verb.
Unlike Schulte im Walde (2000), we find the use of these semanticfeatures helpful in classification--using only TRANS and its related features, VBN andPASS, we achieve only 55% accuracy, in comparison to 69.8% using the full set of fea-tures.
This can perhaps be seen as support for our hypothesis that argument structureis the right level of representation forverb class distinctions, ince it appears that ourfeatures that capture thematic differences are useful in classification, while Schulte imWalde's electional restriction features were not.Aone and McKee (1996) also use features that are intended to tap into both sub-categorization and thematic role distinctions--frequencies of the transitive use andanimate subject use.
In our task, we show that subject animacy can be profitably ap-proximated solely with pronoun counts, avoiding the need for reference to externalsources of semantic information used by Aone and McKee.
In addition, our work ex-tends theirs in investigating much finer-grained verb classes, and in classifying verbsthat have multiple argument structures.
While Aone and McKee define ach of theirclasses according to a single argument structure, we demonstrate he usefulness ofsyntactic features that capture relations across different argument structures of a sin-gle verb.
Furthermore, while Aone and McKee, and others, look at relative frequencyof subcategorization frames (as with our TRANS feature), or relative frequency of aproperty of NPs within a particular grammatical function (as with our ANIM feature),we also look at the paradigmatic relations across a text between thematic argumentsin different alternations (with our CAUS feature).McCarthy (2000) shows that a method very similar to ours can be used for identi-fying alternations.
Her qualitative results confirm, however, what was argued in Sec-tion 2 above: counts that tap directly into the thematic assignments are necessary tofully identify a diathesis alternation.
In fact, on close inspection, McCarthy's methoddoes not distinguish between the induced-action alternation (which the unergativesexhibit) and the causative/inchoative alternation (which the unaccusatives xhibit);thus, her method oes not discriminate two of our classes.
It is likely that a combina-tion of our method, which makes the necessary thematic distinctions, and her moresophisticated method of detecting alternations would give very good results.8.
Limitations and Future WorkThe classification results show that our method is powerful, and suited to the clas-sification of unknown verbs.
However, we have not yet addressed the problem ofverbs that can have multiple classifications.
We think that many cases of ambigu-ous classification of the lexical entry for a verb can be addressed with the notionof intersective sets introduced by Dang et al (1998).
This is an important concept,which proposes that "regular" ambiguity in classification--i.e., sets of verbs that havethe same multi-way classifications according to Levin (1993)--can be captured witha finer-grained notion of lexical semantic lasses.
Thus, subsets of verbs that occurin the intersection of two or more Levin classes form in themselves a coherent se-mantic (sub)class.
Extending our work to exploit this idea requires only definingthe classes appropriately; the basic approach will remain the same.
Given the cur-rent demonstration f our method on fine-grained classes that share subcategoriza-402Merlo and Stevenson Statistical Verb Classificationtion alternations, we are optimistic regarding its future performance on intersectivesets.Because we assume that thematic properties are reflected in alternations of argu-ment structure, our features require searching for relations across occurrences of eachverb.
This motivated our initial experimental focus on verb types.
However, whenwe turn to consider ambiguity, we must also address the problem that individual in-stances of verbs may come from different classes, and we may (like Lapata and Brew\[1999\]) want to classify the individual tokens of a verb.
In future research we planto extend our method to the case of ambiguous tokens, by experimenting with thecombination of several sources of information: the classification of each instance willbe a function of a bias for the verb type (using the cross-corpus statistics we collect),but also of features of the usage of the instance being classified (cf., Lapata and Brew\[1999\]; Siegel \[1998\]).Finally, corpus-based learning techniques collect statistical information related tolanguage use, and are a good starting point for studying human linguistic perfor-mance.
This opens the way to investigating the relation of linguistic data in text topeople's linguistic behaviour and use.
For example, Merlo and Stevenson (1998) showthat, contrary to the naive assumption, speakers' preferences in syntactic disambigua-tion are not simply directly related to frequency (i.e., a speaker's preference for oneconstruction over another is not simply modelled by the frequency of the construc-tion, or of the words in the construction).
Thus, the kind of corpus investigation weare advocating--founded on in-depth linguistic analysis--holds promise for buildingmore natural NLP systems which go beyond the simplest assumptions, and tie to-gether statistical computational linguistic results with experimental psycholinguisticdata.9.
ConclusionsIn this paper, we have presented an in-depth case study, in which we investigatemachine learning techniques for automatically classifying a set of verbs into classesdetermined by their argument structures.
We focus on the three major classes of op-tionally intransitive verbs in English, which cannot be discriminated by their subcate-gorizations, and therefore require distinctive features that are sensitive to the thematicproperties of the verbs.
We develop such features and automatically extract hem fromvery large, syntactically annotated corpora.
Results show that a small number of lin-guistically motivated lexical features are sufficient o achieve a 69.8% accuracy ratein a three-way classification task with a baseline (chance) performance of 33.9%, forwhich the best performance achieved by a human expert is 86.5%.Returning to our original questions of what can and need be learned about therelational properties of verbs, we conclude that argument structure is both a highlyuseful and learnable aspect of verb knowledge.
We observe that relevant semanticproperties of verb classes (such as causativity, or animacy of subject) may be suc-cessfully approximated through countable syntactic features.
In spite of noisy data(arising from diverse sources uch as tagging errors, or limitations of our extractionpatterns), the lexical properties of interest are reflected in the corpora robustly enoughto positively contribute to classification.We remark, however, that deep linguistic analysis cannot be eliminated--in ourapproach it is embedded in the selection of the features to count.
Specifically, ourfeatures are derived through a detailed analysis of the differences in thematic role as-signments across the verb classes under investigation.
Thus, an important contributionof the work is the proposed mapping between the thematic assignment properties of403Computational Linguistics Volume 27, Number 3the verb classes, and the statistical distributions of their surface syntactic properties.We think that using such linguistically motivated features makes the approach veryeffective and easily scalable: we report a 54% reduction in error rate (a 68% reduction,when the human expert-based upper bound is considered), using only five featuresthat are readily extractable from automatically annotated corpora.AcknowledgmentsWe gratefully acknowledge the financialsupport of the following organizations: theSwiss NSF (fellowship 8210-46569 to PM);the United States NSF (grants #9702331 and#9818322 to SS); the Canadian NSERC(grant o SS); the University of Toronto; andthe Information Sciences Council of RutgersUniversity.
Much of this research wascarried out while PM was a visiting scientistat IRCS, University of Pennsylvania, ndwhile SS was a faculty member at RutgersUniversity, both of whose generous andsupportive nvironments were of greatbenefit o us.
We thank Martha Palmer,Michael Collins, Natalia Kariaeva, KaminWhitehouse, Julie Boland, Kiva Dickinson,and three anonymous reviewers, for theirhelpful comments and suggestions, and fortheir contributions to this research.
We alsogreatly thank our experts for the graciouscontribution of their time in answering ourelectronic questionnaire.ReferencesAbney, Steven.
1996.
Partial parsing viafinite-state cascades.
In John Carroll,editor, Proceedings ofthe Workshop on RobustParsing at the Eighth Summer School onLogic, Language and Information, umber435 in CSRP, pages 8-15.
University ofSussex, Brighton.Aone, Chinatsu and Douglas McKee.
1996.Acquiring predicate-argument mappinginformation i  multilingual texts.
InBranimir Boguraev and JamesPustejovsky, editors, Corpus Processing forLexical Acquisition.
MIT Press,pages 191-202.Basili, Roberto, Maria-Teresa Pazienza, andPaola Velardi.
1996.
A context-drivenconceptual c ustering method for verbclassification.
In Branimir Boguraev andJames Pustejovsky, editors, CorpusProcessing for Lexical Acquisition.
MITPress, pages 117-142.Boguraev, Branimir and Ted Briscoe.
1989.Utilising the LDOCE grammar codes.
InBranimir Boguraev and Ted Briscoe,editors, Computational Lexicography forNatural Language Processing.
Longman,London, pages 85-116.Boguraev, Branimir and James Pustejovsky.1996.
Issues in text-based lexiconacquisition.
In Branimir Boguraev andJames Pustejovsky, editors, CorpusProcessing for Lexical Acquisition.
MITPress, pages 3-20.Brent, Michael.
1993.
From grammar tolexicon: Unsupervised learning of lexicalsyntax.
Computational Linguistics,19(2):243-262.Briscoe, Ted and John Carroll.
1997.Automatic extraction of subcategorizationfrom corpora.
In Proceedings ofthe FifthApplied Natural Language ProcessingConference, pages 356-363.Brousseau, Anne-Marie and Elizabeth Ritter.1991.
A non-unified analysis of agentiveverbs.
In West Coast Conference on FormalLinguistics, number 20, pages 53-64.Burzio, Luigi.
1986.
Italian Syntax: AGovernment-Binding Approach.
Reidel:Dordrecht.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: the Kappa statistics.Computational Linguistics, 22(2):249-254.Collins, Michael John.
1997.
Threegenerative, lexicalised models forstatistical parsing.
In Proceedings ofthe 35thAnnual Meeting of the ACL, pages 16-23,Madrid, Spain.Cruse, D. A.
1972.
A note on Englishcausatives.
Linguistic Inquiry, 3(4):520-528.Dang, Hoa Trang, Karin Kipper, MarthaPalmer, and Joseph Rosenzweig.
1998.Investigating regular sense extensionsbased on intersective Levin classes.
InProceedings ofthe 36th Annual Meeting of theACL and the 17th International Conference onComputational Linguistics (COLING-ACL"98), pages 293-299, Montreal.
Universit~de Montreal.Dixon, Robert M. W. 1994.
Ergativity.Cambridge University Press, Cambridge.Dorr, Bonnie.
1997.
Large-scale dictionaryconstruction for foreign language tutoringand interlingual machine translation.Machine Translation, 12(4):1-55.Dorr, Bonnie, Joe Garman, and AmyWeinberg.
1995.
From syntactic encodingsto thematic roles: Building lexical entriesfor interlingual MT.
Journal of MachineTranslation, 9(3):71-100.404Merlo and Stevenson Statistical Verb ClassificationDorr, Bonnie and Doug Jones.
1996.
Role ofword sense disambiguation i  lexicalacquisition: Predicting semantics fromsyntactic ues.
In Proceedings ofthe 16thInternational Conference on ComputationalLinguistics, pages 322-327, Copenhagen.Dowty, David.
1991.
Thematic proto-rolesand argument selection.
Language,67(3):547-619.Greenberg, Joseph H. 1966.
LanguageUniversals.
Mouton, The Hague, Paris.Gruber, Jeffrey.
1965.
Studies in LexicalRelation.
MIT Press, Cambridge, MA.Hale, Ken and Jay Keyser.
1993.
Onargument structure and the lexicalrepresentation f syntactic relations.
In K.Hale and J. Keyser, editors, The View fromBuilding 20.
MIT Press, pages 53-110.Jakobson, Roman.
1971.
Signe Z4ro.
InSelected Writings, volume 2, 2d ed.Mouton, The Hague, pages 211-219.Kariaeva, Natalia.
1999.
Discriminatingbetween unaccusative and object-dropverbs: Animacy factor.
Ms., RutgersUniversity.
New Brunswick, NJ.Klavans, Judith and Martin Chodorow.1992.
Degrees of stativity: The lexicalrepresentation f verb aspect.
InProceedings ofthe Fourteenth InternationalConference on Computational Linguistics(COLING "92), pages 1126-1131, Nantes,France.Klavans, Judith and Min-Yen Kan. 1998.Role of verbs in document analysis.
InProceedings ofthe 36th Annual Meeting of theACL and the 17th International Conference onComputational Linguistics (COLING-ACL'98), pages 680-686, Montreal.
Universit4de Montreal.Lapata, Maria.
1999.
Acquiring lexicalgeneralizations from corpora: A casestudy for diathesis alternations.
InProceedings ofthe 37th Annual Meeting of theAssociation for Computational Linguistics(ACL'99), pages 397-404, College Park,MD.Lapata, Maria and Chris Brew.
1999.
Usingsubcategorization to resolve verb classambiguity.
In Proceedings ofJoint SIGDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora,pages 266-274, College Park, MD.Levin, Beth.
1985.
Introduction.
In BethLevin, editor, Lexical Semantics in Review,number 1 in Lexicon Project WorkingPapers.
Centre for Cognitive Science, MIT,Cambridge, MA, pages 1-62.Levin, Beth.
1993.
English Verb Classes andAlternations.
University of Chicago Press,Chicago, IL.Levin, Beth and Malka Rappaport Hovav.1995.
Unaccusativity.
MIT Press,Cambridge, MA.Manning, Christopher D. 1993.
Automaticacquisition of a large subcategorizationdictionary from corpora.
In Proceedings ofthe 31st Annual Meeting of the Association forComputational Linguistics, pages 235-242.Ohio State University.McCarthy, Diana.
2000.
Using semanticpreferences to identify verbalparticipation i  role switchingalternations.
In Proceedings ofANLP-NAACL 2000, pages 256-263,Seattle, WA.McCarthy, Diana and Anna Korhonen.
1998.Detecting verbal participation i  diathesisalternations.
In Proceedings ofthe 36thAnnual Meeting of the ACL and the 17thInternational Conference on ComputationalLinguistics (COLING-ACL "98),pages 1493-1495, Montreal, Universit4 deMontreal.Merlo, Paola and Suzanne Stevenson.
1998.What grammars tell us about corpora: thecase of reduced relative clauses.
InProceedings ofthe Sixth Workshop on VeryLarge Corpora, pages 134-142, Montreal.Merlo, Paola and Suzanne Stevenson.
2000a.Establishing the upper-bound andinter-judge agreement in a verbclassification task.
In Second InternationalConference on Language Resources andEvaluation (LREC-2000), volume 3,pages 1659-1664.Merlo, Paola and Suzanne Stevenson.
2000b.Lexical syntax and parsing architecture.In Matthew Crocker, Martin Pickering,and Charles Clifton, editors, Architecturesand Mechanisms for Language Processing.Cambridge University Press, Cambridge,pages 161-188.Moravcsik, Edith and Jessica Wirth.
1983.Markedness--an Overview.
In FredEckman, Edith Moravcsik, and JessicaWirth, editors, Markedness.
Plenum Press,New York, NY, pages 1-13.Oishi, Akira and Yuji Matsumoto.
1997.Detecting the organization of semanticsubclasses of Japanese verbs.
InternationalJournal of Corpus Linguistics, 2(1):65-89.Palmer, Martha.
2000.
Consistent criteria forsense distinctions.
Special Issue ofComputers and the Humanities,SENSEVAL98: Evaluating Word SenseDisambiguation Systems, 34(1-2):217-222.Perlmutter, David.
1978.
Impersonalpassives and the unaccusative hypothesis.In Proceedings ofthe Annual Meeting of theBerkeley Linguistics Society, volume 4,pages 157-189.405Computational Linguistics Volume 27, Number 3Pinker, Steven.
1989.
Learnability andCognition: the Acquisition of ArgumentStructure.
MIT Press, Cambridge, MA.Quinlan, J. Ross.
1992.
C4.5: Programs forMachine Learning.
Series in MachineLearning.
Morgan Kaufmann, San Mateo,CA.Ratnaparkhi, Adwait.
1996.
A maximumentropy parbof-speech tagger.
InProceedings of the Empirical Methods inNatural Language Processing Conference,pages 133-142, Philadelphia, PA.Resnik, Philip.
1996.
Selectional constraints:an information-theoretic model and itscomputational realization.
Cognition,61(1-2):127-160.Riloff, Ellen and Mark Schmelzenbach.
1998.An empirical approach to conceptual caseframe acquisition.
In Proceedings of the SixthWorkshop on Very Large Corpora,pages 49-56.Rosch, Eleanor.
1978.
Principles ofcategorization.
I  Cognition andCategorization.
Lawrence Erlbaum Assoc,Hillsdale, NJ.Sanfilippo, Antonio and Victor Poznanski.1992.
The acquisition of lexical knowledgefrom combined machine-readabledictionary sources.
In Proceedings of theThird Applied Natural Language ProcessingConference, pages 80-87, Trento, Italy.Schulte im Walde, Sabine.
2000.
Clusteringverbs semantically according to theiralternation behaviour.
In Proceedings ofCOLING 2000, pages 747-753,Saarbruecken, Germany.Siegel, Eric 1998.
Linguistic Indicators forLanguage Understanding: Using machinelearning methods to combine corpus-basedindicators for aspectual c assification of clauses.Ph.D.
thesis, Dept.
of Computer Science,Columbia University.Siegel, Eric.
1999.
Corpus-based linguisticindicators for aspectual classification.
InProceedings of ACL'99, pages 112-119,College Park, MD.
University ofMaryland.Siegel, Sidney and John Castellan.
1988.Nonparametric statistics for the BehavioralSciences.
McGraw-Hill, New York.Silverstein, Michael.
1976.
Hierarchy offeatures and ergativity.
In Robert Dixon,editor, Grammatical Categories in AustralianLanguages.
Australian Institute ofAboriginal Studies, Canberra,pages 112-171.Srinivas, Bangalore and Aravind K. Joshi.1999.
Supertagging: An approach toalmost parsing.
Computational Linguistics,25(2):237-265.Stede, Manfred.
1998.
A generativeperspective on verb alternations.Computational Linguistics, 24(3):401--430.Stevenson, Suzanne and Paola Merlo.
1997a.Architecture and experience in sentenceprocessing.
In Proceedings of the 19thAnnual Conference of the Cognitive ScienceSociety, pages 715-720.Stevenson, Suzanne and Paola Merlo.
1997b.Lexical structure and processingcomplexity.
Language and CognitiveProcesses, 12(1-2):349-399.Stevenson, Suzanne, Paola Merlo, NataliaKariaeva, and Kamin Whitehouse.
1999.Supervised learning of lexical semanticverb classes using frequencydistributions.
In Proceedings of SigLex99:Standardizing Lexical Resources (SigLex'99),pages 15-21, College Park, MD.Trubetzkoy, Nicolaj S. 1939.
Grundzage derPhonologie.
Travaux du CercleLinguistique de Prague, Prague.Webster, Mort and Mitch Marcus.
1989.Automatic acquisition of the lexicalsemantics of verbs from sentence frames.In Proceedings of the 27th Annual Meeting ofthe Association for Computational Linguistics,pages 177-184, Vancouver, Canada.406Merlo and Stevenson Statistical Verb ClassificationAppendix AThe fo l lowing three tables conta in the overall  f requency and the normal i zed  featurevalues for each of the 59 verbs in our exper imental  set.Unergative VerbsFreq VBN PASS TRANS CAUS ANIMMin Value 8 0.00 0.00 0.00 0.00 0.00Max Value 4088 1.00 0.39 0.74 0.00 1.00floated 176 0.43 0.26 0.74 0.00 0.17hurried 86 0.40 0.31 0.50 0.00 0.37jumped 4088 0.09 0.00 0.03 0.00 0.20leaped 225 0.09 0.00 0.05 0.00 0.13marched 238 0.10 0.01 0.09 0.00 0.12paraded 33 0.73 0.39 0.46 0.00 0.50raced 123 0.01 0.00 0.06 0.00 0.15rushed 467 0.22 0.12 0.20 0.00 0.10vaulted 54 0.00 0.00 0.41 0.00 0.03wandered 67 0.02 0.00 0.03 0.00 0.32galloped 12 1.00 0.00 0.00 0.00 0.00glided 14 0.00 0.00 0.08 0.00 0.50hiked 25 0.28 0.12 0.29 0.00 0.40hopped 29 0.00 0.00 0.21 0.00 1.00jogged 8 0.29 0.00 0.29 0.00 0.33scooted 10 0.00 0.00 0.43 0.00 0.00scurried 21 0.00 0.00 0.00 0.00 0.14skipped 82 0.22 0.02 0.64 0.00 0.16tiptoed 12 0.17 0.00 0.00 0.00 0.00trotted 37 0.19 0.17 0.07 0.00 0.18Unaccusative VerbsFreq VBN PASS TRANS CAUS ANIMMin Value 13 0.16 0.00 0.02 0.00 0.00Max Value 5543 0.95 0.80 0.76 0.41 0.36boiled 58 0.92 0.70 0.42 0.00 0.00cracked 175 0.61 0.19 0.76 0.02 0.14dissolved 226 0.51 0.58 0.71 0.05 0.11exploded 409 0.34 0.02 0.66 0.37 0.04flooded 235 0.47 0.57 0.44 0.04 0.03fractured 55 0.95 0.76 0.51 0.00 0.00hardened 123 0.92 0.55 0.56 0.12 0.00melted 70 0.80 0.44 0.02 0.00 0.19opened 3412 0.21 0.09 0.69 0.16 0.36solidified 34 0.65 0.21 0.68 0.00 0.12collapsed 950 0.16 0.00 0.16 0.01 0.02cooled 232 0.85 0.21 0.29 0.13 0.11folded 189 0.73 0.33 0.23 0.00 0.00widened 1155 0.18 0.02 0.13 0.41 0.01changed 5543 0.73 0.23 0.47 0.22 0.08cleared 1145 0.58 0.40 0.50 0.31 0.06divided 1539 0.93 0.80 0.17 0.10 0.05simmered 13 0.83 0.00 0.09 0.00 0.00stabilized 286 0.92 0.13 0.18 0.35 0.00407Computational Linguistics Volume 27, Number 3Object-Drop VerbsFreq VBN PASS TRANS CAUS ANIMMin Value 39 0.10 0.04 0.21 0.00 0.00Max Value 15063 0.95 0.99 1.00 0.24 0.42carved 185 0.85 0.66 0.98 0.00 0.00danced 88 0.22 0.14 0.37 0.00 0.00kicked 308 0.30 0.18 0.97 0.00 0.33knitted 39 0.95 0.99 0.93 0.00 0.00painted 506 0.72 0.18 0.71 0.00 0.38played 2689 0.38 0.16 0.24 0.00 0.00reaped 172 0.56 0.05 0.90 0.00 0.22typed 57 0.81 0.74 0.81 0.00 0.00washed 137 0.79 0.60 1.00 0.00 0.00yelled 74 0.10 0.04 0.38 0.00 0.00borrowed 1188 0.77 0.15 0.60 0.13 0.19inherited 357 0.60 0.13 0.64 0.06 0.32organized 1504 0.85 0.38 0.65 0.18 0.07rented 232 0.72 0.22 0.61 0.00 0.42sketched 44 0.67 0.17 0.44 0.00 0.20cleaned 160 0.83 0.47 0.21 0.05 0.21packed 376 0.84 0.12 0.40 0.05 0.19studied 901 0.66 0.17 0.57 0.05 0.11swallowed 152 0.79 0.44 0.35 0.04 0.22called 15063 0.56 0.22 0.72 0.24 0.16Appendix BPerformance of all the subsets of features, in  order of decreasing accuracy.
To determinewhether  the difference between any two results is statistically signif icant, the 95%confidence interval  can be calculated for each of the two results, and  the two rangeschecked to see whether  they overlap.
To do this, take each accuracy p lus and  minus2.01 t imes its associated s tandard  error to get the 95% confidence range (dr = 49,t = 2.01).
If the two ranges overlap, then the difference in accuracy is not  signif icantat the p < .05 level.Accuracy SE Features Accuracy SE Features69.8 0.5 TRANS PASS VBN CAUS ANIM 57.3 0.5 TRANS CAUS69.8 0.5 TRANS VBN CAUS ANIM 57.3 0.5 PASS VBN ANIM67.3 0.6 TRANS PASS VBN ANIM 56.7 0.5 PASS CAUS ANIM66.7 0.5 TRANS VBN ANIM 55.7 0.5 VBN CAUS66.5 0.5 TRANS PASS CAUS ANIM 55.7 0.1 CAUS64.4 0.5 TRANS VBN CAUS 55.4 0.4 PASS CAUS63.2 0.6 TRANS PASS VBN CAUS 55.0 0.6 TIKANS PASS VBN63.0 0.5 TRANS PASS ANIM 54.7 0.4 TRANS PASS62.9 0.4 TRANS CAUS AN~M 54.2 0.5 TRANS VBN62.1 0.5 CAUS ANIM 52.5 0.5 VBN61.7 0.5 TRANS PASS CAUS 50.9 0.5 PASS ANIM61.6 0.6 PASS VBN CAUS ANIM 50.2 0.6 PASS VBN60.1 0.4 VBN CAUS ANIM 50.2 0.5 PASS59.5 0.6 TRANS ANIM 47.1 0.4 TKANS59.4 0.5 VBN ANIM 35.3 0.5 ANIM57.4 0.6 PASS VBN CAUS408
