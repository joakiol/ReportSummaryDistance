GEMINI :  A NATURAL LANGUAGE SYSTEM FORSPOKEN-LANGUAGE UNDERSTANDING*J ohn  Dowding ,  Jean  Mark  Gawron ,  Doug Appe l t ,John  Bear ,  Lynn  Cherny ,  Rober t  Moore ,  and  Doug las  MoranSR I  In ternat iona l333 Ravenswood AvenueMen lo  Park ,  CA  94025In ternet :  dowding@ai .
s r i .
com1.
INTRODUCTIONGemini is a natural language (NL) under-standing system developed for spoken languageapplications.
This paper describes the details ofthe system, and includes relevant measurementsof size, efficiency, and performance of each of itscomponents.In designing any NL understanding system,there is a tension between robustness and correct-ness.
Forgiving an error risks throwing away cru-cial information; furthermore, devices added to asystem to enhance robustness can sometimes en-rich the ways of finding an analysis, multiplyingthe number of analyses for a given input, and mak-ing it more difficult to find the correct analysis.
Inprocessing spoken language this tension is height-ened because the task of speech recognition in-troduces a new source of error.
The robust sys-tem will attempt o find a sensible interpretation,even in the presence of performance errors by thespeaker, or recognition errors by the speech rec-ognizer.
On the other hand, a system should beable to detect hat a recognized string is not a sen-tence of English, to help filter recognition errors bythe speech recognizer.
Furthermore, if parsing andrecognition are interleaved, then the parser shouldenforce constraints on partial utterances.The approach taken in Gemini is to con-strain language recognition with fairly conven-tional grammar, but to augment hat grammarwith two orthogonal rule-based recognition mod-ules, one for glueing together the fragments foundduring the conventional grammar parsing phase,and another for recognizing and eliminating dis-fluencies known as "repairs."
At the same time,*This research was supported by the Advanced Re-search Projects Agency under Contract ONR N00014-90-C-0085 with the Office of Naval Research.
Theviews and conclusions contained in this document arethose of the authors and should not be interpreted asnecessarily representing the official policies, either ex-pressed or implied, of the Advanced Research ProjectsAgency of the U.S. Government.the multiple analyses arising before and after allthis added robustness are managed in two ways:first, by highly constraining the additional rule-based modules by partitioning the rules into pref-erence classes, and second, through the additionof a postprocessing parse preference component.Processing starts in Gemini when syntac-tic, semantic, and lexical rules are applied by abottom-up all-paths constituent parser to populatea chart with edges containing syntactic, seman-tic, and logical form information.
Then, a secondutterance parser is used to apply a second set ofsyntactic and semantic rules that are required tospan the entire utterance.
If no semantically ac-ceptable utterance-spanning edges are found dur-ing this phase, a component to recognize and cor-rect certain grammatical disfluencies is applied.When an acceptable interpretation is found, a setof parse preferences i  used to choose a single bestinterpretation from the chart to be used for sub-sequent processing.
Quantifier scoping rules areapplied to this best interpretation to produce thefinal logical form, which is then used as input toa query-answering system.
The following sectionsdescribe each of these components in detail, withthe exception of the query-answering subsystem,which is not described in this paper.In our component-by-component view ofGemini, we provide detailed statistics on eachcomponent's size, speed, coverage, and accuracy.These numbers detail our performance on the sub-domain of air-travel planning that is currently be-ing used by the ARPA spoken language under-standing community (MADCOW, 1992).
Gem-ini was trained on a 5875-utterance dataset fromthis domain, with another 688 utterances used asa blind test (not explicitly trained on, but runnmltiple times) to monitor our performance on adataset on which we did not train.
We also reporthere our results on another 756-utterance fair testset that we ran only once.
Table 1 contains a sum-mary of the coverage of the various components onboth the training and fair test sets.
More detailed54explanations ofthese numbers are given in the rel-evant sections.Training TestLexicon 99.1% 95.9%Syntax 94.2% 90.9%Semantics 87.4% 83.7%Syntax (repair correction) 96.0% 93.1%Semantics (repair correction) 89.1% 86.0%Table 1: Domain Coverage by Component2.
SYSTEM DESCRIPT IONGemini maintains a firm separation betweenthe language- and domain-specific portions of thesystem, and the underlying infrastructure and ex-ecution strategies.
The Gemini kernel consists ofa set of compilers to interpret he high-level lan-guages in which the lexicon and syntactic and se-mantic grammar rules are written, as well as theparser, semantic interpretation, quantifier scop-ing, repair correction mechanisms, and all otheraspects of Gemini that are not specific to a lan-guage or domain.
Although this paper describesthe lexicon, grammar, and semantics of English,Gemini has also been used in a Japanese spo-ken language understanding system (Kameyama,1992).2.1.
Grammar  Formal i smGemini ncludes a midsized constituent gram-mar of English (described in section 2.3), a smallutterance grammar for assembling constituentsinto utterances (described in section 2.7), and alexicon.
All three are written in a variant of theunification formalism used in the Core LanguageEngine (Alshawi, 1992) .The basic building block of the grammar for-malism is a category with feature constraints.Here is an example:np: \[wh=ynq, case= (nomVacc),pers_num= (3rdAsg) \]This category can be instantiated by any nounphrase with the value ynq for its wh feature (whichmeans it must be a wh-bearing noun phrase likewhich book, who, or whose mother), either ace (ac-cusative) or nora (nominative) for its case feature,and the conjunctive value 3rdAsg (third and sin-gular) for its person-number feature.
This for-malism is related directly to the Core LanguageEngine, but more conceptually it is closely re-lated to that of other unification-based grammarformalisms with a context-free skeleton, such asPATR-II (Shieber et al, 1983), Categorial Uni-fication Grammar (Uszkoreit, 1986), GeneralizedPhrase-Structure Grammar (Gazdar et al, 1982),and Lexical Functional Grammar (Bresnan, 1982).Gemini differs from other unification for-malisms in the following ways.
Since many ofthe most interesting issues regarding the formal-ism concern typing, we defer discussing motivationuntil section 2.5.Gemini uses typed unification.
Each categoryhas a set of features declared for it.
Each fea-ture has a declared value space of possible values(value spaces may be shared by different fea-tures).
Feature structures in Gemini can be re-cursive, but only by having categories in theirvalue space; so typing is also recursive.
Typedfeature structures are also used in HPSG (Pol-lard and Sag, in press).
One important differ-ence with the use in Gemini s that Gemini hasno type inheritance.Some approaches do not assume a syntacticskeleton of category-introducing rules (for ex-ample, Functional Unification Grammar (Kay,1979)).
Some make such rules implicit (forexample, the various categorial unification ap-proaches, uch as Unification Categorial Gram-mar (Zeevat, Klein, and Calder, 1987)).Even when a syntactic skeleton is assumed,some approaches do not distinguish the categoryof a constituent (for example, rip, vp) from itsother features (for example, pers_aum, gapsin,gapsout).
Thus, for example, in one version ofGPSG, categories were simply feature bundles(attribute value structures) and there was a fea-ture l~hJ taking values like N,V,A, and P whichdetermined the major category of constituent.?
Gemini does not allow rules schematizing oversyntactic ategories.2.2.
Lex iconThe Gemini lexicon uses the same categorynotation as the Gemini syntactic rules.
Lexicalcategories are types as well, with sets of featuresdefined for them.
The lexical component of Gem-ini includes the lexicon of base forms, lexical tem-plates, morphological rules, and the lexical typeand feature default specifications.The Gemini lexicon used for the air-travelplanning domain contains 1,315 base entries.These expand by morphological rules to 2,019.
Inthe 5875-utterance training set, 52 sentences con-tained unknown words (0.9%), compared to 31sentences in the 756-utterance fair test set (4.1%).2.3.
Const i tuent  GrammarA simplified example of a syntactic rule is55syn (whq_ynq_slash_np,\[ s: \[sentence_type=whq, form=tnsd,gapsin=G, gapsout=G\],np: \[wh=ynq, pers_num=N\] ,s : \[sentence_type=ynq, form=tnsd,gapsin=np: \[pets_hum=N\],gapsout =null\] \] ).This syntax rule (named whq_ynq..$1ash..np)says that a sentence (category s) can be built byfinding a noun phrase (category np) followed by asentence.
It requires that the daughter np have thevalue ynq for its wh feature and that it have thevalue 1~ (a variable) for its person-number feature.It requires that the daughter sentence have a cat-egory value for its gaps in  feature, namely an npwith a person number value N, which is the same asthe person number value on the wh-bearing nounphrase.
The interpretation of the entire rule isthat a gapless sentence with sentence_type whqcan be built by finding a wh-phrase followed by asentence with a noun phrase gap in it that has thesame person number as the wh-phrase.Semantic rules are written in much the samerule format, except hat in a semantic rule, each ofthe constituents mentioned in the phrase structureskeleton is associated with a logical form.
Thus,the semantics for the rule above iss em (whq_ynq_slash_np,\[( \ [ ,hq,  S\] ,  s : \['1 ) ,(Np, np: \ [ \ ] ) ,(S, s : \[gapsin=np: \[gapsem=Np\] \] ) ) .Here the semantics of the mother s is just thesemantics of the daughter s with the illocution-ary force marker whq wrapped around it.
In addi-tion, the semantics of the s gap's np's gapsem hasbeen unified with the semantics of the wh-phrase.Through a succession of unifications this will endup assigning the wh-phrase's semantics to the gapposition in the argument structure of the s. Al-though each semantic rule must be keyed to a pre-existing syntactic rule, there is no assumption ofrule-to-rule uniqueness.
Any number of semanticrules may be written for a single syntactic rule.We discuss some further details of the semanticsin section 2.6The constituent grammar used in Gemini con-tains 243 syntactic rules, and 315 semantic rules.Syntactic overage on the 5875-utterance trainingset was 94.2%, and on the 756-utterance t st setit was 90.9%.2.4.
ParserSince Gemini was designed with spoken lan-guage interpretation i  mind, key aspects of theGemini parser are motivated by the increasedneeds for robustness and efficiency that charac-terize spoken language.
Gemini uses essentiallya pure bottom-up chart parser, with some limitedleft-context constraints applied to control creationof categories containing syntactic gaps.Some key properties of the parser are?
The parser is all-paths bottom-up, so that allpossible edges admissible by the grammar arefound.?
The parser uses subsumption checking to reducethe size of the chart.
Essentially, an edge is notadded to the chart if it is less general than apreexisting edge, and preexisting edges are re-moved from the chart if the new edge is moregeneral.?
The parser is on-line (Graham, Harrison, andRusso, 1980), essentially meaning that all edgesthat end at position i are constructed beforeany that end at position i + 1.
This feature isparticularly desirable if the final architecture ofthe speech understanding system couples Gem-ini tightly with the speech recognizer, since itguarantees for any partial recognition input thatall possible constituents will be built.An important feature of the parser is themechanism used to constrain the construction ofcategories containing syntactic gaps.
In earlierwork (Moore and Dowding, 1991), we showed thatapproximately 80% of the edges built in an all-paths bottom-up arser contained gaps, and thatit is possible to use prediction in a bottom-upparser only to constrain the gap categories, with-out requiring prediction for nongapped categories.This limited form of left-context constraint greatlyreduces the total number of edges built for a verylow overhead.
In the 5875-utterance training set,the chart for the average sentence contained 313edges, but only 23 predictions.2.5.
Typ ingThe main advantage of typed unification is forgrammar development.
The type information onfeatures allows the lexicon, grammar, and seman-tics compilers to provide detailed error analysis re-garding the flow of values through the grammar,and to warn if features are assigned improper val-ues, or variables of incompatible types are unified.Since the type-analysis performed statically atcompile time, there is no run-time overhead asso-ciated with adding types to the grammar.The major grammatical category plays a spe-cial role in the typing scheme of Gemini.
For eachcategory, Gemini makes a set of declarations stipu-lating its allowable features and the relevant valuespaces.
Thus, the distinction between the syntac-tic category of a constituent and its other featurescan be cashed out as follows: the syntactic cat-egory can be thought of as the feature structure56type.
The only other types needed by Gemini arethe value spaces used by features.
Thus for ex-ample, the type v (verb) admits a feature vforra,whose value space vforra-types call be instanti-ated with values like present participle, finite, andpast participle.
Since all recursive features arecategory-valued, these two kinds of types suffice.2.6.
In ter leav ing  Syntact i c  and  Se-mant ic  In fo rmat ionSor ta l  Const ra in ts  Selectional restrictionsare imposed in Gemini through the sorts mecha-nism.
Selectional restrictions include both highlydomain-specific information about predicate-argument and very general predicate restrictions.For example, in our application the object ofthe transitive verb depart (as in flights departingBoston) is restricted to be an airport or a city,obviously a domain-specific requirement.
But thesame machinery also restricts a determiner like allto take two propositions, and an adjective like fur-ther to take distances as its measure-specifier (asin thirty miles further).
In fact, sortal constraintsare assigned to every atomic predicate and opera-tor appearing in the logical forms constructed bythe semantic rules.Sorts are located in a conceptual hierarchyand are implemented as Prolog terms such thatmore general sorts subsume more specific sorts(Mellish, 1988).
This allows the subsumptionchecking and packing in the parser to share struc-ture whenever possible.
Semantic coverage withsortal constraints applied was 87.4% on the train-ing set, and on the test set it was 83.7%.In ter leav ing  Semant ics  w i th  Pars ing  InGemini, syntactic and semantic processing is fullyinterleaved.
Building an edge requires that syntac-tic constraints be applied, which results in a treestructure, to which semantic rules can be applied,which results in a logical form to which sortal con-traints can be applied.
Only if the syntactic edgeleads to a well-sorted semantically-acceptable log-ical form fragment is it added to the chart.Interleaving the syntax and semantics in thisway depends on a crucial property of the seman-tics: a semantic interpretation is available for eachsyntactic node.
This is guaranteed by the seman-tic rule formalism and by the fact that every lexicalitem has a semantics associated with it.Table 2 contains average edge counts andparse timing statistics 1 for the 5875-utterancetraining set.1Gemini s implemented primarily in Quintus Pro-log version 3.1.1.
All timing numbers given in thispaper were run on a lightly loaded Sun SPARCsta-tion 2 with at least 48 MB of memory.
Under normalconditions, Gemini runs in under 12 MB of memory.Edges TimeSyntax only 197 3.4 sec.Syntax -t- semantics 234 4.47 sec.Syntax q- semantics ?
sorts 313 13.5 sec.Table 2: Average Number of Edges Built by In-terleaved Processing2.7.
Ut terance  Pars ingThe constituent parser uses the constituentgrammar to build all possible categories bottom-up, independent of location within the string.Thus, the constituent parser does not force anyconstituent o occur either at the beginning ofthe utterance, or at the end.
Those constraintsare stated in what we call the utterance grammar.They are applied after constituent parsing is com-plete by the utterance parser.
The utterance gram-mar specifies ways of combining the categoriesfound by the constituent parser into an analysisof the complete utterance.
It is at this point thatthe system recognizes whether the sentence wasa simple complete sentence, an isolated sentencefragment, a run-on sentence, or a sequence of re-lated fragments.Many systems (Carbonell and Hayes, 1983),(Hobbs et al, 1992), (Seneff, 1992), (Stallard andBobrow, 1992) have added robustness with a sim-ilar postprocessing phase.
The approach takenin Gemini differs in that the utterance grammaruses the same syntactic and semantic rule for-malism used by the constituent grammar.
Thus,the same kinds of logical forms built during con-stituent parsing are the output of utterance pars-ing, with the same sortal constraints enforced.
Forexample, an utterance consisting of a sequenceof modifier fragments (like on Tuesday at threeo'clock on United) is interpreted as a conjoinedproperty of a flight, because the only sort of thingin the ATIS domain that can be on Tuesday atthree o'clock on United is a flight.The utterance parser partitions the utterancegrammar into equivalence classes and considerseach class according to an ordering.
Utteranceparsing terminates when all constituents satisfy-ing the rules of the current equivalence class arebuilt, unless there are none, in which case the nextclass is considered.
The highest ranked class con-sists of rules to identify simple complete sentences,the next highest class consists of rules to iden-tify simple isolated sentence fragments, and so on.Thus, the utterance parser allows us to enforce avery coarse form of parse preferences (for exam-ple, prefering complete sentences to sentence frag-ments).
These coarse preferences could also beenforced by the parse preference component de-57scribed in section 2.9, but for the sake of efficiencywe choose to enforce them here.The utterance grammar is significantlysmaller than the constituent grammar - only 37syntactic rules and 43 semantic rules.2.8.
Repa i rsGrammatical disfluencies occur frequently inspontaneous spoken language.
We have imple-mented a component to detect and correct a largesubclass of these disfluencies (called repairs, orself-corrections) where the speaker intends thatthe meaning of the utterance be gotten by deletingone or more words.
Often, the speaker gives cluesof their intention by repeating words or adding cuewords that signal the repair:(1) a.
How many American airline flights leaveDenver on June June tenth.b.
Can you give me information on all theflights from San Francisco no from Pitts-burgh to San Francisco n Monday.The mechanism used in Gemini to detect andcorrect repairs is currently applied as a fallback ifno semantically acceptable interpretation is foundfor the complete utterance.
The mechanism findssequences of identical or related words, possiblyseparated by a cue word (for example, oh or no)that might indicate the presence of a repair, anddeletes the first occurrence of the matching por-tion.
Since there may be several such sequences ofpossible repairs in the utterance, the mechanismproduces a ranked set of candidate corrected ut-terances.
These candidates are ranked in orderof the fewest deleted words.
The first candidatethat can be given an interpretation is accepted asthe intended meaning of the utterance.
This ap-proach is presented in detail in (Bear, Dowding,and Shriberg, 1992).The repair correction mechanism helps in-crease the syntactic and semantic coverage ofGemini (as reported in Table 1).
In the 5875-utterance training set, 178 sentences containednontrivial repairs 2, of which Gemini found 89(50%).
Of the sentences Gemini corrected, 81 wereanalyzed correctly (91%), and 8 contained repairsbut were corrected wrongly.
Similarly, the 756-utterance test set contained 26 repairs, of whichGemini found 11 (42%).
Of those 11, 8 were ana-lyzed correctly (77%), and 3 were analyzed incor-rectly.Since Gemini's approach is to extend lan-guage analysis to recognize specific patterns char-acteristic of spoken language, it is important for2For these results, we ignored repairs consisting ofonly an isolate fragment word, or sentence-initial fil erwords like "yes" and "okay".components like repair correction (which providethe powerful capability of deleting words) not tobe applied in circumstances where no repair ispresent.
In the 5875-utterance training set, Gem-ini misidentified only 15 sentences (0.25%) as con-taining repairs when they did not.
In the 756-utterance test set, only 2 sentences were misiden-tiffed as containing repairs (0.26%).While the repair correction component cur-rently used in Gemini does not make use of acous-tic/prosodic information, it is clear that acousticscan contribute meaningful cues to repair.
In fu-ture work, we hope to improve the performance ofour repair correction component by incorporatingacoustic/prosodic techniques for repair detection(Bear, Dowding, and Shriberg, 1992) (Nakataniand Hirschberg, 1993) (O'Shaughnessy, 1992).A central question about the repairs moduleconcerns its role in a tightly integrated system inwhich the NL component filters speech recognitionhypotheses.
The open question: should the repairsmodule be part of the recognizer filter or shouldit continue to be a post-processing component?The argument for including it in the filter is thatwithout a repairs module, the NL system rejectsmany sentences with repairs, and will thus dispre-fer essentially correct recognizer hypotheses.
Theargument against including it is efficiency and theconcern that with recognizer errors present, therepair module's precision may suffer: it may at-tempt to repair sentences with no repair in them.Our current best guess is that recognizer errorsare essentially orthogonal to repairs and that afilter including the repairs module will not sufferfrom precision problems.
But we have not yet per-formed the experiments o decide this.2.9.
Parse  P re ference  Mechan ismIn Gemini, parse preferences are enforcedwhen extracting syntactically and semanticallywell-formed parse trees from the chart.
In thisrespect, our approach differs from many otherapproaches to the problem of parse preferences,which make their preference decisions as pars-ing progresses, pruning subsequent parsing paths(Frazier and Fodor, 1978), (Hobbs and Bear,1990), (Marcus 1980).
Applying parse prefer-ences requires comparing two subtrees panningthe same portion of the utterance.The parse preference mechanism begins witha simple strategy to disprefer parse trees contain-ing specific "marked" syntax rules.
As an exampleof a dispreferred rule, consider: Book those threeflights to Boston.
This sentence has a parse onwhich those three is a noun phrase with a miss-ing head (consider a continuation of the discourseThree of our clients have sufficient credit).
Afterpenalizing such dispreferred parses, the preference58mechanism applies attachment heuristics based onthe work by Pereira (1985) and Shieber (1983)Pereira's paper shows how the heuristics ofMinimal Attachment and Right Association (Kim-ball, 1973) can both be implemented using abottom-up shift-reduce parser.
(2)(a) John sang a song for Mary.
(b) John canceled the room Mary reserved yes-terday.Minimal Attachment selects for the tree with thefewest nodes, so in (2a), the parse that makes forMary a complement of sings is preferred.
RightAssociation selects for the tree that incorporatesa constituent A into the rightmost possible con-stituent (where rightmost here means beginningthe furthest o the right).
Thus, in (2b) the parsein which yesterday modifies reserved is preferred.The problem with these heuristics is thatwhen they are formulated loosely, as in the pre-vious paragraph, they appear to conflict.
In par-ticular, in (2a), Right Association seems to call forthe parse that makes for Mary a modifier of song.Pereira's goal is to show how a shift-reduceparser can enforce both heuristics without conflictand enforce the desired preferences for exampleslike (2a) and (2b).
He argues that Minimal At-tachment and Right Association can be enforced inthe desired way by adopting the following heuris-tics for resolving conflicts:1.
Right Association: In a shift-reduce conflict,prefer shifts to reduces.2.
Minimal Attachment: In a reduce-reduce on-flict, prefer longer educes to shorter educes.Since these two principles never apply to the samechoice, they never conflict.For purposes of invoking Pereira's heuristics,the derivation of a parse can be represented asthesequence of S's (Shift) and R's (Reduce) needed toconstruct he parse's unlabeled bracketing.
Con-sider, for example, the choice between two unla-beled bracketings of (2a):(a) \[John \[sang \[a song \] \[for Mary \] \] \]S S S S R S S RRR(b) \[John \[sang \[\[a song\] \[for Mary \]\] \]\]S S S S R S S RRRRThere is a shift for each word and a reduce foreach right bracket.
Comparison of the two parsesconsists imply of pairing the moves in the shift-reduce derivation from left to right.
Any parsemaking a shift move that corresponds to a reducemove loses by Right Association.
Any parse mak-ing a reduce move that corresponds to a longerreduce loses by Minimal Attachment.
In deriva-tion (b) above, the third reduce move builds theconstituent a song for Mary from two constituents,while the corresponding reduce in (a) builds sanga song for Mary from three constituents.
Parse(b) thus loses by Minimal Attachment.Questions about the exact nature of parsepreferences (and thus about the empirical ade-quacy of Pereira's proposal) still remain open, butthe mechanism sketched oes provide plausible re-sults for a number of examples.2.10.
Scop ingThe final logical form produced by Geminiis the result of applying a set of quantifier scop-ing rules to the best interpretation chosen by theparse preference mechanism.
The semantic rulesbuild quasi-logical forms, which contain completesemantic predicate-argument structure, but do notspecify quantifier scoping.
The scoping algorithmthat we use combines yntactic and semantic in-formation with a set of quantifier scoping prefer-ence rules to rank the possible scoped logical formsconsistent with the quasi-logical form selected byparse preferences.
This algorithm is described indetail in (Moran, 1988).3.
CONCLUSIONIn our approach to resolving the tension be-tween overgeneration a d robustness in a spokenlanguage understanding system, some aspects ofGemini are specifically oriented towards limitingovergeneration, such as the on-line property forthe parser, and fully interleaved syntactic and se-mantic processing.
Other components, such as thefragment and run-on processing provided by theutterance grammar, and the correction of recog-nizable grammatical repairs, increase the robust-ness of Gemini.
We believe a robust system canstill recognize and disprefer utterances containingrecognition errors.Research in the construction of the Geminisystem is ongoing to improve Gemini's peed andcoverage, as well as to examine deeper integrationstrategies with speech recognition, and integrationof prosodic information i to spoken !anguage dis-ambiguation.REFERENCESAlshawi, tI.
(ed) (1992).
The Core Language En-gine, MIT Press, Cambridge.Bear, J., Dowding, J., and Shriberg, E.
(1992).
"Integrating Multiple Knowledge Sources forthe Detection and Correction of Repairs inHuman-Computer Dialog", in Proceedings ofthe 30lh Annual Meeting of the Associationfor Computational Linguists, Newark, DE,pp.56-63.59Bresnan, \].
(ed) (1982).
The Mental Represen-tation of Grammatical Relations, MIT Press,Cambridge.Carbonell, J., and Hayes, P. (1983).
"RecoveryStrategies for Parsing Extragrammatical L n-guage", American Journal of ComputationalLinguistics, Vol.
9, Numbers 3-4, pp.
123-146.Frazier, L., and Fodor, J.D.
(1978).
"The SausageMachine: A New Two-Stage Parsing Model",Cognition, Vol.
6, pp.
291-325.Gazdar, G., Klein, E., Pullum, G., and Sag, I.(1982).
Generalized Phrase Structure Gram-mar, Harvard University Press, Cambridge.Graham, S., ttarrison, M., and Ruzzo, W.(1980).
"An Improved Context-Free Recog-nizer", ACM Transactions on ProgrammingLanguages and Systems, Vol.
2, No.
3, pp.
415-462.Hobbs, J., and Bear, J.
(1990).
"Two Principlesof Parse Preference", in Proceedings of the13th International Conference on Computa-tional Linguistics, Helsinki, Vol.
3, pp.
162-167.Hobbs, J., Appelt, D., Bear, J., Tyson, M., andMagerman, D. (1992).
"Robust Processingof Real-World Natural-Language T xts", inText Based Intelligent Systems, ed.
P. Jacobs,Lawrence Erlbaum Associates, Hillsdale, N J,pp.
13-33.Kameyama, M. (1992).
"The Syntax and Seman-tics of the Japanese Language Engine", forth-coming.
In Mazuka, R., and N. Nagai, Eds.Japanese Syntactic Processing, Hillsdale, NJ:Lawrence Erlbaum Associates.Kay, M. (1979).
"Functional Grammar", in Pro-ceedings of the 5th Annual Meeting of theBerkeley Linguistics Society.
pp.
142-158.Kimball, J.
(1973).
"Seven Principles of SurfaceStructure Parsing in Natural Language", Cog-nition, Vol.
2, No.
1, pp.
15-47.MADCOW (1992).
"Multi-site Data Collection fora Spoken Language Corpus", in Proceedingsof the DARPA Speech and Natural LanguageWorkshop, February 23-26, 1992.Marcus, M. (1980).
A Theory of Syntactic Recog-nition for Natural Language, MIT Press,Cambridge.Moran, D. (1988).
"Quantifier Scoping in the SRICore Language Engine", in Proceedings of the26th Annual Meeting of the Association forComputational Linguistics, State Universityof New York at Buffalo, Buffalo, NY, pp.
33-40.Mellish, C. (1988).
"Implementing Systemic Clas-sification by Unification".
Computational Lin-guistics Vol.
14, pp.
40-51.Moore, R., and Dowding, J.
(1991).
"EfficientBottom-up Parsing", in Proceedings of theDARPA Speech and Natural Language Work-shop, February 19-22, 1991, pp.
200-203.Nakatani, C., and Hirschberg, J.
(1993).
"ASpeech-First Model for Repair Detection andCorrection", in Proceedings of the ARPAWorkshop on Human Language Technology,March 21-24, 1993, Plainsboro, NJ.O'Shaughnessy, D. (1992).
"Analysis of FalseStarts in Spontaneous Speech", in Proceed-ings of the 1992 International Conference onSpoken Language Processing, October 12-16,1992, Banff, Alberta, Canada, pp.
931-934.Pereira, F. (1985).
"A New Characterization f At-tachment Preferences", in Natural LanguageParsing, Ed.
by Dowty, D., Karttunen, L.,and Zwicky, A., Cambridge University Press,Cambridge, pp.
307-319.Pollard, C., and Sag, I.
(in press).
Information-Based Syntax and Semantics, Vol.
2, CSLILecture Notes.Seneff, S. (1992).
"A Relaxation Method for Un-derstanding Spontaneous Speech Utterances",in Proceedings of the Speech and Natural Lan-guage Workshop, Harriman, NY, pp.
299-304.Shieber, S. (1983).
"Sentence Disambiguation byaShift-Reduce Parsing Technique", in Proceed-ings of the 21 Annual Meeting of the Associ-ation for Computational Linguistics, Boston,Massachusetts, pp.
113-118.Shieber, S., Uszkoreit, H., Pereira, F., Robinson,J., and Tyson, M. (1983).
"The Formalismand Implementation f PATR-II", in Grosz,B.
and Stickel, M. (eds) Research on Interac-tive Acquisition and Use of Knowledge, SRIInternational, pp.
39-79.Stallard, D., and Bobrow, R. (1992).
"FragmentProcessing in the DELPHI System", in Pro-ceedings of the Speech and Natural LanguageWorkshop, Harriman, NY, pp.
305-310.Uszkoreit, H. (1986).
"Categorial UnificationGrammars", in Proceedings of the 11th Inter-national Conference on Computational Lin-guistics and the 24th Annual Meeting of theAssociation for Computational Linguistics,Institut fur Kummunikkationsforschung undPhonetik, Bonn University.Zeevat, H., Klein, E., and Calder, J.
(1987).
"AnIntroduction to Unification Categorial Gram-mar", in IIaddock, N., Klein, E., Merrill, G.60(eds.)
Edinburgh Working Papers in Cogni-tive Science, Volume 1: Calegorial Grammar,Unification Grammar, and Parsing.61
