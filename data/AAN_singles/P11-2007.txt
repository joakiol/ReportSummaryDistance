Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 37?41,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsThe Arabic Online Commentary Dataset:an Annotated Dataset of Informal Arabic with High Dialectal ContentOmar F. Zaidan and Chris Callison-BurchDept.
of Computer Science, Johns Hopkins UniversityBaltimore, MD 21218, USA{ozaidan,ccb}@cs.jhu.eduAbstractThe written form of Arabic, Modern StandardArabic (MSA), differs quite a bit from thespoken dialects of Arabic, which are the true?native?
languages of Arabic speakers used indaily life.
However, due to MSA?s prevalencein written form, almost all Arabic datasetshave predominantly MSA content.
We presentthe Arabic Online Commentary Dataset, a52M-word monolingual dataset rich in dialec-tal content, and we describe our long-term an-notation effort to identify the dialect level (anddialect itself) in each sentence of the dataset.So far, we have labeled 108K sentences, 41%of which as having dialectal content.
We alsopresent experimental results on the task of au-tomatic dialect identification, using the col-lected labels for training and evaluation.1 IntroductionThe Arabic language is characterized by an interest-ing linguistic dichotomy, whereby the written formof the language, Modern Standard Arabic (MSA),differs in a non-trivial fashion from the various spo-ken varieties of Arabic.
As the variant of choice forwritten and official communication, MSA contentsignificantly dominates dialectal content, and in turnMSA dominates in datasets available for linguisticresearch, especially in textual form.The abundance of MSA data has greatly aided re-search on computational methods applied to Arabic,but only the MSA variant of it.
A state-of-the-artArabic-to-English machine translation system per-forms quite well when translating MSA source sen-tences, but often produces incomprehensible outputwhen the input is dialectal.
For example, most wordsSrc(MSA):??? ??
"#?TL:mtYsnrYh*hAlvlpmnAlmjrmyntxDEllmHAkmp?MT: Whenwill we seethisgroupof offenders subjecttoa trial?Src(Lev):?;????@A"?
@?B?
"#C?TL:AymtYrHn$wf  hAl$lpmnAlmjrmynbttHAkm?MT: AimitysuggestedNcovHalclpBtaathakm of criminals ?Greattranslate!Figure 1: Two roughly equivalent Arabic sentences, onein MSA and one in Levantine Arabic, translated by thesame MT system into English.
An acceptable translationwould be When will we see this group of criminals un-dergo trial (or tried)?.
The MSA variant is handled well,while the dialectal variant is mostly transliterated.of the dialectal sentence of Figure 1 are transliter-ated.1 Granted, it is conceivable that processing di-alectal content is more difficult than MSA, but themain problem is the lack of dialectal training data.2In this paper, we present our efforts to createa dataset of dialectal Arabic, the Arabic OnlineCommentary Dataset, by extracting reader com-mentary from the online versions of three Arabicnewspapers, which have a high degree (about half)of dialectal content (Levantine, Gulf, and Egyptian).Furthermore, we describe a long-term crowdsourcedeffort to have the sentences labeled by Arabic speak-ers for the level of dialect in each sentence and thedialect itself.
Finally, we present experimental re-sults on the task of automatic dialect classificationwith systems trained on the collected dialect labels.1The high transliteration rate is somewhat alarming, as thefirst two words of the sentence are relatively frequent: AymtYmeans ?when?
and rH corresponds to the modal ?will?.2It can in fact be argued that MSA is the variant with themore complex sentence structure and richer morphology.37MaghrebiEgyGulfLevIraqiOtherFigure 2: One possible breakdown of spoken Arabic intodialect groups: Maghrebi, Egyptian, Levantine, Gulf, andIraqi.
Habash (2010) also gives a very similar breakdown.2 The AOC DatasetArabic is the official language in over 20 countries,spoken by more than 250 million people.
The of-ficial status only refers to a written form of Arabicknown as Modern Standard Arabic (MSA).
The spo-ken dialects of Arabic (Figure 2) differ quite a bitfrom MSA and from each other.
The dominance ofMSA in available Arabic text makes dialectal Arabicdatasets hard to come by.3We set out to create a dataset of dialectal Ara-bic to address this need.
The most viable re-source of dialectal Arabic text is online data, whichis more individual-driven and less institutionalized,and therefore more likely to contain dialectal con-tent.
Possible sources of dialectal text include we-blogs, forums, and chat transcripts.
However, we-blogs usually contain relatively little data, and awriter might use dialect in their writing only occa-sionaly, forums usually have content that is of littleinterest or relevance to actual applications, and chattranscripts are difficult to obtain and extract.We instead diverted our attention to online com-mentary by readers of online content.
This sourceof data has several advantages:?
A large amount of data, with more data becom-ing available on a daily basis.?
The data is publicly accessible, exists in a struc-tured, consistent format, and is easy to extract.?
A high level of topic relevance.3The problem is somewhat mitigated in the speech domain,since dialectal data exists in the form of phone conversationsand television program recordings.Al-YoumNews Source Al-Ghad Al-Riyadh Al-Sabe?# articles 6.30K 34.2K 45.7K# comments 26.6K 805K 565K# sentences 63.3K 1,686K 1,384K# words 1.24M 18.8M 32.1Mcomments/article 4.23 23.56 12.37sentences/comment 2.38 2.09 2.45words/sentence 19.51 11.14 23.22Table 1: A summary of the different components of theAOC dataset.
Overall, 1.4M comments were harvestedfrom 86.1K articles, corresponding to 52.1M words.?
The prevalence of dialectal Arabic.The Arabic Online Commentary dataset that wecreated was based on reader commentary from theonline versions of three Arabic newspapers: Al-Ghad from Jordan, Al-Riyadh from Saudi Arabia,and Al-Youm Al-Sabe?
from Egypt.4 The commondialects in those countries are Levantine, Gulf, andEgyptian, respectively.We crawled webpages corresponding to articlespublished during a roughly-6-month period, cover-ing early April 2010 to early October 2010.
Thisresulted in crawling about 150K URL?s, 86.1K ofwhich included reader commentary (Table 1).
Thedata consists of 1.4M comments, corresponding to52.1M words.We also extract the following information for eachcomment, whenever available:?
The URL of the relevant newspaper article.?
The date and time of the comment.?
The author ID associated with the comment.5?
The subtitle header.5?
The author?s e-mail address.5?
The author?s geographical location.5The AOC dataset (and the dialect labels of Sec-tion 3) is fully documented and publicly available.64URL?s: www.alghad.com, www.alriyadh.com, andwww.youm7.com .5These fields are provided by the author.6Data URL: http://cs.jhu.edu/?ozaidan/AOC/.The release also includes all sentences from articles in the 150Kcrawled webpages.383 Augmenting the AOC with DialectLabelsWe have started an ongoing effort to have each sen-tence in the AOC dataset labeled with dialect labels.For each sentence, we would like to know whetheror not it has dialectal content, how much dialectthere is, and which variant of Arabic it is.
Havingthose labels would greatly aid researchers interestedin dialect by helping them focus on the sentencesidentified as having dialectal content.3.1 Amazon?s Mechanical TurkThe dialect labeling task requires knowledge of Ara-bic at a native level.
To gain access to native Arabicspeakers, and a large number of them, we crowd-sourced the annotation task to Amazon?s Mechani-cal Turk (MTurk), an online marketplace that allows?Requesters?
to create simple tasks requiring humanknowledge, and have them completed by ?Workers?from all over the world.3.2 The Annotation TaskOf the 3.1M available sentences, we selected a?small?
subset of 142,530 sentences to be labeled byMTurk Workers.7 We kept the annotation instruc-tions relatively simple, augmenting them with themap from Figure 2 (with the Arabic names of thedialects) to illustrate the different dialect classes.The sentences were randomly grouped into14,253 sets of 10 sentences each.
When a Workerchooses to perform our task, they are shown the 10sentences of some random set, on a single HTMLpage.
For each sentence, they indicate the level ofdialectal Arabic, and which dialect it is (if any).
Weoffer a reward of $0.05 per screen, and request eachone be completed by three distinct Workers.3.3 Quality ControlTo ensure high annotation quality, we insert two ad-ditional control sentences into each screen, takenfrom the article bodies.
Such sentences are almostalways in MSA Arabic.
Hence, a careless Workercan be easily identified if they label many controlsentences as having dialect in them.7There are far fewer sentences available from Al-Ghad thanthe other two sources (fourth line of Table 1).
We have takenthis imbalance into accout and heavily oversampled Al-Ghadsentences when choosing sentences to be labeled.News Source #MSAsentences#words#dialectalsentences#wordsAl-Ghad 18,947 409K 11,350 240KAl-Riyadh 31,096 378K 20,741 288KAl-Youm Al-Sabe?
13,512 334K 12,527 327KALL 63,555 1,121K 44,618 855KTable 2: A breakdown of sentences for which ?
2 anno-tators agreed on whether dialectal content exists or not.Another effective method to judge a Worker?squality of work is to examine their label distributionwithin each news source.
For instance, within thesentences from Al-Youm Al-Sabe?, most sentencesjudged as having dialectal content should be clas-sified as Egyptian.
A similar strong prior exists forLevantine within Al-Ghad sentences, and for Gulfwithin Al-Riyadh sentences.Using those two criteria, there is a very cleardistinction between Workers who are faithful andthose who are not (mostly spammers), and 13.8%of assignments are rejected on these grounds and re-posted to MTurk.3.4 Dataset StatisticsWe have been collecting labels from MTurk for a pe-riod of about four and a half months.
In that period,11,031 HITs were performed to completion (cor-responding to 110,310 sentences, each labeled bythree distinct annotators).
Overall, 455 annotatorstook part, 63 of whom judged at least 50 screens.Our most prolific annotator completed over 6,000screens, with the top 25 annotators supplying about80% of the labels, and the top 50 annotators supply-ing about 90% of the labels.We consider a sentence to be dialectal if it is la-beled as such by at least two annotators.
Similarly,a sentence is considered to be MSA if it has at leasttwo MSA labels.
For a small set of sentences (2%),no such agreement existed, and those sentences werediscarded (they are mostly sentences identified asbeing non-Arabic).
Table 2 shows a breakdown ofthe rest of the sentences.88Data URL: http://cs.jhu.edu/?ozaidan/RCLMT/.39Classification Task Accuracy(%)Precision(%)Recall(%)Al-Ghad MSA vs. LEV 79.6 70.6 78.2Al-Riyadh MSA vs. GLF 75.1 66.9 74.6Al-Youm Al-Sabe?
MSA vs. EGY 80.9 77.7 84.4MSA vs. dialect 77.8 71.2 77.6LEV vs. GLF vs. EGY 83.5 N/A N/AMSA vs. LEV vs. GLF vs. EGY 69.4 N/A N/ATable 3: Accuracy, dialect precision, and dialect recall(10-fold cross validation) for various classification tasks.4 Automatic Dialect ClassificationOne can think of dialect classification as a lan-guage identification task, and techniques for lan-guage identification can be applied to dialect clas-sification.
We use the collected labels to investigatehow well a machine learner can distinguish dialectalArabic from MSA, and how well it can distinguishbetween the different Arabic dialects.We experiment with a language modeling ap-proach.
In a classification task with c classes, webuild c language models, one per class.
At testtime, we score a test sentence with all c models,and choose the class label of the model assigningthe highest score (i.e.
lowest perplexity).
We use theSRILM toolkit to build word trigram models, withmodified Kneser-Ney as a smoothing method, andreport the results of 10-fold cross validation.Table 3 illustrates the performance of this methodunder various two-, three-, and four-way scenarios.We find that it is quite good at distinguishing eachdialect from the corresponding MSA content, anddistinguishing the dialects from each other.We should note that, in practice, accuracy is prob-ably not as important of a measure as (dialect) pre-cision, since we are mainly interested in identifyingdialectal data, and much less so MSA data.
To thatend, one can significantly increase the precision rate(at the expense of recall, naturally) by biasing clas-sification towards MSA, and choosing the dialectallabel only if the ratio of the two LM scores exceedsa certain threshold.
Figure 3 illustrates this tradeofffor the classification task over Al-Ghad sentences.prec=70.6%rec=78.2%20%40%60%80%100%20% 40% 60% 80% 100%RecallPrecisionFigure 3: Dialect precision vs. recall for the classificationtask over Al-Ghad sentences (MSA vs. Levantine).
Thesquare point corresponds to the first line in Table 3.5 Related WorkThe COLABA project (Diab et al, 2010) is an-other large effort to create dialectal Arabic resources(and tools).
They too focus on online sources suchas blogs and forums, and use information retrievaltasks for measuring their ability to properly processdialectal Arabic content.The work of Irvine and Klementiev (2010) is sim-ilar to ours in spirit, as they too use MTurk to find an-notators with relatively uncommon linguistic skills,to create translation lexicons between English and42 rare languages.
In the same vein, Zaidan andCallison-Burch (2011) solicit English translations ofUrdu sentences from non-professional translators,and show that translation quality can rival that ofprofessionals, for a fraction of the cost.Lei and Hansen (2011) build Gaussian mixturemodels to identify the same three dialects we con-sider, and are able to achieve an accuracy rate of71.7% using about 10 hours of speech data for train-ing.
Biadsy et al (2009) utilize a much larger dataset(170 hours of speech data) and take a phone recog-nition and language modeling approach (Zissman,1996).
In a four-way classification task (with Iraqias a fourth dialect), they achieve a 78.5% accuracyrate.
It must be noted that both works use speechdata, and that dialect identification is done on thespeaker level, not the sentence level as we do.406 Current and Future WorkWe have already utilized the dialect labels to identifydialectal sentences to be translated into English, inan effort to create a Dialectal Arabic-to-English par-allel dataset (also taking a crowdsourcing approach)to aid machine translation of dialectal Arabic.Given the recent political unrest in the MiddleEast (early 2011), another rich source of dialectalArabic are Twitter posts (e.g.
with the #Egypttag) and discussions on various political Facebookgroups.
Here again, given the topic at hand andthe individualistic nature of the posts, they are verylikely to contain a high degree of dialectal data.AcknowledgmentsThis research was supported by the HumanLanguage Technology Center of Excellence,by the DARPA GALE program under ContractNo.
HR0011-06-2-0001, and by Raetheon BBNTechnologies.
The views and findings are theauthors?
alone.ReferencesFadi Biadsy, Julia Hirschberg, and Nizar Habash.
2009.Spoken arabic dialect identification using phonotac-tic modeling.
In Proceedings of the EACL Workshopon Computational Approaches to Semitic Languages,pages 53?61.Mona Diab, Nizar Habash, Owen Rambow, MohamedAltantawy, and Yassine Benajiba.
2010.
COLABA:Arabic dialect annotation and processing.
In LRECWorkshop on Semitic Language Processing, pages 66?74.Nizar Y. Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool.Ann Irvine and Alexandre Klementiev.
2010.
Using Me-chanical Turk to annotate lexicons for less commonlyused languages.
In Proceedings of the NAACL HLTWorkshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk, pages 108?113.Yun Lei and John H. L. Hansen.
2011.
Dialect clas-sification via text-independent training and testing forarabic, spanish, and chinese.
IEEE Transactions onAudio, Speech, and Language Processing, 19(1):85?96.Omar F. Zaidan and Chris Callison-Burch.
2011.
Crowd-sourcing translation: Professional quality from non-professionals.
In Proceedings of ACL (this volume).Marc A. Zissman.
1996.
Comparison of four ap-proaches to automatic language identification of tele-phone speech.
IEEE Transactions on Speech and Au-dio Processing, 4(1):31?44.41
