Coling 2010: Poster Volume, pages 1292?1300,Beijing, August 2010Phrase Structure Parsing with Dependency StructureZhiguo Wang and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences{zgwang, cqzong}@nlpr.ia.ac.cnAbstractIn this paper we present a novel phrasestructure parsing approach with the helpof dependency structure.
Different withexisting phrase parsers, in our approachthe inference procedure is guided bydependency structure, which makes theparsing procedure flexibly.
Theexperimental results show our approach ismuch more accurate.
With the help ofgolden dependency trees, F1 score of ourparser achieves 96.08% on Penn EnglishTreebank and 90.61% on Penn ChineseTreebank.
With the help of N-bestdependency trees generated by modifiedMSTParser, F1 score achieves 90.54%for English and 83.93% for Chinese.1 IntroductionOver the past few years, several high-precisionphrase parsers have been presented, and most ofthem are employing probabilistic context-freegrammar (PCFG).
As we all know, the basicPCFG has the problems that the independenceassumption is too strong and lacks of lexicalconditioning (Jurafsky and Martin, 2007).Although researchers have proposed variousmodels and inference algorithms aiming to solvethese problems, the performance of existingphrase parsers is still remained to furtherimprove.
Most of the existing approaches can beclassified into two categories: unlexicalizedPCFG based (Johnson, 1998; Klein andManning, 2003; Levy and Manning, 2003;Matsuzaki et al, 2005; Petrov et al, 2006) andlexicalized PCFG based (Collins, 1999a;Charniak, 1997; Bikel, 2004; Charniak andJohnson, 2005).Unlexicalized PCFG based approach attemptsto weaken the independence assumption byannotating non-terminal symbols with labels ofancestor, siblings and even the latent annotationsencoded by local information.
In lexicalizedPCFG based approach, researchers believe thatthe forms of a constituent and its sub-constituents are determined more by theconstituent?s head than any other of its lexicalitems (Charniak, 1997), so they annotate non-terminal symbols with the head wordsinformation.Both of the two PCFG based approaches haveimproved the basic PCFG based parserssignificantly.
However, neither of them has beenguided by enough linguistic priori knowledge.Their parsing procedures are too mechanical.Because of this, the efficiency is always worse,and much more artificial ambiguities, which aredifferent from linguistic ambiguities (Krotov etal., 1998; Johnson, 1998), are generated.
Webelieve parsing procedure guided by morelinguistic priori knowledge will help toovercome the drawbacks in some extent.
Fromour intuition, dependency structure, another typeof syntactic structure with much linguisticknowledge, will be a good candidate to guidephrase parsing procedure.In this paper we present a novel approach tousing dependency structure to guide phraseparsing.
This novel approach has its virtues frommultiple angles.
First, dependency structureoffers a good compromise between theconflicting demands of analysis depth, whichmakes it much easier to get through handannotating than phrase structure (Nivre, 2004).So, when we want to build a phrase structurecorpus, we can build a dependency structurecorpus first, and get the corresponding phrasestructure automatically with the help ofdependency structure.
Second, many parsingalgorithms with linear-time complexity used independency parsers can still achieve the state-of-the-art results (Johansson, 2007), but almostall phrase parsers with high-precision have noefficient algorithms superior to cubic-timecomplexity.
So, in order to get an efficient1292parser, we can first get a dependency structurethrough linear-time algorithm, and then obtainthe phrase structure with the help of dependencystructure more efficiently.
Third, the lexicalizedPCFG based parsers which just bring the headwords into account have got a highly improvedperformance.
It gives us reasons to believedependency structure which takes therelationship of all the words will bring phraseparser a great help.Remainder of this paper is organized asfollows: Section 2 introduces the related work.Section 3 gives a consistency betweendependency structure and phrase structure, andpresents an approach to parsing phrase structurewith dependency structure.
In Section 4, wediscuss the experiments and analysis.
Finally,we conclude this paper and point out somefuture work in Section 5.2 Related workUnlexicalized PCFG based parsers (Johnson,1998; Klein and Manning, 2003; Levy andManning, 2003; Matsuzaki et al, 2005; Petrovet al, 2006) are the most successful parsingtools.
They regard parsing as a pure machinelearning question.
However, they haven?t takenany extra linguistic priori knowledge directlyinto account.
Lexicalized PCFG based parsers(Collins, 1999a; Charniak, 1997; Bikel, 2004;Charniak and Johnson, 2005) just bring a littlelinguistic priori knowledge (head wordinformation) into learning phase.
In inferencephase, both of the unlexicalized PCFG basedapproach and lexicalized PCFG based approachare using the pure searching algorithms, whichtry to parse a sentence monotonously, eitherfrom left to right or from right to left.
Fromthese states, we can find that manners of currentparsers are too mechanical.
Because of this, theefficiency of phrase parsers is always worse, andmuch more artificial ambiguities are generated.There have been some work (Collins et al,1999b; Xia and Palmer, 2001) about convertingdependency structures to phrase structures.Collins et al (1999b) proposed an algorithm toconvert the Czech dependency Treebank into aphrase structure Treebank and do dependencyparsing through Collins (1999a)?s model.Results showed the accuracy of dependencyparsing for Czech was improved largely.
Xiaand Palmer (2001) proposed a more generalizedalgorithm according to X-bar theory and Collinset al (1999b), and they did some experiments onPenn Treebank.
The results showed theiralgorithm produced phrase structures that werevery close to the ones in Penn Treebank.However, we have to point out that they onlycomputed the unlabeled performance but lost allthe exact syntactic symbols.
Different from tree-transformed PCFG based approach andlexicalized PCFG based approach, both ofCollins et al (1999b) and Xia and Palmer (2001)attempted to build some heuristic rules throughlinguistic theory, but didn?t try to learn anythingfrom Treebank.Li and Zong (2005) presented a hierarchicalparsing algorithm for long complex Chinesesentences with the help of punctuations.
Theyfirst divided a long sentence into short onesaccording to punctuation marks, then parsed theshort ones into sub-trees individually, and at lastcombined all the sub-trees into a whole tree.Experimental results showed the parsing timewas reduced largely, and performance wasimproved too.
Although the procedure of theirparser is more close to human beings?
manner, itappears a little shallow just using thepunctuation marks.In this paper our motivations are to bringmore linguistic priori knowledge into phraseparsing procedure with the help of dependencystructure, and make the parsing procedureflexibly.Matsuzaki et al (2005) defined a generativemodel called PCFG with latent annotations(PCFG-LA).
Using EM-algorithm each non-terminal symbols was annotated with a latentvariable, and a fine-grained model can be got.In order to get a more compact PCFG-LA model,Petrov et al (2006) presented a split-and-mergemethod which can get PCFG-LA modelhierarchically, and their final resultoutperformed state-of-the-art phrase parsers.
Tomake the parsing process of hierarchical PCFG-LA model more efficient, Petrov and Klein(2007) presented a coarse-to-fine inferencealgorithm.
In Section 4 of this paper, we try tocombine the hierarchical PCFG-LA model inlearning phase and coarse-to-fine method ininference phase into our parser in order to get anaccurate and efficient parser.12933 Our frameworkIn this section, we first compare phrase structurewith dependency structure of the same sentence,and get a consistent relationship among them.Then, based on this relationship, we present aninference framework to make the parsingprocedure flexible and more efficient.3.1 Analysis on consistency between phrasestructure and dependency structurePhrase structure and dependency structure aretwo different ways to represent syntacticstructures of sentences.
Phrase structurerepresents sentences by nesting of multi-wordconstituents, while dependency structurerepresents sentences as trees, whose nodes arewords and edges represent the relations amongwords.As we know, there are two kinds ofdependency structures, projective structure andnon-projective structure.
For free-word orderlanguages, non-projectivity is a commonphenomenon, e.g.
Czech.
For languages likeEnglish and Chinese, the dependency structuresare often projective trees.
In this paper, we onlyconsider English parsing based on PennTreebank (PTB) and Chinese parsing based onPenn Chinese Treebank (PCTB), so we justresearch the consistency between phrasestructure and projective dependency structurethrough PTB/PCTB.Information carried by the two structures isn?tequal.
Phrase structure is more flexible, carriesmore information, and even contains all theinformation of dependency structure.
So the taskto convert a phrase structure to dependencystructure is more straight, e.g.
Nivre and Scholz(2004), Johansson and Nugues (2007).
However,the reverse procedure is much more difficult,because dependency structure lacks the syntacticsymbols, which are indispensable in phrasestructure.joinVinken will board as 29the director Nova nonexecutive(a) Dependency structure(1)(2)(3)SNP VPNNPVinkenMD VPwill VB NP PP NPjoinDT NNthe boardINasNP NNP CDNov 29DT JJ NNanonexecutivedirector(b) Phrase structure(1)(2)(3)Figure 1.
The consistency between phrasestructure and dependency structureAlthough the two structures are completelydifferent, they have consistency in some deeplevel.
In this paper we analyze the consistencyfrom a practical perspective in order to dophrase parsing with the help of dependencystructure.
Having investigated the two kinds oftrees with dependency structure and phrasestructure, we find a consistency1 that each sub-tree in dependency structure must correspond toa sub-tree in phrase structure who dominates allthe words appearing in dependency sub-tree.Figure 1 shows this relationship more intuitively.The dependency sub-tree surrounded by circle(1) in Figure 1(a) is a one-layer sub-tree, and hasa corresponding phrase sub-tree surrounded bycircle (1) in Figure 1(b).
Both of the two sub-trees dominate the same word ?Vinken?.
Thisconsistency is also satisfied in other cases, e.g.two-layer sub-tree surrounded by circle (3) andthree-layer sub-tree surrounded by circle (2) inFigure 1(a).
These dependency sub-treesrespectively have their corresponding phrasesub-trees dominating the same words in Figure1(b).This consistency brings us inspiration to makeuse of dependency structure for phrase parsing.In other words, in our method when a phrasesub-tree is generated from a dependency sub-tree, it must dominate all the same words withones in the corresponding dependency sub-tree.3.2 Inference framework1 Be aware that the consistency is irreversible and not everyphrase sub-tree has its corresponding dependency sub-tree.1294As we mentioned in Section 2, most of currentinference algorithms are monotonous, whichgenerate much more artificial ambiguities.
Forexample, in Figure 1, if a sub-tree onlydominating ?board?
and ?as?
is built (actually itis not occurred in golden tree) an artificialambiguity is generated, and it thus will furtherbring a worse effect to other parts.
The finalprecision will certainly descend.
However, if weare given a corresponding dependency structure,those errors will be avoided.
The consistencyanalyzed above tells us that there isn?t a sub-treedominating only ?board?
and ?as?
independency tree, so the two words can?t build asub-tree independently in phrase parsing.According to this strategy, we design aninference framework for phrase parsing.NPNNPVinkenMDwillVBjoinDT NNthe boardINasNNP CDNov 29DT JJ NNa nonexecutive directorNPNNPVinkenMDwillVBNPNPjoinDT NNthe boardINasNPNNP CDNov 29DT JJ NNanonexecutivedirectorjoinVinken will board as 29the director Nova nonexecutiveNPNNPVinkenMDwillVBNPPPNPjoinDT NNthe boardINasNPNNP CDNov 29DT JJ NNanonexecutivedirectorjoinVinken will board as 29the director Nova nonexecutiveSNP VPNNPVinkenMD VPwill VB NP PP NPjoinDT NNthe boardINasNP NNP CDNov 29DT JJ NNanonexecutivedirectorjoinVinken will board as 29the director Nova nonexecutive(a) fill cell[i,i] for each word(b) fill spans guided by two-layer dependency sub-trees(0) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10)d[3,4]d[6,8]d[9,10]d[5,8]d[0,10]cell[3,4]cell[6,8]cell[9,10]cell[5,8]cell[0,10](c) fill spans guided by three-layer dependency sub-trees(d) fill spans guided by four-layer dependency sub-treesFigure 2.
Parsing procedure of ourinference framework guided bydependency structureOur inference framework parses a sentenceflexibly with a traditional inference algorithm.The following terms will help to explain ourwork.
A key data structure is cell[i,j], which isused to store phrase sub-trees spanning wordsfrom positions i to j of the input sentence.
d[i,j]is a dependency sub-tree spanning words frompositions i to j. cells[i,j] is an array to store allthe cells which can be combined to buildcell[i,j].
The pseudo-code of our inferenceframework is shown in Algorithm1.
The lineindicated by (1) and (2) gives us freedom toselect any kinds of inference algorithms andmatching parsing models.Algorithm 1InferenceFramework(sentence S, dependency tree D) initialize a List for the input sentencefor each word wi in S dofill cell[i, i] and add it to a list L parse the cells[] hierarchicallyfor each d[s, t] of D in topological order dofill cells[s, t] with spans in Lfill cell[s, t] with cells[s, t] throughtraditional inference algorithm   (1)add cell[s, t] to L extract the best treeestimate all trees in cell[0, n]through parsing model             (2)return the best phrase treeNow, let?s illustrate the flexible parsingprocedure step by step through an example.Please see Figure 2.
For simplicity, we justdraw sub-trees of the final best tree, and ignoreall the others.
Figure 2(a) shows the procedureof filling cell[i,i] for each word.
In Figure 2(b),there are three two-layer dependency sub-treesd[3,4], d[6,8] and d[9,10].
So we try to generatephrase sub-trees for cell[3,4], cell[6,8] andcell[9,10], which have been annotated with boldedges.
For example, we use sub-trees containedin cell[6,6], cell[7,7] and cell[8,8] to1295build new sub-trees for cell[6,8].
Figure 2(c)and Figure 2(d) show the same procedure forparsing with the help of three-layer dependencysub-trees and four-layer dependency sub-treesindividually.
The generated phrase sub-trees areall annotated with bold edges in the figure.Obviously, the biggest dependency sub-tree isthe whole dependency tree of sentence.
In thisexample, when the four-layer dependency sub-tree is processed, the whole phrase trees are built.Usually, more than one phrase trees with thesimilar skeletons are generated.
So we use amodel to evaluate candidate results, and get outthe one with the highest score as the final result.Benefiting from the dependency structure, wecan parse a sentence flexibly.
Comparing withprevious work on converting dependencystructure to phrase structure (Collins et al,1999b; Xia and Palmer, 2001), we make use ofTreebank knowledge more sufficient with thehelp of traditional parsing technology.
Thesearch space has been pruned tremendously.
Aswe know, the traditional parsing approach oftentries to search all the n*(n+1)/2  cells for inputsentence which has n words, but our parsingframework search cells intelligently with thehelp of corresponding dependency structure.Let?s get a view of this through the sentenceshown in Figure 2.
From the whole parsingprocedure shown in Figure 2, our frameworkjust tries to fill 16 cells, which are cell[i,i] foreach word, cell[3,4], cell[6,8], cell[9,10],cell[5,8] and cell[0,10] hierarchically, buttraditional parsing approach would try to fill all66 cells.
So 75.76% searching space has beenpruned.4 Experiments and resultsIn order to evaluate the effectiveness of ourapproach, we have done some experiments bothfor English parsing and Chinese parsing.4.1 PreparationTo make comparison with previous work fairly,our experiments are based on general Treebankaccording to standard settings.
We choose PennEnglish Treebank for English parsingexperiments and Penn Chinese Treebank forChinese.
Table 1 shows the standard settings wetake.?
????
?????
????
?
???????
?PU NP NP NP PUVP PUVPIP??????????????
?
???????
?PUIPPU PUIPIP(b) Golden phrase tree(c) Parsing result of PCFG model and CYK algorithm?
????
?????
????
?
?????
?PU NP NP NP PUVP PUVPIP(d) Parsing result after pruning strategy added??NP?
????
?????
????
?
???????
?PU NP NP NP PUVP PUVPIP(e) Parsing result of PCFG-LA model(a) Golden dependency tree??/hold?
?/ceremony ?
?/today ?/in ???/Shanghai?
?/collaborate ?
?/project ??/signing?
?
?/America?/China ?/high?
?/technologyFigure 4.
An example showingexperimental resultsEnglish ChineseTrain Set Sections 2-21 Art.
1-270, 400-1151Dev Set Section 22 Articles 301-325Test Set Section 23 Articles 271-300Table 1.
Experimental settingsBecause the two Treebanks are in type ofphrase structure, we should get dependencystructures corresponding with them.
There aretwo ways to accomplish this work.
First, useconverting tools to get dependency trees directlythrough converting the original Treebanks, andthe generated trees are always considered asgolden trees during dependency parsing.
Second,use a dependency parser with state-of-the-art1296performance to parse all the sentencesautomatically.
In this paper, we design twogroups of experiments, as following:(1) Phrase parsing with the help of goldendependency trees.
(2) Phrase parsing with the help of N-bestdependency trees generated automatically.4.2 Phrase parsing with golden dependencytreesIn order to verify how much dependencystructure can help phrase parsing and get anupper bound of our approach, we do phraseparsing with the help of golden dependencytrees in this subsection.Based on the parsing framework shown inFigure 3, we only use the basic PCFG inlearning phase and our inference frameworkwith basic CYK algorithm in inference phase.The parsing results are shown with the mark (1)in Table 2 for English and Table 3 for Chineserespectively.Having investigated the generated trees withgolden trees, we find the consistency ofdependency structure and phrase structure isbroken by some trees.
Let?s get a view of thisthrough an example from Penn ChineseTreebank.
In Figure 4(a), the dependency sub-tree surrounded by circle tells us that there mustbe a phrase sub-tree which dominate the wordsequence of ???????????
?
 ?
?
 ?
(the signing ceremony ofcollaborating in high technology betweenAmerica and China), and the golden phrase treeshown in Figure 4(b) has a corresponding sub-tree surrounded by circle indeed.
However, theparsing tree generated by our approach shown inFigure 4(c) doesn?t conform.
There are threesub-trees dominating the word sequencemutually, but they don?t construct a whole one.In our opinion, the contradiction derived frombinarizing process of CYK 2 .
The binary treesgenerated by our algorithm have consisted withthe consistency originally, but after debinarizingprocess the consistency is broken.Trying to check our opinion, we add apruning strategy to the original inference2 The premise of using CYK is that all the rules must haveCNF form.
So we usually bring some medial nodes tobinarize rules gathered from Treebank.algorithm to prune all the medial nodes whichmay break the consistency during parsingprocedure.
With the help of pruning strategy, theperformances of English and Chinese are allimproved further.
Corresponding figures areshown in Table 2 and Table 3 with the mark (2).The parsing result of above example is shown inFigure 4(d) and the error appearing in Figure 4(c)is corrected naturally after the pruning strategyadded.Comparing with previous work which havedone much work in learning phase, ouralgorithm achieves such amazing results onlyusing basic PCFG model.
From this aspect, ourinference framework is much more effective.However, there are still some errors ourapproach can?t deal with.
For example, in Figure4(d) the sub-tree rooted at NP and dominatingword sequence of  ?????????
(holdin Shanghai today) is separated by two sub-trees.The reason is that the model (basic PCFG) weuse in learning phase is too coarse todisambiguate sufficiently.
So we don?t pin allhopes in inference phase.
We also modify themodel in learning phase.
PCFG-LA is one of themost successful models in phrase parsing, so wechoose PCFG-LA as the model in learning phase.After this modification, performance of ourapproach has been improved delightedly.
F1score is 96.08% for English and 90.06% forChinese.
The line marked with (3) in Table 2and Table 3 shows more details.4.3 Phrase parsing with N-best dependencytrees generated automaticallyThe experimental results shown in subsection4.2 bring us confidence that do phrase parsingwith the help of dependency structure is a highlyeffective approach.
However, we don?t usuallyhave golden dependency structures, and a moreacceptable way is using a dependency parser togenerate dependency trees automatically.
In thissubsection we explore feasibility andeffectiveness of phrase parsing with the help ofdependency trees generated automatically.
Aswe all know, even state-of-the-art dependencyparser cannot generate totally correct result.
So inorder to make our system more robust we use N-best dependency structures to guide phraseparsing procedure.1297length<=40 all sentences ?Precision Recall F1 Precision Recall F1(1) Using PCFG and CYK 90.28 88.41 89.34 90.11 88.32 89.21(2) Using pruning strategy 90.69 89.53 90.11 90.51 89.45 89.97(3) Using PCFG-LA 96.28 95.97 96.13 96.25 95.91 96.08Table 2.
Parsing performance (%) for English with the help of golden dependency tree.length<=40 all sentences ?Precision Recall F1 Precision Recall F1(1) Using PCFG and CYK 86.89 78.25 82.34 85.56 77.43 81.29(2) Using pruning strategy 87.65 82.33 84.91 86.39 81.45 83.85(3) Using PCFG-LA 91.51 91.26 91.38 90.43 90.79 90.61Table 3.
Parsing performance (%) for Chinese with the help of golden dependency tree.We choose MSTParser 3  which is the mostfamous dependency parser and modify it togenerate N-best dependency trees.
The oracleunlabeled accuracy of N-best dependency treesgenerated from 1-order model is shown inTable 4.
To show the effectiveness of ourapproach, we choose Berkeleyparser 4  as thebaseline parser, take the same configuration andcombine it into our general parsing frameworkshown in Figure 3.The experiment of parsing with goldendependency structure gets an amazingperformance.
It brings us a new way to buildPTB/PCTB style phrase structure corpus.Because dependency structure is much easier toget through hand annotating than phrasestructure, we can build a dependency structurecorpus first, and then get phrase structurecorpus through our approach guided by thedependency structure corpus.The experiment of parsing with N-bestdependency structures generated automaticallyuplifts the parsing performance to a new height.It brings us a more applied parsing tool forother NLP applications.Considering the number of dependencystructures (N-best) will affect the final result,we make use of the development set shown inTable1 to turning parameters.
We parse thedevelopment set many times with differentnumber of dependency structures.
The F1scores are shown in Figure 5 for English andFigure 6 5  for Chinese.
From Figure 5 andFigure 6, we can find when we use 10-bestdependency structures the performance is better.So we choose 10-best dependency trees for thetest set.From the experiments in Section 4.2, we canfind that even using the golden dependencystructure we can?t get totally correct phrasestructure.
The reason is that although everydependency sub-tree has its correspondingphrase sub-tree, not every phrase sub-tree hasits corresponding dependency sub-tree.
So theremainder errors can?t be solved only bydependency structure and a better way is tomodify the parsing model.The final performances of test set comparingwith previous work are shown in Table 5 andTable 6.
We can easily find that our approachhas outperformed all the parsers which aren?timproved through reranking stage or semi-supervised approach.
Although there is still amargin between our parser and reranked parseror semi-supervised parser, we believe that theparsing performance can be improved further ifwe bring the reranking or semi-supervisedapproaches into our parsing framework.5 Conclusion and Future workIn this paper, we present a novel phrase parsingapproach with the help of dependency structure.Based on the consistency between phrasestructure and dependency structure, we proposea novel inference framework.
Guided by theinference framework, inference algorithmsparse sentences hierarchically with the help ofdependency structures.
Experimental resultsshow that our approach can efficiently getbetter performance with both goldendependency structure and N-best dependency4.4 Discussion3 http://www.ryanmcd.com/MSTParser/MSTParser.html4 http://code.google.com/p/berkeleyparser/5 F1 score at n=0 is the result of Berkeley parser runningon my machine1298          1EHVW)OHQJWK  DOOVHQWHQFHVFigure 6.
F1 scores (%) of Dev Set for Chinesewith the help of N-best dependency trees          1EHVW)OHQJWK  DOOVHQWHQFHVFigure 5.
F1 scores (%) of Dev Set for English with thehelp of N-best dependency treesstructures generated automatically.However, there are still some problemsremaining to further study.
First, in ourapproach we just use the unlabeled dependencytrees.
The relationship labels carry some usefulinformation too, and we can make use of themto further improve phrase parsing.
Second,phrase structure can also help the process ofdependency parsing (McDonald et al, 2006), sowe can combine phrase parsing process anddependency parsing process together and makethem help each other.English Chineselen<=40 all len<=40 all5-best 90.62 90.49 87.92 84.9310-best 91.6 91.48 89.05 85.920-best 92.36 92.21 89.86 86.7930-best 92.74 92.6 90.3 87.2840-best 92.96 92.83 90.62 87.6350-best 93.08 92.95 90.79 87.87Table 4.
Oracle unlabeled accuracy (%) of N-bestdependency structures generated from MSTParserAcknowledgmentsThe research work has been partially funded bythe Natural Science Foundation of China underGrant No.
60975053, 90820303 and 60736014,the National Key Technology R&D Programunder Grant No.
2006BAH03B02, the Hi-TechResearch and Development Program (?863?Program) of China under Grant No.2006AA010108-4, and also supported by theChina-Singapore Institute of Digital Media(CSIDM) project under grant No.
CSIDM-200804.ReferencesAlexander Krotov, Mark Hepple, Robert Gaizauskas,and Yorick Wilks.
1998.
Compacting the PennTreebank grammar.
In ACL-COLING ?98, pages699-703.?
 <=40 AllCollins(1999) 88.6 88.2Charniak and Johnson(2005) 90.1 89.55Petrov and Klein(2007) 90.6 90.05This Paper 91.13 90.54RerankedCharniak and Johnson(2005) 92.0 91.4Huang(2008) ??
91.7Semi-supervisedMcClosky et al (2006) ??
92.1Table 5.
F1 (%) of Test Set for English ofprevious work and our approach?
<=40 AllChiang et al(2002) 79.93 76.57Bikel Thesis(2004) 81.2 79.0Petrov and Klein(2007) 86.3 83.32This Paper 86.76 83.93Semi-supervisedHuang and Harper(2009) ??
85.18Table 6.
F1 (%) of Test Set for Chinese ofprevious work and our approachDan Klein and Chris Manning.
2003.
AccurateUnlexicalized Parsing, In ACL ?03, pages 423-430.Daniel Jurafsky and James H. Martin.
2007.SPEECH and LANGUAGE PROCESSING--adraft, at Chapter 14.4.Daniel M. Bikel.
2004.
On the Parameter Space ofGenerative Lexicalized Statistical Parsing Models.Ph.D.
thesis, U. of Pennsylvania.Daniel M. Bikel.
2004.
Intricacies of Collins?Parsing Model.
In Computational Linguistics,30(4), pages 479-511.Daniel M. Bikel and David Chiang.
2000.
TwoStatistical Parsing Models Applied to the ChineseTreebank.
In the Proceedings of the SecondChinese Language Processing Workshop.1299David Chiang and Daniel M. Bikel.
2002.Recovering Latent Information in Treebanks.
InCOLING ?02.David McClosky, Eugene Charniak and MarkJohnson.
2006.
Effective self-training for parsing.In ACL-06.Deyi Xiong, Qun Liu and Shouxun Lin.
2005.Lexicalized Beam Thresholding Parsing withPrior and Boundary Estimates.
the 6thConference on Intelligent Text Processing andComputational Linguistics (CICLing), Pages 132?
141.D.H.
Younger.
1967.
Recognition and parsing ofcontext-free-languages in time n3.
Informationand Control, 10(2):189-208.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.Proceedings of the Fourteenth NationalConference on Artificial Intelligence AAAIPress/MIT Press, Menlo Park.Eugene Charniak.
2000.
A maximum-entropyinspired parser.
In NAACL ?00, pages132?139.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEntDiscriminative Reranking.
In ACL ?05.Fei Xia and Martha Palmer.
2001.
ConvertingDependency Structures to Phrase Structures.
The1st Human Language Technology Conference(HLT-2001).H.
Gaifman.
1965.
Dependency Systems and phrase-Structure Systems.
Information and Control,pages 304-337.H.
Yamada and Y. Matsumoto.
2003.
Statisticaldependency analysis with support vectormachines.
In Proceedings of IWPTJ.
Nivre, M. Scholz.
2004.
Deterministic dependencyparsing of English text.
In COLING ?04.Liang Huang.
2008.
Forest reranking:Discriminative parsing with non-local features.
InACL ?08.Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics,24(4):613?632.Michael Collins.
1999a.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, U. of Pennsylvania.Michael Collins, Jan Hajic, Lance Ramshaw andChristoph Tillmann.
1999b.
A Statistical Parserfor Czech.
In ACL ?99.Richard Johansson and Pierre Nugues.
2007.Extended Constituent-to-dependency Conversionfor English.
In Proceedings of NODALIDA.Roger Levy, Christopher Manning.
2003.
Is itharder to parse Chinese, or the Chinese Treebank?In ACL ?03.Ryan McDonald, Koby Grammer and FernandoPereira.
2006.
Online learning of approximatedependency parsing algorithms.
In EACL ?06.Slav Petrov and Dan Klein.
2007.
ImprovedInference for Unlexicalized Parsing.
In HLT-NAACL ?07.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2006.
Learning Accurate, Compact,and Interpretable Tree Annotation.
In COLING-ACL ?06.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.Probabilistic CFG with latent annotations.
InACL ?05, pages 75?82.T.
Kasami.
1965.
An efficient recognition and syntaxanalysis algorithm for context-free languages.Technical Report, AFCRL-65-758, Air ForceCambridge Reserch Lab., Bedford, MAXavier Carreras, Michael Collins, and Terry Koo.TAG, Dynamic Programming and the Perceptronfor Efficient, Feature-rich Parsing.
InCONLL ?08.Xing Li, Chengqing Zong.
2005.
A HierarchicalParsing Approach with Punctuation Processingfor Long Complex Chinese Sentences.
InIJCNLP ?05Yusuke Miyao, Rune S?tre, Kenji Sagae, TakuyaMatsuzaki and Jun'ichi Tsujii.
2008.
Task-oriented Evaluation of Syntactic Parsers andTheir Representations.
In ACL ?08, pages 46-54.Zhongqiang Huang, Mary Harper.
2009.
Self-Training PCFG Grammars with LatentAnnotations Across Languages.
In EMNLP ?09.1300
