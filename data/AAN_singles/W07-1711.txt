Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 82?87,Prague, June 2007. c?2007 Association for Computational LinguisticsMultilingual Word Sense Discrimination: A Comparative Cross-LinguisticStudyAlla RozovskayaDepartment of LinguisticsUniv.
of Illinois at Urbana-ChampaignUrbana, IL 61801rozovska@uiuc.eduRichard SproatDepartment of LinguisticsUniv.
of Illinois at Urbana-ChampaignUrbana, IL 61801rws@uiuc.eduAbstractWe describe a study that evaluates an ap-proach to Word Sense Discrimination onthree languages with different linguisticstructures, English, Hebrew, and Russian.The goal of the study is to determinewhether there are significant performancedifferences for the languages and to iden-tify language-specific problems.
The algo-rithm is tested on semantically ambiguouswords using data from Wikipedia, an onlineencyclopedia.
We evaluate the induced clus-ters against sense clusters created manually.The results suggest a correlation between thealgorithm?s performance and morphologicalcomplexity of the language.
In particular,we obtain FScores of 0.68 , 0.66 and 0.61 forEnglish, Hebrew, and Russian, respectively.Moreover, we perform an experiment onRussian, in which the context terms are lem-matized.
The lemma-based approach signif-icantly improves the results over the word-based approach, by increasing the FScore by16%.
This result demonstrates the impor-tance of morphological analysis for the taskfor morphologically rich languages like Rus-sian.1 IntroductionAmbiguity is pervasive in natural languages and cre-ates an additional challenge for Natural Languageapplications.
Determining the sense of an ambigu-ous word in a given context may benefit many NLPtasks, such as Machine Translation, Question An-swering, or Text-to-Speech synthesis.The Word Sense Discrimination (WSD) or WordSense Induction task consists of grouping togetherthe occurrences of a semantically ambiguous termaccording to its senses.
Word Sense Discriminationis similar to Word Sense Disambiguation, but allowsfor a more unsupervised approach to the problem,since it does not require a pre-defined set of senses.This is important, given the number of potentiallyambiguous words in a language.
Moreover, labelingan occurrence with its sense is not always necessary.For example, in Information Retrieval WSD wouldbe useful for the identification of documents relevantto a query containing an ambiguous term.Different approaches to WSD have been pro-posed, but the evaluation is often conducted usinga single language, so it is difficult to predict per-formance on another language.
To the best of ourknowledge, there has not been a systematic com-parative analysis of WSD systems on different lan-guages.
Yet, it is interesting to see whether thereare significant differences in performance when amethod is applied to several languages that have dif-ferent linguistic structures.
Identifying the reasonsfor performance differences might suggest what fea-tures are useful for the task.The present project adopts an approach to WSDthat is based on similarity measure between contextterms of an ambiguous word.
We compare the per-formance of an algorithm for WSD on English, He-brew, and Russian, using lexically ambiguous wordsand corpora of similar sizes.We believe that testing on the above languages82might give an idea about how accuracy of an algo-rithm for WSD is affected by language choice.
Rus-sian is a member of the Slavic language group and ismorphologically rich.
Verbs, nouns, and adjectivesare characterized by a developed inflectional system,which results in a large number of wordforms.
He-brew is a Semitic language, and is complex in a dif-ferent way.
In addition to the root-pattern morphol-ogy that affects the word stem, it also has a com-plex verb declination system.
Moreover, functionwords, such as prepositions and determiners, cliti-cize, thereby increasing the number of wordforms.Lastly, cliticization, coupled with the absence ofshort vowels in text, introduces an additional levelof ambiguity for Hebrew.There are two main findings to this study.
First,we show that the morphological complexity of thelanguage affects the performance of the algorithmfor WSD.
Second, the lemma-based approach toRussianWSD significantly improves the results overthe word-based approach.The rest of the paper is structured as follows:first, we describe previous work that is related to theproject.
Section 3 provides details about the algo-rithm for WSD that we use.
We then describe theexperiments and the evaluation methodology in Sec-tions 4 and 5, respectively.
We conclude with a dis-cussion of the results and directions for future work.2 Related WorkFirst, we describe several approaches to WSD thatare most relevant to the present project: Since weare dealing with languages that do not have manylinguistic resources available, we chose a most unsu-pervised, knowledge-poor approach to the task thatrelies on words occurring in the context of an am-biguous word.
Next, we consider two papers onWSD that provide evaluation for two languages.
Fi-nally, we describe work that is concerned with therole of morphology for the task.2.1 Approaches to Word Sense DiscriminationPantel and Lin (2002) learn word sense inductionfrom an untagged corpus by finding the set of themost similar words to the target and by clusteringthe words.
Each word cluster corresponds to a sense.Thus, senses are viewed as clusters of words.Another approach is based on clustering the oc-currences of an ambiguous word in a corpus intoclusters that correspond to distinct senses of theword.
Based on this approach, a sense is definedas a cluster of contexts of an ambiguous word.
Eachoccurrence of an ambiguous word is represented as avector of features, where features are based on termsoccurring in the context of the target word.
For ex-ample, Pedersen and Bruce (1997) cluster the oc-currences of an ambiguous word by constructing avector of terms occurring in the context of the tar-get.
Schu?tze (1992) presents a method that exploresthe similarity between the context terms occurringaround the target.
This is accomplished by consider-ing feature vectors of context terms of the ambigu-ous word.
The algorithm is evaluated on natural andartificially-constructed ambiguous English words.Sproat and van Santen (1998) introduce a tech-nique for automatic detection of ambiguous wordsin a corpus and measuring their degree of polysemy.This technique employs a similarity measure be-tween the context terms similar in spirit to the onein (Schu?tze, 1992) and singular value decomposi-tion in order to detect context terms that are impor-tant for disambiguating the target.
They show thatthe method is capable of identifying polysemous En-glish words.2.2 Cross-Linguistic Study of WSDLevinson (1999) presents an approach to WSD thatis evaluated on English and Hebrew.
He finds 50most similar words to the target and clusters theminto groups, the number of groups being the num-ber of senses.
He reports comparable results forthe two languages, but he uses both morphologi-cally and lexically ambiguous words.
Moreover, theevaluation methodology focuses on the success ofdisambiguation for an ambiguous word, and reportsthe number of ambiguous words that were disam-biguated successfully.Davidov and Rappoport (2006) describe an al-gorithm for unsupervised discovery of word cate-gories and evaluate it on Russian and English cor-pora.
However, the focus of their work is on the dis-covery of semantic categories and from the resultsthey report for the two languages it is difficult to in-fer how the languages compare against each other.We conduct a more thorough evaluation.
We also83control cross-linguistically for number of trainingexamples and level of ambiguity of selected words,as described in Section 4.2.3 Morphology and WSDMcRoy (1992) describes a study of different sourcesuseful for word sense disambiguation, includingmorphological information.
She reports that mor-phology is useful, but the focus is on derivationalmorphology of the English language.
In the presentcontext, we are interested in the effect of inflectionalmorphology onWSD, especially for languages, suchas Russian and Hebrew.Gaustad (2004) proposes a lemma-based ap-proach to a Maximum Entropy Word Sense Disam-biguation System for Dutch.
She shows that collaps-ing wordforms of an ambiguous word yields a morerobust classifier due to the availability of more train-ing data.
The results indicate an improvement of thisapproach over classification based on wordforms.3 ApproachOur algorithm relies on the method for selection ofrelevant contextual terms and on distance measurebetween them introduced in (Sproat and van Santen,1998) and on the approach described in (Schu?tze,1998), though the details of clustering differ slightly.The intuition behind the algorithm can be summa-rized as follows: (1) words that occur in the contextof the ambiguous word are useful for determiningits sense; and (2) contextual terms of an ambiguousword belong to topics corresponding to the sensesof the ambiguous word.
Before describing the algo-rithm in detail, we give an overview of the system.The algorithm starts by collecting all the occur-rences of an ambiguous word in the corpus togetherwith the surrounding context.
Next, we build a sym-metric distance matrix D, where rows and columnscorrespond to context terms, and D[i][j] is the dis-tance value of term i and term j.
The distance mea-sure is supposed to reflect how the two terms areclose semantically (whether they are related to thesame topic).
For example, we would expect the dis-tance between the words financial and money to besmaller than the distance between the words finan-cial and river: The first pair is more likely to occurin the same context, than the second one.
Using thedistance measure, the context terms are partitionedinto sense clusters.
Finally, we group the sentencescontaining the ambiguous word into sentence clus-ters using the context term clusters.We now describe each step in detail:1.
We collect contextual terms of an ambigu-ous word w in a context window of 50 wordsaround the target.
Each context term t is as-signed a weight (Sproat and J. van Santen,1998):wt =CO(t|w)FREQ(t)(1)CO(t|w) is the frequency of the term in thecontext of w, and FREQ(t) is the frequencyof the term in the corpus.
Term weights areused to select context terms that will be help-ful in determining the sense of the ambiguousword in a particular context.
Furthermore, termweights are employed in (4) in sentence clus-tering.2.
For each pair ti and tj of context terms, wecompute the distance between them (Sproatand J. van Santen,1998):Dw[i][j] = 1 ?
[COw(ti|tj)FREG(ti)+COw(tj |ti)FREQ(tj)]2(2)COw(ti|tj) is the frequency of ti in the con-text of tj , and FREQ(ti) is the frequency ofti in the training corpus.
We assume that thedistance between ti and tj is inversely propor-tional to the semantic similarity between ti andtj .3.
Using the distance matrix from (2), the con-text terms are clustered using an agglomerativeclustering technique:?
Start by assigning each context term to a separatecluster?
While stopping criterion is false: merge two clus-ters whose distance 1 is the smallest.21There are several ways to define the distance between clus-ters.
Having experimented with three - Single Link, CompleteLink and Group Average, it was found that Complete Link def-inition works best for the present task.
(Complete Link distancebetween clusters i and j is defined as the maximum distance be-tween a term from cluster i and a term from cluster j).2In the present study, the clusters are merged as long as the84The output of step (3) is a set of context termclusters for the target word.
Below are shownselect members for term clusters for the Englishword bass:Cluster 1: songwriter singer joined keyboardistCluster 2: waters fishing trout feet largemouth4.
Finally, the sentences containing the ambigu-ous word are grouped using the context termclusters from (3).
Specifically, given a sen-tence with the ambiguous word, we computethe score of the sentence with respect to eachcontext word cluster in (3) and assign the sen-tence to the cluster with the highest score.
Thescore of the sentence with respect to cluster cis the sum of weights of sentence context termsthat are in c.4 ExperimentsThe algorithm is evaluated on 9 ambiguous wordswith two-sense distinctions.
We select words that(i) have the same two-sense distinction in all threelanguages or (ii) are ambiguous in one of the lan-guages, but each of their senses corresponds to anunambiguous translation in the other two languages.In the latter case, the translations are merged to-gether to create an artificially ambiguous word.
Webelieve that this selection approach allows for a col-lection of a comparable set of ambiguous words forthe three languages.
An example of an ambiguousword is the English word table, that corresponds totwo gross sense distinctions (tabular array, and apiece of furniture).
This word has two translationsinto Russian and Hebrew, that correspond to the twosenses.
The selected words are presented in Table 1.The words display different types of ambigu-ity.
In particular, disambiguating the Hebrew wordgishah (access; approach) or the Russian word mir(peace; world) would be useful in Machine Transla-tion, while determining the sense of a word like lan-guage would benefit an Information Retrieval sys-tem.
It should also be noted that several words pos-sess additional senses, which were ignored becausethey rarely occurred in the corpus.
For example, theRussian word yazyk (language) also has the meaningof tongue (body part).number of clusters exceeds the number of senses of the ambigu-ous word in the test data.The corpus for each language consists of 15Mword tokens, and for the same ambiguous word thesame number of training examples is selected fromeach language.
For each ambiguous word, a set of100-150 examples together with 50 words of con-text is selected from the section of the corpus notused for training.
These examples are manually an-notated for senses and used as the test set for eachlanguage.5 Evaluation MethodologyThe evaluation is conducted by comparing the in-duced sentence clusters to clusters created manually.We use three evaluation measures : cluster purity,entropy, and FScore.
3For a cluster Cr of size qr, where the size is thenumber of examples in that cluster, the dominatingsense Si in that cluster is selected and cluster purityis computed as follows:P (Cr) =nirqr, (3)where nir is the number of examples in cluster Crwith sense Si.For an ambiguous word w, cluster purity P(w) isthe weighted average of purities of the clusters forthat word.
4.
Higher cluster purity score correspondsto a better clustering outcome.Entropy and FScore measures are described in de-tail in Zhao and Karypis (2005).
Entropy indicateshow distinct senses are distributed between the twoclusters.
The perfect distribution is the assignmentof all examples with sense 1 to one cluster and allexamples with sense 2 to the other cluster.
In suchcase, the entropy is 0.
In general, a lower value in-dicates a better cluster quality.
Entropy is computedfor each cluster.
Entropy for word w is the weightedaverage of the entropies of the clusters for that word.Finally, FScore considers both the coverage of thealgorithm and its ability to discriminate between thetwo senses.
FScore is computed as the harmonic3Examples whose scores with respect to all clusters are zero(examples that do not contain any terms found in the distancematrix) are not assigned to any cluster, and thus do not affectcluster purity and cluster entropy.
This is captured by the FS-core measure described below.4In the present study, the number of clusters and the numberof senses for a word is always 285Senses English Hebrew Russianaccess;approach access;approach gishah dostup;podxodactor;player actor;player saxqan akter;igrokevidence; quarrel argument vikuax;nimuq argumentbody part; chief head rosh golova;glavaworld;peace world; peace shalom;olam mirfurniture; tabular array table shulxan;tavlah stol;tablitzaallow;resolve allow;resolve hershah;patar razreshat?ambiance; air atmosphere avira;atmosfera atmosferahuman lang.;program.
lang.
language safah yazykTable 1: Ambiguous words for testing: The first column indicates the senses; unambiguous translations thatwere merged to create an ambiguous word are indicated by a semicolonmean of Precision and Recall, where recall and pre-cision for sense Si with respect to cluster Cr arecomputed by treating cluster Cr as the output of aretrieval system for sense Si .6 Results and DiscussionWe show results for two experiments.
Experi-ment 1 compares the algorithm?s performance cross-linguistically without morphological analysis ap-plied to any of the languages.
Experiment 2 com-pares the performance for Russian in two settings:with and without morphological processing per-formed on the context terms.Table 2 presents experimental results.
Baseline iscomputed by assigning the most common sense toall occurrences of the ambiguous word.
We observethat English achieves the highest performance bothin terms of cluster purity and FScore, while Russianperforms most poorly among the three languages.This behavior may be correlated with the averagefrequency of the context terms that are used to con-struct the distance matrix in the corpus (cf.
7 forEnglish and 4.2 for Russian).
In particular, the dif-ference in the frequencies can be attributed to themorphological complexity of Russian, as comparedto English and Hebrew.
Hebrew is more complexthan English morphologically, which would accountfor a drop in performance for the Hebrew words vs.the English words.
Furthermore, one would expecta higher degree of ambiguity for Hebrew due to theabsence of short vowels in text.It is worth noting that while both Hebrew andRussian possess features that might negatively af-fect the performance, Hebrew performs better thanRussian.
We hypothesize that cliticization and thelack of vowels in text are not as significant factorsfor the performance as the high inflectional natureof a language, such as Russian.
We observe that themajotity of the context terms selected by the algo-rithm for disambiguation belong to the noun cate-gory.
This seems intuitive, since nouns generallyprovide more information content than other partsof speech and thus should be useful for resolvinglexical ambiguity.
While an English or a Hebrewnoun only has several wordforms, a Russian nounmay have up to 12 different forms due to various in-flections.The morphological complexity of Russian affectsthe performance in two ways.
First, cluster purityis affected, since the counts of context terms are notsufficiently reliable to accurately estimate term dis-tances.
Incorrect term distances subsequently affectthe quality of the term clusters.
Second, the percent-age of default occurrences (examples that have nocontext terms occurring in the distance matrix) is theleast for English (0.22) and the highest for Russian(0.27).
The default occurrences affect the recall.The results of experiment 2 support the fact thatmorphological complexity of a language negativelyaffects the performance.
In that experiment, the in-flections are removed from all the context terms.
Weapply a morphological analyzer 5 to the corpus andreplace each word with its lemma.
In 10% of theword tokens, the analyzer gives more than one pos-sible analysis, in which case the first analysis is se-lected.
As can be seen in Table 2 (last row), remov-ing inflections produces a significant improvementboth in recall and precision, while preserving thecluster purity and slightly reducing cluster entropy.Moreover, the performance in terms of recall, pre-cision, and coverage is better than for English and5Available at http://www.aot.ru/86Language Baseline Coverage Precision Recall FScore Purity EntropyEnglish 0.73 0.78 0.77 0.61 0.68 0.79 0.61Hebrew 0.72 0.79 0.76 0.58 0.66 0.82 0.59Russian 0.72 0.73 0.70 0.54 0.61 0.81 0.62Russian(lemma) 0.72 0.80 0.77 0.66 0.71 0.82 0.61Table 2: Results: Baseline is the most frequent sense; coverage is the number of occurrences on which thedecision was made by the algorithmHebrew.7 Conclusions and Future WorkWe have described a cross-linguistic study of aWord Sense Discrimination technique.
An algo-rithm based on context term clustering was appliedto ambiguous words from English, Hebrew, andRussian, and a comparative analysis of the resultswas presented.
Several observations can be made.First, the results suggest that the performance canbe affected by morphological complexity in the caseof a language, such as Russian, specifically, both interms of precision and recall.
Second, removing in-flectional morphology not only boosts the recall, butsignificantly improves the precision.
These resultssupport the view that morphological processing isbeneficial for WSD.For future work, we plan to investigate morethoroughly the role of morphological analysisfor WSD in Russian and Hebrew.
In particular,we will focus on the inflectional morphology ofRussian in order to determine whether removinginflections consistently improves results for Russianambiguous words across different parts of speech.Further, considering the complex structure of theHebrew language, we would like to determine whatkind of linguistic processing is useful for Hebrew inthe WSD context.AcknowledgmentsWe are grateful to Roxana Girju and the anony-mous reviewers for very useful suggestions andcomments.
This work is funded in part by grantsfrom the National Security Agency and the NationalScience Foundation.ReferencesDmitry Davidov and Ari Rappoport 2006.
Efficient Un-supervised Discovery of Word Categories Using Sym-metric Patterns and High Frequency Words.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, 297?304.Sydney, Australia.Richard O. Duda and Peter E. Hart.
1973.
Pattern Classi-fication and Scene Analysis.
John Wiley & Sons, NewYork.Tanja Gaustad.
2004.
A Lemma-Based Approach to aMaximum Entropy Word Sense Disambiguation Sys-tem for Dutch.
In Proceedings of the 20th Interna-tional Conference on Computational Linguistics (Col-ing 2004), 778-784.
Geneva.Dmitry Levinson.
1999.
Corpus-Based Methodfor Unsupervised Word Sense Disambiguation.www.stanford.edu/ dmitryle/acai99w1.ps.Susan Weber McRoy.
1992.
Using Multiple KnowledgeSources for Word Sense Discrimination.
Computa-tional Linguistics, 18(1): 1?30.Patrick Pantel and Dekang Lin.
2002.
DiscoveringWord Senses from Text In In Proceedings of ACMSIGKDD, pages 613-619.
Edmonton.Ted Pedersen and Rebecca Bruce.
1997.
Distinguishingword senses in untagged text.
In Proceedings of theSecond Conference on Empirical Methods in NaturalLanguage Processing, 197-207.
Providence, RI, Au-gust.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Richard Sproat and Jan van Santen.
1998.
Automaticambiguity detection.
In Proceedings of InternationalConference on Spoken Language Processing .
Sydney,Australia, 1998.Ying Zhao and George Karypis.
2005.
HierarchicalClustering Algorithms for Document Datasets DataMining and Knowledge Discovery, 10(2):141?168.87
