Coling 2010: Poster Volume, pages 180?188,Beijing, August 2010Two Methods for Extending Hierarchical Rules from the Bilingual ChartParsingMartin ?Cmejrek and Bowen ZhouIBM T. J. Watson Research Center{martin.cmejrek, zhou}@us.ibm.comAbstractThis paper studies two methods for train-ing hierarchical MT rules independentlyof word alignments.
Bilingual chart pars-ing and EM algorithm are used to train bi-text correspondences.
The first method,rule arithmetic, constructs new rules ascombinations of existing and reliable rulesused in the bilingual chart, significantlyimproving the translation accuracy on theGerman-English and Farsi-English trans-lation task.
The second method is pro-posed to construct additional rules directlyfrom the chart using inside and outsideprobabilities to determine the span of therule and its non-terminals.
The paper alsopresents evidence that the rule arithmeticcan recover from alignment errors, andthat it can learn rules that are difficult tolearn from bilingual alignments.1 IntroductionHierarchical phrase-based systems for machinetranslation usually share the same pattern for ob-taining rules: using heuristic approaches to ex-tract phrase and rule pairs from word alignments.Although these approaches are very successfulin handling local linguistic phenomena, handlinglonger distance reorderings can be more difficult.To avoid the combinatorial explosion, various re-strictions, such as limitations of the phrase lengthor non-terminal span are used, that sometimes pre-vent from extracting good rules.
Another reasonis the deterministic nature of those heuristics thatdoes not easily recover from errors in the wordalignment.In this work, we learn rules for hierarchicalphrase based MT systems directly from the par-allel data, independently of bilingual word align-ments.Let us have an example of a German-Englishsentence pair from the Europarl corpus (Koehn,2005).
(1) GER: die herausforderung besteht darindiese systeme zu den besten der welt zumachenENG: the challenge is to make the systemthe very bestThe two pairs of corresponding sequences diesesysteme ... der welt?the system ... best and zumachen?to make are swapped.
We believe thatthe following rule could handle long distance re-orderings, still with a reasonably low number ofterminals, for example:(2) X ?
?besteht darin X1 zu X2, is to X2X1?,There are 127 sentence pairs out of 300K of thetraining data that contain this pattern, but this rulewas not learned using the conventional approach(Chiang, 2007).
There are three potential risks:(1) alignment errors (the first zu aligned to to, order welt (of the world) aligned to null); (2) maxi-mum phrase length for extracting rules lower than11 words; (3) requirement of non-terminals span-ning at least 2 words.The rule arithmetic (Cmejrek et al, 2009) con-structs the new rule (2) as a combination of goodrule usages:(3) X ?
?besteht darin, is ?X ?
?X1 zu X2, to X2X1?180The approach consists of bilingual chart parsing(BCP) of the training data, combining rules foundin the chart using a rule arithmetic to propose newrules, and using EM to estimate rule probabilities.In this paper, we study the behavior of therule arithmetic on two different language pairs:German-English and Farsi-English.
We also pro-pose an additional method for constructing newrules directly from the bilingual chart, and com-pare it with the rule arithmetic.The paper is structured as follows: In Sec.
1, weexplain our main motivation, summarize previouswork, and briefly introduce the formalism of hi-erarchical phrase-based translation.
In Sec.
2, wedescribe the bilingual chart parsing and the EMalgorithm.
The rule arithmetic is introduced inSec.
3.
The new method for proposing new rulesdirectly from the chart is described in Sec.
4.
Theexperimental setup is described in Sec.
5.
Resultsare thoroughly discussed in Sec.
6.
Finally, weconclude in Sec.
7.1.1 Related workMany previous works use the EM algorithm toestimate probabilities of translation rules: Wu(1997) uses EM to directly estimate joint wordalignment probabilities of Inversion TransductionGrammar (ITG).
Marcu and Wong (2002) useEM to estimate joint phrasal translation model(JPTM).
Birch et al (2006) reduce its com-plexity by using only concepts that match thehigh-confidence GIZA++ alignments.
Similarly,Cherry and Lin (2007) use ITG for pruning.
Mayand Knight (2007) use EM algorithm to train tree-to-string rule probabilities, and use the Viterbiderivations to re-align the training data.
Huangand Zhou (2009) use EM to estimate conditionalrule probabilities P (?|?)
and P (?|?)
for Syn-chronous Context-free Grammar.
Others try toovercome the deterministic nature of using bilin-gual alignments for rule extraction by samplingtechniques (Blunsom et al, 2009; DeNero et al,2008).
Galley et al (2006) define minimalrules for tree-to-string translation, merge theminto composed rules (similarly to the rule arith-metic), and train weights by EM.
While in theirmethod, word alignments are used to define allrules, rule arithmetic proposes new rules indepen-dently of word alignments.
Similarly, Liu andGildea (2009) identify matching long sequences(?big templates?)
using word alignments and ?lib-erate?
matching small subtrees based on chartprobabilities.
Our method of proposing rules di-rectly from the chart does not use word alignmentat all.1.2 Formally syntax-based modelsOur baseline model follows the Chiang?s hierar-chical model (Chiang, 2007; Chiang, 2005; Zhouet al, 2008) based on Synchronous Context-freeGrammar (SCFG).
The rules have formX ?
?
?, ?,?
?, (4)where X is the only non-terminal in the gram-mar, ?
and ?
are source and target strings withterminals and up to two non-terminals, ?
is thecorrespondence between the non-terminals.
Cor-responding non-terminals have to be expanded atthe same time.2 Bilingual chart parsing and EMalgorithmIn this section, we briefly overview the algorithmfor bilingual chart parsing and EM estimation ofSCFG rule features.Let e = eM1 and f = fN1 of source and tar-get sentences.
For each sentence pair e, f , the ?E?step of the EM algorithm will use the bilingualchart parser to enumerate all possible derivations?, compute inside probabilities ?ijkl(X) and out-side probabilities ?ijkl(X), and finally calculateexpected counts c(r) how many times each rule rproduced the corpus C .The inside probabilities can be defined recur-sively and computed dynamically during the chartparsing:?ijkl =??
?tijklP (?.r)?(i?j?k?l?)??.bp?i?j?k?l?
, (5)where tijkl represents the chart cell spanning(eji , f lk), and the data structure ?
stores the rule?.r.
If r has non-terminals, then ?.bp stores back-pointers ?.bp1 and ?.bp2 to the cells representingtheir derivations.181The outside probabilities can be computed re-cursively by iterating the chart in top-down order-ing.
We start from the root cell ?1,M,1,N := 1 andpropagate the probability mass as?
?.bp1+ = P (?.r)?ijkl (6)for rules with one non-terminal, and?
?.bp1 + = P (?.r)?ijkl?
?.bp2 , (7)?
?.bp2 + = P (?.r)?ijkl?
?.bp1 , (8)for rules with two non-terminals.
The top-downordering ensures that each ?ijkl accumulates up-dates from all cells higher in the chart before itsown outside probability is used.The contributions to the rule expected countsare computed asc(?.r)+ = P (?.r)?ijkl?
?.ni=1 ??.bpi?1,M,1,N.
(9)Finally, rule probabilities P (r) are obtained bynormalizing expected counts in the ?M?
step.To improve the grammar coverage, the rule-set is extended by the following rules providing?backoff?
parses and scoring for the SCFG rules:(10) ?X1,X1f?, ?X1, fX1?, ?X1e,X1?,?eX1,X1?,(11) ?X1X2,X2X1?.Rules (10) enable insertions and deletions, whilerule (11) allows for aligning swapped constituentsin addition to the standard glue rule.3 Proposing new rules with rulearithmeticThe main idea of this work is to propose new rulesindependently of the bilingual word alignments.We parse each sentence pair using the baselineruleset extended by the new rule types (10) and(11).
Then we select the most promising rule us-ages and combine each two of them using therule arithmetic to propose new rules.
We put thenew rules into a temporary pool, and parse andcompute probabilities and expected counts again,this time we use rules from the baseline and fromthe temporary pool.
Finally, we dump expectedcounts for proposed rules, and empty the tempo-rary pool.
This way we can try to propose manyrules for each sentence pair, and to filter them laterusing accumulated expected counts from the EM.The term most promising is purposefully vague?
to cover all possible approaches to filtering ruleusages.
In our implementation, we are limited byspace and time, and we have to prune the numberof rules that we can combine.
We use expectedcounts as the main scoring criterion.
When com-puting the contributions to expected counts fromparticular rule usages as described by (9), we re-member the n-best contributors, and use them ascandidates after the expected counts for the givensentence pair have been estimated.The rule arithmetic combines existing rules us-ing addition operation to create new rules.
Theidea is shown in Example 12.
(12) Addition?5, 13, 5, 11, 13, 13?
?4, 10, 6, 10, 5, 5?
X ?
?X1 zu X2, to X2 X1?
?5, 11, 6, 11, 0, 0?
?6, 10, 7, 10, 0, 0?
X ?
?diese X1, the X1?1: ... 4 5 6 ... 11 12 13 3 4 5 6 7 ... 102: ... 0 -1 -1 ... -1 zu -2 0 to -2 -1 -1 ... -13: ... 0 diese -3 ... -3 0 0 0 0 0 the -3 ... -34: ... 0 diese -3 ... -3 zu -2 0 to -2 the -3 ... -35: ?5, 13, 6, 11, 13, 13?
?4, 10, 7, 10, 5, 5?
X ?
?diese X1 zu X2, to X2 the X1?First, create span projections for both sourceand target sides of both rules.
Use symbol 0 forall unspanned positions, copy terminal symbols asthey are, and use symbols -1, -2, -3, and -4 to tran-scribe X1 and X2 from the first rule, and X1 andX2 from the second rule.
Repeat the non-terminalsymbol on all spanned positions.
In Example 12line 1 shows the positions in the sentence, lines 2and 3 show the rule span projections of the tworules.Second, merge source span projections (line 4),record mappings of non-terminal symbols.
We re-quire that merged projections are continuous.
Weallow substituting non-terminal symbols by termi-nals, but we require that the whole span of thenon-terminal is fully replaced.
In other words,shortenings of non-terminal spans are not allowed.Third, collect new rule.
The merged rule us-ages (lines 5) are generalized into rules, so thatthey are not limited to the particular span forwhich they were originally proposed.The rule arithmetic can combine all types ofrules ?
phrase pairs, abstract rules, glues, swaps,insertions and deletions.
However, we require that182at least one of the rules is either a phrase pair oran abstract rule.4 Proposing directly from chartOne of the issues observed while proposing newrules with the rule arithmetic is the selection of thebest candidates.
The number of all candidates thatcan be combined depends on the length of the sen-tence pair and on the number of competing pars-ing hypotheses.
Using a fixed size of the n-bestcan constitute a risk of selecting bad candidatesfrom shorter sentences.
On the other hand, thespans of the best candidates extracted from longsentences can be far from each other, so that mostcombinations are not valid rules (e.g., the combi-nation of two discontinuous phrasal rules is notdefined).In our new approach we propose new rules di-rectly from the bilingual chart, relying on the in-side and outside probabilities computed after theparsing of the sentence pair.
The method has twosteps.
In the first step we identify best matchingparallel sequences; in the second step we propose?holes?
for non-terminals.4.1 Identifying best matching sequencesTo identify the best matching sequences, we scoreall sequences (eji , f lk) by a scoring function:scoreijkl =?ijkl?ijkl?1,M,1,NLex(i, j, k, l), (13)where the lexical score is defined as:Lex(i, j, k, l) =N?j?=1M?i?=0t(fj?|ei?)?ijkli?j?
(14)The t is the lexical probability from the word-to-word translation table, and ?ijkli?j?
is defined as?ins if i?
?
?i, j?
and j?
?
?k, l?, and as ?out ifi?
/?
?i, j?
and j?
/?
?k, l?, and as 0 elsewhere.The purpose of this function is to score only thepairs of words that are both either from within thesequence or from outside the sequence.
Usually0 ?
?out ?
?ins to put more weight on wordswithin the parallel sequence.The scoring function is a combination of ex-pected counts contribution of a sequence (eji , f lk)estimated from the chart with the IBM Model 1lexical score.Since only the sequences spanned by filledchart cells can have non-zero expected counts,we can select the n-best matching sequences rela-tively efficiently.4.2 Proposing non-terminal positionsSimilar approach can be used to propose best po-sitions for non-terminals.
We score every com-bination of non-terminal positions.
The expectedcounts can be estimated using Eq.
9.
Since we areproposing new rules, the probability P (r) used inthat equation is not defined.
Again, we can useModel 1 score instead, and use the following scor-ing function:sijkl(bp1, bp2) = (15)Lex(i,j,k,l,bp1,bp2)?ijkl?bp1?bp2?1,M,1,N ,Lex(i, j, k, l, bp1 , bp2) is defined as in Eq.
14.This time using 0 ?
?out ?
?NT1 = ?NT2 ?
?term, restricting the IBM Model 1 to score onlyword pairs that both belong either to the terminalsof the proposed rule, or to the sequences spannedby the same non-terminal, or outside of the rulespan.
The scoring function for rules with one non-terminal is just a special case of 15.Again, the candidates can be scored efficiently,taking into account only those combinations ofnon-terminal spans that correspond to filled cellsin the chart.The proposed method is again independent ofbilingual alignment, but at the same time utilizesthe information obtained from the bilingual chartparsing.5 ExperimentsWe carried out experiments on two language pairs,German-English and Farsi-English.The German-English data is a subset (297ksentence pairs) of the Europarl (Koehn, 2005) cor-pus.
Since we are focused on speech-to-speechtranslation, the punctuation was removed, and thetext was lowercased.
The dev set and test set con-tain each 1k sentence pairs with one reference.The word alignments were trained by GIZA++toolkit (Och and Ney, 2000).
Phrase pairs were183extracted using grow-diag-final (Koehn et al,2007).
The baseline ruleset was obtained asin (Chiang, 2007).
The maximum phrase lengthfor rule extraction was set to 10, the minimum re-quired non-terminal span was 2.Additional rules for insertion, deletion, andswap were added to improve the parsability of thedata, and to help EM training and rule arithmetic.However, these rules are not used by the decoder,since they would degrade the performance.New rules were proposed after the first iterationof EM1, either by rule arithmetic or directly fromthe chart.Only non-terminal rules proposed by the rulearithmetic from at least two different sentencepairs and ranked (by expected counts c(r)) in thetop 100k were used.
Figure 4 presents a sample ofthe new rules.New rules were also proposed directly from thechart, using the approach in Sec.
4.
5% of bestmatching parallel sequences, and 5 best scoringrules were selected from each parallel sequence.Non-terminal rules from the 200k-best rank wereadded to the model.
Figure 5 presents a sample ofthe new rules.Finally, one more iteration of EM was used toadjust the probabilities of the new and baselinerules.
These probabilities were used as featuresin the decoding.The performance of rule arithmetic was alsoverified on Farsi-English translation.
The train-ing corpus contains conversational spoken datafrom the DARPA TransTac program extendedby movie subtitles and online dictionaries down-loaded from the web (297k sentence pairs).
Thepunctuation was removed, and the text was low-ercased.
The dev set is 1,420 sentence pairs heldout from the training data, with one reference.
Thetest set provided by NIST contains 470 sentenceswith 4 references.
The sentences are about 30%longer and more difficult.The training pipeline was the same as for theGerman-English experiments.
122k new non-terminal rules were proposed using the rule arith-metic.1Since our initial experiments did not show any signifi-cant gain from proposing rules after additional (lengthy) it-erations of EM.The feature weights were tuned on the devset for each translation model separately.
Thetranslation quality was measured automatically byBLEU score (Papineni et al, 2001).6 Discussion of resultsThe BLEU score results are shown in the Ta-ble 3.
The cumulative gain of rule arithmetic andEM (RA + EM-i0) is 1 BLEU point for German-English translation and 2 BLEU points for Farsi-English.
The cumulative gain of rules proposedfrom the chart (DC + EM-i0) is 0.2 BLEU pointsfor German-English.
For comparison of effects ofvarious components of our method, we also showscores after the first five iterations of EM (EM-i0?EM-i4) without adding any new rules, just usingEM-trained probabilities as feature weights, andalso scores for new rules added into the baselinewithout adjusting their costs by EM (RA).The qualities of proposed rules are discussed inthis section.6.1 German-English rules from rulearithmeticThe Figure 4 presents a sample of new rules pro-posed during this experiment.
The table is di-vided into three parts, presenting rules from thetop, middle, and bottom of the 100K list.
Thequality of the rules is high even in the middle partof the table, the tail part is worse.We were surprised by seeing short rules consist-ing of frequent words.
For example ?um X1, inorder X1?.
When looking into word-level align-ments, we realized that these rules following thepattern 16 prevent the baseline approach from ex-tracting the rule.
(16)GER: um Obj zu VENG: in order to V ObjSimilarly many other rules match the pattern ofbeginning of a subordinated clause, such as that iswhy, or insertions, such as of course, which bothhave to be strictly followed by VSO constructionin German, in contrast to the SVO word order inEnglish.We also studied the cases of rule arithmetic cor-recting for systematic word alignment errors.
For184example the new rule ?X1 zu koennen, to X1?
waslearned from the sentence(17)um die in kyoto vereinbarten senkungen beibehalten zu koennenin order to maintain the reductions agreed in kyotoThe English translation often uses a differentmodality, thus the modal verb koennen is alwaysaligned with null.
Since unaligned words are usu-ally not allowed at the edges of sub-phrases gener-alized into non-terminals (Chiang, 2007), this rulecannot be learned by the baseline.We observe that many new proposed rules cor-respond to patterns with a non-terminal spanningone word.
For example ?um X1 zu X2, to X2X1?
corresponds to the same pattern 16, where X2spans one verb.
The line baseline min1 in the Ta-ble 3 shows 0.3 BLEU improvement of a modeltrained without the minimum non-terminal spanrequirement.
However, this improvement comesat a cost of more than four times increased modelsize, as shown in Table 2.
We observe that us-ing the minimum span requirement while learningfrom bitext alignments combined with rule arith-metic that can learn the most reliable rules span-ning one word yields better performance in speed,memory, and precision.We can also study the new rules quantitatively.We want to know how the rules proposed by therule arithmetic are used in decoding.
We tracedthe translation of the 1,000 test set sentences tomark the rules that were used to generate the bestscoring hypotheses.The stats are presented in the Table 1.
Thechance that a new rules will be used in the test setdecoding (0.86%) is more than 7 times higher thanthat of all rules (0.12%).
Encouraging evidence isthat while the rule arithmetic rules constitute only1.87% of total rules, they present 9.17% of rulesused in the decoding.The Figure 1 lists the most frequently used newrules in the decoding.
We can see many ruleswith 2 non-terminals that model complex verbforms (?wird X1 haben,will have X1?
), reorder-ing in clauses (?um X1 zu gewaehrleisten, to en-sure X1?
), or reordering of verbs from the secondposition in German to SVO in English (?heute X1wir X2, today we X1 X2?
).RA Ger.
DC Ger.
RA FarsiSentences translated 1,000 1,000 417|ALL| (all rules) 5.359,751 5.459,751 8.532,691|NEW| (new rules) 100,000 200,000 121,784|NEW||ALL| 1.87% 3.66% 1.43%|hits ALL| 10,122 7,256 2,521|glue| 2,910 271 267|hits ALL unique| 6.303 6,433 2,058|hits ALL unique||ALL| 0.12% 0.12% 0.02|hits NEW| 928 1,541 125|hits NEW unique| 858 1,504 110|hits NEW unique||NEW| 0.86% 0.75 % 0.09|hits NEW||hits ALL| 9.17% 21.23% 4.96%|terminals from NEW| 4,385 7,825 407|terminals from NEW||hits NEW| 4,73 5.08 3.26Table 1: Rule hits for 1,000 test set.Model #phrases #rulesGer-Eng baseline 8.5M 5.3MGer-Eng baseline min1 8.5M 23.MTable 2: Model sizes.We also studied the correlation between therank of the proposed rules (ranked by expectedcounts) and the hit rate during the decoding.
TheFigure 2 measures the hit rate for each of 1,000best ranking rules, and should be read as follows:the rules ranking 0 to 999 were used 70 times, thehit rate decreases as the rank grows so that therewere no hits for rules ranking 90k and more.
Therank is a good indicator of the usefulness of newrules.We hypothesize that the new rules are capableof combining partial solutions to form hypothe-ses with better word order, or better complex verbforms so that these hypotheses are better scoredand are parts of the winning solutions more often.6.2 German-English rules proposed directlyfrom the chartWe also studied why the rules proposed directlyfrom the bilingual chart yield smaller improve-ment than the rule arithmetic.
The number of newrules used in the decoding (1,541) is even higherthan that of the rule arithmetic, and it constitutes21.23% of all cases.
The two experiments were185#hits Ger Eng5 X1 stellt X2 dar X1 is X23 X1 sowohl X2 als auch X1 both X2 and3 X1 ist es X2 it is X2 X13 X1 die X2 ist X1 which is X22 wird X1 haben will have X12 wir X1 damit X2 we X1 so that X22 was X1 hat X2 what X1 has X22 was X1 betrifft so as regards X12 und X1 muessen wir X2 and X1 we must X22 um X1 zu gewaehrleisten to ensure X12 um X1 zu X2 to X2 X12 sowohl X1 als auch both X1 and2 sie X1 auch X2 they also X1 X22 in erster linie X1 X1 in the first instance2 in X1 an in X12 ich X1 meine i X12 heute X1 wir X2 today we X1 X22 herr praesident X1 und herren mr president X1 and gentlemen2 gleich X1 X1 a moment2 es muss X1 werden it must be X1Figure 1: Examples of the most frequently hitrules during the decoding.tuned separately, so that they used different gluerule weights.
That is why we observe the differ-ence in the number of glues (and the number oftotal rules) in the Table 1.
We do not observe anysignificant correlation between the rank of the ruleand the hit rate.
The Figure 3 shows that the first10k-ranked rules are hit several times, and thenthe hit rate stays flat.We offer an explanation based on our observa-tions of rules used for the decoding.
The rulesproposed directly from the chart contain a big por-tion of content words.
These rules do not captureany important differences between the structuresof the two languages that could not be handledby phrasal rules as well.
For example, the rule?die neuen vorschriften sollen X1,the new rulesare X1?
is correct, but a combination of a baselinephrasal rule and glue will produce the same result.We also see many rules with non-terminalsspanning one word.
For example, the sequence(18) die europaeische kommission?theeuropean commissionwill produce the rule(19) ?die X1 kommission, the X1 commission?.Although the sequence and the rule are highscored by 13 and 15, we intuitively feel that gen-Figure 2: Usage of new rules (RA).Figure 3: Usage of new rules (DC).eralizing the word european is not very helpful inthis context.The rule arithmetic could propose the rule 19 as(20) ?die X1, the X1?
+ ?kommission,commission?,but since the candidates for combination are se-lected as rules with the highest expected counts(Sec.
3), the rules 20 will most likely loose to thephrase pair 18 and will not be selected.To conclude our comparison, we observe thatboth methods produce reliable rules that are of-ten reused in decoding.
Nevertheless, since therule arithmetic combines the most successful rulesfrom each parallel parse, the resulting rules enablestructural transformations that could not be han-dled by baseline rules.186German-English Farsi-EnglishModel dev set test set dev set test setbaseline 23.9 25.4 41.1 38.2RA + EM-i0 24.8 26.4 41.8 40.2DC + EM-i0 24.6 25.6EM-i0 24.4 26.1 40.8 39.1EM-i1 24.4 25.8 41.3 38.5EM-i2 24.4 25.9 41.4 38.2EM-i3 24.4 26.0 41.3 39.3EM-i4 24.4 26.0 41.6 39.6RA 24.4 26.1 40.7 38.4baseline min1 24.0 25.7Table 3: BLEU scores6.3 Farsi-English rules from the rulearithmeticAlthough we have only limited resources to quali-tatively analyze the Farsi-English experiments, wenoticed that there are two major groups of newrules.The first group corresponds to the fact that Farsidoes not have definite article and allows pro-drop.We observe many new rules that could not belearned from word alignments, since some defi-nite articles or pronouns in English were alignedto null (and unaligned words are not allowed at theedges of phrases).
However, if the chart containsan insertion (of the determiner or pronoun) with ahigh expected count, the rule arithmetic may pro-pose new rule by combining it with other rules.The second group contains rules that help wordreordering.
We observe rules moving verbs fromthe S PP O V in Farsi into SVO in English as wellas rules reordering wh-clauses.Most of the rules traced during the test set de-coding belong to the second group.
Figure 1shows that the number of new rules hit duringthe decoding is smaller compared to the German-English experiments.
On the other hand, the ruleshave smaller number of terminals so that we as-sume that the positive effect of these rules comesfrom the reordering of non-terminals.um X1 in order X1natuerlich X1 of course X1deshalb X1 this is why X1X1 zu koennen to X1X1 ist it is X1nach der tagesordnung folgt die X1 the next item is the X1herr X1 herr kommissar X2 mr X1 commissioner X2die X1 der X2 X1 the X2im gegenteil X1 on the contrary X1nach der tagesordnung folgt X1 the next item is X1X1 die X2 the X1 the X2die X1 die the X1ausserdem X1 in addition X1daher X1 that is why X1wir X1 nicht X2 we X1 not X2die X1 der X2 the X2 X1deshalb X1 for this reason X1um X1 zu X2 to X2 X1X1 nicht X2 werden X1 not be X2Figure 4: Sample rules (RA).ausserdem X1 wir we X1 alsodie X1 des kommissars the commissioner ?s X1den X1 ratsvorsitz the X1 presidencyich hoffe dass X1 i would hope that X1X1 ist zu X2 geworden X1 has become X2die X1 des vereinigten koenigreichs the uk X1X1 maij weggen X2 X1 maij weggen X2X1 wir auf X2 sind X1 we are on X2ich frage mich X1 i wonder X1Figure 5: Sample rules (DC).7 ConclusionIn this work, we studied two new methods forlearning hierarchical MT rules: the rule arith-metic and proposing directly from the parse for-est.
We discussed systematic patterns where therule arithmetic outperforms alignment-based ap-proaches and verified its significant improvementon two different language pairs (German-Englishand Farsi-English).
We also hypothesized why thesecond method ?
proposing rules directly from thechart ?
improves the baseline less than the rulearithmetic.AcknowledgmentThis work is partially supported by the DARPATRANSTAC program under the contract num-ber NBCH2030007.
Any opinions, findings, andconclusions or recommendations expressed in thismaterial are those of the authors and do not nec-essarily reflect the views of DARPA.187ReferencesBirch, Alexandra, Chris Callison-Burch, Miles Os-borne, and Philipp Koehn.
2006.
Constraining thephrase-based, joint probability statistical translationmodel.
In Proceedings on WSMT?06, pages 154?157.Blunsom, Phil, Trevor Cohn, Chris Dyer, and MilesOsborne.
2009.
A gibbs sampler for phrasal syn-chronous grammar induction.
In ACL ?09, pages782?790.Cherry, Colin.
2007.
Inversion transduction grammarfor joint phrasal translation modeling.
In NAACL-HLT?07/SSST?07.Chiang, David.
2005.
A hierarchical phrase-based model for statistical machine translation.
InACL?05, pages 263?270.Chiang, David.
2007.
Hierarchical phrase-basedtranslation.
Comput.
Linguist., 33(2):201?228.Cmejrek, Martin, Bowen Zhou, and Bing Xiang.
2009.Enriching SCFG rules directly from efficient bilin-gual chart parsing.
In IWSLT?09, pages 136?143.DeNero, John, Alexandre Bouchard-Co?te?, and DanKlein.
2008.
Sampling alignment structure undera bayesian translation model.
In EMNLP ?08, pages314?323.Galley, Michel, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of ACL, pages 961?968.Huang, Songfang and Bowen Zhou.
2009.
An EMalgorithm for SCFG in formal syntax-based transla-tion.
In Proc.
IEEE ICASSP?09, pages 4813?4816.Koehn, Philipp, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In ACL.Koehn, Philipp.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit.Liu, Ding and Daniel Gildea.
2009.
Bayesian learningof phrasal tree-to-string templates.
In EMNLP ?09,pages 1308?1317.Marcu, Daniel and W Wong.
2002.
A phrase-based,joint probability model for statistical machine trans-lation.
In Proceedings of EMNLP?02.May, Jonathan and Kevin Knight.
2007.
Syntactic re-alignment models for machine translation.
In Pro-ceedings of EMNLP-CoNLL?07, pages 360?368.Och, F. J. and H. Ney.
2000.
Improved statisticalalignment models.
In Proc.
of ACL, pages 440?447,Hong Kong, China, October.Papineni, K., S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Technical Report RC22176, IBM T. J.Watson Research Center.Wu, Dekai.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Zhou, Bowen, Bing Xiang, Xiaodan Zhu, and YuqingGao.
2008.
Prior derivation models for formallysyntax-based translation using linguistically syntac-tic parsing and tree kernels.
In Proceedings of theACL?08: HLT SSST-2, pages 19?27.188
