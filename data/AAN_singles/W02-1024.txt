A Hybrid Approach to Natural Language Web SearchJennifer Chu-Carroll, John Prager, Yael Ravin and Christian CesarIBM T.J. Watson Research CenterP.O.
Box 704Yorktown Heights, NY 10598, U.S.A.{jencc,jprager,ravin,cesar}@us.ibm.comAbstractWe describe a hybrid approach to improv-ing search performance by providing anatural language front end to a traditionalkeyword-based search engine.
The keycomponent of the system is iterative queryformulation and retrieval, in which one ormore queries are automatically formulatedfrom the user?s question, issued to thesearch engine, and the results accumulatedto form the hit list.
New queries are gener-ated by relaxing previously-issued queriesusing transformation rules, applied in anorder obtained by reinforcement learning.This statistical component is augmentedby a knowledge-driven hub-page identi-fier that retrieves a hub-page for the mostsalient noun phrase in the question, ifpossible.
Evaluation on an unseen testset over the www.ibm.com public web-site with 1.3 million webpages shows thatboth components make substantial contri-bution to improving search performance,achieving a combined 137% relative im-provement in the number of questions cor-rectly answered, compared to a baselineof keyword queries consisting of two nounphrases.1 IntroductionKeyword-based search engines have been one of themost highly utilized internet tools in recent years.Nevertheless, search performance remains unsatis-factory at most e-commerce sites (Hagen et al,2000).
Librarians and search professionals have tra-ditionally favored Boolean keyword search systems,which, when successful, return a small set of rele-vant hits.
However, the success of these systems crit-ically depends on the choice of the right keywordsand the appropriate Boolean operators.
As the pop-ulation of search engine users has grown beyond asmall dedicated search professional community andas these new users are less familiar with the contentsthey are searching, it has become harder for them toformulate successful keyword queries.
To improvesearch performance, one can improve search engineaccuracy with respect to fixed keyword queries, orprovide the search engine with better queries, thosemore likely to retrieve good results.
While there ismuch on-going work in the IR community on theformer topic, we have taken the latter approach byproviding a natural language search interface andautomatically generating keyword queries that uti-lize advanced search features typically unused byend users.
We believe that natural language ques-tions are easier for users to construct than keywordqueries, thus shifting the burden of optimal queryformulation from the user to the system.
Such ques-tions also eliminate much of the ambiguity of key-word queries that often leads to poor results.
Fur-thermore, the methodology we describe may be ap-plied to different search engines with only minormodification.To transform natural language input into a searchquery, the system must identify information perti-nent for search and utilize it to formulate keywordqueries likely to retrieve relevant answers.
We de-scribe and evaluate a hybrid system, RISQUE, thatadopts an iterative approach to query formulationand retrieval for search on the www.ibm.com pub-Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
180-187.Proceedings of the Conference on Empirical Methods in Naturallic website with 1.3 million webpages.
RISQUEmay issue multiple queries per question, where anew query is generated by relaxing a previously is-sued query via transformation rule application, inan order obtained by reinforcement learning.
In ad-dition, RISQUE identifies a hub-page for the mostsalient noun phrase in the question, if possible,utilizing traditional knowledge-driven mechanisms.Evaluation on an unseen test set showed that boththe machine-learned and knowledge-driven compo-nents made substantial contribution to improvingRISQUE?s performance, resulting in a combined137% relative improvement in the number of ques-tions correctly answered, compared to a baseline ob-tained by queries consisting of two noun phrases(2NP baseline).2 Related WorkThe popularity of natural language search is evi-denced by the growing number of search engines,such as AskJeeves, Electric Knowledge, and North-ern Light,1 that offer such functionality.
For mostsites, we were only able to perform a cursory ex-amination of their proprietary techniques.
Adopt-ing a similar approach as FAQFinder (Hammondet al, 1995), AskJeeves maintains a database ofquestions and webpages that provide answers tothem.
User questions are compared against those inthe database, and links to webpages for the closestmatches are returned.
Similar to our approach, Elec-tric Knowledge transforms a natural language ques-tion into a series of increasingly more general key-word queries (Bierner, 2001).
However, their queryformulation process utilizes hard-crafted regular ex-pressions, while we adopt a more general machinelearning approach for transformation rule applica-tion.Our work is also closely related to question an-swering in the question analysis component (e.g.,(Harabagui et al, 2001; Prager et al, 2000; Clarkeet al, 2001; Ittycheriah et al, 2001)).
In partic-ular, Harabagui et al(2001) also iteratively refor-mulate queries based partly on the search results.However, their mechanism for query reformulationis heuristic-based.
We utilized machine learning to1www.askjeeves.com, www.electricknowledge.com, andwww.northernlight.com, respectively.optimize the query formulation process.3 Data AnalysisTo generate optimal keyword queries from naturallanguage questions, we first analyzed a set of 502questions related to the purchasing and support ofThinkPads (notebook computers) and their acces-sories, such as ?How do I set up hibernation for myThinkPad??
and ?Show me all p3 laptops.?
Ouranalysis focused on three tasks.
First, we attemptedto identify an exhaustive set of correct webpages foreach question, where a correct webpage is one thatcontains either an answer to the question or a hyper-link to such a page.
Second, we manually formu-lated successful keyword queries from the question,i.e., queries which retrieved at least one correct web-page.
Third, we attempted to discover general pat-terns in how the natural language questions may betransformed into successful keyword queries.Our analysis eliminated 110 questions for whichno correct webpage was found.
Of the remaining392 questions, we identified, on average, 4.37 cor-rect webpages and 1.58 successful queries per ques-tion.
We found that the characteristics of success-ful queries varied greatly.
In the simplest case, asuccessful query may contain all the content bear-ing NPs in the question, such as thinkpad AND?answering machine?
for ?Can I use my ThinkPadas an answering machine?
?2 In the vast majorityof cases, however, more complex transformationswere applied to the question to result in a successfulquery.
For instance, a successful query for ?How doI hook an external mouse to my laptop??
is (mouseOR mice) AND thinkpad AND +url:support.
Inthis case, the head noun mouse was inflected,3 thepremodifier external was dropped, hook was deleted,laptop was replaced by thinkpad, and a URL con-straint was applied.We observed that in our corpus, most success-ful queries can be derived by applying one ormore transformation rules to the NPs and verbsin the questions.
Table 1 shows the manually in-2Our search engine (www.alltheweb.com) accepts a con-junction of terms (a word, quoted phrase, or disjunction ofwords/phrases), and inclusion/exclusion of text strings in theURL, such as +url:support.3Many commercial search engines purposely do not inflectsearch words to avoid overgeneralization of queries.Rule FunctionConstrainURL Apply URL constraintsRelaxNP Relax phrase to conjunction of wordsDropNP Remove least salient NPDropModifier Remove premodifiers of nounsDropVerb Remove verbApplySynonyms Add synonyms of NPsInflect Inflect head nouns and verbTable 1: Query Transformation Rulesduced commonly-used transformation rules basedon our corpus analysis.
Though the rules were quitestraightforward to identify, the order in which theyshould be applied to yield optimal overall perfor-mance was non-intuitive.
In fact, the best orderwe manually derived did not yield sufficient per-formance improvement over our baseline (see Sec-tion 7).
We further hypothesize that the optimal ruleapplication sequence may be dependent on ques-tion characteristics.
For example, DropVerb maybe a higher priority rule for buy questions than forsupport questions, since the verbs indicative of buyquestions (typically ?buy?
or ?sell?)
are often ab-sent in the target product pages.
Therefore, we in-vestigated a machine learning approach to automat-ically obtain the optimal rule application sequence.4 A Reinforcement Learning Approach toQuery FormulationOur problem consists of obtaining an optimal strat-egy for choosing transformation rules to generatesuccessful queries.
A key feature of this problem isthat feedback during training is often delayed, i.e.,the positive effect of applying a rule may not be ap-parent until a successful query is constructed afterthe application of other rules.
Thus, we adopt a rein-forcement learning approach to obtain this optimalstrategy.4.1 Q LearningWe adopted the Q learning paradigm (Watkins,1989; Mitchell, 1997) to model our problem as a setof possible states, S, and a set of actions, A, whichcan be performed to alter the current state.
Whilein state s ?
S and performing action a ?
A, thelearner receives a reward r(s, a), and advances tostate s?
= ?
(s, a).To learn an optimal control strategy that maxi-mizes the cumulative reward over time, an evalua-tion function Q(s, a) is defined as follows:Q(s, a) ?
r(s, a) + ?
maxa?Q(?
(s, a), a?)
(1)In other words, Q(s, a) is the immediate reward,r(s, a), plus the discounted maximum future rewardstarting from the new state ?
(s, a).The Q learning algorithm iteratively selects an ac-tion and updates Q?, an estimate of Q, as follows:Q?n(s, a) ?
(1?
?n)Q?n?1(s, a) + (2)?n(r(s, a) + maxa?Q?n?1(s?, a?
))where s?
= ?
(s, a) and ?n is inversely proportionalto the number of times a state/action pair <s,a> hasbeen visited up to the nth iteration of the algorithm.4Once the system learns Q?, it can select from thepossible actions in state s based on Q?
(s, ai).4.2 Query Formulation Using Q LearningTo formulate our problem in the Q learningparadigm, we represent a state as a 6-tuple,<qtype, url constraint, np phrase, num nps,num modifiers, num verbs>, where:?
qtype is buy or support depending on questionclassification.?
url constraint is true or false, and determinesif manually predefined URL restrictions will beapplied in the query.?
np phrase is true or false, and determineswhether each NP will be searched for as aphrase or a conjunction of words.?
num nps is an integer between 1 and 3, anddetermines how many NPs will be included inthe query.?
num modifiers is an integer between 0 and 2,and indicates the maximum number of premod-ifiers in each NP.?
num verbs is 0 or 1, and determines if the verbwill be included in the query.4Equation (2) modifies (1) by taking a decaying weightedaverage of the current Q?
value and the new value to guaranteeconvergence of Q?
in non-deterministic environments.
We ex-plain in the next section why our query formulation problem inthe Q learning framework is non-deterministic.This representation is chosen based on the rulesidentified in Section 3.
The actions, A, include thefirst 5 actions in Table 1, and the ?undo?
counterpartfor each action.5 Except for qtype, which remainsstatic for a question, each remaining element in thetuple can be altered by one of the 5 pairs of actionsin a straightforward manner.
The state, s, and thequestion, q, generate a unique keyword query whichresults in a hit list, h(s, q).
The hit list is evaluatedfor correctness, whose result is used to define thereward function as follows:r(s, a) =????????????
?1 if h(s?, q) contains at least onecorrect webpage0 if h(s?, q) has no correct page &|h(s?, q)| < 10?1 otherwisewhere s?
= ?
(s, a).
Note that our system operates ina non-deterministic environment because the rewardis dependent not only on s and a, but also on q.6Having defined S, A, ?, and r, Q?
is determined byapplying the Q learning algorithm, using the updatefunction in (2), to our corpus of 392 questions.
Foreach question, an initial state is randomly selectedwithin the bounds of the question.
The system theniteratively selects and applies actions, and updatesQ?
until a successful query is generated or the maxi-mum number of iterations is reached (in our imple-mentation, 15).
The training algorithm iterates overall questions in the training set and terminates whenQ?
converges.5 RISQUE: A Hybrid System for NaturalLanguage Search5.1 System OverviewIn addition to motivating machine learning basedquery transformation as our central approach to nat-ural language search, our analysis revealed the needfor several other key system components.
As shownin Figure 1, RISQUE adopts a hybrid architecture5Morphological and synonym expansions are applied at theoutset, which was shown to result in better performance thanoptional application of those rules.6It is theoretically possible to encode pertinent informationin q in the state representation, thus making the environmentdeterministic.
However, data sparseness problems associatedwith such a representation makes it impractical.Hit List AccumulationOntologyHub-Page IdentifierNatural Language QuestionQuestion Pre-ProcessingTop n Hits for QuestionQuestion UnderstandingNP Sequencingand RetrievalQuery FormulationFigure 1: RISQUE Architecturethat combines the utility of traditional knowledge-based methods and statistical approaches.
Givena question, RISQUE first performs question analy-sis by extracting pertinent information to be used inquery formulation, such as the NPs, VPs, and ques-tion type, and then orders the NPs in terms of theirrelative salience.
This information is then used forhit list construction by two modules.
The first com-ponent is the hub-page identifier, which retrieves, ifpossible, a hub page for the most salient NP in thequestion.
The second component is the Q learningbased query formulation and retrieval module thatiteratively generates queries via transformation ruleapplication and issues them to the search engine.The results from both processes are combined andaccumulated until n distinct hits are retrieved.In addition to the above components, RISQUEemploys an ontology for the ThinkPad domain,which consists of 1) a hierarchy of about 500 do-main objects, 2) nearly 400 instances of relation-ships, such as isa and accessory-of, between objects,and 3) a synonym dictionary containing about 1000synsets.
The ontology was manually constructedand took approximately 2 person-months for cov-erage in the ThinkPad domain.
It provides perti-nent information to the question pre-processing andquery formulation modules, which we will describein the next sections.5.2 Question Pre-Processing5.2.1 Question UnderstandingRISQUE?s question understanding component isbased primarily on a rule-driven parser in the slotgrammar framework (McCord, 1989).
The result-ing parse tree is first analyzed for NP/VP extrac-tion.
Each NP includes the head noun and up totwo premodifiers, which covers most NPs in our do-main.
The NPs are further processed by a named-entity recognizer (Prager et al, 2000; Wacholder etal., 1997), with reference to domain-specific propernames in our ontology.
Recognized compoundterms, such as ?hard drive?, are treated as single en-tities, rather than as head nouns (?drive?)
with pre-modifiers (?hard?).
This prevents part of the com-pound term from being dropped when the DropMod-ifier transformation rule is applied.The parse tree is also used to classify the questionas buy or support.
The classifier utilizes a set of rulesbased on lexical and part-of-speech information.
Forexample, ?how?
tagged as a adverb (as in ?How doI ...?)
suggests a support question, while ?buy/sell?used as a verb indicates a buy question.
These ruleswere manually derived based on our training data.5.2.2 NP SequencingOur analysis showed that when a successful queryis to contain fewer NPs than in the question, it isnot straightforward to determine which NPs to elim-inate, as it requires both domain and content knowl-edge.
However, we observed that less salient NPsare often removed first, where salience indicates theimportance of the term in the search process.
Therelative salience of NPs in this context can, for themost part, be determined based on the ontologicalrelationship between the NPs and knowledge aboutthe website organization.
For instance, if A is anaccessory-of B, then A is more salient than B since,on our website, accessories typically have their ownwebpages with significantly more information thanpages about, for instance, the ThinkPads with whichthey are compatible.Our NP sequencer utilizes a rule-based reasoningmechanism to rank a set of NPs based on their rel-ative salience, as determined by their relationshipin the ontology.7 Objects not present in the ontol-7We are aware that factors involving deeper question under-ogy are considered less important than those present.This process produces a list of NPs ranked in de-creasing order of salience.5.3 Hub-Page IdentifierAs with most websites, the ThinkPad pages onwww.ibm.com are organized hierarchically, with adozen or so hub-pages that serve as good startingpoints for specific sub-topics, such as mobile acces-sories and personal printers.
However, since thesehub-pages are typically not content-rich, they oftendo not receive high scores from the search engine(over which we have no control).
Thus, we devel-oped a mechanism to explicitly retrieve these hub-pages when appropriate, and to combine its resultswith the outcome of the actual search process.The hub-page identifier consists of a mappingfrom a subset of the named entities in the ontology totheir corresponding hub-pages.8 For each question,the hub-page identifier retrieves the hub-page for themost salient NP, if possible, which is presented asthe first entry in the hit list.5.4 Reinforcement Learning Based QueryFormulationThis main component of RISQUE iteratively formu-lates queries, issues them to the search engine, andaccumulates the results to construct the hit list.
Thequery formulation process starts with the most con-strained query, and each new query is a relaxation ofa previously issued query, obtained by applying oneor more transformation rules to the current query.The transformation rules are applied in the order ob-tained by the Q learning algorithm as described inSection 4.2.The initial state of the query formulation processis as follows: url constraint and np phrase are setto true, while the other attributes are set to their re-spective maximum values based on the outcome ofthe question understanding process.
This initial staterepresents the most constrained query possible forthe given question, and allows for subsequent relax-ation via the application of transformation rules.standing come into play in determining relative salience.
Weleave investigation of such features as future work.8For reasons of robustness, we actually map a named entityto manually selected keywords which, when issued to the searchengine, retrieves the desired hub-page as the first hit.When a state s, is visited, a query is generatedbased on s and the question.
The query termsare instantiated based on the values of np phrase,num nps, num modifiers, and num verbs in s andthe question itself, while URL constraints may beapplied based on url constraint and qtype.
Finally,synonyms expansion is applied based on the syn-onym dictionary in the ontology, while morphologi-cal expansion is performed on all NPs using a rule-based inflection procedure.After a query is issued, the search results areincorporated into the hit list, and duplicate hitsare removed.
A transformation rule amax =argmaxaQ?
(s, a) is applied to yield the new state.Q?
(s, amax) is then decreased to remove it from fur-ther consideration.
This iterative process continuesuntil the hit list contains 10 or more elements.6 ExampleTo illustrate RISQUE?s end-to-end operation, con-sider the question ?Do you sell a USB hub for aThinkPad?
?The question is classified as a buy question, givenpresence of the verb sell.
In addition, two NPs areidentified:NP1: head = USB hubNP2: head = ThinkPadNote that ?USB hub?
is identified as a compoundnoun by our named-entity recognizer.
The NP se-quencer determines that USB hub is more salientthan ThinkPad since the former is an accessory ofthe latter.The hub-page identifier finds the networking de-vices hub-page for USB hub, presented as the firstentry in the hit list in Figure 2, where correct web-pages are boldfaced.Next, RISQUE invokes its iterative query for-mulation process to populate the remaining hitlist entries.
The initial state is <qtype = buy,url constraint = true, np phrase = true, num nps= 2, num modifiers = 0, num verbs = 0>.
Thisstate generates the query shown as ?Query 2?
in Fig-ure 2, which results in 6 hits, of which 4 are correct.RISQUE selects the optimal transformation rulefor the current state, which is ReinstateModifier.Since neither NP has any modifier, a second rule,RelaxNP is attempted, which resulted in a new querythat did not retrieve any previously unseen hits.Next, RISQUE selects ConstrainNP and RelaxURL,resulting in the query shown as ?Query 3?
in Fig-ure 2.9 Note that relaxing the URL constraint resultsin retrieval of USB hub support pages.7 Performance Evaluation and AnalysisWe evaluated RISQUE?s performance on 102 ques-tions in the ThinkPad domain previously unseento both RISQUE?s knowledge-based and statisticalcomponents.
The top 10 hits returned by RISQUEfor each question were manually evaluated for cor-rectness as in Section 3.
A 2NP baseline was ob-tained by extracting up to two most salient NPs ineach question, searching for the conjunction of allwords in the NPs, and manually evaluating the 10top hits returned.We selected the 2NP baseline based on statisticsof keyword query logs on our website, which showthat 98.2% of all queries contain 4 keywords or less.Furthermore, most three and four-word queries con-tain two distinct noun phrases, such as ?visualage forjava?
and ?application framework for e-business?.Thus, we use the 2NP baseline as an approximationof user keyword search performance for our naturallanguage questions.10We compared RISQUE?s performance to thebaseline using three metrics:111.
Total correct: number of questions for whichat least one correct webpage is retrieved.2.
Average correct: average number of correctwebpages retrieved per question.3.
Average rank: average rank of the first correctwebpage in the hit list.The evaluation results are summarized in Table 2,where the first and last rows show the 2NP base-line and RISQUE?s performance, respectively.
The9A set of negative URL constraints is applied at all times tobest exclude parts of the website unrelated to ThinkPads.10This is likely too high an estimate for current keywordsearch performance, since the majority of user queries employonly one noun phrase.11We chose not to evaluate our results using the traditionalIR recall measure because for our task, it is often sufficient toreturn one page that answers the question instead of attemptingto retrieve all relevant pages.Question: Do you have a USB hub for a ThinkPad?Query 1: hub-page identifier1 Communications, Networking, Input/Output Devices ...Query 2: (thinkpad thinkpads laptop laptops notebook notebooks) (?usb hub?
?usb hubs?
)+url: (commerce accessories proven products thinkpad)-url: research press rs6000 eserver ...2 Mobile Accessories ...3 4-Port Ultra-Mini USB Hub4 ThinkPad TransNote Port Extender5 Belkin ExpressBus 7-Port USB Hub6 Portable Drive Bay 20007 Belkin BusStation 7 Port Modular USB HubQuery 3: (thinkpad thinkpads laptop laptops notebook notebooks) (?usb hub?
?usb hubs?
)-url: research press rs6000 eserver ...8 Java technology for the universal serial bus9 Multiport USB Hub - Printer compatibility list10 Multi-Port USB Hub - OverviewFigure 2: RISQUE Results for Sample QuestionTotal Average AverageCorrect Correct Rank2NPs 30 0.63 4.0Fixed Order 45 1.24 2.71RISQUE w/ohub identifier 56 1.67 2.25RISQUE 71 1.87 2.11Table 2: RISQUE Evaluation Resultsresults show that RISQUE correctly answered 71questions, a 137% relative improvement over thebaseline.
Furthermore, the average number of cor-rect answers found nearly tripled, while, on average,the rank of the first correct answer improved from4.0 to 2.11.Table 2 further shows performance figures thatevaluate the individual contribution of RISQUE?stwo main components, the hub-page identifier andthe iterative query formulation module.
Comparisonbetween the last two rows in Table 2 shows the effec-tiveness of the hub-page identifier, which substan-tially increased the number of questions correctlyanswered, but resulted in only minor gain using theother two performance metrics.
To assess the effec-tiveness of the query formulation module, we usedthe best manually-derived rule application sequenceobtained in Section 3.
We compared these fixed or-der performance figures to those for RISQUE w/ohub identifier which shows that applying Q learningto derive an optimal state-dependent rule applicationorder resulted in fairly substantial improvement us-Maxq 10 5 3 2 1RISQUE 5.07 4.47 2.89 1.98 1RISQUE w/o hpi 4.26 3.86 2.80 1.93 1Table 3: Average Queries Issued for Select Maxqs2030405060700 1 2 3 4 5 6Number of QuestionsAnsweredAverage Number of Queries IssuedRISQUERISQUE w/o hpi2NP baselineFigure 3: # Queries Issued vs. System Performanceing all three metrics.One of RISQUE?s parameters, maxq, specifies themaximum number of distinct queries it can issue tothe search engine for each question.
Table 3 showsthe average number of queries actually issued forselect values of maxq.12 Figure 3 shows how per-formance degrades when fewer queries are issuedas a result of lowering maxq for both RISQUE andRISQUE without the hub-page identifier.
It showsthat, with the exception of RISQUE?s performance12Maxq is 10 for the results in Table 2.when only one query is issued,13 the number ofquestions answered have a near-linear relationshipwith the number of queries issued for both sys-tems.
Notice that without the hub-page identifier,RISQUE?s performance when issuing an average of1.93 queries per question is nearly the same as thatof the 2NP baseline, while it performs worse thanthe baseline when issuing only one query per ques-tion.
This is because our iterative query formula-tion process intentionally begins with the most con-strained query, resulting in an empty hit list in manycases.8 Conclusions and Future WorkIn this paper, we described and evaluated RISQUE,a hybrid system for performing natural languagesearch on a large company public website.
RISQUEutilizes a two-pronged approach to generate hit listsfor answering natural language questions.
On theone hand, RISQUE employs a hub-page identi-fier to retrieve, when possible, a hub-page for themost salient NP in the question.
On the otherhand, RISQUE adopts a statistical iterative queryformulation and retrieval mechanism that generatesnew queries by applying transformation rules topreviously-issued queries.
By employing these twocomponents in parallel, RISQUE takes advantagesof both knowledge-driven and machine learning ap-proaches, and achieves an overall 137% relative im-provement in the number of questions correctly an-swered on an unseen test set, compared to a baselineof 2NP keyword queries.In our current work, we are focusing on expand-ing system coverage to other domains.
In particu-lar, we plan to investigate semi-automatic methodsfor extracting ontological knowledge from existingwebpages and databases.AcknowledgmentsWe would like to thank Dave Ferrucci and NandaKambhatla for helpful discussions, Ruchi Kalra andJerry Cwiklik for data preparation, Eric Brown andthe anonymous reviewers for helpful comments onan earlier draft of this paper, as well as Mike Moran13In most cases, this query is issued by the hub-page identi-fier, which has a higher success rate than the queries generatedby the query formulation module.and Alex Holt for providing technical assistancewith ibm.com search.ReferencesG.
Bierner.
2001.
Alternative phrases and natural lan-guage information retrieval.
In Proc.
39th ACL, pages58?65.C.
Clarke, G. Cormack, and T. Lynam.
2001.
Exploit-ing redundancy in question answering.
In Proc.
24thSIGIR, pages 358?365.P.
Hagen, H. Manning, and Y. Paul.
2000.
Must searchstink?
The Forrester Report, June.K.
Hammond, R. Burke, C. Martin, and S. Lytinen.
1995.Faq finder: A case-based approach to knowledge nav-igation.
In AAAI Spring Symposium on InformationGathering in Heterogeneous Environments, pages 69?73.S.
Harabagui, D. Moldovan, M. Pasca, R. Mihalcea,M.
Surdeanu, R. Bunescu, R. Girju, V. Rus, andP.
Morarescu.
2001.
The role of lexico-semantic feed-back in open-domain textual question-answering.
InProc.
39th ACL, pages 274?281.A.
Ittycheriah, M. Franz, W-J.
Zhu, and A. Ratnaparkhi.2001.
Question answering using maximum entropycomponents.
In Proc.
2nd NAACL, pages 33?39.M.
McCord.
1989.
Slot grammar: A system for simplerconstruction of practical natural language grammars.In Natural Language and Logic, pages 118?145.T.
Mitchell.
1997.
Machine Learning.
McGraw Hill.J.
Prager, E. Brown, A. Coden, and D. Radev.
2000.Question-answering by predictive annotation.
In Proc.23rd SIGIR, pages 184?191.N.
Wacholder, Y. Ravin, and M. Choi.
1997.
Disam-biguation of proper names in text.
In Proc.
5th ANLP.C.
Watkins.
1989.
Learning from Delayed Rewards.Ph.D.
thesis, King?s College.
