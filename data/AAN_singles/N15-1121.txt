Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1150?1160,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsHigh-Order Low-Rank Tensors for Semantic Role LabelingTao Lei1, Yuan Zhang1, Llu?
?s M`arquez2, Alessandro Moschitti2, and Regina Barzilay11Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology2ALT Research Group, Qatar Computing Research Institute1{taolei, yuanzh, regina}@csail.mit.edu2{lmarquez, amoschitti}@qf.org.qaAbstractThis paper introduces a tensor-based approachto semantic role labeling (SRL).
The motiva-tion behind the approach is to automaticallyinduce a compact feature representation forwords and their relations, tailoring them tothe task.
In this sense, our dimensionalityreduction method provides a clear alternativeto the traditional feature engineering approachused in SRL.
To capture meaningful interac-tions between the argument, predicate, theirsyntactic path and the corresponding role la-bel, we compress each feature representationfirst to a lower dimensional space prior to as-sessing their interactions.
This corresponds tousing an overall cross-product feature repre-sentation and maintaining associated parame-ters as a four-way low-rank tensor.
The tensorparameters are optimized for the SRL perfor-mance using standard online algorithms.
Ourtensor-based approach rivals the best perform-ing system on the CoNLL-2009 shared task.In addition, we demonstrate that adding therepresentation tensor to a competitive tensor-free model yields 2% absolute increase in F-score.11 IntroductionThe accuracy of Semantic Role Labeling (SRL) sys-tems depends strongly on the features used by theunderlying classifiers.
For instance, the top perform-ing system on the CoNLL?2009 shared task em-ploys over 50 language-specific templates for fea-ture generation (Che et al, 2009).
The templates1Our code is available at https://github.com/taolei87/SRLParser.are manually created and thus offer specific meansof incorporating prior knowledge into the method.However, finding compact, informative templates isdifficult since the relevant signal may be spread overmany correlated features.
Moreover, the use of lex-icalized features, which are inevitably sparse, leadsto overfitting.
In this case it is advantageous to tryto automatically compress the feature set to use asmall number of underlying co-varying dimensions.Dimensionality reduction of this kind can be incor-porated into the classifier directly by utilizing tensorcalculus.
In this paper, we adopt this strategy.We start by building high-dimensional featurevectors that are subsequently mapped into a low-dimensional representation.
Since this high-dimensional representation has to reflect the inter-action between different indicators of semantic rela-tions, we construct it as a cross-product of smallerfeature vectors that capture distinct facets of seman-tic dependence: predicate, argument, syntactic pathand role label.
By compressing this sparse repre-sentation into lower dimensions, we obtain denserepresentations for words (predicate, argument) andtheir connecting paths, uncovering meaningful inter-actions.
The associated parameters are maintainedas a four-way low-rank tensor, and optimized forSRL performance.
Tensor modularity enables us toemploy standard online algorithms for training.Our approach to SRL is inspired by recent suc-cess of our tensor-based approaches in dependencyparsing (Lei et al, 2014).
Applying analogous tech-niques to SRL brings about new challenges, how-ever.
The scoring function needs to reflect the high-order interactions between the predicate, argument,1150their syntactic path and the corresponding role label.Therefore, we parametrize the scoring function as afour-way tensor.
Generalization to high-order ten-sors also requires new initialization and update pro-cedures.
For instance, the SVD initialization usedin our dependency parsing work results in memoryexplosion when extending to our 4-way tensor.
In-stead, we employ the power method (De Lathauweret al, 1995) to build the initial tensor from smallerpieces, one rank-1 component at a time.
For learn-ing, in order to optimize an overall non-convex ob-jective function with respect to the tensor parame-ters, we modify the passive-aggressive algorithm toupdate all the low-rank components in one step.
Theupdate strategy readily generalizes to any high-ordertensor.We evaluate our tensor-based approach forSRL on the CoNLL?2009 shared task benchmarkdatasets of five languages: English, German, Chi-nese, Catalan and Spanish (Surdeanu et al, 2008).As a baseline, we use a simple SRL model that re-lies on a minimal set of standard features.
Our re-sults demonstrate that the tensor-based model out-performs the original SRL model by a significantmargin, yielding absolute improvements of 2.1% F1score.
We also compare our results against the bestperforming system on this task (Zhao et al, 2009a).On three out of five languages, the tensor-basedmodel outperforms this system.
These results areparticularly notable because the system of Zhao etal.
(2009a) employs a rich set of language-specificfeatures carefully engineered for this task.
Finally,we demonstrate that using four-way tensor yieldsbetter performance than its three-way counterpart,highlighting the importance of modeling the relationbetween role labels and properties of the path.2 Related WorkA great deal of SRL research has been dedicated todesigning rich, expressive features.
The initial workby Gildea and Jurafsky (2002) already identified acompact core set of features, which were widelyadopted by the SRL community.
These features de-scribe the predicate, the candidate argument, andthe syntactic relation between them (path).
Earlysystems primarily extended this core set by includ-ing local context lexicalized patterns (e.g., n-grams),several extended representations of the path fea-tures, and some linguistically motivated syntacticpatterns, as the syntactic frame (Surdeanu et al,2003; Xue and Palmer, 2004; Pradhan et al, 2005).More recent approaches explored a broader rangeof features.
Among others, Toutanova et al (2008),Martins and Almeida (2014) and Yang and Zong(2014) have explored high-order features involvingseveral arguments and even pairs of sentence pred-icates.
Other approaches have focused on seman-tic generalizations of lexical features using selec-tional preferences, neural network embeddings orlatent word language models (Zapirain et al, 2013;Collobert et al, 2011; Deschacht and Moens, 2009;Roth and Woodsend, 2014).
To avoid the intensivefeature engineering inherent in SRL, Moschitti etal.
(2008) employ kernel learning.
Although attrac-tive from this perspective, the kernel-based approachcomes with a high computational cost.
In contrastto prior work, our approach effectively learns low-dimensional representation of words and their roles,eliminating the need for heavy manual feature en-gineering.
Finally, system combination approachessuch as reranking typically outperform individualsystems (Bj?orkelund et al, 2010).
Our method canbe easily integrated as a component in one of thosesystems.In technical terms, our work builds on our recenttensor-based approach for dependency parsing (Leiet al, 2014).
In that work, we use a three-way ten-sor to score candidate dependency relations within afirst-order scoring function.
The tensor captures theinteraction between words and their syntactic (head-modifier) relations.
In contrast, the scoring functionin SRL involves higher-order interactions betweenthe path, argument, predicate and their associatedrole label.
Therefore, we parametrized the scoringfunction with a four-way low-rank tensor.
To helpwith this extension, we developed a new initializa-tion and update strategy.
Our experimental resultsdemonstrate that the new representation tailored toSRL outperforms previous approaches.3 Problem FormulationOur setup follows the CoNLL?2009 sharedtask (Haji?c et al, 2009).
Each token in sentence xis annotated with a predicted POS tag and predicted1151UNESCO      is         holding       its          meetings        in         ParisA0 A1SBJ VC OBJNMODAM-LOCLOCPMODFigure 1: Example sentence from the CoNLL?2009dataset annotated with syntactic and semantic de-pendencies.
The lower graph is the syntactic depen-dency tree for the sentence.
The upper part containsthe semantic dependencies for the predicate ?hold-ing?.word lemma.
Some tokens are also marked aspredicates, i.e., argument-bearing tokens.
The goalis to determine the semantic dependencies for eachpredicate pi(cf.
upper part of Figure 1).
These de-pendencies identify the arguments of each predicateand their role labels.
In this work, we focus onlyon the semantic side ?
that is, identification andclassification of predicate arguments.
To this end,our system takes as input a syntactic dependencytree ysynderived from a state-of-the-art parser(bottom part of Figure 1).More formally, let {pi} ?
x be the set of ver-bal and nominal predicates in the sentence.
For eachpredicate pi(e.g., ?holding?
), our goal is to predicttuples (pi, aij, rij) specifying the semantic depen-dency arcs, where aij?
x is one argument (e.g.,?meetings?
), and rijis the corresponding semanticrole label (e.g., A1).
The semantic parse is then thecollection of predicted arcs zsem= {(pi, aij, rij)}.We decouple syntactic and semantic inferenceproblems into two separate steps.
We first run oursyntactic dependency parser RBGParser2to obtainthe syntactic dependency tree ysyn.
The semanticparse zsemis then found conditionally on the syntac-tic part:z?sem= arg maxzsemSsem(x,ysyn, zsem), (1)Here Ssem(?)
is the parametrized scoring function tobe learned.
We build our scoring function by com-bining a traditional feature scoring function with atensor-based scoring function.2https://github.com/taolei87/RBGParserPredicate word PathPredicate POS Path + arg.
POSArgument word Path + pred.
POSArgument POS Path + arg.
wordPred.
+ arg.
words Path + pred.
wordPred.
+ arg.
POS Voice + pred.
+ arg.
POSVoice + pred.
word Voice + pred.
POSTable 1: Templates for first-order semantic features.These features are also (optionally) combined withrole labels.3.1 Traditional Scoring UsingManually-designed FeaturesIn a typical feature-based approach (Johansson,2009; Che et al, 2009), feature templates give riseto rich feature descriptions of the semantic structure.The score Ssem(x,ysyn, zsem) is then defined as theinner product between the parameter vector and thefeature vector.
In the first-order arc-factored case,Ssem(x,ysyn, zsem) = w ?
?
(x,ysyn, zsem)=?
(p,a,r)?zsemw ?
?
(p, a, r),where w are the model parameters and ?
(p, a, r) isthe feature vector representing a single semantic arc(p, a, r) (we suppress its dependence on x and ysyn).We also experiment with second order features, i.e.,considering two arguments associated with the samepredicate, or two predicates sharing the same tokenas argument.For the arc-factored model, there are mainly fourtypes of atomic information that define the arc fea-tures in ?
(p, a, r):(a) the predicate token p (and its local context);(b) the argument token a (and its local context);(c) the dependency label path that connects p anda in the syntactic tree;(d) the semantic role label r of the arc.These pieces of atomic information are either useddirectly or combined as unigram up to 4-gram fea-tures into traditional models.
To avoid heavy fea-ture engineering and overfitting, we use a light andcompact feature set derived from the information in(a)?(d).
Table 1 shows the complete list of feature1152templates, used as our first-order semantic baselinein the experiments.3.2 Low-rank Scoring via ProjectedRepresentationsNow, we describe the tensor-based scoring function.We characterize each semantic arc (p, a, r) using thecross-product of atomic feature vectors associatedwith the four types of information described above:the predicate vector ?
(p), the argument vector ?
(a),the dependency path vector ?
(path) and the seman-tic role label vector ?(r).
For example, in the sim-plest case ?(p),?
(a) ?
[0, 1]nare one-hot indica-tor vectors, where n is the size of the vocabulary.Similarly, ?
(path) ?
[0, 1]mand ?
(r) ?
[0, 1]lareindicator vectors where m is the number of uniquepaths (seen in the training set) and l is the numberof semantic role labels.
Of course, we can add otheratomic information into these atomic vectors.
Forexample, ?
(p) will not only indicate the word formof the current predicate p, but also the word lemma,POS tag and surrounding tokens as well.
The cross-product of these four vectors is an extremely high-dimensional rank-1 tensor,?(p)?
?(a)?
?(path)?
?
(r) ?
Rn?n?m?lin which each entry indicates the combination offour atomic features appearing in the semantic arc(p, a, r)3.
The rank-1 tensor (cross-product) cap-tures all possible combinations over atomic units,and therefore it is a full feature expansion over themanually selected feature set in Table 1.
Similar tothe traditional scoring, the semantic arc score is theinner product between a 4-way parameter tensor Aand this feature tensor:A ?
Rn?n?m?l:vec(A) ?
vec (?(p)?
?(a)?
?(path)?
?
(r)) ,(2)where vec(?)
denotes the vector representation of amatrix / tensor.Instead of reducing and pruning possible featureconcatenations (e.g., by manual feature template3We always add a bias term into these atomic vectors (e.g., afixed ?1?
attached to the beginning of every vector).
Therefore,their cross-product will contain all unigram to 4-gram concate-nations, not just 4-gram concatenations.construction as in the traditional approach), this ten-sor scoring method avoids parameter explosion andoverfitting by assuming a low-rank factorization ofthe parameters A.
Specifically, A is decomposedinto the sum of k simple rank-1 components,A =k?i=1P (i)?Q(i)?R(i)?
S(i).
(3)Here k is a small constant, P,Q ?
Rk?n, R ?Rk?mand S ?
Rk?lare parameter matrices, andP (i) (and similarly Q(i), R(i) and S(i)) representsthe i-th row vector of matrix P .The advantages of this low-rank assumption areas follows.
First, computing the score no longer re-quires maintaining and constructing extremely largetensors.
Instead, we can project atomic vectors viaP , Q, R and S obtaining small dense vectors, andsubsequently calculating the arc score byk?i=1[P?(p)]i[Q?(a)]i[R?(path)]i[S?
(r)]i.Second, projecting atomic units such as words, POStags and labels into dense, low-dimensional vectorscan effectively alleviate the sparsity problem, and itenables the model to capture high-order feature in-teractions between atomic units, while avoiding theparameter explosion problem.3.3 Combined SystemSimilar to our low-rank syntactic dependency pars-ing model (Lei et al, 2014), our final scoring func-tion Ssem(x,ysyn, zsem) is the combination of the tra-ditional scoring and the low-rank scoring,Ssem(x,ysyn, zsem) =?
w ?
?
(x,ysyn, zsem) + (1?
?)?(p,a,r)?zsemk?i=1[P?(p)]i[Q?(a)]i[R?(path)]i[S?
(r)]i.where ?
?
[0, 1] is a hyper-parameter balancing thetwo scoring terms.
We tune this value on the de-velopment set.
Finally, the set of parameters of ourmodel is denoted as ?
= {w, P,Q,R, S}.
Our goalis to optimize the weight vector w as well as the fourprojection matrices given the training set.11534 LearningWe now describe the learning method for our SRLmodel.
Let D = {(?x(i),?ysyn(i),?zsem(i))}Ni=1be thecollection of N training samples.
The values of theset of parameters ?
= {w, P,Q,R, S} are estimatedon the basis of this training set.
Following standardpractice, we optimize the parameter values in a max-imum soft-margin framework.
That is, for the givensentence?x and the corresponding syntactic tree?ysyn,we adjust parameter values to separate gold seman-tic parse and other incorrect alternatives:?zsem?
Z(?x,?ysyn) :Ssem(?x,?ysyn,?zsem) ?
Ssem(?x,?ysyn, zsem)+ cost(?zsem, zsem) (4)where Z(?x,?ysyn) represent the set of all possible se-mantic parses, and cost(?zsem, zsem) is a non-negativefunction representing the structural difference be-tween?zsemand zsem.
The cost is zero when zsem=?zsem, otherwise it becomes positive and therefore isthe ?margin?
to separate the two parses.
Follow-ing previous work (Johansson, 2009; Martins andAlmeida, 2014), this cost function is defined as thesum of arc errors ?
we add 1.0 for each false-positivearc, 2.0 for each false-negative arc (a missing arc)and 0.5 if the predicate-argument pair (p, a) is inboth parses but the semantic role label r is incorrect.4.1 Online UpdateThe parameters are updated successively after eachtraining sentence.
Each update first checks whetherthe constraint (4) is violated.
This requires ?cost-augmented decoding?
to find the maximum viola-tion with respect to the gold semantic parse:?zsem= arg maxzsemSsem(?x,?ysyn, zsem)+ cost(?zsem, zsem)When the constraint (4) is violated (i.e.
?zsem6=?zsem), we seek a parameter update ??
to fix this vi-olation.
In other words, we define the hinge loss forthis example as follows,loss(?)
= max{ 0, Ssem(?x,?ysyn,?zsem)+ cost(?zsem,?zsem)?
Ssem(?x,?ysyn,?zsem) }and we revise the parameter values to minimize thisloss function.Since this loss function is neither linear nor con-vex with respect to the parameters ?
(more preciselythe low-rank component matrices P , Q, R and S),we can use the same alternating passive-aggressive(PA) update strategy in our previous work (Lei etal., 2014) to update one parameter matrix at onetime while fixing the other matrices.
However,as we demonstrated later, modifying the passive-aggressive algorithm slightly can give us a joint up-date over all components in ?.
Our preliminary ex-periment shows this modified version achieves betterresults compared to the alternating PA.4.2 Joint PA Update for TensorThe original passive-aggressive parameter update??
is derived for a linear, convex loss function bysolving a quadatic optimization problem.
Althoughour scoring function Ssem(?)
is not linear, we cansimply approximate it with its first-order Taylor ex-pansion:S(x,y, z;?
+ ??)
?
S(x,y, z;?)
+dSd???
?In fact, by plugging this into the hinge loss func-tion and the quadratic optimization problem, we geta joint closed-form update which can be simply de-scribed as,??
= max{C,loss(?)?g??2}g?whereg?
=dSd?(?x,?ysyn,?zsem)?dSd?
(?x,?ysyn,?zsem),and C is a regularization hyper-parameter control-ling the maximum step size of each update.
Notethat ?
is the set of all parameters, the update jointlyadjusts all low-rank matrices and the traditionalweight vector.
The PA update is ?adaptive?
in thesense that its step size is propotional to the loss(?
)of the current training sample.
Therefore the stepsize is adaptively decreased as the model fits thetraining data.4.3 Tensor InitializationSince the scoring and loss function with high-ordertensor components is highly non-convex, our model1154performance can be impacted by the initialization ofthe matrices P , Q, R and S. In addition to intial-izing these low-rank components randomly, we alsoexperiment with a strategy to provide a good guessof the low-rank tensor.First, note that the traditional manually-selectedfeature set (i.e., ?
(p, a, r) in our notation) is an ex-pressive and informative subset of the huge featureexpansion covered in the feature tensor.
We can trainour model using only the manual feature set and thenuse the corresponding feature weights w to intializethe tensor.
Specifically, we create a sparse tensorT ?
Rn?n?m?lby putting each parameter weightin w into its corresponding entry in T .
We then tryto find a low-rank approximation of sparse tensor Tby approximately minimizing the squared error:minP,Q,R,S?T ?
?iP (i)?Q(i)?R(i)?
S(i)?22In the low-rank dependency parsing work (Lei etal., 2014), this is achieved by unfolding the sparsetensor T into a n?
nml matrix and taking the SVDto get the top low-rank components.
Unfortunatelythis strategy does not apply in our case (and otherhigh-order tensor cases) because even the numberof columns in the unfolded matrix is huge, nml >1011, and simply taking the SVD would fail becauseof memory limits.Instead, we adopt the generalized high-orderpower method, a.k.a.
power iteration (De Lathauweret al, 1995), to incrementally obtain the most im-portant rank-1 component one-by-one ?
P (i), Q(i),R(i) and S(i) for each i = 1..k. This method isa very simple iterative algorithm and is used to findthe largest eigenvalues and eigenvectors (or singularvalues and vectors in SVD case) of a matrix.
Its gen-eralization directly applies to our high-order tensorcase.5 Implementation DetailsDecoding Following Llu?
?s et al (2013), the de-coding of SRL is formulated as a bipartite maximumassignment problem, where we assign arguments tosemantic roles for each predicate.
We use the maxi-mum weighted assignment algorithm (Kuhn, 1955).For syntactic dependency parsing, we employ therandomized hill-climbing algorithm from our previ-ous work (Zhang et al, 2014).Input: sparse tensor T , rank number iand fixed rank-1 components P (j), Q(j),R(j) and S(j) for j = 1..(i?
1)Output: new component P (i),Q(i),R(i) andS(i).1: Randomly initialize four unit vectors p, q, rand s2: T?= T ?
?jP (j)?Q(j)?R(j)?
S(j)3: repeat4: p = ?T?,?, q, r, s?
and normalize it5: q = ?T?, p,?, r, s?
and normalize it6: r = ?T?, p, q,?, s?
and normalize it7: s = ?T?, p, q, r,?
?8: norm = ?s?229: until norm converges10: P (i) = p and Q(i) = q11: R(i) = r and S(i) = sFigure 2: The iterative power method for high-order tensor initialization.
The operator p =?T?,?, q, r, s?
is the multiplication between thetensor and three vectors, defined as pi=?jklTijklqjrksl.
Similarly, qj=?iklTijklpirksletc.Features Table 1 summarizes the first-order fea-ture templates.
These features are mainly drawnfrom previous work (Johansson, 2009).
In addition,we extend each template with the argument label.Table 2 summarizes the atomic features used in?
(p) and ?
(a) for the tensor component.
For eachpredicate or argument, the feature vector includes itsword form and POS tag, as well as the POS tags ofthe context words.
We also add unsupervised wordembeddings learned on raw corpus.4For atomicvectors ?
(path) and ?
(r) representing the path andthe semantic role label, we use the indicator featureand a bias term.6 Experimental SetupDataset We evaluate our model on the Englishdataset and other 4 datasets in the CoNLL-2009shared task (Surdeanu et al, 2008).
We use the4https://github.com/wolet/sprml13-word-embeddings1155word word-l word-rpos pos-l pos-rpos-l + pos pos + pos-r pos + wordpos-l + pos + pos-r voice embeddingsTable 2: Predicate/argument atomic features used byour tensor for SRL.
word stands for the word form(and also lemma), pos stands for the predicted POStag and voice stands for the voice of the predicate.The suffixes -l and -r refer to the left and right ofthe current token respectively.
For example, pos-lmeans the POS tag to the left of the current word inthe sentence.official split for training, development and testing.For English, the data is mainly drawn from the WallStreet Journal.
In addition, a subset of the Browncorpus is used as the secondary out-of-domain testset, in order to evaluate how well the model gen-eralizes to a different domain.
Following the offi-cial practice, we use predicted POS tags, lemmasand morphological analysis provided in the datasetacross all our experiments.
The predicates in eachsentence are also given during both training and test-ing.
However, we neither predict nor use the sensefor each predicate.Systems for Comparisons We compare againstthree systems that achieve the top average perfor-mance in the joint syntactic and semantic parsingtrack of the CoNLL-2009 shared task (Che et al,2009; Zhao et al, 2009a; Gesmundo et al, 2009).All approaches extensively explored rich featuresfor the SRL task.
We also compare with the state-of-the-art parser (Bj?orkelund et al, 2010) for En-glish, an improved version of systems participated inCoNLL-2009.
This system combines the pipeline ofdependency parser and semantic role labeler with aglobal reranker.
Finally, we compare with the recentapproach which employs distributional word repre-sentations for SRL (Roth and Woodsend, 2014).
Wedirectly obtain the outputs of all these systems fromthe CoNLL-2009 website5or the authors.Model Variants Our full model utilizes 4-waytensor component and a standard feature set5http://ufal.mff.cuni.cz/conll2009-st/results/results.phpfrom (Johansson, 2009).
We also compare againstour model without the tensor component, as well asa variant with a 3-way tensor by combining the pathand semantic role label parts into a single mode (di-mension).Evaluation Measures Following standard prac-tice in the SRL evaluation, we measure the perfor-mance using labeled F-score.
To this end, we applythe evaluation script provided on the official web-site.6The standard evaluation script considers thepredicate sense prediction as a special kind of se-mantic label.7Since we are neither predicting norusing the predicate sense information, we excludethis information in most of the evaluation.
In addi-tion, we combine the predicate sense classificationoutput of (Bj?orkelund et al, 2010) with our seman-tic role labeling output, to provide results directlycomparable to previous reported numbers.Experimental Details Across all experiments, wefix the rank of the tensor to 50 and train our modelfor a maximum of 20 epochs.
Following com-mon practice, we average parameters over all it-erations.
For each experimental setting, we tunethe hyper-parameter ?
?
{0.3, 0.5, 0.7, 0.9} andC ?
{0.01, 0.1, 1} on the development set and applythe best model on the test set.
Each model is eval-uated on the development set after every epoch topick the the best number of training epoch.
For theexperiments with random initialization on the ten-sor component, the vectors are initialized as randomunit vectors.
We combine our SRL model with oursyntactic dependency parser, RBGParser v1.1 (Leiet al, 2014), for joint syntactic and semantic pars-ing.
The labeled attachment score (LAS) of RBG-Parser is 90.4 on English, when we train the ?stan-dard?
model type using the unsupervised word vec-tors.7 ResultsWe first report the performance of our methodsand other state-of-the-art SRL systems on Englishdatasets (See Table 3).
We single out performance6http://ufal.mff.cuni.cz/conll2009-st/scorer.html7Note that the original script includes such prediction in theF-score calculation, although the predicate sense is typicallypredicted in a separate step before semantic label classification.1156ModelExcluding predicate senses Including predicate sensesWSJ-dev WSJ-test Brown-test WSJ-test Brown-test1st-order w/o tensor 79.42 80.84 69.38 85.46 74.66+ 3-way tensor 80.77 82.19 69.76 86.34 74.94+ 4-way tensor 81.03 82.51* 70.77 86.58* 75.57CoNLL-2009 1st place ?
82.08 69.84 86.15 74.58CoNLL-2009 2nd place ?
81.20 68.86 85.51 73.82CoNLL-2009 3rd place ?
78.66 65.89 83.24 70.65(Roth and Woodsend, 2014) ?
80.87 69.33 85.50 74.67(Bj?orkelund et al, 2010) 78.85 81.35 68.34 85.80 73.92Model + Reranker WSJ-dev WSJ-test Brown-test WSJ-test Brown-test(Roth and Woodsend, 2014) + reranking ?
82.10 71.12 86.34 75.88(Bj?orkelund et al, 2010) + reranking 80.50 82.87 70.91 86.86 75.71Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL sharedtask.
We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRLfeatures.
For the latter, we consider implementations with 3-way and 4-way tensors.
Winning systems (withand without a reranker) are marked in bold.
Statistical significance with p < 0.05 is marked with ?.on English corpora because these datasets are mostcommonly used for system evalutation.
As a sin-gle system without reranking, our model outper-forms the five top performing systems (second blockin Table 3) on both in-domain and out-of-domaindatasets.
The improvement from the F-score of82.08% to our result 82.51% on the WSJ in-domaintest set is significant with p < 0.05, which is com-puted using a randomized test tool8based on Yeh(2000).
For comparison purposes, we also reportF-score performance when predicate senses are in-cluded in evaluation.
The relative performancebetween the systems is consistent independent ofwhether the predicate senses are included or ex-cluded.Table 4 shows the results of our system on otherlanguages in the CoNLL-2009 shared task.
Outof five languages, our model rivals the best per-forming system on three languages, achieving sta-tistically significant gains on English and Chinese.Note that our model uses the same feature config-uration for all the languages.
In contrast, Zhao etal.
(2009b) rely on language-specific configurationsobtained via ?huge feature engineering?
(as noted bythe authors).Results in Table 3 and 4 also highlight the con-8http://www.nlpado.de/?sebastian/software/sigf.shtmlWSJ-test Brown-test1st-orderw/o tensor80.84 69.38+ 3-waytensorRnd.
Init.
81.87 69.82PM.
Init.
82.19 69.76+ 4-waytensorRnd.
Init.
81.63 70.63PM.
Init.
82.51 70.77Table 5: SRL labeled F-score for different initializa-tion strategies of the first order model.
Rnd standsfor the random initialization, and PM for the powermethod initialization.tribution of the tensor to the model performance,which is consistent across languages.
Without thetensor component, our system trails the top two per-forming systems.
However, adding the tensor com-ponent provides on average 2.1% absolute gain, re-sulting in competitive performance.
The mode ofthe tensor also contributes to the performance ?
the4-way tensor model performs better than the 3-waycounterpart, demonstrating the importance of mod-eling the interactions between dependency paths andsemantic role labels.Table 5 shows the impact of initialization on theperformance of the tensor-based model.
The initial-ization based on the power method yields superiorresults compared to random initialization, for both1157LanguageTest setOurs(4-way tensor)Ours(no tensor)CoNLL 1st CoNLL 2ndEnglish 82.51* 80.84 82.08 81.20Catalan 74.67 71.86 76.78* 74.02Chinese 69.16* 68.43 68.52 68.71German 76.94 74.03 74.65 76.27Spanish 75.58 72.85 77.33* 74.01Average 75.77 73.60 75.87 74.84Table 4: Semantic labeled F-score excluding predicate senses on 5 languages in the CoNLL-2009 sharedtask.
Statistical significance with p < 0.05 is marked with ?.
Adding the tensor leads to more than 2%absolute gain on average F-score.
Our method with the same feature configuration (a standard set + 4-way tensor) rivals the best CoNLL-2009 system which explores much richer feature sets, language-specificfeature engineering, and n-best parse combination (Zhao et al, 2009a).Our method WSJ-test Gain1st order w/o tensor 80.84 ?+ 4-way tensor 82.51 +1.67+ 3-way tensor 82.19 +1.35(Roth and Woodsend) WSJ-test Gainoriginal baseline 80.38 ?+ pred & arg 80.23 -0.15+ deppath 80.63 +0.25+ span 80.87 +0.49Table 6: Comparision between our low-rank ten-sor method and (Roth and Woodsend, 2014) forleveraing word compositions.3-way and 4-way tensors.
However, random initial-ization still delivers reasonable performance, outper-forming the tensor-free model by more than 1% inF-score.Finally, we compare our tensor-based approachagainst a simpler model that captures interactionsbetween predicate, argument and syntactic path us-ing word embeddings (Roth and Woodsend, 2014).Table 6 demonstrates that modeling feature inter-actions using tensor yields higher gains than usingword embeddings alone.
For instance, the highestgain achieved by Roth and Woodsend (2014) whenthe embeddings of the arguments are averaged is0.5%, compared to 1.6% obtained by our model.8 ConclusionsIn this paper we introduce a tensor-based approachto SRL that induces a compact feature representa-tion for words and their relations.
In this sense, ourdimensionality reduction method provides a clearalternative to a traditional feature engineering ap-proach used in SRL.
Augmenting a simple, yetcompetitive SRL model with the tensor componentyields significant performance gains.
We demon-strate that our full model outperforms the best per-forming systems on the CoNLL-2009 shared task.AcknowledgmentsThe authors acknowledge the support of the MURIprogram (W911NF-10-1-0533) and the DARPABOLT program.
This research is developed in a col-laboration of MIT with the Arabic Language Tech-nologies (ALT) group at Qatar Computing ResearchInstitute (QCRI) within the Interactive sYstems forAnswer Search (IYAS) project.
We are grateful toAnders Bj?okelund and Michael Roth for providingthe outputs of their systems.
We thank Yu Xin,Tommi Jaakkola, the MIT NLP group and the ACLreviewers for their comments.
Any opinions, find-ings, conclusions, or recommendations expressed inthis paper are those of the authors, and do not neces-sarily reflect the views of the funding organizations.1158ReferencesAnders Bj?orkelund, Bernd Bohnet, Love Hafdell, andPierre Nugues.
2010.
A high-performance syntacticand semantic dependency parser.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics: Demonstrations.
Association for Compu-tational Linguistics.Wanxiang Che, Zhenghua Li, Yongqiang Li, YuhangGuo, Bing Qin, and Ting Liu.
2009.
Multilingualdependency-based syntactic and semantic parsing.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning (CoNLL 2009):Shared Task, pages 49?54, Boulder, Colorado, June.Association for Computational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,11(Aug):2493?2537.Lieven De Lathauwer, Pierre Comon, Bart De Moor, andJoos Vandewalle.
1995.
Higher-order power method.Nonlinear Theory and its Applications, NOLTA95, 1.Koen Deschacht and Marie-Francine Moens.
2009.Semi-supervised semantic role labeling using the La-tent Words Language Model.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 21?29, Singapore, Au-gust.
Association for Computational Linguistics.Andrea Gesmundo, James Henderson, Paola Merlo, andIvan Titov.
2009.
A latent variable model of syn-chronous syntactic-semantic parsing for multiple lan-guages.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning:Shared Task.
Association for Computational Linguis-tics.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2009): Shared Task, pages1?18, Boulder, Colorado, June.
Association for Com-putational Linguistics.Richard Johansson.
2009.
Statistical bistratal depen-dency parsing.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing, pages 561?569, Singapore.Harold W Kuhn.
1955.
The hungarian method for the as-signment problem.
Naval research logistics quarterly,2(1-2):83?97.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1381?1391, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Xavier Llu?
?s, Xavier Carreras, and Llu?
?s M`arquez.
2013.Joint arc-factored parsing of syntactic and semanticdependencies.
Transactions of the Association forComputational Linguistics, 1.Andr?e F. T. Martins and Mariana S. C. Almeida.
2014.Priberam: A turbo semantic parser with second or-der features.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval 2014),pages 471?476, Dublin, Ireland, August.
Associationfor Computational Linguistics and Dublin City Uni-versity.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Jurafsky.2005.
Support vector learning for semantic argumentclassification.
Machine Learning, 60(1):11?39.Michael Roth and Kristian Woodsend.
2014.
Compo-sition of word representations improves semantic rolelabelling.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP).
Association for Computational Linguistics.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 8?15, Sapporo, Japan.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
Theconll-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe Twelfth Conference on Computational NaturalLanguage Learning, pages 159?177.
Association forComputational Linguistics.Kristina Toutanova, Aria Haghighi, and ChristopherManning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2):161?191.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 88?94, Barcelona, Spain, July.
Association forComputational Linguistics.1159Haitong Yang and Chengqing Zong.
2014.
Multi-predicate semantic role labeling.
In Proceedings ofthe 2014 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 363?373,Doha, Qatar, October.
Association for ComputationalLinguistics.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th conference on Computational linguistics-Volume 2.
Association for Computational Linguistics.Benat Zapirain, Eneko Agirre, Llu?
?s M`arquez, and Mi-hai Surdeanu.
2013.
Selectional preferences for se-mantic role classification.
Computational Linguistics,39(3):631?664.Yuan Zhang, Tao Lei, Regina Barzilay, and TommiJaakkola.
2014.
Greed is good if randomized: Newinference for dependency parsing.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP).
Association forComputational Linguistics.Hai Zhao, Wenliang Chen, Jun?ichi Kazama, KiyotakaUchimoto, and Kentaro Torisawa.
2009a.
Multi-lingual dependency learning: Exploiting rich featuresfor tagging syntactic and semantic dependencies.
InProceedings of the Thirteenth Conference on Com-putational Natural Language Learning: Shared Task,pages 61?66.
Association for Computational Linguis-tics.Hai Zhao, Wenliang Chen, Chunyu Kity, and GuodongZhou.
2009b.
Multilingual dependency learning: Ahuge feature engineering method to semantic depen-dency parsing.
In Proceedings of the Thirteenth Con-ference on Computational Natural Language Learning(CoNLL 2009): Shared Task.
Association for Compu-tational Linguistics.1160
