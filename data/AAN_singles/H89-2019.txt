A PROPOSAL FOR SLS EVALUATIONSean BoisenLance RamshawDamafis AyusoMadeleine BatesBBN Systems and Technologies CorporationCambr idge,  MA 02138ABSTRACTThis paper proposes an automatic, essentially domain-independent means of evaluating Spoken Language Sys-tems (SLS) which combines oftware we have devel-oped for that purpose (the "Comparator") and a set ofspecifications for answer expressions (the "Common An-swer Specification", or CAS).
The Comparator checkswhether the answer provided by a SLS accords with acanonical answer, returning either true or false.
TheCommon Answer Specification determines the syntax ofanswer expressions, the minimal content hat must beincluded in them, the data to be included in and ex-cluded from test corpora, and the procedures used bythe Comparator.
Though some details of the CAS areparticular to individual domains, the Comparator soft-ware is domain-independent, as is the CAS approach.1 INTRODUCTIONThe DARPA community has recently moved forwardin beginning to define methods for common evaluationof spoken language systems.
We consider the existingconsensus to include at least the following points:?
Common evaluation involves working on a com-mon domain (or domains).
A common corpusof development queries (in both spoken and tran-scribed form), and answers to those queries in somecanonical format, are therefore required.?
One basis for system evaluation will be answers toqueries from a common database, perhaps in addi-tion to other measures.?
Automatic evaluation methods should be usedwhenever they are feasible.?
System output will be scored by NIST, though allsites will be able to use the evaluation program in-ternally.?
Development and test corpora should be subdividedinto several categories to support different kinds ofevaluation (particularly concerning discourse phe-nomena).An implicit assumption here is that we are consider-ing database query systems, rather than any of the vari-ous other natural language processing domains (messageunderstanding, command and control, etc.).
Evaluatingsystems for these other domains will naturally requireother evaluation procedures.Building on the points of consensus listed above,this proposal presents an evaluation procedure for theDARPA Common Task which is essentially domain-independent.
The key component is a program, desig-nated the Comparator, for comparing canonical answersto the answers upplied by a Spoken Language System.A specification for such answers, which incorporates therequirements of the Comparator, is presented in Sec-tion 2.
This specification, called the Common AnswerSpecification (CAS), is not intended to be suitable forinteractive systems: rather, it is designed to facilitate au-tomatic evaluation.
While we have attempted to coveras broad a range of queries and phenomena as possible,data which fall outside the scope of the CAS can simplybe left out of test corpora for now.Section 3 presents ome of the justification supportingthe proposal in Section 2, as well as amplifying severalpoints.
Details on the Comparator are given in Section 4.Section 5 concludes with a discussion of corpus devel-opment, looking at what kind of data should be collectedand how corpora should be annotated to facilitate testingvarious types of natural language.1352 THE PROPOSALHere we present he basic substance of our proposal fora Common Answer Specification (CAS), deferring somedetails and elaboration to Section 3.
The CAS, which isdesigned to support system evaluation using the Com-parator, covers four basic areas."1.
The notation used for answer expressions.2.
The minimal content of canonical answers.3.
The material included in test corpora.4.
The procedure used by the Comparator for compar-ing canonical answers to system output.We assume an evaluation architecture like that in Fig-tire 1.
Everything on the right hand side of the figure isthe developer's responsibility 1.
Items on the left side ofthe diagram will be provided as part of the evaluationprocess.The Common Answer Specification was devised to en-able common, automatic evaluation: it is expressly notdesigned to meet the needs of human users, and shouldbe considered external.
This means that developers arefree to implement any output format they find conve-nient for their own use: we only propose to dictate theform of what is supplied as input to the Comparator.
Itis assumed that some simple post-processing of systemoutput by the developer will be required to conform tothe CAS.While the central points of the CAS are domain-independent, certain details relating to content can onlybe determined relative to a specific domain.
These por-tions of the proposal are specified for the personnel do-main and SLS Personnel Database (Boisen, 1989), andall examples are taken from that domain as well.2.1 NotationS-1 The basic CAS expression will be a relation, thatis, a set of tuples.
The types of the elements of tupleswill be one of the following: Boolean, number, string,or date.
Relations consisting of a single tuple with onlyone element can alternatively be represented as a scalar.1BBN has offered their ERL interpreter (Ramshaw, 1989) as abackend atabase interface for those who desire one: use of the ERLinterpreter is explicitly not required for common evaluation, however,and developers are free to use whatever database interface they findsuitable.S-2 CAS expressions will be expressed as a Lisp-styleparenthesized list of non-empty lists.
Scalar answersmay alternatively be represented as atomic expressions.The two Boolean values are t rue  and fa l se .
Numericanswers can be either integers or real numbers.
Dateswill be an 9-character string like "01- JAN-80" .
Thenumber and types of the elements of tuples must be thesame across tuples.
An empty relation will be repre-sented by the empty list.
Alphabetic ase, and white-space between elements, will be disregarded, except instrings.A BNF specification for the syntax of the CommonAnswer Specification is found in Appendix A.
Here aresome examples of answers in the CAS format:((false))FALSE2.9999999999(( 3 ))"04-JUL-89"( (2341 "SMITH") (5573 "JONES") )()2.2 Minimal ContentCertain queries only require scalar answers, among themyes/no questions, imperatives like "Count" and "Sum",questions like "How much/many __  ?
", "How longago __  ?
", and "When __  ?".
Other queries mayrequire a relation for an answer: for these cases, theCAS must specify (in linguistic terms) which of the en-titles referred to in the English expression are requiredto be included in the answer.
For required entities, it isalso necessary to specify the database fields that identifythem.S-3 For WH-questions, the required entity is the syn-tactic head of the WH noun phrase.
For imperatives,the required NP is the head of the object NP.
The nouns"'list", "'table", and "'display" will not be considered"true" heads.Examples: For the query "Which chiefscientists in department 45 make more than$70000?
", the required entity is the scientists,not the department or their salaries.In the case of "Give me every employee'sphone number", the only required entity is thephone number.136S SComparator FSLS kernel_l backendI CAS formatting IFigure 1: The evaluation processFor "Show me a list of all the depart-ments", the required entity is the departments,not a list.For the query "Count the people in depart-ment 43', only the scalar value is required.Entities often can be identified by more than onedatabase field: for example, a person could be repre-sented by their employee identification number, their firstand last names, their Social Security Number, etc.
Forany given domain, therefore, the fields that identify enti-ties must be determined.
If only one database identifieris available (i.e., only the field sa la ry  can representa person's alary), the choice is clear: in other cases, itmust be stipulated.S--4 (Personnel) In any evaluation domain,canonical database identifiers for entities in the domainmust be determined.
For the personnel domain, Table 1will specify the database fields that are canonical identi-fiers for entities with multiple database representations.Example: For the query "List department 44employees", the required database field isempXoyee- id ,  so a suitable answer wouldbe((4322) (5267) .
.
.
)where 4322, 5267, etc.
are employee identifi-cation numbers.Certain English expressions in the personnel domainare vague, in that they provide insufficient informationto determine their interpretation.
We therefore stipulatehow such expressions are to be construed.S-5 (Personnel) In any evaluation domain, the inter-pretation of vague references must be determined.
Forthe personnel domain, Table 2 will designate the refer-ents for several vague nominal expressions.Example: For the query "What is Paul Tai'sphone number?
", the expression will be inter-preted as a request for Tai's work phone num-ber, not his home phone.137entity database fieldemployees employee-idcountries country-codedegrees degree,employee-id,school-codedepartments departmentdivisions divisionmajors major-codeschools school-codestates state-codeTable 1: Canonical field identifiers for domain entitieswith multiple database representations.expression I database fieldtelephone work -phonename (of a person) Zast -nanmaddress streetTable 2: Database fields for vague nominal expressions.Another case involves the specificity of job rifles.In the Common Personnel Database, there are severaljob rifles that are related to a specific type of pro-fession: for example, "STAFF SCIENTIST", "SCIEN-TIST", "CHIEF SCIENTIST", etc.
We propose that allthese be considered scientists in the generic sense forqueries like "Is Mary Graham a scientist?"
and "Howmany scientists are there in department 45?
".S--6 (Personnel) Someone will be considered a scien-tist if and only if that person's j ob- t l t le  contains thestring "SCIENTIST".
The same is true for the followinggeneric profession titles: engineer, programmer, clerk,and accountant.
The terms manager and supervisor willbe treated interchangeably, and someone will be consid-ered a manager or supervisor only if that person is thesupervisor of some other person.2.3 Test  CorporaS--7 The primary corpus will be composed of sim-ple pairs of queries and their answers.
In addition,a distinct corpus will be used for evaluating the sim-ple discourse capability of reference to an immediatelypreceding query.
This corpus will contain triplets of( querltl , query ,  answer2), where quer!ll contains thereferent of some portion of querFz.
The canonical an-swer will then be compared to answer2.Example: One entry in the discourse corpusmight be the following triplet:query1: "What is Mary Smith's job fl0e?
"querFz: "What is her salary?
"answeR:  ((52000))S-8 For the time being, we propose to exclude fromthe test corpora queries that:?
are ambiguous?
are overly vague (i.e., "'Tell me about John Smith")?
don't have an answer because no data is available?
involve presupposition failure?
require sorting?
require "'meta-information" (e.g., "'List the ethnicgroups you know about")?
or otherwise lack a generally accepted answer.2.4 The  ComparatorS-9 The Comparator will use an "epsilon" measurefor comparing real numbers.
The number in the canon-ical answer will be multiplied by the epsilon figure todetermine the allowable deviation : numbers that differby more than this amount will be scored as incorrect.The value of the epsilon figure will be initially set at0.0001.Example: If the canonical answer is53200.0, the maximum deviation allowed willbe (53200.0 x 0.0001), or approximately 5.32.Thus a system answer of 53198.8 would scoreas correct, but 53190.9 would be incorrect.138S-10 Extra fields may be present in an answer elation,and will be ignored by the Comparator.
The order offields is also not specified: the mapping from fields in asystem answer to fields in the canonical answer will bedetermined by the Comparator.Example: For the query "Show Paul Tai'sname and employee id", with the canonicalanswer((4456 "TAI"))any of the following would be an acceptableanswer:( (4456 "TAI .... PAUL") ) )( ("TAI" 4456) ) )S-11 The only output of the Comparator will be "cor-rect" or "'incorrect".
Capturing more subtle degreesof correctness will be accomplished by the quantity andvariety of the test data.3 DISCUSSIONThis section presents some of the justification supportingthe proposal in Section 2, as well as amplifying severalpoints and discussing some possible shortcomings andextensions.
It may be usefully omitted by those who arenot interested in these details.
The organization followsthe four areas of the proposal: notation, minimal content,test corpora, and the Comparator procedures.3.1 NotationThe proposal in S-2 allows scalar values to be repre-sented either as relations or as atomic expressions, i.e.,either offa lse((false))Treating the answer to questions like "Is Paul Tai aclerk?"
as a relation seems omewhat unfortunate.
Thisallows for a completely homogeneous representation fanswers, however, and is permitted for this reason.The range of types in S- l ,  while adequate for thepersonnel domain, may need to be enlarged for otherdomains.
One obvious omission is a type for amounts:units are not specified for numbers.
In the personneldomain, there are only two amounts, years and dollars,neither of which is likely to require expression i  otherunits (though one could conceivably ask "How manydays has Paul Tai worked for the company?").
Otherdomains would not be similarly restricted, and mightrequire unit labels for amounts of length, speed, volume,etc.
Of course, it is always possible to simply specifycanonical units for the domain and require all numbers tobe in those units: this would increase the effort requiredto conform to the CAS, however.Answers to requests for percentages should expressthe ratio directly, not multiplying by 100: so if 45 outof 423 employees have PhD's, the answer to "What per-centage of employees have PhD's?"
should be0.10643.2 Minimal ContentUnder section S-3 of the proposal, one should note thepossibility of a required NP being conjoined or modi-fied by a "conjoining" PP modifier, as in the followingexamples:List the clerks and their salaries.List the clerks with their salaries.Clearly in both of these cases the salaries as well as theemployee IDs should be required in the answer.One possible objection to the approach proposedin S-3 is that it ignores ome well-documented concernsabout he "informativeness" or pragmatic apabilities ofquestion-answering systems.
For example, in a normalcontext, a suitably informative answer forList the salary of each employee.would provide not just a list of salaries but some identi-fication of the employees as well, under the assumptionthat this is what the user really wants.
Since the pur-pose of the CAS is automatic evaluation rather than userconvenience, this objection seems irrelevant, at least un-til a metric for measuring pragmatic apabilities can bedefined.
Note that S-10 means developers are free toinclude extra fields for pragmatic purposes: such fieldsare simply not required for correctness.
A similar pointcan be made concerning vague expressions (S-5).
Onlythe street field is explicitly required for references toan address, since that should be sufficient o determinecorrectness, but developers may also include c i ty  andstate if they wish.139One might argue that the proposed treatment of man-ager/supervisor in S-6 is inconsistent with the approachtaken for scientists, engineers, programmers, etc.
Ourdecision is essentially to consider manager and super-visor as technical terms which indicate a supervisoryrelation to another employee, rather than generic de-scriptions of a profession.
This has the possibly un-fortunate consequence that employees in the CommonPersonnel Database with job rifles like "SUPERVISOR"and "PROJECT MANAGER" may not be consideredsupervisors or managers in this technical sense.
Never-theless, given that these rifles seem less like descriptionsof a profession, the approach is not inconsistent.There are probably other vague expressions which willhave to be dealt with on a case-by-case basis, eitherby agreeing on a referent or eliminating them from thecorpora.3.3 Test  CorporaSome comments are in order about various items ex-cluded by section S-8 of the proposal.
For example,ambiguity (as opposed to vagueness) is barred at present,primarily to simplify the Common Answer Specification.It would not be difficult to enhance the canonical answerto include several alternatives for queries which weregenuinely ambiguous: then the Comparator could see ifa system answer matched any of them, counting at leastone match as correct.
A more challenging test would beto only score answers as correct hat matched all reason-able answers.
This would obviously require substantialconsensus on which queries were ambiguous and whatthe possible readings were.Presupposition is another area where one could imag-ine extensions to the proposal: for queries likeList the female managers who make more than$50000.there is an implicit assumption that there are, in fact,female managers.
If no managers are female, the answerwould presumably be an empty relation.
The reasonthe answer set would be empty, however, would havenothing to do with the data failing to meet some set ofrestrictions, as in ordinary cases.
Rather, the object NPwould have no referent: this is a different kind of failure,and systems that can detect it have achieved a greaterdegree of sophistication, which presumably ought to bereflected in their evaluation.
Such failure to refer can besubdivided into two cases:necessary failure: cases which are impossible by def-inition.
For example, the set of men and the setof women are disjoint: therefore "the female men"necessarily fails to refer to any entity.contingent failure: cases which happen to fail becauseof the state of the world (as in the example above).One modification to the CAS that would include suchcases would be to extend S-1 to include types for fail-ure, with several possible values indicating the failureencountered.
Then the canonical answer to the exampleabove might be the special tokencontingent-failurerather than simply()Until a broader consensus on this issue is achieved, how-ever, we consider the best approach to be the elimination"of such cases from the test corpora.Section S-8 excludes queries whose answer is notavailable due to missing data.
As an example, in theSLS Personnel Database the home phone number forRichard Young, an attorney, is missing, and is thereforeentered as NIL.
We therefore propose to exclude querieslike "What is Richard Young's home phone?"
from testcorpora, since no data is available to answer the ques-tion.
On the other hand, the query "List the home phonenumbers of all the attorneys" would not be excluded.The answer here would be the set of phone numbers,including Richard Young's:(("214-545-0306") (NIL)("214-665-5043") ...)Queries involving sorting are currently omitted pendingresolution of several technical problems:?
Given that extra fields are allowed, the primary sortkeys would have to be specified by the CAS.Different sites might have different and incompati-ble approaches to sub-sorts, ff the primary keys arenot unique.?
Since relations are assumed to not be ordered, a dif-ferent notation for sorted answers would be needed.140In addition to these problems, evaluating queries thatrequire sorting would seem to contribute little to under-standing the key technological capabilities of an SLS,and is therefore at best a marginal issue.
In light ofthese points, we consider it expedient to simply omitsuch cases for the present.3.4 ComparatorThe epsilon measure proposed in S-9 assumes that, ifan SLS does any rounding of real numbers, it will notexceed the epsilon figure.
Two particular cases wherethis might be problematic are percentages and years: re-peating an earlier example, the correct answer to a querylike "What percentage of employees have PhDs?"
mightbe0.1064and rounding this to 0 .11  would score as incorrect.Similarly, for a query like "How many years has SharonLease worked here?
", the years must not be treated aswhole units: an answer like36.87would score as correct, but 37 would be incorrect.One consequence of S-10 is the small possibility ofa incorrect system answer being spuriously scored ascorrect.
This is especially likely when there are only afew tuples and elements, and the range of possible valuesis small.
Yes/no questions are an extreme xample: anunscrupulous developer could always get such queriescorrect by simply answering( (true false) )since the Comparator would generously choose the rightcase.
Eliminating such aberrations would require distin-guishing queries whose answers are relations from thosethat produce scalars, and imposing this distinction in theCAS.
We therefore assume for the time being that de-velopers will pursue more pnncipled approaches, and werely on the variety of test corpora to de-emphasize thesignificance of these marginal cases.4 THE COMPARATORIn this section we describe the Comparator, a CommonLisp program for comparing system output (conformingto the CAS) with canonical answers.
We have chosenthis name to reflect an important but subtle point: eval-uation requires human judgement, and therefore the bestwe can expect from a program is comparison, not eval-uation.
Since the degree to which system output reflectssystem capabilities i  always imperfect, we view the re-suits of the Comparator as only one facet of the entire ef-fort of evaluating Spoken Language Systems.
The Com-parator software is available without charge from BBNSystems and Technologies Corporation, which reservesall fights to the software.
To obtain a copy, contactsboisen@bbn, conThe Comparator takes two inputs: the answer from aparticular SLS, and the canonical answer.
The output isa Boolean value indicating whether the system-suppliedanswer matched the canonical one.
To make that judge-ment, the Comparator needs to perform type-appropriatecomparisons on the individual data items, and to handlecorrectly system answers that contain extra values.As described in S-9, real numbers are compared usingan epsilon test that compares only a fixed number of themost significant digits of the two answers.
The number"of digits compared is intended to generously reflect theaccuracy range of the least accurate systems involved.Note that there is still some danger of numeric impreci-sion causing an answer to be counted wrong if the testset includes certain pathological types of queries, likethose asking for the difference between two very similarreal numbers.The other, more major issue for the Comparator con-cerns the fact that table answers are allowed to includeextra columns of information, as long as they also in-clude the minimal information required by the canonicalanswer(S-10).
Note that these additional columns canmean that the system answer will also include extra tu-pies not present in the canonical answer.
For example, ifSmith and Jones both make exactly $40,000, they wouldcontribute only one tuple to a simple list of salaries, butif a column of last names were included in the answertable, there would be two separate tuples.What the Comparator does with table answers is to ex-plore each possible mapping from the required columnsfound in the canonical answer to the actual columnsfound in the system-supplied answer.
(Naturally, theremust be at least as many columns as in the canonicalanswer, or the system answer is clearly incorrect.)
Ap-plying each mapping in turn to the provided answer,the Comparator builds a reduced answer containing only141those columns indicated by the mapping, with any du-plicate tuples in the reduced answer eliminated.
It isthis reduced answer that is compared with the canonicalanswer, in terms of set equivalence.Finally, it should be stressed that the Comparatorworks within the context of relational database princi-ples.
It treats answer tables as sets of tuples, rather thanlists.
This means first that order of tuples is irrelevant.It also means that duplicate tuples are given no seman-tic weight; any duplicates in a provided answer will beremoved by the Comparator before the comparison ismade.5 CORPUS DEVELOPMENT AND TAG-G INGAny corpus which is collected for SLS development andtesting will be more useful if it is easily sub-dividedinto easier and harder cases.
Different systems have dif-ferent capabilities, particularly with respect o handlingdiscourse phenomena: the ideal corpus will thereforeinclude both the most basic case (i.e., no discourse phe-nomena) and enough difficult cases to drive more ad-vanced research.We propose the tagging of corpora using a hierarchi-cal categorization that reflects the richness of contextrequired to handle queries.
These categories primarilydistinguish levels of effort for Spoken Language Sys-tems, such that the lowest category should be attemptedby every site, and the highest category attempted onlyby the most ambitious.
Two other criteria for the cate-gorization are the following:?
Categories hould be maximally distinctive: thereis no need for fine distinctions that in practice onlyseparate a few cases.?
Categories should be easily distinguishable by thosewho will do the tagging.
That is, they should beobjective and clearly defined rather than relying onsophisticated linguistic judgements.Here is a candidate categorization, where the categorynumber increases as the context required becomes uc-cessively richer:Category 0: no extra-Sentential context is required (i.e.,"0" context): the sentence can be understood inisolation.
This is the default case.Category 1: "local" extra-sentential reference, exclud-ing reference to answers: that is, the sentence canbe understood if the text of the previous questionis available.
One is not allowed to go back morethan one question, or look at the answer, to find thereferent.Category 2: ellipsis cases, such as the following se-quence:"What's Sharon Lease's salary?
""How about Paul Tai?
"Category 3: "non-local" reference.
The referent is inthe answer to the previous query, or in the text ofa query earlier the previous one.
This probably in-cludes several other kinds of phenomena that wouldbe usefully separated out at a later date.Category X: all cases excluded from corpora, both fordiscot~se and other reasons (see S-8).We propose two initial uses of this categorization forSLS evaluation: creating basic test corpora of Category0 queries, and designing simple discourse corpora thatinclude Category 1 queries (see S-7).
The other cat-egories would enable developers either to focus on orto eliminate from consideration more difficult kinds ofdiscourse phenomena.There may be other categories which are of interestto particular developers: for such cases, it is suggestedthat the developer do their own site-specific tagging,using those features which seem reasonable to them.This scheme is offered solely to expedite community-wide evaluation: there are many possible categorizationswhich might be useful for other purposes, but they areindependent of the considerations of common evalua-tion.6 CORPUS COLLECT IONThe evaluation scheme described in the previous sec-tions assumes the existence of realistic test corpora: itsays nothing, however, about how such corpora will becollected.
This section describes our approach to corpuscollection, which uses a human simulation of a SpokenLanguage System.
Since the test and training corporawill be shared, they need only be collected once, andthen distributed to SLS developers.
The setup described142here has been delivered to Texas Instruments for usein their effort to collect a common corpus for the SLScommunity.Our approach to corpus collection is to simulate thebehavior of a spoken language system (SLS) for databaseaccess which is beyond the state of the art, but withinabout 5 years of effort.
We are concerned with obtaininga large corpus of spontaneous task-oriented utterances,without being restricted by current capabilities.
Such acorpus would help in prioritizing research for the nextfew years, while providing a challenging base of exam-pies for system evaluation.Our methodology has been guided by the followingprimary goals:?
The subject should be as unbiased as possible interms of the kinds of language which are possible.?
The subject should be performing a task, in orderto get realistic input exhibiting context-dependentphenomena.?
The simulation mechanism ust be fast enough tomaintain the interaction i  a natural fashion.?
The simulation must be accurate nough that thesubject can rely on it: otherwise interactions tend todegenerate into consistency checking by the subject.?
The system must behave cooperatively, especiallywhen it is unable to answer a query, so as not tofruslrate the subject.?
Minimal training should be required so that rela-tively naive subjects can be utilized.We accomplish these goals by providing ahuman sim-ulator (here referred to as the Wizard) of a speech recog-nition system, using a "PNAMBC" setup ("Pay No At-tention to the Man Behind the Curtain").
The Wizard isprovided with tools to obtain and display answers to userqueries, and to provide appropriate diagnostic messageswhen the query cannot be satisfied.6.1 The SetupIn a data collection session, the subject is first providedwith the following:?
general information describing the session?
a description of the database?
a list of possible tasks for the subject o pursue (in-cluding specific details, but not examples of Englishqueries)?
pen and paperThe subject and the Wizard are kept in separate rooms,in order to remove any influence of the Wizard's actionson the subject.
The Wizard is only allowed to commu-nicate with the subject by displaying information on thesubject's console, either as an answer to a request, or byusing one of a fixed set of canned messages.
A "push totalk" microphone is used by the subject when directingrequests to the Wizard, and his utterances are recorded.A diagram of the setup is in Figure 2.There is a cyclic interaction which begins by the sub-ject verbalizing a query, which the Wizard transcribesand sends as text to the subject's creen.
The Wizardthen submits the text to a natural language (NL) tool: wehave used Parlance TM, BBN's commercial NL interface,for this component.
If the query is correctly processed,the Wizard echoes the response to the subject's creen.If the NL tool fails, the Wizard may revise the requestand try again until it is understood, or until a predeter-mined time limit expires (usually around a minute).
Allrevision is "behind the curtain", and unseen by the sub-ject.
The speed of the NL tool (a few seconds per queryfor Parlance) allows for several revisions of a request ifnecessary, while still keeping a fast turnaround time.
Ifthe Wizard is unable to obtain a correct answer, or therequest is outside the scope of the simulation, the Wiz-ard sends one of a predefined set of canned messages tothe user (see the next section for examples).
All inputand output o the Wizard's window is recorded in a log.file.With this setup we are able to collect approximately50 queries per hour, using one subject per hour.6.2 The WizardBeyond the simple transcription of the spoken query, therole of the Wizard is to extend the linguistic overage ofthe NL tool (by revising queries when necessary), whilekeeping the simulation within the bounds of a system thatis realistically attainable in a few years.
In our prelim-inary experiments, the Wizard's involvement increasedthe coverage of requests that were within the scope ofthe simulated system from 67% to 94%.143Subject WizardI 2:  speech!_   n, record, echo II ~ !
transcribe \[' ~ I-reviseI ~ if~ 1  necessaryFigure 2: Data Collection SetupAnother important aspect of the Wizard's job is todecide when a query is beyond the simulated system'sscope and give an appropriate reply.
Any request hatcan be understood by the NL tool is considered safelywithin the scope of our simulated system.
However,other equests may be problematic, for example, queriesthat require meta-level knowledge like "Are you able tocompute percentages?".
When a query cannot be an-swered within the bounds of the simulation, or the Wiz-ard is unable to obtain an answer due to an NL toollimitation (even though the query is considered reason-able), the Wizard selects an appropriate canned messageand dispatches it to the subject.
Currently there are 11such messages, including the following:?
"No information is available on ~ ": the infor-marion requested is not in the database---the Wizardfills in the blank.?
"No information about the information itself isavailable", when the subject's request assumesmeta-level knowledge.?
"Sorry, the system is unable to answer your query",when the NL tool fails to process a request hat iswithin the scope of the simulated system.A successful Wizard needs to know the domain,though "expert" status is not necessary.
In addition, theWizard must be familiar with the database and with theextent of coverage provided by the particular Parlanceconfiguration.
Some linguistic knowledge is also usefulto quickly diagnose likely problems with complex lin-guistic constructions.
We've found that a large set ofsample sentences, and few days experience with the NLtool, is sufficient to train Wizard apprentices.1446.3 The NL ToolThe NL tool clearly performs a crucial role in the Ozsimulations.
In principle, this tool could be just a low-level database query language interface.
This wouldsacrifice turnaround speed, however, since the Wiz-ard would have to compose and enter long, complexdatabase queries.Using Parlance as a NL tool provides a very fast andreliable system for processing the natural anguage re-quests.
In our initial experiments simulating an SLSfront end to a personnel database, 67% of the requeststhat were within the bounds of the simulated system werehandled by Parlance without revision.
Parlance also pro-vides us with the Learner TM, its knowledge acquisitiontool.
With the Learner, Parlance can be quickly broughtto high coverage on a new database, allowing for corpuscollection in various domains ff so desired.A A BNF SPECIFICATION FOR THECAS NOTATIONThe following BNF describes the syntax of the CommonAnswer Specification:answer , scalar-value I relationboolean-value , true I falsedate , " digit digit - month - digitdigit "digit , 0 - 9month , JAN ...DECnumber-value , integer I real-numberrelation ~ ( tuple* )scalar-value , boolean-value I date \[number-value \ [ s t r ingtuple , ( value + )value , scalar-value I NILWe assume as primitives the values integer, real-number, and string.
Integers and reals are not distin-guished.
Note that, unlike the special tokens t rue ,fa l se  and n i l ,  strings are case-sensitive, and in thepersonnel domain should usually be upper-case (sincethat's the form used in the SLS Personnel Database).Only non-exponential real numbers are allowed.Answer relations must be derived from the existingrelations in the database, either by subsetting and com-bining relations or by operations like averaging, summa-tion, etc.
For the SLS Personnel Database, this impliesallowing NIL as a special case for any value.Empty tuples are not allowed (but empty relationsare).
Reiterating S-2, all the tuples in a relation musthave the same number of values, those values must beof the same respective types (boolean, string, date, ornumber), and the types in the answer must be the sameas the types in the database (i.e., database values like"1355" cannot be converted from strings to numbers inanswer expressions).B A SAMPLE TEST CORPUSBased on the Common Personnel Database, we haveprepared a small corpus of queries and their answers inthe Common Answer Specification.
These provide con-crete examples of the answer expressions that a SpokenLanguage System is expected to match, within the lee-way allowed by the Comparator (Section 4).
The intenthere is not to provide a wide range of natural anguagecontructions, but rather to provide a check on the clar-ity and completeness of the CAS.
Note that discoursecapabilities are not exercised in this corpus.
"How many employees are there in department 45?
"48"When did Sharon Lease join the company?
""06-0CT-52""Show me a list of the Hispanic employees.
"((1468) (4688) (6213))"Are there any clerks who make over $50000?
"false"What's the number of female scientists in the com-pany?
"20"How long ago did Sharon Lease join the company?
"36.87"How many years has Sharon Lease worked here?
"36.87145"Does Sharon Lease work in department 10?
"true"Are Sharon Lease and Paul Hayes both in department10?
"false"Sum the salaries of the managers.
"2331300"Give me the average salary of managers who graduatedfrom Harvard.
"72400.0"What are the ethnic groups of the members of depart-ment 11?
"(("BLACK") ("HISPANIC")( "WHITE" ) )"Show me every manager's last name and the numberof people they supervise.
"( (71 "GRAY") (i "BRADY")(40 "JACOBSON") (4 "BEEK")(3 "EASTON") (i "BOFF")(8 "BANDANZA") (20 "BURRUSS")(96 "MCGLEW") (12 "FRANKLIN")(24 "PAYNE") (5 "BUONO")(28 "MACLEAN") (4 "LANDER")(29 "FAY") (47 "SEABORN")(3 "LI") (26 "DI"))"List the degrees of Sharon Lease, Jose Vasquez, andMartha Nash.
"(("HA" 1028 3761) ("MA" 14583514) ("BA" 1458 3434) ("PHD"2099 3434) ("MA" 2099 3434)("HA" 2099 2074))"Tell me the names of all senior scientists.
"(("BURRUSS") ("SPATOLA")(" SYKLEY" ) ("CAVANAUGH")("KANYUCK") ("SEABORN")("KINGSLEY") ("MESSENGER")("ZXNrZ"))"What is Sharon Lease's phone number?
""6218""What's the average salary for level 31 employees?
"((270275.0))"List the home phones of people living in Irving.
"((NIL) ("214-545-0306")("214-665-5043")("214-492-3575 ")("214-696-8397")("214-625-4682")("214-884-1708")("214-484-3185")("214-646-4192"))"List all the programmers in department 20.
"()"What percentage ofemployees have PhDs?
"0.1064"Give me a list of the addresses of all the accountants.
"( ( "9 LAKEVIEW TERR" )("35 MYOPIA HILL RD")("58 HIGH ST")( "30 DELANO PARK" )("74 GODEN ST")("'49 LAWMARISSA RD")("240 BRATTLE ST")("380 WALTHAM ST")("223 BROADWAY")("33 SUMMIT AVE") ("269 RICE ST")("21 HAWTHORNE ST") ("76 WEBSTER ST")("55 OAK ST") ("147 GLOUCESTER ST"))AcknowledgementsThe work reported here was supported by the AdvancedResearch Projects Agency and was monitored by the Of-rice of Naval Research under Contract No.
00014-89-C-0008.
The views and conclusions contained in thisdocument are those of the authors and should not be in-terpreted as necessarily representing the official policies,either expressed or implied, of the Defense AdvancedResearch Projects Agency or the United States Govern-ment.ReferencesS.
Boisen.
The SLS Personnel Database (Release 1).SLS Notes No.
2, BBN Systems and Technologies Cor-poration, 1989.L.
Ramshaw.
Manual for SLS ERL (Release 1).
SLSNotes No.
3, BBN Systems and Technologies Corpora-tion, 1989.146
