Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: MiningBiological Semantics, pages 1?8, Detroit, June 2005. c?2005 Association for Computational LinguisticsWeakly Supervised Learning Methods for Improving the Quality ofGene Name Normalization DataBen Wellnerwellner@mitre.orgThe MITRE Corporation202 Burlington RdBedford MA 01730Computer Science DepartmentBrandeis UniversityWaltham MA 02454AbstractA pervasive problem facing many bio-medical text mining applications is that ofcorrectly associating mentions of entitiesin the literature with corresponding con-cepts in a database or ontology.
Attemptsto build systems for automating this proc-ess have shown promise as demonstratedby the recent BioCreAtIvE Task 1Bevaluation.
A significant obstacle to im-proved performance for this task, how-ever, is a lack of high quality trainingdata.
In this work, we explore methods forimproving the quality of (noisy) Task 1Btraining data using variants of weakly su-pervised learning methods.
We presentpositive results demonstrating that thesemethods result in an improvement intraining data quality as measured by im-proved system performance over the samesystem using the originally labeled data.1 IntroductionA primary set of tasks facing biomedical text proc-essing systems is that of categorizing, identifyingand classifying entities within the literature.
A keystep in this process involves grouping mentions ofentities together into equivalence classes that de-note some underlying entity.
In the biomedicaldomain, however, we are fortunate to have struc-tured data resources such as databases and ontolo-gies with entries denoting these equivalenceclasses.
In biomedical text mining, then, this proc-ess involves associating mentions of entities withknown, existing unique identifiers for those entitiesin databases or ontologies ?
a process referred to asnormalization.
This ability is required for textprocessing systems to associate descriptions ofconcepts in free text with a grounded, organizedsystem of knowledge more readily amenable tomachine processing.The recent BioCreAtIvE Task 1B evaluationchallenged a number of systems to identify genesassociated with abstracts for three different organ-isms: mouse, fly and yeast.
The participants wereprovided with a large set of noisy training data anda smaller set of higher quality development testdata.
They were also provided with a lexicon con-taining all the potential gene identifiers that mightoccur and a list of known, though incomplete,names and synonyms that refer to each of them.To prepare the training data, the list of uniquegene identifiers associated with each full text arti-cle was obtained from the appropriate model or-ganism database.
However, the list had to bepruned to correspond to the genes mentioned in theabstract.
This was done by searching the abstractfor each gene on the list or its synonyms, usingexact string matching.
This process has the poten-tial to miss genes that were referred to in the ab-stract using a phrase that does not appear in thesynonym list.
Additionally, the list may be incom-plete, because not all genes mentioned in the arti-cle were curated, so there are mentions of genes inan abstract that did not have a corresponding iden-tifier on the gene list.This paper explores a series of methods for at-tempting to recover some of these missing gene1identifiers from the Task 1B training data abstracts.We start with a robust, machine learning-basedbaseline system: a reimplementation of the systemin [1].
Briefly, this system utilizes a classifier toselect or filter matches made against the synonymlist with a loose matching criterion.
From thisbaseline, we explore various methods for re-labeling the noisy training data, resulting in im-proved scores on the overall Task 1B developmenttest and evaluation data.
Our methods are based onweakly supervised learning techniques such as co-training [2] and self-training [3, 4] for learningwith both labeled and unlabeled data.The setting here is different than the typical set-ting for weakly supervised learning, however, inthat we have a large amount of noisily labeled data,as opposed to completely unlabeled data.
Themain contribution of this work is a framework forapplying weakly supervised methods to this prob-lem of re-labeling noisy training data.Our approach is based on partitioning the train-ing data into two sets and viewing the problem astwo mutually supporting weakly supervised learn-ing problems.
Experimental results demonstratethat these methods, carefully tuned, improve per-formance for the gene name normalization taskover those previously reported using machinelearning-based techniques.2 Background and Related Work2.1 Gene Name Normalization and Extrac-tionThe task of normalizing and identifying biologicalentities, genes in particular, has received consider-able attention in the biological text mining com-munity.
The recent Task 1B from BioCreAtIvE[5] challenged systems to identify unique geneidentifiers associated with paper abstracts from theliterature for three organisms: mouse, fly andyeast.
Task 1A from the same workshop focusedon identifying (i.e.
tagging) mentions of genes inbiomedical journal abstracts.2.2 NLP with Noisy and Un-labeled TrainingDataWithin biomedical text processing, a number ofapproaches for both identification and normaliza-tion of entities have attempted to make use of themany available structured biological resources to?bootstrap?
systems by deriving noisy training datafor the task at hand.
A novel method for usingnoisy (or ?weakly labeled?)
training data from bio-logical databases to learn to identify relations inbiomedical texts is presented in [6].
Noisy trainingdata was created in [7] to identify gene name men-tions in text.
Similarly, [8] employed essentiallythe same approach using the FlyBase database toidentify normalized genes within articles.2.3 Weakly Supervised LearningWeakly supervised learning remains an active areaof research in machine learning.
Such methods arevery appealing: they offer a way for a learning sys-tem provided with only a small amount of labeledtraining data and a large amount of un-labeled datato perform better than using the labeled data alone.In certain situations (see [2]) the improvement canbe substantial.Situations with small amounts of labeled dataand large amounts of unlabeled data are verycommon in real-world applications where labelinglarge quantities of data is prohibitively expensive.Weakly supervised learning approaches can bebroken down into multi-view and single-viewmethods.Multi-view methods [2] incrementally labelunlabeled data as follows.
Two classifiers aretrained on the training data with different ?views?of the data.
The different views are realized bysplitting the set of features in such a way that thefeatures for one classifier are conditionally inde-pendent of features for the other given the classlabel.
Each classifier then selects the most confi-dently classified instances from the unlabeled data(or some random subset thereof) and adds them tothe training set.
The process is repeated until alldata has been labeled or some other stopping crite-rion is met.
The intuition behind the approach isthat since the two classifiers have different viewsof the data, a new training instance that was classi-fied with high confidence by one classifier (andthus is ?redundant?
from that classifier?s point ofview) will serve as an informative, novel, newtraining instance for the other classifier and vice-versa.Single-view methods avoid the problem of find-ing an appropriate feature split which is not possi-ble or appropriate in many domains.
One commonapproach here [4] involves learning an ensemble of2classifiers using bagging.
With bagging, the train-ing data is randomly sampled, with replacement,with a separate classifier trained on each sample.Un-labeled instances are then labeled if all of theseparate classifiers agree on the label for that in-stance.
Other approaches are based on the expec-tation maximization algorithm (EM) [9].3 System DescriptionThe baseline version of our system is essentially areproduction of the system described in [1] with afew modifications.
The great appeal of this sys-tem is that, being machine learning based, it has noorganism-specific aspects hard-coded in; movingto a new organism involves only re-training (as-suming there is training data) and setting one ortwo parameters using a held-out data set or cross-validation.The system is given a set of abstracts (and asso-ciated gene identifiers at training time) and a lexi-con.
The system first proposes candidate phrasesbased on all possible phrases up to 8 words in len-gth with some constraints based on part-of-speech1.
Matches against the lexicon are then car-ried out by performing exact matching but ignoringcase and removing punctuation from the both thelexical entries and candidate mentions.
Only maxi-mal matching strings were used ?
i.e.
sub-strings ofmatching strings that match the same id are re-moved.The resulting set of matches of candidate men-tions with their matched identifiers results in a setof instances.
These instances are then providedwith a label - ?yes?
or ?no?
depending on whetherthe match in the abstract is correct (i.e.
if the geneidentifier associated with the match was annotatedwith the abstract).
These instances are used totrain a binary maximum entropy classifier that ul-timately decides if a match is valid or not.Maximum entropy classifiers model the condi-tional probability of a class, y, (in our setting,y=?yes?
or y=?no?)
given some observed data, x.The conditional probability has the following formin the binary case (where it is equivalent to logisticregression):1Specifically, we excluded phrases that began with verbsprepositions, adverbs or determiners; we found this constraintdid not affect recall while reducing the number of candidatementions by more than 50%.
)()),(exp()|(xZyxfxyP iii =?where Z(x) is thenormalization function, the i?
are real-valuedmodel parameters and the if  are arbitrary real-valued feature functions.One advantage of maximum entropy classifiersis the freedom to use large numbers of statisticallynon-independent features.
We used a number ofdifferent feature types in the classifier:?
the matching phrase?
the matched gene identifier?
the previous and subsequent two words of thephrase?
the number of words in the matching phrase?
the total number of genes that matched againstthe phrase?
all character prefixes and suffixes up to length 4for words within the phraseAn example is shown below in Figure 1 below.Abstract Excerpt:?This new receptor, TOR (thymus or-phan receptor)?
?Feature Class Specific FeaturePhrase TORGENEID MGI104856Previous-1 ,Previous-2 receptorSubsequent-1 (Subsequent-2 thymusNumber of Matches 2Number of Words 1Prefix-1 TPrefix-2 TOPrefix-3 TORSuffix-1 RSuffix-2 ORSuffix-3 TORFigure 1.
An abstract excerpt with the matchingphrase ?TOR?.
The resulting features for the matchare detailed in the table.In addition to these features we created addi-tional features constituting conjunctions of some ofthese ?atomic?
features.
For example, the con-joined feature Phrase=TOR AND GE-NEID=MGI104856 is ?on?
when both conjunctsare true of the instance.To assign identifiers to a new abstract a set fea-tures are extracted for each matching phrase and3gene id pair just as in training (this constitutes aninstance) and presented to the classifier for classi-fication.
As the classifier returns a probability foreach instance, the gene id associated with the in-stance with highest probability is returned as agene id associated with the abstract, except in thecase where the probability is less than somethreshold 10, ??
TT  in which case no gene id isreturned for that phrase.Training the model involves finding the pa-rameters that maximize the log-likelihood of thetraining data.
As is standard with maximum en-tropy models we employ a Gaussian prior over theparameters which bias them towards zero to reduceoverfitting.Our model thus has just two parameters whichneed to be tuned to different datasets (i.e.
differentorganisms): the Gaussian prior and the threshold,T .
Tuning the parameters can be done on a heldout set (we used the Task 1B development data) orby cross validation:4 Weakly Supervised Methods for Re-labeling Noisy Normalization DataThe primary contribution of this work is a novelmethod for re-labeling the noisy training instanceswithin the Task 1B training data sets.
Recall thatthe Task 1B training data were constructed bymatching phrases in the abstract against the syno-nym lists for the gene ids curated for the full textarticle for which the abstract was written.
In manycases, mentions of the gene in the abstract do notappear exactly as they do in the synonym list,which would result in a missed association of thatgene id with the abstract.
In other cases, the data-base curators simply did not curate a gene id men-tioned in the abstract as it was not relevant to theirparticular line of interest.Our method for re-labeling potentially misla-beled instances draws upon existing methods forweakly supervised learning.
We describe here thegeneric algorithm and include specific variationsbelow in the experimental setup.The first step is to partition the training datainto two disjoint sets, D1 and D2.2 We then createtwo instances of the weakly supervised learning2Note that instances in D1 and D2 are also derived form dis-joint sets of abstracts.
This helps ensure that very similarinstances are unlikely to appear in different partitions.problem where in one instance, D1 is viewed as thelabeled training data and D2 is viewed as the unla-beled data, and in the other instance their roles arereversed.
Re-labeling of instances in D1 is carriedout by a classifier or ensemble of classifiers, C2trained on D2.
Similarly, instances in D2 are re-labeled by C1 trained on D1.
Those instances forwhich the classifier assigns high confidence (i.e.for which )|""( xyesyP = is high) but for whichthe existing label disagrees with the classifier arecandidates for re-labeling.
Figure 2 diagrams thisprocess below.Figure 2.
Diagram illustrating the method for re-labeling instances.
The solid arrows indicate thetraining of a classifier from some set of data, whileblock arrows describe the data flow and re-labelingof instances.One assumption behind this approach is that notall of the errors in the training data labels are corre-lated.
As such, we would expect that for a particu-lar mislabeled instance in D1, there may be similarpositive instances in D2 that provide evidence forre-labeling the mislabeled in D1.Initial experiments using this approach metwith failure or negligible gains in performance.We initially attributed this to too many correlatederrors.
Detailed error analysis revealed, however,that a significant portion of training instances be-ing re-labeled were derived from matches againstthe lexicon that were not, in fact, references togenes ?
i.e.
they were other more common Englishwords that happened to appear in the synonym listsfor which the classifier mistakenly assigned themhigh probability.D1     D2C2 C1D2?
D1?Final ClassifierOriginalTraining DataModified Train-ing DataRe-labelingclassifiers4Our solution to this problem was to impose aconstraint on instances to be re-labeled:  Thephrase in the abstract associated with the instanceis required to have been tagged as a gene name bya gene name tagger in addition to the instance re-ceiving a high probability by the re-labeling classi-fier.
Use of a gene name tagger introduces a checkagainst the classifier (trained on the noisy trainingdata) and helps to reduce the chance of introducingfalse positives into the labeled data.We trained our entity tagger, Carafe, on a theGenia corpus [10] together with the BioCreativeTask 1A gene name training corpus.
Not all of theentity types annotated in the Genia corpus aregenes, however.
Therefore we used an appropriatesubset of the entity types found in the corpus.
Ca-rafe is based on Conditional Random Fields [11](CRFs) which, for this task, employed a similar setof features to the CRF described in [12].5 Experiments and ResultsThe main goal of our experiments was to demon-strate the benefits of re-labeling potentially noisytraining instances in the task 1B training data.
Inthis work we focus the weakly supervised re-labeling experiments on the mouse data set.
In themouse data there is a strong bias towards falsenegatives in the training data ?
i.e.
many traininginstances have a negative label and should have apositive one.
Our reasons for focusing on this dataare twofold: 1) we believe this situation is likely tobe more common in practice since an organismmay have impoverished synonym lists or ?gaps?
inthe curated databases and 2) the experiments andresulting analyses are made clearer by focusing onre-labeling instances in one direction only (i.e.from negative to positive).In this section, we first describe an initial ex-periment comparing the baseline system (describedabove) using the original training data with a ver-sion trained with an augmented data set where la-bels changed based on a simple heuristic.
We thendescribe our main body of experiments using vari-ous weakly supervised learning methods for re-labeling the data.
Finally, we report our overallscores on the evaluation data for all three organ-isms using the best system configurations derivedfrom the development test data.5.1 Data and MethodologyWe used the BioCreative Task 1B data for all ourexperiments.
For the three data sets, there were5000 abstracts of training data and 250, 110 and108 abstracts of development test data for mouse,fly and yeast, respectively.
The final evaluationdata consisted of 250 abstracts for each organism.In the training data, the ratios of positive to nega-tive instances are the following: for mouse:40279/111967, for fly: 75677/493959 and foryeast: 25108/3856.
The number of features in eachtrained model range from 322110 for mouse,881398 for fly and 108948 for yeast.Given a classifier able to rank all the test in-stances (in our case, the ranks derive from theprobabilities output by the maximum entropy clas-sifier), we return only the top n gene identifiers,where n is the number of correct identifiers in thedevelopment test data ?
this results in a balancedF-measure score.
We use this metric for all ex-periments on the development test data as it allowsbetter comparison between systems by factoringout the need to tune the threshold.On the evaluation data, we do not know n. Thesystem returns a number of identifiers based on thethreshold, T.  For these experiments, we set T onthe development test data and choose three appro-priate values for three different evaluation ?sub-missions?.5.2 Experiment Set 1: Effect of match-basedre-labelingOur first set of experiments uses the baseline sys-tem described earlier.
We compare the results ofthis system using the Task 1B training data ?asprovided?
with the results obtained by re-labelingsome of the negative instances provided to theclassifier as positive instances.
We re-labeled anyinstances as positive that matched a gene identifierassociated with the abstract regardless of the (po-tentially incorrect) label associated with the identi-fier.
The Task 1B dataset creators marked anidentifier ?no?
if an exact lexicon match wasn?tfound in the abstract.
As our system matchingphase is a bit different (i.e.
we remove punctuationand ignore case), this amounts to re-labeling thetraining data using this looser criterion.
The resultsof this match-based re-labeling are shown in Table1 below.5Baseline Re-labeledMouse 68.8 72.0Fly 70.8 75.3Yeast 89.7 90.0Table 1 Balanced F-measure scores comparing thebaseline vs. a system trained with the match-basedre-labeled instances on the development test data.5.3 Experiment Set 2: Effect of Weakly Su-pervised Re-labelingIn our next set of experiments we tested a numberof different weakly supervised learning configura-tions.
These different methods simply amount todifferent rankings of the instances to re-label(based on confidence and the gene name tags).The basic algorithm (outlined in Figure 1) remainsthe same in all cases.
Specifically, we investigatedthree methods for ranking the instances to re-label:1) na?ve self-training, 2) self-training with bagging,and 3) co-training.Na?ve self-training consisted of training a singlemaximum entropy classifier with the full featureset on each partition and using it to re-label in-stances from the other partition based on confi-dence.Self training with bagging followed the sameidea but used bagging.
For each partition, wetrained 20 separate classifiers on random subsets ofthe training data using the full feature set.
The con-fidence assigned to a test instance was then definedas the product of the confidences of the individualclassifiers.Co-training involved training two classifiers foreach partition with feature split.
We split the fea-tures into context-based features such as the sur-rounding words and the number of gene idsmatching the current phrase, and lexically-basedfeatures that included the phrase itself, affixes, thenumber of tokens in the phrase, etc.
We computedthe aggregated confidences for each instance as theproduct of the confidences assigned by the result-ing context-based and lexically-based classifiers.We ran experiments for each of these three op-tions both with the gene tagger and without thegene tagger.
The systems that included the genetagger ranked all instances derived from taggedphrases above all instances derived from phrasesthat were not tagged regardless of the classifierconfidence.A final experimental condition we explored wascomparing batch re-labeling vs. incremental re-labeling.
Batch re-labeling involved training theclassifiers once and re-labeling all k instances us-ing the same classifier.
Incremental re-labelingconsisted of iteratively re-labeling n instances overk/n epochs where the classifiers were re-trained oneach epoch with the newly re-labeled training data.Interestingly, incremental re-labeling did not per-form better than batch re-labeling in our experi-ments.
All results reported here, therefore, usedbatch re-labeling.After the training data was re-labeled, a singlemaximum entropy classifier was trained on theentire (now re-labeled) training set.
This resultingclassifier was then applied to the development setin the manner described in Section 3.MAX With Tagger Without TaggerSelf-Na?ve 74.4 (4000) 72.3 (5000)Self-Bagging 74.8 (4000) 73.5 (6000)Co-Training 74.6 (4000) 72.7 (6000)AVG With Tagger Without TaggerSelf-Na?ve 72.2 71.2Self-Bagging 72.2 71.5Co-Training 71.9 71.2Table 2.
Maximum and average balanced f-measurescores on the mouse data set for each of the six sys-tem configurations for all values of k ?
the number ofinstances re-labeled.
The numbers in parenthesesindicate for which value of k the maximum value wasachieved.We tested each of these six configurations fordifferent values of k, where k is the total number ofinstances re-labeled3.
Table 2 highlights the maxi-mum and average balanced f-measure scoresacross all values of k for the different system con-figurations.
Both the maximum and averagedscores appear noticeably higher when constrainingthe instances to re-label with the tagger.
The threeweakly supervised methods perform comparablywith bagging performing slightly better.3The values of k considered here were: 0, 10, 20, 50, 100,200, 300, 500, 800, 1000, 2000, 3000, 4000, 5000, 6000,7000, 8000, 9000, 10000, 12000 and 15000.6Figure 3.
The top graph shows balanced F-measurescores against the number of instances re-labeledwhen using the tagger as a constraint.
The bottomgraph compares the re-labeling of instances with thegene tagger as a constraint and without.In order to gain further insight into re-labeling in-stances, we have plotted the balanced F-measureperformance on the development test for variousvalues of k.  The upper graph indicates that thethree different methods correlate strongly.
Thebottom graph makes apparent the benefits of tag-ging as a constraint.
It also points to the weaknessof the tagger, however.
At k=7000 and k=8000,the system tends to perform worse when using thetags as a constraint.
This indicates that tagger re-call errors have the potential to filter out good can-didates for re-labeling.Another observation from the graphs is that per-formance actually drops for small values of k.  Thiswould imply that many of the instances the classi-fiers are most confident about re-labeling are infact spurious.
To support this hypothesis, wetrained the baseline system on the entire trainingset and computed its calibration error on the de-velopment test data.
The calibration error measureshow ?realistic?
the probabilities output by the clas-sifier are.
See [13] for details.Figure 4.
Classifier calibration error on the devel-opment test data.Figure 4 illustrates the estimated calibration er-ror at different thresholds.
As can be seen, the er-ror is greatest for high confidence values indicatingthat the classifier is indeed very confidently pre-dicting an instance as positive when it is negative.Extrapolating this calibration error to the re-lablingclassifiers (each trained on one half of the trainingdata) offers some explanation as to why re-labelingstarts off so poorly.
The error mass is exactlywhere we do not want it - at the highest confidencevalues.
This also offers an explanation as to whyincremental re-labeling did not help.
Fortunately,introducing a gene tagger as a constraint mitigatesthis problem.5.4 Experiment Set 3: Final EvaluationWe report our results using the best overall systemconfigurations on the Task 1B evaluation data.
We?submitted?
3 runs for two different mouse con-figurations and one for both fly and yeast.
Thehighest scores over the 3 runs are reported in Table3.
MouseWS used the best weakly supervisedmethod as determined on the development testdata: bagging with k=4000.
MouseMBR, Ye-astMBR and FlyMBR used match-based re-labelingdescribed in Section 5.2.
The Gaussian prior wasset to 2.0 for all runs and the 3 submissions foreach configuration only varied in the thresholdvalue T.F-measure Precision RecallMouseWS 0.784 0.81 0.759MouseMBR 0.768 0.795 0.743FlyMBR 0.767 0.767 0.767YeastMBR 0.902 0.945 0.902Table 3.
Final evaluation results.7These results are competitive compared withthe BioCreAtIvE Task 1B results where the highestF-measures for mouse, fly and yeast were 79.1,81.5 and 92.1 with the medians at 73.8, 66.1 and85.8, respectively.
The results for mouse and flyimprove upon previous best reported results withan organism invariant, automatic system [1].6 ConclusionsThe quality of training data is paramount to thesuccess of fully automatic, organism invariant ap-proaches to the normalization problem.
In this pa-per we have demonstrated the utility of weaklysupervised learning methods in conjunction with agene name tagger for re-labeling noisy trainingdata for gene name normalization.
The result be-ing higher quality data with corresponding higherperformance on the BioCreAtIvE Task 1B genename normalization task.Future work includes applying method outlinedhere for correcting noisy data to other classifica-tion problems.
Doing so generally requires an in-dependent ?filter?
to restrict re-labeling ?
theequivalent of the gene tagger used here.
We alsohave plans to improve classifier calibration.
Inte-grating confidence estimates produced by the genename tagger, following [14], is another avenue forinvestigation.AcknowledgementsWe thank Alex Morgan, Lynette Hirschman, Marc Colosimo,Jose Castano and James Pustejovsky for helpful commentsand encouragement.
This work was supported under MITRESponsored Research 51MSR123-A5.References1.
Crim, J., R. McDonald, and F. Pereira.
Auto-matically Annotating Documents with Normal-ized Gene Lists.
in EMBO Workshop - Acritical assessment of text mining methods inmolecular biology.
2004.
Granada, Spain.2.
Blum, A. and T. Mitchell.
Combining Labeledand Unlabeled Data with Co-training.
1998.Proceedings of the Workshop on Computa-tional Learning Theory: Morgan Kaufmann.3.
Banko, M. and E. Brill.
Scaling to very verylarge corpora for natural language disam-biguation.
in ACL/EACL.
2001.4.
Ng, V. and C. Cardie.
Weakly SupervisedNatural Language Learning Without Redun-dant Views.
in Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics(HLT/NAACL).
2003.5.
Hirschman, L., et al, Overview of BioCreAtIvEtask 1B: Normalized Gene Lists.
BioMed Cen-tral BioInformatics, 2005(Special Issue onBioCreAtIvE).6.
Craven, M. and J. Kumlien, Constructing Bio-logical Knowledge Bases by Extracting Infor-mation from Text Sources.
1999: p. 77-86.7.
Morgan, A., et al, Gene Name Extraction Us-ing FlyBase Resources.
ACL Workshop onNatural Language Processing in Biomedicine,2003.8.
Morgan, A.A., et al, Gene name identificationand normalization using a model organism da-tabase.
J Biomed Inform, 2004.
37(6): p. 396-410.9.
Nigam, K. and R. Ghani.
Analyzing the effec-tiveness and applicability of co-training.
in In-formation and Knowledge Management.
2000.10.
Kim, J.-D., et al, GENIA Corpus -- a semanti-cally annotated corpus for bio-text mining.Bioinformatics, 2003.
19((Supppl 1)): p. 180-182.11.
Lafferty, J., A. McCallum, and F. Pereira.Conditional Random Fields: ProbabilisticModels for Segmenting and Labeling SequenceData.
in 18th International Conf.
on MachineLearning.
2001.
San Francisco, CA: MorganKaufmann.12.
McDonald, R. and F. Pereira.
Identifying Geneand Protein Mentions in Text Using Condi-tional Random Fields.
in A critical assessmentof text mining methods in molecular biology,BioCreative 2004.
2004.
Grenada, Spain.13.
Cohen, I. and M. Goldszmidt.
Properties andBenefits of Calibrated Classifiers.
inEMCL/PKDD.
2004.
Pisa, Italy.14.
Culotta, A. and A. McCallum.
Confidence Es-timation for Information Extraction.
in Pro-ceedings of Human Language TechnologyConference and North American Chapter ofthe Association for Computational Linguistics(HLT-NAACL).
2004.
Boston, MA.8
