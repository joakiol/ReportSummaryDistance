Distinguishing Word Senses in Untagged TextTed Pedersen and Rebecca BruceDepar tment  of Computer  Sc ience and  Eng ineer ingSouthern  Method is t  Un ivers i tyDal las,  TX  75275-0112{pedersen , rbruce)@seas .smu.eduAbst ractThis paper describes an experimental com-parison of three unsupervised learning al-gorithms that distinguish the sense ofan ambiguous word in untagged text.The methods described in this paper,McQuitty's similarity analysis, Ward'sminimum-variance method, and the EMalgorithm, assign each instance of an am-biguous word to a known sense definitionbased solely on the values of automaticallyidentifiable features in text.
These meth-ods and feature sets are found to be moresuccessful in disambiguating nouns ratherthan adjectives or verbs.
Overall, the mostaccurate of these procedures i McQuitty'ssimilarity analysis in combination with ahigh dimensional feature set.1 I n t roduct ionStatistical methods for natural language process-ing are often dependent on the availability of costlyknowledge sources uch as manually annotated textor semantic networks.
This limits the applicabilityof such approaches to domains where this hard toacquire knowledge is already available.
This paperpresents three unsupervised learning algorithms thatare able to distinguish among the known senses (i.e.,as defined in some dictionary) of a word, based onlyon features that can be automatically extracted fromuntagged text.The object of unsupervised learning is to deter-mine the class membership of each observation (i.e.each object to be classified), in a sample without us-ing training examples of correct classifications.
Wediscuss three algorithms, McQuitty's imilarity anal-ysis (McQuitty, 1966), Ward's minimum-variancemethod (Ward, 1963) and the EM algorithm (Demp-ster, Laird, and Rubin, 1977), that can be used todistinguish among the known senses of an ambigu-ous word without the aid of disambiguated exam-ples.
The EM algorithm produces maximum likeli-hood estimates of the parameters of a probabilisticmodel, where that model has been specified in ad-vance.
Both Ward's and McQuitty's methods are ag-glomerative clustering algorithms that form classesof unlabeled observations that minimize their respec-tive distance measures between class members.The rest of this paper is organized as follows.First, we present introductions to Ward's and Mc-Quitty's methods (Section 2) and the EM algorithm(Section 3).
We discuss the thirteen words (Section4) and the three feature sets (Section 5) used in ourexperiments.
We present our experimental results(Section 6) and close with a discussion of relatedwork (Section 7).2 Agglomerative ClusteringIn general, clustering methods rely on the assump-tion that classes occupy distinct regions in the fea-ture space.
The distance between two points in amulti-dimensional space can be measured using anyof a wide variety of metrics (see, e.g.
(Devijverand Kittler, 1982)).
Observations are grouped inthe manner that minimizes the distance between themembers of each class.Ward's and McQuitty's method are agglomerativeclustering algorithms that differ primarily in howthey compute the distance between clusters.
Allsuch algorithms begin by placing each observationin a unique cluster, i.e.
a cluster of one.
The twoclosest clusters are merged to form a new clusterthat replaces the two merged clusters.
Merging ofthe two closest clusters continues until only somespecified number of clusters remain.However, our data does not immediately lend it-self to a distance-based interpretation.
Our featuresrepresent part-of-speech (POS) tags, morphologicalcharacteristics, and word co-occurrence; such fea-tures are nominal and their values do not have scale.Given a POS feature, for example, we could choosenoun = 1, verb = 2, adjective = 3, and adverb =4.
That adverb is represented by a larger numberthan noun is purely coincidental nd implies nothingabout the relationship between ouns and adverbs.Thus, before we employ either clustering algo-19710 2 51 2 13 2 510 2 5Figure 1: Matrix of Feature Values0 2 1 02 0 2 21 2 0 10 2 1 0Figure 2: Dissimilarity Matrixrithm, we represent our data sample in terms of adissimilarity matrix.
Suppose that we have N ob-servations in a sample where each observation has qfeatures.
This data is represented in a N x N dis-similarity matrix such that the value in cell ( i , j) ,where i represents the row number and j representsthe column, is equal to the number of features inobservations i and j that do not match.For example, in Figure 1 we have four observa-tions.
We record the values of three nominal fea-tures for each observation.
This sample can be rep-resented by the 4 x 4 dissimilarity matrix shown inFigure 2.
In the dissimilarity matrix, cells (1, 2) and(2, 1) have the value 2, indicating that the first andsecond observations in Figure 1 have different valuesfor two of the three features.
A value of 0 indicatesthat observations i and j are identical.When clustering our data, each observation is rep-resented by its corresponding row (or column) in thedissimilarity matrix.
Using this representation, ob-servations that fall close together in feature space arelikely to belong to the same class and are groupedtogether into clusters.
In this paper, we use Ward'sand McQuitty's methods to form clusters of obser-vations, where each observation is represented by arow in a dissimilarity matrix.2.1 Ward 's  minimum-var iance methodIn Ward's method, the internal variance of a clusteris the sum of squared distances between each obser-vation in the cluster and the mean observation forthat cluster (i.e., the average of all the observationsin the cluster).
At each step in Ward's method, anew cluster, CKL, with the smallest possible inter-nal variance, is created by merging the two clusters,CK and CL, that have the minimum variance be-tween them.
The variance between CK and eL iscomputed as follows:II~K -~rII 2VKL-  , + I (1)NK I~FLwhere XK is the mean observation for cluster CK,NK is the number of observations in CK, and ~Land NL are defined similarly for CL.Implicit in Ward's method is the assumption thatthe sample comes from a mixture of normal distri-butions.
While NLP data is typically not well char-acterized by a normal distribution (see, e.g.
(Zipf,1935), (Pedersen, Kayaalp, and Bruce, 1996)), thereis evidence that our data, when represented by a dis-similarity matrix, can be adequately characterizedby a normal distribution.
However, we will continueto investigate the appropriateness of this assump-tion.2.2 McQuitty's similarity analysisIn McQuitty's method, clusters are based on a sim-ple averaging of the feature mismatch counts foundin the dissimilarity matrix.At each step in McQuitty's method, a new cluster,CKL, is formed by merging the clusters CK and CLthat have the fewest number of dissimilar featuresbetween them.
The clusters to be merged, CK andCL, are identified by finding the cell (/, k) (or (k, I)),where k ~ l, that has the minimum value in thedissimilarity matrix.Once the new cluster CKL is created, the dissim-ilarity matrix is updated to reflect the number ofdissimilar features between CKL and all other exist-ing clusters.
The dissimilarity between any existingcluster Ci and CKL is computed as:DgI -l- DLIDKL-I  = 2 (2)where DKi is the number of dissimilar features be-tween clusters CK and Ci and DLI is similarly de-fined for clusters CL and C1.
This is simply theaverage number of mismatches between each com-ponent of the new cluster and the existing cluster.Unlike Ward's method, McQuitty's method makesno assumptions concerning the distribution of thedata sample.3 EM A lgor i thmThe expectation maximization algorithm (Demp-ster, Laird, and Rubin, 1977), commonly known asthe EM algorithm, is an iterative estimation proce-dure in which a problem with missing data is recastto make use of complete data estimation techniques.In our work, the sense of an ambiguous word is rep-resented by a feature whose value is missing.In order to use the EM algorithm, the paramet-ric form of the model representing the data mustbe known.
In these experiments, we assume thatthe model form is the Naive Bayes (Duda andHart, 1973).
In this model, all features are con-ditionally independent given the value of the clas-sification feature, i.e., the sense of the ambigu-ous word.
This assumption is based on the suc-198cess of the Naive Bayes model when applied to su-pervised word-sense disambiguation (e.g.
(Gale,Church, and Yarowsky, 1992), (Leacock, Towell, andVoorhees, 1993), (Mooney, 1996), (Pedersen, Bruce,and Wiebe, 1997), (Pedersen and Bruce, 1997a)).There are two potential problems when using theEM algorithm.
First, it is computationally expen-sive and convergence can be slow for problems withlarge numbers of model parameters.
Unfortunatelythere is little to be done in this case other than re-ducing the dimensionality of the problem so thatfewer parameters are estimated.
Second, if the like-lihood function is very irregular it may always con-verge to a local maxima nd not find the global max-imum.
In this case, an alternative is to use the morecomputationally expensive method of Gibbs Sam-pling (Geman and Geman, 1984).3.1 DescriptionAt the heart of the EM Algorithm lies the Q-function.
This is the expected value of the log-likelihood function for the complete data D = (Y, S),where Y is the observed ata and S is the missingsense value:Q(/9/1/9) = E\[lnp(Y, SI/9')I/9, Y)\] (3)Here, /9 is the current value of the maximum likeli-hood estimates ofthe model parameters and/9i s theimproved estimate that we are seeking; p(Y, SI/9 i) isthe likelihood of observing the complete data giventhe improved estimate of the model parameters.When approximating the maximum of the likeli-hood function, the EM algorithm starts from a ran-domly generated initial estimate of/9 and then re-places /9 by the /9i which maximizes Q(/9/I/9)- Thisprocess is broken down into two steps: expecta-tion (the E-step), and maximization (the M-step).The E-step finds the expected values of the sufficientstatistics of the complete model using the current es-timates of the model parameters.
The M-step makesmaximum likelihood estimates of the model param-eters using the sufficient statistics from the E-step.These steps iterate until the parameter estimates/9and/91 converge.The M-step is usually easy, assuming it is easyfor the complete data problem; the E-step is notnecessarily so.
However, for decomposable models,such as the Naive Bayes, the E-step simplifies to thecalculation of the expected counts in the marginaldistributions of interdependent features, where theexpectation is with respect o/9.
The M-step sim-plifies to the calculation of new parameter stimatesfrom these counts.
Further, these expected countscan be calculated by multiplying the sample size Nby the probability of the complete data within eachmarginal distribution given/9 and the observed atawithin each marginal Ym- This simplifies to:counti(Sm, Y,~) = P(SmIYm.)
x count(Ym)where count i is the current estimate of the expectedcount and P(Sm \[Ym) is formulated using 0.3.2 ExampleFor the Naive Bayes model with 3 observable fea-tures A, B, C and an unobservable classification fea-ture S, where 8 = {P(a, s), P(b, s), P(c, s), P(s)},the E and M-steps are:1.
E-step: The expected values of the sufficientstatistics are computed as follows:eoun#(s, a) = P(sla) x count(a)coun#(s, b) -= P(slb) ?
count(b)eounti(s, c) = P(slc) x count(c)count'(s) -- ~ {P(sla, b, c) x count(a, b, e)}a,b,cwhere:P(sla) = E P(sla' b, c)hieP(sla, b, c) = P(s, a, b, c)P(a, b, c)P(s, a, b, c) = P(s, a) x P(s, b) x P(s, c)P(s) ~P(a, b, c) = E P(s, a) ?
P(s, b) ?
P(s, c), P(s) 22.
M-step: The sufficient statistics from the E-step are used to re-estimate he model param-eters/9i:Pi(s, a) = c?unti(s' a)Npi(s, b) - -  c?unti(s' b)NPi(s, c) -- 'c?unti(s' c)Ncounti(s) Pi(s) = Nwhere s, a, b, and c denote specific values of S, A, B,and C respectively, and P(slb) and P(s\]c) are de-fined analogously to P(sIa ).4 Exper imenta l  P rocedureExperiments were conducted to disambiguate 13dif-ferent words using 3 different feature sets.
In theseexperiments, each of the 3 unsupervised disambigua-tion methods i applied to each of the 13 words usingeach of the 3 feature sets; this defines a total of 117different experiments.
In addition, each experimentwas repeated 25 times in order to study the varianceintroduced by randomly selecting initial parameterestimates, in the case of the EM algorithm, and ran-domly selecting among equally distant groups whenclustering using Ward's and McQuitty's methods.199In order to evaluate the unsupervised learning al-gorithms we use sense-tagged text in these exper-iments.
However, this text is only used to evalu-ate the accuracy of our methods.
The classes dis-covered by the unsupervised learning algorithms aremapped to dictionary senses in a manner that max-imizes their agreement with the sense-tagged text.If the sense-tagged text were not available, as wouldoften be the case in an unsupervised experiment, thismapping would have to be performed manually.The words disambiguated and their sense distri-butions are shown in Figure 3.
All data, with the ex-ception of the data for line, come from the ACL/DCIWall Street Journal corpus (Marcus, Santorini, andMarcinkiewicz, 1993).
With the exception of line,each ambiguous word is tagged with a single sensedefined in the Longman Dictionary of Contempo-rary English (LDOCE) (Procter, 1978).
The datafor the 12 words tagged using LDOCE senses aredescribed in more detail in (Bruce, Wiebe, and Ped-ersen, 1996).The line data comes from both the ACL/DCIWSJ corpus and the American Printing House forthe Blind corpus.
Each occurrence of line is taggedwith a single sense defined in WordNet (Miller,1995).
This data is described in more detail in (Lea-cock, Towell, and Voorhees, 1993).Every experiment utilizes all of the sentencesavailable for each word.
The number of sentencesavailable per word is shown as "total count" in Fig-ure 3.
We have reduced the sense inventory of thesewords so that only the two or three most frequentsenses are included in the text being disambiguated.For several of the words, there are minority sensesthat form a very small percentage (i.e., < 5%) ofthe total sample.
Such minority classes are not yetwell handled by unsupervised techniques; thereforewe do not consider them in this study.5 Feature  SetsWe define three different feature sets for use in theseexperiments.
Our objective is to evaluate the effectthat different ypes of features have on the accuracyof unsupervised learning algorithms uch as thosediscussed here.
We are particularly interested in theimpact of the overall dimensionality of the featurespace, and in determining how indicative differentfeature types are of word senses.
Our feature sets arecomposed of various combinations of the followingfive types of features.Morpho logy  The feature M represents the mor-phology of the ambiguous word.
For nouns, M isbinary indicating singular or plural.
For verbs, thevalue of M indicates the tense of the verb and canhave up to 7 possible values.
This feature is not usedfor adjectives.Adjective Senses ~'1chief.
(total count: 1048)highest in rank: 86most important; main: 14~'c~common: (total count: 1060) !as in the phrase 'common stock': 84~belonging to or shared by 2 or more: 8~happening often; usual: 8?~lasl: (total count: 3004)on the occasion nearest in the past: 94?~after all others: 6c~public: (total count: 715)concerning people in general: 68cXconcerning the government and people: 19~not secret or private: 13?~Noun Sensesbill: (total count: 1341)a proposed law under consideration: 68~a piece of paper money or treasury bill: 22?~a list of things bought and their price: 10~concern: (total count: 1235) |a business; firm: 64 %worry; anxiety: 36c~drug: (total count: 1127)a medicine; used to make medicine: 57?~a habit-forming substance: 43% !interest: (total count: 2113) \]money paid for the use of money: 59 %a share in a company or business: 24%readiness to give attention: 17%line: (total count: 1149)a wire connecting telephones: 37 %a cord; cable: 32%an orderly series: 30%Verb Sensesagree: (total count: 1109)to concede after disagreement: 74~to share the same opinion: 26% Iclose: (total count: 1354)to (cause to) end: 77%to (cause to) stop operation: 23%help: (total count: 1267)to enhance - inanimate object: 78?~to assist - human object: 22~include: (total count: 1526) Ito contain in addition to other parts: 91%to be a part of -  human subject: 9~Figure 3: Distribution of Senses200wordchiefcommonlastpublicbillconcerndruginterestlineagreeclosehelpincludeC1officershareyearofferingtreasurymillionfdaratehemilliontradingitmillionC2 Qexecutive presidentmillion stockweek millionmillion companybillion housecompany marketcompany genericmillion companyit telephonecompany payexchange stocksay hecompany yearFigure 4: Co-occurrence FeaturesPar t -o f -Speech  Features of the form PLi repre-sent the part-of-speech (POS) of the word i posi-tions to the left of the ambiguous word.
PRi repre-sents the POS of the word i positions to the right.In these experiments, we used 4 POS features, PL1,PL2, PR1, and PR2 to record the POS of the words1 and 2 positions to the left and right of the am-biguous word.
Each POS feature can have one of5 possible values: noun, verb, adjective, adverb orother.Co-occur rences  Features of the form Ci are bi-nary co-occurrence f atures.
They indicate the pres-ences or absences of a particular content word in thesame sentence as the ambiguous word.
We use 3 bi-nary co-occurrence f atures, C1, C2, and Ca to rep-resent he presences or absences of each of the threemost frequent content words, C1 being the most fre-quent content word, C2 the second most frequentand C3 the third.
Only sentences containing the am-biguous word were used to establish word frequen-cies.Frequency based features like this one contain lit-tle information about low frequency classes.
Forwords with skewed sense distribution, it is likely thatthe most frequent content words will be associatedonly with the dominate sense.As an example, consider the 3 most frequent con-tent words occurring in the sentences that containchi@ officer, executive and president.
Chief has amajority class distribution of 86% and, not surpris-ingly, these three content words are all indicative ofthe dominate sense which is "highest in rank".The set of content words used in formulating theco-occurrence f atures are shown in Figure 4.
Notethat million and company occur frequently.
Theseare not likely to be indicative of a particular sensebut more reflect he general nature of the Wall StreetJournal corpus.Unrest r i c ted  Co l locat ions  Features of the formULi and URi indicate the word occurring in the po-sition i places to the left or right, respectively, of theambiguous word.
All features of this form have 21possible values.
Nineteen correspond to the 19 mostfrequent words that occur in that fixed position inall of the sentences that contain the particular am-biguous word.
There is also a value, (none), thatindicates when the position i to the left or right isoccupied by a word that is not among the 19 mostfrequent, and a value, (null), indicating that the po-sition i to the left or right falls outside of the sentenceboundary.In these experiments we use 4 unrestricted collo-cation features, UL2, UL1,UR1, and UR2.
As anexample, the values of these features for concern areas follows:?
UL2: and, the, a, of, to, financial, have, be-cause, an, 's, real, cause, calif., york, u.s., other,mass., german, (null), (none)?
UL1 : the, services, of, products, banking, 's,pharmaceutical, energy, their, expressed, elec-tronics, some, biotechnology, aerospace, en-vironmental, such, japanese, gas, investment,(null), (none)?
URI: about, said, that, over, 's, in, with, had,are, based, and, is, has, was, to, for, among,will, did, (null), (none)?
URn: the, said, a, it, in, that, to, n't, is, which,by, and, was, has, its, possible, net, but, annual,(null), (none)Content  Col locat ions  Features of the form CL1and CR1 indicate the content word occurring in theposition 1 place to the left or right, respectively, ofthe ambiguous word.
The values of these featuresare defined much like the unrestricted collocationsabove, except hat these are restricted to the 19 mostfrequent content words that occur only one positionto the left or right of the ambiguous word.To contrast this set of features with the unre-stricted collocations, consider concern again.
Thevalues of the features representing the 19 most fre-quent content words 1 position to the left and rightare as follows:?
CLI: services, products, banking, pharmaceu-tical, energy, expressed, electronics, biotechnol-ogy, aerospace, environmental, japanese, gas,Feature  Sets A, B and C The 3 featureused in these experiments are designated A, BC and are formulated as follows:investment, food, chemical, broadcasting, u.s.,industrial, growing, (null), (none)CRi: said, had, are, based, has, was, did,owned, were, regarding, have, declined, ex-pressed, currently, controlled, bought, an-nounced, reported, posted, (null), (none)setsand201* A: M, PLe, PL1, PRx, PR~, C1, C2, C3Dimensionality: 5,000 - 35,000?
B: M, UL2, UL1,UR1,UR2Dimensionality: 194,481- 1,361,367?
C: M, PL2, PL1, PR1, PRy, CL1, CR1Dimensionality: 275,625- 1,929,375The dimensionality is the number of possible com-binations of feature values and thus the size of thefeature space.
These values vary since the number ofpossible values for M varies with the part-of-speechof the ambiguous word.
The lower number is asso-ciated with adjectives and the higher with verbs.To get a feeling for the adequacy of these featuresets, we performed supervised learning experimentswith the interest data using the Naive Bayes model.We disambiguated 3 senses using a 10:1 training-to-test ratio.
The average accuracies for each featureset over 100 random trials were as follows: A 80.9%,B 87.7%, and C 82.7%.The window size, the number of values for thePOS features, and the number of words consideredin the collocation features are kept deliberately smallin order to control the dimensionality of the prob-lem.
In future work, we will expand all of the abovetypes of features and employ techniques to reducedimensionality along the lines suggested in (Dudaand Hart, 1973) and (Gale, Church, and Yarowsky,1995).6 Exper imenta l  Resu l tsFigure 5 shows the average accuracy and standarddeviation of disambiguation over 25 random trialsfor each combination of word, feature set and learn-ing algorithm.
Those cases where the average accu-racy of one algorithm for a particular feature setis significantly higher than another algorithm, asjudged by the t-test (p=.01), are shown in bold face.For each word, the most accurate overall experiment(i.e., algorithm/feature s t combination), and thosethat are not significantly less accurate are under-lined.
Also included in Figure 5 is the percentage ofeach sample that is composed of the majority sense.This is the accuracy that can be obtained by a ma-jority classifier; a simple classifier that assigns eachambiguous word to the most frequent sense in a sam-ple.
However, bear in mind that in unsupervised ex-periments the distribution of senses is not generallyknown.Perhaps the most striking aspect of these resultsis that, across all experiments, only the nouns aredisambiguated with accuracy greater than that ofthe majority classifier.
This is at least partially ex-plained by the fact that, as a class, the nouns havethe most uniform distribution of senses.
This pointwill be elaborated on in Section 6.1.
While the choiceof feature set impacts accuracy, overall it is only toa small degree.
We return to this point in Section6.2.
The final result, to be discussed in Section 6.3,is that the differences in the accuracy of these threealgorithms are statistically significant both on aver-age and for individual words.6.1 Distribution of ClassesExtremely skewed distributions pose a challenginglearning problem since the sample contains preciouslittle information regarding minority classes.
Thismakes it difficult to learn their distributions with-out prior knowledge.
For unsupervised approaches,this problem is exacerbated by the difficultly in dis-tinguishing the characteristics of the minority classesfrom noise.In this study, the accuracy of the unsupervised al-gorithms was less than that of the majority classifierin every case where the percentage of the majoritysense exceeded 68%.
However, in the cases wherethe performance of these algorithms was less thanthat of the majority classifier, they were often stillproviding high accuracy disambiguation (e.g., 91%accuracy for last).
Clearly, the distribution of classesis not the only factor affecting disambiguation accu-racy; compare the performance of these algorithmson bill and public which have roughly the same classdistributions.It is difficult to quantify the effect of the distri-bution of classes on a learning algorithm particu-larly when using naturally occurring data.
In previ-ous unsupervised experiments with interest, using amodified version of Feature Set A, we were able toachieve an increase of 36 percentage points over theaccuracy of the majority classifier when the 3 classeswere evenly distributed in the sample (Pedersen andBruce, 1997b).
Here, our best performance using alarger sample with a natural distribution of sensesis only an increase of 20 percentage points over theaccuracy of the majority classifier.Because skewed distributions are common in lexi-cal work (Zipf, 1935), they are an important consid-eration in formulating disambiguation experiments.In future work, we will investigate procedures forfeature selection that are more sensitive to minor-ity classes.
Reliance on frequency based features, asused in this work, means that the more skewed thesample is, the more likely it is that the features willbe indicative of only the majority class.6.2 Feature SetDespite varying the feature sets, the relative accu-racy of the three algorithms remains rather consis-tent.
For 6 of the 13 words there was a single al-gorithm that was always significantly more accuratethan the other two across all features.The EM algorithm was most accurate for last andline with all three feature sets.
McQuitty's methodwas significantly more accurate for chief, common,public, and help regardless of the feature set.202chiefcommonlastpublicadjectivesMll:oncern:trugnterestine3ouns~gree:loseaelpincludeFeature SetMaj.
McQuitty.861.844?.05.842.648?.12!.940 .791?.121.683.560?
.08.832.711?.15.681.669?.08.638 .629?.07.567.530?.03.593.601?.04.373 .420?.03.570 .570?.10A Feature SetWard I EM McQuitty Ward.721?.01.729?.06 .831?.06.611?.01.513?.08.521:t=.00 797?.04.444?.04.598?.09,903?.00 .541 ?.
11 .659?.03.450?.05.473?.03 .558?.07.461?.03.
71?.
12.657?.
18 .682=t=.15.544?.10.647?.
11.537?.05 .753?.05.600?.04.741?.04 842?.00 .679?.04.697?.02.557?.06,658?.03 .521?.01 .528?.00~619?.04,616?.06.653?.06.552?.06.441?.03.457?.01 .403?.02.428?.03.601?.
12,622?.
14 .602?.
11 .561?.
10.547?.03.631?.08.678?.08.531?.02.560?.08 .667?.07.664?.0C.591?.05.586?.05 .636?.
11.519?.01.707?.08.725?.02 .767?.09.770?.0~BEM.646?.01.464?.06.909?.00.411?.03.6 8?.20.624?.08.840?.02.551?.05.615?.05.474?.03.621?.13.740.610?.08 .613?.04\[,683?.14.771 !.616?.09 .672?.06.780.713?.0  .526?.00.9101.880?.06 ,.783?.07Feature Set CMcQuitty.856?.00.799?.06.636?.07.628?.05.730?.11.561?.10.614?.08.573?.06.651?.02.410?.02.562?.10.685?.07.720?.11.5--66?.o6.~?.17Ward  EM.673?.03.697?.06.561?.05.543?.09.601?.08,874?.07.488?.04.507?.03.581?.08.655?.16.515?.04,569?.04.758?.04,758?.09.632?
.06,652?.04.615?.04,649?.09.427?.02.458?.01.589?.12,617?.12.601?.00,685?.14;.645?.04.648?.05.570=t=.03.602?.031.558?.04.535?.00verbs 1.800\[.705 .131.594?.08\[.626?.09Jl.687 .101.642 .10 666:t: .1211.718?.111.593:t:.05\[.618?.09 Iverall \[734655?4\[589?.634?4\[l.653h2\[.58.1\[63?.6ll6-6.2?3\[588?9l.629?3Figure 5: Experimental Results- accuracy ?
standard eviationDespite this consistency, there were some observ-able trends associated with changes in feature set.For example, McQuitty's method was significantlymore accurate overall in combination with featureset C while the EM algorithm was more accuratewith Feature Set A, and the accuracy of Ward'smethod was the least favorable with Feature Set B.For the nouns, there was no significant differ-ence between Feature Sets A and B when usingthe EM algorithm.
For the verbs there was nosignificant difference between the three feature setswhen using McQuitty's method.
The adjectives weresignificantly more accurate when using McQuitty'smethod and Feature Set C.One possible explanation for the consistency ofresults as feature sets varied is that perhaps the fea-tures most indicative of word senses are included inall the sets due to the selection methods and thecommonality of feature types.
These common fea-tures may be sufficient for the level of disambigua-tion achieved here.
This explanation seems moreplausible for the EM algorithm, where features areweighted, but less so for McQuitty's and Ward'swhich use a representation that does not allow fea-ture weighting.6.3 D isambiguat ion  A lgor i thmBased on the average accuracy over part-of-speechcategories, the EM algorithm performs with thehighest accuracy for nouns while McQuitty's methodperforms most accurately for verbs and adjectives.This is true regardless of the feature set employed.The standard eviations give an indication of theeffect of ties on the clustering algorithms and theeffect of the random initialization on the the EM al-gorithm.
In few cases is the standard eviation verysmall.
For the clustering algorithms, a high standarddeviation indicates that ties are having some effecton the cluster analysis.
This is undesirable and maypoint to a need to expand the feature set in order toreduce ties: For the EM algorithm, a high standarddeviation means that the algorithm is not settling onany particular maxima.
Results may become moreconsistent if the number of parameters that must beestimated was reduced.Figures 6, 7 and 8 show the confusion matricesassociated with the disambiguation of concern, in-terest, and help, using Feature Sets A, B, and C,respectively.
A confusion matrix shows the numberof cases where the sense discovered by the algorithmagrees with the manually assigned sense along themain diagonal; disagreements are shown in the restof the matrix.In general, these matrices reveal that both the EMalgorithm and Ward's method are more biased to-ward balanced distributions of senses than is Mc-Quitty's method.
This may explain the better per-formance of McQuitty's method in disambiguatingthose words with the most skewed sense distribu-tions, the adjectives and adverbs.
It is possible toadjust the EM algorithm away from this tendencytowards discovering balanced distributions by pro-viding prior knowledge of the expected sense distri-bution.
This will be explored in future work.203DiscoveredActual worry businessworry 166 281 447business 181 607 788347 888 1235McQuitty - 773 correctDiscoveredActual worry businessworry 288 159 447business 155 633 788443 792 1235Ward-  921 correctActualworrybusinessDiscoveredworry business384 63132 656516 7194477881235EM - 1040 correctFigure 6: concern - Feature Set ADiscoveredActual attention share moneyattention 53 6 302 361share 58 187 255 500money 108 4 1140 1252219 197 1697 2113McQuitty - 1380 correctDiscoveredActual attention share moneyattention 280 3 78 361share 240 197 63 500money 559 0 693 12521079 200 834 2113Ward - 1170 correctDiscoveredActual attention share moneyattention 127 230 4 361share 134 364 2 500money 320 124 808 1252581 718 814 2113EM - 1299 correctFigure 7: interest - Feature Set BDiscoveredActual assist enhanceassist 45 234 279enhance 146 842 988191 1076 1267McQuitty - 887 correctActualassistenhanceDiscoveredassist enhance88 191354 634442 8252799881267Ward - 722 correctActualassistenhanceDiscoveredassist enhance119 160 279344 644 988463 804 1267EM - 763 correctFigure 8: help - Feature Set C7 Re la ted  WorkWord-sense disambiguation has more commonlybeen cast as a problem in supervised learning (e.g.,(Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993),(Leacock, Towell, and Voorhees, 1993), (Bruce andWiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996),(Pedersen, Bruce, and Wiebe, 1997), (Pedersen andBruce, 1997a)).
However, all of these methods re-quire that manually sense tagged text be availableto train the algorithm.
For most domains uch textis not available and is expensive to create.
It seemsmore reasonable to assume that such text will notusually be available and attempt o pursue unsuper-vised approaches that rely only on the features in atext that can be automatically identified.7.1 BootstrappingBootstrapping approaches require a small amountof disambiguated text in order to initialize the un-supervised learning algorithm.
An early example ofsuch an approach is described in (Hearst, 1991).
Asupervised learning algorithm is trained with a smallamount of manually sense tagged text and appliedto a held out test set.
Those examples in the test setthat are most confidently disambiguated are addedto the training sample.A more recent bootstrapping approach is de-scribed in (Yarowsky, 1995).
This algorithm requiresa small number of training examples to serve as aseed.
There are a variety of options discussed for204automatically selecting seeds; one is to identify col-locations that uniquely distinguish between senses.For plant, the collocations manufacturing plant andliving plant make such a distinction.
Based on 106examples of manufacturing plant and 82 examples ofliving plant this algorithm is able to distinguish be-tween two senses of plant for 7,350 examples with 97percent accuracy.
Experiments with 11 other wordsusing collocation seeds result in an average accuracyof 96 percent.While (Yarowsky, 1995) does not discuss distin-guishing more than 2 senses of a word, there is noimmediate reason to doubt that the "one sense percollocation" rule (Yarowsky, 1993) would still holdfor a larger number of senses.
In future work wewill evaluate using the "one sense per collocation"rule to seed our various methods.
This may helpin dealing with very skewed distributions of sensessince we currently select collocations based simplyon frequency.7.2 C luster ingClustering has most often been applied in naturallanguage processing as a method for inducing syn-tactic or semantically related groupings of words(e.g., (Rosenfeld, Huang, and Schneider, 1969),(Kiss, 1973), (Ritter and Kohonen, 1989), (Pereira,Tishby, and Lee, 1993), (Sch/itze, 1993), (Resnik,1995a)).An early application of clustering to word-sensedisambiguation is described in (Sch/itze, 1992).There words are represented in terms of the co-occurrence statistics of four letter sequences.
Thisrepresentation uses 97 features to characterize aword, where each feature is a linear combination ofletter four-grams formulated by a singular value de-composition of a 5000 by 5000 matrix of letter four-gram co-occurrence frequencies.
The weight associ-ated with each feature reflects all usages of the wordin the sample.
A context vector is formed for eachoccurrence of an ambiguous word by summing thevectors of the contextual words (the number of con-textual words considered in the sum is unspecified).The set of context vectors for the word to be dis-ambiguated are then clustered, and the clusters aremanually sense tagged.The features used in this work are complex anddifficult to interpret and it isn't clear that this com-plexity is required.
(Yarowsky, 1995) compares hismethod to (Schiitze, 1992) and shows that for fourwords the former performs ignificantly better in dis-tinguishing between two senses.Other clustering approaches to word-sense disam-biguation have been based on measures of semanticdistance defined with respect o a semantic networksuch as WordNet.
Measures of semantic distanceare based on the path length between concepts in anetwork and are used to group semantically similarconcepts (e.g.
(Li, Szpakowicz, and Matwin, 1995)).
(Resnik, 1995b) provides an information theoreticdefinition of semantic distance based on WordNet.
(McDonald et al, 1990) apply another cluster-ing approach to word-sense disambiguation (alsosee (Wilks et al, 1990)).
They use co-occurrencedata gathered from the machine-readable version ofLDOCE to define neighborhoods of related words.Conceptually, the neighborhood of a word is a typeof equivalence class.
It is composed of all other wordsthat co-occur with the designated word a significantnumber of times in the LDOCE sense definitions.These neighborhoods are used to increase the num-ber of words in the LDOCE sense definitions, whilestill maintaining some measure of lexical cohesion.The "expanded" sense definitions are then comparedto the context of an ambiguous word, and the sense-definition with the greatest number of word over-laps with the context is selected as correct.
(Guthrieet al, 1991) propose that neighborhoods be subjectdependent.
They suggest hat a word should po-tentially have different neighborhoods correspond-ing to the different LDOCE subject code.
Subject-specific neighborhoods are composed of words hav-ing at least one sense marked with that subject code.7.3 EM algorithmThe only other application of the EM algorithmto word-sense disambiguation is described in (Gale,Church, and Yarowsky, 1995).
There the EM algo-rithm is used as part of a supervised learning algo-rithm to distinguish city names from people's names.A narrow window of context, one or two words toeither side, was found to perform better than widerwindows.
The results presented are preliminary butshow an accuracy percentage in the mid-ninetieswhen applied to Dixon, a name found to be quiteambiguous.It should be noted that the EM algorithm relatesto a large body of work in speech processing.
TheBaum-Welch forward-backward algorithm (Baum,1972) is a specialized form of the EM algorithmthat assumes the underlying parametric model is ahidden Markov model.
The Baum-Welch forward-backward algorithm has been used extensively inspeech recognition (e.g.
(Levinson, Rabiner, andSondhi, 1983), (Kupiec, 1992)), (Jelinek, 1990)).8 Conclus ionsSupervised learning approaches to word-sense dis-ambiguation fall victim to the knowledge acquisi-tion bottleneck.
The creation of sense tagged textsufficient o serve as a training sample is expensiveand time consuming.
This bottleneck is eliminatedthrough the use of unsupervised learning approacheswhich distinguish the sense of a word based only onfeatures that can be automatically identified.In this study, we evaluated the performance ofthree unsupervised learning algorithms on the dis-205ambiguation f 13 words in naturally occurring text.The algorithms are McQuitty's imilarity analysis,Ward's minimum-variance m thod, and the EM al-gorithm.
Our findings show that each of these al-gorithms is negatively impacted by highly skewedsense distributions.
Our methods and feature setswere found to be most successful in disambiguatingnouns rather than adjectives or verbs.
Overall, themost successful of our procedures was McQuitty'ssimilarity analysis in combination with a high di-mensional feature set.
In future work, we will inves-tigate modifications of these algorithms and featureset selection that are more effective on highly skewedsense distributions.9 AcknowledgmentsThis research was supported by the Office of NavalResearch under grant number N00014-95-1-0776.Re ferencesBaum, L. 1972.
An inequality and associated max-imization technique in statistical estimation forprobabilistic functions of a Markov process.
InO.
Shisha, editor, Inequalities, volume 3.
Aca-demic Press, New York, NY, pages 1-8.Black, E. 1988.
An experiment in computationaldiscrimination f English word senses.
IBM Jour-nal of Research and Development, 32(2):185-194.Bruce, R. and J. Wiebe.
1994.
Word-sense disam-biguation using decomposable models.
In Proceed-ings of the 32rid Annual Meeting of the Associ-ation for Computational Linguistics, pages 139-146.Bruce, R., J. Wiebe, and T. Pedersen.
1996.
Themeasure of a model.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 101-112.Dempster, A., N. Laird, and D. Rubin.
1977.
Maxi-mum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical SocietyB, 39:1-38.Devijver, P. and J. Kittler.
1982.
Pattern Classi-fication: A Statistical Approach.
Prentice Hall,Englewood Cliffs, NJ.Duda, R. and P. Hart.
1973.
Pattern Classificationand Scene Analysis.
Wiley, New York, NY.Gale, W., K. Church, and D. Yarowsky.
1992.
Amethod for disambiguating word senses in a largecorpus.
Computers and the Humanities, 26:415-439.Gale, W., K. Church, and D. Yarowsky.
1995.Discrimination decisions for 100,000 dimensionalspaces.. Journal of Operations Research, 55:323-344.Geman, S. and D. Geman.
1984.
Stochastic re-laxation, Gibbs distributions and the Bayesianrestoration of images.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 6:721-741.Guthrie, J., L. Guthrie, Y. Wilks, and H. Aidine-jad.
1991.
Subject-dependent co-occurrence andword sense disambiguation.
In Proceedings ofthe 29th Meeting of the Association for Computa-tional Linguistics, pages 146-152, Berkeley, CA,June.Hearst, M. 1991.
Noun homograph disambiguationusing local context in large text corpora.
In Pro-ceedings of the 7th Annual Conference of the UWCentre for the New OED and Text Research: Us-ing Corpora, Oxford.Jelinek, F. 1990.
Self-organized language model-ing for speech recognition.
In Waibel and Lee,editors, Readings in Speech Recognition.
MorganKaufmann, San Mateo, CA.Kiss, G. 1973.
Grammatical word classes: A learn-ing process and its simulation.
Psychology ofLearning and Motivation, 7:1-41.Kupiec, J.
1992.
Robust part-of-speech tagging us-ing a hidden Markov model.
Computer Speech andLanguage, 6:225-243.Leacock, C., G. Towell, and E. Voorhees.
1993.Corpus-based statistical sense resolution.
In Pro-ceedings of the ARPA Workshop on Human Lan-guage Technology, pages 260-265, March.Levinson, S., L. Rabiner, and M. Sondhi.
1983.
Anintroduction to the application of the theory ofprobabilistic functions of a Markov process to au-tomatic speech recognition.
Bell System TechnicalJournal, 62:1035-1074.Li, X., S. Szpakowicz, and S. Matwin.
1995.
AWordNet-based algorithm for word sense disam-biguation.
In Proceedings of the 14th Interna-tional Joint Conference on Artificial Intelligence,Montreal, August.Marcus, M., B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Lin-guistics, 19(2):313-330.McDonald, J., T. Plate,, and R. Schvaneveldt.
1990.Using pathfinder to extract semantic informationfrom text.
In R. Schvaneveldt, editor, PathfinderAssociative Networks: Studies in Knowledge Or-ganization.
Ablex, Norwood, NJ.McQuitty, L. 1966.
Similarity analysis by recipro-cal pairs for discrete and continuous data.
Edu-cational and Psychological Measurement, 26:825-831.206Miller, G. 1995.
WordNet: A lexical database.Communications of the ACM, 38(11):39-41,November.Mooney, R. 1996.
Comparative experiments on dis-ambiguating word senses: An illustration of therole of bias in machine learning.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 82-91, May.Ng, H.T.
and H.B.
Lee.
1996.
Integrating multi-ple knowledge sources to disambiguate word sense:An exemplar-based approach.
In Proceedings ofthe 3~th Annual Meeting of the Society for Com-putational Linguistics, pages 40-47.Pedersen, T. and R. Bruce.
1997a.
A new super-vised learning algorithm for word sense disam-biguation.
In Proceedings of the Fourteenth Na-tional Conference on Artificial Intelligence, Prov-idence, RI, July.Pedersen, T. and R. Bruce.
1997b.
Unsupervisedtext mining.
Technical Report 97-CSE-9, South-ern Methodist University, June.Pedersen, T., R. Bruce, and J. Wiebe.
1997.
Se-quential model selection for word sense disam-biguation.
In Proceedings of the Fifth Conferenceon Applied Natural Language Processing, pages388-395, Washington, DC, April.Pedersen, T., M. Kayaalp, and R. Bruce.
1996.
Sig-nificant lexical relationships.
In Proceedings of theThirteenth National Conference on Artificial In-telligence, pages 455-460, Portland, OR, August.Pereira, F., N. Tishby, and L. Lee.
1993.
Distri-butional clustering of English words.
In Proceed-ings of the 31st Annual Meeting of the Associ-ation for Computational Linguistics, pages 183-190, Columbus, OH.Procter, P., editor.
1978.
Longman Dictionary ofContemporary English.
Longman Group Ltd., Es-sex, UK.Resnik, P. 1995a.
Disambiguating noun groupingswith respect o WordNet senses.
In Proceedings ofthe Third Workshop on Very Large Corpora, MIT,June.Resnik, P. 1995b.
Using information content o eval-uate semantic similarity in a taxonomy.
In Pro-ceedings of the 14th International Joint Confer-ence on Artificial Intelligence, Montreal, August.Ritter, H. and T. Kohonen.
1989.
Self-organizingsemantic maps.
Biological Cybernetics, 62:241-254.Rosenfeld, A., H. Huang, and V. Schneider.
1969.An application of cluster detection to text andpicture processing.
IEEE Transactions on Infor-mation Theory, 15:672-681.Schfitze, H. 1992.
Dimensions of meaning.
In Pro-ceedings of Supercomputing '92, pages 787-796,Minneapolis, MN.Schfitze, H. 1993.
Word space.
In S. Hanson,J.
Cowan, and C. Giles, editors, Advances inNeural Information Processing Systems 5.
MorganKaufmann Publishers.Ward, J.
1963.
Hierarchical grouping to optimizean objective function.
Journal of the AmericanStatistical Association, 58:236-244.Wilks, Y., D. Fuss, C. Guo, J. McDonald, T. Plate,and B. Slator.
1990.
Providing machine tractabledictionary tools.
In J. Pustejovsky, editor, The-oretical and Computational Issues in Lexical Se-mantics.
MIT Press, Cambridge, MA.Yarowsky, D. 1992.
Word-sense disambiguation us-ing statistical models of Roget's categories trainedon large corpora.
In Proceedings of the 14thInternational Conference on Computational Lin-guistics (COLING-92), pages 454-460, Nantes,France, July.Yarowsky, D. 1993.
One sense per collocation.
InProceedings of the ARPA Workshop on HumanLanguage Technology, pages 266-271.Yarowsky, D. 1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the 33rd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 189-196, Cambridge, MA.Zipf, G. 1935.
The Psycho-Biology of Language.Houghton Mifflin, Boston, MA.207
