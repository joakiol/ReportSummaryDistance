Why can?t Jose?
read?The problem of learning semantic associations in a robot environmentPeter CarbonettoDepartment of Computer ScienceUniversity of British Columbiapcarbo@cs.ubc.caNando de FreitasDepartment of Computer ScienceUniversity of British Columbianando@cs.ubc.caAbstractWe study the problem of learning to recogniseobjects in the context of autonomous agents.We cast object recognition as the process ofattaching meaningful concepts to specific re-gions of an image.
In other words, given aset of images and their captions, the goal is tosegment the image, in either an intelligent ornaive fashion, then to find the proper mappingbetween words and regions.
In this paper, wedemonstrate that a model that learns spatial re-lationships between individual words not onlyprovides accurate annotations, but also allowsone to perform recognition that respects thereal-time constraints of an autonomous, mobilerobot.1 IntroductionIn writing this paper we hope to promote a discussion onthe design of an autonomous agent that learns semanticassociations in its environment or, more precisely, thatlearns to associate regions of images with discrete con-cepts.
When an image region is labeled with a conceptin an appropriate and consistent fashion, we say that theobject has been recognised (Duygulu et al, 2002).
Weuse our laboratory robot, Jose?
(Elinas et al, 2002), as aprototype, but the ideas presented here extend to a widevariety of settings and agents.Before we proceed, we must elucidate on the require-ments for achieving semantic learning in an autonomousagent context.Primarily, we need a model that learns associations be-tween objects given a set of images paired with user in-put.
Formally, the task is to find a function that separatesthe space of image patch descriptions into nw semanticconcepts, where nw is the total number of concepts in theFigure 1: The image on the left is Jose?
(Elinas et al,2002), the mobile robot we used to collect the image data.The images on the right are examples the robot has cap-tured while roaming in the lab, along with labels used fortraining.
We depict image region annotations in later fig-ures, but we emphasize that the robot receives only thelabels as input for training.
That is, the robot does notknow what words correspond to the image regions.training set (from now on we use the word ?patch?
to re-fer to a contiguous region in an image).
These suppliedconcepts could be in the form of text captions, speech,or anything else that might convey semantic information.For the time being, we restrict the set of concepts to En-glish nouns (e.g.
?face?, ?toothbrush?, ?floor?).
See Fig-ure 1 for examples of images paired with captions com-posed of nouns.
Despite this restriction, we still leaveourselves open to a great deal of ambiguity and uncer-tainty, in part because objects can be described at severaldifferent levels of specificity, and at the same level us-ing different words (e.g.
is it ?sea?, ?ocean?, ?wave?
or?water??).
Ideally, one would like to impose a hierarchyof lexical concepts, as in WordNet (Fellbaum, 1998).
Wehave yet to explore WordNet for our proposed framework,though it has been used successfully for image clustering(Barnard et al, 2001; Barnard et al, 2002).Image regions, or patches, are described by a set oflow-level features such as average and standard deviationof colour, average oriented Gabor filter responses to rep-resent texture, and position in space.
The set of patchdescriptions forms an nf -dimensional space of real num-bers, where nf is the number of features.
Even complexlow-level features are far from adequate for the task ofclassifying patches as objects ?
at some point we needto move to representations that include high-level infor-mation.
In this paper we take a small step in that directionsince our model learns spatial relations between concepts.Given the uncertainty regarding descriptions of objectsand their corresponding concepts, we further require thatthe model be probabilistic.
In this paper we use Bayesiantechniques to construct our object recognition model.Implicitly, we need a thorough method for decompos-ing an image into conceptually contiguous regions.
Thisis not only non-trivial, but also impossible without con-sidering semantic associations.
This motivates the seg-mentation of images and learning associations betweenpatches and words as tightly coupled processes.The subject of segmentation brings up another impor-tant consideration.
A good segmentation algorithm suchas Normalized Cuts (Shi and Malik, 1997) can take on theorder of a minute to complete.
For many real-time appli-cations this is an unaffordable expense.
It is importantto abide by real-time constraints in the case of a mobilerobot, since it has to simultaneously recognise and nego-tiate obstacles while navigating in its environment.
Ourexperiments suggest that the costly step of a decoupledsegmentation can be avoided without imposing a penaltyto object recognition performance.Autonomous semantic learning must be considered asupervised process or, as we will see later on, a partially-supervised process since the associations are made fromthe perspective of humans.
This motivates a second re-quirement: a system for the collection of data, ideally inan on-line fashion.
As mentioned above, user input couldcome in the form of text or speech.
However, the col-lection of data for supervised classification is problem-atic and time-consuming for the user overseeing the au-tonomous agent, since the user is required to tediouslyfeed the agent with self-annotated regions of images.
Ifwe relax our requirement on training data acquisition byrequesting captions at an image level, not at a patch level,the acquisition of labeled data is suddenly much less chal-lenging.
Throughout this paper, we use manual annota-tions purely for testing only ?
we emphasize that thetraining data includes only the labels paired with images.We are no longer exploring object recognition as astrict classification problem, and we do so at a cost sincewe are no longer blessed with the exact associations be-tween image regions and nouns.
As a result, the learningproblem is now unsupervised.
For a single training im-age and a particular word token, we must now learn boththe probability of generating that word given an objectdescription and the correct association to one of the re-gions with the image.
Fortunately, there is a straightfor-ward parallel between our object recognition formulationand the statistical machine translation problem of build-ing a lexicon from an aligned bitext (Brown et al, 1993;Al-Onaizan et al, 1999).
Throughout this paper, we rea-son about object recognition with this analogy in mind(Duygulu et al, 2002).What other requirements should we consider?
Sinceour discussion involves autonomous agents, we shouldpursue a dynamic data acquisition model.
We can con-sider the problem of learning an object recognition modelas an on-line conversation between the robot and the user,and it follows the robot should be able to participate.
Ifthe agent ventures into ?unexplored territory?, we wouldlike it to make unprompted requests for more assistance.One could use active learning to implement a scheme forrequesting user input based on what information wouldbe most valuable to classification.
This has yet to be ex-plored for object recognition, but it has been applied tothe related domain of image retrieval (Tong and Chang,2001).
Additionally, the learning process could be cou-pled with reinforcement ?
in other words, the robotcould offer hypotheses for visual input and await feed-back from user.In the next section, we outline our proposed contextualtranslation model.
In Section 3, we weigh the merits ofseveral different error measures for the purposes of eval-uation.
The experimental results on the robot data aregiven in Section 4.
We leave discussion of results andfuture work to the final section of this paper.Figure 2: The alignment variables represent the corre-spondences between label words and image patches.
Inthis example, the correct association is an2 = 4.2 A contextual translation model forobject recognitionIn this paper, we cast object recognition as a machinetranslation problem, as originally proposed in (Duyguluet al, 2002).
Essentially, we translate patches (regionsof an image) into words.
The model acts as a lexicon, adictionary that predicts one representation (words) givenanother representation (patches).
First we introduce somenotation, and then we build a story for our proposed prob-abilistic translation model.We consider a set of N images paired with theircaptions.
Each training example n is composed ofa set of patches {bn1, ..., bnMn} and a set of words{wn1, ..., wnLn}.
Mn is the number of patches in imagen and Ln is the number of words in the image?s caption.Each bnj Rnf is a vector containing a set of feature val-ues representing colour, texture, position, etc, where nf isthe number of features.
For each patch bnj , our objectiveis to align it to a word from the attached caption.
We rep-resent this unknown association by a variable anj , suchthat ainj = 1 if bnj translates to wni; otherwise, ainj = 0.Therefore, p(ainj) , p(anj = i) is the probability thatpatch bnj is aligned with word wni in document n. SeeFigure 2 for an illustration.
nw is the total number ofword tokens in the training set.We construct a joint probability over the translation pa-rameters and latent alignment variables in such a way thatmaximizing the joint results in what we believe should bethe best object recognition model (keeping in mind thelimitations placed by our set of features!).
Without lossof generality, the joint probability isp(b,a|w) =N?n=1Mn?j=1p(anj |an,1:j?1, bn,1:j?1,wn, ?)?
p(bnj |an,1:j , bn,1:j?1,wn, ?)
(1)where wn denotes the set of words in the nth caption,an,1:j?1 is the set of latent alignments 1 to j?1 in imagen, bn,1:j?1 is the set of patches 1 to j?
1, and ?
is the setof model parameters.Generally speaking, alignments between words andpatches depend on all the other alignments in the im-age, simply because objects are not independent ofeach other.
These dependencies are represented ex-plicitly in equation 1.
However, one usually assumesp(anj |an,1:j?1, bn,1:j?1,wn, ?)
= p(anj = i|wn, ?)
toguarantee tractability.
In this paper, we relax the indepen-dence assumption in order to exploit spatial context in im-ages and words.
We allow for interactions between neigh-bouring image annotations through a pairwise Markovrandom field (MRF).
That is, the probability of a patchbeing aligned to a particular word depends on the wordassignments of adjacent patches in the image.
It is rea-sonable to make the assumption that given the alignmentfor a particular patch, translation probability is indepen-dent from the other patch-word alignments.
A simplifiedversion of the graphical model for illustrative purposes isshown in Figure 3.?
(an1,an2)an1bn1 bn2bn3 bn4an2an3 an4?(an3,an4)?
(an1,a n3)?
(an2,a n4)Image nb11w11 w12b14b12b13lion    sky?n1 ?n2?n3 ?n4Figure 3: The graphical model for a simple set with onedocument.
The shaded circles are the observed nodes (i.e.the data).
The white circles are unobserved variables ofthe model parameters.
Lines represent the undirected de-pendencies between variables.
The potential ?
controlsthe consistency between annotations, while the potentials?nj represent the patch-to-word translation probabilities.In Figure 3, the potentials ?nj , p(bnj |w?)
arethe patch-to-word translation probabilities, where w?denotes a particular word token.
We assign a Gaus-sian distribution to each word token, so p(bnj |w?)
=N (bnj ;?w?
,?w?).
The potential ?
(anj , ank) encodesthe compatibility of the two alignments, anj and ank.The potentials are the same for each image.
That is, weuse a single W ?W matrix ?, where W is the number ofword tokens.
The final joint probability is a product of thetranslation potentials and the inter-alignment potentials:p(b,a|w)=N?n=11Zn??
?Mn?j=1Ln?i=1[N (bnj ;?w?
,?w?)?w?(wni)]ainj??
(r,s)  CnLn?i=1Ln?j=1[?
(w?, w)?w?(wni)?w(wnj)]ainr?ajns??
?where ?w?
(wni) = 1 if the ith word in the nth caption isthe word w?
; otherwise, it is 0.To clarify the unsupervised model described up tothis point, it helps to think in terms of counting word-to-patch alignments for updating the model parameters.Loosely speaking, we update the translation parameters?w?
and ?w?
by counting the number of times partic-ular patches are aligned with word w?.
Similarly, weupdate ?
(w?, w) by counting the number of times theword tokensw?
andw are found in adjacent patch align-ments.
We normalize the latter count by the overall align-ment frequency to prevent counting alignment frequen-cies twice.In addition, we use a hierarchical Bayesian scheme toprovide regularised solutions and to carry out automaticfeature weighting or selection (Carbonetto et al, 2003).In summary, our learning objective is to find good val-ues for the unknown model parameters ?
, {?,?, ?, ?
},where ?
and ?
are the means and covariances of theGaussians for each word, ?
is the set of alignment po-tentials and ?
is the set of shrinkage hyper-parameters forfeature weighting.
For further details on how to computethe model parameters using approximate EM and loopybelief propagation, we refer the reader to (Carbonetto etal., 2003; Carbonetto and de Freitas, 2003)3 Evaluation metric considerationsBefore we discuss what makes a good evaluation met-ric, it will help if we answer this question: ?what makesa good image annotation??
As we will see, there is nostraightforward answer.Figure 4: Examples of images which demonstrate that theimportance of concepts has little or no relation to thearea these concepts occupy.
On the left, ?polar bear?is at least pertinent as ?snow?
even though it takes upless area in the image.
In the photograph on the right,?train?
is most likely the focus of attention.It is fair to say that certain concepts in an image aremore prominent than others.
One might take the approachthat objects that consume the most space in an image arethe most important, and this is roughly the evaluation cri-terion used in previous papers (Carbonetto et al, 2003;Carbonetto and de Freitas, 2003).
Consider the image onthe left in Figure 4.
We claim that ?polar bear?
is at leastas important as snow.
There is an easy way to test thisassertion ?
pretend the image is annotated either entirelyas ?snow?
or entirely as ?polar bear?.
In our experience,people find the latter annotation as appealing, if not more,than the former.
Therefore, one would conclude that it isbetter to weight all concepts equally, regardless of size,which brings us to the image on the right.
If we treatall words equally, having many words in a single labelobfuscates the goal of getting the most important con-cept,?train?, correct.Ideally, when collecting user-annotated images for thepurpose of evaluation, we should tag each word with aweight to specify its prominence in the scene.
In practice,this is problematic because different users focus their at-tention on different concepts, not to mention the fact thatit is an burdensome task.For lack of a good metric, we evaluate the proposedtranslation models using two error measures.
Error mea-sure 1 reports an error of 1 if the model annotation withthe highest probability results in an incorrect patch anno-tation.
The error is averaged over the number of patchesin each image, and then again over the number of imagesin the data set.
Error measure 2 is similar, only we av-erage the error over the patches corresponding to word(according to the manual annotations).
The equations aregiven byE .m.
1 , 1NN?n=11MnMn?j=1(1?
?a?nj (a?nj)) (2)E .m.
2 , 1NN?n=11LnLn?i=11|Pni|Pni?j(1?
?a?nj (a?nj)) (3)where Pni is the set of patches in image n that aremanually-annotated using word i, a?nj is the model align-ment with the highest probability, a?nj is the provided?true?
annotation, and ?a?nj (a?nj) is 1 if a?nj = a?nj .Our intuition is that the metric where we weight allconcepts equally, regardless of size, is better overall.
Aswe will see in the next section, our translation models donot perform as well under this error measure.
This is dueto the fact that the joint probability shown in equation 1maximises the first error metric, not the second.
Since theagent cannot know the true annotations beforehand, it isdifficult to construct a model that maximises the seconderror measure, but we are currently pursuing approxima-tions to this metric.4 ExperimentsWe built a data set by having Jose?
the robot roam aroundthe lab taking pictures, and then having laboratory mem-bers create captions for the data using a consistent set ofwords.
For evaluation purposes, we manually annotatedthe images.
The robomedia data set is composed of 107training images and 43 test images 1.
The training andtest sets contain a combined total of 21 word tokens.
Theword frequencies in the labels and manual annotations areshown in figure 5.In our experiments, we consider two scenarios.
In thefirst, we use Normalized Cuts (Shi and Malik, 1997) tosegment the images into distinct patches.
In the secondscenario, we take on the object recognition task with-out the aid of a sophisticated segmentation algorithm,and instead construct a uniform grid of patches over theimage.
Examples of different segmentations are shownalong with the anecdotal results in Figure 8.
For the crudesegmentation, we used patches of height and width ap-proximately 1/6th the size of the image.
We found thatsmaller patches introduced too much noise to the featuresand resulted in poor test performance, and larger patchescontained too many objects at once.
In future work, we1Experiment data and Matlab code are available athttp://www.cs.ubc.ca/?pcarbo.LABEL% ANNOTATION%?
PRECISIONWORD TRAIN TEST?
TRAIN TEST TRAIN TESTbackpack 0.019 0.011 0.008 0.002 0.158 0.115boxes 0.022 0.011 0.038 0.028 0.218 0.081cabinets 0.080 0.066 0.118 0.081 0.703 0.792ceiling 0.069 0.066 0.061 0.063 0.321 0.347chair 0.131 0.148 0.112 0.101 0.294 0.271computer 0.067 0.071 0.052 0.065 0.149 0.144cooler 0.004 n/a 0.002 n/a 0.250 n/adoor 0.084 0.055 0.067 0.042 0.291 0.368face 0.011 0.022 0.001 0.002 0.067 0.042fan 0.022 0.011 0.012 0.005 0.114 0.133filers 0.030 0.033 0.028 0.019 0.064 0.077floor 0.004 n/a 0.004 n/a 0.407 n/aperson 0.022 0.049 0.018 0.040 0.254 0.340poster 0.037 0.033 0.026 0.021 0.471 0.368robot 0.011 0.016 0.008 0.011 0.030 0.014screen 0.041 0.049 0.042 0.051 0.289 0.263shelves 0.082 0.071 0.115 0.120 0.276 0.281table 0.032 0.038 0.027 0.049 0.160 0.121tv 0.026 0.027 0.007 0.007 0.168 0.106wall 0.103 0.104 0.109 0.122 0.216 0.216whiteboard 0.105 0.120 0.146 0.171 0.274 0.278Totals 1.000 1.000 1.000 1.000 0.319 0.290Figure 5: The first four columns list the probability offinding a particular word in a label and a manually an-notated patch, in the robomedia training and test sets.The final two columns show the precision of the transla-tion model tMRF using the grid segmentation for eachtoken, averaged over the 12 trials.
Precision is definedas the probability the model?s prediction is correct for aparticular word and patch.
Since precision is 1 minusthe error of equation 3, the total precision on both thetraining and test sets matches the average performanceof tMRF-patch on Error measure 2, as shown in in Fig-ure 7.
While not presented in the table, the precision onindividual words varies significantly from one one trial tothe next.
Note that some words do not appear in both thetraining and test sets, hence the n/a.
?The model predicts words without access to the test im-age labels.
We provide this information for completeness.
?We can use the manual annotations for evaluation pur-poses, but we underline the fact that an agent would nothave access to the information presented in the ?Annota-tion %?
column.will investigate a hierarchical patch representation to takeinto account both short and long range patch interactions,as in (Freeman and Pasztor, 1999).We compare two models.
The first is the translationmodel where dependencies between alignments are re-moved for the sake of tractability, called tInd.
The secondis the translation model in which we assume dependencesFigure 6: Correct annotations for Normalized Cuts, gridand manual segmentations.
When there are multiple an-notations in a single patch, any one of them is correct.Even when both are correct, the grid segmentation is usu-ally more precise and, as a result, more closely approxi-mates generic object recognition.between adjacent alignments in the image.
This modelis denoted by tMRF.
We represent the sophisticated andcrude segmentation scenarios by -seg and -patch, respec-tively.One admonition regarding the evaluation procedure: atranslation is deemed correct if at least one of the patchescorresponds to the model?s prediction.
In a manner ofspeaking, when a segment encompasses several concepts,we are giving the model the benefit of the doubt.
Forexample, according to our evaluation the annotations forboth the grid and Normalized Cuts segmentations shownin Figure 6 correct.
However, from observation the gridsegmentation provides a more precise object recognition.As a result, evaluation can be unreliable when Normal-ized Cuts offers poor segmentations.
It is also importantto remember that the true result images shown in the sec-ond column of Figure 8 are idealisations.Experimental results on 12 trials are shown in Figure7, and selected annotations predicted by the tMRF modelon the test set are shown in Figure 8.
The most signif-icant result is that the contextual translation model per-forms the best overall, and performs equally well whensupplied with either Normalized Cuts or a naive segmen-tations.
We stress that even though the models trainedusing both the grid and Normalized Cuts segmentationsare displayed on the same plots, in Figure 6 we indi-cate that object recognition using the grid segmentation isgenerally more precise, given the same evaluation resultin Figure 7.
Learning contextual dependencies betweenalignment appears to improve performance, despite thelarge amount of noise and the increase in the number ofmodel parameters that have to be learned.
The contex-Figure 7: Results using Error measures 1 and 2 on the robomedia training and test sets, displayed using a Box-and-Whisker plot.
The middle line of a box represents the median.
The central box represents the values from the 25to 75 percentile, using the upper and lower statistical medians.
The horizontal line extends from the minimum to themaximum value, excluding outside and far out values which are displayed as separate points.
The dotted line at the topis the random prediction upper bound.
Overall, the contextual model tMRF is an improvement over the independentmodel, tInd.
On average, tMRF tends to perform equally well using the sophisticated or naive patch segmentations.tual model also tends to produce more visually appeal-ing annotations since they the translations smoothed overneighbourhoods of patches.The performance of the contextual translation modelon individual words on the training and test sets is shownin Figure 5, averaged over the trials.
Since our approx-imate EM training a local maximum point estimate forthe joint posterior and the initial model parameters areset to random values, we obtain a great deal of variancefrom one trial to the next, as observed in the Box-and-Whisker plots in Figure 7.
While not shown in Figure5, we have noticed considerable variation in what wordsare predicted with high precision.
For example, the word?ceiling?
is predicted with an average success rate of0.347, although the precision on individual trials rangesfrom 0 to 0.842.Figure 8: Selected annotations on the robomedia test data predicted by the contextual (tMRF) translation model.
Weshow our model?s predictions using both sophisticated and crude segmentations.
The ?true?
annotations are shown inthe second column.
Notice that the annotations using Normalized Cuts tend to be more visually appealing comparedto the rectangular grid, but intuition is probably misleading: the error measures in Figure 7 demonstrate that bothsegmentations produce equally accurate results.
It is also important to note that these annotations are probabilistic;for clarity we only display results with the highest probability.From the Bayesian feature weighting priors ?
placedon the word cluster means, we can deduce the relativeimportance of our feature set.
In our experiments, lumi-nance and vertical position in the image are the two mostimportant features.5 Discussion and conclusionOur experiments suggest that we can eliminate the costlystep of segmentation without incurring a penalty to theobject recognition task.
This realisation allows us to re-move the main computational bottleneck and pursue real-time learning in a mobile robot setting.
Moreover, by in-troducing spatial relationships into the model, we main-tain a degree of consistency between individual patch an-notations.
We can consider this to be an early form ofsegmentation that takes advantage of both high-level andlow-level information.
Thus, we are solving both thesegmentation and recognition problems simultaneously.However, we emphasize the need for further investiga-tion to pin down the role of segmentation in the imagetranslation process.Our translation model is disposed to predicting certainwords better than others.
However, at this point we can-not make make any strong conclusions as to why certainwords easy to classify (e.g.
cabinets), while others aredifficult (e.g.
filers).
From Figure 5, it appears to be thecase that words that occur frequently and possess a con-sistent set of features tend to be more easily classified.Initially, we were doubtful that spatial context in themodel would improve results given that the robot roamsin a fairly homogeneous environment.
This contrasts withexperiments on the Corel data sets (Carbonetto and deFreitas, 2003), whereby the photographs were capturedfrom a wide variety of settings.
However, the experi-ments on the robomedia data demonstrate that there issomething to be gained by introducing inter-alignmentdependencies in the model, even in environments withrelatively noisy and unreliable data.Generic object recognition in the context of roboticsis a challenging task.
Standard low-level features suchas colour and texture are particularly ineffective in a lab-oratory environment.
For example, chairs can come in avariety of shapes and colours, and ?wall?
refers to a verti-cal surface that has virtually no relation to colour, textureand position.
Moreover, it is much more difficult to de-lineate specific concepts in a scene, even for humans ?does a table include the legs, and where does one drawthe line between shelves, drawers, cabinets and the ob-jects contained in them?
(This explains why many of themanually-annotated patches in Figures 6 and 8 are leftempty.)
Object recognition on the Corel data set is com-paratively easy because the photos are captured to arti-ficially delineate specific concepts.
Colour and texturetend to be more informative in natural scenes.In order to tackle concerns mentioned above, one ap-proach would be to construct a more sophisticated repre-sentation of objects.
A more realistic alternative would beto reinforce our representation with high-level features,including more complex spatial relations.One important criterion we did not address explicitlyis on-line learning.
Presently, we train our models as-suming that all the images are collected at one time.
Re-search shows that porting batch learning to an on-lineprocess using EM does not pose significant challenges(Smith and Makov, 1978; Sato and Ishii, 2000; Brochu etal., 2003).
With the discussion presented in this paper inmind, real-time interactive learning of semantic associa-tions in Jose?
?s environment is very much within reach.AcknowledgementsWe would like to acknowledge the help of Eric Brochuin revising and proofreading this paper, Kobus Barnardand David Forsyth for enlightening discussions, and theJose?
team, in particular Don Murray and Pantelis Eli-nas, for helping us collect invaluable data.
Additionally,the workshop reviewer committee offered very insight-ful suggestions and criticisms, so we would like to thankthem as well.ReferencesY.
Al-Onaizan, J. Curin, Michael Jahr, K. Knight, J. Lafferty,I.
D. Melamed, F.-J.
Och, D. Purdy, N. A. Smith and D.Yarowsky.
1999.
Statistical machine translation: final re-port.
Johns Hopkins University Workshop on Language En-gineering.Kobus Barnard, Pinar Duygulu and David Forsyth.
2001.Clustering art.
Conference on Computer Vision and PatternRecognition.Kobus Barnard, Pinar Duygulu and David Forsyth.
2002.Modelling the statistics of image features and associated text.Document Recognition and Retrieval IX, Electronic Imaging.Eric Brochu, Nando de Freitas and Kejie Bao.
2003.
TheSound of an album cover: probabilistic multimedia and IR.Workshop on Artificial Intelligence and Statistics.P.
Brown, S. A. Della Pietra, V.J.
Della Pietra and R. L. Mercer.1993.
The Mathematics of statistical machine translation.Computational Linguistics, 19(2):263?311.Peter Carbonetto and Nando de Freitas.
2003.
A statisticaltranslation model for contextual object recognition.
Unpub-lished manuscript.P.
Carbonetto, N. de Freitas, P. Gustafson and N. Thompson.2003.
Bayesian feature weighting for unsupervised learning,with application to object recognition.
Workshop on Artifi-cial Intelligence and Statistics.P.
Duygulu, K. Barnard, N. de Freitas and D. A. Forsyth.
2002.Object recognition as machine translation: learning a lexi-con for a fixed image vocabulary.
European Conference onComputer Vision.P.
Elinas, J. Hoey, D .Lahey, J .D.
Montgomery, D. Murray,S.
Se and J. J.
Little.
2002.
Waiting with Jose?, a vision-based mobile robot.
International Conference on Roboticsand Automation.Christiane Fellbaum.
1998.
WordNet: an electronic lexicaldatabase.
MIT Press.William T. Freeman and Egon C. Pasztor.
1999.
Learning low-level vision.
International Conference on Computer Vision.Masa-aki Sato and Shin Ishii.
2000.
On-line EM algorithmfor the Normalized Gaussian Network.
Neural Computation,12(2):407-432.Jianbo Shi and Jitendra Malik.
1997.
Normalized cuts andimage segmentation.
Conference on Computer Vision andPattern Recognition.A.
F. M. Smith and U. E. Makov.
1978.
A Quasi-Bayes se-quential procedure for mixtures.
Journal of the Royal Statis-tical Society, Series B, 40(1):106-111.Simon Tong and Edward Chang.
2001 Support vector machineactive learning for image retrieval.
ACM Multimedia.
