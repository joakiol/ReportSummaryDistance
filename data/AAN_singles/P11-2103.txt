Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 587?591,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSubjectivity and Sentiment Analysis of Modern Standard ArabicMuhammad Abdul-MageedDepartment of Linguistics &School of Library & Info.
Science,Indiana University,Bloomington, USA,mabdulma@indiana.eduMona T. DiabCenter for ComputationalLearning Systems,Columbia University, NYC, USA,mdiab@ccls.columbia.eduMohammed KorayemSchool of Informaticsand Computing,Indiana University,Bloomington, USA,mkorayem@indiana.eduAbstractAlthough Subjectivity and Sentiment Analysis(SSA) has been witnessing a flurry of novel re-search, there are few attempts to build SSAsystems for Morphologically-Rich Languages(MRL).
In the current study, we report effortsto partially fill this gap.
We present a newlydeveloped manually annotated corpus of Mod-ern Standard Arabic (MSA) together with anew polarity lexicon.The corpus is a collec-tion of newswire documents annotated on thesentence level.
We also describe an automaticSSA tagging system that exploits the anno-tated data.
We investigate the impact of differ-ent levels of preprocessing settings on the SSAclassification task.
We show that by explicitlyaccounting for the rich morphology the systemis able to achieve significantly higher levels ofperformance.1 IntroductionSubjectivity and Sentiment Analysis (SSA) is an areathat has been witnessing a flurry of novel research.In natural language, subjectivity refers to expressionof opinions, evaluations, feelings, and speculations(Banfield, 1982; Wiebe, 1994) and thus incorporatessentiment.
The process of subjectivity classificationrefers to the task of classifying texts into either ob-jective (e.g., Mubarak stepped down) or subjective(e.g., Mubarak, the hateful dictator, stepped down).Subjective text is further classified with sentiment orpolarity.
For sentiment classification, the task refersto identifying whether the subjective text is positive(e.g., What an excellent camera!
), negative (e.g., Ihate this camera!
), neutral (e.g., I believe there willbe a meeting.
), or, sometimes, mixed (e.g., It is good,but I hate it!)
texts.Most of the SSA literature has focused on En-glish and other Indio-European languages.
Very fewstudies have addressed the problem for morphologi-cally rich languages (MRL) such as Arabic, Hebrew,Turkish, Czech, etc.
(Tsarfaty et al, 2010).
MRLpose significant challenges to NLP systems in gen-eral, and the SSA task is expected to be no excep-tion.
The problem is even more pronounced in someMRL due to the lack in annotated resources for SSAsuch as labeled corpora, and polarity lexica.In the current paper, we investigate the task ofsentence-level SSA on Modern Standard Arabic(MSA) texts from the newswire genre.
We runexperiments on three different pre-processing set-tings based on tokenized text from the Penn Ara-bic Treebank (PATB) (Maamouri et al, 2004)and employ both language-independent and Arabic-specific, morphology-based features.
Our workshows that explicitly using morphology-based fea-tures in our models improves the system?s perfor-mance.
We also measure the impact of using a widecoverage polarity lexicon and show that using a tai-lored resource results in significant improvement inclassification performance.2 ApproachTo our knowledge, no SSA annotated MSA data ex-ists.
Hence we decided to create our own SSA an-notated data.12.1 Data set and AnnotationCorpus: Two college-educated native speakersof Arabic annotated 2855 sentences from Part1 V 3.0 of the PATB.
The sentences make upthe first 400 documents of that part of PATBamounting to a total of 54.5% of the PATBPart 1 data set.
For each sentence, the an-notators assigned one of 4 possible labels: (1)OBJECTIVE (OBJ), (2) SUBJECTIVE-POSITIVE(S-POS), (3) SUBJECTIVE-NEGATIVE (S-NEG),and (4) SUBJECTIVE-NEUTRAL (S-NEUT).
Fol-lowing (Wiebe et al, 1999), if the primary goal1The data may be obtained by contacting the first author.587of a sentence is judged as the objective reportingof information, it was labeled as OBJ.
Otherwise, asentence would be a candidate for one of the threeSUBJ classes.
Inter-annotator agreement reached88.06%.2 The distribution of classes in our data setwas as follows: 1281 OBJ, a total of 1574 SUBJ,where 491 were deemed S-POS, 689 S-NEG, and394 S-NEUT.
Moreover, each of the sentences in ourdata set is manually labeled by a domain label.
Thedomain labels are from the newswire genre and areadopted from (Abdul-Mageed, 2008).Polarity Lexicon: We manually created a lexiconof 3982 adjectives labeled with one of the follow-ing tags {positive, negative, neutral}.
The adjectivespertain to the newswire domain.2.2 Automatic ClassificationTokenization scheme and settings: We run experi-ments on gold-tokenized text from PATB.
We adoptthe PATB+Al tokenization scheme, where procli-tics and enclitics as well as Al are segmented outfrom the stem words.
We experiment with three dif-ferent pre-processing lemmatization configurationsthat specifically target the stem words: (1) Surface,where the stem words are left as is with no furtherprocessing of the morpho-tactics that result from thesegmentation of clitics; (2) Lemma, where the stemwords are reduced to their lemma citation forms, forinstance in case of verbs it is the 3rd person mas-culine singular perfective form; and (3) Stem, whichis the surface form minus inflectional morphemes, itshould be noted that this configuration may result innon proper Arabic words (a la IR stemming).
Ta-ble 1 illustrates examples of the three configurationschemes, with each underlined.Features: The features we employed are of twomain types: Language-independent features andMorphological features.Language-Independent Features: This group offeatures has been employed in various SSA studies.Domain: Following (Wilson et al, 2009), we ap-ply a feature indicating the domain of the documentto which a sentence belongs.
As mentioned earlier,each sentence has a document domain label manu-ally associated with it.2A detailed account of issues related to the annotation taskwill appear in a separate publication.UNIQUE: Following Wiebe et al (2004) we ap-ply a unique feature.
Namely words that occur in ourcorpus with an absolute frequency < 5, are replacedwith the token ?UNIQUE?.N-GRAM: We run experiments with N-grams?
4and all possible combinations of them.ADJ: For subjectivity classification, we followBruce & Wiebe?s (1999) in adding a binaryhas adjective feature indicating whether or not anyof the adjectives in our manually created polaritylexicon exists in a sentence.
For sentiment classi-fication, we apply two features, has POS adjectiveand has NEG adjective, each of these binary fea-tures indicate whether a POS or NEG adjective oc-curs in a sentence.MSA-Morphological Features: MSA exhibits avery rich morphological system that is templatic,and agglutinative and it is based on both derivationaland inflectional features.
We explicitly model mor-phological features of person, state, gender, tense,aspect, and number.
We do not use POS informa-tion.
We assume undiacritized text in our models.2.3 Method: Two-stage Classification ProcessIn the current study, we adopt a two-stage classifica-tion approach.
In the first stage (i.e., Subjectivity),we build a binary classifier to sort out OBJ fromSUBJ cases.
For the second stage (i.e., Sentiment)we apply binary classification that distinguishes S-POS from S-NEG cases.
We disregard the neutralclass of S-NEUT for this round of experimentation.We use an SVM classifier, the SVMlight package(Joachims, 2008).
We experimented with variouskernels and parameter settings and found that linearkernels yield the best performance.
We ran experi-ments with presence vectors: In each sentence vec-tor, the value of each dimension is binary either a 1(regardless of how many times a feature occurs) or0.Experimental Conditions: We first run ex-periments using each of the three lemmatizationsettings Surface, Lemma, Stem using various N-grams and N-gram combinations and then itera-tively add other features.
The morphological fea-tures (i.e., Morph) are added only to the Stem setting.Language-independent features (i.e., from the fol-lowing set {DOMAIN, ADJ, UNIQUE}) are addedto the Lemma and Stem+Morph settings.
With all588Word POS Surface form Lemma Stem GlossAlwlAyAt Noun Al+wlAyAt Al+wlAyp Al+wlAy the statesltblgh Verb l+tblg+h l+>blg+h l+blg+h to inform himTable 1: Examples of word lemmatization settingsthe three settings, clitics that are split off words arekept as separate features in the sentence vectors.3 Results and EvaluationWe divide our data into 80% for 5-fold cross-validation and 20% for test.
For experiments on thetest data, the 80% are used as training data.
We havetwo settings, a development setting (DEV) and a testsetting (TEST).
In the development setting, we runthe typical 5 fold cross validation where we train on4 folds and test on the 5th and then average the re-sults.
In the test setting, we only ran with the bestconfigurations yielded from the DEV conditions.
InTEST mode, we still train with 4 folds but we test onthe test data exclusively, averaging across the differ-ent training rounds.It is worth noting that the test data is larger thanany given dev data (20% of the overall data set fortest, vs. 16% for any DEV fold).
We report resultsusing F-measure (F).
Moreover, for TEST we re-port only experiments on the Stem+Morph settingand Stem+Morph+ADJ, Stem+Morph+DOMAIN,and Stem+Morph+UNIQUE.
Below, we only reportthe best-performing results across the N-GRAM fea-tures and their combinations.
In each case, our base-line is the majority class in the training set.3.1 SubjectivityAmong all the lemmatization settings, the Stem wasfound to perform best with 73.17% F (with 1g+2g),compared to 71.97% F (with 1g+2g+3g) for Sur-face and 72.74% F (with 1g+2g) for Lemma.
In ad-dition, adding the inflectional morphology featuresimproves classification (and hence the Stem+Morphsetting, when ran under the same 1g+2g conditionas the Stem, is better by 0.15% F than the Stemcondition alone).
As for the language-independentfeatures, we found that whereas the ADJ featuredoes not help neither the Lemma nor Stem+Morphsetting, the DOMAIN feature improves the re-sults slightly with the two settings.
In addition,the UNIQUE feature helps classification with theLemma, but it hurts with the Stem+Morph.Table 2 shows that although performance on thetest set drops with all settings on Stem+Morph, re-sults are still at least 10% higher than the bseline.With the Stem+Morph setting, the best performanceon the TEST set is 71.54% Fand is 16.44% higherthan the baseline.3.2 SentimentSimilar to the subjectivity results, the Stem set-ting performs better than the other two lemmatiza-tion scheme settings, with 56.87% F compared to52.53% F for the Surface and 55.01% F for theLemma.
These best results for the three lemmatiza-tion schemes are all acquired with 1g.
Again, addingthe morphology-based features helps improve theclassification: The Stem+Morph outperforms Stemby about 1.00% F. We also found that whereasadding the DOMAIN feature to both the Lemma andthe Stem+Morph settings improves the classificationslightly, the UNIQUE feature only improves classi-fication with the Stem+Morph.Adding the ADJ feature improves performancesignificantly: An improvement of 20.88% F for theLemma setting and 33.09% F for the Stem+Morphis achieved.
As Table 3 shows, performance on testdata drops with applying all features except ADJ, thelatter helping improve performance by 4.60% F. Thebest results we thus acquire on the 80% training datawith 5-fold cross validation is 90.93% F with 1g,and the best performance of the system on the testdata is 95.52% F also with 1g.4 Related WorkSeveral sentence- and phrase-level SSA systemshave been built, e.g., (Yi et al 2003; Hu and Liu.,2004; Kim and Hovy., 2004; Mullen and Collier2004; Pang and Lee 2004; Wilson et al 2005;Yu and Hatzivassiloglou, 2003).
Yi et al (2003)present an NLP-based system that detects all ref-589Stem+Morph +ADJ +DOMAIN +UNIQUEDEV 73.32 73.30 73.43 72.92TEST 65.60 71.54 64.67 65.66Baseline 55.13 55.13 55.13 55.13Table 2: Subjectivity results on Stem+Morph+language independent featuresStem+Morph +ADJ +DOMAIN +UNIQUEDEV 57.84 90.93 58.03 58.22TEST 52.12 95.52 53.21 51.92Baseline 58.38 58.38 58.38 58.38Table 3: Sentiment results on Stem+Morph+language independent featureserences to a given subject, and determines senti-ment in each of the references.
Similar to (2003),Kim & Hovy (2004) present a sentence-level sys-tem that, given a topic detects sentiment towards it.Our approach differs from both (2003) and Kim &Hovy (2004) in that we do not detect sentiment to-ward specific topics.
Also, we make use of N-gramfeatures beyond unigrams and employ elaborate N-gram combinations.Yu & Hatzivassiloglou (2003) build a document-and sentence-level subjectivity classification systemusing various N-gram-based features and a polaritylexicon.
They report about 97% F-measure on docu-ments and about 91% F-measure on sentences fromthe Wall Street Journal (WSJ) corpus.
Some of ourfeatures are similar to those used by Yu & Hatzivas-siloglou, but we exploit additional features.
Wiebeet al (1999) train a sentence-level probabilisticclassifier on data from the WSJ to identify subjectiv-ity in these sentences.
They use POS features, lex-ical features, and a paragraph feature and obtain anaverage accuracy on subjectivity tagging of 72.17%.Again, our feature set is richer than Wiebe et al(1999).The only work on Arabic SSA we are aware ofis that of Abbasi et al (2008).
They use an en-tropy weighted genetic algorithm for both Englishand Arabic Web forums at the document level.
Theyexploit both syntactic and stylistic features.
Abbasiet al use a root extraction algorithm and do not usemorphological features.
They report 93.6% accu-racy.
Their system is not directly comparable to oursdue to the difference in data sets and tagging granu-larity.5 ConclusionIn this paper, we build a sentence-level SSA sys-tem for MSA contrasting language independent onlyfeatures vs. combining language independent andlanguage-specific feature sets, namely morpholog-ical features specific to Arabic.
We also investi-gate the level of stemming required for the task.We show that the Stem lemmatization setting outper-forms both Surface and Lemma settings for the SSAtask.
We illustrate empirically that adding languagespecific features for MRL yields improved perfor-mance.
Similar to previous studies of SSA for otherlanguages, we show that exploiting a polarity lexi-con has the largest impact on performance.
Finally,as part of the contribution of this investigation, wepresent a novel MSA data set annotated for SSA lay-ered on top of the PATB data annotations that willbe made available to the community at large, in ad-dition to a large scale polarity lexicon.ReferencesA.
Abbasi, H. Chen, and A. Salem.
2008.
Sentimentanalysis in multiple languages: Feature selection foropinion classification in web forums.
ACM Trans.
Inf.Syst., 26:1?34.M.
Abdul-Mageed.
2008.
Online News Sites andJournalism 2.0: Reader Comments on Al JazeeraArabic.
tripleC-Cognition, Communication, Co-operation, 6(2):59.A.
Banfield.
1982.
Unspeakable Sentences: Narration590and Representation in the Language of Fiction.
Rout-ledge Kegan Paul, Boston.R.
Bruce and J. Wiebe.
1999.
Recognizing subjectivity.a case study of manual tagging.
Natural LanguageEngineering, 5(2).T.
Joachims.
2008.
Svmlight: Support vector ma-chine.
http://svmlight.joachims.org/, Cornell Univer-sity, 2008.S.
Kim and E. Hovy.
2004.
Determining the senti-ment of opinions.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics,pages 1367?1373.M.
Maamouri, A. Bies, T. Buckwalter, and W. Mekki.2004.
The penn arabic treebank: Building a large-scale annotated arabic corpus.
In NEMLAR Confer-ence on Arabic Language Resources and Tools, pages102?109.R.
Tsarfaty, D. Seddah, Y. Goldberg, S. Kuebler, Y. Ver-sley, M. Candito, J.
Foster, I. Rehbein, and L. Tounsi.2010.
Statistical parsing of morphologically rich lan-guages (spmrl) what, how and whither.
In Proceedingsof the NAACL HLT 2010 First Workshop on StatisticalParsing of Morphologically-Rich Languages, Los An-geles, CA.J.
Wiebe, R. Bruce, and T. O?Hara.
1999.
Developmentand use of a gold standard data set for subjectivity clas-sifications.
In Proc.
37th Annual Meeting of the Assoc.for Computational Linguistics (ACL-99), pages 246?253, University of Maryland: ACL.J.
Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.2004.
Learning subjective language.
Computationallinguistics, 30(3):277?308.J.
Wiebe.
1994.
Tracking point of view in narrative.Computional Linguistics, 20(2):233?287.T.
Wilson, J. Wiebe, and P. Hoffmann.
2009.
Recogniz-ing Contextual Polarity: an exploration of features forphrase-level sentiment analysis.
Computational Lin-guistics, 35(3):399?433.J.
Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
2003.Sentiment analyzer: Extracting sentiments about agiven topic using natural language processing tech-niques.
In Proceedings of the 3rd IEEE InternationalConference on Data Mining, pages 427?434.H.
Yu and V. Hatzivassiloglou.
2003.
The penn arabictreebank: Building a large-scale annotated arabic cor-pus.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 129?136.591
