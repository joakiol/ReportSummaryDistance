2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 656?666,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsLearning from Bullying Traces in Social MediaJun-Ming Xu, Kwang-Sung Jun, Xiaojin ZhuDepartment of Computer SciencesUniversity of Wisconsin-MadisonMadison, WI 53706, USA{xujm,deltakam,jerryzhu}@cs.wisc.eduAmy BellmoreDepartment of Educational PsychologyUniversity of Wisconsin-MadisonMadison, WI 53706, USAabellmore@wisc.eduAbstractWe introduce the social study of bullying tothe NLP community.
Bullying, in both physi-cal and cyber worlds (the latter known as cy-berbullying), has been recognized as a seri-ous national health issue among adolescents.However, previous social studies of bully-ing are handicapped by data scarcity, whilethe few computational studies narrowly re-strict themselves to cyberbullying which ac-counts for only a small fraction of all bullyingepisodes.
Our main contribution is to presentevidence that social media, with appropriatenatural language processing techniques, canbe a valuable and abundant data source for thestudy of bullying in both worlds.
We iden-tify several key problems in using such datasources and formulate them as NLP tasks, in-cluding text classification, role labeling, senti-ment analysis, and topic modeling.
Since thisis an introductory paper, we present baselineresults on these tasks using off-the-shelf NLPsolutions, and encourage the NLP communityto contribute better models in the future.1 Introduction to BullyingBullying, also called peer victimization, has beenrecognized as a serious national health issue bythe White House (The White House, 2011), theAmerican Academy of Pediatrics (The AmericanAcademy of Pediatrics, 2009), and the AmericanPsychological Association (American PsychologicalAssociation, 2004).
One is being bullied or victim-ized when he or she is exposed repeatedly over timeto negative actions on the part of others (Olweus,1993).
Far-reaching and insidious sequelae of bul-lying include intrapersonal problems (Juvonen andGraham, 2001; Jimerson, Swearer, and Espelage,2010) and lethal school violence in the most extremecases (Moore et al, 2003).
Youth who experiencepeer victimization report more symptoms of depres-sion, anxiety, loneliness, and low self-worth com-pared to their nonvictimized counterparts (Bellmoreet al, 2004; Biggs, Nelson, and Sampilo, 2010; Gra-ham, Bellmore, and Juvonen, 2007; Hawker andBoulton, 2000).
Other research suggests that victim-ized youth have more physical complaints (Fekkeset al, 2006; Nishina and Juvonen, 2005; Gini andPozzoli, 2009).
Victimized youth are absent fromschool more often and get lower grades than nonvic-timized youth (Ladd, Kochenderfer, and Coleman,1997; Schwartz et al, 2005; Juvonen and Gross,2008).Bullying happens traditionally in the physicalworld and, recently, online as well; the latter isknown as cyberbullying (Cassidy, Jackson, andBrown, 2009; Fredstrom, Adams, and Gilman,2011; Wang, Iannotti, and Nansel, 2009; Vande-bosch and Cleemput, 2009).
Bullying usually startsin primary school, peaks in middle school, and lastswell into high school and beyond (Nansel et al,2001; Smith, Madsen, and Moody, 1999; Cook etal., 2010).
Across a national sample of students ingrades 4 through 12, 38% of students reported be-ing bullied by others and 32% reported bullying oth-ers (Vaillancourt et al, 2010).656reinforcerbystanderbully victimassistant defender reporteraccuserFigure 1: The roles in a bullying episode.
Solid circlesrepresent traditional roles in social science, while dottedcircles are new roles we augmented for social media.
Thewidth of the edges represents interaction strength.1.1 The Structure of a Bullying EpisodeBullying takes multiple forms, most noticeably face-to-face physical (e.g., hitting), verbal (e.g., name-calling), and relational (e.g., exclusion) (Archer andCoyne, 2005; Little et al, 2003; Nylund et al,2007).
Cyberbullying reflects a venue (other thanface to face contact) through which verbal and rela-tional forms can occur.A main reason individuals are targeted with bul-lying is perceived differences, i.e., any characteristicthat makes an individual stand out differently fromhis or her peers.
These include race, socio-economicstatus, gender, sexuality, physical appearance, andbehaviors.Participants in a bullying episode take well-defined roles (see Figure 1).
More than one personcan have the same role in a bullying episode.
Rolesinclude the bully (or bullies), the victims, bystanders(who saw the event but did not intervene), defend-ers of the victim, assistants to the bully (who didnot initiate but went along with the bully), and rein-forcers (who did not directly join in with the bullybut encouraged the bully by laughing, for exam-ple) (Salmivalli, 1999).
This recognition that bully-ing involves multiple roles makes evident the broad-ranging impact of bullying; any child or adolescentis susceptible to participation in bullying, even thosewho are not directly involved (Janosz et al, 2008;Rivers et al, 2009).1.2 Some Scientific Questions NLP can AnswerLike many complex social issues, effective solutionsto bullying go beyond technology alone and requirethe concerted efforts of parents, educators, and lawenforcement.
To guide these efforts it is paramountto study the dynamics of bullying.
Such study criti-cally depends on text in the form of self-report socialstudy surveys and electronic communication amongparticipants.
Such text is often fragmental, noisy,and covers only part of a bullying episode from aspecific role?s perspective.
As such, the NLP com-munity can help answer a host of scientific ques-tions: Which pieces of text refer to the same under-lying bullying episode?
What is the form, reason,location, time, etc.
of a bullying episode?
Who arethe participants of each episode, and what are theirroles?
How does a person?s role evolve over time?This paper presents our initial investigation on someof these questions, while leaving others to future re-search by the NLP community.1.3 Limitations of the State-of-the-ArtThe social science study of bullying has a long his-tory.
However, a fundamental problem there is dataacquisition.
The standard approach is to conducttime-consuming personal surveys in schools.
Thesample size is typically in the hundreds, and partici-pants typically write 3 to 4 sentences about each bul-lying episode (Nishina and Bellmore, 2010).
Such asmall corpus fails to assess the true frequency of bul-lying over the population, and cannot determine theevolution of roles.
The computational study of bul-lying is largely unexplored, with the exception of afew studies on cyberbullying (Lieberman, Dinakar,and Jones, 2011; Dinakar, Reichart, and Lieber-man, 2011; Ptaszynski et al, 2010; Kontostathis,Edwards, and Leatherman, 2010; Bosse and Stam,2011; Latham, Crockett, and Bandar, 2010).
Thesestudies did not consider the much more frequent bul-lying episodes in the physical world.2 Bullying Traces in Social MediaThe main contribution of the present paper is noton novel algorithms, but rather on presenting evi-dence that social media data and off-the-shelf NLPtools can be an effective combination for the studyof bullying.
Participants of a bullying episode (in ei-ther physical or cyber venues) often post social me-dia text about the experience.
We collectively callsuch social media posts bullying traces.
Bullying657traces include but far exceed incidences of cyberbul-lying.
Most of them are in fact responses to a bul-lying experience ?
the actual attack is hidden fromview.
Bullying traces are valuable, albeit fragmentaland noisy, data which we can use to piece togetherthe underlying episodes.In the rest of the paper, we focus on publiclyavailable Twitter ?tweets,?
though our methodsapply readily to other social media services, too.Here are some examples of bullying traces:?
Reporting a bullying episode: ?some tweensgot violent on the n train, the one boy got offafter blows 2 the chest...
Saw him cryin as hewalkd away :( bullying not cool??
Accusing someone as a bully: ?
@USERNAMEi didnt jump around and act like a monkey T Twhich of your eye saw that i acted like a monkey:( you?re a bully??
Revealing self as a victim: ?People bullied mefor being fat.
7 years later, I was diagnosedwith bulimia.
Are you happy now???
Cyber-bullying direct attack: ?Lauren is a fatcow MOO BITCH?Bullying traces are abundant.
From the publiclyavailable 2011 TREC Microblog track corpus (16million tweets sampled between January 23rd andFebruary 8th, 2011), we uniformly sampled 990tweets for manual inspection by five experienced an-notators (not the authors of the present paper).
Ofthe 990 tweets, the annotators labeled 617 as non-English, 371 as English but not bullying traces, and2 as English bullying traces.
The Maximum Likeli-hood Estimate of the frequency of English bullyingtraces, out of all tweets, is 2/990 ?
0.002.
Theexact Binomial 95% confidence interval is (0.0002,0.0073).
This is a tiny fraction.
Nonetheless, it rep-resents an abundance of tweets: by some estimates,Twitter produces 250 million tweets per day in late2011.
Even with the lower bound in the confidenceinterval, it translates into 50,000 English bullyingtraces per day.
The actual number can be muchhigher.Bullying traces contain valuable information.
Forexample, Figure 2 shows the daily number of bully-ing traces identified by our classifier, to be discussedFigure 2: Temporal variation of bullying tracesin section 3.
A weekly pattern was obvious in lateAugust.
A small peak was caused by 14-year-oldbullying victim Jamey Rodemeyer?s suicide on Sept.18.
This was followed by a large peak after LadyGaga dedicated a song to him on Sept. 24.In the following sections, we identify several keyproblems in using social media for the study of bul-lying.
We formulate each key problem as an NLPtask.
We then present standard off-the-shelf NLP ap-proaches to establish baseline performances.
Sincebullying traces account for only a tiny fraction of alltweets, it posed a significant challenge for our an-notators to find enough bullying traces without la-beling an unreasonable amount of tweets.
For thisreason, in the rest of the paper we restrict ourselvesto an ?enriched dataset.?
This enriched dataset is ob-tained by collecting tweets using the public Twitterstreaming API, such that each tweet contains at leastone of the following keywords: ?bully, bullied, bul-lying.?
We further removed re-tweets (the analogueof forwarded emails) by excluding tweets containingthe acronym ?RT.?
The enrichment process is meantto retain many first-hand bullying traces at the costof a selection bias.3 NLP Task A: Text CategorizationOne important task is to distinguish bullying tracesfrom other social media posts.
Our enriched dataset,generated by simple keyword filtering, still containsmany irrelevant tweets.
For example, ?Forced veg-anism by removing a persons choice is just anotherform of bullying?
is not a bullying trace, since it does658not describe a bullying episode.
Our task is to dis-tinguish posts like this from true bullying traces suchas those mentioned in the previous section.
We for-mulate it as a binary text categorization task.Methods.
The same annotators who labeled theTREC corpus labeled 1762 tweets sampled uni-formly from the enriched dataset on August 6, 2011.Among them, 684 (39%) were labeled as bullyingtraces.Following (Settles, 2011), these 1762 tweets werecase-folded but without any stemming or stop-word removal.
Any user mentions preceded by a?@?
were replaced by the anonymized user name?@USERNAME?.
Any URLs starting with ?http?were replaced by the token ?HTTPLINK?.
Hashtags(compound words following ?#?)
were not split andwere treated as a single token.
Emoticons, such as?:)?
or ?
:D?, were also included as tokens.After these preprocessing procedures, we createdthree different sets of feature representations: un-igrams (1g), unigrams+bigrams (1g2g), and POS-colored unigrams+bigrams (1g2gPOS).
POS tag-ging was done with the Stanford CoreNLP pack-age (Toutanova et al, 2003).
POS-coloring wasdone by expanding each token into token:POS.We chose four commonly used text classifiers,namely, Naive Bayes, SVM with linear kernel(SVM(linear)), SVM with RBF kernel (SVM(RBF))and Logistic Regression (equivalent to MaxEnt).
Weused the WEKA (Hall et al, 2009) implementationfor the first three (calling LibSVM (Chang and Lin,2011) with WEKA?s interfaces for SVMs), and theL1General package (Schmidt, Fung, and Rosales,2007) for the fourth.We held out 262 tweets for test, and systemat-ically varied training set size among the remain-ing tweets, from 100 to 1500 with the step-size100.
We tuned all parameters jointly by 5-foldcross validation on the training set with the grid{2?8, 2?6, .
.
.
, 28}.
All the four text classifiers weretrained on the training sets and tested on the test set.The whole procedure was repeated 30 times for eachfeature representation.Results.
Figure 3 reports the held-out set accu-racy as the training set size increases.
The error barsare ?1 standard error.
With the largest training setsize (1500), the combination of SVM(linear) + 1gachieves an average accuracy 79.7%.
SVM(linear)+ 1g2g achieves 81.3%, which is significantly bet-ter (t-test, p = 4 ?
10?6).
It shows that in-cluding bigrams can significantly improve the clas-sification performance.
SVM(linear) + 1g2gPOSachieves 81.6%, though the improvement is not sta-tistically significant (p = 0.088), which indicatesthat POS coloring does not help too much on thistask.
SVM(RBF) gives similar performance, Logis-tic Regression is slightly worse and Naive Bayes ismuch worse, for a large range of training set sizes.In summary, SVM(linear) + 1g2g is the preferredmodel because of its accuracy and simplicity.
Wealso note that these accuracies are much better thanthe majority class baseline of 61%.
On the held-out set, SVM(linear) + 1g2g achieves precisionP=0.76, recall R=0.79, and F-measure 0.77.Discussions.
Note that the learning curves arestill increasing, suggesting that better accuracy canbe obtained if we annotate more training data.
As towhy the best accuracy is not close to 1, one hypoth-esis is noisy labels caused by intrinsic disagreementamong labelers.
Tweets are short and some are am-biguous.
Without prior knowledge about the usersand their other tweets, labelers interpret the tweetsin their own ways.
For example, for the very shorttweet feels like a bully..... our annotators disagreedon whether it is a bullying trace.
Labelers may havedifferent views on these ambiguous tweets and cre-ated noisy bullying trace labels.A future direction is to categorize bullying tracesat a finer granularity, e.g., by forms, reasons, etc.This can be solved by multi-class classificationmethods.
Another direction is to extend the clas-sifiers from the ?enriched data?
to the full range oftweets.
Recall that the difference is whether we pre-filter the tweets by keywords.
Clearly, they havedifferent tweet distributions.
Techniques used forcovariate shift may be adapted to solve this prob-lem (Blitzer, 2008).4 NLP Task B: Role LabelingIdentifying participants?
bullying roles (Figure 1) isanother important task, which is also a prerequi-site of studying how a person?s role evolves overtime.
For bullying traces in social media, we aug-ment the traditional role system with two new roles:reporter (may not be present during the episode, un-659(a) 1g (b) 1g2g (c) 1g2gPOSFigure 3: Learning Curves for different feature sets and classification algorithmslike a bystander) and accuser (accusing someone asthe bully).
Both roles can be a victim, a defender,or a bystander in the traditional sense ?
there is justnot enough information in the tweet.
Accuser (A),bully (B), reporter (R) and victim (V) are the fourmost frequent roles observed in social media.
Wemerged all remaining roles into a generic category?other?
(O) in the following study.
Our task is toclassify the role (A, B, R, V, O) of the tweet authorand any person-mentions in a tweet.
For example,AUTHOR(R): ?We(R) visited my(V) cousin(V) today& #Itreallymakesmemad that he(V) barely eats beche(V) was bullied .
:( I(R) wanna kick the crap outof those mean(B) kids(B).?
Note that the special to-ken ?AUTHOR?
is introduced to hold the label ofthe author?s role.Labeling author?s role and other person-mention?srole are two different sub-tasks.
The former can beformulated as a multi-class text classification task;the latter is better formulated as a sequential taggingtask.
We will discuss them separately below.4.1 Author?s RolesMethods.
Our annotators labeled the author?s rolefor each of the 684 positive bullying traces in Task A(296 R, 162 V, 98 B, 86 A, 42 O).
We used the sameclassifiers and features in Section 3.
We conducted10-fold cross validation to evaluate all combinationsof classifiers and feature sets.
Like before, we tunedall parameters jointly by 5-fold cross validation onthe training set with the grid {2?8, 2?6, .
.
.
, 28}.Results.
The best combination is SVM(linear)+ 1g2g with cross validation accuracy 61%.
Eventhough it is far from perfect, it is significantly betterthan the majority class (R) baseline of 43%.
It showspredicted asA B R V OA 33 3 39 10 1B 5 25 57 11 0R 15 5 249 27 0V 1 4 48 109 0O 1 1 37 3 0Table 1: Confusion Matrix of Author Role Classificationthat there is signal in the text to infer the authors?roles.Table 1 shows the confusion matrix of the bestmodel.
Most R and V authors are correctly rec-ognized, but not B and A.
The model misclassifiedmany authors as R. It is possible that the tweets au-thored by reporters are diverse in topic and style, andoverlap with other classes in the feature space.Discussions.
As tweets are short, our feature rep-resentation may not be the best for predicting au-thor?s role.
Many authors mentioned themselvesin the tweets with first-person pronouns, makingit advantageous to consider joint classification bymerging sections 4.1 and 4.2.
Furthermore, assum-ing roles change infrequently, it may be helpful tojointly classify many tweets authored by the sameperson.4.2 Person-Mention?s RolesThis sub-task labels each person-mention with abullying role.
It uses Named Entity Recognition(NER) (Finkel, Grenager, and Manning, 2005; Rati-nov and Roth, 2009; Ritter et al, 2011) as a sub-routine to identify named person entities, though weare also interested in unnamed persons such as ?myteacher?
and pronouns.
It is related to Semantic Role660Labeling (SRL) (Gildea and Jurafsky, 2002; Pun-yakanok, Roth, and Yih, 2008) but differs criticallyin that our roles are not tied to specific verb predi-cates.Methods.
Our annotators labeled each tokenin the 684 bullying traces with the tags A, B,R, V, O and N for not-a-person.
There are11,751 tokens in total.
Similar to the sequen-tial tagging formulation (Ma`rquez et al, 2005; Liuet al, 2010), we trained a linear CRF to labeleach token in the tweet with the CRF++ package(http://crfpp.sourceforge.net/).As standard in linear CRFs, we used pairwise la-bel features f(yi?1, yi) and input features f(yi,w),where f ?s are binary indicator functions on the val-ues of their arguments and w is the text.
In the fol-lowing, we introduce our input features using the ex-ample tweet ?
@USERNAME i?ll tell vinny you bul-lied me.?
with the current token wi =?vinny?
:(i) The token, lemma, and POS tag of thefive tokens around position i.
For example,fbully,wi?1=tell(yi,w) will be 1 if the current to-ken has label yi = ?bully??
and wi?1 = ?tell?
?.Similarly, fvictim,POSi+2=V BD(yi,w) will be 1 ifyi = ?victim??
and the POS of wi+2 is VBD.
(ii) The NER tag of wi.
(iii) Whether wi is a person mention.
This is aBoolean feature which is true if wi is tagged as PER-SON by NER, or if POSi = pronoun (excluding?it?
), or if wi is @USERNAME.
For example, thisfeature is true on ?vinny?
because it is tagged asPERSON by NER.
(iv) The relevant verb vi of wi, vi?s lemma, POS,and the combination of vi with the lemma/POS ofwi.
The relevant verb vi of wi is defined by thesemantic dependency between wi and the verb, ifone exists.
Otherwise, vi is the closest verb to wi.For example, the relevant verb of wi = ?vinny??
isvi = ?tell??
because ?vinny?
is found as the objectof ?tell?
by dependency parsing.
(v) The distance, relative position (left or right)and dependency type between vi and wi.
For ex-ample, the distance between ?vinny?
and its relevantverb ?tell?
is 1.
?vinny?
is on the right and is theobject of ?tell?.The lemma, POS tags, NER tags and dependencyrelationship were obtained using Stanford CoreNLP.As a baseline, we trained SVM(linear) with theAccuracy Precision Recall F-1CRF 0.87 0.53 0.42 0.47SVM 0.85 0.42 0.31 0.36Table 2: Cross Validation Result of Person-MentionRolessame input features as CRF.
Classification is doneindividually on each token.
We randomly split the684 tweets into 10 folds and conducted cross vali-dation based on this split.
For CRF, we trained onthe tweets in the training set with their labels, andtested the model on those in the test set.
For SVM,we trained and tested at the token level in the corre-sponding sets.Results.
Table 2 reports the cross validation ac-curacy, precision, recall and F-1 measure.
Accu-racy measures the percentage of tokens correctlyassigned the groundtruth labels, including N (not-a-person) tokens.
Precision measures the fractionof correctly labeled person-mention tokens over alltokens that are not N according to the algorithm.Recall measures the fraction of correctly labeledperson-mention tokens over all tokens that are notN according to the groundtruth.
F-1 is the har-monic mean of precision and recall.
Linear CRFachieved an accuracy 0.87, which is higher than thebaseline of majority class predictor (N, 0.80) (t-test, p = 10?10).
However, the precision and re-call is low potentially because the tweets are shortand noisy.
CRF outperforms SVM in all measures,showing the value of joint classification.Discussions.
Table 3 shows the confusion ma-trix of person-mention role labeling by linear CRF.There are several reasons for these mistakes.
First,words like ?teacher?, ?sister?, or ?girl?
were missedby our person mention feature (iii).
Second, theNER tagger was trained on formal English which isa mismatch for the informal tweets, leading to NERerrors.
Third, noisy labeling continues to affect ac-curacy.
For example, some annotators considered?other people?
as an entity and labeled both tokensas person mentions; others labeled ?people?
only.In general, bullying role labeling may be im-proved by jointly considering multiple tweets at theepisode level.
Co-reference resolution should im-prove the performance as well.661predicted asA B R V O NA 0 4 5 10 0 4B 0 406 13 125 103 302R 0 28 31 67 0 13V 0 142 28 380 43 202O 0 112 4 42 156 86N 0 78 4 41 16 9306Table 3: Confusion Matrix of Person-Mention Roles byCRF5 NLP Task C: Sentiment AnalysisSentiment analysis on participants involved in a bul-lying episode is of significant importance.
As Fig-ure 4 suggests, there are a wide range of emotions inbullying traces.
For example, victims usually expe-rience negative emotions such as depression, anxietyand loneliness; Some emotions are more violent oreven suicidal.
Detecting at-risk individuals via sen-timent analysis enables potential interventions.
Inaddition, social scientists are interested in sentimentanalysis of bullying participants to understand theirmotivations.In the present paper we investigate a special formof sentiment in bullying traces, namely teasing.
Weobserved that many bullying traces were written jok-ingly.
One example of a teasing post is ?
@USER-NAME lol stop being a cyber bully lol :p.?
Teas-ing may indicate the lack of severity of a bullyingepisode; It may also be a manifest of coping strate-gies in bullying victims.
Therefore, there is consid-erable interest among social scientists to understandteasing in bullying traces.Methods.
One first task is to identify teasing bul-lying traces.
We formulated it as a binary classifi-cation problem, similar to classic positive/negativesentiment classification (Pang and Lee, 2004).
Ourannotators labeled each of the 684 bullying traces inTask A as teasing (99) or not (585).
We used thesame feature representations, classifiers and param-eter tuning as in Section 3 and 10-fold cross valida-tion procedure.Results.
The best cross validation accuracy of89% is obtained by SVM(linear) + 1g2g.
Thisis significantly better than the majority class (not-teasing) baseline of 86% (t-test, p = 10?33).
Itshows that even simple features and off-the-shelfpredicted asTease NotTease 52 47Not 26 559Table 4: Confusion Matrix of Teasing Classificationclassifier can detect some signal in the text.
How-ever, the accuracy is not high.
Table 4 shows theconfusion matrix.
About half of the tease exampleswere misclassified.
We found several possible ex-planations.
First, teasing is not always accompaniedby joking emoticons or tokens like ?LOL,?
?lmao,??haha.?
For example, ?I may bully you but I loveyou lots.
Just like jelly tots!?
and ?Been bullied intowatching a scary film, I love my friends!?
Such teas-ing sentiment requires deeper NLP or much largertraining sets.
Second, tweets containing those jok-ing emoticons and tokens are not necessarily teas-ing.
For example, ?This Year I?m Standing Up ForThe Kids That Are Being Bullied All Over The Na-tion :) .?
Third, the joking tokens have diversespellings.
For example, ?lol?
was spelled as ?loll,??lolol,?
?lollll,?
?loool,?
?LOOOOOOOOOOOL?;?haha?
was spelled as ?HAHAHAHA,?
?Hahaha,??Bwahahaha,?
?ahahahah,?
?hahah.?Discussions.
Specialized word normalization forsocial media text may significantly improve perfor-mance.
For example, word lengthening can be iden-tified and used as cues for teasing (Brody and Di-akopoulos, 2011).
Teasing is diverse in its formand content.
Our training set is perhaps too small.Borrowing training data from other corpora, such asone-liner jokes (Mihalcea and Strapparava, 2005),may be helpful.6 NLP Task D: Latent Topic ModelingMethods.
Given the large volume of bullying traces,methods for automatically analyzing what peopleare talking about are needed.
Latent topic modelsallow us to extract the main topics in bullying tracesto facilitate understanding.
We used latent Dirich-let alocation (LDA) (Blei, Ng, and Jordan, 2003) asour exploratory tool.
Specifically, we ran a collapsedGibbs sampling implementation of LDA (Griffithsand Steyvers, 2004).The corpus consists of 188K enriched tweets fromAug.
21 to Sept. 17, 2011 that are classified as662bullying traces by our classifier in Task A.
We per-formed stopword removal and further removed wordtypes occurring less than 7 times, resulting in a vo-cabulary of size 12K.
We set the number of topicsto 50, Dirichlet parameter for word multinomials to?
= 0.01, Dirichlet parameter for document topicmultinomial to ?
= 1, and ran Gibbs sampling for10K iterations.Results.
Space precludes a complete list of top-ics.
Figure 4 shows six selected topics discovered byLDA.
Recall that each topic in LDA is a multinomialdistribution over the vocabulary.
The figure showseach topic?s top 20 words with size proportional top(word | topic).
The topic names are manually as-signed.These topics contain semantically coherent wordsrelevant to bullying: (feelings) how people feelabout bullying; (suicide) discussions of suicideevents; (family) sibling names probably used in agood buddy sense; (school) the school environmentwhere bullying commonly occurs; (verbal bullying)derogatory words such as fat and ugly; (physical bul-lying) actions such as kicking and pushing.We also ran a variational inference implementa-tion of LDA (Blei, Ng, and Jordan, 2003).
The re-sults were similar, thus we omit discussion of them.Discussions.
Some recovered topics, includingthe ones shown here, provide valuable insight intobullying traces.
However, not all topics are inter-pretable to social scientists.
It may be helpful to al-low scientists the ability to combine their domainknowledge with latent topic modeling, thus arriv-ing at more useful topics.
For example, the scien-tists can formulate their knowledge in First-OrderLogic, which can then be combined with LDA withstochastic optimization (Andrzejewski et al, 2011).7 Conclusion and Future WorkWe introduced social media as a large-scale, nearreal-time, dynamic data source for the study of bul-lying.
Social media offers a broad range of bully-ing traces that include but go beyond cyberbullying.In the present paper, we have identified several keyproblems in using social media to study bullying andformulated them as familiar NLP tasks.
Our baselineperformance with standard off-the-shelf approachesshows that it is feasible to learn from bullying traces.?feelings?
?suicide??family?
?school?
?verbal bullying?
?physical bullying?Figure 4: Selected topics discovered by latent Dirichletallocation.Much work remains in this new research direc-tion.
In the short term, we need to develop spe-cialized NLP tools for processing bullying traces insocial media, similar to (Ritter et al, 2011; Liu etal., 2010), to achieve better performance than mod-els trained on formal English.
In the long term, weneed to tackle the problem of piecing together theunderlying bullying episodes from fragmental bully-ing traces.
Consider two separate bullying episodeswith the following participants and roles:E1: B: Buffy, V: Vivian & Virginia, O: DebraE2: B: Burton, V: Buffy, O: IreneThe corresponding bullying traces can be three postsin this order:w1 Debra: Virginia, I heard Buffy call you andVivian fat?ignore her!w2 Buffy to Irene: Burton picked on me againbecause I?m only 5?1w3 Vivian: Buffy I?m not fat!
Stop calling me that.Reconstructing E1, E2 fromw1,w2,w3 is challeng-ing for a number of reasons: (1) There is no explicitepisode index in the posts.
(2) Posts from a singleepisode may be dispersed in time (e.g., w1,w3 be-long to E1, but not w2), each containing only part663of an episode.
(3) The number of episodes and peo-ple can grow indefinitely as more posts arrive.
(4)People may switch roles in different episodes (e.g.,Buffy was the bully in E1 but the victim in E2).
Jointprobabilistic modeling over multiple posts using so-cial network structures hold great promise in solvingthis problem.To facilitate bullying research in the NLP com-munity, we make our annotations and softwarepublicly available at http://research.cs.wisc.edu/bullying.AcknowledgmentsWe thank Wei-Ting Chen, Rachael Hansen, Ting-Lan Ma, Ji-in You and Bryan Gibson for their helpon the data and the paper.References[American Psychological Association2004] AmericanPsychological Association.
2004.
APA reso-lution on bullying among children and youth.http://www.apa.org/about/governance/council/policy/bullying.pdf.
[Andrzejewski et al2011] Andrzejewski, David, XiaojinZhu, Mark Craven, and Ben Recht.
2011.
A frame-work for incorporating general domain knowledge intoLatent Dirichlet Allocation using First-Order Logic.In the 22nd IJCAI, pages 1171?1177.
[Archer and Coyne2005] Archer, John and Sarah M.Coyne.
2005.
An integrated review of indirect, re-lational, and social aggression.
Personality and SocialPsychology Review, 9:212?230.
[Bellmore et al2004] Bellmore, Amy D., Melissa R.Witkow, Sandra Graham, and Jaana Juvonen.
2004.Beyond the individual: The impact of ethnic contextand classroom behavioral norms on victims?
adjust-ment.
Developmental Psychology, 40:1159?1172.
[Biggs, Nelson, and Sampilo2010] Biggs, Bridget K.,Jennifer Mize Nelson, and Marilyn L. Sampilo.
2010.Peer relations in the anxiety-depression link: Testof a mediation model.
Anxiety, Stress & Coping,23(4):431?447.
[Blei, Ng, and Jordan2003] Blei, David M., Andrew Y.Ng, and Michael I. Jordan.
2003.
Latent dirichlet allocation.
JMLR, 3:993?1022.
[Blitzer2008] Blitzer, John.
2008.
Domain Adaptation ofNatural Language Processing Systems.
Ph.D. thesis,University of Pennsylvania.
[Bosse and Stam2011] Bosse, Tibor and Sven Stam.2011.
A normative agent system to prevent cyberbul-lying.
In WI-IAT 2011, pages 425?430.
[Brody and Diakopoulos2011] Brody, Samuel andNicholas Diakopoulos.
2011.
Cooooooooooooooolll-lllllllllll!!!!!!!!!!!!!!
using word lengthening to detectsentiment in microblogs.
In EMNLP 2011, pages562?570.
[Cassidy, Jackson, and Brown2009] Cassidy, Wanda,Margaret Jackson, and Karen N. Brown.
2009.
Sticksand stones can break my bones, but how can pixelshurt me?
students?
experiences with cyber-bullying.School Psychology Int?l, 30(4):383?402.
[Chang and Lin2011] Chang, Chih-Chung and Chih-JenLin.
2011.
LIBSVM: A library for support vector ma-chines.
ACM Trans.
on Intelligent Systems and Tech-nology, 2:27:1?27:27.
[Cook et al2010] Cook, Clayton R., Kirk R. Williams,Nancy G. Guerra, Tia E. Kim, and Shelly Sadek.2010.
Predictors of bullying and victimization inchildhood and adolescence: A meta-analytic investi-gation.
School Psychology Quarterly, 25(2):65?83.
[Dinakar, Reichart, and Lieberman2011] Dinakar, K.,R.
Reichart, and H. Lieberman.
2011.
Modeling thedetection of textual cyberbullying.
In InternationalConference on Weblog and Social Media - SocialMobile Web Workshop, Barcelona, Spain.
[Fekkes et al2006] Fekkes, Minne, Frans I.M.
Pijpers,A.
Miranda Fredriks, Ton Vogels, and S. PaulineVerloove-Vanhorick.
2006.
Do bullied children getill, or do ill children get bullied?
a prospective cohortstudy on the relationship between bullying and health-related symptoms.
Pediatrics, 117:1568?1574.
[Finkel, Grenager, and Manning2005] Finkel, Jenny R.,Trond Grenager, and Christopher Manning.
2005.
In-corporating non-local information into information ex-traction systems by Gibbs sampling.
In the 43rd ACL,pages 363?370.
[Fredstrom, Adams, and Gilman2011] Fredstrom, Brid-get K., Ryan E. Adams, and Rich Gilman.
2011.
Elec-tronic and school-based victimization: Unique con-texts for adjustment difficulties during adolescence.
J.Youth and Adolescence, 40(4):405?415.
[Gildea and Jurafsky2002] Gildea, Daniel and Daniel Ju-rafsky.
2002.
Automatic labeling of semantic roles.Comput.
Linguist., 28(3):245?288.
[Gini and Pozzoli2009] Gini, Gianluca and Tiziana Poz-zoli.
2009.
Association between bullying and psy-chosomatic problems: a meta-analysis.
Pediatrics,123(3):1059?1065.
[Graham, Bellmore, and Juvonen2007] Graham, Sandra,Amy Bellmore, and Jaana Juvonen.
2007.
Peer vic-timization in middle school: When self- and peer664views diverge.
In Joseph E. Zins, Maurice J. Elias, andCharles A. Maher, editors, Bullying, victimization, andpeer harassment: A handbook of prevention and inter-vention.
Haworth Press, New York, NY, pages 121?141.
[Griffiths and Steyvers2004] Griffiths, Thomas L. andMark Steyvers.
2004.
Finding scientific topics.PNAS, 101(suppl.
1):5228?5235.
[Hall et al2009] Hall, Mark, Eibe Frank, GeoffreyHolmes, Bernhard Pfahringer, Peter Reutemann, andIan H. Witten.
2009.
The weka data mining software:an update.
ACM SIGKDD Explorations Newsletter,11:10?18.
[Hawker and Boulton2000] Hawker, David S. J. andMichael J. Boulton.
2000.
Twenty years?
research onpeer victimization and psychosocial maladjustment: Ameta-analytic review of cross-sectional studies.
J. ofChild Psychology And Psychiatry, 41(4):441?455.
[Janosz et al2008] Janosz, Michel, Isabelle Archambault,Linda S. Pagani, Sophie Pascal, Alexandre J.S.
Morin,and Franc?ois Bowen.
2008.
Are there detrimentaleffects of witnessing school violence in early adoles-cence?
J. of Adolescent Health, 43(6):600?608.
[Jimerson, Swearer, and Espelage2010] Jimerson,Shane R., Susan M. Swearer, and Dorothy L.Espelage.
2010.
Handbook of Bullying in Schools:An international perspective.
Routledge/Taylor &Francis Group, New York, NY.
[Juvonen and Graham2001] Juvonen, Jaana and SandraGraham.
2001.
Peer harassment in school: The plightof the vulnerable and victimized.
Guilford Press, NewYork, NY.
[Juvonen and Gross2008] Juvonen, Jaana and Elisheva F.Gross.
2008.
Extending the school grounds?
?
Bul-lying experiences in cyberspace.
J. of School Health,78:496?505.
[Kontostathis, Edwards, and Leatherman2010]Kontostathis, April, Lynne Edwards, and AmandaLeatherman.
2010.
Text mining and cybercrime.In Michael W. Berry and Jacob Kogan, editors, TextMining: Applications and Theory.
John Wiley &Sons, Ltd, Chichester, UK.
[Ladd, Kochenderfer, and Coleman1997] Ladd, Gary W.,Becky J. Kochenderfer, and Cynthia C. Coleman.1997.
Classroom peer acceptance, friendship, and vic-timization: Distinct relational systems that contributeuniquely to children?s school adjustment?
Child De-velopment, 68:1181?1197.
[Latham, Crockett, and Bandar2010] Latham, Annabel,Keeley Crockett, and Zuhair Bandar.
2010.
Aconversational expert system supporting bullyingand harassment policies.
In the 2nd ICAART, pages163?168.
[Lieberman, Dinakar, and Jones2011] Lieberman, Henry,Karthik Dinakar, and Birago Jones.
2011.
Let?s gangup on cyberbullying.
Computer, 44:93?96.
[Little et al2003] Little, Todd D., Christopher C. Hen-rich, Stephanie M. Jones, and Patricia H. Hawley.2003.
Disentangling the ?whys?
from the ?whats?
ofaggressive behavior.
Int?l J. of Behavioral Develop-ment, 27:122?133.
[Liu et al2010] Liu, Xiaohua, Kuan Li, Bo Han, MingZhou, Long Jiang, Zhongyang Xiong, and ChangningHuang.
2010.
Semantic role labeling for news tweets.In the 23rd COLING, pages 698?706.
[Ma`rquez et al2005] Ma`rquez, Llu?
?s, Pere Comas, Jesu?sGime?nez, and Neus Catala`.
2005.
Semantic role la-beling as sequential tagging.
In the 9th CoNLL, pages193?196.
[Mihalcea and Strapparava2005] Mihalcea, Rada andCarlo Strapparava.
2005.
Making computers laugh:Investigations in automatic humor recognition.
InEMNLP 2005, pages 531?538.
[Moore et al2003] Moore, Mark H., Carol V. Petrie, An-thony A. Braga, and Brenda L. McLaughlin.
2003.Deadly lessons: Understanding lethal school violence.The National Academies Press, Washington, DC.
[Nansel et al2001] Nansel, Tonja R., Mary Overpeck,Ramani S. Pilla, W. June Ruan, Bruce Simons-Morton, and Peter Scheidt.
2001.
Bullying behav-iors among US youth: prevalence and association withpsychosocial adjustment.
J. Amer.
Medical Assoc.,285(16):2094?2100.
[Nishina and Bellmore2010] Nishina, Adrienne andAmy D. Bellmore.
2010.
When might aggression,victimization, and conflict matter most?
: Contextualconsiderations.
J. of Early Adolescence, pages 5?26.
[Nishina and Juvonen2005] Nishina, Adrienne and JaanaJuvonen.
2005.
Daily reports of witnessing and ex-periencing peer harassment in middle school.
ChildDevelopment, 76:435?450.
[Nylund et al2007] Nylund, Karen, Amy Bellmore, Adri-enne Nishina, and Sandra Graham.
2007.
Subtypes,severity, and structural stability of peer victimization:What does latent class analysis say?
Child Develop-ment, 78:1706?1722.
[Olweus1993] Olweus, Dan.
1993.
Bullying at school:What we know and what we can do.
Blackwell, Ox-ford, UK.
[Pang and Lee2004] Pang, Bo and Lillian Lee.
2004.
Asentimental education: Sentiment analysis using sub-jectivity summarization based on minimum cuts.
Inthe 42nd ACL, pages 271?278.
[Ptaszynski et al2010] Ptaszynski, Michal, Pawel Dy-bala, Tatsuaki Matsuba, Fumito Masui, Rafal Rzepka,665and Kenji Araki.
2010.
Machine learning and af-fect analysis against cyber-bullying.
In the 36th AISB,pages 7?16.
[Punyakanok, Roth, and Yih2008] Punyakanok, Vasin,Dan Roth, and Wen-tau Yih.
2008.
The importanceof syntactic parsing and inference in semantic rolelabeling.
Comput.
Linguist., 34(2):257?287.
[Ratinov and Roth2009] Ratinov, Lev and Dan Roth.2009.
Design challenges and misconceptions innamed entity recognition.
In the 13th CoNLL, pages147?155.
[Ritter et al2011] Ritter, Alan, Sam Clark, Mausam, andOren Etzioni.
2011.
Named entity recognition intweets: An experimental study.
In EMNLP 2011,pages 1524?1534.
[Rivers et al2009] Rivers, Ian, V. Paul Poteat, NathalieNoret, and Nigel Ashurst.
2009.
Observing bullyingat school: The mental health implications of witnessstatus.
School Psychology Quarterly, 24(4):211?223.
[Salmivalli1999] Salmivalli, Christina.
1999.
Participantrole approach to school bullying: Implications for in-tervention.
J. of Adolescence, 22(4):453?459.
[Schmidt, Fung, and Rosales2007] Schmidt, Mark W.,Glenn Fung, and Ro?mer Rosales.
2007.
Fast opti-mization methods for l1 regularization: A comparativestudy and two new approaches.
In the 18th ECML,pages 286?297.
[Schwartz et al2005] Schwartz, David, Andrea Hop-meyer Gorman, Jonathan Nakamoto, and Robin L. To-blin.
2005.
Victimization in the peer group and chil-dren?s academic functioning.
J. of Educational Psy-chology, 87:425?435.
[Settles2011] Settles, Burr.
2011.
Closing the loop: Fast,interactive semi-supervised annotation with queries onfeatures and instances.
In the EMNLP 2011, pages1467?1478.
[Smith, Madsen, and Moody1999] Smith, Peter K.,Kirsten C. Madsen, and Janet C. Moody.
1999.
Whatcauses the age decline in reports of being bullied atschool?
Towards a developmental analysis of risks ofbeing bullied.
Educational Research, 41(3):267?285.
[The American Academy of Pediatrics2009] The Ameri-can Academy of Pediatrics.
2009.
Policy statement?role of the pediatrician in youth violence prevention.Pediatrics, 124(1):393?402.
[The White House2011] The White House.
2011.Background on White House conference on bul-lying prevention.
http://www.whitehouse.gov/the-press-office/2011/03/10/background-white-house-conference-bullying-prevention.
[Toutanova et al2003] Toutanova, Kristina, Dan Klein,Christopher D. Manning, and Yoram Singer.
2003.Feature-rich part-of-speech tagging with a cyclic de-pendency network.
In NAACL-HLT 2003, pages 173?180.
[Vaillancourt et al2010] Vaillancourt, Tracy, Vi Trinh,Patricia McDougall, Eric Duku, Lesley Cunning-ham, Charles Cunningham, Shelley Hymel, and KathyShort.
2010.
Optimizing population screening of bul-lying in school-aged children.
J. of School Violence,9:233?250.
[Vandebosch and Cleemput2009] Vandebosch, Heidi andKatrien Van Cleemput.
2009.
Cyberbullying amongyoungsters: profiles of bullies and victims.
New media& society, 11(8):1349?1371.
[Wang, Iannotti, and Nansel2009] Wang, Jing, Ronald J.Iannotti, and Tonja R. Nansel.
2009.
School bully-ing among adolescents in the united states: Physical,verbal, relational, and cyber.
J. Adolescent Health,45(4):368?375.666
