Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 80?88,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsProbabilistic Type Theory for Incremental Dialogue ProcessingJulian Hough and Matthew PurverCognitive Science Research GroupSchool of Electronic Engineering and Computer ScienceQueen Mary University of London{j.hough,m.purver}@qmul.ac.ukAbstractWe present an adaptation of recent workon probabilistic Type Theory with Records(Cooper et al., 2014) for the purposes ofmodelling the incremental semantic pro-cessing of dialogue participants.
Afterpresenting the formalism and dialogueframework, we show how probabilisticTTR type judgements can be integratedinto the inference system of an incremen-tal dialogue system, and discuss how thiscould be used to guide parsing and dia-logue management decisions.1 IntroductionWhile classical type theory has been the predomi-nant mathematical framework in natural languagesemantics for many years (Montague, 1974, in-ter alia), it is only recently that probabilistic typetheory has been discussed for this purpose.
Sim-ilarly, type-theoretic representations have beenused within dialogue models (Ginzburg, 2012);and probabilistic modelling is common in dia-logue systems (Williams and Young, 2007, interalia), but combinations of the two remain scarce.Here, we attempt to make this connection, taking(Cooper et al., 2014)?s probabilistic Type Theorywith Records (TTR) as our principal point of de-parture, with the aim of modelling incremental in-ference in dialogue.To our knowledge there has been no practi-cal integration of probabilistic type-theoretic in-ference into a dialogue system so far; here we dis-cuss computationally efficient methods for imple-mentation in an extant incremental dialogue sys-tem.
This paper demonstrates their efficacy in sim-ple referential communication domains, but we ar-gue the methods could be extended to larger do-mains and additionally used for on-line learningin future work.2 Previous WorkType Theory with Records (TTR) (Betarte andTasistro, 1998; Cooper, 2005) is a rich type the-ory which has become widely used in dialoguemodels, including information state models fora variety of phenomena such as clarification re-quests (Ginzburg, 2012; Cooper, 2012) and non-sentential fragments (Ferna?ndez, 2006).
It has alsobeen shown to be useful for incremental semanticparsing (Purver et al., 2011), incremental genera-tion (Hough and Purver, 2012), and recently forgrammar induction (Eshghi et al., 2013).While the technical details will be given in sec-tion 3, the central judgement in type theory s ?
T(that a given object s is of type T ) is extendedin TTR so that s can be a (potentially complex)record and T can be a record type ?
e.g.
s couldrepresent a dialogue gameboard state and T couldbe a dialogue gameboard state type (Ginzburg,2012; Cooper, 2012).
As TTR is highly flexiblewith a rich type system, variants have been con-sidered with types corresponding to real-number-valued perceptual judgements used in conjunctionwith linguistic context, such as visual perceptualinformation (Larsson, 2011; Dobnik et al., 2012),demonstrating its potential for embodied learningsystems.
The possibility of integration of per-ceptron learning (Larsson, 2011) and naive Bayesclassifiers (Cooper et al., 2014) into TTR showhow linguistic processing and probabilistic con-ceptual inference can be treated in a uniform waywithin the same representation system.Probabilistic TTR as described by Cooper et al.
(2014) replaces the categorical s ?
T judgementwith the real number valued p(s ?
T ) = v wherev ?
[0,1].
The authors show how standard proba-bility theoretic and Bayesian equations can be ap-plied to TTR judgements and how an agent mightlearn from experience in a simple classificationgame.
The agent is presented with instances of80a situation and it learns with each round by updat-ing its set of probabilistic type judgements to bestpredict the type of object in focus ?
in this caseupdating the probability judgement that somethingis an apple given its observed colour and shapep(s ?
Tapple?
s ?
TShp, s ?
TCol) where Shp ?
{shp1, shp2} and Col ?
{col1, col2}.
From acognitive modelling perspective, these judgementscan be viewed as probabilistic perceptual informa-tion derived from learning.
We use similar meth-ods in our toy domain, but show how prior judge-ments could be constructed efficiently, and howclassifications can be made without exhaustive it-eration through individual type classifiers.There has also been significant experimentalwork on simple referential communication gamesin psycholinguistics, computational and formalmodelling.
In terms of production and genera-tion, Levelt (1989) discusses speaker strategiesfor generating referring expressions in a simpleobject naming game.
He showed how speakersuse informationally redundant features of the ob-jects, violating Grice?s Maxim of Quantity.
Innatural language generation (NLG), referring ex-pression generation (REG) has been widely stud-ied (see (Krahmer and Van Deemter, 2012) fora comprehensive survey).
The incremental algo-rithm (IA) (Dale and Reiter, 1995) is an iterativefeature selection procedure for descriptions of ob-jects based on computing the distractor set of ref-erents that each adjective in a referring expressioncould cause to be inferred.
More recently Frankand Goodman (2012) present a Bayesian modelof optimising referring expressions based on sur-prisal, the information-theoretic measure of howmuch descriptions reduce uncertainty about theirintended referent, a measure which they claim cor-relates strongly to human judgements.The element of the referring expression do-main we discuss here is incremental processing.There is evidence from (Brennan and Schober,2001)?s experiments that people reason at an in-credibly time-critical level from linguistic infor-mation.
They demonstrated self-repair can speedup semantic processing (or at least object refer-ence) in such games, where an incorrect objectbeing partly vocalized and then repaired in theinstructions (e.g.
?the yell-, uh, purple square?
)yields quicker response times from the onset ofthe target (?purple?)
than in the case of the flu-ent instructions (?the purple square?).
This exam-ple will be addressed in section 5.
First we willset out the framework in which we want to modelsuch processing.3 Probabilistic TTR in an incrementaldialogue frameworkIn TTR (Cooper, 2005; Cooper, 2012), the princi-pal logical form of interest is the record type (?RT?from here), consisting of sequences of fields of theform [ l ?
T ] containing a label l and a type T .1RTs can be witnessed (i.e.
judged as inhabited)by records of that type, where a record is a set oflabel-value pairs [ l = v ].
The central type judge-ment in TTR that a record s is of (record) typeR, i.e.
s ?
R, can be made from the componenttype judgements of individual fields; e.g.
the one-field record [ l = v ] is of type [ l ?
T ] just in casev is of type T .
This is generalisable to records andRTs with multiple fields: a record s is of RT R ifs includes fields with labels matching those occur-ring in the fields of R, such that all fields in R arematched, and all matched fields in s must have avalue belonging to the type of the correspondingfield in R. Thus it is possible for s to have morefields than R and for s ?
R to still hold, but notvice-versa: s ?
R cannot hold if R has more fieldsthan s.R1???????????l1?
T1l2?
T2l3?
T3(l1)??????????
R2 ?
[l1?
T1l2?
T2?]
R3?
[]Figure 1: Example TTR record typesFields can have values representing predicatetypes (ptypes), such as T3in Figure 1, and conse-quently fields can be dependent on fields preced-ing them (i.e.
higher) in the RT, e.g.
l1is bound inthe predicate type field l3, so l3depends on l1.Subtypes, meets and joins A relation betweenRTs we wish to explore is ?
(?is a subtype of?
),which can be defined for RTs in terms of fields assimply: R1?
R2if for all fields [ l ?
T2] in R2,R1contains [ l ?
T1] where T1?
T2.
In Figure 1,both R1?
R3and R2?
R3; and R1?
R2iffT2?
T2?
.
The transitive nature of this relation (ifR1?
R2and R2?
R3then R1?
R3) can be usedeffectively for type-theoretic inference.1We only introduce the elements of TTR relevant to thephenomena discussed below.
See (Cooper, 2012) for a de-tailed formal description.81We also assume the existence of manifest (sin-gleton) types, e.g.
Ta, the type of which only a isa member.
Here, we write manifest RT fields suchas [ l ?
Ta] where Ta?
T using the syntactic sugar[ l=a ?
T ].
The subtype relation effectively allowsprogressive instantiation of fields (as addition offields to R leads to R?
where R?
?
R), which ispractically useful for an incremental dialogue sys-tem as we will explain.We can also define meet and join types of twoor more RTs.
The representation of the meet typeof two RTs R1and R2is the result of a mergeoperation (Larsson, 2010), which in simple termshere can be seen as union of fields.
A meet typeis also equivalent to the extraction of a maxi-mal common subtype, an operation we will callMaxSub(Ri..Rn):2if R1= [ l1 ?
T1l2?
T2] and R2= [ l2 ?
T2l3?
T3]R1?R2=?????????l1?
T1l2?
T2l3?
T3????????
?= MaxSub(R1,R2)R1and R2here are common supertypes of theresulting R1?
R2.
On the other hand, the join oftwo RTs R1and R2, the type R1?
R2cannot berepresented by field intersection.
It is defined interms of type checking, in that s ?
R1?
R2iffs ?
R1or s ?
R2.
It follows that if R1?
R2thens ?
R1?R2iff s ?
R1, and s ?
R1?
R2iff s ?
R2.While technically the maximally common su-pertype of R1and R2is the join type R1?
R2,here we introduce the maximally common simple(non disjunctive) supertype of two RTs R1and R2as field intersection:if R1= [ l1 ?
T1l2?
T2] and R2= [ l2 ?
T2l3?
T3]MaxSuper(R1, R2) = [ l2?
T2]We will explore the usefulness of this new op-eration in terms of RT lattices in sec.
4.3.1 Probabilistic TTRWe follow Cooper et al.
(2014)?s recent extensionof TTR to include probabilistic type judgements ofthe form p(s ?
R) = v where v ?
[0,1], i.e.
the realvalued judgement that a record s is of RT R. Here2Here we concern ourselves with simple examples thatavoid label-type clashes between two RTs (i.e.
where R1con-tains l1?
T1 and R2contains l1?
T2); in these cases the op-erations are more complex than field concatenation/sharing.we use probabilistic TTR to model a common psy-cholinguistic experimental set up in section 5.
Werepeat some of Cooper et al.
?s calculations herefor exposition, but demonstrate efficient graphicalmethods for generating and incrementally retriev-ing probabilities in section 4.Cooper et al.
(2014) define the probability of themeet and join types of two RTs as follows:p(s ?
R1?R2) = p(s ?
R1)p(s ?
R2?
s ?
R1)p(s ?
R1?R2) = p(s ?
R1) + p(s ?
R2) ?
p(s ?
R1?R2)(1)It is practically useful, as we will describe be-low, that the join probability can be computed interms of the meet.
Also, there are equivalences be-tween meets, joins and subtypes in terms of typejudgements as described above, in that assumingif R1?
R2then p(s ?
R2?
s ?
R1) = 1, we have:if R1?
R2p(s ?
R1?R2) = p(s ?
R1)p(s ?
R1?R2) = p(s ?
R2)p(s ?
R1) ?
p(s ?
R2)(2)The conditional probability of a record being oftype R2given it is of type R1is:p(s ?
R2?
s ?
R1) = p(s ?
R1 ?
s ?
R2)p(s ?
R1) (3)We return to an explanation for these classicalprobability equations holding within probabilisticTTR in section 4.Learning and storing probabilistic judgementsWhen dealing with referring expression games, orindeed any language game, we need a way of stor-ing perceptual experience.
In probabilistic TTRthis can be achieved by positing a judgement set Jin which an agent stores probabilistic type judge-ments.3 We refer to the sum of the value of proba-bilistic judgements that a situation has been judgedto be of type Riwithin J as ?Ri?Jand the sum ofall probabilistic judgements in J simply as P (J );thus the prior probability that anything is of typeRiunder the set of judgements J is ?Ri?JP (J ) .
Theconditional probability p(s ?
R1?
s ?
R2) un-der J can be reformulated in terms of these setsof judgements:pJ(s ?
R1?
s ?
R2) = { ?R1?R2?J?R2?J iff ?R2?J ?
00 otherwise(4)3(Cooper et al., 2014) characterise a type judgement as anAustinian proposition that a situation is of a given type witha given probability, encoded in a TTR record.82where the sample spaces ?R1?
R2?Jand ?R2?Jconstitute the observations of the agent so far.
Jcan have new judgements added to it during learn-ing.
We return to this after introducing the incre-mental semantics needed to interface therewith.3.2 DS-TTR and the DyLan dialogue systemIn order to permit type-theoretic inference in adialogue system, we need to provide suitableTTR representations for utterances and the cur-rent pragmatic situation from a parser, dialoguemanager and generator as instantaneously and ac-curately as possible.
For this purpose we usean incremental framework DS-TTR (Eshghi etal., 2013; Purver et al., 2011) which integratesTTR representations with the inherently incre-mental grammar formalism Dynamic Syntax (DS)(Kempson et al., 2001).
?, T y(t),???????????
?x=john ?
ee=arrive ?
esp=subj(e,x) ?
thead=p ?
t???????????
?Ty(e),[ x=john ?
ehead=x ?
e ]Ty(e ?
t),?r ?
[ head ?
e ] .???????????
?x=r.head ?
ee=arrive ?
esp=subj(e,x) ?
thead=p ?
t???????????
?Figure 2: DS-TTR treeDS produces an incrementally specified, partiallogical tree as words are parsed/generated; follow-ing Purver et al.
(2011), DS tree nodes are dec-orated not with simple atomic formulae but withRTs, and corresponding lambda abstracts repre-senting functions of type RT ?
RT (e.g.
?r ?
[ l1?
T1].
[ l2=r.l1?
T1] where r.l1is a path ex-pression referring to the label l1in r) ?
see Fig-ure 2.
Using the idea of manifestness of fieldsas mentioned above, we have a natural represen-tation for underspecification of leaf node content,e.g.
[x ?
e ] is unmanifest whereas [x=john ?
e ]4is manifest and the latter is a subtype of the for-mer.
Functional application can apply incremen-tally, allowing a RT at the root node to be com-piled for any partial tree, which is incrementallyfurther specified as parsing proceeds (Hough andPurver, 2012).
Within a given parse path, due to4This is syntactic sugar for [ x ?
ejohn] and the = sign isnot the same semantically as that in a record.DS-TTR?s monotonicity, each maximal RT of thetree?s root node is a subtype of the parser?s previ-ous maximal output.Following (Eshghi et al., 2013), DS-TTR treenodes include a field head in all RTs which cor-responds to the DS tree node type.
We also as-sume a neo-Davidsonian representation of predi-cates, with fields corresponding to an event termand to each semantic role; this allows all availablesemantic information to be specified incrementallyin a strict subtyping relation (e.g.
providing thesubj() field when subject but not object has beenparsed) ?
see Figure 2.We implement DS-TTR parsing and genera-tion mechanisms in the DyLan dialogue system5within Jindigo (Skantze and Hjalmarsson, 2010),a Java-based implementation of the incrementalunit (IU) framework of (Schlangen and Skantze,2009).
In this framework, each module has inputand output IUs which can be added as edges be-tween vertices in module buffer graphs, and be-come committed should the appropriate condi-tions be fulfilled, a notion which becomes im-portant in light of hypothesis change and repairsituations.
Dependency relations between differ-ent graphs within and between modules can bespecified by groundedIn links (see (Schlangen andSkantze, 2009) for details).The DyLan interpreter module (Purver et al.,2011) uses Sato (2011)?s insight that the context ofDS parsing can be characterized in terms of a Di-rected Acyclic Graph (DAG) with trees for nodesand DS actions for edges.
The module?s state ischaracterized by three linked graphs as shown inFigure 3:?
input: a time-linear word graph posted by theASR module, consisting of word hypothesisedge IUs between vertices Wn?
processing: the internal DS parsing DAG,which adds parse state edge IUs between ver-tices SngroundedIn the corresponding wordhypothesis edge IU?
output: a concept graph consisting of domainconcept IUs (RTs) as edges between verticesCn, groundedIn the corresponding path in theDS parsing DAGHere, our interest is principally in the parser out-put, to support incremental inference; a DS-TTRgenerator is also included which uses RTs as goalconcepts (Hough and Purver, 2012) and uses the5Available from http://dylan.sourceforge.net/83same parse graph as the interpreter to allow self-monitoring and compound contributions, but weomit the details here.Figure 3: Normal incremental parsing in Dylan4 Order theoretic and graphical methodsfor probabilistic TTRRT lattices to encode domain knowledge Tosupport efficient inference in DyLan, we representdialogue domain concepts via partially orderedsets (posets) of RT judgements, following similarinsights used in inducing DS-TTR actions (Eshghiet al., 2013).
A poset has several advantages overan unordered list of un-decomposed record types:the possibility of incremental type-checking; in-creased speed of type-checking, particularly forpairs of/multiple type judgements; immediate useof type judgements to guide system decisions; in-ference from negation; and the inclusion of learn-ing within a domain.
We leave the final challengefor future work, but discuss the others here.We can construct a poset of type judgementsfor any single RT by decomposing it into its con-stituent supertype judgements in a record type lat-tice.
Representationally, as per set-theoretic lat-tices, this can be visualised as a Hasse diagramsuch as Figure 4, however here the ordering arrowsshow ?
(?subtype of?)
relations from descendant toancestor nodes.To characterize an RT lattice G ordered by ?,we adapt Knuth (2005)?s description of lattices inline with standard order theory: for a pair of RTelements Rxand Ry, their lower bound is the setof all Rz?
G such that Rz?
Rxand Rz?
Ry.In the event that a unique greatest lower bound ex-ists, this is their meet, which in G happily corre-sponds to the TTR meet type Rx?
Ry.
Dually, iftheir unique least upper bound exists, this is theirR1200= [] = ?R120= [ a ?
b ] R121= [ c ?
d ] R110= [ e ?
f ]R10= [ a ?
bc ?
d ] R11 = [ a ?
be ?
f ] R12 = [ c ?
de ?
f ]R1=????????
?a ?
bc ?
de ?
f?????????
= ?Figure 4: Record Type lattice ordered by the sub-type relationjoin and in TTR terms is MaxSuper(Rx, Ry) butnot necessarily their join type Rx?
Ryas herewe concern ourselves with simple RTs.
One el-ement covers another if it is a direct successor toit in the subtype ordering relation hierarchy.
Ghas a greatest element (?)
and least element (?
),with the atoms being the elements that cover ?
;in Figure 4 if R1is viewed as ?
, the atoms areR{10,11,12}.
An RT element Rx has a comple-ment if there is a unique element ?Rxsuch thatMaxSuper(Rx,?Rx) = ?
and Rx?
?Rx= ?
(the lattice in Figure 4 is complemented as thisholds for every element).Graphically, the join of two elements can befound by following the connecting edges upwarduntil they first converge on a single RT, giving usMaxSuper(R10, R12) = R121in Figure 4, and themeet can be found by following the lines down-ward until they connect to give their meet type,i.e.
R10?R12= R1.If we consider R1to be a domain concept ina dialogue system, we can see how its RT latticeG can be used for incremental inference.
As in-crementally specified RTs become available fromthe interpreter they are matched to those in G todetermine how far down towards the final domainconcept R1our current state allows us to be.
Dif-ferent sequences of words/utterances lead to dif-ferent paths.
However, any practical dialogue sys-tem must entertain more than one possible domainconcept as an outcome; G must therefore containmultiple possible final concepts, constituting itsatoms, each with several possible dialogue movesequences, which correspond to possible down-ward paths ?
e.g.
see the structure of Figure 5.Our aim here is to associate each RT in G with aprobabilistic judgement.Initial lattice construction We define a simplebottom-up procedure in Algorithm 1 to build a RT84lattice G of all possible simple domain RTs andtheir prior probabilistic judgements, initialised bythe disjunction of possible final state judgements(the priors),6 along with the absurdity ?, stipu-lated a priori as the least element with probability0 and the meet type of the atomic priors.
The al-gorithm recursively removes one field from the RTbeing processed at a time (except fields referencedin a remaining dependent ptype field), then ordersthe new supertype RT in G appropriately.Each node in G contains its RT Riand a sumof probability judgements {?Rk?J+ .. + ?Rn?J}corresponding to the probabilities of the priors itstands in a supertype relation to.
These sums arepropagated up from child to parent node as it isconstructed.
It terminates when all simple maxi-mal supertypes7 have been processed, leaving themaximally common supertype as ?
(possibly theempty type [ ]), associated with the entire proba-bility mass P (J ), which constitutes the denomina-tor to all judgements- given this, only the numer-ator of equation ?Ri?JP (J ) needs to be stored at eachnode.Algorithm 1 Probabilistic TTR record type latticeconstruction algorithmINPUT: priors ?
use the initial prior judgements for G?s atomsOUTPUT: GG = newGraph(priors) ?
P(J) set to equal sum of prior probsagenda = priors ?
Initialise agendawhile not agenda is empty doRT = agenda.pop()for field ?
RT doif field ?
RT.paths then ?
Do not remove bound fieldscontinuesuperRT = RT - fieldif superRT ?
G then ?
not new?
order w.r.t.
RT and inherit RT?s priorsG.order(RT.address,G.getNode(superRT),?
)else ?
new?superNode = G.newNode(superRT) ?
create new node w. empty priorsfor node ?
G do ?
order superNode w.r.t.
other nodes in Gif superRT.fields ?
node.fields thenG.order(node,superNode,?)
?
superNode inherits node?s priorsagenda.append(superRT) ?
add to agenda for further supertypingDirect inference from the lattice To explainhow our approach models incremental inference,we assume Brennan and Schober (2001)?s experi-mental referring game domain described in section2: three distinct domain situation RTs R1, R2andR3correspond to a purple square, a yellow squareand a yellow circle, respectively.The RT lattice G constructed initially upon ob-servation of the game (by instructor or instructee)shown in Figure 5 uses a uniform distribution for6Although the priors?
disjunctive probability sums to 1 af-ter G is constructed, i.e.
in Figure 5 ?R1?J+?R2?J+?R3?JP (J ) = 1,the real values initially assigned to them need not sum tounity, as they form the atoms of G (see (Knuth, 2005)).7Note that it does not generate the join types but maximalcommon supertypes defined by field intersection.the three disjunctive final situations.
Each nodeshows an RT Rion the left and the derivation ofits prior probability pJ(Ri) that any game situa-tion record will be of type Rion the right, purelyin terms of the relevant priors and the global de-nominator P (J ).G can be searched to make inferences in lightof partial information from an ongoing utterance.We model inference as predicting the likelihoodof relevant type judgements Ry?
G of a situa-tion s, given the judgement s ?
Rxwe have so far.To do this we use conditional probability judge-ments following Knuth?s work on distributive lat-tices, using the ?
relation to give a choice function:pJ(s ?
Ry?
s ?
Rx) =??????????
?1 if Rx?
Ry0 if Rx?Ry= ?p otherwise, where 0 ?
p ?
1(5)The third case is the degree of inclusion of Ryin Rx, and can be calculated using the conditionalprobability calculation (4) in sec.
3.
For nega-tive RTs, a lattice generated from Algorithm 1 willbe distributive but not guaranteed to be comple-mented, however we can still derive pJ(s ?
Ry?s ?
?Rx) by obtaining pJ(s ?
Ry) in G modulo theprobability mass of Rxand that of its subtypes:pJ(s ?
Ry?
s ?
?Rx) = {0 if Ry ?
RxpJ(s?Ry)?pJ(s?Rx?Ry)pJ(s??
)?pJ(s?Rx) otherwise(6)The subtype relations and atomic, join and meettypes?
probabilities required for (1) - (6) can becalculated efficiently through graphical search al-gorithms by characterising G as a DAG: the re-verse direction of the subtype ordering edges canbe viewed as reachability edges, making ?
thesource and ?
the sink.
With this characterisation,if Rxis reachable from Rythen Rx?
Ry.In DAG terms, the probability of the meet oftwo RTs Rxand Rycan be found at their highestcommon descendant node ?
e.g.
pJ(R4?
R5) inFigure 5 can be found as 13directly at R1.
Note ifRxis reachable from Ry, i.e.
Rx?
Ry, then dueto the equivalences listed in (2), pJ(Rx?
Ry) canbe found directly at Rx.
If the meet of two nodesis ?
(e.g.
R4and R3in Figure 5), then their meetprobability is 0 as pJ(?
)=0.While the lattice does not have direct access tothe join types of its elements, a join type prob-ability pJ(Rx?
Ry) can be calculated in termsof pJ(Rx?
Ry) by the join equation in (1),which holds for all probabilistic distributive lat-85PRIORS:?R1?J= 13?R2?J= 13?R3?J= 13R8= [ x ?ind ] ?R1?J +?R2?J +?R3?JP (J ) = ?
= 1R4= [ x ?
indshpsq?
square(x) ] ?R1?J +?R2?JP (J ) R5 = [ x ?
indcolp?
purple(x) ] ?R1?JP (J ) R6 = [ x ?
indcoly?
yellow(x) ] ?R2?J +?R3?JP (J ) R7 = [ x ?
indshpc?
circle(x) ] ?R3?JP (J )R1=????????
?x ?
indcolp?
purple(x)shpsq?
square(x)?????????
?R1?JP (J ) R2 =????????
?x ?
indcoly?
yellow(x)shpsq?
square(x)?????????
?R2?JP (J ) R3 =????????
?x ?
indcoly?
yellow(x)shpc?
circle(x)?????????
?R3?JP (J )R0= ?
= 0Figure 5: Record type lattice with initial uniform prior probablitiestices (Knuth, 2005).8 As regards efficiency, worstcase complexity for finding the meet probability atthe common descendant of Rxand Ryis a linearO(m+ n) where m and n are the number of edgesin the downward (possibly forked) paths Rx?
?and Ry?
?.95 Simulating incremental inference andself-repair processingInterpretation in DyLan and its interface to theRT lattice G follows evidence that dialogue agentsparse self-repairs efficiently and that repaired di-alogue content (reparanda) is given special sta-tus but not removed from the discourse context.To model Brennan and Schober (2001)?s findingsof disfluent spoken instructions speeding up ob-ject recognition (see section 2), we demonstratea self-repair parse in Figure 6 for ?The yell-, uh,purple square?
in the simple game of predictingthe final situation from {R1, R2, R3} continuouslygiven the type judgements made so far.
We de-scribe the stages T1-T4 in terms of the currentword being processed- see Figure 6:At T1:?the?
the interpreter will not yield a sub-type checkable in G so we can only condition onR8(?
), giving us pJ(s ?
Ri?
s ?
R8) = 13fori ?
{1, 2, 3}, equivalent to the priors.
At T2:8The search for the meet probability is generalisable toconjunctive types by searching for the conjuncts?
highestcommon descendant.
The join probability is generalisable tothe disjunctive probability of multiple types, used, albeit pro-gramatically, in Algorithm 1 for calculating a node?s proba-bility from its child nodes.9While we do not give details here, simple graphicalsearch algorithms for conjunctive and disjunctive multipletypes are linear in the number of conjuncts and disjuncts, sav-ing considerable time in comparison to the algebraic calcula-tions of the sum and product rules for distributive lattices.
?yell-?, the best partial word hypothesis is now?yellow?
;10 the interpreter therefore outputs an RTwhich matches the type judgement s ?
R6(i.e.
thatthe object is a yellow object).
Taking this judge-ment as the conditioning evidence using function(5) we get pJ(s ?
R1?
s ?
R6) = 0 and us-ing (4) we get pJ(s ?
R2?
s ?
R6) = 0.5 andpJ(s ?
R3?
s ?
R6) = 0.5 (see the schematicprobability distribution at stage T2 in Figure 6 forthe three objects).
The meet type probabilitiesrequired for the conditional probabilities can befound graphically as described above.At T3:?uh purple?, low probability in the in-terpreter output causes a self-repair to be recog-nised, enforcing backtracking on the parse graphwhich informally operates as follows (see Houghand Purver (2012)) :Self-repair:IF from parsing word W the edge SEnis in-sufficiently likely to be constructed from ver-tex SnOR IF there is no sufficiently likelyjudgement p(s ?
Rx) for Rx?
GTHEN parse word W from vertex Sn?1.
IFsuccessful add a new edge to the top path,without removing any committed edges be-ginning at Sn?1; ELSE set n=n?1 and repeat.This algorithm is consistent with a local modelfor self-repair backtracking found in corpora(Shriberg and Stolcke, 1998; Hough and Purver,2013).
As regards inference in G, upon detectionof a self-repair that revokes s ?
R6, the type judge-ment s ?
?R6, i.e.
that this is not a yellow object,10In practice, ASR modules yielding partial results are lessreliable than their non-incremental counterparts, but progressis being made here (Schlangen and Skantze, 2009).86Figure 6: Incremental DS-TTR self-repair parsing.
Inter-graph groundedIn links go top to bottom.is immediately available as conditioning evidence.Using (6) our distribution of RT judgements nowshifts: pJ(s ?
R1?
s ?
?R6) = 1, pJ(s ?
R2?s ?
?R6) = 0 and pJ(s ?
R3?
s ?
?R6) = 0 be-fore ?purple?
has been parsed ?
thus providing aprobabilistic explanation for increased subsequentprocessing speed.
Finally at T4: ?square?
givenpJ(s ?
R1?
s ?
R1) = 1 and R1?R2= R1?R3= ?,the distribution remains unchanged.The system?s processing models how listen-ers reason about the revocation itself rather thanpredicting the outcome through positive evidencealone, in line with (Brennan and Schober, 2001)?sresults.6 ExtensionsDialogue and self-repair in the wild To movetowards domain-generality, generating the latticeof all possible dialogue situations for interestingdomains is computationally intractable.
We in-tend instead to consider incrementally occurringissues that can be modelled as questions (Lars-son, 2002).
Given one or more issues manifest inthe dialogue at any time, it is plausible to gener-ate small lattices dynamically to estimate possibleanswers, and also assign a real-valued relevancemeasure to questions that can be asked to resolvethe issues.
We are exploring how this could beimplemented using the inquiry calculus (Knuth,2005), which defines information theoretic rele-vance in terms of a probabilistic question lattice,and furthermore how this could be used to modelthe cause of self-repair as a time critical trade-offbetween relevance and accuracy.Learning in a dialogue While not our focushere, lattice G?s probabilities can be updatedthrough observations after its initial construction.If a reference game is played over several rounds,the choice of referring expression can changebased on mutually salient functions from wordsto situations- see e.g.
(DeVault and Stone, 2009).Our currently frequentist approach to learning is:given an observation of an existing RT Riis madewith probability v, then ?Ri?J, the overall denom-inator P (J ) , and the nodes in the upward pathfrom Rito ?
are incremented by v. The approachcould be converted to Bayesian update learning byusing the prior probabilities in G for calculating vbefore it is added.
Furthermore, observations canbe added to G that include novel RTs: due to theDAG structure of G, their subtype ordering andprobability effects can be integrated efficiently.7 ConclusionWe have discussed efficient methods for construct-ing probabilistic TTR domain concept lattices or-dered by the subtype relation and their use inincremental dialogue frameworks, demonstratingtheir efficacy for realistic self-repair processing.We wish to explore inclusion of join types, thescalability of RT lattices to other domains andtheir learning capacity in future work.AcknowledgementsWe thank the two TTNLS reviewers for their com-ments.
Purver is supported in part by the EuropeanCommunity?s Seventh Framework Programme un-der grant agreement no 611733 (ConCreTe).87ReferencesG.
Betarte and A. Tasistro.
1998.
Extension of Martin-Lo?f type theory with record types and subtyping.
InG.
Sambin and J. Smith, editors, 25 Years of Con-structive Type Theory.
Oxford University Press.S.
Brennan and M. Schober.
2001.
How listenerscompensate for disfluencies in spontaneous speech.Journal of Memory and Language, 44(2):274?296.R.
Cooper, S. Dobnik, S. Lappin, and S. Larsson.
2014.A probabilistic rich type theory for semantic inter-pretation.
In Proceedings of the EACL Workshopon Type Theory and Natural Language Semantics(TTNLS).R.
Cooper.
2005.
Records and record types in se-mantic theory.
Journal of Logic and Computation,15(2):99?112.R.
Cooper.
2012.
Type theory and semantics in flux.In R. Kempson, N. Asher, and T. Fernando, edi-tors, Handbook of the Philosophy of Science, vol-ume 14: Philosophy of Linguistics, pages 271?323.North Holland.R.
Dale and E. Reiter.
1995.
Computational interpreta-tions of the gricean maxims in the generation of re-ferring expressions.
Cognitive Science, 19(2):233?263.D.
DeVault and M. Stone.
2009.
Learning to interpretutterances using dialogue history.
In Proceedings ofthe 12th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL).S.
Dobnik, R. Cooper, and S. Larsson.
2012.
Mod-elling language, action, and perception in type the-ory with records.
In Proceedings of the 7th Inter-national Workshop on Constraint Solving and Lan-guage Processing (CSLP12).A.
Eshghi, J. Hough, and M. Purver.
2013.
Incre-mental grammar induction from child-directed di-alogue utterances.
In Proceedings of the 4th An-nual Workshop on Cognitive Modeling and Compu-tational Linguistics (CMCL).R.
Ferna?ndez.
2006.
Non-Sentential Utterances in Di-alogue: Classification, Resolution and Use.
Ph.D.thesis, King?s College London, University of Lon-don.M.
C. Frank and N. D. Goodman.
2012.
Predictingpragmatic reasoning in language games.
Science,336(6084):998?998.J.
Ginzburg.
2012.
The Interactive Stance: Meaningfor Conversation.
Oxford University Press.J.
Hough and M. Purver.
2012.
Processing self-repairsin an incremental type-theoretic dialogue system.
InProceedings of the 16th SemDial Workshop on theSemantics and Pragmatics of Dialogue (SeineDial).J.
Hough and M. Purver.
2013.
Modelling expectationin the self-repair processing of annotat-, um, listen-ers.
In Proceedings of the 17th SemDial Workshopon the Semantics and Pragmatics of Dialogue (Di-alDam).R.
Kempson, W. Meyer-Viol, and D. Gabbay.
2001.Dynamic Syntax: The Flow of Language Under-standing.
Blackwell.K.
H. Knuth.
2005.
Lattice duality: The origin of prob-ability and entropy.
Neurocomputing, 67:245?274.E.
Krahmer and K. Van Deemter.
2012.
Computa-tional generation of referring expressions: A survey.Computational Linguistics, 38(1):173?218.S.
Larsson.
2002.
Issue-based Dialogue Management.Ph.D.
thesis, Go?teborg University.
Also publishedas Gothenburg Monographs in Linguistics 21.S.
Larsson.
2010.
Accommodating innovative mean-ing in dialogue.
Proc.
of Londial, SemDial Work-shop, pages 83?90.S.
Larsson.
2011.
The TTR perceptron: Dynamicperceptual meanings and semantic coordination.
InProceedings of the 15th Workshop on the Semanticsand Pragmatics of Dialogue (SemDial 2011 - LosAngelogue).W.
Levelt.
1989.
Speaking: From Intention to Articu-lation.
MIT Press.R.
Montague.
1974.
Formal Philosophy: Selected Pa-pers of Richard Montague.
Yale University Press.M.
Purver, A. Eshghi, and J. Hough.
2011.
Incremen-tal semantic construction in a dialogue system.
InJ.
Bos and S. Pulman, editors, Proceedings of the9th International Conference on Computational Se-mantics.Y.
Sato.
2011.
Local ambiguity, search strate-gies and parsing in Dynamic Syntax.
In E. Gre-goromichelaki, R. Kempson, and C. Howes, editors,The Dynamics of Lexical Interfaces.
CSLI Publica-tions.D.
Schlangen and G. Skantze.
2009.
A general, ab-stract model of incremental dialogue processing.
InProceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009).E.
Shriberg and A. Stolcke.
1998.
How far do speakersback up in repairs?
A quantitative model.
In Pro-ceedings of the International Conference on SpokenLanguage Processing.G.
Skantze and A. Hjalmarsson.
2010.
Towards incre-mental speech generation in dialogue systems.
InProceedings of the SIGDIAL 2010 Conference.J.
Williams and S. Young.
2007.
Scaling POMDPsfor spoken dialog management.
IEEE Transac-tions on Audio, Speech, and Language Processing,15(7):2116?2129.88
