Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761?769,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDiscriminative Learning with Natural Annotations:Word Segmentation as a Case StudyWenbin Jiang 1 Meng Sun 1 Yajuan Lu?
1 Yating Yang 2 Qun Liu 3, 11Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences{jiangwenbin, sunmeng, lvyajuan}@ict.ac.cn2Multilingual Information Technology Research CenterThe Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciencesyangyt@ms.xjb.ac.cn3Centre for Next Generation LocalisationFaculty of Engineering and Computing, Dublin City Universityqliu@computing.dcu.ieAbstractStructural information in web text pro-vides natural annotations for NLP prob-lems such as word segmentation and pars-ing.
In this paper we propose a discrim-inative learning algorithm to take advan-tage of the linguistic knowledge in largeamounts of natural annotations on the In-ternet.
It utilizes the Internet as an externalcorpus with massive (although slight andsparse) natural annotations, and enables aclassifier to evolve on the large-scaled andreal-time updated web text.
With Chineseword segmentation as a case study, exper-iments show that the segmenter enhancedwith the Chinese wikipedia achieves sig-nificant improvement on a series of testingsets from different domains, even with asingle classifier and local features.1 IntroductionProblems related to information retrieval, machinetranslation and social computing need fast and ac-curate text processing, for example, word segmen-tation and parsing.
Taking Chinese word seg-mentation for example, the state-of-the-art mod-els (Xue and Shen, 2003; Ng and Low, 2004;Gao et al, 2005; Nakagawa and Uchimoto, 2007;Zhao and Kit, 2008; Jiang et al, 2009; Zhang andClark, 2010; Sun, 2011b; Li, 2011) are usuallytrained on human-annotated corpora such as thePenn Chinese Treebank (CTB) (Xue et al, 2005),and perform quite well on corresponding test sets.Since the text used for corpus annotating are usu-ally drawn from specific fields (e.g.
newswire orfinance), and the annotated corpora are limited inthink that NLP                  has already ...n ?
?
?
?
?
?
?
?
?
?
ni-1 i j j+1(a) Natural annotation by hyperlinkn ?
?
?
?
?
?
?
?
?
?
ni-1 i j j+1n ?
?
?
?
?
?
?
?
?
?
ni-1 i j j+1(b) Knowledge for word segmentation(c) Knowledge for dependency parsingFigure 1: Natural annotations for word segmenta-tion and dependency parsing.size (e.g.
tens of thousands), the performance ofword segmentation tends to degrade sharply whenapplied to new domains.Internet provides large amounts of raw text, andstatistics collected from it have been used to im-prove parsing performance (Nakov and Hearst,2005; Pitler et al, 2010; Bansal and Klein, 2011;Zhou et al, 2011).
The Internet alo gives mas-sive (although slight and sparse) natural annota-tions in the forms of structural information includ-ing hyperlinks, fonts, colors and layouts (Sun,2011a).
These annotations usually imply valuableknowledge for problems such as word segmen-tation and parsing, based on the hypothesis thatthe subsequences marked by structural informa-tion are meaningful fragments in sentences.
Fig-ure 1 shows an example.
The hyperlink indicates761a Chinese phrase (meaning NLP), and it probablycorresponds to a connected sub-graph for depen-dency parsing.
Creators of web text give valuableannotations during editing, the whole Internet canbe treated as a wide-coveraged and real-time up-dated corpus.Different from the dense and accurate annota-tions in human-annotated corpora, natural annota-tions in web text are sparse and slight, it makesdirect training of NLP models impracticable.
Inthis work we take for example a most importantproblem, word segmentation, and propose a noveldiscriminative learning algorithm to leverage theknowledge in massive natural annotations of webtext.
Character classification models for word seg-mentation usually factorize the whole predictioninto atomic predictions on characters (Xue andShen, 2003; Ng and Low, 2004).
Natural anno-tations in web text can be used to get rid of im-plausible predication candidates for related char-acters, knowledge in the natural annotations istherefore introduced in the manner of searchingspace pruning.
Since constraint decoding in thepruned searching space integrates the knowledgeof the baseline model and natural annotations, itgives predictions not worse than the normal decod-ing does.
Annotation differences between the out-puts of constraint decoding and normal decodingare used to train the enhanced classifier.
This strat-egy makes the usage of natural annotations simpleand universal, which facilitates the utilization ofmassive web text and the extension to other NLPproblems.Although there are lots of choices, we choosethe Chinese wikipedia as the knowledge sourcedue to its high quality.
Structural information, in-cluding hyperlinks, fonts and colors are used to de-termine the boundaries of meaningful fragments.Experimental results show that, the knowledge im-plied in the natural annotations can significantlyimprove the performance of a baseline segmentertrained on CTB 5.0, an F-measure increment of0.93 points on CTB test set, and an average incre-ment of 1.53 points on 7 other domains.
It is an ef-fective and inexpensive strategy to build word seg-menters adaptive to different domains.
We hope toextend this strategy to other NLP problems suchas named entity recognition and parsing.In the rest of the paper, we first briefly intro-duce the problems of Chinese word segmentationand the character classification model in sectionType Templates Instancesn-gram C?2 C?2=@C?1 C?1=?C0 C0=gC1 C1=,C2 C2=?C?2C?1 C?2C?1=@?C?1C0 C?1C0=?gC0C1 C0C1=g,C1C2 C1C2=,?C?1C1 C?1C1=?,function Pu(C0) Pu(C0)=falseT (C?2:2) T (C?2:2)= 44444Table 1: Feature templates and instances forcharacter classification-based word segmentationmodel.
Suppose we are considering the i-th char-acter ?g?
in ?...@?
g ,???n??...
?.2, then describe the representation of the knowl-edge in natural annotations of web text in section3, and finally detail the strategy of discriminativelearning on natural annotations in section 4.
Af-ter giving the experimental results and analysis insection 5, we briefly introduce the previous relatedwork and then give the conclusion and the expec-tation of future research.2 Character Classification ModelCharacter classification models for word segmen-tation factorize the whole prediction into atomicpredictions on single characters (Xue and Shen,2003; Ng and Low, 2004).
Although natural anno-tations in web text do not directly support the dis-criminative training of segmentation models, theydo get rid of the implausible candidates for predic-tions of related characters.Given a sentence as a sequence of n charac-ters, word segmentation splits the sequence intom(?
n) subsequences, each of which indicates ameaningful word.
Word segmentation can be for-malized as a character classification problem (Xueand Shen, 2003), where each character in the sen-tence is given a boundary tag representing its posi-tion in a word.
We adopt the boundary tags of Ngand Low (2004), b, m, e and s, where b, m ande mean the beginning, the middle and the end of aword, and s indicates a single-character word.
thedecoding procedure searches for the labeled char-acter sequence y that maximizes the score func-762Algorithm 1 Perceptron training algorithm.1: Input: Training corpus C2: ~??
03: for t?
1 .. T do ?
T iterations4: for (x, y?)
?
C do5: y ?
argmaxy ?
(x, y) ?
~?6: if y 6= y?
then7: ~??
~?+?
(x, y?)?
?
(x, y)8: Output: Parameters ~?tion:f(x) = argmaxyS(y|~?,?, x)= argmaxy?
(x, y) ?
~?= argmaxy?(i,t)?y?
(i, t, x, y) ?
~?
(1)The score of the whole sequence y is accumulatedacross all its character-label pairs, (i, t) ?
y (s.t.1 ?
i ?
n and t ?
{b,m, e, s}).
The featurefunction ?
maps a labeled sequence or a character-label pair into a feature vector, ~?
is the parame-ter vector and ?
(x, y) ?
~?
is the inner product of?
(x, y) and ~?.Analogous to other sequence labeling prob-lems, word segmentation can be solved through aviterbi-style decoding procedure.
We omit the de-coding algorithm in this paper due to its simplicityand popularity.The feature templates for the classifier is shownin Table 1.
C0 denotes the current character, whileC?k/Ck denote the kth character to the left/rightof C0.
The function Pu(?)
returns true for a punc-tuation character and false for others, the functionT (?)
classifies a character into four types, 1, 2, 3and 4, representing number, date, English letterand others, respectively.The classifier can be trained with online learn-ing algorithms such as perceptron, or offline learn-ing models such as support vector machines.We choose the perceptron algorithm (Collins,2002) to train the classifier for the characterclassification-based word segmentation model.
Itlearns a discriminative model mapping from theinputs x ?
X to the outputs y?
?
Y , where X is theset of sentences in the training corpus and Y is theset of corresponding labeled results.
Algorithm 1shows the perceptron algorithm for tuning the pa-rameter ~?.
The ?averaged parameters?
technology(Collins, 2002) is used for better performance.n ?
?
?
?
?
?
?
?
?
?
ni-1 i j j+1(a) Original searching spacennnnnnnnbmesbmesbmesbmesbmesbmesbmesbmesbmesbmesn ?
?
?
?
?
?
?
?
?
?
ni-1 i j j+1(b) Shrinked searching spacennnnnnnnbmesesbsbmesbmesbmesbmesesbsbmesFigure 2: Shrink of searching space for the charac-ter classification-based word segmentation model.3 Knowledge in Natural AnnotationsWeb text gives massive natural annotations in theform of structural informations, including hyper-links, fonts, colors and layouts (Sun, 2011a).
Al-though slight and sparse, these annotations implyvaluable knowledge for problems such as wordsegmentation and parsing.As shown in Figure 1, the subsequence P =i..j of sentence S is composed of bolded charac-ters determined by a hyperlink.
Such natural anno-tations do not clearly give each character a bound-ary tag, or define the head-modifier relationshipbetween two words.
However, they do help toshrink the set of plausible predication candidatesfor each character or word.
For word segmenta-tion, it implies that characters i ?
1 and j are therightmost characters of words, while characters iand j + 1 are the leftmost characters of words.For i ?
1 or j, the plausible predication set ?
be-comes {e, s}; For i and j + 1, it becomes {b, s};For other characters c except the two at sentenceboundaries, ?
(c) is still {b,m, e, s}.
For depen-dency parsing, the subsequence P tends to forma connected dependency graph if it contains morethan one word.
Here we use ?
to denote the set ofplausible head of a word (modifier).
There mustbe a single word w ?
P as the root of subse-quence P , whose plausible heads fall out of P ,that is, ?
(w) = {x|x ?
S ?
P}.
For the wordsin P except the root, the plausible heads for each763Algorithm 2 Perceptron learning with natural an-notations.1: ~??
TRAIN(C)2: for x ?
F do3: y ?
DECODE(x, ~?
)4: y?
?
CONSTRAINTDECODE(x, ~?,?
)5: if y 6= y?
then6: C?
?
C?
?
{y?
}7: ~??
TRAIN(C ?
C?
)word w are the words in P except w itself, that is,?
(w) = {x|x ?
P ?
{w}}.Creators of web text give valuable structuralannotations during editing, these annotations re-duce the predication uncertainty for atomic char-acters or words, although not exactly definingwhich predication is.
Figure 2 shows an exam-ple for word segmentation, depicting the shrinkof searching space for the character classification-based model.
Since the decrement of uncertaintyindicates the increment of knowledge, the wholeInternet can be treated as a wide-coveraged andreal-time updated corpus.
We choose the Chinesewikipedia as the external knowledge source, andstructural information including hyperlinks, fontsand colors are used in the current work due to theirexplicitness of representation.4 Learning with Natural AnnotationsDifferent from the dense and accurate annotationsin human-annotated corpora, natural annotationsare sparse and slight, which makes direct trainingof NLP models impracticable.
Annotations im-plied by structural information do not give an ex-act predication to a character, however, they helpto get rid of the implausible predication candidatesfor related characters, as described in the previoussection.Previous work on constituency parsing or ma-chine translation usually resort to some kinds ofheuristic tricks, such as punctuation restrictions,to eliminate some implausible candidates duringdecoding.
Here the natural annotations also bringknowledge in the manner of searching space prun-ing.
Conditioned on the completeness of the de-coding algorithm, a model trained on an exist-ing corpus probably gives better or at least notworse predications, by constraint decoding in thepruned searching space.
The constraint decodingprocedure integrates the knowledge of the baselineAlgorithm 3 Online version of perceptron learn-ing with natural annotations.1: ~??
TRAIN(C)2: for x with natural annotations do3: y ?
DECODE(x, ~?
)4: y?
?
CONSTRAINTDECODE(x, ~?,?
)5: if y 6= y?
then6: ~??
~?
+?
(x, y?)??
(x, y)7: output ~?
at regular timemodel and natural annotations, the predication dif-ferences between the outputs of constraint decod-ing and normal decoding can be used to train theenhanced classifier.Restrictions of the searching space according tonatural annotations can be easily incorporated intothe decoder.
If the completeness of the searchingalgorithm can be guaranteed, the constraint decod-ing in the pruned searching space will give predi-cations not worse than those given by the normaldecoding.
If a predication of constraint decodingdiffers from that of normal decoding, it indicatesthat the annotation precision is higher than the lat-ter.
Furthermore, the degree of difference betweenthe two predications represents the amount of newknowledge introduced by the natural annotationsover the baseline.The baseline model ~?
is trained on an exist-ing human-annotated corpus.
A set of sentencesF with natural annotations are extracted from theChinese wikipedia, and we reserve the ones forwhich constraint decoding and normal decodinggive different predications.
The predictions of re-served sentences by constraint decoding are usedas additional training data for the enhanced classi-fier.
The overall training pipeline is analogous toself-training (McClosky et al, 2006), Algorithm2 shows the pseudo-codes.
Considering the onlinecharacteristic of the perceptron algorithm, if weare able to leverage much more (than the Chinesewikipedia) data with natural annotations, an onlineversion of learning procedure shown in Algorithm3 would be a better choice.
The technology of ?av-eraged parameters?
(Collins, 2002) is easily to beadapted here for better performance.When constraint decoding and normal decod-ing give different predications, we only know thatthe former is probably better than the latter.
Al-though there is no explicit evidence for us to mea-sure how much difference in accuracy between the764Partition Sections # of wordCTBTraining 1?
270 0.47M400 ?
9311001 ?
1151Developing 301 ?
325 6.66KTesting 271 ?
300 7.82KTable 2: Data partitioning for CTB 5.0.two predications, we can approximate how muchnew knowledge that a naturally annotated sentencebrings.
For a sentence x, given the predications ofconstraint decoding and normal decoding, y?
andy, the difference of their scores ?
= S(y) ?
S(y?
)indicates the degree to which the current modelmistakes.
This indicator helps us to select morevaluable training examples.The strategy of learning with natural annota-tions can be adapted to other situations.
For ex-ample, if we have a list of words or phrases (espe-cially in a specific domain such as medicine andchemical), we can generate annotated sentencesautomatically by string matching in a large amountof raw text.
It probably provides a simple andeffective domain adaptation strategy for alreadytrained models.5 ExperimentsWe use the Penn Chinese Treebank 5.0 (CTB)(Xue et al, 2005) as the existing annotated cor-pus for Chinese word segmentation.
For conve-nient of comparison with other work in word seg-mentation, the whole corpus is split into three par-titions as follows: chapters 271-300 for testing,chapters 301-325 for developing, and others fortraining.
We choose the Chinese wikipedia 1 (ver-sion 20120812) as the external knowledge source,because it has high quality in contents and it ismuch better than usual web text.
Structural infor-mations, including hyperlinks, fonts and colors areused to derive the annotation information.To further evaluate the improvement broughtby the fuzzy knowledge in Chinese wikipedia, aseries of testing sets from different domains areadopted.
The four testing sets from SIGHANBakeoff 2010 (Zhao and Liu, 2010) are used, theyare drawn from the domains of literature, finance,computer science and medicine.
Although the ref-erence sets are annotated according to a different1http://download.wikimedia.org/backup-index.html.95.695.89696.296.496.696.89797.297.41  2  3  4  5  6  7  8  9  10Accuracy(F1%)Training iterationsFigure 3: Learning curve of the averaged percep-tron classifier on the CTB developing set.word segmentation standard (Yu et al, 2001), thequantity of accuracy improvement is still illustra-tive since there are no vast diversities between thetwo segmentation standards.
We also annotatedanother three testing sets 2, their texts are drawnfrom the domains of chemistry, physics and ma-chinery, and each contains 500 sentences.5.1 Baseline Classifier for WordSegmentationWe train the baseline perceptron classifier forword segmentation on the training set of CTB5.0, using the developing set to determine thebest training iterations.
The performance mea-surement for word segmentation is balanced F-measure, F = 2PR/(P +R), a function of preci-sion P and recall R, where P is the percentage ofwords in segmentation results that are segmentedcorrectly, and R is the percentage of correctly seg-mented words in the gold standard words.Figure 3 shows the learning curve of the aver-aged perceptron on the developing set.
The sec-ond column of Table 3 lists the performance ofthe baseline classifier on eight testing sets, wherenewswire denotes the testing set of the CTB it-self.
The classifier performs much worse on thedomains of chemistry, physics and machinery, itindicates the importance of domain adaptation forword segmentation (Gao et al, 2004; Ma andWay, 2009; Gao et al, 2010).
The accuracy on thetesting sets from SIGHAN Bakeoff 2010 is evenlower due to the difference in both domains andword segmentation standards.2They are available at http://nlp.ict.ac.cn/ jiangwenbin/.765Dataset Baseline (F%) Enhanced (F%)Newswire 97.35 98.28 +0.93Out-of-DomainChemistry 93.61 95.68 +2.07Physics 95.10 97.24 +2.14Machinery 96.08 97.66 +1.58Literature 92.42 93.53 +1.11Finance 92.50 93.16 +0.66Computer 89.46 91.19 +1.73Medicine 91.88 93.34 +1.46Average 93.01 94.54 +1.53Table 3: Performance of the baseline classifier andthe classifier enhanced with natural annotations inChinese wikipedia.5.2 Classifier Enhanced with NaturalAnnotationsThe Chinese wikipedia contains about 0.5 millionitems.
From their description text, about 3.9 mil-lions of sentences with natural annotations are ex-tracted.
With the CTB training set as the exist-ing corpus C, about 0.8 million sentences are re-served according to Algorithm 2, the segmenta-tions given by constraint decoding are used as ad-ditional training data for the enhanced classifier.According to the previous description, the dif-ference of the scores of constraint decoding andnormal decoding, ?
= S(y) ?
S(y?
), indicatesthe importance of a constraint segmentation to theimprovement of the baseline classifier.
The con-straint segmentations of the reserved sentences aresorted in descending order according to the dif-ference of the scores of constraint decoding andnormal decoding, as described previously.
Fromthe beginning of the sorted list, different amountsof segmented sentences are used as the additionaltraining data for the enhanced character classifier.Figure 4 shows the performance curve of the en-hanced classifiers on the developing set of CTB.We found that the highest accuracy was achievedwhen 160, 000 sentences were used, while moreadditional training data did not give continuousimprovement.
A recent related work about self-training for segmentation (Liu and Zhang, 2012)also reported a very similar trend, that only a mod-erate amount of raw data gave the most obviousimprovements.The performance of the enhanced classifier islisted in the third column of Table 3.
On theCTB testing set, training data from the Chinese97.197.297.397.497.597.697.797.8Accuracy(F1%)Count of selected sentences10000 20000 40000 80000 160000 320000 640000using selected sentencesusing all sentencesFigure 4: Performance curve of the classifier en-hanced with selected sentences of different scales.Model Accuracy (F%)(Jiang et al, 2008) 97.85(Kruengkrai et al, 2009) 97.87(Zhang and Clark, 2010) 97.79(Wang et al, 2011) 98.11(Sun, 2011b) 98.17Our Work 98.28Table 4: Comparison with state-of-the-art work inChinese word segmentation.wikipedia brings an F-measure increment of 0.93points.
On out-of-domain testing sets, the im-provements are much larger, an average incrementof 1.53 points is achieved on seven domains.
Itis probably because the distribution of the knowl-edge in the CTB training data is concentrated inthe domain of newswire, while the contents ofthe Chinese wikipedia cover a broad range of do-mains, it provides knowledge complementary tothat of CTB.Table 4 shows the comparison with otherwork in Chinese word segmentation.
Our modelachieves an accuracy higher than that of thestate-of-the-art models trained on CTB only, al-though using a single classifier with only localfeatures.
From the viewpoint of resource uti-lization, the comparison between our system andprevious work without using additional trainingdata is unfair.
However, we believe this workshows another interesting way to improve Chi-nese word segmentation, it focuses on the utiliza-tion of fuzzy and sparse knowledge on the Internetrather than making full use of a specific human-annotated corpus.
On the other hand, since onlya single classifier and local features are used inour method, better performance could be achieved766resorting to complicated features, system com-bination and other semi-supervised technologies.What is more, since the text on Internet is wide-coveraged and real-time updated, our strategy alsohelps a word segmenter be more domain adaptiveand up to date.6 Related WorkLi and Sun (2009) extracted character classifi-cation instances from raw text for Chinese wordsegmentation, resorting to the indication of punc-tuation marks between characters.
Sun and Xu(Sun and Xu, 2011) utilized the features derivedfrom large-scaled unlabeled text to improve Chi-nese word segmentation.
Although the two workalso made use of large-scaled raw text, our methodis essentially different from theirs in the aspectsof both the source of knowledge and the learningstrategy.Lots of efforts have been devoted to semi-supervised methods in sequence labeling and wordsegmentation (Xu et al, 2008; Suzuki and Isozaki,2008; Haffari and Sarkar, 2008; Tomanek andHahn, 2009; Wang et al, 2011).
A semi-supervised method tries to find an optimal hyper-plane of both annotated data and raw data, thus toresult in a model with better coverage and higheraccuracy.
Researchers have also investigated un-supervised methods in word segmentation (Zhaoand Kit, 2008; Johnson and Goldwater, 2009;Mochihashi et al, 2009; Hewlett and Cohen,2011).
An unsupervised method mines the latentdistribution regularity in the raw text, and auto-matically induces word segmentation knowledgefrom it.
Our method also needs large amounts ofexternal data, but it aims to leverage the knowl-edge in the fuzzy and sparse annotations.
It isfundamentally different from semi-supervised andunsupervised methods in that we aimed to exca-vate a totally different kind of knowledge, the nat-ural annotations implied by the structural informa-tion in web text.In recent years, much work has been devoted tothe improvement of word segmentation in a vari-ety of ways.
Typical approaches include the in-troduction of global training or complicated fea-tures (Zhang and Clark, 2007; Zhang and Clark,2010), the investigation of word internal structures(Zhao, 2009; Li, 2011), the adjustment or adapta-tion of word segmentation standards (Wu, 2003;Gao et al, 2004; Jiang et al, 2009), the integratedsolution of segmentation and related tasks such aspart-of-speech tagging and parsing (Zhou and Su,2003; Zhang et al, 2003; Fung et al, 2004; Gold-berg and Tsarfaty, 2008), and the strategies of hy-brid or stacked modeling (Nakagawa and Uchi-moto, 2007; Kruengkrai et al, 2009; Wang et al,2010; Sun, 2011b).In parsing, Pereira and Schabes (1992) pro-posed an extended inside-outside algorithm thatinfers the parameters of a stochastic CFG from apartially parsed treebank.
It uses partial bracket-ing information to improve parsing performance,but it is specific to constituency parsing, and itscomputational complexity makes it impractical formassive natural annotations in web text.
Thereare also work making use of word co-occurrencestatistics collected in raw text or Internet n-gramsto improve parsing performance (Nakov andHearst, 2005; Pitler et al, 2010; Zhou et al, 2011;Bansal and Klein, 2011).
When enriching the re-lated work during writing, we found a work on de-pendency parsing (Spitkovsky et al, 2010) whoutilized parsing constraints derived from hypertextannotations to improve the unsupervised depen-dency grammar induction.
Compared with theirmethod, the strategy we proposed is formal anduniversal, the discriminative learning strategy andthe quantitative measurement of fuzzy knowledgeenable more effective utilization of the natural an-notation on the Internet when adapted to parsing.7 Conclusion and Future WorkThis work presents a novel discriminative learningalgorithm to utilize the knowledge in the massivenatural annotations on the Internet.
Natural anno-tations implied by structural information are usedto decrease the searching space of the classifier,then the constraint decoding in the pruned search-ing space gives predictions not worse than the nor-mal decoding does.
Annotation differences be-tween the outputs of constraint decoding and nor-mal decoding are used to train the enhanced classi-fier, linguistic knowledge in the human-annotatedcorpus and the natural annotations of web textare thus integrated together.
Experiments on Chi-nese word segmentation show that, the enhancedword segmenter achieves significant improvementon testing sets of different domains, although us-ing a single classifier with only local features.Since the contents of web text cover a broadrange of domains, it provides knowledge comple-767mentary to that of human-annotated corpora withconcentrated distribution of domains.
The contenton the Internet is large-scaled and real-time up-dated, it compensates for the drawback of expen-sive building and updating of corpora.
Our strat-egy, therefore, enables us to build a classifier moredomain adaptive and up to date.
In the future, wewill compare this method with self-training to bet-ter illustrate the importance of boundary informa-tion, and give error analysis on what types of er-rors are reduced by the method to make this inves-tigation more complete.
We will also investigatemore efficient algorithms to leverage more mas-sive web text with natural annotations, and furtherextend the strategy to other NLP problems such asnamed entity recognition and parsing.AcknowledgmentsThe authors were supported by NationalNatural Science Foundation of China (Con-tracts 61202216), 863 State Key Project (No.2011AA01A207), and National Key TechnologyR&D Program (No.
2012BAH39B03).
Qun Liu?swork was partially supported by Science Foun-dation Ireland (Grant No.07/CE/I1142) as partof the CNGL at Dublin City University.
Sincerethanks to the three anonymous reviewers for theirthorough reviewing and valuable suggestions!ReferencesMohit Bansal and Dan Klein.
2011.
Web-scale fea-tures for full-scale parsing.
In Proceedings of ACL.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP, pages 1?8, Philadelphia, USA.Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-feng Chen.
2004.
A maximum-entropy chineseparser augmented by transformation-based learning.In Proceedings of TALIP.Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin.
2004.Adaptive chinese word segmentation.
In Proceed-ings of ACL.Jianfeng Gao, Mu Li, Andi Wu, and Chang-NingHuang.
2005.
Chinese word segmentation andnamed entity recognition: A pragmatic approach.Computational Linguistics.Wenjun Gao, Xipeng Qiu, and Xuanjing Huang.
2010.Adaptive chinese word segmentation with onlinepassive-aggressive algorithm.
In Proceedings ofCIPS-SIGHAN Workshop.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gen-erative model for joint morphological segmentationand syntactic parsing.
In Proceedings of ACL-HLT.Gholamreza Haffari and Anoop Sarkar.
2008.Homotopy-based semi-supervised hidden markovmodels for sequence labeling.
In Proceedings ofCOLING.Daniel Hewlett and Paul Cohen.
2011.
Fully unsu-pervised word segmentation with bve and mdl.
InProceedings of ACL.Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.2008.
A cascaded linear model for joint chineseword segmentation and part-of-speech tagging.
InProceedings of ACL.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and pos tagging?a case study.
InProceedings of the 47th ACL.Mark Johnson and Sharon Goldwater.
2009.
Improv-ing nonparameteric bayesian inference: experimentson unsupervised word segmentation with adaptorgrammars.
In Proceedings of NAACL.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun.ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hy-brid model for joint chinese word segmentation andpos tagging.
In Proceedings of ACL-IJCNLP.Zhongguo Li and Maosong Sun.
2009.
Punctuation asimplicit annotations for chinese word segmentation.Computational Linguistics.Zhongguo Li.
2011.
Parsing the internal structure ofwords: A new paradigm for chinese word segmenta-tion.
In Proceedings of ACL.Yang Liu and Yue Zhang.
2012.
Unsupervised domainadaptation for joint segmentation and pos-tagging.In Proceedings of COLING.Yanjun Ma and Andy Way.
2009.
Bilingually moti-vated domain-adapted word segmentation for statis-tical machine translation.
In Proceedings of EACL.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the HLT-NAACL.Daichi Mochihashi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian unsupervised word segmen-tation with nested pitman-yor language modeling.In Proceedings of ACL-IJCNLP.Tetsuji Nakagawa and Kiyotaka Uchimoto.
2007.
Ahybrid approach to word segmentation and pos tag-ging.
In Proceedings of ACL.Preslav Nakov and Marti Hearst.
2005.
Using theweb as an implicit training set: Application to struc-tural ambiguity resolution.
In Proceedings of HLT-EMNLP.768Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Proceedings ofEMNLP.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed cor-pora.
In Proceedings of ACL.Emily Pitler, Shane Bergsma, Dekang Lin, and Ken-neth Church.
2010.
Using web-scale n-grams toimprove base np parsing performance.
In Proceed-ings of COLING.Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-shawi.
2010.
Profiting from mark-up: Hyper-textannotations for guided parsing.
In Proceedings ofACL.Weiwei Sun and Jia Xu.
2011.
Enhancing chineseword segmentation using unlabeled data.
In Pro-ceedings of EMNLP.Maosong Sun.
2011a.
Natural language processingbased on naturally annotated web resources.
CHI-NESE INFORMATION PROCESSING.Weiwei Sun.
2011b.
A stacked sub-word model forjoint chinese word segmentation and part-of-speechtagging.
In Proceedings of ACL.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In Proceedings of ACL.Katrin Tomanek and Udo Hahn.
2009.
Semi-supervised active learning for sequence labeling.
InProceedings of ACL.Kun Wang, Chengqing Zong, and Keh-Yih Su.
2010.A character-based joint model for chinese word seg-mentation.
In Proceedings of COLING.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Tori-sawa.
2011.
Improving chinese word segmentationand pos tagging with semi-supervised methods us-ing large auto-analyzed data.
In Proceedings of IJC-NLP.Andi Wu.
2003.
Customizable segmentation of mor-phologically derived words in chinese.
Computa-tional Linguistics and Chinese Language Process-ing.Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-mann Ney.
2008.
Bayesian semi-supervised chineseword segmentation for statistical machine transla-tion.
In Proceedings of COLING.Nianwen Xue and Libin Shen.
2003.
Chinese wordsegmentation as lmr tagging.
In Proceedings ofSIGHAN Workshop.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In NaturalLanguage Engineering.Shiwen Yu, Jianming Lu, Xuefeng Zhu, HuimingDuan, Shiyong Kang, Honglin Sun, Hui Wang,Qiang Zhao, and Weidong Zhan.
2001.
Processingnorms of modern chinese corpus.
Technical report.Yue Zhang and Stephen Clark.
2007.
Chinese seg-mentation with a word-based perceptron algorithm.In Proceedings of ACL 2007.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and pos-tagging usinga single discriminative model.
In Proceedings ofEMNLP.Huaping Zhang, Hongkui Yu, Deyi Xiong, and QunLiu.
2003.
Hhmm-based chinese lexical analyzerictclas.
In Proceedings of SIGHAN Workshop.Hai Zhao and Chunyu Kit.
2008.
Unsupervisedsegmentation helps supervised learning of charac-ter tagging for word segmentation and named entityrecognition.
In Proceedings of SIGHAN Workshop.Hongmei Zhao and Qun Liu.
2010.
The cips-sighanclp 2010 chinese word segmentation bakeoff.
InProceedings of CIPS-SIGHAN Workshop.Hai Zhao.
2009.
Character-level dependencies in chi-nese: Usefulness and learning.
In Proceedings ofEACL.Guodong Zhou and Jian Su.
2003.
A chinese effi-cient analyser integrating word segmentation, part-ofspeech tagging, partial parsing and full parsing.
InProceedings of SIGHAN Workshop.Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.2011.
Exploiting web-derived selectional prefer-ence to improve statistical dependency parsing.
InProceedings of ACL.769
