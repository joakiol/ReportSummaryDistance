Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1449?1459,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsKB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations,and FactsDana Movshovitz-AttiasComputer Science DepartmentCarnegie Mellon Universitydma@cs.cmu.eduWilliam W. CohenMachine Learning DepartmentCarnegie Mellon Universitywcohen@cs.cmu.eduAbstractMany existing knowledge bases (KBs), in-cluding Freebase, Yago, and NELL, relyon a fixed ontology, given as an inputto the system, which defines the data tobe cataloged in the KB, i.e., a hierar-chy of categories and relations betweenthem.
The system then extracts facts thatmatch the predefined ontology.
We pro-pose an unsupervised model that jointlylearns a latent ontological structure of aninput corpus, and identifies facts from thecorpus that match the learned structure.Our approach combines mixed member-ship stochastic block models and topicmodels to infer a structure by jointly mod-eling text, a latent concept hierarchy, andlatent semantic relationships among theentities mentioned in the text.
As a casestudy, we apply the model to a corpusof Web documents from the software do-main, and evaluate the accuracy of the var-ious components of the learned ontology.1 IntroductionKnowledge base (KB) construction methods canbe broadly categorized along several dimensions.One dimension is ontology-guided construction,where the list of categories and relations that de-fine the schema of the KB are explicit, versus openIE methods, where they are not.
Another dimen-sion is the type of relations and types included inthe KB: some KBs, like WordNet, are hierarchi-cal, in that they contain mainly concept types, su-pertypes and instances, while other KBs containmany types of relationships between concepts.
Hi-erarchical knowledge can be learned by methodsincluding distributional clustering (Pereira et al,1993), as well as Hearst patterns (Hearst, 1992)and similar techniques (Snow et al, 2006).
Re-verb (Fader et al, 2011) and TextRunner (Yateset al, 2007) are open methods for learning multi-relation KBs.
Finally, NELL (Carlson et al, 2010;Mitchell et al, 2015), FreeBase (Google, 2011)and Yago (Suchanek et al, 2007; Hoffart et al,2013) are ontology-guided methods for extractingKBs containing both hierarchies and relations.One advantage of ontology-guided methods isthat the extracted knowledge is easier to reasonwith.
An advantage of open IE methods is thatontologies may be incomplete, and are expensiveto construct for a new domain.
Ontology designinvolves assembling a set of categories, organizedin a meaningful hierarchical structure, often pro-viding seeds, i.e., representative examples for eachcategory, and finally, defining inter-category rela-tions.
This process is often done manually (Carl-son et al, 2010) leading to a rigid set of categories.Redesigning a new ontology for a specialized do-main represents an additional challenge as it re-quires extensive knowledge of the domain.In this paper, we propose an unsupervisedmodel that learns a latent hierarchical structureof categories from an input corpus, learns latentsemantic relations between categories, and alsoidentifies facts from the corpus that match thelearned structure.
In other words, the model learnsboth the schema for a KB, and a set of facts thatare related to that schema, thus combining theprocesses of KB population and ontology con-struction.
The intent is to build systems that ex-tract facts which can be interpreted relative to ameaningful ontology without requiring the effortof manual ontology construction.The input to the learning method is a cor-pus of documents, plus two sets of resources ex-tracted from the same corpus: a set of hypernym-hyponym pairs (e.g., ?animal?, ?horse?)
extractedusing Hearst patterns, and a set of subject-verb-object triples (e.g., ?horse?, ?eats?, ?hay?)
ex-tracted from parsed sentences.
These resourcesare analogous to the output of open IE systems for1449hierarchies and relations, and as we demonstrate,our method can be used to highlight domain-specific data from open IE repositories.Our approach combines mixed membershipstochastic block models and topic models to in-fer a structure by jointly modeling text docu-ments, and links that indicate hierarchy and rela-tion among the entities mentioned in the text.
Jointmodeling allows information on topics of nouns(referred to as instances) and verbs (referred toas relations) to be shared between text documentsand an ontological structure, resulting in a set ofcompelling topics.
This model offers a completesolution for KB construction based on an inputcorpus, and we therefore name it KB-LDA.We additionally propose a method for recover-ing meaningful names for concepts in the learnedhierarchy.
These are equivalent to category namesin other KBs, however, following our method weextract from the data a set of potential alterna-tive concepts describing each category, includingprobabilities for their strength of association.To show the effectiveness of our method, we ap-ply the model to a dataset of Web based documentsfrom the software domain, and learn a softwareKB.
This is an example of a specialized domain inwhich, to our knowledge, no broad-coverage on-tology exists.
We evaluate the model on the in-duced categories, relations, and facts, and we com-pare the proposed categories with an independentset of human-provided labels for documents.
Fi-nally, we use KB-LDA to retrieve domain-specificrelations from an open IE resource.
We providethe learned software KB as supplemental material.2 KB-LDAModeling latent sets of entities from observed in-teractions among them is a well researched task,often encountered in social network analysis forthe purpose of identifying specialized communi-ties in the network.
Mixed Membership Stochas-tic Blockmodels (Airoldi et al, 2009; Parkkinenet al, 2009) model entities as graph nodes withpairwise relations drawn from latent blocks withmixed membership.
A related approach is takenby topic models such as LDA (Latent DirichletAllocation; (Blei et al, 2003)), which model doc-uments as generated by a mixture of latent topics,and words in the documents as generated by topic-specific word distributions.
The KB-LDA modelcombines the two approaches.
It models links be-piO ?
multinomial over ontology topic pairs, with Dirichletprior ?OpiR ?
multinomial over relation topic tuples, with Dirichletprior ?R?d ?
topic multinomial for document d, with Dirichletprior ?D?k ?
multinomial over instances for topic k, with Dirichletprior ?I?k?
?
multinomial over relations for topic k?, with Dirichletprior ?RCIi= ?Ci, Ii?
?
i-th ontological assignment pairSVOj= ?Sj, Oj, Vj?
?
j-th relation assignment tuplezCIi = ?zCi, zIi?
?
topic pair chosen for example ?Ci, Ii?zSV Oj = ?zSj, zOj, zVj?
?
topic tuple chosen for example?Sj, Oj, Vj?zDE1, zDE2?
topic chosen for instance entityE1, or relationentity E2, respectively, in a documentnIz,i ?
number of times instance i is observed under topicz (in either zD, zCIor zSV O)nRz,r ?
number of times relation r is observed under topicz (in either zDor zSV O)nO?zc,zi?
?
count of ontological pairs assigned the topicpair ?zc, zi?
(in zCI)nR?zs,zo,zv?
?
count of relation tuples assigned the topictuple ?zs, zo, zv?
(in zSV O)Table 1: KB-LDA notation.tween tuples of two or three entities using stochas-tic block models, and these are additionally influ-enced by latent topic assignments of the entities ina document corpus.In the KB-LDA model, shown as a plate dia-gram in Figure 1 with notation in Table 1, informa-tion is shared between three components, throughcommon latent topics over noun and verb entities.The Ontology component (upper right) modelshierarchical links between Concept-Instance (CI)entity pairs.
The Relations component (left) mod-els links between Subject-Verb-Object (SVO) en-tity triples, where the subject and object are nounsand the verb represents a relation between them.Finally, the Documents component (lower left) isa link-LDA model (Erosheva et al, 2004) of textdocuments containing a combination of noun andverb entity types.
In this formulation, distribu-tions over noun and verb entities that are relatedaccording to hierarchical or relational constraints,are linked with a text model via shared parameters.In more detail, the Documents component pro-vides the context in which noun and verb entitiesare being used in text.
It is modeled as an exten-sion of LDA, viewing documents as sets of ?bagsof words?, where in this case, each bag containseither noun or verb entities.
Each entity type has atopic-wise multinomial distribution over the set ofentities in the vocabulary of that type.1450SjOjVjzSjzOjzVj?RpiRNRRelations?I?kK?R?k?K?OpiOzCizIiCiIiNOOntology?D?dzEl1zEl2El1El2Nd,INd,RNDDocumentsFigure 1: Plate Diagram of KB-LDA.The Ontology component is a generative modelrepresenting hierarchal links between pairs ofnouns.
The examples for this component are ex-tracted using a small collection of Hearst patternsindicating concept-instance or concept-conceptlinks, including, ?X such as Y?, and ?X includingY?.
For example, the sentence ?websites such asStackOverflow?
indicates that Stackoverflow is atype of website, leading to the extracted noun pair?websites, StackOverflow?.
We refer to the exam-ples extracted using these hierarchical patterns asconcept-instance pairs, and to the individual enti-ties as instances.The pairs have an underlying block structurederived from a sparse block model (Parkkinen etal., 2009).
They are generated by topic specificinstance distributions conditioned on topic pairedges, which are defined by the multinomial piOover the Cartesian product of the noun topic setwith itself.
The individual instances, therefore,have a mixed membership in topics.
Note thatwe allow for a concept and instance to be drawnfrom different noun topics, defined by ?.
For ex-ample, we may learn a topic highlighting concepttokens like ?websites?, ?platforms?, ?applications?.Another topic can highlight instances shared bythese concepts, such as, ?stackoverflow?, ?google?,and ?facebook?.
Finally, the observation that theformer topic frequently contains concepts of in-stances from the latter topic, is encoded in themultinomial distribution piO.
From this we inferthat the former topic should be placed higher inthe induced hierarchy.Similarly, the Relations component representsrelational links between a noun subject, a verb anda noun object.
The examples for this componentLet K be the number of target latent topics.1.
Generate topics: For topic k?1, .
.
.
,K, sample:?
?k?Dirichlet(?I), the per-topic instance distribution?
?k?Dirichlet(?R), the per-topic relation distribution2.
Generate ontology: Sample piO?Dirichlet(?O), theinstance topic pair distribution.?
For each concept-instance pair CIi, i?1, .
.
.
, NO:?
Sample topic pair zCIi?Multinomial(piO)?
Sample instances Ci?Multinomial(?zCi), Ii?Multinomial(?zIi), then CIi= ?Ci, Ii?3.
Generate relations: Sample piR?Dirichlet(?R), therelation topic tuple distribution.?
For each tuple SVOj, j?1, .
.
.
, NR:?
Sample topic tuple zSV Oj?Multinomial(piR)?
Sample instances, Sj?Multinomial(?zSj), Oj?Multinomial(?zOj), and sample a relation Vj?Multinomial(?zVj)4.
Generate documents: For document d?1, .
.
.
, D:?
Sample ?d?Dirichlet(?D), the topic mixing distri-bution for document d.?
For every noun entity (El1) and verb entity (El2), l1?1, .
.
.
, Nd,I, l2?1, .
.
.
, Nd,R:?
Sample topics zEl1, zEl2?Multinomial(?d)?
Sample entities El1?Multinomial(?zEl1) andEl2?Multinomial(?zEl2)Table 2: KB-LDA generative process.are extracted from SVO patterns found in the doc-ument corpus, following Talukdar et al (2012).An extracted example looks like: ?websites, ex-ecute, javascript?.
Subject and object topics aredrawn from the noun topics (?
), while the verbtopics is drawn from the verb topics, defined by?.
The multinomial piRencodes the interaction ofnoun and verb topics based on the extracted rela-tional links, and it is defined over the Cartesianproduct of the noun topic set with itself and with1451the verb topic set.The generative process of KB-LDA is de-scribed in Table 2.
Given the hyperparameters(?O, ?R, ?D, ?I, ?R), the joint distribution overCI pairs, SVO tuples, documents, topics and topicassignments is given byp(piO, piR, ?, ?,CI, zCI,SVO, zSV O,?,E, zD|?O, ?R, ?D, ?I, ?R) =K?k=1Dir(?k|?I)?K?k?=1Dir(?k?|?R)?
(1)Dir(piO|?O)NO?i=1pi?zCi,zIi?O?CizCi?IizIi?Dir(piR|?R)NR?j=1pi?zSj,zOj,zVj?R?SjzSj?OjzOj?VjzVj?ND?d=1Dir(?d|?D)Nd,I?l1=1?zDEl1d?El1zDEl1Nd,R?l2=1?zDEl2d?El2zDEl22.1 Inference in KB-LDAExact inference is intractable in the KB-LDAmodel.
We use a collapsed Gibbs sampler (Grif-fiths and Steyvers, 2004) to perform approximateinference in order to query the topic distributionsand assignments.
It samples a latent topic pair fora CI pair in the corpus conditioned on the assign-ments to all other CI pairs, SVO tuples, and docu-ment entities, using the following expression, aftercollapsing piO:p?
(zCIi|CIi, zCI?i, zSV O, zD,CI?i, ?O, ?I) (2)?
(nO?izCIi+ ?O)?
(nI?izCi,Ci+ ?I)(nI?izIi,Ii+ ?I)(?CnI?izCi,C+ TI?I)(?InI?izIi,I+ TI?I)where counts of observations from the training setare noted by n (see Table 1), and TIis the numberof instance entities (size of noun vocabulary).We similarly sample topics for each SVO tupleconditioned on the assignments to all other tuples,CI pairs and document entities, using the follow-ing expression, after collapsing piR:p?
(zSV Oj|SVOj, zSV O?j, zCI, zD, SVO?j, ?R, ?I, ?R) (3)?
(nR?jzSVOj+ ?R)?
(nI?jzSj,Sj+ ?I)(nI?jzOj,Oj+ ?I)(nR?jzVj,Vj+ ?R)(?InI?jzSi,I+TI?I)(?InI?jzOi,I+TI?I)(?VnR?jzVj,V+TR?R)We sample a latent topic for an entity mentionin a document from the text corpus conditionedon the assignments to all other entity mentions af-ter collapsing ?d.
The following expression showstopic sampling for a noun entity in a document:p?
(zEl1|E,CI,SVO, zD, zCI, zSV O, ?D, ?I) (4)?
(n?l1d,z+ ?D)nI?l1zEl1,El1+ ?I?E?l1nI?l1zEl1,E?l1+ TI?IThe per-topic multinomial parameters and topicdistributions of CI pairs, SVO tuples and docu-ments can be recovered with MLE estimates usingtheir observation counts:?
?Ik=nIk,I+ ?I?I?nIk,I?+ TI?I,?
?Rk=nRk,R+ ?R?R?nRk,R?+ TR?R?
?zd=nz,d+ ?D?z?nz?,d+K?Dp?i?zC,zI?O=nO?zC,zI?+ ?O?z?C,z?InO?z?C,z?I?+K2?
?Op?i?zS,zO,zV?R=nR?zS,zO,zV?+ ?R?z?S,z?O,z?VnR?z?S,z?O,z?V?+K3?
?RUsing the KB-LDA model we can describe thelatent topic hierarchy underlying the input cor-pus.
We consider the multinomial of the Ontologycomponent, piO, as an adjacency matrix describ-ing a network where the nodes are instance topicsand edges indicate a hypernym-to-hyponym rela-tion.
By extracting the maximum spanning treeover this adjacency matrix, we recover a hierarchyover the input data.
We recover relations amonginstance topics by extracting from the Relationsmultinomial, piR, the set of most probable tuplesof a ?subject topic, verb topic, object topic?.Our model is implemented using a fast, parallelapproximation of collapsed Gibbs sampling, fol-lowing Newman et al (2009).
In each samplingiteration, topics are sampled locally on a subset ofthe training examples.
At the end of each iteration,data from worker threads is joined and model pa-rameters are updated with complete information.In the next iteration, thread-local sampling startswith complete topic assignment information fromthe previous iteration.
In each thread, the processcan be viewed as a reordering of the input exam-ples, where the examples sampled in that thread1452are viewed first.
It has been shown that parallel ap-proaches considerably speed up iterative inferencemethods such as collapsed Gibbs sampling, result-ing in test data log probabilities indistinguishablefrom those obtained using serial methods (Porte-ous et al, 2008; Newman et al, 2009).
A paral-lel approach is especially important when trainingthe KB-LDA model due to the large dimensionsof the multinomials of the Ontology and Relationscomponents (K2andK3, respectively for a modelwith K topics).
We train KB-LDA over 2000 iter-ations, more than what has traditionally been usedfor collapsed Gibbs samplers.2.2 Data-driven discovery of topic conceptsThe KB-LDA model described above clustersnoun entities into sets of instance topics, and re-covers a latent hierarchical structure among thesetopics.
Each instance topic can be described by amultinomial distribution of the underlying nouns.It is often more intuitive, however, to refer to atopic containing a set of high probability nouns bya name, or category, just as traditional ontologiesdescribe hierarchies over categories.Our model is trained over nouns that originatefrom concept-instance example pairs (used to trainthe Ontology component).
We describe a methodfor selecting a category name for a topic, based onconcepts that best represent high probability nounsof the topic in the concept-instance examples.We calculate the probability that a concept nounc describes the set of instances I that have beenassigned the topic z usingp(c, z|I) ?
p(I|c, z) ?
p(c, z) (5)= p(I|c, z) ?
p(z|c) ?
p(c)Let rep(c, z) =?i:Ci=cnIz,Iidescribe how wellconcept c represents topic z according to the as-signments of instances with concept c to the topic.Then,p(z|c) =rep(c, z)?z?rep(c, z?
)(6)The concept prior, p(c), is based on the relativeweight of instances with concept c in the concept-instance example set, and is an indicator of thegenerality of a concept:p(c) =?i:Ci=cwc,Ii?c?
?i:Ci=c?wc?,Ii(7)where wC,Iis the number of occurrences ofconcept-instance pair ?C, I?
in the corpus.Finally, p(I|c, z) measures how specific are thetopic instances to the concept c,p(I|c, z) =?i:Ii?I,Ci=cwc,Ii?i:Ci=cwc,Ii/Z (8)where I is the set of training instances assignedwith topic z, and Z is a normalizer over all con-cepts and topics.Following this method we extract concepts thathave a high probability p(c, z|I) with respect to atopic z.
These can be thought of as equivalent tothe single, fixed, category name provided by tra-ditional KB ontologies; however, here we extractfrom the data a set of potential alternative nounphrases describing each topic, including a proba-bility for the strength of this association.3 Experimental EvaluationWe evaluate the KB-LDA model on a corpusof 5.5M documents from the software domain;thereby we are using the model to construct a soft-ware domain knowledge base.
Our evaluation ex-plores the following questions:?
Can KB-LDA learn categories, relations, ahierarchy and topic concepts with high pre-cision??
How well do KB-LDA topics correspondwith human-provided document labels??
Is KB-LDA useful in extracting facts fromexisting open IE resources?3.1 DataWe use data from the Q&A website StackOver-flow1where users ask and answer technical ques-tions about software development, tools, algo-rithms, etc?.
We extracted 562K concept-instanceexample pairs from the data, and kept the 17K ex-amples appearing at least twice.
Noun phrasesin these examples make up our Instance Dictio-nary.
Out of 6.8M SVO examples found in thedata we keep 37K in which the subject and ob-ject are in the Instance Dictionary, and the exam-ple appears at least twice in the corpus.
The verbsin these SVOs make up our Relation Dictionary.Finally, we consider as documents the 5.5M ques-tions from StackOverflow with all their answers.3.2 Evaluating the learned KB precisionIn this section we evaluate the direct output of amodel trained with 50 topics: the extracted in-1Data source: https://archive.org/details/stackexchange14535 10 15 20 25 30Top Tokens0.600.650.700.750.800.85MatchPrecision5 10 15 20 25 30Top Tokens0.650.700.750.800.850.900.951.00GroupPrecisionFigure 2: Average Match (top) and Group (bot-tom) precision of top tokens of 50 topics learnedwith KB-LDA, according to expert (dark blue) andnon-expert (light blue, stripes) labeling.stance topics, topic hierarchy, relations amongtopics and extracted topic concepts.
In each ofthe experiments below, we extract facts based onone of the learned components and evaluate eachfact based on annotations from human judges: twoexperts and three non-expert users, collected us-ing Mechanical Turk, that were pre-tested on abasic familiarity with concepts from the softwaredomain, such as programming languages, versioncontrol systems, and databases.3.2.1 Precision of Instance TopicsWe measure the coherence of instance topics us-ing an approach called word intrusion (Chang etal., 2009).
We extract the top 30 instance tokensof a topic ranked by the instance topic multinomial?.
We present to workers tokens 1-5,6-10,. .
.
,26-30, where each 5 tokens are randomly ordered andaugmented with an extra token that is ranked lowfor the topic, (the intruder).
We ask workers toselect all tokens that do not belong in the group(and at least one).
We define the topic Match Pre-cision as the fraction of questions for which thereviewer identified the correct intruder (out of 6questions per topic), and the topic Group Precisionas the fraction of correct tokens (those not selectedas not belonging in the group).
Thus Match Pre-cision measures how well labelers understand thetopic, and Group Precision measures what fractionof words appeared relevant to the topic.Figure 2 shows the average Match and Groupprecision over the top tokens of all 50 topicslearned with the model, as evaluated by expert andnon-expert workers.
Both groups find the intrudertoken in over 75% of questions.
In the more subtletask of validating each topic token (Group preci-sion) we see a greater variance among the two la-beler groups.
This highlights the difficulty of eval-uating domain specific facts with non-expert users.Table 3 displays the top 20 instance topics learnedwith KB-LDA, ranked by expert Group precision.3.2.2 Precision of Topic ConceptsWe assess the precision of the top 5 concept namesproposed for instance topics, following the methodpresented in Section 2.2.
Top concepts for a sub-set of topics are shown in Table 3.
For each topic,we present to the user a hypernym-hyponym pat-tern of the topic based on the top concepts and topinstances of the topic.
As an example, if the top 5instances of a topic are ie, firefox, chrome, buttons,safari and the top 5 concepts for this topic are webbrowsers, web browser, browser, ie, chrome, thepattern presented to workers is?
[ie, firefox, chrome, buttons, safari] is a [web browsers,web browser, browser, ie, chrome]Workers were asked to match at least 3 instancesto a proposed concept name.
In addition, the sameassessment was applied for each topic using ran-domly sampled concepts.
We present in Table 4the number and precision of patterns based on ex-tracted concepts (Concepts) and random concepts(Random), that were labeled by 1, 2 or 3 workers,as well as the average results among experts.
Weachieve nearly 90% precision according to expertlabeling, however we do not observe large agree-ment among non-expert labelers.3.2.3 Precision of RelationsTo assess the precision of the relations learned inthe KB-LDA model, we extract the top 100 rela-tions learned according to their probability in therelation multinomial piR.
Relation patterns werepresented to workers as sets of the top subject-verb-object tokens of the respective topics in therelation.
An example relation is?
Subject words: [user, users, people, customer, client]?
Verb words: [clicks, selects, submits, click, hits]?
Object words: [function, method, class, object, query]and workers are asked to state whether the pat-tern indicates a valid relation or not, by check-ing whether a reasonable number of combinationsof subject-verb-object triples extracted from eachof the relation groups can produce valid relations.1454Top 2 Topic Concepts Top 10 Topic Tokenstable, key table, query, database, sql, column, data, tables, mysql, index, columnsproperties, css image, code, images, problem, point, color, data, size, screen, pointscredentials, user information name, images, id, number, text, password, address, strings, files, stringpage, content page, html, code, file, image, javascript, browser, http, jquery, jsorm tools, orm tool tomcat, hibernate, server, boost, apache, spring, mongodb, framework, nhibernate, pngclients, apps app, application, http, android, device, phone, code, api, iphone, googleapplications, systems devices, systems, applications, services, platforms, tools, sites, apps, system, servicesystems, platforms google, windows, linux, facebook, git, ant, database, gmail, android, solimits, limit memory, time, thread, code, threads, process, file, program, data, objectdata, table query, table, data, list, example, number, results, search, database, rowstype, value code, function, value, type, pointer, array, memory, compiler, example, stringtable, request data, information, types, properties, details, fields, values, content, resources, attributesdependencies, jar file libraries, library, framework, frameworks, formats, format, database, databases, tools, servertype, object value, focus, place, property, method, reference, interface, effect, pointer, datakinds, code languages, language, features, objects, functions, methods, code, operations, structures, typeselement, elements button, form, link, item, file, mouse, image, value, option, rowjavascript libraries, javascript framework jquery, mysql, http, json, xml, library, html, sqlite, asp, phpprocess, operating system server, client, connection, data, http, socket, message, request, port, servicefolder, files file, files, directory, folder, path, code, name, resources, project, foldersvalue, array array, list, value, values, number, string, code, elements, loop, objectTable 3: Top 20 instance topics learned with KB-LDA.
For each topic we show the top 2 conceptsrecovered for the topic, and top 10 tokens.
In italics are words marked as out-of-topic by expert labelers.Workers Concepts Relations SubsumptionsKB-LDA (p) Random (p) KB-LDA (p) Random (p) KB-LDA (p) Random (p)1 48 (0.96) 6 (0.12) 90 (0.9) 69 (0.69) 31 (0.63) 28 (0.57)2 42 (0.84) 0 (0.0) 63 (0.63) 22 (0.22) 16 (0.33) 9 (0.18)3 26 (0.52) 0 (0.0) 15 (0.15) 4 (0.05) 3 (0.06) 4 (0.08)Experts 44 (0.88) 0 (0.0) 70 (0.7) 13 (0.13) 25 (0.51) 4 (0.08)Table 4: Precision of topic concepts, relations, and subsumptions.
For items extracted from the model(KB-LDA), and randomly (Random), we show the number of items marked as correct, and precision inparentheses (p), as labeled by 1, 2, or 3 non-expert workers, and the average precision by experts.We present in Table 4 the number and precision ofpatterns based on the top 100 relations (Relations)and 100 random relations (Random), that were la-beled by 1, 2 or 3 workers, and the average resultsamong experts.
We achieve 80% precision accord-ing to experts, and only 18% on random relations.We observe similar agreement among expert andnon-expert workers as in the concept evaluationexperiment, however we note that random rela-tions prove more confusing for non-experts andmore of them are (falsely) labeled as correct.3.2.4 Precision of HierarchyWe assess the precision of subsumption relationsmaking up the ontology hierarchy.
These are ex-tracted using the maximum spanning tree over thegraph represented by the Ontology component, piO(see Section 2.1 for details), resulting in 49 sub-sumption relations.
We compare their quality tothat of 49 randomly sampled subsumption rela-tions.
Subsumptions are presented to the workerusing is a patterns, similar to the ones describedabove for concept evaluation, however in this case,the concept tokens are the top tokens of the hyper-nym topic.
An example subsumption relation is?
[java, python, javascript, lists, ruby] is a [languages,language, features, objects, functions]The results shown in Table 4 indicate a low pre-cision among the extracted subsumption relations.This might be explained by the fact that at the finaltraining iteration (2K) of the model, the perplexityof the Ontology component was still improving,while the perplexity of the other model compo-nents seemed closer to convergence.
It is possiblethat the low precision observed here indicates thatmore training iterations are needed to achieve anaccurate ontology using KB-LDA.1455Topic string, character, characters, text, lineTags regex, string, python, php, rubyTopic element, div, css, elements, httpTags css, html, jquery, html5, javascriptTopic table, query, database, sql, columnTags sql, mysql, database, performance, phpTopic jquery, mysql, http, json, xmlTags jquery, json, javascript, ruby, stringTable 5: Top tags associated with sample topics.3.3 Overlap of KB-LDA topics withhuman-provided labelsWe evaluated how well topics from KB-LDA cor-respond to document labels provided by humans,over a randomly sampled set of 40K documentsfrom our corpus.
In StackOverflow, questions(which we consider as documents) can be labeledwith predefined tags.
Here, we estimate the over-lap with the most frequently used tags.
First, fortopic k, we aggregate tags from documents wherek = argmaxk?
?k?d, where ?dis the document topicdistribution.
Table 5 shows examples of the toptags associated with sample topics, indicating agood correlation between top topic words and theunderlying concepts.Next, for each tested document d ?
D, let Wdbe the top 30 words of the most probable topic in?d, and Tdthe set of human provided documenttags.
We consider the following metrics:Docs-Overlap =?Dd1{?t?Td:t?Wd}|D|measures the ratio of documents for which at leastone tag overlaps with a top topic word.
The aver-age ratio of overlapping tags per document isTag-Overlap =1|D|D?d|t : t ?
Td?
t ?Wd||Td|As a baseline, we measure similar overlap metricsusing the 30 most frequent instance tokens in thedocument corpus.
The results in Table 6 indicatean overlap of nearly half of the 20, 50, 100, and500 most frequent tags with top topic tokens ?
sig-nificantly higher than the overlap with frequent to-ken.
Our evaluation is based on the subset of tagsfound in the instance dictionary of KB-LDA.Top Found in KB-LDA Frequent TokensTags Dictionary Docs Tag Docs Tag20 14 0.45 0.42 0.21 0.1650 36 0.48 0.42 0.20 0.14100 72 0.45 0.38 0.20 0.13500 322 0.44 0.33 0.18 0.10Table 6: Docs and Tag overlap of human-providedtags with KB-LDA topics, and frequent tokens.Top 10 ranked triples: ?server, not found, error?,?user, can access, file?, ?method, not found, error?,?user, can change, password?, ?page, not found, error?,?user, can upload, videos?, ?compiler, will generate,error?, ?users, can upload, files?, ?users, can upload,files?, ?object, not found, error?Bottom 10 ranked triples: ?france, will visit,germany?, ?utilities, may include, heat?, ?iran, has had,russia?, ?russia, can stop, germany?, ?macs, do notsupport, windows media player?, ?cell phones, do notmake, phone calls?, ?houses, have made, equipment?,?guests, will find, restaurants?, ?guests, can request,bbq?, ?inspectors, do not make, appointments?Table 7: Top and bottom ReVerb software triplesranked with KB-LDA.3.4 Extracting facts from an open IEresourceWe use KB-LDA to extract domain specific triplesfrom an existing open IE KB, the 15M relationsextracted using ReVerb (Fader et al, 2011) fromClueWeb09.
By extracting the relations in whichthe subject, verb and object noun phrases are in-cluded in the KB-LDA dictionary, we are left withunder 5K triples, indicating the low coverage ofsoftware related triples using open domain extrac-tion, in comparison with the 37K triples extractedfrom StackOverflow and given as an input to KB-LDA.Due to word polysemy, many of the 5Kextracted triples are themselves not specificto the domain.
This suggests a hybrid ap-proach in which KB-LDA is used to rankopen IE triples for relevance to a domain.
Weranked the 5K open triples by the probabilityof the triple given a trained KB-LDA model:p(s, v, o)=?Kks?Kkv?Kkopi?ks,kv,ko?R?sks?oko?vkv.Table 7 shows the top and bottom 10 triplesaccording to this ranking, which suggests thatthe triples ranked higher by KB-LDA are morerelevant to the software domain.We compare the ranking based on KB-LDA to14560.0 0.2 0.4 0.6 0.8 1.0Recall0.00.20.40.60.81.0PrecisionKB-LDA versus ReVerb RankingKB-LDA, Best F1=0.73, AUC=0.67ReVerb, Best F1=0.72, AUC=0.57Figure 3: Precision-recall curves of rankers ofopen IE triples by software relevance, based onKB-LDA probabilities (blue), and ReVerb confi-dence (red).
A star is pointing the highest F1.a ranking using a confidence score for the tripleas assigned by ReVerb.
We manually labeled 500of the triples according to their relevance to thesoftware domain, and measured the precision andrecall of the two rankings at any cutoff thresh-old.
Figure 3 shows precision-recall curves forthe two rankings, demonstrating that the rankingusing probabilities based on KB-LDA leads to amore accurate detection of domain-relevant triples(with AUC of 0.67 for KB-LDA versus 0.57 forReVerb).4 Related WorkKB-LDA is an extension to LDA and link-LDA(Blei et al, 2003; Erosheva et al, 2004), model-ing documents as a mixed membership over en-tity types with additional annotated metadata, suchas links (Nallapati et al, 2008; Chang and Blei,2009).
It is a generalization of Block-LDA (Bal-asubramanyan and Cohen, 2011), however, KB-LDA models two link components, and the inputlinks have a meaningful semantic correspondenceto a KB structure (hierarchical and relational).
Ina related approach, Dalvi et al (2012) cluster webtable concepts to non-probabilistically create hier-archies with assigned concept names.Our work is related to latent tensor representa-tion of KBs, aimed at enhancing the ontologicalstructure of existing KBs with relational data in theform of tensor structures.
Nickel et al (2012) fac-torized the ontology of Yago 2 for relational learn-ing.
A related approach was using Neural TensorNetworks to extract new facts from an existing KB(Chen et al, 2013; Socher et al, 2013).
In con-trast, in KB-LDA, relational data is learned jointlywith the model through the Relations component.Statistical language models have recently beenadapted for modeling software code and textdocuments.
Most tasks focused on enhancingthe software development workflow with codeand comment completion (Hindle et al, 2012;Movshovitz-Attias and Cohen, 2013), learningcoding conventions (Allamanis et al, 2014), andextracting actionable tasks from software doc-umentation (Treude et al, 2014).
In relatedwork, specific semantic relations, coordinate re-lations, have been extracted for a restricted classof software entities, ones that refer to Java classes(Movshovitz-Attias and Cohen, 2015).
KB-LDAextends previous work by reasoning over a largevariety of semantic relations among general soft-ware entities, as found in a document corpus.5 ConclusionsWe presented a model that jointly learns a latentontological structure of a corpus augmented by re-lations, and identifies facts matching the learnedstructure.
The quality of the produced structurewas demonstrated through a series of real-worldevaluations employing human judges, which mea-sured the semantic coherence of instance topics,relations, topic concepts, and hierarchy.
We fur-ther validated the semantic meaning of topic con-cepts, by their correspondence to an independentsource of human-provided document tags.
The ex-perimental evaluation validates the usefulness ofthe proposed model for corpus exploration.The results highlight the benefits of generaliz-ing pattern-based facts (hypernym-hyponym pairsand subject-verb-object tuples), using text docu-ments in a topic model framework.
This modularapproach offers opportunities to further improvean induced KB structure by posing additional con-straints on corpus entities in the form of additionalcomponents to the model.AcknowledgmentsThe authors wish to thank Premkumar Devanbuand Kathryn Rivard Mazaitis for helpful discus-sions, and the anonymous reviewers for their in-sightful comments.
This work was funded by NSFunder grant CCF-1414030.1457ReferencesEdoardo M Airoldi, David M Blei, Stephen E Fienberg,and Eric P Xing.
2009.
Mixed membership stochas-tic blockmodels.
In Advances in Neural InformationProcessing Systems, pages 33?40.Miltiadis Allamanis, Earl T Barr, and Charles Sutton.2014.
Learning natural coding conventions.
arXivpreprint arXiv:1402.4182.Ramnath Balasubramanyan and William W. Cohen.2011.
Block-lda: Jointly modeling entity-annotatedtext and entity-entity links.
In Proceedings of the 7thSIAM International Conference on Data Mining.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research.A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, E.R.Hruschka Jr, and T.M.
Mitchell.
2010.
Toward anarchitecture for never-ending language learning.
InProceedings of the Twenty-Fourth Conference on Ar-tificial Intelligence (AAAI 2010).Jonathan Chang and David M Blei.
2009.
Rela-tional topic models for document networks.
In In-ternational Conference on Artificial Intelligence andStatistics, pages 81?88.Jonathan Chang, Sean Gerrish, Chong Wang, Jordan LBoyd-graber, and David M Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InAdvances in neural information processing systems,pages 288?296.Danqi Chen, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2013.
Learning newfacts from knowledge bases with neural tensor net-works and semantic word vectors.
arXiv preprintarXiv:1301.3618.Bhavana Bharat Dalvi, William W Cohen, and JamieCallan.
2012.
Websets: Extracting sets of entitiesfrom the web using unsupervised information ex-traction.
In Proceedings of the fifth ACM interna-tional conference on Web search and data mining,pages 243?252.
ACM.Elena Erosheva, Stephen Fienberg, and John Lafferty.2004.
Mixed-membership models of scientific pub-lications.
Proceedings of the National Academy ofSciences of the United States of America.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 1535?1545.
Association for ComputationalLinguistics.Google.
2011.
Freebase data dumps.http://download.freebase.com/datadumps/.Thomas L Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
Proc.
of the National Academy ofSciences of the United States of America.Marti A Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th conference on Computational linguistics.ACL.Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,and Premkumar Devanbu.
2012.
On the naturalnessof software.
In Software Engineering (ICSE), 201234th International Conference on, pages 837?847.IEEE.Johannes Hoffart, Fabian M Suchanek, KlausBerberich, and Gerhard Weikum.
2013.
Yago2: aspatially and temporally enhanced knowledge basefrom wikipedia.
Artificial Intelligence, 194:28?61.T.
Mitchell, W. Cohen, E. Hruschka, P. Talukdar,J.
Betteridge, A. Carlson, B. Dalvi, M. Gardner,B.
Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,T.
Mohamed, N. Nakashole, E. Platanios, A. Rit-ter, M. Samadi, B.
Settles, R. Wang, D. Wijaya,A.
Gupta, X. Chen, A. Saparov, M. Greaves, andJ.
Welling.
2015.
Never-ending learning.
In Pro-ceedings of the Twenty-Ninth AAAI Conference onArtificial Intelligence (AAAI-15).Dana Movshovitz-Attias and William W. Cohen.
2013.Natural language models for predicting program-ming comments.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics.
Association for Computational Linguistics.Dana Movshovitz-Attias and William W. Cohen.
2015.Grounded discovery of coordinate term relation-ships between software entities.
arXiv preprintarXiv:1505.00277.Ramesh M Nallapati, Amr Ahmed, Eric P Xing, andWilliam W Cohen.
2008.
Joint latent topic mod-els for text and citations.
In Proceedings of the 14thACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 542?550.ACM.David Newman, Arthur Asuncion, Padhraic Smyth,and Max Welling.
2009.
Distributed algorithms fortopic models.
The Journal of Machine Learning Re-search, 10:1801?1828.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2012.
Factorizing yago: scalable machinelearning for linked data.
In Proceedings of the 21stinternational conference on World Wide Web, pages271?280.
ACM.Juuso Parkkinen, Janne Sinkkonen, Adam Gyenge, andSamuel Kaski.
2009.
A block model suitable forsparse graphs.
In Proceedings of the 7th Inter-national Workshop on Mining and Learning withGraphs (MLG 2009), Leuven.1458Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
InProceedings of the 31st annual meeting on Associa-tion for Computational Linguistics, pages 183?190.Association for Computational Linguistics.Ian Porteous, David Newman, Alexander Ihler, ArthurAsuncion, Padhraic Smyth, and Max Welling.
2008.Fast collapsed gibbs sampling for latent dirichlet allocation.
In Proceedings of the 14th ACM SIGKDDinternational conference on Knowledge discoveryand data mining, pages 569?577.
ACM.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics, pages 801?808.
Association forComputational Linguistics.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
In Ad-vances in Neural Information Processing Systems,pages 926?934.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In Proceedings of the 16th international con-ference on World Wide Web, pages 697?706.
ACM.Partha Pratim Talukdar, Derry Wijaya, and TomMitchell.
2012.
Acquiring temporal constraints be-tween relations.
In Proceedings of the 21st ACMinternational conference on Information and knowl-edge management, pages 992?1001.
ACM.Christoph Treude, M Robillard, and Barth?el?emy Dage-nais.
2014.
Extracting development tasks to nav-igate software documentation.
IEEE Transactionson Software Engineering.Alexander Yates, Michael Cafarella, Michele Banko,Oren Etzioni, Matthew Broadhead, and StephenSoderland.
2007.
Textrunner: open informationextraction on the web.
In Proceedings of HumanLanguage Technologies: The Annual Conference ofthe North American Chapter of the Association forComputational Linguistics: Demonstrations, pages25?26.
Association for Computational Linguistics.1459
