EFFICIENT CEPSTRAL NORMALIZATION FORROBUST SPEECH RECOGNITIONFu-Hua Liu, Richard M. Stern, Xuedong Huang, Alejandro AceroDepartment of Electrical and Computer EngineeringSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTIn this paper we describe and compare the performance of a seriesof cepstrum-based procedures that enable the CMU SPHINX-IIspeech recognition system to maintain a high level of recognitionaccuracy over a wide variety of acoustical environments.
Wedescribe the MFCDCN algorithm, an environment-independentextension of the efficient SDCN and FCDCN algorithms devel-oped previously.
We compare the performance of these algorithmswith the very simple RASTA and cepstral mean normalizationprocedures, describing the performance of these algorithms inthecontext of the 1992 DARPA CSR evaluation using secondarymicrophones, and in the DARPA stress-test evaluation.1.
INTRODUCTIONThe need for speech recognition systems and spoken lan-guage systems to be robust with respect to their acousticalenvironment has become more widely appreciated in recentyears (e.g.
\[1\]).
Results of many studies have demonstratedthat even automatic speech recognition systems that aredesigned to be speaker independent can perform verypoorly when they are tested using a different type of micro-phone or acoustical environment from the one with whichthey were trained (e.g.
\[2,3\]), even in a relatively quietoffice environment.
Applications uch as speech recogni-tion over telephones, in automobiles, on a factory floor, oroutdoors demand an even greater degree of environmentalrobusmess.Many approaches have been considered in the developmentof robust speech recognition systems including techniquesbased on autoregressive analysis, the use of special distor-tion measures, the use of auditory models, and the use ofmicrophone arrays, among many other approaches (asreviewed in \[1,4\]).In this paper we describe and compare the performance ofaseries of cepstrum-based procedures that enable the CMUSPHINX-II speech recognition system to maintain a highlevel of recognition accuracy over a wide variety of acousti-cal environments.
The most recently-developed algorithmis multiple fixed codeword-dependent c pstral normaliza-tion (MFCDCN).
MFCDCN is an extension of a similaralgorithm, FCDCN, which provides an additive nviron-mental compensation to cepstral vectors, but in an environ-ment-speci f ic  fashion \[5\].
MFCDCN is lesscomputationally complex than the earlier CDCN algorithm,and more accurate than the related SDCN and BSDCNalgorithms \[6\], and it does not require domain-specificpaining to new acoustical environments.
In this paper wedescribe the performance of MFCDCN and related algo-rithms, and we compare it to the popular RASTA approachto robustness.2.
EFF IC IENT CEPSTRUM-BASEDCOMPENSATION TECHNIQUESIn this section we describe several of the cepstral normal-ization techniques we have developed to compensatesimultaneously for additive noise and linear filtering.
Mostof these algorithms are completely data-driven, as the com-pensation parameters are determined by comparisonsbetween the testing environment and simultaneously-recorded speech samples using the DARPA standard clos-etalking Sennheiser HMD-414 microphone (referred to asthe CLSTLK microphone in this paper).
The remainingalgorithm, codeword-dependent cepstral normalization(CDCN), is model-based, asthe speech that is input to therecognition system is characterized as speech from theCLSTLK microphone that undergoes unknown linear filter-ing and corruption by unknown additive noise.In addition, we discuss two other procedures, the RASTAmethod, and cepstral mean normalization, that may bereferred to as cepstral-filtedng techniques.
These proce-dures do not provide as much improvement as CDCN,MFCDCN and related algorithms, but they can be imple-mented with virtually no computational cost.2.1.
Cepstral Normalization TechniquesSDCN.
The simplest compensation algorithm, SNR-Dependent Cepstral Normalization (SDCN) \[2,4\], appliesan additive corr~tion in the cepstral domain that dependsexclusively on the instantaneous SNR of the signal.
Thiscorrection vector equals the average difference in cepstra69between simultaneous " tereo" recordings of speech sam-pies from both the training and testing environments at eachSNR of speech in the testing environment.
At high SNRs,this conection vector primarily compensates for differencesin speclzal tilt between the training and testing environ-ments (in a manner similar to the blind deconvolufion pro-cedure tirst proposed by Stockham et al \[7\]), while at lowSNRs the vector provides a form of noise subtraction (in amanner similar to the spectral subtraction algorithm firstproposed by Boll \[8\]).
The SDCN algorithm is simple andeffective, but it requires environment-specific training.FCDCN.
Fixed codeword-dependent cepstral normaliza-tion (FCDCN) \[4,6\] was developed to provide a form ofcompensation that provides greater ecognition accuracythan SDCN but in a more computationally-efticient fashionthan the CDCN algorithm which is summarized below.The FCDCN algorithm applies an additive correction thatdepends on the instantaneous SNR of the input (likeSDCN), but that can also vary from codeword to codeword(like CDCN)= z+r \ [k , l \ ]where for each frame $ represents he estimated cepstralvector of the compensated speech, z is the cepstral vector ofthe incoming speech in the target environment, k is an indexidentifying the VQ codeword, I is an index identifying theSNR, and r \[k, !\] is the correction vector.The selection of the appropriate codeword is done at theVQ stage, so that the label k is chosen to minimizeIlz+ r \ [k ,  l\] - c \[k\] II2where the c \[k\] are the VQ codewords of the speech in thetraining database.
The new correction vectors are estimatedwith an EM algorithm that maximizes the likelihood of thedata.The probability density function of x is assumed to be amixture of Gaussian densities as in \[2,4\].K - Ip(x)  = ~ P\[k\] (NxC\[k\],Xk)k=0The cepstra of the corrupted speech are modeled as Gauss-ian random vectors, whose variance depends also on theinstantaneous SNR, l, of the input.p (zl ~ r, 1 f f~\ [ / \ ]  exp /\] - c \[k\] II2k 20"In \[4\] it is shown that the solution to the EM algorithm isthe following iterative algorithm.
In practice, convergenceis reached after 2 or 3 iterations if we choose the initial val-ues of the correction vectors to be the ones specified by lheSDCN algorithm.1.
Assume initial values for r' \[k, l\] and 02 \[l\] .2.
Estimate f i \[k\], the aposteriori probabilities ofthe mix-ture components given the correction vectors r' \[k, li\], vari-ances 02 \[li\], and codebook vectors c \[k\]exp I" 1202 \[ l i\]~\[k\]  = r -~exp(- 1p = 0 202 \[li\]- - - I l z i+  r' \[k,/\] - c \ [k \ ]  I12)- - - I I  zi + ,' tp, li\] - c \[p\] II2)where I i is the instantaneous SNR of the i th frame.3.
Maximize the likelihood of the complete data by obtainingnew estimates for the correction vectors r' \[k, l\] and cor-responding o \[ l\] :N-1(x i- zi)f i \[k\] ~ i t -  tili=0 r\[k, l\] = N- IEf i  \[ k\] ~i \[ l - li\]i=0N- IK - IE E Ilxi-zi-'t~OllZfitk\]Stl-ll \]o2\[l\] = i=0k=0N- IK - IE E J~\[k\]S\[t-li\]i=Ok=O4.
Stop if convergence has been reached, otherwise go to Step2.In the current version of FCDCN the SNR is varied over arange of 30 dB in 1-dB steps, with the lowest SNR set equalto the estimated noise level.
At each SNR compensation vec-tors are computed for each of 8 separate VQ clusters.Figure 1 illustrates ome typical compensation vectorsobtained with the FCDCN algorithm, computed using thestandard closetalking Sennheiser HMD-414 microphone andthe unidirectional desktop PCC-160 microphone used as thetarget environment.
The vectors are computed at the extremeSNRs of 0 and 29 dB, as well as at 5 dB.
These curves areobtained by calculating the cosine transform of the cepstralcompensation vectors, so they provide an estimate of theeffective spectral profile of the compensation vectors.
Thehorizontal axis represents frequency, warped nonlinearlyaccording to the mel scale \[9\].
The maximum frequency cor-responds to the Nyquist frequency, 8000 Hz.
We note that hespectral profile of the compensation vector varies with SNR,and that especially for the intermediate SNRs the various VQclusters require compensation vectors of different spectralshapes.
The compensation curves for 0-dB SNR average tozero dB at low frequencies by design.70 25 k~20~ SNR=29dBi 10" $!
I I !0 0.00 0.25 0.50 0.75 1.00Normalized Warped Frequency~25r20 SNR = 5 dB~ lO11, , , , ;t 0.00 0.25 0.50 0.75 1.00 Normalized Warped Frequency~25\["SNR = 0 dBO~ 0.25 0.50 O.
75 1.00Normalized Warped Frequency-5Figure 1: Comparison of compensation vectors using theFCDCN method with the PCC-160 unidirectional desktopmicrophone, atthree different signal-to-noise ratios.
Themaximum SNR used by the FCDCN algorithm is 29 dB.The computational complexity of the FCDCN algorithm isvery low because the correction vectors are precomputed.However, FCDCN does require simultaneously-recordeddata from the training and testing environments.
Inpreviousstudies \[6\] we found that the FCDCN algorithm provided alevel of recognition accuracy that exceeded what wasobtained with all other algorithms, including CDCN.MFCDCN.
Multiple fixed codeword-dependent cepstralnormalization (MFCDCN) is a simple extension to theFCDCN algorithm, with the goal of exploiting the simplic-ity and effectiveness of FCDCN but without he need forenvironment-specific training.In MFCDCN, compensation vectors are precomputed inparallel for a set of target envkonments, using the FCDCNprocedure as described above.
When an utterance from anunknown environment is input to the recognition system,compensation vectors computed using each of the possibletarget environments are applied successively, and the envi-ronment is chosen that minimizes the average residual VQdistortion over the entire utterance,llz + r\[k, l,m\] -c  \[k\] I zwhere k refers to the VQ codeword, I to the SNR, and m tothe target environment used to train the ensemble of com-pensation vectors.
This general approach is similar in spiritto that used by the BBN speech system \[13\], which per-forms a classification among six groups of secondarymicrophones and the CLSTLK microphone to determinewhich of seven sets of phonetic models hould be used toprocess peech from unknown environments.The success of MFCDCN depends on the availability oftraining data with stereo pairs of speech recorded from thetraining environment and from a variety of possible targetenvironments, and on the extent o which the environmentsin the training data are representative of what is actuallyencountered in testing.IMFCDCN.
While environment selection for the compen-sation vectors of MFCDCN is generally performed on anutterance-by-utterance basis, the probability of a correctselection can be improved by allowing the classificationprocess to make use of cepstral vectors from previous utter-ances in a given session as well.
We refer to this type ofunsupervised incremental daptation as Incremental Multi-ple Fixed Codeword-Dependent Cepstral NormalizationOMFCDCN).CDCN.
One of the best known compensation algorithmsdeveloped at CMU is Codeword-Dependent Cepstral Nor-malization (CDCN) \[2,4\].
CDCN uses EM techniques tocompute ML estimates of the parameters characterizing thecontributions of additive noise and linear filtering that whenapplied in inverse fashion to the cepstra of an incomingutterance produce an ensemble of cepstral coefficients hatbest match (in the ML sense) the cepstral coefficients of theincoming speech in the testing environment tothe locationsof VQ codewords in the training environment.The CDCN algorithm has the advantage that it does notrequire a priori knowledge of the testing environment (inthe form of any sort of simultaneously-recorded "stereo"training data in the training and testing environments).However, it has the disadvantage of a somewhat more com-putationally demanding compensation process thanMFCDCN and the other algorithms described above.
Com-pared to MFCDCN and similar algorithms, CDCN uses agreater amount of structural knowledge about he nat~e ofthe degradations tothe speech signal in order to improverecognition accuracy.
Liu et al \[5\] have shown that thestructural knowledge mbodied in the CDCN algorithmenables it to adapt o new envkonments much more rapidly71than an algorithm closely related to SDCN, but this experi-ment has not yet been repeated for FCDCN.2.2.
Cepstral Filtering TechniquesIn this :section we describe two extremely simple tech-niques, RASTA and cepstral mean normalization, whichcan achieve a considerable amount of environmentalrobusmess atalmost negligible cost.RASTA.
In RASTA filtering \[10\], a high-pass filter isapplied to a log-spectral representation f speech such asthe cepstral coefficients.
The SRI DECIPHER TM system,for example, uses the highpass filter described by the differ-ence equationy\[n\] = x\[n\] -x \ [n -  1\] +0.97y\ [n-  1\]where x \[n\] and y \[n\] are the time-varying cepstral vectorsof the utterance before and after RASTA filtering, and theindex n refers to the analysis frames \[11\].Cepstral mean normalization.
Cepstral mean normaliza-tion (CMN) is an alternate way to high-pass filter cepstralcoefficients.
In cepstral mean normalization the mean of thecepstral vectors is subtracted from the cepstral coefficientsof that utterance on a sentence-by-sentence basis:y \[n\]N1= x In\] - ~ 2 .
,  x \[n\]1n=lwhere N is the total number frames in an utterance.Figure 2 shows the low-frequency portions of the transferfunctions of the RASTA and CMN filters.
Both curvesexhibit a deep notch at zero frequency.
The shape of theCMN curve depends on the duration of the utterance, and isplotted in Figure 2 for the average duration in the DARPAWall Street Journal task, 7 seconds.
The Nyquist frequencyfor the time-varying cepstral vectors is 50 frames per sec-ond.Algorithms like RASTA and CMN compensate for theeffects of unknown linear filtering because linear filters pro-duce a static compensation vector in the cepstral domainthat is the average difference between the cepstra of speechin the training and testing environments.
Because theRASTA and CMN filters are highpass, they force the aver-age values of cepstral coefficients to be zero in both thetraining and testing domains.
Nevertheless, neither CMNnor RASTA can compensate directly for the combinedeffects of additive noise and linear filtering.
It is seen inFigure 1 that the compensation vectors that maximize thelikelihood of the data vary as a function of the SNR of indi-vidual frames of the utterance.
Hence we expect compensa-tion algorithms like MFCDCN (which incorporate thisknowledge) to be more effective than RASTA or CMN(which do not).~ I0"-10 "~20 ~-30"-40"/ CMN, .
'~  2 4 6 8 \ Cepstral Frequency (Frames/sec)RASTAFigure 2: Comparison of the frequency response of thehighpass cepstral filters implemented by the RASTA algo-rithm as used by SRI (dotted curve), and as implied byCMN (solid curve).
The CMN curve assumes an utteranceduration of 7 seconds.3.
EXPERIMENTAL  RESULTSIn this section we describe the ability of the various envi-ronmental compensation algorithms to improve the recogni-tion accuracy obtained with speech from unknown ordegraded microphones.The environmental compensation algorithms were evalu-ated using the SPHINX-II recognition system \[12\] in thecontext of the November, 1992, evaluations of continuousspeech recognition systems using a 5000-word closed-vocabulary task consisting of dictation of sentences fromthe Wall Street Journal.
A component of that evaluationinvolved utterances from a set of unknown "secondary"microphones, including desktop microphones, telephonehandsets and speakerphones, stand-mounted microphones,and lapel-mounted microphones.3.1.
Results from November CSR EvaluationsWe describe in this section results of evaluations of theMFCDCN and CDCN algorithms using speech from sec-ondary microphones in the November, 1992, CSR evalua-tions.Because of the desire to benchmark multiple algorithmsunder several conditions in this evaluation combined withlimited resources and the severe time constraints imposedby the evaluation protocol, this evaluation was performedusing a version of SPHINX-II that was slightly reduced inperformance, but that could process the test data more rap-idly than the system described in \[12\].
Specifically, theselection of phonetic models (across genders) was per-formed by minimizing mean VQ distortion of the cepstralvectors before recognition was attempted, rather than on thebasis of a posteriori probability after classification.
In addi-tion, neither the unified stochastic engine (USE) describedin \[12\] nor the cepstral mean normalization algorithms wereapplied.
Finally, the CDCN evaluations were conductedwithout making use of the CART decision tree or alternate72pronunciations in the recognition dictionary.
The effect ofthese computational shortcuts was to increase the baselineerror rate for the 5000-word task from 6.9% as reported in\[12\] to 8.1% for the MFCDCN evaluation, and to 8.4% forthe CDCN evaluation.~502010oMFCDCN ALGORITHMSecondaryrniceophone%CLSTLKmicrophoneD \[\]!
INo Processing MFCDCN~502010(~ CDCN ALGORITHMSecondary~.
~ microphonesCLSTLKmicrophone\[ \]  oi i0 No Processing CDCNFigure 3: Performance ofthe MFCDCN algorithm (upperpanel) and the CDCN algorithm (lower panel) on the offi-cial DARPA CSR evaluations of November, 1992Figure 3 summarizes the results obtained in the officialNovember, 1992, evaluations.
For these experiments, theMFCDCN algorithm was trained using the 15 environmentsin the training set and developmental est set for this evalua-tion.
It is seen that both the CDCN and MFCDCN algo-rithms significantly improve the recognition accuracyobtained with the secondary microphones, with little or noloss in performance when applied to speech from the clos-etalking Sennheiser HMD-414 (CLSTLK) microphone.The small degradation i recognition accuracy observed forspeech from the CLSTLK microphone using the MFCDCNalgorithm may be at least in part a consequence of errors inselecting the environment for the compensation vectors.Environment-classification errors occurred on 48.8% of theCLSTLK utterances and on 28.5% of the utterances fromsecondary microphone.
In the case of the secondary micro-phones, however, ecognition accuracy was no better usingthe FCDCN algorithm which presumes knowledge of thecorrect environment, so confusions appear to have takenplace primarily between acoustically-similar environments.In a later study we repeated the evaluation using MFCDCNcompensation vectors obtained using only the seven catego-ries of microphones suggested by BBN rather than the orig-inal 15 environments.
This simplification produced only amodest increase in error rate for speech from secondarymicrophones (from 17.7% to 18.9%) and actually improvedthe error rate for speech from the CLSTLK microphone(from 9.4% to 8.3%).Figure 4 summarizes the results of a series of (unofficial)experiments run on the same data that explore the interac-tion between MFCDCN and the various cepstral filteringtechniques.
The vertical dotted line identifies the systemdescribed in \[12\].
"2O100CLSTLKmicrophoneCSR BASELINEEVALUATION SYSTEM0 "microphones"0" - "0I I I I INo RASTA CMN CMN and CMN andProcessing MFCDCN IMFCDCNFigure 4: Comparison of the effects of MFCDCN,IMFCDCN, cepstral mean normalization (CMN), and theRASTA algorithm on recognition accuracy of the Sen-nbeiser HMD-414 microphone (solid curve) and the secondary microphones (dashed curve), from the November 1992DARPA CSR evaluation data.It can be seen in Figure 4 that RASTA filtering providesonly a modest improvement in errors using secondarymicrophones, and degrades peech from the CLSTLKmicrophone.
CMN, on the other hand, provides almost asmuch improvement in recognition accuracy as MFCDCN,without degrading speech from the CLSTLK microphone.We do not yet know why our results using CMN are somuch better than the results obtained using RASTA.
In con-trast, Schwartz et al obtained approximately comparableresults using these two procedures \[13\].Finally, adding MFCDCN to CMN improves the error ratefrom 21.4% to 16.2%, and the use of IMFCDCN provides afurther eduction in error ate to 16.0% for this task.3.2.
Results from the "Stress Test" EvaluationIn addition to the evaluation described above, a secondunofficial "stress-test" evaluation was conducted in Decem-ber, 1992, which included spontaneous speech, utterancescontaining out-of-vocabulary words, and speech fromunknown microphones and environments, all related to theWall Street Journal domain.73The version of SPHINX-II used for this evaluation was con-figured to maximize the robustness of the recognition pro-cess.
It was trained on 13,000 speaker- independentutterances from the Wall Street Journal task and 14,000utterances of spontaneous speech from the ATIS travelplanning domain.
The trigram grammar for the system wasderived from 70.0 million words of text without verbalizedpunctuation and 11.6 million words with verbalized punctu-ation.
Two parallel versions of the SPHINX-II system wererun, with and without IMFCDCN.
Results obtained aresummarized in the Table I below.In Out of STRESS BASEVocab Vocab TOTAL CSRb'K CLSTLK 9.4% - 9.4% 5.3%5K other mie 13.4% - 13.4% 17.7%20K CLSTLK 16.8% 22.8% 18.1% 12.4%20K other mic 23.7% 24.8% 24.0% -Spontaneous \[11.9% 27.2% 22.4% -Table 1: Error rates obtained by SPHINX-II in theDecember, 1992, "Stress-Test" Evaluation.
The baselineCSR results are provided fox comparison only, and were notobtained using a comparably-configured system.We also compared these results with the performance of thebaseline SPHINX-H system on the same data.
The baselinesystem achieved a word error rate of 22.9% using only thebigram language model.
Adding IMFCDCN reduced theerror rate only to 22.7%, compared to 20.8% for the stress-test system using IMFCDCN.
We bel ieve that theIMFCDCN algoxithm provided only a small benefit becauseonly a small percentage of data in this test was from sec-ondary microphones.In general, we are very encouraged by these results, whichare as good or better than the best results obtained only oneyear ago under highly controlled conditions.
We believethat the stress-test protocol is a good paradigm for futureevaluations.ACKNOWLEDGMENTSThis research was sponsored by the Department of the Navy,Naval Research Laboratory, under Grant No.
N00014-93-2005.The views and conclusions contained in this document are thoseof the authors and should not be interpreted asrepresenting theofficial policies, either expressed or implied, of the U.S. Govern-ment.
We thank Raj Reddy and the rest of the speech group fortheir contributions tothis work.2.3.4.5.6.7.8.9.10.11.12.13.REFERENCESJuang, B. H. "Speech Recognition i  Adverse Environ-ments".
Comp.
Speech and Lang.
5:275-294, 1991.Acero, A. and Stern, R. M. "Environmental Robusmess inAutomatic Speech Recognition".
ICASSP-90, pages 849-852.
April, 1990.Erell, A. and Weintranb, M. Estimation "Using Log-Spec-tral-Distance Criterion for Noise-Robust Speech Recogni-tion".
ICASSP-90, pages 853-856.
April, 1990.Acerro, A. Acoustical and Environmental Robustness inAutomatic Speech Recognition.
Kluwer Academic Publish-ers, Boston, MA, 1993.Liu, E-H., Acero, A., and Stern, R. M. "Efficient JointCompensation f Speech for the Effects of Additive Noiseend Linear Filtering".
ICASSP-92, pages 865-868.
March,1992.Acero, A. and Stern, R. M. "Robust Speech Recognition byNormalization of the Acoustic Space".
ICASSP-91, pages893-896.
May, 1991.Stockham, T.G., Cannon, T. M,.
end Ingebretsen, R.
B.
"Blind Deconvolution Through Digital Signal Processing".Proc.
IEEE.
63:678-692, 1975.Boll, S. E "Suppression ofAcoustic Noise in SpeechUsing Spectral Subtraction".
IEEE Trans.
Acoust.
Speech.and Sig.
Proc.
2:113-120, 1979.Davis, S.B, Mermelstein, P."Comparison of ParametricRepresentations of Monosyllabic Word Recognition iContinuously Spoken Sentences'; IEEE Trans.
Acoust.Speech.
and Sig.
Proc.
28:357-366, 1980.Hermansky, H., Morgan, N., Bayya, A., Kohn, E "RASTA-PLP Speech Analysis Technique".
ICASSP-92, pages 121-124.
March, 1992Murveit, H., Weintraub, M. "Speaker-Independent Con-nected-Speech Recognition Using Hidden Markov Mod-els".
Proc.
DARPA Speech and Natural LanguageWorkshop.
February, 1992.Huang, X.,Alleva, E, Hweng.
M.-Y., Rosenfeld, R. "AnOverview of the SPHINX-II Speech Recognition System".Proc.
DARPA Human Language Technology Workshop.March, 1993.Schwartz, R., Anastasakos,A., Kubala, F., Makhoul, J.,Nguyen" L., Zavaliagkos, G. "Comparative Experimentson Large Vocabulary Speech Recognition".
Proc.
DARPAHuman Language Technology Workshop, March, 1993.74
