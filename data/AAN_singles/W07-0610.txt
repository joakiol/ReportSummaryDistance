Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 73?80,Prague, Czech Republic, June 2007 c?2007 Association for Computational LinguisticsThe Topology of Synonymy and Homonymy NetworksJames Gorman and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{jgorman2,james}@it.usyd.edu.auAbstractSemantic networks have been used suc-cessfully to explain access to the men-tal lexicon.
Topological analyses of thesenetworks have focused on acquisition andgeneration.
We extend this work to lookat models that distinguish semantic rela-tions.
We find the scale-free propertiesof association networks are not found insynonymy-homonymy networks, and thatthis is consistent with studies of childhoodacquisition of these relationships.
We fur-ther find that distributional models of lan-guage acquisition display similar topolog-ical properties to these networks.1 IntroductionSemantic networks have played an important role inthe modelling of the organisation of lexical knowl-edge.
In these networks, words are connected bygraph edges based on their semantic relations.
In re-cent years, researchers have found that many seman-tic networks are small-world, scale-free networks,having a high degree of structure and a short distancebetween nodes (Steyvers and Tenenbaum, 2005).Early models were taxonomic and explained someaspects of human reasoning (Collins and Quillian,1969) (and are still used in artificial reasoning sys-tems), but were replaced by models that focused ongeneral graph structures (e.g.
Collins and Loftus,1975).
These better modelled many observed phe-nomena but explained only the searching of seman-tic space, not its generation or properties that existat a whole-network level.Topological analyses, looking at the statisticalregularities of whole semantic networks, can beused to model phenomena not easily explained fromthe smaller scale data found in human experiments.These networks are typically formed from corpora,expert compiled lexical resources, or human word-association data.Existing work has focused language acquisition(Steyvers and Tenenbaum, 2005) and generation(Cancho and Sole?, 2001).
These models use the gen-eral notion of semantic association which subsumesall specific semantic relations, e.g.
synonymy.There is evidence that there are distinct cogni-tive processes for different semantic relations (e.g.Casenhiser, 2005).
We perform a graph analysisof synonymy, nearness of meaning, and homonymy,shared lexicalisation.We find that synonymy and homonymy producegraphs that are topologically distinct from those pro-duced using association.
They still produce small-world networks with short path lengths but lackscale-free properties.
Adding edges of different se-mantic relations, in particular hyponymy, producesgraphs more similar to the association networks.
Weargue our analyses consistent with other semanticnetwork models where nodes of a common typeshare edges of different types (e.g.
Collins and Lof-tus, 1975).We further analyse the distributional model of lan-guage acquisition.
We find that it does not wellexplain whole-language acquisition, but provides amodel for synonym and homonym acquisition.732 Graph TheoryOur overview of graph theory follows Watts (1999).A graph consists of a set of n vertices (nodes) anda set of edges, or arcs, which join pairs of ver-tices.
Edges are undirected and arcs are directed.Edges and arcs can be weighted or unweighted,with weights indicating the relative strength or im-portance of the edges.
We will only consider un-weighted, undirected networks.
Although there isevidence that semantic relations are both directed(Tversky, 1977) and weighted (Collins and Loftus,1975), we do not have access to this information ina consistent and meaningful format for all our re-sources.Two vertices connected by an edge are calledneighbours.
The degree k of a vertex is the countof it neighbours.
From this we measure the averagedegree ?k?
for the graph and the degree distributionP(k) for all values of k. The degree distribution isthe probability of a vertex having a degree k.The neighbourhood ?v of a vertex v is the set of allneighbours of v not including v. The neighbourhood?S of a subgraph S is the set of all neighbours of S ,not including the members of S .The distance between any two vertices is theshortest path length, or the minimum number ofedges that must be traversed, to reach the first fromthe second.
The characteristic path length L is theaverage distance between vertices.1 The diameterD of a graph is the maximum shortest path lengthbetween any two vertices.
At most D steps are re-quired to reach any vertex from any other vertex but,on average, only L are required.For very large graphs, calculating the values for Land D is computationally difficult.
We instead sam-ple n?
n nodes and find the mean values of L andD across the samples.
The diameter produced willalways be less than or equal to the true diameter.
Wefound n?
= 100 to be most efficient.It is not a requirement that every vertex be reach-able from every other vertex and in these cases bothL and D will be infinite.
In these cases we analysethe largest connected subgraph.1Here we follow Steyvers and Tenenbaum (2005) as it ismore commonly used in the cognitive science literature.
Watts(1999) defines the characteristic path length as the median ofthe means of shortest path lengths for each vertex.2.1 Small-world NetworksTraditional network models assume that networksare either completely random or completely regu-lar.
Many natural networks are somewhere betweenthese two extremes.
These small-world networks ahave the high degree of clustering of a regular latticeand the short average path length of a random net-work (Watts and Strogatz, 1998).
The clustering isindicative of organisation, and the short paths makefor easier navigation.The clustering coefficient Cv is used to measurethe degree of clustering around a vertex v:Cv =|E(?v)|(kv2)where |E(?v)| is the number of edges in the neigh-bourhood ?v and(kv2)is the total number of possibleedges in ?v.
The clustering coefficient C of a graphis the average over the coefficients of all the vertices.2.2 The Scale of NetworksAmaral et al (2000) describe three classes of smallworld networks based on their degree distributions:Scale-free networks are characterised by theirdegree distribution decaying as a power law, havinga small number of vertices with many links (hubs)and many vertices with few links.
Networks in thisclass include the internet (Faloutsos et al, 1999)and semantic networks (Steyvers and Tenenbaum,2005).Broad-scale networks are characterised by theirdegree distribution decaying as a power law fol-lowed by a sharp cut-off.
This class includes col-laborative networks (Watts and Strogatz, 1998).Single-scale networks are characterised by fastdecaying degree distribution, such exponential orGaussian, in which hubs are scarce or nonexistent.This class includes power grids (Watts and Strogatz,1998) and airport traffic (Amaral et al, 2000).Amaral et al (2000) model these differences us-ing a constrained preferential attachment model,where new nodes prefer to attach to highly con-nected nodes.
Scale-free networks result when thereare no constraints.
Broad-scale networks are pro-duced when ageing and cost-to-add-link constraintsare added, making it more difficult to produce veryhigh degree hubs.
Single-scale networks occur when74these constraints are strengthened.
This is one ofseveral models for scale-free network generation,and different models will result in different internalstructures and properties (Keller, 2005).3 Semantics NetworksSemantic networks represent the structure of hu-man knowledge through the connections of words.Collins and Quillian (1969) proposed a taxonomicrepresentation of knowledge, where words are con-nected by hyponym relations, like in the WordNetnoun hierarchy (Fellbaum, 1998).
While this struc-ture predicted human reaction times for verifyingfacts it allows only a limited portion of knowledgeto be expressed.
Later models represented knowl-edge as semi-structured networks, and focused onexplaining performance in memory retrieval tasks.One such model is spreading-activation, in whichthe degree to which a concept is able to be recalled isrelated to its similarity both to other concepts in gen-eral and to some particular prime or primes (Collinsand Loftus, 1975).
In this way, if one is asked toname a red vehicle, fire truck is more likely re-sponse than car: while both are strongly associatedwith vehicle, fire truck is more strongly associatedwith red than is car.More recently, graph theoretic approaches haveexamined the topologies of various semantic net-works.
Cancho and Sole?
(2001) examine graphs ofEnglish modelled from the British National Corpus.Since co-occurrence is non-trivial ?
words in a sen-tence must share some semantic content for the sen-tence to be coherent ?
edges were formed betweenadjacent words, with punctuation skipped.
Twographs were formed: one from all co-occurrencesand the other from only those co-occurrences witha frequency greater than chance.
Both models pro-duced scale-free networks.
They find this modelcompelling for word choice during speech, not-ing function words are the most highly connected.These give structure without conveying significantmeaning, so can be omitted without rendering asentence incoherent, but when unavailable renderspeech non-fluent.
This is consistent with work byAlbert et al (2000) showing that scale-free networksare tolerant to random deletion but sensitive to tar-geted removal of highly connected vertices.Sigman and Cecchi (2002) investigate the struc-ture of WordNet to study the effects of nounal pol-ysemy on graph navigation.
Beginning with synsetsand the hyponym tree, they find adding polysemyboth reduces the characteristic path length and in-creases the clustering coefficient, producing a small-world network.
They propose, citing word primingexperiments as evidence, that these changes in struc-ture give polysemy a role in metaphoric thinking andgeneralisation by increasing the navigability of se-mantic networks.Steyvers and Tenenbaum (2005) examine thegrowth of semantic networks using graphs formedfrom several resources: the free association indexcollected by Nelson et al (1998), Wordnet andthe 1911 Roget?s thesaurus.
All these producedscale-free networks, and, using an age of acquisi-tion and frequency weighted preferential attache-ment model, show that this corresponds to age-of-acquisition norms for a small set of words.
This iscompared to networks produced by Latent SemanticAnalysis (LSA, Landauer and Dumais, 1997), andconclude that LSA is an inadequate model for lan-guage growth as it does not produce the same scale-free networks as their association models.3.1 Synonymy and HomonymyWhile there have been many studies using humansubjects on the acquisition of particular semantic re-lations, there have been no topological studies differ-entiating these from the general notion of semanticassociation.
This is interesting as psycholinguisticstudies have shown that semantic relationships aredistinguishable (e.g.
Casenhiser, 2005).
Here weconsider synonymy and homonymy.There are very few cases of true synonymy, wheretwo words are substitutable in all contexts.
Near-synonymy, where two words share some close com-mon meaning, is more common.
Sets of synonymscan be grouped together into synsets, representing acommon idea.Homonymy occurs when a word has multiplemeanings.
Formally, homonymy is occurs whenwords do not share an etymological root (in lin-guistics) or when the distinction between meaningsis coarse (in cognitive science).
When the wordsshare a root or meanings are close, the relationshipis called polysemy.
This distinction is significant75in language acquisition, but as yet little researchhas been performed on the learning of polysemes(Casenhiser, 2005).
It is also significant for NaturalLanguage Processing.
The effect of disambiguatinghomonyms is markedly different from polysemes inInformation Retrieval (Stokoe, 2005).We do not have access to these distinctions, asthey are not available in most resources, nor arethere techniques to automatically acquire these dis-tinctions (Kilgarriff and Yallop, 2000).
For simplic-ity, will conflate the categories under homonymy.There have been several studies into synonymyand homonymy acquisition in children, and thesehave shown that it lags behind vocabulary growth(Doherty and Perner, 1998; Garnham et al, 2000).A child will associate both rabbit and bunny withthe same concept, but before the age of four, mostchildren have difficulty in choosing the word bunnyif they have already been presented with the wordrabbit.
Similarly, a young child asked to point to twopictures that have the same name but mean differentthings will have difficulty, despite knowing each ofthe things independently.Despite this improvement with age, there aretendencies for language to avoid synonyms andhomonyms as a more general principle of economy(Casenhiser, 2005).
This is balanced by the utility ofambiguous relations for mental navigation (Sigmanand Cecchi, 2002) which goes some way to explain-ing why they still play such a large role in language.4 The Topology of Synonymy andHomonymy RelationsFor each of our resources we form a graph based onthe relations between lexical items.
This differs tothe earlier work of Sigman and Cecchi (2002), whouse synsets as vertices, and Steyvers and Tenenbaum(2005) who use both lexical items and synsets..This is motivated largely by our automatic ac-quisition techniques, and also by human studies, inwhich we can only directly access relationships be-tween words.
This also allows us to directly com-pare resources where we have information aboutsynsets to those without.
We distinguish parts ofspeech as disambiguation across them is relativelyeasy psychologically (Casenhiser, 2005) and com-putationally (e.g.
Ratnaparkhi, 1996).4.1 Lexical Semantic ResourcesA typical resource for providing this informationare manually constructed lexical semantic resources.We will consider three: Roget?s, WordNet and MobyRoget?s thesaurus is a common language the-saurus providing a hierarchy of synsets.
Synsetswith the same general or overlapping meaning andpart of speech are collected into paragraphs.
Theparts of speech covered are nouns, verbs, adjectives,adverbs, prepositions, phrases, pronouns, interjec-tions, conjunctions, and interrogatives.
Paragraphswith similar meaning are collated by part of speechinto labeled categories.
Categories are then collectedinto classes using a three-tiered hierarchy, with themost general concept at the top.
Where a word hasseveral senses, it will appear in several synsets.
Sev-eral editions of Roget?s have been released repre-senting the change in language since the first edi-tion in 1852.
The last freely available edition is the1911, which uses outdated vocabulary, but the globaltopology has not changed with more recent editions(Old, 2003).
As our analysis is not concerned withthe specifics of the vocabulary, this is the edition wewill use.
It consists of a vocabulary of 29,460 nouns,15,173 verbs, 13,052 adjectives and 3,005 adverbs.WordNet (Fellbaum, 1998) is an electronic lex-ical database.
Like Roget?s, it main unit of or-ganisation is the synset, and a word with severalsenses will appear in several synsets.
These are di-vided into four parts of speech: nouns, verbs, ad-jectives and adverbs.
Synsets are connected by se-mantic relationships, e.g antonymy, hyponymy andmeronym.
WordNet 2.1 provides a vocabulary of117,097 nouns, 11,488 verbs, 22,141 adjectives and4,601 adverbs.The Moby thesaurus provides synonymy listsfor over 30,000 words, with a total vocabulary of322,263 words.
These lists are not distinguished bypart of speech.
A separate file is supplied containingpart of speech mappings for words in the vocabu-lary.
We extracted separate synonym lists for nouns,verbs, adjectives and adverbs using this list com-bined with WordNet part of speech information.2This produces a vocabulary of 42,821 nouns, 11,957verbs, 16,825 adjectives and 3,572 adverbs.Table 1 presents the statistics for the largest con-2http://aspell.sourceforge.net/wl/76Roget?s WordNet MobyNoun Verb Adj Adv Noun Verb Adj Adv Noun Verb Adj Advn 15,517 8,060 6,327 626 11,746 6,506 4,786 62 42,819 11,934 16,784 3501?k?
8.97 8.46 7.40 7.17 4.58 6.34 5.16 4.97 34.65 51.98 39.26 16.07L 6.5 6.0 6.4 10.5 9.8 6.0 9.5 5.6 3.7 3.1 3.4 3.7D 21.4 17 17 31 27 15.3 26.4 14 9.6 9.8 9.3 9.8C 0.74 0.68 0.69 0.77 0.63 0.62 0.66 0.57 0.60 0.49 0.57 0.55Lr 4.7 4.5 4.6 3.5 6.3 5.0 5.9 3.3 3.4 2.8 2.9 3.2Dr 8.5 8.4 9.0 7 13.3 10.1 11.8 8 5.5 5 5 6Cr 0.00051 0.0011 0.0012 0.0090 0.00036 0.00099 0.00094 0.028 0.00081 0.0043 0.0023 0.0047Table 1: Topological statistics for nouns, verbs, adjectives and adverbs for our three gold standard resources1e-040.0010.010.111  10  100  1000  10000P(k)kRoget?sWordNetMobyRandomFigure 1: Degree distributions for nounsnected subgraph for the four parts of speech con-sidered, along with statistics for random graphs ofequivalent size and average degree (subscript r).
Inall cases the clustering coefficient is significantlyhigher than that for the random graph.
While thecharacteristic path length and diameter are largerthan for the random graphs, they are still short incomparison to an equivalent latice.
This, combinedwith the high clustering coefficient, indicates thatthey are producing small-world networks.
The di-ameter is larger still than for the random graphs.
To-gether these indicate a more lattice like structure,which is consistent with the intuition that dissimi-lar words are unlikely to share similar words.
Thisis independent of part of speech.Figure 1 shows the degree distributions for nouns,and for a random graph plotted on log-log axes.Other parts of speech produce equivalent graphs.These clearly show that we have not produced scale-free networks as we are not seeing straight linepower law distributions.
Instead we are seeing whatis closer to single- or broad-scale distributions.The differences in the graphs is explained by theWordNet Roget?sHyp Synset Para Catn 11,746 118,264 15,517 27,989 29,431?k?
4.58 6.61 8.97 26.84 140.36L 9.8 6.3 6.5 4.3 2.9D 27 16.4 21.4 12.6 7C 0.63 0.74 0.74 0.85 0.86Table 2: Effect of adding hyponym relationsgranularity of the synonymy relations presented, asindicated by the characteristic path length.
WordNethas fine grained synsets and the smallest characteris-tic path length, while Moby has coarse grained syn-onyms and the largest characteristic path length.4.2 Synonymy-Like RelationsHaving seen that synonymy and homonymy alonedo not produce scale-free networks, we investigatethe synonymy-like relations of hyponymy and topicrelatedness.
Hyponymy is the IS-A class subsump-tion relationship and occurs between noun synsets inWordNet.
Topic relatedness occurs in the groupingof synsets in Roget?s in paragraphs and categories.Table 2 compares adding hyponym edges to thegraph of WordNet nouns and increasing the gran-ularity of Roget?s synsets using edges between allwords in a paragraph or category.
Adding hyponymyrelations increases the connectivity of the graph sig-nificantly and there are no longer any disconnectedsubgraphs.
At the same time the diameter is nearlyhalved and characteristic path length reduce onethird, but average degree only increases by one third.To achieving the same reduction in path length anddiameter by the granularity of Roget?s requires theaverage degree to increase by nearly three times.Figure 2 shows the degree distributions when hy-ponyms are added to WordNet nouns and the granu-larity of Roget?s is increased.
Roget?s category levelgraph is omitted for clarity.
We can see that the orig-771e-051e-040.0010.010.111  10  100  1000  10000P(k)kRoget?sParagraphWordNetHyponymFigure 2: Degree distributions adding hyponym re-lations to nounsinally broad-scale structure of the Roget?s distribu-tion is tending to have a more gaussian distribution.The addition of hyponyms produces a power law dis-tribution for k > 10 of P(k) ?
k?1.7.Additional constraints on attachment reduce theability of networks to be scale-free (Amaral etal., 2000).
The difference between synonymy-homonymy networks and association networks canbe explained by this.
Steyvers and Tenenbaum(2005) propose a plausible attachment model fortheir association networks which has no additionalconstraint function.
If we use the tendency for lan-guages to avoid lexical ambiguity from synonymyand homonymy as a constraint to the production ofedges we will produce broad-scale networks ratherthan scale-free networks.As hyponymy is primarily semantic and does notproduce lexical ambiguity, adding hyponym edgesweakens the constraint on ambiguity, producing ascale-free network.
Generalising synonymy to in-clude topicality weakens the constraints, but at thesame time reduces preference in attachment.
Theresults of this is the gaussian-like distribution withvery few low degree nodes.
The difference betweenthis thesaurus based topicality and that found in hu-man association data is that human association dataonly includes the most similar words.5 Distributional Similarity NetworksLexical semantic resources can be automatically ex-tracted using distributional similarity.
Here wordsare projected into a vector space using the contextsin which they appear as axes.
Contexts can be as1e-051e-040.0010.010.111  10  100  1000  10000P(k)kk=5*k=5k=50*k=50Figure 3: Degree distributions of Jaccardwide as document (Landauer and Dumais, 1997)or close as grammatical dependencies (Grefenstette,1994).
The distance between words in this space ap-proximates the similarity measured by synonymy.We use the noun similarities produced by Gor-man and Curran (2006) using the weighted Jac-card measure and the t-test weight and grammat-ical relations extracted from their LARGE corpus,the method found to perform best against their gold-standard evaluation.
Only words with a corpus fre-quency higher than 100 are included.
This methodis comparable to that used in LSA, although usinggrammatical relations as context produces similar-ity much more like synonymy than those taken at adocument level (Kilgarriff and Yallop, 2000).Distributional similarity produces a list of vocab-ulary words, their similar neighbours and the sim-ilarity to the neighbours.
These lists approximatesynonymy by measuring substitutability in context,and do not only find synonyms as near neighboursas both antonyms and hyponyms are frequently sub-stitutable in a grammatical context (Weeds, 2003).From this we generate graphs by taking either the knearest neighbours to each word (k-NN), or by us-ing a threshold.
To produce a threshold we take themean similarity of the kth neighbour of all words (*k-NN).
We compare both these methods.Figure 3 compares the degree distributions ofthese.
Using k-NN produces a degree distributionthat is close to a Gaussian, where as *k-NN pro-duces a distribution much more like that of our ex-pert compiled resources.
This is unsurprising whenthe distribution of distributional distances is consid-ered.
Some words will have many near neighbours,78Roget?s WordNet Hyp k-NN *k-NNn 15,517 11,746 118,264 35,592 19,642?k?
8.97 4.58 6.61 8.26 13.86L 6.5 9.8 6.3 6.2 6.4D 21.4 27 16.4 12 25.6C 0.74 0.63 0.74 0.18 0.37Table 3: Comparing nouns in expert and distribu-tional resourcesand other few.
In the first case, k-NN will fail to in-clude some near neighbours, and in the second willinclude some distant neighbours that are note se-mantically related.
This result is consistent betweenk = 5 and 50.
Introduction of random edges fromthe noise of distant neighbours reduces the diameterand missing near neighbours reduces the clusteringcoefficient (Table 3).In Table 3 we also compare these to noun syn-onymy in Roget?s, and to synonymy and hyponymyin WordNet.
Distributional similarity (*k-NN) pro-duces a network with similar degree, characteristicpath length and diameter.
The clustering coefficientis much less than that from expert resources, is stillseveral orders of magnitude larger than an equivalentrandom graph (Table 1).Figure 4 compares a distributional network to net-works WordNet and Moby.
We can see the samebroad-scale in the distributional and synonym net-works, and a distinct difference with the scale-freeWordNet hyponym distribution.The distributional similarity distribution is sim-ilar to that found in networks formed from LSAby Steyvers and Tenenbaum (2005).
Steyvers andTenenbaum hypothesise that the distributions pro-duced by LSA might be due more to frequency dis-tribution effects that correct language modelling.In light of our analysis of synonymy relations,we propose a new explanation.
Given that: dis-tributional similarity has been shown to approx-imate the semantic similarity in synonymy rela-tions found in thesaurus type resources (Curran,2004); distributional similarity produces networkswith similar statistical properties to those formed bysynonym and homonymy relations; and, the syn-onym and homonymy relations found in thesauriproduce networks with different statistical proper-ties to those found in the association networks anal-ysed by Steyvers and Tenenbaum; it can be plausibly1e-040.0010.010.111  10  100  1000  10000P(k)kWordNetHyponymMoby*k=5Figure 4: Degree distributions for nounshypothesised that distributional techniques are mod-eling the acquisition of synonyms and homonyms,rather than all semantic relationships.This is given further credence by experimentalfindings that acquisition of homonyms occurs at adifferent rate to the acquisition of vocabulary.
Thisindicates that there are different mechanisms forlearning the meaning of lexical items and learningto relate the meanings of lexical items.
Any whole-language model would then be composed of a com-mon set of lexical items related by disparate rela-tions, such as synonymy, homonymy and hyponymy.This type of model is predicted by spreading activa-tion (Collins and Loftus, 1975).It is unfortunate that there is a lack of datawith which to validate this model, or our constraintmodel, empirically.
This should not prevent furtheranalysis of network models that distiguish semanticrelations, so long as this limitation is understood.6 ConclusionSemantic networks have been used successfully toexplain access to the mental lexicon.
We use bothexpert-compiled and automatically extracted seman-tic resources, we compare the networks formed fromsemantic association and synonymy and homonymy.These relations produce small-world networks, butdo not share the same scale-free properties as for se-mantic association.We find that this difference can be explained usinga constrained attachment model informed by child-hood language acquisition experiments.
It is alsopredicted by spreading-activation theories of seman-79tic access where a common set of lexical items isconnected by a disparate set of relations.
We furtherfind that distributional models of language acquisi-tion produce relations that approximate synonymyand networks topologically similar to synonymy-homonymy networks.7 AcknowledgementsWe would like to thank the anonymous reviewersfor their helpful feedback and corrections.
Thiswork has been supported by the Australian ResearchCouncil under Discovery Projects DP0453131 andDP0665973.ReferencesRe?ka Albert, Hawoong Jeong, and Albert-La?szlo?
Baraba?si.2000.
Error and attack tolerance of complex networks.
Na-ture, 406:378?381.Lu?
?s A. Nunes Amaral, Antonio Scala, Marc Barthe?le?my, andH.
Eugene Stanley.
2000.
Classes of small-world net-works.
Proceedings of the National Academy of Sciences,97(21):11149?11152, October 10.Ramon F. i Cancho and Ricard V. Sole?.
2001.
The smallworld of human language.
Proceedings of The Royal Societyof London.
Series B, Biological Sciences, 268(1482):2261?2265, November.Devin M. Casenhiser.
2005.
Children?s resistance tohomonymy: an experimental study of pseudohomonyms.Journal of Child Language, 32:319?343.Allan M. Collins and Elizabeth F. Loftus.
1975.
A spreading-activation theory of semantic processing.
Psychological re-view, 82(6):407?428.Allan M. Collins and M. Ross Quillian.
1969.
Retrieval timefrom semantic memory.
Journal of Verbal Learning and Ver-bal Behavior, 8:240?247.James R. Curran.
2004.
From Distributional to Semantic Simi-larity.
Ph.D. thesis, University of Edinburgh.Martin Doherty and Josef Perner.
1998.
Metalinguistic aware-ness and theory of mind: just two words for the same thing?Congitive Development, 13:279?305.Michalis Faloutsos, Petros Faloutsos, and Christos Faloutsos.1999.
On power-law relationships of the internet topology.In Proceedings of the conference on Applications, technolo-gies, architectures, and protocols for computer communica-tion, pages 251?262.Christiane Fellbaum, editor.
1998.
WordNet: an electronic lex-ical database.
The MIT Press, Cambridge, MA, USA.Wendy A. Garnham, Julie Brooks, Alan Garnham, and Anne-Marie Ostenfeld.
2000.
From synonyms to homonyms:exploring the role of metarepresentation in language under-standing.
Developmental Science, 3(4):428?441.James Gorman and James R. Curran.
2006.
Scaling distribu-tional similarity to large corpora.
In Proceedings of the 44thAnnual Meeting of the Association for Computational Lin-guistics, Sydney, Australia, 17?21 July.Gregory Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers, Boston.Evelyn F. Keller.
2005.
Revisiting ?scale-free?
networks.Bioessays, 27(10):1060?1068, October.Adam Kilgarriff and Colin Yallop.
2000.
What?s in a the-saurus?
In Proceedings of the Second International Confer-ence on Language Resources and Evaluation, pages 1371?1379.Thomas K. Landauer and Susan T. Dumais.
1997.
A solu-tion to plato?s problem: The latent semantic analysis theoryof acquisition, induction, and representation of knowledge.Psychological Review, 104(2):211?240, April.Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.Schreiber.
1998.
The university of south floridaword association, rhyme, and word fragment norms.http://www.usf.edu/FreeAssociation/.L.
John Old.
2003.
The Semantic Structure of Roget?s, a Whole-Language Thesaurus.
Ph.D. thesis, Indiana University.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages 133?142, 17?18 May.Mariano Sigman and Guillermo A. Cecchi.
2002.
Global or-ganization of the WordNet lexicon.
Proceedings of the Na-tional Academy of Sciences, 99(3):1742?1747.Mark Steyvers and Joshua B. Tenenbaum.
2005.
The large-scale structure of semantic networks: statistical analyses anda model of semantic growth.
Cognitive Science, 29(1):41?78.Christopher Stokoe.
2005.
Differentiating homonymy andpolysemy in information retrieval.
In Proceedings of theConference on Human Language Technology and EmpiricalMethods in Natural Language Processing, pages 403?410.Amos Tversky.
1977.
Features of similarity.
PsychologicalReview, 84(4):327?352, July.Duncan J. Watts and Steven H. Strogatz.
1998.
Collective dy-namics of small-world networks, 393:440?442, 4 June.Duncan J. Watts.
1999.
Small Worlds: The Dynamics of Net-works between Order and Randomness.
Princeton Univer-sity Press, Princeton, NJ, USA.Julie E. Weeds.
2003.
Measures and Applications of LexicalDistributional Similarity.
Ph.D. thesis, University of Sussex.80
