Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 60?67,Sydney, July 2006. c?2006 Association for Computational LinguisticsA fast and accurate method for detecting English-Japanese parallel textsKen?ichi Fukushima, Kenjiro Taura and Takashi ChikayamaUniversity of Tokyoken@tkl.iis.u-tokyo.ac.jp{tau,chikayama}@logos.ic.i.u-tokyo.ac.jpAbstractParallel corpus is a valuable resource usedin various fields of multilingual naturallanguage processing.
One of the mostsignificant problems in using parallel cor-pora is the lack of their availability.
Re-searchers have investigated approaches tocollecting parallel texts from the Web.
Abasic component of these approaches isan algorithm that judges whether a pairof texts is parallel or not.
In this paper,we propose an algorithm that acceleratesthis task without losing accuracy by pre-processing a bilingual dictionary as wellas the collection of texts.
This methodachieved 250,000 pairs/sec throughput ona single CPU, with the best F1 scoreof 0.960 for the task of detecting 200Japanese-English translation pairs out of40, 000.
The method is applicable to textsof any format, and not specific to HTMLdocuments labeled with URLs.
We reportdetails of these preprocessing methods andthe fast comparison algorithm.
To thebest of our knowledge, this is the first re-ported experiment of extracting Japanese?English parallel texts from a large corporabased solely on linguistic content.1 Introduction?Parallel text?
is a pair of texts which is writtenin different languages and is a translation of eachother.
A compilation of parallel texts offered in aserviceable form is called a ?parallel corpus?.
Par-allel corpora are very valuable resources in variousfields of multilingual natural language processingsuch as statistical machine translation (Brown etal., 1990), cross-lingual IR (Chen and Nie, 2000),and construction of dictionary (Nagao, 1996).However, it is generally difficult to obtain paral-lel corpora of enough quantity and quality.
Therehave only been a few varieties of parallel corpora.In addition, their languages have been biased to-ward English?French and their contents toward of-ficial documents of governmental institutions orsoftware manuals.
Therefore, it is often difficultto find a parallel corpus that meets the needs ofspecific researches.To solve this problem, approaches to collectparallel texts from the Web have been proposed.In the Web space, all sorts of languages are usedthough English is dominating, and the content ofthe texts seems to be as diverse as all activities ofthe human-beings.
Therefore, this approach has apotential to break the limitation in the use of par-allel corpora.Previous works successfully built parallel cor-pora of interesting sizes.
Most of them uti-lized URL strings or HTML tags as a clue to ef-ficiently find parallel documents (Yang and Li,2002; Nadeau and Foster, 2004).
Depending onsuch information specific to webpages limits theapplicability of the methods.
Even for webpages,many parallel texts not conforming to the presup-posed styles will be left undetected.
In this work,we have therefore decided to focus on a generallyapplicable method, which is solely based on thetextual content of the documents.
The main chal-lenge then is how to make judgements fast.Our proposed method utilizes a bilingual dictio-nary which, for each word in tne language, givesthe list of translations in the other.
The methodpreprocesses both the bilingual dictionary and thecollection of texts to make a comparison of textpairs in a subsequent stage faster.
A comparison60of a text pair is carried out simply by compar-ing two streams of integers without any dictionaryor table lookup, in time linear in the sum of thetwo text sizes.
With this method, we achieved250,000 pairs/sec throughput on a single XeonCPU (2.4GHz).
The best F1 score is 0.960, for adataset which includes 200 true pairs out of 40,000candidate pairs.
Further comments on these num-bers are given in Section 4.In addition, to the best of our knowledge,this is the first reported experiment of extracitngJapanese?English parallel texts using a methodsolely based on their linguistic contents.2 Related WorkThere have been several attempts to collect paral-lel texts from the Web.
We will mention two con-trasting approaches among them.2.1 BITSMa and Liberman collected English?German par-allel webpages (Ma and Liberman, 1999).
Theybegan with a list of websites that belong to a do-main accosiated with German?speaking areas andsearched for parallel webpages in these sites.
Foreach site, they downloaded a subset of the siteto investigate what language it is written in, andthen, downloaded all pages if it was proved to beEnglish?German bilingual.
For each pair of En-glish and German document, they judged whetherit is a mutual translation.
They made a decisionin the following manner.
First, they searched abilingual dictionary for all English?German wordpairs in the text pair.
If a word pair is found inthe dictionary, it is recognized as an evidence oftranslation.
Finally, they divided the number ofrecognized pairs by the sum of the length of thetwo texts and regard this value as a score of trans-lationality.
When this score is greater than a giventhreshold, the pair is judged as a mutual transla-tion.
They succeeded in creating about 63MB par-allel corpus with 10 machines through 20 days.The number of webpages is considered to haveincreased far more rapidly than the performance ofcomputers in the past seven years.
Therefore, wethink it is important to reduce the cost of calcula-tion of a system.2.2 STRANDIf we simply make a dicision for all pairs in a col-lection of texts, the calculation takes ?
(n2) com-parisons of text pairs where n is the number ofdocuments in the collection.
In fact, most re-searches utilize properties peculiar to certain par-allel webpages to reduce the number of candidatepairs in advance.
Resnik and Smith focused on thefact that a page pair tends to be a mutual transla-tion when their URL strings meet a certain condi-tion, and examined only page pairs which satisfyit (Resnik and Smith, 2003).
A URL string some-times contains a substring which indicates the lan-guage in which the page is written.
For example,a webpage written in Japanese sometimes have asubstring such as j, jp, jpn, n, euc or sjis inits URL.
They regard a pair of pages as a candidatewhen their URLs match completely after remov-ing such language-specific substrings and, only forthese candidates, did they make a detailed com-parison with bilingual dictionary.
They were suc-cessful in collecting 2190 parallel pairs from 8294candidates.
However, this URL condition seemsso strict for the purpose that they found 8294 can-didate pairs from as much as 20 Tera bytes of web-pages.3 Proposed Method3.1 Problem settingsThere are several evaluation criteria for paralleltext mining algorithms.
They include accuracy,execution speed, and generality.
We say an algo-rithm is general when it can be applied to texts ofany format, not only to webpages with associatedinformation specific to webpages (e.g., URLs andtags).
In this paper, we focus on developing a fastand general algorithm for determining if a pair oftexts is parallel.In general, there are two complementary waysto improve the speed of parallel text mining.
Oneis to reduce the number of ?candidate pairs?
to becompared.
The other is to make a single compar-ison of two texts faster.
An example of the for-mer is Resnik and Smith?s URL matching method,which is able to mine parallel texts from a verylarge corpora of Tera bytes.
However, this ap-proach is very specific to the Web and, even if werestrict our interest to webpages, there may be asignificant number of parallel pages whose URLsdo not match the prescribed pattern and thereforeare filtered out.
Our method is in the latter cat-egory, and is generally applicable to texts of anyformat.
The approach depends only on the lin-guistic content of texts.
Reducing the number of61 fffffiflffifffffiflffifi fiflffi!fi"#fifl$fi fiflffi!fi"#fifl$%&'&'()*+,-+-../.0/10234567899:;<===>?@A@B>?@@CDE>3>?@C?FA@>>?
@@3 GH IJHHHKLHMNKJMFigure 1: Outline of the methodcomparisons while maintaining the generality willbe one of our future works.The outline of the method is as follows.
Firstwe preprocess a bilingual dictionary and build amapping from words to integers, which we call?semantic ID.?
Texts are then preprocessed, con-verting each word to its corresponding semanticID plus its position of the occurrence.
Then wecompare all pairs of texts, using their convertedrepresentations (Figure 1).
Comparing a pair oftexts is fast because it is performed in time linearin the length of the texts and does not need anytable lookup or string manipulation.3.2 Preprocessing a bilingual dictionaryWe take only nouns into account in our algorithm.For the language pair of English and Japanese,a correspondence of parts of speech of a wordand its translation is not so clear and may makethe problem more difficult.
A result was actuallyworse when every open-class word was consideredthan when only nouns were.The first stage of the method is to assign an in-teger called semantic ID to every word (in bothlanguages) that appears in a bilingual dictionary.The goal is to assign the same ID to a pair of wordsthat are translations of each other.
In an ideal situa-tion where each word of one language correspondsone-to-one with a word of the other language, allyou need to do is to assign differnt IDs to everytranslational relationship between two words.
Themain purpose of this conversion is to make a com-parison of two texts in a subsequent stage faster.However, it?s not exactly that simple.
A wordvery often has more than one words as its trans-lation so the naive method described above is notdirectly applicable.
We devised an approximatesolution to address this complexity.
We builda bigraph whose nodes are words in the dictio-nary and edges translational relationships betweenthem.
This graph consists of many small con-nected components, each representing a group ofwords that are expected to have similar meanings.We then make a mapping from a word to its se-mantic ID.
Two words are considered translationsof each other when they have the same semanticID.This method causes a side-effect of connectingtwo words not directly related in the dictionary.
Ithas both good and bad effects.
A good effect isthat it may connect two words that do not explic-itly appear as translations in the dictionary, but areused as translations in practice (see section 4.3).In other words, new translational word pairs aredetected.
A bad effect, on the other hand, is that itpotentially connects many words that do not sharemeanings at all.
Figure 2 shows an actual exam-ple of such an undesirable component observed inour experiment.
You can go from fruit to armythrough several hops and these words are treatedas identical entity in subsequent steps of our tech-nique.
Futhermore, in the most extreme case, avery large connected component can be created.Table 1 shows the statistics of the component sizesfor the English-Japanese dictionary we have usedin our experiment (EDR Electronic Dictionary).62  Figure 2: Example of a undesirable graphMost components are fairly small (< 10 words).The largest connected component, however, con-sisted of 3563 nodes out of the total 28001 nodesin the entire graph and 3943 edges out of 19413.As we will see in the next section, this had a dev-astating effect on the quality of judgement so weclearly need a method that circumvents the sit-uation.
One possibility is to simply drop verylarge components.
Another is to divide the graphinto small components.
We have tried both ap-proaches.Table 1: Statistics of the component sizes# of nodes # of components2 66293 14984 4635 2126 1257 698 449 3210?
106For partitioning graphs, we used a very sim-ple greedy method.
Even though a more com-plex method may be possible that takes advan-tages of linguistic insights, this work uses a verysimple partitioning method that only looks at thegraph structure in this work.
A graph is partitionedinto two parts having an equal number of nodesand a partition is recursively performed until eachpart becomes smaller than a given threshold.
Thethreshold is chosen so that it yields the best resultfor a training set and then applied to a test data.For each bisection, we begin with a random par-tition and improves it by a local greedy search.Given the current partition, it seeks a pair of nodeswhich, if swapped, maximumly reduces the num-ber of edges crossing the two parts.
Ties are bro-ken arbitrarily when there are many such pairs.
Ifno single swap reduces the number of edges acrossparts, we simply stop (i.e., local search).
A seman-tic ID is then given to each part.This process would lose connections betweenwords that are originally translations in the dictio-nary but are separated by the partitioning.
We willdescribe a method to partially recover this loss inthe end of the next section, after describing howtexts are preprocessed.3.3 Preprocessing textsEach text (document) is preprocessed as follows.Texts are segmented into words and tagged with apart-of-speech.
Inflection problems are addressedwith lemmatization.
Each word is converted intothe pair (nid, pos), where nid is the semantic ID ofthe partition containing the word and pos its posi-tion of occurrence.
The position is normalized andrepresented as a floating point number between 0.0and 1.0.
Any word which does not appear in thedictionary is simply ignored.
The position is usedto judge if words having an equal ID occur in sim-ilar positions in both texts, so they suggest a trans-lation.After converting each word, all (nid, pos) pairsare sorted first by their semantic IDs breaking tieswith positions.
This sorting takes O(n log n) timefor a document of n words.
This preprocessingneeds to be performed only once for each docu-ment.We recover the connections between word pairsseparated by the partitioning in the following man-ner.
Suppose words J and E are translations ofeach other in the dictionary, J is in a partitionwhose semantic ID is x and E in another partitionwhose semantic ID is y.
In this case, we translateJ into two elements x and y.
This result is as iftwo separate words, one in component x and an-other in y, appeared in the original text, so it maypotentially have an undesirable side-effect on thequality of judgement.
It is therefore important tokeep the number of such pairs reasonably small.We experimented with both cases, one in whichwe recover separate connections and the other inwhich we don?t.3.4 Comparing document pairsWe judge if a text pair is likely to be a translationby comparing two sequences obtained by the pre-processing.
We count the number of word pairs63that have an equal semantic ID and whose posi-tions are within a distance threshold.
The bestthreshold is chosen to yield the best result for atraining set and then applied to test set.
This pro-cess takes time linear in the length of texts sincethe sequences are sorted.
First, we set cursorsat the first element of each of the two sequences.When the semantic IDs of the elements under thecursors are equal and the difference between theirpositions is within a threshold, we count them asan evidence of translationality and move both cur-sors forward.
Otherwise, the cursor on the ele-ment which is less according to the sorting cri-teria is moved forward.
In this step, we do notperform any further search to determine if origi-nal words of the elements were related directly inthe bilingual dictionary giving preference to speedover accuracy.
We repeat this operation until anyof the cursors reaches the end of the sequence.
Fi-nally, we divide the number of matching elementsby the sum of the lengths of the two documents.We define this value as ?tscore,?
which stands fortranslational score.
At least one cursor moves af-ter each comparison, so this algorithm finishes intime linear in the length of the texts.4 Experiments4.1 PreparationTo evaluate our method, we used The EDR Elec-tronic Dictionary1 for a bilingual dictionary andFry?s Japanese-English parallel web corpus (Fry,2005) for sample data.
In this experiment, weconsidered only nouns (see section 3.2) and gota graph which consists of 28001 nodes, 19413edges and 9178 connected components of whichthe largest has 3563 nodes and 3943 edges.
Largecomponents including it need to be partitioned.We conducted partitioning with differnt thresh-olds and developed various word?ID mappings.For each mapping, we made several variations intwo respect.
One is whether cut connections arerecovered or not.
The other is whether and howmany numerals, which can be easily utilized toboost the vocaburary of the dictionary, are addedto a bilingual dictionary.The parallel corpus we used had been collectedby Fry from four news sites.
Most texts in the cor-pus are news report on computer technology andthe rest is on various fields of science.
A single1EDR Electronic Dictionary.http://www2.nict.go.jp/kk/e416/EDR/document is typically 1,000?6,000 bytes.
He de-tected parallel texts based only on HTML tags andlink structures, which depend on websites, with-out looking at textual content, so there are manyfalse pairs in his corpus.
Therefore, to evaluateour method precisely, we used only 400 true par-allel pairs that are randomly selected and checkedby human inspection.
We divided them evenly andrandomly into two parts and use one half for atraining set and the other for a test set.
In exper-iments described in section 4.4 and 4.5, we usedother portion of the corpus to scale experiments.For tokenization and pos-tagging, we usedMeCab2 to Japanese texts and SS Tagger3 to En-glish texts.
Because SS Tagger doesn?t act as lem-matizer, we used morphstr() function in Word-Net library4.4.2 Effect of large components and apartitioningFigure 3 shows the results of experiments on sev-eral conditions.
There are three groups of bars; (A)treat every connected component equally regard-less of its size, (B) simply drop the largest compo-nent and (C) divide large components into smallerparts.
In each group, the upper bar correspondsto the case the algorithm works without a distancethreshold and the lower with it (0.2).
The figuresattached to each bar are the maxF1 score, whichis a popular measure to evaluate a classification al-gorithm, and indicate how accurately a method isable to detect 200 true text pairs from the test setof 40,000 pairs.
We didn?t recover word connec-tions broken in the partitioning step and didn?t addany numerals to the vocabrary of the bilingual dic-tionary this time.The significant difference between (A) and (B)clearly shows the devastating effect of large com-ponents.
The difference between (B) and (C)shows that the accurary can be further improvedif large components are partitioned into small onesin order to utilize as much information as possible.In addtion, the accuracy consistently improves byusing the distance threshold.Next, we determined the best word?ID mapping2MeCab: Yet Another Part-of-Speech and MorphologicalAnalyzer.http://mecab.sourceforge.jp/3SS Tagger - a part-of-speech tagger for English.http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/4WordNet - a lexical database for the English language.http://wordnet.princeton.edu/64                     fffiflffiflfffiflffiflFigure 3: Effect of the graph partitioningand distance threshold and tested its performancethrough a 2?fold cross validation.
The best map-ping among those was the one which?
divides a component recursively until thenumber of nodes of each language becomesno more than 30,?
does not recover connections that are cut inthe partitioning, and?
adds numerals from 0 to 999.The best distance threshold was 0.2, and tscorethreshold 0.102.
We tested this rule and thresholdson the test set.
The result was F1 = 0.960.4.3 Effect of false translation pairsOur method of matching words differs from Maand Liberman?s one.
While they only count wordpairs that directly appear in a bilingual dictionary,we identify all words having the same seman-tic ID.
Potential merits and drawbacks to accu-racy have been described in the section 3.2.
Wecompared the accuracy of the two algorithms toinvestigate the effect of our approximate match-ing.
To this end, we implemented Ma and Liber-man?s method with all other conditions and in-put data being equal to the one in the last sec-tion.
We got maxF1 = 0.933 as a result, whichis slightly worse than the figure reported in theirpaper.
Though it is difficult to conclude wherethe difference stems from, there are several fac-tors worth pointing out.
First, our experiment isdone for English-Japanese, while Ma and Liber-man?s experiment for English-German, which aremore similar than English and Japanese are.
Sec-ond, their data set contains much more true pairs(240 out of 300) than our data set does (200 out of40,000).           fffiflffiFigure 4: The two word-matching policyThis number is also worse than that of our ex-periment (Figure 4).
This shows that, at least inthe experiment, our approach of identifying morepairs than the original dictionary causes moregood effects than bad in total.
We looked at wordpairs which are not matched in Ma and Liberman?smethod but in ours.
While most of the pairs can behardly considered as a strict translation, some ofthem are pairs practically used as translations.
Ex-amples of such pairs are shown in Figure 5.    ff fiflffi! "#$% &' ()Figure 5: Word pairs not in the dictionary4.4 Execution SpeedWe have argued that the execution speed is a majoradvantage of our method.
We achieved 250,000pairs/sec throughput on single Xeon (2.4GHz)processor.
It?s difficult to make a fair com-parison of the execution speed because Ma andLiberman?s paper does not describe enough de-tails about their experimants other than processing3145 websites with 10 sparc stations for 10 days.Just for a rough estimate, we introduce some boldassumptions.
Say, there were a thousand pages foreach language in a website or, in other words, amillion page pairs, and the performance of proces-sors has grown by 32 times in the past seven years,our method works more than 40 times faster thanMa and Liberman?s one.
This difference seems65 fffififlfififfiffflffiflff!fl"ff flff#$%&'()*+,-.'/0'1234+56'/078'9:;:<=>'23?@ABCD*EFGH?ICJKLMN*5OPQRSTUVWXY9Z[\]_^`a'bc*de9ZF\fg45hij4kLG'lm<nijo5123'pq*rst'u'vw4xyFuj123Rz{23W4K|}~K<LH??Z?LF*5??'??'??o5()'123*uj1234??F?c<???4<A?FGH?I?K?LF\??'uj123'?c4,L?o56'??'???u???4CFGH*?y?Z<L[?5????????*???G??LF\?^'_`??.K[??5????|???|'123???K5[H?????^?'??<????????(23HLmlm45??*??HK?LF???'23?@ABK56Z?????')?4?CGH*?i?H9Z?LF\?7??????????????
???????????
?Figure 6: A example of false?positive text pairsto be caused by a difference of the complexitybetween the two algorithms.
To the extent writ-ten in their paper, Ma and Liberman calculated ascore of translationality by enumerating all com-binations of two words within a distance thresholdand search a bilingual dictionary for each combi-nation of words.
This algorithm takes ?
(n2) timewhere n is the length of a text, while our methodtakes O(n) time.
In addition, our method doesn?tneed any string manipulation in the comparisonstep.4.5 Analysis of miss detectionsWe analyzed text pairs for which judgements dif-fer between Fry?s and ours.Among pairs Fry determined as a translation,we examined the 10 pairs ranked highest in ouralgorithm.
Two of them are in fact translations,which were not detected by Fry?s method with-out any linguistic information.
The rest eight pairsare not translations.
Three of the eight pairs areabout bioscience, and a word ?cell?
occurred manytime (Figure 6).
When words with an identicalsemantic ID appear repeatedly in two texts beingcompared, their distances are likely to be within adistance threshold and the pair gets unreasonablyhigh tscore.
Therefore, if we take the number ofeach semantic ID in a text into account, we mightbe able to improve the accuracy.We performed the same examination on the 10pairs ranked lowest among those Fry determinednot to be a translation.
But no interesting featurecould be found at the moment.5 Summary and Future WorkIn this paper, we proposed a fast and accuratemethod for detecting parallel texts from a col-lection.
This method consists of major threeparts; preprocess a bilingual dictionary into word?ID conversion rule, convert texts into ID se-quences, compare sequences.
With this method,we achieved 250,000 pairs/sec on a single CPUand best F1 score of 0.960.
In addition, thismethod utilizes only linguistic information of atextual content so that it is generally applicable.This means it can detect parallel documents in anyformat.
Furthermore, our method is independenton languages in essence.
It can be applied to anypair of languages if a bilingual dictionary betweenthe languages are available (a general languagedictionary suffices.
)Our future study will include improving bothaccuracy and speed while retaining the generail-ity.
For accuracy, as we described in Section 4.5,tscore tends to increase when an identical semanticID appears many times in a text.
We might be ableto deal with this problem by taking into accountthe probability that the distance between words iswithin a threshold.
Large connected componentswere partitioned by a very simple method at thepresent work.
More involved partitioning meth-ods may improve the accuracy of the judgement.For speed, reducing the number of comparisons isthe most important issue that needs be addressed.66ReferencesPeter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D.Lafferty, Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machine translation.Comput.
Linguist., 16(2):79?85.Jiang Chen and Jian-Yun Nie.
2000.
Automaticconstruction of parallel english-chinese corpus forcross-language information retrieval.
In Proceed-ings of the sixth conference on Applied natural lan-guage processing, pages 21?28, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.John Fry.
2005.
Assembling a parallel corpus fromRSS news feeds.
In Workshop on Example-BasedMachine Translation, MT Summit X, Phuket, Thai-land, September.Xiaoyi Ma and Mark Liberman.
1999.
BITS: Amethod for bilingual text search over the web.
InMachine Translation Summit VII, September.David Nadeau and George Foster.
2004.
Real-timeidentification of parallel texts from bilingual news-feed.
In Computational Linguistic in the North-East(CLiNE 2004), pages 21?28.Makoto Nagao, editor.
1996.
Natural Language Pro-cessing.
Number 15 in Iwanami Software Science.Iwanami Shoten.
In Japanese.Philip Resnik and Noah A. Smith.
2003.
The web as aparallel corpus.
Comput.
Linguist., 29(3):349?380.C.
C. Yang and K. W. Li.
2002.
Mining English/Chinese parallel documents from the World WideWeb.
In Proceedings of the International WorldWide Web Conference, Honolulu, Hawaii, May.67
