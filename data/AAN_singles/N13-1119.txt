Proceedings of NAACL-HLT 2013, pages 989?999,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsThree Knowledge-Free Methodsfor Automatic Lexical Chain ExtractionSteffen Remus and Chris BiemannFG Language TechnologyDepartment of Computer ScienceTechnische Universita?t Darmstadtremus@kdsl.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.deAbstractWe present three approaches to lexical chain-ing based on the LDA topic model and eval-uate them intrinsically on a manually anno-tated set of German documents.
After motivat-ing the choice of statistical methods for lexi-cal chaining with their adaptability to differentlanguages and subject domains, we describeour new two-level chain annotation scheme,which rooted in the concept of cohesive har-mony.
Also, we propose a new measurefor direct evaluation of lexical chains.
Ourthree LDA-based approaches outperform twoknowledge-based state-of-the art methods tolexical chaining by a large margin, which canbe attributed to lacking coverage of the knowl-edge resource.
Subsequent analysis shows thatthe three methods yield a different chainingbehavior, which could be utilized in tasks thatuse lexical chaining as a component withinNLP applications.1 IntroductionA text that is understandable by its nature exhibitsan underlying structure which makes the text co-herent; that is, the structure is responsible for mak-ing the text ?hang?
together (Halliday and Hasan,1976).
The theoretic foundation of this structure isdefined as coherence and cohesion.
While the for-mer is concerned with the meaning of a text, the lat-ter can be seen as a collection of devices for cre-ating it.
Cohesion and coherence build the basisfor most of the current natural language processingproblems that deal with text understanding.
Lex-ical cohesion ties together words or phrases thatare semantically related.
Once all the cohesive tiesare identified the involved items can be grouped to-gether to form so-called lexical chains, which form atheoretically well-founded building block in variousnatural language processing applications, such asword sense disambiguation (Okumura and Honda,1994), summarization (Barzilay and Elhadad, 1997),malapropism detection and correction (Hirst and St-Onge, 1998), document hyperlinking (Green, 1996),text segmentation (Stokes et al 2004), topic track-ing (Carthy, 2004), and others.
The performance ofthe individual task heavily depends on the quality ofthe identified lexical chains.1.1 Motivation for Corpus-driven ApproachPrevious approaches mainly focus on the use ofknowledge resources like lexical semantic databases(Hirst and St-Onge, 1998) or thesauri (Morris andHirst, 1991) as background information in order toresolve possible semantic relations.
A major draw-back of this strategy is the dependency on the cov-erage of the resource, which has a direct impact onthe lexical chains.
Their quality can be expected tobe poor for resource-scarce languages or specializedapplication domains.Statistical methods to modeling language seman-tics have proven to deliver good results in many nat-ural language processing applications.
In particu-lar, probabilistic topic models have been success-fully used for tasks such as summarization (Gongand Liu, 2001; Hennig, 2009), text segmentation(Misra et al 2009), lexical substitution (Dinu andLapata, 2010) or word sense disambiguation (Cai etal., 2007; Boyd-Graber et al 2007).989In this work, we address the question, whetherstatistical methods for the extraction of lexi-cal chains can yield better results than existingknowledge-based methods, especially for underre-sourced languages or domains, following principlesof Structure Discovery (Biemann, 2012).
To addressthis, we have developed a methodology for evaluat-ing the quality of lexical chains intrinsically, havecarried out an annotation study, and report results ona corpus of manually annotated German news docu-ments.After defining a measure for the comparison of(manually or automatically created) lexical chainsin Section 2, Section 3 describes our annotationmethodology and discusses issues regarding the in-herent subjectivity of lexical chain annotation.
InSection 4, three statistical approaches for lexicalchaining are developed on the basis of the LDA topicmodel.
Experiments that demonstrate the advantageof these approaches over a knowledge-baseline areconducted and evaluated in Section 5, and Section 6concludes and provides an outlook future directions.1.2 Previous Work on Lexical ChainsMorris and Hirst (1991) initially proposed an al-gorithm for lexical chaining based on Roget?s the-saurus (Roget, 1852), and manually assessed thequality of their algorithm.
Hirst and St-Onge (1998)first presented a computational approach to lexicalchaining based on WordNet showing that the lexi-cal database is a reasonable replacement to Roget?s.The basic idea behind these algorithms is that se-mantically close words should be connected to formchains.
Subsequent approaches mainly concentratedon disambiguation of words to WordNet concepts(WSD), since ambiguous words can lead to the over-generation of connections.
Barzilay and Elhadad(1997) improved the implicit word sense disam-biguation (WSD) by keeping a list of different inter-pretations of the text and finally choosing the mostplausible senses for chaining.
Silber and McCoy(2002) introduced an efficient variant of the algo-rithm with linear complexity in the number of can-didate terms.
Galley and McKeown (2003) furtherimproved accuracy by first performing WSD, andthen using the remaining links between the disam-biguated concepts only.
They also introduced a so-called disambiguation graph, a representation thathas also been utilized by the method of Medelyan(2007), where she applied a graph clustering algo-rithm to the disambiguation graph to cut weak links,performing implicit WSD.
A combination of statis-tical and knowledge-based methods is presented byMarathe and Hirst (2010), who combine distribu-tional co-occurrence information with semantic in-formation from a lexicographic resource for extract-ing lexical chains and evaluate them by text segmen-tation.
We are not aware of previous lexical chain-ing algorithms that do not rely on a lexicographicresource at all.A major issue in developing a new lexical chain-ing algorithm is the comparison to previous systems.Most of previous approaches are validated by theevaluation in a certain task like summarization, wordsense disambiguation, keyphrase extraction or infor-mation retrieval (Stairmand, 1996).
Hence, these ex-trinsic evaluations are heavily influenced by the par-ticular task at hand.
We propose to re-consider lexi-cal chaining as a task on its own, and propose objec-tive criteria for directly comparing lexical chains tothis end.2 Comparing Lexical ChainsThe comparison of lexical chains is a non-trivialtask.
We adopt the idea of interpreting lexical chainsas clusters and a particular set of lexical chains asa clustering, and develop a suitable cluster com-parison measure.
As stated by Meila?
(2005) andAmigo?
et al(2009), a best clustering comparisonmeasure for the general case does not exist.
It shouldbe stressed that the appropriate clustering measurehighly depends on the task at hand.After exploring a number of measures1, we de-cided on a combination of the adjusted Rand in-dex (ARI , Hubert and Arabie (1985)) and the basicmerge distance (BMD, Menestrina et al(2010))for our new measure.
Menestrina et al(2010) in-troduced a linear time algorithm for computing thegeneralized merge distance (GMD), which counts1Explored measures which are unsatisfactory for the giventask are: Closest Cluster F1 (Benjelloun et al 2009), K (Ajmeraet al 2002), Pairwise F1 (Manning et al 2008), Variation ofInformation (Meila?, 2005), B3(Bagga and Baldwin, 1998), V-Measure (Rosenberg and Hirschberg, 2007), Normalized Mu-tual Information (Strehl, 2002).
The last two measures areequal.
A proof of this can be found in the appendix.990split and merge cluster editing operations.
Using aconstant factor of 1 for both splits and merges givesthe basic merge distance (BMD): Considering >as the most general clustering of a dataset D, whereall elements are grouped into the same cluster, andfurther considering ?
as the most specific cluster-ing of D, where each element builds its own clus-ter, the lattice between > and ?
spans all possibleclusterings and the BMD can be interpreted as theshortest path from a clustering C to a clustering C ?in the lattice with some restrictions (see Menestrinaet al(2010) for details).
We normalize the BMDscore by the maximum BMD2 to the normalizedbasic merge distance (NBMD).
ARI is is basedon pair comparisons, and is computed as3:index = TPexpected index =(TP + FP )?
(TP + FN)TP + TN + FP + FNmax index = TP +12(FP + FN)ARI(C,C ?)
=index?
expected indexmax index?
expected indexThe reasons for choosing these two particularmeasures are the following: ARI is a well knownmeasure which is adjusted (corrected) for decisionsmade by chance.
But since it is based on pairwiseelement comparison it completely disregards single-ton clusters (chains) and some types of errors are notadequately penalized.
The NBMD on the other handpenalizes various errors almost equally.We combine the two single measures into anew lccm (lexical chain comparison measure), de-fined as the arithmetic mean between ARI and1?NBMD.
An lccm of 1 indicates perfect con-gruence and an lccm = 0 indicates that not a singlepair of items in C is found in a cluster together inC ?.lccm(C,C ?)
=12[1?NBMD(C,C ?)
+ARI(C,C ?)].2BMD(>,?)
for |D| ?
2, BMD(>,?)
+ 1 otherwise3TP : pairs in D and D?, FP : pairs in D?
but not in D, FN :pairs in D but not in D?, TN : pairs not in D and not in D?, whereD is the underlying dataset of C, D?
is the underlying dataset ofC?, and pairs means all unique combinations of elements thatare in the same cluster.3 Annotating Lexical ChainsA challenge with the annotation of lexical chains isthe subjective interpretation of the text by individ-ual annotators (Morris and Hirst, 2004), which alsosubstantiates the fact that currently no gold stan-dard exist, and all previous automatic approachesare evaluated by performing a certain NLP task.Hollingsworth and Teufel (2005) as well as Crameret al(2008) conclude from their lexical chain anno-tation projects that high inter-annotator agreement isvery hard to achieve.
We argue that directly evalu-ating on lexical chains should enable us to optimizetowards higher-quality chain annotations, which isa task of its own right and which has the potentialto improve all subsequent applications.
For this, wedevise an annotation scheme that gets us reasonableinter-annotator agreement, inspired by the conceptof cohesive harmony (Hasan, 1984), and report onan annotation project for German newswire texts.Documents from the SALSA 2.0 (Burchardt et al2006) corpus were chosen to form the basis for theannotation of lexical chain information.
SALSA isbased on the semi-automatically annotated TIGERTreebank 2.1 (Brants et al 2002).
The TIGERtreebank provides manual annotations, such as lem-mas, part-of-speech tags, and syntactic structure, theSALSA part of the corpus is also partially annotatedwith FrameNet-style (Baker et al 1998) frame an-notation.
The documents are general domain newsarticles from a German newspaper comprising about1,550 documents and around 50,000 sentences in to-tal, with a median document length of 275 tokens.3.1 Annotation SchemeIn order to minimize the subjectiveness of choicesby different annotators, annotation guidelines weredeveloped comprising a total of ten pages.
Wedecided to consider only nouns, noun compoundsand non-compositional adjective noun phrases like?dirty money?
as candidate terms for lexical chain-ing, which is consistent with the procedures ofHollingsworth and Teufel (2005) and Cramer et al(2008).
For annotation, we used the MMAX24(Mu?ller and Strube, 2006) tool.We introduce the term dense chain, which refersto a type of lexical chain in which every element is4http://mmax2.sourceforge.net991related to every other element in that chain.
Termsare considered to be related if they share the sametopic, i.e.
common sense and knowledge of the lan-guage is needed to decide which terms belong to-gether in the same topic and whether a chosen topicis neither too broad nor too narrow.
A single densechain can thus be assigned a definite topical descrip-tion of its items.
Whereas Hollingsworth and Teufel(2005) dealt with the inherent fuzziness of member-ship of terms to lexical chains by allowing termsto occur in different lexical chains, we follow theconcept of cohesive harmony introduced by Hasan(1984) here, where complete chains can be linkedto others.
For this purpose, we introduce so-calledlevel two links, which are cohesive ties between lex-ical items in distinct dense chains.
Having sucha link between two chains, both chains can be as-signed a topical description which is broader thanthe description of the individual chains.
This resultsin a two-level representation of chains.
We reporton dense lexical chains and merged lexical chains(dense chains are merged into a common chain if alevel two link exists between them) separately.In total, 100 documents were annotated by twoexpert annotators.
Documents were chosen aroundthe length median and consist of 248 ?
304 tokens.The two rightmost columns of Table 3 show thecharacteristics of the annotated data set.
It can beconcluded that there is a moderate to high agree-ment regarding the annotator selections of candidateterms, which is ensured by preselection of candidateterms by part-of-speech patterns.
A value of 81% inthe average agreement on lexical items (cf.
Figure 1)shows that even though the choice of lexical itemsis limited to nouns and adjective noun phrases only,the decision on candidate termhood is somewhat dif-ferent between the annotators, but compares favor-ably with previous findings of 63% average pairwiseagreement (Morris and Hirst, 2004).Figure 2 shows the annotator agreement on theindividual documents using the lccm (cf.
Sec.
2),sorted in the same way as in Figure 1.
In order to usethe level two link information the figure also showsa second agreement score, which was computed onmerged chains.The agreement scores of the assignment of lexicalitems to lexical chains depend partially on the agree-ment scores of the identified lexical items them-0600 2068 1069 0337 0364 0897 1323 0100 1388 0962 0545 1156 1190 0122 1194 1855 1808 0356 1779 2040 0219 1734 1216 0947 0937 2026 1548 1232 1826 1224 1251 1239 2044 1119 0174 1650 2014 0630 0199 0655 1818 0310 1150 1726 2035 0382 1639 0275 1047 1707 0228 1050 1607 0984 0755 0752 0747 1162 1018 0182 0942 0492 0675 0928 1399 1072 1310 0268 1033 1168 1523 1737 0524 0338 0976 0639 0299 1524 1387 0694 0278 1692 1534 1037 0499 0721 0283 0736 0258 1130 0988 0649 1777 0309 0110 1319 0729 0730 0390 0925Document ID020406080100%Figure 1: Agreement of lexical items annotated by anno-tator A and annotator B as a percentage of lexical itemsannotated by annotator A or annotator B.
The averageagreement is 81%.0600 2068 1069 0337 0364 0897 1323 0100 1388 0962 0545 1156 1190 0122 1194 1855 1808 0356 1779 2040 0219 1734 1216 0947 0937 2026 1548 1232 1826 1224 1251 1239 2044 1119 0174 1650 2014 0630 0199 0655 1818 0310 1150 1726 2035 0382 1639 0275 1047 1707 0228 1050 1607 0984 0755 0752 0747 1162 1018 0182 0942 0492 0675 0928 1399 1072 1310 0268 1033 1168 1523 1737 0524 0338 0976 0639 0299 1524 1387 0694 0278 1692 1534 1037 0499 0721 0283 0736 0258 1130 0988 0649 1777 0309 0110 1319 0729 0730 0390 0925Document ID0.00.20.40.60.81.0lccmFigure 2: Individual annotator agreement scores on 100documents sorted by their agreement on candidate terms.The red circles show the agreement of both annotators onthe dense lexical chains disregarding the cohesive links,and the green dots show the agreement of both annotatorson the merged lexical chains (via the cohesive links) bothusing the proposed lexical chain comparison measure.selves, which is a desired property.
Across all doc-uments, a perfect agreement was never achieved,which confirms the difficulty of annotating such asubjective task: The average lccm per document onthe manual annotations is 0.56 (dense chains), re-spectively 0.54 (merged chains).
However, the con-siderable overlap between the annotators still en-ables us to evaluate automatic chaining methods,and the lccm agreement score serves as an upperbound.
Note that by performing no reconciliationof the annotations we explicitly allow the possibil-ity of different interpretations which is in our opin-ion appropriate here due to the subjectiveness of thetask itself.
By doing so, we evaluate our algorithmsagainst individual annotator interpretations.4 Statistical Methods for Lexical ChainingThis work employs a well-studied statistical methodfor creating something that Barzilay (1997) calledan automatic thesaurus which will then be adaptedfor lexical chaining.
For our automatic approaches,candidate lexical items in a text are preselected bythe same heuristic that is also applied in Section 3for the annotation process.Topic models (TMs) are a suite of unsuper-992vised algorithms designed for unveiling some hid-den structure in large data collections.
The key ideais that documents can be represented as compos-ites of so-called topics where a topic itself repre-sents as a composite of words.
Hofmann (1999)defined a topic to be a probability distribution overwords and a document to be a probability distribu-tion over a fixed set of topics.
We use the latentDirichlet alcation (LDA, Blei et al(2003)) topicmodel for estimating the semantic closeness of can-didate terms, and explore different ways of utilizingLDA?s topic information in automatic lexical chain-ers.
Specifically, we use the GibbsLDA++5 frame-work for topic model estimation and inference, andexamine the following LDA parameters: number oftopics T , Dirichlet hyperparameters for document-topic distribution ?
and topic-term distribution ?.We now describe three LDA-based approaches tolexical chaining.4.1 LDA Mode Method (LDA-MM)The LDA-MM approach places all word tokens thatshare the same topic ID into the same chain.
Thepoint is now how to decide to which topic a wordbelongs to.
Since single samples of topics per wordexhibit a large variance (Riedl and Biemann, 2012),we follow these authors by sampling several timesand using the mode (most frequently assigned) topicID per word as the topic assignment.
This strategyreduced the variance in the lccm to a tenth6.More formally, let samples(d,w) be the vectorof assignments that have been collected for a cer-tain word w in a certain document d with eachsamples(d,w)i referring to the i-th sampled topic IDfor (d,w).
In other words, samples(d,w) can be seenas the Markov chain for a particular word in a par-ticular document.
Further let z(d,w) be the topic IDthat was most assigned to the word w with respectto the samples in samples(d,w).
Precisely, z(d,w) isdefined to be the sampled mode in samples(d,w) ?in case of multiple modes a random mode is chosen,5http://gibbslda.sourceforge.net6Preliminary experiments yielded a variance of 2.6 ?
10?6in lccm using the mode method and 3.07?10?5 using a singlesample for lexical chain assignment.which never happened in our experiments.z(d,w) = mode (samples(d,w))?
argmaxj(P (z = j|w, d))The LDA-MM assigns for every word w whichis a candidate lexical item of a certain document dwhich is assigned the same topic z(d,w) to the samechain; hence implicitly disambiguating the terms.The possibility to create level two links is givenby taking the second most occurring topic for a givenword if it exceeds a certain threshold.4.2 LDA Graph Method (LDA-GM)The LDA-GM algorithm creates a similarity graphbased on the comparison of topic distributions forgiven words and then applies a clustering algorithmin order to find semantically related words.Let ?
(d,w) be the per-word topic distributionP (z|w, d).
Analogously to the LDA-MM, ?
(d,w)can be obtained by counting the occurrences ofa certain topic ID z in the sample collectionsamples(d,w) for a particular word w and documentd.The semantic relatedness between any two wordswi and wj can then be measured by their similarityscore of the topic distributions ?
(d,wi) and ?
(d,wj),which is stored in a term similarity matrix.
Thismatrix can also be interpreted as an adjacency ma-trix of a graph, with candidate items being nodesand edges being weighted with the similarity valuesimij for any two nodes i, j : i 6= j ?
i, j ?
{1, 2, .
.
.
, Nd}.
We test two similarity measures:Euclidian (dis-)similarity and cosine similarity.Let G = (V,E) be the graph represen-tation of a document with term verticesV = {v1, .
.
.
, vNd} and weighted edges E ={(v1, v2, sim12), .
.
.
(vNd, vNd?1, simNdNd?1)},where simij is either the cosine or Euclideansimilarity of term vectors.
For simplicity, we reducethis representation to an unweighted graph byonly retaining edges (of unit weight) that have asimilarity above a parameter threshold sim.
Toidentify chains as clusters in this graph, we followMedelyan (2007) and apply the Chinese Whispersgraph clustering algorithm (CW, Biemann (2006)),which finds the number of clusters automatically.The CW algorithm implementation comes with993three parameters to regulate the node weight basedon its degree, which influences cluster size andgranularity.
We test options ?top?, ?dist log?
and?dist lin?.The final chaining procedure is straightforward:The LDA-GM algorithm assigns every candidatelexical item wi of a certain document d which isassigned the same class label ci to the same chain.Level two links are drawn using the second domi-nant class of a vertex?s neighborhood, which is pro-vided by the CW implementation.4.3 LDA Top-N Method (LDA-TM)The LDA-TM method is different to the others inthat it uses the information of the per-topic word dis-tribution ?
(z) = P (w|z) and the per-document topicdistribution ?
(d) = P (z|d).
Given a parameter n re-ferring to the top n topics to choose from ?
(d) and aparameter m referring to the top m words to choosefrom ?
(z) the main procedure can be described asfollows: for all z ?
top n topics in ?
(d): chain thetop m words in ?
(z) .Note that although the number of chains and chainmembers for each chain is bound and could lead tothe same number and sizes of chains, in practice thenumber of generated chains as well as the number ofchain members still varies considerably across doc-uments: often some of the top m words for a (glob-ally computed) topic do not even occur in a partic-ular document.
This implies that the parameters nand m must not be set globally but dependent onthe particular document.
To overcome this to someextent, additional thresholding parameters ?
and ?are used for further bounding the respective n or mparameter.
The procedure works like this: for all z?
top n topics in ?
(d) ?
?
(d)z < ?
: chain the top mwords w in ?
(z) ?
?
(z)w < ?.Level two links are created by computing the co-sine similarity between every pair of the top n topicdistributions, and thresholding with a link parame-ter.4.4 Repetition HeuristicAll methods described above can be applied to newunseen documents that are not in the training set.
Toalleviate a possible vocabulary mismatch betweentraining set and test set, which happens when termsin the test set have not been contained in our trainingdocuments, we add a heuristic that chains repetitionsof (previously unknown) words as a post-processingstep to all methods.5 Empirical AnalysisIn order to provide a realistic estimate of the qual-ity of our methods to unseen material, we randomlysplit our annotated documents in two parts of 50documents each.
One part is used as a developmentset for optimizing the parameters of the methods (i.e.model selection), the other part forms our test set forevaluation.The training corpus, on the other hand, consistsof all 1,211 SALSA/Tiger documents that are notpart of the development and test corpus and nei-ther very long nor very short.
These documentsare taken from the German newspaper ?FrankfurterRundschau?
around 1992.
Additionally the trainingcorpus is enriched with 12,264 news texts from thesame newspaper around 1997 with similar charac-teristics7, making up a total of 13,457 training doc-uments for the estimation of topic models.Input to the LDA model training are verbs, nounsand adjectives, as well as candidate terms as de-scribed in Section 3.1, all in their lemmatized form.We further filter words that occur in more than 1/3of the training documents, as well as known stop-words, and words that occur in less than two doc-uments which results in a vocabulary size of about100K words.5.1 Experimental SetupFor comparison, we implemented three baselines,which we describe below.
One baseline is trivial,two baselines are state-of-the art knowledge-basedsystems adapted to German.Random: Candidate lexical items are randomlytied together to form sets of lexical chains.Level two links are created analogously.
Weregulate the process to yield the same averagenumber of chains and links as in the develop-ment and test data.S&M GermaNet: Algorithm by Silber and McCoy(2002) with GermaNet as its knowledge re-source.7as provided by Projekt Deutscher Wortschatz,http://wortschatz.uni-leipzig.de/994G&M GermaNet: Algorithm by Galley and McK-eown (2003), also using GermaNet.GermaNet (Hamp and Feldweg, 1997) is a largeWordNet-like resource for German, containing al-most 100,000 lexical units and over 87,000 concep-tual relations between synsets.
While its size is onlyabout half of WordNet, it is one of the largest non-English lexical semantic resources.5.2 Model SelectionWe optimize two sets of parameters: parameters forthe LDA topic model (number of topics K, Dirich-let hyperparameters ?
and ?)
are optimized for theLDA-MM method only, and the same LDA modelis used in the other two LDA-based methods.
Pa-rameters particular to the respective method are op-timized individually.
For LDA, we tested sensiblecombinations in the ranges K = 50..1000, ?
=0.05/K..50/K and ?
= 0.001..0.1.
The highestperformance of the LDA-MM method was found forK = 500, ?
= 50/K, ?
= 0.001, and the result-ing topic model is used across all methods.
The finalparameter values for the other methods, found by ex-haustive search, are summarized in Table 1.Method ParameterLDA-GM similarityfunction = cosine similaritylabelweightscheme = dist logsim = 0.95LDA-TM n = 10, m = 20, ?
= 0.2, ?
= 0.2Table 1: Final parameter values.5.3 EvaluationFor evaluation purposes, terms that consist of multi-ple words are mapped to its rightmost term whichis assumed to be the head, e.g.
?dirty money?
ismapped to ?money?.
Additionally, singleton chains,i.e.
chains that contain only a single lexical itemare omitted unless the respective lexical item is notlinked by a level two link.Dense Chains Comparative results of the ap-proaches in terms of lccm for both annotators aresummarized in Table 2 (upper half).
We observethat all our new methods beat the random baselineand the two knowledge-based baselines by a largemargin.
The knowledge-based baselines, both usingAnno A Anno B AverageLDA-MM 0.320 0.306 0.313LDA-TM 0.307 0.299 0.303LDA-GM 0.328 0.314 0.321G&M 0.255 0.215 0.235S&M 0.248 0.209 0.229Random 0.126 0.145 0.135LDA-MM 0.316 0.300 0.308LDA-TM 0.303 0.280 0.291LDA-GM 0.279 0.267 0.273G&M 0.184 0.166 0.176S&M 0.179 0.159 0.169Random 0.196 0.205 0.201Table 2: Results of the evaluation based on dense chains(upper half) and merged chains (lower half).
The annota-tor agreement on the test set?s chains = 0.585; on mergedchains = 0.553GermaNet, produce very similar lccm scores, whichhighlights the important role of the knowledge re-source.
Data analysis revealed that while chains pro-duced by knowledge-based baselines are sensible,the main problem is a lack of coverage in terms ofvocabulary and relations in GermaNet.
Comparingthe statistical methods, the LDA-GM method excelsover the others.Level Two Links Table 2 (lower half) summarizesthe evaluation results of the merged chains via leveltwo links.
Because of merging, a text now containsfewer chains with more lexical items each.
Note thatknowledge-based baselines do not construct leveltwo links, which is why they are heavily penalizedin this setup.Again, the statistical methods beat the baselinesby a substantial amount.
In this evaluation, the ran-dom baseline performs above the knowledge-basedmethods, which is rooted in the fact that lccm penal-izes small, correct chains, whereas the random base-line with linking often produces very large chainscontaining most of the terms ?
something that wealso observe for many manually annotated docu-ments.
The large overlap in the biggest chain thenleads to the comparatively high random baselinescore.
In this evaluation, the LDA-MM is the clearwinner, with LDA-GM being clearly inferior thistime.995LDA-MM LDA-GM LDA-TM S&M G&M Anno A Anno Bavg.
num.
of lexical items per doc.
38.20 29.32 30.82 14.40 15.29 38.66 38.96avg.
num.
of chains per doc.
13.80 9.12 7.32 5.83 5.71 11.25 7.38avg.
num.
of links per doc.
8.60 2.06 1.44 ?
?
5.47 2.41avg.
size lexical chains 2.82 3.41 4.61 2.48 2.68 3.69 5.57avg.
num.
of merged lexical chains 5.76 7.06 5.98 ?
?
6.10 4.99avg.
size merged lexical chains 8.29 4.45 5.57 ?
?
7.60 8.91Table 3: Quantitiative characteristics of automatic and manual lexical chains.
In average, a document contains 51.58candidate terms as extracted by our noun phrase patternsDavud Bouchehri,[Davud Bouchehri,]seit[since]der[the]letzten[last]Spielzeit[playing period]als[as]Dramaturg[dramaturg]in[in]Basel[Basle]ta?tig,[acting,]wechselt[switches]zur[to the]Saison 1996 / 97[1996 / 97 season]als[as]ku?nstlerischer[art]Gescha?ftsfu?hrer[director]des[of the]Schauspiels[play]an[to]das[the]Staatstheater[state theater]Darmstadt.[Darmstadt.
]Der[The]aus[from]dem[the]Iran[Iran]stammende[coming]34ja?hrige[34-year-old]soll[shall]daneben[besides]auch[also]fu?r[for]spartenu?bergreifende[multi discipline]Projekte[projects]zusta?ndig[responsible]sein,[be,]teilte[aquainted]das[the]Basler[Basle?s]Theater[theather]am[on]Donnerstag[Thursday]mit.[with.
]LDA-MM:c1: {Spielzeit, Schauspiels, Staatstheater}c2: {Dramaturg, Theater}c3: {Saison}l1: (Theater?
Spielzeit)l2: (Spielzeit?
Saison)LDA-GM:c1: {Dramaturg, Theater}c2: {Schauspiels, Staatstheater}LDA-TM:c1: {Schauspiels, Staatstheater, Theater}c2: {Dramaturg}c3: {Spielzeit, Saison}l1: (Theater?
Dramaturg)S&M-GermaNet:?G&M-GermaNet:c1: {Staatstheater, Theater}Figure 3: Diverse output of the various lexical chaining systems after applying them on a short German example textfrom the used TIGER/SALSA corpus.
For a better understanding the text is calqued.
Candidate items are highlightedand the ci are the resulting dense lexical chains and the li are the level two links produced by the various methods.Data Analysis Table 3 shows quantitative num-bers of the extracted lexical chains in the test set.The LDA-MM approach chains and links a lotmore items than the other statistical methods: it cre-ates a lot more links between items that would oth-erwise be removed because they form unlinked sin-gleton chains.
As opposed to this, the graph method(LDA-GM), as well as the top-n method (LDA-TM)perform an implicit filtering on the candidate lexi-cal items by creating less level two links, yet largerdense chains.
The knowledge based algorithms bySilber and McCoy (2002) and Galley and McKeown(2003) extract fewer and smaller chains than the sta-tistical approaches, which reflects GermaNet?s spar-sity issues.
While higher lexical coverage in theunderlying resource would increase the coverage ofour knowledge-based systems, this is only one partof the story.
The other part is rooted in the factthat lexical cohesion relations, which are used inlexical chains, encompass many more semantic re-lations than listed in today?s lexical semantic net-works.
This especially holds for cases where sev-eral expressions refer to the same event or theme forwhich no well-defined relation exists, such as e.g.?captain?
and ?harbor?.Comparing the three LDA-based approaches, nooverall best method could be determined.
the LDA-MM seems especially suited for a high coverageand coarse (level two) chains, the LDA-GM appearsmost suited for dense chains, and LDA-TM pro-duces the longest chains on average.Figure 3 shows the resulting dense lexical chainsand level two links after applying our chainers to ashort example text from our corpus.
In the exam-ple the LDA-TM produces the most adequate lexi-cal chains, at least in our intuition.
The LDA-GMand the LDA-MM produce slightly wrong chains,yet the LDA-MM additionally creates some mean-ingfull level two links which the LDA-GM does not.Both knowledge-based approaches perform poorlycompared to the knowledge-free approaches, wherethe S&M algorithm creates no chains at all and the996G&M algorithm produces only a single chain con-taining only two words.
This is mostly due to Ger-maNet?s lacking lexical and relational coverage andthe scope of the algorithms for finding relations be-tween the words.6 ConclusionIn this paper, we presented experiments for auto-matic lexical chain annotation and evaluated themdirectly on a manually annotated dataset for Ger-man.
A new two-level annotation scheme for lexi-cal chains was proposed and motivated by the con-cept of cohesive harmony.
We further proposed anew measure for comparing lexical chain annota-tions that is especially suited for the characteristicsof lexical chain annotations.
Three variants of sta-tistical lexical chaining methods based on the LDAtopic model were proposed and evaluated againsttwo knowledge-based baseline systems.
Our sta-tistical methods exhibit a substantially higher per-formance than the knowledge-based systems on ourdataset.
This can partially be attributed to miss-ing relations, partially to the lack of lexical cov-erage of GermaNet, which was used in these sys-tems.
Since GermaNet is a large lexical-semanticnet, however, this strengthens our main point: Espe-cially for under-resourced languages or subject do-mains, statistical and data-driven methods should bepreferred over their knowledge-based counterparts,since they do not require the development of lexical-semantic nets and adopt easily to subject domains bytraining their unsupervised models on an in-domaincollection.In future work, we would like to explore betterways of selecting candidate items.
While our POS-pattern-based selection mechanism works for practi-cal purposes, it currently only extracts noun phrasesand over-generates on compositional adjective mod-ifiers.
We would like to define a better filter to re-duce over-generation.
Further, especially for com-pounding languages such as German, we would liketo decompose one-word compounds as to be able tolink their heads in lexical chains.While we found it important to directly evalu-ate our lexical chaining algorithms on manually an-notated data, a natural next step in this line of re-search is to use our lexical chaining methods aspre-processing steps for applications such as sum-marization, text segmentation or word sense disam-biguation.
This would enable to find out advantagesand disadvantages of our three variants with respectto an application.The manually annotated data, the open source an-notation tool, the annotation guidelines and the im-plementations of all described methods and base-lines are available for download8.AcknowledgmentsThis work has been supported by the Hessian re-search excellence program Landes-Offensive zurEntwicklung Wissenschaftlich-o?konomischer Exzel-lenz (LOEWE) as part of the research center DigitalHumanities.Proof: Equality of NMI and VUsing the standard notation from information retrievalH(X)= Entropy,I(X,Y )= Information, H(X|Y )= Conditional Entropy, NMI(X,Y)=Normalized Mutual Information, V(X,Y)= V-Measure:V (C,K) = 2?h?
ch+ c(1)h = 1?H(C|K)H(C), c = 1?H(K|C)H(K)(2)andNMI(C,K) =I(C,K)H(C)+H(K)2= 2?I(C,K)H(C) +H(K)(3)reformulate h and c using the fact that I(C,K) = H(C) ?H(C|K) = H(K)?H(K|C):h = 1?H(C|K)H(C)=H(C)H(C)?H(C|K)H(C)=I(C,K)H(C)(4)c = 1?H(K|C)H(K)=H(K)H(K)?H(K|C)H(K)=I(C,K)H(K)(5)simplifying h?
c using (4) and (5):h?
c =I(C,K)H(C)?I(C,K)H(K)=I(C,K)2H(C)H(K)(6)simplifying h+ c using (4) and (5):h+ c =I(C,K)H(C)+I(C,K)H(K)=I(C,K)H(K) + I(C,K)H(C)H(C)H(K)=I(C,K)[(H(K) +H(C)]H(C)H(K)(7)simplifying h?ch+c using (6) and (7):h?
ch+ c=I(C,K)2H(C)H(K)?H(C)H(K)I(C,K)[H(K) +H(C)]=I(C,K)H(K) +H(C)(8)8http://www.ukp.tu-darmstadt.de/data/lexical-chains-for-german/997substituting (8) into (1) shows that NMI and V are equal:V (C,K) = 2?h?
ch+ c= 2?I(C,K)H(K) +H(C)= NMI(C,K) (9)ReferencesJitendra Ajmera, Herve?
Bourlard, and I. Lapidot.
2002.Unknown-Multiple Speaker clustering using HMM.In Proceedings of the International Conference of Spo-ken Language Processing, ICSLP ?02, Denver, Col-orado, USA.Enrique Amigo?, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A comparison of extrinsic clusteringevaluation metrics based on formal constraints.
Infor-mation Retrieval, 12:461?486.Amit Bagga and Breck Baldwin.
1998.
Algorithms forScoring Coreference Chains.
In Proceedings of theLinguistic Coreference Workshop at The First Interna-tional Conference on Language Resources and Evalu-ation, LREC ?98, pages 563?566, Granada, Spain.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In COLING?98: Proceedings of the 17th International Conferenceon Computational Linguistics, volume 1, pages 86?90,Montreal, Quebec, Canada.Regina Barzilay and Michael Elhadad.
1997.
Using Lex-ical Chains for Text Summarization.
In Proceedings ofthe ACL Workshop on Intelligent Scalable Text Sum-marization, pages 10?17, Madrid, Spain.Regina Barzilay.
1997.
Lexical Chains for Summariza-tion.
Master?s thesis, Ben-Gurion University of theNegev, Beersheva, Israel.Omar Benjelloun, Hector Garcia-Molina, David Men-estrina, Qi Su, Steven Whang, and Jennifer Widom.2009.
Swoosh: a generic approach to entity resolu-tion.
The VLDB Journal, 18:255?276.Chris Biemann.
2006.
Chinese Whispers ?
an Effi-cient Graph Clustering Algorithm and its Applicationto Natural Language Processing.
In Proceedings ofTextGraphs: the Second Workshop on Graph BasedMethods for Natural Language Processing, pages 73?80, New York City, USA.Chris Biemann.
2012.
Structure Discovery in NaturalLanguage.
Theory and Applications of Natural Lan-guage Processing.
Springer Berlin / Heidelberg.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A Topic Model for Word Sense Disambigua-tion.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 1024?1033, Prague, CzechRepublic.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
TIGER Treebank.In Proceedings of the Workshop on Treebanks and Lin-guistic Theories (TLT02), Sozopol, Bulgaria.Aljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA Corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th interna-tional conference on Language Resources and evalua-tion (LREC-2006), Genoa, Italy.Junfu Cai, Wee Sun Lee, and Yee Whye Teh.
2007.Improving Word Sense Disambiguation Using TopicFeatures.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 1015?1023, Prague, CzechRepublic.Joe Carthy.
2004.
Lexical Chains versus Keywordsfor Topic Tracking.
In A. Gelbukh, editor, Compu-tational Linguistics and Intelligent Text Processing,volume 2945 of Lecture Notes in Computer Science,pages 507?510.
Springer, Berlin / Heidelberg.Irene Cramer, Marc Finthammer, Alexander Kurek,Lukas Sowa, Melina Wachtling, and Tobias Claas.2008.
Experiments on Lexical Chaining for Ger-man Corpora: Annotation, Extraction, and Applica-tion.
Journal for Language Technology and Computa-tional Linguistics (JLCL), 23(2):34?48.Georgiana Dinu and Mirella Lapata.
2010.
Topic Mod-els for Meaning Similarity in Context.
In COLING?10: Proceedings of the 23rd International Conferenceon Computational Linguistics: Posters, pages 250?258, Beijing, China.Michel Galley and Kathleen McKeown.
2003.
Im-proving word sense disambiguation in lexical chain-ing.
In IJCAI?03: Proceedings of the 18th interna-tional joint conference on Artificial intelligence, pages1486?1488, Acapulco, Mexico.Yihong Gong and Xin Liu.
2001.
Generic Text Sum-marization Using Relevance Measure and Latent Se-mantic Analysis.
In SIGIR 2001: Proceedings of the24th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 19?25, New Orleans, Louisiana, USA.Stephen J.
Green.
1996.
Using Lexical Chains to BuildHypertext Links in Newspaper Articles.
In AAAI-96 Workshop on Internet-based Information Systems,pages 115?141, Portland, Oregon, USA.Michael A. K. Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
English language series.
Longman,London.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet- a Lexical-Semantic Net for German.
In Proceed-998ings of the ACL/EACL-97 workshop Automatic Infor-mation Extraction and Building of Lexical SemanticResources for NLP Applications, Madrid, Spain.Ruqaiya Hasan.
1984.
Coherence and Cohesive Har-mony.
In James Flood, editor, Understanding ReadingComprehension, Cognition, Language, and the Struc-ture of Prose, pages 181?220.
International ReadingAssociation, Newark, Delaware, USA.Leonhard Hennig.
2009.
Topic-based multi-documentsummarization with probabilistic latent semantic anal-ysis.
In Proceedings of the International ConferenceRANLP-2009, pages 144?149, Borovets, Bulgaria.Graeme Hirst and David St-Onge.
1998.
Lexical Chainsas representation of context for the detection and cor-rection malapropisms.
In Christiane Fellbaum, edi-tor, WordNet: An Electronic Lexical Database, Lan-guage, Speech, and Communication, pages 305?332.The MIT Press, Cambridge, Massachusetts, USA.Thomas Hofmann.
1999.
Probabilistic Latent Seman-tic Analysis.
In Proceedings of the Fifteenth Confer-ence on Uncertainty in Artificial Intelligence, UAI ?99,pages 289?296, Stockholm, Sweden.William Hollingsworth and Simone Teufel.
2005.
Hu-man annotation of lexical chains: Coverage and agree-ment measures.
In Proceedings of the WorkshopELECTRA: Methodologies and Evaluation of LexicalCohesion Techniques in Real-world Applications, InAssociation with SIGIR ?05, Salvador, Brazil.Lawrence Hubert and Phipps Arabie.
1985.
Comparingpartitions.
Journal of Classification, 2(1):193?218.Christopher Manning, Prabhakar Raghavan, and HinrichSchu?tze.
2008.
An Introduction to Information Re-trieval.
Cambridge University Press, Cambridge, UK.Meghana Marathe and Graeme Hirst.
2010.
LexicalChains Using Distributional Measures of Concept D.In Proceedings of the 11th International Conferenceon Computational Linguistics and Intelligent Text Pro-cessing, CICLing?10, pages 291?302, Ias?i, Romania.Olena Medelyan.
2007.
Computing lexical chains withgraph clustering.
In Proceedings of the 45th AnnualMeeting of the ACL: Student Research Workshop, ACL?07, pages 85?90, Prague, Czech Republic.Marina Meila?.
2005.
Comparing clusterings: an ax-iomatic view.
In Proceedings of the 22nd Interna-tional Conference on Machine Learning, ICML ?05,pages 577?584, Bonn, Germany.David Menestrina, Steven Euijong Whang, and Hec-tor Garcia-Molina.
2010.
Evaluating entity resolu-tion results.
Proceedings of the VLDB Endowment,3(1):208?219.Hemant Misra, Franc?ois Yvon, Joemon Jose, and OlivierCappe?.
2009.
Text Segmentation via Topic Mod-eling: An Analytical Study.
In Proceedings of the18th ACM Conference on Information and KnowledgeManagement, CIKM 2009, pages 1553?1556, HongKong, China.Jane Morris and Graeme Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indicator of thestructure of text.
Computational Linguistics, 17:21?48.Jane Morris and Graeme Hirst.
2004.
The Subjectivity ofLexical Cohesion in Text.
In Proceedings of the AAAISpring Symposium on Exploring Attitude and Affect inText: Theories and Applications, Palo Alto, California,USA.Christoph Mu?ller and Michael Strube.
2006.
Multi-levelannotation of linguistic data with MMAX2.
In SabineBraun, Kurt Kohn, and Joybrato Mukherjee, editors,Corpus Technology and Language Pedagogy: New Re-sources, New Tools, New Methods, pages 197?214.
Pe-ter Lang, Frankfurt a.M., Germany.Manabu Okumura and Takeo Honda.
1994.
Word sensedisambiguation and text segmentation based on lexicalcohesion.
In COLING ?94: Proceedings of the 15thConference on Computational Linguistics, volume 2,pages 755?761, Kyoto, Japan.Martin Riedl and Chris Biemann.
2012.
Sweepingthrough the Topic Space: Bad luck?
Roll again!
InROBUS-UNSUP 2012: Joint Workshop on Unsuper-vised and Semi-Supervised Learning in NLP held inconjunction with EACL 2012, pages 19?27, Avignon,France.Peter Mark Roget.
1852.
Roget?s Thesaurus of EnglishWords and Phrases.
Longman Group Ltd., Harlow,UK.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 410?420,Prague, Czech Republic.H.
Gregory Silber and Kathleen F. McCoy.
2002.
Effi-ciently computed lexical chains as an intermediate rep-resentation for automatic text summarization.
Compu-tational Linguistics, 28(4):487?496.Mark A. Stairmand.
1996.
A Computational Analysisof Lexical Cohesion with Applications in InformationRetrieval.
Ph.D. thesis, Center for Computational Lin-guistics, UMIST, Manchester.Nicola Stokes, Joe Carthy, and Alan F. Smeaton.
2004.SeLeCT: A Lexical Cohesion Based News Story Seg-mentation System.
AI Communications, 17(1):3?12.Alexander Strehl.
2002.
Relationship-based Cluster-ing and Cluster Ensembles for High-dimensional DataMining.
Ph.D. thesis, University of Texas, Austin.999
