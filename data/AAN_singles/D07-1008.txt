Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
73?82, Prague, June 2007. c?2007 Association for Computational LinguisticsLarge Margin Synchronous Generationand its Application to Sentence CompressionTrevor Cohn and Mirella LapataSchool of InformaticsUniversity of EdinburghEdinburgh, United Kingdom{tcohn,mlap}@inf.ed.ac.ukAbstractThis paper presents a tree-to-tree transduc-tion method for text rewriting.
Our modelis based on synchronous tree substitutiongrammar, a formalism that allows local dis-tortion of the tree topology and can thusnaturally capture structural mismatches.
Wedescribe an algorithm for decoding in thisframework and show how the model canbe trained discriminatively within a largemargin framework.
Experimental results onsentence compression bring significant im-provements over a state-of-the-art model.1 IntroductionRecent years have witnessed increasing interest intext-to-text generation methods for many naturallanguage processing applications ranging from textsummarisation to question answering and machinetranslation.
At the heart of these methods lies theability to perform rewriting operations according toa set of prespecified constraints.
For example, textsimplification identifies which phrases or sentencesin a document will pose reading difficulty for a givenuser and substitutes them with simpler alternatives(Carroll et al, 1999).
Sentence compression pro-duces a summary of a single sentence that retains themost important information while remaining gram-matical (Jing, 2000).Ideally, we would like a text-to-text rewriting sys-tem that is not application specific.
Given a parallelcorpus of training examples, we should be able tolearn rewrite rules and how to combine them in orderto generate new text.
A great deal of previous workhas focused on the rule induction problem (BarzilayandMcKeown, 2001; Pang et al, 2003; Lin and Pan-tel, 2001; Shinyama et al, 2002), whereas relativelylittle emphasis has been placed on the actual gen-eration task (Quirk et al, 2004).
A notable excep-tion is sentence compression for which end-to-endrewriting systems are commonly developed (Knightand Marcu, 2002; Turner and Charniak, 2005; Gal-ley and McKeown, 2007; Riezler et al, 2003; Mc-Donald, 2006).
The appeal of this task lies in itssimplified formulation as a single rewrite operation,namely word deletion (Knight and Marcu, 2002).Solutions to the compression task have been castmostly in a supervised learning setting (but seeClarke and Lapata (2006a), Hori and Furui (2004),and Turner and Charniak (2005) for unsupervisedmethods).
Rewrite rules are learnt from a parsedparallel corpus and subsequently used to find thebest compression from the set of all possible com-pressions for a given sentence.
A common assump-tion is that the tree structures representing long sen-tences and their compressions are isomorphic.
Con-sequently, the models are not generally applicableto other text rewriting problems since they cannotreadily handle structural mismatches and more com-plex rewriting operations such as substitutions orinsertions.
A related issue is that the tree structureof the compressed sentences is often poor; most al-gorithms delete words or constituents without pay-ing too much attention to the structure of the com-pressed sentence.
However, without an explicit gen-eration mechanism that allows tree transformations,there is no guarantee that the compressions will havewell-formed syntactic structures.
And it will not beeasy to process them for subsequent generation oranalysis tasks.In this paper we present a text-to-text rewriting73model that scales to non-isomorphic cases and canthus naturally account for structural and lexical di-vergences.
Our approach is inspired by synchronoustree substitution grammar (STSG, Eisner (2003))a formalism that allows local distortion of the treetopology.
We show how such a grammar can be in-duced from a parallel corpus and propose a largemargin model for the rewriting task which can beviewed as a weighted tree-to-tree transducer.
Ourlearning framework makes use of the algorithm putforward by Tsochantaridis et al (2005) which ef-ficiently learns a prediction function to minimise agiven loss function.
Experiments on sentence com-pression show significant improvements over thestate-of-the-art.
Beyond sentence compression andrelated text-to-text generation problems (e.g., para-phrasing), our model is generally applicable to tasksinvolving structural mapping.
Examples include ma-chine translation (Eisner, 2003) or semantic parsing(Zettlemoyer and Collins, 2005).2 Related WorkKnight and Marcu (2002) proposed a noisy-channelformulation of sentence compression based on syn-chronous context-free grammar (SCFG).
The lat-ter is a generalisation of the context-free grammar(CFG) formalism to simultaneously produce stringsin two languages.
In the case of sentence compres-sion, the grammar rules have two right hand sides,one corresponding to the source (long) sentence andthe other to its target compression.
The synchronousderivations are learnt from a parallel corpus and theirprobabilities are estimated generatively.Given a long sentence, l, the aim is to find thecorresponding compressed sentence, s, which max-imises P(s)P(l|s) (here P(s) is the source modeland P(l|s) the channel model.)
Modifications of thismodel are reported in Turner and Charniak (2005)and Galley and McKeown (2007) with improved re-sults.
The channel model is limited to tree deletionand does not allow any type of tree re-organisation.Non-isomorphic tree structures are common whentranslating between languages.
It is therefore notsurprising that most previous work on tree rewrit-ing falls within the realm of machine translation.Proposals include Eisner?s (2003) synchronous treesubstitution grammar (STSG), Melamed?s (2004)multitext grammar, and Graehl and Knight?s (2004)tree-to-tree transducers.
Despite differences in for-malism, all these approaches model the translationprocess using tree-based probabilistic transductionrules.
The grammar induction process requires EMtraining which can be computationally expensive es-pecially if all synchronous rules are considered.Our work formulates sentence compression in theframework of STSG (Eisner, 2003).
We propose anovel grammar induction algorithm that does notrequire EM training and is coupled with a sepa-rate large margin training process (Tsochantaridiset al, 2005) for weighting each rule.
McDonald(2006) also presents a sentence compression modelthat uses a discriminative large margin algorithm.However, we differ in two important respects.
First,our generation algorithm is more powerful, perform-ing complex tree transformations, whereas McDon-ald only considers simple word deletion.
Being tree-based, the generation algorithm is better able to pre-serve the grammaticality of the compressed output.Second, our model can be tuned to a wider range ofloss functions (e.g.,tree-based measures).3 Problem FormulationWe formulate sentence compression as an instanceof the general problem of learning a mapping frominput patterns x ?
X to discrete structured objectsy ?
Y .
Our training sample consists of a parallelcorpus of input (uncompressed) and output (com-pressed) pairs (x1,y1) .
.
.
(xn,yn) ?
X ?
Y and ourtask is to predict a target labelled tree y from asource labelled tree x.
As we describe below, y isnot precisely a target tree, but instead derivationswhich generate both the source and the target tree.We model the dependency between x and y as aweighted STSG.
Grammar rules are of the form?X ,Y ?
?
??,?,??
where ?
and ?
are elementarytrees composed of a mixture of terminal and non-terminals rooted with non-terminals X andY respec-tively, and ?
is a set of variable correspondencesbetween pairs of frontier non-terminals in ?
and ?.A grammar rule specifies that we can substitute thetrees ?
and ?
for corresponding X and Y nodes in thesource and target trees respectively.
For example, therule:?NP, NP?
?
?
[DT 1 ADJP NN 2 ]NP, [DT 1 NN 2 ]NP?74allows adjective phrases to be dropped from thesource tree within an NP.
The indices x are used tospecify the variable correspondences, ?.Each grammar rule has a score from which theoverall score of a compression y for sentence xcan be derived.
These scores are learnt discrimina-tively using the large margin technique proposed byTsochantaridis et al (2005).
The synchronous rulesare combined using a chart-based parsing algorithm(Eisner, 2003) to generate the derivation (i.e., com-pressed tree) with the highest score.We begin by describing our STSG generation al-gorithm in Section 3.1.
We next explain how a syn-chronous grammar is induced from a parallel corpusof original sentences and their compressions (Sec-tion 3.2) and give the details of our learning frame-work (Section 3.3).3.1 GenerationGeneration aims to find the best target tree for agiven source tree using the transformations specifiedby the synchronous grammar.
(We discuss how weobtain this grammar in the following section.)y?
=maxy?Yscore(x,y;w) (1)where y ranges over all target derivations (and there-fore trees), w is a parameter vector and score(?)
isan objective function measuring the quality of thederivation.
In common with many parsing methods,we encounter a problem with spurious ambiguity:i.e., there may be many derivations (sequences ofrule applications) which produce the same targettree.
Ideally we would sum up the scores over allthese derivations, however for the sake of tractabilitywe instead take the maximum score.
This allows usto pose the maximisation problem over derivationsrather than target trees.The generation algorithm uses a dynamic pro-gram defined over the constituents in the sourcetree as shown in Figure 1 (see also Eisner (2003)).The algorithm makes the assumption that the scor-ing function decomposes with the derivation, suchthat a partial score can be evaluated at each step,i.e., score(x,y;w) = ?r?y score(r;w) where r arethe rules used in the derivation.
This method buildsa chart of the best scoring partial derivation foreach source subtree headed by a given target non-terminal.
The inductive step is applied recursively1: for all nodes, n, in source tree (bottom-up) do2: for all rules, r with left side matching node, nr = n do3: s = score(r)4: for all variables v in r do5: score = score+ chart[nv,cv]6: end for7: update chart[n,cr] with score, s, if better than current8: end for9: end for10: cbest = argmaxc chart[root,c]11: find best derivation using back-pointers from (root,cbest)Figure 1: Generation algorithm to find the bestderivation.
nr and nv are the source nodes indexedby the rule?s source side (root and variable), whilecr and cv are the non-terminal categories of the rule?starget side (root and variable).is very good and includes ...AUX RB JJ CCVPVPADJPVPFigure 2: Example of a rule application during gen-eration.
The dashed area shows a matching rule forthe VP node.bottom-up, and involves applying a grammar ruleto a node in the source tree.
Rules with substitutionvariables in their frontier are scored with referenceto the chart for the matching nodes and target non-terminal categories.
Once the process is complete,we can read the best score from the chart cell for theroot node, and the best derivation can be constructedby traversing back-pointers also stored in the chart.This is illustrated in Figure 2 where the rule?VP,VP??
?
[[isAUX ADJP 1 ]VP CCVP]VP, [isAUX NP 1 ]VP?
isapplied to the top VP node.
The score of the result-ing tree would reference the chart to calculate thescore for the best target tree at the ADJP node withsyntactic category NP.3.2 Grammar InductionOur induction algorithm automatically finds gram-mar rules from a word-aligned parsed parallel cor-pus.
The rules are pairs of elementary trees (i.e., treefragments) whose leaf nodes are linked by the wordalignments.
These leaves can be either terminal ornon-terminal symbols.
Initially, the algorithm ex-75tracts tree pairs from word aligned text by choos-ing aligned constituents in the source and the tar-get.
These pairs are then generalised using subtreeswhich are also extracted, resulting in synchronousrules with variable nodes.
The set of aligned treepairs are extracted using the alignment templatemethod (Och and Ney, 2004), constrained to syntac-tic constituent pairs:C = {(nS,nT ), (?
(s, t) ?
A ?
s ?
Y (nS)?
t ?
Y (nT ))?
(@(s, t) ?
A ?
(s ?
Y (nS)Y t ?
Y (nT )))}where nS and nT are source and target tree nodes(subtrees),A = {(s, t)} is the set of word alignments(pairs of word-indices), Y (?)
returns the yield spanfor a subtree and Y is the exclusive-or operator.The next step is to generalise the candidate pairsby replacing subtrees with variable nodes.
We couldfully trust the word alignments and adopt a strat-egy in which the rules are generalised as much aspossible and thus include little lexicalisation.
Fig-ure 3 shows a simple sentence pair and the result-ing synchronous rules according to this generalisa-tion strategy.
Alternatively, we could extract everypossible rule by including unlexicalised rules, lexi-calised rules and their combination.
The downsidehere is that the total number of possible rules is fac-torial in the size of the candidate set.
We address thisproblem by limiting the number of variables and therecursion depth, and by filtering out singleton rules.There is no guarantee that the induced rules willgeneralise well to a testing set.
For example, the test-ing data may have a rule which was not seen in thetraining set (e.g., a new terminal or non terminal).In this case no rule can be applied and subsequentlygeneration fails.
For this reason we allow the modelto duplicate any CFG production from the sourcetree, and uses a feature to flag that this rule was un-seen in training.
These SCFG rules are then mergedwith the induced rules and fed into the feature detec-tion module (see Section 3.3 for details).3.3 The Large Margin ModelWe now describe how the parameters of our STSGgeneration system are fit to a supervised training set.For a given source tree, the space of sister targettrees implied by the synchronous grammar is oftenvery large, and the majority of these trees are un-Thedocumentationis verygoodandincludesa tutorialto getyoustarted.Documentationisverygood.S.VPVPNPSVPVPSVPVBNNPPRPVBTONNDTVBZCCVPADJPJJRBAUXNPNNDTS.VPADJPJJRBAUXNP NN?S,S?
?
?
[NP 1 VP 2 .
3 ]S, [NP 1 VP 2 .
3 ]S??NP,NP?
?
?
[DT NN 1 ]NP, [NN 1 ]NP??NN,NN?
?
?documentationNN ,DocumentationNN??VP,VP?
?
?VP 1 CC VP,VP 1 ??VP,VP?
?
?AUX 1 ADJP 2 ,AUX 1 ADJP 2 ?
?AUX ,AUX?
?
?isAUX , isAUX ??ADJP,ADJP?
?
?
[RB 1 JJ 2 ]ADJP, [RB 1 JJ 2 ]ADJP??RB,RB?
?
?veryRB,veryRB??JJ,JJ?
?
?goodADJ ,goodADJ?
?., .?
?
?.., ..?Figure 3: Induced synchronous grammar from a sen-tence pair using a strategy that extracts general rules.grammatical or are poor compressions.
The train-ing procedure learns weights such that the modelcan discriminate between these trees and predict agood target tree.
For this we develop a discriminativetraining process which learns a weighted tree-to-treetransducer.
Our model is based on Tsochantaridis etal.
?s (2005) framework for learning Support VectorMachines (SVMs) with structured output spaces, us-ing the SVMstruct implementation.1 We briefly sum-marise the approach below; for a more detailed de-scription we refer the interested reader to Tsochan-taridis et al (2005).Traditionally SVMs learn a linear classifier thatseparates two or more classes with the largest pos-sible margin.
Analogously, structured SVMs at-tempt to separate the correct structure from all other1http://svmlight.joachims.org/svm struct.html76structures with a large margin.
Given an input in-stance x, we search for the optimum output y underthe assumption that x and y can be adequately de-scribed using a combined feature vector representa-tion ?(x,y).
Recall that x are the source trees and yare synchronous derivations which generate both xand a target tree.f (x;w) = argmaxy?Y?w,?(x,y)?
(2)The goal of the training procedure is to find a param-eter vector w such that it satisfies the condition:?i,?y ?
Y \yi : ?w,?(xi,yi)??(xi,y)?
?
0 (3)where xi,yi are the ith training source tree and tar-get derivation.
To obtain a unique solution ?
therewill be several parameter vectors w satisfying (3)if the training instances are linearly separable ?Tsochantaridis et al (2005) select the w that max-imises the minimum distance between yi and theclosest runner-up structure.The framework also incorporates a loss function.This property is particularly appealing in the contextof sentence compression and generally text-to-textgeneration.
For example, a compression that differsfrom the gold standard with respect to one or twowords should be treated differently from a compres-sion that bears no resemblance to it.
Another impor-tant factor is the length of the compression.
Com-pressions whose length is similar to the gold stan-dard should be be preferable to longer or shorteroutput.
A loss function ?
(yi,y) quantifies the accu-racy of prediction y with respect to the true outputvalue yi.
We give details of the loss functions weemployed for the compression task below.We are now ready to state the learning objectivefor the structured SVM.
We use the soft-margin for-mulation which allows errors in the training set, viathe slack variables ?i:minw,?12||w||2 +Cnn?i=1?i, ?i ?
0 (4)?i,?y ?
Y \yi : ?w,??(y)?
?
1??i?
(yi,y)Slack variables ?i are introduced here for each train-ing example xi, C is a constant that controls thetrade-off between training error minimisation andmargin maximisation, and ??
(y) is a shorthand for?(xi,yi)??
(xi,y) (see (3)).
Note that slack vari-ables are rescaled with the inverse loss incurred ineach of the linear constraints.2The optimisation problem in (4) is approximatedusing a polynomial time cutting plane algorithm(Tsochantaridis et al, 2005).
This optimisation cru-cially relies on finding the constraint incurring themaximum cost.
The cost function for slack rescalingcan be formulated as:H(y) = (1????i(y),w?)?
(yi,y) (5)In order to adapt this framework to our genera-tion problem, we must provide the feature map-ping ?
(x,y), a loss function ?
(yi,y), and a max-imiser y?
= argmaxy?Y H(y) (see (5)).
The followingsections describe how these are instantiated in thesentence compression task.Feature Mapping We devised a general featureset suitable for compression and paraphrasing.
Ourfeature space is defined over source trees (x) andtarget derivations (y).
All features apply to a singlegrammar rule; a feature vector for a derivation is ex-pressed as the sum of the feature vectors for eachrule in this derivation.We make use of syntactic, lexical, and com-pression specific features.
Our simplest syntac-tic feature is the identity of a synchronous rule.Specifically, we record its source tree, its targettree and their combination.
We also include rulefrequencies ?
(target|source), ?
(source|target) and?
(source, target).
Another feature records the fre-quencies of the CFG productions used in the tar-get side of a rule.
This allows the model to learnthe weights of a CFG generation grammar, as aproxy for a language model.
Using scores from apre-trained CFG grammar or an n-gram languagemodel might be preferable when the training sampleis small, however we leave this as future work.
Ourlast syntactic feature keeps track of the source rootand the target root non-terminals.
Our lexical fea-tures contain the list of tokens in the source yield,target yield, and both.
We also use words as features.2Alternatively, the loss function can be used to rescale themargin.
This approach is less desirable as it is not scale invari-ant (Tsochantaridis et al, 2005).
We also found empirically thatslack-rescaling slightly outperforms margin rescaling on ourcompression task.77Finally, we have implemented a set ofcompression-specific features.
These include afeature that detects if the yield of the target sideof a synchronous rule is a subset of the yield ofits source.
We also take note of the edit operations(i.e., removal, insertion) required to transform thesource side into the target.
Edit operations arerecorded separately for trees and their yields.
Inorder to encourage compression, we also count thenumber of words on the target, the number of rulesused in the derivation and the number of droppedvariables.Loss Functions The large margin configurationsketched above is quite modular and in theory a widerange of loss functions could be specified.
Examplesinclude edit-distance, precision, F-score, BLEU andtree-based measures.
In practice, the loss functionshould be compatible with our maximisation algo-rithm which requires the objective function to de-compose along the same lines as the tree derivation.3Given this restriction, we define a loss basedon position-independent unigram precision (Prec)which penalises errors in the yield independentlyfor each word.
Although fairly intuitive, this lossis far from ideal.
First, it maximally rewards re-peatedly predicting the same word if the latter isin the reference target tree.
Secondly, it may biastowards overly short output which drops core in-formation ?
one-word compressions will tend tohave higher precision than longer output.
To coun-teract this, we introduce two brevity penalty mea-sures (BP) inspired by BLEU (Papineni et al, 2002)which we incorporate into the loss function, using aproduct, loss = 1?Prec ?BP:BP1 = exp(1?max(1,rc)) (6)BP2 = exp(1?max(cr,rc))where r is the reference length and c is the candidatelength.BP1 is asymmetric, it has value one when c ?
rand decays to zero when c < r. Note that precisionshould decay when c > r as extra output will oftennot match the reference.
BP2 is two-sided: it has3Optimising non-decompositional loss functions compli-cates the objective function, which then cannot be solved ef-ficiently using a dynamic program.value one when c = r and decays towards zero forc < r and c > r. In both cases, brevity is assessedagainst the gold standard target (not the source) toallow the system to learn the correct degree of com-pression from the training data.Maximisation Algorithm Our algorithm finds themaximising derivation for H(y) in (5).
This deriva-tion will have a high loss and a high score under themodel, and therefore represents the most-violatedconstraint which is then added to the SVM?s work-ing set of constraints (see (4)).The standard generation method from Section 3.1cannot be used without modification to find the bestscoring derivation since it does not account for theloss function or the gold standard derivation.
In-stead, we stratify the generation chart with the num-ber of true and false positive tokens predicted, as de-scribed in Joachims (2005).
These contingency val-ues allow us to compute the precision and brevitypenalty (see (6)) for each complete derivation.
Thisis then combined with the derivation score and thegold standard derivation score to give H(y).The gold standard derivation features, ?
(xi,yi),must be calculated from a derivation linking thesource tree to the gold target tree.
As there maybe many such derivations, we find a unique deriva-tion using the smallest rules possible (for maximumgenerality).
This is done using a dynamic program,similar to the inside-outside algorithm used in pars-ing.
Other strategies are also possible, however weleave this to future work.
Finally, we can find theglobal maximum H(y) by maximising over all theroot chart entries.4 Evaluation Set-upIn this section we present our experimental set-upfor assessing the performance of the max marginmodel described above.
We give details of the cor-pora used, briefly introduce McDonald?s (2006) sen-tence compression model used for comparison withour approach, and explain how system output wasevaluated.Corpora We evaluated our system on two dif-ferent corpora.
The first is the compression cor-pus of Knight and Marcu (2002) derived automati-cally from the document-abstract pairs of the Ziff-78Davis corpus.
Previous compression work has al-most exclusively used this corpus.
Our experimentsfollow Knight and Marcu?s partition of training, test,and development sets (1,002/36/12 instances).
Wealso present results on Clarke and Lapata?s (2006a)Broadcast News corpus.4 This corpus was createdmanually (annotators were asked to produce com-pressions for 50 Broadcast news stories) and posesmore of a challenge than Ziff-Davis.
Being a speechcorpus, it often contains incomplete and ungram-matical utterances and speech artefacts such as dis-fluencies, false starts and hesitations.
Furthermore,spoken utterances have varying lengths, some arevery wordy whereas others cannot be reduced anyfurther.
Thus a hypothetical compression systemtrained on this domain should be able to leave somesentences uncompressed.
Again we used Clarke andLapata?s training, test, and development set split(882/410/78 instances).Comparison with State-of-the-art We evaluatedour approach against McDonald?s (2006) discrimi-native model.
This model is a good basis for compar-ison for several reasons.
First, it achieves compet-itive performance with Knight and Marcu?s (2002)decision tree and noisy channel models.
Second, italso uses large margin learning.
Sentence compres-sion is formulated as a string-to-substring mappingproblem with a deletion-based Hamming loss.
Re-call that our formulation involves a tree-to-tree map-ping.
Third, it uses a feature space complementary toours.
For example features are defined between ad-jacent words, and syntactic evidence is incorporatedindirectly into the model.
In contrast our model re-lies on synchronous rules to generate valid compres-sions and does not explicitly incorporate adjacencyfeatures.
We used an implementation of McDonald(2006) for comparison of results (Clarke and Lapata,2007).Evaluation Measures In line with previous workwe assessed our model?s output by eliciting hu-man judgements.
Participants were presented withan original sentence and its compression and askedto rate the latter on a five point scale based on the in-formation retained and its grammaticality.
We con-ducted two separate elicitation studies, one for the4The corpus can be downloaded from http://homepages.inf.ed.ac.uk/s0460084/data/.O: I just wish my parents and my other teachers couldbe like this teacher, so we could communicate.M: I wish my teachers could be like this teacher.S: I wish my teachers could be like this, so we couldcommunicate.G: I wish my parents and other teachers could be likethis, so we could communicate.O: Earlier this week, in a conference call with analysts,the bank said it boosted credit card reserves by $350million.M: Earlier said credit card reserves by $350 million.S: In a conference call with analysts, the bank boostedcard reserves by $350 million.G: In a conference call with analysts the bank said itboosted credit card reserves by $350 million.Table 1: Compression examples from the Broadcastnews corpus (O: original sentence, M: McDonald(2006), S: STSG, G: gold standard)Ziff-Davis and one for the Broadcast news dataset.In both cases our materials consisted of 96 source-target sentences.
These included gold standard com-pressions and the output of our system and Mc-Donald?s (2006).
We were able to obtain ratings onthe entire Ziff-Davis test set as it has only 32 in-stances; this was not possible for Broadcast newsas the test section consists of 410 instances.
Conse-quently, we randomly selected 32 source-target sen-tences to match the size of the Ziff-Davis test set.5We collected ratings from 60 unpaid volunteers, allself reported native English speakers.
Both studieswere conducted over the Internet.
Examples of ourexperimental items are given in Table 1.We also report results using F1 computed overgrammatical relations (Riezler et al, 2003).
Wechose F1 (as opposed to accuracy or edit distance-based measures) as Clarke and Lapata (2006b) showthat it correlates reliably with human judgements.5 ExperimentsThe framework presented in Section 3 is quite flex-ible.
Depending on the grammar induction strategy,choice of features, loss function and maximisationalgorithm, different classes of models can be de-rived.
Before presenting our results in detail we dis-cuss the specific model employed in our experimentsand explain how its parameters were instantiated.In order to build a compression model we need5A Latin square design ensured that subjects did not see twodifferent compressions of the same sentence.7960 65 70 75 80 8545505560compression rateF1llllPrecPrec.BP1Prec.BP2Figure 4: Compression rate vs. grammatical rela-tions F1 using unigram precision alone and in com-bination with two brevity penalties.a parallel corpus of syntax trees.
We obtained syn-tactic analyses for source and target sentences withBikel?s (2002) parser.
Our corpora were automat-ically aligned with Giza++ (Och et al, 1999) inboth directions between source and target and sym-metrised using the intersection heuristic (Koehn etal., 2003).
Each word in the lexicon was also alignedwith itself.
This was necessary in order to informGiza++ about word identity.
Unparseable sentencesand those longer than 50 tokens were removed fromthe data set.We induced a synchronous tree substitution gram-mar from the Ziff-Davis and Broadcast news cor-pora using the method described in Section 3.2.
Weextracted all maximally general synchronous rules.These were complemented with more specific rulesfrom conjoining pairs of general rules.
The specificrules were pruned to remove singletons and thoserules with more than 3 variables.
Grammar ruleswere represented by the features described in Sec-tion 3.3.An important parameter for our compression taskis the appropriate choice of loss function.
Ideally, wewould like a loss function that encourages compres-sion without overly aggressive information loss.
Fig-ure 4 plots compression rate against grammatical re-lations F1 using each of the loss functions presentedin Section 3.3 on the Ziff-Davis development set.6As can be seen with unigram precision alone (Prec)6We obtained a similar plot for the Broadcast News corpusbut omit it due to lack of space.Ziff-Davis CompR RelF1McDonald06 66.2 45.8STSG 56.8 54.3Gold standard 57.2 ?Broadcast News CompR RelF1McDonald06 68.6 47.6STSG 73.7 53.4?Gold standard 76.1 ?Table 2: Results using grammatical relations F1(?
: sig.
diff.
from McDonald06; p < 0.01 using theStudent t test)the system produces overly short output, whereasthe one-sided brevity penalty (BP1) achieves the op-posite effect.
The two-sided brevity penalty (BP2)seems to strike the right balance: it encourages com-pression while achieving good F-scores.
This sug-gests that important information is retained in spiteof significant compression.
We also varied the regu-larisation parameter C (see (4)) over a range of val-ues on the development set and found that setting itto 0.01 yields overall good performance across cor-pora and loss functions.We now present our results on the test set.
Thesewere obtained with a model that uses slack rescal-ing and a precision-based loss function with a two-sided brevity penalty (C = 0.01).
Table 2 shows theaverage compression rates (CompR) for McDonald(2006) and our model (STSG) as well as their perfor-mance according to grammatical relations F1.
Therow ?Gold standard?
displays human-produced com-pression rates.
Notice that our model obtains com-pression rates similar to the gold standard, whereasMcDonald tends to compress less on Ziff-Davis andmore on Broadcast news.
As far as F1 is concerned,we see that STSG outperforms McDonald on bothcorpora.
The difference in F1 is statistically signifi-cant on Broadcast news but not on Ziff-Davis (whichconsists solely of 32 sentences).Table 3 presents the results of our elicitationstudy.
We carried out an Analysis of Variance(ANOVA) to examine the effect of system type (Mc-Donald06, STSG, Gold standard) on the compres-sion ratings.
The ANOVA revealed a reliable effecton both corpora.
We used post-hoc Tukey tests to80Model Ziff-Davis Broadcast newsMcDonald06 2.82?
2.16?STSG 3.20??
2.63?Gold standard 3.72 3.05Table 3: Mean ratings on compression outputelicited by humans (?
: sig.
diff.
from McDon-ald06 (?
< 0.05); ?
sig.
diff.
from Gold standard(?
< 0.01); using post-hoc Tukey tests)examine whether the mean ratings for each sys-tem differed significantly.
The Tukey tests showedthat STSG is perceived as significantly better thanMcDonald06.
There is no significant difference be-tween STSG and the gold standard compressions onthe Broadcast news; both systems are significantlyworse than the gold standard on Ziff-Davis.These results are encouraging, indicating that ourhighly expressive framework is a good model forsentence compression.
Under several experimentalconditions we obtain better performance than previ-ous work.
Importantly, the model described here isnot compression-specific, it could be easily adaptedto other tasks, corpora or languages (for whichsyntactic analysis tools are available).
Being su-pervised, our model learns to fit the compressionrate of the training data.
In this sense, it is some-what inflexible as it cannot easily adapt to a spe-cific rate given by a user or imposed by an appli-cation (e.g., when displaying text on small screens).Compression rate can be indirectly manipulated byadopting loss functions that encourage or discouragecompression (see Figure 4), but admittedly in otherframeworks (e.g., Clarke and Lapata (2006a)) thelength of the compression can be influenced morenaturally.In our formulation of the compression problem,a derivation is characterised by a single inventoryof features.
This entails that the feature space can-not in principle distinguish between derivations thatuse the same rules, applied in a different order.
Al-though, this situation does not arise often in ourdataset, we believe that it can be ameliorated by in-tersecting a language model with our generation al-gorithm (Chiang, 2005).6 Conclusions and Future WorkIn this paper we have presented a novel methodfor sentence compression cast in the framework ofstructured learning.
We develop a system that gener-ates compressions using a synchronous tree substi-tution grammar whose weights are discriminativelytrained within a large margin model.
We also de-scribe an appropriate algorithm than can be used inboth training (i.e., learning the model weights) anddecoding (i.e., finding the most plausible compres-sion under the model).
The proposed formulation al-lows us to capture rewriting operations that go be-yond word deletion and can be easily tuned to spe-cific loss functions directly related to the problem athand.
We empirically evaluate our approach againsta state-of-the art model (McDonald, 2006) and showperformance gains on two compression corpora.Future research will follow three directions.
First,we will extend the framework to incorporate po-sition dependent loss functions.
Examples includethe Hamming distance or more sophisticated func-tions that take the tree structure of the source andtarget sentences into account.
Such functions canbe supported by augmenting our generation algo-rithm with a beam search.
Secondly, the present pa-per used a relatively simple feature set.
Our inten-tion was to examine our model?s performance with-out extensive feature engineering.
Nevertheless, im-provements should be possible by incorporating fea-tures defined over n-grams and dependencies (Mc-Donald, 2006).
Finally, the experiments presentedin this work use a grammar acquired from the train-ing corpus.
However, there is nothing inherent in ourformalisation that restricts us to this particular gram-mar.
We therefore plan to investigate the potentialof our method with unsupervised or semi-supervisedgrammar induction techniques for additional rewrit-ing tasks including paraphrase generation and ma-chine translation.Acknowledgements The authors acknowledge the sup-port of EPSRC (grants GR/T04540/01 and GR/T04557/01).We are grateful to James Clarke for sharing his implementationof McDonald (2006) with us.
Special thanks to Philip Blunsomfor insightful comments and suggestions.81ReferencesR.
Barzilay, K. McKeown.
2001.
Extracting paraphrasesfrom a parallel corpus.
In Proceedings of ACL/EACL,50?57, Toulouse, France.D.
Bikel.
2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceedingsof HLT, 24?27, San Diego, CA.J.
Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,J.
Tait.
1999.
Simplifying text for language impairedreaders.
In Proceedings of EACL, 269?270, Bergen,Norway.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the43rd ACL, 263?270, Ann Arbor, MI.J.
Clarke, M. Lapata.
2006a.
Constraint-based sentencecompression: An integer programming approach.
InProceedings of COLING/ACLMain Conference PosterSessions, 144?151, Sydney, Australia.J.
Clarke, M. Lapata.
2006b.
Models for sentence com-pression: A comparison across domains, training re-quirements and evaluation measures.
In Proceedingsof COLING/ACL, 377?384, Sydney, Australia.J.
Clarke, M. Lapata.
2007.
Modelling compressionwith discourse constraints.
In Proceedings of EMNLP-CoNLL, Prague, Czech Republic.J.
Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In Proceedings of the ACLInteractive Poster/Demonstration Sessions, 205?208,Sapporo, Japan.M.
Galley, K. McKeown.
2007.
Lexicalized Markovgrammars for sentence compression.
In Proceedingsof NAACL/HLT, 180?187, Rochester, NY.J.
Grael, K. Knight.
2004.
Training tree transducers.
InProceedings of NAACL/HLT, 105?112, Boston, MA.C.
Hori, S. Furui.
2004.
Speech summarization: an ap-proach through word extraction and a method for eval-uation.
IEICE Transactions on Information and Sys-tems, E87-D(1):15?25.H.
Jing.
2000.
Sentence reduction for automatic textsummarization.
In Proceedings of ANLP, 310?315,Seattle, WA.T.
Joachims.
2005.
A support vector method for mul-tivariate performance measures.
In Proceedings ofICML, 377?384, Bonn, Germany.K.
Knight, D. Marcu.
2002.
Summarization beyond sen-tence extraction: a probabilistic approach to sentencecompression.
Artificial Intelligence, 139(1):91?107.P.
Koehn, F. J. Och, D. Marcu.
2003.
Statistical phrase-based translation.
In Proceedings of HLT/NAACL, 48?54, Edmonton, Canada.D.
Lin, P. Pantel.
2001.
Discovery of inference rules forquestion answering.
Natural Language Engineering,7(4):342?360.R.
McDonald.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proceedings ofEACL, 297?304, Trento, Italy.I.
D. Melamed.
2004.
Statistical machine translation byparsing.
In Proceedings of ACL, 653?660, Barcelona,Spain.F.
J. Och, H. Ney.
2004.
The alignment template ap-proach to statistical machine translation.
Computa-tional Linguistics, 30(4):417?449.F.
J. Och, C. Tillmann, H. Ney.
1999.
Improved align-ment models for statistical machine translation.
InProceedings of EMNLP/VLC, 20?28, College Park,MD.B.
Pang, K. Knight, D. Marcu.
2003.
Syntax-basedalignment of multiple translations: Extracting para-phrases and generating new sentences.
In Proceedingsof NAACL, 181?188, Edmonton, Canada.K.
Papineni, S. Roukos, T. Ward, W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of ACL, 311?318,Philadelphia, PA.C.
Quirk, C. Brockett, W. Dolan.
2004.
Monolingualmachine translation for paraphrase generation.
In Pro-ceedings of EMNLP, 142?149, Barcelona, Spain.S.
Riezler, T. H. King, R. Crouch, A. Zaenen.
2003.
Sta-tistical sentence condensation using ambiguity pack-ing and stochastic disambiguation methods for lexical-functional grammar.
In Proceedings of HLT/NAACL,118?125, Edmonton, Canada.Y.
Shinyama, S. Sekine, K. Sudo, R. Grishman.
2002.Automatic paraphrase acquisition from news articles.In Proceedings of HLT, 40?46, San Diego, CA.I.
Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun.2005.
Large margin methods for structured and in-terdependent output variables.
Journal of MachineLearning Research, 6:1453?1484.J.
Turner, E. Charniak.
2005.
Supervised and unsuper-vised learning for sentence compression.
In Proceed-ings of ACL, 290?297, Ann Arbor, MI.L.
S. Zettlemoyer, M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Pro-ceedings of UAI, 825?830, Edinburgh, UK.82
