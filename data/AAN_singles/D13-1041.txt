Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 426?435,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsEfficient Collective Entity Linking with StackingZhengyan He?
Shujie Liu?
Yang Song?
Mu Li?
Ming Zhou?
Houfeng Wang???
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China?
Microsoft Research Asiahezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.comsongyangmagic@gmail.com wanghf@pku.edu.cnAbstractEntity disambiguation works by linking am-biguous mentions in text to their correspond-ing real-world entities in knowledge base.
Re-cent collective disambiguation methods en-force coherence among contextual decisionsat the cost of non-trivial inference processes.We propose a fast collective disambiguationapproach based on stacking.
First, we train alocal predictor g0 with learning to rank as baselearner, to generate initial ranking list of can-didates.
Second, top k candidates of relatedinstances are searched for constructing expres-sive global coherence features.
A global pre-dictor g1 is trained in the augmented featurespace and stacking is employed to tackle thetrain/test mismatch problem.
The proposedmethod is fast and easy to implement.
Exper-iments show its effectiveness over various al-gorithms on several public datasets.
By learn-ing a rich semantic relatedness measure be-tween entity categories and context document,performance is further improved.1 IntroductionWhen extracting knowledge from natural languagetext into a machine readable format, ambiguousnames must be resolved in order to tell which real-world entity the name refers to.
The task of linkingnames to knowledge base is known as entity linkingor disambiguation (Ji et al 2011).
The resulting textis populated with semantic rich links to knowledgebase like Wikipedia, and ready for various down-stream NLP applications.
?Corresponding authorPrevious researches have proposed several kindsof effective approaches for this problem.
Learningto rank (L2R) approaches use hand-crafted featuresf(d, e) to describe the similarity or dissimilarity be-tween contextual document d and entity definitione.
L2R approaches are very flexible and expres-sive.
Features like name matching, context similar-ity (Li et al 2009; Zheng et al 2010; Lehmann etal., 2010) and category context correlation (Bunescuand Pasca, 2006) can be incorporated with ease.Nevertheless, decisions are made independently andinconsistent results are found from time to time.Collective approaches utilize dependencies be-tween different decisions and resolve all ambiguousmentions within the same context simultaneously(Han et al 2011; Hoffart et al 2011; Kulkarniet al 2009; Ratinov et al 2011).
Collective ap-proaches can improve performance when local ev-idence is not confident enough.
They often utilizesemantic relations across different mentions, and iswhy they are called global approaches, while L2Rmethods fall into local approaches (Ratinov et al2011).
However, collective inference processes areoften expensive and involve an exponential searchspace.We propose a collective entity linking methodbased on stacking.
Stacked generalization (Wolpert,1992) is a powerful meta learning algorithm thatuses two levels of learners.
The predictions of thefirst learner are taken as augmented features for thesecond learner.
The nice property of stacking is thatit does not restrict the form of the base learner.
Inthis paper, our base learner, an L2R ranker, is firstemployed to generate a ranking list of candidates.426At the next level, we search for semantic coherententities from the top k candidates of neighboringmentions.
The second learner is trained on the aug-mented feature space to enforce semantic coherence.Stacking is employed to handle train/test mismatchproblem.
Compared with existing collective meth-ods, the inference process of our method is muchfaster because of the simple form of its base learner.Wikipedians annotate each entity with categorieswhich provide another source of valuable seman-tic information.
(Bunescu and Pasca, 2006) pro-pose to generalize beyond context-entity correla-tion s(d, e) with word-category correlation s(w, c).However, this method works at word level, and doesnot scale well to large number of categories.
Weexplore a representation learning technique to learnthe category-context association in latent semanticspace, which scales much better to large knowledgebase.Our contributions are as follows: (1) We pro-pose a fast and accurate stacking-based collectiveentity linking method, which combines the benefitsof both coherence modeling of collective approachesand expressivity of L2R methods.
We show aneffective usage of ranking list as global features,which is a key improvement for the global predictor.
(2) To overcome problems of scalability and shal-low word-level comparison, we learn the category-context correlation with recent advances of repre-sentation learning, and show that this extra seman-tic information indeed helps improve entity linkingperformance.2 Related WorkMost popular entity linking systems use the L2Rframework (Bunescu and Pasca, 2006; Li et al2009; Zheng et al 2010; Lehmann et al 2010).Its discriminative nature gives the model enoughflexibility and expressivity.
It can include any fea-tures that describe the similarity or dissimilarity ofcontext d and candidate entity e. They often per-form well even on small training set, with carefully-designed features.
This category falls into the localapproach as the decision processes for each mentionare made independently (Ratinov et al 2011).
(Cucerzan, 2007) first suggests to optimize an ob-jective function that is similar to the collective ap-proach.
However, the author adopts an approxi-mation method because of the large search space(which is O(nm) for a document with m mentions,each with n candidates).
Various other methodslike integer linear programming (Kulkarni et al2009), personalized PageRank (Han et al 2011) andgreedy graph cutting (Hoffart et al 2011) have beenexplored in literature.
Our method without stackingresembles the method of (Ratinov et al 2011) inthat they use the predictions of a local ranker to gen-erate features for global ranker.
The differences arethat we use stacking to train the local ranker to han-dle the train/test mismatch problem and top k candi-dates to generate features for the global ranker.Stacked generalization (Wolpert, 1992) is a metalearning algorithm that uses multiple learners out-puts to augment the feature space of subsequentlearners.
It utilizes a cross-validation strategy to ad-dress the train set / testset label mismatch problem.Various applications of stacking in NLP have beenproposed, such as collective document classification(Kou and Cohen, 2007), stacked dependency parsing(Martins et al 2008) and joint Chinese word seg-mentation and part-of-speech tagging (Sun, 2011).
(Kou and Cohen, 2007) propose stacked graphicallearning which captures dependencies between datawith relational template.
Our method is inspired bytheir approach.
The difference is our base learner isan L2R model.
We search related entity candidatesin a large semantic relatedness graph, based on theassumption that true candidates are often semanti-cally correlated while false ones scattered around.Wikipedians annotate entries in Wikipedia withcategory network.
This valuable information gener-alizes entity-context correlation to category-contextcorrelation.
(Bunescu and Pasca, 2006) utilizecategory-word as features in their ranking model.
(Kataria et al 2011) employ a hierarchical topicmodel where each inner node in the hierarchy is acategory.
Both approaches must rely on pruned cate-gories because the large number of noisy categories.We try to address this problem with recent advancesof representation learning (Bai et al 2009), whichlearns the relatedness of category and context in la-tent continuous space.
This method scales well topotentially large knowledge base.4273 MethodIn this section, we first introduce our base learnerand local features used; next, the stacking train-ing strategy is given, followed by an explana-tion of our global coherence model with aug-mented feature space; finally we explain how tolearn category-context correlation with representa-tion learning technique.3.1 Base learner and local predictor g0Entity linking is formalized as follows: givenan ambiguous name mention m with its con-textual document d, a list of candidate entitiese1, e2, .
.
.
, en(m) ?
C(m) is generated for m, ourpredictor g will generate a ranking score g(ei) foreach candidate ei.
The ranking score will be usedto construct augmented features for the next levellearner, or used by our end system to select the an-swer:e?
= arg maxe?C(m)g(e) (1)In an L2R framework, the model is often definedas a linear combination of features.
Here, our fea-tures f?
(d, e) are derived from document d and can-didate e. The model is defined as g(e) = w?f?
(d, e).In our problem, we are given a list of training dataD = {(di, ei)}.
We want to optimize the parameterw?, such that the correct entity has a higher score overnegative ones.
This is done via a preference learningtechnique SVM rank, first introduced by (Joachims,2002).
The following margin based loss is mini-mized w.r.t w?
:L = 12?w?
?2 + C??d,e?
(2)s.t.
w?(f?
(d, e)?
f?
(d, e?))
?
1?
?d,e?
(3)?d,e?
?
0 (4)where C is a trade-off between training error andmargin size; ?
is slacking variable and loops overall query documents d and negative candidates e?
?C(m)?
{e}.This model is expressive enough to include anyform of features describing the similarity and dis-similarity of d and e. We only include some typicalfeatures seen in literature.
The inclusion of thesefeatures is not meant to be exhaustive.
Our purposeis to build a moderate model in which some of theSurface matching:1. mention string m exactly matches candidatee, i.e.
m = e2.
neither m is a substring of e nor e is a sub-string of m3.
m ?= e and m is a substring of e4.
m ?= e and e is a substring of m5.
m ?= e and m is a redirect pointing to e inWikipedia6.
m ?= e and e starts with m7.
m ?= e and e ends with mContext matching:1. cosine similarity of TF-IDF score betweencontext and entire Wikipedia page of candidate2.
cosine similarity of TF-IDF score betweencontext and introduction of Wikipedia page3.
jaccard distance between context and entireWikipedia page of candidate4.
jaccard distance between context and intro-duction of Wikipedia pagePopularity or prominence feature:percentage of Wikipedia hyperlinks pointing toe given mention m, i.e.
P(e|m)Category-context coherence model:cat0 and cat1 (details in Section 3.4)Table 1: Features for local predictor g0.useful features like string matching and entity pop-ularity cannot be easily expressed by collective ap-proaches like (Hoffart et al 2011; Han et al 2011).The features for level 0 predictor g0 are describedin Table 1.
The reader can consult (Li et al 2009;Zheng et al 2010; Lehmann et al 2010) for furtherreference.3.2 Stacking training for global predictor g1Stacked generalization (Wolpert, 1992) is a metalearning algorithm that stacks two ?levels?
of pre-dictors.
Level 0 includes one or more predictorsh(0)1 , h(0)2 , .
.
.
, h(0)K : Rd ?
R, each one is trained onthe original d-dimensional feature space.
The level1 predictor h(1) : Rd+K ?
R is trained in the aug-mented (d+K)-dimensional feature space, in whichpredictions at level 0 are taken as extra features inh(1).
(Kou and Cohen, 2007) proposed stacked graphi-428cal learning for learning and inference on relationaldata.
In stacked graphical learning, dependenciesamong data are captured by relational template, withwhich one searches for related instances of the cur-rent instance.
The augmented feature space doesnot necessarily to be d + K. Instead, one can con-struct any declarative feature with the original dataand predictions of related instances.
For instance,in collective document classification (Kou and Co-hen, 2007) employ relational template to extractdocuments that link to this document, then apply aCOUNT aggregator over each category on neighbor-ing documents as level 1 features.In our entity linking task, we use a single predic-tor g0 trained with local features at level 0.
Com-pared with (Kou and Cohen, 2007), both g0 and g1are L2R models rather than classifier.
At level 1, foreach document-candidate entity pair, we use the re-lational templateN (x) to find related entities for en-tity x, and construct global features with some func-tion G({g0(n)|n ?
N (x)}) (details in Sec.
3.3).The global predictor g1 receives as input the origi-nal features plus G.One problem is that if we use g0 trained on the en-tire training set to predict related instances in train-ing set, the accuracy can be somehow different (typ-ically lower) for future unseen data.
g1 with this pre-diction as input doesn?t generalize well to test data.This is known as train/test mismatch problem.
Tomimic test time behavior, training is performed in across-validation-like way.
Let D be the entire train-ing set:1.
Split D into L partitions {D1, .
.
.
,DL}2.
For each split Di:2.1 Train an instance of g0 on D ?Di2.2 Predict all related instances inDi with thispredictor g02.3 Augment feature space for x ?
Di, with Gapplied on predictions of N (x)3.
Train level 0 predictor g0 on entire D, for ex-panding feature space for test data4.
Train level 1 predictor g1 on entire D, in theaugmented feature space.In the next subsection, we will describe how toconstruct global features from the predictions of g0on neighbors N (x) with G.3.3 Enforcing coherence with global features GIf one wants to identify the correct entity for an am-biguous name, he would possibly look for relatedentities in its surrounding context.
However, sur-rounding entities can also exhibit some degree ofambiguity.
In ideal cases, most true candidates areinter-connected with semantic links while negativecandidates are scattered around (Fig.
1).
Thus, weask the following question: Is there any highly rele-vant entity to this candidate in context?
Or, is thereany mention with highly relevant entity to this can-didate in the top k ranking list of this mention?
Andhow many those mentions are?
The reason to lookup top k candidates is to improve recall.
g0 may notperfectly rank related entity at the first place, e.g.
?Mitt Romney?
in Figure 1.Assume the ambiguous mention set is M .
Foreach mention mi ?
M , we rank each entity ei,j ?C(mi) by its score g0(ei,j).
Denote its rank asRank(ei,j).
For each entity e in the candidate setE = {ei,j |?ei,j ?
C(mi), ?mi ?
M}, we searchrelated instances for e as follows:1. search in E for entities with semantic related-ness above a threshold ({0.1,0.3,0.5,0.7,0.9});2. select those entities in step (1) with Rank(e)less than or equal to k (k ?
{1, 3, 5});3. map entities in step (2) to unique set of men-tions U , excluding current m, i.e.
e ?
C(m).This process is relatively fast.
It only involves asparse matrix slicing operation on the large pre-computed semantic relatedness matrix in step (1),and logical operation in step (2,3).
The followingfeatures are fired concerning the unique set U :- if U is empty;- if U is not empty;- if the percentage |U |/|M | is above a threshold(e.g.
0.3).The above process generates a total of 45 (5?3?3)global features.429Barack Obama Democratic Party (United States)Mitt RomneyRepublican Party (United States)Obama, FukuiObama, NagasakiDemocratic Party (Italy)Democratic Party (Serbia)Republican Party of MinnesotaRepublicanismRomney, West VirginiaHMS Romney (1694)... ... ... ...received national attention during his campaign  ...  with his vectory in the March   [[Obama|Barack Obama]][[Democratic Party|Democratic Party (United States)]] primary  ...
He was re-elected president in November2012, defeating [[Republican|Republican Party (United States)]] nominee [[Romney|Mitt Romney]]Figure 1: Semantic links for collective entity linking.
Annotation [[mention|entity]] follows Wikipedia conventions.Finally, the semantic relatedness measure of twoentities ei,ej is defined as the common in-links of eiand ej in Wikipedia (Milne and Witten, 2008; Hanet al 2011):SR(ei, ej) = 1?log(max(|A|, |B|))?
log(|A ?B|)log(|W |)?
log(min(|A|, |B|))(5)where A and B are the set of in-links for entity eiand ej respectively, andW is the set of all Wikipediapages.Our method is a trade-off between exact collec-tive inference and approximating related instancewith top ranked entities produced by g0.
Mostcollective approaches take all ambiguous mentionsinto consideration and disambiguate them simulta-neously, resulting in difficulty when inference inlarge search space (Kulkarni et al 2009; Hoffartet al 2011).
Others resolve to some kinds of ap-proximation.
(Cucerzan, 2007) construct features asthe average of all candidates for one mention, in-troducing considerable noise.
(Ratinov et al 2011)also employ a two level architecture but only taketop 1 prediction for features.
This most resemblesour approach, except we use stacking to tackle thetrain/test mismatch problem, and construct differentset of features from top k candidates predicted byg0.
We will show in our experiments that this indeedhelps boost performance.3.4 Learning category-context coherencemodel catEntities in Wikipedia are annotated with rich se-mantic structures.
Category network provides uswith another valuable information for entity link-ing.
Take the mention ?Romney?
as an exam-ple, one candidate ?Mitt Romney?
with category?Republican party presidential nominee?
co-occursfrequently with context like ?election?
and ?cam-paign?, while another candidate ?Milton Romney?with category ?Utah Utes football players?
is fre-quently observed with context like ?quarterback?and ?backfield?.
The category network forms a di-rected acyclic graph (DAG).
Some entities can sharecategory through the network, e.g.
?Barack Obama?with category ?Democratic Party presidential nom-inees?
shares the category ?United States presiden-tial candidates by party?
with ?Mitt Romney?
whentravelling two levels up the network.
(Bunescu and Pasca, 2006) propose to learn thecategory-context correlation at word level throughcategory-word pair features.
This method createssparsity problem and does not scale well becausethe number of features grows linearly with both thenumber of categories and the vocabulary size.
More-over, the category network is somewhat noisy, e.g.travelling up four levels of the hierarchy can resultin over ten thousand categories, with many irrelevantones.Rather than learning the correlation at word level,we explore a representation learning method thatlearns category-context correlation in the latent se-mantic space.
Supervised Semantic Indexing (SSI)(Bai et al 2009) is trained on query-document pairsto predict their degree of matching.
The compar-ison is performed in the latent semantic space, sothat synonymy and polysemy are implicitly handledby its inner mechanism.
The score function betweenquery q and document d is defined as:f(q, d) = qTWd (6)430where W is learned with supervision like click-through data.Given training data {(qi, di)}, training is done byrandomly sampling a negative target d?.
The modeloptimizes W such that f(q, d+) > f(q, d?).
Thus,the training objective is to minimize the followingmargin-based loss function:?q,d+,d?max(0, 1?
f(q, d+) + f(q, d?))
(7)which is also known as contrastive estimation(Smith and Eisner, 2005).W can become very large and inefficient when wehave a big vocabulary size.
This is addressed by re-placing W with its low rank approximation:W = UTV + I (8)here, the identity term I is a trade-off between thelatent space model and a vector space model.
Thegradient step is performed with Stochastic GradientDescent (SGD):U ?U + ?V (d+ ?
d?
)qT ,if 1?
f(q, d+) + f(q, d?)
> 0 (9)V ?V + ?Uq(d+ ?
d?
)T ,if 1?
f(q, d+) + f(q, d?)
> 0.
(10)where ?
is the learning rate.The query and document are not necessary realquery and document.
In our case, we treat ourproblem as: given the occurring context of an en-tity, retrieving categories corresponding to this en-tity.
Thus, we use context as query q and the cat-egories of this candidate entity as d. We also treatthe definition page of an entity as its context, andfirst train the model with definition pages, becausedefinition pages exhibit more focused topic.
Thisconsiderably accelerates the training process.
Toreduce noise, We input the categories directly con-nected with one entity as a word vector.
The inputcan be a TF-IDF vector or binary vector.
We denotemodel trained with normalized TF-IDF and with bi-nary input as cat0 and cat1 respectively.4 Experiments4.1 DatasetsPrevious researches have used diverse datasets forevaluation, which makes it hard for comparisonwith others?
approaches.
TAC-KBP has severalyears of data for evaluating entity linking system,but is not well suited for evaluating collective ap-proaches.
Recently, (Hoffart et al 2011) anno-tated a clean and much larger dataset AIDA 1 forcollective approaches evaluation based on CoNLL2003 NER dataset.
(Ratinov et al 2011) also re-fined previous work and contribute four publiclyavailable datasets 2.
Thanks to their great works,we have enough data to evaluate against.
Accord-ing to the setting of (Hoffart et al 2011), wesplit the AIDA dataset for train/development/testwith 946/216/231 documents.
We train a separatemodel on the Wikipedia training set for evaluatingACE/QUAINT/WIKI dataset (Ratinov et al 2011).Table 2 gives a brief overview of the datasets used.For knowledge base, we use the Wikipedia XMLdump 3 to extract over 3.3 million entities.
We useannotation from Wikipedia to build a name dictio-nary from mention string m to entity e for can-didate generation, including redirects, disambigua-tion pages and hyperlinks, follows the approach of(Cucerzan, 2007).
For candidate generation, wekeep the top 30 candidates by popularity (Tbl.
1).Note that our name dictionary is different from(Ratinov et al 2011) and has a much higher recall.Since (Ratinov et al 2011) evaluate on ?solvable?mentions and we have no way to recover those men-tions, we re-implement their global features and thefinal scores are not directly comparable to theirs.4.2 Methods under comparisonWe compare our algorithm with several state-of-the-art collective entity disambiguation systems.
TheAIDA system proposed by (Hoffart et al 2011) usea greedy graph cutting algorithm that iteratively re-move entities with low confidence scores.
(Han etal., 2011) employ personalized PageRank to prop-agate evidence between different decisions.
Bothalgorithms use simple local features without dis-criminative training.
(Kulkarni et al 2009) pro-pose to use integer linear programming (ILP) forinference.
Except our re-implementation of Han?s1available at http://www.mpi-inf.mpg.de/yago-naga/aida/2http://cogcomp.cs.illinois.edu/Data, we don?t find theMSNBC dataset in the zip file.3available at http://dumps.wikimedia.org/enwiki/, we usethe 20110405 xml dump.431Dataset ndocs non-NILidentified solvableAIDA dev 216 4791 4791 4707AIDA test 231 4485 4485 4411ACE 36 257 238 209(185)AQUAINT 50 727 697 668(588)Wikipedia 40 928 918 854(843)Table 2: Number of mentions in each dataset.
?identi-fied?
means the mention exists in our name dictionaryand ?solvable?
means the true entity are among the top 30candidates by popularity.
Number in parenthesis showsthe results of (Ratinov et al 2011).method, both AIDA and ILP solution are quite slowat running time.
The online demo of AIDA takesover 10 sec to process one document with mod-erate size, while the ILP solution takes around 2-3 sec/doc.
In contrast, our method takes only 0.3sec/doc, and is easy to implement.
(Ratinov et al 2011) also utilize a two layerlearner architecture.
The difference is that theirmethod use top 1 candidate generated by locallearner for global feature generation , while wesearch the top k candidates.
Moreover, stacking isused to tackle the train/test mismatch problem inour model.
We re-implement the global features of(Ratinov et al 2011) and use our local predictorg0 for level 0 predictor.
Note that we only imple-ment their global features concerning common in-links and inter-connection (totally 9 features) for faircomparison because all other models don?t use com-mon outgoing links for global coherence.4.3 SettingsWe implement SVM rank with an adaptation of lin-ear SVM in scikit-learn (which is a wrapper of Li-blinear).
The category-context coherence model isimplemented with Numpy configured with Open-Blas library, and we train this model on the entireWikipedia hyperlink annotation.
It takes about 1.5dfor one pass over the entire dataset.
The learningrate ?
is set to 1e-4 and training cost before updateis below 0.02.Parameter tuning: there aren?t many parametersto tune for both g0 and g1.
The context documentwindow size is fixed as 100 for compatibility with(Ratinov et al 2011; Hoffart et al 2011).
The num-ber of candidates is fixed to top 30 ranked by entity?spopularity.
Increase this value will generally boostrecall at the cost of lower precision.We introduce the following default parameter forglobal features in g1.
The number of fold for stack-ing is set to {1,5,10} (see Table 4, default is 10; 1means no stacking, i.e.
training g0 with all trainingdata and generating level 1 features for training datadirectly with this g0).
The number k for searchingneighboring entities with relational template is setto {1,3,5,7} (e.g.
in step 2 of Section 3.3 k = 5;default is 5).For category-context modeling, the vocabularysizes of context and category are set to top 10k and6k unigrams by frequency.
The latent dimension oflow rank approximation is set to 200.Performance measures: For all non-NILqueries, we evaluate performance with micro pre-cision averaged over queries and macro precisionaveraged over documents.
Mean Reciprocal Rank(MRR) is an information retrieval measure and isdefined as 1|Q|?|Q|i1ranki , where ranki is the rankof correct answer in response to query i. ForACE/AQUAINT/WIKI we also give the accuracy of?solvable?
mentions, but this is not directly compa-rable to (Ratinov et al 2011).
Our name dictionaryis different from theirs and ours has a higher recallrate (Tbl.
2).
Hence, the ?solvable?
set is different.k recall k recall1 78.56 6 96.312 89.59 7 97.043 93.01 8 97.374 94.97 9 97.625 95.78 10 97.81Table 3: Top k recall for local predictor g0.4.4 DiscussionsTable 4 shows the evaluation results on AIDAdataset and Table 5 shows results on datasetsACE/AQUAINT/WIKI.Effect of cat:The first group in Table 4 showssome baseline features for comparison.
We can seeeven if the categories only carry incomplete andnoisy information about an entity, it performs much432Methods Devset Testsetmicrop@1macrop@1MRR microp@1macrop@1MRRcosine 33.25 28.61 46.03 33.33 28.63 46.54jaccard 44.71 36.56 57.76 45.66 36.89 57.08cat0 54.75 47.14 67.70 61.52 54.72 72.55cat1 60.15 54.64 72.98 65.46 61.04 76.84popularity 69.21 67.59 79.26 69.07 72.63 79.45g0 76.04 73.63 84.21 76.16 78.17 84.58g0+global(Ratinov) 81.30 78.03 88.14 81.45 81.89 88.70g1+1fold 82.01 78.52 88.90 83.59 83.58 90.05g1+5fold 81.99 78.42 88.87 83.52 83.37 89.99g1+10fold 82.01 78.53 88.91 83.59 83.55 90.03g1+top1 81.65 78.76 88.51 81.81 82.55 89.06g1+top3 82.20 78.64 88.98 83.52 83.34 89.94g1+top5 82.01 78.57 88.90 83.63 83.76 90.05g1+top7 82.05 78.40 88.90 83.75 83.58 90.08g0+cat 79.36 76.14 86.66 79.64 80.47 87.32g1+cat 82.24 78.49 89.02 84.88 84.49 90.65g1+cat+all context 82.99 78.56 89.51 86.49 85.11 91.55(Hoffart et al 2011) - - - 82.29 82.02 -(Shirakawa et al 2011) - - - 81.40 83.57 -(Kulkarni et al 2009) - - - 72.87 76.74 -(Han et al 2011) - - - 78.97 75.77 -Table 4: Performance on AIDA dataset.
Maximal value in each group are highlighted with bold font.
top k means upto k candidates are used for searching related instances with relational template.better than word level features.
Group 5 in Table4 shows cat information generally boosts perfor-mance for both predictor g0 and g1.Effect of stacking: Group 3 in Table 4 shows theresults with different fold in stacking training.
1 foldmeans training g0 with all training data and directlyaugment training data with this g0.
Surprisingly, wedo not observe any substantial difference with vari-ous fold size.
We deduce it is possible the way wefire global features with top k candidates that alle-viates the problem of train/test mismatch when ex-tending feature space for g1.
Despite the ranking oftrue entity can be lower in testset than in trainingset, the semantic coherence information can still becaptured with searching over top k candidates.Effect of top k global features: Group 4 in Table4 shows the effect of k on g1 performance.
Clearly,increasing k generally improves precision and onepossible reason is the improvement in recall whensearching for related instances.
Table 3 shows thetop k recall of local predictor g0.
Further increasingk does not show any improvement.Our method benefits from such a searching strat-egy, and consistently outperforms the global fea-tures of (Ratinov et al 2011).
While their methodis a trade-off between expensive exact search overall mentions and greedy assigning all mentionswith local predictor, we show this idea can be fur-ther extended, somewhat like increasing the beamsearch size without additional computational over-head.
The only exception is the ACE dataset, sincethis dataset is so small, the difference translates toonly one mention.
One may notice the improvementon ACE/AQUAINT datasets is a little inconsistent.These datasets are much smaller and the results onlydiffer within 4 mentions.
Because these models are433Method microp@1macrop@1MRR correct/ solv-ableACEg0 77.43 81.30 79.03 95.22Ratinov 77.43 80.70 78.81 95.22g1+5fold 77.04 79.85 78.96 94.74g0+cat 77.82 81.48 79.31 95.69g1+cat 77.43 80.16 79.25 95.22AQUAINTg0 84.46 84.69 87.49 91.92Ratinov 85.14 85.29 87.90 92.66g1+5fold 85.83 85.55 88.27 93.41g0+cat 85.01 85.00 87.89 92.51g1+cat 85.28 85.14 88.23 92.81Wikipedia testg0 83.19 84.30 86.63 90.40Ratinov 84.48 85.96 87.62 91.80g1+5fold 84.81 86.29 88.13 92.15g0+cat 84.38 86.13 87.51 91.69g1+cat 85.45 87.16 88.31 92.86Table 5: Evaluation on ACE/AQUAINT/WIKI datasets.trained on Wikipedia, the annotation style can bequite different.Finally, as we analyze the development set ofAIDA, we discover that some location entities relyon more distant information across the context, aswe increase the context to the entire contextual doc-ument, we can gain extra performance boost.4.5 Error analysisAs we analyze the development set of AIDA, we findsome general problems with location names.
Loca-tion name generally is not part of the main topicof one document.
Thus, comparing context withits definition is not realistic.
Most of the time, wecan find some related location names in context; butother times, it is not easily distinguished.
For in-stance, in ?France beats Turkey in men?s football...?France refers to ?France national football team?
butour system links it to the country page ?France?
be-cause it is more popular.
This can be addressed bymodeling finer context (Sen, 2012) or local syntac-tic pattern (Hoffart et al 2011).
In other cases,our system misclassifies ?New York City?
for ?NewYork?
and ?Netherlands?
for ?Holland?
and ?Peo-ple?s Republic of China?
for ?China?, because inall these cases, the latter ones are the most popu-lar in Wikipedia.
It is even hard for us humans totell the difference based only on context or globalcoherence.5 ConclusionsWe propose a stacking based collective entity link-ing method, which stacks a global predictor on topof a local predictor to collect coherence informationfrom neighboring decisions.
It is fast and easy to im-plement.
Our method trades off between inefficientexact search and greedily assigning mention with lo-cal predictor.
It can be seen as searching relatedentities with relational template in stacked graphi-cal learning, with beam size k. Furthermore, weadopt recent progress in representation learning tolearn category-context coherence model.
It scalesbetter than existing approaches on large knowledgebase and performs comparison in the latent semanticspace.
Combining these two techniques, our modelconsistently outperforms all existing more sophisti-cated collective approaches in our experiments.AcknowledgmentsThis research was partly supported by Ma-jor National Social Science Fund of China(No.12&ZD227),National High Technology Researchand Development Program of China (863 Program)(No.
2012AA011101) and National Natural ScienceFoundation of China (No.91024009).ReferencesB.
Bai, J. Weston, D. Grangier, R. Collobert, O. Chapelle,and K. Weinberger.
2009.
Supervised semantic index-ing.
In The 18th ACM Conference on Information andKnowledge Management (CIKM).R.
Bunescu and M. Pasca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
In Pro-ceedings of EACL, volume 6, pages 9?16.S.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on wikipedia data.
In Proceedings ofEMNLP-CoNLL, volume 6, pages 708?716.X.
Han, L. Sun, and J. Zhao.
2011.
Collective entitylinking in web text: a graph-based method.
In Pro-434ceedings of the 34th international ACM SIGIR con-ference on Research and development in InformationRetrieval, pages 765?774.
ACM.J.
Hoffart, M.A.
Yosef, I. Bordino, H. Fu?rstenau,M.
Pinkal, M. Spaniol, B. Taneva, S. Thater, andG.
Weikum.
2011.
Robust disambiguation of namedentities in text.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 782?792.
Association for Computational Lin-guistics.Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-fitt, and Joe Ellis.
2011.
Overview of the tac 2011knowledge base population track.
In Proceedings ofthe Fourth Text Analysis Conference.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 133?142.ACM.S.S.
Kataria, K.S.
Kumar, R. Rastogi, P. Sen, and S.H.Sengamedu.
2011.
Entity disambiguation with hierar-chical topic models.
In Proceedings of KDD.Zhenzhen Kou and William W Cohen.
2007.
Stackedgraphical models for efficient inference in markov ran-dom fields.
In SDM.S.
Kulkarni, A. Singh, G. Ramakrishnan, andS.
Chakrabarti.
2009.
Collective annotation ofwikipedia entities in web text.
In Proceedings ofthe 15th ACM SIGKDD international conferenceon Knowledge discovery and data mining, pages457?466.
ACM.J.
Lehmann, S. Monahan, L. Nezda, A. Jung, and Y. Shi.2010.
Lcc approaches to knowledge base populationat tac 2010.
In Proc.
TAC 2010 Workshop.F.
Li, Z. Zheng, F. Bu, Y. Tang, X. Zhu, and M. Huang.2009.
Thu quanta at tac 2009 kbp and rte track.
InProceedings of Test Analysis Conference 2009 (TAC09).Andre?
FT Martins, Dipanjan Das, Noah A Smith, andEric P Xing.
2008.
Stacking dependency parsers.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 157?166.
As-sociation for Computational Linguistics.D.
Milne and I.H.
Witten.
2008.
Learning to link withwikipedia.
In Proceedings of the 17th ACM con-ference on Information and knowledge management,pages 509?518.
ACM.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambiguationto wikipedia.
In Proceedings of the Annual Meeting ofthe Association of Computational Linguistics (ACL).P.
Sen. 2012.
Collective context-aware topic modelsfor entity disambiguation.
In Proceedings of the 21stinternational conference on World Wide Web, pages729?738.
ACM.M.
Shirakawa, H. Wang, Y.
Song, Z. Wang,K.
Nakayama, T. Hara, and S. Nishio.
2011.Entity disambiguation based on a probabilistictaxonomy.
Technical report, Technical ReportMSR-TR-2011-125, Microsoft Research.N.A.
Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 354?362.
Asso-ciation for Computational Linguistics.Weiwei Sun.
2011.
A stacked sub-word model forjoint chinese word segmentation and part-of-speechtagging.
In ACL, pages 1385?1394.David H Wolpert.
1992.
Stacked generalization.
Neuralnetworks, 5(2):241?259.Zhicheng Zheng, Fangtao Li, Minlie Huang, and XiaoyanZhu.
2010.
Learning to link entities with knowledgebase.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages483?491, Los Angeles, California, June.
Associationfor Computational Linguistics.435
