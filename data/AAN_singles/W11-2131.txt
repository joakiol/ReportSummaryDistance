Proceedings of the 6th Workshop on Statistical Machine Translation, pages 272?283,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsInstance Selection for Machine Translation using Feature DecayAlgorithmsErgun Bi?iciKo?
University34450 Sariyer, Istanbul, Turkeyebicici@ku.edu.trDeniz YuretKo?
University34450 Sariyer, Istanbul, Turkeydyuret@ku.edu.trAbstractWe present an empirical study of instance se-lection techniques for machine translation.
Inan active learning setting, instance selectionminimizes the human effort by identifyingthe most informative sentences for transla-tion.
In a transductive learning setting, se-lection of training instances relevant to thetest set improves the final translation qual-ity.
After reviewing the state of the art inthe field, we generalize the main ideas in aclass of instance selection algorithms that usefeature decay.
Feature decay algorithms in-crease diversity of the training set by devalu-ing features that are already included.
Weshow that the feature decay rate has a verystrong effect on the final translation qualitywhereas the initial feature values, inclusionof higher order features, or sentence lengthnormalizations do not.
We evaluate the bestinstance selection methods using a standardMoses baseline using the whole 1.6 millionsentence English-German section of the Eu-roparl corpus.
We show that selecting thebest 3000 training sentences for a specifictest sentence is sufficient to obtain a scorewithin 1 BLEU of the baseline, using 5% ofthe training data is sufficient to exceed thebaseline, and a?
2 BLEU improvement overthe baseline is possible by optimally selectedsubset of the training data.
In out-of-domaintranslation, we are able to reduce the train-ing set size to about 7% and achieve a similarperformance with the baseline.1 IntroductionStatistical machine translation (SMT) makes useof a large number of parallel sentences, sentenceswhose translations are known in the target lan-guage, to derive translation tables, estimate param-eters, and generate the actual translation.
Not allof the parallel corpus nor the translation table thatis generated is used during decoding a given setof test sentences and filtering is usually performedfor computational advantage (Koehn et al, 2007).Some recent regression-based statistical machinetranslation systems rely on a small sized trainingdata to learn the mappings between source and tar-get features (Wang and Shawe-Taylor, 2008; Ser-rano et al, 2009; Bicici and Yuret, 2010).
Regres-sion has some computational disadvantages whenscaling to large number of training instances.Previous work shows that the more the trainingdata, the better the translations become (Koehn,2006).
However, with the increased size of theparallel corpus there is also the added noise, mak-ing relevant instance selection important.
Phrase-based SMT systems rely heavily on accuratelylearning word alignments from the given parallelcorpus.
Proper instance selection plays an impor-tant role in obtaining a small sized training set withwhich correct alignments can be learned.
Word-level translation accuracy is also affected by thenumber of times a word occurs in the parallel cor-pus (Koehn and Knight, 2001).
Koehn and Knightfind that about 50 examples per word are requiredto achieve a performance close to using a bilinguallexicon in their experiments.
Translation perfor-mance can improve as we include multiple possi-ble translations for a given word, which increases272the diversity of the training set.Transduction uses test instances, which cansometimes be accessible at training time, to learnspecific models tailored towards the test set whichalso reduces computation by not using the fulltraining set.
Transductive retrieval selects train-ing data close to the test set given a parallel corpusand a test set.
This work shows that transductiveretrieval of the training set for statistical machinetranslation allows us to achieve a performance bet-ter than using all of the parallel corpus.
When se-lecting training data, we seek to maximize the cov-erage or the percentage of test source and targetfeatures (i.e.
n-grams) found in the training set us-ing minimal number of target training features anda fixed number of training instances.
Diversifyingthe set of training sentences can help us increasethe coverage.
We show that target coverage boundsthe achievable BLEU score with a given trainingset and small increases can result in large increaseson this BLEU bound.We develop the feature decay algorithms (FDA)that aim to maximize the coverage of the targetlanguage features and achieve significant gains intranslation performance.
We find that decayingfeature weights has significant effect on the per-formance.
We achieve improvements of ?2 BLEUpoints using about 20% of the available trainingdata in terms of target words and ?1 BLEU pointswith only about 5%.
We show that selecting 3000instances for a test sentence is sufficient to obtaina score within 1 BLEU of the baseline.
In the out-of-domain translation task, we are able to reducethe training set size to its 7% to achieve a similarperformance with the baseline.The next section reviews related previous work.We discuss the FDA in section 3.
Section 4presents our coverage and translation results bothin and out-of-domain and includes an instance se-lection method also designed for improving wordalignment results.
We list our contributions in thelast section.2 Related WorkTransductive learning makes use of test instances,which can sometimes be accessible at trainingtime, to learn specific models tailored towards thetest set.
Selection of training instances relevant tothe test set improves the final translation quality asin transductive learning and decreases human ef-fort by identifying the most informative sentencesfor translation as in active learning.
Instance se-lection in a transductive learning framework se-lects the best instances for a given test set (L?
etal., 2007).
Active learning selects training samplesthat will benefit the learning algorithm the mostover the unlabeled dataset U from a labeled train-ing set L or from U itself after labeling (Banko andBrill, 2001).
Active learning in SMT selects whichinstances to add to the training set to improve theperformance of a baseline system (Haffari et al,2009; Ananthakrishnan et al, 2010).
Recent workinvolves selecting sentence or phrase translationtasks for external human effort (Bloodgood andCallison-Burch, 2010).
Below we present exam-ples of both with a label indicating whether theyfollow an approach close to active learning [AL] ortransductive learning [TL] and in our experimentswe use the transductive framework.TF-IDF [TL]: L?
et al (2007) use tf-idf infor-mation retrieval technique based cosine score to se-lect a subset of the parallel corpus close to the testset for SMT training.
They outperform the baselinesystem when the top 500 training instances per testsentence are selected.
The terms used in their tf-idfmeasure correspond to words where this work fo-cuses on bigram feature coverage.
When the com-bination of the top N selected sentences are usedas the training set, they show increase in the per-formance at the beginning and decrease when 2000sentences are selected for each test sentence.N-gram coverage [AL]: Eck et al (2005) usen-gram feature coverage to sort and select traininginstances using the following score:?NGRAM (S) =?ni=1?unseen x ?
Xi(S) C(x)|S|,(1)for sentence S with Xi(S) storing the i-gramsfound in S and C(x) returning the count of x inthe parallel corpus.
?NGRAM score sums over un-seen n-grams to increase the coverage of the train-ing set.
The denominator involving the length ofthe sentence takes the translation cost of the sen-tence into account.
Eck et al (2005) also notethat longer sentences are more difficult for train-ing SMT models.
In their experiments, they arenot able to reach a performance above the baseline2273system?s BLEU score, which is using all of the par-allel corpus, but they achieve close performance byusing about 15% of the parallel corpus.DWDS [AL]: Density weighted diversity sam-pling (DWDS) (Ambati et al, 2010) score tries toselect sentences containing the n-gram features inthe unlabeled dataset U while increasing the di-versity among the sentences selected, L (labeled).DWDS increases the score of a sentence with in-creasing frequency of its n-grams found in U anddecreases with increasing frequency in the alreadyselected set of sentences, L, in favor of diversity.Let PU (x) denote the probability of feature x in Uand CL(x) denote its count in L. Then:d(S) =?x?X(S) PU (x)e?
?CL(x)|X(S)|(2)u(S) =?x?X(S) I(x 6?
X(L))|X(S)|(3)?DWDS(S) =2d(S)u(S)d(S) + u(S), (4)where X(S) stores the features of S and ?
is adecay parameter.
d(S) denotes the density of Sproportional to the probability of its features in Uand inversely proportional to their counts in L andu(S) its uncertainty, measuring the percentage ofnew features in S. These two scores are combinedusing harmonic mean.
DWDS tries to select sen-tences containing similar features in U with highdiversity.
In their active learning experiments, theyselected 1000 training instances in each iterationand retrained the SMT system.Log-probability ratios [AL]: Haffari etal.
(2009) develop sentence selection scores usingfeature counts in L and U , increasing for frequentfeatures in U and decreasing for frequent featuresin L. They use geometric and arithmetic averagesof log-probability ratios in an active learningsetting where 200 sentences from U are selectedand added to L with their translations for 25iterations (Haffari et al, 2009).
Later, Haffariet al (2009) distinguish between features foundin the phrase table, xreg, and features not found,xoov.
OOV features are segmented into subfeatures(i.e.
feature ?go to school?
is segmented as:(go to school), (go)(to school), (go to)(school),(go)(to)(school)).
Expected log probability ratio(ELPR) score is used:?ELPR(S) = 0.4|Xreg(S)|?x?Xreg(S)logPU (x)PL(x)+ 0.6|Xoov(S)|?x?Xoov(S)?h?H(x)1|H(x)|?y?Yh(x)logPU (y)PL(y),(5)where H(x) return the segmentations of x andYh(x) return the features found in segment h.?ELPR performs better than geometric average intheir experiments (Haffari and Sarkar, 2009).Perplexity [AL & TL]: Perplexity of the train-ing instance as well as inter-SMT-system disagree-ment are also used to select training data for trans-lation models (Mandal et al, 2008).
The increaseddifficulty in translating a parallel sentence or itsnovelty as found by the perplexity adds to its im-portance for improving the SMT model?s perfor-mance.
A sentence having high perplexity (a raresentence) in L and low perplexity (a common sen-tence) in U is considered as a candidate for addi-tion.
They are able to improve the performanceof a baseline system trained on some initial corpustogether with additional parallel corpora using theinitial corpus and part of the additional data.Alignment [TL]: Uszkoreit et al (2010) mineparallel text to improve the performance of a base-line translation model on some initial documenttranslation tasks.
They retrieve similar documentsusing inverse document frequency weighted cosinesimilarity.
Then, they filter nonparallel sentencesusing their word alignment performance, which isestimated using the following score:score(A) =?
(s,t)?Alnp(s, t)p(s)p(t), (6)where A stands for an alignment between sourceand target words and the probabilities are estimatedusing a word aligned corpus.
The produced paral-lel data is used to expand a baseline parallel corpusand shown to improve the translation performanceof machine translation systems.3 Instance Selection with FeatureDecayIn this section we will describe a class of instanceselection algorithms for machine translation that3274use feature decay, i.e.
increase the diversity of thetraining set by devaluing features that have alreadybeen included.
Our abstraction makes three com-ponents of such algorithms explicit permitting ex-perimentation with their alternatives:?
The value of a candidate training sentence asa function of its features.?
The initial value of a feature.?
The update of the feature value as instancesare added to the training set.A feature decay algorithm (FDA) aims to max-imize the coverage of the target language features(such as words, bigrams, and phrases) for the testset.
A target language feature that does not ap-pear in the selected training instances will be dif-ficult to produce regardless of the decoding algo-rithm (impossible for unigram features).
In gen-eral we do not know the target language features,only the source language side of the test set is avail-able.
Unfortunately, selecting a training instancewith a particular source language feature does notguarantee the coverage of the desired target lan-guage feature.
There may be multiple translationsof a feature appropriate for different senses or dif-ferent contexts.
For each source language featurein the test set, FDA tries to find as many train-ing instances as possible to increase the chancesof covering the appropriate target language feature.It does this by reducing the value of the featuresthat are already included after picking each train-ing instance.
Algorithm 1 gives the pseudo-codefor FDA.The input to the algorithm is a parallel corpus,the number of desired training instances, and thesource language features of the test set.
We useunigram and bigram features; adding trigram fea-tures does not seem to significantly affect the re-sults.
The user has the option of running the algo-rithm for each test sentence separately, then possi-bly combining the resulting training sets.
We willpresent results with these variations in Section 4.The first foreach loop initializes the value ofeach test set feature.
We experimented with ini-tial feature values that are constant, proportionalto the length of the n-gram, or log-inverse of thecorpus frequency.
We have observed that the ini-tial value does not have a significant effect on theAlgorithm 1: The Feature Decay AlgorithmInput: Bilingual corpus U , test set features F ,and desired number of traininginstances N .Data: A priority queue Q, sentence scoresscore, feature values fvalue.Output: Subset of the corpus to be used as thetraining data L ?
U .foreach f ?
F do1fvalue(f)?
init(f,U)2foreach S ?
U do3score(S)?
?f?features(S) fvalue(f)4push(Q, S,score(S))5while |L| < N do6S ?
pop(Q)7score(S)?
?f?features(S) fvalue(f)8if score(S) ?
topval(Q) then9L ?
L ?
{S}10foreach f ?
features(S) do11fvalue(f)?
decay(f,U ,L)12else13push(Q, S,score(S))14quality of training instances selected.
The featuredecay rule dominates the behavior of the algorithmafter the first few iterations.
However, we preferthe log-inverse values because they lead to fewerscore ties among candidate instances and result infaster running times.The second foreach loop initializes the score foreach candidate training sentence and pushes themonto a priority queue.
The score is calculated as thesum of the feature values.
Note that as we changethe feature values, the sentence scores in the prior-ity queue will no longer be correct.
However theywill still be valid upper bounds because the fea-ture values only get smaller.
Features that do notappear in the test set are considered to have zerovalue.
This observation can be used to speed upthe initialization by using a feature index and onlyiterating over the sentences that have features incommon with the test set.Finally the while loop populates the training setby picking candidate sentences with the highestscores.
This is done by popping the top scoringcandidate S from the priority queue at each itera-tion.
We recalculate its score because the values4275of its features may have changed.
We compare therecalculated score of S with the score of the nextbest candidate.
If the score of S is equal or betterwe are sure that it is the top candidate because thescores in the priority queue are upper bounds.
Inthis case we place S in our training set and decaythe values of its features.
Otherwise we push Sback on the priority queue with its updated score.The feature decay function on Line 12 is theheart of the algorithm.
Unlike the choice of fea-tures (bigram vs trigram) or their initial values(constant vs log?inverse?frequency) the rate of de-cay has a significant effect on the performance.
Wefound it is optimal to reduce feature values at a rateof 1/n where n is the current training set countof the feature.
The results get significantly worsewith no feature decay.
They also get worse withfaster, exponential feature decay, e.g.
1/2n.
Ta-ble 1 presents the experimental results that supportthese conclusions.
We use the following settingsfor the experiments in Section 4:init(f,U) = 1 or log(|U|/cnt(f,U))decay(f,U ,L) =init(f,U)1 + cnt(f,L)orinit(f,U)1 + 2cnt(f,L)init decay en?de de?en1 none .761 .484 .698 .556log(1/f) none .855 .516 .801 .6041 1/n .967 .575 .928 .664log(1/f) 1/n .967 .570 .928 .6561 1/2n .967 .553 .928 .653log(1/f) 1/2n .967 .557 .928 .651Table 1: FDA experiments.
The first two columnsgive the initial value and decay formula used forfeatures.
f is the corpus frequency of a featureand n is its count in selected instances.
The nextfour columns give the expected coverage of thesource and target language bigrams of a test sen-tence when 100 training sentences are selected.4 ExperimentsWe perform translation experiments on theEnglish-German language pair using the parallelcorpus provided in WMT?10 (Callison-Burch etal., 2010).
The English-German section of the Eu-roparl corpus contains about 1.6 million sentences.We perform in-domain experiments to discriminateamong different instance selection techniques bet-ter in a setting with low out-of-vocabulary rate.
Werandomly select the test set test with 2, 588 tar-get words and separate development set dev with26, 178 target words.
We use the language modelcorpus provided in WMT?10 (Callison-Burch etal., 2010) to build a 5-gram model.We use target language bigram coverage, tcov,as a quality measure for a given training set, whichmeasures the percentage of the target bigram fea-tures of the test sentence found in a given trainingset.
We compare tcov and the translation perfor-mance of FDA with related work.
We also performsmall scale SMT experiments where only a coupleof thousand training instances are used for each testsentence.4.1 The Effect of Coverage on TranslationBLEU (Papineni et al, 2001) is a precision basedmeasure and uses n-gram match counts up to or-der n to determine the quality of a given transla-tion.
The absence of a given word or translatingit as another word interrupts the continuity of thetranslation and decreases the BLEU score even ifthe order among the words is determined correctly.Therefore, the target coverage of an out-of-domaintest set whose translation features are not found inthe training set bounds the translation performanceof an SMT system.We estimate this translation performance boundfrom target coverage by assuming that the miss-ing tokens can appear randomly at any location ofa given sentence where sentence lengths are nor-mally distributed with mean 25.6 and standard de-viation 14.1.
This is close to the sentence lengthstatistics of the German side Europarl corpus usedin WMT?10 (WMT, 2010).
We replace all un-known words found with an UNK token and calcu-late the BLEU score.
We perform this experimentfor 10, 000 instances and repeat for 10 times.The obtained BLEU scores for target cover-age values is plotted in Figure 1 with label esti-mate.
We also fit a third order polynomial func-tion of target coverage 0.025 BLEU scores abovethe estimate values to show the similarity with the52760.0 0.2 0.4 0.6 0.8 1.0tcov0.00.20.40.60.81.01.2BLEUBLEU vs. tcovestimatef(x)=ax^3 + bx^2 + cx + dFigure 1: Effect of coverage on translation perfor-mance.
BLEU bound is a third-order function oftarget coverage.
High coverage?
High BLEU.BLEU scores bound estimated, whose parametersare found to be [0.56, 0.53,?0.09, 0.003] with aleast-squares fit.
Figure 1 shows that the BLEUscore bound obtained has a third-order polyno-mial relationship with target coverage and smallincreases in the target coverage can result in largeincreases on this BLEU bound.4.2 Coverage ResultsWe select N training instances per test sentenceusing FDA (Algorithm 1), TF-IDF with bigramfeatures, NGRAM scoring (Equation 1), DWDS(Equation 4), and ELPR (Equation 5) techniquesfrom previous work.
For the active learning algo-rithms, source side test corpus becomes U and theselected training set L. For all the techniques, wecompute 1-grams and 2-grams as the features usedin calculating the scores and add only one sentenceto the training set at each iteration except for TF-IDF.
We set ?
parameter of DWDS to 1 as givenin their paper.
We adaptively select the top scor-ing instance at each step from the set of possiblesentences U with a given scorer ?(.)
and add theinstance to the training set, L, until the size of Lreaches N for the related work other than TF-IDF.We test all algorithms in this transductive setting.We measure the bigram coverage when all ofthe training sentences selected for each test sen-tence are combined.
The results are presented inFigure 2 where the x-axis is the number of wordsof the training set and y-axis is the target cover-age obtained.
FDA has a steep slope in its increaseand it is able to reach target coverage of ?
0.84.DWDS performs worse initially but its target cov-erage improve after a number of instances are se-lected due to its exponential feature decay proce-dure.
TF-IDF performs worse than DWDS and itprovides a fast alternative to FDA instance selec-tion but with some decrease in coverage.
ELPRand NGRAM instance selection techniques per-form worse.
NGRAM achieves better coveragethan ELPR, although it lacks a decay procedure.When we compare the sentences selected, weobserve that FDA prefers longer sentences due tosumming feature weights and it achieves larger tar-get coverage value.
NGRAM is not able to discrim-inate between sentences well and a lot of sentencesof the same length get the same score when the un-seen n-grams belong to the same frequency class.The statistics of L obtained with the instance se-lection techniques differ from each other as givenin Table 2, where N = 1000 training instances se-lected per test sentence.
We observe that DWDShas fewer unique target bigram features than TF-IDF although it selects longer target sentences.NGRAM obtains a large number of unique targetbigrams although its selected target sentences havesimilar lengths with DWDS and ELPR prefers shortsentences.Technique Unique bigrams Words per sent tcovFDA 827,928 35.8 .74DWDS 412,719 16.7 .67TF-IDF 475,247 16.2 .65NGRAM 626,136 16.6 .55ELPR 172,703 10.9 .35Table 2: Statistics of the obtained target L forN =1000.4.3 Translation ResultsWe develop separate phrase-based SMT modelsusing Moses (Koehn et al, 2007) using default set-tings with maximum sentence length set to 80 andobtained baseline system score as 0.3577 BLEU.We use the training instances selected by FDA in6277104 105 106 107Training Set Size (words)0.10.20.30.40.50.60.70.80.9tcovtcov vs. Training Set Size (words)DWDSELPRFDANGRAMTFIDFFigure 2: Target coverage curve comparison with previous work.
Figure shows the rate of increase intcov as the size of L increase.three learning settings:L?
L is the union of the instances selected foreach test sentence.L?F L is selected using all of the features foundin the test set.LI L is the set of instances selected for each testsentence.We develop separate Moses systems with eachtraining set and LI corresponds to developing aMoses system for each test sentence.
L?
resultsare plot in Figure 3 where we increasingly selectN ?
{100, 200, 500, 1000, 2000, 3000, 5000,10000} instances for each test sentence for train-ing.
The improvements over the baseline are sta-tistically significant with paired bootstrap resam-pling using 1000 samples (Koehn, 2004).
As weselect more instances, the performance of the SMTsystem increases as expected and we start to see adecrease in the performance after selecting ?107target words.
We obtain comparable results for thede-en direction.
The performance increase is likelyto be due to the reduction in the number of noisy orirrelevant training instances and the increased pre-cision in the probability estimates in the generatedphrase tables.105 106 107 108Training Set Size (words)0.300.310.320.330.340.350.360.370.38BLEU0.30580.33180.3410.36450.36970.37580.36220.36530.3577BLEU vs. Training Set Size (words)Figure 3: BLEU vs. the number of target words inL?.L?F results given in Table 3 show that we canachieve within 1 BLEU performance using about3% of the parallel corpus target words (30,000 in-stances) and better performance using only about5% (50,000 instances).The results with LI when building an individ-7278# sent # target words BLEU NIST10,000 449,116 0.3197 5.778820,000 869,908 0.3417 6.005330,000 1,285,096 0.3492 6.024650,000 2,089,403 0.3711 6.1561100,000 4,016,124 0.3648 6.1331ALL 41,135,754 0.3577 6.0653Table 3: Performance for en-de using L?F .
ALLcorresponds to the baseline system using all of theparallel corpus.
bold correspond to statisticallysignificant improvement over the baseline result.ual Moses model for each test sentence are givenin Table 4.
Individual SMT training and transla-tion can be preferable due to smaller computationalcosts and high parallelizability.
As we translatea single sentence with each SMT system, tuningweights becomes important.
We experiment threesettings: (1) using 100 sentences for tuning, whichare randomly selected from dev.1000, (2) using themean of the weights obtained in (1), and (3) us-ing the weights obtained in the union learning set-ting (L?).
We observe that we can obtain a perfor-mance within 2 BLEU difference to the baselinesystem by training on 3000 instances per sentence(underlined) using the mean weights and 1 BLEUdifference using the union weights.
We also exper-imented with increasing the N -best list size usedduring MERT optimization (Hasan et al, 2007),with increased computational cost, and observedsome increase in the performance.N 100 dev sents Mean Union1000 0.3149 0.3242 0.33542000 0.3258 0.3352 0.33953000 0.3270 0.3374 0.35015000 0.3217 0.3303 0.3458Table 4: LI performance for en-de using 100 sen-tences for tuning or mean of the weights or devweights obtained with the union setting.Comparison with related work: Table 5presents the translation results compared with pre-vious work selecting 1000 instances per test sen-tence.
We observe that coverage and translationperformance are correlated.
Although the cover-age increase of DWDS and FDA appear similar,due to the third-order polynomial growth of BLEUwith respect to coverage, we achieve large BLEUgains in translation.
We observe increased BLEUgains when compared with the results of TF-IDF,NGRAM, and ELPR in order.FDA DWDS TF-IDF NGRAM ELPR0.3645 0.3547 0.3405 0.2572 0.2268Table 5: BLEU results using different techniqueswith N = 1000.
High coverage?
High BLEU.We note that DWDS originally selects instancesusing the whole test corpus to estimate PU (x) andselects 1000 instances at each iteration.
We exper-imented with both of these settings and obtained0.3058 and 0.3029 BLEU respectively.
Lowerperformance suggest the importance of updatingweights after each instance selection step.4.4 Instance Selection for AlignmentWe have shown that high coverage is an integralpart of training sets for achieving high BLEU per-formance.
SMT systems also heavily rely on theword alignment of the parallel corpus to derivea phrase table that can be used for translation.GIZA++ (Och and Ney, 2003) is commonly usedfor word alignment and phrase table generation,which is prone to making more errors as the lengthof the training sentence increase (Ravi and Knight,2010).
Therefore, we analyze instance selectiontechniques that optimize coverage and word align-ment performance and at the same time do notproduce very long sentences.
Too few words persentence may miss the phrasal structure, whereastoo many words per sentence may miss the actualword alignment for the features we are interested.We are also trying to retrieve relevant training sen-tences for a given test sentence to increase the fea-ture alignment performance.Shortest: A baseline strategy that can minimizethe training feature set?s size involves selecting theshortest translations containing each feature.Co-occurrence: We use co-occurrence ofwords in the parallel corpus to retrieve sentencescontaining co-occurring items.
Dice?s coeffi-cient (Dice, 1945) is used as a heuristic word align-ment technique giving an association score foreach pair of word positions (Och and Ney, 2003).8279We define Dice?s coefficient score as:dice(x, y) =2C(x, y)C(x)C(y), (7)where C(x, y) is the number of times x and y co-occur and C(x) is the count of observing x in theselected training set.
Given a test source sentence,SU , we can estimate the goodness of a trainingsentence pair, (S, T ), by the sum of the alignmentscores:?dice(SU , S, T ) =Xx?X(SU )|T |Xj=1Xy?Y (x)dice(y, Tj)|T | log |S|,(8)where X(SU ) stores the features of SU and Y (x)lists the tokens in feature x.
The difficulty of wordaligning a pair of training sentences, (S, T ), can beapproximated by |S||T |.
We use a normalizationfactor proportional to |T | log |S|.The average target words per sentence using?dice drops to 26.2 compared to 36.3 of FDA.We still obtain a better performance than the base-line en-de system with the union of 1000 train-ing instances per sentence with 0.3635 BLEU and6.1676 NIST scores.
Coverage comparison withFDA shows slight improvement with lower numberof target bigrams and similar trend for others (Fig-ure 4).
We note that shortest strategy achieves bet-ter performance than both ELPR and NGRAM.
Weobtain 0.3144 BLEU and 5.5 NIST scores in theindividual translation task with 1000 training in-stances per sentence and 0.3171 BLEU and 5.4662NIST scores when the mean of the weights is used.4.5 Out-of-domain Translation ResultsWe have used FDA and dice algorithms to selecttraining sets for the out-of-domain challenge testsets used in (Callison-Burch et al, 2011).
Theparallel corpus contains about 1.9 million trainingsentences and the test set contain 3003 sentences.We built separate Moses systems using all of theparallel corpus for the language pairs en-de, de-en,en-es, and es-en.
We created training sets usingall of the features of the test set to select train-ing instances.
The results given in Table 6 showthat we can achieve similar BLEU performance us-ing about 7% of the parallel corpus target words(200,000 instances) using dice and about 16% us-ing FDA.
In the out-of-domain translation task, weare able to reduce the training set size to achievea performance close to the baseline.
The samplepoints presented in the table is chosen proportionalto the relative sizes of the parallel corpus sizes ofWMT?10 and WMT?11 datasets and the trainingset size of the peak in Figure 3.
We may be ableto achieve better performance in the out-of-domaintask as well.
The sample points in Table 6 may beon either side of the peak.5 ContributionsWe have introduced the feature decay algorithms(FDA), a class of instance selection algorithms thatuse feature decay, which achieves better target cov-erage than previous work and achieves significantgains in translation performance.
We find that de-caying feature weights has significant effect on theperformance.
We demonstrate that target coverageand translation performance are correlated, show-ing that target coverage is also a good indicator ofBLEU performance.
We have shown that targetcoverage provides an upper bound on the transla-tion performance with a given training set.We achieve improvements of ?2 BLEU pointsusing about 20% of the available training data interms of target words with FDA and ?
1 BLEUpoints with only about 5%.
We have also shownthat by training on only 3000 instances per sen-tence we can reach within 1 BLEU difference tothe baseline system.
In the out-of-domain transla-tion task, we are able to reduce the training set sizeto achieve a similar performance with the baseline.Our results demonstrate that SMT systems canimprove their performance by transductive train-ing set selection.
We have shown how to select in-stances and achieved significant performance im-provements.ReferencesVamshi Ambati, Stephan Vogel, and Jaime Carbonell.2010.
Active learning and crowd-sourcing for ma-chine translation.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, Mike Rosner,and Daniel Tapias, editors, Proceedings of the Seventhconference on International Language Resources andEvaluation (LREC?10), Valletta, Malta, May.
Euro-pean Language Resources Association (ELRA).9280104 105 106 107Total Training Set Size (words)0.30.40.50.60.70.80.9tcovtcov vs. Total Training Set Size (words)FDAdiceshortest0 5000 10000 15000 20000 25000 30000 35000 40000Average Training Set Size (words)0.10.20.30.40.50.60.70.8tcovtcov vs. Average Training Set Size (words)FDAdiceshortestFigure 4: Target coverage per target words comparison.
Figure shows the rate of increase in tcov asthe size of L increase.
Target coverage curves for total training set size is given on the left plot and foraverage training set size per test sentence on the right plot.en-de de-en en-es es-enBLEUALL 0.1376 0.2074 0.2829 0.2919FDA 0.1363 0.2055 0.2824 0.2892dice 0.1374 0.2061 0.2834 0.2857# target words ?106ALL 47.4 49.6 52.8 50.4FDA 7.9 8.0 8.7 8.2dice 6.9 7.0 3.9 3.6% of ALLFDA 17 16 16 16dice 14 14 7.4 7.1Table 6: Performance for the out-of-domain task of (Callison-Burch et al, 2011).
ALL corresponds tothe baseline system using all of the parallel corpus.10281Sankaranarayanan Ananthakrishnan, Rohit Prasad,David Stallard, and Prem Natarajan.
2010.
Dis-criminative sample selection for statistical machinetranslation.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 626?635, Cambridge, MA, October.Association for Computational Linguistics.Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of 39th Annual Meeting of theAssociation for Computational Linguistics, pages 26?33, Toulouse, France, July.
Association for Computa-tional Linguistics.Ergun Bicici and Deniz Yuret.
2010.
L1 regularizedregression for reranking and system combination inmachine translation.
In Proceedings of the ACL 2010Joint Fifth Workshop on Statistical Machine Transla-tion and Metrics MATR, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Michael Bloodgood and Chris Callison-Burch.
2010.Bucking the trend: Large-scale cost-focused activelearning for statistical machine translation.
In Pro-ceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 854?864, Uppsala, Sweden, July.
Association for Compu-tational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, and Omar Zaidan, editors.
2010.
Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR.
Associa-tion for Computational Linguistics, Uppsala, Sweden,July.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, and Omar Zaidan, editors.
2011.
Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.
Edinburgh, England, July.Lee R. Dice.
1945.
Measures of the amount of ecologicassociation between species.
Ecology, 26(3):297?302.Matthias Eck, Stephan Vogel, and Alex Waibel.
2005.Low cost portability for statistical machine transla-tion based on n-gram coverage.
In Proceedings ofthe 10th Machine Translation Summit, MT Summit X,pages 227?234, Phuket, Thailand, September.Gholamreza Haffari and Anoop Sarkar.
2009.
Activelearning for multilingual statistical machine transla-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP, pages 181?189, Suntec, Sin-gapore, August.
Association for Computational Lin-guistics.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active learning for statistical phrase-based ma-chine translation.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 415?423, Boulder,Colorado, June.
Association for Computational Lin-guistics.Sa?a Hasan, Richard Zens, and Hermann Ney.
2007.Are very large N-best lists useful for SMT?
In HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Companion Volume, Short Pa-pers, pages 57?60, Rochester, New York, April.
As-sociation for Computational Linguistics.Philipp Koehn and Kevin Knight.
2001.
Knowledgesources for word-level translation models.
In Pro-ceedings of the 2001 Conference on Empirical Meth-ods in Natural Language Processing.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InAnnual Meeting of the Assoc.
for Computational Lin-guistics, pages 177?180, Prague, Czech Republic,June.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Philipp Koehn.
2006.
Statistical machine translation:the basic, the novel, and the speculative.
Tutorial atEACL 2006.Yajuan L?, Jin Huang, and Qun Liu.
2007.
Improvingstatistical machine translation performance by train-ing data selection and optimization.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages343?350, Prague, Czech Republic, June.
Associationfor Computational Linguistics.A.
Mandal, D. Vergyri, W. Wang, J. Zheng, A. Stol-cke, G. Tur, D. Hakkani-Tur, and N.F.
Ayan.
2008.Efficient data selection for machine translation.
InSpoken Language Technology Workshop, 2008.
SLT2008.
IEEE, pages 261 ?264.Franz Josef Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automatic eval-uation of machine translation.
In ACL ?02: Proceed-ings of the 40th Annual Meeting on Association for11282Computational Linguistics, pages 311?318, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Sujith Ravi and Kevin Knight.
2010.
Does giza++make search errors?
Computational Linguistics,36(3):295?302.Nicolas Serrano, Jesus Andres-Ferrer, and FranciscoCasacuberta.
2009.
On a kernel regression approachto machine translation.
In Iberian Conference on Pat-tern Recognition and Image Analysis, pages 394?401.Jakob Uszkoreit, Jay Ponte, Ashok Popat, and MosheDubiner.
2010.
Large scale parallel document miningfor machine translation.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (Coling 2010), pages 1101?1109, Beijing, China,August.
Coling 2010 Organizing Committee.Zhuoran Wang and John Shawe-Taylor.
2008.
Kernelregression framework for machine translation: UCLsystem description for WMT 2008 shared translationtask.
In Proceedings of the Third Workshop on Sta-tistical Machine Translation, pages 155?158, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.WMT.
2010.
ACL Workshop: Joint Fifth Workshop onStatistical Machine Translation and Metrics MATR,July.12283
