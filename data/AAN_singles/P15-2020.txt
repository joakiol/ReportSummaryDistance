Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 119?124,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsExploiting Image Generality for Lexical Entailment DetectionDouwe KielaComputer LaboratoryUniversity of Cambridgedouwe.kiela@cl.cam.ac.ukLaura RimellComputer LaboratoryUniversity of Cambridgelaura.rimell@cl.cam.ac.ukIvan Vuli?cDepartment of Computer ScienceKU Leuvenivan.vulic@cs.kuleuven.beStephen ClarkComputer LaboratoryUniversity of Cambridgestephen.clark@cl.cam.ac.ukAbstractWe exploit the visual properties of con-cepts for lexical entailment detection byexamining a concept?s generality.
We in-troduce three unsupervised methods fordetermining a concept?s generality, basedon its related images, and obtain state-of-the-art performance on two standard se-mantic evaluation datasets.
We also intro-duce a novel task that combines hypernymdetection and directionality, significantlyoutperforming a competitive frequency-based baseline.1 IntroductionAutomatic detection of lexical entailment is usefulfor a number of NLP tasks including search queryexpansion (Shekarpour et al, 2013), recognisingtextual entailment (Garrette et al, 2011), metaphordetection (Mohler et al, 2013), and text genera-tion (Biran and McKeown, 2013).
Given two se-mantically related words, a key aspect of detectinglexical entailment, or the hyponym-hypernym re-lation, is the generality of the hypernym comparedto the hyponym.
For example, bird is more generalthan eagle, having a broader intension and a largerextension.
This property has led to the introduc-tion of lexical entailment measures that comparethe entropy of distributional word representations,under the assumption that a more general term hasa higher-entropy distribution (Herbelot and Gane-salingam, 2013; Santus et al, 2014).A strand of distributional semantics has recentlyemerged that exploits the fact that meaning is of-ten grounded in the perceptual system, known asmulti-modal distributional semantics (Bruni et al,2014).
Such models enhance purely linguisticmodels with extra-linguistic perceptual informa-tion, and outperform language-only models on arange of tasks, including modelling semantic sim-ilarity and conceptual relatedness (Silberer andLapata, 2014).
In fact, under some conditionsuni-modal visual representations outperform tradi-tional linguistic representations on semantic tasks(Kiela and Bottou, 2014).We hypothesize that visual representations canbe particularly useful for lexical entailment detec-tion.
Deselaers and Ferrari (2011) have shown thatsets of images corresponding to terms at higherlevels in the WordNet hierarchy have greater vi-sual variability than those at lower levels.
We ex-ploit this tendency using sets of images returnedby Google?s image search.
The intuition is thatthe set of images returned for animal will consistof pictures of different kinds of animals, the set ofimages for bird will consist of pictures of differ-ent birds, while the set for owl will mostly consistonly of images of owls, as can be seen in Figure 1.Here we evaluate three different vision-basedmethods for measuring term generality on the se-mantic tasks of hypernym detection and hypernymdirectionality.
Using this simple yet effective un-supervised approach, we obtain state-of-the-art re-sults compared with supervised algorithms whichuse linguistic data.2 Related WorkIn the linguistic modality, the most closely relatedwork is by Herbelot and Ganesalingam (2013) andSantus et al (2014), who use unsupervised distri-butional generality measures to identify the hyper-nym in a hyponym-hypernym pair.
Herbelot andGanesalingam (2013) use KL divergence to com-pare the probability distribution of context words,given a term, to the background probability dis-tribution of context words.
Santus et al (2014)use the median entropy of the probability distribu-tions associated with a term?s top-weighted con-119Figure 1: Example of how vulture and owl are less dispersed concepts than bird and animal, accordingto images returned by Google image search.text words as a measure of information content.In the visual modality, the intuition that visualrepresentations may be useful for detecting lexi-cal entailment is inspired by Deselaers and Ferrari(2011).
Using manually annotated images fromImageNet (Deng et al, 2009), they find that con-cepts and categories with narrower intensions andsmaller extensions tend to have less visual vari-ability.
We extend this intuition to the unsuper-vised setting of Google image search results andapply it to the lexical entailment task.3 ApproachWe use two standard evaluations for lexical entail-ment: hypernym directionality, where the task is topredict which of two words is the hypernym; andhypernym detection, where the task is to predictwhether two words are in a hypernym-hyponymrelation (Weeds et al, 2014; Santus et al, 2014).We also introduce a third, more challenging, eval-uation that combines detection and directionality.For the directionality experiment, we evaluateon the hypernym subset of the well-known BLESSdataset (Baroni and Lenci, 2011), which consistsof 1337 hyponym-hypernym pairs.
In this case, itis known that the words are in an entailment re-lation and the task is to predict the directionalityof the relation.
BLESS data is always presentedwith the hyponym first, so we report how often ourmeasures predict that the second term in the pair ismore general than the first.For the detection experiment, we evaluate on theBLESS-based dataset of Weeds et al (2014), whichconsists of 1168 word pairs and which we callWBLESS.
In this dataset, the positive examples arehyponym-hypernym pairs.
The negative examplesBLESSturtle?animal 1WBLESSowl?creature 1owl?vulture 0animal?owl 0BIBLESSowl?creature 1owl?vulture 0animal?owl -1Table 1: Examples for evaluation datasets.include pairs in the reversed hypernym-hyponymorder, as well as holonym-meronym pairs, co-hyponyms, and randomly matched nouns.
Ac-curacy on WBLESS reflects the ability to distin-guish hypernymy from other relations, but doesnot require detection of directionality, since re-versed pairs are grouped with the other negatives.For the combined experiment, we assign re-versed hyponym-hypernym pairs a value of -1 in-stead of 0.
We call this more challenging datasetBIBLESS.
Examples of pairs in the respectivedatasets can be found in Table 1.3.1 Image representationsFollowing previous work in multi-modal seman-tics (Bergsma and Goebel, 2011; Kiela et al,2014), we obtain images from Google Images1for the words in the evaluation datasets.
It hasbeen shown that images from Google yield higher-quality representations than comparable resourcessuch as Flickr and are competitive with ?hand pre-pared datasets?
(Bergsma and Goebel, 2011; Fer-gus et al, 2005).1www.google.com/imghp.
Images were retrieved on10 April, 2015 from Cambridge in the United Kingdom.120For each image, we extract the pre-softmaxlayer from a forward pass in a convolutional neuralnetwork (CNN) that has been trained on the Im-ageNet classification task using Caffe (Jia et al,2014).
As such, this work is an instance of deeptransfer learning; that is, a deep learning represen-tation trained on one task (image classification) isused to make predictions on a different task (im-age generality).
We chose to use CNN-derived im-age representations because they have been foundto be of higher quality than the traditional bag ofvisual words models (Sivic and Zisserman, 2003)that have previously been used in multi-modal dis-tributional semantics (Bruni et al, 2014; Kiela andBottou, 2014).3.2 Generality measuresWe propose three measures that can be used to cal-culate the generality of a set of images.
The imagedispersion d of a concept word w is defined as theaverage pairwise cosine distance between all im-age representations { ~w1... ~wn} of the set of im-ages returned for w:d(w) =2n(n?
1)?i<j?n1?
cos( ~wi, ~wj) (1)This measure was originally introduced to accountfor the fact that perceptual information is more rel-evant for e.g.
elephant than it is for happiness.
Itacts as a substitute for the concreteness of a wordand can be used to regulate how much perceptualinformation should be included in a multi-modalmodel (Kiela et al, 2014).Our second measure follows Deselaers and Fer-rari (2011), who take a similar approach but in-stead of calculating the pairwise distance calculatethe distance to the centroid ~?
of { ~w1... ~wn}:c(w) =1n?1?i?n1?
cos( ~wi, ~?)
(2)For our third measure we follow Lazaridou et al(2015), who try different ways of modulating theinclusion of perceptual input in their multi-modalskip-gram model, and find that the entropy of thecentroid vector ~?
works well (where p(?j) =?j||~?||and m is the vector length):H(w) = ?m?j=1p(?j) log2(p(?j)) (3)3.3 Hypernym Detection and DirectionalityWe calculate the directionality of a hyponym-hypernym pair with a measure f using the follow-ing formula for a word pair (p, q).
Since even co-hyponyms will not have identical values for f , weintroduce a threshold ?which sets a minimum dif-ference in generality for hypernym identification:s(p, q) = 1?f(p) + ?f(q)(4)In other words, s(p, q) > 0 iff f(q) > f(p) + ?,i.e.
if the second word (q) is (sufficiently) moregeneral.
To avoid false positives where one wordis more general but the pair is not semanticallyrelated, we introduce a second threshold ?
whichsets f to zero if the two concepts have low cosinesimilarity.
This leads to the following formula:s?
(p, q) ={1?f(p)+?f(q)if cos( ~?p, ~?q) ?
?0 otherwise(5)We experimented with different methods for ob-taining the mean vector representations for co-sine (hereafter ?c) in Equation (5), and foundthat multi-modal representations worked best.
Weconcatenate an L2-normalized linguistic vectorwith the L2-normalized centroid of image vectorsto obtain a multi-modal representation, followingKiela and Bottou (2014).
For a word p with im-age representations {pimg1... pimgn}, we thus set?c= pling||1n?nipimgi, after normalizing bothrepresentations.
For comparison, we also reportresults for a visual-only ?c.For BLESS, we know the words in a pair standin an entailment relation, so we set ?
= ?
=0 and evaluate whether s(p, q) > 0, indicatingthat q is a hypernym of p. For WBLESS, we set?
= 0.02 and ?
= 0.2 without tuning, and eval-uate whether s?
(p, q) > 0 (hypernym relation) ors?
(p, q) ?
0 (no hypernym relation).
For BIB-LESS, we set ?
= 0.02 and ?
= 0.25 with-out tuning, and evaluate whether s?
(p, q) > 0(hyponym-hypernym), s(p, q) = 0 (no relation),or s(p, q) ?
0 (hypernym-hyponym).4 ResultsThe results can be found in Table 2.
We com-pare our methods with a frequency baseline, set-ting f(p) = freq(p) in Equation 4 and usingthe frequency scores from Turney et al (2011).Frequency has been proven to be a surprisingly121BLESS WBLESS BIBLESSFrequency 0.58 0.57 0.39WeedsPrec 0.63 ?
?WeedsSVM ?
0.75 ?WeedsUnSup ?
0.58 ?SLQS 0.87 ?
?Dispersion 0.88 0.75 (0.74) 0.57 (0.55)Centroid 0.87 0.74 (0.74) 0.57 (0.54)Entropy 0.83 0.71 (0.71) 0.56 (0.53)Table 2: Accuracy.
For WBLESS and BIBLESS wereport results for multi-modal ?c, with visual-only?cin brackets.challenging baseline for hypernym directionality(Herbelot and Ganesalingam, 2013; Weeds et al,2014).
In addition, we compare to the reported re-sults of Santus et al (2014) for WeedsPrec (Weedset al, 2004), an early lexical entailment mea-sure, and SLQS, the entropy-based method ofSantus et al (2014).
Note, however, that theseare on a subsampled corpus of 1277 word pairsfrom BLESS, so the results are indicative but notdirectly comparable.
On WBLESS we compareto the reported results of Weeds et al (2014):we include results for the highest-performing su-pervised method (WeedsSVM) and the highest-performing unsupervised method (WeedsUnSup).For BLESS, both dispersion and centroid dis-tance reach or outperform the best other measure(SLQS).
They beat the frequency baseline by alarge margin (+30% and +29%).
Taking the en-tropy of the mean image representations does notappear to do as well as the other two methodsbut still outperforms the baseline and WeedsPrec(+25% and +20% respectively).In the case of WBLESS and BIBLESS, wesee a similar pattern in that dispersion and cen-troid distance perform best.
For WBLESS, thesemethods outperform the other unsupervised ap-proach, WeedsUnsup, by +17% and match thebest-performing support vector machine (SVM)approach in Weeds et al (2014).
In fact, Weeds etal.
(2014) report results for a total of 6 supervisedmethods (based on SVM and k-nearest neighbor(k-NN) classifiers): our unsupervised image dis-persion method outperforms all of these except forthe highest-performing one, reported here.We can see that the task becomes increasinglydifficult as we go from directionality to detectionto the combination: the dispersion-based methodgoes from 0.88 to 0.75 to 0.57, for example.
BIB-LESS is the most difficult, as shown by the fre-Figure 2: Accuracy by WordNet shortest pathbucket (1 is shortest, 5 is longest).quency baseline obtaining only 0.39.
Our methodsdo much better than this baseline (+18%).
Imagedispersion appears to be the most robust measure.To examine our results further, we divided thetest data into buckets by the shortest WordNet pathconnecting word pairs (Miller, 1995).
We expectour method to be less accurate on word pairs withshort paths, since the difference in generality maybe difficult to discern.
It has also been suggestedthat very abstract hypernyms such as object andentity are difficult to detect because their linguisticdistributions are not supersets of their hyponyms?distributions (Rimell, 2014), a factor that shouldnot affect the visual modality.
We find that con-cept comparisons with a very short path (bucket 1)are indeed the least accurate.
We also find somedrop in accuracy on the longest paths (bucket 5),especially for WBLESS and BIBLESS, perhaps be-cause semantic similarity is difficult to detect inthese cases.
For a histogram of the accuracy scoresaccording to WordNet similarity, see Figure 2.5 ConclusionsWe have evaluated three unsupervised methods fordetermining the generality of a concept based onits visual properties.
Our best-performing method,image dispersion, reaches the state-of-the-art ontwo standard semantic evaluation datasets.
Weintroduced a novel, more difficult task combin-ing hypernym detection and directionality, andshowed that our methods outperform a frequencybaseline by a large margin.We believe that image generality may be par-ticularly suited to entailment detection because itdoes not suffer from the same issues as linguis-tic distributional generality.
Herbelot and Gane-salingam (2013) found that general terms like liq-uid do not always have higher entropy distribu-tions than their hyponyms, since speakers usethem in very specific contexts, e.g.
liquid is oftencoordinated with gas.We also acknowledge that our method depends122to some degree on Google?s search result diversifi-cation, but do not feel this detracts from the utilityof the method, since the fact that general conceptsachieve greater maximum image dispersion thanspecific concepts is not dependent on any partic-ular diversification algorithm.
In future work, weplan to explore more sophisticated visual gener-ality measures, other semantic relations and dif-ferent ways of fusing visual representations withlinguistic knowledge.AcknowledgmentsDK and LR are supported by EPSRC grantEP/I037512/1.
IV is supported by the PARISproject (IWT-SBO 110067) and the PDM Kortpostdoctoral fellowship from KU Leuven.
SCis supported by ERC Starting Grant DisCoTex(306920) and EPSRC grant EP/I037512/1.
Wethank the anonymous reviewers for their helpfulcomments.ReferencesMarco Baroni and Alessandro Lenci.
2011.
Howwe BLESSed distributional semantic evaluation.
InProceedings of the GEMS 2011 Workshop, pages 1?10.Shane Bergsma and Randy Goebel.
2011.
Using vi-sual information to predict lexical preference.
InProceedings of RANLP, pages 399?405.Or Biran and Kathleen McKeown.
2013.
Classifyingtaxonomic relations between pairs of wikipedia arti-cles.
In Proceedings of IJCNLP, pages 788?794.Johan Bos and Katja Markert.
2005.
Recognising tex-tual entailment with logical inference.
In Proceed-ings of EMNLP, pages 628?635.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.2014.
Multimodal distributional semantics.
Journalof Artificial Intelligence Research, 49:1?47.Daoud Clarke.
2009.
Context-theoretic semantics fornatural language: An overview.
In Proceedings ofthe GEMS 2009 Workshop, pages 112?119.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Fei-Fei Li.
2009.
ImageNet: A large-scale hier-archical image database.
In Proceedings of CVPR,pages 248?255.Thomas Deselaers and Vittorio Ferrari.
2011.
Visualand semantic similarity in imagenet.
In Proceedingsof CVPR, pages 1777?1784.Robert Fergus, Li Fei-Fei, Pietro Perona, and AndrewZisserman.
2005.
Learning object categories fromGoogle?s image search.
In Proceedings of ICCV,pages 1816?1823.Dan Garrette, Katrin Erk, and Raymond Mooney.2011.
Integrating logical representations with prob-abilistic information using Markov logic.
In Pro-ceedings of IWCS, pages 105?114.M.
Geffet and I. Dagan.
2005.
The distributional in-clusion hypotheses and lexical entailment.
In Pro-ceedings of ACL, pages 107?114.Aur?elie Herbelot and Mohan Ganesalingam.
2013.Measuring semantic content in distributional vec-tors.
In Proceedings of ACL, pages 440?445.Yangqing Jia, Evan Shelhamer, Jeff Donahue, SergeyKarayev, Jonathan Long, Ross Girshick, SergioGuadarrama, and Trevor Darrell.
2014.
Caffe: Con-volutional architecture for fast feature embedding.arXiv preprint arXiv:1408.5093.Hongyan Jing.
1998.
Usage of wordnet in naturallanguage generation.
In Proceedings of COLING-ACL?98 Workshop on Usage of WordNet in NaturalLanguage Processing Systems.Douwe Kiela and L?eon Bottou.
2014.
Learning imageembeddings using convolutional neural networks forimproved multi-modal semantics.
In Proceedings ofEMNLP, pages 36?45.Douwe Kiela, Felix Hill, Anna Korhonen, and StephenClark.
2014.
Improving multi-modal representa-tions using image dispersion: Why less is sometimesmore.
In Proceedings of ACL, pages 835?841.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359?389.Angeliki Lazaridou, Nghia The Pham, and Marco Ba-roni.
2015.
Combining language and vision witha multimodal skip-gram model.
In Proceedings ofNAACL-HLT.Alessandro Lenci and Giulia Benotto.
2012.
Identify-ing hypernyms in distributional semantic spaces.
InProceedings of *SEM, pages 75?79.Omer Levy, Steffen Remus, Chris Biemann, and IdoDagan.
2015.
Do supervised distributional methodsreally learn lexical inference relations?
In Proceed-ings of NAACL.J.H.
Martin.
1990.
A Computational Model ofMetaphor Interpretation.
Academic Press Profes-sional, Inc.George A. Miller.
1995.
WordNet: A lexical databasefor English.
In Communications of the ACM, vol-ume 38, pages 39?41.123Michael Mohler, David Bracewell, Marc Tomlinson,and David Hinote.
2013.
Semantic signatures forexample-based linguistic metaphor detection.
InProceedings of the 1st Workshop on Metaphor inNLP.Laura Rimell.
2014.
Distributional lexical entailmentby topic coherence.
In Proceedings of EACL, pages511?519.Enrico Santus, Alessandro Lenci, Qin Lu, andSabine Schulte im Walde.
2014.
Chasing hyper-nyms in vector spaces with entropy.
In Proceedingsof EACL, pages 38?42.Saeedeh Shekarpour, Konrad H?offner, Jens Lehmann,and S?oren Auer.
2013.
Keyword query expansionon linked data using linguistic and semantic features.In Proceedings of the 7th IEEE International Con-ference on Semantic Computing, pages 191?197.Carina Silberer and Mirella Lapata.
2014.
Learn-ing grounded meaning representations with autoen-coders.
In Proceedings of ACL, pages 721?732.Josef Sivic and Andrew Zisserman.
2003.
VideoGoogle: A text retrieval approach to object match-ing in videos.
In Proceedings of ICCV, pages 1470?1477.Peter D. Turney, Yair Neuman, Dan Assaf, and YohaiCohen.
2011.
Literal and metaphorical sense iden-tification through concrete and abstract context.
InProceedings of EMNLP, pages 680?690.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of COLING.Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,and Bill Keller.
2014.
Learning to distinguish hy-pernyms and co-hyponyms.
In Proceedings of COL-ING, pages 2249?2259.W.
A.
Woods, Stephen Green, Paul Martin, and AnnHouston.
2001.
Aggressive morphology and lexi-cal relations for query expansion.
In Proceedings ofTREC.M.
Zhitomirsky-Geffet and I. Dagan.
2009.
Bootstrap-ping distributional feature vector quality.
Computa-tional Linguistics, 35(3):435461.124
