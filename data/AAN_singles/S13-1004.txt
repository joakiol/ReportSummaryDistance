Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 32?43, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics*SEM 2013 shared task: Semantic Textual SimilarityEneko AgirreUniversity of the Basque Countrye.agirre@ehu.esDaniel CerStanford Universitydanielcer@stanford.eduMona DiabGeorge Washington Universitymtdiab@gwu.eduAitor Gonzalez-AgirreUniversity of the Basque Countryagonzalez278@ikasle.ehu.esWeiwei GuoColumbia Universityweiwei@cs.columbia.eduAbstractIn Semantic Textual Similarity (STS), sys-tems rate the degree of semantic equivalence,on a graded scale from 0 to 5, with 5 be-ing the most similar.
This year we set uptwo tasks: (i) a core task (CORE), and (ii)a typed-similarity task (TYPED).
CORE issimilar in set up to SemEval STS 2012 taskwith pairs of sentences from sources relatedto those of 2012, yet different in genre fromthe 2012 set, namely, this year we includednewswire headlines, machine translation eval-uation datasets and multiple lexical resourceglossed sets.
TYPED, on the other hand, isnovel and tries to characterize why two itemsare deemed similar, using cultural heritageitems which are described with metadata suchas title, author or description.
Several types ofsimilarity have been defined, including simi-lar author, similar time period or similar lo-cation.
The annotation for both tasks lever-ages crowdsourcing, with relative high inter-annotator correlation, ranging from 62% to87%.
The CORE task attracted 34 participantswith 89 runs, and the TYPED task attracted 6teams with 14 runs.1 IntroductionGiven two snippets of text, Semantic Textual Simi-larity (STS) captures the notion that some texts aremore similar than others, measuring the degree ofsemantic equivalence.
Textual similarity can rangefrom exact semantic equivalence to complete un-relatedness, corresponding to quantified values be-tween 5 and 0.
The graded similarity intuitively cap-tures the notion of intermediate shades of similaritysuch as pairs of text differ only in some minor nu-anced aspects of meaning only, to relatively impor-tant differences in meaning, to sharing only somedetails, or to simply being related to the same topic,as shown in Figure 1.One of the goals of the STS task is to create aunified framework for combining several semanticcomponents that otherwise have historically tendedto be evaluated independently and without character-ization of impact on NLP applications.
By providingsuch a framework, STS will allow for an extrinsicevaluation for these modules.
Moreover, this STSframework itself could in turn be evaluated intrin-sically and extrinsically as a grey/black box withinvarious NLP applications such as Machine Trans-lation (MT), Summarization, Generation, QuestionAnswering (QA), etc.STS is related to both Textual Entailment (TE)and Paraphrasing, but differs in a number of waysand it is more directly applicable to a number of NLPtasks.
STS is different from TE inasmuch as it as-sumes bidirectional graded equivalence between thepair of textual snippets.
In the case of TE the equiv-alence is directional, e.g.
a car is a vehicle, but a ve-hicle is not necessarily a car.
STS also differs fromboth TE and Paraphrasing (in as far as both taskshave been defined to date in the literature) in that,rather than being a binary yes/no decision (e.g.
a ve-hicle is not a car), we define STS to be a graded sim-ilarity notion (e.g.
a vehicle and a car are more sim-ilar than a wave and a car).
A quantifiable gradedbidirectional notion of textual similarity is useful fora myriad of NLP tasks such as MT evaluation, infor-mation extraction, question answering, summariza-tion, etc.32?
(5) The two sentences are completely equivalent, as they mean the same thing.The bird is bathing in the sink.Birdie is washing itself in the water basin.?
(4) The two sentences are mostly equivalent, but some unimportant details differ.In May 2010, the troops attempted to invade Kabul.The US army invaded Kabul on May 7th last year, 2010.?
(3) The two sentences are roughly equivalent, but some important information differs/missing.John said he is considered a witness but not a suspect.
?He is not a suspect anymore.?
John said.?
(2) The two sentences are not equivalent, but share some details.They flew out of the nest in groups.They flew into the nest together.?
(1) The two sentences are not equivalent, but are on the same topic.The woman is playing the violin.The young lady enjoys listening to the guitar.?
(0) The two sentences are on different topics.John went horse back riding at dawn with a whole group of friends.Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.Figure 1: Annotation values with explanations and examples for the core STS task.In 2012 we held the first pilot task at SemEval2012, as part of the *SEM 2012 conference, withgreat success: 35 teams participated with 88 sys-tem runs (Agirre et al 2012).
In addition, we helda DARPA sponsored workshop at Columbia Uni-versity1.
In 2013, STS was selected as the officialShared Task of the *SEM 2013 conference.
Ac-cordingly, in STS 2013, we set up two tasks: Thecore task CORE, which is similar to the 2012 task;and a pilot task on typed-similarity TYPED betweensemi-structured records.For CORE, we provided all the STS 2012 dataas training data, and the test data was drawn fromrelated but different datasets.
This is in contrastto the STS 2012 task where the train/test datawere drawn from the same datasets.
The 2012datasets comprised the following: pairs of sentencesfrom paraphrase datasets from news and video elic-itation (MSRpar and MSRvid), machine transla-tion evaluation data (SMTeuroparl, SMTnews) andpairs of glosses (OnWN).
The current STS 2013dataset comprises the following: pairs of news head-lines, SMT evaluation sentences (SMT) and pairs ofglosses (OnWN and FNWN).The typed-similarity pilot task TYPED attempts1http://www.cs.columbia.edu/?weiwei/workshop/to characterize, for the first time, the reason and/ortype of similarity.
STS reduces the problem of judg-ing similarity to a single number, but, in some appli-cations, it is important to characterize why and howtwo items are deemed similar, hence the added nu-ance.
The dataset comprises pairs of Cultural Her-itage items from Europeana,2 a single access pointto millions of books, paintings, films, museum ob-jects and archival records that have been digitizedthroughout Europe.
It is an authoritative source ofinformation coming from European cultural and sci-entific institutions.
Typically, the items comprisemeta-data describing a cultural heritage item and,sometimes, a thumbnail of the item itself.Participating systems in the TYPED task need tocompute the similarity between items, using the tex-tual meta-data.
In addition to general similarity, par-ticipants need to score specific kinds of similarity,like similar author, similar time period, etc.
(cf.
Fig-ure 3).The paper is structured as follows.
Section 2 re-ports the sources of the texts used in the two tasks.Section 3 details the annotation procedure.
Section4 presents the evaluation of the systems, followedby the results of CORE and TYPED tasks.
Section 6draws on some conclusions and forward projections.2http://www.europeana.eu/33Figure 2: Annotation instructions for CORE taskyear dataset pairs source2012 MSRpar 1500 news2012 MSRvid 1500 videos2012 OnWN 750 glosses2012 SMTnews 750 MT eval.2012 SMTeuroparl 750 MT eval.2013 HDL 750 news2013 FNWN 189 glosses2013 OnWN 561 glosses2013 SMT 750 MT eval.2013 TYPED 1500 Cultural Heritage itemsTable 1: Summary of STS 2012 and 2013 datasets.2 Source DatasetsTable 1 summarizes the 2012 and 2013 datasets.2.1 CORE taskThe CORE dataset comprises pairs of news head-lines (HDL), MT evaluation sentences (SMT) andpairs of glosses (OnWN and FNWN).For HDL, we used naturally occurring news head-lines gathered by the Europe Media Monitor (EMM)engine (Best et al 2005) from several different newssources.
EMM clusters together related news.
Ourgoal was to generate a balanced data set across thedifferent similarity ranges, hence we built two setsof headline pairs: (i) a set where the pairs comefrom the same EMM cluster, (ii) and another setwhere the headlines come from a different EMMcluster, then we computed the string similarity be-tween those pairs.
Accordingly, we sampled 375headline pairs of headlines that occur in the sameEMM cluster, aiming for pairs equally distributedbetween minimal and maximal similarity using sim-ple string similarity.
We sample another 375 pairsfrom the different EMM cluster in the same manner.The SMT dataset comprises pairs of sentencesused in machine translation evaluation.
We have twodifferent sets based on the evaluation metric used:an HTER set, and a HYTER set.
Both metrics usethe TER metric (Snover et al 2006) to measure thesimilarity of pairs.
HTER typically relies on several(1-4) reference translations.
HYTER, on the otherhand, leverages millions of translations.
The HTERset comprises 150 pairs, where one sentence is ma-chine translation output and the corresponding sen-tence is a human post-edited translation.
We sam-ple the data from the dataset used in the DARPAGALE project with an HTER score ranging from 0to 120.
The HYTER set has 600 pairs from 3 sub-sets (each subset contains 200 pairs): a. reference34Figure 3: Annotation instructions for TYPED taskvs.
machine translation.
b. reference vs. Finite StateTransducer (FST) generated translation (Dreyer andMarcu, 2012).
c. machine translation vs. FST gen-erated translation.
The HYTER data set is used in(Dreyer and Marcu, 2012).The OnWN/FnWN dataset contains gloss pairsfrom two sources: OntoNotes-WordNet (OnWN)and FrameNet-WordNet (FnWN).
These pairs aresampled based on the string similarity ranging from0.4 to 0.9.
String similarity is used to measure thesimilarity between a pair of glosses.
The OnWNsubset comprises 561 gloss pairs from OntoNotes4.0 (Hovy et al 2006) and WordNet 3.0 (Fellbaum,1998).
370 out of the 561 pairs are sampled from the110K sense-mapped pairs as made available fromthe authors.
The rest, 291 pairs, are sampled fromunmapped sense pairs with a string similarity rang-ing from 0.5 to 0.9.
The FnWN subset has 189manually mapped pairs of senses from FrameNet 1.5(Baker et al 1998) to WordNet 3.1.
They are ran-domly selected from 426 mapped pairs.
In combi-nation, both datasets comprise 750 pairs of glosses.2.2 Typed-similarity TYPED taskThis task is devised in the context of the PATHSproject,3 which aims to assist users in accessingdigital libraries looking for items.
The projecttests methods that offer suggestions about items thatmight be useful to recommend, to assist in the inter-pretation of the items, and to support the user in thediscovery and exploration of the collections.
Hencethe task is about comparing pairs of items.
The pairsare generated in the Europeana project.A study in the PATHS project suggested that userswould be interested in knowing why the system issuggesting related items.
The study suggested sevensimilarity types: similar author or creator, similarpeople involved, similar time period, similar loca-3http://www.paths-project.eu35Figure 4: TYPED pair on our survey.
Only general and author similarity types are shown.tion, similar event or action, similar subject and sim-ilar description.
In addition, we also include generalsimilarity.
Figure 3 shows the definition of each sim-ilarity type as provided to the annotators.The dataset is generated in semi-automatically.First, members of the project manually select 25pairs of items for each of the 7 similarity types (ex-cluding general similarity), totalling 175 manuallyselected pairs.
After removing duplicates and clean-ing the dataset, we got 163 pairs.
Second, we usethese manually selected pairs as seeds to automat-ically select new pairs as follows: Starting fromthose seeds, we use the Europeana API to get similaritems, and we repeat this process 5 times in order todiverge from the original items (we stored the vis-ited items to avoid looping).
Once removed fromthe seed set, we select the new pairs following twoapproaches:?
Distance 1: Current item and similar item.?
Distance 2: Current item and an item that issimilar to a similar item (twice removed dis-tance wise)This yields 892 pairs for Distance 1 and 445 ofDistance 2.
We then divide the data into train andtest, preserving the ratios.
The train data contains82 manually selected pairs, 446 pairs with similaritydistance 1 and 222 pairs with similarity distance 2.The test data follows a similar distribution.Europeana items cannot be redistributed, so weprovide their urls and a script which uses the official36Europeana API to access and extract the correspond-ing metadata in JSON format and a thumbnail.
Inaddition, the textual fields which are relevant for thetask are made accessible in text files, as follows:?
dcTitle: title of the item?
dcSubject: list of subject terms (from some vo-cabulary)?
dcDescription: textual description of the item?
dcCreator: creator(s) of the item?
dcDate: date(s) of the item?
dcSource: source of the item3 Annotation3.1 CORE taskFigure 1 shows the explanations and values foreach score between 5 and 0.
We use the Crowd-Flower crowd-sourcing service to annotate theCORE dataset.
Annotators are presented with thedetailed instructions given in Figure 2 and are askedto label each STS sentence pair on our 6 point scaleusing a dropdown box.
Five sentence pairs at a timeare presented to annotators.
Annotators are paid0.20 cents per set of 5 annotations and we collect5 separate annotations per sentence pair.
Annota-tors are restricted to people from the following coun-tries: Australia, Canada, India, New Zealand, UK,and US.To obtain high quality annotations, we create arepresentative gold dataset of 105 pairs that are man-ually annotated by the task organizers.
During an-notation, one gold pair is included in each set of 5sentence pairs.
Crowd annotators are required torate 4 of the gold pairs correct to qualify to workon the task.
Gold pairs are not distinguished in anyway from the non-gold pairs.
If the gold pairs areannotated incorrectly, annotators are told what thecorrect annotation is and they are given an explana-tion of why.
CrowdFlower automatically stops lowperforming annotators ?
those with too many incor-rectly labeled gold pairs ?
from working on the task.The distribution of scores in the headlines HDLdataset is uniform, as in FNWN and OnWN, al-though the scores are slightly lower in FNWN andslightly higher in OnWN.
The scores for SMT arenot uniform, with most of the scores uniformly dis-tributed between 3.5 and 5, a few pairs between 2and 3.5, and nearly no pairs with values below 2.3.2 TYPED taskThe dataset is annotated using crowdsourcing.
Thesurvey contains the 1500 pairs of the dataset (750 fortrain and 750 for test), plus 20 gold pairs for qualitycontrol.
Each participant is shown 4 training goldquestions at the beginning, and then one gold every2 or 4 questions depending on the accuracy.
If accu-racy dropped to less than 66.7% percent the surveyis stopped and the answers from that particular an-notator are discarded.
Each annotator is allowed torate a maximum of 20 pairs to avoid getting answersfrom people that are either tired or bored.
To ensurea good comprehension of the items, the task is re-stricted to only accept annotators from some Englishspeaking countries: UK, USA, Australia, Canadaand New Zealand.Participants are asked to rate the similarity be-tween pairs of cultural heritage items from rang-ing from 5 to 0, following the instructions shownin Figure 3.
We also add a ?Not Applicable?
choicefor cases in which annotators are not sure or didn?tknow.
For those cases, we calculate the similarityscore using the values of the rest of the annotators (ifnone, we convert it to 0).
The instructions given tothe annotators are the ones shown in Figure 3.
Fig-ure 4 shows a pair from the dataset, as presented toannotators.The similarity scores for the pairs follow a similardistribution in all types.
Most of the pairs have ascore between 4 and 5, which can amount to as muchas 50% of all pairs in some types.3.3 Quality of annotationIn order to assess the annotation quality, we measurethe correlation of each annotator with the average ofthe rest of the annotators.
We then averaged all thecorrelations.
This method to estimate the quality isidentical to the method used for evaluation (see Sec-tion 4.1) and it can be thus used as the upper boundfor the systems.
The inter-tagger correlation in theCORE dataset for each of dataset is as follows:?
HDL: 85.0%?
FNWN: 69.9%?
OnWN: 87.2%?
SMT: 65.8%For the TYPED dataset, the inter-tagger correla-tion values for each type of similarity is as follows:?
General: 77.0%37?
Author: 73.1%?
People Involved: 62.5%?
Time period: 72.0%?
Location: 74.3%?
Event or Action: 63.9%?
Subject: 74.5%?
Description: 74.9%In both datasets, the correlation figures are high,confirming that the task is well designed.
The weak-est correlations in the CORE task are SMT andFNWN.
The first might reflect the fact that someautomatically produced translations are confusingor difficult to understand, and the second could becaused by the special style used to gloss FrameNetconcepts.
In the TYPED task the weakest correla-tions are for the People Involved and Event or Actiontypes, as they might be the most difficult to spot.4 Systems Evaluation4.1 Evaluation metricsEvaluation of STS is still an open issue.
STS ex-periments have traditionally used Pearson product-moment correlation, or, alternatively, Spearmanrank order correlation.
In addition, we also need amethod to aggregate the results from each datasetinto an overall score.
The analysis performed in(Agirre and Amigo?, In prep) shows that Pearson andaveraging across datasets are the best suited com-bination in general.
In particular, Pearson is moreinformative than Spearman, in that Spearman onlytakes the rank differences into account, while Pear-son does account for value differences as well.
Thestudy also showed that other alternatives need to beconsidered, depending on the requirements of thetarget application.We leave application-dependent evaluations forfuture work, and focus on average weighted Pear-son correlation.
When averaging, we weight eachindividual correlation by the size of the dataset.In addition, participants in the CORE task are al-lowed to provide a confidence score between 1 and100 for each of their scores.
The evaluation scriptdown-weights the pairs with low confidence, follow-ing weighted Pearson.4 In order to compute sta-tistical significance among system results, we use4http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Calculating_a_weighted_correlationa one-tailed parametric test based on Fisher?s z-transformation (Press et al 2002, equation 14.5.10).4.2 The Baseline SystemsFor the CORE dataset, we produce scores using asimple word overlap baseline system.
We tokenizethe input sentences splitting at white spaces, andthen represent each sentence as a vector in the mul-tidimensional token space.
Each dimension has 1if the token is present in the sentence, 0 otherwise.Vector similarity is computed using the cosine sim-ilarity metric.
We also run two freely available sys-tems, DKPro (Bar et al 2012) and TakeLab (S?aric?
etal., 2012) from STS 2012,5 and evaluate them on theCORE dataset.
They serve as two strong contenderssince they ranked 1st (DKPro) and 2nd (TakeLab) inlast year?s STS task.For the TYPED dataset, we first produce XMLfiles for each of the items, using the fields as pro-vided to participants.
Then we run named entityrecognition and classification (NERC) and date de-tection using Stanford CoreNLP.
This is followed bycalculating the similarity score for each of the typesas follows.?
General: cosine similarity of TF-IDF vectors oftokens from all fields.?
Author: cosine similarity of TF-IDF vectors fordc:Creator field.?
People involved, time period and location:cosine similarity of TF-IDF vectors of loca-tion/date/people recognized by NERC in allfields.?
Events: cosine similarity of TF-IDF vectors ofverbs in all fields.?
Subject and description: cosine similarity ofTF-IDF vectors of respective fields.IDF values are calculated from a subset of theEuropeana collection (Culture Grid collection).
Wealso run a random baseline several times, yieldingclose to 0 correlations in all datasets, as expected.4.3 ParticipationParticipants could send a maximum of three systemruns.
After downloading the test datasets, they hada maximum of 120 hours to upload the results.
34teams participated in the CORE task, submitting 895Code is available at http://www-nlp.stanford.edu/wiki/STS38Team and run Head.
OnWN FNWN SMT Mean # Team and run Head.
OnWN FNWN SMT Mean #baseline-tokencos .5399 .2828 .2146 .2861 .3639 73 KnCe2013-all .3475 .3505 .1073 .1551 .2639 86DKPro .7347 .7345 .3405 .3256 .5652 - KnCe2013-diff .4028 .3537 .1284 .1804 .2934 84TakeLab-best .6559 .6334 .4052 .3389 .5221 - KnCe2013-set .0462 -.1526 .0376 -.0605 -.0397 90TakeLab-sts12 .4858 .6334 .2693 .2787 .4340 - LCL Sapienza-ADW1 .6943 .4661 .3571 .3311 .4880 43aolney-w3c3 .5248 .4701 .1777 .2744 .3986 67 LCL Sapienza-ADW2 .6520 .5280 .3598 .3681 .5019 32BGU-1 .5075 .3252 .0768 .1843 .3181 81 LCL Sapienza-ADW3 .6205 .5108 .4462 .3838 .4996 34BGU-2 .3608 .3777 -.0173 .0698 .2363 88 LIPN-tAll .7063 .6937 .4037 .3005 .5425 16BGU-3 .3591 .3360 .0072 .2122 .2748 85 LIPN-tSp .5791 .7199 .3522 .3721 .5261 24BUAP-RUN1 .5005 .2579 .1766 .2322 .3234 78 MayoClinicNLP-r1wtCDT .6584 .7775 .3735 .3605 .5649 6BUAP-RUN2 .4860 .2872 .2082 .2117 .3216 79 MayoClinicNLP-r2CDT .6827 .6612 .3960 .3946 .5572 8BUAP-RUN3 .4817 .2711 .2511 .1990 .3156 82 MayoClinicNLP-r3wtCD .6440 .8295 .3202 .3561 .5671 5CFILT-1 .5336 .2381 .2261 .2906 .3531 75 NTNU-RUN1 .7279 .5952 .3215 .4015 .5519 9CLaC-RUN1 .6774 .7667 .3793 .3068 .5511 10 NTNU-RUN2 .5909 .1634 .3650 .3786 .3946 68CLaC-RUN2 .6921 .7366 .3793 .3375 .5587 7 NTNU-RUN3 .7274 .5882 .3115 .4035 .5498 12CLaC-RUN3 .5276 .6495 .4158 .3082 .4755 47 PolyUCOMP-RUN1 .5176 .1517 .2496 .2914 .3284 77CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36 SOFTCARDINALITY-run1 .6410 .7360 .3442 .3035 .5273 23CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33 SOFTCARDINALITY-run2 .6713 .7412 .3838 .2981 .5402 18CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30 SOFTCARDINALITY-run3 .6603 .7401 .3347 .2900 .5294 22CPN-combined.RandSubSpace .6771 .5135 .3314 .3369 .4939 39 sriubc-System1?
.6083 .2915 .2790 .3065 .4011 66CPN-combined.SVM .6685 .5096 .3621 .3408 .4939 38 sriubc-System2?
.6359 .3664 .2713 .3476 .4420 57CPN-individual.RandSubSpace .6771 .5484 .3314 .2769 .4826 45 sriubc-System3?
.5443 .2843 .2705 .3275 .3842 70DeepPurple-length .6542 .5105 .2507 .2803 .4598 56 SXUCFN-run1 .6806 .5355 .3181 .3980 .5198 27DeepPurple-linear .6878 .5105 .2693 .2787 .4721 50 SXUCFN-run2 .4881 .6146 .4237 .3844 .4797 46DeepPurple-lineara .6227 .5105 .3265 .2952 .4607 55 SXUCFN-run3 .6761 .6481 .3025 .4003 .5458 14deft-baseline .6532 .8431 .5083 .3265 .5795 3 SXULLL-1 .4840 .7146 .0415 .1543 .3944 69deft-baseline2 .5706 .8111 .5503 .3325 .5495 13 UCam-A .5510 .3099 .2385 .1171 .3200 80DLS@CU-char .3867 .2386 .3726 .3337 .3309 76 UCam-B .6399 .4440 .3995 .3400 .4709 53DLS@CU-charSemantic .4669 .4165 .3859 .3411 .4056 64 UCam-C .4962 .5639 .1724 .3006 .4207 62DLS@CU-charWordSemantic .4921 .3769 .4647 .3492 .4135 63 UCSP-NC?
.1736 .0853 .1151 .1658 .1441 89ECNUCS-Run1 .5656 .2083 .1725 .2949 .3533 74 UMBC EBIQUITY-galactus .7428 .7053 .5444 .3705 .5927 2ECNUCS-Run2 .7120 .5388 .2013 .2504 .4720 51 UMBC EBIQUITY-ParingWords .7642 .7529 .5818 .3804 .6181 1ECNUCS-Run3 .6799 .5284 .2203 .3595 .4967 35 UMBC EBIQUITY-saiyan .7838 .5593 .5815 .3563 .5683 4HENRY-run1 .7601 .4631 .3516 .2801 .4917 41 UMCC DLSI-1 .5841 .4847 .2917 .2855 .4352 58HENRY-run2 .7645 .4631 .3905 .3593 .5229 26 UMCC DLSI-2 .6168 .5557 .3045 .3407 .4833 44HENRY-run3 .7103 .3934 .3364 .3308 .4734 48 UMCC DLSI-3 .3846 .1342 -.0065 .2736 .2523 87IBM EG-run2 .7217 .6110 .3364 .3460 .5365 19 UNIBA-2STEPSML .4255 .4801 .1832 .2710 .3673 71IBM EG-run5 .7410 .5987 .4133 .3426 .5452 15 UNIBA-DSM PERM .6319 .4910 .2717 .3155 .4610 54IBM EG-run6 .7447 .6257 .4381 .3275 .5502 11 UNIBA-STACKING .6275 .4658 .2111 .2588 .4293 61ikernels-sys1 .7352 .5432 .3842 .3180 .5188 28 Unimelb NLP-bahar .7119 .3490 .3813 .3507 .4733 49ikernels-sys2 .7465 .5572 .3875 .3409 .5339 21 Unimelb NLP-concat .7085 .6790 .3374 .3230 .5415 17ikernels-sys3 .7395 .4228 .3596 .3294 .4919 40 Unimelb NLP-stacking .7064 .6140 .1865 .3144 .5091 29INAOE-UPV-run1 .6392 .3249 .2711 .3491 .4332 59 Unitor-SVRegressor run1 .6353 .5744 .3521 .3285 .4941 37INAOE-UPV-run2 .6390 .3260 .2662 .3457 .4319 60 Unitor-SVRegressor run2 .6511 .5610 .3580 .3096 .4902 42INAOE-UPV-run3 .6468 .6295 .4090 .3047 .5085 31 Unitor-SVRegressor run3 .6027 .5489 .3269 .3192 .4716 52KLUE-approach 1 .6521 .6507 .3996 .3367 .5254 25 UPC-AE .6092 .5679 -.1268 .2090 .4037 65KLUE-approach 2 .6510 .6869 .4189 .3360 .5355 20 UPC-AED .4136 .4770 -.0852 .1662 .3050 83UPC-AED T .5119 .6386 -.0464 .1235 .3671 72Table 2: Results on the CORE task.
The first rows on the left correspond to the baseline and to two publicly availablesystems, see text for details.
Note: ?
signals team involving one of the organizers, ?
for systems submitting past the120 hour window.system runs.
For the TYPED task, 6 teams partici-pated, submitting 14 system runs.6Some submissions had minor issues: one teamhad a confidence score of 0 for all items (we re-placed them by 100), and another team had a fewNot-a-Number scores for the SMT dataset, whichwe replaced by 5.
One team submitted the resultspast the 120 hours.
This team, and the teams that in-6Due to lack of space we can?t detail the full names of au-thors and institutions that participated.The interested reader canuse the name of the runs in Tables 2 and 3 to find the relevantpaper in these proceedings.cluded one of the organizers, are explicitly marked.We want to stress that in these teams the organizersdid not allow the developers of the system to accessany data or information which was not available forthe rest of participants.
After the submission dead-line expired, the organizers published the gold stan-dard in the task website, in order to ensure a trans-parent evaluation process.4.4 CORE Task ResultsTable 2 shows the results of the CORE task, withruns listed in alphabetical order.
The correlation in39Team and run General Author People involved Time Location Event Subject Description Mean #baseline .6691 .4278 .4460 .5002 .4835 .3062 .5015 .5810 .4894 8BUAP-RUN1 .6798 .6166 .0670 .2761 .0163 .1612 .5167 .5283 .3577 14BUAP-RUN2 .6745 .6093 .1285 .3721 .0163 .1660 .5094 .5546 .3788 13BUAP-RUN3 .6992 .6345 .1055 .1461 .0000 -.0668 .3729 .5120 .3004 15BUT-1 .3686 .7468 .3920 .5725 .3604 .2906 .2270 .5882 .4433 9ECNUCS-Run1 .6040 .7362 .3663 .4685 .3844 .4057 .5229 .6027 .5113 5ECNUCS-Run2 .6064 .5684 .3663 .4685 .3844 .4057 .5563 .6027 .4948 7PolyUCOMP-RUN1 .4888 .6940 .3223 .3820 .3621 .1625 .3962 .4816 .4112 12PolyUCOMP-RUN2 .4893 .6940 .3253 .3777 .3628 .1968 .3962 .4816 .4155 11PolyUCOMP-RUN3 .4915 .6940 .3254 .3737 .3667 .2207 .3962 .4816 .4187 10UBC UOS-RUN1?
.7256 .4568 .4467 .5762 .4858 .3090 .5015 .5810 .5103 6UBC UOS-RUN2?
.7457 .6618 .6518 .7466 .7244 .6533 .7404 .7751 .7124 4UBC UOS-RUN3?
.7461 .6656 .6544 .7411 .7257 .6545 .7417 .7763 .7132 3Unitor-SVRegressor lin .7564 .8076 .6758 .7090 .7351 .6623 .7520 .7745 .7341 2Unitor-SVRegressor rbf .7981 .8158 .6922 .7471 .7723 .6835 .7875 .7996 .7620 1Table 3: Results on TYPED task.
The first row corresponds to the baseline.
Note: ?
signals team involving one of theorganizers.each dataset is given, followed by the mean cor-relation (the official measure), and the rank of therun.
The baseline ranks 73.
The highest correla-tions are for OnWN (84%, by deft) and HDL (78%,by UMBC), followed by FNWN (58%, by UMBC)and SMT (40%, by NTNU).
This fits nicely with theinter-tagger correlations (respectively 87, 85, 70 and65, cf.
Section 3).
It also shows that the systems getclose to the human correlations in the OnWN andHDL dataset, with bigger differences for FNWN andSMT.The result of the best run (by UMBC) is signif-icantly different (p-value < 0.05) than all runs ex-cept the second best.
The second best run is onlysignificantly different to the runs ranking 7th andbelow, and the third best to the 14th run and be-low.
The difference between consecutive runs wasnot significant.
This indicates that many system runsperformed very close to each other.Only 13 runs included non-uniform confidencescores.
In 10 cases the confidence value allowedto improve performance, sometimes as much as .11absolute points.
For instance, SXUCFN-run3 im-proves from .4773 to .5458.
The most notable ex-ception is MayoClinicNLP-r2CDT, which achievesa mean correlation of .5879 instead of .5572 if theyprovide uniform confidence values.The Table also shows the results of TakeLaband DKPro.
We train the DKPro and TakeLab-sts12 models on all the training and test STS 2012data.
We additionally train another variant sys-tem of TakeLab, TakeLab-best, where we use tar-geted training where the model yields the best per-formance for each test subset as follows: (1) HDLis trained on MSRpar 2012 data; (2) OnWN istrained on all 2012 data; (3) FnWN is trained on2012 OnWN data; (4) SMT is trained on 2012 SM-Teuroparl data.
Note that Takelab-best is an upperbound, as the best combination is selected on thetest dataset.
TakeLab-sts12, TakeLab-best, DKProrank as 58th, 27th and 6th in this year?s system sub-missions, respectively.
The different results yieldedfrom TakeLab depending on the training data sug-gests that some STS systems are quite sensitive tothe source of the sentence pairs, indicating that do-main adaptation techniques could have a role in thistask.
On the other hand, DKPro performed ex-tremely well when trained on all available training,with no special tweaking for each dataset.4.5 TYPED Task ResultsTable 3 shows the results of TYPED task.
Thecolumns show the correlation for each type of sim-ilarity, followed by the mean correlation (the offi-cial measure), and the rank of the run.
The best sys-tem (from Unitor) is best in all types.
The baselineranked 8th, but the performance difference with thebest system is quite significant.
The best result issignificantly different (p-value < 0.02) to all runs.The second and third best runs are only significantlydifferent from the run ranking 5th and below.
Notethat in this dataset the correlations of the best systemare higher than the inter-tagger correlations.
Thismight indicate that the task has been solved, in thesense that the features used by the top systems areenough to characterize the problem and reach hu-man performance, although the correlations of some40AcronymsDistributionalmemoryDistributionalthesaurusMonolingualcorporaMultilingualcorporaOpinionandSentimentTablesofparaphrasesWikipediaWiktionaryWordembeddingsWordNetCorreferenceDependencyparseDistributionalsimilarityKBSimilarityLDALemmatizerLexicalSubstitutionLogicalinferenceMetaphororMetonymyMultiwordrecognitionNamedEntityrecognitionPOStaggerROUGEpackageScopingSearchengineSemanticRoleLabelingStringsimilaritySyntaxTextualentailmentTimeanddateresolutionTreekernelsWordSenseDisambiguationaolney-w3c3 x x xBGU-1 x x x x x x xBGU-2 x x x x x x xBGU-3 x x x x x x xCFILT-APPROACH x x x x xCLaC-Run1 x x x x x x x xCLaC-Run2 x x x x x x x xCLaC-Run3 x x x x x x x xCNGL-LPSSVR x x x x xCNGL-LPSSVRTL x x x x xCNGL-LSSVR x x x x xCPN-combined.RandSubSpace x x x x x x x xCPN-combined.SVM x x x x x x x xCPN-individual.RandSubSpace x x x x x x x xDeepPurple-length x x x x x x xDeepPurple-linear x x x x x x xDeepPurple-lineara x x x x x x xdeft-baseline x x x xdeft-baseline x x x x x xDLS@CU-charSemantic x x x xDLS@CU-charWordSemantic x x x x x xDLS@CU-charWordSemantic x x xECNUCS-Run1 x x x x x x xECNUCS-Run2 x x x x x x xECNUCS-Run3 x x x x x x xHENRY-run1 x x x x x x x x xHENRY-run2 x x x x x x x xIBM EG-run2 x x x x x xIBM EG-run5 x x x x x xIBM EG-run6 x x x x xikernels-sys1 x x x x x x x x x x xikernels-sys2 x x x x x x x x x x xikernels-sys3 x x x x x x x x x x xINAOE-UPV-run1 x x x x x x xINAOE-UPV-run2 x x x x x x xINAOE-UPV-run3 x x x x x x xKLUE-approach 1 x x x x x x xKLUE-approach 2 x x x x x xKnCe2013-all x x x x x x x xKnCe2013-div x x x x x x x xKnCe2013-div x x x x x x x xLCL Sapienza-ADW1 x x xLCL Sapienza-ADW2 x x xLCL Sapienza-ADW3 x x xLIPN-tAll x x x x x x x x x xLIPN-tSp x x x x x x x x x xMayoClinicNLP-r1wtCDT x x x x x x x x x x x xMayoClinicNLP-r2CDT x x x x x x x x x x x xMayoClinicNLP-r3wtCD x x x x x x x x x x x xNTNU-RUN1 x x x x x x x x x x x x x x x x x x x x x x xNTNU-RUN2 x x x x x x x x x x x x x x x x x x x x x x xNTNU-RUN3 x x x x x x x x x x x x x x x x x x x x x x xPolyUCOMP-RUN1 x x x xSOFTCARDINALITY-run1 xSOFTCARDINALITY-run2 x x xSOFTCARDINALITY-run3 x x xSXUCFN-run1 x x xSXUCFN-run2 x x xSXUCFN-run3 x x xSXULLL-1 x xUCam-A x x x xUCam-B x x x xUCam-C x x x xUCSP-NC x x x x xUMBC EBIQUITY-galactus x x x x x x xUMBC EBIQUITY-ParingWords x x x x x xUMBC EBIQUITY-saiyan x x x x x x xUMCC DLSI-1 x x x x x x x x x xUMCC DLSI-2 x x x x x x x x x xUMCC DLSI-3 x x x x x x x x xUNIBA-2STEPSML x x x x x x x x x x xUNIBA-DSM PERM x x x x x xUNIBA-STACKING x x x x x x x x x x xUnimelb NLP-bahar x xUnimelb NLP-concat x x x x x x x x x xUnimelb NLP-stacking x x x x x x x x x xUnitor-SVRegressor run1 x x x x x xUnitor-SVRegressor run2 x x x x x xUnitor-SVRegressor run3 x x x x x xTotal 11 2 12 54 12 5 11 36 7 3 54 3 3 48 40 2 67 14 3 3 10 24 55 3 3 4 9 6 34 9 13 6 6Table 4: CORE task: Resources and tools used by the systems that submitted a description file.
Leftmost columnscorrespond to the resources, and rightmost to tools, in alphabetic order.41types could be too low for practical use.5 Tools and resources usedThe organizers asked participants to submit a de-scription file, making special emphasis on the toolsand resources that were used.
Tables 4 and 5 showschematically the tools and resources as reported bysome of the participants for the CORE and TYPEDtasks (respectively).
In the last row, the totals showthat WordNet and monolingual corpora were themost used resources for both tasks, followed byWikipedia and the use of acronyms (for CORE andTYPED tasks respectively).
Dictionaries, multilin-gual corpora, opinion and sentiment analysis, andlists and tables of paraphrases are also used.For CORE, generic NLP tools such as lemmati-zation and PoS tagging are widely used, and to alesser extent, distributional similarity, knowledge-based similarity, syntactic analysis, named entityrecognition, lexical substitution and time and dateresolution (in this order).
Other popular tools areSemantic Role Labeling, Textual Entailment, StringSimilarity, Tree Kernels and Word Sense Disam-biguation.
Machine learning is widely used to com-bine and tune components (and so, it is not men-tioned in the tables).
Several less used tools arealso listed but are used by three or less systems.The top scoring systems use most of the resourcesand tools listed (UMBC EBIQUITY-ParingWords,MayoClinicNLP-r3wtCD).
Other well ranked sys-tems like deft-baseline are only based on distribu-tional similarity.
Although not mentioned in thedescriptions files, some systems used the publiclyavailable DKPro and Takelab systems.For the TYPED task, the most used tools are lem-matizers, Named Entity Recognizers, and PoS tag-gers.
Distributional and Knowledge-base similarityis also used, and at least four systems used syntacticanalysis and time and date resolution.76 Conclusions and Future WorkWe presented the 2013 *SEM shared task on Seman-tic Textual Similarity.8 Two tasks were defined: a7For a more detailed analysis, the reader is directed to thepapers in this volume.8All annotations, evaluation scripts and system outputs areavailable in the website for the task9.
In addition, a collabora-tively maintained site10, open to the STS community, containsAcronymsMonolingualcorporaWikipediaWordNetDistributionalsimilarityKBSimilarityLemmatizerMultiwordrecognitionNamedEntityrecognitionPOStaggerSyntaxTimeanddateresolutionTreekernelsBUT-1 x x x x x x xPolyUCOMP-RUN2 x x x xECNUCS-Run1 x x xECNUCS-Run2 x x x x x x xPolyUCOMP-RUN1 x x x xPolyUCOMP-RUN3 x x x xUBC UOS-RUN1 x x x x x x x x x x xUBC UOS-RUN2 x x x x x x x x x x x xUBC UOS-RUN3 x x x x x x x x x x x xUnitor-SVRegressor lin x x x x x x xUnitor-SVRegressor rbf x x x x x x xTotal 4 7 3 7 7 4 11 3 11 11 4 4 2Table 5: TYPED task: Resources and tools used bythe systems that submitted a description file.
Leftmostcolumns correspond to the resources, and rightmost totools, in alphabetic order.core task CORE similar to the STS 2012 task, anda new pilot on typed-similarity TYPED.
We had 34teams participate in both tasks submitting 89 systemruns for CORE and 14 system runs for TYPED, intotal amounting to a 103 system evaluations.
COREuses datasets which are related to but different fromthose used in 2012: news headlines, MT evalua-tion data, gloss pairs.
The best systems attainedcorrelations close to the human inter tagger corre-lations.
The TYPED task characterizes, for the firsttime, the reasons why two items are deemed simi-lar.
The results on TYPED show that the trainingdata provided allowed systems to yield high corre-lation scores, demonstrating the practical viabilityof this new task.
In the future, we are planning onadding more nuanced evaluation data sets that in-clude modality (belief, negation, permission, etc.
)and sentiment.
Also given the success rate of theTYPED task, however, the data in this pilot is rel-atively structured, hence in the future we are inter-ested in investigating identifying reasons why twopairs of unstructured texts as those present in COREare deemed similar.AcknowledgementsWe are grateful to the OntoNotes team for sharing OntoNotesto WordNet mappings (Hovy et al2006).
We thank Lan-guage Weaver, INC, DARPA and LDC for providing the SMTdata.
This work is also partially funded by the Spanish Ministryof Education, Culture and Sport (grant FPU12/06243).
Thisa comprehensive list of evaluation tasks, datasets, software andpapers related to STS.42work was partially funded by the DARPA BOLT and DEFT pro-grams.We want to thank Nikolaos Aletras, German Rigau andMark Stevenson for their help designing, annotating and col-lecting the typed-similarity data.
The development of thetyped-similarity dataset was supported by the PATHS project(http://paths-project.eu) funded by the European Community?sSeventh Framework Program (FP7/2007-2013) under grantagreement no.
270082.
The tasks were partially financed bythe READERS project under the CHIST-ERA framework (FP7ERA-Net).
We thank Europeana and all contributors to Euro-peana for sharing their content through the API.ReferencesEneko Agirre and Enrique Amigo?.
In prep.
Exploringevaluation measures for semantic textual similarity.
InUnpublished manuscript.Eneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In *SEM 2012:The First Joint Conference on Lexical and Computa-tional Semantics ?
Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 385?393,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In COLING ?98Proceedings of the 17th international conference onComputational linguistics - Volume 1.Daniel Bar, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the 6th International Work-shop on Semantic Evaluation, in conjunction with the1st Joint Conference on Lexical and ComputationalSemantics.Clive Best, Erik van der Goot, Ken Blackler, Tefilo Gar-cia, and David Horby.
2005.
Europe media monitor -system description.
In EUR Report 22173-En, Ispra,Italy.Markus Dreyer and Daniel Marcu.
2012.
Hyter:Meaning-equivalent semantics for translation evalua-tion.
In Human Language Technologies: Conferenceof the North American Chapter of the Association ofComputational Linguistics.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the ACL.W.H.
Press, S.A. Teukolsky, W.T.
Vetterling, and B.P.Flannery.
2002.
Numerical Recipes: The Art of Sci-entific Computing V 2.10 With Linux Or Single-ScreenLicense.
Cambridge University Press.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas.Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,and Bojana Dalbelo Bas?ic?.
2012.
Takelab: Sys-tems for measuring semantic text similarity.
In Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 441?448,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.43
