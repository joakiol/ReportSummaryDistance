Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 51?56,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPAcquiring High Quality Non-Expert Knowledge fromOn-demand WorkforceDonghui Feng           Sveva Besana          Remi ZajacAT&T Interactive ResearchGlendale, CA, 91203{dfeng, sbesana, rzajac}@attinteractive.comAbstractBeing expensive and time consuming, humanknowledge acquisition has consistently been amajor bottleneck for solving real problems.
Inthis paper, we present a practical frameworkfor acquiring high quality non-expert knowl-edge from on-demand workforce using Ama-zon Mechanical Turk (MTurk).
We show howto apply this framework to collect large-scalehuman knowledge on AOL query classifica-tion in a fast and efficient fashion.
Based onextensive experiments and analysis, we dem-onstrate how to detect low-quality labels frommassive data sets and their impact on collect-ing high-quality knowledge.
Our experimentalfindings also provide insight into the bestpractices on balancing cost and data quality forusing MTurk.1 IntroductionHuman knowledge acquisition is critical fortraining intelligent systems to solve real prob-lems, both for industry applications and aca-demic research.
For example, many machinelearning and natural language processing tasksrequire non-trivial human labeled data for super-vised learning-based approaches.
Traditionallythis has been collected from domain experts,which we refer to as expert knowledge.However, acquiring in-house expert knowl-edge is usually very expensive, time consuming,and has consistently been a major bottleneck formany research problems.
For example, tremen-dous efforts have been put into creating TRECcorpora (Voorhees, 2003).As a result, several research projects spon-sored by NSF and DARPA aim to constructvaluable data resources via human labeling; theseare exemplified by PennTree Bank (Marcus etal., 1993), FrameNet (Baker et al, 1998), andOntoNotes (Hovy et al, 2006).In addition, there are projects such as OpenMind Common Sense (OMCS) (Stork, 1999;Singh et al, 2002), ISI LEARNER (Chklovski,2003), and the Fact Entry Tool by Cycorp (Be-lasco et al, 2002) where knowledge is gatheredfrom volunteers.One interesting approach followed by vonAhn and Dabbish (2004), applied to image label-ing on the Web, is to collect valuable input fromentertained labelers.
Turning label acquisitioninto a computer game addresses tediousness,which is one of the main reasons that it is hard togather large quantities of data from volunteers.More recently researchers have begun to ex-plore approaches for acquiring human knowl-edge from an on-demand workforce such asAmazon Mechanical Turk1.
MTurk is a market-place for jobs that require human intelligence.There has been an increase in demand forcrowdsourcing prompted by both the academiccommunity and industry needs.
For instance,Microsoft/Powerset uses MTurk for search rele-vance evaluation and other companies are lever-aging turkers to clean their data sources.However, while it is cheap and fast to obtainlarge-scale non-expert labels using MTurk, it isstill unclear how to leverage its capability moreefficiently and economically to obtain sufficientuseful and high-quality data for solving realproblems.In this paper, we present a practical frame-work for acquiring high quality non-expertknowledge using MTurk.
As a case study wehave applied this framework to obtain humanclassifications on AOL queries (determiningwhether a query might be a local search or not).Based on extensive experiments and analysis, weshow how to detect bad labelers/labels frommassive data sets and how to build high-qualitylabeling sets.
Our experiments also provide in-1 Amazon Mechanical Turk:  http://www.mturk.com/51sight into the best practices for balancing costand data quality when using MTurk.The remainder of this paper is organized asfollows: In Section 2, we review related workusing MTurk.
We describe our methodology inSection 3 and in Section 4 we present our ex-perimental results and further analysis.
In Sec-tion 5 we draw conclusions and discuss our plansfor future work.2 Related WorkIt is either infeasible or very time and cost con-suming to acquire in-house expert human knowl-edge.
To obtain valuable human knowledge (e.g.,in the format of labeled data), many researchprojects in the natural language community havebeen funded to create large-scale corpora andknowledge bases, such as PenTreeBank (Marcuset al, 1993), FrameNet (Baker et al, 1998),PropBank (Palmer et al, 2005), and OntoNotes(Hovy et al, 2006).MTurk has been attracting much attentionwithin several research areas since its release.
Suet al (2007) use MTurk to collect large-scalereview data.
Kaisser and Lowe (2008) reporttheir work on generating research collections ofquestion-answering pairs using MTurk.
Sorokinand Forsyth (2008) outsource image-labelingtasks to MTurk.
Kittur et al (2008) use MTurkas the paradigm for user studies.
In the naturallanguage community Snow et al (2008) reporttheir work on collecting linguistic annotation fora variety of natural language tasks includingword sense disambiguation, word similarity, andtextual entailment recognition.However, most of the reported work focuseson how to apply data collected from MTurk totheir applications.
In our work, we concentrateon presenting a practical framework for usingMTurk by separating the process into a valida-tion phase and a large-scale submission phase.By analyzing workers?
behavior and their dataquality, we investigate how to detect low-qualitylabels and their impact on collected humanknowledge; in addition, during the validationstep we study how to best use MTurk to balancepayments and data quality.
Although our work isbased on the submission of a classification task,the framework and approaches can be adaptedfor other types of tasks.In the next section, we will discuss in moredetail our practical framework for using MTurk.3 Methodology3.1 Amazon Mechanical TurkAmazon launched their MTurk service in 2005.This service was initially used for internal pro-jects and eventually fulfilled the demand for us-ing human intelligence to perform various tasksthat computers currently cannot do or do verywell.MTurk users naturally fall into two roles: a re-quester and a turker.
As a requester, you can de-fine your Human Intelligent Tasks (HITs), de-sign suitable templates, and submit your tasks tobe completed by turkers.
A turker may choosefrom HITs that she is eligible to work on and getpaid after the requester approves her work.
Thework presented in this paper is mostly from theperspective of a requester.3.2 Key IssuesWhile it is quite easy to start using MTurk, re-questers have to confront the following: how canwe obtain sufficient useful and high-quality datafor solving real problems efficiently and eco-nomically?In practice, there are three key issues to con-sider when answering this question.Key Issues DescriptionDataQualityIs the labeled data good enough forpractical use?Cost What is the sweet spot for payment?Scale How efficiently can MTurk be usedwhen handling large-scale data sets?Can the submitted job be done in atimely manner?Table 1.
Key issues for using MTurk.Requesters want to obtain high-quality data ona large scale without overpaying turkers.
Ourproposed framework will address these key is-sues.3.3 ApproachesSince not all tasks collecting non-expert knowl-edge share the same characteristics and suitableapplications, there is not a one-size-fits-all solu-tion as the best practice when using MTurk.In our approach, we divide the process intotwo phases:?
Validation Phase.?
Large-scale Submission Phase.The first phase gives us information used todetermine if MTurk is a valid approach for agiven problem and what the optimal parametersfor high quality and a short turn-around time are.52We have to determine the right cost for the taskand the optimal number of labels.
We empiri-cally determine these parameters with an MTurksubmission using a small amount of data.
Theseoptimal parameters are then used for the large-scale submission phase.Most data labeling tasks require subjectivejudgments.
One cannot expect labeling resultsfrom different labelers to always be the same.The degree of agreement among turkers variesdepending on the complexity and ambiguity ofindividual tasks.
Typically we need to obtainmultiple labels for each HIT by assigning multi-ple turkers to the same task.Researchers mainly use the following twoquantitative measures to assess inter-agreement:observed agreement and kappa statistics.P(A) is the observed agreement among anno-tators.
It represents the portion where annotatorsproduce identical labels.
This is very natural andstraightforward.
However, people argue this maynot necessarily reflect the exact degree of agree-ment due to chance agreement.P(E)  is the hypothetical probability of chanceagreement.
In other words, P(E)  represents thedegree of agreement if both annotators conductannotations randomly (according to their ownprior probability).We can also use the kappa coefficient as aquantitative measure of inter-person agreement.It is a commonly used measure to remove theeffect of chance agreement.
It was first intro-duced in statistics (Cohen, 1960) and has beenwidely used in the language technology commu-nity, especially for corpus-driven approaches(Carletta, 1996; Krippendorf, 1980).
Kappa isdefined with the following equation:kappa = P(A) ?
P(E)1?
P(E)Generally it is viewed more robust than ob-served agreement P(A)  because it removeschance agreement P(E) .DetectOutlier( P)for each turker p ?
Pcollect the label set L  from pfor each label l ?
L/* compared with others?
majority voting */compute its agreement with otherscompute P(A) p  (or kappa p )analyze the distribution of P(A)return outlier turkersFigure 1.
Outlier detection algorithm.We use these measures to automatically detectoutlier turkers producing low-quality results.Figure 1 shows our algorithm for automaticallydetecting outlier turkers.4 ExperimentsBased on our proposed framework and ap-proaches, as a case study we conducted experi-ments on a classification task using MTurk.The classification task requires the turker todetermine whether a web query is a local searchor not.
For example, is the user typing this querylooking for a local business or not?
The labeleddata set can be used to train a query classifier fora web search system.This capability will make search systems ableto distinguish local search queries from othertypes of queries and to apply specific search al-gorithms and data resources to better serve users?information needs.For example, if a person types ?largest biomedcompany in San Diego?
and the web search sys-tems can recognize this query as a local searchquery, it will apply local search algorithms onlisting data instead of or as well as generating ageneral web search request.4.1 Validation PhaseWe downloaded the publicly available AOLquery log2 and used this as our corpus.
We firstscanned all queries with geographic locations(including states, cities, and neighborhoods) andthen randomly selected a set of queries for ourexperiments.For the validation phase, 700 queries werefirst labeled in-house by domain experts and werefer to this set as expert labels.
To obtain theoptimal parameters including the desired numberof labels and payment price, we designed ourHITs and experiments in the following way:We put 10 queries into one HIT, requested 15labels for each query/HIT, and varied paymentfor each HIT in four separate runs.
Our paymentsinclude $0.01, $0.02, $0.05, and $0.10 per HIT.The goal is to have HITs completed in a timelyfashion and have them yield high-quality data.We submitted our HITs to MTurk in four dif-ferent runs with the following prices: $0.01,$0.02, $0.03, and $0.10.
According to our pre-defined evaluation measures and our outlier de-tection algorithm, we investigated how to obtainthe optimal parameters.
Figure 2. shows the taskcompletion statistics for the four different runs.2 AOL Log Data: http://www.gregsadetsky.com/aol-data/53Figure 2.
Task completion statistics.As shown in Figure 2, with the increase ofpayments, the average hourly rate increases from$0.72 to $9.73 and the total turn-around timedramatically decreases from more than 47 hoursto about 1.5 hours.
In the meantime, people tendto become more focused on the tasks and spendless time per HIT.In addition, as we increase payment, morepeople tend to stay with the task and take it moreseriously as evidenced by the quality of the la-beled data.
This results in fewer numbers ofworkers overall as well as fewer outliers asshown in Figure 3.Figure 3.
Total number of workers and outliers.We investigate two types of agreements, inter-turker agreement and agreement between turkersand our in-house experts.
For inter non-expertagreements, we compute each turker?s agreementwith all others?
majority voting results.Payment(USD) 0.01 0.02 0.05 0.10Median ofinter-turkeragreement 0.8074 0.8583 0.9346 0.9028Table 2.
Median of inter-turker agreements.As in our outlier detection algorithm, we ana-lyzed the distribution of inter-turker agreements.Table 2 shows the median values of inter-turkeragreement as we vary the payment prices.
Themedian value keeps on increasing when the priceincreases from $0.01, to $0.02 and $0.05.
How-ever, it drops as the price increases from $0.05 to$0.10.
This implies that turkers do not necessar-ily improve their work quality as they get paidmore.
One of the possible explanations for thisphenomenon is that when the reward is highpeople tend to work towards completing the taskas fast as possible instead of focusing on submit-ting high-quality data.
This trend may be intrin-sic to the task we have submitted and further ex-periments will show if this turker behavior istask-independent.Figure 4.
Agreement with experts.Figure 5.
Inter non-expert agreement.We also analyzed agreement between non-experts and experts.
Figure 4 depicts the trend ofthe agreement scores with the increase of numberof labels and payments.
For example, givenseven labels per query, in the experiment withthe $0.05 payment, the majority voting of non-expert labels has an agreement of 0.9465 withexpert labeling.
As explained earlier we do notnecessarily obtain the best data qual-ity/agreement with the $0.10 payment.
Instead,we get the highest agreement with the $0.05payment.
We have determined this rate to be the54sweet spot in terms of cost.
Also, seven labelsper query produce a very high agreement with nofurther significant improvement when we in-crease the number of labels.For inter non-expert agreements, we foundsimilar trends in terms of different payments andnumber of labels as shown in Figure 5.As mentioned above, our algorithm is able todetect turkers producing low-quality data.
Onenatural question is: how will their labels affectthe overall data quality?We studied this problem in two differentways.
We evaluated the data quality by removingeither all polluted queries or only outliers?
labels.Here polluted queries refer to those queries re-ceiving at least one label from outliers.
By re-moving polluted queries, we only investigate theclean data set without any outlier labels.
Theother alternative is to only remove outliers?
la-bels for specific queries but others?
labels forthose queries will be kept.
Both the agreementbetween experts and non-experts and inter-non-experts agreement show similar trends: dataquality without outliers?
labels is slightly bettersince there is less noise.
However, as outliers?labels may span a large number of queries, itmay not be feasible to remove all polluted que-ries.
For example, in one of our experiments,outliers?
labels pollute more than half of all therecords.
We cannot simply remove all the querieswith outliers?
labels due to consideration of cost.On the other hand, the effect of outliers?
labelsis not that significant if a certain number of re-quested labels per query are collected.
As shownin Figure 6, noisy data from outliers can be over-ridden by assigning more labelers.Figure 6.
Agreement with Experts (removingoutliers?
labels (payment = $0.05)).From the validation phase of the query classi-fication task, we determine that the optimal pa-rameters are paying $0.05 per HIT and request-ing seven labels per query.
Given this number oflabels, the effect of outliers?
labels can be over-ridden for the final result.4.2 Large-scale Submission PhaseHaving obtained the optimal parameters from thevalidation phase, we are then ready to make alarge-scale submission.For this phase, we paid $0.05 per HIT and re-quested seven labels per query/HIT.
Followingsimilar filtering and sampling approaches as inthe validation phase, we selected 22.5k queriesfrom the AOL search log.
Table 3 shows the de-tected outliers for this large-scale submission.Total Number of Turkers 228Number of Outlier Turkers 23Outlier Ratio 10.09%Table 3.
Number of turkers and outliers.Based on the distribution of inter-turkeragreement, any turkers with agreement less than0.6501 are recognized as outliers.
For a totalnumber of 15,750 HITs, 228 turkers contributedto the labeling effort and 10.09% of them wererecognized as outliers.Table 4 shows the number of labels from theoutliers and the approval ratio of collected data.About 10.08% of labels are from outlier turkersand rejected.Total Number of Labels 157,500Number of Outlier Labels 15,870Approval Ratio 89.92%Table 4.
Total number of labels.We have experimented using MTurk for a webquery classification task.
With learned optimalparameters from the validation phase, we col-lected large-scale high-quality non-expert labelsin a fast and economical way.
These data will beused to train query classifiers to enhance websearch systems handling local search queries.5 Conclusions and Future WorkIn this paper, we presented a practical frameworkfor acquiring high quality non-expert knowledgefrom an on-demand and scalable workforce.
Us-ing Amazon Mechanical Turk, we collectedlarge-scale human classification knowledge onweb search queries.To learn the best practices when using MTurk,we presented a two-phase approach, a validationphase and a large-scale submission phase.
Weconducted extensive experiments to obtain theoptimal parameters on the number of labelersand payments in the validation phase.
We alsopresented an algorithm to automatically detect55outlier turkers based on the agreement analysisand investigated the effect of removing an inac-curately labeled set.Acquiring high-quality human knowledge willremain a major concern and a bottleneck for in-dustry applications and academic problems.
Un-like traditional ways of collecting in-house hu-man knowledge, MTurk provides an alternativeway to acquire non-expert knowledge.
As shownin our experiments, given appropriate qualitycontrol, we have been able to acquire high-quality data in a very fast and efficient way.
Webelieve MTurk will attract more attention andusage in broader areas.In the future, we are planning to investigatehow this framework can be applied to differenttypes of human knowledge acquisition tasks andhow to leverage large-scale labeled data sets forsolving natural language processing problems.ReferencesBaker, C.F., Fillmore, C.J., and Lowe, J.B. 1998.
TheBerkeley FrameNet Project.
In Proc.
of COLING-ACL-1998.Belasco, A., Curtis, J., Kahlert, R., Klein, C., Mayans,C., and Reagan, P. 2002.
Representing KnowledgeGaps Effectively.
In Practical Aspects of Knowl-edge Management, (PAKM).Carletta, J.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Lin-guistics.
22(2):249?254.Chklovski, T. 2003.
LEARNER: A System for Ac-quiring Commonsense Knowledge by Analogy.
InProc.
of Second International Conference onKnowledge Capture (KCAP 2003).Cohen, J.
1960.
A coefficient of agreement for nomi-nal scales.
Educational and Psychological Meas-urement.
Vol.20, No.1, pp.37-46.Colowick, S.M.
and Pool, J.
2007.
Disambiguating forthe web: a test of two methods.
In Proc.
of the 4thinternational Conference on Knowledge Capture(K-CAP 2007).Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., andWeischedel, R. 2006.
OntoNotes: The 90% Solu-tion.
In Proc.
of HLT-NAACL-2006.Kaisser, M. and Lowe, J.B. 2008.
Creating a ResearchCollection of Question Answer Sentence Pairs withAmazon's Mechanical Turk.
In Proc.
of the FifthInternational Conference on Language Resourcesand Evaluation (LREC-2008).Kittur, A., Chi, E. H., and Suh, B.
2008.
Crowdsourc-ing user studies with Mechanical Turk.
In Proc.
ofthe 26th Annual ACM Conference on Human Fac-tors in Computing Systems (CHI-2008).Krippendorf, K. 1980.
Content Analysis: An introduc-tion to its methodology.
Sage Publications.Marcus, M., Marcinkiewicz, M.A., and Santorini, B.1993.
Building a large annotated corpus of English:the Penn Treebank.
Computational Linguistics.19:2, June 1993.Nakov, P. 2008.
Paraphrasing Verbs for Noun Com-pound Interpretation.
In Proc.
of the Workshop onMultiword Expressions (MWE-2008).Palmer, M., Gildea, D., and Kingsbury, P. 2005.
TheProposition Bank: A Corpus Annotated with Se-mantic Roles.
Computational Linguistics.
31:1.Sheng, V.S., Provost, F., and Ipeirotis, P.G.
2008.
Getanother label?
improving data quality and datamining using multiple, noisy labelers.
In Proc.
ofthe 14th ACM SIGKDD international Conferenceon Knowledge Discovery and Data Mining (KDD-2008).Singh, P., Lin, T., Mueller, E., Lim, G., Perkins, T.,and Zhu, W. 2002.
Open Mind Common Sense:Knowledge acquisition from the general public.
InMeersman, R. and Tari, Z.
(Eds.
), LNCS: Vol.2519.
On the Move to Meaningful Internet Sys-tems: DOA/CoopIS/ODBASE (pp.
1223-1237).Springer-Verlag.Snow, R., O?Connor, B., Jurafsky, D., and Ng, A.Y.2008.
Cheap and Fast ?
But is it Good?
EvaluatingNon-Expert Annotations for Natural LanguageTasks .
In Proc.
of EMNLP-2008.Sorokin, A. and Forsyth, D. 2008.
Utility data annota-tion with Amazon Mechanical Turk.
In Proc.
of theFirst IEEE Workshop on Internet Vision at CVPR-2008.Stork, D.G.
1999.
The Open Mind Initiative.
IEEEExpert Systems and Their Applications.
pp.
16-20,May/June 1999.Su, Q., Pavlov, D., Chow, J., and Baker, W.C. 2007.Internet-scale collection of human-reviewed data.In Proc.
of the 16th international Conference onWorld Wide Web (WWW-2007).Von Ahn, L. and Dabbish, L. 2004.
Labeling Imageswith a Computer Game.
In Proc.
of ACM Confer-ence on Human Factors in Computing Systmes(CHI).
pp.
319-326.Voorhees, E.M. 2003.
Overview of TREC 2003.
InProc.
of TREC-2003.56
