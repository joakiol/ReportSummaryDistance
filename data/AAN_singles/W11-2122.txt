Proceedings of the 6th Workshop on Statistical Machine Translation, pages 177?186,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsMultiple-stream Language Models for Statistical Machine TranslationAbby LevenbergDept.
of Computer ScienceUniversity of Oxfordablev@cs.ox.ac.ukMiles OsborneSchool of InformaticsUniversity of Edinburghmiles@inf.ed.ac.ukDavid MatthewsSchool of InformaticsUniversity of Edinburghdave.matthews@ed.ac.ukAbstractWe consider using online language models fortranslating multiple streams which naturallyarise on the Web.
After establishing that us-ing just one stream can degrade translationson different domains, we present a series ofsimple approaches which tackle the problemof maintaining translation performance on allstreams in small space.
By exploiting the dif-fering throughputs of each stream and howthe decoder translates prior test points fromeach stream, we show how translation perfor-mance can equal specialised, per-stream lan-guage models, but do this in a single languagemodel using far less space.
Our results holdeven when adding three billion tokens of addi-tional text as a background language model.1 IntroductionThere is more natural language data available todaythan there has ever been and the scale of its produc-tion is increasing quickly.
While this phenomenonprovides the Statistic Machine Translation (SMT)community with a potentially extremely useful re-source to learn from, it also brings with it nontrivialcomputational challenges of scalability.Text streams arise naturally on the Web wheremillions of new documents are published each day inmany different languages.
Examples in the stream-ing domain include the thousands of multilingualwebsites that continuously publish newswire stories,the official proceedings of governments and otherbureaucratic organisations, as well as the millionsof ?bloggers?
and host of users on social networkservices such as Facebook and Twitter.Recent work has shown good results using an in-coming text stream as training data for either a staticor online language model (LM) in an SMT setting(Goyal et al, 2009; Levenberg and Osborne, 2009).A drawback of prior work is the oversimplified sce-nario that all training and test data is drawn from thesame distribution using a single, in-domain stream.In a real world scenario multiple incoming streamsare readily available and test sets from dissimilar do-mains will be translated continuously.
As we show,using stream data from one domain to translate an-other results in poor average performance for bothstreams.
However, combining streams naively to-gether hurts performance further still.In this paper we consider this problem of multiplestream translation.
Since monolingual data is veryabundant, we focus on the subtask of updating an on-line LM using multiple incoming streams.
The chal-lenges in multiple stream translation include dealingwith domain differences, variable throughput rates(the size of each stream per epoch), and the needto maintain constant space.
Importantly, we imposethe key requirement that our model match transla-tion performance reached using the single stream ap-proach on all test domains.We accomplish this using the n-gram history ofprior translations plus subsampling to maintain aconstant bound on memory required for languagemodelling throughout all stream adaptation.
In par-ticular, when considering two test streams, we areable to improve performance on both streams froman average (per stream) BLEU score of 39.71 and37.09 using a single stream approach (Tables 2 and3) to an average BLEU score of 41.28 and 42.73 us-ing multiple streams within a single LM using equalmemory (Tables 6 and 7).
We also show additive im-177provements using this approach when using a largebackground LM consisting of over one billion n-grams.
To our knowledge our approach is the firstin the literature to deal with adapting an online LMto multiple streams in small space.2 Previous Work2.1 Randomised LMsRandomised techniques for LMs from Talbot andOsborne (2007) and Talbot and Brants (2008) arecurrently industry state-of-the-art for fitting verylarge datasets into much smaller amounts of mem-ory than lossless representations for the data.
Insteadof representing the n-grams exactly, the randomisedrepresentation exchanges a small, one-sided error offalse positives for massive space savings.2.2 Stream-based LMsAn unbounded text stream is an input source of natu-ral language documents that is received sequentiallyand so has an implicit timeline attached.
In Leven-berg and Osborne (2009) a text stream was used toinitially train and subsequently adapt an online, ran-domised LM (ORLM) with good results.
However,a weakness of Levenberg and Osborne (2009) is thatthe experiments were all conducted over a single in-put stream.
It is an oversimplification to assume thatall test material for a SMT system will be from a sin-gle domain.
No work was done on the multi-streamcase where we have more than one incoming streamfrom arbitrary domains.2.3 Domain Adaptation for SMTWithin MT there has been a variety of approachesdealing with domain adaptation (for example (Wu etal., 2008; Koehn and Schroeder, 2007)).
Our workis related to domain adaptation but differs in that weare not skewing the distribution of an out-of-domainLM to accommodate some test data for which wehave little or no training data for.
Rather, we havevarying amounts of training data from all the do-mains via the incoming streams and the LM mustaccount for each domain appropriately.
However,known domain adaptation techniques are potentiallyapplicable to multi-stream translation as well.3 Multiple Streams and their PropertiesAny source that provides a continuous sequenceof natural language documents over time can bethought of as an unbounded stream which is time-stamped and access to it is given in strict chronolog-ical order.
The ubiquity of technology and the In-ternet means there are many such text streams avail-able already and their number is increasing quickly.For SMT, multiple text streams provide a potentiallyabundant source of new training data that may beuseful for combating model sparsity.Of primary concern is building models whosespace complexity is independent of the size of theincoming stream.
Allowing unbounded memory tohandle unbounded streams is unsatisfactory.
Whendealing with more than one stream we must alsoconsider how the properties of single streams inter-act in a multiple stream setting.Every text stream is associated with a particulardomain.
For example, we may draw a stream froma newswire source, a daily web crawl of new blogs,or the output of a company or organisation.
Obvi-ously the distribution over the text contained in thesestreams will be very different from each other.
Asis well-known from the work on domain adaptationthroughout the SMT literature, using a model fromone domain to translate a test document from an-other domain would likely produce poor results.Each stream source will also have a differentrate of production, or throughput, which may varygreatly between sources.
Blog data may be receivedin abundance but the newswire data may have a sig-nificantly lower throughput.
This means that the textstream with higher throughput may dominate andoverwhelm the more nuanced translation options ofthe stream with less data in the LM during decod-ing.
This is bad if we want to translate well for alldomains in small space using a single model.4 Multi-Stream RetrainingIn a stream-based translation setting we can expectto translate test points from various domains on anynumber of incoming streams.
Our goal is a singleunified LM that obtains equal performance in lessspace than when using a separate LM per stream.The underlying LMs could be exact, but here we userandomised versions based on the ORLM.178?stream a1LM2m3stream a1LM2mistream a1LM2mnpum3pumipumKNaive Combination ApproachtLwmLroch tLwmLrochFigure 1: In the naive approach all K streams are simplycombined into a single LM for each new epoch encoun-tered.Given an incoming number K of unboundedstreams over a potentially infinite timeline T , witht ?
T an epoch or windowed subset of the timeline,the full set of n-grams in all K streams over all Tis denoted with S. By St we denote n-grams fromall K streams and Skt, k ?
[1,K], as the n-gramsin the kth stream over epoch t. Since the streamsare unbounded, we do not have access to all the n-grams in S at once.
Instead we select n-grams fromeach stream Skt ?
S. We define the collection ofn-grams encoded in the LM at time t over all Kstreams as Ct.
Initially, at time t = 0 the LM iscomposed of the n-grams in the stream so C0 = S0.Since it is unsatisfactory to allow unboundedmemory usage for the model and more bits areneeded as we see more novel n-grams from thestreams, we enforce a memory constraint and usean adaptation scheme to delete n-grams from theLM Ct?1 before adding any new n-grams from thestreams to get the current n-gram set Ct. Belowwe describe various approaches of updating the LMwith data from the streams.4.1 Naive CombinationsApproach The first obvious approach for an onlineLM using multiple input streams is to simply storeall the streams in one LM.
That is, n-grams fromall the streams are only inserted into the LM onceand their stream specific counts are combined into asingle value in the composite LM.Modelling the Stream In the naive case we retrainthe LM Ct in full at epoch t using all the new datafrom the streams.
We have simplyCt =K?k=1Skt (1)stream 1 LM 1stream 1 LM 2stream 1 LM 3input stream 1stream 2 LM 1stream 2 LM 2stream 2 LM 3input stream 2?stream K LM 1stream K LM 2stream K LM 3input stream KMultiple LM Approachnew epoch new epochFigure 2: Each stream 1 .
.
.
K gets its own stream-basedLM using the multiple LM approach.where each of the K streams is combined into a sin-gle model and the n-grams counts are merged lin-early.
Here we carry no n-grams over from the LMCt?1 from the previous epoch.
The space needed isthe number of unique n-grams present in the com-bined streams for each epoch.Resulting LM To query the resulting LM Ct dur-ing decoding with a test n-gram wni = (wi, .
.
.
, wn)we use a simple smoothing algorithm called StupidBackoff (Brants et al, 2007).
This returns theprobability of an n-gram asP (wi|wi?1i?n+1) :=??
?Ct(wii?n+1)Ct(wi?1i?n+1)if Ct(wii?n+1) > 0?P (wi|wi?1i?n+2) otherwise(2)where Ct(.)
denotes the frequency count returned bythe LM for an n-gram and ?
is a backoff parameter.The recursion ends once the unigram is reached inwhich case the probability is P (wi) := wi/N whereN is the size of the current training corpus.Each stream provides a distribution over the n-grams contained in it and, for SMT, if a separateLM was constructed for each domain it would mostlikely cause the decoder to derive different 1-besthypotheses than using a LM built from all the streamdata.
Using the naive approach blurs the distributiondistinctions between streams and negates any streamspecific differences when the decoder produces a 1-best hypothesis.
It has been shown that doing lin-ear combinations of this type produces poor perfor-mance in theory (Mansour et al, 2008).1794.2 Weighted InterpolationApproach An improved approach to using multi-ple streams is to build a separate LM for each streamand using a weighted combination of each duringdecoding.
Each stream is stored in isolation and weinterpolate the information contained within eachduring decoding using a weighting on each stream.Modelling the Stream Here we model the streamsby simply storing each stream at time t in its ownLM so Ckt = Skt for each stream Sk.
Then the LMafter epoch t isCt = {C1t, .
.
.
, CKt}.We use more space here than all other approachessince we must store each n-gram/count occurring ineach stream separately as well as the overhead in-curred for each separate LM in memory.Resulting LM During decoding, the probability ofa test n-gram wni is a weighted combination of allthe individual stream LMs.
We can writeP (wni ) :=K?k=1fkPCkt(wni ) (3)where we query each of the individual LMs Ckt toget a score from each LM using Equation 2 andcombine them together using a weighting fk spe-cific to each LM.
Here we impose the restriction onthe weights that?Kk=1 fk = 1.
(We discuss specificweight selections in the next section.
)By maintaining multiple stream specific LMs wemaintain the particular distribution of the individualstreams.
This keeps the more nuanced translationsfrom the lower throughput streams available duringdecoding without translations being dominated by astream with higher throughput.
However using mul-tiple distinct LMs is wasteful of memory.4.3 Combining Models via HistoryApproach We want to combine the streams intoa single LM using less memory than when storingeach stream separately but still achieve at least asgood a translation for each test point.
Naively com-bining the streams removes stream specific transla-tions but using the history of n-grams selected by thedecoder during the previous test point in the streamwas done in Levenberg and Osborne (2009) for thesingle stream case with good results.
This is appli-cable to the multi-stream case as well.Modelling the Stream For multiple streams andepoch t > 0 we model the stream combination asCt = fT (Ct?1) ?K?k=1(Skt).
(4)where for each epoch a selected subset of the previ-ous n-grams in the LM Ct?1 is merged with all thenewly arrived stream data to create the new LM setCt.
The parameter fT denotes a function that filtersover the previous set of n-grams in the model.
Itrepresents the specific adaptation scheme employedand stays constant throughout the timeline T .
In thiswork we consider any n-grams queried by the de-coder in the last test point as potentially useful tothe next point.
Since all of the n-grams St in thestream at time t are used the space required is of thesame order of complexity as the naive approach.Resulting LM Since all the n-grams from thestreams are now encoded in a single LM Ct we canquery it using Equation 2 during decoding.
The goalof retraining using decoding history is to keep use-ful n-grams in the current model so a better modelis obtained and performance for the next transla-tion point is improved.
Note that making use of thehistory for hypothesis combination is theoreticallywell-founded and is the same approach used here forhistory based combination.
(Mansour et al, 2008)4.4 SubsamplingApproach The problem of multiple streams withhighly varying throughput rates can be seen as a typeof class imbalance problem in the machine learningliterature.
Given a binary prediction problem withtwo classes, for instance, the imbalance problem oc-curs when the bulk of the examples in the trainingdata are instances of one class and only a muchsmaller proportion of examples are available fromthe other class.
A frequently used approach to bal-ancing the distribution for the statistical model is touse random under sampling and select only a sub-set of the dominant class examples during training(Japkowicz and Stephen, 2002).This approach is applicable to the multiple streamtranslation problem with imbalanced throughputrates between streams.
Instead of storing the n-grams from each stream separately, we can apply a180?stream a1LM2m3stream a1LM2mistream a1LM2mnpum3LM 2 + (subset of LM 1)LM 3 + (subset of LM 2)Naive ComebnatAvaetoprr eAchnew epoch new epochSMT DecoderFigure 3: Using decoding history all the streams are com-bined into a unified LM.subsampling selection scheme directly to the incom-ing streams to balance each stream?s contribution inthe final LM.
Note that subsampling is also relatedto weighting interpolation.
Since all returned LMscores are based on frequency counts of the n-gramsand their prefixes, taking a weighting on a full prob-ability of an n-gram is akin to having fewer countsof the n-grams in the LM to begin with.Modelling the Stream To this end we use theweighted function parameter fk from Equation 3 toserve as the sampling probability rate for acceptingan n-gram from a given stream k. The samplingrate serves to limit the amount of stream data froma stream that ends up in the model.
For K > 1 wehaveCt = fT (Ct?1) ?K?k=1fk(Skt) (5)where fk is the probability a particular n-gram fromstream Sk at epoch t will be included in Ct. Theadaptation function fT remains the same as in Equa-tion 4.
The space used in this approach is now de-pendent on the rate fk used for each stream.Resulting LM Again, since we obtain a single LMfrom all the streams, we use Equation 2 to get theprobability of an n-gram during decoding.The subsampling method is applicable to all of theapproaches discussed in this section.
However, sincewe are essentially limiting the amount of data thatwe store in the final LM we can expect to take a per-formance hit based on the rate of acceptance givenby the parameters fk.
By using subsampling withthe history combination approach we obtain goodperformance for all streams in small space.Stream 1-grams 3-grams 5-gramsEP 19K 520K 760KGW (xie) 120K 3M 5MRCV1 630K 21M 42MTable 1: Sample statistics of unique n-gram counts fromthe streams from epoch 2 of our timeline.
The throughputrate varies a lot between streams.5 ExperimentsHere we report on our SMT experiments with multi-ple streams for translation using the approaches out-lined in the previous section.5.1 Experimental SetupThe SMT setup we employ is standard and all re-sources used are publicly available.
We translatefrom Spanish into English using phrase-based de-coding with Moses (Koehn and Hoang, 2007) as ourdecoder.
Our parallel data came from Europarl.We use three streams (all are timestamped):RCV1 (Rose et al, 2002), Europarl (EP) (Koehn,2003), and Gigaword (GW) (Graff et al, 2007).
GWis taken from six distinct newswire sources but inour initial experiments we limit the incoming streamfrom Gigaword to one of the sources (xie).
GW andRCV1 are both newswire domain streams with highrates of incoming data whereas EP is a more nu-anced, smaller throughput domain of spoken tran-scripts taken from sessions of the European Parlia-ment.
The RCV1 corpus only spans one calenderyear from October, 1996 through September, 1997so we selected only data in this time frame fromthe other two streams so our timeline consists of thesame full calendar year for all streams.For this work we use the ORLM.
The crux of theORLM is an online perfect hash function that pro-vides the ability to insert and delete from the datastructure.
Consequently the ORLM has the abil-ity to adapt to an unbounded input stream whilstmaintaining both constant memory usage and errorrate.
All the ORLMs were 5-gram models built withtraining data from the streams discussed above andused Stupid Backoff smoothing for n-gram scoring(Brants et al, 2007).
All results are reported usingthe BLEU metric (Papineni et al, 2001).For testing we held-out three random test points181LM Type Test 1 Test 2 Test 3RCV1 (Static) 39.30 38.28 33.06RCV1 (Online) 39.30 40.64 39.19EP (Online) 30.22 30.31 26.66RCV1+EP (Online) 39.00 40.15 39.46RCV1+EP+GW (Online) 41.29 41.73 40.41Table 2: Results for the RCV1 test points.
RCV1 and GWstreams are in-domain and EP is out-of-domain.
Transla-tion results are improved using more stream data sincemost n-grams are in-domain to the test points.from both the RCV1 and EP stream?s timeline fora total of six test points.
This divided the streamsinto three epochs, and we updated the online LMusing the data encountered in the epoch prior to eachtranslation point.
The n-grams and their counts fromthe streams are combined in the LM using one of theapproaches from the previous section.Using the notation from Section 4 we have theRCV1, EP, and GW streams described above andK = 3 as the number of incoming streams from twodistinct domains (newswire and spoken dialogue).Our timeline T is one year?s worth of data split intothree epochs, t ?
{1, 2, 3}, with test points at theend of each epoch t. Since we have no test pointsfrom the GW stream it acts as a background streamfor these experiments.
15.2 Baselines and Naive CombinationsIn this section we report on our translation exper-iments using a single stream and the naive linearcombination approach with multiple incoming datastreams from Section 4.1.Using the RCV1 corpus as our input stream wetested single stream translation first.
Here we arereplicating the experiments from Levenberg and Os-borne (2009) so both training and test data comesfrom a single in-domain stream.
Results are in Table2 where each row represents a different LM type.RCV1 (Static) is the traditional baseline with noadaptation where we use the training data for the firstepoch of the stream.
RCV1 (Online) is the onlineLM adapted with data from the in-domain stream.Confirming the previous work we get improvements1A background stream is one that only serves as trainingdata for all other test domains.LM Type Test 1 Test 2 Test 3EP (Static) 42.09 44.15 36.42EP (Online) 42.09 45.94 37.22RCV1 (Online) 36.46 42.10 32.73EP+RCV1 (Online) 40.82 44.07 35.01EP+RCV1+GW (Online) 40.91 44.05 35.56Table 3: EP results using in and out-of-domain streams.The last two rows show that naive combination gets poorresults compared to single stream approaches.when using an online LM that incorporates recentdata against a static baseline.We then ran the same experiments using a streamgenerated from the EP corpus.
EP consists of theproceedings of the European Parliament and is a sig-nificantly different domain than the RCV1 newswirestream.
We updated the online LM using n-gramsfrom the latest stream epoch before translating eachin-domain EP test set.
Results are in Table 3 and fol-low the same naming convention as Table 2 (exceptnow in-domain is EP and out-of-domain is RCV1).Using a single stream we also cross tested andtranslated each test point using the online LMadapted on the out-of-domain stream.
As expected,translation performance decreases (sometimes dras-tically) in this case since the data of the out-of-domain stream are not suited to the domain of thecurrent test point being translated.We then tested the naive approach and combinedboth streams into a single LM by taking the union ofthe n-grams and adding their counts together.
Thisis the RCV1+EP (Online) row in Tables 2 and 3 andclearly, though it contains more data compared toeach single stream LM, the naively combined LMdoes not help the RCV1 test points much and de-grades the performance of the EP translation results.This translation hit occurs as the throughput of eachstream is significantly different.
The EP stream con-tains far less data per epoch than the RCV1 counter-part (see Table 1) hence using a naive combinationmeans that the more abundant newswire data fromthe RCV1 stream overrides the probabilities of themore domain specific EP n-grams during decoding.When we added a third newswire stream from aportion of GW, shown in the last row of Tables 2and 3, improvements are obtained for the RCV1 test182Weighting Test 1 Test 2 Test 3.33R + .33E + .33G 38.97 39.78 35.66.50R + .25E + .25G 39.59 40.40 37.22.25R + .50E + .25G 36.57 38.03 34.23.70R + 0.0E + .30G 40.54 41.46 39.23Table 4: Weighted LM interpolation results for the RCV1test points where E = Europarl, R = RCV1, and G =Gigaword (xie).points due to the addition of in-domain data but theEP test performance still suffers.This highlights why naive combination is unsat-isfactory.
While using more in-domain data aidsin the translation of the newswire tests, for the EPtest sets, naively combining the n-grams from allstreams means the hypotheses the decoder selectsare weighted heavily in favor of the out-of-domaindata.
As the out-of-domain stream?s throughput issignificantly larger it swamps the model.5.3 Interpolating Weighted StreamsStraightforward linear stream combination into asingle LM results in degradation of translations fortest points whose in-domain training data is drawnfrom a stream with lower throughput than the otherdata streams.
We could maintain a separate MT sys-tem for each streaming domain but intuitively somecombination of the streams may benefit average per-formance since using all the data available shouldbenefit test points from streams with low through-put.
To test this we used an alternative approach de-scribed in Section 4.2 and used a weighted combi-nation of the single stream LMs during decoding.We tested this approach using our three streams:RCV1, EP and GW (xie).
We used a separateORLM for each stream and then, during testing, theresult returned for an n-gram queried by the decoderwas a value obtained from some weighted interpola-tion of each individual LM?s score for that n-gram.To get the interpolation weights for each streamingLM we minimised the perplexity of all the mod-els on held-out development data from the streams.2 Then we used the corresponding stream specific2Due to the lossy nature of the encoding of the ORLMmeans that the perplexity measures were approximations.Nonetheless the weighting from this approach had the best per-formance.Weighting Test 1 Test 2 Test 3.33E + .33R + .33G 40.75 45.65 35.77.50E + .25R + .25G 41.46 46.37 36.94.25E + .50R + .25G 40.57 44.90 35.77.70E + .20R + .10G 42.47 46.83 38.08Table 5: EP results in BLEU for the interpolated LMs.weights to decode the test points from that domain.Results are shown in Tables 4 and 5 using theweighting scheme described above plus a selec-tion of random parameter settings for comparison.Using the notation from Section 4.2, a caption of?.5R+ .25E+ .25G?, for example, denotes a weight-ing of fRCV 1 = 0.5 for the scores returned from theRCV1 stream LM while fEP and fGW = 0.25 forthe EP and GW stream LMs.The weighted interpolation results suggest thatwhile naive combination of the streams may be mis-guided, average translation performance can be im-proved upon when using more than a single in-domain stream.
Comparing the best results in Tables2 and 3 to the single stream baselines in Tables 4 and5 we achieve comparable, if not improved, transla-tion performance for both domains.
This is espe-cially true for test domains such as EP which havelow training data throughput from the stream.
Hereadding some information from the out-of-domainstream that contains a lot more data aids signifi-cantly in the translation of in-domain test points.However, the optimal weighting differs betweeneach test domain.
For instance, the weighting thatgives the best results for the EP tests results in muchpoorer translation performance for the RCV1 testpoints requiring us to track which stream we aredecoding and then select the appropriate weighting.This adds unnecessary complexity to the SMT sys-tem.
And, since we store each stream separately,memory usage is not optimal using this scheme.5.4 History and SubsamplingFor space efficiency we want to represent multi-ple streams non-redundantly instead of storing eachstream/domain in its own LM.
Here we report onexperiments using both the history combination andsubsampling approaches from Sections 4.3 and 4.4.Results are in Tables 6 and 7 for the RCV1 and183LM Type Test 1 Test 2 Test 3Multi-fk 41.19 41.73 39.23Multi-fT 41.29 42.23 40.51Multi-fk + fT 41.19 42.52 40.12Table 6: RCV1 test results using history and subsamplingapproaches.LM Type Test 1 Test 2 Test 3Multi-fk 40.91 43.50 36.11Multi-fT 40.91 47.84 39.29Multi-fk + fT 40.91 48.05 39.23Table 7: Europarl test results with history and subsam-pling approaches.EP test sets respectively with the column headersdenoting the test point.
The row Multi-fk showsresults using only the random subsampling param-eter fk and the rows Multi-fT show results with justthe time-based adaptation parameter without sub-sampling.
The final row Multi-fk + fT uses boththe f parameters with random subsampling as wellas taking decoding history into account.Multi-fk uses the random subsampling parame-ter fk to filter out higher order n-grams from thestreams.
All n-grams that are sampled from thestreams are then combined into the joint LM.
Thecounts of n-grams sampled from more than onestream are added together in the composite LM.
Theparameter fk is set dependent on a stream?s through-put rate, we only subsample from the streams withhigh throughput, and the rate was chosen based onthe weighted interpolation results described previ-ously.
In Tables 6 and 7 the subsampling rate fk =0.3 for the combined newswire streams RCV1 andGW and we kept all of the EP data.
We experi-mented with various other values for the fk samplingrates and found translation results only minorly im-pacted.
Note that the subsampling is truly randomso two adaptation runs with equal subsampling ratesmay produce different final translations.
Nonethe-less, in our experiments we saw expected perfor-mance, observing slight variation in performance forall test points that correlated to the percentage of in-domain data residing in the model.The next row, Multi-fT , uses recency criteria tokeep potentially useful n-grams but uses no subsam-pling and accepts all n-grams from all streams intothe LM.
Here we get better results than naive combi-nation or plain subsampling at the expense of morememory for the same error rate for the ORLM.The final row, Multi-fk + fT uses both the sub-sampling function fk and fT so maintains a historyof the n-grams queried by the decoder for the priortest points.
This approach achieves significantly bet-ter results than naive adaptation and compares to us-ing all the data in the stream.
Combining translationhistory as well as doing random subsampling overthe stream means we match the performance of butuse far less memory than when using multiple onlineLMs whilst maintaining the same error rate.5.5 Experiments SummaryWe have shown that using data from multiplestreams benefits SMT performance.
Our best ap-proach, using history based combination along withsubsampling, combines all incoming streams into asingle, succinct LM and obtains translation perfor-mance equal to single stream, domain specific LMson all test domains.
Crucially we do this in boundedspace, require less memory than storing each streamseparately, and do not incur translation degradationson any single domain.A note on memory usage.
The multiple LM ap-proach uses the most memory since this requiresall overlapping n-grams in the streams to be storedseparately.
The naive and history combination ap-proaches use less memory since they store all n-grams from all the streams in a unified LM.
For thesampling the exact amount of memory is of coursedependent on the sampling rate used.
For the resultsin Tables 6 and 7 we used significantly less memory(300MB) but still achieved comparable performanceto approaches that used more memory by storing thefull streams (600MB).6 Scaling UpThe experiments described in the preceding sectionused combinations of relatively small (compared tocurrent industry standards) input streams.
The ques-tion remains if using such approaches aids in the per-formance of translation if used in conjunction withlarge static LMs trained on large corpora.
In thissection we describe scaling up the previous stream-184Order Count1-grams 3.7M2-grams 46.6M3-grams 195.5M4-grams 366.8M5-grams 454.2MTotal 1067MTable 8: Singleton-pruned n-gram counts (in millions)for the GW3 background LM.LM Type Test 1 Test 2 Test 3GW (static) 41.69 42.40 35.48+ RCV1 (online) 42.44 43.83 40.55+ EP (online) 42.80 43.94 38.82Table 9: Test results for the RCV1 stream using the largebackground LM.
Using stream data benefits translation.based translation experiments using a large back-ground LM trained on a billion n-grams.We used the same setup described in Section 5.1.However, instead of using only a subset of the GWcorpus as one of our incoming streams, we traineda static LM using the full GW3 corpus of over threebillion tokens and used it as a background LM.
Asthe n-gram statistics for this background LM showin Table 8, it contains far more data than each of thestream specific LMs (Table 1).
We tested whetherusing streams atop this large background LM had apositive effect on translation for a given domain.Baseline results for all test points using only theGW background LM are shown in the top row inTables 9 and 10.
We then interpolated the ORLMswith this LM.
For each stream test point we interpo-lated with the big GW LM an online LM built withthe most recent epoch?s data.
Here we used sepa-rate models per stream so the RCV1 test points usedthe GW LM along with a RCV1 specific ORLM.
Weused the same mechanism to obtain the interpolationweights as described in Section 5.3 and minimisedthe perplexity of the static LM along with the streamspecific ORLM.
Interestingly, the tuned weights re-turned gave approximately a 50-50 weighting be-tween LMs and we found that simply using a 50-50weighting for all test points resulted had no negativeeffect on BLEU.
In the third row of the Tables 9 and10 we show the results of interpolating the big back-LM Type Test 1 Test 2 Test 3GW (static) 40.78 44.26 34.36+ EP (online) 43.94 47.82 38.71+ RCV1 (online) 43.07 47.72 39.15Table 10: EP test results using the background GW LM.ground LM with ORLMs built using the approachdescribed in Section 4.4.
In this case all streamswere combined into a single LM using a subsam-pling rate for higher order n-grams.
As before oursampling rate for the newswire streams was 30%chosen by the perplexity reduction weights.The results show that even with a large amountof static data adding small amounts of stream spe-cific data relevant to a given test point has an im-pact on translation quality.
Compared to only us-ing the large background model we obtain signifi-cantly better results when using a streaming ORLMto compliment it for all test domains.
However thelarge amount of data available to the decoder inthe background LM positively impacts translationperformance compared to single-stream approaches(Tables 2 and 3).
Further, when we combine thestreams into a single LM using the subsampling ap-proach we get, on average, comparable scores for alltest points.
Thus we see that the patterns for multi-ple stream adaptation seen in previous sections holdin spite of big amounts of static data.7 Conclusions and Future WorkWe have shown how multiple streams can be effi-ciently incorporated into a translation system.
Per-formance need not degrade on any of the streams.As well, these results can be additive.
Even whenusing large amounts of additional background data,adding stream specific data continues to improvetranslation.
Further, we achieve all results inbounded space.
Future work includes investigatingmore sophisticated adaptation for multiple streams.We also plan to explore alternative ways of samplingthe stream when incorporating data.AcknowledgementsSpecial thanks to Adam Lopez and Conrad Hughesand Phil Blunosm for helpful discussion and advice.This work was sponsored in part by the GALE pro-185gram, DARPA Contract No.
HR0011-06-C-0022and by ESPRC Grant No.
EP/I010858/1bb.ReferencesThorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 858?867.Amit Goyal, Hal Daume?
III, and Suresh Venkatasubra-manian.
2009.
Streaming for large scale NLP: Lan-guage modeling.
In North American Chapter of theAssociation for Computational Linguistics (NAACL),Boulder, CO.David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.2007.
English Gigaword Third Edition.
LinguisticData Consortium (LDC-2007T07).Nathalie Japkowicz and Shaju Stephen.
2002.
The classimbalance problem: A systematic study.
Intell.
DataAnal., 6:429?449, October.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 868?876.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, pages 224?227, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Philipp Koehn.
2003.
Europarl: A multilingual corpusfor evaluation of machine translation.
Available at:http://www.statmt.org/europarl/.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for SMT.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP).Yishay Mansour, Mehryar Mohri, and Afshin Ros-tamizadeh.
2008.
Domain adaptation with multiplesources.
In NIPS, pages 1041?1048.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalua-tion of machine translation.
In ACL ?02: Proceedingsof the 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318, Morristown, NJ,USA.
Association for Computational Linguistics.Tony Rose, Mark Stevenson, and Miles Whitehead.2002.
The reuters corpus volume 1 - from yester-days news to tomorrows language resources.
In InProceedings of the Third International Conference onLanguage Resources and Evaluation, pages 29?31.David Talbot and Thorsten Brants.
2008.
Randomizedlanguage models via perfect hash functions.
In Pro-ceedings of ACL-08: HLT, pages 505?513, Columbus,Ohio, June.
Association for Computational Linguis-tics.David Talbot and Miles Osborne.
2007.
SmoothedBloom filter language models: Tera-scale LMs on thecheap.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 468?476.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine translationwith domain dictionary and monolingual corpora.
InProceedings of the 22nd International Conference onComputational Linguistics (Coling 2008), pages 993?1000.
Coling 2008 Organizing Committee, August.186
