Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 132?137, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUBC UOS-TYPED: Regression for Typed-similarityEneko AgirreUniversity of the Basque CountryDonostia, 20018, Basque Countrye.agirre@ehu.esNikolaos AletrasUniversity of SheffieldSheffield, S1 4DP, UKn.aletras@dcs.shef.ac.ukAitor Gonzalez-AgirreUniversity of the Basque CountryDonostia, 20018, Basque Countryagonzalez278@ikasle.ehu.esGerman RigauUniversity of the Basque CountryDonostia, 20018, Basque Countrygerman.rigau@ehu.esMark StevensonUniversity of SheffieldSheffield, S1 4DP, UKm.stevenson@dcs.shef.ac.ukAbstractWe approach the typed-similarity task usinga range of heuristics that rely on informationfrom the appropriate metadata fields for eachtype of similarity.
In addition we train a linearregressor for each type of similarity.
The re-sults indicate that the linear regression is keyfor good performance.
Our best system wasranked third in the task.1 IntroductionThe typed-similarity dataset comprises pairs of Cul-tural Heritage items from Europeana1, a single ac-cess point to digitised versions of books, paintings,films, museum objects and archival records from in-stitutions throughout Europe.
Typically, the itemscomprise meta-data describing a cultural heritageitem and, sometimes, a thumbnail of the item itself.Participating systems need to compute the similaritybetween items using the textual meta-data.
In addi-tion to general similarity, the dataset includes spe-cific kinds of similarity, like similar author, similartime period, etc.We approach the problem using a range of sim-ilarity techniques for each similarity types, thesemake use of information contained in the relevantmeta-data fields.In addition, we train a linear regres-sor for each type of similarity, using the training dataprovided by the organisers with the previously de-fined similarity measures as features.We begin by describing our basic system in Sec-tion 2, followed by the machine learning system in1http://www.europeana.eu/Section 3.
The submissions are explained in Section4.
Section 5 presents our results.
Finally, we drawour conclusions in Section 6.2 Basic systemThe items in this task are taken from Europeana.They cannot be redistributed, so we used the urlsand scripts provided by the organizers to extract thecorresponding metadata.
We analysed the text in themetadata, performing lemmatization, PoS tagging,named entity recognition and classification (NERC)and date detection using Stanford CoreNLP (Finkelet al 2005; Toutanova et al 2003).
A preliminaryscore for each similarity type was then calculated asfollows:?
General: cosine similarity of TF.IDF vectors oftokens, taken from all fields.?
Author: cosine similarity of TF.IDF vectors ofdc:Creator field.?
People involved, time period and location:cosine similarity of TF.IDF vectors of loca-tion/date/people entities recognized by NERCin all fields.?
Events: cosine similarity of TF.IDF vectors ofevent verbs and nouns.
A list of verbs andnouns possibly denoting events was derived us-ing the WordNet Morphosemantic Database2.?
Subject and description: cosine similarity ofTF.IDF vectors of respective fields.IDF values were calculated using a subset of Eu-ropeana items (the Culture Grid collection), avail-able internally.
These preliminary scores were im-2urlhttp://wordnetcode.princeton.edu/standoff-files/morphosemantic-links.xls132proved using TF.IDF based on Wikipedia, UKB(Agirre and Soroa, 2009) and a more informed timesimilarity measure.
We describe each of these pro-cesses in turn.2.1 TF.IDFA common approach to computing document sim-ilarity is to represent documents as Bag-Of-Words(BOW).
Each BOW is a vector consisting of thewords contained in the document, where each di-mension corresponds to a word, and the weight isthe frequency in the corresponding document.
Thesimilarity between two documents can be computedas the cosine of the angle between their vectors.
Thisis the approached use above.This approach can be improved giving moreweight to words which occur in only a few docu-ments, and less weight to words occurring in manydocuments (Baeza-Yates and Ribeiro-Neto, 1999).In our system, we count document frequencies ofwords using Wikipedia as a reference corpus sincethe training data consists of only 750 items associ-ated with short textual information and might not besufficient for reliable estimations.
The TF.IDF sim-ilarity between items a and b is defined as:simtf.idf(a, b) =?w?a,b tfw,a ?
tfw,b ?
idf2w?
?w?a(tfw,a ?
idfw)2 ??
?w?b(tfw,b ?
idfw)2where tfw,x is the frequency of the term w in x ?
{a, b} and idfw is the inverted document frequencyof the word w measured in Wikipedia.
We substi-tuted the preliminary general similarity score by theobtained using the TF.IDF presented in this section.2.2 UKBThe semantic disambiguation UKB3 algorithm(Agirre and Soroa, 2009) applies personalizedPageRank on a graph generated from the EnglishWordNet (Fellbaum, 1998), or alternatively, fromWikipedia.
This algorithm has proven to be verycompetitive in word similarity tasks (Agirre et al2010).To compute similarity using UKB we representWordNet as a graph G = (V,E) as follows: graphnodes represent WordNet concepts (synsets) and3http://ixa2.si.ehu.es/ukb/dictionary words; relations among synsets are rep-resented by undirected edges; and dictionary wordsare linked to the synsets associated to them by di-rected edges.Our method is provided with a pair of vectors ofwords and a graph-based representation of WordNet.We first compute the personalized PageRank overWordNet separately for each of the vector of words,producing a probability distribution over WordNetsynsets.
We then compute the similarity betweenthese two probability distributions by encoding themas vectors and computing the cosine between thevectors.
We present each step in turn.Once personalized PageRank is computed, itreturns a probability distribution over WordNetsynsets.
The similarity between two vectors ofwords can thus be implemented as the similarity be-tween the probability distributions, as given by thecosine between the vectors.We used random walks to compute improved sim-ilarity values for author, people involved, locationand event similarity:?
Author: UKB over Wikipedia using person en-tities recognized by NERC in the dc:Creatorfield.?
People involved and location: UKB overWikipedia using people/location entities recog-nized by NERC in all fields.?
Events: UKB over WordNet using event nounsand verbs recognized in all fields.Results on the training data showed that perfor-mance using this approach was quite low (with theexception of events).
This was caused by the largenumber of cases where the Stanford parser did notfind entities which were in Wikipedia.
With thosecases on mind, we combined the scores returned byUKB with the similarity scores presented in Section2 as follows: if UKB similarity returns a score, wemultiply both, otherwise we return the square of theother similarity score.
Using the multiplication ofthe two scores, the results on the training data im-proved.2.3 Time similarity measureIn order to measure the time similarity between apair of items, we need to recognize time expres-sions in both items.
We assume that the year of133creation or the year denoting when the event tookplace in an artefact are good indicators for time sim-ilarity.
Therefore, information about years is ex-tracted from each item using the following pattern:[1|2][0 ?
9]{3}.
Using this approach, each item isrepresented as a set of numbers denoting the yearsmentioned in the meta-data.Time similarity between two items is computedbased on the similarity between their associatedyears.
Similarity between two years is defined as:simyear(y1, y2) = max{0, 1?
|y1?
y2| ?
k}k is a parameter to weight the difference betweentwo years, e.g.
for k = 0.1 all items that have differ-ence of 10 years or more assigned a score of 0.
Weobtained best results for k = 0.1.Finally, time similarity between items a and b iscomputed as the maximum of the pairwise similaritybetween their associated years:simtime(a, b) = max?i?a?j?b{0, simyear(ai, bj)}We substituted the preliminary time similarityscore by the measure obtained using the method pre-sented in this section.3 Applying Machine LearningThe above heuristics can be good indicators for therespective kind of similarity, and can be thus applieddirectly to the task.
In this section, we take thoseindicators as features, and use linear regression (asmade available by Weka (Hall et al 2009)) to learnmodels that fit the features to the training data.We generated further similarity scores for gen-eral similarity, including Latent Dirichlet Allocation(LDA) (Blei et al 2003), UKB and Wikipedia LinkVector Model (WLVM)(Milne, 2007) using infor-mation taken from all fields, as explained below.3.1 LDALDA (Blei et al 2003) is a statistical method thatlearns a set of latent variables called topics from atraining corpus.
Given a topic model, documentscan be inferred as probability distributions over top-ics, ?.
The distribution for a document i is denotedas ?i.
An LDA model is trained using the train-ing set consisting of 100 topics using the gensimpackage4.
The hyperparameters (?, ?)
were set to1num of topics .
Therefore, each item in the test set isrepresented as a topic distribution.The similarity between a pair of items is estimatedby comparing their topic distributions following themethod proposed in Aletras et al(2012; Aletras andStevenson (2012).
This is achieved by consideringeach distribution as a vector (consisting of the topicscorresponding to an item and its probability) thencomputing the cosine of the angle between them, i.e.simLDA(a, b) =~?a ?
~?b|~?a| ?
| ~?b|where ~?a is the vector created from the probabilitydistribution generated by LDA for item a.3.2 Pairwise UKBWe run UKB (Section 2.2) to generate a probabil-ity distribution over WordNet synsets for all of thewords of all items.
Similarity between two wordsis computed by creating vectors from these distri-butions and comparing them using the cosine of theangle between the two vectors.
If a words does notappear in WordNet its similarity value to every otherword is set to 0.
We refer to that similarity metric asUKB here.Similarity between two items is computed by per-forming pairwise comparison between their words,for each, selecting the highest similarity score:sim(a, b) =12(?w1?aargmaxw2?b UKB(w1, w2)|a|+?w2?bargmaxw1?a UKB(w2, w1)|b|)where a and b are two items, |a| the number oftokens in a and UKB(w1, w2) is the similarity be-tween words w1 and w2.3.3 WLVMAn algorithm described by Milne and Witten (2008)associates Wikipedia articles which are likely to berelevant to a given text snippet using machine learn-ing techniques.
We make use of that method to rep-resent each item as a set of likely relevant Wikipedia4http://pypi.python.org/pypi/gensim134articles.
Then, similarity between Wikipedia arti-cles is measured using the Wikipedia Link VectorModel (WLVM) (Milne, 2007).
WLVM uses boththe link structure and the article titles of Wikipediato measure similarity between two Wikipedia arti-cles.
Each link is weighted by the probability of itoccurring.
Thus, the value of the weight w for a linkx?
y between articles x and y is:w(x?
y) = |x?
y| ?
log(t?z=1tz ?
y)where t is the total number of articles in Wikipedia.The similarity of articles is compared by formingvectors of the articles which are linked from themand computing the cosine of their angle.
For exam-ple the vectors of two articles x and y are:x = (w(x?
l1), w(x?
l2), ..., w(x?
ln))y = (w(y ?
l1), w(y ?
l2), ..., w(y ?
ln))where x and y are two Wikipedia articles and x?
liis a link from article x to article li.Since the items have been mapped to Wikipediaarticles, similarity between two items is computedby performing pairwise comparison between articlesusing WLVM, for each, selecting the highest simi-larity score:sim(a, b) =12(?w1?aargmaxw2?bWLVM(w1, w2)|a|+?w2?bargmaxw1?aWLVM(w2, w1)|b|)where a and b are two items, |a| the number ofWikipedia articles in a and WLVM(w1, w2) is thesimilarity between concepts w1 and w2.4 SubmissionsWe selected three systems for submission.
The firstrun uses the similarity scores of the basic system(Section 2) for each similarity types as follows:?
General: cosine similarity of TF.IDF vectors,IDF based on Wikipedia (as shown in Section2.1).?
Author: product of the scores obtained ob-tained using TF.IDF vectors and UKB (asshown in Section 2.2) using only the data ex-tracted from dc:Creator field.?
People involved and location: product of co-sine similarity of TF.IDF vectors and UKB (asshown in Section 2.2) using the data extractedfrom all fields.?
Time period: time similarity measure (asshown in Section 2.3).?
Events: product of cosine similarity of TF.IDFvectors and UKB (as shown in Section 2.2) ofevent nouns and verbs recognized in all fields.?
Subject and description: cosine similarity ofTF.IDF vectors of respective fields (as shownin Section 2).For the second run we trained a ML model foreach of the similarity types, using the following fea-tures:?
Cosine similarity of TF.IDF vectors as shownin Section 2 for the eight similarity types.?
Four new values for general similarity: TF.IDF(Section 2.1), LDA (Section 3.1), UKB andWLVM (Section 3.3).?
Time similarity as shown in Section 2.3.?
Events similarity computed using UKB initial-ized with the event nouns and verbs in all fields.We decided not to use the product of TF.IDFand UKB presented in Section 2.2 in this systembecause our intention was to measure the power ofthe linear regression ML algorithm to learn on thegiven raw data.The third run is similar, but includes all availablefeatures (21).
In addition to the above, we included:?
Author, people involved and location similar-ity computed using UKB initialized with peo-ple/location recognized by NERC in dc:Creatorfield for author, and in all fields for people in-volved and location.?
Author, people involved, location and eventsimilarity scores computed by the product ofTF.IDF vectors and UKB values as shown inSection 2.2.5 ResultsEvaluation was carried out using the official scorerprovided by the organizers, which computes thePearson Correlation score for each of the eight sim-ilarity types plus an additional mean correlation.135Team and run General Author People involved Time Location Event Subject Description MeanUBC UOS-RUN1 0.7269 0.4474 0.4648 0.5884 0.4801 0.2522 0.4976 0.5389 0.5033UBC UOS-RUN2 0.7777 0.6680 0.6767 0.7609 0.7329 0.6412 0.7516 0.8024 0.7264UBC UOS-RUN3 0.7866 0.6941 0.6965 0.7654 0.7492 0.6551 0.7586 0.8067 0.7390Table 1: Results of our systems on the training data, using cross-validation when necessary.Team and run General Author People involved Time Location Event Subject Description Mean RankUBC UOS-RUN1 0.7256 0.4568 0.4467 0.5762 0.4858 0.3090 0.5015 0.5810 0.5103 6UBC UOS-RUN2 0.7457 0.6618 0.6518 0.7466 0.7244 0.6533 0.7404 0.7751 0.7124 4UBC UOS-RUN3 0.7461 0.6656 0.6544 0.7411 0.7257 0.6545 0.7417 0.7763 0.7132 3Table 2: Results of our submitted systems.5.1 DevelopmentThe three runs mentioned above were developed us-ing the training data made available by the organiz-ers.
In order to avoid overfitting we did not changethe default parameters of the linear regressor, and10-fold cross-validation was used for evaluating themodels on the training data.
The results of our sys-tems on the training data are shown on Table 1.
Thetable shows that the heuristics (RUN1) obtain lowresults, and that linear regression improves resultsconsiderably in all types.
Using the full set of fea-tures, RUN3 improves slightly over RUN2, but theimprovement is consistent across all types.5.2 TestThe test dataset was composed of 750 pairs of items.Table 2 illustrates the results of our systems in thetest dataset.
The results of the runs are very similarto those obtained on the training data, but the dif-ference between RUN2 and RUN3 is even smaller.Our systems were ranked #3 (RUN 3), #4 (RUN2) and #6 (RUN 1) among 14 systems submittedby 6 teams.
Our systems achieved good correlationscores for almost all similarity types, with the excep-tion of author similarity, which is the worst rankedin comparison with the rest of the systems.6 Conclusions and Future WorkIn this paper, we presented the systems submittedto the *SEM 2013 shared task on Semantic Tex-tual Similarity.
We combined some simple heuris-tics for each type of similarity, based on the appro-priate metadata fields.
The use of lineal regressionimproved the results considerably across all types.Our system fared well in the competition.
We sub-mitted three systems and the highest-ranked of theseachieved the third best results overall.AcknowledgementsThis work is partially funded by the PATHSproject (http://paths-project.eu) funded by the Eu-ropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementno.
270082.
Aitor Gonzalez-Agirre is supported bya PhD grant from the Spanish Ministry of Education,Culture and Sport (grant FPU12/06243).ReferencesEneko Agirre and Aitor Soroa.
2009.
Personalizingpagerank for word sense disambiguation.
In Proceed-ings of the 12th conference of the European chapter ofthe Association for Computational Linguistics (EACL-2009), Athens, Greece.Eneko Agirre, Montse Cuadros, German Rigau, and AitorSoroa.
2010.
Exploring knowledge bases for sim-ilarity.
In Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC10).
European Language Resources Associa-tion (ELRA).
ISBN: 2-9517408-6-7.
Pages 373?377.
?.Nikolaos Aletras and Mark Stevenson.
2012.
Computingsimilarity between cultural heritage items using multi-modal features.
In Proceedings of the 6th Workshopon Language Technology for Cultural Heritage, So-cial Sciences, and Humanities, pages 85?93, Avignon,France.Nikolaos Aletras, Mark Stevenson, and Paul Clough.2012.
Computing similarity between items in a digi-tal library of cultural heritage.
J. Comput.
Cult.
Herit.,5(4):16:1?16:19, December.R.
Baeza-Yates and B. Ribeiro-Neto.
1999.
Modern In-formation Retrieval.
Addison Wesley Longman Lim-ited, Essex.136David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022, March.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.D.
Milne and I. Witten.
2008.
Learning to Linkwith Wikipedia.
In Proceedings of the ACM Con-ference on Information and Knowledge Management(CIKM?2008), Napa Valley, California.D.
Milne.
2007.
Computing semantic relatedness usingWikipedia?s link structure.
In Proceedings of the NewZealand Computer Science Research Student Confer-ence.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.137
