Proceedings of NAACL HLT 2007, pages 396?403,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsIncremental Non-Projective Dependency ParsingJoakim NivreVa?xjo?
University, School of Mathematics and Systems EngineeringUppsala University, Department of Linguistics and Philologyjoakim.nivre@{msi.vxu.se,lingfil.uu.se}AbstractAn open issue in data-driven dependencyparsing is how to handle non-projectivedependencies, which seem to be requiredby linguistically adequate representations,but which pose problems in parsing withrespect to both accuracy and efficiency.Using data from five different languages,we evaluate an incremental deterministicparser that derives non-projective depen-dency structures in O(n2) time, supportedby SVM classifiers for predicting the nextparser action.
The experiments show thatunrestricted non-projective parsing givesa significant improvement in accuracy,compared to a strictly projective baseline,with up to 35% error reduction, leadingto state-of-the-art results for the givendata sets.
Moreover, by restricting theclass of permissible structures to limiteddegrees of non-projectivity, the parsingtime can be reduced by up to 50% withouta significant decrease in accuracy.1 IntroductionData-driven dependency parsing has been shown togive accurate and efficient parsing for a wide rangeof languages, such as Japanese (Kudo and Mat-sumoto, 2002), English (Yamada and Matsumoto,2003), Swedish (Nivre et al, 2004), Chinese (Chenget al, 2004), and Czech (McDonald et al, 2005).Whereas most of the early approaches were limitedto strictly projective dependency structures, wherethe projection of a syntactic head must be contin-uous, attention has recently shifted to the analysisof non-projective structures, which are required forlinguistically adequate representations, especially inlanguages with free or flexible word order.The most popular strategy for capturing non-projective structures in data-driven dependencyparsing is to apply some kind of post-processing tothe output of a strictly projective dependency parser,as in pseudo-projective parsing (Nivre and Nilsson,2005), corrective modeling (Hall and Nova?k, 2005),or approximate non-projective parsing (McDonaldand Pereira, 2006).
And it is rare to find parsersthat derive non-projective structures directly, the no-table exception being the non-projective spanningtree parser proposed by McDonald et al (2005).There are essentially two arguments that havebeen advanced against using parsing algorithmsthat derive non-projective dependency structures di-rectly.
The first is that the added expressivity com-promises efficiency, since the parsing problem for agrammar that allows arbitrary non-projective depen-dency structures has been shown to beNP complete(Neuhaus and Bro?ker, 1997).
On the other hand,most data-driven approaches do not rely on gram-mars, and with a suitable factorization of depen-dency structures, it is possible to achieve parsing ofunrestricted non-projective structures inO(n2) time,as shown by McDonald et al (2005).The second argument against non-projective de-pendency parsing comes from the observation that,even in languages with free or flexible word order,396most dependency structures are either projective orvery nearly projective.
This can be seen by con-sidering data from treebanks, such as the PragueDependency Treebank of Czech (Bo?hmova?
et al,2003), the TIGER Treebank of German (Brants etal., 2002), or the Slovene Dependency Treebank(Dz?eroski et al, 2006), where the overall proportionof non-projective dependencies is only about 2%even though the proportion of sentences that con-tain some non-projective dependency is as high as25%.
This means that an approach that starts by de-riving the best projective approximation of the cor-rect dependency structure is likely to achieve highaccuracy, while an approach that instead attemptsto search the complete space of non-projective de-pendency structures runs the risk of finding struc-tures that depart too much from the near-projectivenorm.
Again, however, the results of McDonald etal.
(2005) suggest that the latter risk is minimized ifinductive learning is used to guide the search.One way of improving efficiency, and potentiallyalso accuracy, in non-projective dependency parsingis to restrict the search to a subclass of ?mildly non-projective?
structures.
Nivre (2006) defines degreesof non-projectivity in terms of the maximum numberof intervening constituents in the projection of a syn-tactic head and shows that limited degrees of non-projectivity give a much better fit with the linguisticdata than strict projectivity, but also enables more ef-ficient processing than unrestricted non-projectivity.However, the results presented by Nivre (2006) areall based on oracle parsing, which means that theyonly provide upper bounds on the accuracy that canbe achieved.In this paper, we investigate to what extent con-straints on non-projective structures can improveaccuracy and efficiency in practical parsing, usingtreebank-induced classifiers to predict the actions ofa deterministic incremental parser.
The parsing al-gorithm used belongs to the family of algorithms de-scribed by Covington (2001), and the classifiers aretrained using support vector machines (SVM) (Vap-nik, 1995).
The system is evaluated using treebankdata from five languages: Danish, Dutch, German,Portuguese, and Slovene.The paper is structured as follows.
Section 2defines syntactic representations as labeled depen-dency graphs and introduces the notion of degreeused to constrain the search.
Section 3 describes theparsing algorithm, including modifications neces-sary to handle degrees of non-projectivity, and sec-tion 4 describes the data-driven prediction of parseractions, using history-based models and SVM clas-sifiers.
Section 5 presents the experimental setup,section 6 discusses the experimental results, and sec-tion 7 contains our conclusions.2 Dependency GraphsA dependency graph is a labeled directed graph, thenodes of which are indices corresponding to the to-kens of a sentence.
Formally:Definition 1 Given a set R of dependency types(arc labels), a dependency graph for a sentencex = (w1, .
.
.
, wn) is a labeled directed graph G =(V,E, L), where:1.
V = {0, 1, 2, .
.
.
, n}2.
E ?
V ?
V3.
L : E ?
RThe set V of nodes (or vertices) is the set of non-negative integers up to and including n. This meansthat every token index i of the sentence is a node(1 ?
i ?
n) and that there is a special node 0, whichwill always be a root of the dependency graph.
Theset E of arcs (or edges) is a set of ordered pairs(i, j), where i and j are nodes.
Since arcs are usedto represent dependency relations, we will say that iis the head and j is the dependent of the arc (i, j).The function L assigns a dependency type (label)r ?
R to every arc e ?
E. We use the notationi ?
j to mean that there is an arc connecting i andj (i.e., (i, j) ?
E); we use the notation i r?
j ifthis arc is labeled r (i.e., ((i, j), r) ?
L); and weuse the notation i ??
j and i ??
j for the reflexiveand transitive closure of the arc relation E and thecorresponding undirected relation, respectively.Definition 2 A dependency graph G is well-formedif and only if:1.
The node 0 is a root, i.e., there is no node i suchthat i ?
0 (ROOT).2.
G is weakly connected, i.e., i ??
j for everypair of nodes i, j (CONNECTEDNESS).3.
Every node has at most one head, i.e., if i?
jthen there is no node k such that k 6= i andk ?
j (SINGLE-HEAD).397(?Only one of them concerns quality.?
)0 1RZ(Out-of ?AuxP2Pnichthem ?Atr3VBjeis ?Pred4Tjenonly ?AuxZ5Cjednaone-FEM-SG ?Sb6Rnato ?AuxP7N4kvalituquality? Adv8Z:..) ?AuxZFigure 1: Dependency graph for Czech sentence from the Prague Dependency TreebankThe well-formedness conditions are independent inthat none of them is entailed by any (combination)of the others, but they jointly entail that the graphis a tree rooted at the node 0.
By way of example,figure 1 shows a Czech sentence from the PragueDependency Treebank (Bo?hmova?
et al, 2003) witha well-formed dependency graph according to Defi-nitions 1 and 2.The constraints imposed on dependency graphs inDefinition 2 are assumed in almost all versions ofdependency grammar, especially in computationalsystems, and are sometimes complemented by afourth constraint:4.
The graph G is projective, i.e., if i ?
j theni ??
k, for every node k such that i < k < jor j < k < i (PROJECTIVITY).Most theoretical formulations of dependency gram-mar regard projectivity as the norm but recognizethe need for non-projective representations to cap-ture non-local dependencies (Mel?c?uk, 1988; Hud-son, 1990).
Finding a way of incorporating a suit-ably restricted notion of non-projectivity into prac-tical parsing systems is therefore an important steptowards a more adequate syntactic analysis, as dis-cussed in the introduction of this paper.In order to distinguish classes of dependencygraphs that fall in between arbitrary non-projectiveand projective, Nivre (2006) introduces a notionof degree of non-projectivity, such that projectivegraphs have degree 0 while arbitrary non-projectivegraphs have unbounded degree.Definition 3 Let G = (V,E, L) be a well-formeddependency graph, let G(i,j) be the subgraph of Gdefined by V(i,j) = {i, i+1, .
.
.
, j?1, j}, and letmin(e) be the smallest and max(e) the largest ele-ment of an arc e in the linear order <:1.
The degree of an arc e ?
E is the number ofconnected components (i.e., weakly connectedsubgraphs) in G(min(e)+1,max(e)?1) that are notdominated by the head of e in G(min(e),max(e)).2.
The degree of G is the maximum degree of anyarc e ?
E.To exemplify the notion of degree, we note that thedependency graph in figure 1 has degree 1.
The onlynon-projective arc in the graph is (5, 1) and G(2,4)contains three connected components, each consist-ing of a single root node (2, 3, 4).
Since exactly oneof these, 3, is not dominated by 5 in G(1,5), the arc(5, 1) has degree 1.Nivre (2006) presents an empirical study, basedon data from the Prague Dependency Treebank ofCzech (Bo?hmova?
et al, 2003) and the Danish De-pendency Treebank (Kromann, 2003), showing thatmore than 99.5% of all sentences occurring in thetwo treebanks have a dependency graph with a max-imum degree of 2; about 98% have a maximum de-gree of 1; but only 77% in the Czech data and 85% inthe Danish data have degree 0 (which is equivalent toassuming PROJECTIVITY).
This suggests that lim-ited degrees of non-projectivity may allow a parserto capture a larger class of naturally occurring syn-tactic structures, while still constraining the searchto a proper subclass of all possible structures.11Alternative notions of mildly non-projective dependencystructures are explored in Kuhlmann and Nivre (2006).3983 Parsing AlgorithmCovington (2001) describes a parsing strategy fordependency representations that has been knownsince the 1960s but not presented in the literature.The left-to-right (or incremental) version of thisstrategy can be formulated in the following way:PARSE(x = (w1, .
.
.
, wn))1 for j = 1 up to n2 for i = j ?
1 down to 03 LINK(i, j)LINK(i, j) is a nondeterministic operation that addsthe arc i ?
j (with some label), adds the arc j ?
i(with some label), or does nothing at all.
In thisway, the algorithm builds a graph by systematicallytrying to link every pair of nodes (i, j) (i < j).We assume that LINK(i,j) respects the ROOT andSINGLE-HEAD constraints and that it does not in-troduce cycles into the graph, i.e., it adds an arci ?
j only if j 6= 0, there is no k 6= i such thatk ?
j, and it is not the case that j ??
i. Giventhese constraints, the graph G given at terminationcan always be turned into a well-formed dependencygraph by adding arcs from the root 0 to any root nodein {1, .
.
.
, n}.Assuming that LINK(i, j) can be performed insome constant time c, the running time of the al-gorithm is?ni=1 c(i ?
1) = c(n22 ?n2 ), which interms of asymptotic complexity is O(n2).
CheckingROOT and SINGLE-HEAD in constant time is easy,but in order to prevent cycles we need to be ableto find, for any node k, the root of the connectedcomponent to which k belongs in the partially builtgraph.
This problem can be solved efficiently us-ing standard techniques for disjoint sets, includingpath compression and union by rank, which guaran-tee that the necessary checks can be performed inaverage constant time (Cormen et al, 1990).In the experiments reported in this paper, we mod-ify the basic algorithm by making the performanceof LINK(i, j) conditional on the arcs (i, j) and (j, i)being permissible under different degree constraints:PARSE(x = (w1, .
.
.
, wn), d)1 for j = 1 up to n2 for i = j ?
1 down to 03 if PERMISSIBLE(i, j, d)4 LINK(i, j)The function PERMISSIBLE(i, j, d) returns true ifand only if i ?
j and j ?
i have a degree lessthan or equal to d given the partially built graph G.Setting d = 0 gives strictly projective parsing, whiled = ?
corresponds to unrestricted non-projectiveparsing.
With low values of d, we will reduce thenumber of calls to LINK(i, j), which will reducethe overall parsing time provided that the time re-quired to compute PERMISSIBLE(i, j, d) is insignif-icant compared to the time needed for LINK(i, j).This is typically the case in data-driven systems,where LINK(i, j) requires a call to a trained classi-fier, while PERMISSIBLE(i, j, d) only needs accessto the partially built graph G.24 History-Based ParsingHistory-based parsing uses features of the parsinghistory to predict the next parser action (Black et al,1992).
In the current setup, this involves using fea-tures of the partially built dependency graph G andthe input x = (w1, .
.
.
, wn) to predict the outcomeof the nondeterministic LINK(i, j) operation.
Giventhat we use a deterministic parsing strategy, this re-duces to a pure classification problem.Let ?
(i, j, G) = (?1,.
.
.
,?m) be a feature vec-tor representation of the parser history at the timeof performing LINK(i, j).
The task of the history-based classifier is then to map ?
(i, j, G) to one ofthe following actions:1.
Add the arc i r?
j (for some r ?
R).2.
Add the arc j r?
i (for some r ?
R).3.
Do nothing.Training data for the classifier can be generated byrunning the parser on a sample of treebank data, us-ing the gold standard dependency graph as an ora-cle to predict LINK(i, j) and constructing one train-ing instance (?
(i, j, G), a) for each performance ofLINK(i, j) with outcome a.The features in ?
(i, j, G) = (?1, .
.
.
, ?m) canbe arbitrary features of the input x and the partiallybuilt graph G but will in the experiments below berestricted to linguistic attributes of input tokens, in-cluding their dependency types according to G.2Checking PERMISSIBLE(i, j, d), again requires finding theroots of connected components and can therefore be done inaverage constant time.399Language Tok Sen T/S Lem CPoS PoS MSF Dep NPT NPSDanish 94 5.2 18.2 no 10 24 47 52 1.0 15.6Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4German 700 39.2 17.8 no 52 52 0 46 2.3 27.8Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2Table 1: Data sets; Tok = number of tokens (*1000); Sen = number of sentences (*1000); T/S = tokensper sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained part-of-speech tags;PoS = number of (fine-grained) part-of-speech tags; MSF = number of morphosyntactic features (split intoatoms); Dep = number of dependency types; NPT = proportion of non-projective dependencies/tokens (%);NPS = proportion of non-projective dependency graphs/sentences (%)The history-based classifier can be trained withany of the available supervised methods for func-tion approximation, but in the experiments below wewill rely on SVM, which has previously shown goodperformance for this kind of task (Kudo and Mat-sumoto, 2002; Yamada and Matsumoto, 2003).5 Experimental SetupThe purpose of the experiments is twofold.
First, wewant to investigate whether allowing non-projectivestructures to be derived incrementally can improveparsing accuracy compared to a strictly projectivebaseline.
Secondly, we want to examine whetherrestricting the degree of non-projectivity can im-prove efficiency compared to an unrestricted non-projective baseline.
In order to investigate both theseissues, we have trained one non-projective parserfor each language, allowing arbitrary non-projectivestructures as found in the treebanks during training,but applying different constraints during parsing:1.
Non-projective (d = ?)2.
Max degree 2 (d = 2)3.
Max degree 1 (d = 1)These three versions of the non-projective parser arecompared to a strictly projective parser (d = 0),which uses the same parsing algorithm but only con-siders projective arcs in both training and testing.3The experiments are based on treebank data fromfive languages: the Danish Dependency Treebank3An alternative would have been to train all parsers on non-projective data, or restrict the training data for each parseraccording to its parsing restriction.
Preliminary experimentsshowed that the setup used here gave the best performance forall parsers involved.
(Kromann, 2003), the Alpino Treebank of Dutch(van der Beek et al, 2002), the TIGER Treebank ofGerman (Brants et al, 2002), the Floresta Sinta?cticaof Portuguese (Afonso et al, 2002), and the SloveneDependency Treebank (Dz?eroski et al, 2006).4 Thedata sets used are the training sets from the CoNLL-X Shared Task on multilingual dependency parsing(Buchholz and Marsi, 2006), with 20% of the datareserved for testing using a pseudo-random split.
Ta-ble 1 gives an overview of the five data sets, showingthe number of tokens and sentences, the presenceof different kinds of linguistic annotation, and theamount of non-projectivity.The features used in the history-based model forall languages include the following core set of 20features, where i and j are the tokens about to belinked and the context stack is a stack of root nodesk in G(i+1,j?1), added from right to left (i.e., withthe top node being closest to i):1.
Word form: i, j, j+1, h(i).2.
Lemma (if available): i.3.
Part-of-speech: i?1, i, j, j+1, j+2, k, k?1.4.
Coarse part-of-speech (if available): i, j, k.5.
Morphosyntactic features (if available): i, j.6.
Dependency type: i, j, l(i), l(j), r(i).In the specification of features, we use k and k?1 torefer to the two topmost tokens on the context stack,and we use h(?
), l(?)
and r(?)
to refer to the head,4This set does not include the Prague Dependency Treebankof Czech (Bo?hmova?
et al, 2003), one of the most widely usedtreebanks in studies of non-projective parsing.
The reason isthat the sheer size of this data set makes extensive experimentsusing SVM learning extremely time consuming.
The work onCzech was therefore initially postponed but is now ongoing.400Danish Dutch German Portuguese SloveneConstraint AS ER AS ER AS ER AS ER AS ERNon-projective 88.13 8.34 86.79 36.18 89.78 21.51 90.59 11.39 76.52 6.83Max degree 2 88.08 7.95 86.15 33.09 89.74 21.20 90.58 11.30 76.48 6.67Max degree 1 88.00 7.33 85.12 28.12 89.49 19.28 90.48 10.36 76.40 6.35Projective 87.05 ?
79.30 ?
86.98 ?
89.38 ?
74.80 ?Table 2: Parsing accuracy; AS = attachment score; ER = error reduction w.r.t.
projective baseline (%)the leftmost dependent and the rightmost dependentof a token ?
in the partially built dependency graph.5In addition to the core set of features, the modelfor each language has been augmented with a smallnumber of additional features, which have provenuseful in previous experiments with the same dataset.
The maximum number of features used is 28(Danish); the minimum number is 23 (German).The history-based classifiers have been trainedusing SVM learning, which combines a maximummargin strategy with the use of kernel functionsto map the original feature space to a higher-dimensional space.
More specifically, we use LIB-SVM (Chang and Lin, 2001) with a quadratic kernelK(xi, xj) = (?xTi xj +r)2.
We use the built-in one-versus-one strategy for multi-class classification andconvert symbolic features to numerical features us-ing the standard technique of binarization.Parsing accuracy is measured by the unlabeled at-tachment score (AS), i.e., the proportion of wordsthat are assigned the correct head (not countingpunctuation).
Although the parsers do derive labeleddependency graphs, we concentrate on the graphstructure here, since this is what is concerned in thedistinction between projective and non-projectivedependency graphs.
Efficiency is evaluated by re-porting the parsing time (PT), i.e., the time requiredto parse the respective test sets.
Since both trainingsets and test sets vary considerably in size betweenlanguages, we are primarily interested in the rela-tive differences for parsers applied to the same lan-guage.
Experiments have been performed on a Sun-Blade 2000 with one 1.2GHz UltraSPARC-III pro-cessor and 2GB of memory.5The lack of symmetry in the feature set reflects the asym-metry in the partially built graph G, where, e.g., only i can havedependents to the right at decision time.
This explains why thereare more features defined in terms of graph structure for i andmore features defined in terms of string context for j.6 Results and DiscussionTable 2 shows the parsing accuracy of the non-projective parser with different maximum degrees,both the raw attachment scores and the amount oferror reduction with respect to the baseline parser.Our first observation is that the non-projective parserinvariably achieves higher accuracy than the pro-jective baseline, with differences that are statisti-cally significant across the board (using McNemar?stest).
The amount of error reduction varies be-tween languages and seems to depend primarily onthe frequency of non-projective structures, which isnot surprising.
Thus, for Dutch and German, thetwo languages with the highest proportion of non-projective structures, the best error reduction is over35% and over 20%, respectively.
However, thereseems to be a sparse data effect in that Slovene,which has the smallest training data set, has thesmallest error reduction despite having more non-projective structures than Danish and Portuguese.Our second observation is that the highest score isalways obtained with an unbounded degree of non-projectivity during parsing.
This seems to corrobo-rate the results obtained by McDonald et al (2005)with a different parsing method, showing that theuse of inductive learning to guide the search dur-ing parsing eliminates the potentially harmful ef-fect of increasing the size of the search space.
Al-though the differences between different degrees ofnon-projectivity are not statistically significant forthe current data sets,6 the remarkable consistencyacross languages suggests that they are neverthelessgenuine.
In either case, however, they must be con-sidered marginal, except possibly for Dutch, whichleads to our third and final observation about accu-6The only exception is the difference between a maximumdegree of 1 and unrestricted non-projective for Dutch, which issignificant according to McNemar?s test with ?= .05.401Danish Dutch German Portuguese SloveneConstraint PT TR PT TR PT TR PT TR PT TRNon-projective 426 ?
3791 ?
24454 ?
3708 ?
204 ?Max degree 2 395 7.29 2068 45.46 17903 26.79 3004 18.99 130 36.39Max degree 1 346 18.72 1695 55.28 13079 46.52 2446 34.04 108 47.05Projective 211 50.53 784 79.32 7362 69.90 1389 62.55 429 79.00Table 3: Parsing time; PT = parsing time (s); TR = time reduction w.r.t.
non-projective baseline (%)System Danish Dutch German Portuguese SloveneCoNLL-X McDonald et al 84.79 79.19 87.34 86.82 73.44CoNLL-X Nivre et al 84.77 78.59 85.82 87.60 70.30Incremental non-projective 84.85 77.91 85.90 87.12 70.86Table 4: Related work (labeled attachment score)racy, namely that restricting the maximum degree ofnon-projectivity to 2 or 1 has a very marginal effecton accuracy and is always significantly better thanthe projective baseline.Turning next to efficiency, table 3 shows the pars-ing time for the different parsers across the five lan-guages.
Our first observation here is that the pars-ing time can be reduced by restricting the degreeof non-projectivity during parsing, thus corroborat-ing the claim that the running time of the history-based classifier dominates the overall parsing time.As expected, the largest reduction is obtained withthe strictly projective parser, but here we must alsotake into account that the training data set is smaller(because of the restriction to projective potentiallinks), which improves the average running time ofthe history-based classifier in itself.
Our second ob-servation is that the amount of reduction in parsingtime seems to be roughly related to the amount ofnon-projectivity, with a reduction of about 50% ata max degree of 1 for the languages where morethan 20% of all sentences are non-projective (Dutch,German, Slovene) but significantly smaller for Por-tuguese and especially for Danish.
On the whole,however, the reduction in parsing time with limiteddegrees of non-projectivity is substantial, especiallyconsidering the very marginal drop in accuracy.In order to compare the performance to the stateof the art in dependency parsing, we have retrainedthe non-projective parser on the entire training dataset for each language and evaluated it on the finaltest set from the CoNLL-X shared task (Buchholzand Marsi, 2006).
Thus, table 4 shows labeled at-tachment scores, the main evaluation metric used inthe shared task, in comparison to the two highestscoring systems from the original evaluation (Mc-Donald et al, 2006; Nivre et al, 2006).
The incre-mental non-projective parser has the best reportedscore for Danish and outperforms at least one of theother two systems for four languages out of five,although most of the differences are probably toosmall to be statistically significant.
But whereas thespanning tree parser of McDonald et al (2006) andthe pseudo-projective parser of Nivre et al (2006)achieve this performance only with special pre- orpost-processing,7 the approach presented here de-rives a labeled non-projective graph in a single incre-mental process and hence at least has the advantageof simplicity.
Moreover, it has better time complex-ity than the approximate second-order spanning treeparsing of McDonald et al (2006), which has expo-nential complexity in the worst case (although thisdoes not appear to be a problem in practice).7 ConclusionIn this paper, we have investigated a data-driven ap-proach to dependency parsing that combines a deter-ministic incremental parsing algorithm with history-based SVM classifiers for predicting the next parseraction.
We have shown that, for languages with a7McDonald et al (2006) use post-processing for non-projective dependencies and for labeling.
Nivre et al (2006) usepre-processing of training data and post-processing of parseroutput to recover non-projective dependencies.402non-negligible proportion of non-projective struc-tures, parsing accuracy can be improved signifi-cantly by allowing non-projective structures to bederived.
We have also shown that the parsing timecan be reduced substantially, with only a marginalloss in accuracy, by limiting the degree of non-projectivity allowed during parsing.
A comparisonwith results from the CoNLL-X shared task showsthat the parsing accuracy is comparable to that of thebest available systems, which means that incremen-tal non-projective dependency parsing is a viable al-ternative to approaches based on post-processing ofprojective approximations.AcknowledgmentsThe research presented in this paper was partiallysupported by a grant from the Swedish ResearchCouncil.
I want to thank Johan Hall and Jens Nils-son for their contributions to MaltParser, which wasused to perform the experiments.
I am also gratefulto three anonymous reviewers for finding importanterrors in the preliminary version and for suggestingseveral other improvements for the final version.ReferencesS.
Afonso, E. Bick, R. Haber, and D. Santos.
2002.
?Florestasinta?(c)tica?
: a treebank for Portuguese.
In Proc.
of LREC,1698?1703.E.
Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mer-cer, and S. Roukos.
1992.
Towards history-based grammars:Using richer models for probabilistic parsing.
In Proc.
of theDARPA Speech and Natural Language Workshop, 31?37.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In A.
Abeille?, ed.,Treebanks: Building and Using Parsed Corpora, chapter 7.Kluwer, Dordrecht.S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002.The TIGER treebank.
In Proc.
of TLT.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared task onmultilingual dependency parsing.
In Proc.
of CoNLL, 149?164.C.-C. Chang and C.-J.
Lin, 2001.
LIBSVM: A Libraryfor Support Vector Machines.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.Y.
Cheng, M. Asahara, and Y. Matsumoto.
2004.
Determinis-tic dependency structure analyzer for Chinese.
In Proc.
ofIJCNLP, 500?508.T.
H. Cormen, C. E. Leiserson, and R. L. Rivest.
1990.
Intro-duction to Algorithms.
MIT Press.M.
A. Covington.
2001.
A fundamental algorithm for depen-dency parsing.
In Proc.
of the Annual ACM Southeast Con-ference, 95?102.S.
Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z.
Z?abokrtsky, andA.
Z?ele.
2006.
Towards a Slovene dependency treebank.
InProc.
of LREC.Keith Hall and Vaclav Nova?k.
2005.
Corrective modeling fornon-projective dependency parsing.
In Proc.
of IWPT, 42?52R.
A. Hudson.
1990.
English Word Grammar.
Blackwell.M.
T. Kromann.
2003.
The Danish dependency treebank andthe underlying linguistic theory.
In Proc.
of TLT.T.
Kudo and Y. Matsumoto.
2002.
Japanese dependency analy-sis using cascaded chunking.
In Proc.
of CoNLL, 63?69.M.
Kuhlmann and J. Nivre.
2006.
Mildly non-projective de-pendency structures.
In Proc.
of COLING-ACL, Posters,507?514.R.
McDonald and F-.
Pereira.
2006.
Online learning of approx-imate dependency parsing algorithms.
In Proc.
of EACL,81?88.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005.Non-projective dependency parsing using spanning tree al-gorithms.
In Proc.
of HLT-EMNLP, 523?530.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multilingualdependency analysis with a two-stage discriminative parser.In Proc.
of CoNLL, 216?220.I.
Mel?c?uk.
1988.
Dependency Syntax: Theory and Practice.State University of New York Press.P.
Neuhaus and N. Bro?ker.
1997.
The complexity of recog-nition of linguistically adequate dependency grammars.
InProc.
of ACL-EACL, 337?343.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective dependencyparsing.
In Proc.
of ACL, 99?106.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-based depen-dency parsing.
In Proc.
of CoNLL, 49?56.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006.Labeled pseudo-projective dependency parsing with supportvector machines.
In Proc.
of CoNLL, 221?225.J.
Nivre.
2006.
Constraints on non-projective dependencygraphs.
In Proc.
of EACL, 73?80.L.
van der Beek, G. Bouma, R. Malouf, and G. van Noord.2002.
The Alpino dependency treebank.
In ComputationalLinguistics in the Netherlands (CLIN).V.
N. Vapnik.
1995.
The Nature of Statistical Learning Theory.Springer.H.
Yamada and Y. Matsumoto.
2003.
Statistical dependencyanalysis with support vector machines.
In Proc.
of IWPT,195?206.403
