Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 32?39,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsFeature-Based Segmentation of Narrative DocumentsDavid KauchakPalo Alto Research Center andUniversity of California, San DiegoSan Diego, CA 92093dkauchak@cs.ucsd.eduFrancine ChenPalo Alto Research Center3333 Coyote Hill Rd.Palo Alto, CA 94304fchen@parc.comAbstractIn this paper we examine topic segmen-tation of narrative documents, which arecharacterized by long passages of textwith few headings.
We first present resultssuggesting that previous topic segmenta-tion approaches are not appropriate fornarrative text.
We then present a feature-based method that combines features fromdiverse sources as well as learned features.Applied to narrative books and encyclope-dia articles, our method shows results thatare significantly better than previous seg-mentation approaches.
An analysis of in-dividual features is also provided and thebenefit of generalization using outside re-sources is shown.1 IntroductionMany long text documents, such as magazine arti-cles, narrative books and news articles contain fewsection headings.
The number of books in narrativestyle that are available in digital form is rapidly in-creasing through projects such as Project Gutenbergand the Million Book Project at Carnegie MellonUniversity.
Access to these collections is becom-ing easier with directories such as the Online BooksPage at the University of Pennsylvania.As text analysis and retrieval moves from retrievalof documents to retrieval of document passages, theability to segment documents into smaller, coherentregions enables more precise retrieval of meaningfulportions of text (Hearst, 1994) and improved ques-tion answering.
Segmentation also has applicationsin other areas of information access, including docu-ment navigation (Choi, 2000), anaphora and ellipsisresolution, and text summarization (Kozima, 1993).Research projects on text segmentation have fo-cused on broadcast news stories (Beeferman et al,1999), expository texts (Hearst, 1994) and synthetictexts (Li and Yamanishi, 2000; Brants et al, 2002).Broadcast news stories contain cues that are indica-tive of a new story, such as ?coming up?, or phrasesthat introduce a reporter, which are not applicable towritten text.
In expository texts and synthetic texts,there is repetition of terms within a topical segment,so that the similarity of ?blocks?
of text is a usefulindicator of topic change.
Synthetic texts are createdby concatenating stories, and exhibit stronger topicchanges than the subtopic changes within a docu-ment; consequently, algorithms based on the simi-larity of text blocks work well on these texts.In contrast to these earlier works, we present amethod for segmenting narrative documents.
In thisdomain there is little repetition of words and the seg-mentation cues are weaker than in broadcast newsstories, resulting in poor performance from previousmethods.We present a feature-based approach, where thefeatures are more strongly engineered using linguis-tic knowledge than in earlier approaches.
The key tomost feature-based approaches, particularly in NLPtasks where there is a broad range of possible featuresources, is identifying appropriate features.
Select-ing features in this domain presents a number of in-teresting challenges.
First, features used in previousmethods are not sufficient for solving this problem.We explore a number of different sources of infor-mation for extracting features, many previously un-used.
Second, the sparse nature of text and the high32cost of obtaining training data requires generaliza-tion using outside resources.
Finally, we incorporatefeatures from non-traditional resources such as lexi-cal chains where features must be extracted from theunderlying knowledge representation.2 Previous ApproachesPrevious topic segmentation methods fall into threegroups: similarity based, lexical chain based, andfeature based.
In this section we give a briefoverview of each of these groups.2.1 Similarity-basedOne popular method is to generate similarities be-tween blocks of text (such as blocks of words,sentences or paragraphs) and then identify sectionboundaries where dips in the similarities occur.The cosine similarity measure between term vec-tors is used by Hearst (1994) to define the simi-larity between blocks.
She notes that the largestdips in similarity correspond to defined boundaries.Brants et al (2002) learn a PLSA model using EMto smooth the term vectors.
The model is parame-terized by introducing a latent variable, representingthe possible ?topics?.
They show good performanceon a number of different synthetic data sets.Kozima and Furugori (1994) use another similar-ity metric they call ?lexical cohesion?.
The ?cohe-siveness?
of a pair of words is calculated by spread-ing activation on a semantic network as well as wordfrequency.
They showed that dips in lexical cohe-sion plots had some correlation with human subjectboundary decisions on one short story.2.2 Lexical ChainsSemantic networks define relationships be-tween words such as synonymy, specializa-tion/generalization and part/whole.
Stokes et al(2002) use these relationships to construct lexicalchains.
A lexical chain is a sequence of lexicograph-ically related word occurrences where every wordoccurs within a set distance from the previous word.A boundary is identified where a large numbers oflexical chains begin and end.
They showed thatlexical chains were useful for determining the textstructure on a set of magazine articles, though theydid not provide empirical results.2.3 Feature-basedBeeferman et al (1999) use an exponential modeland generate features using a maximum entropy se-lection criterion.
Most features learned are cue-based features that identify a boundary based on theoccurrence of words or phrases.
They also include afeature that measures the difference in performanceof a ?long range?
vs. ?short range?
model.
Whenthe short range model outperforms the long rangemodel, this indicates a boundary.
Their method per-formed well on a number of broadcast news datasets, including the CNN data set from TDT 1997.Reynar (1999) describes a maximum entropymodel that combines hand selected features, includ-ing: broadcast news domain cues, number of contentword bigrams, number of named entities, number ofcontent words that are WordNet synonyms in the leftand right regions, percentage of content words in theright segment that are first uses, whether pronounsoccur in the first five words, and whether a wordfrequency based algorithm predicts a boundary.
Hefound that for the HUB-4 corpus, which is composedof transcribed broadcasts, that the combined featuremodel performed better than TextTiling.Mochizuki et al (1998) use a combination of lin-guistic cues to segment Japanese text.
Although anumber of cues do not apply to English (e.g., top-ical markers), they also use anaphoric expressionsand lexical chains as cues.
Their study was small,but did indicate that lexical chains are a useful cuein some domains.These studies indicate that a combination of fea-tures can be useful for segmentation.
However,Mochizuki et al (1998) analyzed Japanese texts, andReynar (1999) and Beeferman et al (1999) evalu-ated on broadcast news stories, which have manycues that narrative texts do not.
Beeferman et al(1999) also evaluated on concatenated Wall StreetJournal articles, which have stronger topic changesthan within a document.
In our work, we examinethe use of linguistic features for segmentation of nar-rative text in English.3 Properties of Narrative TextCharacterizing data set properties is the first steptowards deriving useful features.
The approachesin the previous section performed well on broad-33Table 1: Previous approaches evaluated on narrativedata from BiohazardWord Sent.
WindowModel Error Error Diffrandom 0.486 0.490 0.541TextTiling 0.481 0.497 0.526PLSA 0.480 0.521 0.559cast news, expository and synthetic data sets.
Manyproperties of these documents are not shared by nar-rative documents.
These properties include: 1) cuephrases, such as ?welcome back?
and ?joining us?that feature-based methods used in broadcast news,2) strong topic shifts, as in synthetic documents cre-ated by concatenating newswire articles, and 3) largedata sets such that the training data and testing dataappeared to come from similar distributions.In this paper we examine two narrative-stylebooks: Biohazard by Ken Alibek and The Demonin the Freezer by Richard Preston.
These books aresegmented by the author into sections.
We manu-ally examined these author identified boundaries andthey are reasonable.
We take these sections as truelocations of segment boundaries.
We split Biohaz-ard into three parts, two for experimentation (exp1and exp2) and the third as a holdout for testing.
De-mon in the Freezer was reserved for testing.
Biohaz-ard contains 213 true and 5858 possible boundaries.Demon has 119 true and 4466 possible boundaries.Locations between sentences are considered possi-ble boundaries and were determined automatically.We present an analysis of properties of the bookBiohazard by Ken Alibek as an exemplar of nar-rative documents (for this section, test=exp1 andtrain=exp2).
These properties are different from pre-vious expository data sets and will result in poor per-formance for the algorithms mentioned in Section 2.These properties help guide us in deriving featuresthat may be useful for segmenting narrative text.Vocabulary The book contains a single topic with anumber of sub-topics.
These changing topics, com-bined with the varied use of words for narrative doc-uments, results in many unseen terms in the test set.25% of the content words in the test set do not oc-cur in the training set and a third of the words in thetest set occur two times or less in the training set.This causes problems for those methods that learna model of the training data such as Brants et al(2002) and Beeferman et al (1999) because, with-out outside resources, the information in the trainingdata is not sufficient to generalize to the test set.Boundary words Many feature-based methods relyon cues at the boundaries (Beeferman et al, 1999;Reynar, 1999).
474 content terms occur in the firstsentence of boundaries in the training set.
Of theseterms, 103 occur at the boundaries of the test set.However, of those terms that occur signicantly ata training set boundary (where significant is de-termined by a likelihood-ratio test with a signifi-cance level of 0.1), only 9 occur at test boundaries.No words occur significantly at a training boundaryAND also significantly at a test boundary.Segment similarity Table 1 shows that twosimilarity-based methods that perform well on syn-thetic and expository text perform poorly (i.e., onpar with random) on Biohazard.
The poor perfor-mance occurs because block similarities provide lit-tle information about the actual segment boundarieson this data set.
We examined the average similarityfor two adjacent regions within a segment versus theaverage similarity for two adjacent regions that crossa segment boundary.
If the similarity scores wereuseful, the within segment scores would be higherthan across segment scores.
Similarities were gener-ated using the PLSA model, averaging over multiplemodels with between 8 and 20 latent classes.
Theaverage similarity score within a segment was 0.903with a standard deviation of 0.074 and the averagescore across a segment boundary was 0.914 with astandard deviation of 0.041.
In this case, the acrossboundary similarity is actually higher.
Similar val-ues were observed for the cosine similarities used bythe TextTiling algorithm, as well as with other num-bers of latent topics for the PLSA model.
For allcases examined, there was little difference betweeninter-segment similarity and across-boundary simi-larity, and there was always a large standard devia-tion.Lexical chains Lexical chains were identified assynonyms (and exact matches) occurring withina distance of one-twentieth the average segmentlength and with a maximum chain length equal tothe average segment length (other values were ex-34amined with similar results).
Stokes et al (2002)suggest that high concentrations of lexical chain be-ginnings and endings are indicative of a boundarylocation.
On the narrative data, of the 219 over-all chains, only 2 begin at a boundary and only 1ends at a boundary.
A more general heuristic iden-tifies boundaries where there is an increase in thenumber of chains beginning and ending near a possi-ble boundary while also minimizing chains that spanboundaries.
Even this heuristic does not appear in-dicative on this data set.
Over 20% of the chainsactually cross segment boundaries.
We also mea-sured the average distance from a boundary and thenearest beginning and ending of a chain if a chainbegins/ends within that segment.
If the chains are agood feature, then these should be relatively small.The average segment length is 185 words, but theaverage distance to the closest beginning chain is 39words away and closest ending chain is 36 wordsaway.
Given an average of 4 chains per segment,the beginning and ending of chains were not concen-trated near boundary locations in our narrative data,and therefore not indicative of boundaries.4 Feature-Based SegmentationWe pose the problem of segmentation as a classifi-cation problem.
Sentences are automatically iden-tified and each boundary between sentences is apossible segmentation point.
In the classificationframework, each segmentation point becomes an ex-ample.
We examine both support vector machines(SVMlight (Joachims, 1999)) and boosted decisionstumps (Weka (Witten and Frank, 2000)) for ourlearning algorithm.
SVMs have shown good per-formance on a variety of problems, including nat-ural language tasks (Cristianini and Shawe-Taylor,2000), but require careful feature selection.
Classifi-cation using boosted decisions stumps can be a help-ful tool for analyzing the usefulness of individualfeatures.
Examining multiple classification meth-ods helps avoid focusing on the biases of a particularlearning method.4.1 Example ReweightingOne problem with formulating the segmentationproblem as a classification problem is that there aremany more negative than positive examples.
To dis-courage the learning algorithm from classifying allresults as negative and to instead focus on the posi-tive examples, the training data must be reweighted.We set the weight of positive vs. negative exam-ples so that the number of boundaries after testingagrees with the expected number of segments basedon the training data.
This is done by iteratively ad-justing the weighting factor while re-training and re-testing until the predicted number of segments on thetest set is approximately the expected number.
Theexpected number of segments is the number of sen-tences in the test set divided by the number of sen-tences per segment in the training data.
This valuecan also be weighted based on prior knowledge.4.2 PreprocessingA number of preprocessing steps are applied to thebooks to help increase the informativeness of thetexts.
The book texts were obtained using OCRmethods with human correction.
The text is pre-processed by tokenizing, removing stop words, andstemming using the Inxight LinguistiX morpholog-ical analyzer.
Paragraphs are identified using for-matting information.
Sentences are identified usingthe TnT tokenizer and parts of speech with the TnTpart of speech tagger (Brants, 2000) with the stan-dard English Wall Street Journal n-grams.
Namedentities are identified using finite state technology(Beesley and Karttunen, 2003) to identify variousentities including: person, location, disease and or-ganization.
Many of these preprocessing steps helpprovide salient features for use during segmentation.4.3 Engineered FeaturesSegmenting narrative documents raises a number ofinteresting challenges.
First, labeling data is ex-tremely time consuming.
Therefore, outside re-sources are required to better generalize from thetraining data.
WordNet is used to identify words thatare similar and tend to occur at boundaries for the?word group?
feature.
Second, some sources of in-formation, in particular entity chains, do not fit intothe standard feature based paradigm.
This requiresextracting features from the underlying informationsource.
Extracting these features represents a trade-off between information content and generalizabil-ity.
In the case of entity chains, we extract featuresthat characterize the occurrence distribution of the35entity chains.
Finally, the ?word groups?
and ?entitygroups?
feature groups generate candidate featuresand a selection process is required to select usefulfeatures.
We found that a likelihood ratio test for sig-nificance worked well for identifying those featuresthat would be useful for classification.
Throughoutthis section, when we use the term ?significant?
weare referring to significant with respect to the likeli-hood ratio test (with a significance level of 0.1).We selected features both a priori and dynami-cally during training (i.e., word groups and entitygroups are selected dynamically).
Feature selectionhas been used by previous segmentation methods(Beeferman et al, 1999) as a way of adapting bet-ter to the data.
In our approach, knowledge aboutthe task is used more strongly in defining the fea-ture types, and the selection of features is performedprior to the classification step.
We also used mutualinformation, statistical tests of significance and clas-sification performance on a development data set toidentify useful features.Word groups In Section 3 we showed that there arenot consistent cue phrases at boundaries.
To general-ize better, we identify word groups that occur signif-icantly at boundaries.
A word group is all words thathave the same parent in the WordNet hierarchy.
Abinary feature is used for each learned group basedon the occurrence of at least one of the words in thegroup.
Groups found include months, days, tempo-ral phrases, military rankings and country names.Entity groups For each entity group (i.e.
namedentities such as person, city, or disease tagged by thenamed entity extractor) that occurs significantly ata boundary, a feature indicating whether or not anentity of that group occurs in the sentence is used.Full name The named entity extraction systemtags persons named in the document.
A roughco-reference resolution was performed by group-ing together references that share at least one to-ken (e.g., ?General Yury Tikhonovich Kalinin?
and?Kalinin?).
The full name of a person is the longestreference of a group referring to the same person.This feature indicates whether or not the sentencecontains a full name.Entity chains Word relationships work well whenthe documents have disjoint topics; however, whentopics are similar, words tend to relate too easily.
Wepropose a more stringent chaining method called en-tity chains.
Entity chains are constructed in the samefashion as lexical chains, except we consider namedentities.
Two entities are considered related (i.e.
inthe same chain) if they refer to the same entity.
Weconstruct entity chains and extract features that char-acterize these chains: How many chains start/end atthis sentence?
How many chains cross over this sen-tence/previous sentence/next sentence?
Distance tothe nearest dip/peak in the number of chains?
Sizeof that dip/peak?Pronoun Does the sentence contain a pronoun?Does the sentence contain a pronoun within 5 wordsof the beginning of the sentence?Numbers During training, the patterns of numbersthat occur significantly at boundaries are selected.Patterns considered are any number and any numberwith a specified length.
The feature then checks ifthat pattern appears in the sentence.
A commonlyfound pattern is the number pattern of length 4,which often refers to a year.Conversation Is this sentence part of a conversa-tion, i.e.
does this sentence contain ?direct speech?
?This is determined by tracking beginning and end-ing quotes.
Quoted regions and single sentences be-tween two quoted regions are considered part of aconversation.Paragraph Is this the beginning of a paragraph?5 ExperimentsIn this section, we examine a number of narra-tive segmentation tasks with different segmentationmethods.
The only data used during developmentwas the first two thirds from Biohazard (exp1 andexp2).
All other data sets were only examined afterthe algorithm was developed and were used for test-ing purposes.
Unless stated otherwise, results for thefeature based method are using the SVM classifier.15.1 Evaluation MeasuresWe use three segmentation evaluation metrics thathave been recently developed to account for ?closebut not exact?
placement of hypothesized bound-aries: word error probability, sentence error prob-ability, and WindowDiff.
Word error probability1SVM and boosted decision stump performance is similar.For brevity, only SVM results are shown for most results.36Table 2: Experiments with BiohazardWord Sent.
Window Sent errError Error Diff improvBiohazardrandom (sent.)
0.488 0.485 0.539 ?
?-random (para.)
0.481 0.477 0.531 (base)Biohazardexp1 ?
holdout 0.367 0.357 0.427 25%exp2 ?
holdout 0.344 0.325 0.395 32%3x cross validtn.
0.355 0.332 0.404 24%Train BiohazardTest Demon 0.387 0.364 0.473 25%(Beeferman et al, 1999) estimates the probabilitythat a randomly chosen pair of words k words apartis incorrectly classified, i.e.
a false positive or falsenegative of being in the same segment.
In contrast tothe standard classification measures of precision andrecall, which would consider a ?close?
hypothesizedboundary (e.g., off by one sentence) to be incorrect,word error probability gently penalizes ?close?
hy-pothesized boundaries.
We also compute the sen-tence error probability, which estimates the proba-bility that a randomly chosen pair of sentences s sen-tences apart is incorrectly classified.
k and s are cho-sen to be half the average length of a section in thetest data.
WindowDiff (Pevzner and Hearst, 2002)uses a sliding window over the data and measuresthe difference between the number of hypothesizedboundaries and the actual boundaries within the win-dow.
This metric handles several criticisms of theword error probability metric.5.2 Segmenting Narrative BooksTable 2 shows the results of the SVM-segmenter onBiohazard and Demon in the Freezer.
A baselineperformance for segmentation algorithms is whetherthe algorithm performs better than naive segment-ing algorithms: choose no boundaries, choose allboundaries and choose randomly.
Choosing allboundaries results in word and sentence error proba-bilities of approximately 55%.
Choosing no bound-aries is about 45%.
Table 2 also shows the resultsfor random placement of the correct number of seg-ments.
Both random boundaries at sentence loca-tions and random boundaries at paragraph locationsare shown (values shown are the averages of 500random runs).
Similar results were obtained for ran-dom segmentation of the Demon data.Table 3: Performance on Groliers articlesWord Sent.
WindowError Error Diffrandom 0.482 0.483 0.532TextTile 0.407 0.412 0.479PLSA 0.420 0.435 0.507features (stumps) 0.387 0.400 0.495features (SVM) 0.385 0.398 0.503For Biohazard the holdout set was not used dur-ing development.
When trained on either of the de-velopment thirds of the text (i.e., exp1 or exp2) andtested on the test set, a substantial improvement isseen over random.
3-fold cross validation was doneby training on two-thirds of the data and testing onthe other third.
Recalling from Table 1 that bothPLSA and TextTiling result in performance simi-lar to random even when given the correct numberof segments, we note that all of the single train/testsplits performed better than any of the naive algo-rithms and previous methods examined.To examine the ability of our algorithm to performon unseen data, we trained on the entire Biohaz-ard book and tested on Demon in the Freezer.
Per-formance on Demon in the Freezer is only slightlyworse than the Biohazard results and is still muchbetter than the baseline algorithms as well as previ-ous methods.
This is encouraging since Demon wasnot used during development, is written by a differ-ent author and has a segment length distribution thatis different than Biohazard (average segment lengthof 30 vs. 18 in Biohazard).5.3 Segmenting ArticlesUnfortunately, obtaining a large number of narrativebooks with meaningful labeled segmentation is dif-ficult.
To evaluate our algorithm on a larger data setas well as a wider variety of styles similar to narra-tive documents, we also examine 1000 articles fromGroliers Encyclopedia that contain subsections de-noted by major and minor headings, which we con-sider to be the true segment boundaries.
The articlescontained 8,922 true and 102,116 possible bound-aries.
We randomly split the articles in half, andperform two-fold cross-validation as recommendedby Dietterich (1998).
Using 500 articles from onehalf of the pair for testing, 50 articles are randomlyselected from the other half for training.
We used37Table 4: Ave. human performance (Hearst, 1994)Word Sent.
WindowError (%) Error (%) Diff (%)Sequoia 0.275 0.272 0.351Earth 0.219 0.221 0.268Quantum 0.179 0167 0.316Magellan 0.147 0.147 0.157a subset of only 50 articles due to the high cost oflabeling data.
Each split yields two test sets of 500articles and two training sets.
This procedure of two-fold cross-validation is performed five times, for atotal of 10 training and 10 corresponding test sets.Significance is then evaluated using the t-test.The results for segmenting Groliers Encyclope-dia articles are given in Table 3.
We comparethe performance of different segmentation models:two feature-based models (SVMs, boosted deci-sion stumps), two similarity-based models (PLSA-based segmentation, TextTiling), and randomly se-lecting segmentation points.
All segmentation sys-tems are given the estimated number of segmenta-tion points based based on the training data.
Thefeature based approaches are significantly2 betterthan either PLSA, TextTiling or random segmenta-tion.
For our selected features, boosted stump per-formance is similar to using an SVM, which rein-forces our intuition that the selected features (andnot just classification method) are appropriate forthis problem.Table 1 indicates that the previous TextTiling andPLSA-based approaches perform close to randomon narrative text.
Our experiments show a perfor-mance improvement of >24% by our feature-basedsystem, and significant improvement over othermethods on the Groliers data.
Hearst (1994) ex-amined the task of identifying the paragraph bound-aries in expository text.
We provide analysis of thisdata set here to emphasize that identifying segmentsin natural text is a difficult problem and since cur-rent evaluation methods were not used when thisdata was initially presented.
Human performanceon this task is in the 15%-35% error rate.
Hearstasked seven human judges to label the paragraph2For both SVM and stumps at a level of 0.005 us-ing a t-test except SVM TextTile-WindowDiff (at 0.05)and stumps TextTile-WindowDiff and SVM/stumps PLSA-WindowDiff (not significantly different)Table 5: Feature occurrences at boundary and non-boundary locationsboundary non-boundaryParagraph 74 621Entity groups 44 407Word groups 39 505Numbers 16 59Full name 2 109Conversation 0 510Pronoun 8 742Pronoun ?
5 1 330boundaries of four different texts.
Since no groundtruth was available, true boundaries were identifiedby those boundaries that had a majority vote as aboundary.
Table 4 shows the average human perfor-mance for each text.
We show these results not fordirect comparison with our methods, but to highlightthat even human segmentation on a related task doesnot achieve particularly low error rates.5.4 Analysis of FeaturesThe top section of Table 5 shows features that areintuitively hypothesized to be positively correlatedwith boundaries and the bottom section shows nega-tively correlated.
For this analysis, exp1 from Alibekwas used for training and the holdout set for testing.There are 74 actual boundaries and 2086 possiblylocations.
Two features have perfect recall: para-graph and conversation.
Every true section bound-ary is at a paragraph and no section boundaries arewithin conversation regions.
Both the word groupand entity group features have good correlation withboundary locations and also generalized well to thetraining data by occurring in over half of the positivetest examples.The benefit of generalization using outside re-sources can be seen by comparing the boundarywords found using word groups versus those foundonly in the training set as in Section 3.
Using wordgroups triples the number of significant words foundin the training set that occur in the test set.
Also, thenumber of shared words that occur significantly inboth the training and test set goes from none to 9.More importantly, significant words occur in 37 ofthe test segments instead of none without the groups.386 Discussion and SummaryBased on properties of narrative text, we proposedand investigated a set of features for segmenting nar-rative text.
We posed the problem of segmentationas a feature-based classification problem, which pre-sented a number of challenges: many different fea-ture sources, generalization from outside resourcesfor sparse data, and feature extraction from non-traditional information sources.Feature selection and analyzing feature interac-tion is crucial for this type of application.
The para-graph feature has perfect recall in that all boundariesoccur at paragraph boundaries.
Surprisingly, for cer-tain train/test splits of the data, the performance ofthe algorithm was actually better without the para-graph feature than with it.
We hypothesize that thenoisiness of the data is causing the classifier to learnincorrect correlations.In addition to feature selection issues, posing theproblem as a classification problem loses the se-quential nature of the data.
This can produce veryunlikely segment lengths, such as a single sentence.We alleviated this by selecting features that captureproperties of the sequence.
For example, the entitychains features represent some of this type of infor-mation.
However, models for complex sequentialdata should be examined as possible better methods.We evaluated our algorithm on two books andencyclopedia articles, observing significantly bet-ter performance than randomly selecting the correctnumber of segmentation points, as well as two pop-ular, previous approaches, PLSA and TextTiling.AcknowledgmentsWe thank Marti Hearst for the human subject perfor-mance data and the anonymous reviewers for theirvery helpful comments.
Funded in part by the Ad-vanced Research and Development Activity NIMDprogram (MDA904-03-C-0404).ReferencesDoug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.
Ma-chine Learning, 34:177?210.Kenneth R. Beesley and Lauri Karttunen.
2003.
FiniteState Morphology.
CSLI Publications, Palo Alto, CA.Thorsten Brants, Francine Chen, and Ioannis Tsochan-taridis.
2002.
Topic-based document segmentationwith probabilistic latent semantic analysis.
In Pro-ceedings of CIKM, pg.
211?218.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speechtagger.
In Proceedings of the Applied NLP Confer-ence.Freddy Choi.
2000.
Improving the efficiency of speechinterfaces for text navigation.
In Proceedings of IEEEColloquium: Speech and Language Processing forDisabled and Elderly People.Nello Cristianini and John Shawe-Taylor.
2000.
An In-troduction to Support Vector Machines.
CambridgeUniversity Press.Thomas Dietterich.
1998.
Approximate statistical testsfor comparing supervised classification learning algo-rithms.
Neural Computation, 10:1895?1923.Marti A. Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Meeting of ACL, pg.
9?16.Thorsten Joachims, 1999.
Advances in Kernel Methods -Support Vector Learning, chapter Making large-ScaleSVM Learning Practical.
MIT-Press.Hideki Kozima and Teiji Furugori.
1994.
Segmentingnarrative text into coherent scenes.
In Literary andLinguistic Computing, volume 9, pg.
13?19.Hideki Kozima.
1993.
Text segmentation based on sim-ilarity between words.
In Meeting of ACL, pg.
286?288.Hang Li and Kenji Yamanishi.
2000.
Topic analysis us-ing a finite mixture model.
In Proceedings of JointSIGDAT Conference of EMNLP and Very Large Cor-pora, pg.
35?44.Hajime Mochizuki, Takeo Honda, and Manabu Okumura.1998.
Text segmentation with multiple surface lin-guistic cues.
In COLING-ACL, pg.
881?885.Lev Pevzner and Marti Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, pg.
19?36.Jeffrey Reynar.
1999.
Statistical models for topic seg-mentation.
In Proceedings of ACL, pg.
357?364.Nicola Stokes, Joe Carthy, and Alex Smeaton.
2002.Segmenting broadcast news streams using lexicalchains.
In Proceedings of Starting AI ResearchersSymposium, (STAIRS 2002), pg.
145?154.Ian H. Witten and Eibe Frank.
2000.
Data Mining:Practical machine learning tools with Java implemen-tations.
Morgan Kaufmann.39
