Proceedings of NAACL HLT 2007, pages 41?48,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsSource-Language Features and Maximum Correlation Trainingfor Machine Translation EvaluationDing Liu and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractWe propose three new features for MTevaluation: source-sentence constrainedn-gram precision, source-sentence re-ordering metrics, and discriminative un-igram precision, as well as a method oflearning linear feature weights to directlymaximize correlation with human judg-ments.
By aligning both the hypothe-sis and the reference with the source-language sentence, we achieve better cor-relation with human judgments than pre-viously proposed metrics.
We furtherimprove performance by combining indi-vidual evaluation metrics using maximumcorrelation training, which is shown to bebetter than the classification-based frame-work.1 IntroductionEvaluation has long been a stumbling block in thedevelopment of machine translation systems, due tothe simple fact that there are many correct trans-lations for a given sentence.
The most commonlyused metric, BLEU, correlates well over large testsets with human judgments (Papineni et al, 2002),but does not perform as well on sentence-level eval-uation (Blatz et al, 2003).
Later approaches to im-prove sentence-level evaluation performance can besummarized as falling into four types:?
Metrics based on common loose sequences ofMT outputs and references (Lin and Och, 2004;Liu and Gildea, 2006).
Such metrics wereshown to have better fluency evaluation per-formance than metrics based on n-grams suchBLEU and NIST (Doddington, 2002).?
Metrics based on syntactic similarities such asthe head-word chain metric (HWCM) (Liu andGildea, 2005).
Such metrics try to improve flu-ency evaluation performance for MT, but theyheavily depend on automatic parsers, which aredesigned for well-formed sentences and cannotgenerate robust parse trees for MT outputs.?
Metrics based on word alignment between MToutputs and the references (Banerjee and Lavie,2005).
Such metrics do well in adequacy evalu-ation, but are not as good in fluency evaluation,because of their unigram basis (Liu and Gildea,2006).?
Combination of metrics based on machinelearning.
Kulesza and Shieber (2004) usedSVMs to combine several metrics.
Theirmethod is based on the assumption thathigher classification accuracy in discriminat-ing human- from machine-generated transla-tions will yield closer correlation with humanjudgment.
This assumption may not alwayshold, particularly when classification is diffi-cult.
Lita et al (2005) proposed a log-linearmodel to combine features, but they only didpreliminary experiments based on 2 features.Following the track of previous work, to improveevaluation performance, one could either proposenew metrics, or find more effective ways to combinethe metrics.
We explore both approaches.
Muchwork has been done on computing MT scores based41on the pair of MT output/reference, and we aim toinvestigate whether some other information couldbe used in the MT evaluation, such as source sen-tences.
We propose two types of source-sentencerelated features as well as a feature based on part ofspeech.
The three new types of feature can be sum-marized as follows:?
Source-sentence constrained n-gram precision.Overlapping n-grams between an MT hypothe-sis and its references do not necessarily indicatecorrect translation segments, since they couldcorrespond to different parts of the source sen-tence.
Thus our constrained n-gram precisioncounts only overlapping n-grams in MT hy-pothesis and reference which are aligned to thesame words in the source sentences.?
Source-sentence reordering agreement.
Withthe alignment information, we can compare thereorderings of the source sentence in the MThypothesis and in its references.
Such compar-ison only considers the aligned positions of thesource words in MT hypothesis and references,and thus is oriented towards evaluating the sen-tence structure.?
Discriminative unigram precision.
We dividethe normal n-gram precision into many sub-precisions according to their part of speech(POS).
The division gives us flexibility to trainthe weights of each sub-precision in frame-works such as SVM and Maximum Correla-tion Training, which will be introduced later.The motivation behind such differentiation isthat different sub-precisions should have dif-ferent importance in MT evaluation, e.g., sub-precision of nouns, verbs, and adjectives shouldbe important for evaluating adequacy, andsub-precision in determiners and conjunctionsshould mean more in evaluating fluency.Along the direction of feature combination, sinceindirect weight training using SVMs, based on re-ducing classification error, cannot always yield goodperformance, we train the weights by directly opti-mizing the evaluation performance, i.e., maximizingthe correlation with the human judgment.
This typeof direct optimization is known as Minimum ErrorRate Training (Och, 2003) in the MT community,and is an essential component in building the state-of-art MT systems.
It would seem logical to applysimilar methods to MT evaluation.
What is more,Maximum Correlation Training (MCT) enables usto train the weights based on human fluency judg-ments and adequacy judgments respectively, andthus makes it possible to make a fluency-oriented oradequacy-oriented metric.
It surpasses previous MTmetrics?
approach, where a a single metric evaluatesboth fluency and adequacy.
The rest of the paper isorganized as follows: Section 2 gives a brief recap ofn-gram precision-based metrics and introduces ourthree extensions to them; Section 3 introduces MCTfor MT evaluation; Section 4 describes the experi-mental results, and Section 5 gives our conclusion.2 Three New Features for MT EvaluationSince our source-sentence constrained n-gram preci-sion and discriminative unigram precision are bothderived from the normal n-gram precision, it isworth describing the original n-gram precision met-ric, BLEU (Papineni et al, 2002).
For every MThypothesis, BLEU computes the fraction of n-gramswhich also appear in the reference sentences, as wellas a brevity penalty.
The formula for computingBLEU is shown below:BLEU = BPNNXn=1PCPngram?C Countclip(ngram)PCPngram?
?C Count(ngram?
)where C denotes the set of MT hypotheses.Countclip(ngram) denotes the clipped number ofn-grams in the candidates which also appear in thereferences.
BP in the above formula denotes thebrevity penalty, which is set to 1 if the accumulatedlength of the MT outputs is longer than the arith-metic mean of the accumulated length of the refer-ences, and otherwise is set to the ratio of the two.For sentence-level evaluation with BLEU, we com-pute the score based on each pair of MT hypothe-sis/reference.
Later approaches, as described in Sec-tion 1, use different ways to manipulate the morpho-logical similarity between the MT hypothesis and itsreferences.
Most of them, except NIST, consider thewords in MT hypothesis as the same, i.e., as long asthe words in MT hypothesis appear in the references,42they make no difference to the metrics.1 NIST com-putes the n-grams weights as the logarithm of the ra-tio of the n-gram frequency and its one word lowern-gram frequency.
From our experiments, NIST isnot generally better than BLEU, and the reason, weconjecture, is that it differentiates the n-grams toomuch and the frequency estimated upon the evalua-tion corpus is not always reliable.
In this section wewill describe two other strategies for differentiatingthe n-grams, one of which uses the alignments withthe source sentence as a further constraint, while theother differentiates the n-gram precisions accordingto POS.2.1 Source-sentence Constrained N-gramPrecisionThe quality of an MT sentence should be indepen-dent of the source sentence given the reference trans-lation, but considering that current metrics are allbased on shallow morphological similarity of theMT outputs and the reference, without really under-standing the meaning in both sides, the source sen-tences could have some useful information in dif-ferentiating the MT outputs.
Consider the Chinese-English translation example below:Source: wo bu neng zhe me zuoHypothesis: I must hardly not do thisReference: I must not do thisIt is clear that the word not in the MT output can-not co-exist with the word hardly while maintain-ing the meaning of the source sentence.
None ofthe metrics mentioned above can prevent not frombeing counted in the evaluation, due to the simplereason that they only compute shallow morphologi-cal similarity.
Then how could the source sentencehelp in the example?
If we reveal the alignmentof the source sentence with both the reference andthe MT output, the Chinese word bu neng wouldbe aligned to must not in the reference and musthardly in the MT output respectively, leaving theword not in the MT output not aligned to any word inthe source sentence.
Therefore, if we can somehowfind the alignments between the source sentence andthe reference/MT output, we could be smarter in se-lecting the overlapping words to be counted in the1In metrics such as METEOR, ROUGE, SIA (Liu andGildea, 2006), the positions of words do make difference, butit has nothing to do with the word itself.for all n-grams wi, ..., wi+n?1 in MT hypothesisdomax val = 0;for all reference sentences dofor all n-grams rj , ..., rj+n?1 in current ref-erence sentence doval=0;for k=0; k ?
n-1; k ++ doif wi+k equals rj+k AND MTaligniequals REFalignj thenval += 1n ;if val ?
max val thenmax val = val;hit count += max val;return hit countMThypothesislength ?
length penalty;Figure 1: Algorithm for Computing Source-sentence Constrained n-gram Precisionmetric: only select the words which are aligned tothe same source words.
Now the question comesto how to find the alignment of source sentence andMT hypothesis/references, since the evaluation dataset usually does not contain alignment information.Our approach uses GIZA++2 to construct the many-to-one alignments between source sentences and theMT hypothesis/references respectively.3 GIZA++could generate many-to-one alignments either fromsource sentence to the MT hypothesis, in which caseevery word in MT hypothesis is aligned to a setof (or none) words in the source sentence, or fromthe reverse direction, in which case every word inMT hypothesis is aligned to exactly one word (ornone) word in the source sentence.
In either case,using MTaligni and REFaligni to denote the po-sitions of the words in the source sentences whichare aligned to a word in the MT hypothesis and aword in the reference respectively, the algorithm forcomputing source-sentence constrained n-gram pre-cision of length n is described in Figure 1.Since source-sentence constrained n-gram preci-sion (SSCN) is a precision-based metric, the vari-2GIZA++ is available athttp://www.fjoch.com/GIZA++.html3More refined alignments could be got for source-hypothesisfrom the MT system, and for source-references by using manualproof-reading after the automatic alignment.
Doing so, how-ever, requires the MT system?s cooperation and some costly hu-man labor.43able length penalty is used to avoid assigning ashort MT hypothesis a high score, and is computedin the same way as BLEU.
Note that in the algo-rithm for computing the precision of n-grams longerthan one word, not all words in the n-grams shouldsatisfy the source-sentence constraint.
The reason isthat the high order n-grams are already very sparsein the sentence-level evaluation.
To differentiate theSSCNs based on the source-to-MT/Ref (many-to-one) alignments and the MT/Ref-to-source (many-to-one) alignments, we use SSCN1 and SSCN2 todenote them respectively.
Naturally, we could com-bine the constraint in SSCN1 and SSCN2 by eithertaking their union (the combined constrained is sat-isfied if either one is satisfied) or intersecting them(the combined constrained is satisfied if both con-straints are satisfied).
We use SSCN u and SSCN ito denote the SSCN based on unioned constraintsand intersected constraints respectively.
We couldalso apply the stochastic word mapping proposed inSIA (Liu and Gildea, 2006) to replace the hard wordmatching in Figure 1, and the corresponding met-rics are denoted as pSSCN1, pSSCN2, pSSCN u,pSSCN i, with the suffixed number denoting differ-ent constraints.2.2 Metrics Based on Source Word ReorderingMost previous MT metrics concentrate on the co-occurrence of the MT hypothesis words in the ref-erences.
Our metrics based on source sentence re-orderings, on the contrary, do not take words identi-ties into account, but rather compute how similarlythe source words are reordered in the MT output andthe references.
For simplicity, we only consider thepairwise reordering similarity.
That is, for the sourceword pair wi and wj , if their aligned positions in theMT hypothesis and a reference are in the same order,we call it a consistent word pair.
Our pairwise re-ordering similarity (PRS) metric computes the frac-tion of the consistent word pairs in the source sen-tence.
Figure 2 gives the formal description of PRS.SrcMTi and SrcRefk,i denote the aligned positionof source word wi in the MT hypothesis and the kthreference respectively, and N denotes the length ofthe source sentence.Another criterion for evaluating the reordering ofthe source sentence in the MT hypothesis is howwell it maintains the original word order in thefor all word pair wi, wj in the source sentencesuch that i < j dofor all reference sentences rk doif (SrcMTi == SrcMTj ANDSrcRefk,i == SrcRefk,j) OR((SrcMTi ?
SrcMTj) ?
(SrcRefk,i ?SrcRefk,j) > 0) thencount + +; break;return 2?countN?
(N?1) ;Figure 2: Compute Pairwise Reordering Similarityfor all word pair wi, wj in the source sentence,such that i < j doif SrcMTi ?
SrcMTj < 0 thencount + +;return 2?countN?
(N?1) ;Figure 3: Compute Source Sentence Monotonic Re-ordering Ratiosource sentence.
We know that most of the time,the alignment of the source sentence and the MT hy-pothesis is monotonic.
This idea leads to the metricof monotonic pairwise ratio (MPR), which computesthe fraction of the source word pairs whose alignedpositions in the MT hypothesis are of the same order.It is described in Figure 3.2.3 Discriminative Unigram Precision Basedon POSThe Discriminative Unigram Precision Based onPOS (DUPP) decomposes the normal unigram pre-cision into many sub-precisions according to theirPOS.
The algorithm is described in Figure 4.These sub-precisions by themselves carry thesame information as standard unigram precision, butthey provide us the opportunity to make a bettercombined metric than the normal unigram precisionwith MCT, which will be introduced in next section.for all unigram s in the MT hypothesis doif s is found in any of the references thencountPOS(s) += 1precisionx = countxmt hypothesis length?x ?
POSFigure 4: Compute DUPP for N-gram with length n44Such division could in theory be generalized to workwith higher order n-grams, but doing so would makethe n-grams in each POS set much more sparse.
Thepreprocessing step for the metric is tagging boththe MT hypothesis and the references with POS.
Itmight elicit some worries about the robustness of thePOS tagger on the noise-containing MT hypothesis.This should not be a problem for two reasons.
First,compared with other preprocessing steps like pars-ing, POS tagging is easier and has higher accuracy.Second, because the counts for each POS are accu-mulated, the correctness of a single word?s POS willnot affect the result very much.3 Maximum Correlation Training forMachine Translation EvaluationMaximum Correlation Training (MCT) is an in-stance of the general approach of directly optimiz-ing the objective function by which a model willultimately be evaluated.
In our case, the model isthe linear combination of the component metrics, theparameters are the weights for each component met-ric, and the objective function is the Pearson?s corre-lation of the combined metric and the human judg-ments.
The reason to use the linear combination ofthe metrics is that the component metrics are usu-ally of the same or similar order of magnitude, and itmakes the optimization problem easy to solve.
Us-ing w to denote the weights, and m to denote thecomponent metrics, the combined metric x is com-puted as:x(w) =?jwjmj (1)Using hi and x(w)i denote the human judgmentand combined metric for a sentence respectively, andN denote the number of sentences in the evaluationset, the objective function is then computed as:Pearson(X(w), H) =PNi=1 x(w)ihi ?PNi=1 x(w)iPNi=1 hiNq(PNi=1 x(w)2i ?
(PNi=1 x(w)i)2N )(PNi=1 h2i ?
(PNi=1 hi)2N )Now our task is to find the weights for each compo-nent metric so that the correlation of the combinedmetric with the human judgment is maximized.
Itcan be formulated as:w = argmaxwPearson(X(w), H) (2)The function Pearson(X(w), H) is differentiablewith respect to the vector w, and we compute thisderivative analytically and perform gradient ascent.Our objective function not always convex (one caneasily create a non-convex function by setting thehuman judgments and individual metrics to someparticular value).
Thus there is no guarantee that,starting from a random w, we will get the glob-ally optimal w using optimization techniques suchas gradient ascent.
The easiest way to avoid endingup with a bad local optimum to run gradient ascentby starting from different random points.
In our ex-periments, the difference in each run is very small,i.e., by starting from different random initial valuesof w, we end up with, not the same, but very similarvalues for Pearson?s correlation.4 ExperimentsExperiments were conducted to evaluate the perfor-mance of the new metrics proposed in this paper,as well as the MCT combination framework.
Thedata for the experiments are from the MT evalua-tion workshop at ACL05.
There are seven sets ofMT outputs (E09 E11 E12 E14 E15 E17 E22), eachof which contains 919 English sentences translatedfrom the same set of Chinese sentences.
There arefour references (E01, E02, E03, E04) and two setsof human scores for each MT hypothesis.
Each hu-man score set contains a fluency and an adequacyscore, both of which range from 1 to 5.
We create aset of overall human scores by averaging the humanfluency and adequacy scores.
For evaluating the au-tomatic metrics, we compute the Pearson?s correla-tion of the automatic scores and the averaged humanscores (over the two sets of available human scores),for overall score, fluency, and adequacy.
The align-ment between the source sentences and the MT hy-pothesis/references is computed by GIZA++, whichis trained on the combined corpus of the evalua-tion data and a parallel corpus of Chinese-Englishnewswire text.
The parallel newswire corpus con-tains around 75,000 sentence pairs, 2,600,000 En-glish words and 2,200,000 Chinese words.
The45stochastic word mapping is trained on a French-English parallel corpus containing 700,000 sentencepairs, and, following Liu and Gildea (2005), we onlykeep the top 100 most similar words for each En-glish word.4.1 Performance of the Individual MetricsTo evaluate our source-sentence based metrics, theyare used to evaluate the 7 MT outputs, with the 4 setsof human references.
The sentence-level Pearson?scorrelation with human judgment is computed foreach MT output, and the averaged results are shownin Table 1.
As a comparison, we also show the re-sults of BLEU, NIST, METEOR, ROUGE, WER,and HWCM.
For METEOR and ROUGE, WORD-NET and PORTER-STEMMER are enabled, and forSIA, the decay factor is set to 0.6.
The numberin brackets, for BLEU, shows the n-gram length itcounts up to, and for SSCN, shows the length of then-gram it uses.
In the table, the top 3 results in eachcolumn are marked bold and the best result is alsounderlined.
The results show that the SSCN2 met-rics are better than the SSCN1 metrics in adequacyand overall score.
This is understandable since whatSSCN metrics need is which words in the sourcesentence are aligned to an n-gram in the MT hy-pothesis/references.
This is directly modeled in thealignment used in SSCN2.
Though we could alsoget such information from the reverse alignment, asin SSCN1, it is rather an indirect way and could con-tain more noise.
It is interesting that SSCN1 getsbetter fluency evaluation results than SSCN2.
TheSSCN metrics with the unioned constraint, SSCN u,by combining the strength of SSCN1 and SSCN2,get even better results in all three aspects.
We cansee that SSCN metrics, even without stochastic wordmapping, get significantly better results than theirrelatives, BLEU, which indicates the source sen-tence constraints do make a difference.
SSCN2 andSSCN u are also competitive to the state-of-art MTmetrics such as METEOR and SIA.
The best SSCNmetric, pSSCN u(2), achieves the best performanceamong all the testing metrics in overall and ade-quacy, and the second best performance in fluency,which is just a little bit worse than the best fluencymetric SIA.The two reordering based metrics, PRS and MPR,are not as good as the other testing metrics, in termsFluency Adequacy OverallROUGE W 24.8 27.8 29.0ROUGE S 19.7 30.9 28.5METEOR 24.4 34.8 33.1SIA 26.8 32.1 32.6NIST 1 09.6 22.6 18.5WER 22.5 27.5 27.7PRS 14.2 19.4 18.7MPR 11.0 18.2 16.5BLEU(1) 18.4 29.6 27.0BLEU(2) 20.4 31.1 28.9BLEU(3) 20.7 30.4 28.6HWCM(2) 22.1 30.3 29.2SSCN1(1) 24.2 29.6 29.8SSCN2(1) 22.9 33.0 31.3SSCN u(1) 23.8 34.2 32.5SSCN i(1) 23.4 28.0 28.5pSSCN1(1) 24.9 30.2 30.6pSSCN2(1) 23.8 34.0 32.4pSSCN u(1) 24.5 34.6 33.1pSSCN i(1) 24.1 28.8 29.3SSCN1(2) 24.0 29.6 29.7SSCN2(2) 23.3 31.5 31.8SSCN u(2) 24.1 34.5 32.8SSCN i(2) 23.1 27.8 28.2pSSCN1(2) 24.9 30.2 30.6pSSCN2(2) 24.3 34.4 32.8pSSCN u(2) 25.2 35.4 33.9pSSCN i(2) 23.9 28.7 29.1Table 1: Performance of Component Metricsof the individual performance.
It should not be sur-prising since they are totally different kind of met-rics, which do not count the overlapping n-grams,but the consistent/monotonic word pair reorderings.As long as they capture some property of the MThypothesis, they might be able to boost the per-formance of the combined metric under the MCTframework.4.2 Performance of the Combined MetricsTo test how well MCT works, the following schemeis used: each set of MT outputs is evaluated by MCT,which is trained on the other 6 sets of MT outputsand their corresponding human judgment; the aver-aged correlation of the 7 sets of MT outputs with thehuman judgment is taken as the final result.4.2.1 Discriminative Unigram Precision basedon POSWe first use MCT to combine the discriminativeunigram precisions.
To reduce the sparseness of theunigrams of each POS, we do not use the originalPOS set, but use a generalized one by combining46all POS tags with the same first letter (e.g., the dif-ferent verb forms such as VBN, VBD, and VBZ aretransformed to V).
The unified POS set contains 23POS tags.
To give a fair comparison of DUPP withBLEU, the length penalty is also added into it as acomponent.
Results are shown in Table 2.
DUPP f,DUPP a and DUPP o denote DUPP trained on hu-man fluency, adequacy and overall judgment respec-tively.
This shows that DUPP achieves obvious im-provement over BLEU, with only the unigrams andlength penalty, and DUPP f/ a/ o gets the best re-sult in fluency/adequacy/overall evaluation, showingthat MCT is able to make a fluency- or adequacy-oriented metric.4.2.2 Putting It All TogetherThe most interesting question in this paper is, withall these metrics, how well we can do in the MTevaluation.
To answer the question, we put all themetrics described into the MCT framework and usethe combined metric to evaluate the 7 MT outputs.Note that to speed up the training process, we donot directly use 24 DUPP components, instead, weuse the 3 combined DUPP metrics.
With the met-rics shown in Table 1, we then have in total 31 met-rics.
Table 2 shows the results of the final combinedmetric.
We can see that MCT trained on fluency,adequacy and overall human judgment get the bestresults among all the testing metrics in fluency, ade-quacy and overall evaluation respectively.
We did at-test with Fisher?s z transform for the combined re-sults and the individual results to see how significantthe difference is.
The combined results in adequacyand overall are significantly better at 99.5% confi-dence than the best results of the individual metrics(pSSCN u(2)), and the combined result in fluencyis significantly better at 96.9% confidence than thebest individual metric (SIA).
We also give the upperbound for each evaluation aspect by training MCTon the testing MT outputs, e.g., we train MCT onE09 and then use it to evaluate E09.
The upper-bound is the best we can do with the MCT basedon linear combination.
Another linear framework,Classification SVM (CSVM),4 is also used to com-bine the testing metrics except DUPP.
Since DUPPis based on MCT, to make a neat comparison, werule out DUPP in the experiments with CSVM.
The4http://svmlight.joachims.org/Fluency Adequacy OverallDUPP f 23.6 30.1 30.1DUPP a 22.1 32.9 30.9DUPP o 23.2 32.8 31.3MCT f(4) 30.3 36.7 37.2MCT a(4) 28.0 38.9 37.4MCT o(4) 29.4 38.8 38.0Upper bound 35.3 43.4 42.2MCT f(3) 29.2 34.7 35.3MCT a(3) 27.4 38.4 36.8MCT o(3) 28.8 38.0 37.2CSVM(3) 27.3 36.9 35.5Table 2: Combination of the Testing Metricstesting scheme is the same as MCT, except that weonly use 3 references for each MT hypothesis, andthe positive samples for training CSVM are com-puted as the scores of one of the 4 references basedon the other 3 references.
The slack parameter ofCSVM is chosen so as to maximize the classifica-tion accuracy of a heldout set of 800 negative and800 positive samples, which are randomly selectedfrom the training set.
The results are shown in Ta-ble 2.
We can see that MCT, with the same numberof reference sentences, is better than CSVM.
Notethat the resources required by MCT and CSVM aredifferent.
MCT uses human judgments to adjust theweights, while CSVM needs extra human referencesto produce positive training samples.To have a rough idea of how the component met-rics contribute to the final performance of MCT, weincrementally add metrics into the MCT in descend-ing order of their overall evaluation performance,with the results shown in Figure 5.
We can see thatthe performance improves as the number of metricsincreases, in a rough sense.
The major improvementhappens in the 3rd, 4th, 9th, 14th, and 30th metrics,which are METEOR, SIA, DUPP a, pSSCN1(1),and PRS.
It is interesting to note that these are notthe metrics with the highest individual performance.Another interesting observation is that there are notwo metrics belonging to the same series in the mostbeneficial metrics, indicating that to get better com-bined metrics, individual metrics showing differentsentence properties are preferred.5 ConclusionThis paper first describes two types of new ap-proaches to MT evaluation, which includes making470 5 10 15 20 25 30 350.240.260.280.30.320.340.360.380.4the number of metrics (o: adequacy, x: fluency, +: overall)correlationwithhumanfluency/overall/adequacyjudgementsFigure 5: Performance as a Function of the Numberof Interpolated Metricsuse of source sentences, and discriminating unigramprecisions based on POS.
Among all the testing met-rics including BLEU, NIST, METEOR, ROUGE,and SIA, our new metric, pSSCN u(2), based onsource-sentence constrained bigrams, achieves thebest adequacy and overall evaluation results, and thesecond best result in fluency evaluation.
We fur-ther improve the performance by combining the in-dividual metrics under the MCT framework, whichis shown to be better than a classification basedframework such as SVM.
By examining the contri-bution of each component metric, we find that met-rics showing different properties of a sentence aremore likely to make a good combined metric.Acknowledgments This work was supported byNSF grants IIS-0546554, IIS-0428020, and IIS-0325646.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judegments.
In Proceedings ofthe ACL-04 workshop on Intrinsic and Extrinsic Eval-uation Measures for Machine Translation and/or Sum-marization, Ann Arbor, Michigan.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2003.
Confidence estimation formachine translation.
Technical report, Center for Lan-guage and Speech Processing, Johns Hopkins Univer-sity, Baltimore.
Summer Workshop Final Report.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In In HLT 2002, Human Language TechnologyConference, San Diego, CA.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the 10th International Conferenceon Theoretical and Methodological Issues in MachineTranslation (TMI), Baltimore, MD, October.Chin-Yew Lin and Franz Josef Och.
2004.
Automaticevaluation of machine translation quality using longestcommon subsequence and skip-bigram statistics.
InProceedings of the 42th Annual Conference of theAssociation for Computational Linguistics (ACL-04),Barcelona, Spain.Lucian Vlad Lita, Monica Rogati, and Alon Lavie.
2005.Blanc: Learning evaluation metrics for mt.
Vancouver.Ding Liu and Daniel Gildea.
2005.
Syntactic features forevaluation of machine translation.
In ACL 2005 Work-shop on Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summarization.Ding Liu and Daniel Gildea.
2006.
Stochastic iterativealignment for machine translation evaluation.
Sydney.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of ACL-03.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL-02, Philadelphia, PA.48
