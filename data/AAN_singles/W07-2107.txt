Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 480?483,Prague, June 2007. c?2007 Association for Computational LinguisticsWIT: Web People Search Disambiguation using Random WalksJose?
Iria, Lei Xia, Ziqi ZhangThe University of Sheffield211 Portobello StreetSheffield S1 4DP, United Kingdom{j.iria, l.xia, z.zhang}@sheffield.ac.ukAbstractIn this paper, we describe our work on a ran-dom walks-based approach to disambiguat-ing people in web search results, and the im-plementation of a system that supports suchapproach, which we used to participate atSemeval?07 Web People Search task.1 IntroductionFinding information about people on the Web us-ing a search engine is far from being a quick andeasy process.
There is very often a many-to-manymapping of person names to the actual persons, thatis, several persons may share the same name, andseveral names may refer to the same person.
Infact, person names are highly ambiguous: (Guha andGarg, 2004) reports that only 90.000 thousand dif-ferent names are shared by 100 million people ac-cording to the U.S. Census Bureau.
This creates theneed to disambiguate the several referents typicallyfound in the web pages returned by a query for agiven person name.The Semeval?07 Web People Search challenge(Artiles et al, 2007) formally evaluated systems onthis task.
In this paper, we describe our work ona random walks-based approach to disambiguatingpeople in web search results, heavily influenced by(Minkov et al, 2006).
This particular model waschosen due to its elegance in seamlessly combininglexico-syntactic features local to a given webpagewith topological features derived from its place inthe network formed by the hyperlinked web pagesreturned by the query, to arrive at one single mea-sure of similarity between any two pages.2 Proposed MethodIn a nutshell, our approach 1) uses a graph to modelthe web pages returned by the search engine query,2) discards irrelevant web pages using a few sim-ple hand-crafted heuristics, 3) computes a similaritymatrix for web pages using random walks over thegraph, and 4) finally clusters the web pages given thesimilarity matrix.
The next subsections detail thesesteps.2.1 Web People Search GraphWe build a directed weighted typed graph from thecorpus.
The graph is a 5-tuple G = (V,E, t, l, w),where V is the set of nodes,E : V ?V is the orderedset of edges, t : V ?
T is the node type function(T = {t1, .
.
.
, t|T |} is a set of types), l : E ?
L isthe edge label function (L = {l1, .
.
.
, l|L|} is a set oflabels), and w : L ?
R is the label weight function.We structure our problem domain with the types andlabels presented in Figure 1.In order to transform the text into a graph thatconforms to the model shown, we take the output ofstandard NLP tools and input it as nodes and edgesinto the graph, indexing nodes by string value to en-sure that identical contents for any given node typeare merged into a single node in the graph.
To pro-cess the corpus, we run a standard NLP pipelineseperately over the metadata, title and body of theHTML pages, but not before having transformedits contents as much into plain text as possible, byremoving HTML tags, javascript code, etc.
Thepipeline used is composed of tokenization, removalof stop words and infrequent words, and stemmingwith Porter?s algorithm.
The resulting graph at this480Figure 1: The data representation model adoptedstage consists of the nodes of type Token, Webpage,Metadata, Title and Body, properly interconnected.We then run a named entity recognizer to associateNE tags to the respective documents, via the con-stituent words of the NE.
The information about theoriginal URL of page is given by the corpus, whileHost is trivially obtained from it.
We finalise thegraph by inserting an edge of type linked by betweenany web page linked by another in the corpus, andan edge of type related to between any web page re-lated to another in the corpus, as given by Google?srelated: operator.For the named entity recognition task, we havecompared GATE and OpenNLP toolkits.
Althoughboth toolkits show comparable results, OpenNLPdemonstrated faster performance.
Moreover, somedocuments in the corpus consisted of very exten-sive lists of names (e.g.
phonebook records) whichslowed the NER to a halt in practice.
To compen-sate for this, we applied a chunking window at thebeginning and end of each body content and aroundeach occurrence of the person name being consid-ered (and its variants determined heuristically).
Thewindow size used was 3000 characters in length,and an overlap between windows results in a mergedwindow.2.2 Discarding using heuristicsTo discard irrelevant documents within the corpus,we manually devised two heuristics rules for classi-fication by observing the training data at hand.
Theheuristics are 1) whether the page has content at all,2) whether the page contains at least one appearanceof mentioned person name with its variants.
Thissimple classification showed high precision and lowrecall on the training data.
We also tried a SVM-based classifier trained on a typical bag-of-wordsfeature vector space obtained from the training data,but found the such classifier not to be sufficiently re-liable.2.3 Random Walks ModelWe aim to determine the similarity between any twonodes of type Webpage in the graph.
In our work,similarity between two nodes in the graph is ob-tained by employing a random walks model.
A ran-dom walk, sometimes called a ?drunkard?s walk,?
isa formalization of the intuitive idea of taking suc-cessive steps in a graph, each in a random direction(Lova?sz, 2004).
Intuitively, the ?harder?
it is for adrunkard to arrive at a given webpage starting fromanother, the less similar the two pages are.Our model defines weights for each edge type,which, informally, determine the relevance of eachfeature type to establish a similarity between anytwo pages.
Let Ltd = {l(x, y) : (x, y) ?
E ?T (x) = td} be the set of possible labels for edgesleaving nodes of type td.
We require that the weightsform a probability distribution over Ltd , i.e.
?l?Ltdw(l) = 1 (1)We build an adjacency matrix of locally appropriatesimilarity between nodes asWij ={ ?lk?Lw(lk)|(i,?)?E:l(i,?
)=lk|, (i, j) ?
E0, otherwise(2)where Wij is the ith-line and jth-column entry ofW , indexed by V .
Equation 2 distributes uniformlythe weight of edges of the same type leaving a givennode.
We could choose to distribute them otherwise,e.g.
we could distribute the weights according tosome string similarity function or language model(Erkan, 2006), depending on the label.We associate the state of a Markov chain to ev-ery node of the graph, that is, to each node i weassociate the one-step probability P (0)(j|i) of a ran-dom walker traversing to an adjacent node j. These481probabilities are expressed by the row stochastic ma-trix D?1W , where D is the diagonal degree ma-trix given by Dii =?k Wik.
The ?reinforced?similarity between two nodes in the graph is givenby the t-step transition probability P (t)(j|i), whichcan be simply computed by a matrix power, i.e.,P (t)(j|i) = [(D?1W )t]ij .Note that t should not be very large in our case.The probability distribution of an infinite randomwalk over the nodes, called the stationary distribu-tion of the graph, is uninteresting to us for cluster-ing purposes since it gives an information related tothe global structure of the graph.
It is often used asa measure to rank the structural importance of thenodes in a graph (Page et al, 1998).
For clustering,we are more interested in the local similarities insidea cluster of nodes that separate them from the rest ofthe graph.
Also, in practice, using t > 2 leads tohigh computational cost requirements, as the matrixbecomes more dense as t grows.Equation 2 introduces the need to learn the func-tion w. In other words, we need to tune the model touse the most relevant features for this particular task.Tuning is performed on the training set by compar-ing the standard purity and inverse purity measuresof the clusters against the gold standard, and usinga simulated annealing optimization method as de-scribed in (Nie et al, 2005).2.4 Commute Time DistanceThe algorithm takes as input a symmetric similaritymatrix S, which we derive from the random walkmodel of the previous section as follows.
We com-pute the Euclidean Commute Time (ECT) distance(Saerens et al, 2004) of any two nodes of type Web-page in the graph.
The ECT distance is (also) basedon a random walk model, and presents the inter-esting property of decreasing when the number ofpaths connecting two nodes increases or when thelength of any path decreases, which makes it well-suited for clustering tasks.
Another nice propertyof ECT is that it is non-parametric, so no tuningis required here.
ECT has connections with princi-pal component analysis and spectral theory (Saerenset al, 2004).In particular, we are interested in the averagecommute time quantity, n(i, j), which is defined asthe average number of steps a random walker, start-ing in state i, will take before entering a given state jfor the first time, and go back to i.
That is, n(i, j) =m(j|i) + m(i|j), where the quantity m(j|i), calledthe average first-passage time, is defined as the av-erage number of steps a random walker, starting instate i, will take to enter state j for the first time.
Wecompute the average first-passage time iteratively bymeans of the following recurrence:{m(i|j) = 1 +?|V |k=1,k 6=i P(t)(k|j)m(i|k), j 6= im(i|i) = 0(3)where P (t)(?|?)
is the t-step transition probability ofthe random walk model over G presented in the pre-vious section.Informally, we may regard the random walkmodel presented in the previous section as a ?re-fined?
document similarity measure, replacing, e.g.,the typical TF-IDF measure with a measure thatworks in a similar way but over all features rep-resented in the graph, whereas we can regard theECTmeasure presented in this section as a ?booster?to a basic clustering techniques (cf.
next section),achieved by means of coupling clustering with a ran-dom walk-based distance which has been shown tobe competitive with state-of-the-art algorithms suchas spectral clustering (Luh Yen et al, 2007).2.5 ClusteringClustering aims at partitioning n given data pointsinto k clusters, such that points within a cluster aremore similar to each other than ones taken from dif-ferent clusters.
An important feature of the clus-tering algorithm that we require for the problem athand is its ability to determine the number k of nat-ural clusters, since any number of referents may bepresent in the web search results.
However, mostclustering algorithms require this number to be aninput, which means that they may break up or com-bine natural clusters, or even create clusters when nonatural ones exist in the data.We use a form of group-average agglomerativeclustering as described in (Fleischman and Hovy,2004), shown in Table 1, which works fast for thisproblem.
A difficult problem (with any clusteringapproach) has to do with the number of initial clus-ters or, alternatively, with setting a threshold forwhen to stop clustering.
This threshold could po-482Input: symmetric similarity matrix S, threshold ?Output: a set of clusters C1.
(i, j)?
find min score in S2.
if Sij > ?
then exit3.
place i and j in the same cluster in C (mergingexisting clusters of i and j if needed)4.
(average pairs of edges connecting to nodes i,jfrom any node k)4a.
Sik ?
(Sik + Sjk)/2, k 6= i, j4b.
Ski ?
(Ski + Skj)/2, k 6= i, j5.
remove j-th column and j-th line from S (effec-tively merging nodes i,j into a single node)6. goto 17. return clusters CTable 1: The simple group-average agglomerativeclustering algorithm usedtentially also be optimized using the training data;however, we have opted for unsupervised heuristicsto do that, e.g.
the well-known Calinski&Harabaszstopping rule (Calinski&Harabasz, 1974).3 Results ObtainedThe results obtained by the system are presented inthe following table.
The evaluation measures usedwere f-measure, purity and inverse purity - for a de-tailed description refer to the task description (Ar-tiles et al, 2007).aver f05 aver f02 aver pur aver inv pur0,49 0,66 0,36 0,93The results are below average for this Semevaltask, and should not be regarded as representativeof the approach adopted, since the authors have hadlimited time available to ensure a pristine implemen-tation of the whole approach.ReferencesArtiles, J., Gonzalo, J., & Sekine, S. (2007).
TheSemEval-2007 WePS Evaluation: Establishing abenchmark for the Web People Search Task.
In Pro-ceedings of Semeval 2007, Association for Computa-tional Linguistics.Calinski and Harabasz (1974).
A Dendrite Method forCluster Analysis Communications in Statistics, 3(1),1974, 1-27.Erkan, G. (2006).
Language model-based documentclustering using random walks.
Proceedings of themain conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics (pp.
479?486).Association for Computational Linguistics.Fleischman, M. B., & Hovy, E. (2004).
Multi-documentperson name resolution.
Proceedings of the ACL 2004.Association for Computational Linguistics.Guha, R. V., & Garg, A.
(2003).
Disambiguating Peoplein Search.
TAP: Building the Semantic Web.. ACMPress.Luh Yen, Francois Fouss, C. D., Francq, P., & Saerens,M.
(2007).
Graph nodes clustering based on thecommute-time kernel.
To appear in the proceedings ofthe 11th Pacific-Asia Conference on Knowledge Dis-covery and Data Mining (PAKDD 2007).
LectureNotes in Computer Science (LNCS).Minkov, E., Cohen, W. W., & Ng, A. Y.
(2006).
Con-textual search and name disambiguation in email us-ing graphs.
SIGIR ?06: Proceedings of the 29thannual international ACM SIGIR conference on Re-search and development in information retrieval (pp.27?34).
ACM Press.Nie, Z., Zhang, Y., Wen, J. R., & Ma, W. Y.
(2005).Object-level ranking: Bringing order to web objects.Proceedings of WWW?05.Page, L., Brin, S., Motwani, R., & Winograd, T. (1998).The pagerank citation ranking: Bringing order tothe web (Technical Report).
Stanford Digital LibraryTechnologies Project.Saerens, M., Fouss, F., Yen, L., & Dupont, P. (2004).
Theprincipal components analysis of a graph, and its re-lationships to spectral clustering.
Proceedings of the15th European Conference on Machine Learning.La?szlo?
Lova?sz (1993).
RandomWalks on Graphs: A Sur-vey.
Combinatorics, Paul Erdos is Eighty (Volume 2),Keszthely (Hungary), 1993, p 1-46..483
