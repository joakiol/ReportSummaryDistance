A Hybrid Approach for the Acquisition ofInformation Extraction PatternsMihai Surdeanu, Jordi Turmo, and Alicia AgenoTechnical University of CatalunyaBarcelona, Spain{surdeanu,turmo,ageno}@lsi.upc.eduAbstractIn this paper we present a hybrid ap-proach for the acquisition of syntactico-semantic patterns from raw text.
Ourapproach co-trains a decision list learnerwhose feature space covers the set of allsyntactico-semantic patterns with an Ex-pectation Maximization clustering algo-rithm that uses the text words as attributes.We show that the combination of the twomethods always outperforms the decisionlist learner alone.
Furthermore, using amodular architecture we investigate sev-eral algorithms for pattern ranking, themost important component of the decisionlist learner.1 IntroductionTraditionally, Information Extraction (IE) identi-fies domain-specific events, entities, and relationsamong entities and/or events with the goals of:populating relational databases, providing event-level indexing in news stories, feeding link discov-ery applications, etcetera.By and large the identification and selective ex-traction of relevant information is built around aset of domain-specific linguistic patterns.
For ex-ample, for a ?financial market change?
domainone relevant pattern is <NOUN fall MONEYto MONEY>.
When this pattern is matched onthe text ?London gold fell $4.70 to $308.35?, achange of $4.70 is detected for the financial in-strument ?London gold?.Domain-specific patterns are either hand-crafted or acquired automatically (Riloff, 1996;Yangarber et al, 2000; Yangarber, 2003; Steven-son and Greenwood, 2005).
To minimize annota-tion costs, some of the latter approaches use lightlysupervised bootstrapping algorithms that requireas input only a small set of documents annotatedwith their corresponding category label.
The focusof this paper is to improve such lightly supervisedpattern acquisition methods.
Moreover, we focuson robust bootstrapping algorithms that can han-dle real-world document collections, which con-tain many domains.Although a rich literature covers bootstrap-ping methods applied to natural language prob-lems (Yarowsky, 1995; Riloff, 1996; Collins andSinger, 1999; Yangarber et al, 2000; Yangar-ber, 2003; Abney, 2004) several questions remainunanswered when these methods are applied tosyntactic or semantic pattern acquisition.
In thispaper we answer two of these questions:(1) Can pattern acquisition be improved withtext categorization techniques?Bootstrapping-based pattern acquisition algo-rithms can also be regarded as incremental textcategorization (TC), since in each iteration docu-ments containing certain patterns are assigned thecorresponding category label.
Although TC is ob-viously not the main goal of pattern acquisitionmethodologies, it is nevertheless an integral part ofthe learning algorithm: each iteration of the acqui-sition algorithm depends on the previous assign-ments of category labels to documents.
Hence, ifthe quality of the TC solution proposed is bad, thequality of the acquired patterns will suffer.Motivated by this observation, we introduce aco-training-based algorithm (Blum and Mitchell,1998) that uses a text categorization algorithm asreinforcement for pattern acquisition.
We show,using both a direct and an indirect evaluation, thatthe combination of the two methodologies alwaysimproves the quality of the acquired patterns.48(2) Which pattern selection strategy is best?While most bootstrapping-based algorithms fol-low the same framework, they vary significantlyin what they consider the most relevant patterns ineach bootstrapping iteration.
Several approacheshave been proposed in the context of word sensedisambiguation (Yarowsky, 1995), named entity(NE) classification (Collins and Singer, 1999),pattern acquisition for IE (Riloff, 1996; Yangarber,2003), or dimensionality reduction for text catego-rization (TC) (Yang and Pedersen, 1997).
How-ever, it is not clear which selection approach isthe best for the acquisition of syntactico-semanticpatterns.
To answer this question, we have im-plemented a modular pattern acquisition architec-ture where several of these ranking strategies areimplemented and evaluated.
The empirical studypresented in this paper shows that a strategy previ-ously proposed for feature ranking for NE recogni-tion outperforms algorithms designed specificallyfor pattern acquisition.The paper is organized as follows: Sec-tion 2 introduces the bootstrapping frameworkused throughout the paper.
Section 3 introducesthe data collections.
Section 4 describes the di-rect and indirect evaluation procedures.
Section 5introduces a detailed empirical evaluation of theproposed system.
Section 6 concludes the paper.2 The Pattern Acquisition FrameworkIn this section we introduce a modular pattern ac-quisition framework that co-trains two differentviews of the document collection: the first viewuses the collection words to train a text categoriza-tion algorithm, while the second view bootstrapsa decision list learner that uses all syntactico-semantic patterns as features.
The rules acquiredby the latter algorithm, of the form p ?
y, wherep is a pattern and y is a domain label, are the out-put of the overall system.
The system can be cus-tomized with several pattern selection strategiesthat dramatically influence the quality and orderof the acquired rules.2.1 Co-training Text Categorization andPattern AcquisitionGiven two views of a classification task, co-training (Blum and Mitchell, 1998) bootstraps aseparate classifier for each view as follows: (1)it initializes both classifiers with the same smallamount of labeled data (i.e.
seed documents in ourcase); (2) it repeatedly trains both classifiers us-ing the currently labeled data; and (3) after eachlearning iteration, the two classifiers share all or asubset of the newly labeled examples (documentsin our particular case).The intuition is that each classifier providesnew, informative labeled data to the other classi-fier.
If the two views are conditional independentand the two classifiers generally agree on unla-beled data they will have low generalization error.In this paper we focus on a ?naive?
co-training ap-proach, which trains a different classifier in eachiteration and feeds its newly labeled examples tothe other classifier.
This approach was shown toperform well on real-world natural language prob-lems (Collins and Singer, 1999).Figure 1 illustrates the co-training frameworkused in this paper.
The feature space of thefirst view contains only lexical information, i.e.the collection words, and uses as classifier Ex-pectation Maximization (EM) (Dempster et al,1977).
EM is actually a class of iterative algo-rithms that find maximum likelihood estimates ofparameters using probabilistic models over incom-plete data (e.g.
both labeled and unlabeled docu-ments) (Dempster et al, 1977).
EM was theoret-ically proven to converge to a local maximum ofthe parameters?
log likelihood.
Furthermore, em-pirical experiments showed that EM has excellentperformance for lightly-supervised text classifica-tion (Nigam et al, 2000).
The EM algorithm usedin this paper estimates its model parameters us-ing the Naive Bayes (NB) assumptions, similarlyto (Nigam et al, 2000).
From this point further,we refer to this instance of the EM algorithm asNB-EM.The feature space of the second view containsthe syntactico-semantic patterns, generated usingthe procedure detailed in Section 3.2.
The secondlearner is the actual pattern acquisition algorithmimplemented as a bootstrapped decision list clas-sifier.The co-training algorithm introduced in this pa-per interleaves one iteration of the NB-EM algo-rithm with one iteration of the pattern acquisitionalgorithm.
If one classifier converges faster (e.g.NB-EM typically converges in under 20 iterations,whereas the acquisition algorithms learns new pat-terns for hundreds of iterations) we continue boot-strapping the other classifier alone.2.2 The Text Categorization AlgorithmThe parameters of the generative NB model, ?
?, in-clude the probability of seeing a given category,49patternInitializeacquisitionLabeled seed documentsUnlabeled documents IterationNB?EM PatternacquisitioniterationPatternacquisitionterminated?NB?EMconverged?RankingmethodInitializeNB?EMNoYesPatternsYesNoFigure 1: Co-training framework for pattern acquisition.1.
Initialization:?
Initialize the set of labeled examples with n la-beled seed documents of the form (di, yi).
yi isthe label of the ith document di.
Each docu-ment di contains a set of patterns {pi1, pi2, ..., pim}.?
Initialize the list of learned rules R = {}.2.
Loop:?
For each label y, select a small set of patternrules r = p ?
y, r /?
R.?
Append all selected rules r to R.?
For all non-seed documents d that contain apattern in R, set label(d) = argmaxp,y strength(p, y).3.
Termination condition:?
Stop if no rules selected or maximum numberof iterations reached.Figure 2: Pattern acquisition meta algorithmP (c|??
), and the probability of seeing a word givena category, P (w|c; ??).
We calculate both simi-larly to Nigam (2000).
Using these parameters,the word independence assumption typical to theNaive Bayes model, and the Bayes rule, the prob-ability that a document d has a given category c iscalculated as:P (c|d; ??)
= P (c|??
)P (d|c; ??
)P (d|??
)(1)= P (c|??
)?|d|i=1P (wi|c; ??
)?qj=1 P (cj |??
)?|d|i=1P (wi|cj ; ??
)(2)2.3 The Pattern Acquisition AlgorithmThe lightly-supervised pattern acquisition algo-rithm iteratively learns domain-specific IE pat-terns from a small set of labeled documents anda much larger set of unlabeled documents.
Dur-ing each learning iteration, the algorithm acquiresa new set of patterns and labels more documentsbased on the new evidence.
The algorithm outputis a list R of rules p ?
y, where p is a patternin the set of patterns P , and y a category label inY = {1...k}, k being the number of categories inthe document collection.
The list of acquired rulesR is sorted in descending order of rule importanceto guarantee that the most relevant rules are ac-cessed first.
This generic bootstrapping algorithmis formalized in Figure 2.Previous studies called the class of algorithmsillustrated in Figure 2 ?cautious?
or ?sequential?because in each iteration they acquire 1 or a smallset of rules (Abney, 2004; Collins and Singer,1999).
This strategy stops the algorithm from be-ing over-confident, an important restriction for analgorithm that learns from large amounts of unla-beled data.
This approach was empirically shownto perform better than a method that in each itera-tion acquires all rules that match a certain criterion(e.g.
the corresponding rule has a strength over acertain threshold).The key element where most instances of thisalgorithm vary is the select procedure, which de-cides which rules are acquired in each iteration.Although several selection strategies have beenpreviously proposed for various NLP problems, toour knowledge no existing study performs an em-pirical analysis of such strategies in the context ofacquisition of IE patterns.
For this reason, we im-plement several selection methods in our system(described in Section 2.4) and evaluate their per-formance in Section 5.The label of each collection document is givenby the strength of its patterns.
Similarly to (Collinsand Singer, 1999; Yarowsky, 1995), we define thestrength of a pattern p in a category y as the pre-cision of p in the set of documents labeled withcategory y, estimated using Laplace smoothing:strength(p, y) = count(p, y) + count(p) + k (3)where count(p, y) is the number of documents la-beled y containing pattern p, count(p) is the over-all number of labeled documents containing p, andk is the number of domains.
For all experimentspresented here we used  = 1.Another point where acquisition algorithms dif-fer is the initialization procedure: some start with asmall number of hand-labeled documents (Riloff,1996), as illustrated in Figure 2, while others startwith a set of seed rules (Yangarber et al, 2000;Yangarber, 2003).
However, these approaches areconceptually similar: the seed rules are simplyused to generate the seed documents.This paper focuses on the framework introducedin Figure 2 for two reasons: (a) ?cautious?
al-50gorithms were shown to perform best for severalNLP problems (including acquisition of IE pat-terns), and (b) it has nice theoretical properties:Abney (2004) showed that, regardless of the selec-tion procedure, ?sequential?
bootstrapping algo-rithms converge to a local minimum of K, whereK is an upper bound of the negative log likelihoodof the data.
Obviously, the quality of the localminimum discovered is highly dependent of theselection procedure, which is why we believe anevaluation of several pattern selection strategies isimportant.2.4 Selection CriteriaThe pattern selection component, i.e.
the selectprocedure of the algorithm in Figure 2, consists ofthe following: (a) for each category y all patternsp are sorted in descending order of their scores inthe current category, score(p, y), and (b) for eachcategory the top k patterns are selected.
For allexperiments in this paper we have used k = 3.We provide four different implementations for thepattern scoring function score(p, y) according tofour different selection criteria.Criterion 1: RiloffThis selection criterion was developed specificallyfor the pattern acquisition task (Riloff, 1996) andhas been used in several other pattern acquisitionsystems (Yangarber et al, 2000; Yangarber, 2003;Stevenson and Greenwood, 2005).
The intuitionbehind it is that a qualitative pattern is yielded by acompromise between pattern precision (which is agood indicator of relevance) and pattern frequency(which is a good indicator of coverage).
Further-more, the criterion considers only patterns that arepositively correlated with the corresponding cate-gory, i.e.
their precision is higher than 50%.
TheRiloff score of a pattern p in a category y is for-malized as:score(p, y) ={prec(p, y) log(count(p, y)),if prec(p, y) > 0.5;0, otherwise.
(4)prec(p, y) = count(p, y)count(p) (5)where prec(p, y) is the raw precision of pattern pin the set of documents labeled with category y.Criterion 2: CollinsThis criterion was used in a lightly-supervised NErecognizer (Collins and Singer, 1999).
Unlike theprevious criterion, which combines relevance andfrequency in the same scoring function, Collinsconsiders only patterns whose raw precision isover a hard threshold T and ranks them by theirglobal coverage:score(p, y) ={count(p), if prec(p, y) > T ;0, otherwise.
(6)Similarly to (Collins and Singer, 1999) we usedT = 0.95 for all experiments reported here.Criterion 3: ?2 (Chi)The ?2 score measures the lack of independencebetween a pattern p and a category y.
It is com-puted using a two-way contingency table of p andy, where a is the number of times p and y co-occur,b is the number of times p occurs without y, c isthe number of times y occurs without p, and d isthe number of times neither p nor y occur.
Thenumber of documents in the collection is n. Sim-ilarly to the first criterion, we consider only pat-terns positively correlated with the correspondingcategory:score(p, y) ={?2(p, y), if prec(p, y) > 0.5;0, otherwise.
(7)?2(p, y) = n(ad?
cb)2(a + c)(b + d)(a + b)(c + d) (8)The ?2 statistic was previously reported to bethe best feature selection strategy for text catego-rization (Yang and Pedersen, 1997).Criterion 4: Mutual Information (MI)Mutual information is a well known informationtheory criterion that measures the independence oftwo variables, in our case a pattern p and a cate-gory y (Yang and Pedersen, 1997).
Using the samecontingency table introduced above, the MI crite-rion is estimated as:score(p, y) ={MI(p, y), if prec(p, y) > 0.5;0, otherwise.
(9)MI(p, y) = log P (p ?
y)P (p)?
P (y) (10)?
log na(a + c)(a + b) (11)3 The Data3.1 The Document CollectionsFor all experiments reported in this paper we usedthe following three document collections: (a) theAP collection is the Associated Press (year 1999)subset of the AQUAINT collection (LDC catalognumber LDC2002T31); (b) the LATIMES collec-tion is the Los Angeles Times subset of the TREC-5 collection1; and (c) the REUTERS collection isthe by now classic Reuters-21578 text categoriza-tion collection2.1http://trec.nist.gov/data/docs eng.html2http://trec.nist.gov/data/reuters/reuters.html51Collection # of docs # of categories # of words # of patternsAP 5000 7 24812 140852LATIMES 5000 8 29659 69429REUTERS 9035 10 12905 36608Table 1: Document collections used in the evaluationSimilarly to previous work, for the REUTERScollection we used the ModApte split and selectedthe ten most frequent categories (Nigam et al,2000).
Due to memory limitations on our test ma-chines, we reduced the size of the AP and LA-TIMES collections to their first 5,000 documents(the complete collections contain over 100,000documents).The collection words were pre-processed as fol-lows: (i) stop words and numbers were discarded;(ii) all words were converted to lower case; and(iii) terms that appear in a single document wereremoved.
Table 1 lists the collection characteris-tics after pre-processing.3.2 Pattern GenerationIn order to extract the set of patterns available ina document, each collection document undergoesthe following processing steps: (a) we recognizeand classify named entities3, and (b) we generatefull parse trees of all document sentences using aprobabilistic context-free parser.Following the above processing steps, we ex-tract Subject-Verb-Object (SVO) tuples using a se-ries of heuristics, e.g.
: (a) nouns preceding activeverbs are subjects, (b) nouns directly attached to averb phrase are objects, (c) nouns attached to theverb phrase through a prepositional attachment areindirect objects.
Each tuple element is replacedwith either its head word, if its head word is notincluded in a NE, or with the NE category oth-erwise.
For indirect objects we additionally storethe accompanying preposition.
Lastly, each tuplecontaining more than two elements is generalizedby maintaining only subsets of two and three of itselements and replacing the others with a wildcard.Table 2 lists the patterns extracted from onesample sentence.
As Table 2 hints, the systemgenerates a large number of candidate patterns.
Itis the task of the pattern acquisition algorithm toextract only the relevant ones from this complexsearch space.4 The Evaluation Procedures4.1 The Indirect Evaluation ProcedureThe goal of our evaluation procedure is to measurethe quality of the acquired patterns.
Intuitively,3We identify six categories: persons, locations, organiza-tions, other names, temporal and numerical expressions.Text The Minnesota Vikings beat the ArizonaCardinals in yesterday?s game.Patterns s(ORG) v(beat)v(beat) o(ORG)s(ORG) o(ORG)v(beat) io(in game)s(ORG) io(in game)o(ORG) io(in game)s(ORG) v(beat) o(ORG)s(ORG) v(beat) io(in game)v(beat) o(ORG) io(in game)Table 2: Patterns extracted from one sample sentence.
sstands for subject, v for verb, o for object, and io for indirectobject.the learned patterns should have high coverage andlow ambiguity.
We indirectly measure the qualityof the acquired patterns using a text categorizationstrategy: we feed the acquired rules to a decision-list classifier, which is then used to classify a newset of documents.
The classifier assigns to eachdocument the category label given by the first rulewhose pattern matches.
Since we expect higher-quality patterns to appear higher in the rule list,the decision-list classifier never changes the cate-gory of an already-labeled document.The quality of the generated classification ismeasured using micro-averaged precision and re-call:P =?qi=1 TruePositivesi?qi=1(TruePositivesi + FalsePositivesi)(12)R =?qi=1 TruePositivesi?qi=1(TruePositivesi + FalseNegativesi)(13)where q is the number of categories in the docu-ment collection.For all experiments and all collections with theexception of REUTERS, which has a standarddocument split for training and testing, we used 5-fold cross validation: we randomly partitioned thecollections into 5 sets of equal sizes, and reserveda different one for testing in each fold.We have chosen this evaluation strategy becausethis indirect approach was shown to correlate wellwith a direct evaluation, where the learned patternswere used to customize an IE system (Yangarberet al, 2000).
For this reason, much of the fol-lowing work on pattern acquisition has used thisapproach as a de facto evaluation standard (Yan-garber, 2003; Stevenson and Greenwood, 2005).Furthermore, given the high number of domainsand patterns (we evaluate on 25 domains), an eval-uation by human experts is extremely costly.
Nev-ertheless, to show that the proposed indirect eval-uation correlates well with a direct evaluation, twohuman experts have evaluated the patterns in sev-eral domains.
The direct evaluation procedure isdescribed next.524.2 The Direct Evaluation ProcedureThe task of manually deciding whether an ac-quired pattern is relevant or not for a given domainis not trivial, mainly due to the ambiguity of thepatterns.
Thus, this process should be carried outby more than one expert, so that the relevance ofthe ambiguous patterns can be agreed upon.
Forexample, the patterns s(ORG) v(score) o(goal) ands(PER) v(lead) io(with point) are clearly relevantonly for the sports domain, whereas the patternsv(sign) io(as agent) and o(title) io(in DATE) mightbe regarded as relevant for other domains as well.The specific procedure to manually evaluate thepatterns is the following: (1) two experts sepa-rately evaluate the acquired patterns for the con-sidered domains and collections; and (2) the re-sults of both evaluations are compared.
For anydisagreement, we have opted for a strict evalua-tion: all the occurrences of the corresponding pat-tern are looked up in the collection and, wheneverat least one pattern occurrence belongs to a docu-ment assigned to a different domain than the do-main in question, the pattern will be considered asnot relevant.Both the ambiguity and the high number ofthe extracted patterns have prevented us from per-forming an exhaustive direct evaluation.
For thisreason, only the top (most relevant) 100 patternshave been evaluated for one domain per collection.The results are detailed in Section 5.2.5 Experimental Evaluation5.1 Indirect EvaluationFor a better understanding of the proposed ap-proach we perform an incremental evaluation:first, we evaluate only the various pattern selectioncriteria described in Section 2.4 by disabling theNB-EM component.
Second, using the best selec-tion criteria, we evaluate the complete co-trainingsystem.In both experiments we initialize the systemwith high-precision manually-selected seed ruleswhich yield seed documents with a coverage of10% of the training partitions.
The remaining 90%of the training documents are maintained unla-beled.
For all experiments we used a maximum of400 bootstrapping iterations.
The acquired rulesare fed to the decision list classifier which assignscategory labels to the documents in the test parti-tions.Evaluation of the pattern selection criteriaFigure 3 illustrates the precision/recall chartsof the four algorithms as the number of patternsmade available to the decision list classifier in-creases.
All charts show precision/recall pointsstarting after 100 learning iterations with 100-iteration increments.
It is immediately obviousthat the Collins selection criterion performs sig-nificantly better than the other three criteria.
Forthe same recall point, Collins yields a classifica-tion model with much higher precision, with dif-ferences ranging from 5% in the REUTERS col-lection to 20% in the AP collection.Theorem 5 in (Abney, 2002) provides a theo-retical explanation for these results: if certain in-dependence conditions between the classifier rulesare satisfied and the precision of each rule is largerthan a threshold T , then the precision of the finalclassifier is larger than T .
Although the rule inde-pendence conditions are certainly not satisfied inour real-world evaluation, the above theorem in-dicates that there is a strong relation between theprecision of the classifier rules on labeled data andthe precision of the final classifier.
Our results pro-vide the empirical proof that controling the preci-sion of the acquired rules (i.e.
the Collins crite-rion) is important.The Collins criterion controls the recall of thelearned model by favoring rules with high fre-quency in the collection.
However, since the othertwo criteria do not use a high precision thresh-old, they will acquire more rules, which translatesin better recall.
For two out of the three collec-tions, Riloff and Chi obtain a slightly better recall,about 2% higher than Collins?, albeit with a muchlower precision.
We do not consider this an im-portant advantage: in the next section we showthat co-training with the NB-EM component fur-ther boosts the precision and recall of the Collins-based acquisition algorithm.The MI criterion performs the worst of the fourevaluated criteria.
A clue for this behavior lies inthe following equivalent form for MI: MI(p, y) =logP (p|y)?logP (p).
This formula indicates that,for patterns with equal conditional probabilitiesP (p|y), MI assigns higher scores to patterns withlower frequency.
This is not the desired behaviorin a TC-oriented system.Evaluation of the co-training systemFigure 4 compares the performance of thestand-alone pattern acquisition algorithm (?boot-strapping?)
with the performance of the acquisi-tion algorithm trained in the co-training environ-530.30.350.40.450.50.550.60.650.70.750.80.850.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55PrecisionRecallcollinsriloffchimi(a)0.250.30.350.40.450.50.550.60.650.70.750.1  0.15  0.2  0.25  0.3  0.35  0.4PrecisionRecallcollinsriloffchimi(b)0.650.70.750.80.850.90.950.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45PrecisionRecallcollinsriloffchimi(c)Figure 3: Performance of the pattern acquisition algorithm for various pattern selection strategies and multiple collections:(a) AP, (b) LATIMES, and (c) REUTERSment (?co-training?).
For both setups we used thebest pattern selection criterion for pattern acqui-sition, i.e.
the Collins criterion.
To put things inperspective, we also depict the performance ob-tained with a baseline system, i.e.
the system con-figured to use the Riloff pattern selection criterionand without the NB-EM algorithm (?baseline?
).To our knowledge, this system, or a variation ofit, is the current state-of-the-art in pattern acqui-sition (Riloff, 1996; Yangarber et al, 2000; Yan-garber, 2003; Stevenson and Greenwood, 2005).All algorithms were initialized with the same seedrules and had access to all documents.Figure 4 shows that the quality of the learnedpatterns always improves if the pattern acquisi-tion algorithm is ?reinforced?
with EM.
For thesame recall point, the patterns acquired in theco-training environment yield classification mod-els with precision (generally) much larger thanthe models generated by the pattern acquisitionalgorithm alone.
When using the same pat-tern acquisition criterion, e.g.
Collins, the dif-ferences between the co-training approach andthe stand-alone pattern acquisition method (?boot-strapping?)
range from 2-3% in the REUTERScollection to 20% in the LATIMES collection.These results support our intuition that the sparsepattern space is insufficient to generate good clas-sification models, which directly influences thequality of all acquired patterns.Furthermore, due to the increased coverage ofthe lexicalized collection views, the patterns ac-quired in the co-training setup generally have bet-ter recall, up to 11% higher in the LATIMES col-lection.Lastly, the comparison of our best system (?co-training?)
against the current state-of-the-art (our?baseline?)
draws an even more dramatic picture:Collection Domain Relevant Relevant Initialpatterns patterns inter-expertbaseline co-training agreementAP Sports 22% 68% 84%LATIMES Financial 67% 76% 70%REUTERS Corporate 38% 46% 66%AcquisitionsTable 3: Percentage of relevant patterns for one domain percollection by the baseline system (Riloff) and the co-trainingsystem.for the same recall point, the co-training systemobtains a precision up to 35% higher for AP andLATIMES, and up to 10% higher for REUTERS.5.2 Direct EvaluationAs stated in Section 4.2, two experts have man-ually evaluated the top 100 acquired patterns forone different domain in each of the three collec-tions.
The three corresponding domains have beenselected intending to deal with different degrees ofambiguity, which are reflected in the initial inter-expert agreement.
Any disagreement between ex-perts is solved using the algorithm introduced inSection 4.2.
Table 3 shows the results of this di-rect evaluation.
The co-training approach outper-forms the baseline for all three collections.
Con-cretely, improvements of 9% and 8% are achievedfor the Financial and the Corporate Acquisitionsdomains, and 46%, by far the largest difference, isfound for the Sports domain in AP.
Table 4 liststhe top 20 patterns extracted by both approachesin the latter domain.
It can be observed that forthe baseline, only the top 4 patterns are relevant,the rest being extremely general patterns.
On theother hand, the quality of the patterns acquired byour approach is much higher: all the patterns arerelevant to the domain, although 7 out of the 20might be considered ambiguous and according tothe criterion defined in Section 4.2 have been eval-uated as not relevant.540.450.50.550.60.650.70.750.80.850.90.3  0.35  0.4  0.45  0.5  0.55  0.6PrecisionRecallco-trainingbootstrappingbaseline(a)0.40.450.50.550.60.650.70.750.80.850.2  0.25  0.3  0.35  0.4  0.45PrecisionRecallco-trainingbootstrappingbaseline(b)0.70.750.80.850.90.950.15  0.2  0.25  0.3  0.35  0.4  0.45PrecisionRecallco-trainingbootstrappingbaseline(c)Figure 4: Comparison of the bootstrapping pattern acquisition algorithm with the co-training approach: (a) AP, (b) LATIMES,and (c) REUTERSBaseline Co-trainings(he) o(game) v(win) o(title)v(miss) o(game) s(I) v(play)v(play) o(game) s(he) v(game)v(play) io(in LOC) s(we) v(play)v(go) o(be) v(miss) o(game)s(he) v(be) s(he) v(coach)s(that) v(be) v(lose) o(game)s(I) v(be) s(I) o(play)s(it) v(go) o(be) v(make) o(play)s(it) v(be) v(play) io(in game)s(I) v(think) v(want) o(play)s(I) v(know) v(win) o(MISC)s(I) v(want) s(he) o(player)s(there) v(be) v(start) o(game)s(we) v(do) s(PER) o(contract)v(do) o(it) s(we) o(play)s(it) o(be) s(team) v(win)s(we) v(are) v(rush) io(for yard)s(we) v(go) s(we) o(team)s(PER) o(DATE) v(win) o(Bowl)Table 4: Top 20 patterns acquired from the Sports domainby the baseline system (Riloff) and the co-training system forthe AP collection.
The correct patterns are in bold.6 ConclusionsThis paper introduces a hybrid, lightly-supervisedmethod for the acquisition of syntactico-semanticpatterns for Information Extraction.
Our approachco-trains a decision list learner whose featurespace covers the set of all syntactico-semanticpatterns with an Expectation Maximization clus-tering algorithm that uses the text words as at-tributes.
Furthermore, we customize the decisionlist learner with up to four criteria for pattern se-lection, which is the most important component ofthe acquisition algorithm.For the evaluation of the proposed approach wehave used both an indirect evaluation based onText Categorization and a direct evaluation wherehuman experts evaluated the quality of the gener-ated patterns.
Our results indicate that co-trainingthe Expectation Maximization algorithm with thedecision list learner tailored to acquire only highprecision patterns is by far the best solution.
Forthe same recall point, the proposed method in-creases the precision of the generated models upto 35% from the previous state of the art.
Further-more, the combination of the two feature spaces(words and patterns) also increases the coverageof the acquired patterns.
The direct evaluation ofthe acquired patterns by the human experts vali-dates these results.ReferencesS.
Abney.
2002.
Bootstrapping.
In Proceedings of the 40thAnnual Meeting of the Association for Computational Lin-guistics.S.
Abney.
2004.
Understanding the Yarowsky algorithm.Computational Linguistics, 30(3).A.
Blum and T. Mitchell.
1998.
Combining labeled and un-labeled data with co-training.
In Proceedings of the 11thAnnual Conference on Computational Learning Theory.M.
Collins and Y.
Singer.
1999.
Unsupervised models fornamed entity classification.
In Proceedings of EMNLP.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.
Max-imum likelihood from incomplete data via the EM algo-rithm.
Journal of the Royal Statistical Society, Series B,39(1).K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000.Text classification from labeled and unlabeled documentsusing EM.
Machine Learning, 39(2/3).E.
Riloff.
1996.
Automatically generating extraction patternsfrom untagged text.
In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence (AAAI-96).M.
Stevenson and M. Greenwood.
2005.
A semantic ap-proach to ie pattern induction.
In Proceedings of the 43rdMeeting of the Association for Computational Linguistics.Y.
Yang and J. O. Pedersen.
1997.
A comparative studyon feature selection in text categorization.
In Proceed-ings of the Fourteenth International Conference on Ma-chine Learning.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hutunen.2000.
Automatic acquisition of domain knowledge for in-formation extraction.
In Proceedings of the 18th Interna-tional Conference of Computational Linguistics (COLING2000).R.
Yangarber.
2003.
Counter-training in discovery of se-mantic patterns.
In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics (ACL2003).D.
Yarowsky.
1995.
Unsupervised word sense disambigua-tion rivaling supervised methods.
In Proceedings of the33rd Annual Meeting of the Association for Computa-tional Linguistics.55
