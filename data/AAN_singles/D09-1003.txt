Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21?29,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPSemi-supervised Semantic Role LabelingUsing the Latent Words Language ModelKoen DeschachtDepartment of computer scienceK.U.Leuven, Belgiumkoen.deshahts.kuleuven.beMarie-Francine MoensDepartment of computer scienceK.U.Leuven, Belgiumsien.moenss.kuleuven.beAbstractSemantic Role Labeling (SRL) has provedto be a valuable tool for performing auto-matic analysis of natural language texts.Currently however, most systems rely ona large training set, which is manually an-notated, an effort that needs to be repeatedwhenever different languages or a differ-ent set of semantic roles is used in a cer-tain application.
A possible solution forthis problem is semi-supervised learning,where a small set of training examplesis automatically expanded using unlabeledtexts.
We present the Latent Words Lan-guage Model, which is a language modelthat learns word similarities from unla-beled texts.
We use these similarities fordifferent semi-supervised SRL methods asadditional features or to automatically ex-pand a small training set.
We evaluate themethods on the PropBank dataset and findthat for small training sizes our best per-forming system achieves an error reduc-tion of 33.27% F1-measure compared toa state-of-the-art supervised baseline.1 IntroductionAutomatic analysis of natural language is still avery hard task to perform for a computer.
Al-though some successful applications have been de-veloped (see for instance (Chinchor, 1998)), im-plementing an automatic text analysis system isstill a labour and time intensive task.
Many ap-plications would benefit from an intermediate rep-resentation of texts, where an automatic analysisis already performed which is sufficiently generalto be useful in a wide range of applications.Syntactic analysis of texts (such as Part-Of-Speech tagging and syntactic parsing) is an ex-ample of such a generic analysis, and has proveduseful in applications ranging from machine trans-lation (Marcu et al, 2006) to text mining in thebio-medical domain (Cohen and Hersh, 2005).
Asyntactic parse is however a representation that isvery closely tied with the surface-form of naturallanguage, in contrast to Semantic Role Labeling(SRL) which adds a layer of predicate-argumentinformation that generalizes across different syn-tactic alternations (Palmer et al, 2005).
SRL hasreceived a lot of attention in the research commu-nity, and many systems have been developed (seesection 2).
Most of these systems rely on a largedataset for training that is manually annotated.
Inthis paper we investigate whether we can develop asystem that achieves state-of-the-art semantic rolelabeling without relying on a large number of la-beled examples.
We aim to do so by employing theLatent Words Language Model that learns latentwords from a large unlabeled corpus.
Latent wordsare words that (unlike observed words) did not oc-cur at a particular position in a text, but given se-mantic and syntactic constraints from the contextcould have occurred at that particular position.In section 2 we revise existing work on SRL andon semi-supervised learning.
Section 3 outlinesour supervised classifier for SRL and section 4 dis-cusses the Latent Words Language Model.
In sec-tion 5 we will combine the two models for semi-supervised role labeling.
We will test the modelon the standard PropBank dataset and compare itwith state-of-the-art semi-supervised SRL systemsin section 6 and finally in section 7 we draw con-clusions and outline future work.2 Related workGildea and Jurafsky (2002) were the first to de-scribe a statistical system trained on the data fromthe FrameNet project to automatically assign se-mantic roles.
This approach was soon followedby other researchers (Surdeanu et al, 2003; Prad-han et al, 2004; Xue and Palmer, 2004), focus-21ing on improved sets of features, improved ma-chine learning methods or both, and SRL becamea shared task at the CoNLL 2004, 2005 and 2008conferences1 .
The best system (Johansson andNugues, 2008) in CoNLL 2008 achieved an F1-measure of 81.65% on the workshop?s evaluationcorpus.Semi-supervised learning has been suggestedby many researchers as a solution to the annota-tion bottleneck (see (Chapelle et al, 2006; Zhu,2005) for an overview), and has been applied suc-cessfully on a number of natural language pro-cessing tasks.
Mann and McCallum (2007) ap-ply Expectation Regularization to Named EntityRecognition and Part-Of-Speech tagging, achiev-ing improved performance when compared to su-pervised methods, especially on small numbers oftraining data.
Koo et al (2008) present an algo-rithm for dependency parsing that uses clusters ofsemantically related words, which were learnedin an unsupervised manner.
There has been lit-tle research on semi-supervised learning for SRL.We refer to He and Gildea (2006) who tested ac-tive learning and co-training methods, but foundlittle or no gain from semi-supervised learning,and to Swier and Stevenson (2004), who achievedgood results using semi-supervised methods, buttested their methods on a small number of Verb-Net roles, which have not been used by other SRLsystems.
To the best of our knowledge no sys-tem was able to reproduce the successful resultsof (Swier and Stevenson, 2004) on the PropBankroleset.
Our approach most closely resembles thework of F?rstenau and Lapata (2009) who auto-matically expand a small training set using an au-tomatic dependency alignment of unlabeled sen-tences.
This method was tested on the FrameNetcorpus and improved results when compared to afully-supervised classifier.
We will discuss theirmethod in detail in section 5.3 Semantic role labelingFillmore (1968) introduced semantic structurescalled semantic frames, describing abstract ac-tions or common situations (frames) with commonroles and themes (semantic roles).
Inspired by thisidea different resources were constructed, includ-ing FrameNet (Baker et al, 1998) and PropBank(Palmer et al, 2005).
An alternative approach tosemantic role labeling is the framework developed1See http://www.cnts.ua.ac.be/conll/ for an overview.by Halliday (1994) and implemented by Mehayet al (2005).
PropBank has thus far received themost attention of the research community, and isused in our work.3.1 PropBankThe goal of the PropBank project is to add seman-tic information to the syntactic nodes in the En-glish Penn Treebank.
The main motivation for thisannotation is the preservation of semantic rolesacross different syntactic realizations.
Take for in-stance the sentences1.
The window broke.2.
John broke the window.In both sentences the constituent ?the window?
isbroken, although it occurs at different syntacticpositions.
The PropBank project defines for alarge collection of verbs (excluding auxiliaryverbs such as ?will?, ?can?, ...) a set of senses,that reflect the different meanings and syntacticalternations of this verb.
Every sense has anumber of expected roles, numbered from Arg0to Arg5.
A small number of arguments are sharedamong all senses of all verbs, such as temporals(Arg-TMP), locatives (Arg-LOC) and directionals(Arg-DIR).
Additional to the frame definitions,PropBank has annotated a large training corpuscontaining approximately 113.000 annotatedverbs.
An example of an annotated sentence is[John Arg0][broke BREAK.01] [the window Arg1].Here BREAK.01 is the first sense of the ?break?verb.
Note that (1) although roles are defined forevery frame separately, in reality roles with iden-tical names are identical or very similar for allframes, a fact that is exploited to train accurate roleclassifiers and (2) semantic role labeling systemstypically assume that a frame is fully expressed ina single sentence and thus do not try to instanti-ate roles across sentence boundaries.
Although theoriginal PropBank corpus assigned semantic rolesto syntactic phrases (such as noun phrases), we usethe CoNLL dataset, where the PropBank corpuswas converted to a dependency representation, as-signing semantic roles to single (head) words.3.2 FeaturesIn this section we discuss the features used in thesemantic role labeling system.
All features but the22Split path feature are taken from existing seman-tic role labeling systems, see for example (Gildeaand Jurafsky, 2002; Lim et al, 2004; Thompsonet al, 2006).
The number in brackets denotes thenumber of unique features for that type.Word We split every sentence in (unigram) wordtokens, including punctuation.
(37079)Stem We reduce the word tokens to their stem,e.g.
?walks?
-> ?walk?.
(28690)POS The part-of-speech tag for every word, e.g.?NNP?
(for a singular proper noun).
(77)Neighbor POS?s The concatenated part-of-speech tags of the word before and the wordjust after the current word, e.g.
?RBS_JJR?.
(1787)Path This important feature describes the paththrough the dependency tree from the currentword to the position of the predicate, e.g.
?coord?obj?adv?root?dep?nmod?pmod?,where ???
indicates going up a constituentand ???
going down one constituent.
(829642)Split Path Because of the nature of the path fea-ture, an explosion of unique features is foundin a given data set.
We reduce this by split-ting the path in different parts and using everypart as a distinct feature.
We split, for exam-ple, the previous path in 6 different features:?coord?, ?
?obj?, ?
?adv?, ?
?root?, ??dep?,?
?nmod?, ??pmod?.
Note that the split pathfeature includes the POS feature, since thefirst component of the path is the POS tag forthe current word.
This feature has not beenused previously for semantic role detection.
(155)For every word wi in the training and test set weconstruct the feature vector f(wi), where at everyposition in this vector 1 indicates the presence forthe corresponding feature and 0 the absence of thatfeature.3.3 Discriminative modelDiscriminative models have been found to outper-form generative models for many different tasksincluding SRL (Lim et al, 2004).
For this reasonwe also employ discriminative models here.
Thestructure of the model was inspired by a similarFigure 1: Discriminative model for SRL.
Greycircles represent observed variables, white circleshidden variables and arrows directed dependen-cies.
s ranges over all sentences in the corpus andj over the n words in the sentence.
(although generative) model in (Thompson et al,2006) where it was used for semantic frame clas-sification.
The model (fig.
1) assumes that the rolelabel ri j for the word wi is conditioned on the fea-tures fi and on the role label ri?1 j of the previousword and that the predicate label p j for word w j isconditioned on the role labels R j and on the fea-tures f j.
This model can be seen as an extensionof the standard Maximum Entropy Markov Model(MEMM, see (Ratnaparkhi, 1996)) with an extradependency on the predicate label, we will hence-forth refer to this model as MEMM+pred.To estimate the parameters of the MEMM+predmodel we turn to the successful Maximum En-tropy (Berger et al, 1996) parameter estimationmethod.
The Maximum Entropy principle statesthat the best model given the training data is themodel such that the conditional distribution de-fined by the model has maximum entropy subjectto the constraints represented by the training ex-amples.
There is no closed form solution to findthis maximum and we thus turn to an iterativemethod.
In this work we use Generalized Itera-tive Scaling2, but other methods such as (quasi-)Newton optimization could also have been used.4 Latent Words Language Model4.1 RationaleAs discussed in sections 1 and 3 most SRL sys-tems are trained today on a large set of manuallyannotated examples.
PropBank for example con-tains approximately 50000 sentences.
This man-ual annotation is both time and labour-intensive,and needs to be repeated for new languages or2We use the maxent package available onhttp://maxent.sourceforge.net/23for new domains requiring a different set of roles.One approach that can help to solve this problemis semi-supervised learning, where a small set ofannotated examples is used together with a largeset of unlabeled examples when training a SRLmodel.Manual inspection of the results of the super-vised model discussed in the previous sectionshowed that the main source of errors was in-correct labeling of a word because the word to-ken did not occur, or occurred only a small num-ber of times in the training set.
We hypothesizethat knowledge of semantic similar words couldovercome this problem by associating words thatoccurred infrequently in the training set to sim-ilar words that occurred more frequently.
Fur-thermore, we would like to learn these similar-ities automatically, to be independent of knowl-edge sources that might not be available for alllanguages or domains.The Distributional Hypothesis, supported bytheoretical linguists such as Harris (1954), statesthat words that occur in the same contexts tendto have similar meanings.
This suggests that onecan learn the similarity between two words auto-matically by comparing their relative contexts ina large unlabeled corpus, which was confirmed bydifferent researchers (e.g.
(Lin, 1998; McDonaldand Ramscar, 2001; Grefenstette, 1994)).
Differ-ent methods for computing word similarities havebeen proposed, differing between methods to rep-resent the context (using dependency relationshipor a window of words) and between methods that,given a set of contexts, compute the similarity be-tween different words (ranging from cosine simi-larity to more complex metrics such as the Jaccardindex).
We refer to (Lin, 1998) for a comparisonof the different similarity metrics.In the next section we propose a novel methodto learn word similarities, the Latent Words Lan-guage Model (LWLM) (Deschacht and Moens,2009).
This model learns similar words and learnsthe a distribution over the contexts in which cer-tain types of words occur typically.4.2 DefinitionThe LWLM introduces for a text T = w1...wN oflength N for every observed word wi at position ia hidden variable hi.
The model is a generativemodel for natural language, in which the latentvariable hi is generated by its context C(hi) and theobserved word wi is generated by the latent vari-able hi.
In the current model we assume that thecontext is C(hi) = hi?1i?2hi+2i+1 where hi?1i?2 = hi?2hi?1is the two previous words and hi+2i+1 = hi+1hi+2 isthe two next words.
The observed wi has a valuefrom the vocabulary V , while the hidden variablehi is unknown, and is modeled as a probabilitydistribution over all words of V .
We will see inthe next section how this distribution is estimatedfrom a large unlabeled training corpus.
The aimof this model is to estimate, at every position i,a distribution for hi, assigning high probabilitiesto words that are similar to wi, given the contextof this word C(hi), and low probabilities to wordsthat are not similar to wi in this context.A possible interpretation of this model statesthat every hidden variable hi models the ?mean-ing?
for a particular word in a particular context.In this probabilistic model, when generating a sen-tence, we generate the meaning of a word (whichis an unobserved representation) with a certainprobability, and then we generate a certain obser-vation by writing down one of the possible wordsthat express this meaning.Creating a representation that models the mean-ing of a word is an interesting (and controversial)topic in its own right, but in this work we makethe assumption that the meaning of a particularword can be modeled using other words.
Model-ing the meaning of a word with other words is notan unreasonable one, since it is already employedin practice by humans (e.g.
by using dictionar-ies and thesauri) and machines (e.g.
relying on alexical resource such as WordNet) in word sensedisambiguation tasks.4.3 Parameter estimationAs we will further see the LWLM model has threeprobability distributions: P(wi|hi), the probabilityof the observed word w j given the latent variableh j, P(hi|hi?1i?2), the probability of the hidden wordh j given the previous variables h j?2 and h j?1, andP(hi|hi+2i+1), the probability of the hidden word h jgiven the next variables h j+1 and h j+2.
These dis-tributions need to be learned from a training textTtrain =< w0...wz > of length Z.4.3.1 The Baum-Welch algorithmThe attentive reader will have noticed the sim-ilarity between the proposed model and a stan-dard second-order Hidden Markov Model (HMM)where the hidden state is dependent on the two24previous states.
However, we are not able to usethe standard Baum-Welch (or forward-backward)algorithm, because the hidden variable hi is mod-eled as a probability distribution over all wordsin the vocabulary V .
The Baum-Welch algorithmwould result in an execution time of O(|V |3NG)where |V | is the size of the vocabulary, N is thelength of the training text and G is the number ofiterations needed to converge.
Since in our datasetthe vocabulary size is more than 30K words (seesection 3.2), using this algorithm is not possible.Instead we use techniques of approximate infer-ence, i.e.
Gibbs sampling.4.3.2 InitializationGibbs sampling starts from a random initializa-tion for the hidden variables and then improvesthe estimates in subsequent iterations.
In prelimi-nary experiments it was found that a pure randominitialization results in a very long burn-in-periodand a poor performance of the final model.
Forthis reason we initially set the distributions for thehidden words equal to the distribution of words asgiven by a standard language model3.4.3.3 Gibbs samplingWe store the initial estimate of the hidden vari-ables in M0train =< h0...hZ >, where hi generateswi at every position i. Gibbs sampling is a MarkovChain Monte Carlo method that updates the esti-mates of the hidden variables in a number of it-erations.
M?train denotes the estimate of the hid-den variables in iteration ?
.
In every iteration anew estimate M?+1train is generated from the previ-ous estimate M?train by selecting a random posi-tion j and updating the value of the hidden vari-able at that position.
The probability distributionsP?
(w j|h j), P?
(h j|h j?1j?2) and P?
(h j|hj+2j+1) are con-structed by collecting the counts from all positionsi 6= j.
The hidden variable h j is dependent on h j?2,h j?1, h j+1, h j+2 and w j and we can compute thedistribution of possible values for the variable h jasP?
(h j|w j,h j?10 ,hZj+1) =P?
(w j|h j)P?
(h j|h j?1j?2hj+2j+1)?hi P?(wi|hi)P?
(h j|h j?1j?2h j+2j+1)We set P(h j|h j?1j?2hj+2j+1) = P(h j|hj?1j?2) ?P(h j|hj+2j+1)which can be easily computed given the above dis-3We used the interpolated Kneser-Ney model as describedin (Goodman, 2001).tributions.
We select a new value for the hiddenvariable according to P?
(h j|w j,h j?10 ,hZj+1) andplace it at position j in M?+1train.
The current esti-mate for all other unobserved words remains thesame.
After performing this iteration a large num-ber of times (|V | ?10 in this experiment), the dis-tribution approaches the true maximum likelihooddistribution.
Gibbs sampling however samples thisdistribution, and thus will never reach it exactly.
Anumber of iterations (|V | ?100) is then performedin which Gibbs sampling oscillates around the cor-rect distribution.
We collect independent samplesof this distribution every |V | ?10 iterations, whichare then used to construct the final model.4.4 Evaluation of the Language ModelA first evaluation of the quality of the automat-ically learned latent words is by translation ofthis model into a sequential language model andby measuring its perplexity on previously unseentexts.
In (Deschacht and Moens, 2009) we per-form a number of experiments, comparing differ-ent corpora (news texts from Reuters and fromAssociated Press, and articles from Wikipedia)and n-gram sizes (3-gram and 4-gram).
We alsocompared the proposed model with two state-of-the-art language models, Interpolated Kneser-Neysmoothing and fullibmpredict (Goodman, 2001),and found that LWLM outperformed both modelson all corpora, with a perplexity reduction rangingbetween 12.40% and 5.87%.
These results showthat the estimated distributions over latent wordsare of a high quality and lead us to believe theycould be used to improve automatic text analysis,like SRL.5 Role labeling using latent wordsThe previous section discussed how the LWLMlearns similar words and how these similarities im-proved the perplexity on an unseen text of the lan-guage model derived from this model.
In this sec-tion we will see how we integrate the latent wordsmodel in two novel semi-supervised SRL modelsand compare these with two state-of-the-art semi-supervised models for SRL and dependency pars-ing.Latent words as additional featuresIn a first approach we estimate the distribution oflatent words for every word for both the trainingand test set.
We then use the latent words at every25position as additional probabilistic features for thediscriminative model.
More specifically, we ap-pend |V | extra values to the feature vector f(w j),containing the probability distribution over the |V |possible words for the hidden variable hi4.
We callthis the LWFeatures method.This method has the advantage that it is simpleto implement and that many existing SRL systemscan be easily extended by adding additional fea-tures.
We also expect that this method can be em-ployed almost effortless in other information ex-traction tasks, such as Named Entity Recognitionor Part-Of-Speech labeling.We compare this approach to the semi-supervised method in Koo et al (2008) who em-ploy clusters of related words constructed by theBrown clustering algorithm (Brown et al, 1992)for syntactic processing of texts.
Interestingly,this clustering algorithm has a similar objective asLWLM since it tries to optimize a class-based lan-guage model in terms of perplexity on an unseentest text.
We employ a slightly different clusteringmethod here, the fullibmpredict method discussedin (Goodman, 2001).
This method was shownto outperform the class based model proposed in(Brown et al, 1992) and can thus be expected todiscover better clusters of words.
We append thefeature vector f(w j) with c extra values (where c isthe number of clusters), respectively set to 1 if theword wi belongs to the corresponding cluster or to0 otherwise.
We call this method the ClusterFea-tures method.Automatic expansion of the training set usingpredicate argument alignmentWe compare our approach with a method proposedby F?rstenau and Lapata (2009).
This approach ismore tailored to the specific case of SRL and issummarized here.Given a set of labeled seed verbs with annotatedsemantic roles, for every annotated verb a numberof occurrences of this verb is found in unlabeledtexts where the context is similar to the context ofthe annotated example.
The context is defined hereas all words in the sentence that are direct depen-dents of this verb, given the syntactic dependencytree.
The similarity between two occurrences of aparticular verb is measured by finding all differentalignments ?
: M?
?
{1...n} (M?
?
{1, ...,m})4Probabilities smaller than 1e10?4 were set to 0 for effi-ciency reasons.between the m dependents of the first occurrenceand the n dependents of the second occurrence.Every alignment ?
is assigned a score given by?i?M?
(A ?
syn(gi,g?
(i))+ sem(wi,w?
(i))?B)where syn(gi,g?
(i)) denotes the syntactic simi-larity between grammatical role5 gi of word wiand grammatical role g?
(i) of word w?
(i), andsem(wi,w?
(i)) measures the semantic similaritybetween words wi and w?(i).
A is a constantweighting the importance of the syntactic simi-larity compared to semantic similarity, and B canbe interpreted as the lowest similarity value forwhich an alignment between two arguments ispossible.
The syntactic similarity syn(gi,g?
(i)) isdefined as 1 if the dependency relations are iden-tical, 0 < a < 1 if the relations are of the sametype but of a different subtype6 and 0 otherwise.The semantic similarity sem(wi,w?
(i)) is automat-ically estimated as the cosine similarity betweenthe contexts of wi and w?
(i) in a large text cor-pus.
For details we refer to (F?rstenau and Lapata,2009).For every verb in the annotated training set wefind the k occurrences of that verb in the unlabeledtexts where the contexts are most similar given thebest alignment.
We then expand the training setwith these examples, automatically generating anannotation using the discovered alignments.
Thevariable k controls the trade-off between anno-tation confidence and expansion size.
The finalmodel is then learned by running the supervisedtraining method on the expanded training set.
Wecall this method AutomaticExpansionCOS7 .
Thevalues for k, a, A and B are optimized automati-cally in every experiment on a held-out set (dis-joint from both training and test set).We adapt this approach by employing a differentmethod for measuring semantic similarity.
Giventwo words wi and w?
(i) we estimate the distri-bution of latent words, respectively L(hi) and5Note that this is a syntactic role, not a semantic role asthe ones discussed in this article.6Subtypes are fine-grained distinctions made by the parsersuch as the underlying grammatical roles in passive construc-tions.7The only major differences with (F?rstenau and Lap-ata, 2009) are the dependency parser which was used (theMALT parser (Nivre et al, 2006) instead of the RASP parser(Briscoe et al, 2006)) and the corpus employed to learn se-mantic similarities (the Reuters corpus instead of the BritishNational Corpus).
We expect that these differences will onlyinfluence the results minimally.265% 20% 50% 100%Supervised 40.49% 67.23% 74.93% 78.65%LWFeatures 60.29% 72.88% 76.42% 80.98%ClusterFeatures 59.51% 66.70% 70.15% 72.62%AutomaticExpansionCOS 47.05% 53.72% 64.51% 70.52%AutomaticExpansionLW 45.40% 53.82% 65.39% 72.66%Table 1: Results (in F1-measure) on the CoNLL 2008 test set for the different methods, comparingthe supervised method (Supervised) with the semi-supervised methods LWFeatures, ClusterFeatures,AutomaticExpansionCOS and AutomaticExpansionLW.
See section 5 for details on the different methods.Best results are in bold.L(h?(i)).
We then compute the semantic similaritymeasure as the Jensen-Shannon (Lin, 1997) diver-genceJS(L(hi)||L(h?
(i))) =12[D(L(hi)||avg)+D(L(h?
(i))||avg)]where avg = (L(hi) + L(h?
(i)))/2 is the averagebetween the two distributions and D(L(hi)||avg)is the Kullback?Leiber divergence (Cover andThomas, 2006).Although this change might appear only a slightdeviation from the original model discussed in(F?rstenau and Lapata, 2009) it is potentially animportant one, since an accurate semantic similar-ity measure will greatly influence the accuracy ofthe alignments, and thus of the accuracy of the au-tomatic expansion.
We call this method Automat-icExpansionLW.6 ExperimentsWe perform a number of experiments where wecompare the fully supervised model with the semi-supervised models proposed in the previous sec-tion.
We first train the LWLM model on an unla-beled 5 million word Reuters corpus8.We perform different experiments for the super-vised and the four different semi-supervised meth-ods (see previous section).
Table 1 shows the re-sults of the different methods on the test set of theCoNLL 2008 shared task.
We experimented withdifferent sizes for the training set, ranging from5% to 100%.
When using a subset of the full train-ing set, we run 10 different experiments with ran-dom subsets and average the results.We see that the LWFeatures method performsbetter than the other methods across all train-ing sizes.
Furthermore, these improvements are8See http://www.daviddlewis.com/resourceslarger for smaller training sets, showing that theapproach can be applied successfully in a settingwhere only a small number of training examplesis available.When comparing the LWFeatures method withthe ClusterFeatures method we see that, althoughthe ClusterFeatures method has a similar perfor-mance for small training sizes, this performancedrops for larger training sizes.
A possible expla-nation for this result is the use of the clusters em-ployed in the ClusterFeatures method.
By defini-tion the clusters merge many words into one clus-ter, which might lead to good generalization (moreimportant for small training sizes) but can poten-tially hurt precision (more important for largertraining sizes).A third observation that can be made from table1 is that, although both automatic expansion meth-ods (AutomaticExpansionCOS and AutomaticEx-pansionCOS) outperform the supervised methodfor the smallest training size, for other sizes of thetraining set they perform relatively poorly.
An in-formal inspection showed that for some examplesin the training set, little or no correct similar occur-rences were found in the unlabeled text.
The algo-rithm described in section 5 adds the most similark occurrences to the training set for every anno-tated example, also for these examples where lit-tle or no similar occurrences were found.
Oftenthe automatic alignment fails to generate correctlabels for these occurrences and introduces errorsin the training set.
In the future we would like toperform experiments that determine dynamically(for instance based on the similarity measure be-tween occurrences) for every annotated examplehow many training examples to add.277 Conclusions and future workWe have presented the Latent Words LanguageModel and showed how it learns, from unla-beled texts, latent words that capture the mean-ing of a certain word, depending on the con-text.
We then experimented with different meth-ods to incorporate the latent words for SemanticRole Labeling, and tested different methods on thePropBank dataset.
Our best performing methodshowed a significant improvement over the su-pervised model and over methods previously pro-posed in the literature.
On the full training setthe best method performed 2.33% better than thefully supervised model, which is a 10.91% errorreduction.
Using only 5% of the training data thebest semi-supervised model still achieved 60.29%,compared to 40.49% by the supervised model,which is an error reduction of 33.27%.
These re-sults demonstrate that the latent words learned bythe LWLM help for this complex information ex-traction task.
Furthermore we have shown that thelatent words are simple to incorporate in an ex-isting classifier by adding additional features.
Wewould like to perform experiments on employingthis model in other information extraction tasks,such as Word Sense Disambiguation or NamedEntity Recognition.
The current model uses thecontext in a very straightforward way, i.e.
the twowords left and right of the current word, but inthe future we would like to explore more advancedmethods to improve the similarity estimates.
Lin(1998) for example discusses a method where asyntactic parse of the text is performed and thecontext of a word is modeled using dependencytriples.The other semi-supervised methods proposedhere were less successful, although all improvedon the supervised model for small training sizes.In the future we would like to improve the de-scribed automatic expansion methods, since wefeel that their full potential has not yet beenreached.
More specifically we plan to experimentwith more advanced methods to decide whethersome automatically generated examples should beadded to the training set.AcknowledgmentsThe work reported in this paper was supportedby the EU-IST project CLASS (Cognitive-LevelAnnotation using Latent Statistical Structure, IST-027978) and the IWT-SBO project AMASS++(IWT-SBO-060051).
We thank the anonymous re-viewers for their helpful comments and Dennis N.Mehay for his help on clarifying the linguistic mo-tivation of our models.ReferencesC.F.
Baker, C.J.
Fillmore, and J.B. Lowe.
1998.
TheBerkeley FrameNet project.
In Proceedings of the36th Annual Meeting of the Association for Com-putational Linguistics and 17th International Con-ference on Computational Linguistics, volume 98.Montreal, Canada.A.L.
Berger, V.J.
Della Pietra, and S.A. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational linguistics,22(1):39?71.T.
Briscoe, J. Carroll, and R. Watson.
2006.
The sec-ond release of the RASP system.
In Proceedings ofthe Interactive Demo Session of COLING/ACL, vol-ume 6.P.F.
Brown, R.L.
Mercer, V.J.
Della Pietra, and J.C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.O.
Chapelle, B. Sch?lkopf, and A. Zien, editors.
2006.Semi-Supervised Learning.
MIT Press, Cambridge,MA.N.A.
Chinchor.
1998.
Overview of MUC-7/MET-2.
InProceedings of the Seventh Message UnderstandingConference (MUC-7), volume 1.A.M.
Cohen and W.R. Hersh.
2005.
A survey of cur-rent work in biomedical text mining.
Briefings inBioinformatics, 6(1):57?71.T.M.
Cover and J.A.
Thomas.
2006.
Elements of In-formation Theory.
Wiley-Interscience.Koen Deschacht and Marie-Francine Moens.
2009.The Latent Words Language Model.
In Proceed-ings of the 18th Annual Belgian-Dutch Conferenceon Machine Learning.C.
J. Fillmore.
1968.
The case for case.
In E. Bach andR.
Harms, editors, Universals in Linguistic Theory.Rinehart & Winston.Hagen F?rstenau and Mirella Lapata.
2009.
Semi-supervised semantic role labeling.
In Proceedings ofthe 12th Conference of the European Chapter of theACL (EACL 2009), pages 220?228, Athens, Greece.Association for Computational Linguistics.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.Joshua T. Goodman.
2001.
A bit of progress in lan-guage modeling, extended version.
Technical re-port, Microsoft Research.28G.
Grefenstette.
1994.
Explorations in automatic the-saurus discovery.
Springer.M.A.K.
Halliday.
1994.
An Introduction to FunctionalGrammar (second edition).
Edward Arnold, Lon-don.Zellig S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.S.
He and D. Gildea.
2006.
Self-training and Co-training for Semantic Role Labeling: Primary Re-port.
Technical report.
TR 891.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysis withpropbank and nombank.
In CoNLL 2008: Pro-ceedings of the Twelfth Conference on Computa-tional Natural Language Learning, pages 183?187,Manchester, England, August.
Coling 2008 Orga-nizing Committee.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
In Proceed-ings of the Annual Meeting of the Association forComputational Linguistics (ACL), pages 595?603.J.-H. Lim, Y.-S. Hwang, S.-Y.
Park, and H.-C. Rim.2004.
Semantic role labeling using maximum en-tropy model.
In Proceedings of the Eighth Confer-ence on Computational Natural Language Learning,pages 122?125, Boston, Massachusetts, USA.
ACL.D.
Lin.
1997.
Using syntactic dependency as localcontext to resolve word sense ambiguity.
In Pro-ceedings of the 35th Annual Meeting of the Asso-ciation for Computational Linguistics, volume 35,pages 64?71.
ACL.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th inter-national conference on Computational Linguistics,pages 768?774.
Association for Computational Lin-guistics Morristown, NJ, USA.G.S.
Mann and A. McCallum.
2007.
Simple, ro-bust, scalable semi-supervised learning via expecta-tion regularization.
In Proceedings of the 24th In-ternational Conference on Machine Learning, pages593?600.
ACM Press New York, USA.D.
Marcu, W. Wang, A. Echihabi, and K. Knight.
2006.SPMT: Statistical machine translation with syntact-ified target language phrases.
In Proceedings of theConference on Empirical Methods for Natural Lan-guage Processing, pages 44?52.S.
McDonald and M. Ramscar.
2001.
Testing the dis-tributional hypothesis: The influence of context onjudgements of semantic similarity.
In Proceedingsof the 23rd Annual Conference of the Cognitive Sci-ence Society, pages 611?616.Dennis Mehay, Rik De Busser, and Marie-FrancineMoens.
2005.
Labeling generic semantic roles.
InProceedings of the Sixth International Workshop onComputational Semantics.J.
Nivre, J.
Hall, and J. Nilsson.
2006.
MaltParser: Adatadriven parser-generator for dependency parsing.In Proceedings of the Fifth International Confer-ence on Language Resources and Evaluation, pages2216?2219.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.S.
Pradhan, W. Ward, K. Hacioglu, J. Martin, andD.
Jurafsky.
2004.
Shallow semantic parsing usingsupport vector machines.
In Proceedings of the Hu-man Language Technology Conference/North Amer-ican chapter of the Association of ComputationalLinguistics, Boston, MA.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 133?142.
Association for Com-putational Linguistics.M.
Surdeanu, S. Harabagiu, J. Williams, andP.
Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics, pages 8?15.R.S.
Swier and S. Stevenson.
2004.
Unsupervised se-mantic role labelling.
In Proceedings of the 2004Conference on Empirical Methods in Natural Lan-guage Processing, pages 95?102.C.
Thompson, R. Levy, and C. Manning.
2006.
A gen-erative model for FrameNet semantic role labeling .In Proceedings of the 14th European Conference onMachine Learning, Cavtat-Dubrovnik, Croatia.N.
Xue and M. Palmer.
2004.
Calibrating features forsemantic role labeling.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, volume 4.X.
Zhu.
2005.
Semi-supervised learning literature sur-vey.
Technical Report 1530, Computer Sciences,University of Wisconsin-Madison.29
