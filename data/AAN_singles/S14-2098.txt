Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560?565,Dublin, Ireland, August 23-24, 2014.SimCompass: Using Deep Learning Word Embeddingsto Assess Cross-level SimilarityCarmen Banea, Di Chen,Rada Mihalcea?University of MichiganAnn Arbor, MIClaire CardieCornell UniversityIthaca, NYJanyce WiebeUniversity of PittsburghPittsburgh, PAAbstractThis article presents our team?s partici-pating system at SemEval-2014 Task 3.Using a meta-learning framework, weexperiment with traditional knowledge-based metrics, as well as novel corpus-based measures based on deep learningparadigms, paired with varying degrees ofcontext expansion.
The framework en-abled us to reach the highest overall per-formance among all competing systems.1 IntroductionSemantic textual similarity is one of the keycomponents behind a multitude of natural lan-guage processing applications, such as informa-tion retrieval (Salton and Lesk, 1971), relevancefeedback and text classification (Rocchio, 1971),word sense disambiguation (Lesk, 1986; Schutze,1998), summarization (Salton et al., 1997; Linand Hovy, 2003), automatic evaluation of machinetranslation (Papineni et al., 2002), plagiarism de-tection (Nawab et al., 2011), and more.To date, semantic similarity research has pri-marily focused on comparing text snippets of simi-lar length (see the semantic textual similarity tasksorganized during *Sem 2013 (Agirre et al., 2013)and SemEval 2012 (Agirre et al., 2012)).
Yet,as new challenges emerge, such as augmenting aknowledge-base with textual evidence, assessingsimilarity across different context granularities isgaining traction.
The SemEval Cross-level seman-tic similarity task is aimed at this latter scenario,and is described in more details in the task paper(Jurgens et al., 2014).?
{carmennb,chenditc,mihalcea}@umich.eduThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/2 Related WorkOver the past years, the research community hasfocused on computing semantic relatedness us-ing methods that are either knowledge-based orcorpus-based.
Knowledge-based methods derivea measure of relatedness by utilizing lexical re-sources and ontologies such as WordNet (Miller,1995) or Roget (Rog, 1995) to measure defi-nitional overlap, term distance within a graph-ical taxonomy, or term depth in the taxonomyas a measure of specificity.
There are manyknowledge-based measures that were proposed inthe past, e.g., (Leacock and Chodorow, 1998;Lesk, 1986; Resnik, 1995; Jiang and Conrath,1997; Lin, 1998; Jarmasz and Szpakowicz, 2003;Hughes and Ramage, 2007).On the other side, corpus-based measures suchas Latent Semantic Analysis (LSA) (Landauerand Dumais, 1997), Explicit Semantic Analy-sis (ESA) (Gabrilovich and Markovitch, 2007),Salient Semantic Analysis (SSA) (Hassan andMihalcea, 2011), Pointwise Mutual Informa-tion (PMI) (Church and Hanks, 1990), PMI-IR(Turney, 2001), Second Order PMI (Islam andInkpen, 2006), Hyperspace Analogues to Lan-guage (Burgess et al., 1998) and distributionalsimilarity (Lin, 1998) employ probabilistic ap-proaches to decode the semantics of words.
Theyconsist of unsupervised methods that utilize thecontextual information and patterns observed inraw text to build semantic profiles of words.
Un-like knowledge-based methods, which suffer fromlimited coverage, corpus-based measures are ableto induce the similarity between any two words, aslong as they appear in the corpus used for training.3 System Description3.1 Generic FeaturesOur system employs both knowledge and corpus-based measures as detailed below.560Knowledge-based featuresKnowledge-based metrics were shown to providehigh correlation scores with the goldstandard intext similarity tasks (Agirre et al., 2012; Agirre etal., 2013).
We used three WordNet-based simi-larity measures that employ information content.We chose these metrics because they are able toincorporate external information derived from alarge corpus: Resnik (Resnik, 1995) (RES), Lin(Lin, 1998) (LIN ), and Jiang & Conrath (Jiangand Conrath, 1997) (JCN ).Corpus based featuresOur corpus based features are derived from adeep learning vector space model that is able to?understand?
word meaning without human in-put.
Distributed word embeddings are learned us-ing a skip-gram recurrent neural net architecturerunning over a large raw corpus (Mikolov et al.,2013b; Mikolov et al., 2013a).
A primary advan-tage of such a model is that, by breaking awayfrom the typical n-gram model that sees individualunits with no relationship to each other, it is able togeneralize and produce word vectors that are simi-lar for related words, thus encoding linguistic reg-ularities and patterns (Mikolov et al., 2013b).
Forexample, vec(Madrid)-vec(Spain)+vec(France) iscloser to vec(Paris) than any other word vec-tor (Mikolov et al., 2013a).
We used the pre-trained Google News word2vec model (WTV )built over a 100 billion words corpus, and con-taining 3 million 300-dimension vectors for wordsand phrases.
The model is distributed with theword2vec toolkit.1Since the methods outlined above provide similar-ity scores at the sense or word level, we derive textlevel metrics by employing two methods.VectorSum.
We add the vectors corresponding tothe non-stopwords tokens in bag of words (BOW)A and B, resulting in vectors VAand VB, respec-tively.
The assumption is that these vectors areable to capture the semantic meaning associatedwith the contexts, enabling us to gauge their relat-edness using cosine similarity.Align.
Given two BOW A and B as input, wecompare them using a word-alignment-based sim-ilarity measure (Mihalcea et al., 2006).
We calcu-late the pairwise similarity between the words inA and B, and match each word in A with its mostsimilar counterpart in B.
For corpus-based fea-1https://code.google.com/p/word2vec/tures, the similarity measure represents the aver-age over these scores, while for knowledge-basedmeasures, we consider the top 40% ranking pairs.We use the DKPro Similarity package (B?ar etal., 2013) to compute knowledge-based metrics,and the word2vec implementation from the Gen-sim toolkit (Rehurek and Sojka, 2010).3.2 Feature VariationsSince our system participated in all four lexicallevels evaluations, we describe below the modifi-cations pertaining to each.word2sense.
At the word2sense level, we em-ploy both knowledge and corpus-based features.Since the information available in each pair is ex-tremely limited (only a word and a sense key)we infuse contextual information by drawing onWordNet (Miller, 1995).
In WordNet, the senseof each word is encapsulated in a uniquely iden-tifiable synset, consisting of the definition (gloss),usage examples and its synonyms.
We can derivethree variations (where the word and sense com-ponents are represented by BOW A and B, respec-tively): a) no expansion (A={word}, B={sense}),b) expand right (R) (A={word}, B={sense gloss& example}), c) expand left (L) & right (R)(A={word glosses & examples}, B={sense gloss& example}).
After applying the Align method,we obtain measures JNC, LIN , RES andWTV 1; VectorSum results in WTV 2.phrase2word.
As this lexical level also suf-fers from low context, we adapt the above vari-ations, where the phrase and word componentsare represented by BOW A and BOW B, re-spectively.
Thus, we have: a) no expan-sion (A={phrase}, B={word}), b) expand R(A={phrase}, B={word glosses and examples}),c) expand L & R (A={phrase glosses & exam-ples}, B={word glosses and examples}).
We ex-tract the same measures as for word2sense.sentence2phrase.
For this variation, we use onlycorpus based measures; BOW A represents thesentence component, B, the phrase.
Since there issufficient context available, we follow the no ex-pansion variation, and obtain metrics WTV 1 (byapplying Align) and WTV 2 (using VectorSum).paragraph2sentence.
At this level, due to thelong context that entails one-to-many mappingsbetween the words in the sentence and those inthe paragraph, we use a text clustering techniqueprior to calculating the features?
weights.561a) no clustering.
We use only corpus based mea-sures, where the paragraph represents BOW A,and the sentence represents BOW B.
Then we ap-ply Align and VectorSum, resulting in WTV 1 andWTV 2, respectively.b) paragraph centroids extraction.
Since thelonger text contains more information comparedto the shorter one, we extract k topic vectors afterK-means clustering the left context.2These cen-troids are able to model topics permeating acrosssentences, and by comparing them with the wordvectors pertaining to the short text, we seek to cap-ture how much of the information is covered in theshorter text.
Each word is paired with the centroidthat it is closest to, and the average is computedover these scores, resulting in WTV 3.c) sentence centroids extraction.
Under a dif-ferent scenario, assuming that one sentence cov-ers only a few strongly expressed topics, unlikea paragraph that may digress and introduce unre-lated noise, we apply clustering on the short text.The centroids thus obtained are able to capturethe essence of the sentence, so when compared toevery word in the paragraph, we can gauge howmuch of the short text is reflected in the longerone.
Each centroid is paired with the word that it ismost similar to, and we average these scores, thusobtaining WTV 4.
In a way, methods b) and c)provide a macro, respectively micro view of howthe topics are reflected across the two spans of text.3.3 Meta-learningThe measures of similarity described above pro-vide a single score per each long text - short textpair in the training and test data.
These scores thenbecome features for a meta-learner, which is ableto optimize their impact on the prediction process.We experimented with multiple regression algo-rithms by conducting 10 fold cross-validation onthe training data.
The strongest performer acrossall lexical levels was Gaussian processes with aradial basis function (RBF) kernel.
Gaussian pro-cesses regression is an efficient probabilistic pre-diction framework that assumes a Gaussian pro-cess prior on the unobservable (latent) functionsand a likelihood function that accounts for noise.An individual classifier3was trained for each lex-ical level and applied to the test data sets.2Implementation provided in the Scikit library (Pedregosaet al., 2011), where k is set to 3.3Implementation available in the WEKA machine learn-ing software (Hall et al., 2009) using the default parameters.4 Evaluations & DiscussionOur system participated in all cross-level subtasksunder the name SimCompass, competing with 37other systems developed by 20 teams.Figure 1 highlights the Pearson correlations atthe four lexical levels between the gold standardand each similarity measure introduced in Section3, as well as the predictions ensuing as a resultof meta-learning.
The left and right histograms ineach subfigure present the scores obtained on thetrain and test data, respectively.In the case of word2sense train data, we no-tice that expanding the context provides additionalinformation and improves the correlation results.For corpus-based measures, the correlations arestronger when the expansion involves only theright side of the tuple, namely the sense.
Wenotice an increase of 0.04 correlation points forWTV1 and 0.09 for WTV2.
As soon as the wordis expanded as well, the context incorporates toomuch noise, and the correlation levels drop.
Inthe case of knowledge-based measures, expandingthe context does not seem to impact the results.However, these trends do not carry out to the testdata, where the corpus-based features without ex-pansion reach a correlation higher than 0.3, whilethe knowledge-based features score significantlylower (by 0.16).
Once all these measures are usedas features in a meta learner (All) using Gaus-sian processes regression (GP), the correlation in-creases over the level attained by the best perform-ing individual feature, reaching 0.45 on the traindata and 0.36 on the test data.
SimCompass rankssecond in this subtask?s evaluations, falling shortof the leading system by 0.025 correlation points.Turning now to the phrase2word subfigure, wenotice that the context already carries sufficientinformation, and expanding it causes the perfor-mance to drop (the more extensive the expan-sion, the steeper the drop).
Unlike the scenarioencountered for word2sense, the trend observedhere on the training data also gets mirrored in thetest data.
Same as before, knowledge-based mea-sures have a significantly lower performance, butdeep learning-based features based on word2vec(WTV) only show a correlation variation by atmost 0.05, proving their robustness.
Leveragingall the features in a meta-learning framework en-ables the system to predict stronger scores for boththe train and the test data (0.48 and 0.42, respec-tively).
Actually, for this variation, SimCompass56200.10.20.30.40.5word2senseTrain Test00.10.20.30.40.5BL JNCLINRESWTV1WTV2GPJNCLINRESWTV1WTV2GPphrase2wordNo expansion Expansion R Expansion L&R All00.20.40.60.8sentence2phraseTrain TestBL WTV1WTV2WTV3WTV4GPWTV1WTV2WTV3WTV4GP00.20.40.60.8paragraph2sentenceFigure 1: Pearson correlation of individual measures on the train and test data sets.
As these measures be-come features in a regression algorithm (GP), prediction correlations are included as well.
BL representsthe baseline computed by the organizers.obtains the highest score among all competing sys-tems, surpassing the second best by 0.10.Noticing that expansion is not helpful when suf-ficient context is available, for the next variationswe use the original tuples.
Also, due to the re-duced impact of knowledge-based features on thetraining outcome, we only focus on deep learningfeatures (WTV1, WTV2, WTV3, WTV4).Shifting to sentence2phrase, WTV2 (con-structed using VectorSum) is the top perform-ing feature, surpassing the baseline by 0.19,and attaining 0.69 and 0.73 on the train andtest sets, respectively.
Despite also consideringa lower performing feature (WTV1), the meta-learner maintains high scores, surpassing the cor-relation achieved on the train data by 0.04 (from0.70 to 0.74).
In this variation, our system ranksfifth, at 0.035 from the top system.For the paragraph2sentence variation, due tothe availability of longer contexts, we introduceWTV3 and WTV4 that are based on clustering theleft and the right sides of the tuple, respectively.WTV2 fares slightly better than WTV3 and WTV4.WTV1 surpasses the baseline this time, leaving itsmark on the decision process.
When training theGP learner on all features, we obtain 0.78 correla-tion on the train data, and 0.81 on test data, 0.10higher than those attained by the individual fea-tures alone.
SimCompass ranks seventh in perfor-mance on this subtask, at 0.026 from the first.Considering the overall system performance,SimCompass is remarkably versatile, rankingamong the top at each lexical level, and taking thefirst place in the SemEval Task 3 overall evalu-ation with respect to both Pearson (0.58 averagecorrelation) and Spearman correlations.5 ConclusionWe described SimCompass, the system we partic-ipated with at SemEval-2014 Task 3.
Our exper-iments suggest that traditional knowledge-basedfeatures are cornered by novel corpus-based wordmeaning representations, such as word2vec, whichemerge as efficient and strong performers undera variety of scenarios.
We also explored whethercontext expansion is beneficial to the cross-levelsimilarity task, and remarked that only when thecontext is particularly short, this enrichment is vi-able.
However, in a meta-learning framework, theinformation permeating from a set of similaritymeasures exposed to varying context expansionscan attain a higher performance than possible withindividual signals.
Overall, our system ranked firstamong 21 teams and 38 systems.AcknowledgmentsThis material is based in part upon work sup-ported by National Science Foundation CAREERaward #1361274 and IIS award #1018613 andby DARPA-BAA-12-47 DEFT grant #12475008.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the views563of the National Science Foundation or the DefenseAdvanced Research Projects Agency.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: Apilot on semantic textual similarity.
In Proceedingsof the 6th International Workshop on Semantic Eval-uation (SemEval 2012), in conjunction with the FirstJoint Conference on Lexical and Computational Se-mantics (*SEM 2012), Montreal, Canada.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 SharedTask: Semantic Textual Similarity, including a Pi-lot on Typed-Similarity.
In The Second Joint Con-ference on Lexical and Computational Semantics(*SEM 2013).Daniel B?ar, Torsten Zesch, and Iryna Gurevych.
2013.DKPro Similarity: An open source framework fortext similarity.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics: System Demonstrations, pages 121?126,Sofia, Bulgaria.Curt Burgess, Kay Livesay, and Kevin Lund.
1998.Explorations in context space: words, sentences,discourse.
Discourse Processes, 25(2):211?257.Kenneth Church and Patrick Hanks.
1990.
Word as-sociation norms, mutual information, and lexicogra-phy.
Computational Linguistics, 16(1):22?29.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using Wikipedia-based explicit semantic analysis.
In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence, pages 1606?1611, Hyderabad, India.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Up-date.
SIGKDD Explorations, 11(1).Samer Hassan and Rada Mihalcea.
2011.
Measuringsemantic relatedness using salient encyclopedic con-cepts.
Artificial Intelligence, Special Issue.Thad Hughes and Daniel Ramage.
2007.
Lexical se-mantic knowledge with random graph walks.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, Prague, CzechRepublic.Aminul Islam and Diana Zaiu Inkpen.
2006.
Secondorder co-occurrence PMI for determining the seman-tic similarity of words.
In Proceedings of the FifthConference on Language Resources and Evaluation,volume 2, pages 1033?1038, Genoa, Italy, July.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?sthesaurus and semantic similarity.
In Proceedingsof the conference on Recent Advances in NaturalLanguage Processing RANLP-2003, Borovetz, Bul-garia, September.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
In Proceeding of the International Confer-ence Research on Computational Linguistics (RO-CLING X), Taiwan.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
SemEval-2014 Task 3:Cross-Level Semantic Similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval-2014), Dublin, Ireland.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and repre-sentation of knowledge.
Psychological Review, 104.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet sense similarityfor word sense identification.
In WordNet, An Elec-tronic Lexical Database.
The MIT Press.Michael E. Lesk.
1986.
Automatic sense disambigua-tion using machine readable dictionaries: How totell a pine cone from an ice cream cone.
In Pro-ceedings of the SIGDOC Conference 1986, Toronto,June.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of Human Lan-guage Technology Conference (HLT-NAACL 2003),Edmonton, Canada, May.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the Fifteenth In-ternational Conference on Machine Learning, pages296?304, Madison, Wisconsin.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In AmericanAssociation for Artificial Intelligence (AAAI-2006),Boston, MA.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Distributed Representations of Wordsand Phrases and their Compositionality .
In NIPS,pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In NAACL HLT, pages 746?751, Atlanta, GA, USA.George A. Miller.
1995.
WordNet: a Lexical databasefor English.
Communications of the Association forComputing Machinery, 38(11):39?41.Rao Muhammad Adeel Nawab, Mark Stevenson, andPaul Clough.
2011.
External plagiarism detectionusing information retrieval and sequence alignment:564Notebook for PAN at CLEF 2011.
In Proceedingsof the 5th International Workshop on UncoveringPlagiarism, Authorship, and Social Software Misuse(PAN 2011).Kishore.
Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,PA.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
The Jour-nal of Machine Learning Research, 12:2825?2830.Radim Rehurek and Petr Sojka.
2010.
Software frame-work for topic modelling with large corpora.
In Pro-ceedings of the LREC 2010 Workshop on New Chal-lenges for NLP Frameworks, pages 45?50, Valletta,Malta, May.
ELRA.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In Pro-ceedings of the 14th International Joint Conferenceon Artificial Intelligence, pages 448?453, Montreal,Quebec, Canada.
Morgan Kaufmann Publishers Inc.J.
Rocchio, 1971.
Relevance feedback in informationretrieval.
Prentice Hall, Ing.
Englewood Cliffs, NewJersey.1995.
Roget?s II: The New Thesaurus.
Houghton Mif-flin.Gerard Salton and Michael E. Lesk, 1971.
The SMARTRetrieval System: Experiments in Automatic Doc-ument Processing, chapter Computer evaluation ofindexing and text processing.
Prentice Hall, Ing.
En-glewood Cliffs, New Jersey.Gerard Salton, Amit Singhal, Mandar Mitra, and ChrisBuckley.
1997.
Automatic text structuring and sum-marization.
Information Processing and Manage-ment, 2(32).Hinrich Schutze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?124.Peter D. Turney.
2001.
Mining the Web for Synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe 12th European Conference on Machine Learning(ECML?01), pages 491?502, Freiburg, Germany.565
