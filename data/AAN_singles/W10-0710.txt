Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 62?65,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCan Crowds Build Parallel Corpora for Machine Translation Systems?Vamshi Ambati and Stephan Vogel{vamshi,vogel}@cs.cmu.eduLanguage Technologies Institute, Carnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213, USAAbstractCorpus based approaches to machine transla-tion (MT) rely on the availability of parallelcorpora.
In this paper we explore the effec-tiveness of Mechanical Turk for creating par-allel corpora.
We explore the task of sen-tence translation, both into and out of a lan-guage.
We also perform preliminary experi-ments for the task of phrase translation, whereambiguous phrases are provided to the turkerfor translation in isolation and in the contextof the sentence it originated from.1 IntroductionLarge scale parallel data generation for new lan-guage pairs requires intensive human effort andavailability of bilingual speakers.
Only a few lan-guages in the world enjoy sustained research inter-est and continuous financial support for develop-ment of automatic translation systems.
For mostremaining languages there is very little interest orfunding available and limited or expensive access toexperts for data elicitation.
Crowd-sourcing com-pensates for the lack of experts with a large pool ofexpert/non-expert crowd.
However, crowd-sourcinghas thus far been explored in the context of elicit-ing annotations for a supervised classification task,typically monolingual in nature (Snow et al, 2008).In this shared task we test the feasibility of elicitingparallel data for Machine Translation (MT) usingMechanical Turk (MTurk).
MT poses an interestingchallenge as we require turkers to have understand-ing/writing skills in both the languages.
Our work issimilar to some recent work on crowd-sourcing andmachine translation (Ambati et al, 2010; Callison-Burch, 2009), but focuses primarily on the setup anddesign of translation tasks on MTurk with varyinggranularity levels, both at sentence- and phrase-leveltranslation.2 Language Landscape on MTurkWe first conduct a pilot study by posting 25 sen-tences each from a variety of language pairs andprobing to see the reception on MTurk.
Language-pair selection was based on number of speakers inthe language and Internet presence of the popula-tion.
Languages like Spanish, Chinese, English,Arabic are spoken by many and have a large pres-ence of users on the Internet.
Those like Urdu,Tamil, Telugu although spoken by many are not wellrepresented on the Web.
Languages like Swahili,Zulu, Haiti are neither spoken by many nor havea great presence on the Web.
For this pilot studywe selected Spanish, Chinese, English, Urdu, Tel-ugu, Hindi, Haitian Creole languages.
We do notselect German, French and other language pairs asthey have already been explored by Callison-Burch(2009).
Our pilot study helped us calibrate the costsfor different language pairs as well as helped us se-lect the languages to pursue further experiments.
Wefound that at lower pay rates like 1 cent, it is difficultto find a sufficient number of translators to completethe task.
For example, we could not find turkersto complete the translation from English to Haitian-Creole even after a period of 10 days.
Haitian creoleis spoken by a small population and it seems thatonly a very small portion of that was on MTurk.
Fora few other languages pairs, while we could find a62Pair Cost per sen DaysSpanish-Eng $0.01 1Telugu-Eng $0.02 2Eng-Creole $0.06 -Urdu-Eng $0.03 1Hindi-Eng $0.03 1Chinese-Eng $0.02 1Table 1: Cost vs.
Completion for Language pairsfew turkers attempting the task, the price had to beincreased to attract any attention.
Table 1 shows thefindings of our pilot study.
We show the minimumcost at which we could start getting turkers to pro-vide translations and the number of days they took tocomplete the task.
MTurk has so far been a suppli-ers?
market, and translation of rare-languages showshow a limited supply of turkers leads to a buyer?smarket; only fair.3 Challenges for Crowd-Sourcing andMachine TranslationWe use MTurk for all our crowd-sourcing experi-ments.
In case of MT, a HIT on MTurk is one ormore sentences in the source language that need tobe translated to a target language.
Making sure thatthe workers understand the task is the first step to-wards a successful elicitation using the crowd.
Weprovide detailed instructions on the HIT for bothcompletion of the task and its evaluation.
Mechan-ical turk also has a provision to seek annotationsfrom qualified workers, from a specific location witha specific success rate in their past HITs.
For allour HITs we set the worker qualification thresholdto 90%.
We use the terms HIT vs. task and turkervs.
translator interchangeably.3.1 Quality AssuranceQuality assurance is a concern with an online crowdwhere the expertise of the turkers is unknown.
Wealso notice from the datasets we receive that consis-tently poor and noisy translators exist.
Problems likeblank annotations, mis-spelling, copy-pasting of in-put are prevalent, but easy to identify.
Turkers whodo not understand the task but attempt it anyway arethe more difficult ones to identify, but this is to beexpected with non-experts.
Redundancy of transla-tions for the input and computing majority consen-sus translation is agreed to be an effective solution toidentify and prune low quality translation.
We dis-cuss in following section computation of majorityvote using fuzzy matching.For a language pair like Urdu-English, we noticeda strange scenario, where the translations from twoturkers were significantly worse in quality, but con-sistently matched each other, there by falsely boost-ing the majority vote.
We suspect this to be a case ofcheating, but this exposes a loop in majority votingwhich needs to be addressed, perhaps by also usinggold standard data.Turking Machines: We also have the problemof machines posing as turkers ?
?Turking machine?problem.
With the availability of online translationsystems like Google translate, Yahoo translate (Ba-belfish) and Babylon, translation tasks on MTurkbecome easy targets to this problem.
Turkers ei-ther use automatic scripts to get/post data from au-tomatic MT systems, or make slight modificationsto disguise the fact.
This defeats the purpose of thetask, as the resulting corpus would then be biased to-wards some existing automatic MT system.
It is ex-tremely important to keep gamers in check; not onlydo they pollute the quality of the crowd data, buttheir completion of a HIT means it becomes unavail-able to genuine turkers who are willing to providevaluable translations.
We, therefore, collect transla-tions from existing automatic MT services and usethem to match and block submissions from gamers.We rely on some gold-standard to identify genuinematches with automatic translation services.3.2 Output Space and Fuzzy MatchingDue to the natural variability in style of turkers, therecould be multiple different, but perfectly valid trans-lations for a given sentence.
Therefore it is dif-ficult to match translation outputs from two turk-ers or even with gold standard data.
We there-fore need a fuzzy matching algorithm to accountfor lexical choices, synonymy, word ordering andmorphological variations.
This problem is similarto the task of automatic translation output evalua-tion and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com-paring two sentences.
METEOR has an internalaligner that matches words in the sentences given63and scores them separately based on whether thematch was supported by synonymy, exact match orfuzzy match.
The scores are then combined to pro-vide a global matching score.
If the score is above athreshold ?, we treat the sentences to be equivalenttranslations of the source sentence.
We can set the?
parameter to different values, based on what is ac-ceptable to the application.
In our experiments, weset ?
= 0.7.
We did not choose BLEU scoring met-ric as it is strongly oriented towards exact matchingand high precision, than towards robust matching forhigh recall.4 Sentence TranslationThe first task we setup on MTurk was to translatefull sentences from a source language into a tar-get language.
The population we were interested inwas native speakers of one of the languages.
Weworked with four languages - English, Spanish, Tel-ugu and Urdu.
We chose 100 sentences for eachlanguage-pair and requested three different transla-tions for each sentence.
The Spanish data was takenfrom BTEC (Takezawa et al, 2002) corpus, consist-ing of short sentences in the travel domain.
Telugudata was taken from the sports and politics sectionof a regional newspaper.
For Urdu, we used theNIST-Urdu Evaluation 2008 data.
We report resultsin Table 2.
Both Spanish and Urdu had gold stan-dard translations, as they were taken from parallelcorpora created by language experts.
As the datasets are small, we chose to perform manual inspec-tion rather than use automatic metrics like BLEU toscore match against gold-standard data.4.1 Translating into EnglishThe first batch of HITs were posted to collect trans-lations into English.
We noticed from manual in-spection of the quality of translations that most ofour translators were non-native speakers of English.This calls for adept and adequate methods for evalu-ating the translation quality.
For example more than50% of the Spanish-English tasks were completed inIndia, and in some cases a direct output of automatictranslation services.4.2 Translating out of EnglishThe second set of experiments were to test the ef-fectiveness of translating out of English.
The idealLanguage Pair Cost #Days #TurkersSpanish-English $0.01 1 16Telugu-English $0.02 4 12Urdu-English $0.03 2 13English-Spanish $0.01 1 19English-Telugu $0.02 3 35English-Urdu $0.03 2 21Table 2: Sentence translation datatarget population for this task were native speakersof the target language who also understood English.Most participant turkers who provided Urdu and Tel-ugu translations, were from India and USA and werenon-native speakers of English.
However, one prob-lem with enabling this task was the writing system.Most turkers do not have the tools to create contentin their native language.
We used ?Google Translit-erate?
API 1 to enable production of non-Englishcontent.
This turned out to be an interesting HITfor the turkers, as they were excited to create theirnative language content.
This is evident from theincreased number of participant turkers.
Manual in-spection of translations revealed that this directionresulted in higher quality translations for both Urduand Telugu and slightly lower quality for Spanish.5 Phrase TranslationPhrase translation is useful in reducing the costand effort of eliciting translations by focusing onthose parts of the sentence that are difficult totranslate.
It fits well into the paradigm of crowd-sourcing where small tasks can be provided to a lotof translators.
For this task, we were interested inunderstanding how well non-experts translate sub-sentential segments, and whether exposure to ?con-text?
was helpful.
For this set of experiments we usethe Spanish-English language pair, where the turk-ers were presented with Spanish phrases to trans-late.
The phrases were selected from the standardphrase tables produced by statistical phrase-basedMT (Koehn et al, 2007), that was trained on the en-tire 128K BTEC corpus for Spanish-English.
Wecomputed an entropy score for each entry in thephrase table under the translation probability distri-butions in both directions and picked the set of 501http://www.google.com/transliterate/64Type %Agreement %Gold matchOut of Context 64% 32%In Context 68% 33%Table 3: Phrase Translation: Spanish-EnglishLength Count Example1 2 cierras2 11 vienes aqu3 26 hay una en4 8 a conocer su decisin5 4 viene bien a esa horaTable 4: Details of Spanish-English phrases usedmost ambiguous phrases according to this metric.Table 4 shows sample and the length distribution ofthe phrases selected for this task.5.1 In Context vs. Out of ContextWe performed two kinds of experiments to studyphrase translation and role of context.
In the firstcase, the task was designed to be as simple as possi-ble with each phrase to be translated as an individualHIT.
We provided a source phrase and request turk-ers to translate a phrase under any hypothesized con-text.
For the second task, we gave a phrase associ-ated with the sentence that it originated from and re-quested the turkers to translate the phrase only in thecontext of the sentence.
For both cases, we analyzedthe data for inter-translator agreement;% of caseswhere there was a consensus translation), and agree-ment with the gold standard; % of times the trans-lated phrase was present in the gold standard transla-tion of the source sentence it came from.
As shownin Table 3, translating in-context produced a bettermatch with gold standard data and scored slightlybetter on the inter-translator agreement.
We thinkthat when translating out of context, most translatorschoose as appropriate for a context in their mind andso the inter-translator agreement could be lower, butwhen translating within the context of a sentence,they make translation choices to suit the sentencewhich could lead to better agreement scores.
In fu-ture, we will extend these experiments to other lan-guage pairs and choose phrases not by entropy met-ric, but to study specific language phenomenon.6 ConclusionOur experiments helped us better understand theformulation of translation tasks on MTurk and itschallenges.
We experimented with both translatinginto and out of English and use transliteration foraddressing the writing system issue.
We also ex-periment with in-context and out-of-context phrasetranslation task.
While working with non-experttranslators it is important to address quality concernsalongside keeping in check any usage of automatictranslation services.
At the end of the shared task wehave sampled the ?language landscape?
on MTurkand have a better understanding of what to expectwhen building MT systems for different languagepairs.ReferencesVamshi Ambati, Stephan Vogel, and Jaime Carbonell.2010.
Active learning and crowd-sourcing for ma-chine translation.
In Proceedings of the LREC 2010,Malta, May.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In EMNLP 2009, pages 286?295, Sin-gapore, August.
Association for Computational Lin-guistics.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In ACL Demonstration Session.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: an au-tomatic metric for mt evaluation with high levels ofcorrelation with human judgments.
In WMT 2007,pages 228?231, Morristown, NJ, USA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?evaluating non-expert annotations for natural languagetasks.
In Proceedings of the EMNLP 2008, pages 254?263, Honolulu, Hawaii, October.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,Hirofumi Yamamoto, and Seiichi Yamamoto.
2002.Towards a broad-coverage bilingual corpus for speechtranslation of travel conversation in the real world.
InProceedings of LREC 2002, Las Palmas, Spain.65
