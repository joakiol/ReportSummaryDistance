Learning Reliability of Parses for Domain Adaptation ofDependency ParsingDaisuke Kawahara and Kiyotaka UchimotoNational Institute of Information and Communications Technology,3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan{dk, uchimoto}@nict.go.jpAbstractThe accuracy of parsing has exceeded 90%recently, but this is not high enough to useparsing results practically in natural lan-guage processing (NLP) applications such asparaphrase acquisition and relation extrac-tion.
We present a method for detecting re-liable parses out of the outputs of a singledependency parser.
This technique is alsoapplied to domain adaptation of dependencyparsing.
Our goal was to improve the per-formance of a state-of-the-art dependencyparser on the data set of the domain adap-tation track of the CoNLL 2007 shared task,a formidable challenge.1 IntroductionDependency parsing has been utilized in a varietyof natural language processing (NLP) applications,such as paraphrase acquisition, relation extractionand machine translation.
For newspaper articles, theaccuracy of dependency parsers exceeds 90% (forEnglish), but it is still not sufficient for practical usein these NLP applications.
Moreover, the accuracydeclines significantly for out-of-domain text, such asweblogs and web pages, which have commonly beenused as corpora.
From this point of view, it is impor-tant to consider the following points to use a parserpractically in applications:?
to select reliable parses, especially for knowl-edge acquisition,?
and to adapt the parser to new domains.This paper proposes a method for selecting reli-able parses from parses output by a single depen-dency parser.
We do not use an ensemble methodbased on multiple parsers, but use only a singleparser, because speed and efficiency are importantwhen processing a massive volume of text.
Theresulting highly reliable parses would be useful toautomatically construct dictionaries and knowledgebases, such as case frames (Kawahara and Kuro-hashi, 2006).
Furthermore, we incorporate the reli-able parses we obtained into the dependency parserto achieve domain adaptation.The CoNLL 2007 shared task tackled domainadaptation of dependency parsers for the first time(Nivre et al, 2007).
Sagae and Tsujii applied anensemble method to the domain adaptation trackand achieved the highest score (Sagae and Tsujii,2007).
They first parsed in-domain unlabeled sen-tences using two parsers trained on out-of-domainlabeled data.
Then, they extracted identical parsesthat were produced by the two parsers and addedthem to the original (out-of-domain) training set totrain a domain-adapted model.Dredze et al yielded the second highest score1in the domain adaptation track (Dredze et al, 2007).However, their results were obtained without adap-tation.
They concluded that it is very difficult to sub-stantially improve the target domain performanceover that of a state-of-the-art parser.
To confirmthis, we parsed the test set (CHEM) of the domainadaptation track by using one of the best dependencyparsers, second-order MSTParser (McDonald et al,1Dredze et al achieved the second highest score on theCHEM test set for unlabeled dependency accuracy.7092006)2.
Though this parser was trained on the pro-vided out-of-domain (Penn Treebank) labeled data,surprisingly, its accuracy slightly outperformed thehighest score achieved by Sagae and Tsujii (unla-beled dependency accuracy: 83.58 > 83.42 (Sagaeand Tsujii, 2007)).
Our goal is to improve a state-of-the-art parser on this domain adaptation track.Dredze et al also indicated that unlabeled de-pendency parsing is not robust to domain adaptation(Dredze et al, 2007).
This paper therefore focuseson unlabeled dependency parsing.2 Related WorkWe have already described the domain adaptationtrack of the CoNLL 2007 shared task.
For the mul-tilingual dependency parsing track, which was theother track of the shared task, Nilsson et al achievedthe best performance using an ensemble method(Hall et al, 2007).
They used a method of com-bining several parsers?
outputs in the framework ofMST parsing (Sagae and Lavie, 2006).
This methoddoes not select parses, but considers all the outputparses with weights to decide a final parse of a givensentence.Reichart and Rappoport also proposed an ensem-ble method to select high-quality parses from theoutputs of constituency parsers (Reichart and Rap-poport, 2007a).
They regarded parses as being ofhigh quality if 20 different parsers agreed.
They didnot apply their method to domain adaptation or otherapplications.Reranking methods for parsing have a relationto parse selection.
They rerank the n-best parsesthat are output by a generative parser using a lotof lexical and syntactic features (Collins and Koo,2005; Charniak and Johnson, 2005).
There areseveral related methods for 1-best outputs, suchas revision learning (Nakagawa et al, 2002) andtransformation-based learning (Brill, 1995) for part-of-speech tagging.
Attardi and Ciaramita proposeda method of tree revision learning for dependencyparsing (Attardi and Ciaramita, 2007).As for the use of unlabeled data, self-trainingmethods have been successful in recent years.
Mc-Closky et al improved a state-of-the-art con-stituency parser by 1.1% using self-training (Mc-2http://sourceforge.net/projects/mstparser/Table 1: Labeled and unlabeled data provided forthe shared task.
The labeled PTB data is used fortraining, and the labeled BIO data is used for devel-opment.
The labeled CHEM data is used for the finaltest.name source labeled unlabeledPTB Penn Treebank 18,577 1,625,606BIO Penn BioIE 200 369,439CHEM Penn BioIE 200 396,128Closky et al, 2006a).
They also applied self-trainingto domain adaptation of a constituency parser (Mc-Closky et al, 2006b).
Their method simply addsparsed unlabeled data without selecting it to thetraining set.
Reichart and Rappoport applied self-training to domain adaptation using a small set ofin-domain training data (Reichart and Rappoport,2007b).Van Noord extracted bilexical preferences from aDutch parsed corpus of 500M words without selec-tion (van Noord, 2007).
He added some features intoan HPSG (head-driven phrase structure grammar)parser to consider the bilexical preferences, and ob-tained an improvement of 0.5% against a baseline.Kawahara and Kurohashi extracted reliable de-pendencies from automatic parses of Japanese sen-tences on the web to construct large-scale caseframes (Kawahara and Kurohashi, 2006).
Thenthey incorporated the constructed case frames into aprobabilistic dependency parser, and outperformedtheir baseline parser by 0.7%.3 The Data SetThis paper uses the data set that was used in theCoNLL 2007 shared task (Nivre et al, 2007).
Table1 lists the data set provided for the domain adapta-tion track.We pre-processed all the unlabeled sentences us-ing a conditional random fields (CRFs)-based part-of-speech tagger.
This tagger is trained on thePTB training set that consists of 18,577 sentences.The features are the same as those in (Ratnaparkhi,1996).
As an implementation of CRFs, we usedCRF++3.
If a method of domain adaptation is ap-plied to the tagger, the accuracy of parsing unlabeledsentences will improve (Yoshida et al, 2007).
This3http://crfpp.sourceforge.net/710paper, however, does not deal with domain adapta-tion of a tagger but focuses on that of a parser.4 Learning Reliability of ParsesOur approach assesses automatic parses of a singleparser in order to select only reliable parses fromthem.
We compare automatic parses and their gold-standard ones, and regard accurate parses as positiveexamples and the remainder as negative examples.Based on these examples, we build a binary classi-fier that classifies each sentence as reliable or not.To precisely detect reliable parses, we make use ofseveral linguistic features inspired by the notion ofcontrolled language (Mitamura et al, 1991).
That isto say, the reliability of parses is judged based on thedegree of sentence difficulty.Before describing our base dependency parser andthe algorithm for detecting reliable parses, we firstexplain the data sets used for them.
We preparedthe following three labeled data sets to train the basedependency parser and the reliability detector.PTB base train: training set for the base parser:14,862 sentencesPTB rel train: training set for reliability detector:2,500 sentences4BIO rel dev: development set for reliability detec-tor: 200 sentences (= labeled BIO data)PTB base train is used to train the base depen-dency parser, and PTB rel train is used to train ourreliability detector.
BIO rel dev is used for tuningthe parameters of the reliability detector.4.1 Base Dependency ParserWe used the MSTParser (McDonald et al, 2006),which achieved top results in the CoNLL 2006(CoNLL-X) shared task, as a base dependencyparser.
To enable second-order features, the param-eter order was set to 2.
The other parameters wereset to default.
We used PTB base train (14,862 sen-tences) to train this parser.4.2 Algorithm to Detect Reliable ParsesWe built a binary classifier for detecting reliable sen-tences from a set of automatic parses produced by41,215 labeled PTB sentences are left as another develop-ment set for the reliability detector, but they are not used in thispaper.the base dependency parser.We used support vector machines (SVMs) as a bi-nary classifier with a third-degree polynomial ker-nel.
We parsed PTB rel train (2,500 sentences) us-ing the base parser, and evaluated each sentence withthe metric of unlabeled dependency accuracy.
Weregarded the sentences whose accuracy is better thana threshold, ?
, as positive examples, and the othersas negative ones.
In this experiment, we set the ac-curacy threshold ?
at 100%.
As a result, 736 out of2,500 examples (sentences) were judged to be posi-tive.To evaluate the reliability of parses, we take ad-vantage of the following features that can be relatedto the difficulty of sentences.sentence length: The longer the sentence is, thepoorer the parser performs (McDonald and Nivre,2007).
We determine sentence length by the numberof words.dependency lengths: Long-distance dependen-cies exhibit bad performance (McDonald and Nivre,2007).
We calculate the average of the dependencylength of each word.difficulty of vocabulary: It is hard for super-vised parsers to learn dependencies that include low-frequency words.
We count word frequencies in thetraining data and make a word list in descending or-der of frequency.
For a given sentence, we calculatethe average frequency rank of each word.number of unknown words: Similarly, depen-dency accuracy for unknown words is notoriouslypoor.
We count the number of unknown words in agiven sentence.number of commas: Sentences with multiplecommas are difficult to parse.
We count the num-ber of commas in a given sentence.number of conjunctions (and/or): Sentenceswith coordinate structures are also difficult to parse(Kurohashi and Nagao, 1994).
We count the num-ber of coordinate conjunctions (and/or) in a givensentence.To apply these features to SVMs in practice, thenumbers are binned at a certain interval for each fea-ture.
For instance, the number of conjunctions issplit into four bins: 0, 1, 2 and more than 2.711Table 2: Example BIO sentences judged as reliable.
The underlined words have incorrect modifying heads.dep.
accuracy sentences judged as reliable12/12 (100%) No mutations resulting in truncation of the APC protein were found .12/13 (92%) Conventional imaging techniques did not show two in 10 of these patients .6/6 (100%) Pancreatic juice was sampled endoscopically .11/12 (92%) The specificity of p53 mutation for pancreatic cancer is very high .9/10 (90%) K-ras mutations are early genetic changes in colon cancer .010203040506070809010080  82  84  86  88  90  92  94  96  98  100Sentence coverage (%)Dependency accuracy (%)Figure 1: Accuracy-coverage curve on BIO rel dev.4.3 Experiments on Detecting Reliable ParsesWe conducted an experiment on detecting the reli-ability of parses.
Our detector was applied to theautomatic parses of BIO rel dev, and only reliableparses were selected from them.
When parsing thisset, the POS tags contained in the set were substi-tuted with automatic POS tags because it is prefer-able to have the same environment as when applyingthe parser to unlabeled data.We evaluated unlabeled dependency accuracy ofthe extracted parses.
The accuracy-coverage curveshown in Figure 1 was obtained by changing the softmargin parameter C 5 of SVMs from 0.0001 to 10.In this figure, the coverage is the ratio of selectedsentences out of all the sentences (200 sentences),and the accuracy is unlabeled dependency accuracy.A coverage of 100% indicates that the accuracy of200 sentences without any selection was 80.85%.If the soft margin parameter C is set to 0.001,we can obtain 19 sentences out of 200 at a depen-dency accuracy of 93.85% (183/195).
The averagesentence length was 10.3 words.
Out of obtained19 sentences, 14 sentences achieved a dependencyaccuracy of 100%, and thus the precision of the reli-ability detector itself was 73.7% (14/19).
Out of 200sentences, 36 sentences were correctly parsed by the5A higher soft margin value allows more classification er-rors, and thus leads to the increase of recall and the decrease ofprecision.base parser, and thus the recall is 38.9% (14/36).Table 2 shows some sentences that were evaluatedas reliable using the above setting (C = 0.001).
Ma-jor errors were caused by prepositional phrase (PP)-attachment.
To improve the accuracy of detectingreliable parses, it would be necessary to consider thenumber of PP-attachment ambiguities in a given sen-tence as a feature.5 Domain Adaptation of DependencyParsingFor domain adaptation, we adopt a self-trainingmethod.
We combine in-domain unlabeled (auto-matically labeled) data with out-of-domain labeleddata to make a training set.
There are many possiblemethods for combining unlabeled and labeled data(Daume?
III, 2007), but we simply concatenate unla-beled data with labeled data to see the effectivenessof the selected reliable parses.
The in-domain unla-beled data to be added are selected by the reliabilitydetector.
We set the soft margin parameter at 0.001to extract highly reliable parses.
As mentioned inthe previous section, the accuracy of selected parseswas approximately 94%.We parsed the unlabeled sentences of BIO andCHEM (approximately 400K sentences for each) us-ing the base dependency parser that is trained on theentire PTB labeled data.
Then, we applied the reli-ability detector to these parsed sentences to obtain31,266 sentences for BIO and 31,470 sentences forCHEM.
We call the two sets of obtained sentences?BIO pool?
and ?CHEM pool?.For each training set of the experiments describedbelow, a certain number of sentences are randomlyselected from the pool and combined with the entireout-of-domain (PTB) labeled data.5.1 Experiment on BIO Development DataWe first conducted an experiment of domain adapta-tion using the BIO development set.7128383.58484.5850  5000  10000  15000  20000  25000Accuracy (%)Number of Unlabeled Sentencesreliable parsesrandomly selected parseswithout additionFigure 2: Dependency accuracies on BIO when thenumber of added unlabeled data is changed.Figure 2 shows how the accuracy changes whenthe number of added reliable parses is changed.
Thesolid line represents our proposed method, and thedotted line with points represents a baseline method.This baseline is a self-training method that simplyadds unlabeled data without selection to the PTBlabeled data.
Each experimental result is the aver-age of five trials done to randomly select a certainnumber of parses from the BIO pool.
The horizontaldotted line (84.07%) represents the accuracy of theparser without adding unlabeled data (trained onlyon the PTB labeled data).From this figure, we can see that the proposedmethod always outperforms the baseline by approxi-mately 0.4%.
The best accuracy was achieved when18,000 unlabeled parses were added.
However, ifmore than 18,000 sentences are added, the accuracydeclines.
This can be attributed to the balance of thenumber of labeled data and unlabeled data.
Sincethe number of added unlabeled data is more thanthe number of labeled data, the entire training setmight be unreliable, though the accuracy of addedunlabeled data is relatively high.
To address thisproblem, it is necessary to weigh labeled data orto change the way information from acquired unla-beled data is handled.5.2 Experiment on CHEM Test DataThe addition of 18,000 sentences showed the high-est accuracy for the BIO development data.
To adaptthe parser to the CHEM test set, we used 18,000 reli-able unlabeled sentences from the CHEM pool withthe PTB labeled sentences to train the parser.
Ta-ble 3 lists the experimental results.
In this table, theTable 3: Experimental results on CHEM test data.system accuracyPTB+unlabel (18,000 sents.)
84.12only PTB (baseline) 83.581st (Sagae and Tsujii, 2007) 83.422nd (Dredze et al, 2007) 83.383rd (Attardi et al, 2007) 83.08third row lists the three highest scores of the domainadaptation track of the CoNLL 2007 shared task.The baseline parser was trained only on the PTBlabeled data (as described in Section 1).
The pro-posed method (PTB+unlabel (18,000 sents.))
out-performed the baseline by approximately 0.5%, andalso beat all the systems submitted to the domainadaptation track.
These systems include an en-semble method (Sagae and Tsujii, 2007) and anapproach of tree revision learning with a selec-tion method of only using short training sentences(shorter than 30 words) (Attardi et al, 2007).6 Discussion and ConclusionThis paper described a method for detecting reliableparses out of the outputs of a single dependencyparser.
This technique was also applied to domainadaptation of dependency parsing.To extract reliable parses, we did not adopt an en-semble method, but used a single-parser approachbecause speed and efficiency are important in pro-cessing a gigantic volume of text to benefit knowl-edge acquisition.
In this paper, we employed theMSTParser, which can process 3.9 sentences/s on aXEON 3.0GHz machine in spite of the time com-plexity of O(n3).
If greater efficiency is required,it is possible to apply a pre-filter that removes longsentences (e.g., longer than 30 words), which areseldom selected by the reliability detector.
In ad-dition, our method does not depend on a particu-lar parser, and can be applied to other state-of-the-art parsers, such as Malt Parser (Nivre et al, 2006),which is a feature-rich linear-time parser.In general, it is very difficult to improve the accu-racy of the best performing systems by using unla-beled data.
There are only a few successful studies,such as (Ando and Zhang, 2005) for chunking and(McClosky et al, 2006a; McClosky et al, 2006b) onconstituency parsing.
We succeeded in boosting theaccuracy of the second-order MST parser, which is713a state-of-the-art dependency parser, in the CoNLL2007 domain adaptation task.
This was a difficultchallenge as many participants in the task failed toobtain any meaningful gains from unlabeled data(Dredze et al, 2007).
The key factor in our successwas the extraction of only reliable information fromunlabeled data.However, that improvement was not satisfactory.In order to achieve more gains, it is necessary to ex-ploit a much larger number of unlabeled data.
In thispaper, we adopted a simple method to combine un-labeled data with labeled data.
To use this methodmore effectively, we need to balance the labeled andunlabeled data very carefully.
However, this methodis not scalable because the training time increasessignificantly as the size of a training set expands.
Wecan consider the information from more unlabeleddata as features of machine learning techniques.
An-other approach is to formalize a probabilistic modelbased on unlabeled data.ReferencesRie Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method for text chunking.
In Proceed-ings of ACL2005, pages 1?9.Giuseppe Attardi and Massimiliano Ciaramita.
2007.
Tree re-vision learning for dependency parsing.
In Proceedings ofNAACL-HLT2007, pages 388?395.Giuseppe Attardi, Felice Dell?Orletta, Maria Simi, AtanasChanev, and Massimiliano Ciaramita.
2007.
Multilingualdependency parsing and domain adaptation using DeSR.
InProceedings of EMNLP-CoNLL2007, pages 1112?1118.Eric Brill.
1995.
Transformation-based error-driven learningand natural language processing.
Computational Linguis-tics, 21(4):543?565.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
In Pro-ceedings of ACL2005, pages 173?180.Michael Collins and Terry Koo.
2005.
Discriminative rerank-ing for natural language parsing.
Computational Linguistics,31(1):25?69.Hal Daume?
III.
2007.
Frustratingly easy domain adaptation.
InProceedings of ACL2007, pages 256?263.Mark Dredze, John Blitzer, Partha Pratim Talukdar, KuzmanGanchev, Joa?o V. Grac?a, and Fernando Pereira.
2007.
Frus-tratingly hard domain adaptation for dependency parsing.
InProceedings of EMNLP-CoNLL2007, pages 1051?1055.Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit, Bea?taMegyesi, Mattias Nilsson, and Markus Saers.
2007.
Singlemalt or blended?
a study in multilingual parser optimization.In Proceedings of EMNLP-CoNLL2007, pages 933?939.Daisuke Kawahara and Sadao Kurohashi.
2006.
A fully-lexicalized probabilistic model for Japanese syntactic andcase structure analysis.
In Proceedings of HLT-NAACL2006,pages 176?183.Sadao Kurohashi and Makoto Nagao.
1994.
A syntactic anal-ysis method of long Japanese sentences based on the detec-tion of conjunctive structures.
Computational Linguistics,20(4):507?534.David McClosky, Eugene Charniak, and Mark Johnson.
2006a.Effective self-training for parsing.
In Proceedings of HLT-NAACL2006, pages 152?159.David McClosky, Eugene Charniak, and Mark Johnson.
2006b.Reranking and self-training for parser adaptation.
In Pro-ceedings of COLING-ACL2006, pages 337?344.Ryan McDonald and Joakim Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
In Pro-ceedings of EMNLP-CoNLL2007, pages 122?131.Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006.Multilingual dependency analysis with a two-stage discrim-inative parser.
In Proceedings of CoNLL-X, pages 216?220.TerukoMitamura, Eric Nyberg, and Jaime Carbonell.
1991.
Anefficient interlingua translation system for multi-lingual doc-ument production.
In Proceedings of MT Summit III, pages55?61.Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto.
2002.
Re-vision learning and its application to part-of-speech tagging.In Proceedings of ACL2002, pages 497?504.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?l sen Eryi git, andSvetoslav Marinov.
2006.
Labeled pseudo-projective de-pendency parsing with support vector machines.
In Proceed-ings of CoNLL-X, pages 221?225.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007.
TheCoNLL 2007 shared task on dependency parsing.
In Pro-ceedings of EMNLP-CoNLL2007, pages 915?932.Adwait Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of EMNLP1996,pages 133?142.Roi Reichart and Ari Rappoport.
2007a.
An ensemble methodfor selection of high quality parses.
In Proceedings ofACL2007, pages 408?415.Roi Reichart and Ari Rappoport.
2007b.
Self-training forenhancement and domain adaptation of statistical parserstrained on small datasets.
In Proceedings of ACL2007, pages616?623.Kenji Sagae and Alon Lavie.
2006.
Parser combination byreparsing.
In Proceedings of the Companion Volume to HLT-NAACL2006, pages 129?132.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency parsingand domain adaptation with LR models and parser ensem-bles.
In Proceedings of EMNLP-CoNLL2007, pages 1044?1050.Gertjan van Noord.
2007.
Using self-trained bilexical prefer-ences to improve disambiguation accuracy.
In Proceedingsof IWPT2007, pages 1?10.Kazuhiro Yoshida, Yoshimasa Tsuruoka, Yusuke Miyao, andJun?ichi Tsujii.
2007.
Ambiguous part-of-speech taggingfor improving accuracy and domain portability of syntacticparsers.
In Proceedings of IJCAI-07, pages 1783?1788.714
