Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 993?1001,Beijing, August 2010Corpus-based Semantic Class Mining:Distributional vs. Pattern-Based ApproachesShuming Shi1    Huibin Zhang2*    Xiaojie Yuan2    Ji-Rong Wen11 Microsoft Research Asia2 Nankai University{shumings, jrwen}@microsoft.comzhanghuibin@126.com; yuanxj@nankai.edu.cnAbstractMain approaches to corpus-based seman-tic class mining include distributionalsimilarity (DS) and pattern-based (PB).In this paper, we perform an empiricalcomparison of them, based on a publiclyavailable dataset containing 500 millionweb pages, using various categories ofqueries.
We further propose a frequency-based rule to select appropriate approach-es for different types of terms.1 Introduction1Computing the semantic relationship betweenterms, which has wide applications in naturallanguage processing and web search, has been ahot topic nowadays.
This paper focuses on cor-pus-based semantic class mining (Lin 1998; Pan-tel and Lin 2002; Pasca 2004; Shinzato andTorisawa, 2005; Ohshima, et al, 2006; Zhang etal., 2009), where peer terms (or coordinate terms)are discovered from a corpus.Existing approaches to semantic class miningcould roughly be divided into two categories:distributional similarity (DS), and pattern-based(PB).
The first type of work (Hindle, 1990; Lin1998; Pantel and Lin 2002) is based on the distri-butional hypothesis (Harris, 1985), saying thatterms occurring in analogous (lexical or syntactic)contexts tend to be similar.
DS approaches basi-cally exploit second-order co-occurrences to dis-cover strongly associated concepts.
In pattern-based approaches (Hearst 1992; Pasca 2004;Shinzato and Torisawa, 2005; Ohshima, et al,2006; Zhang et al, 2009), patterns are applied to* Work done during an internship at Microsoftdiscover specific relationships between terms,from the general first-order co-occurrences.
Forexample, ?NP such as NP, NP?, and NP?
is apopular and high-quality pattern for extractingpeer terms (and also hyponyms).
Besides the nat-ural language patterns, some HTML tag tree pat-terns (e.g., the drop down list) are also effectivein semantic class mining.It is worth-noting that the word ?pattern?
alsoappears in some DS approaches (Pasca et al,2006; Tanev and Magnini, 2006; Pennacchiottiand Pantel, 2009), to represent the context of aterm or a term-pair, e.g., ?
(invent, subject-of)?for the term ?Edison?, and ?- starring -?
for theterm-pair ?
(The Terminal, Tom Hanks)?.
Alt-hough ?patterns?
are utilized, we categorize themas DS approaches rather than PB, because theymatch the DS framework well.
In this paper, PBonly refers to the approaches that utilize patternsto exploit first-order co-occurrences.
And thepatterns in DS approaches are called contexts inthe following part of this paper.Progress has been made and promising resultshave been reported in the past years for both DSand PB approaches.
However, most previous re-search work (some exceptions are discussed inrelated work) involves solely one category of ap-proach.
And there is little work studying thecomparison of their performance for differenttypes of terms (we use ?term?
to represent a sin-gle word or a phrase).In this paper, we make an empirical study ofthis problem, based on a large-scale, publiclyavailable dataset containing 500 million webpages.
For each approach P, we build a term-similarity graph G(P), with vertices representingterms, and edges being the confidence that thetwo terms are peers.
Approaches are comparedby the quality of their corresponding term graphs.993We measure the quality of a term graph by setexpansion.
Two query sets are adopted: One con-tains 49 semantic classes of named entities and20220 trials (queries), collected by Pantel et al(2009) from Wikipedia2; and the other contains100 queries of five lexical categories (propernouns, common nouns, verbs, adjectives, andadverbs), built in this paper for studying the per-formance comparison on different term types.With the dataset and the query sets, we study thecomparison of DS and PB.
Key observations andpreliminary conclusions are,?
DS vs. PB: DS approaches perform muchbetter on common nouns, verbs, adjectives,and adverbs; while PB generates higher-quality semantic classes for proper nouns.?
Lexical vs. Html-tag patterns: If only lexi-cal patterns are adopted in PB, the perfor-mance drops significantly; while the perfor-mance only becomes slightly worse with onlyHtml-tag patterns being included.?
Corpus-size: For proper nouns, PB beatsDS even based on a much smaller corpus;similarly, for other term types, DS performsbetter even with a smaller corpus.Given these observations, we further study thefeasibility of selecting appropriate approaches fordifferent term types to obtain better results.
Asimple and effective frequency-based rule is pro-posed for approach-selection.
Our online seman-tic mining system (NeedleSeek)3 adopts both PBand DS to build semantic classes.2 Related WorkExisting efforts for semantic class mining hasbeen done upon various types of data sources,including text-corpora, search-results, and querylogs.
In corpus-based approaches (Lin 1998; Linand Pantel 2001; Pantel and Lin 2002; Pasca2004; Zhang et al, 2009), semantic classes areobtained by the offline processing of a corpuswhich can be unstructured (e.g., plain text) orsemi-structured (e.g., web pages).
Search-results-based approaches (Etzioni et al, 2004; Kozarevaet al, 2008; Wang and Cohen, 2008) assume thatmultiple terms (or, less often, one term) in a se-mantic class have been provided as seeds.
Otherterms in the class are retrieved by sending queries2 http://www.wikipedia.org/3 http://needleseek.msra.cn/(constructed according to the seeds) to a websearch engine and mining the search results.
Que-ry logs are exploited in (Pasca 2007; Komachiand Suzuki, 2008; Yamaguchi 2008) for semanticclass mining.
This paper focuses on corpus-basedapproaches.As has been mentioned in the introductionpart, primarily two types of methodologies areadopted: DS and PB.
Syntactic context infor-mation is used in (Hindle, 1990; Ruge, 1992; Lin1998; Lin and Pantel, 2001; Pantel and Lin, 2002)to compute term similarities.
The construction ofsyntactic contexts requires sentences to be parsedby a dependency parser, which may be extremelytime-consuming on large corpora.
As an alterna-tive, lexical context (such as text window) hasbeen studied (Pantel et al, 2004; Agirre et al,2009; Pantel et al, 2009).
In the pattern-basedcategory, a lot of work has been done to discoverterm relations by sentence lexical patterns(Hearst 1992; Pasca 2004), HTML tag patterns(Shinzato and Torisawa, 2005), or both (Shi et al,2008; Zhang et al, 2009).
In this paper, our focusis not one specific methodology, but the compari-son and combination of them.A small amount of existing work is related tothe comparison or combination of multiple meth-ods.
Pennacchiotti and Pantel (2009) proposed afeature combination framework (named ensemblesemantic) to combine features generated by dif-ferent extractors (distributional and ?pattern-based?)
from various data sources.
As has beendiscussed in the introduction, in our terminology,their ?pattern-based?
approaches are actually DSfor term-pairs.
In addition, their study is based onthree semantic classes (actors, athletes, and musi-cians), all of which are proper nouns.
Differently,we perform the comparison by classifying termsaccording to their lexical categories, based onwhich additional insights are obtained about thepros and cons of each methodology.
Pantel et al,(2004) proposed, in the scenario of extracting is-a relations, one pattern-based approach and com-pared it with a baseline syntactic distributionalsimilarity method (called syntactic co-occurrencein their paper).
Differently, we study the compar-ison in a different scenario (semantic class min-ing).
In addition, they did not differentiate thelexical types of terms in the study.
The third dif-ference is that we proposed a rule for method-selection while they did not.
In (Pasca and Durme,9942008), clusters of distributional similar termswere adopted to expand the labeled semanticclasses acquired from the ?such as | including?pattern.
Although both patterns and distributionalsimilarity were used in their paper, they did notdo any comparison about their performance.Agirre et al (2009) compared DS approacheswith WordNet-based methods in computing wordsimilarity and relatedness; and they also studiedthe combination of them.
Differently, the meth-ods for comparison in our paper are DS and PB.3 Similarity Graph ConstructionA key operation in corpus-based semantic classmining is to build a term similarity graph, withvertices representing terms, and edges being thesimilarity (or distance) between terms.
Given thegraph, a clustering algorithm can be adopted togenerate the final semantic classes.
Now we de-scribe the state-of-the-art DS and PB approachesfor computing term similarities.3.1 Distributional SimilarityDS approaches are based on the distributionalhypothesis (Harris, 1985), which says that termsappearing in analogous contexts tend to be simi-lar.
In a DS approach, a term is represented by afeature vector, with each feature corresponding toa context in which the term appears.
The similari-ty between two terms is computed as the similari-ty between their corresponding feature vectors.Different approaches may have different ways of1) defining a context, 2) assigning feature values,or 3) measuring the similarity between two fea-ture vectors.ContextsText window (window size: 2, 4)SyntacticFeature value PMISimilarity measure Cosine, JaccardTable 1.
DS approaches implemented in this paperMainly two kinds of contexts have been exten-sively studied: syntactic context and lexical con-text.
The construction of syntactic contexts relieson the syntactic parsing trees of sentences, whichare typically the output of a syntactic parser.
Giv-en a syntactic tree, a syntactic context of a term wcan be defined as the parent (or one child) of w inthe tree together with their relationship (Lin,1998; Pantel and Lin, 2002; Pantel et al, 2009).For instance, in the syntactic tree of sentence?this is an interesting read for anyone studyinglogic?, one context of the word ?logic?
can bedefined as ?study V:obj:N?.
In this paper, weadopt Minipar (Lin, 1994) to parse sentences andto construct syntactic trees.One popular lexical context is text window,where a context c for a term w in a sentence S isdefined as a substring of the sentence containingbut removing w. For example, for sentence??w1w2w3ww4w5w6?
?, a text window context(with size 4) of w can be ?w2w3w4w5?.
It is typi-cally time-consuming to construct the syntactictrees for a large-scale dataset, even with a light-weight syntactic parser like Minipar.
The con-struction of lexical contexts is much more effi-cient because it does not require the syntacticdependency between terms.
Both contexts arestudied in this paper.After defining contexts for a term w, the nextstep is to construct a feature vector for the term:F(w)=(fw1, fw2?, fw,m), where m is the number ofdistinct contexts, and fw,c is the feature value ofcontext c with respect to term w. Among all theexisting approaches, the dominant way of assign-ing feature values (or context values) is compu-ting the pointwise mutual information (PMI) be-tween the feature and the term,(3.1)where F(w,c) is the frequency of context c occur-ring for term w, F(w,*) is the total frequency ofall contexts for term w, F(*,c) is the frequency ofcontext c for all terms, and F(*,*) is the total fre-quency of all context for all terms.
They are cal-culated as follows respectively,???
?
(3.2)where m and n are respectively the distinct num-bers of contexts and terms.Following state-of-the-art, we adopt PMI inthis paper for context weighting.Given the feature vectors of terms, the simi-larity of any two terms is naturally computed asthe similarity of their corresponding feature vec-tors.
Cosine similarity and Jaccard similarity(weighted) are implemented in our experiments,?
??????(3.3)995?
???
?
?
(3.4)Jaccard similarity is finally used in presentingour experimental results (in Section 6), because itachieves higher performance.3.2 Pattern-based ApproachesIn PB approaches, a list of carefully-designed (orautomatically learned) patterns is exploited andapplied to a text collection, with the hypothesisthat the terms extracted by applying each of thepatterns to a specific piece of text tend to be simi-lar.
Two categories of patterns have been studiedin the literature: sentence lexical patterns, andHTML tag patterns.
Table-2 lists some popularpatterns utilized in existing semantic class miningwork (Heast 1992; Pasca 2004; Kozareva et al,2008; Zhang et al, 2009).
In the table, ?T?
meansa term (a word or a phrase).
Exactly the same setof patterns is employed in implementing our pat-tern-based approaches in this paper.Type PatternLexicalT {, T}*{,} (and|or) {other} T(such as | including) T (and|,|.
)T, T, T {,T}*Tag<ul>  <li> T </li>  ?
<li> T </li>  </ul><ol> <li> T </li> ?
<li> T </li> </ol><select> <option> T ?<option> T </select><table>  <tr> <td> T </td> ?
<td> T </td> </tr> ... </table>Other Html-tag repeat patternsTable 2.
Patterns employed in this paper (Lexical:sentence lexical patterns; Tag: HTML tag patterns)We call the set of terms extracted by applyinga pattern one time as a raw semantic class(RASC).
The term similarity graph needs to bebuilt by aggregating the information of the ex-tracted RASCs.One basic idea of estimating term similarity isto count the number of RASCs containing both ofthem.
This idea is extended in the state-of-the-artapproaches (Zhang et al, 2009) to distinguish thereliability of different patterns and to punish termsimilarity contributions from the same domain(or site), as follows,?
?
(3.5)where Ci,j is a RASC containing both term a andterm b, P(Ci,j) is the pattern via which the RASCis extracted, and w(P) is the weight of pattern P.The above formula assumes all these RASCs be-long to m sites (or domains) with Ci,j extractedfrom a page in site i, and ki being the number ofRASCs corresponding to site i.In this paper, we adopt an extension of theabove formula which considers the frequency ofa single term, as follows,Sim*(a, b) = Sim(a, b)  ?
(3.6)where IDF(a)=log(1+N/N(a)), N is the total num-ber of RASCs, and N(a) is the number of RASCscontaining a.
In the experiments, we simply setthe weight of every pattern type to be the samevalue (1.0).4 Compare PB and DSWe compare PB and DS by the quality of theterm similarity graphs they generated.
The quali-ty of a term graph is measured by set expansion:Given a list of seed terms (e.g., S={lent, epipha-ny}) belonging to a semantic class, our task is tofind other members of this class, such as advent,easter, and christmas.In this section, we first describe our set expan-sion algorithm adopted in our study.
Then DSand PB are compared in terms of their set-expansion performance.
Finally we discuss waysof selecting appropriate approaches for differenttypes of seeds to get better expansion results.4.1 Set Expansion AlgorithmHaving at hand the similarity graph, set expan-sion can be implemented by selecting the termsmost similar to the seeds.
So given a queryQ={s1, s2, ?, sk}, the key is to compute       ,the similarity between a term t and the seed-setQ.
Naturally, we define it as the weighted aver-age similarity between t and every seed in Q,?
(4.1)where   is the weight of seed   , which can be aconstant value, or a function of the frequency ofterm    in the corpus.
Although Formula 3.6 canbe adopted directly for calculating Sim(t,si), weuse the following rank-based formula because itgenerate better expansion results.
(4.2)where         is the rank of term t among theneighbors of   .In our experiments, we fix  =1 and  =10.9964.2 Compare DS with PBIn order to have a comprehensive comparison ofthe two approaches, we intentionally chooseterms of diverse types and do experiments basedon various data scales.
We classify terms into 5types by their lexical categories: proper nouns,common nouns, verbs, adjectives, and adverbs.The data scales for experiments are from one mil-lion to 500 million web pages.
Please refer tosections 5.1 and 5.2 for more details about thecorpora and seeds used for experiments.Experimental results (refer to Section 6) willshow that, for proper nouns, the ranking of ap-proaches (in terms of performance) is:PB > PB-HtmlTag > DS  PB-LexicalWhile for common nouns, verbs, adjectives,and adverbs, we have:DS > PBHere ?PB-lexical?
means only the lexical pat-terns of Table 2 are adopted.
Similarly, ?PB-HtmlTag?
represents the PB approach with onlyHtml-tag patterns being utilized.Please pay attention that this paper by nomeans covers all PB or DS approaches (althoughwe have tried our best to include the most popu-lar ones).
For PB, there are of course other kindsof patterns (e.g., patterns based on deeper linguis-tic analysis).
For DS, other types of contexts mayexist in addition to those listed in Table 1.
So ininterpreting experimental results, making obser-vations, and drawing preliminary conclusions, weonly means the patterns in Table 2 for PB andTable 1 for DS.
It will be an interesting futurework to include more DS and PB approaches inthe study.In order to understand why PB performs sowell in dealing with proper nouns while so badlyfor other term categories, we calculated the fre-quency of each seed term in the extracted RASCs,the output of the pattern-matching algorithm.
Wedefine the normalized frequency of a term to beits frequency in the RASCs divided by the fre-quency in the sentences of the original documents(with duplicate sentences merged).
Then we de-fine the mean normalized frequency (MNF) of aseed set S, as follows,?
(4.3)where Fnorm(t) is the normalized frequency of t.The MNF values for the five seed sets arelisted in Table 3, where we can see that propernouns have the largest MNF values, followed bycommon nouns.
In other words, the patterns inTable 2 capture the relations of more propernouns than other term categories.Seed Categories Terms MNFProper nouns 40 0.2333Common nouns 40 0.0716Verbs 40 0.0099Adjectives 40 0.0126Adverbs 40 0.0053Table 3.
MNF values of different seed categoriesAs mentioned in the introduction, the PB andDS approaches we studied capture first-order andsecond-order term co-occurrences respectively.Some existing work (e.g., Edmonds, 1997)showed that second-order co-occurrence leads tobetter results for detecting synonymy.
Consider-ing that a high proportion of coordinate terms ofverbs, adjectives, and adverbs are their synonymsand antonyms, it is reasonable that DS behavesbetter for these term types because it exploits se-cond-order co-occurrence.
For PB, different fromthe standard way of dealing with first-order co-occurrences where statistics are performed on allpairs of near terms, a subset of co-occurred termsare selected in PB by specific patterns.
The pat-terns in Table-2 help detecting coordinate propernouns, because they are frequently occurred to-gether obeying the patterns in sentences or webpages.
But it is not the case for other term types.It will be interesting to study the performance ofPB when more pattern types are added.4.3 Approach SelectionHaving observed that the two approaches per-form quite differently on every type of querieswe investigated, we hope we can improve theexpansion performance by smartly selecting anapproach for each query.
In this section, we pro-pose and study several approach-selection meth-ods, by which we hope to gain some insightsabout the possibility and effectiveness of combin-ing DS and PB for better set expansion.Oracle selection: In order to get an insightabout the upper bound that we could obtain whencombing the two methods, we implement an ora-cle that chooses, for each query, the approachthat generates better expansion results.997Frequency-based selection: It is shown inTable 3 that the mean normalized frequency ofproper nouns is much larger than other terms.Motivated by this observation, we select a setexpansion methodology for each query as fol-lows: Select PB if the normalized frequency val-ues of all terms in the query are larger than 0.1;otherwise choose DS.We demonstrate, in Section 6.3, the effective-ness of the above selection methods.5 Experimental Setup5.1 Dataset and Exp.
EnvironmentWe adopt a public-available dataset in our exper-iments: ClueWeb094.
This is a very large datasetcollected by Carnegie Mellon University in early2009 and has been used by several tracks of theText Retrieval Conference (TREC)5.
The wholedataset consists of 1.04 billion web pages in tenlanguages while only those in English, about 500million pages, are used in our experiments.
Thereason for selecting such a dataset is twofold:First, it is a corpus large enough for conductingweb-scale experiments and getting meaningfulresults.
Second, since it is publicly available, it ispossible for other researchers to reproduce theexperiments in the paper.CorporaDocs(millions)Sentences(millions)DescriptionClue500 500 13,000 All En pages in ClueWeb09Clue050  50   1,600 ClueWeb09 category BClue010  10      330 Sampling from Clue050Clue001   1       42 Sampling from Clue050Table 4.
Corpora used in experimentsTo test the impact of corpus size on set expan-sion performance, four corpora are derived fromthe dataset, as outlined in Table 4.
The Clue500corpus contains all the 500 million English webpages in the dataset; while Clue050 is a subset ofClueWeb09 (named category B) containing 50million English web pages.
The remaining twocorpora are respectively the 1/5 and 1/50 randomsampling of web pages from Clue050.Documents in the corpora are stored and pro-cessed in a cluster of 40 four-core machines.4 http://boston.lti.cs.cmu.edu/Data/clueweb09/5 http://trec.nist.gov/5.2 Query SetsWe perform our study using two query sets.WikiGold: It was collected by Pantel et al(2009) from the ?List of?
pages in Wikipedia andused as the gold standard in their paper.
This goldstandard consists of 49 entity sets, and 20220 tri-als (used as queries) of various numbers of seeds.Most seeds in the query set are named entities.Please refer to Pantel et al (2009) for details ofthe gold standard.Mix100: This query set consists of 100 queriesin five categories: verbs, adjectives, adverbs,common nouns, and proper nouns.
There are 20queries in every category and two seeds in everyquery.
The query set was built by the followingsteps: First, 20 terms of each category were ran-domly selected from a term list (which is con-structed by part-of-speech tagging the Clue050corpus and removing low-frequency terms), andwere treated as the first seed of the each query.Then, we manually added one additional seed foreach query.
The reason for utilizing two seedsinstead of one is the observation that a large por-tion of the terms selected in the previous step be-long to multiple categories.
For example, ?color-ful?
is both an adjective and a proper noun (aJapanese manga).5.3 Results LabelingNo human labeling efforts are needed for the ex-pansion results of the WikiGold query set.
Everyreturned term is automatically judged to be?Good?
(otherwise ?Bad?)
if it appears in thecorresponding gold standard entity set.For Mix100, the search results of various ap-proaches are merged and labeled by three humanlabelers.
Each labeler assigns each term in thesearch results a label of ?Good?, ?Fair?
or ?Bad?.The labeling agreement values (measured by per-centage agreement) between labelers I and II, Iand III, II and III are respectively 0.82, 0.81, and0.81.
The ultimate judgment of each result termis obtained from the three labelers by majorityvoting.
In the case of three labelers giving mutu-ally different results (i.e., one ?Good?, one ?Fair?and one ?Bad?
), the ultimate judgment is set to?Fair?
(the average).5.4 Evaluation MetricsAfter removing seeds from the expansion results,we adopt the following metrics to evaluate the998results of each query.
The evaluation score on aquery set is the average over all the queries.Precision@k: The percentage of relevant(good or fair) terms in the top-k expansion results(terms labeled as ?Fair?
are counted as 0.5)Recall@k: The ratio of relevant terms in thetop-k results to the total number of relevant termsR-Precision: Precision@R where R is the totalnumber of terms labeled as ?Good?Mean average precision (MAP): The averageof precision values at the positions of all good orfair results6 Experimental Results6.1 Overall Performance ComparisonTable 5 lists the performance (measured byMAP, R-precision, and the precisions at ranks 25,50, and 100) of some key approaches on corpusClue050 and query set WikiGold.
The results ofquery set Mix100 are shown in Table 6.
In theresults, TWn represents the DS approach withtext-window of size n as contexts, Syntactic is theDS approach with syntactic contexts, PB-Lexicalmeans only the lexical patterns of Table 2 areadopted, and PB-HtmlTag represents the PB ap-proach with only Html-tag patterns utilized.Approach MAP R-Prec P@25 P@50 P@100TW2 0.218 0.287 0.359 0.278 0.204TW4 0.152 0.210 0.325 0.244 0.173Syntactic 0.170 0.247 0.314 0.242 0.178PB-Lexical 0.227 0.276 0.352 0.272 0.190PB-HtmlTag 0.354 0.417 0.513 0.413 0.311PB 0.362 0.424 0.520 0.418 0.314Pantel-24M N/A 0.264 0.353 0.298 0.239Pantel-120M N/A 0.356 0.377 0.319 0.250Pantel-600M N/A 0.404 0.407 0.347 0.278Table 5.
Performance comparison on the Clue050 cor-pus (query set: WikiGold)It is shown that PB gets much higher evalua-tion scores than other approaches on the WikiG-old query set and the proper-nouns category ofMix100.
While for other seed categories inMix100, TW2 return significantly better results.We noticed that most seeds in WikiGold areproper nouns.
So the experimental results tend toindicate that the performance comparison be-tween state-of-the-art DS and PB approaches de-pends on the types of terms to be mined, specifi-cally, DS approaches perform better in miningsemantic classes of common nouns, verbs, adjec-tives, and adverbs; while state-of-the-art PB ap-proaches are more suitable for mining semanticclasses of proper nouns.
The performance of PBis low in dealing with other types of terms (espe-cially adverbs).
The performance of PB dropssignificantly if only lexical patterns are used; andthe HtmlTag-only version of PB performs onlyslightly worse than PB.The observations are verified by the precision-recall graph in Figure 1 on Clue500.
The resultsof the syntactic approach on Clue500 are not in-cluded, because it is too time-consuming to parseall the 500 million web pages by a dependencyparser (even using a high-performance parser likeMinipar).
It took overall about 12,000 CPU-hoursto parse all the sentences in Clue050 by Minipar.Query types &ApproachesMAP P@5 P@10 P@20ProperNounsTW2 0.302 0.835 0.810 0.758PB 0.336 0.920 0.838 0.813CommonNounsTW2 0.384 0.735 0.668 0.595PB 0.212 0.640 0.548 0.485VerbsTW2 0.273 0.655 0.543 0.465PB 0.176 0.415 0.373 0.305AdjectivesTW2 0.350 0.655 0.563 0.473PB 0.120 0.335 0.285 0.234AdverbsTW2 0.432 0.605 0.505 0.454PB 0.043 0.100 0.095 0.089Table 6.
Performance comparison on different querytypes (Corpus: Clue050; query set: Mix100)Figure 1.
Precision and recall of various approaches(query set: WikiGold)The methods labeled Pantel-24M etc.
(in Table5 and Figure 1) are the approaches presented in(Pantel et al, 2009) on their corpus (calledWeb04, Web20, and Web100 in the paper) con-taining respectively 24 million, 120 million, and600 million web pages.
Please pay attention thattheir results and ours may not be directly compa-rable, because different corpora and set-00.10.20.30.40.50.60.70 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8RecallPrecisionTW221 (Clue500) PB (Clue500)PB (Clue010) PB (Clue001)Pantel-600M Pantel-120M999expansion algorithms were used.
Their results arelisted here for reference purpose only.6.2 Corpus Size EffectTable 7 shows the performance (measured byMAP) of two approaches on query set Mix100,by varying corpus size.
We observed that the per-formance of TW2 improves rapidly along withthe growth of corpus size from one million to 50million documents.
From Clue050 to Clue500,the performance is slightly improved.Query types &ApproachesClue001 Clue010 Clue050 Clue500ProperNounsTW2 0.209 0.265 0.302 0.311PB 0.355 0.351 0.336 0.327CommonNounsTW2 0.259 0.348 0.384 0.393PB 0.200 0.234 0.212 0.205VerbsTW2 0.224 0.268 0.273 0.278PB 0.101 0.134 0.176 0.148AdjectivesTW2 0.309 0.326 0.350 0.353PB 0.077 0.158 0.120 0.129AdverbsTW2 0.413 0.423 0.432 0.437PB 0.028 0.058 0.043 0.059Table 7.
Effect of different corpus size (query set:Mix100; metric: MAP)For PB, however, the performance change isnot that simple.
For proper nouns, the best per-formance (in terms of MAP) is got on the twosmall corpora Clue001 and Clue010; and thescore does not increase when corpus size grows.Different observations are made on WikiGold(see Figure 1), where the performance improves alot with the data growth from Clue001 toClue010, and then stabilizes (from Clue010 toClue500).
For other term types, the MAP scoresdo not grow much after Clue010.
To our currentunderstanding, the reason may be due to the two-fold effect of incorporating more data in mining:bringing useful information as well as noise.Clue001 contains enough information, which isfully exploited by the PB approach, for expand-ing the proper-nouns in Mix100.
So the perfor-mance of PB on Clue001 is excellent.
The namedentities in WikiGold are relatively rare, whichrequires a larger corpus (Clue010) for extractingpeer terms from.
But when the corpus gets larger,we may not be able to get more useful infor-mation to further improve results quality.Another interesting observation is that, forproper nouns, the performance of PB on Clue001is even much better than that of TW2 on corpusClue500.
Similarly, for other query types (com-mon nous, verbs, adjectives, and adverbs), TW2easily beats PB even with a much smaller corpus.6.3 Approach SelectionHere we demonstrate the experimental results ofcombining DS and PB with the methods we pro-posed in Section 4.3.
Table 8 shows the combina-tion of PB and TW2 on corpus Clue050 and que-ry set Mix100.
The overall performance relies onthe number (or percentage) of queries in eachcategory.
Two ways of mixing the queries aretested: avg(4:1:1:1:1) and avg(1:1:1:1:1), wherethe numbers are the proportion of proper nouns,common nouns, verbs, adjectives, and adverbs.ApproachAvg (1:1:1:1:1) Avg (4:1:1:1:1)P@5 P@10 P@20 P@5 P@10 P@20TW2 0.697 0.618 0.548 0.749 0.690 0.627PB 0.482 0.428 0.385 0.646 0.581 0.545Oracle 0.759 0.663 0.591 0.836 0.759 0.695Freq-based 0.721 0.633 0.570 0.799 0.723 0.671Table 8.
Experiments of combining both approaches(Corpus: Clue050; query set: Mix100)The expansion performance is improved a lotwith our frequency-based combination method.As expected, oracle selection achieves great per-formance improvement, which shows the largepotential of combining DS and PB.
Similar re-sults (omitted due to space limitations) are ob-served on the other corpora.Our online semantic mining system (Needle-Seek, http://needleseek.msra.cn) adopts both PBand DS for semantic class construction.7 ConclusionWe compared two mainstream methods (DS andPB) for semantic class mining, based on a datasetof 500 million pages and using five term types.We showed that PB is clearly adept at extractingsemantic classes of proper nouns; while DS isrelatively good at dealing with other types ofterms.
In addition, a small corpus is sufficient foreach approach to generate better semantic classesof its ?favorite?
term types than those obtainedby its counterpart on a much larger corpus.
Final-ly, we tried a frequency-based method of com-bining them and saw apparent performance im-provement.1000ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, Aitor Soroa.
A Study onSimilarity and Relatedness Using Distributionaland WordNet-based Approaches.
NAACL-HLT2009.Philip Edmonds.
1997.
Choosing the Word most Typ-ical in Context Using a Lexical Co-OccurrenceNetwork.
ACL'97, pages 507-509.Oren Etzioni, Michael Cafarella, Doug Downey, Stan-ley Kok, Ana-Maria Popescu, Tal Shaked, StephenSoderland, Daniel Weld, and Alexander Yates.2004.
Web-Scale Information Extraction inKnowItAll.
WWW?04, pages 100?110, New York.Zelig S. Harris.
1985.
Distributional Structure.
ThePhilosophy of Linguistics.
New York: Oxford Uni-versity Press.Marti A. Hearst.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
COLING?92,Nantes, France.Donald Hindle.
1990.
Noun Classification from Predi-cate-Argument Structures.
In ACL?90, pages 268?275, Pittsburg, Pennsylvania, June.Mamoru Komachi and Hisami Suzuki.
Minimally Su-pervised Learning of Semantic Knowledge fromQuery Logs.
IJCNLP 2008, pages 358?365, 2008.Zornitsa Kozareva, Ellen Riloff, Eduard Hovy.
2008.Semantic Class Learning from the Web with Hypo-nym Pattern Linkage Graphs.
ACL?08: HLT.Dekang Lin.
1994.
Principar - an Efficient, Broad-Coverage, Principle-based Parser.
COLING?94, pp.482-488.Dekang Lin.
1998.
Automatic Retrieval and Cluster-ing of Similar Words.
COLING-ACL?98, pages768-774.Dekang Lin and Patrick Pantel.
2001.
Induction ofSemantic Classes from Natural Language Text.SIGKDD?01, pages 317-322.Hiroaki Ohshima, Satoshi Oyama and KatsumiTanaka.
2006.
Searching Coordinate Terms withtheir Context from the Web.
WISE?06.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu and Vishnu Vyas.
2009.
Web-ScaleDistributional Similarity and Entity Set Expansion.EMNLP?09.
Singapore.Patrick Pantel and Dekang Lin.
2002.
DiscoveringWord Senses from Text.
SIGKDD'02.Patric Pantel, Deepak Ravichandran, and EduardHovy.
2004.
Towards Terascale Knowledge Acqui-sition.
COLING?04, Geneva, Switzerland.Marius Pasca.
2004.
Acquisition of CategorizedNamed Entities for Web Search.
CIKM?04.Marius Pasca.
2007.
Weakly-Supervised Discovery ofNamed Entities Using Web Search Queries.CIKM?07.
pp.
683-690.Marius Pasca and Benjamin Van Durme.
2008.
Weak-ly-supervised Acquisition of Open-Domain Classesand Class Attributes from Web Documents andQuery Logs.
ACL?08.Marius Pasca, Dekang Lin, Jeffrey Bigham, AndreiLifchits, and Alpa Jain.
2006.
Organizing andSearching the World Wide Web of Facts - StepOne: The One-Million Fact Extraction Challenge.In AAAI?06.Marco Pennacchiotti and Patrick Pantel.
2009.
EntityExtraction via Ensemble Semantics.
EMNLP?09.Gerda Ruge.
1992.
Experiments on Linguistically-Based Term Associations.
Information Processing& Management, 28(3): 317-32.Keiji Shinzato and Kentaro Torisawa.
2005.
A SimpleWWW-based Method for Semantic Word ClassAcquisition.
Recent Advances in Natural LanguageProcessing (RANLP?05), Borovets, Bulgaria.Shuming Shi, Xiaokang Liu, Ji-Rong Wen.
2008.
Pat-tern-based Semantic Class Discovery with Multi-Membership Support.
CIKM?08, Napa Valley, Cali-fornia, USA.Hristo Tanev and Bernardo Magnini.
2006.
WeaklySupervised Approaches for Ontology Population.EACL'2006, Trento, Italy.Richard C. Wang and William W. Cohen.
2008.
Itera-tive Set Expansion of Named Entities Using theWeb.
ICDM?08, pages 1091?1096.Masashi Yamaguchi, Hiroaki Ohshima, Satoshi Oya-ma, and Katsumi Tanaka.
Unsupervised Discoveryof Coordinate Terms for Multiple Aspects fromSearch Engine Query Logs.
The 2008IEEE/WIC/ACM International Conference on WebIntelligence and Intelligent Agent Technology.Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong Wen.
2009.
Employing Topic Models forPattern-based Semantic Class Discovery.
ACL?09,Singapore.1001
