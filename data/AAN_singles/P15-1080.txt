Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 825?835,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsNon-linear Learning for Statistical Machine TranslationShujian Huang, Huadong Chen, Xinyu Dai and Jiajun ChenState Key Laboratory for Novel Software TechnologyNanjing UniversityNanjing 210023, China{huangsj, chenhd, daixy, chenjj}@nlp.nju.edu.cnAbstractModern statistical machine translation(SMT) systems usually use a linear com-bination of features to model the qualityof each translation hypothesis.
The linearcombination assumes that all the featuresare in a linear relationship and constrainsthat each feature interacts with the rest fea-tures in an linear manner, which mightlimit the expressive power of the modeland lead to a under-fit model on the cur-rent data.
In this paper, we propose a non-linear modeling for the quality of transla-tion hypotheses based on neural networks,which allows more complex interactionbetween features.
A learning framework ispresented for training the non-linear mod-els.
We also discuss possible heuristicsin designing the network structure whichmay improve the non-linear learning per-formance.
Experimental results show thatwith the basic features of a hierarchicalphrase-based machine translation system,our method produce translations that arebetter than a linear model.1 IntroductionOne of the core problems in the research of statis-tical machine translation is the modeling of trans-lation hypotheses.
Each modeling method definesa score of a target sentence e = e1e2...ei...eI,given a source sentence f = f1f2...fj...fJ, whereeach eiis the ith target word and fjis the jthsource word.
The well-known modeling methodstarts from the Source-Channel model (Brown etal., 1993)(Equation 1).
The scoring of e decom-poses to the calculation of a translation model anda language model.Pr(e|f) = Pr(e)Pr(f |e)/Pr(f) (1)The modeling method is extended to log-linearmodels by Och and Ney (2002), as shown in Equa-tion 2, where hm(e|f) is the mth feature functionand ?mis the corresponding weight.Pr(e|f) = p?M1(e|f)=exp[?Mm=1?mhm(e|f)]?e?exp[?Mm=1?mhm(e?|f)](2)Because the normalization term in Equation 2 isthe same for all translation hypotheses of the samesource sentence, the score of each hypothesis, de-noted by sL, is actually a linear combination of allfeatures, as shown in Equation 3.sL(e) =M?m=1?mhm(e|f) (3)The log-linear models are flexible to incorpo-rate new features and show significant advantageover the traditional source-channel models, thusbecome the state-of-the-art modeling method andare applied in various translation settings (Yamadaand Knight, 2001; Koehn et al, 2003; Chiang,2005; Liu et al, 2006).It is worth noticing that log-linear models try toseparate good and bad translation hypotheses us-ing a linear hyper-plane.
However, complex inter-actions between features make it difficult to lin-early separate good translation hypotheses frombad ones (Clark et al, 2014).Taking common features in a typical phrase-based (Koehn et al, 2003) or hierarchical phrase-based (Chiang, 2005) machine translation systemas an example, the language model feature favorsshorter hypotheses; the word penalty feature en-courages longer hypotheses.
The phrase trans-lation probability feature selects phrases that oc-curs more frequently in the training corpus, whichsometimes is long with a lower translation proba-bility, as in translating named entities or idioms;825sometimes is short but with a high translationprobability, as in translating verbs or pronouns.These three features jointly decide the choice oftranslations.
Simply use the weighted sum of theirvalues may not be the best choice for modelingtranslations.
As a result, log-linear models mayunder-fit the data.
This under-fitting may preventsthe further improvement of translation quality.In this paper, we propose a non-linear model-ing of translation hypotheses based on neural net-works.
The traditional features of a machine trans-lation system are used as the input to the net-work.
By feeding input features to nodes in a hid-den layer, complex interactions among features aremodeled, resulting in much stronger expressivepower than traditional log-linear models.
(Sec-tion 3)Employing a neural network for SMT model-ing has two issues to be tackled.
The first is-sue is the parameter learning.
Log-linear modelsrely on minimum error rate training (MERT) (Och,2003) to achieve best performance.
When thescoring function become non-linear, the intersec-tion points of these non-linear functions could notbe effectively calculated and enumerated.
ThusMERT is no longer suitable for learning the pa-rameters.
To solve the problem, we present aframework for effective training including severalcriteria to transform the training problem into a bi-nary classification task, a unified objective func-tion and an iterative training algorithm.
(Sec-tion 4)The second issue is the structure of neural net-work.
Single layer neural networks are equivalentto linear models; two-layer networks with suffi-cient nodes are capable of learning any continuousfunction (Bishop, 1995).
Adding more layers intothe network could model complex functions withless nodes, but also brings the problem of van-ishing gradient (Erhan et al, 2009).
We adapt atwo-layer feed-forward neural network to keep thetraining process efficient.
We notice that one ma-jor problem that prevents a neural network trainingreaching a good solution is that there are too manylocal minimums in the parameter space.
Thus wediscuss how to constrain the learning of neural net-works with our intuitions and observations of thefeatures.
(Section 5)Experiments are conducted to compare vari-ous settings and verify the effectiveness of ourproposed learning framework.
Experimental re-sults show that our framework could achieve bettertranslation quality even with the same traditionalfeatures as previous linear models.
(Section 6)2 Related workMany research has been attempting to bring non-linearity into the training of SMT.
These effortscould be roughly divided into the following threecategories.The first line of research attempted to re-interpret original features via feature transforma-tion or additional learning.
For example, Maskeyand Zhou (2012) use a deep belief network tolearn representations of the phrase translation andlexical translation probability features.
Clark etal.
(2014) used discretization to transform real-valued dense features into a set of binary indica-tor features.
Lu et al (2014) learned new fea-tures using a semi-supervised deep auto encoder.These work focus on the explicit representationof the features and usually employ extra learningprocedure.
Our proposed method only takes theoriginal features, with no transformation, as theinput.
Feature transformation or combination areperformed implicitly during the training of the net-work and integrated with the optimization of trans-lation quality.The second line of research attempted to usenon-linear models instead of log-linear models,which is most similar in spirit with our work.
Duhand Kirchhoff (2008) used the boosting methodto combine several results of MERT and achievedimprovement in a re-ranking setting.
Liu etal.
(2013) proposed an additive neural networkwhich employed a two-layer neural network forembedding-based features.
To avoid local min-imum, they still rely on a pre-training and post-training from MERT or PRO.
Comparing to theseefforts, our proposed method takes a further stepthat it is integrated with iterative training, insteadof re-ranking, and works without the help of anypre-trained linear models.The third line of research attempted to addnon-linear features/components into the log-linearlearning framework.
Neural network based mod-els are trained as language models (Vaswani etal., 2013; Auli and Gao, 2014), translation mod-els (Gao et al, 2014) or joint language and transla-tion models (Auli et al, 2013; Devlin et al, 2014).Liu et al (2013) also introduced word embed-ding for source and target sides of the translation826inputhiddenlayeroutputlayerMoMhFigure 1: A two-layer feed-forward neural net-work.rules as local features.
In this paper, we focus onenhancing the expressive power of the modeling,which is independent of the research of enhanc-ing translation systems with new designed fea-tures.
We believe additional improvement couldbe achieved by incorporating more features intoour framework.3 Non-linear TranslationThe non-linear modeling of translation hypothe-ses could be used in both phrase-based system andsyntax-based systems.
In this paper, we take thehierarchical phrase based machine translation sys-tem (Chiang, 2005) as an example and introducehow we fit the non-linearity into the system.3.1 Two-layer Neural NetworksWe employ a two-layer neural network as the non-linear model for scoring translation hypotheses.The structure of a typical two-layer feed-forwardneural network includes an input layer, a hiddenlayer, and a output layer (as shown in Figure 1).We use the input layer to accept input features,the hidden layer to combine different input fea-tures, the output layer with only one node to out-put the model score for each translation hypothesisbased on the value of hidden nodes.
More specifi-cally, the score of hypothesis e, denoted as sN, isdefined as:sN(e) = ?o(Mo?
?h(Mh?hm1(e|f)+bh)+bo) (4)where M , b is the weight matrix, bias vector ofthe neural nodes, respectively; ?
is the activationfunction, which is often set to non-linear functionssuch as the tanh function or sigmoid function; sub-script h and o indicates the parameters of hiddenlayer and output layer, respectively.3.2 FeaturesWe use the standard features of a typical hier-archical phrase based translation system(Chiang,2005).
Adding new features into the framework isleft as a future direction.
The features as listed asfollowing:?
p(?|?)
and p(?|?
): conditional probabilityof translating ?
as ?
and translating ?
as ?,where ?
and ?
is the left and right hand sideof a initial phrase or hierarchical translationrule, respectively;?
pw(?|?)
and pw(?|?
): lexical probability oftranslating words in ?
as words in ?
andtranslating words in ?
as words in ?;?
plm: language model probability;?
wc: accumulated count of individual wordsgenerated during translation;?
pc: accumulated count of initial phrases used;?
rc: accumulated count of hierarchical rulephrases used;?
gc: accumulated count of glue rule used inthis hypothesis;?
uc: accumulated count of unknown sourceword.
which has no entry in the translationtable;?
nc: accumulated count of source phrases thattranslate into null;3.3 DecodingThe basic decoding algorithm could be kept al-most the same as traditional phrase-based orsyntax-based translation systems (Yamada andKnight, 2001; Koehn et al, 2003; Chiang, 2005;Liu et al, 2006).
For example, in the experimentsof this paper, we use a CKY style decoding algo-rithm following Chiang (2005).Our non-linear translation system is differentfrom traditional systems in the way to calculatethe score for each hypothesis.
Instead of calculat-ing the score as a linear combination, we use neu-ral networks (Section 3.1) to perform a non-linearcombination of feature values.We also use the cube-pruning algorithm (Chi-ang, 2005) to keep the decoding efficient.
Al-though the non-linearity in model scores maycause more search errors (Huang and Chiang,8272007) finding the highest scoring hypothesis, inpractice it still achieves reasonable results.4 Non-linear Learning FrameworkTraditional machine translation systems rely onMERT to tune the weights of different features.MERT performs efficient search by enumeratingthe score function of all the hypotheses and us-ing intersections of these linear functions to formthe ?upper-envelope?
of the model score func-tion (Och, 2003).
When the scoring function isnon-linear, it is not feasible to find the intersec-tions of these functions.
In this section, we discussalternatives to train the parameters for non-linearmodels.4.1 Training CriteriaThe task of machine translation is a complex prob-lem with structural output space.
Decoding algo-rithms search for the translation hypothesis withthe highest score, according to a given scoringfunction, from an exponentially large set of candi-date hypotheses.
The purpose of training is to se-lect the scoring function, so that the function scorethe hypotheses ?correctly?.
The correctness is of-ten introduced by some extrinsic metrics, such asBLEU (Papineni et al, 2002).We denote the scoring function as s(f , e;??
), orsimply s, which is parameterized by??
; denote theset of all translation hypotheses as C; denote theextrinsic metric as eval(?)1.
Note that, in linearcases, s is a linear function as in Equation 3, whilein the non-linear case described in this paper, s isthe scoring function in Equation 4.Ideally, the training objective is to select a scor-ing function s?, from all functions S , that scores thecorrect translation (or references)?e, higher thanany other hypotheses (Equation 5).s?
= {s ?
S|s(?e) > s(e)?e ?
C} (5)In practice, the candidate set C is exponentiallylarge and hard to enumerate; the correct translation?emay not even exist in the current search space forvarious reasons, e.g.
unknown source word.
As aresult, we use the n-best set Cnbestto approximateC, use the extrinsic metric eval(?)
to evaluate thequality of hypotheses in Cnbestand use the fol-lowing three alternatives as approximations to theideal objective.1In our experiments, we use sentence level BLEU with +1smoothing as the evaluation metric.Best v.s.
Rest (BR) To score the best hypothesisin the n-best set?e higher than the rest hy-potheses.
This objective is very similar toMERT in that it tries to optimize the scoreof?e and doesn?t concern about the ranking ofrest hypotheses.
In this case,?e is an approxi-mation of?e.Best v.s.
Worst (BW) To score the best hypoth-esis higher than the worst hypothesis in then-best set.
This objective is motivated by thepractice of separating the ?hope?
and ?fear?translation hypotheses (Chiang, 2012).
Wetake a simpler strategy which uses the bestand worst hypothesis in Cnbestas the ?hope?and ?fear?
hypothesis, respectively, in orderto avoid multi-pass decoding.Pairwise (PW) To score the better hypothesis insampled hypothesis pairs higher than theworse one in the same pair.
This objectiveis adapted from the Pairwise Ranking Opti-mization (PRO) (Hopkins and May, 2011),which tries to ranking all the hypotheses in-stead of selecting the best one.
We use thesame sampling strategy as their original pa-per.Note that each of the above criteria transformsthe original problem of selecting best hypothe-ses from an exponential space to a certain pair-wise comparison problem, which could be easilytrained using binary classifiers.4.2 Training ObjectiveFor the binary classification task, we use a hingeloss following Watanabe (2012).
Because the net-work has a lot of parameters compared with thelinear model, we use a L1norm instead of L2norm as the regularization term, to favor sparse so-lutions.
We define our training objective functionin Equation 6.argmin?1N?f?D?
(e1,e2)?T (f)?
(f , e1, e2; ?
)+ ?
?
||?||1with?(?)
= max{s(f , e1; ?)?
s(f , e2; ?)
+ 1, 0}(6)where D is the given training data; (e1, e2) is atraining hypothesis-pair, with e1to be the one with828higher eval(?)
score; N is the total number ofhypothesis-pairs in D; T (f), or simply T , is theset of hypothesis-pairs for each source sentence f .The set T is decided by the criterion used fortraining.
For the BR setting, the best hypothesis ispaired with every other hypothesis in the n-best list(Equation 7); while for the BW setting, it is onlypaired with the worst hypothesis (Equation 8).
Thegeneration of T in PW setting is the same withPRO sampling, we refer the readers to the originalpaper of Hopkins and May (2011).TBR= {(e1, e2)|e1= arg maxe?Cnbesteval(e),e2?
Cnbestand e1?= e2}(7)TBW= {(e1, e2)|e1= arg maxe?Cnbesteval(e),e2= arg mine?Cnbesteval(e)}(8)4.3 Training ProcedureIn standard training algorithm for classification,the training instances stays the same in each itera-tion.
In machine translation, decoding algorithmsusually return a very different n-best set with dif-ferent parameters.
This is due to the exponentiallylarge size of search space.
MERT and PRO extendthe current n-best set by merging the n-best setof all previous iterations into a pool (Och, 2003;Hopkins and May, 2011).
In this way, the enlargedn-best set may give a better approximation of thetrue hypothesis set C and may lead to better andmore stable training results.We argue that the training should still focus onhypotheses obtained in current round, because ineach iteration the searching for the n-best set is in-dependent of previous iterations.
To compromisethe above two goals, in our practice, training hy-pothesis pairs are first generated from the currentn-best set, then merged with the pairs generatedfrom all previous iterations.
In order to make themodel focus more on pairs from current iteration,we assign pairs in previous iterations a small con-stant weight and assign pairs in current iteration arelatively large constant weight2.
This is inspiredby the AdaBoost algorithm (Schapire, 1999) inweighting instances.Following the spirit of MERT, we propose aiterative training procedure (Algorithm 1).
The2In our experiments, we empirically set the constants tobe 0.1 and 0.9, respectively.Algorithm 1 Iterative Training AlgorithmInput: the set of training sentencesD, max num-ber of iteration I1: ?0?
RandomInit(),2: for i = 0 to I do3: Ti?
?
;4: for each f ?
D do5: Cnbest?
NbestDecode(f ; ?i)6: T ?
GeneratePair(Cnbest)7: Ti?
Ti?
T8: end for9: Tall?WeightedCombine(?i?1k=0Tk, Ti)10: ?i+1?
Optimize(Tall, ?i)11: end fortraining starts by randomly initialized model pa-rameters ?0(line 1).
In ith iteration, the decod-ing algorithm decodes each sentence f to get then-best set Cnbest(line 5).
Training hypothesispairs T are extracted from Cnbestaccording to thetraining criterion described in Section 4.2 (line 6).Newly collected pairs Tiare combined with pairsfrom previous iterations before used for training(line 9).
?i+1is obtained by solving Equation 6using the Conjugate Sub-Gradient method (Le etal., 2011) (line 10).5 Structure of the NetworkAlthough neural networks bring strong expressivepower to the modeling of translation hypothesis,training a neural network is prone to resulting inlocal minimum which may affect the training re-sults.
We speculate that one reason for these localminimums is that the structure of a well-connectednetwork has too many parameters.
Take a neu-ral network with k nodes in the input layer and mnodes in the hidden layer as an example.
Everynode in the hidden layer is connected to each ofthe k input nodes.
This simple structure resultingin at least k ?m parameters.In Section 4.2, we use L1norm in the objec-tive function in order to get sparser solutions.
Inthis section, we propose some constrained networkstructures according to our prior knowledge of thefeatures.
These structures have much less param-eters or simpler structures comparing to originalneural networks, thus reduce the possibility of get-ting stuck in local minimums.8295.1 Network with two-degree Hidden LayerWe find the first pitfall of the standard two-layerneural network is that each node in the hiddenlayer receives input from every input layer node.Features used in SMT are usually manually de-signed, which has their concrete meanings.
For anetwork of several hidden nodes, combining everyfeatures into every hidden node may be redundantand not necessary to represent the quality of a hy-pothesis.As a result, we take a harsh step and constrainthe nodes in hidden layer to have a in-degree oftwo, which means each hidden node only acceptsinputs from two input nodes.
We do not use anyother prior knowledge about features in this set-ting.
So for a network with k nodes in the in-put layer, the hidden layer should contain C2k=k(k?
1)/2 nodes to accept all combinations fromthe input layer.
We name this network structure asTwo-Degree Hidden Layer Network (TDN).It is easy to see that a TDN has C2k?
2 =k(k ?
1) parameters for the hidden layer becauseof the constrained degree.
This is one order ofmagnitude less than a standard two-layer networkwith the same number of hidden nodes, which hasC2k?
k = k2(k ?
1)/2 parameters.Note that we perform a 2-degree combinationthat looks similar in spirit with those combina-tion of atomic features in large scale discrimina-tive learning for other NLP tasks, such as POS tag-ging and parsing.
However, unlike the practice inthese tasks that directly combines values of differ-ent features to generate a new feature type, we firstlinearly combine the value of these features andperform non-linear transformation on these valuesvia an activation function.5.2 Network with Grouped FeaturesIt might be a too strong constraint to require thehidden node have in-degree of 2.
In order to re-lax this constraint, we need more prior knowledgefrom the features.Our first observation is that there are differenttypes of features.
These types are different fromeach other in terms of value ranges, sources, im-portance, etc.
For example, language model fea-tures usually take a very small value of probability,and word count feature takes a integer value andusually has a much higher weight in linear casethan other count features.The second observation is that features of thesame type may not have complex interaction witheach other.
For example, it is reasonable to com-bine language model features with word count fea-tures in a hidden node.
But it may not be neces-sary to combine the count of initial phrases and thecount of unknown words into a hidden node.Based on the above two intuitions, we designa new structure of network that has the followingconstraints: given a disjoint partition of features:G1, G2,..., Gk, every hidden node takes input froma set of input nodes, where any two nodes in thisset come from two different feature groups.
Un-der this constraint, the in-degree of a hidden nodeis at most k. We name this network structure asGrouped Network (GN).In practice, we divide the basic features in Sec-tion 3.2 into five groups: language model features,translation probability features, lexical probabilityfeatures, the word count feature, and the rest ofcount features.
This division considers not onlythe value ranges, but also types of features and thepossibility of them interact with each other.6 Experiments and Results6.1 General SettingsWe conduct experiments on a large scale machinetranslation tasks.
The parallel data comes fromLDC, including LDC2002E18, LDC2003E14,LDC2004E12, LDC2004T08, LDC2005T10,LDC2007T09, which consists of 8.2 millionof sentence pairs.
Monolingual data includesXinhua portion of Gigaword corpus.
We usemulti-references data MT03 as training data,MT02 as development data, and MT04, MT05as test data.
These data are mainly in the samegenre, avoiding the extra consideration of domainadaptation.Data Usage Sents.LDC TM train 8,260,093Gigaword LM train 14,684,074MT03 train 919MT02 dev 878MT04 test 1,789MT05 test 1,083Table 1: Experimental data and statistics.The Chinese side of the corpora is word seg-mented using ICTCLAS3.
Our translation sys-3http://ictclas.nlpir.org/830Criteria MT03(train) MT02(dev) MT04 MT05BRc35.02 36.63 34.96 34.15BR 38.66 40.04 38.73 37.50BW 39.55 39.36 38.72 37.81PW 38.61 38.85 38.73 37.98Table 2: BLEU4 in percentage on different training criteria (?BR?, ?BW?
and ?PW?
refer to experimentswith ?Best v.s.
Rest?, ?Best v.s.
Worst?
and ?Pairwise?
training criteria, respectively.
?BRc?
indicatesgenerate hypothesis pairs from n-best set of current iteration only presented in Section 4.3.tem is an in-house implementation of the hier-archical phrase-based translation system(Chiang,2005).
We set the beam size to 20.
We train a5-gram language model on the monolingual datawith MKN smoothing(Chen and Goodman, 1998).For each parameter tuning experiments, we ran thesame training procedure 3 times and present theaverage results.
The translation quality is evalu-ated use 4-gram case-insensitive BLEU (Papineniet al, 2002).
Significant test is performed usingbootstrap re-sampling implemented by Clark etal.
(2011).
We employ a two-layer neural networkwith 11 input layer nodes, corresponding to fea-tures listed in Section 3.2 and 1 output layer node.The number of nodes in the hidden layer varies indifferent settings.
The sigmoid function is used asthe activation function for each node in the hiddenlayer.
For the output layer we use a linear activa-tion function.
We try different ?
for the L1normfrom 0.01 to 0.00001 and use the one with bestperformance on the development set.
We solve theoptimization problem with ALGLIB package4.6.2 Experiments of Training CriteriaThis set experiments evaluates different trainingcriteria discussed in Section 4.1.
We generatehypothesis-pair according to BW, BR and PW cri-teria, respectively, and perform training with thesepairs.
In the PW criterion, we use the samplingmethod of PRO (Hopkins and May, 2011) and getthe 50 hypothesis pairs for each sentence.
We use20 hidden nodes for all three settings to make afair comparison.The results are presented in Table 2.
Thefirst two rows compare training with and with-out the weighted combination of hypothesis pairswe discussed in Section 4.3.
As the result sug-gested, with the weighted combination of hypothe-sis pairs from previous iterations, the performanceimproves significantly on both test sets.4http://www.alglib.net/Although the system performance on the devset varies, the performance on test sets are al-most comparable.
This suggest that although thethree training criteria are based on different as-sumptions, their are basically equivalent for train-ing translation systems.Criteria Pairs/iteration Accuracy(%)BR 19 70.7BW 1 79.5PW 100 67.3Table 3: Comparison of different training criteriain number of new instances per iteration and train-ing accuracy.We also compares the three training criteria intheir number of new instances per iteration andfinal training accuracy (Table 3).
Compared toBR which tries to separate the best hypothesisfrom the rest hypotheses in the n-best set, and PWwhich tries to obtain a correct ranking of all hy-potheses, BW only aims at separating the best andworst hypothesis of each iteration, which is a eas-ier task for learning a classifiers.
It requires theleast training instances and achieves the best per-formance in training.
Note that, the accuracy foreach system in Table 3 are the accuracy each sys-tem achieves after training stops.
They are not cal-culated on the same set of instances, thus not di-rectly comparable.
We use the differences in accu-racy as an indicator for the difficulties of the cor-responding learning task.For the rest of this paper, we use the BW crite-rion because it is much simpler compared to sam-pling method of PRO (Hopkins and May, 2011).6.3 Experiments of Network StructuresWe make several comparisons of the networkstructures and compare them with a baseline hi-erarchical phrase-based translation system (HPB).Table 4 shows the translation performance of831Systems MT03(train) MT02(dev) MT04 MT05 Test AverageHPB 39.25 39.07 38.81 38.01 38.41TLayer2039.55?39.36?38.72 37.81 38.27(-0.14)TLayer3039.70+39.71?38.89 37.90 38.40(-0.01)TLayer5039.26 38.97 38.72 38.79+38.76(+0.35)TLayer10039.42 38.77 38.65 38.65+38.69(+0.28)TLayer20039.69 38.68 38.72 38.80+38.74(+0.32)TDN 39.60+38.94 38.99?38.13 38.56(+0.15)GN 39.73+39.41+39.45+38.51+38.98(+0.57)Table 4: BLEU4 in percentage for comparing of systems using different network structures (HPB refersto the baseline hierarchical phrase-based system.
TLayer, TDN, GN refer to the standard 2-layer network,Two-Degree Hidden Layer Network, Grouped Network, respectively.
Subscript of TLayer indicates thenumber of nodes in the hidden layer.
)+,?marks results that are significant better than the baselinesystem with p < 0.01 and p < 0.05.Systems # Hidden Nodes # Parameters Training Time per iter.
(s)HPB - 11 1041TLayer2020 261 671TLayer3030 391 729TLayer5050 651 952TLayer100100 1,301 1,256TLayer200200 2,601 2,065TDN 55 221 808GN 214 1,111 1,440Table 5: Comparison of network scales and training time of different systems, including the number ofnodes in the hidden layer, the number of parameters, the average training time per iteration (15 iterations).The notations of systems are the same as in Table4.different systems5.
All 5 two-layer feed forwardneural networks models could achieve compara-ble or better performance comparing to the base-line system.
We can see that training a larger net-work may lead to better translation quality (fromTLayer20and TLayer30to TLayer50).
However,increasing the number of hidden node to 100 and200 does not bring further improvement.
One pos-sible reason is that training a larger network witharbitrary connections brings in too many param-eters which may be difficult to train with limitedtraining data.TDN and GN are the two network structuresproposed in Section 5.
With the constraint thatall input to the hidden node should be of degree2, TDN performs comparable to the baseline sys-tem.
With the grouped feature, we could designnetworks such as GN, which shows significant im-provement over the baseline systems (+0.57) andachieves the best performance among all neuralsystems.5TLayer20is the same system as BW in Table 2Table 4 shows statistics related to the efficiencyissue of different systems.
The baseline system(HPB) uses MERT for training.
HPB has a verysmall number of parameters and searches for thebest parameters exhaustively in each iteration.
Thenon-linear systems with few nodes (TLayer20andTLayer30) train faster than HPB in each iterationbecause they perform back-propagation instead ofexhaustive search.
We iterate 15 iterations for eachnon-linear system, while MERT takes about 10rounds to reach its best performance.When the number of nodes in the hidden layerincreases (from 20 to 200), the number of param-eters in the system also increases, which requireslonger time to compute the score for each hypoth-esis and to update the parameters through back-propagation.
The network with 200 hidden nodestakes about twice the time to train for each itera-tion, compared to the linear system6.TDN and GN have larger numbers of hidden6Matrix operation is CPU intensive.
The cost will in-crease when multiple tasks are running.832nodes.
However, because of our intuitions in de-signing the structure of the networks, the degreeof the hidden node is constrained.
So these twonetworks are sparser in parameters and take sig-nificant less training time than standard neural net-works.
For example, GN has a comparable num-ber of hidden nodes with TLayer200, but only hashalf of its parameters and takes about 70% time totrain in each iteration.
In other words, our pro-posed network structure provides more efficienttraining in these cases and achieve better results.7 ConclusionIn this paper, we discuss a non-linear frameworkfor modeling translation hypothesis for statisti-cal machine translation system.
We also presenta learning framework including training criterionand algorithms to integrate our modeling into astate of the art hierarchical phrase based machinetranslation system.
Compared to previous effortin bringing in non-linearity into machine transla-tion, our method uses a single two-layer neuralnetworks and performs training independent withany previous linear training methods (e.g.
MERT).Our method also trains its parameters without anypre-training or post-training procedure.
Experi-ment shows that our method could improve thebaseline system even with the same feature asinput, in a large scale Chinese-English machinetranslation task.In training neural networks with hidden nodes,we use heuristics to reduce the complexity of net-work structures and obtain extra advantages overstandard networks.
It shows that heuristics and in-tuitions of the data and features are still importantto a machine translation system.Neural networks are able to perform featurelearning by using hidden nodes to model the in-teraction among a large vector of raw features,as in image and speech processing (Krizhevsky etal., 2012; Hinton et al, 2012).
We are trying tomodel the interaction between hand-crafted fea-tures, which is indeed similar in spirit with learn-ing features from raw features.
Although our fea-tures already have concrete meaning, e.g.
theprobability of translation, the fluency of target sen-tence, etc.
Combining these features may have ex-tra advantage in modeling the translation process.As future work, it is necessary to integrate morefeatures into our learning framework.
It is also in-teresting to see how the non-linear modeling fitsin to more complex learning tasks which involvesdomain specific learning techniques.AcknowledgmentsThe authors would like to thank Yue Zhangand the anonymous reviewers for their valu-able comments.
This work is supported by theNational Natural Science Foundation of China(No.
61300158, 61223003), the Jiangsu Provin-cial Research Foundation for Basic Research (No.BK20130580).ReferencesMichael Auli and Jianfeng Gao.
2014.
Decoder in-tegration and expected BLEU training for recurrentneural network language models.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics, ACL 2014, June 22-27,2014, Baltimore, MD, USA, Volume 2: Short Papers,pages 136?142.Michael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint language and translationmodeling with recurrent neural networks.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP 2013,18-21 October 2013, Grand Hyatt Seattle, Seattle,Washington, USA, A meeting of SIGDAT, a SpecialInterest Group of the ACL, pages 1044?1054.Christopher M. Bishop.
1995.
Neural Networks forPattern Recognition.
Oxford University Press, Inc.,New York, NY, USA.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matic of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.S.
F. Chen and J. T. Goodman.
1998.
An empiricalstudy of smoothing techniques for language mod-eling.
Technical report, Computer Science Group,Harvard University, Technical Report TR-10-98.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In annualmeeting of the Association for Computational Lin-guistics.David Chiang.
2012.
Hope and fear for discriminativetraining of statistical translation models.
J. Mach.Learn.
Res., 13(1):1159?1187, April.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for opti-mizer instability.
In Proceedings of the 49th Annual833Meeting of the Association for Computational Lin-guistics: Human Language Technologies: Short Pa-pers - Volume 2, HLT ?11, pages 176?181, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Jonathan Clark, Chris Dyer, and Alon Lavie.
2014.Locally non-linear learning for statistical machinetranslation via discretization and structured regular-ization.
Transactions of the Association for Compu-tational Linguistics, 2:393?404.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard M. Schwartz, and John Makhoul.2014.
Fast and robust neural network joint modelsfor statistical machine translation.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics, ACL 2014, June 22-27,2014, Baltimore, MD, USA, Volume 1: Long Papers,pages 1370?1380.Kevin Duh and Katrin Kirchhoff.
2008.
Beyond log-linear models: Boosted minimum error rate train-ing for n-best re-ranking.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics on Human Language Technolo-gies: Short Papers, HLT-Short ?08, pages 37?40,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Dumitru Erhan, Pierre antoine Manzagol, Yoshua Ben-gio, Samy Bengio, and Pascal Vincent.
2009.
Thedifficulty of training deep architectures and the ef-fect of unsupervised pre-training.
In David V.Dyk and Max Welling, editors, Proceedings of theTwelfth International Conference on Artificial In-telligence and Statistics (AISTATS-09), volume 5,pages 153?160.
Journal of Machine Learning Re-search - Proceedings Track.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
2014.
Learning continuous phrase repre-sentations for translation modeling.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics, ACL 2014, June 22-27,2014, Baltimore, MD, USA, Volume 1: Long Papers,pages 699?709.Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,Abdel-rahman Mohamed, Navdeep Jaitly, AndrewSenior, Vincent Vanhoucke, Patrick Nguyen, Tara NSainath, et al 2012.
Deep neural networks foracoustic modeling in speech recognition: The sharedviews of four research groups.
Signal ProcessingMagazine, IEEE, 29(6):82?97.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 1352?1362, Stroudsburg, PA,USA.
Association for Computational Linguistics.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages144?151, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-ton.
2012.
Imagenet classification with deep con-volutional neural networks.
In F. Pereira, C.J.C.Burges, L. Bottou, and K.Q.
Weinberger, editors,Advances in Neural Information Processing Systems25, pages 1097?1105.
Curran Associates, Inc.Quoc V. Le, Jiquan Ngiam, Adam Coates, AhbikLahiri, Bobby Prochnow, and Andrew Y. Ng.
2011.On optimization methods for deep learning.
In Pro-ceedings of the 28th International Conference onMachine Learning, ICML 2011, Bellevue, Washing-ton, USA, June 28 - July 2, 2011, pages 265?272.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 44th Annual Meet-ing of the Association of Computational Linguistics.The Association for Computer Linguistics.Lemao Liu, Taro Watanabe, Eiichiro Sumita, andTiejun Zhao.
2013.
Additive neural networks forstatistical machine translation.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics, ACL 2013, 4-9 August 2013,Sofia, Bulgaria, Volume 1: Long Papers, pages 791?801.Shixiang Lu, Zhenbiao Chen, and Bo Xu.
2014.Learning new semi-supervised deep auto-encoderfeatures for statistical machine translation.
In Pro-ceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics (Volume 1:Long Papers), pages 122?132, Baltimore, Maryland,June.
Association for Computational Linguistics.Sameer Maskey and Bowen Zhou.
2012.
Unsuper-vised deep belief features for speech translation.
InINTERSPEECH 2012, 13th Annual Conference ofthe International Speech Communication Associa-tion, Portland, Oregon, USA, September 9-13, 2012.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
pages 295?302.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In ACL ?02: Proceed-ings of the 40th Annual Meeting on Association for834Computational Linguistics, pages 311?318, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Robert E. Schapire.
1999.
A brief introduction toboosting.
In Proceedings of the 16th InternationalJoint Conference on Artificial Intelligence - Volume2, IJCAI?99, pages 1401?1406, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with large-scaleneural language models improves translation.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2013, 18-21 October 2013, Grand Hyatt Seattle,Seattle, Washington, USA, A meeting of SIGDAT,a Special Interest Group of the ACL, pages 1387?1392.Taro Watanabe.
2012.
Optimized online rank learningfor machine translation.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, NAACL HLT ?12, pages253?262, Stroudsburg, PA, USA.
Association forComputational Linguistics.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof the 39th Annual Meeting of the Association ofComputational Linguistics, pages 523?530.835
