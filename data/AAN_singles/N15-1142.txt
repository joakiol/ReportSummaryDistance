Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1299?1304,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsTwo/Too Simple Adaptations of Word2Vec for Syntax ProblemsWang Ling Chris Dyer Alan Black Isabel TrancosoL2F Spoken Systems Lab, INESC-ID, Lisbon, PortugalLanguage Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USAInstituto Superior T?ecnico, Lisbon, Portugal{lingwang,cdyer,awb}@cs.cmu.eduisabel.trancoso@inesc-id.ptAbstractWe present two simple modifications to themodels in the popular Word2Vec tool, in or-der to generate embeddings more suited totasks involving syntax.
The main issue withthe original models is the fact that they areinsensitive to word order.
While order in-dependence is useful for inducing semanticrepresentations, this leads to suboptimal re-sults when they are used to solve syntax-basedproblems.
We show improvements in part-of-speech tagging and dependency parsing usingour proposed models.1 IntroductionWord representations learned from neural languagemodels have been shown to improve many NLPtasks, such as part-of-speech tagging (Collobert etal., 2011), dependency parsing (Chen and Man-ning, 2014; Kong et al, 2014) and machine trans-lation (Liu et al, 2014; Kalchbrenner and Blunsom,2013; Devlin et al, 2014; Sutskever et al, 2014).These low-dimensional representations are learnedas parameters in a language model and trained tomaximize the likelihood of a large corpus of rawtext.
They are then incorporated as features alongside hand-engineered features (Turian et al, 2010),or used to initialize the parameters of neural net-works targeting tasks for which substantially lesstraining data is available (Hinton and Salakhutdinov,2012; Erhan et al, 2010; Guo et al, 2014).One of the most widely used tools for buildingword vectors are the models described in (Mikolovet al, 2013), implemented in the Word2Vec tool,in particular the ?skip-gram?
and the ?continuousbag-of-words?
(CBOW) models.
These two mod-els make different independence and conditioningassumptions; however, both models discard wordorder information in how they account for context.Thus, embeddings built using these models havebeen shown to capture semantic information be-tween words, and pre-training using these modelshas been shown to lead to major improvements inmany tasks (Collobert et al, 2011).
While more so-phisticated approaches have been proposed (Dhillonet al, 2011; Huang et al, 2012; Faruqui and Dyer,2014; Levy and Goldberg, 2014; Yang and Eisen-stein, 2015), Word2Vec remains a popular choicedue to their efficiency and simplicity.However, as these models are insensitive to wordorder, embeddings built using these models are sub-optimal for tasks involving syntax, such as part-of-speech tagging or dependency parsing.
This is be-cause syntax defines ?what words go where?
?, whilesemantics than ?what words go together?.
Obvi-ously, in a model where word order is discarded,the many syntactic relations between words can-not be captured properly.
For instance, while mostwords occur with the word the, only nouns tend tooccur exactly afterwords (e.g.
the cat).
This issupported by empirical evidence that suggests thatorder-insensitivity does indeed lead to substandardsyntactic representations (Andreas and Klein, 2014;Bansal et al, 2014), where systems using pre-trainedwith Word2Vec models yield slight improvementswhile the computationally far more expensive whichuse word order information embeddings of Col-lobert et al (2011) yielded much better results.1299In this work, we describe two simple modifica-tions to Word2Vec, one for the skip-gram modeland one for the CBOW model, that improve thequality of the embeddings for syntax-based tasks1.Our goal is to improve the final embeddings whilemaintaining the simplicity and efficiency of the orig-inal models.
We demonstrate the effectiveness ofour approaches by training, on commodity hard-ware, on datasets containing more than 50 millionsentences and over 1 billion words in less than aday, and show that our methods lead to improve-ments when used in state-of-the-art neural networksystems for part-of-speech tagging and dependencyparsing, relative to the original models.2 Word2VecThe work in (Mikolov et al, 2013) is a popularchoice for pre-training the projection matrix W ?<d?|V |where d is the embedding dimension withthe vocabulary V .
As an unsupervised task that istrained on raw text, it builds word embeddings bymaximizing the likelihood that words are predictedfrom their context or vice versa.
Two models weredefined, the skip-gram model and the continuousbag-of-words model, illustrated in Figure 1.The skip-gram model?s objective function is tomaximize the likelihood of the prediction of contex-tual words given the center word.
More formally,given a document of T words, we wish to maximizeL =1TT?t=1?
?c?j?c,j 6=0log p(wt+j| wt) (1)Where c is a hyperparameter defining the windowof context words.
To obtain the output probabil-ity p(wo|wi), the model estimates a matrix O ?<|V |?dw, which maps the embeddings rwiinto a|V |-dimensional vector owi.
Then, the probabilityof predicting the word wogiven the word wiis de-fined as:p(wo| wi) =eowi(wo)?w?Veowi(w)(2)This is referred as the softmax objective.
However,for larger vocabularies it is inefficient to compute1The code developed in this work is made available inhttps://github.com/wlin12/wang2vec.owi, since this requires the computation of a |V |?dwmatrix multiplication.
Solutions for problem are ad-dressed in the Word2Vec by using the hierarchicalsoftmax objective function or resorting to negativesampling (Goldberg and Levy, 2014).The CBOW model predicts the center word wogiven a representation of the surrounding wordsw?c, ..., w?1, w1, wc.
Thus, the output vectorow?c,...,w?1,w1,wcis obtained from the product of thematrix O ?
<|V |?dwwith the sum of the embed-dings of the context words?
?c?j?c,j 6=0rwj.We can observe that in both methods, the order ofthe context words does not influence the predictionoutput.
As such, while these methods may find sim-ilar representations for semantically similar words,they are less likely to representations based on thesyntactic properties of the words.OOOw-2inputw-1w1w2SUMprojection outputw0Oinput projection outputOw0w-2w-1w1w2CBOWSkip-NgramFigure 1: Illustration of the Skip-gram and ContinuousBag-of-Word (CBOW) models.3 Structured Word2VecTo account for the lack of order-dependence in theabove models, we propose two simple modificationsto these methods that include ordering information,which we expect will lead to more syntactically-oriented embeddings.
These models are illustratedin Figure 2.3.1 Structured Skip-gram ModelThe skip-gram model uses a single output matrixO ?
<|V |?dto predict every contextual wordw?c, ..., w?1, w1, ..., wc, given the embeddings ofthe center word w0.
Our approach adapts themodel so that it is sensitive to the positioning of thewords.
It defines a set of c ?
2 output predictorsO?c, ..., O?1, O1, Oc, with size O ?
<(|V |)?d.
Eachof the output matrixes is dedicated to predicting the1300output for a specific relative position to the centerword.
When making a prediction p(wo| wi), weselect the appropriate output matrix Oo?ito projectthe word embeddings to the output vector.
Note, thatthe number of operations that must be performed forthe forward and backward passes in the network re-mains the same, as we are simply switching the out-put layer O for each different word index.3.2 Continuous Window ModelThe Continuous Bag-Of-Words words modeldefines a window of words w?c, ..., wcwithsize c, where the prediction of the center wordw0is conditioned on the remaining wordsw?c, ..., w?1, w1, ..., wc.
The prediction matrixO ?
<(|V |)?dis fed with the sum of the embed-dings of the context words.
As such, the orderof the contextual words does not influence theprediction of the center word.
Our approachdefines a different output predictor O ?
<(|V |?2cdwhich receives as input a (2c ?
d)-dimensionalvector that is the concatenation of the embeddingsof the context words in the order they occur[e(w?c), .
.
.
, e(w?1), e(w1), .
.
.
, e(wc)].
As matrixO defines a parameter for the word embeddings foreach relative position, this allows the words to betreated differently depending on where they occur.This model, denoted as CWindow, is essentially thewindow-based model described in (Collobert et al,2011), with the exception that we do not projectthe vector of word embeddings into a windowembedding before making the final prediction.In both models, we are increasing the numberof parameters of matrix O by a factor of c ?
2,which can lead to sparcity problems when trainingon small datasets.
However, these models are gener-ally trained on datasets in the order of 100 millionsof words, where these issues are not as severe.4 ExperimentsWe conducted experiments in two mainstreamsyntax-based tasks part-of-speech Tagging and De-pendency parsing.
Part-of-speech tagging is a wordlabeling task, where each word is to be labelled withits corresponding part-of-speech.
In dependencyparsing, the goal is to predict a tree built of syntacticrelations between words.
In both tasks, it has beenO2O1O-1w-2inputw-1w1w2projection outputw0Oinput projection outputO-2w0w-2w-1w1w2CWINDOWStructured Skip-NgramFigure 2: Illustration of the Structured Skip-gram andContinuous Window (CWindow) models.shown that pre-trained embeddings can be used toachieve better generalization (Collobert et al, 2011;Chen and Manning, 2014).4.1 Building Word VectorsWe built vectors for English in two very differentdomains.
Firstly, we used an English Wikipediadump containing 1,897 million words (60 millionsentences), collected in September of 2014.
Webuilt word embeddings using the original and ourproposed methods on this dataset.
These embed-dings will be denoted as WIKI(L).
Then, we took asample of 56 million English tweets with 847 mil-lion words collected in (Owoputi et al, 2013), andapplied the same procedure to build the TWITTERembeddings.
Finally, we also use the Wikipediadocuments, with 16 million words, provided in theWord2Vec package for contrastive purposes, de-noted as WIKI(S).
As preprocessing, the text waslowercased and groups of contiguous digits were re-placed by a special word.
For all corpora, we trainedthe network with a c = 5, with a negative samplingvalue of 10 and filter out words with less than 40instances.
WIKI(L) and TWITTER have a vocabu-lary of 424,882 and 216,871 types, respectively, andembeddings of 50 dimensions.Table 1 shows the similarity for a few selectedkeywords for each of the different embeddings.
Wecan see hints that our models tend to group wordsthat are more syntactically related.
In fact, for theword breaking, the CWindow model?s top five wordsare exclusively composed by verbs in the contin-uous form, while the Structured Skip-gram modeltends to combine these with other forms of the verbbreak.
The original models tend to be less keen on1301Embeddings WIKI(S) TWITTER WIKI(L)query breaking amazing personCBOW breaks incredible someoneturning awesome anyonebroke fantastic oneselfbreak phenomenal womanstumbled awsome ifSkip-gram break incredible harasserbreaks awesome themselfbroke fantastic declarantdown phenominal someonebroken phenomenal right-thinkingCWindow putting incredible woman(this work) turning amaaazing mansticking awesome childpulling amzing grandparentpicking a-mazing servicememberStructured break incredible declarantSkip-gram turning awesome circumstance(this work) putting amaaazing womanout ah-mazing schoolchildbreaks amzing someoneTable 1: Most similar words using different word embed-ding models for the words breaking, amazing and person.Each word is queried in a different dataset.preserving such properties.
As for the TWITTERembeddings, we can observe that our adapted em-beddings are much better at finding lexical variationsof the words, such as a-mazing, resembling the re-sults obtained using brown clusters (Owoputi et al,2013).
Finally, for the query person, we can seethat our models tend to associate this term to otherwords in the same class, such as man, woman andchild, while original models tend to include unre-lated words, such as if and right-thinking.In terms of computation speed, the Skip-gramand CBOW models, achieve a processing rate of71.35k and 342.17k words per second, respectively.The Structured Skip-gram and CWindow modelscan process 34.64k and 124.43k words per second,respectively.
There is a large drop in computa-tional speed in the CWindow model compared tothe CBOW model, as it uses a larger output ma-trix, which grows with the size of the window.
TheStructured Skip-gram model processes words at al-most half the speed of the Skip-gram model.
This isexplained by the fact that the Skip-gram model sub-samples context words, varying the size of the win-dow size stochastically, so that words closer to thecenter word are sampled more frequently.
That is,when defining a window size of 5, the actual win-dow size used for each sample is a random valuebetween 1 and 5.
As we use a separate output layerfor each position, we did not find this property to beuseful as it provides less training samples for out-put matrixes with higher indexes.
While our modelsare slower they are still suitable for processing largedatasets as all the embeddings we use were all builtwithin a day.4.2 Part-Of-Speech TaggingWe reimplemented the window-based model pro-posed in (Collobert et al, 2011), which defines a3-layer perceptron.
In this network, words are firstprojected into embeddings, which are concatenatedand projected into a window embedding.
These arefinally projected into an output layer with size of thePOS tag vocabulary, followed by a softmax.
In ourexperiments, we used a window size of 5, word em-beddings of size 50 and window embeddings of size500.
Word embeddings were initialized using thepre-trained vectors and these parameters are updatedas the rest of the network.
Additionally, we also adda capitalization feature which indicates whether thefirst letter of the work is uppercased, as all word fea-tures are lowercased words.
Finally, for words un-seen in the training set and in the pre-trained em-beddings, we replace them with a special unknowntoken, which is also modelled as a word type with aset of 50 parameters.
At training time, we stochasti-cally replace word types that only occur once in thetraining dataset with the unknown token.
Evaluationis performed with the part-of-speech tag accuracy,which denotes the percentage of words labelled cor-rectly.Experiments are performed on two datasets, theEnglish Penn Treebank (PTB) dataset using thestandard train, dev and test splits, and the ARKdataset (Gimpel et al, 2011), with 1000 training,327 dev and 500 labelled English tweets from Twit-ter.
For the PTB dataset, we use the WIKI(L) em-beddings and use TWITTER embeddings for theARK dataset.
Finally, the set of parameters with thehighest accuracy in the dev set are used to report thescore for the test set.Results are shown in Table 2, where we observethat our adapted models tend to yield better re-1302PTB TwitterDev Test Dev TestCBOW 95.89 96.13 87.85 87.54Skip-gram 96.62 96.68 88.84 88.73CWindow 96.99 97.01 89.72 89.63Structured Skip-gram 96.62 97.05 89.69 89.79SENNA 96.54 96.58 84.96 84.85Table 2: Results for part-of-speech tagging using differ-ent word embeddings (rows) on different datasets (PTBand Twitter).
Cells indicate the part-of-speech accuracyof each experiment.sults than the original models in both datasets.
Inthe Twitter dataset, our results slightly higher thanthe accuracy reported using only Brown clustersin (Owoputi et al, 2013), which was 89.50.
We alsotry initializing our embeddings with those in (Col-lobert et al, 2011), which are in the ?Senna?
row.Even though results are higher in our models, wecannot conclude that our method is better as theyare trained crawls from Wikipedia in different timeperiods.
However, it is a good reference to show thatour embeddings are on par with those learned usingmore sophisticated models.4.3 Dependency ParsingThe evaluation on dependency parsing is performedon the English PTB, with the standard train, dev andtest splits with Stanford Dependencies.
We use neu-ral network defined in (Chen and Manning, 2014),with the default hyper-parameters2and trained for5000 iterations.
The word projections are initial-ized using WIKI(L) embeddings.
Evaluation isperformed with the labelled (LAS) and unlabeled(UAS) attachment scores.In Table 3, we can observe that results are consis-tent with those in part-of-speech tagging, where ourmodels obtain higher scores than the original modelsand with competitive results compared to Senna em-beddings.
This suggests that our models are suitedat learning syntactic relations between words.5 ConclusionsIn this work, we present two modifications to theoriginal models in Word2Vec that improve the wordembeddings obtained for syntactically motivated2Found in http://nlp.stanford.edu/software/nndep.shtmlDev TestUAS LAS UAS LASCBOW 91.74 88.74 91.52 88.93Skip-gram 92.12 89.30 91.90 89.55CWindow 92.38 89.62 92.00 89.70Structured Skip-gram 92.49 89.78 92.24 89.92SENNA 92.24 89.30 92.03 89.51Table 3: Results for dependency parsing on PTB usingdifferent word embeddings (rows).
Columns UAS andLAS indicate the labelled attachment score and the unla-belled parsing scores, respectively.tasks.
This is done by introducing changes that makethe network aware of the relative positioning of con-text words.
With these models we obtain improve-ments in two mainstream NLP tasks, namely part-of-speech tagging and dependency parsing, and re-sults generalize in both clean and noisy domains.AcknowledgementsThis work was partially supported by FCT (INESC-ID multiannual funding) through the PIDDAC Pro-gram funds, and also through projects CMU-PT/HuMach/0039/2008 and CMU-PT/0005/2007.The PhD thesis of Wang Ling is supported by FCTgrant SFRH/BD/51157/2010.
The authors also wishto thank the anonymous reviewers for many helpfulcomments.ReferencesJacob Andreas and Dan Klein.
2014.
How much do wordembeddings encode about syntax.
In Proceedings ofACL.Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.Tailoring continuous word representations for depen-dency parsing.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 740?750.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.1303Fast and robust neural network joint models for statis-tical machine translation.
In 52nd Annual Meeting ofthe Association for Computational Linguistics, Balti-more, MD, USA, June.Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.2011.
Multi-view learning of word embeddings viacca.
In Advances in Neural Information ProcessingSystems, pages 199?207.Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio.2010.
Why does unsupervised pre-training help deeplearning?
The Journal of Machine Learning Research,11:625?660.Manaal Faruqui and Chris Dyer.
2014.
Improving vectorspace word representations using multilingual correla-tion.
In Proceedings of EACL.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A Smith.
2011.
Part-of-speech tagging for twit-ter: Annotation, features, and experiments.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies: short papers-Volume 2, pages 42?47.Association for Computational Linguistics.Yoav Goldberg and Omer Levy.
2014. word2vecexplained: deriving mikolov et al?s negative-sampling word-embedding method.
arXiv preprintarXiv:1402.3722.Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu.2014.
Revisiting embedding features for simple semi-supervised learning.
In Proceedings of EMNLP.Geoffrey E Hinton and Ruslan Salakhutdinov.
2012.
Abetter way to pretrain deep boltzmann machines.
InAdvances in Neural Information Processing Systems,pages 2447?2455.Eric H Huang, Richard Socher, Christopher D Manning,and Andrew Y Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Long Papers-Volume 1, pages 873?882.
Association for Computa-tional Linguistics.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In EMNLP, pages1700?1709.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah A. Smith.
2014.
A dependency parser fortweets.
In Proc.
of EMNLP, pages 1001?1012, Doha,Qatar, October.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics, volume 2.Shujie Liu, Nan Yang, Mu Li, and Ming Zhou.
2014.
Arecursive recurrent neural network for statistical ma-chine translation.
In Proceedings of ACL, pages 1491?1500.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems,pages 3111?3119.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A Smith.
2013.Improved part-of-speech tagging for online conversa-tional text with word clusters.
In HLT-NAACL, pages380?390.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural networks.In Advances in Neural Information Processing Sys-tems, pages 3104?3112.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, ACL ?10, pages 384?394, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Yi Yang and Jacob Eisenstein.
2015.
Unsupervisedmulti-domain adaptation with feature embeddings.
InProceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies.Association for Computational Linguistics.1304
