Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 328?337,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsDynamic Topic Adaptation for Phrase-based MTEva Hasler1, Phil Blunsom2, Philipp Koehn1, Barry Haddow11School of Informatics, University of Edinburgh2Dept.
of Computer Science, University of OxfordAbstractTranslating text from diverse sourcesposes a challenge to current machinetranslation systems which are rarelyadapted to structure beyond corpus level.We explore topic adaptation on a diversedata set and present a new bilingual vari-ant of Latent Dirichlet Allocation to com-pute topic-adapted, probabilistic phrasetranslation features.
We dynamically in-fer document-specific translation proba-bilities for test sets of unknown origin,thereby capturing the effects of documentcontext on phrase translations.
We showgains of up to 1.26 BLEU over the base-line and 1.04 over a domain adaptationbenchmark.
We further provide an anal-ysis of the domain-specific data and showadditive gains of our model in combinationwith other types of topic-adapted features.1 IntroductionIn statistical machine translation (SMT), there hasbeen a lot of interest in trying to incorporate in-formation about the provenance of training exam-ples in order to improve translations for specifictarget domains.
A popular approach are mixturemodels (Foster and Kuhn, 2007) where each com-ponent contains data from a specific genre or do-main.
Mixture models can be trained for cross-domain adaption when the target domain is knownor for dynamic adaptation when the target domainis inferred from the source text under translation.More recent domain adaptation methods employcorpus or instance weights to promote relevanttraining examples (Matsoukas et al., 2009; Fos-ter et al., 2010) or do more radical data selectionbased on language model perplexity (Axelrod etal., 2011).
In this work, we are interested in thedynamic adaptation case, which is challenging be-cause we cannot tune our model towards any spe-cific domain.In previous literature, domains have often beenloosely defined in terms of corpora, for exam-ple, news texts would be defined as belonging tothe news domain, ignoring the specific content ofnews documents.
It is often assumed that the datawithin a domain is homogeneous in terms of styleand vocabulary, though that is not always true inpractice.
The term topic on the other hand candescribe the thematic content of a document (e.g.politics, economy, medicine) or a latent cluster in atopic model.
Topic modelling for machine transla-tion aims to find a match between thematic contextand topic clusters.
We view topic adaptation asfine-grained domain adaptation with the implicitassumption that there can be multiple distributionsover translations within the same data set.
If thesedistributions overlap, then we expect topic adapta-tion to help separate them and yield better trans-lations than an unadapted system.
Topics can beof varying granularity and are therefore a flexi-ble means to structure data that is not uniformenough to be modelled in its entirety.
In recentyears there have been several attempts to integrat-ing topical information into SMT either by learn-ing better word alignments (Zhao and Xing, 2006),by adapting translation features cross-domain (Suet al., 2012), or by dynamically adapting lexicalweights (Eidelman et al., 2012) or adding sparsetopic features (Hasler et al., 2012).We take a new approach to topic adaptation byestimating probabilistic phrase translation featuresin a completely Bayesian fashion.
The motivationis that automatically identifying topics in the train-ing data can help to select the appropriate transla-tion of a source phrase in the context of a docu-ment.
By adapting a system to automatically in-duced topics we do not have to trust data from agiven domain to be uniform.
We also overcomethe problem of defining the level of granularity fordomain adaptation.
With more and more trainingdata automatically extracted from the web and lit-tle knowledge about its content, we believe thisis an important area to focus on.
Translation ofweb sites is already a popular application for MTsystems and could be helped by dynamic modeladaptation.
We present results on a mixed dataset of the TED corpus, parts of the Commoncrawlcorpus which contains crawled web data and partsof the News Commentary corpus which contains328Figure 1: Phrasal LDA model for inference ontraining data.documents about politics and economics.
We be-lieve that the broad range of this data set makes it asuitable testbed for topic adaptation.
We focus ontranslation model adaptation to learn how wordsand phrases translate in a given document-contextwithout knowing the origin of the document.
Bylearning translations over latent topics and com-bining several topic-adapted features we achieveimprovements of more than 1 BLEU point.2 Bilingual topic models over phrasepairsOur model is based on LDA and infers topicsas distributions over phrase pairs instead of overwords.
It is specific to machine translation in thatthe conditional dependencies between source andtarget phrases are modelled explicitly, and there-fore we refer to it as phrasal LDA.
Topic distribu-tions learned on a training corpus are carried overto tuning and test sets by running a modified in-ference algorithm on the source side text of thosesets.
Translation probabilities are adapted sepa-rately to each source text under translation whichmakes this a dynamic topic adaptation approach.In the following we explain our approach to topicmodelling with the objective of estimating betterphrase translation probabilities for data sets thatexhibit a heterogeneous structure in terms of vo-cabulary and style.
The advantage from a mod-elling point of view is that unlike with mixturemodels, we avoid sparsity problems that wouldarise if we treated documents or sets of documentsas domains and learned separate models for them.2.1 Latent Dirichlet Allocation (LDA)LDA is a generative model that learns latent top-ics in a document collection.
In the originalformulation, topics are multinomial distributionsover words of the vocabulary and each docu-ment is assigned a multinomial distribution overtopics (Blei et al., 2003).
Our goal is to learntopic-dependent phrase translation probabilitiesand hence we modify this formulation by replac-ing words with phrase pairs.
This is straightfor-ward when both source and target phrases are ob-served but requires a modified inference approachwhen only source phrases are observed in an un-known test set.
Different from standard LDA andprevious uses of LDA for MT, we define a bilin-gual topic model that learns topic distributionsover phrase pairs.
This allows us to model theunits of interest in a more principled way, withoutthe need to map per-word or per-sentence topics tophrase pairs.
Figure 1 shows a graphical represen-tation of the following generative process.For each of N documents in the collection1.
Choose topic distribution ?d?
Dirichlet(?).2.
Choose the number of phrases pairs Pdin thedocument, Pd?
Poisson(?).3.
For every position diin the document corre-sponding to a phrase pair pd,iof source andtarget phrase siand ti1:(a) Choose a topic zd,i?Multinomial(?d).
(b) Conditioned on topic zd,i, choose asource phrase sd,i?Multinomial(?zd,i).
(c) Conditioned on zd,iand sd,i, choose tar-get phrase td,i?Multinomial(?sd,i,zd,i).
?, ?
and ?
are parameters of the Dirichlet dis-tributions, which are asymmetric for k = 0.
Ourinference algorithm is an implementation of col-lapsed variational Bayes (CVB), with a first-orderGaussian approximation (Teh et al., 2006).
It hasbeen shown to be more accurate than standard VBand to converge faster than collapsed Gibbs sam-pling (Teh et al., 2006; Wang and Blunsom, 2013),with little loss in accuracy.
Because we have todo inference over a large number of phrase pairs,CVB is more practical than Gibbs sampling.2.2 Overview of training strategyUltimately, we want to learn translation probabil-ities for all possible phrase pairs that apply to agiven test document during decoding.
Therefore,topic modelling operates on phrase pairs as theywill be seen during decoding.
Given word-alignedparallel corpora from several domains, we extractlists of per-document phrase pairs produced by theextraction algorithm in the Moses toolkit (Koehnet al., 2007) which contain all phrase pairs consis-tent with the word alignment.
We run CVB on theset of all training documents to learn latent topicswithout providing information about the domains.1Parallel documents are modelled as bags of phrase pairs.329Using the trained model, CVB with modified in-ference is run on all test documents with the set ofpossible phrase translations that a decoder wouldload from a phrase table before decoding.
Whentest inference has finished, we compute adaptedtranslation probabilities at the document-level bymarginalising over topics for each phrase pair.3 Bilingual topic inference3.1 Inference on training documentsThe aim of inference on the training data is tofind latent topics in the distributions over phrasepairs in each document.This is done by repeatedlyvisiting all phrase pair positions in all documents,computing conditional topic probabilities and up-dating counts.
To bias the model to cluster stopword phrases in one topic, we place an asymmet-ric prior over the hyperparameters2as described in(Wallach et al., 2009) to make one of the topics apriori more probable in every document.
We usea fixed-point update (Minka, 2012) to update thehyperparameters after every iteration.
For CVBthe conditional probability of topic zd,igiven thecurrent state of all variables except zd,iisP(zd,i= k|z?
(d,i),s, t,d,?,?,?)
?(Eq?[n?(d,i).,k,s,t]+?)(Eq?[n?(d,i).,k,s,.]+Ts??)(Eq?[n?(d,i).,k,s,.
]+ ?)(Eq?[n?(d,i).,k,.
]+S ?
?)?(Eq?[n?(d,i)d,k,.]+?)
(1)where s and t are all source and target phrases inthe collection.
n?
(d,i).,k,s,t, n?
(d,i).,k,s,.and n?
(d,i)d,k,.are cooc-currence counts of topics with phrase pairs, sourcephrases and documents respectively.
Eq?is theexpectation under the variational posterior and incomparison to Gibbs sampling where the posteriorwould otherwise look very similar, counts are re-placed by their means.
n?
(d,i).,k,.is a topic occurrencecount, Tsis the number of possible target phrasesfor a given source phrase and S is the total num-ber of source phrases.
By modelling phrase trans-lation probabilities separately as P(ti|si,zi= k, ..)and P(si|zi= k, ..), we can put different priors onthese distributions.
For example, we want a sparsedistribution over target phrases for a given sourcephrase and topic to express our translation prefer-ence under each topic.
The algorithm stops whenthe variational posterior has converged for all doc-uments or after a maximum of 100 iterations.3.2 Inference on tuning and test documentsTo compute translation probabilities for tuningand test documents where target phrases are not2Omitted from the following equations for simplicity.observed, the variational posterior is adapted asshown in Equation 2P(zd,i= k, ti, j|z?
(d,i),s, t?(d,i),d,?,?,?)
?(Eq?[n?(d,i).,k,s,tj]+?)(Eq?[n?(d,i).,k,s,.]+Ts??)(Eq?[n?(d,i).,k,s,.
]+ ?)(Eq?[n?(d,i).,k,.
]+S ?
?)?(Eq?[n?(d,i)d,k,.]+?)
(2)which now computes the joint conditional prob-ability of a topic k and a target phrase ti, j, given thesource phrase siand the test document d. There-fore, the size of the support changes from K toK ?Ts.
While during training inference we computea distribution over topics for each source-targetpair, in test inference we can use the posterior tomarginalise out the topics and get a distributionover target phrases for each source phrase.We use the Moses decoder to produce lists oftranslation options for each document in the tun-ing and test sets.
These lists comprise all phrasepairs that will enter the search space at decod-ing time.
By default, only 20 target phrases persource phrase are loaded from the phrase table,so in order to allow for new phrase pairs to en-ter the search space and for translation probabil-ities to be computed more accurately, we allowfor up to 200 target phrases per source.
For eachsource sentence, we consider all possible phrasesegmentations and applicable target phrases.
Un-like in training, we do not iterate over all phrasepairs in the list but over blocks of up to 200 targetphrases for a given source phrase.
The algorithmstops when all marginal translation probabilitieshave converged though in practice we stopped ear-lier to avoid overfitting.3.3 Phrase translation probabilitiesAfter topic inference on the tuning and test data,the forward translation probabilities P(t|s,d) arecomputed.
This is done separately for every doc-ument d because we are interested in the trans-lation probabilities that depend on the inferredtopic proportions for a given document.
For ev-ery document, we iterate over source positions pd,iand use the current variational posterior to com-pute P(ti, j|si,d) for all possible target phrases bymarginalizing over topics:P(ti, j|si,d) =?kP(zi= k, ti, j|z?
(d,i),s, t?
(d,i),d)This is straightforward because during test in-ference the variational posterior is normalised toa distribution over topics and target phrases fora given source phrase.
If a source phrase oc-curs multiple times in the same document, theprobabilities are averaged over all occurrences.The inverse translation probabilities can be com-puted analogously except that in cases where we330do not have variational posteriors for a given pairof source and target phrases, an approximation isneeded.
We omit the results here since our exper-iments so far did not indicate improvements withthe inverse features included.4 More topic-adapted featuresInspired by previous work on topic adaptation forSMT, we add three additional topic-adapted fea-tures to our model.
All of these features makeuse of the topic mixtures learned by our bilingualtopic model.
The first feature is an adapted lexi-cal weight, similar to the features in the work ofEidelman et al.
(2012).
Our feature is different inthat we marginalise over topics to produce a singleadapted feature where v[k] is the kthelement of adocument topic vector for document d and w(t|s,k)is a topic-dependent word translation probability:lex(?t|s?,d) =|t|?i1{ j|(i, j) ?
a}??
(i, j)?a?kw(t|s,k) ?
v[k]?
??
?w(t|s)(3)The second feature is a target unigram featuresimilar to the lazy MDI adaptation of Ruiz andFederico (2012).
It includes an additional termthat measures the relevance of a target word wibycomparing its document-specific probability Pdocto its probability under the asymmetric topic 0:trgUnigramst=|t|?i=1f (Pdoc(wi)Pbaseline(wi))?
??
?lazy MDI?
f (Pdoc(wi)Ptopic0(wi))?
??
?relevance(4)f (x) =21+1x, x > 0 (5)The third feature is a document similarity fea-ture, similar to the semantic feature described byBanchs and Costa-juss?
(2011):docSimt= maxi(1?
JSD(vtrain doci,vtest doc)) (6)where vtrain_dociand vtest_docare document topicvector of training and test documents.
Becausetopic 0 captures phrase pairs that are common tomany documents, we exclude it from the topicvectors before computing similarities.4.1 Feature combinationWe tried integrating the four topic-adapted fea-tures separately and in all possible combinations.As we will see in the results section, while all fea-tures improve over the baseline in isolation, theadapted translation feature P(t|s,d) is the strongestfeature.
For the features that have a counterpart inthe baseline model (p(t|s,d) and lex(t|s,d)), we ex-perimented with either adding or replacing them inData Mixed CC NC TEDTrain 354K (6450) 110K 103K 140KDev 2453 (39) 818 817 818Test 5664 (112) 1892 1878 1894Table 1: Number of sentence pairs and documents(in brackets) in the French-English data sets.
Thetraining data has 2.7M English words per domain.the log-linear model.
We found that while addingthe features worked well and yielded close to zeroweights for their baseline counterparts after tun-ing, replacing them yielded better results in com-bination with the other adapted features.
We be-lieve the reason could be that fewer phrase tablefeatures in total are easier to optimise.5 Experimental setup5.1 Data and baselinesOur experiments were carried out on a mixeddata set, containing the TED corpus (Cettolo etal., 2012), parts of the News Commentary cor-pus (NC) and parts of the Commoncrawl corpus(CC) from the WMT13 shared task (Bojar et al.,2013) as described in Table 1.
We were guidedby two constraints in chosing our data set.
1) thedata has document boundaries and the content ofeach document is assumed to be topically related,2) there is some degree of topical variation withineach data set.
In order to compare to domain adap-tation approaches, we chose a setup with data fromdifferent corpora.
We want to abstract away fromadaptation effects that concern tuning of lengthpenalties and language models, so we use a mixedtuning set containing data from all three domainsand train one language model on the concatenationof (equally sized) target sides of the training data.Word alignments are trained on the concatenationof all training data and fixed for all models.Our baseline (ALL) is a phrase-based French-English system trained on the concatenation ofall parallel data.
It was built with the Mosestoolkit (Koehn et al., 2007) using the 14 standardcore features including a 5gram language model.Translation quality is evaluated on a large test set,using the average feature weights of three optimi-sation runs with PRO (Hopkins and May, 2011).We use the mteval-v13a.pl script to compute case-insensitive BLEU.
As domain-aware benchmarksystems, we use the phrase table fill-up method(FILLUP) of Bisazza et al.
(2011) which pre-serves the translation scores of phrases from theIN model and the linear mixture models (LIN-TM) of Sennrich (2012b) (both available in theMoses toolkit).
For both systems, we build sepa-rate phrase tables for each domain and use a wrap-per to decode tuning and test sets with domain-specific tables.
Both benchmarks have an advan-331Model Mixed CC NC TEDIN 26.77 18.76 29.56 32.47ALL 26.86 19.61 29.42 31.88Table 2: BLEU of in-domain and baseline models.Model Avg JSD Rank1-diffTed-IN vs ALL 0.15 10.8%CC-IN vs ALL 0.17 18.4%NC-IN vs ALL 0.13 13.3%Table 3: Average JSD of IN vs. ALL models.Rank1-diff: % PT entries where preferred transla-tion changes.tage over our model because they are aware of do-main boundaries in the test set.
Further, LIN-TMadapts phrase table features in both translation di-rections while we only adapt the forward features.Table 2 shows BLEU scores of the baseline sys-tem as well as the performance of three in-domainmodels (IN) tuned under the same conditions.
Forthe IN models, every portion of the test set is de-coded with a domain-specific model.
Results onthe test set are broken down by domain but alsoreported for the entire test set (mixed).
For Tedand NC, the in-domain models perform better thanALL, while for CC the all-domain model improvesquite significantly over IN.5.2 General properties of the data setsIn this section we analyse some internal propertiesof our three data sets that are relevant for adapta-tion.
All of the scores were computed on the setsof source side tokens of the test set which werelimited to contain content words (nouns, verbs, ad-jectives and adverbs).
The test set was tagged withthe French TreeTagger (Schmid, 1994).
The top ofTable 3 shows the average Jensen-Shannon diver-gence (using log2, JSD ?
[0,1]) of each in-domainmodel in comparison to the all-domain model,which is an indicator of how much the distribu-tions in the IN model change when adding out-of-domain data.
Likewise, Rank1-diff gives the per-centage of word tokens in the test set where thepreferred translation according to p(e| f ) changesbetween IN and ALL.
These are the words thatare most affected by adding data to the IN model.Both numbers show that for Commoncrawl the INand ALL models differ more than in the other twodata sets.
According to the JS divergence betweenNC-IN and ALL, translation distibutions in the NCphrase table are most similar to the ALL phrasetable.
Table 4 shows the average JSD for each INmodel compared to a model trained on half of itsin-domain data.
This score gives an idea of howdiverse a data set is, measured by comparing dis-tributions over translations for source words in thetest set.
According to this score, Commoncrawlis the most diverse data set and Ted the most uni-Model Avg JSDTed-half vs Ted-full 0.07CC-half vs CC-full 0.17NC-half vs NC-full 0.09Table 4: Average JSD of in-domain modelstrained on half vs. all of the data.form.
Note however, that these divergence scoresdo not provide information about the relative qual-ity of the systems under comparison.
For CC,the ALL model yields a much higher BLEU scorethan the IN model and it is likely that this is due tonoisy data in the CC corpus.
In this case, the highdivergence is likely to mean that distributions arecorrected by out-of-domain data rather than beingshifted away from in-domain distributions.5.3 Topic-dependent decodingThe phrase translation probabilities and additionalfeatures described in the last two sections are usedas features in the log-linear translation model inaddition to the baseline translation features.
Whencombining all four adapted features, we replaceP(t|s) and lex(t|s) by their adapted counterparts.We construct separate phrase tables for each doc-ument in the development and test sets and use awrapper around the decoder to ensure that each in-put document is paired with a configuration filepointing to its document-specific translation table.Documents are decoded in sequence so that onlyone phrase table needs to be loaded at a time.
Us-ing the wrapped decoder we can run parameter op-timisation (PRO) in the usual way to get one set oftuned weights for all test documents.6 ResultsIn this section we present experimental resultswith phrasal LDA.
We show BLEU scores in com-parison to a baseline system and two domain-aware benchmark systems.
We also evaluatethe adapted translation distributions by looking attranslation probabilities under specific topics andinspect translations of ambiguous source words.6.1 Analyis of bilingual topic modelsWe experimented with different numbers of top-ics for phrasal LDA.
The diagrams in Figure 2shows blocks of training and test documents ineach of the three domains for a model with 20 top-ics.
Darker shading means that documents havea higher proportion of a particular topic in theirdocument-topic distribution.
The first topic is theone that was affected by the asymmetric prior andinspecting its most probable phrase pairs showedthat it had ?collected?
a large number of stop wordphrases.
This explains why it is the topic thatis most shared across documents and domains.332Figure 2: Document-topic distributions for train-ing (top) and test (bottom) documents, grouped bydomain and averaged into blocks for visualisation.Topic 8 Topic 11europ?enne?
european crise?
crisispolitiques?
political taux?
ratepolitique?
policy financi?re?financialint?r?ts?
interests mon?taire?
monetaryTopic 14 Topic 19h?tel?
hotel web?
webplage?
beach utiliser?
usesitu??
located logiciel?
softwarechambres?
bedrooms donn?es?
dataFigure 3: Frequent phrase pairs in learned topics.There is quite a clear horizontal separation be-tween documents of different domains, for exam-ple, topics 6, 8, 19 occur mostly in Ted, NC andCC documents respectively.
The overall structureis very similar between training (top) and test (bot-tom) documents, which shows that test inferencewas successful in carrying over the informationlearned on training documents.
There is also somedegree of topic sharing across domains, for exam-ple topics 4 and 15 occur in documents of all threedomains.
Figure 3 shows examples of latent topicsfound during inference on the training data.
Topic8 and 11 seem to be about politics and economyand occur frequently in documents from the NCcorpus.
Topic 14 contains phrases related to ho-tels and topic 19 is about web and software, bothfrequent themes in the CC corpus.6.2 Comparison according to BLEUIn Table 5 we compare our topic-adapted featureswhen added separately to the baseline phrase ta-ble.
The inclusion of each feature improves overthe concatenation baseline but the combinationof all four features gives the best overall results.Though the relative performance differs slightlyfor each domain portion in the test set, overall theadapted lexical weight is the weakest feature andthe adapted translation probability is the strongestfeature.
We also performed feature ablation testsand found that no combination of features was su-perior to combining all four features.
This con-firms that the gains of each feature lead to additiveimprovements in the combined model.In Table 6 we compare topic-adapted modelsModel Mixed CC NC TEDlex(e|f,d) 26.99 19.93 29.34 32.19trgUnigrams 27.15 19.90 29.54 32.50docSim 27.22 20.11 29.63 32.40p(e|f,d) 27.31 20.23 29.52 32.58All features 27.67 20.40 30.04 33.08Table 5: BLEU scores of pLDA features (50 top-ics), separately and combined.Model Mixed CC NC TEDALL -26.86 19.61 29.42 31.883 topics -26.95 19.83 29.46 32.025 topics *27.48 19.98 29.94 33.0410 topics *27.65 20.34 29.99 33.1420 topics *27.63 20.39 29.93 33.0950 topics *27.67 20.40 30.04 33.08100 topics *27.65 20.54 30.00 32.90>ALL +0.81 +0.93 +0.62 +1.26Table 6: BLEU scores of baseline and topic-adapted systems (pLDA) with all 4 features andlargest improvements over baseline.with varying numbers of topics to the concatena-tion baseline.
We see a consistent gain on all do-mains when increasing the number of topics fromthree to five and ten topics.
This is evidence thatthe number of domain labels is in fact smallerthan the number of underlying topics.
The opti-mal number of latent topics varies for each domainand reflects our insights from section 5.2.
The CCdomain was shown to be the most diverse and thebest performance on the CC portion of the test setis achieved with 100 topics.
Likewise, the TEDdomain was shown to be least diverse and herethe best performance is achieved with only 10 top-ics.
The best performance on the entire test set isachieved with 50 topics, which is also the optimalnumber of topics for the NC domain.
The bot-ton row of the table indicates the relative improve-ment of the best topic-adapted model per domainover the ALL model.
Using all four topic-adaptedfeatures yields an improvement of 0.81 BLEU onthe mixed test set.
The highest improvement on agiven domain is achieved for TED with an increaseof 1.26 BLEU.
The smallest improvement is mea-sured on the NC domain.
This is in line with theobservation that distributions in the NC in-domaintable are most similar to the ALL table, thereforewe would expect the smallest improvement for do-main or topic adaptation.
We used bootstrap re-sampling (Koehn, 2004) to measure significanceon the mixed test set and marked all statisticallysignificant results compared to the respective base-lines with asterisk (*: p?
0.01).To demonstrate the benefit of topic adaptationover more standard domain adaptation approachesfor a diverse data set, we show the performance333Model Mixed CC NC TEDFILLUP -27.12 19.36 29.78 32.71LIN-TM -27.24 19.61 29.87 32.73pLDA *27.67 20.40 30.04 33.08>FILLUP +0.55 +1.04 +0.26 +0.37>LIN-TM +0.43 +0.79 +0.17 +0.35Table 7: Comparison of best pLDA system withtwo domain-aware benchmark systems.Model Mixed CC NC TEDLIN-LM+ ALL -27.16 19.71 29.77 32.46+ FILLUP -27.20 19.37 29.84 32.90+ LIN-TM -27.34 19.59 29.92 33.02+ pLDA *27.84 20.48 30.03 33.57>ALL +0.68 +0.77 +0.26 +1.11Table 8: Combination of all models with addi-tional LM adaptation (pLDA: 50 topics).of two state-of-the-art domain-adapted systems inTable 7.
Both FILLUP and LIN-TM improve overthe ALL model on the mixed test set, by 0.26 and0.38 BLEU respectively.
The largest improvementis on TED while on the CC domain, FILLUP de-creases in performance and LIN-TM yields no im-provement either.
This shows that relying on in-domain distributions for adaptation to a noisy anddiverse domain like CC is problematic.
The pLDAmodel yields the largest improvement over thedomain-adapted systems on the CC test set, within increase of 1.04 BLEU over FILLUP and 0.79over LIN-TM.
The improvements on the other twodomains are smaller but consistent.We also compare the best model from Table 6to all other models in combination with linearlyinterpolated language models (LIN-LM), interpo-lated separately for each domain.
Though theimprovements are slightly smaller than withoutadapted language models, there is still a gain overthe concatenation baseline of 0.68 BLEU on themixed test set and similar improvements to beforeover the benchmarks (on TED the improvementsare actually even larger).
Thus, we have shownthat topic-adaptation is effective for test sets ofdiverse documents and that we can achieve sub-stantial improvements even in comparison withdomain-adapted translation and language models.6.3 Properties of adapted distributions andtopic-specific translationsThe first column of Table 9 shows the average en-tropy of phrase table entries in the adapted modelsaccording to p(t|s,d) versus the all-domain model,computed over source tokens in the test set thatare content words.
The entropy decreases in theadapted tables in all cases which is an indicatorthat the distributions over translations of contentSet Model Avg entropy Avg perplexityCCpLDA 3.74 9.21ALL 3.99 10.13NCpLDA 3.42 6.96ALL 3.82 7.51TEDpLDA 3.33 9.17ALL 4.00 9.71Table 9: Average entropy of translation distribu-tions and test set perplexity of the adapted model.r?gimetopic 6 diet = 0.79 diet aids = 0.04topic 8 regime* = 0.82 rule = 0.05topic 19 restrictions = 0.53 diplomats = 0.10noyautopic 9 nucleus* = 0.89 core = 0.01topic 11 core* = 0.93 inner = 0.03topic 19 kernel = 0.58 core = 0.11d?montopic 6 devil = 0.89 demon = 0.07topic 8 demon* = 0.98 devil = 0.01topic 19 daemon = 0.95 demon = 0.04Table 10: The two most probable translations ofr?gime, noyau and d?mon and probabilities underdifferent latent topics (*: preferred by ALL).words have become more peaked.
The second col-umn shows the average perplexity of target tokensin the test set which is a measure of how likely amodel is to produce words in the reference trans-lation.
We use the alignment information betweensource and reference and therefore limit our anal-ysis to pairs of aligned words, but neverthelessthis shows that the adapted translation distribu-tions model the test set distributions better than thebaseline model.
Therefore, the adapted distribu-tions are not just more peaked but also more oftenpeaked towards the correct translation.Table 10 shows examples of ambiguous Frenchwords that have different preferred translations de-pending on the latent topic.
The word r?gime canbe translated as diet, regime and restrictions andthe model has learned that the probability overtranslations changes when moving from one topicto another (preferred translations under the ALLmodel are marked with *).
For example, the trans-lation to diet is most probable under topic 6 andthe translation to regime which would occur ina political context is most probable under topic8.
Topic 6 is most prominent among Ted docu-ments while topic 8 is found most frequently inNews Commentary documents which have a highpercentage of politically related text.
The Frenchword noyau can be translated to nucleus (physics),core (generic) and kernel (IT) among other trans-lations and the topics that exhibit these preferredtranslations can be attributed to Ted (which con-tains many talks about physics), NC and CC (with334Src: ?il suffit d?
?jecter le noyau et d?en ins?rer un autre, comme ce qu?on fait pour le cl?nage.
?BL: ?it is the nucleus eject and insert another, like what we do to the cl?nage.
?pLDA: ?he just eject the nucleus and insert another, like what we do to the cl?nage.?
(nucleus = 0.77)Ref: ?you can just pop out the nucleus and pop in another one, and that?s what you?ve all heard about with cloning.
?Src: ?pourtant ceci obligerait les contribuables des pays de ce noyau ?
fournir du capital au sud?BL: ?but this would force western taxpayers to provide the nucleus of capital in the south?pLDA: ?but this would force western taxpayers to provide the core of capital in the south?
(core = 0.78)Ref: ?but this would unfairly force taxpayers in the core countries to provide capital to the south?Src: ?le noyau contient de nombreux pilotes, afin de fonctionner chez la plupart des utilisateurs.
?BL: ?the nucleus contains many drivers, in order to work for most users.
?pLDA: ?the kernel contains many drivers, to work for most users.?
(kernel = 0.53)Ref: ?the precompiled kernel includes a lot of drivers, in order to work for most users.
?Figure 4: pLDA correctly translates noyau in test docs from Ted, NC and CC (adapted probabilities inbrackets).
The baseline (nucleus = 0.27, core = 0.27, kernel = 0.23) translates all instances to nucleus.many IT-related documents).
The last example,d?mon, has three frequent translations in English:devil, demon and daemon.
The last translationrefers to a computer process and would occur in anIT context.
The topic-phrase probabilities revealthat its mostly likely translation as daemon occursunder topic 19 which clusters IT-related phrasepairs and is frequent in the CC corpus.
Theseexamples show that our model can disambiguatephrase translations using latent topics.As another motivating example, in Figure 4 wecompare the output of our adapted models to theoutput produced by the all-domain baseline for theword noyau from Table 10.
While the ALL base-line translates each instance of noyau to nucleus,the adapted model translates each instance differ-ently depending on the inferred topic mixtures foreach document and always matches the referencetranslation.
The probabilities in brackets showthat the chosen translations were indeed the mostlikely under the respective adapted model.
Whilethe ALL model has a flat distribution over pos-sible translations, the adapted models are peakedtowards the correct translation.
This shows thattopic-specific translation probabilities are neces-sary when the translation of a word shifts betweentopics or domains and that peaked, adapted distri-butions can lead to more correct translations.7 Related workThere has been a lot of previous work using topicinformation for SMT, most of it using monolin-gual topic models.
For example, Gong and Zhou(2011) use the topical relevance of a target phrase,computed using a mapping between source andtarget side topics, as an additional feature in de-coding.
Axelrod et al.
(2012) build topic-specifictranslation models from the TED corpus and se-lect topic-relevant data from the UN corpus to im-prove coverage.
Su et al.
(2012) perform phrasetable adaptation in a setting where only monolin-gual in-domain data and parallel out-of-domaindata are available.
Eidelman et al.
(2012) usetopic-dependent lexical weights as features in thetranslation model, which is similar to our workin that topic features are tuned towards useful-ness of topic information and not towards a tar-get domain.
Hewavitharana et al.
(2013) per-form dynamic adaptation with monolingual top-ics, encoding topic similarity between a conversa-tion and training documents in an additional fea-ture.
This is similar to the work of Banchs andCosta-juss?
(2011), both of which inspired ourdocument similarity feature.
Also related is thework of Sennrich (2012a) who explore mixture-modelling on unsupervised clusters for domainadaptation and Chen et al.
(2013) who computephrase pair features from vector space representa-tions that capture domain similarity to a develop-ment set.
Both are cross-domain adaptation ap-proaches, though.
Instances of multilingual topicmodels outside the field of MT include Boyd-Graber and Blei (2009; Boyd-Graber and Resnik(2010) who learn cross-lingual topic correspon-dences (but do not learn conditional distributionslike our model does).
In terms of model structure,our model is similar to BiTAM (Zhao and Xing,2006) which is an LDA-style model to learn topic-based word alignments.
The work of Carpuat andWu (2007) is similar to ours in spirit, but they pre-dict the most probable translation in a context atthe token level while our adaptation operates at thetype level of a document.8 ConclusionWe have presented a novel bilingual topic modelbased on LDA and applied it to the task of transla-tion model adaptation on a diverse French-Englishdata set.
Our model infers topic distributions overphrase pairs to compute document-specific trans-lation probabilities and performs dynamic adap-tation on test documents of unknown origin.
Wehave shown that our model outperforms a concate-nation baseline and two domain-adapted bench-mark systems with BLEU gains of up to 1.26 ondomain-specific test set portions and 0.81 overall.We have also shown that a combination of topic-adapted features performs better than each featurein isolation and that these gains are additive.
Ananalysis of the data revealed that topic adaptationcompares most favourably to domain adaptationwhen the domain in question is rather diverse.335AcknowledgementsThis work was supported by funding from theScottish Informatics and Computer Science Al-liance (Eva Hasler) and funding from the Eu-ropean Union Seventh Framework Programme(FP7/2007-2013) under grant agreement 287658(EU BRIDGE) and grant agreement 288769 (AC-CEPT).
Thanks to Chris Dyer for an initial discus-sion about the phrasal LDA model.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of EMNLP.
Associationfor Computational Linguistics.Amittai Axelrod, Xiaodong He, Li Deng, Alex Acero,and Mei-Yuh Hwang.
2012.
New methods andevaluation experiments on translating TED talks inthe IWSLT benchmark.
In Proceedings of ICASSP.IEEE.Rafael E. Banchs and Marta R. Costa-juss?.
2011.
Asemantic feature for statistical machine translation.In Proceedings of the Fifth Workshop on Syntax,Semantics and Structure in Statistical Translation,SSST-5.
Association for Computational Linguistics.Arianna Bisazza, Nick Ruiz, and Marcello Federico.2011.
Fill-up versus Interpolation Methods forPhrase-based SMT Adaptation.
In Proceedings ofIWSLT.David M. Blei, Andrew Y. Ng, Michael I. Jordan, andJohn Lafferty.
2003.
Latent dirichlet allocation.JMLR.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of WMT 2013.
Asso-ciation for Computational Linguistics.Jordan Boyd-Graber and David Blei.
2009.
Multilin-gual Topic Models for Unaligned Text.
In Proceed-ings of the Twenty-Fifth Conference on Uncertaintyin Artificial Intelligence.
AUAI Press.Jordan Boyd-Graber and Philip Resnik.
2010.
HolisticSentiment Analysis Across Languages: MultilingualSupervised Latent Dirichlet Allocation.
In Proceed-ings of EMNLP.
Association for Computational Lin-guistics.Marine Carpuat and Dekai Wu.
2007.
How phrasesense disambiguation outperforms word sense dis-ambiguation for SMT.
In International Conferenceon Theoretical and Methodological Issues in MT.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
Wit3: Web inventory of transcribedand translated talks.
In Proceedings of EAMT.Boxing Chen, Roland Kuhn, and George Foster.
2013.Vector space model for adaptation in SMT.
InProceedings of ACL.
Association for ComputationalLinguistics.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic models for dynamic translationmodel adaptation.
In Proceedings of ACL.
Associa-tion for Computational Linguistics.G.
Foster and R. Kuhn.
2007.
Mixture-model adapta-tion for SMT.
In Proceedings of WMT.
Associationfor Computational Linguistics.G.
Foster, C. Goutte, and R. Kuhn.
2010.
Discrimi-native instance weighting for domain adaptation inSMT.
In Proceedings of EMNLP.
Association forComputational Linguistics.Zhengxian Gong and Guodong Zhou.
2011.
Employ-ing topic modeling for SMT.
In Proceedings ofIEEE (CSAE), volume 4.Eva Hasler, Barry Haddow, and Philipp Koehn.
2012.Sparse lexicalised features and topic adaptation forSMT.
In Proceedings of IWSLT.S.
Hewavitharana, D. Mehay, S. Ananthakrishnan, andP.
Natarajan.
2013.
Incremental topic-based TMadaptation for conversational SLT.
In Proceedingsof ACL.
Association for Computational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of EMNLP.
Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for SMT.
In ACL 2007: Demo andposter sessions.
Association for Computational Lin-guistics.Philipp Koehn.
2004.
Statistical significance testsfor machine translation evaluation.
In Proceedingsof EMNLP.
Association for Computational Linguis-tics.S.
Matsoukas, A. Rosti, and B. Zhang.
2009.
Discrim-inative corpus weight estimation for MT.
In Pro-ceedings of EMNLP.
Association for ComputationalLinguistics.Thomas P Minka.
2012.
Estimating a Dirichlet distri-bution.
Technical report.Nick Ruiz and Marcello Federico.
2012.
MDI Adap-tation for the Lazy: Avoiding Normalization in LMAdaptation for Lecture Translation.
In Proceedingsof IWSLT.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing.336Rico Sennrich.
2012a.
Mixture-modeling with unsu-pervised clusters for domain adaptation in SMT.
InProceedings of EAMT.Rico Sennrich.
2012b.
Perplexity Minimization forTranslation Model Domain Adaptation in SMT.
InProceedings of EACL.
Association for Computa-tional Linguistics.J.
Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong,and Q. Liu.
2012.
Translation model adaptationfor SMT with monolingual topic information.
InProceedings of ACL.
Association for ComputationalLinguistics.Yee Whye Teh, David Newman, and Max Welling.2006.
A collapsed variational Bayesian inferencealgorithm for LDA.
In Proceedings of NIPS.Hanna M. Wallach, David M. Mimno, and Andrew Mc-Callum.
2009.
Rethinking LDA: Why priors matter.In Proceedings of NIPS.Pengyu Wang and Phil Blunsom.
2013.
Collapsedvariational Bayesian inference for Hidden MarkovModels.
In AISTATS, volume 31 of JMLR Proceed-ings, pages 599?607.Bing Zhao and Eric P. Xing.
2006.
Bilingual topic ad-mixture models for word alignment.
In Proceedingsof ACL.
Association for Computational Linguistics.337
