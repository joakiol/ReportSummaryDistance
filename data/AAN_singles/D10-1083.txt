Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 853?861,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsSimple Type-Level Unsupervised POS TaggingYoong Keok Lee Aria Haghighi Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{yklee, aria42, regina}@csail.mit.eduAbstractPart-of-speech (POS) tag distributions areknown to exhibit sparsity ?
a word is likelyto take a single predominant tag in a corpus.Recent research has demonstrated that incor-porating this sparsity constraint improves tag-ging accuracy.
However, in existing systems,this expansion come with a steep increase inmodel complexity.
This paper proposes a sim-ple and effective tagging method that directlymodels tag sparsity and other distributionalproperties of valid POS tag assignments.
Inaddition, this formulation results in a dramaticreduction in the number of model parame-ters thereby, enabling unusually rapid training.Our experiments consistently demonstrate thatthis model architecture yields substantial per-formance gains over more complex taggingcounterparts.
On several languages, we reportperformance exceeding that of more complexstate-of-the art systems.11 IntroductionSince the early days of statistical NLP, researchershave observed that a part-of-speech tag distributionexhibits ?one tag per discourse?
sparsity ?
wordsare likely to select a single predominant tag in a cor-pus, even when several tags are possible.
Simplyassigning to each word its most frequent associatedtag in a corpus achieves 94.6% accuracy on the WSJportion of the Penn Treebank.
This distributionalsparsity of syntactic tags is not unique to English1The source code for the work presented in this paper isavailable at http://groups.csail.mit.edu/rbg/code/typetagging/.?
similar results have been observed across multi-ple languages.
Clearly, explicitly modeling such apowerful constraint on tagging assignment has a po-tential to significantly improve the accuracy of anunsupervised part-of-speech tagger learned withouta tagging dictionary.In practice, this sparsity constraint is difficultto incorporate in a traditional POS induction sys-tem (Me?rialdo, 1994; Johnson, 2007; Gao and John-son, 2008; Grac?a et al, 2009; Berg-Kirkpatricket al, 2010).
These sequence models-based ap-proaches commonly treat token-level tag assignmentas the primary latent variable.
By design, they read-ily capture regularities at the token-level.
However,these approaches are ill-equipped to directly repre-sent type-based constraints such as sparsity.
Pre-vious work has attempted to incorporate such con-straints into token-level models via heavy-handedmodifications to inference procedure and objectivefunction (e.g., posterior regularization and ILP de-coding) (Grac?a et al, 2009; Ravi and Knight, 2009).In most cases, however, these expansions come witha steep increase in model complexity, with respectto training procedure and inference time.In this work, we take a more direct approach andtreat a word type and its allowed POS tags as a pri-mary element of the model.
The model starts by gen-erating a tag assignment for each word type in a vo-cabulary, assuming one tag per word.
Then, token-level HMM emission parameters are drawn condi-tioned on these assignments such that each word isonly allowed probability mass on a single assignedtag.
In this way we restrict the parameterization of a853Language Original caseEnglish 94.6Danish 96.3Dutch 96.6German 95.5Spanish 95.4Swedish 93.3Portuguese 95.6Table 1: Upper bound on tagging accuracy assuming eachword type is assigned to majority POS tag.
Across alllanguages, high performance can be attained by selectinga single tag per word type.token-level HMM to reflect lexicon sparsity.
Thismodel admits a simple Gibbs sampling algorithmwhere the number of latent variables is proportionalto the number of word types, rather than the size ofa corpus as for a standard HMM sampler (Johnson,2007).There are two key benefits of this model architec-ture.
First, it directly encodes linguistic intuitionsabout POS tag assignments: the model structurereflects the one-tag-per-word property, and a type-level tag prior captures the skew on tag assignments(e.g., there are fewer unique determiners than uniquenouns).
Second, the reduced number of hidden vari-ables and parameters dramatically speeds up learn-ing and inference.We evaluate our model on seven languages ex-hibiting substantial syntactic variation.
On severallanguages, we report performance exceeding that ofstate-of-the art systems.
Our analysis identifies threekey factors driving our performance gain: 1) select-ing a model structure which directly encodes tagsparsity, 2) a type-level prior on tag assignments,and 3) a straightforward na?
?ve-Bayes approach toincorporate features.
The observed performancegains, coupled with the simplicity of model imple-mentation, makes it a compelling alternative to ex-isting more complex counterparts.2 Related WorkRecent work has made significant progress on unsu-pervised POS tagging (Me?rialdo, 1994; Smith andEisner, 2005; Haghighi and Klein, 2006; Johnson,2007; Goldwater and Griffiths, 2007; Gao and John-son, 2008; Ravi and Knight, 2009).
Our work isclosely related to recent approaches that incorporatethe sparsity constraint into the POS induction pro-cess.
This line of work has been motivated by em-pirical findings that the standard EM-learned unsu-pervised HMM does not exhibit sufficient word tagsparsity.The extent to which this constraint is enforcedvaries greatly across existing methods.
On one endof the spectrum are clustering approaches that assigna single POS tag to each word type (Schutze, 1995;Lamar et al, 2010).
These clusters are computed us-ing an SVD variant without relying on transitionalstructure.
While our method also enforces a singetag per word constraint, it leverages the transitiondistribution encoded in an HMM, thereby benefitingfrom a richer representation of context.Other approaches encode sparsity as a soft con-straint.
For instance, by altering the emission distri-bution parameters, Johnson (2007) encourages themodel to put most of the probability mass on fewtags.
This design does not guarantee ?structural ze-ros,?
but biases towards sparsity.
A more force-ful approach for encoding sparsity is posterior reg-ularization, which constrains the posterior to havea small number of expected tag assignments (Grac?aet al, 2009).
This approach makes the training ob-jective more complex by adding linear constraintsproportional to the number of word types, whichis rather prohibitive.
A more rigid mechanism formodeling sparsity is proposed by Ravi and Knight(2009), who minimize the size of tagging grammaras measured by the number of transition types.
Theuse of ILP in learning the desired grammar signif-icantly increases the computational complexity ofthis method.In contrast to these approaches, our method di-rectly incorporates these constraints into the struc-ture of the model.
This design leads to a significantreduction in the computational complexity of train-ing and inference.Another thread of relevant research has exploredthe use of features in unsupervised POS induc-tion (Smith and Eisner, 2005; Berg-Kirkpatrick etal., 2010; Hasan and Ng, 2009).
These methodsdemonstrated the benefits of incorporating linguis-tic features using a log-linear parameterization, butrequires elaborate machinery for training.
In our854work, we demonstrate that using a simple na?
?ve-Bayes approach also yields substantial performancegains, without the associated training complexity.3 Generative StoryWe consider the unsupervised POS induction prob-lem without the use of a tagging dictionary.
A graph-ical depiction of our model as well as a summaryof random variables and parameters can be found inFigure 1.
As is standard, we use a fixed constant Kfor the number of tagging states.Model Overview The model starts by generatinga tag assignment T for each word type in a vocab-ulary, assuming one tag per word.
Conditioned onT , features of word types W are drawn.
We referto (T ,W ) as the lexicon of a language and ?
forthe parameters for their generation; ?
depends on asingle hyperparameter ?.Once the lexicon has been drawn, the model pro-ceeds similarly to the standard token-level HMM:Emission parameters ?
are generated conditioned ontag assignments T .
We also draw transition param-eters ?.
Both parameters depend on a single hy-perparameter ?.
Once HMM parameters (?, ?)
aredrawn, a token-level tag and word sequence, (t, w),is generated in the standard HMM fashion: a tag se-quence t is generated from ?.
The correspondingtoken words w are drawn conditioned on t and ?.2Our full generative model is given by:P (T ,W , ?, ?, ?, t,w|?, ?)
=P (T ,W , ?|?)
[Lexicon]P (?, ?|T , ?, ?)
[Parameter]P (w, t|?, ?)
[Token]We refer to the components on the right hand sideas the lexicon, parameter, and token component re-spectively.
Since the parameter and token compo-nents will remain fixed throughout experiments, webriefly describe each.Parameter Component As in the standardBayesian HMM (Goldwater and Griffiths, 2007),all distributions are independently drawn fromsymmetric Dirichlet distributions:2Note that t and w denote tag and word sequences respec-tively, rather than individual tokens or tags.P (?, ?|T , ?, ?)
=K?t=1(P (?t|?
)P (?t|T , ?
))The transition distribution ?t for each tag t is drawnaccording to DIRICHLET(?,K), where ?
is theshared transition and emission distribution hyperpa-rameter.
In total there are O(K2) parameters asso-ciated with the transition parameters.In contrast to the Bayesian HMM, ?t is notdrawn from a distribution which has support foreach of the n word types.
Instead, we conditionon the type-level tag assignments T .
Specifically,let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord-ing to the tag assignments T .
Then ?t is drawn fromDIRICHLET(?, St), a symmetric Dirichlet whichonly places mass on word types indicated by St.This ensures that each word will only be assigneda single tag at inference time (see Section 4).Note that while the standard HMM, has O(Kn)emission parameters, our model has O(n) effectiveparameters.3Token Component Once HMM parameters (?, ?
)have been drawn, the HMM generates a token-levelcorpus w in the standard way:P (w, t|?, ?)
=?(w,t)?(w,t)??
?jP (tj |?tj?1)P (wj |tj , ?tj )?
?Note that in our model, conditioned on T , there isprecisely one t which has non-zero probability forthe token component, since for each word, exactlyone ?t has support.3.1 Lexicon ComponentWe present several variations for the lexical com-ponent P (T ,W |?
), each adding more complex pa-rameterizations.Uniform Tag Prior (1TW) Our initial lexiconcomponent will be uniform over possible tag assign-ments as well as word types.
Its only purpose is3This follows since each ?t has St ?
1 parameters andPt St = n.855?KTWNTYPETOKENw1t1??w2t2??wmtm??N??
?K: Word types                           (obs)VARIABLESW: Tag assigns(T1, .
.
.
, Tn)(W1, .
.
.
,Wn)T: Token word seqs  (obs)w: Token tag assigns (det by     )t??PARAMETERST?
: Lexicon parameters: Token word emission parameters: Token tag transition parameters?
?Figure 1: Graphical depiction of our model and summary of latent variables and parameters.
The type-level tagassignments T generate features associated with word types W .
The tag assignments constrain the HMM emissionparameters ?.
The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure.The hyperparameters ?
and ?
represent the concentration parameters of the token- and type-level components of themodel respectively.
They are set to fixed constants.to explore how well we can induce POS tags usingonly the one-tag-per-word constraint.
Specifically,the lexicon is generated as:P (T ,W |?)
=P (T )P (W |T )=n?i=1P (Ti)P (Wi|Ti) =(1Kn)nThis model is equivalent to the standard HMM ex-cept that it enforces the one-word-per-tag constraint.Learned Tag Prior (PRIOR) We next assumethere exists a single prior distribution ?
over tag as-signments drawn from DIRICHLET(?,K).
This al-ters generation of T as follows:P (T |?)
=n?i=1P (Ti|?
)Note that this distribution captures the frequency ofa tag across word types, as opposed to tokens.
TheP (T |?)
distribution, in English for instance, shouldhave very low mass for the DT (determiner) tag,since determiners are a very small portion of the vo-cabulary.
In contrast, NNP (proper nouns) form alarge portion of vocabulary.
Note that these observa-tions are not modeled by the standard HMM, whichinstead can model token-level frequency.Word Type Features (FEATS): Past unsuper-vised POS work have derived benefits from featureson word types, such as suffix and capitalization fea-tures (Hasan and Ng, 2009; Berg-Kirkpatrick et al,2010).
Past work however, has typically associ-ated these features with token occurrences, typicallyin an HMM.
In our model, we associate these fea-tures at the type-level in the lexicon.
Here, we con-sider suffix features, capitalization features, punctu-ation, and digit features.
While possible to utilizethe feature-based log-linear approach described inBerg-Kirkpatrick et al (2010), we adopt a simplerna?
?ve Bayes strategy, where all features are emittedindependently.
Specifically, we assume each wordtype W consists of feature-value pairs (f, v).
Foreach feature type f and tag t, a multinomial ?tf isdrawn from a symmetric Dirichlet distribution withconcentration parameter ?.
The P (W |T , ?)
termin the lexicon component now decomposes as:P (W |T , ?)
=n?i=1P (Wi|Ti, ?)=n?i=1???
(f,v)?WiP (v|?Tif )?
?8564 Learning and InferenceFor inference, we are interested in the posteriorprobability over the latent variables in our model.During training, we treat as observed the languageword types W as well as the token-level corpus w.We utilize Gibbs sampling to approximate our col-lapsed model posterior:P (T ,t|W ,w, ?, ?)
?
P (T , t,W ,w|?, ?
)=?P (T , t,W ,w, ?, ?, ?,w|?, ?
)d?d?d?Note that given tag assignments T , there is only onesetting of token-level tags t which has mass in theabove posterior.
Specifically, for the ith word type,the set of token-level tags associated with token oc-currences of this word, denoted t(i), must all takethe value Ti to have non-zero mass.
Thus in the con-text of Gibbs sampling, if we want to block sampleTi with t(i), we only need sample values for Ti andconsider this setting of t(i).The equation for sampling a single type-level as-signment Ti is given by,P (Ti, t(i)|T?i,W , t(?i),w, ?, ?)
=P (Ti|W ,T?i, ?
)P (t(i)|Ti, t(?i),w, ?
)where T?i denotes all type-level tag assignment ex-cept Ti and t(?i) denotes all token-level tags exceptt(i).
The terms on the right-hand-side denote thetype-level and token-level probability terms respec-tively.
The type-level posterior term can be com-puted according to,P (Ti|W ,T?i, ?)
?P (Ti|T?i, ?)?
(f,v)?WiP (v|Ti, f,W?i,T?i, ?
)All of the probabilities on the right-hand-side areDirichlet, distributions which can be computed an-alytically given counts.The token-level term is similar to the standardHMM sampling equations found in Johnson (2007).The relevant variables are the set of token-level tagsthat appear before and after each instance of the ithword type; we denote these context pairs with the set{(tb, ta)} and they are contained in t(?i).
We use w0 5 10 15 20 25 300.20.30.40.50.60.7Iteration1?1AccuracyEnglishDanishDutchGermanyPortugueseSpanishSwedishFigure 2: Graph of the one-to-one accuracy of our fullmodel (+FEATS) under the best hyperparameter settingby iteration (see Section 5).
Performance typically stabi-lizes across languages after only a few number of itera-tions.to represent the ith word type emitted by the HMM:P (t(i)|Ti, t(?i),w, ?)
??
(tb,ta)P (w|Ti, t(?i),w(?i), ?
)P (Ti|tb, t(?i), ?
)P (ta|Ti, t(?i), ?
)All terms are Dirichlet distributions and parameterscan be analytically computed from counts in t(?i)and w(?i) (Johnson, 2007).Note that each round of sampling Ti variablestakes time proportional to the size of the corpus, aswith the standard token-level HMM.
A crucial dif-ference is that the number of parameters is greatlyreduced as is the number of variables that are sam-pled during each iteration.
In contrast to results re-ported in Johnson (2007), we found that the per-formance of our Gibbs sampler on the basic 1TWmodel stabilized very quickly after about 10 full it-erations of sampling (see Figure 2 for a depiction).5 ExperimentsWe evaluate our approach on seven languages: En-glish, Danish, Dutch, German, Portuguese, Spanish,and Swedish.
On each language we investigate thecontribution of each component of our model.
Forall languages we do not make use of a tagging dic-tionary.857Model Hyper- English Danish Dutch German Portuguese Spanish Swedishparam.
1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-11TW best 45.2 62.6 37.2 56.2 47.4 53.7 44.2 62.2 49.0 68.4 34.3 54.4 36.0 55.3median 45.1 61.7 32.1 53.8 43.9 61.0 39.3 68.4 48.5 68.1 33.6 54.3 34.9 50.2+PRIOR best 47.9 65.5 42.3 58.3 51.4 65.9 50.7 62.2 56.2 70.7 42.8 54.8 38.9 58.0median 46.5 64.7 40.0 57.3 48.3 60.7 41.7 68.3 52.0 70.9 37.1 55.8 36.8 57.3+FEATS best 50.9 66.4 52.1 61.2 56.4 69.0 55.4 70.4 64.1 74.5 58.3 68.9 43.3 61.7median 47.8 66.4 43.2 60.7 51.5 67.3 46.2 61.7 56.5 70.1 50.0 57.2 38.5 60.6Table 3: Multi-lingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languagesunder several experimental settings (Section 5).
For each language and setting, we report one-to-one (1-1) and many-to-one (m-1) accuracies.
For each cell, the first row corresponds to the result using the best hyperparameter choice,where best is defined by the 1-1 metric.
The second row represents the performance of the median hyperparametersetting.
Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (seeSection 3).Language # Tokens # Word Types # TagsEnglish 1173766 49206 45Danish 94386 18356 25Dutch 203568 28393 12German 699605 72325 54Portuguese 206678 28931 22Spanish 89334 16458 47Swedish 191467 20057 41Table 2: Statistics for various corpora utilized in exper-iments.
See Section 5.
The English data comes fromthe WSJ portion of the Penn Treebank and the other lan-guages from the training set of the CoNLL-X multilin-gual dependency parsing shared task.5.1 Data SetsFollowing the set-up of Johnson (2007), we usethe whole of the Penn Treebank corpus for train-ing and evaluation on English.
For other languages,we use the CoNLL-X multilingual dependency pars-ing shared task corpora (Buchholz and Marsi, 2006)which include gold POS tags (used for evaluation).We train and test on the CoNLL-X training set.Statistics for all data sets are shown in Table 2.5.2 SetupModels To assess the marginal utility of each com-ponent of the model (see Section 3), we incremen-tally increase its sophistication.
Specifically, weevaluate three variants: The first model (1TW) onlyencodes the one tag per word constraint and is uni-form over type-level tag assignments.
The secondmodel (+PRIOR) utilizes the independent prior overtype-level tag assignments P (T |?).
The final model(+FEATS) utilizes the tag prior as well as features(e.g., suffixes and orthographic features), discussedin Section 3, for the P (W |T , ?)
component.Hyperparameters Our model has two Dirichletconcentration hyperparameters: ?
is the shared hy-perparameter for the token-level HMM emission andtransition distributions.
?
is the shared hyperparam-eter for the tag assignment prior and word featuremultinomials.
We experiment with four values foreach hyperparameter resulting in 16 (?, ?)
combi-nations:?
?0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10Iterations In each run, we performed 30 iterationsof Gibbs sampling for the type assignment variablesW .4 We use the final sample for evaluation.Evaluation Metrics We report three metrics toevaluate tagging performance.
As is standard, wereport the greedy one-to-one (Haghighi and Klein,2006) and the many-to-one token-level accuracy ob-tained from mapping model states to gold POS tags.We also report word type level accuracy, the fractionof word types assigned their majority tag (where themapping between model state and tag is determinedby greedy one-to-one mapping discussed above).5For each language, we aggregate results in the fol-lowing way: First, for each hyperparameter setting,4Typically, the performance stabilizes after only 10 itera-tions.5We choose these two metrics over the Variation Informa-tion measure due to the deficiencies discussed in Gao and John-son (2008).858we perform five runs with different random initial-ization of sampling state.
Hyperparameter settingsare sorted according to the median one-to-one met-ric over runs.
We report results for the best and me-dian hyperparameter settings obtained in this way.Specifically, for both settings we report results onthe median run for each setting.Tag set As is standard, for all experiments, we setthe number of latent model tag states to the size ofthe annotated tag set.
The original tag set for theCoNLL-X Dutch data set consists of compoundedtags that are used to tag multi-word units (MWUs)resulting in a tag set of over 300 tags.
We tokenizeMWUs and their POS tags; this reduces the tag setsize to 12.
See Table 2 for the tag set size of otherlanguages.
With the exception of the Dutch data set,no other processing is performed on the annotatedtags.6 Results and AnalysisWe report token- and type-level accuracy in Table 3and 6 for all languages and system settings.
Ouranalysis and comparison focuses primarily on theone-to-one accuracy since it is a stricter metric thanmany-to-one accuracy, but also report many-to-onefor completeness.Comparison with state-of-the-art taggers Forcomparison we consider two unsupervised tag-gers: the HMM with log-linear features of Berg-Kirkpatrick et al (2010) and the posterior regular-ization HMM of Grac?a et al (2009).
The systemof Berg-Kirkpatrick et al (2010) reports the bestunsupervised results for English.
We consider twovariants of Berg-Kirkpatrick et al (2010)?s richestmodel: optimized via either EM or LBFGS, as theirrelative performance depends on the language.
Ourmodel outperforms theirs on four out of five lan-guages on the best hyperparameter setting and threeout of five on the median setting, yielding an aver-age absolute difference across languages of 12.9%and 3.9% for best and median settings respectivelycompared to their best EM or LBFGS performance.While Berg-Kirkpatrick et al (2010) consistentlyoutperforms ours on English, we obtain substantialgains across other languages.
For instance, on Span-ish, the absolute gap on median performance is 10%.Top 5 Bottom 5Gold NNP NN JJ CD NNS RBS PDT # ?
,1TW CD WRB NNS VBN NN PRP$ WDT : MD .+PRIOR CD JJ NNS WP$ NN -RRB- , $ ?
.+FEATS JJ NNS CD NNP UH , PRP$ # .
?Table 5: Type-level English POS Tag Ranking: We listthe top 5 and bottom 5 POS tags in the lexicon and thepredictions of our models under the best hyperparametersetting.Our second point of comparison is with Grac?aet al (2009), who also incorporate a sparsity con-straint, but does via altering the model objective us-ing posterior regularization.
We can only comparewith Grac?a et al (2009) on Portuguese (Grac?a et al(2009) also report results on English, but on the re-duced 17 tag set, which is not comparable to ours).Their best model yields 44.5% one-to-one accuracy,compared to our best median 56.5% result.
How-ever, our full model takes advantage of word featuresnot present in Grac?a et al (2009).
Even without fea-tures, but still using the tag prior, our median resultis 52.0%, still significantly outperforming Grac?a etal.
(2009).Ablation Analysis We evaluate the impact ofincorporating various linguistic features into ourmodel in Table 3.
A novel element of our model isthe ability to capture type-level tag frequencies.
Forthis experiment, we compare our model with the uni-form tag assignment prior (1TW) with the learnedprior (+PRIOR).
Across all languages, +PRIORconsistently outperforms 1TW, reducing error on av-erage by 9.1% and 5.9% on best and median settingsrespectively.
Similar behavior is observed whenadding features.
The difference between the feature-less model (+PRIOR) and our full model (+FEATS)is 13.6% and 7.7% average error reduction on bestand median settings respectively.
Overall, the differ-ence between our most basic model (1TW) and ourfull model (+FEATS) is 21.2% and 13.1% for thebest and median settings respectively.
One strikingexample is the error reduction for Spanish, whichreduces error by 36.5% and 24.7% for the best andmedian settings respectively.
We observe similartrends when using another measure ?
type-level ac-curacy (defined as the fraction of words correctlyassigned their majority tag), according to which859Language Metric BK10 EM BK10 LBFGS G10 FEATS Best FEATS MedianEnglish1-1 48.3 56.0 ?
50.9 47.8m-1 68.1 75.5 ?
66.4 66.4Danish1-1 42.3 42.6 ?
52.1 43.2m-1 66.7 58.0 ?
61.2 60.7Dutch1-1 53.7 55.1 ?
56.4 51.5m-1 67.0 64.7 ?
69.0 67.3Portuguese1-1 50.8 43.2 44.5 64.1 56.5m-1 75.3 74.8 69.2 74.5 70.1Spanish1-1 ?
40.6 ?
58.3 50.0m-1 ?
73.2 ?
68.9 57.2Table 4: Comparison of our method (FEATS) to state-of-the-art methods.
Feature-based HMM Model (Berg-Kirkpatrick et al, 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGSoptimization algorithm; Posterior regulariation model (Grac?a et al, 2009): The G10 model uses the posterior regular-ization approach to ensure tag sparsity constraint.Language 1TW +PRIOR +FEATSEnglish 21.1 28.8 42.8Danish 10.1 20.7 45.9Dutch 23.8 32.3 44.3German 12.8 35.2 60.6Portuguese 18.4 29.6 61.5Spanish 7.3 27.6 49.9Swedish 8.9 14.2 33.9Table 6: Type-level Results: Each cell report the type-level accuracy computed against the most frequent tag ofeach word type.
The state-to-tag mapping is obtainedfrom the best hyperparameter setting for 1-1 mappingshown in Table 3.our full model yields 39.3% average error reductionacross languages when compared to the basic con-figuration (1TW).Table 5 provides insight into the behavior of dif-ferent models in terms of the tagging lexicon theygenerate.
The table shows that the lexicon tag fre-quency predicated by our full model are the closestto the gold standard.7 Conclusion and Future WorkWe have presented a method for unsupervised part-of-speech tagging that considers a word type and itsallowed POS tags as a primary element of the model.This departure from the traditional token-based tag-ging approach allows us to explicitly capture type-level distributional properties of valid POS tag as-signments as part of the model.
The resulting modelis compact, efficiently learnable and linguisticallyexpressive.
Our empirical results demonstrate thatthe type-based tagger rivals state-of-the-art tag-leveltaggers which employ more sophisticated learningmechanisms to exploit similar constraints.In this paper, we make a simplifying assump-tion of one-tag-per-word.
This assumption, how-ever, is not inherent to type-based tagging models.A promising direction for future work is to explicitlymodel a distribution over tags for each word type.We hypothesize that modeling morphological infor-mation will greatly constrain the set of possible tags,thereby further refining the representation of the taglexicon.AcknowledgmentsThe authors acknowledge the support of the NSF(CAREER grant IIS-0448168, and grant IIS-0904684).
We are especially grateful to Taylor Berg-Kirkpatrick for running additional experiments.
Wethank members of the MIT NLP group for their sug-gestions and comments.
Any opinions, findings,conclusions, or recommendations expressed in thispaper are those of the authors, and do not necessar-ily reflect the views of the funding organizations.ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless un-860supervised learning with features.
In Proceedings ofNAACL-HLT, pages 582?590.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In In Proc.of CoNLL, pages 149?164.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofbayesian estimators for unsupervised hidden markovmodel pos taggers.
In Proceedings of the EMNLP,pages 344?352.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages744?751.Joa?o Grac?a, Kuzman Ganchev, Ben Taskar, and FernandoPereira.
2009.
Posterior vs. parameter sparsity in la-tent variable models.
In Proceeding of NIPS, pages664?672.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings of theHLT-NAACL, pages 320?327.Kazi Saidul Hasan and Vincent Ng.
2009.
Weakly super-vised part-of-speech tagging for morphologically-rich,resource-scarce languages.
In Proceedings of EACL,pages 363?371.Mark Johnson.
2007.
Why doesn?t em find good hmmpos-taggers?
In Proceedings of EMNLP-CoNLL,pages 296?305.Michael Lamar, Yariv Maron, Marko Johnson, and ElieBienstock.
2010.
Svd Clustering for UnsupervisedPOS Tagging.
In Proceedings of ACL, pages 215?219.Bernard Me?rialdo.
1994.
Tagging english text witha probabilistic model.
Computational Linguistics,20(2):155?171.Sujith Ravi and Kevin Knight.
2009.
Minimized modelsfor unsupervised part-of-speech tagging.
In Proceed-ings of ACL-IJCNLP, pages 504?512.Hinrich Schutze.
1995.
Distributional part of speech tag-ging.
In Proceedings of the EACL, pages 141?148.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the ACL.861
