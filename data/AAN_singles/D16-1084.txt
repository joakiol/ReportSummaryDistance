Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 876?885,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsStance Detection with Bidirectional Conditional EncodingIsabelle Augenstein and Tim Rockta?schelDepartment of Computer ScienceUniversity College Londoni.augenstein@ucl.ac.uk, t.rocktaschel@cs.ucl.ac.ukAndreas Vlachos and Kalina BontchevaDepartment of Computer ScienceUniversity of Sheffield{a.vlachos, k.bontcheva}@sheffield.ac.ukAbstractStance detection is the task of classifying theattitude Previous work has assumed that ei-ther the target is mentioned in the text or thattraining data for every target is given.
This pa-per considers the more challenging version ofthis task, where targets are not always men-tioned and no training data is available forthe test targets.
We experiment with condi-tional LSTM encoding, which builds a rep-resentation of the tweet that is dependent onthe target, and demonstrate that it outperformsencoding the tweet and the target indepen-dently.
Performance is improved further whenthe conditional model is augmented with bidi-rectional encoding.
We evaluate our approachon the SemEval 2016 Task 6 Twitter StanceDetection corpus achieving performance sec-ond best only to a system trained on semi-automatically labelled tweets for the test tar-get.
When such weak supervision is added,our approach achieves state?of-the-art results.1 IntroductionThe goal of stance detection is to classify the attitudeexpressed in a text towards a given target, as ?pos-itive?, ?negative?, or ?neutral?.
Such informationcan be useful for a variety of tasks, e.g.
Mendozaet al (2010) showed that tweets stating actual factswere affirmed by 90% of the tweets related to them,while tweets conveying false information were pre-dominantly questioned or denied.
In this paper wefocus on a novel stance detection task, namely tweetstance detection towards previously unseen targets(mostly entities such as politicians or issues of pub-lic interest), as defined in the SemEval Stance De-tection for Twitter task (Mohammad et al, 2016).This task is rather difficult, firstly due to not havingtraining data for the targets in the test set, and sec-ondly, due to the targets not always being mentionedin the tweet.
For example, the tweet ?
@realDon-aldTrump is the only honest voice of the @GOP?expresses a positive stance towards the target Don-ald Trump.
However, when stance is annotated withrespect to Hillary Clinton as the implicit target, thistweet expresses a negative stance, since supportingcandidates from one party implies negative stancetowards candidates from other parties.Thus the challenge is twofold.
First, we need tolearn a model that interprets the tweet stance towardsa target that might not be mentioned in the tweet it-self.
Second, we need to learn such a model withoutlabelled training data for the target with respect towhich we are predicting the stance.
In the exampleabove, we need to learn a model for Hillary Clintonby only using training data for other targets.
Whilethis renders the task more challenging, it is a morerealistic scenario, as it is unlikely that labelled train-ing data for each target of interest will be available.To address these challenges we develop a neu-ral network architecture based on conditional encod-ing (Rockta?schel et al, 2016).
A long-short termmemory (LSTM) network (Hochreiter and Schmid-huber, 1997) is used to encode the target, followedby a second LSTM that encodes the tweet usingthe encoding of the target as its initial state.
Weshow that this approach achieves better F1 than anSVM baseline, or an independent LSTM encodingof the tweet and the target.
Results improve fur-876ther (0.4901 F1) with a bidirectional version of ourmodel, which takes into account the context on ei-ther side of the word being encoded.
In the contextof the shared task, this would have been the secondbest result, except for an approach which uses auto-matically labelled tweets for the test targets (F1 of0.5628).
Lastly, when our bidirectional conditionalencoding model is trained on such data, it achievesstate-of-the-art performance (0.5803 F1).2 Task SetupThe SemEval 2016 Stance Detection for Twittershared task (Mohammad et al, 2016) consists oftwo subtasks, Task A and Task B.
In Task A thegoal is to detect the stance of tweets towards tar-gets given labelled training data for all test targets(Climate Change is a Real Concern, Feminist Move-ment, Atheism, Legalization of Abortion and HillaryClinton).
In Task B, which is the focus of this paper,the goal is to detect stance with respect to an un-seen target, Donald Trump, for which labeled train-ing/development data is not provided.Systems need to classify the stance of each tweetas ?positive?
(FAVOR), ?negative?
(AGAINST) or?neutral?
(NONE) towards the target.
The officialmetric reported for the shared task is F1 macro-averaged over the classes FAVOR and AGAINST.Although the F1 of NONE is not considered, sys-tems still need to predict it to avoid precision errorsfor the other two classes.Even though participants were not allowed tomanually label data for the test target Donald Trump,they were allowed to label data automatically.
Thetwo best-performing systems submitted to Task B,pkudblab (Wei et al, 2016) and LitisMind (Zarrellaand Marsh, 2016) made use of this, thus changingthe task to weakly supervised seen target stance de-tection, instead of an unseen target task.
Althoughthe goal of this paper is to present stance detec-tion methods for targets for which no training datais available, we show that they can also be usedsuccessfully in a weakly supervised framework andoutperform the state-of-the-art on the SemEval 2016Stance Detection for Twitter dataset.3 MethodsA common stance detection approach is to treat itas a sentence-level classification task similar to sen-timent analysis (Pang and Lee, 2008; Socher et al,2013).
However, such an approach cannot capturethe stance of a tweet with respect to a particular tar-get, unless training data is available for each of thetest targets.
In such cases, we could learn that atweet mentioning Donald Trump in a positive man-ner expresses a negative stance towards Hillary Clin-ton.
Despite this limitation, we use two such base-lines, one implemented with a Support Vector Ma-chine (SVM) classifier and one with an LSTM net-work, in order to assess whether we are successfulin incorporating the target in stance prediction.A naive approach to incorporate the target instance prediction would be to generate features con-catenating the target with words from the tweet.
Ig-noring the issue that such features would be rathersparse, a classifier could learn that some words havetarget-dependent stance weights, but it still assumesthat training data is available for each target.In order to learn how to combine the stance targetwith the tweet in a way that generalises to unseentargets, we focus on learning distributed represen-tations and ways to combine them.
The followingsections develop progressively the proposed bidirec-tional conditional LSTM encoding model, startingfrom independently encoding the tweet and the tar-get using LSTMs.3.1 Independent EncodingOur initial attempt to learn distributed representa-tions for the tweets and the targets is to encodethe target and tweet independently as k-dimensionaldense vectors using two LSTMs (Hochreiter andSchmidhuber, 1997).H =[xtht?1]it = ?
(WiH+ bi)ft = ?
(WfH+ bf )ot = ?
(WoH+ bo)ct = ft  ct?1 + it  tanh(WcH+ bc)ht = ot  tanh(ct)877x1c?1c?1h?1h?1x2c?2c?2h?2h?2x3c?3c?3h?3h?3x4c?4c?4h?4h?4x5c?5c?5h?5h?5x6c?6c?6h?6h?6x7c?7c?7h?7h?7x8c?8c?8h?8h?8x9c?9c?9h?9h?9Legalization of Abortion A foetus has rights too !Target TweetFigure 1: Bidirectional encoding of tweet conditioned on bidirectional encoding of target ([c?3 c?1 ]).
The stance is predicted usingthe last forward and reversed output representations ([h?9 h?4 ]).Here, xt is an input vector at time step t, ct denotesthe LSTM memory, ht ?
Rk is an output vector andthe remaining weight matrices and biases are train-able parameters.
We concatenate the two output vec-tor representations and classify the stance using thesoftmax over a non-linear projectionsoftmax(tanh(Wtahtarget +Wtwhtweet + b))into the space of the three classes for stance detec-tion where Wta,Wtw ?
R3?k are trainable weightmatrices and b ?
R3 is a trainable class bias.
Thismodel learns target-independent distributed repre-sentations for the tweets and relies on the non-linear projection layer to incorporate the target in thestance prediction.3.2 Conditional EncodingIn order to learn target-dependent tweet representa-tions, we use conditional encoding as previously ap-plied to the task of recognising textual entailment(Rockta?schel et al, 2016).
We use one LSTM to en-code the target as a fixed-length vector.
Then, weencode the tweet with another LSTM, whose stateis initialised with the representation of the target.Finally, we use the last output vector of the tweetLSTM to predict the stance of the target-tweet pair.Formally, let (x1, .
.
.
,xT ) be a sequence of tar-get word vectors, (xT+1, .
.
.
,xN ) be a sequence oftweet word vectors and [h0 c0] be a start state ofzeros.
The two LSTMs map input vectors and a pre-vious state to a next state as follows:[h1 c1] = LSTMtarget(x1,h0, c0).
.
.
[hT cT ] = LSTMtarget(xT ,hT?1, cT?1)[hT+1 cT+1] = LSTMtweet(xT+1,h0, cT ).
.
.
[hN cN ] = LSTMtweet(xN ,hN?1, cN?1)Finally, the stance of the tweet w.r.t.
the target isclassified using a non-linear projectionc = tanh(WhN )where W ?
R3?k is a trainable weight matrix.This effectively allows the second LSTM to read thetweet in a target-specific manner, which is crucialsince the stance of the tweet depends on the target(recall the Donald Trump example above).3.3 Bidirectional Conditional EncodingBidirectional LSTMs (Graves and Schmidhuber,2005) have been shown to learn improved represen-tations of sequences by encoding a sequence fromleft to right and from right to left.
Therefore, weadapt the conditional encoding model from Sec-tion 3.2 to use bidirectional LSTMs, which repre-sent the target and the tweet using two vectors foreach of them, one obtained by reading the target878and then the tweet left-to-right (as in the conditionalLSTM encoding) and one obtained by reading themright-to-left.
To achieve this, we initialise the stateof the bidirectional LSTM that reads the tweet bythe last state of the forward and reversed encodingof the target (see Figure 1).
The bidirectional encod-ing allows the model to construct target-dependentrepresentations of the tweet such that when a wordis considered, both its left- and the right-hand sidecontext are taken into account.3.4 Unsupervised PretrainingIn order to counter-balance the relatively smallamount of training data available (5,628 instancesin total), we employ unsupervised pre-trainingby initialising the word embeddings used in theLSTMs with an appropriately trained word2vecmodel (Mikolov et al, 2013).
Note that these em-beddings are used only for initialisation, as we allowthem to be optimised further during training.In more detail, we train a word2vec model on acorpus of 395,212 unlabelled tweets, collected withthe Twitter Keyword Search API1 between Novem-ber 2015 and January 2016, plus all the tweets con-tained in the official SemEval 2016 Stance Detec-tion datasets (Mohammad et al, 2016).
The unla-belled tweets are collected so that they contain thetargets considered in the shared task, using up totwo keywords per target, namely ?hillary?, ?clin-ton?, ?trump?, ?climate?, ?femini?, ?aborti?.
Notethat Twitter does not allow for regular expressionsearch, so this is a free text search disregarding pos-sible word boundaries.
We combine this large unla-belled corpus with the official training data and traina skip-gram word2vec model (dimensionality 100, 5min words, context window of 5).Tweets and targets are tokenised with the Twitter-adapted tokeniser twokenize2.
Subsequently, all to-kens are lowercased, URLs are removed, and stop-word tokens are filtered (i.e.
punctuation characters,Twitter-specific stopwords (?rt?, ?#semst?, ?via?
).As it will be shown in our experiments, unsuper-vised pre-training is quite helpful, since it is difficultto learn representations for all the words using onlythe relatively small training datasets available.1https://dev.twitter.com/rest/public/search2https://github.com/leondz/twokenizeCorpus Favor Against None AllTaskA Tr+Dv 1462 2684 1482 5628TaskA Tr+Dv HC 224 722 332 1278TaskB Unlab - - - 278,013TaskB Auto-lab* 4681 5095 4026 13,802TaskB Test 148 299 260 707Crawled Unlab* - - - 395,212Table 1: Data sizes of available corpora.
TaskA Tr+Dv HCis the part of TaskA Tr+Dv with tweets for the target HillaryClinton only, which we use for development.
TaskB Auto-lab is an automatically labelled version of TaskB Unlab.Crawled Unlab is an unlabelled tweet corpus collected by us.Finally, to ensure that the proposed neural net-work architectures contribute to the performance,we also use the word vectors from word2vec to de-velop a Bag-of-Word-Vectors baseline (BOWV), inwhich the tweet and target representations are fedinto a logistic regression classifier with L2 regular-ization (Pedregosa et al, 2011).4 ExperimentsExperiments are performed on the SemEval 2016Task 6 corpus for Stance Detection on Twitter (Mo-hammad et al, 2016).
We report experiments fortwo different experimental setups: one is the unseentarget setup (Section 5), which is the main focus ofthis paper, i.e.
detecting the stance of tweets towardspreviously unseen targets.
We show that conditionalencoding, by reading the tweets in a target-specificway, generalises to unseen targets better than base-lines which ignore the target.
Next, we compareour approach to previous work in a weakly super-vised framework (Section 6) and show that our ap-proach outperforms the state-of-the-art on the Se-mEval 2016 Stance Detection Subtask B corpus.Table 1 lists the various corpora used in the ex-periments and their sizes.
TaskA Tr+Dv is theofficial SemEval 2016 Twitter Stance DetectionTaskA training and development corpus, whichcontain instances for the targets Legalization ofAbortion, Atheism, Feminist Movement, ClimateChange is a Real Concern and Hillary Clinton.TaskA Tr+Dv HC is the part of the corpus whichcontains only the Hillary Clinton tweets, whichwe use for development purposes.
TaskB Testis the TaskB test corpus on which we report re-sults containing Donald Trump testing instances.879TaskB Unlab is an unlabelled corpus containingDonald Trump tweets supplied by the task organ-isers, and TaskB Auto-lab* is an automatically la-belled version of a small portion of the corpus forthe weakly supervised stance detection experimentsreported in Section 6.
Finally, Crawled Unlab* isa corpus we collected for unsupervised pre-training(see Section 3.4).For all experiments, the official task evalua-tion script is used.
Predictions are post pro-cessed so that if the target is contained in atweet, the highest-scoring non-neutral stance ischosen.
This was motivated by the observationthat in the training data most target-containingtweets express a stance, with only 16% of thembeing neutral.
The code used in our experi-ments is available from https://github.com/sheffieldnlp/stance-conditional.4.1 MethodsWe compare the following baseline methods:?
SVM trained with word and character tweetn-grams features (SVM-ngrams-comb) Mo-hammad et al (2016)?
a majority class baseline (Majority baseline),reported in (Mohammad et al, 2016)?
bag of word vectors (BoWV) (see Section 3.4)?
independent encoding of tweet and the targetwith two LSTMs (Concat) (see Section 3.1)?
encoding of the tweet only with an LSTM(TweetOnly) (see Section 3.1)to three versions of conditional encoding:?
target conditioned on tweet (TarCondTweet)?
tweet conditioned on target (TweetCondTar)?
a bidirectional encoding model (BiCond)5 Unseen Target Stance DetectionAs explained earlier, the challenge is to learn amodel without any manually labelled training datafor the test target, but only using the data from theTask A targets.
In order to avoid using any la-belled data for Donald Trump, while still having a(labelled) development set to tune and evaluate ourmodels, we used the tweets labelled for Hillary Clin-ton as a development set and the tweets for the re-maining four targets as training.
We refer to this asMethod Stance P R F1BoWVFAVOR 0.2444 0.0940 0.1358AGAINST 0.5916 0.8626 0.7019Macro 0.4188TweetOnlyFAVOR 0.2127 0.5726 0.3102AGAINST 0.6529 0.4020 0.4976Macro 0.4039ConcatFAVOR 0.1811 0.6239 0.2808AGAINST 0.6299 0.4504 0.5252Macro 0.4030TarCondTweetFAVOR 0.3293 0.3649 0.3462AGAINST 0.4304 0.5686 0.4899Macro 0.4180TweetCondTarFAVOR 0.1985 0.2308 0.2134AGAINST 0.6332 0.7379 0.6816Macro 0.4475BiCondFAVOR 0.2588 0.3761 0.3066AGAINST 0.7081 0.5802 0.6378Macro 0.4722Table 2: Results for the unseen target stance detection devel-opment setup.the development setup, and all models are tuned us-ing this setup.
The labelled Donald Trump tweetswere only used in reporting our final results.For the final results we train on all the data fromthe development setup and evaluate on the officialTask B test set, i.e.
the Donald Trump tweets.
Werefer to this as our test setup.Based on a small grid search using the develop-ment setup, the following settings for LSTM-basedmodels were chosen: input layer size 100 (equal tothe word embedding dimension), hidden layer sizeof 60, training for max 50 epochs with initial learn-ing rate 1e-3 using ADAM (Kingma and Ba, 2014)for optimisation, dropout 0.1.
Models were trainedusing cross-entropy loss.
The use of one, relativelysmall hidden layer and dropout help to avoid over-fitting.5.1 Results and DiscussionResults for the unseen target setting show how wellconditional encoding is suited for learning target-dependent representations of tweets, and crucially,how well such representations generalise to unseentargets.
The best performing method on both de-velopment (Table 2) and test setups (Table 3) is Bi-Cond, which achieves an F1 of 0.4722 and 0.4901respectively.
Notably, Concat, which learns an in-880Method Stance P R F1BoWVFAVOR 0.3158 0.0405 0.0719AGAINST 0.4316 0.8963 0.5826Macro 0.3272TweetOnlyFAVOR 0.2767 0.3851 0.3220AGAINST 0.4225 0.5284 0.4695Macro 0.3958ConcatFAVOR 0.3145 0.5270 0.3939AGAINST 0.4452 0.4348 0.4399Macro 0.4169TarCondTweetFAVOR 0.2322 0.4188 0.2988AGAINST 0.6712 0.6234 0.6464Macro 0.4726TweetCondTarFAVOR 0.3710 0.5541 0.4444AGAINST 0.4633 0.5485 0.5023Macro 0.4734BiCondFAVOR 0.3033 0.5470 0.3902AGAINST 0.6788 0.5216 0.5899Macro 0.4901Table 3: Results for the unseen target stance detection testsetup.EmbIni NumMatr Stance P R F1RandomSingFAVOR 0.1982 0.3846 0.2616AGAINST 0.6263 0.5929 0.6092Macro 0.4354SepFAVOR 0.2278 0.5043 0.3138AGAINST 0.6706 0.4300 0.5240Macro 0.4189PreFixedSingFAVOR 0.6000 0.0513 0.0945AGAINST 0.5761 0.9440 0.7155Macro 0.4050SepFAVOR 0.1429 0.0342 0.0552AGAINST 0.5707 0.9033 0.6995Macro 0.3773PreContSingFAVOR 0.2588 0.3761 0.3066AGAINST 0.7081 0.5802 0.6378Macro 0.4722SepFAVOR 0.2243 0.4103 0.2900AGAINST 0.6185 0.5445 0.5792Macro 0.4346Table 4: Results for the unseen target stance detection develop-ment setup using BiCond, with single vs separate embeddingsmatrices for tweet and target and different initialisationsdependent encoding of the target and the tweets,does not achieve big F1 improvements over Twee-tOnly, which learns a representation of the tweetsonly.
This shows that it is not sufficient to just takethe target into account, but is is important to learntarget-dependent encodings for the tweets.
ModelsMethod inTwe Stance P R F1ConcatYesFAVOR 0.3153 0.6214 0.4183AGAINST 0.7438 0.4630 0.5707Macro 0.4945NoFAVOR 0.0450 0.6429 0.0841AGAINST 0.4793 0.4265 0.4514Macro 0.2677TweetCondTarYesFAVOR 0.3529 0.2330 0.2807AGAINST 0.7254 0.8327 0.7754Macro 0.5280NoFAVOR 0.0441 0.2143 0.0732AGAINST 0.4663 0.5588 0.5084Macro 0.2908BiCondYesFAVOR 0.3585 0.3689 0.3636AGAINST 0.7393 0.7393 0.7393Macro 0.5515NoFAVOR 0.0938 0.4286 0.1538AGAINST 0.5846 0.2794 0.3781Macro 0.2660Table 5: Results for the unseen target stance detection devel-opment setup for tweets containing the target vs tweets not con-taining the target.that learn to condition the encoding of tweets on tar-gets outperform all baselines on the test set.It is further worth noting that the Bag-of-Word-Vectors baseline achieves results comparable withTweetOnly, Concat and one of the conditional en-coding models, TarCondTweet, on the dev set, eventhough it achieves significantly lower performanceon the test set.
This indicates that the pre-trainedword embeddings on their own are already very use-ful for stance detection.
This is consistent with find-ings of other works showing the usefulness of sucha Bag-of-Word-Vectors baseline for the related tasksof recognising textual entailment Bowman et al(2015) and sentiment analysis Eisner et al (2016).Our best result in the test setup with BiCond is thesecond highest reported result on the Twitter StanceDetection corpus, however the first, third and fourthbest approaches achieved their results by automati-cally labelling Donald Trump training data.
BiCondfor the unseen target setting outperforms the thirdand fourth best approaches by a large margin (5 and7 points in Macro F1, respectively), as can be seenin Table 7.
Results for weakly supervised stance de-tection are discussed in Section 6.881Pre-Training Table 4 shows the effect of unsu-pervised pre-training of word embeddings with aword2vec skip-gram model, and furthermore, the re-sults of sharing of these representations between thetweets and targets, on the development set.
The firstset of results is with a uniformly Random embed-ding initialisation in [?0.1, 0.1].
PreFixed uses thepre-trained skip-gram word embeddings, whereasPreCont initialises the word embeddings with onesfrom SkipGram and continues training them dur-ing LSTM training.
Our results show that, in theabsence of a large labelled training dataset, pre-training of word embeddings is more helpful thanrandom initialisation of embeddings.
Sing vs Sepshows the difference between using shared vs twoseparate embeddings matrices for looking up theword embeddings.
Sing means the word represen-tations for tweet and target vocabularies are shared,whereas Sep means they are different.
Using sharedembeddings performs better, which we hypothesiseis because the tweets contain some mentions of tar-gets that are tested.Target in Tweet vs Not in Tweet Table 5 showsresults on the development set for BiCond, com-pared to the best unidirectional encoding model,TweetCondTar and the baseline model Concat,split by tweets that contain the target and those thatdo not.
All three models perform well when thetarget is mentioned in the tweet, but less so whenthe targets are not mentioned explicitly.
In the casewhere the target is mentioned in the tweet, bicon-ditional encoding outperforms unidirectional encod-ing and unidirectional encoding outperforms Con-cat.
This shows that conditional encoding is ableto learn useful dependencies between the tweets andthe targets.6 Weakly Supervised Stance DetectionThe previous section showed the usefulness of con-ditional encoding for unseen target stance detec-tion and compared results against internal baselines.The goal of experiments reported in this sectionis to compare against participants in the SemEval2016 Stance Detection Task B.
While we consideran unseen target setup, most submissions, includ-ing the three highest ranking ones for Task B, pkud-blab (Wei et al, 2016), LitisMind (Zarrella andMethod Stance P R F1BoWVFAVOR 0.5156 0.6689 0.5824AGAINST 0.6266 0.3311 0.4333Macro 0.5078TweetOnlyFAVOR 0.5284 0.6284 0.5741AGAINST 0.5774 0.4615 0.5130Macro 0.5435ConcatFAVOR 0.5506 0.5878 0.5686AGAINST 0.5794 0.4883 0.5299Macro 0.5493TarCondTweetFAVOR 0.5636 0.6284 0.5942AGAINST 0.5947 0.4515 0.5133Macro 0.5538TweetCondTarFAVOR 0.5868 0.6622 0.6222AGAINST 0.5915 0.4649 0.5206Macro 0.5714BiCondFAVOR 0.6268 0.6014 0.6138AGAINST 0.6057 0.4983 0.5468Macro 0.5803Table 6: Stance Detection test results for weakly super-vised setup, trained on automatically labelled pos+neg+neutralTrump data, and reported on the official test set.Marsh, 2016) and INF-UFRGS (Dias and Becker,2016) considered a different experimental setup.They automatically annotated training data for thetest target Donald Trump, thus converting the taskinto weakly supervised seen target stance detection.The pkudblab system uses a deep convolutional neu-ral network that learns to make 2-way predictions onautomatically labelled positive and negative trainingdata for Donald Trump.
The neutral class is pre-dicted according to rules which are applied at testtime.Since the best performing systems which partic-ipated in the shared task consider a weakly super-vised setup, we further compare our proposed ap-proach to the state-of-the-art using such a weaklysupervised setup.
Note that, even though pkudblab,LitisMind and INF-UFRGS also use regular expres-sions to label training data automatically, the result-ing datasets were not available to us.
Therefore, wehad to develop our own automatic labelling methodand dataset, which are publicly available from ourcode repository.Weakly Supervised Test Setup For this setup, theunlabelled Donald Trump corpus TaskB Unlab isannotated automatically.
For this purpose we cre-882ated a small set of regular expressions3, based oninspection of the TaskB Unlab corpus, expressingpositive and negative stance towards the target.
Theregular expressions for the positive stance were:?
make( ?
)america( ?
)great( ?)again?
trump( ?
)(for|4)( ?)president?
votetrump?
trumpisright?
the truth?
#trumprulesThe keyphrases for negative stance were:#dumptrump, #notrump, #trumpwatch, racist,idiot, firedA tweet is labelled as positive if one of the posi-tive expressions is detected, else negative if a nega-tive expressions is detected.
If neither are detected,the tweet is annotated as neutral randomly with 2%chance.
The resulting corpus size per stance isshown in Table 1.
The same hyperparameters forthe LSTM-based models are used as for the unseentarget setup described in the previous section.6.1 Results and DiscussionTable 6 lists our results in the weakly supervised set-ting.
Table 7 shows all our results, including thoseusing the unseen target setup, compared against thestate-of-the-art on the stance detection corpus.
Ta-ble 7 further lists baselines reported by Moham-mad et al (2016), namely a majority class base-line (Majority baseline), and a method using 1 to3-gram bag-of-word and character n-gram features(SVM-ngrams-comb), which are extracted fromthe tweets and used to train a 3-way SVM classifier.Bag-of-word baselines (BoWV, SVM-ngrams-comb) achieve results comparable to the majoritybaseline (F1 of 0.2972), which shows how diffi-cult the task is.
The baselines which only extractfeatures from the tweets, SVM-ngrams-comb andTweetOnly perform worse than the baselines whichalso learn representations for the targets (BoWV,Concat).
By training conditional encoding modelson automatically labelled stance detection data weachieve state-of-the-art results.
The best result (F1of 0.5803) is achieved with the bi-directional condi-tional encoding model (BiCond).
This shows that3Note that ?|?
indiates ?or?, ( ?)
indicates optional spaceMethod Stance F1SVM-ngrams-comb (Unseen Target)FAVOR 0.1842AGAINST 0.3845Macro 0.2843Majority baseline (Unseen Target)FAVOR 0.0AGAINST 0.5944Macro 0.2972BiCond (Unseen Target)FAVOR 0.3902AGAINST 0.5899Macro 0.4901INF-UFRGS (Weakly Supervised*)FAVOR 0.3256AGAINST 0.5209Macro 0.4232LitisMind (Weakly Supervised*)FAVOR 0.3004AGAINST 0.5928Macro 0.4466pkudblab (Weakly Supervised*)FAVOR 0.5739AGAINST 0.5517Macro 0.5628BiCond (Weakly Supervised)FAVOR 0.6138AGAINST 0.5468Macro 0.5803Table 7: Stance Detection test results, compared against thestate of the art.
SVM-ngrams-comb and Majority baselineare reported in Mohammad et al (2016), pkudblab in Wei et al(2016), LitisMind in Zarrella and Marsh (2016), INF-UFRGSin Dias and Becker (2016)such models are suitable for unseen, as well as seentarget stance detection.7 Related WorkStance Detection: Previous work mostly consideredtarget-specific stance prediction in debates (Hasanand Ng, 2013; Walker et al, 2012) or studentessays (Faulkner, 2014).
The task considered inthis paper is more challenging than stance detec-tion in debates because, in addition to irregular lan-guage, the Mohammad et al (2016) dataset is of-fered without any context, e.g., conversational struc-ture or tweet metadata.
The targets are also notalways mentioned in the tweets, which is an addi-tional challenge (Augenstein et al, 2016) and dis-tinguishes this task from target-dependent (Vo andZhang, 2015; Zhang et al, 2016; Alghunaim etal., 2015) and open-domain target-dependent sen-timent analysis (Mitchell et al, 2013; Zhang etal., 2015).
Related work on rumour stance detec-tion either requires training data from the same ru-883mour (Qazvinian et al, 2011), i.e., target, or is rule-based (Liu et al, 2015) and thus potentially hard togeneralise.
Finally, the target-dependent stance de-tection task tackled in this paper is different fromthat of Ferreira and Vlachos (2016), which while re-lated concerned with the stance of a statement in nat-ural language towards another statement.Conditional Encoding: Conditional encodinghas been applied to the related task of recognisingtextual entailment (Rockta?schel et al, 2016), using adataset of half a million training examples (Bowmanet al, 2015) and numerous different hypotheses.
Ourexperiments here show that conditional encoding isalso successful on a relatively small training set andwhen applied to an unseen testing target.
Moreover,we augment conditional encoding with bidirectionalencoding and demonstrate the added benefit of un-supervised pre-training of word embeddings on un-labelled domain data.8 Conclusions and Future WorkThis paper showed that conditional LSTM encod-ing is a successful approach to stance detection forunseen targets.
Our unseen target bidirectional con-ditional encoding approach achieves the second bestresults reported to date on the SemEval 2016 TwitterStance Detection corpus.
In the weakly supervisedseen target scenario, as considered by prior work,our approach achieves the best results to date on theSemEval Task B dataset.
We further show that in theabsence of large labelled corpora, unsupervised pre-training can be used to learn target representationsfor stance detection and improves results on the Se-mEval corpus.
Future work will investigate furtherthe challenge of stance detection for tweets whichdo not contain explicit mentions of the target.AcknowledgmentsThis work was partially supported by the EuropeanUnion under grant agreement No.
611233 PHEME4and by Microsoft Research through its PhD Scholar-ship Programme.4http://www.pheme.euReferencesAbdulaziz Alghunaim, Mitra Mohtarami, Scott Cyphers,and Jim Glass.
2015.
A Vector Space Approach forAspect Based Sentiment Analysis.
In Proceedings ofthe 1st Workshop on Vector Space Modeling for Nat-ural Language Processing, pages 116?122, Denver,Colorado, June.
Association for Computational Lin-guistics.Isabelle Augenstein, Andreas Vlachos, and KalinaBontcheva.
2016.
USFD: Any-Target Stance Detec-tion on Twitter with Autoencoders.
In Proceedings ofthe International Workshop on Semantic Evaluation,SemEval ?16, San Diego, California.Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning.
2015.
A large annotatedcorpus for learning natural language inference.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 632?642,Lisbon, Portugal, September.
Association for Compu-tational Linguistics.Marcelo Dias and Karin Becker.
2016.
INF-UFRGS-OPINION-MINING at SemEval-2016 Task 6: Auto-matic Generation of a Training Corpus for Unsuper-vised Identification of Stance in Tweets.
In Proceed-ings of the International Workshop on Semantic Eval-uation, SemEval ?16, San Diego, California, June.Ben Eisner, Tim Rockta?schel, Isabelle Augenstein,Matko Bos?njak, and Sebastian Riedel.
2016.emoji2vec: Learning Emoji Representations fromtheir Description.
In Proceedings of the InternationalWorkshop on Natural Language Processing for SocialMedia, SocialNLP ?16, Austin, Texas.Adam Faulkner.
2014.
Automated Classification ofStance in Student Essays: An Approach Using StanceTarget Information and the Wikipedia Link-BasedMeasure.
In William Eberle and Chutima Boonthum-Denecke, editors, FLAIRS Conference.
AAAI Press.William Ferreira and Andreas Vlachos.
2016.
Emer-gent: a novel data-set for stance classification.
In Pro-ceedings of the 2016 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 1163?1168, San Diego, California, June.
Association forComputational Linguistics.Alex Graves and Ju?rgen Schmidhuber.
2005.
Framewisephoneme classification with bidirectional LSTM andother neural network architectures.
Neural Networks,18(5):602?610.Kazi Saidul Hasan and Vincent Ng.
2013.
Stance Clas-sification of Ideological Debates: Data, Models, Fea-tures, and Constraints.
In IJCNLP, pages 1348?1356.Asian Federation of Natural Language Processing /ACL.884Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Diederik P. Kingma and Jimmy Ba.
2014.
Adam:A Method for Stochastic Optimization.
CoRR,abs/1412.6980.Xiaomo Liu, Armineh Nourbakhsh, Quanzhi Li, RuiFang, and Sameena Shah.
2015.
Real-time Ru-mor Debunking on Twitter.
In Proceedings of the24th ACM International on Conference on Informa-tion and Knowledge Management, CIKM ?15, pages1867?1870, New York, NY, USA.
ACM.Marcelo Mendoza, Barbara Poblete, and Carlos Castillo.2010.
Twitter Under Crisis: Can We Trust WhatWe RT?
In Proceedings of the First Workshop onSocial Media Analytics (SOMA?2010), pages 71?79,New York, NY, USA.
ACM.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Margaret Mitchell, Jacqui Aguilar, Theresa Wilson, andBenjamin Van Durme.
2013.
Open Domain TargetedSentiment.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,pages 1643?1654, Seattle, Washington, USA, October.Association for Computational Linguistics.Saif M. Mohammad, Svetlana Kiritchenko, ParinazSobhani, Xiaodan Zhu, and Colin Cherry.
2016.SemEval-2016 Task 6: Detecting stance in tweets.
InProceedings of the International Workshop on Seman-tic Evaluation, SemEval ?16, San Diego, California,June.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine Learning in Python.Journal of Machine Learning Research, 12:2825?2830.Vahed Qazvinian, Emily Rosengren, Dragomir R. Radev,and Qiaozhu Mei.
2011.
Rumor Has It: Identify-ing Misinformation in Microblogs.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?11, pages 1589?1599.Tim Rockta?schel, Edward Grefenstette, Karl Moritz Her-mann, Toma?s?
Koc?isky`, and Phil Blunsom.
2016.
Rea-soning about Entailment with Neural Attention.
In In-ternational Conference on Learning Representations(ICLR).Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and Christo-pher Potts.
2013.
Recursive Deep Models for Se-mantic Compositionality Over a Sentiment Treebank.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages1631?1642, Seattle, Washington, USA, October.
As-sociation for Computational Linguistics.Duy-Tin Vo and Yue Zhang.
2015.
Target-DependentTwitter Sentiment Classification with Rich AutomaticFeatures.
In Qiang Yang and Michael Wooldridge, ed-itors, IJCAI, pages 1347?1353.
AAAI Press.Marilyn Walker, Pranav Anand, Rob Abbott, and RickyGrant.
2012.
Stance Classification using DialogicProperties of Persuasion.
In Proceedings of the 2012Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 592?596.Wan Wei, Xiao Zhang, Xuqin Liu, Wei Chen, andTengjiao Wang.
2016. pkudblab at SemEval-2016Task 6: A Specific Convolutional Neural Network Sys-tem for Effective Stance Detection.
In Proceedings ofthe International Workshop on Semantic Evaluation,SemEval ?16, San Diego, California, June.Guido Zarrella and Amy Marsh.
2016.
MITRE atSemEval-2016 Task 6: Transfer Learning for StanceDetection.
In Proceedings of the International Work-shop on Semantic Evaluation, SemEval ?16, SanDiego, California, June.Meishan Zhang, Yue Zhang, and Duy Tin Vo.
2015.Neural Networks for Open Domain Targeted Senti-ment.
In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing,pages 612?621, Lisbon, Portugal, September.
Associ-ation for Computational Linguistics.Meishan Zhang, Yue Zhang, and Duy-Tin Vo.
2016.Gated Neural Networks for Targeted Sentiment Anal-ysis.
In Proceedings of the Thirtieth AAAI Con-ference on Artificial Intelligence, Phoenix, Arizona,USA, February.
Association for the Advancement ofArtificial Intelligence.885
