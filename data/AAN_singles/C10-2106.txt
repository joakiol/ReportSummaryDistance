Coling 2010: Poster Volume, pages 919?927,Beijing, August 2010A Study on Position Information in Document SummarizationYou Ouyang       Wenjie Li       Qin Lu       Renxian ZhangDepartment of Computing, the Hong Kong Polytechnic University{csyouyang,cswjli,csluqin,csrzhang}@comp.polyu.edu.hkAbstractPosition information has been proved tobe very effective in documentsummarization, especially in genericsummarization.
Existing approachesmostly consider the information ofsentence positions in a document, basedon a sentence position hypothesis thatthe importance of a sentence decreaseswith its distance from the beginning ofthe document.
In this paper, we consideranother kind of position information, i.e.,the word position information, which isbased on the ordinal positions of wordappearances instead of sentencepositions.
An extractive summarizationmodel is proposed to provide anevaluation framework for the positioninformation.
The resulting systems areevaluated on various data sets todemonstrate the effectiveness of theposition information in differentsummarization tasks.
Experimentalresults show that word positioninformation is more effective andadaptive than sentence positioninformation.1 IntroductionPosition information has been frequently used indocument summarization.
It springs fromhuman?s tendency of writing sentences ofgreater topic centrality at particular positions ina document.
For example, in newswiredocuments, topic sentences are usually writtenearlier.
A sentence position hypothesis is thengiven as: the first sentence in a document is themost important and the importance decreases asthe sentence gets further away from thebeginning.
Based on this sentence positionhypothesis, sentence position features aredefined by the ordinal position of sentences.These position features have been proved to bevery effective in generic documentsummarization.
In more recent summarizationtasks, such as query-focused and updatesummarization tasks, position features are alsowidely used.Although in these tasks position features maybe used in different ways, they are all based onthe sentence position hypothesis.
So we regardthem as providing the sentence positioninformation.
In this paper, we study a new kindof position information, i.e., the word positioninformation.
The motivation of word positioninformation comes from the idea of assigningdifferent importance to multiple appearances ofone word in a document.As to many language models such as the bag-of-words model, it is well acknowledged that aword which appears more frequently is usuallymore important.
If we take a closer look at allthe appearances of one word, we can view thisas a process that the different appearances of thesame word raise the importance of each other.Now let?s also take the order of the appearancesinto account.
When reading a document, we canview it as a word token stream from the firsttoken to the last.
When a new token is read, weattach more importance to previous tokens thathave the same lemma because they are justrepeated by the new token.
Inspired by this, wepostulate a word position hypothesis here: forall the appearances of a fixed word, theimportance of each appearance depends on allits following appearances.
Therefore, the firstappearance of a word is the most important andthe importance decreases with the ordinal919positions of the appearances.
Then, a novel kindof position features can be defined for the wordappearances based on their ordinal positions.We believe that these word position featureshave some advantages when compared totraditional sentence position features.
Accordingto the sentence position hypothesis, sentenceposition features generally prefer earliersentences in a document.
As to the wordposition features that attempt to differentiateword appearances instead of sentences, asentence which is not the first one in thedocument may still not be penalized as long asits words do not appear in previous sentences.Therefore, word position features are able todiscover topic sentences in deep positions of thedocument.
On the other hand, the assertion thatthe first sentence is always the most important isnot true in actual data.
It depends on the writingstyle indeed.
For example, some authors maylike to write some background sentences beforetopic sentences.
In conclusion, we can expectword position features  to be more adaptive todocuments with different structures.In the study of this paper, we define severalword position features based on the ordinalpositions of word appearances.
We also developa word-based summarization system to evaluatethe effectiveness of the proposed word positionfeatures on a series of summarization data sets.The main contributions of our work are:(1) representation of word position information,which is a new kind of position information indocument summarization area.
(2) empirical results on various data sets thatdemonstrate the impact of position informationin different summarization tasks.2 Related WorkThe use of position information in documentsummarization has a long history.
In the seminalwork by (Luhn, 1958), position information wasalready considered as a good indicator ofsignificant sentences.
In (Edmundson, 1969), alocation method was proposed that assignspositive weights to the sentences to their ordinalpositions in the document.
Position informationhas since been adopted by many successfulsummarization systems, usually in the form ofsentence position features.
For example, Radevet al (2004) developed a feature-based systemMEAD based on word frequencies and sentencepositions.
The position feature was defined as adescending function of the sentence position.The MEAD system performed very well in thegeneric multi-document summarization task ofthe DUC 2004 competition.
Later, positioninformation is also applied to moresummarization tasks.
For example, in query-focused task, sentence position features arewidely used in learning-based summarizationsystems as a component feature for calculatingthe composite sentence score (Ouyang et al2007; Toutanova et al 2007).
However, theeffect of position features alone was not studiedin these works.There were also studies aimed at analyzingand explaining the effectiveness of positioninformation.
Lin and Hovy (1997) provided anempirical validation on the sentence positionhypothesis.
For each position, the sentenceposition yield was defined as the average valueof the significance of the sentences with thefixed position.
It was observed that the averagesignificance at earlier positions was indeedlarger.
Nenkova (2005) did a conclusiveoverview on the DUC 2001-2004 evaluationresults.
It was reported that position informationis very effective in generic summarization.
Ingeneric single-document summarization, a lead-based baseline that simply takes the leadingsentences as the summary can outperform mostsubmitted summarization system in DUC 2001and 2002.
As in multi-document summarization,the position-based baseline system iscompetitive in generating short summaries butnot in longer summaries.
Schilder andKondadadi (2008) analyzed the effectiveness ofthe features that are used in their learning-basedsentence scoring model for query-focusedsummarization.
By comparing the ROUGE-2results of each individual feature, it wasreported that position-based features are lesseffective than frequency-based features.
In(Gillick et al, 2009), the effect of positioninformation in the update summarization taskwas studied.
By using ROUGE to measure thedensity of valuable words at each sentenceposition, it was observed that the first sentenceof newswire document was especially importantfor composing update summaries.
They defineda binary sentence position feature based on the920observation and the feature did improve theperformance on the update summarization data.3 MethodologyIn the section, we first describe the word-basedsummarization model.
The word positionfeatures are then defined and incorporated intothe summarization model.3.1 Basic Summarization ModelTo test the effectiveness of position informationin document summarization, we first propose aword-based summarization model for applyingthe position information.
The system follows atypical extractive style that constructs the targetsummary by selecting the most salient sentences.Under the bag-of-words model, theprobability of a word w in a document set D canbe scaled by its frequency, i.e., p(w)=freq(w)/|D|,where freq(w) indicates the frequency of w in Dand |D| indicates the total number of words in D.The probability of a sentence s={w1, ?, wN} isthen calculated as the product of the wordprobabilities, i.e., p(s)=?i p(wi).
Moreover, theprobability of a summary consisting a set ofsentences, denoted as S={s1, ?, sM}, can becalculated by the product of the sentenceprobabilities, i.e., p(S)=?j p(sj).
To obtain theoptimum summary, an intuitive idea is to selectthe sentences to maximize the overall summaryprobability p(S), equivalent to maximizinglog(p(S)) = ?j?i log(p(wji)) = ?j?i (logfreq(wji)-log|D|) = ?j?i log freq(wji) - |S|?log |D|,where wji indicates the ith word in sj and |S|indicates the total number of words in S. As topractical summarization tasks, a maximumsummary length is usually postulated.
So herewe just assume that the length of the summaryis fixed.
Then, the above optimization target isequivalent to maximizing ?j?i logfreq(wji).From the view of information theory, the sumcan also be interpreted as a simple measure onthe total information amount of the summary.
Inthis interpretation, the information of a singleword wji is measured by log freq(wji) and thesummary information is the sum of the wordinformation.
So the optimization target can alsobe interpreted as including the most informativewords to form the most informative summarygiven the length limit.In extractive summarization, summaries arecomposed by sentence selection.
As to theabove optimization target, the sentence scoringfunction for ranking the sentences should becalculated as the average word information, i.e.,score(s) = ?i log freq(wi) / |s|.After ranking the sentences by their rankingscores, we can select the sentences into thesummary by the descending order of their scoreuntil the length limit is reached.
By this process,the summary with the largest  p(S) can becomposed.3.2 Word Position FeaturesWith the above model, word position featuresare defined to represent the word positioninformation and are then incorporated into themodel.
According to the motivation, the featuresare defined by the ordinal positions of wordappearances, based on the position hypothesisthat earlier appearances of a word are moreinformative.
Formally, for the ith appearanceamong the total n appearances of a word w, fourposition features are defined based on i and nusing different formulas as described below.
(1) Direct proportion (DP) With the wordposition hypothesis, an intuitive idea is to regardthe information degree of the first appearance as1 and the last one as 1/n, and then let the degreedecrease linearly to the position i.
So we canobtain the first position feature defined by thedirect proportion function, i.e., f(i)=(n-i+1)/n.
(2) Inverse proportion (IP).
Besides the linearfunction, other functions can also be used tocharacterize the relationship between theposition and the importance.
The secondposition feature adopts another widely-usedfunction, the inversed proportion function, i.e.,f(i)=1/i.
This measure is similar to the aboveone, but the information degree decreases by theinverse proportional function.
Therefore, thedegree decreases more quickly at smallerpositions, which implies a stronger preferencefor leading sentences.
(3) Geometric sequence (GS).
For the thirdfeature, we make an assumption that the degreeof every appearance is the sum of the degree ofall the following appearances, i.e., f(i) = f(i+1)+f(i+2)+?+ f(n).
It can be easily derived that thesequence also satisfies f(i) = 2?f(i-1).
That is, theinformation degree of each new appearance is921halved.
Then the feature value of the ithappearance can be calculated as f(i) = (1/2)i-1.
(4) Binary function (BF).
The final feature is abinary position feature that regards the firstappearance as much more informative than theall the other appearances, i.e., f(i)=1, if i=1; ?else, where ?
is a small positive real number.3.3 Incorporating the Position FeaturesTo incorporate the position features into theword-based summarization model, we use themto adjust the importance of the word appearance.For the ith appearance of a word w, its originalimportance is multiplied by the position featurevalue, i.e., log freq(w)?pos(w, i), where pos(w, i)is calculated by one of the four position featuresintroduced above.
By this, the position feature isalso incorporated into the sentence scores, i.e.,score?
(s) = ?i [log freq(wi) ?
pos(wi)] / |s|3.4 Sentence Position FeaturesIn our study, another type of position features,which model sentence position information, isdefined for comparison with the word positionfeatures.
The sentence position features are alsodefined by the above four formulas.
However,for each appearance, the definition of i and n inthe formulas are changed to the ordinal positionof the sentence that contains this appearanceand the total number of sentences in thedocument respectively.
In fact, the effects of thefeatures defined in this way are equivalent totraditional sentence position features.
Since iand n are now defined by sentence positions, thefeature values of the word tokens in the samesentence s are all equal.
Denote it by pos(s), andthe sentence score with the position feature canbe written asscore?
(s) = ( ?w in slogfreq(w) ?
pos(s))/|s|= pos(s)?(?
logw in s freq(w)/|s|),which can just be viewed as the product of theoriginal score and a sentence position feature.3.5 DiscussionBy using the four functions to measure word orsentence position information, we can generatea total of eight position features.
Among thefour functions, the importance drops fastestunder the binary function and the order is BF >GS > IP > DP.
Therefore, the features based onthe binary function are the most biased to theleading sentences in the document and thefeatures based on the direct proportion functionare the least.
On the other hand, as mentioned inthe introduction, sentence-based features havelarger preferences for leading sentences thanword-based position features.An example is given below to illustrate thedifference between word and sentence positionfeatures.
This is a document from DUC 2001.1.
GENERAL ACCIDENT, the leading Britishinsurer, said yesterday that insurance claimsarising from Hurricane Andrew could 'cost it asmuch as Dollars 40m.'2.
Lord Airlie, the chairman who wasaddressing an extraordinary shareholders'meeting, said: 'On the basis of emerginginformation, General Accident advise that thelosses to their US operations arising fromHurricane Andrew, which struck Florida andLouisiana, might in total reach the level atwhich external catastrophe reinsurance coverswould become exposed'.3.
What this means is that GA is able to pass onits losses to external reinsurers once a certainclaims threshold has been breached.4.
It believes this threshold may be breached inrespect of Hurricane Andrew claims.5.
However, if this happens, it would suffer apost-tax loss of Dollars 40m (Pounds 20m).6.
Mr Nelson Robertson, GA's chief generalmanager, explained later that the company has a1/2 per cent share of the Florida market.7.
It has a branch in Orlando.8.
The company's loss adjusters are in the areatrying to estimate the losses.9.
Their guess is that losses to be faced by allinsurers may total more than Dollars 8bn.10.
Not all damaged property in the area isinsured and there have been estimates that thestorm caused more than Dollars 20bn ofdamage.11.
However, other insurers have estimated thatlosses could be as low as Dollars 1bn in total.12 Mr Robertson said: 'No one knows at thistime what the exact loss is'.For the word ?threshold?
which appearstwice in the document, its original importance islog(2), for the appearance of ?threshold?
in the4th sentence, the modified score based on wordposition feature with the direct proportionfunction is 1/2?log(2).
In contrast, the scorebased on sentence position feature with the922same function is 9/12?log(2), which is larger.For the appearance of the word ?estimate?
in the8th sentence, its original importance is log(3)(the three boldfaced tokens are regarded as oneword with stemming).
The word-based andsentence-based scores are log(3) and 5/12?log(3)respectively.
So its importance is larger underword position feature.
Therefore, the systemwith word position features may prefer the 8thsentence that is in deeper positions but thesystem with sentence position feature mayprefer the 4th sentence.
As for this document, thetop 5 sentences selected by sentence positionfeature are {1, 4, 3, 5, 2} and the those selectedby the word position features are {1, 8, 3, 6, 9}.This clearly demonstrates the differencebetween the position features.4 Experimental Results4.1 Experiment SettingsWe conduct the experiments on the data setsfrom the Document Understanding Conference(DUC) run by NIST.
The DUC competitionstarted at year 2001 and has successfullyevaluated various summarization tasks up tonow.
In the experiments, we evaluate theeffectiveness of position information on severalDUC data sets that involve varioussummarization tasks.
One of the evaluationcriteria used in DUC, the automaticsummarization evaluation package ROUGE, isused to evaluate the effectiveness of theproposed word position features in the contextof document summarization1.
The recall scoresof ROUGE-1 and ROUGE-2, which are basedon unigram and bigram matching betweensystem summaries and reference summaries, areadopted as the evaluation criteria.In the data sets used in the experiments, theoriginal documents are all pre-processed bysentence segmentation, stop-word removal andword stemming.
Based on the word-basedsummarization model, a total of nine systemsare evaluated in the experiments, including thesystem with the original ranking model (denotedas None), four systems with each word positionfeature (denoted as WP) and four systems witheach sentence position feature (denoted as SP).1 We run ROUGE-1.5.5 with the parameters ?-x -m -n 2 -2 4 -u -c 95 -p 0.5 -t 0?For reference, the average ROUGE scores of allthe human summarizers and all the submittedsystems from the official results of NIST arealso given (denoted as Hum and NISTrespectively).4.2 Redundancy RemovalTo reduce the redundancy in the generatedsummaries, we use an approach similar to themaximum marginal relevance (MMR) approachin the sentence selection process (Carbonell andGoldstein, 1998).
In each round of the sentenceselection, the candidate sentence is comparedagainst the already-selected sentences.
Thesentence is added to the summary only if it isnot significantly similar to any already-selectedsentence, which is judged by the condition thatthe cosine similarity between the two sentencesis less than 0.7.4.3 Generic SummarizationIn the first experiment, we use the DUC 2001data set for generic single-documentsummarization and the DUC 2004 data set forgeneric multi-document summarization.
TheDUC 2001 data set contains 303 document-summary pairs; the DUC 2004 data set contains45 document sets, with each set consisting of 10documents.
A summary is required for eachdocument set.
Here we need to adjust theranking model for the multi-document task, i.e.,the importance of a word is calculated as itstotal frequency in the whole document setinstead of a single document.
For both tasks, thesummary length limit is 100 words.Table 1 and 2 below provide the averageROUGE-1 and ROUGE-2 scores (denoted as R-1 and R-2) of all the systems.
Moreover, weused paired two sample t-test to calculate thesignificance of the differences between a pair ofword and sentence position features.
The boldedscore in the tables indicates that that score issignificantly better than the correspondingpaired one.
For example, in Table 1, the boldedR-1 score of system WP DP means that it issignificantly better than the R-1 score of systemSP DP.
Besides the ROUGE scores, twostatistics, the number of ?first sentences 2 ?among the selected sentences (FS-N) and the2 A ?first sentence?
is the sentence at the fist positionof a document.923average position of the selected sentences (A-SP), are also reported in the tables for analysis.System R-1 R-2 FS-N A-SPWP DP 0.4473 0.1942 301 4.00SP DP 0.4396 0.1844 300 3.69WP IP 0.4543 0.2023 290 4.30SP IP 0.4502 0.1964 303 3.08WP GS 0.4544 0.2041 278 4.50SP GS 0.4509 0.1974 303 2.93WP BF 0.4544 0.2036 253 5.57SP BF 0.4239 0.1668 303 9.64None 0.4193 0.1626 265 10.06NIST 0.4445 0.1865 - -Hum 0.4568 0.1740 - -Table 1.
Results on the DUC 2001 data setSystem R-1 R-2 FS-N A-SPWP DP 0.3728 0.0911 89 4.16SP DP 0.3724 0.0908 112 2.68WP IP 0.3756 0.0912 108 3.77SP IP 0.3690 0.0905 201 1.01WP GS 0.3751 0.0916 110 3.67SP GS 0.3690 0.0905 201 1.01WP BF 0.3740 0.0926 127 3.14SP BF 0.3685 0.0903 203 1None 0.3550 0.0745 36 10.98NIST 0.3340 0.0686 - -Hum 0.4002 0.0962 - -Table 2.
Results on the DUC 2004 data setFrom Table 1 and Table 2, it is observed thatposition information is indeed very effective ingeneric summarization so that all the systemswith position features performed better than thesystem None which does not use any positioninformation.
Moreover, it is also clear that theproposed word position features consistentlyoutperform the corresponding sentence positionfeatures.
Though the gaps between the ROUGEscores are not large, the t-tests proved that wordposition features are significantly better on theDUC 2001 data set.
On the other hand, theadvantages of word position features oversentence position features are less significant onthe DUC 2004 data set.
One reason may be thatthe multiple documents have provided morecandidate sentences for composing the summary.Thus it is possible to generate a good summaryonly from the leading sentences in thedocuments.
According to Table 2, the average-sentence-position of system SP BF is 1, whichmeans that all the selected sentences are ?firstsentences?.
Even under this extreme condition,the performance is not much worse.The two statistics also show the differentpreferences of the features.
Compared to wordposition features, sentence position features arelikely to select more ?first sentences?
and alsohave smaller average-sentence-positions.
Theabnormally large average-sentence-position ofSP BF in DUC 2001 is because it does notdifferentiate all the other sentences except thefirst one.
The corresponding word-position-based system WP BF can differentiate thesentences since it is based on word positions, soits average-sentence-position is not that large.4.4 Query-focused SummarizationSince year 2005, DUC has adopted query-focused multi-document summarization tasksthat require creating a summary from a set ofdocuments to a given query.
This task has beenspecified as the main evaluation task over threeyears (2005-2007).
The data set of each yearcontains about 50 DUC topics, with each topicincluding 25-50 documents and a query.
In thisexperiment, we adjust the calculation of theword importance again for the query-focusedissue.
It is changed to the total number of theappearances that fall into the sentences with atleast one word in the query.
Formally, given thequery which is viewed as a set of wordsQ={w1, ?, wT}, a sentence set SQ is defined asthe set of sentences that contain at least one wiin Q.
Then the importance of a word w iscalculated by its frequency in SQ.
For the query-focused task, the summary length limit is 250words.Table 3 below provides the average ROUGE-1 and ROUGE-2 scores of all the systems on theDUC 2005-2007 data sets.
The boldfaced termsin the tables indicate the best results in eachcolumn.
According to the results, on query-focused summarization, position informationseems to be not as effective as on genericsummarization.
The systems with positionfeatures can not outperform the system None.
Infact, this is reasonable due to the requirementspecified by the pre-defined query.
Given thequery, the content of interest may be in any924position of the document and thus the positioninformation becomes less meaningful.On the other hand, we find that though thesystems with word position features cannotoutperform the system None, it doessignificantly outperform the systems withsentence position features.
This is also due tothe role of the query.
Since it may refer to thespecified content in any position of thedocuments, sentence position features are morelikely to fail in discovering the desiredsentences since they always prefer leadingsentences.
In contrast, word position featuresare less sensitive to this problem and thusperform better.
Similarly, we can see that thedirect proportion (DP), which has the least biasfor leading sentences, has the best performanceamong the four functions.System 2005 2006 2007 R-1 R-2 R-1 R-2 R-1 R-2WP DP 0.3791 0.0805 0.3909 0.0917 0.4158 0.1135SP DP 0.3727 0.0776 0.3832 0.0869 0.4118 0.1103WP IP 0.3772 0.0791 0.3830 0.0886 0.4106 0.1121SP IP 0.3618 0.0715 0.3590 0.0739 0.3909 0.1027WP GS 0.3767 0.0794 0.3836 0.0879 0.4109 0.1119SP GS 0.3616 0.0716 0.3590 0.0739 0.3909 0.1027WP BF 0.3740 0.0741 0.3642 0.0796 0.3962 0.1037SP BF 0.3647 0.0686 0.3547 0.0742 0.3852 0.1013NONE 0.3788 0.0791 0.3936 0.0924 0.4193 0.1140NIST 0.3353 0.0592 0.3707 0.0741 0.0962 0.3978Hum 0.4392 0.1022 0.4532 0.1101 0.4757 0.1402Table 3.
Results on the DUC 2005 - 2007 data setsSystem 2008 A 2008 B 2009 A 2009 B R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2WP DP 0.3687 0.0978 0.3758 0.1036 0.3759 0.1015 0.3693 0.0922SP DP 0.3687 0.0971 0.3723 0.1011 0.3763 0.1031 0.3704 0.0946WP IP 0.3709 0.1014 0.3741 0.1058 0.3758 0.1030 0.3723 0.0906SP IP 0.3619 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956WP GS 0.3705 0.1004 0.3732 0.1048 0.3770 0.1051 0.3731 0.0917SP GS 0.3625 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956WP BF 0.3661 0.0975 0.3678 0.0992 0.3720 0.1069 0.3650 0.0936SP BF 0.3658 0.0965 0.3674 0.0980 0.3683 0.1043 0.3654 0.0945NONE 0.3697 0.0978 0.3656 0.0915 0.3653 0.0934 0.3595 0.0834NIST 0.3389 0.0799 0.3192 0.0676 0.3468 0.0890 0.3315 0.0761Hum 0.4105 0.1156 0.3948 0.1134 0.4235 0.1249 0.3901 0.1059Table 4.
Results on the TAC 2008 - 2009 data sets4.5 Update SummarizationSince year 2008, the DUC summarization trackhas become a part of the Text AnalysisConference (TAC).
In the update summarizationtask, each document set is divided into twoordered sets A and B.
The summarization targeton set A is the same as the query-focused task inDUC 2005-2007.
As to the set B, the target is towrite an update summary of the documents inset B, under the assumption that the reader hasalready read the documents in set A.
The dataset of each year contains about 50 topics, andeach topic includes 10 documents for set A, 10documents for set B and an additional query.For set A, we follow exactly the same methodused in section 4.4; for set B, we make anadditional novelty check for the sentences in Bwith the MMR approach.
Each candidatesentence for set B is now compared to both theselected sentences in set B and in set A to925ensure its novelty.
In the update task, thesummary length limit is 100 words.Table 4 above provides the average ROUGE-1 and ROUGE-2 scores of all the systems on theTAC 2008-2009 data sets.
The results on set Aand set B are shown individually.
For the taskon set A which is almost the same as the DUC2005-2007 tasks, the results are also verysimilar.
A small difference is that the systemswith position features perform slightly betterthan the system None on these two data sets.Also, the difference between word positionfeatures and sentence position features becomessmaller.
One reason may be that the shortersummary length increases the chance ofgenerating good summaries only from theleading sentences.
This is somewhat similar tothe results reported in (Nenkova, 2005) thatposition information is more effective for shortsummaries.For the update set B, the results show thatposition information is indeed very effective.
Inthe results, all the systems with position featuressignificantly outperform the system None.
Weattribute the reason to the fact that we are moreconcerned with novel information whensummarizing update set B.
Therefore, the effectof the query is less on set B, which means thatthe effect of position information may be morepronounced in contrast.
On the other hand,when comparing the position features, we cansee that though the difference of the positionfeatures is quite small, word position featuresare still better in most cases.4.6 DiscussionBased on the experiments, we briefly concludethe effectiveness of position information indocument summarization.
In different tasks, theeffectiveness varies indeed.
It depends onwhether the given task has a preference for thesentences at particular positions.
Generally, ingeneric summarization, the position hypothesisworks well and thus the ordinal positioninformation is effective.
In this case, thoseposition features that are more distinctive, suchas GS and BF, can achieve better performances.In contrast, in the query-focused task that relatesto specified content in the documents, ordinalposition information is not so useful.
Therefore,the more distinctive a position feature is, theworse performance it leads to.
However, in theupdate summarization task that also involvesqueries, position information becomes effectiveagain since the role of the query is lessdominant on the update document set.On the other hand, by comparing the sentenceposition features and word position features onall the data sets, we can draw an overallconclusion that word position features areconsistently more appreciated.
For both generictasks in which position information is effectiveand query-focused tasks in which it is not soeffective, word position features show theiradvantages over sentence position features.
Thisis because of the looser position hypothesispostulated by them.
By avoiding arbitrarilyregarding the leading sentences as moreimportant, they are more adaptive to differenttasks and data sets.5 Conclusion and Future WorkIn this paper, we proposed a novel kind of wordposition features which consider the positions ofword appearances instead of sentence positions.The word position features were compared tosentence position features under the proposedsentence ranking model.
From the results on aseries of DUC data sets, we drew the conclusionthat the word position features are moreeffective and adaptive than traditional sentenceposition features.
Moreover, we also discussedthe effectiveness of position information indifferent summarization tasks.In our future work, we?d like to conduct moredetailed analysis on position information.Besides the ordinal positions, more kinds ofposition information can be considered to bettermodel the document structures.
Moreover, sinceposition hypothesis is not always correct in alldocuments, we?d also like to consider a pre-classification method, aiming at identifying thedocuments for which position information ismore suitable.Acknowledgement The work described inthis paper was supported by Hong Kong RGCProjects (PolyU5217/07E).
We are grateful toprofessor Chu-Ren Huang for his insightfulsuggestions and discussions with us.926ReferencesEdmundson, H. P.. 1969.
New methods in automaticExtracting.
Journal of the ACM, volume 16, issue2, pp 264-285.Gillick, D., Favre, B., Hakkani-Tur, D., Bohnet, B.,Liu, Y., Xie, S.. 2009.
The ICSI/UTDSummarization System at TAC 2009.
Proceedingsof Text Analysis Conference 2009.Jaime G. Carbonell and Jade Goldstein.
1998.
Theuse of MMR, diversity-based reranking forreordering documents and producing summaries.Proceedings of the 21st annual international ACMSIGIR conference on Research and developmentin information retrieval, pp 335-336.Lin, C. and Hovy, E.. 1997.
Identifying Topics byPosition.
Proceedings of the fifth conference onApplied natural language processing 1997, pp283-290.Luhn, H. P.. 1958.
The automatic creation ofliterature abstracts.
IBM J. Res.
Develop.
2, 2, pp159-165.Nenkova.
2005.
Automatic text summarization ofnewswire: lessons learned from the documentunderstanding conference.
Proceedings of the20th National Conference on ArtificialIntelligence, pp 1436-1441.Ouyang, Y., Li, S., Li, W.. 2007.
Developinglearning strategies for topic-based summarization.Proceedings of the sixteenth ACM conference onConference on information and knowledgemanagement, pp 79-86.Radev, D., Jing, H., Sty?s, M. and Tam, D.. 2004.Centroid-based summarization of multipledocuments.
Information Processing andManagement, volume 40, pp 919?938.Schilder, F., Kondadadi, R.. 2008.
FastSum: fast andaccurate query-based multi-documentsummarization.
Proceedings of the 46th AnnualMeeting of the Association for ComputationalLinguistics on Human Language Technologies,short paper session, pp 205-208.Toutanova, K. et al 2007.
The PYTHYsummarization system: Microsoft research atDUC 2007.
Proceedings of DocumentUnderstanding Conference 2007.927
