Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 573?579, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUNITOR-HMM-TK: Structured Kernel-based Learningfor Spatial Role LabelingEmanuele Bastianelli(?)(?
), Danilo Croce(?
), Daniele Nardi(?
), Roberto Basili(?)(?)
DICIIUniversity of Roma Tor VergataRome, Italy{bastianelli}@ing.uniroma2.it(?)
DIIUniversity of Roma Tor VergataRome, Italy{croce,basili}@info.uniroma2.it(?)
DIAGUniversity of Roma La SapienzaRome, Italy{nardi}@dis.uniroma1.itAbstractIn this paper the UNITOR-HMM-TK systemparticipating in the Spatial Role Labelingtask at SemEval 2013 is presented.
Thespatial roles classification is addressed as asequence-based word classification problem:the SVMhmm learning algorithm is applied,based on a simple feature modeling and a ro-bust lexical generalization achieved through aDistributional Model of Lexical Semantics.
Inthe identification of spatial relations, roles arecombined to generate candidate relations, laterverified by a SVM classifier.
The SmoothedPartial Tree Kernel is applied, i.e.
a con-volution kernel that enhances both syntacticand lexical properties of the examples, avoid-ing the need of a manual feature engineeringphase.
Finally, results on three of the five tasksof the challenge are reported.1 IntroductionReferring to objects or entities in the space, as wellas to relations holding among them, is one of themost important functionalities in natural languageunderstanding.
The detection of spatial utterancesthus finds many applications, such as in GPS navi-gation systems, or Human-Robot Interaction (HRI).In Computational Linguistics, the task of recog-nizing spatial information is known as Spatial RoleLabeling (SpRL), as discussed in (KordJamshidi etal., 2010).
Let us consider the sentence:[A man]TRAJECTOR is sitting [on]SPATIAL INDICATOR[a chair]LANDMARK and talking on the phone.
(1)where three roles are labeled: the phrase ?A man?refers to a TRAJECTOR, ?a chair?
to a LAND-MARK and they are related by the spatial expres-sion ?on?
denoted as SPATIAL INDICATOR.
Thelast role establishes the type of the spatial relation,e.g.
Regional.
The ambiguity of natural languagemakes this task very challenging.
For example, inthe same Example 1, another preposition ?on?
canbe considered, but the phrase ?the phone?
is not aspatial role, as it refers to a communication mean.This mainly depends on the semantics of the gram-matical head words, i.e.
chair and phone.
Such phe-nomena are crucial in many learning frameworks,as in kernel-based learning (Shawe-Taylor and Cris-tianini, 2004), where the decision is based on thesimilarity between training and testing data.This paper describes the UNITOR-HMM-TK sys-tem participating in the Semeval 2013 Spatial RoleLabeling Task (Kolomiyets et al 2013), addressingthree of the five defined sub-tasks:?
Task A: Spatial Role Classification.
It con-sists in labeling short sentences with spatialroles among SPATIAL INDICATOR, TRAJEC-TOR and LANDMARK.?
Task B: Relation Identification.
It consists inthe identification of relations among roles iden-tified in Task A.
This task does not involve thesemantic relation classification.?
Task C: Spatial Role Classification.
It con-sists in labeling short documents with spa-tial roles among the extended role set: TRA-JECTOR, LANDMARK, SPATIAL INDICATOR,MOTION INDICATOR, PATH, DIRECTION andDISTANCE.The UNITOR-HMM-TK system addresses both theproblems of identifying spatial roles and relations asa sequence of two main classification steps.573In the first step, each word in the sentence isclassified by a sequence-based classifier with re-spect to the possible spatial roles.
It is in linewith other methods based on sequence-based clas-sifier for SpRL (Kordjamshidi et al 2011; Kord-jamshidi et al 2012b).
Our labeling has been in-spired by the work in (Croce et al 2012), wherethe SVMhmm learning algorithm, formulated in (Al-tun et al 2003), has been applied to the classi-cal FrameNet-based Semantic Role Labeling.
Themain contribution in (Croce et al 2012) is the adop-tion of shallow grammatical features (e.g.
POS-tag sequences) instead of the full syntax of the sen-tence, in order to avoid over-fitting over trainingdata.
Moreover, lexical information has been gen-eralized through the use of a Word Space, in linewith (Schutze, 1998; Sahlgren, 2006): it consistsin a Distributional Model of Lexical Semantics de-rived from the unsupervised analysis of an unla-beled large-scale corpus.
The result is a geometri-cal space where words with similar meaning, e.g.involved in a paradigmatic or almost-synonymic re-lations, will be projected in similar vectors.
As anexample, we expect that a word like ?table?, maybea LANDMARK in a training example, is more similarto ?chair?
as compared with ?phone?.In the second step, all roles found in a sentence arecombined to generate candidate relations, which arethen verified by a Support Vector Machine (SVM)classifier.
As the entire sentence is informative to de-termine the proper conjunction of all roles, we applya kernel function within the classifier, that enhancesboth syntactic and lexical information of the exam-ples.
We adopted the Smoothed Partial Tree Kernel(SPTK), defined in (Croce et al 2011): it is con-volution kernel that allows to measure the similar-ity between syntactic structures, which are partiallysimilar and whose nodes can differ, but are semanti-cally related.
Each example is represented as a treestructure directly derived from the sentence depen-dency parse, thus avoiding the manual definition offeatures.
Similarity between lexical nodes is mea-sured in the same Word Space mentioned above.In the rest of the paper, Section 2 discusses theSVMhmm based approach.
The SPTK-based learn-ing algorithm will be presented in Section 3.
Finally,results obtained in the competition are discussed inSection 4.2 Sequential Tagging for Spatial RoleClassificationThe system proposed for the Spatial Role Classi-fication task is based on the SVMhmm formula-tion discussed in (Altun et al 2003).
It extendsclassical SVMs by learning a discriminative modelisomorphic to a k-order Hidden Markov Modelthrough the Structural SVM formulation (Tsochan-taridis et al 2005).
In the discriminative viewof SVMhmm, given an observed input word se-quence x = (x1 .
.
.
xl) ?
X of feature vectorsx1 .
.
.
xl, the model predicts a sequence of labelsy = (y1 .
.
.
yl) ?
Y after learning a linear discrim-inant function F : X ?
Y ?
R over input/outputpairs.
Each word is then modeled as a set of linearfeatures that express lexical information as well assyntactic information surrogated by POS n-grams.With respect to other works using SVMhmm forSpRL, such as (Kordjamshidi et al 2012b), we in-vestigate another set of possible features, as the onesproposed in (Croce et al 2012): the aim is to pro-vide an agile system that takes advantages in adopt-ing only shallow grammatical features, thus ignoringthe full syntactic information of a sentence.
The syn-tactic features derived from a dependency parse pro-cess are surrogated by POS n-grams.
According tothis, our feature modeling adopts the IOB notationdiscussed in (Croce et al 2012).
It provides a classlabel for each token, mapping them into artificialclasses representing the beginning (B), the inside (I)or ending (O) of a spatial role, plus the label of theclassified role (i.e.
BSPIND for the starting token of aSPATIAL INDICATOR); words external to every roleare labeled with the special class ( ).
According tothis notation, the labeling of Example 1 can be ex-pressed as follows: ?A/BTRAJ man/OTRAJ is/ sitting/on/BSPIND a/BLAND chair/OLAND and/ .
.
.
?.In order to reduce the complexity of the entireclassification task, two phases are applied.
In TaskA, as in (Kordjamshidi et al 2011), the first phaseaims at labeling only SPATIAL INDICATOR, as theyshould relate remaining spatial expressions.
For thesame reason, in Task C we first label only SPA-TIAL INDICATOR and MOTION INDICATOR.
Rolesclassified in this step are considered pivot and theycan be used as features for the classification of theother roles: TRAJECTORS and LANDMARKS for574Task A while TRAJECTORS, LANDMARKS, PATHS,DISTANCES and DIRECTIONS for Task C.For the classification of SPATIAL and MOTIONINDICATOR, each word, such as the first ?on?
occur-rence in the Example 1, is modeled through the fol-lowing features: its lemma (on) and POS tag (IN);the left and right lexical contexts, represented by then words before (man::NN is::VBZ sitting::VBG) andafter (a::DT chair::NN and::CC); the left and rightsyntactic contexts as the POS n-grams occurring be-fore (i.e.
NN VBZ VBZ VBG NN VBZ VBG) andafter (i.e.
DT NN NN CC DT NN CC) the word.For the TRAJECTOR and LANDMARK classifica-tion in Task A, each word is represented by the samefeatures described above, plus the following ones(with respect to Example 1, the token relative tothe word man): lemma of the SPATIAL INDICATOR(on); Positional Feature: distance from the SPATIALINDICATOR in terms of number of tokens (-3); rel-ative position with respect to the SPATIAL INDICA-TOR, that is before or after (before); a boolean fea-ture that indicates whether or not the current token isa SPATIAL INDICATOR; the number of words com-posing the SPATIAL INDICATOR (here 1).In Task C, for the classification with respect tothe complete set of roles, each word is modeled bythe previous features together with the following:distance from the MOTION INDICATOR in termsof number of tokens; relative position with respectto the MOTION INDICATOR (before and after); aboolean feature that indicates whether or not the cur-rent token is a MOTION INDICATOR; the number ofwords that composes the MOTION INDICATOR.
Inboth Tasks A and C the symbols SI and MI to rep-resent a SPATIAL INDICATOR or a MOTION INDI-CATOR are used respectively to represent the targetpivot role within any n-gram.In order to increase the robustness of our model-ing, we extended the lexical information with fea-tures derived from a distributional analysis overlarge texts.
In essence, we represent the lexical se-mantic similarity between different words with sim-ilar meaning.
We extend a supervised approachthrough the adoption of vector based models of lex-ical meaning: a large-scale corpus is statistically an-alyzed and a Word Space, (Sahlgren, 2006), is ac-quired as follows.
A word-by-context matrix Mis obtained through a large scale corpus analysis.Then the Latent Semantic Analysis (Landauer andDumais, 1997) technique is applied to reduce thespace dimensionality.
Moreover it provides a wayto project a generic word wi into a k-dimensionalspace where each row corresponds to the representa-tion vector ~wi.
In such a space, the distance betweenvectors reflects the similarity between correspond-ing words.
The resulting feature vector representingwi is then augmented with ~wi, as in (Croce et al2010), where the benefits of such information havebeen reported in the FrameNet-based Semantic RoleLabeling task.3 Relation identificationThe UNITOR-HMM-TK system tackles Relation Iden-tification task by determining which spatial roles,discovered in the previous classification phase, canbe combined to determine valid spatial relations.Our method is inspired by the work of (Roberts andHarabagiu, 2012), where all possible spatial rolesare first generated through heuristics and then com-binatorially combined to acquire candidate relations;valid spatial relations are finally determined using aSVM classifier.
We aim at reducing the potentiallyhuge search space, by considering only spatial rolesproposed by our sequential tagging approach, de-scribed in Section 2.
Most importantly, we avoid themanual feature engineering phase of (Roberts andHarabagiu, 2012).
Candidate relations are not rep-resented as vectors, whose dimensions are manuallydefined features useful for the target classification.We directly apply the Smoothed Partial Tree-Kernel(SPTK), proposed in (Croce et al 2011), to estimatethe similarity among a specific tree representation.Tree kernels exploit syntactic similarity throughthe idea of convolutions among substructures.Any tree kernel computes the number of commonsubstructures between two trees T1 and T2 withoutexplicitly considering the whole fragment space.
Itsgeneral equation is reported hereafter:TK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2)where NT1 and NT2 are the sets of the T1?s andT2?s nodes respectively, and ?
(n1, n2) is equal tothe number of common fragments rooted in the n1and n2 nodes1.
The SVM classifier is thus trained in1To have a similarity score between 0 and 1, a normalization575a implicit very high-dimensional space, where eachdimension reflects a possible tree sub-structure, thusavoiding the need of an explicit feature definition.The function ?
determines the nature of such space.For example, Syntactic Tree Kernel (STK) are usedto model complete context free rules as in (Collinsand Duffy, 2001).The algorithm for SPTK (Croce et al 2011)pushes for more emphasis on lexical nodes.
The?
function allows to recursively matches tree struc-tures and lexical nodes: this allows to match frag-ments having same structure but different lexicalnodes, by assigning a score proportional to theproduct of the lexical similarities, thus generalizinggrammatical and lexical information in training data.While similarity can be modeled directly over lexi-cal resources, e.g.
WordNet as discussed in (Peder-sen et al 2004), their development can be very ex-pensive, thus limiting the coverage of the resultingconvolution kernel, especially in specific applicationdomains.
Again, a Word Space model is adopted:given two words, the term similarity function ?
isestimated as the cosine similarity between the corre-sponding projections.As proposed in (Croce et al 2011), the SPTK isapplied to examples modeled according the Gram-matical Relation Centered Tree (GRCT) representa-tion, which is derived from the original dependencyparse structure.
Figure 1 shows the GRCT for Exam-ple 1: non-terminal nodes reflect syntactic relations,such as subject (NSUBJ); pre-terminals are the POS,such as nouns (NN), and leaves are lexemes, such asman::n2.
Non-terminal nodes associated with a roleare enriched with the role name, e.g.
NSUBJTRAJ.All nodes not covering any role are pruned out, sothat all information not concerning spatial aspectsthat would introduce noise is ignored.In this setting, positive examples are provided byconsidering sentences labeled by roles involved ina valid relation.
The definition of negative exam-ples is more difficult.
We considered all roles la-belled by the SVMhmm based system, discussed inSection 2.
For each incorrect labeling over the an-in the kernel space, i.e.
TK(T1,T2)?TK(T1,T1)?TK(T2,T2)is applied.2Each word is lemmatized to reduce data sparseness, butthey are enriched with POS tags to avoid confusing words fromdifferent grammatical categories.ROOTPREPSPINDPOBJLANDNNchair::nDETLANDDTa::dINon::iVBGsit::vNSUBJTRAJNNman::nDETTRAJDTa::dFigure 1: GRCT representation of a positive example de-rived from a correct labeling from Example 1ROOTCONJPREPSPINDPOBJLANDNNphone::nDETLANDDTthe::dINon::iVBGtalk::vVBGsit::vNSUBJTRAJNNman::nDETTRAJDTa::dFigure 2: GRCT representation of a negative example de-rived from a wrong labeling from Example 1notated material, a set of negative examples is ac-quired by combining all proposed roles.
In orderto avoid over-fitting, a n-fold schema has been ap-plied: it is needed to avoid the SVMhmm label-ing the same sentences used for training.
More-over, constraints over the relation are imposed toavoid violations of the Spatial Role theory: inTask B each relation must be composed at least bya SPATIAL INDICATOR, LANDMARK and a TRA-JECTOR or by a SPATIAL INDICATOR, implicitLANDMARK and a TRAJECTOR.
Let us considera possible labeling of Example 1: ?
[A man]TRAJis sitting [on]SPIND [a chair]LAND and talking[on]SPIND[the phone]LAND?
; here, the second SPA-TIAL INDICATOR ?on?
and the LANDMARK ?thephone?
are incorrectly labeled.
A negative exampleis thus obtained by considering these roles togheterwith the TRAJECTOR ?the phone?, as shown in Fig-ure 2.
Other two negative examples can be generatedby combining the remaining two roles.4 ResultsIn this section experimental results of theUNITOR-HMM-TK system in the Spatial RoleLabeling task at SemEval 2013 are reported.
InTasks A and B, the dataset is a corrected version576of the same training dataset employed in (Kord-jamshidi et al 2012a)3.
The dataset for Task C waspart of the Confluence corpus4.
More details aboutthe dataset are provided in (Kolomiyets et al 2013).In all experiments, sentences are processed with theStanford CoreNLP5, for Part-of-Speech tagging,lemmatization (Task A and C) and dependencyparsing (Task B).The sequential labeling system described in Sec-tion 2 has been made available by the SVMhmmsoftware6.
The estimation of the semanticallySmoothed Partial Tree Kernel (SPTK), described inSection 3 is made available by an extended ver-sion of SVM-LightTK software7 (Moschitti, 2006),implementing the smooth matching between treenodes.
Similarity between lexical nodes is estimatedas the cosine similarity in the co-occurrence WordSpace described above, as in (Croce et al 2011).The co-occurrence Word Space is acquiredthrough the distributional analysis of the UkWaCcorpus (Baroni et al 2009).
First, all words oc-curring more than 100 times (i.e.
the targets) arerepresented through vectors.
The original space di-mensions are generated from the set of the 20,000most frequent words (i.e.
features) in the UkWaCcorpus.
One dimension describes the Pointwise Mu-tual Information score between one feature, as it oc-curs on a left or right window of 3 tokens around atarget.
Left contexts of targets are treated differentlyfrom the right ones, in order to capture asymmetricsyntactic behaviors (e.g., useful for verbs): 40,000dimensional vectors are thus derived for each tar-get.
The Singular Value Decomposition is appliedand the space dimensionality is reduced to k = 100.4.1 Results in Task ATwo different runs were submitted for Task A. Thefirst takes into account all roles labeled accordinglyto the approach described in Section 2.
Results, interm of precision, recall and F-measure for each spa-tial role are shown in Table 1.
The second run con-siders only those roles composing the relations that3The initial number of sentences was of 600, but it decreasedafter the elimination of 21 duplicated sentences.4Three of the original 95 files were ignored because of someissues with their format.
See http://confluence.org5http://nlp.stanford.edu/software/corenlp.shtml6http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html7http://disi.unitn.it/moschitti/Tree-Kernel.htmare positively classified in Task B and it will be dis-cussed in Section 4.2.A tuning phase has been carried out through a 10-fold cross validation: it allowed to find the best clas-sifier parameters.
The evaluation of the system per-formances is measured using a character based mea-sure, i.e.
considering the number of characters in thespan that overlap a role in the gold-standard test.Spatial Role Precision Recall F-MeasureSPATIAL INDICATOR 0.967 0.889 0.926TRAJECTOR 0.684 0.681 0.682LANDMARK 0.741 0.835 0.785Table 1: Task A results (first run)The overall performances of the first run arevery promising in terms of both precision and re-call.
In particular, the SPATIAL INDICATOR label-ing achieves a significant F-Measure of 0.926 with aprecision of 0.967.
The sequence labeling approachprovides good results for the LANDMARK and theTRAJECTOR roles too.
Unfortunately, these resultsare not comparable with the performances obtainedthe last year edition of the SpRL task, where a gram-matical head word-based measure has been applied.The main difficulty in the SPATIAL INDICATORclassification concerns the tagging of a larger orsmaller span for the roles, as for ?at the back?
that istagged as ?at the back of?.
On the contrary, for roleslike ?to the left and the right?
the system produces atag covering just the first three words, ?to the left?,because this shortest sequence was far more repre-sented within the training set.
Some roles corre-sponding to unknown word sequences, such as ?onthe very right?, were not labeled, leading to the littledrop in terms of recall for the SPATIAL INDICATOR.Another issue in the TRAJECTOR and LAND-MARK labeling is due to the absence of specific rolesequences in the training set, such as LANDMARK-TRAJECTOR-SPATIAL INDICATOR labeled in thetest sentence ?there is a [coffee table]LANDMARK witha [sofa]TRAJECTOR [around]SP.IND?
: the SVMhmmclassifier in fact tends to discard any sequence un-seen during training.
Another issue concerns thedifficulty in assigning the TRAJECTOR role to theproper SPATIAL INDICATOR: in the sentence ?abench with a person lying on it?
where both ?abench?
and ?a person?
are tagged as TRAJECTOR.5774.2 Results in Task BTask B has been tackled using the SPTK-based Re-lation Identification approach, described in Section3.
In particular, the SVM classifier is fed with 741positive examples, corresponding to the number ofgold relations, while the negative examples genera-tion process, described in Section 3, yielded 2,256examples.
The same Word Space described in theprevious section has been used to compute the se-mantic similarity within the SPTK.
For the tuningphase, a 80-20 fixed split has been applied.For this task, two different measures are pre-sented.
The Relaxed measure considers a relationcorrect if each role composing it has at least onecharacter overlapping the corresponding gold role.The Strict measure considers a relation correct onlyif each role in it has all the characters overlappingwith the gold role.
The first measure is more com-parable with the one used in (Kordjamshidi et al2012a), where a relation is considered correct onlyif each grammatical head word of the involved roleswere correctly labeled.
The results achieved in thistask by our system are reported in Table 2.Spatial Role Precision Recall F-MeasureRELAXED 0.551 0.391 0.458STRICT 0.431 0.306 0.358Table 2: Task B resultsThe problem for this task is more challenging.
Infact, the overall task is strictly biased by the qualityof the SVMhmm based classifier and inherits all thelimitations underlined in Section 4.2.
This mostlyaffects the recall, because every error generated dur-ing the role classification is cumulative and losingonly one role in Task A implies a misclassificationof the whole relation.
However, it is important tonotice that these results have been achieved withoutany manual feature engineering nor any heuristics orhand coded lexical resource.Spatial Role Precision Recall F-MeasureSPATIAL INDICATOR 0.968 0.585 0.729TRAJECTOR 0.682 0.493 0.572LANDMARK 0.801 0.560 0.659Table 3: Task A results (second run)In the second run of Task A, we evaluate the con-tribution of this syntactic information to filter outroles.
In Table 3 results of the second run for TaskA are reported (see previous Section).
As expected,the recall measure shows a performance drop withrespect to results shown in Table 1: the results pro-posed in the first run represents an upperbound to therecall as any novel role is added here.
However, theprecision measure for the LANDMARK role classifi-cation is improved of about 10%.Spatial Role Precision Recall F-MeasureSPATIAL INDICATOR 0.609 0.479 0.536MOTION INDICATOR 0.892 0.294 0.443TRAJECTOR 0.565 0.317 0.406LANDMARK 0.662 0.476 0.554PATH 0.775 0.295 0.427DIRECTION 0.312 0.229 0.264DISTANCE 0.946 0.331 0.490Table 4: Task C results4.3 Results in Task CIn Task C the extended set of roles is considered.According to this, the number of possible labels tobe learnt by the system increases, thus making theproblem more challenging.
As for Task A, here theSVMhmm has been trained over the whole trainingset, using a 10-fold cross validation in the tuningphase.
Moreover, the sentences of the Confluencecorpus are far more complex than the ones from theCLEF corpus.
Confluence sentences have a morenarrative nature with respect to the CLEF sentences,that are simple description of images.
The combina-tion of these two factors resulted in a large drop inthe performance, especially for the recall.As shown by the results in Table 4, DIRECTIONis the most difficult role to be classified, probablybecause it is represented by many different word se-quences.
Other roles are found in few instances, butalmost all correct, as for DISTANCE and MOTIONINDICATOR.
The high value of Precision for theDISTANCE role is justified by the fact that when thisrole is composed by a number, (i.e.
?530 meters?
),the system identified and classified it well, while fora representation with only words (i.e.
?very close?
)the system did not retrieved it at all.Acknowledgements This work has been partiallyfunded by European Union VII Framework Pro-gramme under the project Speaky for Robot withinthe framework of the ECHORD Project.578ReferencesY.
Altun, I. Tsochantaridis, and T. Hofmann.
2003.
Hid-den Markov support vector machines.
In Proceedingsof the International Conference on Machine Learning.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proceedings of NeuralInformation Processing Systems (NIPS?2001), pages625?632.Danilo Croce, Cristina Giannone, Paolo Annesi, andRoberto Basili.
2010.
Towards open-domain semanticrole labeling.
In ACL, pages 237?246.Danilo Croce, Alessandro Moschitti, and Roberto Basili.2011.
Structured lexical similarity via convolutionkernels on dependency trees.
In Proceedings ofEMNLP, Edinburgh, Scotland, UK.Danilo Croce, Giuseppe Castellucci, and Emanuele Bas-tianelli.
2012.
Structured learning for semantic rolelabeling.
In Intelligenza Artificiale, 6(2):163?176,January.Oleksandr Kolomiyets, Parisa Kordjamshidi, StevenBethard, and Marie-Francine Moens.
2013.
Semeval-2013 task 2: Spatial role labeling.
In Proceedings ofthe 7th International Workshop on Semantic Evalua-tion.
Association for Computational Linguistics.Parisa KordJamshidi, Martijn van Otterlo, and Marie-Francine Moens.
2010.
Spatial role labeling: Taskdefinition and annotation scheme.
In LREC.Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-Francine Moens.
2011.
Spatial role labeling: To-wards extraction of spatial relations from natural lan-guage.
ACM Trans.
Speech Lang.
Process., 8(3):4:1?4:36, December.Parisa Kordjamshidi, Steven Bethard, and Marie-Francine Moens.
2012a.
Semeval-2012 task 3: Spa-tial role labeling.
In SemEval 2012, pages 365?373,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.Parisa Kordjamshidi, Paolo Frasconi, Martijn Van Ot-terlo, Marie-Francine Moens, and Luc De Raedt.2012b.
Relational learning for spatial relation extrac-tion from natural language.
In Proceedings of the21st international conference on Inductive Logic Pro-gramming, ILP?11, pages 204?220, Berlin, Heidel-berg.
Springer-Verlag.T.
Landauer and S. Dumais.
1997.
A solution to plato?sproblem: The latent semantic analysis theory of ac-quisition, induction and representation of knowledge.Psychological Review, 104(2):211?240.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InECML, pages 318?329, Berlin, Germany, September.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity - Measuring the Re-latedness of Concept.
In Proc.
of 5th NAACL, Boston,MA.Kirk Roberts and Sanda Harabagiu.
2012.
Utd-sprl: Ajoint approach to spatial role labeling.
In SemEval2012), pages 419?424, Montre?al, Canada, 7-8 June.Association for Computational Linguistics.Magnus Sahlgren.
2006.
The Word-Space Model.
Ph.D.thesis, Stockholm University.Hinrich Schutze.
1998.
Automatic word sense discrimi-nation.
Journal of Computational Linguistics, 24:97?123.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large margin meth-ods for structured and interdependent output variables.J.
Machine Learning Reserach., 6, December.579
