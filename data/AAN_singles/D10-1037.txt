Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 377?387,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsIncorporating Content Structure into Text Analysis ApplicationsChristina Sauper, Aria Haghighi, Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{csauper, aria42, regina}@csail.mit.eduAbstractIn this paper, we investigate how modelingcontent structure can benefit text analysis ap-plications such as extractive summarizationand sentiment analysis.
This follows the lin-guistic intuition that rich contextual informa-tion should be useful in these tasks.
Wepresent a framework which combines a su-pervised text analysis application with the in-duction of latent content structure.
Both ofthese elements are learned jointly using theEM algorithm.
The induced content struc-ture is learned from a large unannotated cor-pus and biased by the underlying text analysistask.
We demonstrate that exploiting contentstructure yields significant improvements overapproaches that rely only on local context.11 IntroductionIn this paper, we demonstrate that leveraging doc-ument structure significantly benefits text analysisapplications.
As a motivating example, considerthe excerpt from a DVD review shown in Table 1.This review discusses multiple aspects of a product,such as audio and video properties.
While the word?pleased?
is a strong indicator of positive sentiment,the sentence in which it appears does not specify theaspect to which it relates.
Resolving this ambiguityrequires information about global document struc-ture.A central challenge in utilizing such informa-tion lies in finding a relevant representation of con-tent structure for a specific text analysis task.
For1Code and processed data presented here are available athttp://groups.csail.mit.edu/rbg/code/content structure.htmlAudio Audio choices are English, Spanish and FrenchDolby Digital 5.1 ... Bass is still robust and powerful,giving weight to just about any scene ?
most notablythe film?s exciting final fight.
Fans should be pleasedwith the presentation.Extras This single-disc DVD comes packed in a blackamaray case with a glossy slipcover.
Cover art hasclearly been designed to appeal the Twilight crowd ...Finally, we?ve got a deleted scenes reel.
Most of theexcised scenes are actually pretty interesting.Table 1: An excerpt from a DVD review.instance, when performing single-aspect sentimentanalysis, the most relevant aspect of content struc-ture is whether a given sentence is objective or sub-jective (Pang and Lee, 2004).
In a multi-aspectsetting, however, information about the sentencetopic is required to determine the aspect to whicha sentiment-bearing word relates (Snyder and Barzi-lay, 2007).
As we can see from even these closely re-lated applications, the content structure representa-tion should be intimately tied to a specific text anal-ysis task.In this work, we present an approach in which acontent model is learned jointly with a text analy-sis task.
We assume complete annotations for thetask itself, but we learn the content model from raw,unannotated text.
Our approach is implemented ina discriminative framework using latent variables torepresent facets of content structure.
In this frame-work, the original task features (e.g., lexical ones)are conjoined with latent variables to enrich the fea-tures with global contextual information.
For ex-ample, in Table 1, the feature associated with the377word ?pleased?
should contribute most strongly tothe sentiment of the audio aspect when it is aug-mented with a relevant topic indicator.The coupling of the content model and the task-specific model allows the two components to mutu-ally influence each other during learning.
The con-tent model leverages unannotated data to improvethe performance of the task-specific model, whilethe task-specific model provides feedback to im-prove the relevance of the content model.
The com-bined model can be learned effectively using a novelEM-based method for joint training.We evaluate our approach on two complementarytext analysis tasks.
Our first task is a multi-aspectsentiment analysis task, where a system predicts theaspect-specific sentiment ratings (Snyder and Barzi-lay, 2007).
Second, we consider a multi-aspect ex-tractive summarization task in which a system ex-tracts key properties for a pre-specified set of as-pects.
On both tasks, our method for incorporatingcontent structure consistently outperforms structure-agnostic counterparts.
Moreover, jointly learningcontent and task parameters yields additional gainsover independently learned models.2 Related WorkPrior research has demonstrated the usefulness ofcontent models for discourse-level tasks.
Examplesof such tasks include sentence ordering (Barzilayand Lee, 2004; Elsner et al, 2007), extraction-basedsummarization (Haghighi and Vanderwende, 2009)and text segmentation (Chen et al, 2009).
Sincethese tasks are inherently tied to document structure,a content model is essential to performing them suc-cessfully.
In contrast, the applications considered inthis paper are typically developed without any dis-course information, focusing on capturing sentence-level relations.
Our goal is to augment these modelswith document-level content information.Several applications in information extractionand sentiment analysis are close in spirit to ourwork (Pang and Lee, 2004; Patwardhan and Riloff,2007; McDonald et al, 2007).
These approachesconsider global contextual information when de-termining whether a given sentence is relevant tothe underlying analysis task.
All assume that rele-vant sentences have been annotated.
For instance,Pang and Lee (2004) refine the accuracy of sen-timent analysis by considering only the subjectivesentences of a review as determined by an indepen-dent classifier.
Patwardhan and Riloff (2007) takea similar approach in the context of information ex-traction.
Rather than applying their extractor to allthe sentences in a document, they limit it to event-relevant sentences.
Since these sentences are morelikely to contain information of interest, the extrac-tion performance increases.Another approach, taken by Choi and Cardie(2008) and Somasundaran et al (2009) uses lin-guistic resources to create a latent model in a task-specific fashion to improve performance, rather thanassuming sentence-level task relevancy.
Choi andCardie (2008) address a sentiment analysis task byusing a heuristic decision process based on word-level intermediate variables to represent polarity.Somasundaran et al (2009) similarly uses a boot-strapped local polarity classifier to identify sentencepolarity.McDonald et al (2007) propose a modelwhich jointly identifies global polarity as well asparagraph- and sentence-level polarity, all of whichare observed in training data.
While our approachuses a similar hierarchy, McDonald et al (2007) isconcerned with recovering the labels at all levels,whereas in this work we are interested in using la-tent document content structure as a means to benefittask predictions.While our method also incorporates contextualinformation into existing text analysis applications,our approach is markedly different from the aboveapproaches.
First, our representation of context en-codes more than the relevance-based binary distinc-tion considered in the past work.
Our algorithm ad-justs the content model dynamically for a given taskrather than pre-specifying it.
Second, while previ-ous work is fully supervised, in our case relevanceannotations are readily available for only a few ap-plications and are prohibitively expensive to obtainfor many others.
To overcome this drawback, ourmethod induces a content model in an unsupervisedfashion and connects it via latent variables to thetarget model.
This design not only eliminates theneed for additional annotations, but also allows thealgorithm to leverage large quantities of raw data fortraining the content model.
The tight coupling of rel-378evance learning with the target analysis task leads tofurther performance gains.Finally, our work relates to supervised topic mod-els in Blei and McAullife (2007).
In this work, la-tent topic variables are used to generate text as wellas a supervised sentiment rating for the document.However, this architecture does not permit the usageof standard discriminative models which conditionfreely on textual features.3 Model3.1 Problem FormulationIn this section, we describe a model which incorpo-rates content information into a multi-aspect sum-marization task.2 Our approach assumes that attraining time we have a collection of labeled doc-uments DL, each consisting of the document texts and true task-specific labeling y?.
For the multi-aspect summarization task, y?
consists of sequencelabels (e.g., value or service) for the tokens of adocument.
Specifically, the document text s iscomposed of sentences s1, .
.
.
, sn and the label-ings y?
consists of corresponding label sequencesy1, .
.
.
, yn.3As is common in related work, we model each yiusing a CRF which conditions on the observed doc-ument text.
In this work, we also assume a contentmodel, which we fix to be the document-level HMMas used in Barzilay and Lee (2004).
In this contentmodel, each sentence si is associated with a hiddentopic variable Ti which generates the words of thesentence.
We will use T = (T1, .
.
.
, Tn) to refer tothe hidden topic sequence for a document.
We fixthe number of topics to a pre-specified constant K.3.2 Model OverviewOur model, depicted in Figure 1, proceeds as fol-lows: First the document-level HMM generatesa hidden content topic sequence T for the sen-tences of a document.
This content component isparametrized by ?
and decomposes in the standard2In Section 3.6, we discuss how this framework can be usedfor other text analysis applications.3Note that each yi is a label sequence across the words in si,rather than an individual label.y1iymiy2i.
.
.Tiw1iwmiw2i.
.
.Ti?1Ti+1(w2i= pleased) ?
(Ti= 3)w2i= pleased...siFigure 1: A graphical depiction of our model forsequence labeling tasks.
The Ti variable representsthe content model topic for the ith sentence si.
Thewords of si, (w1i , .
.
.
, wmi ), each have a task label(y1i , .
.
.
, ymi ).
Note that each token label has anundirected edge to a factor containing the words ofthe current sentence, si as well as the topic of thecurrent sentence Ti.HMM fashion:4P?
(s,T ) =n?i=1P?(Ti|Ti?1)?w?siP?
(w|Ti) (1)Then the label sequences for each sentence inthe document are independently modeled as CRFswhich condition on both the sentence features andthe sentence topic:P?
(y|s,T ) =n?i=1P?
(yi|si, Ti) (2)Each sentence CRF is parametrized by ?
and takesthe standard form:P?
(y|s, T ) ?exp???
?j?T [fN (yj, s, T ) + fE(yj, yj+1)]??
?4We also utilize a hierarchical emission model so that eachtopic distribution interpolates between a topic-specific distribu-tion as well as a shared background model; this is intended tocapture domain-specific stop words.379Tsy??
?ContentParametersTaskParametersTask LabelsTextContentStructureFigure 2: A graphical depiction of the generativeprocess for a labeled document at training time (SeeSection 3); shaded nodes indicate variables whichare observed at training time.
First the latent un-derlying content structure T is drawn.
Then, thedocument text s is drawn conditioned on the contentstructure utilizing content parameters ?.
Finally, theobserved task labels for the document are modeledgiven s and T using the task parameters ?.
Note thatthe arrows for the task labels are undirected sincethey are modeled discriminatively.where fN (?)
and fE(?)
are feature functions associ-ated with CRF nodes and transitions respectively.Allowing the CRF to condition on the sentencetopic Ti permits predictions to be more sensitive tocontent.
For instance, using the example from Ta-ble 1, we could have a feature that indicates the word?pleased?
conjoined with the segment topic (see Fig-ure 1).
These topic-specific features serve to disam-biguate word usage.This joint process, depicted graphically in Fig-ure 2, is summarized as:P (T , s,y?)
= P?
(T , s)P?
(y?|s,T ) (3)Note that this probability decomposes into adocument-level HMM term (the content component)as well as a product of CRF terms (the task compo-nent).3.3 LearningDuring learning, we would like to find thedocument-level HMM parameters ?
and the summa-rization task CRF parameters ?
which maximize thelikelihood of the labeled documents.
The only ob-served elements of a labeled document are the docu-ment text s and the aspect labels y?.
This objectiveis given by:LL(?, ?)
=?(s,y?
)?DLlogP (s,y?)=?(s,y?
)?DLlog?TP (T , s,y?
)We use the EM algorithm to optimize this objec-tive.E-Step The E-Step in EM requires computing theposterior distribution over latent variables.
In thismodel, the only latent variables are the sentence top-ics T .
To compute this term, we utilize the decom-position in Equation (3) and rearrange HMM andCRF terms to obtain:P (T , s,y?)
= P?
(T , s)P?
(y?|T , s)=(n?i=1P?(Ti|Ti?1)?w?siP?(w|Ti))?(n?i=1P?
(y?i |si, Ti))=n?i=1P?(Ti|Ti?1)?(?w?siP?(w|Ti)P?
(y?i |si, Ti))We note that this expression takes the same form asthe document-level HMM, except that in addition toemitting the words of a sentence, we also have anobservation associated with the sentence sequencelabeling.
We treat each P?
(y?i |si, Ti) as part of thenode potential associated with the document-levelHMM.
We utilize the Forward-Backward algorithmas one would with the document-level HMM in iso-lation, except that each node potential incorporatesthis CRF term.M-Step We perform separate M-Steps for contentand task parameters.
The M-Step for the content pa-rameters is identical to the document-level HMM380content model: topic emission and transition dis-tributions are updated with expected counts derivedfrom E-Step topic posteriors.The M-Step for the task parameters does not havea closed-form solution.
Recall that in the M-Step,we maximize the log probability of all random vari-ables given expectations of latent variables.
Usingthe decomposition in Equation (3), it is clear thatthe only component of the joint labeled documentprobability which relies upon the task parameters islogP?
(y?|s,T ).
Thus for the M-Step, it is sufficientto optimize the following with respect to ?
:ET |s,y?
logP?
(y?|s,T )=n?i=1ETi|si, y?ilogP?
(y?i |si, Ti)=n?i=1K?k=1P (Ti = k|si, y?i ) logP?
(y?i |si, Ti)The first equality follows from the decompositionof the task component into independent CRFs (seeEquation (2)).
Optimizing this objective is equiva-lent to a weighted version of the conditional likeli-hood objective used to train the CRF in isolation.
Anintuitive explanation of this process is that there aremultiple CRF instances, one for each possible hid-den topic T .
Each utilizes different content featuresto explain the sentence sequence labeling.
These in-stances are weighted according to the posterior overT obtained during the E-Step.
While this objectiveis non-convex due to the summation over T , we canstill optimize it using any gradient-based optimiza-tion solver; in our experiments, we used the LBFGSalgorithm (Liu et al, 1989).3.4 InferenceWe must predict a label sequence y for each sen-tence s of the document.
We assume a loss functionover a sequence labeling y and a proposed labelingy?, which decomposes as:L(y, y?)
=?jL(yj, y?j)where each position loss is sensitive to the kind oferror which is made.
Failing to extract a token ispenalized to a greater extent than extracting it withan incorrect label:L(yj, y?j) =????
?0 if y?j = yjc if yj 6= NONE and y?j = NONE1 otherwiseIn this definition, NONE represents the backgroundlabel which is reserved for tokens which do not cor-respond to labels of interest.
The constant c repre-sents a user-defined trade-off between precision andrecall errors.
For our multi-aspect summarizationtask, we select c = 4 for Yelp and c = 5 for Amazonto combat the high-precision bias typical of condi-tional likelihood models.At inference time, we select the single labelingwhich minimizes the expected loss with respect tomodel posterior over label sequences:y?
= miny?Ey|sL(y, y?
)= miny?
?j=1Eyj|sL(yj, y?j)In our case, we must marginalize out the sentencetopic T :P (yj|s) =?TP (yj, T |s)=?TP?
(T |s)P?
(yj|s, T )This minimum risk criterion has been widely used inNLP applications such as parsing (Goodman, 1999)and machine translation (DeNero et al, 2009).
Notethat the above formulation differs from the stan-dard CRF due to the latent topic variables.
Other-wise the inference task could be accomplished bydirectly obtaining posteriors over each yj state usingthe Forward-Backwards algorithm on the sentenceCRF.Finding y?
can be done efficiently.
First, we ob-tain marginal token posteriors as above.
Then, theexpected loss of a token prediction is computed asfollows: ?y?jP (yj|s)L(yj, y?j)Once we obtain expected losses of each token pre-diction, we compute the minimum risk sequence la-beling by running the Viterbi algorithm.
The po-tential for each position and prediction is given by381the negative expected loss.
The maximal scoring se-quence according to these potentials minimizes theexpected risk.3.5 Leveraging unannotated dataOur model allows us to incorporate unlabeled doc-uments, denoted DU , to improve the learning of thecontent model.
For an unlabeled document we onlyobserve the document text s and assume it is drawnfrom the same content model as our labeled docu-ments.
The objective presented in Section 3.3 as-sumed that all documents were labeled; here we sup-plement this objective by capturing the likelihoodof unlabeled documents according to the contentmodel:LU (?)
=?s?DUlogP?(s)=?s?DUlog?TP?
(s,T )Our overall objective function is to maximize thelikelihood of both our labeled and unlabeled data.This objective corresponds to:L(?, ?)
=LU (?)
+ LL(?, ?
)This objective can also be optimized using the EMalgorithm, where the E-Step for labeled and unla-beled documents is outlined above.3.6 GeneralizationThe approach outlined can be applied to a widerrange of task components.
For instance, in Sec-tion 4.1 we apply this approach to multi-aspect sen-timent analysis.
In this task, the target y consists ofnumeric sentiment ratings (y1, .
.
.
, yK) for each ofK aspects.
The task component consists of indepen-dent linear regression models for each aspect sen-timent rating.
For the content model, we associatea topic with each paragraph; T consists of assign-ments of topics to each document paragraph.The model structure still decomposes as in Fig-ure 2, but the details of learning are slightly differ-ent.
For instance, because the task label (aspect sen-timent ratings) is not localized to any region of thedocument, all content model variables influence thetarget response.
Conditioned on the target label, alltopic variables become correlated.
Thus when learn-ing, the E-Step requires computing a posterior overparagraph topic tuples T :P (T |y, s) ?
P (s,T )P (y|T , s)For the case of our multi-aspect sentiment task, thiscomputation can be done exactly by enumeratingT tuples, since the number of sentences and pos-sible topics is relatively small.
If summation is in-tractable, the posterior may be approximated usingvariational techniques (Bishop, 2006), which is ap-plicable to a broad range of potential applications.4 Experimental Set-UpWe apply our approach to two text analysis tasks thatstand to benefit from modeling content structure:multi-aspect sentiment analysis and multi-aspect re-view summarization.4.1 TasksIn the following section, we define each task in de-tail, explain the task-specific adaptation of the modeland describe the data sets used in the experiments.Table 2 summarizes statistics for all the data sets.For all tasks, when using a content model with atask model, we utilize a new set of features whichinclude all the original features as well as a copyof each feature conjoined with the content topic as-signment (see Figure 1).
We also include a fea-ture which indicates whether a given word was mostlikely emitted from the underlying topic or from abackground distribution.Multi-Aspect Sentiment Ranking The goal ofmulti-aspect sentiment classification is to predict aset of numeric ranks that reflects the user satisfactionfor each aspect (Snyder and Barzilay, 2007).
One ofthe challenges in this task is to attribute sentiment-bearing words to the aspects they describe.
Informa-tion about document structure has the potential togreatly reduce this ambiguity.Following standard sentiment ranking ap-proaches (Wilson et al, 2004; Pang and Lee, 2005;Goldberg and Zhu, 2006; Snyder and Barzilay,2007), we employ ordinary linear regression toindependently map bag-of-words representationsinto predicted aspect ranks.
In addition to com-monly used lexical features, this set is augmented382TaskLabeledUnlabeledAvg.
SizeTrain Test Words SentsMulti-aspect sentiment 600 65 ?
1,027 20.5Multi-aspect summarizationAmazon 35 24 12,684 214 11.7Yelp 48 48 33,015 178 11.2Table 2: This table summarizes the size of each corpus.
In each case, the unlabeled texts of both labeled andunlabeled documents are used for training the content model, while only the labeled training corpus is usedto train the task model.
Note that the entire data set for the multi-aspect sentiment analysis task is labeled.with content features as described above.
For thisapplication, we fix the number of HMM states to beequal to the predefined number of aspects.We test our sentiment ranker on a set of DVD re-views from the website IGN.com.5 Each review isaccompanied by 1-10 scale ratings in four categoriesthat assess the quality of a movie?s content, video,audio, and DVD extras.
In this data set, segmentscorresponding to each of the aspects are clearly de-lineated in each document.
Therefore, we can com-pare the performance of the algorithm using auto-matically induced content models against the goldstandard structural information.Multi-Aspect Review Summarization The goalof this task is to extract informative phrases thatidentify information relevant to several predefinedaspects of interest.
In other words, we would like oursystem to both extract important phrases (e.g., cheapfood) and label it with one of the given aspects (e.g.,value).
For concrete examples and lists of aspectsfor each data set, see Figures 3b and 3c.
Variants ofthis task have been considered in review summariza-tion in previous work (Kim and Hovy, 2006; Brana-van et al, 2009).This task has elements of both information extrac-tion and phrase-based summarization ?
the phraseswe wish to extract are broader in scope than in stan-dard template-driven IE, but at the same time, thetype of selected information is restricted to the de-fined aspects, similar to query-based summarization.The difficulty here is that phrase selection is highlycontext-dependent.
For instance, in TV reviews suchas in Figure 3b, the highlighted phrase ?easy to read?might refer to either the menu or the remote; broader5http://dvd.ign.com/index/reviews.htmlcontext is required for correct labeling.We evaluated our approach for this task on twodata sets: Amazon TV reviews (Figure 3b) and Yelprestaurant reviews (Figure 3c).
To eliminate noisyreviews, we only retain documents that have beenrated ?helpful?
by the users of the site; we also re-move reviews which are abnormally short or long.Each data set was manually annotated with aspectlabels using Mechanical Turk, which has been usedin previous work to annotate NLP data (Snow et al,2008).
Since we cannot select high-quality annota-tors directly, we included a control document whichhad been previously annotated by a native speakeramong the documents assigned to each annotator.The work of any annotator who exhibited low agree-ment on the control document annotation was ex-cluded from the corpus.
To test task annotationagreement, we use Cohen?s Kappa (Cohen, 1960).On the Amazon data set, two native speakers anno-tated a set of four documents.
The agreement be-tween the judges was 0.54.
On the Yelp data set, wesimply computed the agreement between all pairs ofreviewers who received the same control documents;the agreement was 0.49.4.2 Baseline Comparison and EvaluationBaselines For all the models, we obtain a baselinesystem by eliminating content features and only us-ing a task model with the set of features describedabove.
We also compare against a simplified vari-ant of our method wherein a content model is in-duced in isolation rather than learned jointly in thecontext of the underlying task.
In our experiments,we refer to the two methods as the No ContentModel (NoCM) and Independent Content Model(IndepCM) settings, respectively.
The Joint Content383M = MovieV = VideoA = AudioE = ExtrasM This collection certainly offers some nostalgicfun, but at the end of the day, the shows themselves,for the most part, just don't hold up.
(5)V Regardless, this is a fairly solid presentation, butit's obvious there was room for improvement.
(7)A Bass is still robust and powerful.
Fans should bepleased with this presentation.
(8)E The deleted scenes were quite lengthy, but onlyshelled out a few extra laughs.
(4)(a) Sample labeled text from the multi-aspect sentiment corpus[R Big multifunction remote] with [R easy-to-read keys].
The on-screen menu is [M easy touse] and you [M can rename the inputs] to oneof several options (DVD, Cable, etc.
).R = RemoteM = MenuI = InputsE = EconomyV = VideoS = SoundA = AppearanceF = FeaturesI bought this TV because the [V overall picturequality is good] and it's [A unbelievably thin].
[I Plenty of inputs], including [I 2 HDMI ports],which is [E unheard of in this price range].
(b) Sample labeled text from the Amazon multi-aspect summa-rization corpus[F All the ingredients are fresh], [V the sizes arehuge] and [V the price is cheap].F = FoodA = AtmosphereV = ValueS = ServiceO = Overall[O This place rocks!]
[V Pricey, but worth it] .
[A The place is a pretty good size] and[S the staff is super friendly].
(c) Sample labeled text from the Yelp multi-aspect summarizationcorpusFigure 3: Excerpts from the three corpora with thecorresponding labels.
Note that sentences from themulti-aspect summarization corpora generally focuson only one or two aspects.
The multi-aspect senti-ment corpus has labels per paragraph rather than persentence.Model (JointCM) setting refers to our full model de-scribed in Section 3, where content and task compo-nents are learned jointly.Evaluation Metrics For multi-aspect sentimentranking, we report the average L2 (squared differ-ence) and L1 (absolute difference) between systemprediction and true 1-10 sentiment rating across testdocuments and aspects.For the multi-aspect summarization task, we mea-sure average token precision and recall of the labelassignments (Multi-label).
For the Amazon corpus,we also report a coarser metric which measures ex-traction precision and recall while ignoring labels(Binary labels) as well as ROUGE (Lin, 2004).
Tocompute ROUGE, we control for length by limitingL1 L2NoCM 1.37 3.15IndepCM 1.28?
* 2.80?
*JointCM 1.25?
2.65?
*Gold 1.18?
* 2.48?
*Table 3: The error rate on the multi-aspect sentimentranking.
We report mean L1 and L2 between systemprediction and true values over all aspects.
Markedresults are statistically significant with p < 0.05: *over the previous model and ?
over NoCM.F1 F2 Prec.
RecallNoCM 28.8% 34.8% 22.4% 40.3%IndepCM 37.9% 43.7% 31.1%?
* 48.6%?
*JointCM 39.2% 44.4% 32.9%?
* 48.6%?Table 4: Results for multi-aspect summarization onthe Yelp corpus.
Marked precision and recall arestatistically significant with p < 0.05: * over theprevious model and ?
over NoCM.each system to predict the same number of tokens asthe original labeled document.Our metrics of statistical significance vary bytask.
For the sentiment task, we use Student?s t-test.
For the multi-aspect summarization task, weperform chi-square analysis on the ROUGE scoresas well as on precision and recall separately, asis commonly done in information extraction (Fre-itag, 2004; Weeds et al, 2004; Finkel and Manning,2009).5 ResultsIn this section, we present the results of the methodson the tasks described above (see Tables 3, 4, and 5).Baseline Comparisons Adding a content modelsignificantly outperforms the NoCM baseline onboth tasks.
The highest F1 error reduction ?
14.7%?
is achieved on multi-aspect summarization on theYelp corpus, followed by the reduction of 11.5% and8.75%, on multi-aspect summarization on the Ama-zon corpus and multi-aspect sentiment ranking, re-spectively.We also observe a consistent performance boostwhen comparing against the IndepCM baseline.This result confirms our hypothesis about the ad-384Multi-label Binary labelsF1 F2 Prec.
Recall F1 F2 Prec.
Recall ROUGENoCM 18.9% 18.0% 20.4% 17.5% 35.1% 33.6% 38.1% 32.6% 43.8%IndepCM 24.5% 23.8% 25.8%?
* 23.3%?
* 43.0% 41.8% 45.3%?
* 40.9%?
* 47.4%?
*JointCM 28.2% 31.3% 24.3%?
33.7%?
* 47.8% 53.0% 41.2%?
57.1%?
* 47.6%?
*Table 5: Results for multi-aspect summarization on the Amazon corpus.
Marked ROUGE, precision, andrecall are statistically significant with p < 0.05: * over the previous model and ?
over NoCM.vantages of jointly learning the content model in thecontext of the underlying task.Comparison with additional context featuresOne alternative to an explicit content model is tosimply incorporate additional features into NoCMas a proxy for contextual information.
In themulti-aspect summarization case, this can be accom-plished by adding unigram features from the sen-tences before and after the current one.6When testing this approach, however, the perfor-mance of NoCM actually decreases on both Ama-zon (to 15.0% F1) and Yelp (to 24.5% F1) corpora.This result is not surprising for this particular task ?by adding these features, we substantially increasethe feature space without increasing the amount oftraining data.
An advantage of our approach isthat our learned representation of context is coarse,and we can leverage large quantities of unannotatedtraining data.Impact of content model quality on task per-formance In the multi-aspect sentiment rankingtask, we have access to gold standard document-level content structure annotation.
This affords usthe ability to compare the ideal content structure,provided by the document authors, with one that islearned automatically.
As Table 3 shows, the manu-ally created document structure segmentation yieldsthe best results.
However, the performance of ourJointCM model is not far behind the gold standardcontent structure.The quality of the induced content model is de-termined by the amount of training data.
As Fig-ure 4 shows, the multi-aspect summarizer improveswith the increase in the size of raw data available forlearning content model.6This type of feature is not applicable to our multi-aspectsentiment ranking task, as we already use unigram features fromthe entire document.1020300% 50% 100%Multi-labelF 1Percentage of unlabeled data22.8 26.028.2Figure 4: Results on the Amazon corpus using thecomplete annotated set with varying amounts of ad-ditional unlabeled data.7Compensating for annotation sparsity We hy-pothesize that by incorporating rich contextual in-formation, we can reduce the need for manual taskannotation.
We test this by reducing the amount ofannotated data available to the model and measur-ing performance at several quantities of unannotateddata.
As Figure 5 shows, the performance increaseachieved by doubling the amount of annotated datacan also be achieved by adding only 12.5% of theunlabeled data.6 ConclusionIn this paper, we demonstrate the benefits of incor-porating content models in text analysis tasks.
Wealso introduce a framework to allow the joint learn-ing of an unsupervised latent content model with asupervised task-specific model.
On multiple tasksand datasets, our results empirically connect modelquality and task performance, suggesting that fur-7Because we append the unlabeled versions of the labeleddata to the unlabeled set, even with 0% additional unlabeleddata, there is a small data set to train the content model.3851012320% 1352% 32%Multi-labelF 1Percentage of unlabeled data302.82608 66828Figure 5: Results on the Amazon corpus using halfof the annotated training documents.
The contentmodel is trained with 0%, 12.5%, and 25% of addi-tional unlabeled data.7 The dashed horizontal linerepresents NoCM with the complete annotated set.ther improvements in content modeling may yieldeven further gains.AcknowledgmentsThe authors acknowledge the support of the NSF(CAREER grant IIS-0448168) and NIH (grant 5-R01-LM009723-02).
Thanks to Peter Szolovits andthe MIT NLP group for their helpful comments.Any opinions, findings, conclusions, or recommen-dations expressed in this paper are those of the au-thors, and do not necessarily reflect the views of thefunding organizations.ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofthe NAACL/HLT, pages 113?120.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning (Information Science and Statis-tics).
Springer-Verlag New York, Inc.David M. Blei and Jon D. McAullife.
2007.
SupervisedTopic Models.
In NIPS.S.
R. K. Branavan, Harr Chen, Jacob Eisenstein, andRegina Barzilay.
2009.
Learning document-level se-mantic properties from free-text annotations.
JAIR,34:569?603.Harr Chen, S. R. K. Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Content modeling using la-tent permutations.
JAIR, 36:129?163.Yejin Choi and Claire Cardie.
2008.
Learning with com-positional semantics as structural inference for sub-sentential sentiment analysis.
In Proceedings of theEMNLP, pages 793?801.J.
Cohen.
1960.
A Coefficient of Agreement for NominalScales.
Educational and Psychological Measurement,20(1):37.John DeNero, David Chiang, and Kevin Knight.
2009.Fast consensus decoding over translation forests.
InProceedings of the ACL/IJCNLP, pages 567?575.Micha Elsner, Joseph Austerweil, and Eugene Charniak.2007.
A unified local and global model for discoursecoherence.
In Proceedings of the NAACL/HLT, pages436?443.Jenny Rose Finkel and Christopher D. Manning.
2009.Joint parsing and named entity recognition.
In Pro-ceedings of the NAACL.Dayne Freitag.
2004.
Trained named entity recogni-tion using distributional clusters.
In Proceedings ofthe EMNLP, pages 262?269.Andrew B. Goldberg and Xiaojin Zhu.
2006.
See-ing stars when there aren?t many stars: Graph-basedsemi-supervised learning for sentiment categoriza-tion.
In Proceedings of the NAACL/HLT Workshop onTextGraphs, pages 45?52.Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25(4):573?605.Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InProceedings of the NAACL/HLT, pages 362?370.Soo-Min Kim and Eduard Hovy.
2006.
Automatic iden-tification of pro and con reasons in online reviews.
InProceedings of the COLING/ACL, pages 483?490.Chin-Yew Lin.
2004.
ROUGE: A package for automaticevaluation of summaries.
In Proceedings of the ACL,pages 74?81.Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-cedal.
1989.
On the limited memory bfgs method forlarge scale optimization.
Mathematical Programming,45:503?528.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
In Proceedings ofthe ACL, pages 432?439.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the ACL,pages 271?278.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the ACL,pages 115?124.386Siddharth Patwardhan and Ellen Riloff.
2007.
Effec-tive information extraction with semantic affinity pat-terns and relevant regions.
In Proceedings of theEMNLP/CoNLL, pages 717?727.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast - but is it good?evaluating non-expert annotations for natural languagetasks.
In Proceedings of the EMNLP.Benjamin Snyder and Regina Barzilay.
2007.
Multipleaspect ranking using the good grief algorithm.
In Pro-ceedings of the NAACL/HLT, pages 300?307.Swapna Somasundaran, Galileo Namata, Janyce Wiebe,and Lise Getoor.
2009.
Supervised and unsupervisedmethods in employing discourse relations for improv-ing opinion polarity classification.
In Proceedings ofthe EMNLP, pages 170?179.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributional simi-larity.
In Proceedings of the COLING, page 1015.Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004.Just how mad are you?
finding strong and weak opin-ion clauses.
In Proceedings of the AAAI, pages 761?769.387
