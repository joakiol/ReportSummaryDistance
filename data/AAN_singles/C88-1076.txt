On the Rble of Old Information in Generating Readable Text:A Psychological nd Computational DefinitionOf "Old" and "New" Information in the NOSVO SystemMark Vincent LaPollaLinguistics Dept,University of Texas, at AustinAustin, Tx 78712CS.LAPOLLA@R20.UTEXAS.EDUNOSVO is a Natural Language Generation postproeessor which is sensitive to oldhi~winformation contrasts.
We believe that generating old information first establishescohesion in text promoting readability.
This paper describes the NOSVO system in detailand the motivations for building it.
We also provide aphychological nd computationaldefinition of "old" and "new" information.Tbere.are situations where the speaker is constrained by a grammaticalrule, and there are situations where he chooses according to Iris meaning ... ;but there are fie situations in the system where "it makes no difference"which way you go.
This is just another way of saying that every contrast alanguage permits to survive is relevant, some time or other.
(Bolinger1972:71).1.0 IntroductionThere are at least two stages of text generation.
One isgenerating the content of the text.
The other is generatingthe language that represents and communicates the content(Thompson 1977).
These two stages, though interrelated,have their owta sets of interesting problems and principles.The first stage, generating the semantic ontent of the text,involves motivating, planning and creating the conceptualand semantic ontent of a piece of text.
Once the semanticrepresentation for a text has been constructed the languageof that text can be generated.
The second stage, languagegeneration, involves communicating the intent and contentof the text Without confusing or misleading the reader.
"~hispaper will address the second stage only.It is not enough to merely generate text.
It is alsonecessary to generate cohesive text.
However a shoppinglist is cohesive, though not "flowing" text by any means.
Aset of sentences that are prepositionally related are cohesivethough are not necessarily beautiful prose.It is not enough to attend to ellipsis andprouominalization to generate r adable prose.
We believethat there are other factors which must be attended to togenerate prose.
The NOSVO system is an attempt to takeinto account old/new information contrasts (Chafe 1974,1976) which we believe will help natural languagegeneration systems produce more readable text.
'assumable' as being there" (Prince 19'/8:819)i This isquite important and expands upon Ch~'e :.
;ir, ce fbr him theimportant hing is that the antecede~t mu~t be in thehearer's consciousness, i.e.
i-~ the l~earer, s tbcus ofattention, while for Prince and LaPolla it need only beappropriate to the situation or in some other waycoCperatively assumable, to ho in the fiearer'sconsciousness.Hajicov~ and Vbrov~t (19811 also takes exception withthe terms "given (or old) " or "new" information andsuggests that "contextuall!?
bound" and "contextuallynon-bound" lexical item would be more appropriate.
"contextually bound" and "contextually non-bound" iseven more appropriate han "already activated" ~md "newlyactivated" because it seems to also convey situationalappropriateness.
However, it seems that Hajicov~t restrictsher terminology, as well as her theory of discourse (focus)strueture~ to linguistic antecedents.
That is, her "sharedstock of knowledge" appears to be closer to, if notcompletely, linguistic in representation.
Thet~f0re, neitherher theory or terminology has the power to deal with mtantecedent that is merely inferable or appropriate to asituation.
We will use the familiar terms "new/oldinfolmation" but will define them a little more preciselylater in the paper.3.0 The System2?0 "Old" and "New"It is important at this point to clarify the use of the terms"old information" and "new information".
The term "oldinformation" is a misnomer, though it expresges the Jintuition needed for this paper.
The term suggests "it iswhat the listener is expected to know already" (Havilandand Clark 1974).
The term "new information" is also aincorrect.
It suggests that what the speaker has uttered iscompletely new to the hearer and is being introduced intothe speaker's consciousness for the very first time.
But asCh~ife (1976) points out a person uttering the sentence "Isaw you father yesterday" is "unlikely to assume that theaddressee had no previous knowledge of his father, eventhough~by the usual criteria "your father" would beconsidered new information", (Chafe 1976.- 30).
Chafe" y" etivated" and "newly suggests that the terms ahead aactivated" would be more anvronriate.Certainly "alreaay actlvatect" and "newly activated"(information) are by far better than the terms "~iven" and"new".
Even so, they are still somewhat mprecise.
"already activated" and "newly activated" imply, as doesChafe, that the concept that is activated, whetherextralinguistic or linguistic, is directly activated by a(linguistic o1" extralinguistic) referent.
As Prince (1978)shows for clefts and as LaPolla (19861 shows forinversion and prepositional phrase fronting this need not bethe case.
Rather the antecedent "simply has to beappropriate to the situation, and hence co6PerativelyN(ISVO (Not Only Subject Verb Object) is apreprocessor that ,aids in the genei:ation of English text.
Itis not in and of itself" a text generator.Rut does contain avery simple predicate to EngliSh translator (simihtr in sphit,though not in complexity, to Simmons 19841.
NOSVO issensitive to the old/flew (i.e.
given/new) contrasts in adiscourse (Chafe 1974, 1976; ttajicov~ and Vbrov~ 19811and file syntactic structures that allow a writer, or sI~aker,to begin a sentence with old information, NOSVOorganizes the semantic ontent of pirxlicates to produce anold information first ordering.
(Appendix A contains ashort example of test with and without he application ofthe old information first principle.
)The rest of this section describes NOSVO and itsmotivation i  detail.
(See Appendi?
B tbr a higll level dataflow diagram of rite system.
)~.1/MotivationOne of the.
goals of communication is to modify andextend the store of information i  memory (Hajicov~i ~mdvbrowt 19811 and language facilitates eormnunicatlon.
Ifthe above statement isaccepted then one might ehmaeterizea discourse as a session where particular slices of memoryare accessed and ehauged~ Since this actz~nulated store Ofinformation i  memory is presumably very large and has acomplicated Structure, it i~ nsefill ff the commmricator canfirst identify tile locations in metuory that are to bemodified or added to before inu'odncing rite modifications372or Mdifions (\[lajicov,5 and Vbmv~ 1981; LaPolla 1986).Begbming a sentence with old information allows aspeakeJ to direct he hearcr's focus of attention to exactlythose elements which he wishes to modify before he uttersthe.rest of the sentence, presamatfly new information(Chaff; 1974).
If a speaker began a sentence with newinformation, the listener woukl not know where to connectit in the discourse.
The listener wonkl have to wait uhtil theold inlbrmation is uttered to locate where the newinformation i s to  be used (LaPolla 1986).
The formerprocess, takes less concentration by the listet,er (Green1980),If we model human mcm(ny, specifically lexical access,as a sp:,:cading-.actiwttion network (Ouillian 1962, 1967;Collins and Loftus 1975) we can describe the effects ofuttering old information before new as p!
'imiag, for at leastthe class of linguistic anWcedents.
Old inflmnation primesan already active node in memory raising its level ofactivation'and making it more accessible than surroundingnodes.
The priming may then spread to related concepts.This causes only the relevant and related portions ofmemory to be available for modification or replacement,and restricts the amount of memory brought into tireprocessing of discourse.
(Our model can bc extended tocover nc,Minguistic and inferable mttecedeuts by using alaybrill representation (Vihdn 1985) \[cf.
Haviland andClai'k~?
974) or Prince (1978) fbr .a definition of inferableanteced~.nts\].)
If new infommtkm were nttered first thenthe level of activation achieved would be equal ,.,r close topreviously primed concepts and the speaker's attentionwould not be properly focused.We can now define old (linguistic) information asalready activated memory.
New (linguistic) information,then, is either information ot in memory or informationthat ha:~ not yet been actNatcd.
Using Quilliau's model, allconcepts that haw; bern, primed by the discourse, eitherdirectly or by spreading-activation, arc old.
Everything elseis uew.Structures like inversion and PIM?-onting, to a name afew, extend the syntactic amt logical possibilities ofinflmnation presentation i  a language.
They allowadjnnct~, e.g.
in PP-fronting~ or objects, e.g.
in passives,or other arguments, e.g.
in inversion, to be presentedbefore other syntactic onstituents.To complete the above definitions we will adopt andexpand Quillian's definidon of "concept".
For QuiUian,and Collins and Loftus, a concept correspond:; "toparticulm" senses of words or phra~,',es" (Collins and l~xfftus1975: 408).
We will expand this to include extralinguisticphrases c,r groups of actions and objects.
A concept then isnot onl 7 senses or words or phrases such as NPs,"machine", VPs, "to machine", and phrases "the pro-titularold car I own" (page 408) but also extralinguistic objects: atficture of a fire engine, and actions, the picture of a fireengine racing down the slreet.
This also includes ituationssuch as eating in a restaurant and paying the check.
Insum, concepts arc linguistic words and phrases as well asmore difficult hings to pin down like situational scripts.We have adopted a well proven model of truman memoryto describe the elf~-ets of linguistic infbtmation on memory.We have used this thex~ry to wodel discourse processing asa type o:i!
memo~ y l)rocessing.
We tuwe also advanced theidea that langnage in its role as a communications facilitatorallows ~a speaker to direct which concepts will be primedand therefore what the hearer will loons his attcnlion on.We have defined old and new inibmmtion within thisflamework and have b, icfly described the contributioninversion, PP-fronting amt passivization make to thepreseutafio, f information iu hmguage.
In the rest of thispaper we will show how we have integrated these ideasand theories into NOSVO.3.2 Syste~l OverviewNOSVO takes as input a syntacfico-semantic predicatercprescmatRm of each sentence in the text to be produced(following Simmons 1984).
(Sec Figure 1 lor an example.The asterisks indicates a backward pointing arc (Simmons1984)0 It determines which of ~ltc predicates constituentsare "old" inlbnnafion.
It thtu~ updates its lexical memory toreflect the predicates nttecan,.
:e on the listener's ownmemory.
The lexicon is a semantic network followingQuillian.
Currently it supports the relations ISA, HAS,HAS-PART, LOC, IS-CAI,LED, EXAMPLE, SUPER-and SUBCLASS.NOSVO assumes that each of the underlying predicatesmaps to a sentence.
This assumption has allowed us tofocus on the oldh~ew information distinction that is the coreof the NOSVO system withont worrying about themapping from "deep" semantic structure to surfacestructure.
In other words, NOSVO is not a robust Englishgenerator and has all of its intelligence devoted to themanipulation of information.
We feel that languagegeneration is a difficult research issue and is beyond thescope of this paper mtd the current system.
(spend tns past ae ((father number sing)agt* (accompany ms pres infl int)ae (me number sing))before* ((be ms past nmnber sing)atr (tall) atr (enongl0agt ((I nmnber sing)agt* ((ride ins pres iufl inf)on* ((coaster number sing)alr (bi~9)ae (me nmnber sing))))agt ((hour number pl) atr (pleasant) atr (many)))Figure 13o2,1 NOSVOFirst, NOSVO determines which parts of the inputpredicate are "old information".
It checks the nodes withthe tive (5) highest levels of activation in its knowledgebase (KB), e.g.
levels 6-10 where 0 is not activated and 10is fully activated.
If any of the heads of any of thearguments from the input predicate is among the activatednodes then that argument is marked as "old".
(NB: We donot address in this paper intenml ordering of constituentsother than sentences.
We only check the heads ofconsfitnents directly below the sentence l vel.
)After NOSVO has marked the old information in theinput predicate, it updates the stalus of its KB to reflect hehypothesized change the generated sentence will have onthe listener.
To do this, NOSVO first parses each inputpredicate into its constituents.
For each argument in theinput predicate NOSVO primes, or reprimes, acorresponding (concept) node in its KBs.
(Note: Thoughwe do not check each part of each constituent whendetermining what is old, every part of every constitueutdoes affect he state of the lexical rnemory of NOSVO.
)When a node in the knowledge base is primed, it istagged with a level of activation The initially primedconcept is tagged with the highest level of activation, call itlevel 10.
The initially primed concept is also tagged as theinitially primed node, i.e.
the node tMmed by the discourseand not by spreading-activation.
Activation then spreadsoutward raising the level of activation of sun-oundingnodes.
As the spreading-activation gets timber away fromthe initially primed node its effect is reduced proportionallyto the distance mtvclcd (following Collins and Loftus1975).
For example, at the initially primed node the levelof activation is 10, at the next node it is 9, at the next 8 andso on.
NOSVO also tags the surrounding nodes with theumnc of the initially primed node.
We realize that we donot know by what exact proportion the activation effect isdiminished.
Nor do we know how long actiwttion lasts orat what rate is deteriorates These are questions for futureresearch.3.2?2 Generating SentencesAfter NOS?O has marked the arguments in thepredicate, the result is passed to a simple Englishgenerator.
The role of the generator iscomplex but the wayit executes its role is simple.
The generator looks at thenuu'ked predicate and chooses the correct English syntax tomap the predicate to English.
We realize that this task isvery complex and our treatment of it is superficial.
Werealize that entire systems have been created to address theproblem, e.g.
MUMBLE (McDonald 1980; see alsoMcDonald, Meteer and Pustejovsky 1987).
We "also realizethat we do not address pragmatic onsiderations in the373generation of discourse (tfovy 1987).
However, recall thatour goal was not to create arobust language generator.
Our?
goal was to create a system that could recognize old!ll.fo~rnation in a phrase being generated.
After this is doneit is the responsibility of the rest of the system to act on thatinfolmation.We assumed fl'om the beginning that the underlyingsemantic ~epresentations NOSVO processes are alreadyorganized into sentences.
This was not to aid NOSVO inits task~ though it does by explicitly defining therelationship between the verb and its arguments, but ratherto aid the simple predicate to English generator.The English generator coupled with NOSVO takesNOSVO's output and analyzes it.
If a prepositional phrase(PP) adjunct has been marked as "old" then the generatorfronts it, e.g.
"In the park, John kissed Mary".
If an objecthas been marked as "old" the generator generates apassive, e.g.
"The apples were eaten by Vincent".
If a ppargument or adjunct of an intransitive is marked as "old"the generator generates an inverted sentence, .g.
"Aroundtile bend lives John".
If no old information is found otherthan in the subject hen a simple Subject Object Verbsentence is generated, e.g.
"Vinnie loves Mark".
If apredicate contains no old information, either explicitly orimplicitly, it is a nonsequetur mid should not be geuerated.At -this stage in the generators development only simplesyntaxes are generated.
Extraposition out of clauses is notaddressed.
This was not our intent.
Also it was not ourintent o argue here that he presentation f old informationfirst is the sole discourse function of structures likeinversion, pp-.fronting and passives but only one of theirdiscourse functions, perhaps the main one.
It was ourintent o build a system that could determine which parts ofa sentence tinder generation were old information.
It wasalso our intent o elm'ify the terms old and new infolxnationand to put their definitions in perspective both linguisticallyand psychologically.
These issues we have addressed.4.0 The System In DetailIn this section, we will present the NOSVO system indetail.4ot Detailed OverviewNOSVO's grammar is segmented into two parts: i) plainvanilla SVO rules, e.g.
S -> NP verb PP, "A little angelstood outside" (Green 1980, page 582), "Uncle Jack livesaround the bend" (LaPolla forth coming), and it) so calledold information first syntax, e.g.
S -> PP verb NP,"Outside stood a little angel" (Green 1980, page 582),"Around the bend lives Uncle Jack" (LaPolla 1986 and1988).
(Cf Green 1980 and LaPolla i986 for details).When NOSVO encounters old information in anyconstituent in a predicate, xcept for the logical subject, ituses the old information first syntax grammar to generatethe seutence.
Otherwise it uses the plain vanilla granunar.NOSVO does not do any extraposition from withinembedded clauses nor does it handle the differencesbetween internal arguments and adjuncts.
It only producesthe three variations on standard, plain vanilla syntaxdiscussed above.
These issues will be addressed in futureversions of NOS VO.NOSVO has two Idnds of knowledge bases from whichto Work: linguistic and conceptual.
There are two linguisticknowledge bases: the lexicon and the discourse base.
Thelexicon maps into either a domain specific or.a non-domainspecific KB.
Both the domain specific and non-domainspecific KBs are hierarchical networks which support herelations ISA, HAS, HAS-PART, LOC, IS-CALLED,EXAMPLE, SUPER- and SUBCLASS.
The domainspecific and non-domain specific knowledge bases are theQuillian .style semantic networks.
In NOSVO's currentavatar, there is a one to one mapping of lexical items toconcepts in .the knowledge bases.
That is, there is nolexical or conceptual ambiguity.
A more robust systemwould allow for multiple mappings in both directionbecause of the power and depth it gives.
A more robustmapping wonld produce two problems though.
The firstwould be the extra computation and heuristics required toresolve the ambiguities.
The second problem would bedetermining when to prime a node.
It might not be correct374to prime a node just because the lexicon accessed it.
Ouemight have to wait for a completed parse before primingthe concept bases.The discourse base is a tree.
As NOSVO generates text,it builds the discourse tree connecting old int\~rmation tonew while retaining the autonamy of each predicate.
Thediscourse base contains the structure of the discourse andis a way to record prinrinl~o The discourse base mapsdirectly into the KBs as well.
One could assume that thediscourse is just a a section of prirnexl memoly, ltowever,it was felt that a more linguistic representation would beusefld in helping to resolve anaphora.
Tile discourse basewas modeled after the discourse mechanism in LaPolla(1986).NOSVO's first step in determining whether a predicateor an argument is "old information" i~?
whether or not it hasbeen introduced into the discourse, that is whether or not itis definite (tleim 1982,1983).
If a referent has alreadybeen introduced into the discourse then it is necessarily oMinformation.
However, the converse is not necessarilytrue.
That is, just because a referent has not beenintroduced into the discourse does not mean it is 'newintbrmation' (Von Stechow 1981).
All it means is that thereferent has not been introduced.
The information mayhave been.The nonliuguistic KBs contain metNinguistic knowledgeabout articles, stories or other appropriate formats and tileexpectations speakers have about hem, knowledge aboutthe topic of the text and specific and generN knowlexlgeabout he lexicon?NOSVO tlies to establish a link from the sentence undergeneration to one of the KBs.
If a link can be found froman argument or an adjunct in the input predicate to theknowledge base, the link is recorded and the oldinformatkm first grammar is used to generate the argumentor adjnnct first, if possible.
(This is a oversimplificationand will be expanded tq~on.
)If NOSVO can not establish a link to the knowledgebase, it searches the meta-knowledge base, i.e.
theknowledge base containing information about authormotivation for writing an article or story, the techniquesauthors use to write articles or sto~ies, and iufonnationabout artMes and stories, theh' parts and subcategories.The recta--knowledge base is primarily used to establishbridges (Clark and Haviland 1974) discourse initially fromthe old information in the predicate to an infeITablemetalinguistic antecedem.
For example, acollege professormay begin a leetm-e (or course) with the discourse initialutterance "What we're going to look at today (this term)iso.." but not "*,What one of my colleagues aid thismorning was..." (Prince 1978, page 889) or "*What I toldmy wife this morning was..." (The asterix meanssemantically nnaceeptable) The first sentence is allowablebecause the context, i.e.
the class room setting, allows adirect inference to studying (for the terns).The recta--knowledge base has two pa~s: a taxonomysimilar to the Domain Specific KB and scripts that haveknowledge about objects and the actions that they perform,e.g.
writers write stories, writers set scenes, stories havescenes.The algorithm which NOSVO uses to detem~ine whichold information first syntax is appropriate isstraightforward.
The complicated part is how NOSVOdecides what is old information.
Currently NOSVOsearches throughall comcepts with activation greater than5.
The value 5 is arbitxary, however, it is still an openresearch question when an antecedent can be considered nolonger in the speaker's/listener's common ground (Chafe1974, 1976), or no longer cooperatively assumable (Prince1978).4.2 'l'he Components of the SystemThis subsection will outline in detail the variouscomponents ofNOSVO and their function.
This subsectionis organized in parallel with the data flow diagram inAppendix B, starting with NOSVOs first componentsubsystem.4.2.1 The Predicate ParserThe Predicate Parser identifies and parses the inputseutential predicate into its component parts.
This is thefirst :;tay, c in identifying old information i a predicate.4.2?2 e'rcdicate Argn~aent TranslatorThe Predicate Argmnent Translator translates thelingnisttc representation f the input predicate consituentsinto tokens, from the lexicon, which map into thediscourse base and the other KBs.
Notice that thereprese~ttation of discourse referents and concepts need notbe the same, only that each referent or concept be indexed,and indexable, by a token.
The tokens are only used toqnery the knowledge bases.
When we speak of finding alink bel ween the input predicate and the knowledge basethat li~}.
: is established through the conceptual translation.4,2?3 The Discom?se Ba~e SearcherThe Oiseourse Base Searcher searches the discourse todetermine whether any of the input predicate arguments inthe predicate have been previously introduced into thediscour.;e.
If an antecedent(s) i  found the link is recordedand the whole predicate, with highlighted old infonaaation,is sent to the LiHguistic Converter and Category Analyzer(l,C~CA).4?2?4 The Domain Specific KB SearcherIf no antecedents are found in the discourse base theDomail, Specific KB Searcher searches the domain specificKB for a possible lin k.First maximally primed nodes are invest gated, i.e.,nodes with priming 10.
Then other less palmed nodes areinvestigated and so on up to priming level 5.
Note: that theamount of search necessary increases as the primingdecreases.
If a link is found to a node, that node is primedand the input predicate is sent to the Linguistic Converterand Category Analyzer with the old informationhighlighted.
If not, control and the input predicate ispassed to the Non-Domain Specific KB Searcher.4.2.5 The Non-Donmin Specific KB SearcherThe Non-.Domain Specific KB Searcher searches for anantecedent in the Non-Domain Specific KB.
In NOSVOscase non-domain specific knowledge is general andprototypical knowledge.
So, for example, if the domainspecific KB is Navy ships, then the non-domain specificKB might contain information about ships in general,water, vehicles, transportation, or guns and fighting ingeneral.
The exact same mechanism is used to search theNon-Domain Specific KB as the Domain Specific KB.
Ifno antecedent is found in this knowledge base the predicateis passed to the Bridge Building Inference Engine.4.206 '~'he Bridge Bttilding Inference EngineIf all the other processes have failed to find a link frompredicate to cornmon gromld, i.e.
the context, bothlinguistic and nonlinguistic, of a discourse, NOSVO triesto build a bridge, an inference, which connects informationin the predicate to a metalinguistically inferable antecedent.Tints component of NOSVO is not very robust.
NOSVOwill eventually be reimplemented using a Valain (1985)type hybred approach.
Then the Bridge Building InferenceEngine will be expanded.At this point, the careful reader may think that given thenature of NOSVO's search mechanisms that it must alwayssucceed in establishing a connection from input toknowledge base.
That is the case.
Indeed it must be thecase.
Consider lhat everything that people say to each othermust in some way link to'.lhe common ground in order tobe nnderstood.
Or else the utterance would be anonsequitur.
Even the selfinlroductions perfomaed by twopeople who do not kiroW each other, and have just met, areexpecled and reasonable?
The formula has a metalinguisticantecedent i  culture.The question tot NOSVO is not whether an antecedentexists but rather what it is.
If NOSVO cannot find anantecedent it assumes that one exists and generates asentence with plan vanilla, SVO structure, leaving it to thereader to establish the connection.
If NOSVO did notassume an antecedent it would have to discard the sentenceas a nonsequitur and potentially confusing and/ormisleading.
This is an important issue and will be dealtwith in a later and expanded version of NOSVO.4.2.7 The LC-CAThe Linguistic Converter and Category Analyzeranalyzes the old information to determine itssyntactico-semantic category.
It checks whether it is aprepositional phrase, agent, theme or instrument, in thatorder.
It then decides if the old infomaation is an internal orexternal argument or a prepositional phrase adjunct.
Withthis informatiion it picks the type of grammar that willplace the particular argument or adjunct first and sends thechoice along with the predicate to the English generator.4.2.8 The English generatorThe English generator is a prolog grammar segmentedinto the various old information first syntaxes, e.g.prepositional phrase first, object first rules, and a plainvanilla syntax.
At this point all, or most, of the intelligentwork has been done and the generator is nothing more thana syntactic manipulator under the direction of the LinguisticCo,wetter and Category Analyzer.5.0 Future Research and DirectionsFuture research and development topics for NOSVOinclude:1. determining when information cannot beassumed to be in the listener's commonground, i.e., at what level of priming is aconcept not in the listeners common ground?;2.
expanding NOSVO's capability to handleellipsis, definiteness, and pronominalizationand investigate how the generation of ellipsisan.d definiteness affects the generation of oldinformation fin:st;3. extending NOSVO to do more of thelinguistic generation from either a more"conceptual" representation r to take as inputanother source langauge such as anothernatural language or a computer program andgenerate English from that underlyingrepresentation, i.e.
expand NOSVO'sbackend;4. extending NOSVO's capabilities to handlethe subtle distinction between arguments andadjuncts;5. determining how much the nonapplicationor missapplication f the old infolxnation firstprinciple, discussed above, makes a differencein reading and understanding text;6. finally, investigating other old informationfirst syntactic structures and phenomena todetermine how they affect a discourse and howthey might be integrated into NOSVO.The next generation ofNOSVO will be written in CLOSand Lisp.
The application will be "generating descriptionsof Lisp programs".
CLOS objects will be used to organizethe knowledge structures and CLOS methods will be usedto do tile actual parsing.
Eventually NOSVO will beexpanded and refined along the directions stated above.AcknowledgementsI would like to thank Kent Bimson, Mirjam Fried,Randy LaPolla, Marie Meteor and Varda Shaked for alltheir help and criticism on this abstract.
Any mistakes arcmy own.375ReferencesBolinger, D. That's that.
The Hague: Mouton, 1972.Chafe, Wallace.
Language and Consciousness.
Language,1974, 50(1), 111-133.Chafe, Wallace.
"Givenness, Contrastiveness,Definiteness, Subjects, Topics and Point of View".
InCharles Li (Ed.
), Subject and Topic.
(ed Li, C. N.),New York: Academic Press, 1976.Collins, Alan M. and Elizabeth F. Loftus (1975) "ASpreading-Activation Theory of Semantic Processing".Psychological Review, 87,407-428.Davison, Alice.
"Peculiar Passives".
Language, March1980, 56(1), 42-66.Green, Georgia M. "Some Wherefores of EnglishInversions".
Language, 1980, 56(3), 582-602.ttajicov~t, Eva and Jarka VrbovL "On the Salience of theElements of the Stock of Shared Knowledge".
FoliaLinguistica, 1981, 15, 291-303.Haviland, Susan E, and Herbert Clark.
"What's New?Acquiring New Information as a Process inComprehension".
Journal of Verbal Learning and VerbalBehavior, 1974, 13, 512-538.Heim, Irene R. The Semantics of Definite and IndefiniteNoun Phrases, Doctoral dissertation, University ofMassachusetts at Amherst, September 1982.Helm, Irene R. "File Change Semantics and theFamiliarity Theory of Definiteness".
In Rainer Bauerle,Christoph Schwarze and Arnim von Stechow (eds.
)Meaning, Use, and Interpretation of Language.
Walterde Gmyter, Berlin, 1983, 164-189.Hovy, E. H. Generating Natural Language UnderPragmatic Constraints.
Unpublished Yale Dissertation,YALEU/CSD/RR #521, 1987.LaPolla, Mark Vincent.
"The Role of Inversion, Cleftingand PP-Fronting in Relating Discourse Elements: SomeImplications for Cognitive and Computational Models ofNatural Language Processing".
In Proceedings from theXI International Conference on ComputationalLinguistics, 1986, 168-173.McDonald, David D. Natural Language Production as aprocess of Decision-mdtking under Constraints,unpublished Ph.D. Dissertation, MIT, ArtificialIntelligence Laboratory, 1980.McDonald, David D., Made W. Meteer (V, aughan) andJames D. Pustejovsky.
"Factors Contributing toEfficiency in Natural Language Generation".
In G.Kempen (ed.)
Natural Language Generation: RecentAdvqnces in Artificial Intelligence, Psychology andLinguistics, Kluwer Academic Publishers,Boston/Dordrecht 1987.Prince, Ellen F. "A Comparison of Wh-Cl~fts and It-Cleftsin Discourse".
Language, 1978, 54(4), 883-905.Quillian, M. R. (1962) "A Revised Design for anUnderstanding Machine", Mechanical Translation, 7,17-29,Quillian, M. R. (1967) "The Teachable LanguageComprehender: A simulation program and theory oflanguage", Commut~ications f the ACM, 12, 459-476.Simmons, Robert g. Computations from the English.Prentice-Hall, 1984.yon Stechow, Arnim, "Topic, Focus and LocalRelevance".
In W. Klein and W. Levelt (eds.
), Crossingthe Boundaries in Linguistics,'1981, 95-130.Thompson, H. "Strategy and tactics: a model for languageproduction".
In Papers from the 13th Regional Meeting,376Chicago Linguistics Society, 1977.Valain, M. "The Restricted Language Architecture of aHybrid System", In the Proceedings of the NinthInternational Joint Conferet~ce on Artificial Intelligence,L.A,, 1985, 547-55I.APPENDIX AIn this Appendix we have given two examples of textthat NOSVO can generate.
The text was based uponnaturally occurring text (Lawrence 1985).
The oldinformation first principle has been applied to the first text.It has not been applied to the second text.
We believe thatthe second text is stilted, less cohesive and harder to read,though this has yet to be proven experimentally.
We alsobelieve that the missapplication f the old information firstprinciple would be worse than its nonapplication.
Theseare topics left for future research.TEXTILong before I was tall enough to ride on the big coastermyself, I spent many pleasant hours persuading myreluctant father to accompany me.As an aficionado f amusement parks I was overjoyedwhen our whole family finally flew to California to tackleWalt Disney's extravaganza..More than two decades later, I'm still journeying toparks.
(page 4)TEXT III spent many pleasant hours persuading my reluctantfather to accompany me long before I was tall enough toride on the big coaster myself.
I was overjoyed, as anaficionado f amusement parks, when our .whole familyfinally flew to California to tackle Walt Disney'sextravaganza.I'm still journeying to parks more than two decades later.APPENDIX B~J  \] ~ ..^\[l:,oraa:tn I ~._  I.on-Domain I .^ Inridg.
IDis~ou~so I ~,/.,,..,.,".,.
"~lspeciz?c I / ~ so  spaolz~c ~ .v ~ui:tding ABase ~?unQ~Knowledg e ~.Found?
~--~IKnowled~e I ~I  ..... ~ -IInferenoe I / ~ Searcher I ~ / IBase Searche.l ~ / I ~  _ ~ _ ~-~ounQY~-~_ ~ -~-/Found?
@ @ @/ / ~ i-\[ Lingulisti$|Converter |_ _ ,~And CategoSy-I Analyzer Ii!
EnglishGenerator377
