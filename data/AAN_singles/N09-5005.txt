Proceedings of NAACL HLT 2009: Demonstrations, pages 17?20,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsWordNet::SenseRelate::AllWords -A Broad Coverage Word Sense Taggerthat Maximizes Semantic RelatednessTed Pedersen and Varada KolhatkarDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812 USA{tpederse,kolha002}@d.umn.eduhttp://senserelate.sourceforge.netAbstractWordNet::SenseRelate::AllWords is a freelyavailable open source Perl package that as-signs a sense to every content word (knownto WordNet) in a text.
It finds the sense ofeach word that is most related to the sensesof surrounding words, based on measuresfound in WordNet::Similarity.
This method isshown to be competitive with results from re-cent evaluations including SENSEVAL-2 andSENSEVAL-3.1 IntroductionWord sense disambiguation is the task of assigninga sense to a word based on the context in which itoccurs.
This is one of the central problems in Nat-ural Language Processing, and has a long history ofresearch.
A great deal of progress has been made inusing supervised learning to build models of disam-biguation that assign a sense to a single target wordin context.
This is sometimes referred to as the lexi-cal sample or target word formulation of the task.However, to be effective, supervised learning re-quires many manually disambiguated examples ofa single target word in different contexts to serveas training data to learn a classifier for that word.While the resulting models are often quite accurate,manually creating training data in sufficient volumeto cover even a few words is very time consumingand error prone.
Worse yet, creating sufficient train-ing data to cover all the different words in a text isessentially impossible, and has never even been at-tempted.Despite these difficulties, word sense disambigua-tion is often a necessary step in NLP and can?t sim-ply be ignored.
The question arises as to how to de-velop broad coverage sense disambiguation modulesthat can be deployed in a practical setting without in-vesting huge sums in manual annotation efforts.
Ouranswer is WordNet::SenseRelate::AllWords (SR-AW), a method that uses knowledge already avail-able in the lexical databaseWordNet to assign sensesto every content word in text, and as such offersbroad coverage and requires no manual annotationof training data.SR-AW finds the sense of each word that is mostrelated or most similar to those of its neighbors in thesentence, according to any of the ten measures avail-able in WordNet::Similarity (Pedersen et al, 2004).It extends WordNet::SenseRelate::TargetWord, alexical sample word sense disambiguation algorithmthat finds the maximum semantic relatedness be-tween a target word and its neighbors (Patward-han et al, 2003).
SR-AW was originally developedby (Michelizzi, 2005) (through version 0.06) and isnow being significantly enhanced.2 MethodologySR-AW processes a text sentence by sentence.
Itproceeds through each sentence word by word fromleft to right, centering each content word in a bal-anced window of context whose size is determinedby the user.
Note that content words at the startor end of a sentence will have unbalanced windowsassociated with them, since the algorithm does notcross sentence boundaries and treats each sentenceindependently.17All of the possible senses of the word in the centerof the window are measured for similarity relative tothe possible senses of each of the surrounding wordsin the window in a pairwise fashion.
The sense ofthe center word that has the highest total when thosepairwise scores are summed is considered to be thesense of that word.
SR-AW then moves the centerof the window to the next content word to the right.The user has the option of fixing the senses of thewords that precede it to those that were discoveredby SR-AW, or allowing all their senses to be consid-ered in subsequent steps.WordNet::Similarity1 offers six similarity mea-sures and four measures of relatedness.
Measuresof similarity are limited to making noun to noun andverb to verb comparisons, and are based on usingthe hierarchical information available for nouns andverbs in WordNet.
These measures may be basedon path lengths (path, wup, lch) or on path lengthsaugmented with Information Content derived fromcorpora (res, lin, jcn).
The measures of relatednessmay make comparisons between words in any partof speech, and are based on finding paths betweenconcepts that are not limited to hierarchical relations(hso), or on using gloss overlaps either for stringmatching (lesk) or for creating a vector space model(vector and vector-pairs) that are used for measuringrelatedness.The availability of ten different measures that canbe used with SR-AW leads to an incredible richnessand variety in this approach.
In general word sensedisambiguation is based on the presumption thatwords that occur together will have similar or relatedmeanings, so SR-AW allows for a wide range of op-tions in deciding how to assess similarity and relat-edness.
SR-AW can be viewed as a graph based ap-proach when using the path based measures, wherewords are assigned the senses that are located mostclosely together in WordNet.
These path basedmethods can be easily augmented with InformationContent in order to allow for finer grained distinc-tions to be made.
It is also possible to lessen theimpact of the physical structure of WordNet by us-ing the content of the glosses as the primary sourceof information.1http://wn-similarity.sourceforge.net3 WordNet::SenseRelate::AllWords UsageInput : The input to SR-AW can either be plainuntagged text (raw), or it may be tagged with PennTreebank part of speech tags (tagged : 47 tags; e.g.,run/VBD), or with WordNet part of speech tags (wn-tagged: 4 tags for noun, verb, adjective, adverb;e.g., run#v).
Penn Treebank tags are mapped toWordNet POS tags prior to SR-AW processing, soeven though this tag set is very rich, it is used sim-ply to distinguish between the four parts of speechWordNet knows, and identify function words (whichare ignored as WordNet only includes open classwords).
In all cases simple morphological process-ing as provided by WordNet is utilized to identifythe root form of a word in the input text.Examples of each input format are shown below:?
(raw) : The astronomer married a movie star.?
(tagged) : The/DT astronomer/NN mar-ried/VBD a/DT movie star/NN?
(wntagged) : The astronomer#n married#v amovie star#nIf the format is raw, SR-AW will identify Word-Net compounds before processing.
These are multi-word terms that are usually nouns with just onesense, so their successful identification can signif-icantly improve overall accuracy.
If a compoundis not identified, then it often becomes impossibleto disambiguate.
For example, if White House istreated as two separate words, there is no combina-tion of senses that will equal the residence of theUS president, where that is the only sense of thecompound White House.
To illustrate the scope ofcompounds, of the 155,287 unique strings in Word-Net 3.0, more than 40% (64,331) of them are com-pounds.
If the input is tagged or wntagged, it isassumed that the user has identified compounds byconnecting the words that make up a compound with(e.g., white house, movie star).In the tagged and wntagged formats, the user mustidentify compounds and also remove punctuation.In the raw format SR-AW will simply ignore punc-tuation unless it happens to be part of a compound(e.g., adam?s apple, john f. kennedy).
In all formatsthe upper/lower case distinction is ignored, and it is18assumed that the input is already formatted one lineper sentence, one sentence per line.SR-AW will then check to see if a stoplist hasbeen provided by the user, or if the user would like touse the default stoplist.
In general a stoplist is highlyrecommended, since there are quite a few words inWordNet that have unexpected senses and might beproblematic unless they are excluded.
For example,who has a noun sense of World Health Organization.A has seven senses, including angstrom, vitamin A,a nucleotide, a purine, an ampere, the letter, and theblood type.
Many numbers have noun senses thatdefine them as cardinal numbers, and some have ad-jective senses as well.In the raw format, the stoplist check is done aftercompounding, because certain compounds includestop words (e.g., us house of representatives).
Inthe wntagged and tagged formats the stoplist checkis still performed, but the stoplist must take into ac-count the form of the part of speech tags.
How-ever, stoplists are expressed using regular expres-sions, making it quite convenient to deal with partof speech tags, and also to specify entire classes ofterms to be ignored, such as numbers or single char-acter words.Disambiguation Options : The user has a numberof options to control the direction of the SR-AW al-gorithm.
These include the very powerful choicesregarding the measure of similarity or relatednessthat is to be used.
There are ten such measures ashas been described previously.
As was also alreadymentioned, the user also can choose to fix the sensesof words that have already been processed.In addition to these options, the user can con-trol the size of the window used to determine whichwords are involved in measuring relatedness or simi-larity.
A window size ofN includes the center word,and then extends out to the left and right of the cen-ter for N/2 content words, unless it encounters thesentence boundaries.
IfN is odd then the number ofwords to the left and right (N ?
1)/2, and if N iseven there are N/2 words to the left, and (N/2)?
1words to the right.When using a measure of similarity and tagged orwntagged text, it may be desirable to coerce the partof speech of surrounding words to that of the wordin the center of the window of context.
If this isnot done, then any word with a part of speech otherthan that of the center word will not be included inthe calculation of semantic similarity.
Coercion isperformed by first checking for forms of the word ina different part of speech, and then checking if thereare any derivational relations from the word to thepart of speech of the center word.
Note that in theraw format part of speech coercion is not necessary,since the algorithm will consider all possible parts ofspeech for each word.
If the sense of previous wordshas already been fixed, then part of speech coerciondoes not override those fixed assignments.Finally, the user is able to control several scoringthresholds in the algorithm.
The user may specify acontext score which indicates a minimum thresholdthat a sense of the center word should achieve withall the words in the context in order to be selected.If this threshold is not met, no sense is assigned andit may be that the window should be increased.The pair score is a finer grained threshold that in-dicates the minimum values that a relatedness scorebetween a sense of the center word and a sense ofone of the neighbors must achieve in order to becounted in the overall score of the center word.
Ifthis threshold is not met then the pair will contribute0 to that score.
This can be useful for filtering outnoise from the scores when set to modest values.Output : The output of SR-AW is the original textwith WordNet sense tags assigned.
WordNet sensetags are given in WPS form, which means word, partof speech, and sense number.
In addition, glosses aredisplayed for each of the selected senses.There are also numerous trace options available,which can be combined in order to provide more de-tailed diagnostic output.
This includes displayingthe window of context with the center word desig-nated (1), the winning score for each context win-dow (2), the non-zero scores for each sense of thecenter word (4), the non-zero pairwise scores (8),the zero values for any of the previous trace levels(16), and the traces from the semantic relatednessmeasures from WordNet::Similarity (32).4 Experimental ResultsWe have evaluated SR-AW using three corpora thathave been manually annotated with senses fromWordNet.
These include the SemCor corpus, and19Table 1: SR-AW Results (%)2 5 15SC P R F P R F P R Flch 56 13 21 54 29 36 52 35 42jcn 65 15 24 64 31 42 62 41 49lesk 58 49 53 62 60 61 62 61 61S2 P R F P R F P R Flch 48 10 16 50 24 32 48 31 38jcn 55 9 15 55 21 31 55 31 39lesk 54 44 48 58 56 57 59 59 59S3 P R F P R F P R Flch 48 13 20 49 29 37 48 35 41jcn 55 14 22 55 31 40 53 38 46lesk 51 43 47 54 52 53 54 53 54the SENSEVAL-2 and SENSEVAL-3 corpora.
Sem-Cor is made up of more than 200,000 words of run-ning text from news articles found in the Brown Cor-pus.
The SENSEVAL data sets are each approxi-mately 4,000 words of running text from Wall StreetJournal news articles from the Penn Treebank.
Notethat only the words known to WordNet in these cor-pora have been sense tagged.
As a result, there are185,273 sense tagged words in SemCor, 2,260 inSENSEVAL-2, and 1,937 in SENSEVAL-3.
We haveused versions of these corpora where the WordNetsenses have been mapped to WordNet 3.02.In Table 4 we report results using Precision (P),Recall (R), and F-Measure (F).
We use three windowsizes in these experiments (2, 5, and 15), threeWord-Net::Similarity measures (lch, jcn, and lesk),andthree different corpora : SemCor (SC), SENSEVAL-2 (S2), SENSEVAL-3 (S3).
These experiments werecarried out with version 0.17 of SR-AW.For all corpora we observe the same patterns.The lesk measure tends to result in much higher re-call with smaller window sizes, since it is able tomeasure similarity between words with any parts ofspeech, whereas lch and jcn are limited to makingnoun-noun and verb-verb measurements.
But, as thewindow size increases so does recall.
Precision con-tinues to increase for lesk as the window size in-creases.
Our best results come from using the leskmeasure with a window size of 15.
For SemCor thisresults in an F-measure of 61%.
For SENSEVAL-2 it2http://www.cse.unt.edu/?rada/downloads.htmlresults in an F-measure of 59%, and for SENSEVAL-3 it results in an F-measure of 54%.
These resultswould have ranked 4th of 22 teams and 15th of 26 inthe respective SENSEVAL events.A well known baseline for all words disambigua-tion is to assign the first WordNet sense to each am-biguous word.
This results in an F-measure of 76%for SemCor, 69% for SENSEVAL-2, and 68% forSENSEVAL-3.
A lower bound can be establishedby randomly assigning senses to words.
This re-sults in an F-Measure of 41% for SemCor, 41% forSENSEVAL-2, and 37% for SENSEVAL-3.
This isrelatively high due to the large number of words thathave just one possible sense (so randomly selectingwill result in a correct assignment).
For example,in SemCor approximately 20% of the ambiguouswords have just one sense.
From these results wecan see that SR-AW lags behind the sense one base-line (which is common among all words systems),but significantly outperforms the random baseline.5 ConclusionsWordNet::SenseRelate::AllWords is a highly flexi-ble method of word sense disambiguation that of-fers broad coverage and does not require training ofany kind.
It uses WordNet and measures of seman-tic similarity and relatedness to identify the sensesof words that are most related to each other in a sen-tence.
It is implemented in Perl and is freely avail-able from the URL on the title page both as sourcecode and via a Web interface.ReferencesJ.
Michelizzi.
2005.
Semantic relatedness applied to allwords sense disambiguation.
Master?s thesis, Univer-sity of Minnesota, Duluth, July.S.
Patwardhan, S. Banerjee, and T. Pedersen.
2003.
Us-ing measures of semantic relatedness for word sensedisambiguation.
In Proceedings of the Fourth Interna-tional Conference on Intelligent Text Processing andComputational Linguistics, pages 241?257, MexicoCity, February.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.Wordnet::Similarity - Measuring the relatedness ofconcepts.
In Proceedings of Fifth Annual Meetingof the North American Chapter of the Association forComputational Linguistics, pages 38?41, Boston, MA.20
