Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 977?984Manchester, August 2008Investigating the Portability of Corpus-Derived Cue Phrases for DialogueAct ClassificationNick Webb and Ting LiuILS InstituteUniversity at Albany, SUNYAlbany, NY, USA{nwebb|tl7612}@albany.eduAbstractWe present recent work in the area ofCross-Domain Dialogue Act tagging.
Ourexperiments investigate the use of a sim-ple dialogue act classifier based on purelyintra-utterance features - principally in-volving word n-gram cue phrases.
We ap-ply automatically extracted cues from onecorpus to a new annotated data set, to de-termine the portability and generality ofthe cues we learn.
We show that our auto-matically acquired cues are general enoughto serve as a cross-domain classificationmechanism.1 IntroductionA number of researchers (Hirschberg and Litman,1993; Grosz and Sidner, 1986) speak of cue or keyphrases in utterances that can serve as useful indi-cators of discourse structure.
We have previouslyinvestigated the use of such cue phrases to predictdialogue acts or DAs (functional tags which rep-resent the communicative intentions behind eachuser utterance) (Webb et al, 2005a).
We devel-oped an approach, in common with the work ofSamuel et al (1999), where word n-grams thatmight serve as cue phrases are automatically de-tected in a corpus and we have previously reportedthe results of experiments evaluating this approachon the SWITCHBOARD corpus, where our resultsrival the best reported over that data (Stolcke et al,2000), although our method adopts a significantlyless complex algorithm.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.An interesting by-product of our approach is theranked list of cue phrases derived from the sourcecorpus.
Visual inspection of these cues revealsthat, as one might expect, there is a high degree ofcorrelation between phrases such as ?can you?
andthe DA <yes/no question>, ?where is?
and ?whois?
with the DA <wh-question> and ?right?
or?ok?
with DA <agree/accept>.
These cues appearto be of a general nature, unrelated to the sourcedomain or application.
Therefore, despite beingautomatically acquired from one domain specificcorpus, these cues should be equally applicable tonew corpora, from a different domain and it is thishypothesis we test.
This paper presents our workon dialogue act classification using cues automat-ically extracted from a corpus from one domain,and applying these cues directly as a classifier overa new corpus from a different domain.The material is presented as follows: Previouswork with dialogue act modelling is outlined inSection 2.
An overview of the corpora used forthe experiments we report can be seen in Section3.
A brief overview of our classification methodis given in Section 4.
Our experiments evaluatingthe cue-based dialogue act classifier tagging new,out-of-domain data are given in Section 5.
Finallywe end with some discussion and an outline of in-tended further work.2 Related WorkDialogue Acts (DAs) (Bunt, 1994), also known asspeech acts or dialogue moves, represent the func-tional performance of a speaker?s utterance, suchas a greeting ?Hello there?, asking a question like?How is your mother??
or making a request ?Canyou move your foot?
?.There are two broad categories of computationalmodel used to interpret these acts.
The first, in-977Corpus AvailabilityUtterancecountDialoguecountWordcountDistinctwordsDialoguetypeSWITCHBOARD public 223606 1155 1431725 21715 ConversationalAMITI?ES GE restricted 30206 1000 228165 7841 Task-orientedFigure 1: Summary data for the dialogue corporacluding the work of Cohen and Perrault (1979) re-lies on processing belief logics, centring on the im-pact each utterance has on the hearer - what thehearer believes the speaker intended to commu-nicate.
These models can be very accurate, butoften are complex, and require significant world-knowledge to create.The second model type is cue-based, and cen-tres on the notion of repeated, predictive cues -subsections of language which are strong indica-tors of specific DAs.
In this second category, muchof the work is cast as a probabilistic classificationtask, solved by training approaches on labelled ex-amples of dialogue acts.
As an example of theseprobabilistic methods, Stolcke et al (2000) applya HMM method to the SWITCHBOARD corpus, onethat exploits both the order of words within ut-terances and the order of dialogue acts over ut-terances.
They use a single split of the data fortheir experiments, with 198k utterances for train-ing and 4k utterances for testing, achieving a DAtagging accuracy of 71.0% on word transcripts.Another learning approach by Samuel et al (1998)uses transformation-based learning over a numberof utterance features, including utterance length,speaker turn and the dialogue act tags of adja-cent utterances.
They achieved an average scoreof 75.12% tagging accuracy over the VERBMO-BIL corpus.
A significant aspect of this work isthe automatic identification of word sequences thatmight serve as useful dialogue act cues (Samuel etal., 1999).
A number of statistical criteria are ap-plied to identify potentially useful word n-gramsthat are then supplied to the transformation-basedlearning method as ?features?.What has been less explored is the portabil-ity or adaptability of these models to new cor-pora and new domains.
Prasad and Walker (2002)look at applying models generated from a Human-Computer corpus to a Human-Human corpus in thesame domain, that of travel planning, and score avery low 36.72% accuracy using their model.
Thework of Tur et al (2006) is closer to the work re-ported here - they apply models derived from theSWITCHBOARD corpus to the ICSI-MRDA corpus(Shriberg et al, 2004) using boosting, applied toa high level of representation (comprising only 5DA categories, one of which they exclude), wherethey achieve 57.37% tagging accuracy.
This seemsto indicate that cross-domain application of mod-els is possible, although the level of accuracy aspresently reported is low.3 Experimental CorporaOur work as described here applies to two corpora- the DA-tagged portion of the SWITCHBOARD cor-pus (Jurafsky et al, 1998), and the AMITI?ES GEcorpus (Hardy et al, 2002; Hardy et al, 2003), cre-ated as part of the AMITI?ES European 5th Frame-work program project (Hardy et al, 2005).
A sum-mary of the two corpora can be seen in Figure 1.3.1 SwitchboardThe annotated portion of the SWITCHBOARD cor-pus comprises 1155 annotated conversations be-tween two human participants, where the dia-logues are of an unstructured, non-directed char-acter.
Participants do not know each other, andare provided only with a set of topics which theymay wish to discuss.
The SWITCHBOARD corpusis annotated using an elaboration of the DAMSLtag set.
In 1998 the Discourse Resource Initia-tive finalised a task-independent set of DAs, calledDAMSL (Dialogue Act Markup in Several Layers),for use across different domains.
DAMSL has beenused to mark-up several dialogue corpora, such asTRAINS (Core and Allen, 1997), and the SWITCH-BOARD corpus (Jurafsky et al, 1998).The annotation over the SWITCHBOARD corpusinvolves 50 major classes, together with a num-ber of diacritic marks, which combine to generate220 distinct labels.
Jurafsky et al (1998) proposea clustering of these 220 tags into 42 larger classesand it is this clustered set that was used both inour experiments and those of Stolcke et al (2000).In measuring the agreement between annotators inlabelling this data, Jurafsky et al (1998) reportan average pair-wise kappa of .80 (Carletta et al,978<Turn Id="utt3" Speaker="A" DA-Type="Open-question"> what do you think was different tenyears ago from now?</Turn><Turn Id="utt4" Speaker="B" DA-Type="Statement-opinion"> Well I would say as far as socialchanges go I think families were more together.</Turn><Turn Id="utt5" Speaker="B" DA-Type="Statement-opinion"> They did more thingstogether</Turn><Turn Id="utt6" Speaker="A" DA-Type="Acknowledge"> Uh-huh</Turn>Figure 2: Excerpt of dialogue from the SWITCHBOARD corpus1997).
An excerpt of dialogue from the SWITCH-BOARD corpus can be seen in Figure 2.3.2 AMITI?ESThe AMITI?ES project (Hardy et al, 2005) collected1000 English human-human dialogues from Euro-pean GE call centres.
These calls are of an in-formation seeking or transactional type, in whichcustomers interact with their financial accountsby phone to check balances, make payments andreport lost credit cards.
The resulting data hasbeen sanitised, to replace identifying features suchas names, addresses and account numbers withgeneric information (?John Doe?, ?1 The Street?
)and the corpus is annotated with DAs using XDML,combining slight variant of the 42-class DAMSL(Hardy et al, 2002) with domain specific seman-tic information such as account numbers and creditcard details (Hardy et al, 2003).The most frequent tag in the AMITI?ES corpusis Influence-on-listener=?Information-request?,which occurs 20% of the time.
For this corpus, theaverage pair-wise kappa score of .59 was signifi-cantly lower than the SWITCHBOARD corpus.
Forthe major categories (questions, answers), averagepair-wise kappa scores were around .70.
Again,according to the work of Carletta et al (1997), aminimum kappa score of 0.67 is required to drawtentative conclusions.
An excerpt of dialoguefrom the AMITI?ES corpus can be seen in Figure 3.4 DA ClassificationIn this section we briefly describe our approach toDA classification, based solely on intra-utterancefeatures.
A key aspect of the approach is the se-lection of the word n-grams to use as cue phrases.Samuel et al (1999) investigate a series of differentstatistical criteria for use in automatically selectingcue phrases, but we use a criterion of predictivity,described below, which is one that Samuel et al(1999) do not consider.4.1 Cue Phrase SelectionFor our experiments, the word n-grams used as po-tential cue phrases during classification are com-puted from the training data.
All word n-grams oflength 1?4 within the data are considered as can-didates.
The phrases chosen as cue phrases areselected principally using a criterion of predictiv-ity, which is the extent to which the presence ofa certain n-gram in an utterance is predictive of ithaving a certain dialogue act category.
For an n-gram n and dialogue act d, this corresponds to theconditional probability: P (d | n), a value that canbe straightforwardly computed.
For each n-gram,we are interested in its maximal predictivity, i.e.the highest predictivity value found for it with anyDA category.
This set of n-grams is then reducedby applying thresholds of predictivity and occur-rence, i.e.
eliminating any n-gram whose maxi-mal predictivity is below some minimum require-ment, or whose maximal number of occurrencesin any category falls below some threshold value.This thresholding removes some low frequency,high predictivity n-grams that skew classificationperformance.
The n-grams that remain are identi-fied as our cue phrases.
The threshold values thatare used in all experiments were arrived at empiri-cally, using a validation set to automatically set thethreshold levels independently of the test data, asdescribed in Webb et al (2005b).4.2 Using Cue Phrases in ClassificationTo classify an utterance, we identify all the wordn-grams it contains, and determine which of thesehas the highest predictivity of some dialogue actcategory (i.e.
is performing as some cue).
If mul-tiple cue phrases share the same maximal predic-tivity, but predict different categories, we select the979<Turn Id="2.1" Speaker="Operator" Info-level="Communication-mgt"Conventional="Opening">good morning customer services sam speaking</Turn><Turn Id="3.1" Speaker="Customer" Info-level="Communication-mgt"Conventional="Opening">erm good morning</Turn><Turn Id="3.2" Speaker="Customer" Info-level="Task"Forward-function="Explanation">erm I was away for about two months and i came backand my card i don?t know whether i have lost it or it is stolen</Turn><Turn Id="4.1" Speaker="Operator" Understanding="Backchannel"Response-to="T3.2">right okay</Turn><Turn Id="4.2" Speaker="Operator" Info-level="Task"Influence-on-listener="Info-request-explicit">can you confirm your namefor me please</Turn>Figure 3: Excerpt of dialogue from the AMITI?ES GE corpusDA for the phrase with the highest frequency.
If thecombination of predicitivity and occurrence countis insufficient to determine a single DA, then a ran-dom choice is made amongst the remaining can-didate DAs.
If no cue phrases are present, then adefault tag is assigned, corresponding to the mostfrequent tag within the training corpus.Our best reported figures on the 202k utteranceSWITCHBOARD corpus are a cross-validated scoreof 69.09%, with a single high score of 71.29%,which compares very favourably with the (notcross-validated) 71% reported in Stolcke et al(2000) for the same corpus.
We also presented in-formation that shows that adding a sequence modelof DA progressions - an n-gram model of DAs -results in no significant increase in performance(Webb et al, 2005a).
This is surprising consid-ering that Stolcke et al (2000) report their best fig-ures when combining a HMM model of the wordsinside utterances with a tri-gram model of the Di-alogue Act sequence, as in the work of Reithingerand Klesen (1997).
When Stolcke et al (2000) addthe sequence model to the HMM language model, itadds around 20% points to the final accuracy scoreover the SWITCHBOARD data.However, our observation is confirmed by bothSerafin and Eugenio (2004) and Ries (March1999).
On the basis of this result, we hypothe-sise that our cues are highly predictive of dialoguestructure, and that much dialogue processing maytake place at a very shallow level.5 Cross-Domain ClassificationThe central purpose of this paper is to examinethe use of automatically extracted cues to tag dataother than the corpus from which they are de-rived.
The hypothesis we wish to test is that thesecues are sufficiently general to work as a classi-fication device on a corpus from a different do-main, even containing interactions of a differentconversational style.
Specifically, SWITCHBOARDis an open domain spoken human-human conver-sational corpus and we have shown state-of-the-arttagging performance over this data using our cue-based model.
We now wish to see how well thesesame cues perform over the AMITI?ES GE corpusof spoken task-based dialogues.
The dialogues inthe AMITI?ES GE corpus are far more goal directed,and contain domain specific cues not found in thegeneral conversational SWITCHBOARD corpus.The ability to apply cues extracted from one cor-pus to new data is an interesting challenge.
It couldconfirm work which indicates the prominence ofsuch word cues in language (Hirschberg and Lit-man, 1993).
A tag mechanism that can operateacross domains presents a range of benefits - forexample it can be used to annotate or partially an-notate new data collections.5.1 DA MappingCross-corpus classification would be simplified ifboth corpora were annotated with identical DA tax-onomies.
In actuality, the SWITCHBOARD corpusand the AMITI?ES GE corpus are annotated withvariants of the DAMSL DA annotation scheme.
Inthe SWITCHBOARD corpus, the hierarchical natureof the DAMSL schema has been flattened and clus-tered, to produce 42 major classes.
In the AMITI?ESGE corpus, the dialogue level schema has been leftlargely untouched from the DAMSL original.
In or-980der to be able to compare automatic classificationperformance across the two corpora, a mapping isrequired between the 42-class schema of SWITCH-BOARD and the DAMSL-like XDML schema ofthe AMITI?ES GE corpus.
In their work, Juraf-sky et al (1998) include such a mapping betweenSWITCHBOARD and DAMSL that covers approxi-mately 80% of the labels in the SWITCHBOARDcorpus.
We have adapted this slightly to coverminor differences between the XDML used in theAMITI?ES GE corpus and the original DAMSL, al-though this leaves us with two issues that we needto address.First there are differences in granularity on bothsides.
Importantly, in many instances we mayidentify the most salient role of the utterance, butmiss modification information which may makelittle interpretative difference.
For example, mark-up in the AMITI?ES GE corpus makes the distinc-tion between <Forward-function=?Assert?> and<Forward-function=?Reassert?>, whereas mark-up in the SWITCHBOARD corpus ignores such adistinction, and annotates both as type <Forward-function=?Assert?> - although the SWITCH-BOARD corpus captures the difference between as-sertions that are opinions, and those that are not,whereas the original DAMSL does not capture thisdistinction.
To address this mismatch we createa set of super classes by relating the annotationsof SWITCHBOARD-DAMSL and the AMITI?ES GE-XDML corpora at the most salient level, accordingto the mapping contained in Jurafsky et al (1998).Whilst the majority of tags have a one-to-one cor-relation, there are elements of both the Forward-Looking Function (see Figure 4) and Backward-Looking Function (Figure 5) that require mappingin both directions.Secondly, there are a number of AMITI?ES GEtags that we know a-priori we have little or nochance to recognise.
For example, the AMITI?ESGE corpus is meticulously annotated to include thatcertain utterances are perceived as answers to priorutterances.
Our approach to DA tagging is purelyintra-utterance, taking no account of the wider dis-course structure, so will not recognise these dis-tinctions.
Although such a model of discoursestructure should be trivial, based for example onan adjacency pair approach, this will be evaluatedfurther in future work.5.2 Evaluation CriteriaThese issues require that we create two evaluationcriteria for our subsequent experiments - strict andlenient.
With strict evaluation, we are required tomatch all elements of the AMITI?ES GE corpus an-notation - despite knowing in advance that this isnot possible for a range of utterances.
We use ourstrict evaluation criteria to establish a lower boundof performance for our classifier.
Our lenient ap-proach is a back-off model, where we require thatwe correctly identify the most critical part of themulti-part annotation - those that are identified asthe most salient.We?ll use the dialogue excerpt shown in Fig-ure 3 as an example of how these two scor-ing mechanisms work.
The first utterance (2.1)is marked as <Info-level=?Communication-mgt?Conventional=?Opening?>.
This has a one-to-one correlation with the SWITCHBOARD-DAMSLtag <conventional-opening>.
In the case of thisexample, and in all instances in the AMITI?ESGE corpus, utterances are marked as <Info-level=?Task?>, unless they are from a smallset of exceptions, including openings, closingsor backchannels, that are annotated as <Info-level=?Communication-mgt?>.
Once an utteranceis tagged as one of these exceptions, we know tochange the <Info-level> assignment accordingly.There will be no difference between our strictand lenient evaluation models for the interpreta-tion of this utterance.
The same is true for the sec-ond (3.1) utterance annotation, which has a directcorrelation with SWITCHBOARD-DAMSL annota-tions.
However, the fourth utterance (4.1) includesa <Response-to=?T3.2?> annotation that we willnot be able to identify using our intra-utterancemodel.
This utterance will be judged correct us-ing the lenient model, and incorrect using the strictmetric.The third utterance (3.2) is marked as<Forward-function=?Explanation?>.
Usingthe Forward-function map shown in Figure4, we see that this maps to the super class<Forward-function=?Assert?>, that in turn mapsto the SWITCHBOARD-DAMSL tags <statement-non-opinion> and <statement-opinion>.
Thismeans that any utterance identified by thepresence of a cue phrase as either <statement-non-opinion> or <statement-opinion> will infact be tagged as <Info-level=?Task?
Forward-function=?Assert?>.
Whilst this annotation981Forward?
function = ?Assert?Forward?
function = ?Reassert?Forward?
function = ?Explanation?Forward?
function = ?Rexplanation?Forward?
function = ?Expression????????Forward?
function = ?Assert?{statement?
non?
opinionstatement?
opinionFigure 4: Partial Forward-Looking Function mapping table (XDML } SUPERCLASS { SWITCHBOARD-DAMSL)Inf ?
on?
list = ?Info?
req ?
explicit?Inf ?
on?
list = ?Info?
req ?
implicit?Inf ?
on?
list = ?Conf ?
req ?
implicit?Inf ?
on?
list = ?Conf ?
req ?
explicit??????Influence?
on?
listener =?Information?
request????????????yes?
no?
questionwh?
questionsopen?
questionsor ?
clausedeclarative?
questiontag ?
questionFigure 5: Partial Backward-Looking Function mapping table (XDML } SUPERCLASS { SWITCHBOARD-DAMSL)captures the salient behaviour of the utterance,it is not an exact match to the original AMITI?ESGE corpus annotation and correspondingly whenscoring the lenient model will score this as correct,whereas the exact model will not.The same is true with the fifth utterance(4.2), annotated in this case as <Influence-on-listener=?Info-request-explicit?>.
A classifiertrained over the SWITCHBOARD corpus wouldidentify this (through the mapping see in Fig-ure 5) as <Influence-on-listener=?Information-request?>, which would be scored as correct usingthe lenient measure, and incorrect using the exact.5.3 Classification ExperimentsThe results of our experiments are summarisedin Figure 6.
First, to establish our baseline tag-ging performance, we take the classification al-gorithm outlined earlier in Section 4, and applyit to the SWITCHBOARD corpus for both trainingand testing, replicating the work reported in Webbet al (2005a).
In this case, 198,000 utterancesare used for training, and a separate 4,000 utter-ances are used for testing.
We achieve a cross-validated score of 69.6%, where the most frequenttag in SWITCHBOARD, <statement-non-opinion>,occurs 36% of the time.
This is a confirmationof the work reported in Webb et al (2005a), anddemonstrates that this simple model works excep-tionally well for this task.For the first of the new experiments to test ourhypothesis, we substitute the AMITI?ES GE corpusfor the SWITCHBOARD corpus in both steps - train-ing and testing - which will give us an upper boundof performance of this particular classification al-gorithm over this data.
In this experiment, we used10% of the corpus for testing - giving us a total of27,000 utterances for training and 3,000 utterancesfor testing.
For all experiments where AMITI?ES GEdata is used as a test corpus, both strict and lenientscoring will be used.
Strict scoring sets a lowerbound for this exercise, and should be greater thanchance, which corresponds to the distribution ofthe most frequent DA tag in each corpus.
For strictscoring, where we are required to match all the el-ements of the AMITI?ES GE XDML tag, we score65.9% accuracy in this experiment.
For lenient,where we must match only the most salient fea-tures, we score 70.8% accuracy.
Whilst there isno direct comparison to other work on this cor-pus, Hardy et al (2005) show partial results for DAclassification on this task, looking only at a fewmajor classes, and achieve a score of 86%.
How-ever, this includes only the 5 most frequent DAcategories, and considers utterances shorter than acertain number of words.Finally, we attempt cross-domain classification:First, we train our classifier using SWITCHBOARDdata, and test using AMITI?ES GE data.
We recordeda strict evaluation score of 39.8% tagging accu-racy.
Using the lenient score, we achieve around55.7% accuracy.
This can be considered a verygood result, given the lower bound score of 20%- that is the count of the most frequent tag.982TrainingcorpusTrainingutterancesTestingcorpusTestutterancesCommontag (%)LenientscoreStrictscoreSWITCHBOARD 198,000 SWITCHBOARD 4,000 36% n/a 69%AMITI?ES GE 27,000 AMITI?ES GE 3,000 20% 70.8% 65.9%SWITCHBOARD 198,000 AMITI?ES GE 30,000 20% 55.7% 39.8%AMITI?ES GE 27,000 SWITCHBOARD 198,000 36% 48.3% 40%SWITCHBOARD 27,000 AMITI?ES GE 3,000 20% 53.2% 38%Figure 6: Experimental ResultsThen we apply the classification in reverse -we train on AMITI?ES GE data, and test on theSWITCHBOARD corpus, using all available datain both cases.
Using the strict evaluation met-ric, we achieve a score of 40.0%, and a lenientscore of 48.3%.
This compares to a baseline of36%, so is not a drastic improvement over ourlower bound.
Some inspection of the data in-formed us that the AMITI?ES GE data did not includemany <backchannel> utterances, so subsequentlymost of these instances in the SWITCHBOARD cor-pus were missed by our classifier.
By changingthe default tag to be <backchannel>, rather thanthe most frequent tag for the training corpus, weachieve a performance gain to 47.7% with strictscoring, and 56.0% with the lenient metric.For the last experiment, we also wanted to studythe effect of limiting the training data on cross-domain classification, by reducing the SWITCH-BOARD data to match that of the AMITI?ES GE train-ing set - that is, to use only 27,000 utterances ofthe SWITCHBOARD corpus as training data to ex-tract cues, which are then applied both to itself (forreference), and to the AMITI?ES GE corpus.
On arelated note, part of the work conducted in Webbet al (2005a) studied the impact of different sizetraining models when classifying SWITCHBOARDdata, using models of 4k, 50k and 202k utterances.Whilst substantial improvement was seen whenmoving from 4k utterances to 50k utterances, thesubsequent increase from 50k to 202k utteranceshad a negligible impact on classification accuracy.With the reduced SWITCHBOARD training set, wescore 53.2% with the lenient metric, and 38% withstrict, indicating that the reduction is size of thetraining data has some effect on classification ac-curacy.6 Discussion, Future WorkWe have shown that the cues extracted from theSWITCHBOARD corpus can be used to success-fully classify utterances in the AMITI?ES GE cor-pus.
We achieve almost 80% of the upper baselineperformance over the AMITI?ES GE corpus, whenjudged using our lenient scoring mechanism - scor-ing 55.7% using the cross-domain cues, comparedto the 70.8% when using in-domain cues.
Whenusing the strict measure we still achieve around60% of the upper bound performance, both resultsbeing a substantial improvement over the baselinemeasure of 20%, corresponding to the most fre-quent tag in the AMITI?ES GE corpus.
This is a sig-nificant result, which confirms the idea that cuescan be sufficiently general across domains to beused in classification.However, whilst the experiment using SWITCH-BOARD corpus derived cues to classify AMITI?ESGE data works well, the same is not true in re-verse.
There are two possible explanations for thisresult.
It could be related to the size of data avail-able for training, although our experiments in thisarea seem to suggest otherwise and so we believethat the composition of the training data is a morecrucial element.
Although the DA distribution inthe SWITCHBOARD corpus is uneven, there is suf-ficient data for the major classes to be effective onnew data that also contains these classes.
Althoughthe AMITI?ES GE contains a lot of questions andstatements, there is very little of the other signif-icant categories, such as <backchannels>, a keyDA in the SWITCHBOARD corpus and conversa-tional speech in general.
Correspondingly, the cuesderived from the AMITI?ES GE data perform wellon a selection of utterances in the SWITCHBOARDcorpus, but very poorly on others.
We want to per-form an in-depth error analysis to see if the errorswe obtain in classification accuracy are consistent.We can also compare our list of automatically de-rived cues phrases, particularly those that overlapbetween the two corpora, to those reported in priorliterature.
It might be interesting to see if morecomplex models, derived using state-of-the art ma-983chine learning approaches, could demonstrate sim-ilar portability - i.e is it the simplicity of our modelthat allows for the observed robust portability?Finally, we wish to combine SWITCHBOARDand AMITI?ES corpora in the cue learning phase, tosee how this effects classification, and apply theresults to a range of other corpora, including theICSI-MRDA corpus (Shriberg et al, 2004).ReferencesBunt, Harry.
1994.
Context and dialogue control.THINK, 3:19?31.Carletta, J. C., A. Isard, S. Isard, J. Kowtko,G.
Doherty-Sneddon, and A. Anderson.
1997.
TheReliability of a Dialogue Structure Coding Scheme.Computational Linguistics, 23:13?31.Cohen, P. R. and C. R. Perrault.
1979.
Elements of aplan based theory of speech acts.
Cognitive Science,3.Core, Mark G. and James Allen.
1997.
Coding di-alogs with the DAMSL annotation scheme.
In AAAIFall Symposium on Communicative Action in Hu-mans and Machines, MIT, Cambridge, MA.Grosz, Barbara and Candace Sidner.
1986.
Attention,Intentions, and the Structure of Discourse.
Compu-tational Linguistics, 19(3).Hardy, Hilda, Kirk Baker, Laurence Devillers, LoriLamel, Sophie Rosset, Tomek Strzalkowski, CristianUrsu, and Nick Webb.
2002.
Multi-layered dialogueannotation for automated multilingual customer ser-vice.
In Proceedings of the ISLE workshop on Dia-logue Tagging for Multimodal Human Computer In-teraction, Edinburgh.Hardy, H., K. Baker, H. Bonneau-Maynard, L. Dev-illers, S. Rosset, and T. Strzalkowski.
2003.
Se-mantic and dialogic annotation for automated mul-tilingual customer service.
In Eurospeech, Geneva,Switzerland.Hardy, H., A. Biermann, R. Bryce Inouye, A. McKen-zie, T. Strzalkowski, C. Ursu, N. Webb, and M. Wu.2005.
The AMITIES System: Data-Driven Tech-niques for Automated Dialogue.
Speech Communi-cation, 48:354?373.Hirschberg, Julia and Diane Litman.
1993.
Empiri-cal Studies on the Disambiguation of Cue Phrases.Computational Linguistics, 19(3):501?530.Jurafsky, Daniel, Rebecca Bates, Noah Coccaro,Rachel Martin, Marie Meteer, Klaus Ries, Eliza-beth Shriberg, Andreas Stolcke, Paul Taylor, andCarol Van Ess-Dykema.
1998.
Switchboad dis-course language modeling project final report.
Re-search Note 30, Center for Language and SpeechProcessing, Johns Hopkins University, Baltimore.Prasad, Rashmi and Marilyn Walker.
2002.
Train-ing a Dialogue Act Tagger for Humna-Human andHuman-Computer Travel Dialogues.
In Proceedingsof the 3rd SIGdial workshop on Discourse and Dia-logue, Philadelphia, Pennsylvania.Reithinger, Norbert and Martin Klesen.
1997.
Dia-logue act classification using language models.
InProceedings of EuroSpeech-97.Ries, Klaus.
March, 1999.
Hmm and neural networkbased speech act classification.
In Proceddings ofthe IEEE Conference on Acoustics, Speech and Sig-nal Processing, volume 1, pages 497?500, Phoenix,AZ.Samuel, Ken, Sandra Carberry, and K. Vijay-Shanker.1998.
Dialogue act tagging with transformation-based learning.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics, Montreal.Samuel, Ken, Sandra Carberry, and K. Vijay-Shanker.1999.
Automatically selecting useful phrases for di-alogue act tagging.
In Proceedings of the FourthConference of the Pacific Association for Computa-tional Linguistics, Waterloo, Ontario, Canada.Serafin, Riccardo and Barbara Di Eugenio.
2004.FLSA: Extending Latent Semantic Analysis withfeatures for dialogue act classification.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, Barcelona, Spain.Shriberg, E., R. Dhillon, S. Bhagat, J. Ang, and H. Car-vey.
2004.
The ICSI meeting recorder dialog act(MRDA) corpus.
In Special Interest Group on Dis-course and Dialogue (SIGdial), Boston, USA.Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer.
2000.
Dialogue act model-ing for automatic tagging and recognition of conver-sational speech.
In Computational Linguistics 26(3),339?373.Tur, Gokhan, Umit Guz, and Dilek Hakkani-Tur.
2006.Model Adaptation for Dialogue Act Tagging.
InIEEE Spoken Language Technology Workshop.Webb, Nick, Mark Hepple, and Yorick Wilks.2005a.
Dialogue Act Classification Based on Intra-Utterance Features.
In Proceedings of the AAAIWorkshop on Spoken Language Understanding.Webb, Nick, Mark Hepple, and Yorick Wilks.
2005b.Empirical determination of thresholds for optimal di-alogue act classification.
In Proceedings of the NinthWorkshop on the Semantics and Pragmatics of Dia-logue.984
