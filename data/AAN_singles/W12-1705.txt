In: R. Levy & D. Reitter (Eds.
), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 51?60,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsConnectionist-Inspired Incremental PCFG ParsingMarten van SchijndelThe Ohio State Universityvanschm@ling.ohio-state.eduAndy ExleyUniversity of Minnesotaexley@cs.umn.eduWilliam SchulerThe Ohio State Universityschuler@ling.ohio-state.eduAbstractProbabilistic context-free grammars (PCFGs)are a popular cognitive model of syntax (Ju-rafsky, 1996).
These can be formulated tobe sensitive to human working memory con-straints by application of a right-corner trans-form (Schuler, 2009).
One side-effect of thetransform is that it guarantees at most a sin-gle expansion (push) and at most a single re-duction (pop) during a syntactic parse.
Theprimary finding of this paper is that this prop-erty of right-corner parsing can be exploited toobtain a dramatic reduction in the number ofrandom variables in a probabilistic sequencemodel parser.
This yields a simpler structurethat more closely resembles existing simplerecurrent network models of sentence compre-hension.1 IntroductionThere may be a benefit to using insights from humancognitive modelling in parsing.
Evidence for in-cremental processing can be seen in garden pathing(Bever, 1970), close shadowing (Marslen-Wilson,1975), and eyetracking studies (Tanenhaus et al,1995; Allopenna et al, 1998; Altmann and Kamide,1999), which show humans begin attempting to pro-cess a sentence immediately upon receiving lin-guistic input.
In the cognitive science community,this incremental interaction has often been mod-elled using recurrent neural networks (Elman, 1991;Mayberry and Miikkulainen, 2003), which utilizea hidden context with a severely bounded repre-sentational capacity (a fixed number of continuousunits or dimensions), similar to models of activation-based memory in the prefrontal cortex (Botvinick,2007), with the interesting possibility that the dis-tributed behavior of neural columns (Horton andAdams, 2005) may directly implement continuousdimensions of recurrent hidden units.
This paperpresents a refinement of a factored probabilistic se-quence model of comprehension (Schuler, 2009) inthe direction of a recurrent neural network modeland presents some observed efficiencies due to thisrefinement.This paper will adopt an incremental probabilis-tic context-free grammar (PCFG) parser (Schuler,2009) that uses a right-corner variant of the left-corner parsing strategy (Aho and Ullman, 1972)coupled with strict memory bounds, as a model ofhuman-like parsing.
Syntax can readily be approxi-mated using simple PCFGs (Hale, 2001; Levy, 2008;Demberg and Keller, 2008), which can be easilytuned (Petrov and Klein, 2007).
This paper willshow that this representation can be streamlined toexploit the fact that a right-corner parse guaranteesat most one expansion and at most one reduction cantake place after each word is seen (see Section 2.2).The primary finding of this paper is that this prop-erty of right-corner parsing can be exploited to ob-tain a dramatic reduction in the number of randomvariables in a probabilistic sequence model parser(Schuler, 2009) yielding a simpler structure thatmore closely resembles connectionist models suchas TRACE (McClelland and Elman, 1986), Short-list (Norris, 1994; Norris and McQueen, 2008), orrecurrent models (Elman, 1991; Mayberry and Mi-ikkulainen, 2003) which posit functional units onlyfor cognitively-motivated entities.The rest of this paper is structured as follows:Section 2 gives the formal background of the right-corner parser transform and probabilistic sequence51model parsing.
The simplification of this model isdescribed in Section 3.
A discussion of the interplaybetween cognitive theory and computational mod-elling in the resulting model may be found in Sec-tion 4.
Finally, Section 5 demonstrates that suchfactoring also yields large benefits in the speed ofprobabilistic sequence model parsing.2 Background2.1 NotationThroughout this paper, PCFG rules are defined oversyntactic categories subscripted with abstract treeaddresses (c??).
These addresses describe a node?slocation as a path from a given ancestor node.
A 0on this path represents a leftward branch and a 1 arightward branch.
Positions within a tree are repre-sented by subscripted ?
and ?
so that c?0 is the leftchild of c?
and c?1 is the right child of c?.
The set ofsyntactic categories in the grammar is denoted by C.Finally, J?K denotes an indicator probability whichis 1 if ?
and 0 otherwise.2.2 Right-Corner ParsingParsers such as that of Schuler (2009) model hierar-chically deferred processes in working memory us-ing a coarse analogy to a pushdown store indexedby an embedding depth d (to a maximum depth D).To make efficient use of this store, a CFG G mustbe transformed using a right-corner transform intoanother CFG G?
with no right recursion.
Given anoptionally arc-eager attachment strategy, this allowsthe parser to clear completed parse constituents fromthe set of incomplete constituents in working mem-ory much earlier than with a conventional syntacticstructure.
The right-corner transform operates de-terministically over a CFG following three mappingrules:c?
?
c?0 c?1 ?
Gc?/c?1 ?
c?0 ?
G?(1)c??
?
c?
?0 c?
?1 ?
G, c?
?
Cc?/c?
?1 ?
c?/c??
c?
?0 ?
G?(2)c??
?
x??
?
G, c?
?
Cc?
?
c?/c??
c??
?
G?
(3)A bottom-up incremental parsing strategy com-bined with the way the right-corner transform pullseach subtree into a left-expanding hierarchy ensuresat most a single expansion (push) will occur atany given observation.
That is, each new observa-tion will be the leftmost leaf of a right-expandingsubtree.
Additionally, by reducing multiply right-branching subtrees to single rightward branches, thetransform also ensures that at most a single reduc-tion (pop) will take place at any given observation.Schuler et al (2010) show near complete cover-age of the Wall Street Journal portion of the PennTreebank (Marcus et al, 1993) can be achieved witha right-corner incremental parsing strategy using nomore than four incomplete contituents (deferred pro-cesses), in line with recent estimates of human work-ing memory capacity (Cowan, 2001).Section 3 will show that, in addition to being de-sirable for bounded working memory restrictions,the single expansion/reduction guarantee reducesthe search space between words to only two decisionpoints ?
whether to expand and whether to reduce.This allows rapid processing of each candidate parsewithin a sequence modelling framework.2.3 Model FormulationThis transform is then extended to PCFGs and inte-grated into a sequence model parser.
Training onan annotated corpus yields the probability of anygiven syntactic state executing an expansion (creat-ing a syntactic subtree) or a reduction (completinga syntactic subtree) to transition from every suffi-ciently probable (in this sense active) hypothesis inthe working memory store.The probability of the most likely sequence ofstore states q?1..D1..T can then be defined as the prod-uct of nonterminal ?Q, preterminal ?P,d, and termi-nal ?X factors:q?1..D1..Tdef= argmaxq1..D1..TT?t=1P?Q(q1..Dt | q1..Dt?1 pt?1)?
P?P,d?
(pt | bd?t ) ?
P?X (xt | pt) (4)where all incomplete constituents qdt are factoredinto active adt and awaited bdt components:qdtdef= adt /bdt (5)and d?
determines the deepest non-empty incompleteconstituent of q1..Dt :52d?
def= max{d | qdt 6= ???}
(6)The preterminal model ?P,d denotes the expecta-tion of a subtree containing a given preterminal, ex-pressed in terms of side- and depth-specific gram-mar rules P?Gs,d(c?
?
c?0 c?1) and expected countsof left progeny categories E?G?,d(c???
c??
...) (seeAppendix A):P?P,d(c??
| c?
)def= E?G?,d(c???
c??
...)??x??P?GL,d(c??
?
x??)
(7)and the terminal model ?X is simply:P?X (x?
| c?)def=P?G(c?
?
x?)?x?
P?G(c?
?
x?
)(8)The Schuler (2009) nonterminal model ?Q iscomputed from a depth-specific store elementmodel ?Q,d and a large final state model ?F,d:P?Q(q1..Dt | q1..Dt?1 pt?1)def=?f1..DtD?d=1P?F,d(fdt | fd+1t qdt?1 qd?1t?1 )?
P?Q,d(qdt | fd+1t fdt qdt?1 qd?1t ) (9)After each time step t and depth d, ?Q generatesa set of final states to generate a new incompleteconstituent qdt .
These final states fdt are factoredinto categories cfdt and boolean variables (0 or 1)encoding whether a reduction has taken place atdepth d and time step t. The depth-specific final statemodel ?F,d gives the probability of generating a finalstate fdt from the preceding qdt and qd?1t which is theprobability of executing a reduction or consolidationof those incomplete constituents:P?F,d(fdt | fd+1t qdt?1 qd?1t?1 )def={if fd+1t = ???
: Jfdt = 0Kif fd+1t 6= ???
: P?F,d,R(fdt | qdt?1 qd?1t?1 )(10)With these depth-specific fdt in hand, the model cancalculate the probabilities of each possible qdt foreach d and t based largely on the probability of tran-sitions (?Q,d,T ) and expansions (?Q,d,E) from the in-complete constituents at the previous time step:P?Q,d(qdt | fd+1t fdt qdt?1 qd?1t?1 )def=??
?if fd+1t = ??
?, fdt = ???
: Jqdt = qdt?1Kif fd+1t 6= ??
?, fdt = ???
: P?Q,d,T (qdt | fd+1t fdt qdt?1 qd?1t )if fd+1t 6= ??
?, fdt 6= ???
: P?Q,d,E (qdt | qd?1t )(11)This model is shown graphically in Figure 1.The probability distributions over reductions(?F,d,R), transitions (?Q,d,T ) and expansions(?Q,d,E) are then defined, also in terms of side- anddepth-specific grammar rules P?Gs,d(c?
?
c?0 c?1)and expected counts of left progeny cate-gories E?G?,d(c???
c??
...) (see Appendix A):P?Q,d,T (qdt | fd+1t fdt qdt?1qd?1t )def={if fdt 6= ???
: P?Q,d,A(qdt | qd?1t fdt )if fdt = ???
: P?Q,d,B (qdt | qdt?1fd+1t )(12)P?F,d,R(fdt | fd+1t qdt?1qd?1t?1 )def={if cfd+1t 6=xt : Jfdt = ??
?Kif cfd+1t =xt : P?F,d,R(fdt | qdt?1qd?1t?1 )(13)P?Q,d,E (c??/c???
| /c?)def=E?G?,d(c???
c??
...) ?
Jx??
= c???
= c?
?K (14)The subcomponent models are obtained by ap-plying the transform rules to all possible trees pro-portionately to their probabilities and marginalizingover all constituents that are not used in the models:?
for active transitions (from Transform Rule 1):P?Q,d,A(c??/c?
?1 | /c?
c??0)def=E?G?,d(c???
c??
...) ?
P?GL,d(c??
?
c?
?0 c??1)E?G?,d(c?+?
c?
?0 ...)(15)53p1x1q11q21q31q41p2x2q12q22q32q42p3x3q13q23q33q43p4x4q14q24q34q44p5x5q15q25q35q45p6x6q16q26q36q46p7x7q17q27q37q47f12f22f32f42f13f23f33f43f14f24f34f44f15f25f35f45f16f26f36f46f17f27f37f47=DT=the=0,DT=NP/NN=NN=fund=0,NP=S/VP=VB=bought=0,VB = S/VP=VP/NP=DT=two=1,DT=S/VP=VP/NN=JJ=regional=1,JJ=S/VP=VP/NN=NN=banks=1,VP = S/RB=RB=todayFigure 1: Schuler (2009) Sequence Model?
for awaited transitions (Transform Rule 2):P?Q,d,B (c?/c?
?1 | c??/c??
c??0)def=Jc?
= c?
?K ?P?GR,d(c??
?
c?
?0 c??1)E?G?,d(c??0?
c?
?0 ...)(16)?
for reductions (from Transform Rule 3):P?F,d,R(c?
?,1 | /c?
c??
?/ )def=Jc??
= c??
?K ?E?G?,d(c?0?
c??
...)E?G?,d(c???
c??
...)(17)P?F,d,R(c?
?,0 | /c?
c??
?/ )def=Jc??
= c??
?K ?E?G?,d(c?+?
c??
...)E?G?,d(c???
c??
...)(18)3 Simplified ModelAs seen in the previous section, the right-cornerparser of Schuler (2009) makes the center embed-ding depth explicit and each memory store elementis modelled as a combination of an active and anawaited component.
Each input can therefore eitherincrease (during an expansion) or decrease (duringa reduction) the store of incomplete constituents orit can alter the active or awaited component of thedeepest incomplete constituent (the affectable ele-ment).
Alterations of the awaited component of theaffectable element can be thought of as the expan-sion and immediate reduction of a syntactic con-stituent.
The grammar models transitions in the ac-tive component implicitly, so these are conceptual-ized as consisting of neither an expansion nor a re-duction.Removing some of the variables in this model re-sults in one that looks much more like a neural net-work (McClelland and Elman, 1986; Elman, 1991;Norris, 1994; Norris and McQueen, 2008) in thatall remaining variables have cognitive correllates ?in particular, they correspond to incomplete con-stituents in working memory ?
while still maintain-ing the ability to explicitly represent phrase struc-ture.
This section will demonstrate how it is possi-ble to exploit this to obtain a large reduction in thenumber of modelled random variables.In the Schuler (2009) sequence model, eight ran-dom variables are used to model the hidden statesat each time step (see Figure 1).
Half of these vari-ables are joint consisting of two further (active andawaited) constituent variables, while the other halfare merely over intermediate final states.
Althoughthe entire store is carried from time step to timestep, only one memory element is affectable at anyone time, and this element may be reduced zero or54one times (using an intermediate final state), and ex-panded zero or one times (using an incomplete con-stituent state), yielding four possible combinations.This means the model only actually needs one of itsintermediate final states.The transition model ?Q can therefore be simpli-fied with terms ?F,d for the probability of expand-ing the incomplete constituent at d, and terms ?A,dand ?B,d for reducing the resulting constituent(defining the active and awaited components ofa new incomplete constituent), along with termsfor copying incomplete constituents above this af-fectable element, and for emptying the elements be-low it:P?Q(q1..Dt | q1..Dt?1 pt?1)def=P?F,d?
(?+?
| bd?t?1 pt?1) ?
P?A,d?
(???
| bd?
?1t?1 ad?t?1)?
Jad?
?1t =ad?
?1t?1 K ?
P?B,d??1(bd?
?1t | bd?
?1t?1 ad?t?1)?
Jq1..d?
?2t =q1..d?
?2t?1 K ?
Jqd?..Dt = ???K+P?F,d?
(?+?
| bd?t?1 pt?1) ?
P?A,d?
(ad?t | bd?
?1t?1 ad?t?1)?
P?B,d?
(bd?t | ad?t ad?+1t?1 )?
Jq1..d?
?1t =q1..d?
?1t?1 K ?
Jqd?+1..Dt = ???K+P?F,d?
(???
| bd?t?1 pt?1) ?
P?A,d?
(???
| bd?t?1 pt?1)?
Jad?t =ad?t?1K ?
P?B,d?
(bd?t | bd?t?1 pt?1)?
Jq1..d?
?1t =q1..d?
?1t?1 K ?
Jqd?+1..Dt = ???K+P?F,d?
(???
| bd?t?1 pt?1) ?
P?A,d?
(ad?+1t | bd?t?1 pt?1)?
P?B,d?
(bd?+1t | ad?+1t pt?1)?
Jq1..d?t =q1..d?t?1 K ?
Jqd?+2..Dt = ??
?K (19)The first element of the sum in Equation 19 com-putes the probability of a reduction with no expan-sion (decreasing d?).
The second corresponds to theprobability of a store undergoing neither an expan-sion nor a reduction (a transition to a new active con-stituent at the same embedding depth).
In the thirdis the probability of an expansion and a reduction(a transition among awaited constituents at the sameembedding depth).
Finally, the last term yields theprobability of an expansion without a reduction (in-creasing d?
).From Equation 19 it may be seen that the unaf-fected store elements of each time step are main-tained sans change as guaranteed by the single-reduction feature of the right-corner parser.
This re-sults in a large representational economy by mak-ing the majority of store state decisions determinis-tic.
This representational economy will later trans-late into computational efficiencies (see section 5).In this sense, cognitive modelling contributes to apractical speed increase.Since the bulk of the state remains the same,the recognizer can access the affectable variableand operate solely over the transition possibili-ties from that variable to calculate the distribu-tion over store states for the next time step to ex-plore.
Reflecting this change, the hidden statesnow model a single final-state variable (f) forresults of the expansion decision, and the af-fectable variable resulting from the reduction de-cision (both its active (a) and awaited (b) cate-gories), as well as the preterminal state (p) definedin the previous section.
These models are again ex-pressed in terms of side- and depth-specific grammarrules P?Gs,d(c?
?
c?0 c?1) and expected counts ofleft progeny categories E?G?,d(c???
c??
...) (seeAppendix A).Expansion probabilities are modelled as a binarydecision depending on whether or not the awaitedcomponent of the affectable variable c?
is likely toexpand immediately into an anticipated pretermi-nal c??
(resulting in a non-empty final state: ?+?)
orif intervening embeddings are necessary given theaffectable active component (yielding no final state:???
):P?F,d(f | c?
c??)def=????????
?if f= ?+?
:E?G?,d (c?0?c??
...)E?G?,d (c???c??
...)if f= ???
:E?G?,d (c?+?c??
...)E?G?,d (c???c??
...)(20)The active component category c??
is defined as de-pending on the category of the awaited componentabove it c?
and its left-hand child c??0:P?A,d(c??
| c?
c?
?0)def=E?G?,d (c?1?c?
?0 ...)E?G?,d (c?+?c?
?0 ...)?
Jc?
?= ??
?K+E?G?,d (c?+?c??
...)?P?GL,d (c???c?
?0 ...)E?G?,d (c?+?c?
?0 ...)(21)The awaited component category c?1 is defined as55depending on the category of its parent c?
and thepreceding sibling c?0:P?B,d(c?1 | c?
c?0)def=P?GR,d (c?
?c?0 c?1)E?G?,d (c?1?c?0 ...)(22)Both of these make sense given the manner in whichthe right-corner parser shifts dependencies to the leftdown the tree in order to obtain incremental infor-mation about upcoming constituents.3.1 Graphical RepresentationIn order to be represented graphically, the workingmemory store ?Q is factored into a single expansionterm ?F and a product of depth-specific reductionterms ?Q,d:P?Q(q1..Dt | q1..Dt?1 pt?1)def=?ftP?F (ft | q1..Dt?1 )?D?d=1P?Q,d(qdt | q1..Dt?1 pt?1 ft qd+1t ) (23)and the depth-specific reduction model ?Q,d is fac-tored into individual decisions over each randomvariable:P?Q,d(qdt | q1..Dt?1 pt?1 ft qd+1t )def=????????????????????????????????
?if qd+1t = ??
?, ft 6= ??
?, d=d?
?1 :Jadt =adt?1K ?
P?B,d(bdt | bdt?1 ad+1t?1 )if qd+1t = ??
?, ft 6= ??
?, d=d?
:P?A,d(adt | bd?1t?1 adt?1) ?
P?B,d(bdt | adt adt?1)if qd+1t = ??
?, ft= ??
?, d=d?
:Jadt =adt?1K ?
P?B,d(bdt | bdt?1 pt?1)if qd+1t = ??
?, ft= ??
?, d=d?+1 :P?A,d(adt | bd?1t?1 pt?1) ?
P?B,d(bdt | adt pt?1)if qd+1t 6= ???
: Jqdt =qdt?1Kotherwise : Jqdt = ??
?K(24)This dependency structure is represented graphicallyin Figure 2.The first conditional in Equation 24 checkswhether the input causes a reduction but no expan-sion (completing a subtree parse).
In this case, d?
isreduced from the previous t, and the relevant qdt?1 iscopied to qdt except the awaited constituent is alteredto reflect the completion of its preceding awaitedsubtree.
In the second case, the parser makes anactive transition as it completes a left subtree andbegins exploring the right subtree.
The third caseis similar to the first except it transitions betweentwo like depths (awaited transition), and depends onthe preterminal just seen to contrive a new subtreeto explore.
In the fourth case, d?
is incremented asanother incomplete constituent opens up in workingmemory.
The final two cases simply update the un-affected store states to reflect their previous states attime t?
1.4 DiscussionThis factoring of redundant hidden states out of theSchuler (2009) probabilistic sequence model showsthat cognitive modelling can more closely approx-imate a simple recurrent network model of lan-guage processing (Elman, 1991).
Probabilistic se-quence model parsers have previously been mod-elled with random variables over incomplete con-stituents (Schuler, 2009).
In the current implementa-tion, each variable can be thought of as a bank of ar-tificial neurons.
These artificial neurons inhibit oneanother through the process of normalization.
Con-versely, they activate artificial neurons at subsequenttime steps by contributing probability mass throughthe transformed grammar.
This point was made byNorris and McQueen (2008) with respect to lexicalaccess; this model extends it to parsing.Recurrent networks can parse simple sentencesbut run into problems when running over more com-plex datasets.
This limitation comes from the unsu-pervised methods typically used to train them, whichhave difficulty scaling to sufficiently large trainingsets for more complex constructions.
The approachdescribed in this paper uses a hidden context simi-lar to that of a recurrent network to inform the pro-gression of the parse, except that the context is interms of random variables with distributions over aset of explicit syntactic categories.
By framing thevariable domains in a linguistically-motivated fash-ion, the problem of acquisition can be divested fromthe problem of processing.
This paper then uses thesemi-supervised grammar training of Petrov et al(2006) in order to develop a simple, accurate modelfor broad-coverage parsing independent of scale.56p1x1q11q21q31q41p2x2q12q22q32q42p3x3q13q23q33q43p4x4q14q24q34q44p5x5q15q25q35q45p6x6q16q26q36q46p7x7q17q27q37q47f2 f3 f4 f5 f6 f7 f8=DT=the=NP/NN=NN=fund=+=S/VP=VB=bought=S/VP=VP/NP=DT=two=S/VP=VP/NN=JJ=regional=S/VP=VP/NN=NN=banks=+=S/RB=RB=today=+Figure 2: Parse using Simplified ModelLike Schuler (2009), the incremental parser dis-cussed here operates in O(n) time where n is thelength of the input.
Further, by its incremental na-ture, this parser is able to run continuously on astream of input, which allows any other processesdependent on the input (such as discourse integra-tion) to run in parallel regardless of the length of theinput.5 Computational BenefitDue to the decreased number of decisions requiredby this simplified model, it is substantially fasterthan previous similar models.
To test this speed in-crease, the simplified model was compared with thatof Schuler (2009).
Both parsers used a grammar thathad undergone 5 iterations of the Petrov et al (2006)split-merge-smooth algorithm as found to be opti-mal by Petrov and Klein (2007), and both used abeam-width of 500 elements.
Sections 02-21 of theWall Street Journal Treebank were used in trainingthe grammar induction for both parsers according toPetrov et al (2006), and Section 23 was used forevaluation.
No tuning was done as part of the trans-form to a sequence model.
Speed results can be seenin Table 1.
While the speed is not state-of-the-art inthe field of parsing at large, it does break new groundfor factored sequence model parsers.To test the accuracy of this parser, it was com-pared using varying beam-widths to the Petrov andKlein (2007) and Roark (2001) parsers.
With theexception of the Roark (2001) parser, all parsersused 5 iterations of the Petrov et al (2006) split-System Sec/SentSchuler 2009 74Current Model 12Table 1: Speed comparison with an unfactored proba-bilistic sequence model using a beam-width of 500 ele-mentsSystem P R FRoark 2001 86.6 86.5 86.5Current Model (500) 86.6 87.3 87.0Current Model (2000) 87.8 87.8 87.8Current Model (5000) 87.8 87.8 87.8Petrov Klein (Binary) 88.1 87.8 88.0Petrov Klein (+Unary) 88.3 88.6 88.5Table 2: Accuracy comparison with state-of-the-art mod-els.
Numbers in parentheses are number of parallel acti-vated hypothesesmerge-smooth algorithm, and the training and test-ing datasets remained the same.
These results maybe seen in Table 2.
Note that the Petrov and Klein(2007) parser allows unary branching within thephrase structure, which is not well-defined under theright-corner transform.
To obtain a fair comparison,it was also run with strict binarization.
The cur-rent approach achieves comparable accuracy to thePetrov and Klein (2007) parser assuming a strictlybinary-branching phrase structure.576 ConclusionThe primary goal of this paper was to demonstratethat a cognitively-motivated factoring of an exist-ing probabilistic sequence model parser (Schuler,2009) is not only more attractive from a modellingperspective but also more efficient.
Such factor-ing yields a much slimmer model where every vari-able has cognitive correlates to working memory el-ements.
This also renders several transition prob-abilities deterministic and the ensuing representa-tional economy leads to a 5-fold increase in pars-ing speed.
The results shown here suggest cognitivemodelling can lead to computational benefits.ReferencesAlfred V. Aho and Jeffery D. Ullman.
1972.
The The-ory of Parsing, Translation and Compiling; Volume.
I:Parsing.
Prentice-Hall, Englewood Cliffs, New Jersey.P.
D. Allopenna, J. S. Magnuson, and M. K. Tanenhaus.1998.
Tracking the time course of spoken word recog-nition using eye movements: evidence for continuousmapping models.
Journal of Memory and Language,38:419?439.G.
T. M. Altmann and Y. Kamide.
1999.
Incrementalinterpretation at verbs: restricting the domain of sub-sequent reference.
Cognition, 73:247?264.Richard Bellman.
1957.
Dynamic Programming.Princeton University Press, Princeton, NJ.Thomas G. Bever.
1970.
The cognitive basis for lin-guistic structure.
In J.
?R.
Hayes, editor, Cognition andthe Development of Language, pages 279?362.
Wiley,New York.Matthew Botvinick.
2007.
Multilevel structure in behav-ior and in the brain: a computational model of fustershierarchy.
Philosophical Transactions of the Royal So-ciety, Series B: Biological Sciences, 362:1615?1626.Nelson Cowan.
2001.
The magical number 4 in short-term memory: A reconsideration of mental storage ca-pacity.
Behavioral and Brain Sciences, 24:87?185.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.Jeffrey L. Elman.
1991.
Distributed representations,simple recurrent networks, and grammatical structure.Machine Learning, 7:195?225.John Hale.
2001.
A probabilistic earley parser as a psy-cholinguistic model.
In Proceedings of the SecondMeeting of the North American Chapter of the Associ-ation for Computational Linguistics, pages 159?166,Pittsburgh, PA.Jonathan C Horton and Daniel L Adams.
2005.
The cor-tical column: a structure without a function.
Philo-sophical Transactions of the Royal Society of London- Series B: Biological Sciences, 360(1456):837?862.Daniel Jurafsky.
1996.
A probabilistic model of lexicaland syntactic access and disambiguation.
CognitiveScience: A Multidisciplinary Journal, 20(2):137?194.Roger Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition, 106(3):1126?1177.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.William D. Marslen-Wilson.
1975.
Sentence per-ception as an interactive parallel process.
Science,189(4198):226?228.Marshall R. Mayberry, III and Risto Miikkulainen.
2003.Incremental nonmonotonic parsing through semanticself-organization.
In Proceedings of the 25th AnnualConference of the Cognitive Science Society, pages798?803, Boston, MA.James L. McClelland and Jeffrey L. Elman.
1986.
Thetrace model of speech perception.
Cognitive Psychol-ogy, 18:1?86.Dennis Norris and James M. McQueen.
2008.
Shortlistb: A bayesian model of continuous speech recognition.Psychological Review, 115(2):357?395.Dennis Norris.
1994.
Shortlist: A connectionist modelof continuous speech recognition.
Cognition, 52:189?234.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofNAACL HLT 2007, pages 404?411, Rochester, NewYork, April.
Association for Computational Linguis-tics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 44thAnnual Meeting of the Association for ComputationalLinguistics (COLING/ACL?06).Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.William Schuler, Samir AbdelRahman, Tim Miller, andLane Schwartz.
2010.
Broad-coverage incrementalparsing using human-like memory constraints.
Com-putational Linguistics, 36(1):1?30.William Schuler.
2009.
Parsing with a bounded stackusing a model-based right-corner transform.
In Pro-ceedings of NAACL/HLT 2009, NAACL ?09, pages344?352, Boulder, Colorado.
Association for Compu-tational Linguistics.58Michael K. Tanenhaus, Michael J. Spivey-Knowlton,Kathy M. Eberhard, and Julie E. Sedivy.
1995.
Inte-gration of visual and linguistic information in spokenlanguage comprehension.
Science, 268:1632?1634.A Grammar FormulationGiven D memory elements indexed by d (see Sec-tion 2.2) and a PCFG ?G, the probability ?
(k)Ts,d of atree rooted at a left or right sibling s ?
{L,R} ofcategory c?
?
C requiring d ?
1..D memory ele-ments is defined recursively over paths of increasinglength k:P?
(0)Ts,d(1 | c?)
def= 0 (25)P?
(k)TL,d(1 | c?)
def=?x?P?G(c?
?
x?)+?c?0,c?1P?G(c?
?
c?0 c?1)?
P?
(k?1)TL,d (1 | c?0) ?
P?
(k?1)TR,d(1 | c?1)(26)P?
(k)TR,d(1 | c?)
def=?x?P?G(c?
?
x?)+?c?0,c?1P?G(c?
?
c?0 c?1)?
P?
(k?1)TL,d+1(1 | c?0) ?
P?
(k?1)TR,d(1 | c?1)(27)Note that the center embedding depth d increasesonly for left children of right children.
This is be-cause in a binary branching structure, center embed-dings manifest as zigzags.
Since the model is alsosensitive to the depth d of each decomposition, theside- and depth-specific probabilities of ?GL,d and?GR,d are defined as follows:P?GL,d(c?
?
x?)def=P?G(c?
?
x?)P?(?
)TL,d(1 | c?)(28)P?GR,d(c?
?
x?)def=P?G(c?
?
x?)P?(?
)TR,d(1 | c?)(29)P?GL,d(c?
?
c?0 c?1)def= P?G(c?
?
c?0 c?1)?
P?(?
)TL,d(1 | c?0) ?
P?(?
)TR,d(1 | c?1)?
P?(?
)TL,d(1 | c?
)?1 (30)P?GR,d(c?
?
c?0 c?1)def= P?G(c?
?
c?0 c?1)?
P?(?
)TL,d+1(1 | c?0) ?
P?(?
)TR,d(1 | c?1)?
P?(?
)TR,d(1 | c?
)?1 (31)The model will also need an expected countE?G?,d(c???
c??
...) of the given child constituentc??
dominating a prefix of constituent c?.
Expectedversions of these counts may later be used to deriveprobabilities of memory store state transitions (seeSections 2.3, 3).E?G?,d(c?0?
c?
...) def=?x?P?GR,d(c?
?
x?)(32)E?G?,d(c?1?
c?0 ...) def=?c?1P?GR,d(c?
?
c?0 c?1)(33)E?G?,d(c?k?
c?
?0 ...) def=?c??E?G?,d(c?k?1?
c??
...)??c??1P?GL,d(c??
?
c?
?0 c??1)(34)E?G?,d(c???
c??
...) def=??k=0E?G?,d(c?k?
c??
...)(35)E?G?,d(c?+?
c??
...) def=??k=1E?G?,d(c?k?
c??
...)(36)Equation 32 gives the probability of a constituentappearing as an observation, and Equation 33 givesthe probability of a constituent appearing as a left59child.
Equation 34 extends the previous two equa-tions to account for a constituent appearing at an ar-bitrarily deep embedded path of length k. Takingthe sum of all k path lengths (as in Equation 35)allows the model to account for constituents any-where in the left progeny of the dominated subtree.Similarly, Equation 36 gives the expectation that theconstituent is non-immediately dominated by c?.
Inpractice the infinite sum is estimated to some con-stant K using value iteration (Bellman, 1957).60
