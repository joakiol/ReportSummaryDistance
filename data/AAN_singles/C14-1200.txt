Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2117?2128, Dublin, Ireland, August 23-29 2014.Using Collections of Human Language Intuitionsto Measure Corpus RepresentativenessReinhard RappAix-Marseille Universit?Laboratoire d'Informatique Fondamentale163 Avenue de Luminy, 13288 Marseille, Francereinhardrapp@gmx.deAbstractIn corpus linguistics there have been numerous attempts to compile balanced corpora, result-ing in text collections such as the Brown Corpus or the British National Corpus.
These cor-pora are meant to reflect the average language use a native speaker typically encounters.
Butis it possible to measure in how far these efforts were successful?
Assuming that humans?
lan-guage intuitions are based on our brain?s capability to statistically analyze perceived languageand to memorize these statistics, we suggest a method for measuring corpus representative-ness which compares corpus statistics to three types of human language intuitions as collectedfrom test persons: Word familiarity, word association, and word relatedness.
We compute arepresentativeness score for a corpus by extracting word frequency, word co-occurrence, andcontextual statistics from it and by comparing these statistics to the human data.
The higherthe similarity, the more representative the corpus should be for the language environments ofthe test persons.
Our findings confirm the expectation that corpus size and corpus balancingmatter.1 IntroductionBalanced corpora, i.e.
corpora consisting of a carefully sampled mix of texts, have often been consid-ered important for providing a standard of average language use.
Well known examples of such cor-pora include the Brown Corpus (Francis & Ku?era, 1989) and the British National Corpus (Burnard &Aston, 1998).
But to obtain a balance many decisions concerning the corpus design have to be made.Biber (1993) mentions, among other things, that it has to be decided for what target population a cor-pus is meant to be representative, that estimates concerning the quantities of various text types are re-quired, and that decisions with regard to the number of individual text samples and their sizes have tobe made.However, there is no easy and well established way to verify the success of these measures.
Currentsuggestions include, for example, to consider a corpus as representative if it is not dominated by sub-language (Temnikova et al., 2014), or to more or less give up on the concept of representativeness andto concentrate on considering the suitability of a corpus for particular tasks.
Saldanha (2009) comes tothe conclusion that ?The problem with making representativeness the defining characteristic of a cor-pus is that it is very difficult to evaluate.
?Our goal here is to make an attempt to measure corpus representativeness in a standardized way,thereby avoiding to observe test persons?
average language input as this would not be very practical.Our starting point is that a representative corpus should reflect as well as possible average languageuse as encountered by native speakers.
We also assume that human language acquisition is essentiallycorpus-based (Rapp, 2011).
This implies the following: The human brain analyzes particular statisticalproperties of perceived language and memorizes them.
During language production these propertiesThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/2117are reproduced.
It has been shown that in certain test situations it is possible to isolate intuitions re-lated to some specific statistics.
These include the following three which we will utilize for measuringcorpus representativeness: Word frequency, word co-occurrence, and common context of words.
Interms of human language intuitions, these three statistical properties relate to word familiarities, wordassociations, and word relatedness.What we suggest is to extract data relating to these three types of statistical properties from a corpusand to compare it to the respective experimental data as obtained from test persons.
The higher theaverage agreement, the more representative the corpus should be for the language environment of thetest persons.1Related work has been conducted by Brisbaert & New (2009), which is mentioned in section 2.1,and in our own previous studies (Rapp, 2014a and Rapp, 2014c), of which the current work is an ex-tension.
A nice summary of how to measure corpus representativeness through psycholinguistic meas-ures is provided in a presentation by Francom & Ussishkin (2011).
Gries (2010), though in a slightlydifferent context, emphasizes the need of external validation: ?For corpus linguists, that means that ourmeasures must be validated against corpus-external evidence because, strictly speaking, as long as wecorpus linguists do not show that our dispersions and adjusted frequencies correspond to somethingoutside of our corpora, we have failed to provide the most elementary aspect of a new measure - itsvalidation.
?The remainder of this paper is structured as follows: We first describe the experimental data used,i.e.
the familiarity norms, the association norms, and the synonym data (describing word relatedness).Next we present the algorithms used to extract the corresponding statistics from the corpora.
By com-paring the human and the corpus-derived data, we introduce three quantitative measures of corpus rep-resentativeness, which we subsequently combine.
The paper concludes with a discussion and an out-look on future work.2 Human language intuitions2.1 Word familiaritiesPsychologists have collected word familiarity ratings from test persons.
For this purpose, the subjectswere asked to come up with subjective familiarities for given words.
Usually a scale between 1 and 7was used, whereby 1 means unfamiliar and 7 means very familiar.
The outcome of such experimentsare the so-called familiarity norms, i.e.
large tables listing the subjects' familiarity ratings.
In the cur-rent work we used the familiarity data for 4920 words from an online version2 of the MRC Psycho-linguistic Database (Coltheart, 1981).In previous studies (e.g.
Rapp, 2005) it has been shown that there is a strong correlation between thehuman familiarity judgments and the log occurrence frequencies of the words in corpora.
For illustra-tion, Table 1 shows the top five most familiar words in the MRC database together with their frequen-cies in the Brown corpus and compares them to some of the least familiar words.
As can be seen, thefamiliar words have consistently much higher corpus frequencies.
To explain this finding, Rapp(2005) hypothesized that human familiarity ratings are based on the word frequencies as observed bythe test persons in the language they perceive in everyday life.However, if we assume that the familiarity norms reflect word frequencies in perceived language,then it should be possible to use them as a standard for measuring the frequency aspect of corpus rep-resentativeness.
A corpus whose word frequencies are highly correlated to the familiarity norms ismore likely to be a good surrogate for everyday language, although word frequency of course reflectsonly one of many properties of a corpus.
Nevertheless, for a corpus to be representative, it is a neces-sary (though not sufficient) condition that its word frequencies are similar to those in everyday lan-guage.1Let us mention that there is some analogy to automatic MT evaluation, namely when computing the BLEU score: There amachine translation is compared to a human translation (which is based on human intuitions) by identifying matches be-tween n-grams of various lengths.
Then a combined score is computed from the results obtained for each n-gram length.2http://websites.psychology.uwa.edu.au/school/MRCDatabase/uwa_mrc.htm2118We should mention that instead of using word familiarity data it is also possible to use reactiontimes as obtained in the word recognition task.3 Brisbaert & New (2009) did so and related the reac-tion times to the corpus frequencies of the words for the purpose of measuring corpus representative-ness.
In essence, although they tested on other corpora, their findings seem to be similar to what wereport here based on word familiarities.FAMILIAR WORDS UNFAMILIAR WORDSWORD FAMILIARITY BROWN FREQUENCY WORD FAMILIARITYBROWNFREQUENCYBREAKFAST 6.6 53 LOQUACITY 1.4 1AFTERNOON 6.5 106 MIEN 1.4 1CLOTHES 6.5 89 YUCCA 1.4 1BEDROOM 6.5 52 BURGHER 1.3 1DAD  6.5 15 PAEAN 1.3 2Table 1: Words with high and low familiarity ratings in the MRC Psycholinguistic Database togetherwith their frequency counts in the Brown Corpus (words with a corpus frequency of zero are not in-cluded).2.2 Word associationsThe second type of human intuitions to be considered are word associations as obtained from test per-sons.
Such data has been collected from native speakers in large scale experiments, as exemplified inthe Edinburgh Associative Thesaurus (EAT; Kiss et al., 1973) which is the largest classical collectionof its kind.
The EAT comprises the associative responses as requested from around 100 British stu-dents for each of 8400 stimulus words and is available online.4To collect the data, the subjects were given questionnaires with lists of stimulus words, and wereasked to write down for each stimulus word the spontaneous association which first came to mind.This leads to collections of associations, the so-called association norms, as exemplified in Table 2.ABOVE CONSTELLAT?ON FEM?N?NEbelow (59) stars (39) masculine (26)high (4) star (33) girl (14)over (4) sky (5) woman (8)sky (4) andromeda (2) female (6)all (3) aquarius (2) sex (3)up (3) plough (2) beauty (2)me (2) aircraft (1) bird (2)under (2) cancer (1) girls (2)Table 2: Top eight associations to three stimulus words as taken from the EAT.
The numbers of sub-jects responding with the respective word are given in brackets.2.3 Word relatednessThe third type of human intuitions which we consider concerns word relatedness.
Landauer & Dumais(1997) introduced a dataset for testing semantic relatedness, namely the synonym portion of the Test ofEnglish as a Foreign Language (TOEFL).
The TOEFL is an often obligatory test for non-nativespeakers of English who intend to study at a university with English as the teaching language.
Thedata used by Landauer & Dumais had been acquired from the Educational Testing Service and com-prises 80 test items.
As summarized in Rapp (2009), each item consists of a problem word embedded3In the so-called word recognition task test persons are presented strings of characters and their task is to decide whether ornot a string matches an English word.
It turns out that the average reaction time is inversely related to the familiarity of aword (i.e.
the less familiar a word, the longer the reaction time).4http://www.eat.rl.ac.uk/2119in a sentence and four alternative words, from which the test taker is asked to choose the one with themost similar meaning to the problem word.
For example, given the test sentence ?Both boats andtrains are used for transporting the materials?
and the four alternative words planes, ships, canoes,and railroads, the subject would be expected to choose the word ships, which is supposed to be theone most similar to boats.However, Landauer & Dumais (1997) did not use the test sentences.
Instead, only the lists of prob-lem words together with their alternatives were used.
A system capable of computing word relatednessshould be able to determine for each problem word the alternative word which comes closest in mean-ing.Although the TOEFL dataset has been widely used (see e.g.
the overview on related work on theACL Wiki5), there are two disadvantages with it: A minor one is that it is not freely available on theweb.
A more severe one is that it is rather small: This means that statistical variation is strong (whichwill be illustrated in section 5.3), and that overfitting can easily happen.
That is, a system trained onthis data may not well perform on other data.For this reason we decided to come up with a new dataset which avoids these problems.
It is basedon the index of Fernald's (1896) synonym and antonym dictionary as provided in the Project Guten-berg version.6 This index lists in alphabetical order English words together with their synonyms.
As inthe dictionary there is no indication as to the quality of a synonym, in order to avoid arbitrary selec-tions, from this list we removed all words for which several synonyms were listed in the index.
In asemi-automatic way, we also removed a number of other items, e.g.
those containing multiword unitsor numbers.
As a result, we obtained a list of 4050 words together with their synonyms.To obtain a dataset analogous to the TOEFL synonym set, we required three alternative words foreach item.
We could have used random words e.g.
taken from the vocabulary of the British NationalCorpus (BNC).
However, as the BNC is from a much later time period, this might have introduced asystematic bias.
So we thought we should better use the words from the synonym dictionary itself.Note that the synonyms corresponding to the 4050 words represent a much smaller vocabulary asmany of the synonyms are synonyms for several words.
For this reason, we used the headwords them-selves and applied the following procedure to generate the alternative words from them:1) We sorted our list of items according to the synonyms in alphabetical order.2) As the first column of alternative words, we used the given words but shifted them by 1000 posi-tions, i.e.
positions 1 to 3050 were matched with 1001 to 4050, and positions 3051 to 4050 wherematched with 1 to 1000.3) Analogous for the second column of alternative words, but here we shifted by 2000 positions.4) Same for the third column of alternative words, but here we shifted by 3000 positions.Word Synonym Alternative Wordsabandoned addicted rescind bliss receiptsabdicate abandon conflict indubitable archaicaberration insanity rational meliorate assuredabetter accessory carnal amicable urbaneabettor accessory imbruted brotherly policyabhorrence abomination kindliness supposition resignationabiding permanent remain life stanchability power chimerical frontier dietabject pitiful despotic blanch frayabjure abandon contest overt disusedTable 3: Ten entries from the synonym dataset derived from Fernald (1896).5http://aclweb.org/aclwiki/index.php?title=TOEFL_Synonym_Questions_(State_of_the_art)6http://www.gutenberg.org/files/28900/28900-h/28900-h.htm2120To give an impression of the dataset, its alphabetically first ten entries are shown in Table 3.
Let usnow quickly discuss some properties of the new dataset:  The pros are that it is about 50 times largerthan the TOEFL dataset and that it can be freely distributed.
The cons are that it is based on somewhatoutdated language (the dictionary was published in 1896) and that the alternative words were not care-fully selected but generated in a somewhat arbitrary fashion.
Also, it is not known how test personswould perform on this dataset, whereas for the TOEFL dataset human performance is known at leastfor some test takers, i.e.
non-native speakers of English.
A commonality between both datasets is thatthe synonyms were produced by experts, i.e.
reflect the experts?
language intuitions.3 CorporaAs in previous work (Rapp, 2014a) our corpus representativeness measure is to be applied to a numberof well known corpora.
These are:1) Brown Corpus (balanced corpus of 1 million words; Francis & Ku?era, 1989)2) British National Corpus (BNC; balanced corpus of 100 million words; Burnard & Aston, 1998)3) English Wikipedia (300 million words of encyclopaedic texts)74) ukWaC (British English web corpus of 2 billion words)85) English Gigaword Corpus 4th edition (4 billion words of newswire text)9Both the MRC familiarity norms and the EAT do not distinguish between uppercase and lowercasecharacters.
For this reason, we also did not make such a distinction and, in a pre-processing step, con-verted all corpora as well as the human data to lowercase only.For the results presented later we had to measure the size of our corpora and also of partial corpora.We do this by counting the number of running words.
Hereby, to avoid language specific sophistica-tions, we count as a word any string which is delimited by either white space (blanks, tabulator, newline) or by transitions between alpha and non-alpha characters.104 Procedure4.1 Corpus statistics concerning word familiarities (statistics of order zero)In the case of word familiarities the statistics extracted from the corpora are the log frequencies of thewords.
The MRC database contains familiarities for 4920 words.
As just two of them are multiwordunits, we considered this an inconsistency and removed them, so that 4918 words remained.Word Word frequency in the BNCWord familiarity inthe MRC databasea 2247100 632abandon 1316 510abandonment 500 359abasement 20 226abatement 137 294abbess 57 187abdication 124 284abdomen 303 426abduction 230 413aberration 149 208Table 4: BNC frequencies and MRC familiarities for the (alphabetically) first ten words covered in thefamiliarity norms of the MRC database.7We use the English part of the Wikipedia XML Corpus (Denoyer & Gallinary, 2006).
Although this is considerably smallerthan current versions, it has the advantage that it is an offline copy so that our results can be replicated.8http://wacky.sslmit.unibo.it/doku.php?id=corpora9http://catalog.ldc.upenn.edu/LDC2009T1310Alternatively, it would also be possible to simply count the number of characters for measuring corpus size (though thisseems less customary).
But word segmentation is required later on anyway (for computing the representativenesss scores).2121The two types of data, namely the word familiarities from the MRC database and the word fre-quencies as extracted from one of the corpora, were merged as exemplified in Table 4 for the case ofthe BNC.
Note that although the test subjects' familiarity judgements were originally on a scale be-tween 1 (not familiar) and 7 (highly familiar), to avoid decimal numbers when averaging results, allratings were multiplied by 100.
Computing corpus representativeness now simply involves taking thelogarithm of the frequencies in column 2, and then computing Pearson's correlation coefficient be-tween the resulting vector and column 3.
However, as especially for small corpora many of the wordfrequencies can be zero, and as the logarithm of zero is not defined, we applied the usual heuristic ofadding one to each frequency count before taking the logarithm.4.2 Corpus statistics concerning word associations (1st order statistics)As described in Rapp (2014c), we assume that there is a relationship between word associations ascollected from human subjects and word co-occurrences as observed in a corpus, and our hypothesis isthat the strength of this relationship can be used as a measure of corpus representativeness.
A corpusleading to simulated associations akin to the ones collected from humans is likely to be a good surro-gate for everyday language, although ?
similarly to what we said about word frequencies ?
word co-occurrence counts constitute only one of many properties of a corpus.For extracting word associations from corpora, in the literature many algorithms were described(e.g.
Wettler & Rapp, 1989; Church & Hanks, 1990; Wettler et al., 2005).
In analogy, we used the fol-lowing procedure: For all words with a BNC corpus frequency of 50 or higher we computed the co-occurrence vectors.
That is, each vector contains the number of co-occurrences of the stimulus wordwith all other co-occurring words.
It counts as a co-occurrence if two words appear together within adistance of at most ten words, i.e.
a text window of ?10 words around the stimulus word is considered.Hereby the exact distance within the window is not taken into account.In a further step an association measure was applied to the co-occurrence vectors, namely Ted Dun-ning's (1993) log-likelihood ratio.
The resulting vectors we call association vectors.
Given these vec-tors, the strongest association to a given stimulus word can be determined by simply looking for thehighest value within the respective association vector.
The corresponding word is considered to be theassociative response predicted by the system.
For the same stimulus words used in Table 2, Table 5shows some sample associations as computed using the British National Corpus.ABOVE CONSTELLAT?ON FEM?N?NEbelow (59) stars (39) masculine (26)level star (33) women (2)average (1) southern genderhigh (4) triangle woman (8)feet bright female (6)water planet (1) menhead rather male (1)see south  moreground find hairleft map softTable 5: Top ten corpus-derived associations for three stimulus words.
The numbers of subjects fromthe EAT responding with the respective word (if larger than zero) are given in brackets.Concerning evaluation, in principle the idea is to find matches between the human and the corpus-based associations.
One possibility is to simply count the number of cases where the primary associa-tive response matches the strongest corpus-based association.
However, when it comes to very smallcorpus sizes of e.g.
just 1000 words (see Section 5), the problem of data sparseness becomes so severethat a more tolerant evaluation method leads to more robust results less susceptible to statistical varia-tion.
This is why for measuring accuracy we count the number of cases where the respective primaryassociative response is listed within the top ten corpus-based associations, rather than insisting on a2122match with the strongest association.
This simple modification leads to improvements in reliabilitywhen measuring very low accuracies.4.3 Corpus statistics concerning word relatedness (2nd order statistics)Our algorithm for computing word relatedness consists of the following three steps:1) Counting word co-occurrences.2) Applying an association measure to the raw co-occurrence counts.3) Computing vector similarities.Steps 1 and 2 are in principle analogous to the previous subsection.
Only, as mentioned in Rapp(2009), for computing vector similarities it turns out that it is better to consider a smaller window size(such as ?1 or ?2 around the given word).
Also, we used a simpler association measure, namelylog(nij+1), whereby nij is the number of co-occurrences between words i  and j, as it slightly outper-formed the log-likelihood ratio in this particular setting.For step 3 (computing vector similarities) we use the standard cosine measure.
Table 6 shows someresults as obtained using the British National Corpus.
For a quantitative evaluation we utilized theTOEFL synonym data as follows: We compared our system's results to the answers as provided in theTOEFL dataset.
Remember that in the TOEFL synonym test the subjects had to choose the word mostsimilar to a given stimulus word from a list of four alternatives.
Accordingly, in the simulation, weassumed that the system made the right decision if the correct answer was ranked best among the fouralternatives.
In a further run, we applied exactly the same procedure to the test set derived from Fer-nald's synonym dictionary.burden responsibility (0.62), expense (0.61), expenditure (0.59), problem (0.59), cost (0.59)arrogant rude (0.62), naive (0.61), stupid (0.61), impatient (0.61), haughty (0.61)desperation panic (0.60), despair (0.60), exasperation (0.59), stillness (0.58), impatience (0.58)memorandum appendix (0.59), document (0.59), submission (0.57), constitution (0.57), disclosure (0.57)trivial unimportant (0.63), ridiculous (0.60), trifling (0.60), straightforward (0.60), bizarre (0.60)Table 6: Semantic similarities extracted from the BNC for five English words using only vocabularyfrom the synonym test set based on Fernald (1896).5 Results5.1 Results based on word familiaritiesThese results are given in Figure 1a.
There we find in graphical form for each of the five corpora thecomputed Pearson's correlation coefficients between the words' familiarities and their log corpus fre-quencies.
For easier comparison with the other results (which are percentages) we multiply these cor-relations by 100 and take the product as the familiarity-based representativeness of a corpus.
Therange of values can thus be between 0 and 100, whereby 0 denotes a complete lack of representative-ness, and 100 denotes perfect representativeness.
The representativeness scores are also computed forpartial corpora, whereby all parts have in common that they start with the beginning of the respectivecorpus.We can see in Fig.
1a that, as expected, the representativeness is almost zero if only the first 100words of a corpus are taken into account, and gradually increases to at least 67 for the full corpora.The horizontal axis has a logarithmic scale, but still the curves flatten with increasing corpus size, es-pecially above 1 million words.5.2 Results based on word associationsThese results are given in Fig.
1b.
For each of the five corpora (and their parts) the percentages ofprimary associative responses are given which ranked among the top ten in the corpus-based associa-tions.
These percentages we take as the association-based representativeness of the respective corpus.The range of values is between 0 and 100.2123(a) Familiarity                                                                            (b) Association(c) Relatedness                                                                                (d) AverageFig.
1: Results for the three approaches and their average.Fig.
2: Results for the TOEFL synonym data.21245.3 Results based on word relatednessThe respective results for the TOEFL synonym data (based on a window size of ?1 words) are givenin Fig.
2.
There we find for each of the five corpora the percentage of TOEFL questions which wereanswered correctly.
These percentages we take as the relatedness-based representativeness of the re-spective corpus.
Note that the level for very small corpora is higher here as in the TOEFL data with alimited number of candidate words there is a better chance to randomly hit the correct word.
As can beseen, the curves are somewhat erratic which is an indication that the test set of 80 items is too small.For this reason, we did not further use these results but replaced them with those from the synonymtest set derived from Fernald (1896).
The respective results are shown in Figure 1c.11 As can be seen,the much higher number of test items leads to smoother curves, but nevertheless the tendencies fromthe TOEFL data are roughly confirmed.5.4 Results based on the overall averageThe average of the curves in Fig.
1a to 1c is shown in Fig.
1d.
The motivation is that this way all threetypes of statistics are taken into account in a straightforward way.
The underlying reasoning is analo-gous to the BLEU score (Papineni et al., 2002) used in machine translation evaluation: There n-grammatches between a machine translation and a reference translation are counted separately for n-gramsof various lengths, and then the individual scores are combined.6 DiscussionIf we compare the curves in Figures 1a, 1b, and 1c it is apparent that the shapes are rather different.This can be explained by the order of the respective statistics: The familiarity-based approach usesstatistics of order zero (word frequencies), the association-based approach first order statistics (wordco-occurrences), and the relatedness-based approach second order statistics (common context).Although for all three methods a flattening of the curves can be expected for large corpora for thereason that there is an upper limit of corpus representativeness (100) leading to saturation, apparentlyfor the first and second order statistics larger corpora would be required to make this happen.Concerning very small partial corpora, for the familiarity based approach the curves quickly rise,whereas for the association-based and the relatedness-based approaches the increases in accuracy aresmall at the beginning.
This is also to be expected because in a partial corpus of e.g.
1000 words thereis still a chance to find a particular word, but there is almost no chance to find a particular co-occur-rence or a common context.So these discrepancies between the approaches are not a major surprise.
Of more interest is a com-parison of the results between the different corpora, i.e.
their relative performance for each of themethods.Following Rapp (2014a), concerning the representativeness of our five corpora and their parts, wehad tried to come up with some hypotheses before we started to compute the results.
These were ourpredictions:1) Representativeness should increase with corpus size.2) The Brown corpus and the BNC should be more representative than unbalanced corpora of thesame size.3) The Brown corpus (1 million words) should be more representative than the first million words ofthe British National Corpus as the latter is balanced only over its full size (100 million words), butnot over its first million words.4) For same sizes, we would expect ukWaC to be more representative than Wikipedia as we thinkthat corpus heterogeneity is a plus for representativeness.
ukWaC is obviously more heterogene-ous as, for example, it is multi genre multi topic whereas Wikipedia is single genre multi topic.5) The Gigaword Corpus should be the least representative for identical sizes.
Although, like Wiki-pedia, it is also single genre multi topic, the distribution of topics is not as wide because in news-ticker texts there are strong foci e.g.
on politics and sports.11As the Synonym-Dataset involves many very rare words, to reduce data sparseness we used a larger window size of ?2words to compute these results.2125If we compare these hypotheses to the actual results shown in Figures 1a, 1b, and 1c, the findings areas follows:Hypothesis 1, namely that the representativeness of all corpora steadily increases with corpus size,is clearly confirmed by all three approaches.Hypothesis 2, saying that the balanced corpora, namely the Brown corpus and the BNC, should bemore representative for their sizes than non-balanced corpora, is also confirmed by all approaches.
At1 million words, these two are the top performers.
At 100 million words, the BNC performs best.Note, however, that the smaller the corpus sizes, the less predictable the results as the sampling errorsincrease.Hypothesis 3 (Brown better than BNC for 1 million words) could sometimes be confirmed but notconsistently.
Instead, for all approaches the results of these two corpora are fairly close.
This indicatesthat the BNC also seems to have a fairly good balance over the first million words.
Concerning theassociation-based approach, the BNC also has the advantage that its British English should reflect theEAT associations (collected in Edinburgh) better than the American English of the Brown corpus.Hypothesis 4, namely that ukWaC is better than Wikipedia, is confirmed for the familiarity- and theassociation-based approach, but not for the relatedness-based approach.
Our explanation for the dis-crepancy is that the relatedness-data contains a larger proportion of outdated and rare words, and thatfor rare words the coverage of a corpus becomes more important.
In this respect, Wikipedia with itswide coverage of topics is likely to have an advantage over the ukWaC corpus.Hypothesis 5, saying that the Gigaword corpus should be the least representative, is confirmed foralmost all corpus sizes.Overall, several of our hypotheses were consistently confirmed by all approaches.
This finding pro-vides some evidence that the computed scores are actually related to what might sensibly be consid-ered as the representativeness of a corpus.Concerning the average representativeness score (Fig.
1d), we can conclude that overall it seems tomake sense to balance a corpus, and that corpus heterogeneity is a plus.7 Summary and outlookIn this work we defined the term corpus representativeness as the ability of a corpus to represent theaverage language use a native speaker encounters in everyday life.
As we cannot easily observe testpersons over years, our suggestion was to utilize human intuitions on word familiarities, on word as-sociations, and on word relatedness.Previous work has provided evidence that human word familiarities are based on word frequenciesin perceived language (Rapp, 2005), that human word associations are based on the co-occurrences ofwords (Wettler et al., 2005), and that human relatedness judgments are based on common context (cf.Harris' (1954) distributional hypothesis).
Although all of this may still be controversial, in the currentwork we took these findings for granted but turned round the perspective.
We said that a corpus is rep-resentative for the language environment of a group of persons if the word familiarities, the word as-sociations, and the predictions of word relatedness derived from it resemble these persons?
intuitions.For full and partial versions of five well known English corpora we computed the word familiari-ties, word associations, and word relatedness scores for test sets of several thousand words.
We then,for each corpus, compared the extracted information to the human data, and computed similarityscores which we took as measures of corpus representativeness.
We also computed a combined scoreby averaging the results from all three measures.A shortcoming of our approach is the following: Our measures are limited in so far as they onlyconsider three particular aspects of corpus representativeness, namely word familiarity word associa-tion, and word relatedness.
They do not explicitly consider higher level features e.g.
concerning syn-tax, semantics, pragmatics, or style.12 We nevertheless hope that what we described can serve as astarting point for further discussion.Concerning future work, a possible strait of research would be to modify the relatedness-based ap-proach in a way that the WordSimilarity-353 Test Collection13 could be used.
This test set provides12In section 5.4, when combining our three approaches, we already mentioned an analogy to the BLEU score.
A relatedcommonality is that the BLEU score also has these shortcomings.13http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/2126direct similarity estimates between words, so a correlation to corpus-derived estimates could be com-puted in analogy to what we did for the familiarity-based approach.We would also like to extend the approach to other corpus statistics which seem relevant for humanlanguage processing.
For example, we might look at associations when given several stimulus words(see Rapp, 2014b), or we could try to predict a word from its WordNet synset.
The latter would havethe advantage that WordNets are available for many languages, so the corpus representativeness scorescould be measured for a number of languages where other human data is scarce.Related to this would be the use of the Princeton evocation data14 which provides human similarityestimates between WordNet synsets.
The aim would be to replicate these similarities using multiwordassociations.AcknowledgementThis research was supported by a Marie Curie Intra European Fellowship within the 7th EuropeanCommunity Framework Programme.ReferencesBiber, D. (1993): Representativeness in Corpus Design.
Literary and Linguistic Computing, Vol.
8, Nov. 4, 243?257.Brisbaert, M.; New, B.
(2009).
Moving beyond Ku?era and Francis: A critical evaluation of current word fre-quency norms and the introduction of a new and improved word frequency measure for American English.Behavior Research Methods 41 (4), 977?990.Burnard, L.; Aston, G. (1998): The BNC Handbook: Exploring the British National Corpus with Sara.
Edin-burgh: University Press.Church, K.W.
; Hanks, P. (1990).
Word association norms, mutual information, and lexicography.
ComputationalLinguistics 16 (1), 22?29.Coltheart, M. (1981): The MRC psycholinguistic database.
Quarterly Journal of Experimental Psychology, 33A,497?505.Denoyer, L.; Gallinari, P. (2006): The Wikipedia XML Corpus.
SIGIR Forum, 40(1), 64?69.Dunning, T. (1993).
Accurate methods for the statistics of surprise and coincidence.
Computational Linguistics,19 (1), 61?74.Fernald, J.C. (1896).
English Synonyms and Antonyms, 19th edition.
New York and London: Funk & WagnallsCompany.Francis, W.N.
; Ku?era, H. (1989): Manual of Information to Accompany a Standard Corpus of Present-Day Ed-ited American English, for Use with Digital Computers.
Providence, R.I.: Brown University, Department ofLinguistics.Francom, J.;  Ussishkin, A.
(2011).
Converging methodologies: assessing corpus representativeness through psy-cholinguistic measures.
American Association for Corpus Linguistics.
Georgia State University, Atlanta, GA.http://francojc.files.wordpress.com/2010/01/aacl-2011-converging-methodologies.pdf.Gries, S.T.
(2010).
Dispersions and adjusted frequencies in corpora: further explorations.
In S.T.
Gries, S. Wulff,& M. Davies (eds.
): Corpus Linguistic Applications: Current Studies, New Directions.
Amsterdam: Rodopi,197?212.Harris, Z. S. (1954).
Distributional structure.
Word, 10(23), 146?162.Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J.
(1973).
An associative thesaurus of English and its computeranalysis.
In Aitken, A.J., Bailey, R.W.
and Hamilton-Smith, N.
(Eds.
): The Computer and Literary Studies.Edinburgh: University Press, 153?165.Landauer, T.K.
; Dumais, S.T.
(1997).
A solution to Plato's problem: The latent semantic analysis theory of ac-quisition, induction, and representation of knowledge.
Psychological Review 104 (2), 211?240.14http://wordnet.cs.princeton.edu/downloads.html2127Papineni, K.; Roukos, S.; Ward, T.; Zhu, W.-J.
(2002).
BLEU: a method for automatic evaluation of machinetranslation.
Proceedings of the 40th Annual Meeting of the ACL, Philadelphia, 311?318.Rapp, R. (2005): On the relationship between word frequency and word familiarity.
In: B. Fisseni; H.-C.Schmitz; B. Schr?der; P. Wagner (eds.
): Sprachtechnologie, mobile Kommunikation und linguistische Res-sourcen.
Beitr?ge zur GLDV-Tagung 2005 in Bonn.
Frankfurt: Peter Lang.
249?263.Rapp, R. (2009).
The automatic generation of thesauri of related words for English, French, German, and Rus-sian.
International Journal of Speech Technology 11 (3 ), 147?156.Rapp, R. (2011).
Language acquisition as the detection, memorization, and reproduction of statistical regularitiesin perceived language.
Journal of Cognitive Science, Vol.
12, No.
3, 297?322.Rapp, R. (2014a).
Using word familiarities and word associations to measure corpus representativeness.
Pro-ceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2014), Reyk-javik, Iceland.Rapp, R. (2014b).
Corpus-based computation of reverse associations.
Proceedings of the Ninth InternationalConference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland.Rapp, R. (2014c).
Using word association norms to measure corpus representativeness.
In: A. Gelbukh: Compu-tational Linguistics and Intelligent Text Processing.
15th International Conference, CICLING 2014, Kath-mandu, Nepal.
Berlin: Springer.
1?13.Saldanha, G. (2009): Principles of corpus linguistics and their application to translation studies research.
Tradu-m?tica 7: 1?7.Temnikova, I.; Baumgartner Jr., W.A.
; Hailu, N.D.; Nikolova, I.; McEnery, T.; Kilgarriff, A.; Angelova, G.;Cohen, K.B.
(2014).
Sublanguage Corpus Analysis Toolkit: a tool for assessing the representativeness andsublanguage characteristics of corpora.
Proceedings of the Ninth International Conference on Language Re-sources and Evaluation (LREC 2014), Reykjavik, Iceland.Wettler, M., Rapp, R. (1989).
A connectionist system to simulate lexical decisions in information retrieval.
In: R.Pfeifer, Z. Schreter, F. Fogelman, L. Steels (eds.
): Connectionism in Perspective.
Amsterdam: Elsevier, 463?469.Wettler, M.; Rapp, R.; Sedlmeier, P. (2005).
Free word associations correspond to contiguities between words intexts.
Journal of Quantitative Linguistics 12(2), 111?122.2128
