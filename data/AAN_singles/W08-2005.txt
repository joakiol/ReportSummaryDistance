Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 33?40Manchester, August 2008Graph-based Clustering for Semantic Classi?cation of OnomatopoeticWordsKenichi IchiokaInterdisciplinary Graduate School ofMedicine and EngineeringUniversity of Yamanashi, Japang07mk001@yamanashi.ac.jpFumiyo FukumotoInterdisciplinary Graduate School ofMedicine and EngineeringUniversity of Yamanashi, Japanfukumoto@yamanashi.ac.jpAbstractThis paper presents a method for seman-tic classi?cation of onomatopoetic wordslike ???????
(hum)?
and ???????
(clip clop)?
which exist in ev-ery language, especially Japanese beingrich in onomatopoetic words.
We useda graph-based clustering algorithm calledNewman clustering.
The algorithm cal-culates a simple quality function to testwhether a particular division is meaning-ful.
The quality function is calculatedbased on the weights of edges betweennodes.
We combined two different sim-ilarity measures, distributional similarity,and orthographic similarity to calculateweights.
The results obtained by usingthe Web data showed a 9.0% improvementover the baseline single distributional sim-ilarity measure.1 IntroductionOnomatopoeia which we call onomatopoetic word(ono word) is the formation of words whose soundis imitative of the sound of the noise or action des-ignated, such as ?hiss?
(McLeod, 1991).
It is oneof the linguistic features of Japanese.
Consider twosentences from Japanese.
(1) ??????????????????????????
?I?m too sleepy because I awoke to the slip-pers in the hall.?c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.
(2) ????????????????????????????????
?I?m too sleepy because I awoke to the pit-a-pat of slippers in the hall.
?Sentences (1) and (2) are almost the same sense.However, sentence (2) which includes ono word,?????
(pit-a-pat)?
is much better to makethe scene alive, or represents an image clearly.Therefore large-scale semantic resource of onowords is indispensable for not only NLP, butalso many semantic-oriented applications such asQuestion Answering, Paraphrasing, and MT sys-tems.
Although several machine-readable dictio-naries which are ?ne-grained and large-scale se-mantic knowledge like WordNet, COMLEX, andEDR dictionary exist, there are none or few ono-matopoetic thesaurus.
Because (i) it is easy to un-derstand its sense of ono word for Japanese, and(ii) it is a fast-changing linguistic expressions, asit is a vogue word.
Therefore, considering this re-source scarcity problem, semantic classi?cation ofono words which do not appear in the resource butappear in corpora is very important.In this paper, we focus on Japanese onomatopo-etic words, and propose a method for classifyingthem into a set with similar meaning.
We usedthe Web as a corpus to collect ono words, as theyappear in different genres of dialogues includingbroadcast news, novels and comics, rather than awell-edited, balanced corpus like newspaper arti-cles.
The problem using a large, heterogeneouscollection of Web data is that the Web counts arefar more noisy than counts obtained from textualcorpus.
We thus used a graph-based clustering al-gorithm, called Newman clustering for classify-ing ono words.
The algorithm does not simply cal-culate the number of shortest paths between pairsof nodes, but instead calculates a quality function33of how good a cluster structure found by an algo-rithm is, and thus makes the computation far moreef?cient.
The ef?cacy of the algorithm dependson a quality function which is calculated by us-ing the weights of edges between nodes.
We com-bined two different similarity measures, and usedthem to calculate weights.
One is co-occurrencebased distributional similarity measure.
We testedmutual information (MI) and a ?2 statistic as asimilarity measure.
Another is orthographic sim-ilarity which is based on a feature of ono wordscalled ?sound symbolism?.
Sound symbolism in-dicates that phonemes or phonetic sequences ex-press their senses.
As ono words imitate the soundsassociated with the objects or actions they refer to,their phonetic sequences provide semantic cluesfor classi?cation.
The empirical results are encour-aging, and showed a 9.0% improvement over thebaseline single distributional similarity measure.2 Previous WorkThere are quite a lot of work on semantic classi?-cation of words with corpus-based approach.
Theearliest work in this direction are those of (Hindle,1990), (Lin, 1998), (Dagan et al, 1999), (Chenand Chen, 2000), (Geffet and Dagan, 2004) and(Weeds and Weir, 2005).
They used distributionalsimilarity.
Similarity measures based on distribu-tional hypothesis compare a pair of weighted fea-ture vectors that characterize two words.
Featurestypically correspond to other words that co-occurwith the characterized word in the same context.Lin (1998) proposed a word similarity measurebased on the distributional pattern of words whichallows to construct a thesaurus using a parsed cor-pus.
He compared the result of automatically cre-ated thesaurus with WordNet and Roget, and re-ported that the result was signi?cantly closer toWordNet than Roget Thesaurus was.Graph representations for word similarity havealso been proposed by several researchers (Jan-nink and Wiederhold, 1999; Galley and McKeown,2003; Muller et al, 2006).
Sinha and Mihalcea(2007) proposed a graph-based algorithm for un-supervised word sense disambiguation which com-bines several semantic similarity measures includ-ing Resnik?s metric (Resnik, 1995), and algorithmsfor graph centrality.
They reported that the resultsusing the SENSEVAL-2 and SENSEVAL-3 En-glish all-words data sets lead to relative error ratereductions of 5 ?
8% as compared to the previouswork (Mihalcea, 2005).In the context of graph-based clustering ofwords, Widdows and Dorow (2002) used a graphmodel for unsupervised lexical acquisition.
Thegraph structure is built by linking pairs of wordswhich participate in particular syntactic relation-ships.
An incremental cluster-building algorithmusing the graph structure achieved 82% accuracy ata lexical acquisition task, evaluated against Word-Net 10 classes, and each class consists of 20 words.Matsuo et al (2006) proposed a method of wordclustering based on a word similarity measure byWeb counts.
They used Newman clustering forclustering algorithm.
They evaluated their methodusing two sets of word classes.
One is derived fromthe Web data, and another is from WordNet.1 Eachset consists of 90 noun words.
They reported thatthe results obtained by Newman clustering werebetter than those obtained by average-link agglom-erative clustering.
Our work is similar to theirmethod in the use of Newman clustering.
How-ever, they classi?ed Japanese noun words, whileour work is the ?rst to aim at detecting seman-tic classi?cation of onomatopoetic words.
More-over, they used only a single similarity metric, co-occurrence based similarity, while Japanese, espe-cially ?kanji?
characters of noun words provide se-mantic clues for classifying words.3 System DescriptionThe method consists of three steps: retrieving co-occurrences using the Web, calculating similaritybetween ono words, and classifying ono words byusing Newman clustering.3.1 Retrieving Co-occurrence using the WebOne criterion for calculating semantic similaritybetween ono words is co-occurrence based similar-ity.
We retrieved frequency of two ono words oc-curring together by using the Web search engine,Google.
The similarity between them is calcu-lated based on their co-occurrence frequency.
Likemuch previous work on semantic classi?cation ofthe lexicons, our assumption is that semanticallysimilar words appear in similar contexts.
A lotof strategies for searching words are provided inGoogle.
Of these we focused on two methods:Boolean search AND and phrase-based search.1They used WordNet hypernym information.
It consistsof 10 classes.
They assigned 90 Japanese noun words to eachclass.34When we use AND boolean search, i.e., (Oi Oj)where Oi and Oj are ono words, we can retrievethe number of documents which include both Oiand Oj .
In contrast, phrase-based search, i.e.,(?Oi Oj?)
retrieves documents which include twoadjacent words Oi and Oj .3.2 Similarity MeasuresThe second step is to calculate semantic similaritybetween ono words.
We combined two differentsimilarity measures: the co-occurrence frequencybased similarity and orthographic similarity mea-sures.3.2.1 Co-occurrence based SimilarityMeasureWe focused on two popular measures: the mu-tual information (MI) and ?2 statistics.1.
Mutual InformationChurch and Hanks (1990) discussed the useof the mutual information statistics as a wayto identify a variety of interesting linguisticphenomena, ranging from semantic relationsof the doctor/nurse type (content word/contentword) to lexico-syntactic co-occurrence prefer-ences between verbs and prepositions (contentword/function word).
Let Oi and Oj be ono wordsretrieved from the Web.
The mutual informationMI(Oi, Oj) is de?ned as:MI(Oi, Oj) = logSall ?
f(Oi, Oj)SOi ?
SOj, (1)where SOi =?k?Oallf(Oi, Ok), (2)Sall =?Oi?OallSOi .
(3)In Eq.
(1), f(Oi, Oj) refers to the frequency of Oiand Oj occurring together, and Oall is a set of allono words retrieved from the Web.2.
?2 statisticThe ?2(Oi, Oj) is de?ned as:?2(Oi, Oj) =f(Oi, Oj)?
E(Oi, Oj)E(Oi, Oj), (4)where E(Oi, Oj) = SOi ?SOjSall.
(5)SOi and Sall in Eq.
(5) refer to Eq.
(2) and (3),respectively.
A major difference between ?2 andMI is that the former is a normalized value.3.2.2 Orthographic Similarity MeasureOrthographic similarity has been widely usedin spell checking and speech recognition systems(Damerau, 1964).
Our orthographic similaritymeasure is based on a unit of phonetic sequence.The key steps of the similarity between two onowords is de?ned as:1.
Convert each ono word into phonetic se-quences.The ?hiragana?
characters of ono word areconverted into phonetic sequences by aunique rule.
Basically, there are 19 conso-nants and 5 vowels, as listed in Table 1.Table 1: Japanese consonants and vowelsConsonant ?, N, Q, h, hy, k, ky, m, my, n,ny, r, ry, s, sy, t, ty, w, yVowel a, i, u, e, oConsider phonetic sequences ?hyu-hyu-?
ofono word ????????
(hum).
It is seg-mented into 4 consonants ?hy?, ?-?, ?hy?
and?-?, and two vowels, ?u?
and ?u?.2.
Form a vector in n-dimensional space.Each ono word is represented as a vectorof consonants(vowels), where each dimen-sion of the vector corresponds to each con-sonant and vowel, and each value of the di-mension is frequencies of its correspondingconsonant(vowel).3.
Calculate orthographic similarity.The orthographic similarity between onowords, Oi and Oj is calculated based on theconsonant and vowel distributions.
We usedtwo popular measures, i.e., the cosine similar-ity, and ?-skew divergence.
The cosine mea-sures the similarity of the two vectors by cal-culating the cosine of the angle between vec-tors.
?-skew divergence is de?ned as:?div(x, y) = D(y || ?
?
x + (1?
?)
?
y),where D(x||y) refers to Kullback-Leiblerand de?ned as:D(x||y) =n?i=1xi ?
logxiyi.
(6)35Lee (1999) reported the best results with ?= 0.9.
We used the same value.
We de?ned asimilarity metric by combining co-occurrencebased and orthographic similarity measures2:Sim(Oi, Oj) =MI(Oi, Oj)?
(Cos(Oi, Oj) + 1) (7)3.3 The Newman Clustering AlgorithmWe classi?ed ono words collected from the WWW.Therefore, the clustering algorithm should be ef?-cient and effective even in the very high dimen-sional spaces.
For this purpose, we chose a graph-based clustering algorithm, called Newman clus-tering.
The Newman clustering is a hierarchicalclustering algorithm which is based on Networkstructure (Newman, 2004).
The network structureconsists of nodes within which the node-node con-nections are edges.
It produces some division ofthe nodes into communities, regardless of whetherthe network structure has any natural such divi-sion.
Here, ?community?
or ?cluster?
have in com-mon that they are groups of densely interconnectednodes that are only sparsely connected with the restof the network.
To test whether a particular divi-sion is meaningful a quality function Q is de?ned:Q =?i(eii ?
a2i )where eij is the sum of the weight of edges be-tween two communities i and j divided by the sumof the weight of all edges, and ai =?j eij , i.e., theexpected fraction of edges within the cluster.
Hereare the key steps of that algorithm:1.
Given a set of n ono words S = {O1, ?
?
?,On}.
Create a network structure which con-sists of nodes O1, ?
?
?, On, and edges.
Here,the weight of an edge between Oi and Ojis a similarity value obtained by Eq.
(7).
Ifthe ?network density?
of ono words is smallerthan the parameter ?, we cut the edge.
Here,?network density?
refers to a ratio selectedfrom the topmost edges.
For example, if it2When we used ?2 statistic as a co-occurrence based sim-ilarity, MI in Eq.
(7) is replaced by ?2.
In a similar way,Cos(Oi, Oj) is replaced by max?
?div(x, y), where maxis the maximum value among all ?div(x, y) values.was 0.9, we used the topmost 90% of alledges and cut the remains, where edges aresorted in the descending order of their simi-larity values.2.
Starting with a state in which each ono wordis the sole member of one of n communities,we repeatedly joined communities together inpairs, choosing at each step the join that re-sults in the greatest increase.3.
Suppose that two communities are mergedinto one by a join operation.
The change inQ upon joining two communities i and j isgiven by:%Qij = eij + eji ?
2aiaj= 2(eij ?
aiaj)4.
Apply step 3. to every pair of communities.5.
Join two communities such that"Q is maxi-mum and create one community.
If"Q < 0,go to step 7.6.
Re-calculate eij and ai of the joined commu-nity, and go to step 3.7.
Words within the same community are re-garded as semantically similar.The computational cost of the algorithm is knownas O((m + n)n) or O(n2), where m and n are thenumber of edges and nodes, respectively.4 Experiments4.1 Experimental SetupThe data for the classi?cation of ono words havebeen taken from the Japanese ono dictionary (Ono,2007) that consisted of 4,500 words.
Of these, weselected 273 words, which occurred at least 5,000in the document URLs from the WWW.
The min-imum frequency of a word was found to be 5,220,while the maximum was about 26 million.
Thesewords are classi?ed into 10 classes.
Word classesand examples of ono words from the dictionary arelisted in Table 2.?Id?
denotes id number of each class.
?Sense?refers to each sense of ono word within the sameclass, and ?Num?
is the number of words whichshould be assigned to each class.
Each word36Table 2: Onomatopoetic words and # of words in each classId Sense Num Onomatopoetic words1 laugh 63 ?????
(a,Q,h,a,Q,h,a),???
(a,h,a,h,a),???
(w,a,h,a,h,a)????
(a,h,a,a,h,a),???
(i,h,i,h,i),?????
(u,Q,s,i,Q,s,i), ?
?
?2 cry 34 ???
(a,?,N),????
(u,w,a,?,N),????
(a,N,a,N),????
(e,N,e,N)????
(u,r,u,u,r,u),????
(u,r,u,r,u,N),???(u,r,u,Q),???
(e,?,N), ?
?
?3 pain 34 ????
(i,k,a,i,k,a),????
(h,i,r,i,h,i,r,i),????
(k,a,s,i,k,a,s,i)????
(k,a,N,k,a,N), ?
?
?4 anger 33 ???(k,a,?,Q),???
(k,a,t,i,N),???
(k,a,t,u,N),??(k,a,Q),???
(k,a,Q,k,a),????
(k,a,m,i,k,a,m,i),????
(k,a,r,i,k,a,r,i),????
(k,a,N,k,a,N), ?
?
?5 spook 31 ???
(a,w,a,w,a),????
(u,ky,a,?),???
(k,a,?,N),??
(k,i,k,u)???(k,i,k,u,Q),???
(k,i,k,u,r,i),???
(k,i,k,u,N), ?
?
?6 panic 25 ????
(a,k,u,s,e,k,u),????
(a,t,a,h,u,t,a),??????
(a,Q,h,u,a,Q,h,u),????
(a,w,a,a,w,a)?
?
?7 bloodless 27 ???(k,a,k,u,Q),???(k,a,k,u,Q),????
(k,a,Q,k,a,r,i),????
(k,a,Q,k,u,r,i)???
(k,a,k,u,N),????
(ky,a,h,u,N),???
(ky,u,?
), ?
?
?8 deem 13 ????
(u,Q,t,o,r,i),????
(ky,u,?,N),???
(ky,u,N)????
(t,u,k,u,t,u,k,u), ?
?
?9 feel delight 6 ????
(u,s,i,u,s,i),??????
(ky,a,h,i,ky,a,h,i)????
(u,?,h,a,?,u,?,h,a),????
(h,o,i,h,o,i),????
(r,u,N,r,u,N), ?
?
?10 balk 7 ????
(i,s,i,i,s,i),????
(u,s,i,u,s,i),????
(o,s,u,o,s,u)????
(k,u,t,a,k,u,t,a),????
(m,o,s,i,m,o,s,i), ?
?
?Total 273marked with bracket denotes phonetic sequencesconsisting of consonants and vowels.We retrieved co-occurrences of ono wordsshown in Table 2 using the search engine, Google.We applied Newman clustering to the input words.For comparison, we implemented standard k-means which is often used as a baseline, as it isone of the simplest unsupervised clustering algo-rithms, and compared the results to those obtainedby our method.
We used Euclidean distance (L2norm) as a distance metric used in the k-means.For evaluation of classi?cation, we usedPrecision(Prec), Recall(Rec), and F -measurewhich is a measure that balances precision and re-call (Bilenko et al, 2004).
The precise de?nitionsof these measures are given below:Prec =#PairsCorrectlyPredictedInSamecluster#TotalPairsPredictedInSameCluster(8)Rec =#PairsCorrectlyPredictedInSameCluster#TotalPairsInSameCluster(9)F ?measure =2?
Prec?Rec(Prec + Rec)(10)4.2 ResultsThe results are shown in Table 3.
?Co-occ.
&Sounds?
in Data refers to the results obtained byour method.
?Co-occ.?
denotes the results ob-tained by a single measure, co-occurrence baseddistributional similarity measure, and ?Sounds?shows the results obtained by orthographic sim-ilarity.
???
in Table 3 shows a parameter ?used in the Newman clustering.3 Table 3 showsbest performance of each method against ?
val-ues.
The best result was obtained when we usedphrase-based search and a combined measure ofco-occurrence(MI) and sounds (cos), and F -scorewas 0.451.4.2.1 AND vs phrase-based searchTable 3 shows that overall the results usingphrase-based search were better than those ofAND search, and the maximum difference of F -score between them was 20.6% when we used acombined measure.
We note that AND booleansearch did not consider the position of a word ina document, while our assumption was that se-mantically similar words appeared in similar con-texts.
As a result, two ono words which werenot semantically similar were often retrieved byAND boolean search.
For example, consider twoantonymous words, ?a,h,a,h,a?
(grinning broadly)and ?w,a,?,N?
(Wah, Wah).
The co-occurrence fre-quency obtained by AND was 5,640, while that ofphrase-based search was only one.
The observa-tion shows that we ?nd phrase-based search to bea good choice.3In case of k-means, we used the weights which satis?esnetwork density.37Table 3: Classi?cation resultsData Algo.
Sim (Co-occ.)
Sim (Sounds) Search method ?
Prec Rec F # of clusters?2AND .050 .134 .799 .229 10cosPhrase .820 .137 .880 .236 10MIAND .050 .134 .562 .216 10k-meansPhrase .150 .190 .618 Grap 10?2AND .680 .134 .801 .229 10?divPhrase .280 .138 .882 .238 10MIAND .040 .134 .602 .219 10Co-occ.
& SoundsPhrase .140 .181 .677 .285 10?2AND .170 .182 .380 .246 9cosPhrase .100 .322 .288 .304 14MIAND .050 .217 .282 Grh- 13NewmanPhrase .080 .397 .520 Gh-b 7?2AND .130 .212 .328 .258 9?divPhrase .090 .414 .298 .347 17MIAND .090 .207 .325 .253 6Phrase .160 .372 .473 .417 8?2AND .460 .138 .644 .227 10k-means ?Phrase .110 .136 .870 .236 10MIAND .040 .134 .599 .219 10Co-occ.Phrase .150 .191 .588 .286 10?2AND .700 .169 .415 .240 8Newman ?Phrase .190 .301 .273 .286 14MIAND .590 .159 .537 .245 3Phrase .140 .275 .527 Gseb 5k-means ?cos ?
.050 .145 .321 .199 10Sounds?div ?
.020 .126 .545 .204 10Newman ?cos ?
.270 .151 .365 Grbs 4?div ?
.350 .138 .408 .206 3hGrGrd Clutin gCfoSmlunc ClSliOWlKIS nOCkWnTo examine the effectiveness of the combinedsimilarity measure, we used a single measure asa quality function of the Newman clustering, andcompared these results with those obtained by ourmethod.
As shown in Table 3, the results withcombining similarity measures improved overallperformance.
In the phrase-based search, for ex-ample, the F-score using a combined measure ?Co-occ(MI) & Sounds(cos)?
was 23.8% better thanthe baseline single measure ?Sounds(cos)?, and9.0% better a single measure ?Co-occ(MI)?.Figure 1 shows F-score by ?Co-occ(MI) &Sounds(cos)?
and ?Co-occ(MI)?
against changesin ?.
These curves were obtained by phrase-based search.
We can see from Figure 1 that theF-score by a combined measure ?Co-occ(MI) &Sounds(cos)?
was better than ?Co-occ(MI)?
with?
value ranged from .001 to .25.
One possible rea-son for the difference of F-score between them isthe edges selected by varying ?.
Figure 2 showsthe results obtained by each single measure, and acombined measure to examine how the edges se-lected by varying ?
affect overall performance, F-measure.
?Precision?
in Figure 2 refers to the ratioof correct ono word pairs (edges) divided by the to-tal number of edges.
Here, correct ono word pairswere created by using the Japanese ono dictionary,i.e., we extracted word pairs within the same senseof the dictionary.
Surprisingly, there were no sig-ni?cant difference between a combined measure?Co-occ(MI) & Sounds(cos)?
and a single mea-sure ?Co-occ(MI)?
curves, while the precision ofa single measure ?Sounds?
was constantly worsethan that obtained by a combined measure.
An-other possible reason for the difference of F-scoreis due to product of MI and Cos in Eq.
(7).
Fur-ther work is needed to analyze these results in de-tail.hGrGskFSnOuCgCy nASOuOitoWlK1SCWe examined the results obtained by standard k-means and Newman clustering algorithms.
As canbe seen clearly from Table 3, the results with New-man clustering were better than those of the stan-dard k-means at all search and similarity measures,especially the result obtained by Newman clus-tering showed a 16.2 % improvement over the k-means when we used Co-occ.
(MI) & Sounds(cos)& phrase-based search.
We recall that we used273 ono words for clustering.
However, Newmanclustering is applicable for a large number of nodesand edges without decreasing accuracy too much,as it does not simply calculate the number of short-380.10.150.20.250.30.350.40.450.50  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1F-measureLower bound valuesCo-occ(MI)Co-occ(MI) & Sounds(cos)Figure 1: F-score against ?
values0.10.150.20.250.30.350.40.450.50.550.60  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionLower bound valuesCo-occ(MI)Co-occ(MI) & Sounds(cos)Sounds(cos)Figure 2: Precision against ?
valuesest paths between pairs of nodes, but instead calcu-lates a simple quality function.
Quantitative eval-uation by applying the method to larger data fromthe Web is worth trying for future work.4.3 Qualitative Analysis of ErrorsFinally, to provide feedback for further devel-opment of our classi?cation approach, we per-formed a qualitative analysis of errors.
Con-sider the following clusters (the Newman outputfor Co-occ.
(MI), Sounds(cos) and phrase-basedsearch), where each parenthetic sequences denotesono word:A1: (t,o,Q) (t,o,Q,t,o) (t,o,Q,k,i,N,t,o,Q,k,i,N)A2: (o,h,o,h,o), (e,h,e,h,e), (h,e,h,e,h,e), (o,-,o,-)A3: (u,s,i,u,s,i), (m,o,s,i,m,o,s,i), (m,o,s,o,m,o,s,o)Three main error types were identi?ed:1.
Morphological idiosyncrasy: This wasthe most frequent error type, exempli?edin A1, where ?(t,o,Q,k,i,N,t,o,Q,k,i,N)?
(pain sense) was incorrectly clustered withother two words (laugh sense) merely be-cause orthographic similarity between themwas large, as the phonetics sequences of?(t,o,Q,k,i,N,t,o,Q,k,i,N)?
included ?t?
and?o?.2.
Sparse data: Many of the low frequency onowords performed poorly.
In A2, ?(o,-,o,-)?
(cry sense) was classi?ed with other threewords (laugh sense) because it occurred fewin our data.3.
Problems of polysemy: In A3,?(m,o,s,o,m,o,s,o)?
(pain sense) wasclustered with other two words (balk sense)of its gold standard class.
However, the onoword has another sense, balk sense when itco-occurred with action verbs.5 ConclusionWe have focused on onomatopoetic words, andproposed a method for classifying them into a setof semantically similar words.
We used a graph-based clustering algorithm, called Newman clus-tering with a combined different similarity mea-sures.
The results obtained by using the Webdata showed a 9.0% improvement over the base-line single distributional similarity measure.
Thereare number of interesting directions for future re-search.The distributional similarity measure we usedis the basis of the ono words, while other contentwords such as verbs and adverbs are also effectivefor classifying ono words.
In the future, we plan toinvestigate the use of these words and work on im-proving the accuracy of classi?cation.
As shownin Table 2, many of the ono words consist of du-plicative character sequences such as ?h?
and ?a?of ?a,h,a,h,a?, and ?h?
and ?i?
of ?i,h,i,h,i?.
More-over, characters which consist of ono words withinthe same class match.
For example, the hiraganacharacter ???
(h,a) frequently appears in laughsense class.
These observations indicate that in-tegrating edit-distance and our current similaritymeasure will improve overall performance.Another interesting direction is a problem ofpolysemy.
It clearly supports the classi?cationof (Ono, 2007) to insist that some ono wordsbelong to more than one cluster.
For example,?(i,s,o,i,s,o)?
has at least two senses, panic and feeldelight sense.
In order to accommodate this, we39should apply an appropriate soft clustering tech-nique (Tishby et al, 1999; Reichardt and Born-holdt, 2006; Zhang et al, 2007).AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful suggestions.
This material is sup-ported in part by the Grant-in-aid for the Japan So-ciety for the Promotion of Science(JSPS).ReferencesBilenko, M., S. Basu, and R. J. Mooney.
2004.
In-tegrating Constraints and Metric Learning in Semi-Supervised Clustering.
In Proc.
of 21st InternationalConference on Machine Learning, pages 81?88.Chen, K. J. and C. J. Chen.
2000.
Automatic Seman-tic Classi?cation for Chinese Unknown CompoundNouns.
In Proc.
of 38th Annual Meeting of the As-sociation for Computational Linguistics, pages 125?130.Church, K. and P. Hunks.
1990.
Word AssociationNorms, Mutual Information and Lexicography.
In InProc.
of 28th Annual Meeting of the Association forComputational Linguistics., pages 76?83.Dagan, I., L. Lee, and F. Pereira.
1999.
Similarity-based Models of Cooccurrence Probabilities.
Ma-chine Learning, 34(1-3):43?69.Damerau, F. 1964.
A Technique for Computer Detec-tion and Correction of Spelling Errors.
Communica-tions of the ACM, 7:171?176.Galley, M. and K. McKeown.
2003.
Improving WordSense Disambiguation in Lexical Chaining.
In Proc.of 19th International Joint Conference on Arti?cialIntelligence, pages 1486?1488.Geffet, M. and I. Dagan.
2004.
Feature Vector Qualityand Distributional Similarity.
In Proc.
of 20th Inter-national Conference on Computational Linguistics,pages 247?253.Hindle, D. 1990.
Noun Classi?cation from Predicate-argument Structures.
In Proc.
of 28th Annual Meet-ing of the Association for Computational Linguistics,pages 268?275.Jannink, J. and G. Wiederhold.
1999.
Thesaurus EntryExtraction from an On-line Dictionary.
In Proc.
ofFusion?99.Lee, L. 1999.
Measures of Distributional Similarity.
InProc.
of the 37th Annual Meeting of the Associationfor Computational Linguistics, pages 25?32.Lin, D. 1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proc.
of 36th Annual Meet-ing of the Association for Computational Linguis-tics and 17th International Conference on Compu-tational Linguistics, pages 768?773.Matsuo, Y., T. Sakaki, K. Uchiyama, and M. Ishizuka.2006.
Graph-based Word Clustering using a WebSearch Engine.
In Proc.
of 2006 Conference onEmpirical Methods in Natural Language Processing(EMNLP2006), pages 542?550.McLeod, W. T. 1991.
The COLLINS Dictionary andThesaurus.
HarperCollinsPublishers.Mihalcea, R. 2005.
Unsupervised Large VocabularyWord Sense Disambiguation with Graph-based Al-gorithms for Sequence Data Labeling.
InProc.
of theHuman Language Technology / Empirical Methodsin Natural Language PRocessing Conference, pages411?418.Muller, P., N. Hathout, and B. Gaume.
2006.
SynonymExtraction Using a Semantic Distance on a Dictio-nary.
In Proc.
of the Workshop on TextGraphs, pages65?72.Newman, M. E. J.
2004.
Fast algorithm for detectingcommunity structure in networks.
In Physics ReviewE, (69, 066133).Ono, M. 2007.
Nihongo Omomatope Jiten (inJapanese).
Shougakukan.Reichardt, J. and S. Bornholdt.
2006.
Statistical Me-chanics of Community Detection.
PHYICAL RE-VIEW E, (74):1?14.Resnik, P. 1995.
Using Information Content to Eval-uate Semantic Similarity in a Taxonomy.
In Proc.of 14th International Joint Conference on Arti?cialIntelligence, pages 448?453.Sinha, R. and R. Mihalcea.
2007.
UnsupervisedGraph-based Word Sense Disambiguation UsingMeasures of Word Semantic Similarity.
In Proc.of the IEEE International Conference on SemanticComputing, pages 46?54.Tishby, N., F. C. Pereira, and W. Bialek.
1999.
The In-formation Bottleneck Method.
In Proc.
of 37th An-nual Allerton Conference on Communication Con-trol and Computing, pages 368?377.Weeds, J. and D. Weir.
2005.
Co-occurrence Retrieval:A Flexible Framework for Lexical DistributionalSimilarity.
Computational Linguistics, 31(4):439?476.Widdows, D. and B. Dorow.
2002.
A Graph Model forUnsupervised Lexical Acquisition.
In Proc.
of 19thInternational conference on Computational Linguis-tics (COLING2002), pages 1093?1099.Zhang, S., R. S. Wang, and X. S. Zhang.
2007.
Iden-ti?cation of Overlapping Community Structure inComplex Networks using Fuzzy C-means Cluster-ing.
PHYSICA A, (374):483?490.40
