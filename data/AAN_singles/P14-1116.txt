Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1231?1240,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsComparing Multi-label Classification with Reinforcement Learning forSummarisation of Time-series DataDimitra Gkatzia, Helen Hastie, and Oliver LemonSchool of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh{dg106, h.hastie, o.lemon}@hw.ac.ukAbstractWe present a novel approach for automaticreport generation from time-series data, inthe context of student feedback genera-tion.
Our proposed methodology treatscontent selection as a multi-label (ML)classification problem, which takes as in-put time-series data and outputs a set oftemplates, while capturing the dependen-cies between selected templates.
We showthat this method generates output closer tothe feedback that lecturers actually gener-ated, achieving 3.5% higher accuracy and15% higher F-score than multiple simpleclassifiers that keep a history of selectedtemplates.
Furthermore, we compare aML classifier with a Reinforcement Learn-ing (RL) approach in simulation and usingratings from real student users.
We showthat the different methods have differentbenefits, with ML being more accurate forpredicting what was seen in the trainingdata, whereas RL is more exploratory andslightly preferred by the students.1 IntroductionSummarisation of time-series data refers to thetask of automatically generating text from vari-ables whose values change over time.
We con-sider the task of automatically generating feed-back summaries for students describing their per-formance during the lab of a Computer Sciencemodule over the semester.
Students?
learning canbe influenced by many variables, such as difficultyof the material (Person et al, 1995), other dead-lines (Craig et al, 2004), attendance in lectures(Ames, 1992), etc.
These variables have two im-portant qualities.
Firstly, they change over time,and secondly they can be dependent on or inde-pendent of each other.
Therefore, when generatingfeedback, we need to take into account all vari-ables simultaneously in order to capture potentialdependencies and provide more effective and use-ful feedback that is relevant to the students.In this work, we concentrate on content selec-tion which is the task of choosing what to say,i.e.
what information is to be included in a report(Reiter and Dale, 2000).
Content selection deci-sions based on trends in time-series data determinethe selection of the useful and important variables,which we refer to here as factors, that should beconveyed in a summary.
The decisions of factorselection can be influenced by other factors thattheir values are correlated with; can be based onthe appearance or absence of other factors in thesummary; and can be based on the factors?
be-haviour over time.
Moreover, some factors mayhave to be discussed together in order to achievesome communicative goal, for instance, a teachermight want to refer to student?s marks as a moti-vation for increasing the number of hours studied.We frame content selection as a simple classifi-cation task: given a set of time-series data, decidefor each template whether it should be includedin a summary or not.
In this paper, with the term?template?
we refer to a quadruple consisting of anid, a factor (bottom left of Table 1), a referencetype (trend, weeks, average, other) and surfacetext.
However, simple classification assumes thatthe templates are independent of each other, thusthe decision for each template is taken in isolationfrom the others, which is not appropriate for ourdomain.
In order to capture the dependencies inthe context, multiple simple classifiers can makethe decisions for each template iteratively.
Aftereach iteration, the feature space grows by 1 fea-ture, in order to include the history of the previoustemplate decisions.
Here, we propose an alterna-tive method that tackles the challenge of interde-pendent data by using multi-label (ML) classifica-tion, which is efficient in taking data dependencies1231Raw Datafactors week 2 week 3 ... week 10marks 5 4 ... 5hours studied 1 2 ... 3... ... ... ... ...Trends from Datafactors trend(1) marks (M) trend other(2) hours studied (HS) trend increasing(3) understandability (Und) trend decreasing(4) difficulty (Diff) trend decreasing(5) deadlines (DL) trend increasing(6) health issues (HI) trend other(7) personal issues (PI) trend decreasing(8) lectures attended (LA) trend other(9) revision (R) trend decreasingSummaryYour overall performance was excellentduring the semester.
Keep up the goodwork and maybe try some more challeng-ing exercises.
Your attendance was vary-ing over the semester.
Have a think abouthow to use time in lectures to improve yourunderstanding of the material.
You spent 2hours studying the lecture material onaverage.
You should dedicate more timeto study.
You seem to find the materialeasier to understand compared to thebeginning of the semester.
Keep up thegood work!
You revised part of the learn-ing material.
Have a think whether revis-ing has improved your performance.Table 1: The table on the top left shows an example of the time-series raw data for feedback generation.The table on the bottom left shows an example of described trends.
The box on the right presents a targetsummary (target summaries have been constructed by teaching staff).into account and generating a set of labels (in ourcase templates) simultaneously (Tsoumakas et al,2010).
ML classification requires no history, i.e.does not keep track of previous decisions, and thushas a smaller feature space.Our contributions to the field are as follows: wepresent a novel and efficient method for tacklingthe challenge of content selection using a ML clas-sification approach; we applied this method to thedomain of feedback summarisation; we present acomparison with an optimisation technique (Rein-forcement Learning), and we discuss the similari-ties and differences between the two methods.In the next section, we refer to the related workon Natural Language Generation from time-seriesdata and on Content Selection.
In Section 4.2, wedescribe our approach and we carry out a compar-ison with simple classification methods.
In Sec-tion 5, we present the evaluation setup and in Sec-tion 6 we discuss the results, obtained in simula-tion and with real students.
Finally, in Section 8,directions for future work are discussed.2 Related WorkNatural Language Generation from time-seriesdata has been investigated for various tasks suchas weather forecast generation (Belz and Kow,2010; Angeli et al, 2010; Sripada et al, 2004),report generation from clinical data (Hunter et al,2011; Gatt et al, 2009), narrative to assist childrenwith communication needs (Black et al, 2010) andaudiovisual debrief generation from sensor datafrom Autonomous Underwater Vehicles missions(Johnson and Lane, 2011).The important tasks of time-series data sum-marisation systems are content selection (what tosay), surface realisation (how to say it) and infor-mation presentation (Document Planning, Order-ing, etc.).
In this work, we concentrate on contentselection.
Previous methods for content selectioninclude Reinforcement Learning (Rieser et al,2010); multi-objective optimisation (Gkatzia etal., 2014); Gricean Maxims (Sripada et al, 2003);Integer Linear Programming (Lampouras and An-droutsopoulos, 2013); collective content selection(Barzilay and Lapata, 2004); interest scores as-signed to content (Androutsopoulos et al, 2013); acombination of statistical and template-based ap-proaches to NLG (Kondadadi et al, 2013); statis-tical acquisition of rules (Duboue and McKeown,2003) and the Hidden Markov model approach forContent Selection and ordering (Barzilay and Lee,2004).Collective content selection (Barzilay and La-pata, 2004) is similar to our proposed method inthat it is a classification task that predicts the tem-plates from the same instance simultaneously.
Thedifference between the two methods lies in that the1232collective content selection requires the considera-tion of an individual preference score (which is de-fined as the preference of the entity to be selectedor omitted, and it is based on the values of entityattributes and is computed using a boosting algo-rithm) and the identification of links between theentities with similar labels.
In contrast, ML clas-sification does not need the computation of linksbetween the data and the templates.
ML classi-fication can also apply to other problems whosefeatures are correlated, such as text classification(Madjarov et al, 2012), when an aligned dataset isprovided.ML classification algorithms have been dividedinto three categories: algorithm adaptation meth-ods, problem transformation and ensemble meth-ods (Tsoumakas and Katakis, 2007; Madjarovet al, 2012).
Algorithm adaptation approaches(Tsoumakas et al, 2010) extend simple classifi-cation methods to handle ML data.
For exam-ple, the k-nearest neighbour algorithm is extendedto ML-kNN by Zhang and Zhou (2007).
ML-kNN identifies for each new instance its k nearestneighbours in the training set and then it predictsthe label set by utilising the maximum a posteri-ori principle according to statistical informationderived from the label sets of the k neighbours.Problem transformation approaches (Tsoumakasand Katakis, 2007) transform the ML classifica-tion task into one or more simple classificationtasks.
Ensemble methods (Tsoumakas et al, 2010)are algorithms that use ensembles to perform MLlearning and they are based on problem transfor-mation or algorithm adaptation methods.
In thispaper, we applied RAkEL (Random k-labelsets)(Tsoumakas et al, 2010): an ensemble problemtransformation method, which constructs an en-semble of simple-label classifiers, where each onedeals with a random subset of the labels.Finally, our domain for feedback generation ismotivated by previous studies (Law et al, 2005;van den Meulen et al, 2010) who show that textsummaries are more effective in decision makingthan graphs therefore it is advantageous to providea summary over showing users the raw data graph-ically.
In addition, feedback summarisation fromtime-series data can be applied to the field of In-telligent Tutoring Systems (Gross et al, 2012).3 DataThe dataset consists of 37 instances referring tothe activities of 26 students.
For a few studentsthere is more than 1 instance.
An example of onesuch instance is presented in Table 1.
Each in-stance includes time-series information about thestudent?s learning habits and the selected tem-plates that lecturers used to provide feedback tothis student.
The time-series information includesfor each week of the semester: (1) the marksachieved at the lab; (2) the hours that the stu-dent spent studying; (3) the understandability ofthe material; (4) the difficulty of the lab exercisesas assessed by the student; (5) the number of otherdeadlines that the student had that week; (6) healthissues; (7) personal issues; (8) the number of lec-tures attended; and (9) the amount of revision thatthe student had performed.
The templates describethese factors in four different ways:1.
<trend>: referring to the trend of a fac-tor over the semester (e.g.
?Your performancewas increasing...?),2.
<weeks>: explicitly describing the factorvalue at specific weeks (e.g.
?In weeks 2, 3and 9...?),3.
<average>: considering the average of afactor value (e.g.
?You dedicated 1.5 hoursstudying on average...?
), and4.
<other>: mentioning other relevant infor-mation (e.g.
?Revising material will improveyour performance?
).For the corpus creation, 11 lecturers selected thecontent to be conveyed in a summary, given theset of raw data (Gkatzia et al, 2013).
As a result,for the same student there are various summariesprovided by the different experts.
This character-istic of the dataset, that each instance is associatedwith more than one solution, additionally moti-vates the use of multi-label classification, whichis concerned with learning from examples, whereeach example is associated with multiple labels.Our analysis of the dataset showed that thereare significant correlations between the factors, forexample, the number of lectures attended (LA)correlates with the student?s understanding of thematerial (Und), see Table 2.
As we will discussfurther in Section 5.1, content decisions are in-fluenced by the previously generated content, forexample, if the lecturer has previously mentionedhealth issues, mentioning hours studied has a highprobability of also being mentioned.1233Factor (1) M (2) HS (3) Und (4) Diff (5) DL (6) HI (7) PI (8) LA (9) R(1) M 1* 0.52* 0.44* -0.53* -0.31 -0.30 -0.36* 0.44* 0.16(2) HS 0.52* 1* 0.23 -0.09 -0.11 0.11 -0.29 0.32 0.47*(3) Und 0.44* 0.23 1* -0.54* 0.03 -0.26 0.12 0.60* 0.32(4) Diff -0.53* -0.09 -0.54* 1* 0.16 -0.06 0.03 -0.19 0.14(5) DL -0.31 -0.11 0.03 0.16 1* 0.26 0.24 -0.44* 0.14(6) HI -0.30 -0.11 -0.26 -0.06 0.26 1* 0.27 -0.50* 0.15(7) PI -0.36* -0.29 0.12 0.03 0.24 0.27 1* -0.46* 0.34*(8) LA 0.44* 0.32 0.60* -0.19 -0.44* -0.50* -0.46* 1* -0.12(9) R 0.16 0.47* 0.03 0.14 0.14 0.15 0.34* -0.12 1*Table 2: The table presents the Pearson?s correlation coefficients of the factors (* means p<0.05).4 MethodologyIn this section, the content selection task and thesuggested multi-label classification approach arepresented.
The development and evaluation of thetime-series generation system follows the follow-ing pipeline (Gkatzia et al, 2013):1.
Time-Series data collection from students2.
Template construction by Learning andTeaching (L&T) expert3.
Feedback summaries constructed by lectur-ers; random summaries rated by lecturers4.
Development of time-series generation sys-tems (Section 4.2, Section 5.3): ML system,RL system, Rule-based and Random system5.
Evaluation: (Section 5)- Offline evaluation (Accuracy and Reward)- Online evaluation (Subjective Ratings)4.1 The Content Selection TaskOur learning task is formed as follows: given aset of 9 time-series factors, select the content thatis most appropriate to be included in a summary.Content is regarded as labels (each template rep-resents a label) and thus the task can be thought ofas a classification problem.
As mentioned, thereare 4 ways to refer to a factor: (1) describing thetrend, (2) describing what happened in every timestamp, (3) mentioning the average and (4) makinganother general statement.
Overall, for all factorsthere are 29 different templates1.
An example ofthe input data is shown in Table 1.
There are twodecisions that need to be made: (1) whether to talkabout a factor and (2) in which way to refer to it.Instead of dealing with this task in a hierarchicalway, where the algorithm will first learn whetherto talk about a factor and then to decide how to1There are fewer than 36 templates, because for some fac-tors there are less than 4 possible ways of referring to them.refer to it, we transformed the task in order to re-duce the learning steps.
Therefore, classificationcan reduce the decision workload by deciding ei-ther in which way to talk about it, or not to talkabout a factor at all.4.2 The Multi-label Classification ApproachTraditional single-label classification is the task ofidentifying which label one new observation is as-sociated with, by choosing from a set of labels L(Tsoumakas et al, 2010).
Multi-label classifica-tion is the task of associating an observation witha set of labels Y ?
L (Tsoumakas et al, 2010).One set of factor values can result in varioussets of templates as interpreted by the differentexperts.
A ML classifier is able to make deci-sions for all templates simultaneously and cap-ture these differences.
The RAndom k-labELsets(RAkEL) (Tsoumakas et al, 2010) was appliedin order to perform ML classification.
RAkEL isbased on Label Powerset (LP), a problem transfor-mation method (Tsoumakas et al, 2010).
LP ben-efits from taking into consideration label correla-tions, but does not perform well when trained withfew examples as in our case (Tsoumakas et al,2010).
RAkEL overcomes this limitation by con-structing a set of LP classifiers, which are trainedwith different random subsets of the set of labels(Tsoumakas et al, 2010).The LP method transforms the ML task, intoone single-label multi-class classification task,where the possible set of predicted variables forthe transformed class is the powerset of labelspresent in the original dataset.
For instance, the setof labels L = {temp0, temp1, ...temp28} could betransformed to {temp0,1,2, temp28,3,17,...}.
Thisalgorithm does not perform well when consider-ing a large number of labels, due to the fact thatthe label space grows exponentially (Tsoumakas1234Classifier Accuracy Precision Recall F score(10-fold)Decision Tree (no history) *75.95% 67.56 75.96 67.87Decision Tree (with predicted history) **73.43% 65.49 72.05 70.95Decision Tree (with real history) **78.09% 74.51 78.11 75.54Majority-class (single label) **72.02% 61.73 77.37 68.21RAkEL (multi-label) (no history) 76.95% 85.08 85.94 85.50Table 3: Average, precision, recall and F-score of the different classification methods (T-test, * denotessignificance with p<0.05 and ** significance with p<0.01, when comparing each result to RAkEL).et al, 2010).
RAkEL tackles this problem by con-structing an ensemble of LP classifiers and train-ing each one on a different random subset of theset of labels (Tsoumakas et al, 2010).4.2.1 The Production Phase of RAkELThe algorithm was implemented using the MU-LAN Open Source Java library (Tsoumakas etal., 2011), which is based on WEKA (Witten andFrank, 2005).
The algorithm works in two phases:1. the production of an ensemble of LP algo-rithms, and2.
the combination of the LP algorithms.RAkEL takes as input the following parameters:(1) the numbers of iterations m (which is devel-oper specified and denotes the number of modelsthat the algorithm will produce), (2) the size of la-belset k (which is also developer specified), (3) theset of labels L, and (4) the training set D. Duringthe initial phase it outputs an ensemble of LP clas-sifiers and the corresponding k-labelsets.
A pseu-docode for the production phase is shown below:Algorithm 1 RAkEL production phase1 : I n p u t : i t e r a t i o n s m, k l a b e l s e t s ,l a b e l s L , t r a i n i n g d a t a D2 : f o r i =0 t o m3 : S e l e c t random k?
l a b e l s e t from L4 : T r a i n an LP on D5 : Add LP t o ensemble6 : end f o r7 : Outpu t : t h e ensemble o f LPswi th c o r r e s p o n d i n g k?
l a b e l s e t s4.2.2 The Combination PhaseDuring the combination phase, the algorithm takesas input the results of the production phase, i.e.the ensemble of LPs with the corresponding k-labelsets, the set of labels L, and the new instancex and it outputs the result vector of predicted la-bels for instance x.
During run time, RAkEL es-timates the average decision for each label in Land if the average is greater than a threshold t (de-termined by the developer) it includes the label inthe predicted labelset.
We used the standard pa-rameter values of t, k and m (t = 0.5, k = 3 andm equals to 58 (2*29 templates)).
In future, wecould perform parameter optimisation by using atechnique similar to (Gabsdil and Lemon, 2004).5 EvaluationFirstly, we performed a preliminary evaluation onclassification methods, comparing our proposedML classification with multiple iterated classifica-tion approaches.
The summaries generated by theML classification system are then compared withthe output of a RL system and two baseline sys-tems in simulation and with real students.5.1 Comparison with Simple ClassificationWe compared the RAkEL algorithm with single-label (SL) classification.
Different SL classifierswere trained using WEKA: JRip, Decision Trees,Naive Bayes, k-nearest neighbour, logistic regres-sion, multi-layer perceptron and support vectormachines.
It was found out that Decision Treesachieved on average 3% higher accuracy.
We,therefore, went on to use Decision Trees that usegeneration history in three ways.Firstly, for Decision Tree (no history), 29decision-tree classifiers were trained, one for eachtemplate.
The input of these classifiers were the9 factors and each classifier was trained in orderto decide whether to include a specific template ornot.
This method did not take into account otherselected templates ?
it was only based on the time-series data.Secondly, for Decision Tree (with predictedhistory), 29 classifiers were also trained, but thistime the input included the previous decisionsmade by the previous classifiers (i.e.
the history)1235as well as the set of time-series data in order toemulate the dependencies in the dataset.
For in-stance, classifier n was trained using the data fromthe 9 factors and the template decisions for tem-plates 0 to n?
1.Thirdly, for Decision Tree (with real his-tory), the real, expert values were used ratherthan the predicted ones in the history.
Theabove-mentioned classifiers are compared with,the Majority-class (single label) baseline, whichlabels each instance with the most frequent tem-plate.The accuracy, the weighted precision, theweighted recall, and the weighted F-score of theclassifiers are shown in Table 3.
It was found thatin 10-fold cross validation RAkEL performs sig-nificantly better in all these automatic measures(accuracy = 76.95%, F-score = 85.50%).
Remark-ably, ML achieves more than 10% higher F-scorethan the other methods (Table 3).
The averageaccuracy of the single-label classifiers is 75.95%(10-fold validation), compared to 73.43% of clas-sification with history.
The reduced accuracy ofthe classification with predicted history is due tothe error in the predicted values.
In this method,at every step, the predicted outcome was used in-cluding the incorrect decisions that the classifiermade.
The upper-bound accuracy is 78.09% cal-culated by using the expert previous decisions andnot the potentially erroneous predicted decisions.This result is indicative of the significance of therelations between the factors showing that the pre-dicted decisions are dependent due to existing cor-relations as discussed in Section 1, therefore thesystem should not take these decisions indepen-dently.
ML classification performs better becauseit does take into account these correlations and de-pendencies in the data.5.2 The Reinforcement Learning SystemReinforcement Learning (RL) is a machine learn-ing technique that defines how an agent learns totake optimal actions so as to maximise a cumu-lative reward (Sutton and Barto, 1998).
Contentselection is seen as a Markov Decision problemand the goal of the agent is to learn to take the se-quence of actions that leads to optimal content se-lection.
The Temporal Difference learning methodwas used to train an agent for content selection.Actions and States: The state consists of thetime-series data and the selected templates.
In or-der to explore the state space the agent selects afactor (e.g.
marks, deadlines etc.)
and then decideswhether to talk about it or not.Reward Function: The reward function reflectsthe lecturers?
preferences on summaries and isderived through linear regression analysis of adataset containing lecturer constructed summariesand ratings of randomly generated summaries.Specifically, it is the following cumulative multi-variate function:Reward = a+n?i=1bi?
xi+ c ?
lengthwhere X = {x1, x2, ..., xn} describes the com-binations of the data trends observed in the time-series data and a particular template.
a, b and c arethe regression coefficients, and their values varyfrom -99 to 221.
The value of xiis given by thefunction:xi=??????????
?1, the combination of a factor trendand a template type is includedin a summary0, if not.The RL system differs from the classificationsystem in the way it performs content selection.In the training phase, the agent selects a factor andthen decides whether to talk about it or not.
If theagent decides to refer to a factor, the template isselected in a deterministic way, i.e.
from the avail-able templates it selects the template that results inhigher expected cumulative future reward.5.3 The Baseline SystemsWe compared the ML system and the RL systemwith two baselines described below by measuringthe accuracy of their outputs, the reward achievedby the reward function used for the RL system,and finally we also performed evaluation with stu-dent users.
In order to reduce the confoundingvariables, we kept the ordering of content in allsystems the same, by adopting the ordering of therule-based system.
The baselines are as follows:1.
Rule-based System: generates summariesbased on Content Selection rules derived by work-ing with a L&T expert and a student (Gkatzia etal., 2013).2.
Random System: initially, selects a factorrandomly and then selects a template randomly,until it makes decisions for all factors.1236Time-Series Accuracy Reward Rating Mode (mean) Data SourceSummarisation SystemsMulti-label Classification 85% 65.4 7 (6.24) Lecturers?
constructed summariesReinforcement Learning **66% 243.82 8 (6.54) Lecturers?
ratings & summariesRule-based **65% 107.77 7, 8 (5.86) L&T expertRandom **45.2% 43.29 *2 (*4.37) RandomTable 4: Accuracy, average rewards (based on lecturers?
preferences) and averages of the means of thestudent ratings.
Accuracy significance (Z-test) with RAkEL at p<0.05 is indicated as * and at p<0.01as **.
Student ratings significance (Mann Whitney U test) with RAkEL at p<0.05 is indicated as *.6 ResultsEach of the four systems described above gener-ated 26 feedback summaries corresponding to the26 student profiles.
These summaries were evalu-ated in simulation and with real student users.6.1 Results in SimulationTable 4 presents the accuracy, reward, and modeof student rating of each algorithm when used togenerate the 26 summaries.
Accuracy was esti-mated as the proportion of the correctly classifiedtemplates to the population of templates.
In or-der to have a more objective view on the results,the score achieved by each algorithm using thereward function was also calculated.
ML clas-sification achieved significantly higher accuracy,which was expected as it is a supervised learningmethod.
The rule-based system and the RL sys-tem have lower accuracy compared to the ML sys-tem.
There is evidently a mismatch between therules and the test-set; the content selection rulesare based on heuristics provided by a L&T Expertrather than by the same pool of lecturers that cre-ated the test-set.
On the contrary, the RL is trainedto optimise the selected content and not to repli-cate the existing lecturer summaries, hence thereis a difference in accuracy.Accuracy measures how similar the generatedoutput is to the gold standard, whereas the rewardfunction calculates a score regarding how goodthe output is, given an objective function.
RL istrained to optimise for this function, and thereforeit achieves higher reward, whereas ML is trainedto learn by examples, therefore it produces out-put closer to the gold standard (lecturer?s producedsummaries).
RL uses exploration and exploitationto discover combinations of content that result inhigher reward.
The reward represents predictedratings that lecturers would give to the summary.The reward for the lecturers?
produced summariesis 124.62 and for the ML method is 107.77.
TheML classification system performed worse thanthis gold standard in terms of reward, which is ex-pected given the error in predictions (supervisedmethods learn to reproduce the gold standard).Moreover, each decision is rewarded with a dif-ferent value as some combinations of factors andtemplates have greater or negative regression coef-ficients.
For instance, the combination of the fac-tors ?deadlines?
and the template that correspondsto <weeks> is rewarded with 57.
On the otherhand, when mentioning the <average> difficultythe summary is ?punished?
with -81 (see descrip-tion of the reward function in Section 5.2).
Conse-quently, a single poor decision in the ML classifi-cation can result in much less reward.6.2 Subjective Results with Students37 first year computer science students partici-pated in the study.
Each participant was showna graphical representation of the time-series dataof one student and four different summaries gen-erated by the four systems (see Figure 1).
The or-der of the presented summaries was randomised.They were asked to rate each feedback summaryon a 10-point rating scale in response to the fol-lowing statement: ?Imagine you are the followingstudent.
How would you evaluate the followingfeedback summaries from 1 to 10?
?, where 10 cor-responds to the most preferred summary and 1 tothe least preferred.The difference in ratings between the ML clas-sification system, the RL system and the Rule-based system is not significant (see Mode (mean)in Table 4, p>0.05).
However, there is a trend to-wards the RL system.
The classification methodreduces the generation steps, by making the de-cision of the factor selection and the template se-lection jointly.
Moreover, the training time for theclassification method is faster (a couple of secondscompared to over an hour).
Finally, the student1237Figure 1: The Figure show the evaluation setup.
Students were presenting with the data in a graphicalway and then they were asked to evaluate each summary in a 10-point Rating scale.
Summaries displayedfrom left to right: ML system, RL, rule-based and random.significantly prefer all the systems over the ran-dom.7 SummaryWe have shown that ML classification for sum-marisation of our time-series data has an accuracyof 76.95% and that this approach significantly out-performs other classification methods as it is ableto capture dependencies in the data when mak-ing content selection decisions.
ML classifica-tion was also directly compared to a RL method.It was found that although ML classification isalmost 20% more accurate than RL, both meth-ods perform comparably when rated by humans.This may be due to the fact that the RL optimi-sation method is able to provide more varied re-sponses over time rather than just emulating thetraining data as with standard supervised learn-ing approaches.
Foster (2008) found similar re-sults when performing a study on generation ofemphatic facial displays.
A previous study byBelz and Reiter (2006) has demonstrated that au-tomatic metrics can correlate highly with humanratings if the training dataset is of high quality.In our study, the human ratings correlate well tothe average scores achieved by the reward func-tion.
However, the human ratings do not correlatewell to the accuracy scores.
It is interesting thatthe two methods that score differently on variousautomatic metrics, such as accuracy, reward, pre-cision, recall and F-score, are evaluated similarlyby users.The comparison shows that each method canserve different goals.
Multi-label classificationgenerates output closer to gold standard whereasRL can optimise the output according to a rewardfunction.
ML classification could be used whenthe goal of the generation is to replicate phenom-ena seen in the dataset, because it achieves highaccuracy, precision and recall.
However, opti-misation methods can be more flexible, providemore varied output and can be trained for differentgoals, e.g.
for capturing preferences of differentusers.12388 Future WorkFor this initial experiment, we evaluated with stu-dents and not with lecturers, since the students arethe recipients of feedback.
In future, we plan toevaluate with students?
own data under real cir-cumstances as well as with ratings from lecturers.Moreover, we plan to utilise the results from thisstudent evaluation in order to train an optimisationalgorithm to perform summarisation according tostudents?
preferences.
In this case, optimisationwould be the preferred method as it would not beappropriate to collect gold standard data from stu-dents.
In fact, it would be of interest to investi-gate multi-objective optimisation techniques thatcan balance the needs of the lecturers to conveyimportant content to the satisfaction of students.9 AcknowledgementsThe research leading to this work has re-ceived funding from the EC?s FP7 programme:(FP7/2011-14) under grant agreement no.
248765(Help4Mood).ReferencesCarole Ames.
1992.
Classrooms: Goals, structures,and student motivation.
Journal of Educational Psy-chology, 84(3):261?71.Ion Androutsopoulos, Gerasimos Lampouras, andDimitrios Galanis.
2013.
Generating natural lan-guage descriptions from owl ontologies: the nat-ural owl system.
Atrificial Intelligence Research,48:671?715.Gabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approachto generation.
In Conference on Empirical Methodsin Natural Language Processing (EMNLP).Regina Barzilay and Mirella Lapata.
2004.
Collec-tive content selection for concept-to-text generation.In Conference on Human Language Technology andEmpirical Methods in Natural Language Processing(HLT-EMNLP).Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics (HLT-NAACL).Anja Belz and Eric Kow.
2010.
Extracting parallelfragments from comparable corpora for data-to-textgeneration.
In 6th International Natural LanguageGeneration Conference (INLG).Anja Belz and Ehud Reiter.
2006.
Comparing auto-matic and human evaluation of nlg systems.
In 11thConference of the European Chapter of the Associa-tion for Computational Linguistics (ACL).Rolf Black, Joe Reddington, Ehud Reiter, NavaTintarev, and Annalu Waller.
2010.
Using NLG andsensors to support personal narrative for childrenwith complex communication needs.
In NAACLHLT 2010 Workshop on Speech and Language Pro-cessing for Assistive Technologies.Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,and Barry Gholson.
2004.
Affect and learning:an exploratory look into the role of affect in learn-ing with autotutor.
Journal of Educational Media,29:241?250.Pable Duboue and K.R.
McKeown.
2003.
Statisticalacquisition of content selection rules for natural lan-guage generation.
In Conference on Human Lan-guage Technology and Empirical Methods in Natu-ral Language Processing (EMNLP).Mary Ellen Foster.
2008.
Automated metrics thatagree with human judgements on generated outputfor an embodied conversational agent.
In 5th Inter-national Natural Language Generation Conference(INLG).Malte Gabsdil and Oliver Lemon.
2004.
Combiningacoustic and pragmatic features to predict recogni-tion performance in spoken dialogue systems.
In42nd Annual Meeting of the Association for Com-putational Linguistics (ACL).Albert Gatt, Francois Portet, Ehud Reiter, JamesHunter, Saad Mahamood, Wendy Moncur, and So-mayajulu Sripada.
2009.
From data to text in theneonatal intensive care unit: Using NLG technologyfor decision support and information management.AI Communications, 22: 153-186.Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-narthanam, and Oliver Lemon.
2013.
Generatingstudent feedback from time-series data using Rein-forcement Learning.
In 14th European Workshop inNatural Language Generation (ENLG).Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.2014.
Finding Middle Ground?
Multi-objectiveNatural Language Generation from time-series data.In 14th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL)(to appear).Sebastian Gross, Bassam Mokbel, Barbara Hammer,and Niels Pinkwart.
2012.
Feedback provisionstrategies in intelligent tutoring systems based onclustered solution spaces.
In J. Desel, J. M. Haake,and C. Spannagel, editors, Tagungsband der 10. e-Learning Fachtagung Informatik (DeLFI), numberP-207 in GI Lecture Notes in Informatics, pages 27?38.
GI.1239Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,Cindy Sykes, and D Westwater.
2011.
Bt-nurse:Computer generation of natural language shift sum-maries from complex heterogeneous medical data.American Medical Informatics Association, 18:621-624.Nicholas Johnson and David Lane.
2011.
Narrativemonologue as a first step towards advanced mis-sion debrief for AUV operator situational aware-ness.
In 15th International Conference on AdvancedRobotics.Ravi Kondadadi, Blake Howald, and Frank Schilder.2013.
A statistical nlg framework for aggregatedplanning and realization.
In 51st Annual Meet-ing of the Association for Computational Linguistics(ACL).Gerasimos Lampouras and Ion Androutsopoulos.2013.
Using integer linear programming in concept-to-text generation to produce more compact texts.
In51st Annual Meeting of the Association for Compu-tational Linguistics (ACL).Anna S. Law, Yvonne Freer, Jim Hunter, Robert H.Logie, Neil McIntosh, and John Quinn.
2005.
Acomparison of graphical and textual presentations oftime series data to support medical decision makingin the neonatal intensive care unit.
Journal of Clini-cal Monitoring and Computing, pages 19: 183?194.Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, andSaso Dzeroski.
2012.
An extensive experimen-tal comparison of methods for multi-label learning.Pattern Recognition, 45(9):3084?3104.Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, andArthur C. Graesser.
1995.
Pragmatics and peda-gogy: Conversational rules and politeness strategiesmay inhibit effective tutoring.
Journal of Cognitionand Instruction, 13(2):161-188.Ehud Reiter and Robert Dale.
2000.
Building natu-ral language generation systems.
Cambridge Uni-versity Press.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising information presentation for spoken dia-logue systems.
In 48th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL).Somayajulu Sripada, Ehud Reiter, Jim Hunter, and JinYu.
2003.
Generating english summaries of time se-ries data using the gricean maxims.
In 9th ACM in-ternational conference on Knowledge discovery anddata mining (SIGKDD).Somayajulu Sripada, Ehud Reiter, I Davy, andK Nilssen.
2004.
Lessons from deploying NLGtechnology for marine weather forecast text gener-ation.
In PAIS session of ECAI-2004:760-764.Richart Sutton and Andrew Barto.
1998.
Reinforce-ment learning.
MIT Press.Grigorios Tsoumakas and Ioannis Katakis.
2007.Multi-label classification: An overview.
Inter-national Journal Data Warehousing and Mining,3(3):1?13.Grigorios Tsoumakas, Ioannis Katakis, and IoannisVlahavas.
2010.
Random k-labelsets for multi-label classification.
IEEE Transactions on Knowl-edge and Data Engineering, 99(1):1079?1089.Grigorios Tsoumakas, Eleftherios Spyromitros-Xioufis, Josef Vilcek, and Ioannis Vlahavas.2011.
Mulan: A java library for multi-labellearning.
Journal of Machine Learning Research,12(1):2411?2414.Marian van den Meulen, Robert Logie, Yvonne Freer,Cindy Sykes, Neil McIntosh, and Jim Hunter.
2010.When a graph is poorer than 100 words: A com-parison of computerised natural language genera-tion, human generated descriptions and graphicaldisplays in neonatal intensive care.
In Applied Cog-nitive Psychology, 24: 77-89.Ian Witten and Eibe Frank.
2005.
Data mining: Practi-cal machine learning tools and techniques.
MorganKaufmann Publishers.Min-Ling Zhang and Zhi-Hua Zhou.
2007.
Ml-knn: Alazy learning approach to multi-label learning.
Pat-tern Recognition, 40(7):2038?2048.1240
