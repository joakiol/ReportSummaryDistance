Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 92?98,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsChimera ?
Three Heads for English-to-Czech TranslationOndr?ej Bojar and Rudolf Rosa and Ales?
TamchynaCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostranske?
na?me?st??
25, Prague, Czech Republicsurname@ufal.mff.cuni.czAbstractThis paper describes our WMT submis-sions CU-BOJAR and CU-DEPFIX, the lat-ter dubbed ?CHIMERA?
because it com-bines on three diverse approaches: Tec-toMT, a system with transfer at the deepsyntactic level of representation, factoredphrase-based translation using Moses, andfinally automatic rule-based correction offrequent grammatical and meaning errors.We do not use any off-the-shelf system-combination method.1 IntroductionTargeting Czech in statistical machine transla-tion (SMT) is notoriously difficult due to thelarge number of possible word forms and com-plex agreement rules.
Previous attempts to resolvethese issues include specific probabilistic models(Subotin, 2011) or leaving the morphological gen-eration to a separate processing step (Fraser et al2012; Marec?ek et al 2011).TectoMT (CU-TECTOMT, Galus?c?a?kova?
et al(2013)) is a hybrid (rule-based and statistical) MTsystem that closely follows the analysis-transfer-synthesis pipeline.
As such, it suffers from manyissues but generating word forms in proper agree-ments with their neighbourhood as well as thetranslation of some diverging syntactic structuresare handled well.
Overall, TectoMT sometimeseven ties with a highly tuned Moses configurationin manual evaluations, see Bojar et al(2011).Finally, Rosa et al(2012) describes Depfix, arule-based system for post-processing (S)MT out-put that corrects some morphological, syntacticand even semantic mistakes.
Depfix was able tosignificantly improve Google output in WMT12,so now we applied it on an open-source system.Our WMT13 system is thus a three-headedcreature where, hopefully: (1) TectoMT providesmissing word forms and safely handles some non-parallel syntactic constructions, (2) Moses ex-ploits very large parallel and monolingual data,and boosts better lexical choice, (3) Depfix at-tempts to fix severe flaws in Moses output.2 System DescriptionTectoMTMosescu-tectomtDepfixcu-bojarcu-depfix = ChimeraInputFigure 1: CHIMERA: three systems combined.CHIMERA is a sequential combination of threediverse MT systems as depicted in Figure 1.
Eachof the intermediate stages of processing has beensubmitted as a separate primary system for theWMT manual evalution, allowing for a more thor-ough analysis.Instead of an off-the-shelf system combinationtechnique, we use TectoMT output as synthetictraining data for Moses as described in Section 2.1and finally we process its output using rule-basedcorrections of Depfix (Section 2.2).
All steps di-rectly use the source sentence.2.1 Moses Setup for CU-BOJARWe ran a couple of probes with reduced trainingdata around the setup of Moses that proved suc-cessful in previous years (Bojar et al 2012a).2.1.1 Pre-processingWe use a stable pre-processing pipeline that in-cludes normalization of quotation marks,1 tok-enization, tagging and lemmatization with tools1We do not simply convert them to unpaired ASCII quotesbut rather balance them and use other heuristics to convertmost cases to the typographically correct form.92Case recaser lc?form utc stcBLEU 9.05 9.13 9.70 9.81Table 1: Letter Casingincluded in the Treex platform (Popel andZ?abokrtsky?, 2010).This year, we evaluated the end-to-end effect oftruecasing.
Ideally, English-Czech SMT should betrained on data where only names are uppercased(and neither the beginnings of sentences, nor all-caps headlines or exclamations etc).
For these ex-periments, we trained a simple baseline system on1 million sentence pairs from CzEng 1.0.Table 1 summarizes the final (case-sensitive!
)BLEU scores for four setups.
The standard ap-proach is to train SMT lowercase and apply a re-caser, e.g.
the Moses one, on the output.
Anotheroption (denoted ?lc?form?)
is to lowercase onlythe source side of the parallel data.
This moreor less makes the translation model responsiblefor identifying names and the language model foridentifying beginnings of sentences.The final two approaches attempt at ?truecas-ing?
the data, i.e.
the ideal lowercasing of ev-erything except names.
Our simple unsupervisedtruecaser (?utc?)
uses a model trained on monolin-gual data (1 million sentences in this case, sameas the parallel training data used in this experi-ment) to identify the most frequent ?casing shape?of each token type when it appears within a sen-tence and then converts its occurrences at the be-ginnings of sentences to this shape.
Our super-vised truecaser (?stc?)
casts the case of the lemmaon the form, because our lemmatizers for Englishand Czech produce case-sensitive lemmas to indi-cate names.
After the translation, only determinis-tic uppercasing of sentence beginnings is needed.We confirm that ?stc?
as we have been using itfor a couple of years is indeed the best option, de-spite its unpleasingly frequent omissions of names(incl.
?Spojene?
sta?ty?, ?the United States?).
Oneof the rules in Depfix tries to cast the case fromthe source to the MT output but due to alignmenterrors, it is not perfect in fixing these mistakes.Surprisingly, the standard recasing workedworse than ?lc?form?, suggesting that two Mosesruns in a row are worse than one joint search.We consider using a full-fledged named entityrecognizer in the future.Tokens [M]Corpus Sents [M] English CzechCzEng 1.0 14.83 235.67 205.17Europarl 0.65 17.61 15.00Common Crawl 0.16 4.08 3.63Table 2: Basic Statistics of Parallel Data.2.1.2 Factored Translation for MorphologicalCoherenceWe use a quite standard factored configuration ofMoses.
We translate from ?stc?
to two factors:?stc?
and ?tag?
(full Czech positional morpholog-ical tag).
Even though tags on the target side makethe data somewhat sparser (a single Czech wordform typically represents several cases, numbersor genders), we do not use any back-off or alterna-tive decoding path.
A high-order language modelon tags is used to promote grammatically correctand coherent output.
Our system is thus less proneto errors in local morphological agreement.2.1.3 Large Parallel DataThe main source of our parallel data was CzEng1.0 (Bojar et al 2012b).
We also used Europarl(Koehn, 2005) as made available by WMT13 orga-nizers.2 The English-Czech part of the new Com-mon Crawl corpus was quite small and very noisy,so we did not include it in our training data.
Ta-ble 2 provides basic statistics of the data.Processing large parallel data can be challeng-ing in terms of time and computational resourcesrequired.
The main bottlenecks are word align-ment and phrase extraction.GIZA++ (Och and Ney, 2000) has been thestandard tool for computing word alignment inphrase-based MT.
A multi-threaded version exists(Gao and Vogel, 2008), which also supports incre-mental extensions of parallel data by applying asaved model on a new sentence pair.
We evaluatedthese tools and measured their wall-clock time3 aswell as the final BLEU score of a full MT system.Surprisingly, single-threaded GIZA++ was con-siderably faster than single-threaded MGIZA.
Us-ing 12 threads, MGIZA outperformed GIZA++but the difference was smaller than we expected.Table 3 summarizes the results.
We checked thedifference in BLEU using the procedure by Clarket al(2011) and GIZA++ alignments were indeed2http://www.statmt.org/wmt13/translation-task.html3Time measurements are only indicative, they were af-fected by the current load in our cluster.93Alignment Wallclock Time BLEUGIZA++ 71 15.5MGIZA 1 thread 114 15.4MGIZA 12 threads 51 15.4Table 3: Rough wallclock time [hours] of wordalignment and the resulting BLEU scores.Corpus Sents [M] Tokens [M]CzEng 1.0 14.83 205.17CWC Articles 36.72 626.86CNC News 28.08 483.88CNA 47.00 830.32Newspapers 64.39 1040.80News Crawl 24.91 444.84Total 215.93 3631.87Table 4: Basic Statistics of Monolingual Data.little but significantly better than MGIZA in threeMERT runs.We thus use the standard GIZA++ aligner.2.1.4 Large Language ModelsWe were able to collect a very large amount ofmonolingual data for Czech: almost 216 millionsentences, 3.6 billion tokens.
Table 4 lists thecorpora we used.
CWC Articles is a section ofthe Czech Web Corpus (Spoustova?
and Spousta,2012).
CNC News refers to a subset of the CzechNational Corpus4 from the news domain.
CNAis a corpus of Czech News Agency stories from1998 to 2012.
Newspapers is a collection of ar-ticles from various Czech newspapers from years1998 to 2002.
Finally, News Crawl is the mono-lingual corpus made available by the organizers ofWMT13.We created an in-domain language model fromall the corpora except for CzEng (where we onlyused the news section).
We were able to train a 4-gram language model using KenLM (Heafield etal., 2013).
Unfortunately, we did not manage touse a model of higher order.
The model file (evenin the binarized trie format with probability quan-tization) was so large that we ran out of memoryin decoding.5 We also tried pruning these largermodels but we did not have enough RAM.To cater for a longer-range coherence, wetrained a 7-gram language model only on the NewsCrawl corpus (concatenation of all years).
In thiscase, we used SRILM (Stolcke, 2002) and prunedn-grams so that (training set) model perplexity4http://korpus.cz/5Due to our cluster configuration, we need to pre-load lan-guage models.Token Order Sents Tokens ARPA.gz Trie[M] [M] [GB] [GB]stc 4 201.31 3430.92 28.2 11.8stc 7 24.91 444.84 13.1 8.1tag 10 14.83 205.17 7.2 3.0Table 5: LMs used in CU-BOJAR.does not increase more than 10?14.
The data forthis LM exactly match the domain of WMT testsets.Finally, we model sequences of morphologicaltags on the target side using a 10-gram LM es-timated from CzEng.
Individual sections of thecorpus (news, fiction, subtitles, EU legislation,web pages, technical documentation and Navajoproject) were interpolated to match WMT test setsfrom 2007 to 2011 best.
This allows even out-of-domain data to contribute to modeling of overallsentence structure.
We filtered the model using thesame threshold 10?14.Table 5 summarizes the resulting LM files asused in CU-BOJAR and CHIMERA.2.1.5 Bigger Tuning SetsKoehn and Haddow (2012) report benefits fromtuning on a larger set of sentences.
We experi-mented with a down-scaled MT system to com-pare a couple of options for our tuning set: thedefault 3003 sentences of newstest2011, the de-fault and three more Czech references that werecreated by translating from German, the defaultand two more references that were created by post-editing a variant of our last year?s Moses systemand also a larger single-reference set consistingof several newstest years.
The preliminary re-sults were highly inconclusive: negligibly higherBLEU scores obtained lower manual scores.
Un-able to pick the best configuration, we picked thelargest.
We tune our systems on ?bigref?, as spec-ified in Table 6.
The dataset consists of 11583source sentences, 3003 of which have 4 referencetranslations and a subset (1997 sents.)
of whichhas 2 reference translations constructed by post-editing.
The dataset does not include 2010 data asa heldout for other foreseen experiments.2.1.6 Synthetic Parallel DataGalus?c?a?kova?
et al(2013) describe several possi-bilities of combining TectoMT and phrase-basedapproaches.
Our CU-BOJAR uses one of the sim-pler but effective ones: adding TectoMT output onthe test set to our training data.
As a contrast to94English Czech # Refs # Sntsnewstest2011 official + 3 more from German 4 3003newstest2011 2 post-edits of a system 2 1997similar to (Bojar et al 2012a)newstest2009 official 1 2525newstest2008 official 1 2051newstest2007 official 1 2007Total 4 11583Table 6: Our big tuning set (bigref).CU-BOJAR, we also examine PLAIN Moses setupwhich is identical but lacks the additional syn-thetic phrase table by TectoMT.In order to select the best balance betweenphrases suggested by TectoMT and our paralleldata, we provide these data as two separate phrasetables.
Each phrase table brings in its own five-tuple of scores, one of which, the phrase-penaltyfunctions as an indicator how many phrases comefrom which of the phrase tables.
The standardMERT is then used to optimize the weights.6,7We use one more trick compared toGalus?c?a?kova?
et al(2013): we deliberatelyoverlap our training and tuning datasets.
Whenpreparing the synthetic parallel data, we use theEnglish side of newstests 08 and 10?13.
TheCzech side is always produced by TectoMT.
Wetune on bigref (see Table 6), so the years 08, 11and 12 overlap.
(We could have overlapped alsoyears 07, 09 and 10 but we had them originallyreserved for other purposes.)
Table 7 summarizesthe situation and highlights that our setup is fair:we never use the target side of our final evaluationset newstest2013.
Some test sets are denoted?could have?
as including them would still becorrect.The overlap allows MERT to estimate how use-ful are TectoMT phrases compared to the standardphrase table not just in general but on the spe-cific foreseen test set.
This deliberate overfittingto newstest 08, 11 and 12 then helps in translatingnewstest13.This combination technique in its current stateis rather expensive as a new phrase table is re-quired for every new input document.
However,if we fix the weights for the TectoMT phrase ta-6Using K-best batch MIRA (Cherry and Foster, 2012) didnot work any better in our setup.7We are aware of the fact that Moses alternative decodingpaths (Birch and Osborne, 2007) with similar phrase tablesclutter n-best lists with identical items, making MERT lessstable (Eisele et al 2008; Bojar and Tamchyna, 2011).
Theissue was not severe in our case, CU-BOJAR needed 10 itera-tions compared to 3 iterations needed for PLAIN.Used inTest Set Training Tuning Final Evalnewstest07 could have en+cs ?newstest08 en+TectoMT en+cs ?newstest09 could have en+cs ?newstest10 en+TectoMT could have ?newstest11 en+TectoMT en+cs ?newstest12 en+TectoMT en+cs ?newstest13 en+TectoMT ?
en+csTable 7: Summary of test sets usage.
?en?
and?cs?
denote the official English and Czech sides,resp.
?TectoMT?
denotes the synthetic Czech.ble, we can avoid re-tuning the system (whetherthis would degrade translation quality needs to beempirically evaluated).
Moreover, if we use a dy-namic phrase table, we could update it with Tec-toMT outputs on the fly, thus bypassing the needto retrain the translation model.2.2 DepfixDepfix is an automatic post-editing tool for cor-recting errors in English-to-Czech SMT.
It is ap-plied as a post-processing step to CU-BOJAR, re-sulting in the CHIMERA system.
Depfix 2013 is animprovement of Depfix 2012 (Rosa et al 2012).Depfix focuses on three major types of languagephenomena that can be captured by employing lin-guistic knowledge but are often hard for SMT sys-tems to get right:?
morphological agreement, such as:?
an adjective and the noun it modifies have toshare the same morphological gender, num-ber and case?
the subject and the predicate have to agree inmorphological gender, number and person, ifapplicable?
transfer of meaning in cases where the samemeaning is expressed by different grammaticalmeans in English and in Czech, such as:?
a subject in English is marked by being a leftmodifier of the predicate, while in Czech asubject is marked by the nominative morpho-logical case?
English marks possessiveness by the preposi-tion ?of?, while Czech uses the genitive mor-phological case?
negation can be marked in various ways inEnglish and Czech?
verb-noun and noun-noun valency?see (Rosaet al 2013)Depfix first performs a complex lingustic anal-95System BLEU TER WMT RankingAppraise MTurkCU-TECTOMT 14.7 0.741 0.455 0.491CU-BOJAR 20.1 0.696 0.637 0.555CU-DEPFIX 20.0 0.693 0.664 0.542PLAIN Moses 19.5 0.713 ?
?GOOGLE TR.
?
?
0.618 0.526Table 8: Overall results.ysis of both the source English sentence and itstranslation to Czech by CU-BOJAR.
The anal-ysis includes tagging, word-alignment, and de-pendency parsing both to shallow-syntax (?analyt-ical?)
and deep-syntax (?tectogrammatical?)
de-pendency trees.
Detection and correction of errorsis performed by rule-based components (the va-lency corrections use a simple statistical valencymodel).
For example, if the adjective-noun agree-ment is found to be violated, it is corrected byprojecting the morphological categories from thenoun to the adjective, which is realized by chang-ing their values in the Czech morphological tagand generating the appropriate word form from thelemma-tag pair using the rule-based generator ofHajic?
(2004).Rosa (2013) provides details of the current ver-sion of Depfix.
The main additions since 2012 arevalency corrections and lost negation recovery.3 Overall ResultsTable 8 reports the scores on the WMT13 testset.
BLEU and TER are taken from the evalu-ation web site8 for the normalized outputs, caseinsensitive.
The normalization affects typeset-ting of punctuation only and greatly increasesautomatic scores.
?WMT ranking?
lists resultsfrom judgments from Appraise and MechanicalTurk.
Except CU-TECTOMT, the manual evalua-tion used non-normalized MT outputs.
The fig-ure is the WMT12 standard interpretation as sug-gested by Bojar et al(2011) and says how oftenthe given system was ranked better than its com-petitor across all 18.6k non-tying pairwise com-parisons extracted from the annotations.We see a giant leap from CU-TECTOMT to CU-BOJAR, confirming the utility of large data.
How-ever, CU-TECTOMT had something to offer since itimproved over PLAIN, a very competitive baseline,by 0.6 BLEU absolute.
Depfix seems to slightlyworsen BLEU score but slightly improve TER; the8http://matrix.statmt.org/System # Tokens % TokensAll 22920 76.44Moses 3864 12.89TectoMT 2323 7.75Other 877 2.92Table 9: CHIMERA components that contribute?confirmed?
tokens.System # Tokens % TokensNone 21633 79.93Moses 2093 7.73TectoMT 2585 9.55Both 385 1.42CU-BOJAR 370 1.37Table 10: Tokens missing in CHIMERA output.manual evaluation is similarly indecisive.4 Combination AnalysisWe now closely analyze the contributions ofthe individual engines to the performance ofCHIMERA.
We look at translations of the new-stest2013 sets produced by the individual systems(PLAIN, CU-TECTOMT, CU-BOJAR, CHIMERA).We divide the newstest2013 reference tokensinto two classes: those successfully produced byCHIMERA (Table 9) and those missed (Table 10).The analysis can suffer from false positives as wellas false negatives, a ?confirmed?
token can violatesome grammatical constraints in MT output andan ?unconfirmed?
token can be a very good trans-lation.
If we had access to more references, theissue of false negatives would decrease.Table 9 indicates that more than 3/4 of to-kens confirmed by the reference were availablein all CHIMERA components: PLAIN Moses, CU-TECTOMT alone but also in the subsequent combi-nations CU-BOJAR and the final CU-DEPFIX.PLAIN Moses produced 13% tokens that Tec-toMT did not provide and TectoMT outputroughly 8% tokens unknown to Moses.
However,note that it is difficult to distinguish the effect ofdifferent model weights: PLAIN might have pro-duced some of those tokens as well if its weightswere different.
The row ?Other?
includes caseswhere e.g.
Depfix introduced a confirmed tokenthat none of the previous systems had.Table 10 analyses the potential of CHIMERAcomponents.
These tokens from the referencewere not produced by CHIMERA.
In almost 80%of cases, the token was not available in any 1-bestoutput; it may have been available in Moses phrase96tables or the input sentence.TectoMT offered almost 10% of missed tokens,but these were not selected in the subsequent com-bination.
The potential of Moses is somewhatlower (about 8%) because our phrase-based com-bination is likely to select wordings that score wellin a phrase-based model.
385 tokens were sug-gested by both TectoMT and Moses alone, but thecombination in CU-BOJAR did not select them, andfinally 370 tokens were produced by the combina-tion while they were not present in 1-best output ofneither TectoMT nor Moses.
Remember, all thesetokens eventually did not get to CHIMERA output,so Depfix must have changed them.4.1 Depfix analysisTable 11 analyzes the performance of the individ-ual components of Depfix.
Each evaluated sen-tence was either modified by a Depfix component,or not.
If it was modified, its quality could havebeen evaluated as better (improved), worse (wors-ened), or the same (equal) as before.
Thus, we canevaluate the performance of the individual compo-nents by the following measures:9precision = #improved#improved+#worsened (1)impact = #modified#evaluated (2)useless = #equal#modified (3)Please note that we make an assumption that ifa sentence was modified by multiple Depfix com-ponents, they all have the same effect on its qual-ity.
While this is clearly incorrect, it is impossibleto accurately determine the effect of each individ-ual component with the evaluation data at hand.This probably skews especially the reported per-formance of ?high-impact?
components, which of-ten operate in combination with other components.The evaluation is computed on 871 hits in whichCU-BOJAR and CHIMERA were compared.The results show that the two newest compo-nents ?
Lost negation recovery and Valency model?
both modify a large number of sentences.
Va-lency model seems to have a slightly negative ef-fect on the translation quality.
As this is the onlystatistical component of Depfix, we believe thatthis is caused by the fact that its parameters werenot tuned on the final CU-BOJAR system, as the9We use the term precision for our primary measure forconvenience, even though the way we define it does not matchexactly its usual definition.Depfix component Prc.
Imp.
Usl.Aux ?be?
agr.
?
1.4% 100%No prep.
without children ?
0.5% 100%Sentence-initial capitalization 0% 0.1% 0%Prepositional morph.
case 0% 2.1% 83%Preposition - noun agr.
40% 3.8% 70%Noun number projection 41% 7.2% 65%Valency model 48% 10.6% 66%Subject - nominal pred.
agr.
50% 3.8% 76%Noun - adjective agr.
55% 17.8% 75%Subject morph.
case 56% 8.5% 57%Tokenization projection 56% 3.0% 38%Verb tense projection 58% 5.2% 47%Passive actor with ?by?
60% 1.0% 44%Possessive nouns 67% 0.9% 25%Source-aware truecasing 67% 2.8% 50%Subject - predicate agr.
68% 5.1% 57%Pro-drop in subject 73% 3.4% 63%Subject - past participle agr.
75% 6.3% 42%Passive - aux ?be?
agr.
77% 4.8% 69%Possessive with ?of?
78% 1.5% 31%Present continuous 78% 1.5% 31%Missing reflexive verbs 80% 1.6% 64%Subject categories projection 83% 3.7% 62%Rehang children of aux verbs 83% 5.5% 62%Lost negation recovery 90% 7.2% 38%Table 11: Depfix components performance analy-sis on 871 sentences from WMT13 test set.tuning has to be done semi-manually and the fi-nal system was not available in advance.
On theother hand, Lost negation recovery seems to havea highly positive effect on translation quality.
Thisis to be expected, as a lost negation often leads tothe translation bearing an opposite meaning to theoriginal one, which is probably one of the mostserious errors that an MT system can make.5 ConclusionWe have reached our chimera to beat GoogleTranslate.
We combined all we have: a deep-syntactic transfer-based system TectoMT, verylarge parallel and monolingual data, factored setupto ensure morphological coherence, and finallyDepfix, a rule-based automatic post-editing sys-tem that corrects grammaticality (agreement andvalency) of the output as well as some features vi-tal for adequacy, namely lost negation.AcknowledgmentsThis work was partially supported by the grantsP406/11/1499 of the Grant Agency of the CzechRepublic, FP7-ICT-2011-7-288487 (MosesCore)and FP7-ICT-2010-6-257528 (Khresmoi) of theEuropean Union and by SVV project number 267314.97ReferencesAlexandra Birch and Miles Osborne.
2007.
CCG Su-pertags in Factored Statistical Machine Translation.In In ACL Workshop on Statistical Machine Trans-lation, pages 9?16.Ondr?ej Bojar and Ales?
Tamchyna.
2011.
ImprovingTranslation Model by Monolingual Data.
In Proc.of WMT, pages 330?336.
ACL.Ondr?ej Bojar, Milos?
Ercegovc?evic?, Martin Popel, andOmar Zaidan.
2011.
A Grain of Salt for the WMTManual Evaluation.
In Proc.
of WMT, pages 1?11.ACL.Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran.2012a.
Probes in a Taxonomy of Factored Phrase-Based Models.
In Proc.
of WMT, pages 253?260.ACL.Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???Mars??
?k, Michal Nova?k, Martin Popel, and Ales?
Tam-chyna.
2012b.
The Joy of Parallelism with CzEng1.0.
In Proc.
of LREC, pages 3921?3928.
ELRA.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProc.
of NAACL/HLT, pages 427?436.
ACL.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for opti-mizer instability.
In Proc.
of ACL/HLT, pages 176?181.
ACL.Andreas Eisele, Christian Federmann, Herve?
Saint-Amand, Michael Jellinghaus, Teresa Herrmann, andYu Chen.
2008.
Using Moses to Integrate Multi-ple Rule-Based Machine Translation Engines into aHybrid System.
In Proc.
of WMT, pages 179?182.ACL.Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-bienne Cap.
2012.
Modeling Inflection and Word-Formation in SMT.
In Proc.
of EACL 2012.
ACL.Petra Galus?c?a?kova?, Martin Popel, and Ondr?ej Bojar.2013.
PhraseFix: Statistical Post-Editing of Tec-toMT.
In Proc.
of WMT13.
Under review.Qin Gao and Stephan Vogel.
2008.
Parallel Implemen-tations of Word Alignment Tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, SETQA-NLP ?08, pages 49?57.
ACL.Jan Hajic?.
2004.
Disambiguation of rich inflection:computational morphology of Czech.
Karolinum.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable ModifiedKneser-Ney Language Model Estimation.
In Proc.of ACL.Philipp Koehn and Barry Haddow.
2012.
Towards Ef-fective Use of Training Data in Statistical MachineTranslation.
In Proc.
of WMT, pages 317?321.
ACL.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Machine Trans-lation Summit X, pages 79?86.David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, andOndr?ej Bojar.
2011.
Two-step translation withgrammatical post-processing.
In Proc.
of WMT,pages 426?432.
ACL.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In ACL.
ACL.Martin Popel and Zdene?k Z?abokrtsky?.
2010.
Tec-toMT: Modular NLP Framework.
In Hrafn Lofts-son, Eirikur Ro?gnvaldsson, and Sigrun Helgadottir,editors, IceTAL 2010, volume 6233 of Lecture Notesin Computer Science, pages 293?304.
Iceland Cen-tre for Language Technology (ICLT), Springer.Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek.2012.
DEPFIX: A system for automatic correctionof Czech MT outputs.
In Proc.
of WMT, pages 362?368.
ACL.Rudolf Rosa, David Marec?ek, and Ales?
Tamchyna.2013.
Deepfix: Statistical Post-editing of StatisticalMachine Translation Using Deep Syntactic Analy-sis.
Ba?lgarska akademija na naukite, ACL.Rudolf Rosa.
2013.
Automatic post-editing of phrase-based machine translation outputs.
Master?s thesis,Charles University in Prague, Faculty of Mathemat-ics and Physics, Praha, Czechia.Johanka Spoustova?
and Miroslav Spousta.
2012.
AHigh-Quality Web Corpus of Czech.
In Proc.
ofLREC.
ELRA.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
Intl.
Conf.
onSpoken Language Processing, volume 2, pages 901?904.Michael Subotin.
2011.
An exponential translationmodel for target language morphology.
In Proc.
ofACL/HLT, pages 230?238.
ACL.98
