An Eva luat ion  o f  Anaphor  Generat ion  in  Ch ineseCh ing-Long YehDept.
of Computer Science and EngineeringTatung Institute of Technology40 Chungshan North Road, Section 3Taipei 104Taiwanchingyeh~cse, tilt.
edu.
twChr i s  Mel l i shDept.
of Artificial IntelligenceUniversity of Edinburgh80 South BridgeEdinburgh EH1 1HNScotlandchrism~aisb, ed.
ac.
ukAbst ractIn this paper, we present an eval-uation of anaphors generated by aChinese natural language generationsystem.
In the evaluation work, theanaphors in five test texts generatedby three test systems employing en-eration rules with different complex-ities ~vere compared with the ones inthe same texts created by twelve nat-ive speakers of Chinese.
We took theaverage number of anaphors matchingbetween the machine and human textsas a measure of the quality of anaphorsgenerated by the test systems.
Theresults suggest hat the one we havechosen and which has the most com-plex rule is better than the other two.There axe, however, real difficulties inestablishing the significance of the res-ults because of the degree of disagree-ment among the native speakers.1 In t roduct ionWe have established several rules for the gener-ation of anaphors in Chinese, including rules tomake the decision between zero, pronominal ndnominal anaphors.
Zero anaphors axe omissionsof noun phrases in surface sentences, pronom-inal anaphors, ta (s/he, it) are like s/he and itin English, and nominal anaphors are like def-inite NPs in English (Che87).
These types ofanaphors are exemplified in (1) by ?i, 1 tai (he)1We use a ?~ to denote a zero anaphor, where thesuperscApt a is the index of the referent.and na ren j (that person), respectively.(1)a.
Zhangsan i jinghuang de wang wai pao,Zhangsan frightened NOM towards outside runZhangsan was frightened and ran outside.b.
?i zhuangdao yige ren j,(he) bump-to a person(He) bumped into a person.c.
t a i kanqing lena ren J de lian,he see-clear ASPECT that person GEN faceHe saw clearly that person's face.d.
?i renchu na ren j shi shui.
(he) recognise that person is who(He) recognised who that person is.In addition, we have established a rule for thechoice of a description if a nominal anaphor isdecided upon which, for instance, would choosebetween fang zuozi (square table), and simplyzuozi (table) if a nominal form is decided uponto refer to a square table.
These rules were im-plemented in our Chinese natural language gen-eration system and a number of texts for de-scribing entities in a national park were gener-ated (Yeh95).
As shown in our previous tud-ies (YM94; YM95; Yeh95), these rules wereobtained from empirical studies.
The exper-imental results show that the anaphors gen-erated by using these rules largely match theones in the test texts we used , assuming thesame semantic structures and contextual in-formation.
This shows the performance of therules.
However, in this previous work the samedata served as both training and test data.
Fur-thermore, the assumed contextual information,for example, discourse structures, may be dif-ficult to implement in a real system.
Thus,111the performance of a real anaphor generationalgorithm based on the previous rules may bedifferent to the .experimental results.
In thispaper, we attempt a post-evaluation by askingsome native speakers of Chinese to judge theresult of the anaphors generated by our system.2 P rev ious  Work  and  Our  ApproachThough the field of natural language genera-tion has progressed towards composing complextexts, the evaluation of natural anguage gener-ation systems has remained at the discussionstage (May90; MM91).
Two broad methodshave been identified for evaluating natural an-guage generation systems: glass box and blackbox evaluation (MM91).
The glass box methodis concerned with examining the internal work-ing of individual components in a system, whilethe latter looks at the behaviour of the inputand output to the generation systems.
The dif-ficulty of the glass box method is the lack ofa clear division between components in gener-ation systems.
Even if the black box methodis adopted, however, it is difficult to determinewhat is the appropriate input for generation andto be objective in evaluating the output text.In this paper, we aim to investigate the qual-ity of anaphors generated by the referring ex-pression component in our Chinese natural an-guage generation system.
The referring expres-sion component lies between the text plannerand the linguistic realisation component in thesystem, as shown in Fig.
1.
On accepting aninput goal from the user, the system invokesthe text planner which uses the operators in theplan library to build up a plan which is a hier-archical discourse structure to satisfy the inputgoal.
After the text planning is finished, thedecision of anaphoric forms and descriptions ithen made by traversing the plan tree.
As is dis-cussed in (YM95; Yeh95), the algorithm of thereferring expression component first determinesan appropriate form for an anaphor to be gen-erated.Suppose that the referring expression com-ponents we wish to compare all adopt the abovebasic algorithm.
Then the essential character-Input goalText plannerReferring expressioncomponentLinguistic realisationSurface sentencesFigure 1: Referring expression component inthe Chinese natural anguage system.istic to distinguish them from each other be-comes the rules used in the components and howthese rules are implemented.
If all of these re-ferring expression components are embedded inthe same Chinese natural language generationsystem, as in Fig.
1, for example, then, givenan input to the system, anaphors in the result-ing texts can be characterised by the rules usedin the referring expression component and theirimplementation.By adopting this approach, we need not worryabout the problems of either of the evaluationmethods stated above, except the objectiveevaluation of output text.
Since there is no ma-chine that can read the generated texts and givean impartial judgement about them, we rely onthe opinions of human readers who are nativespeakers of Chinese to investigate the quality ofthe generated anaphors.
This is an easier taskthan assessing the quality of whole texts.
Tocompensate for possible bias among the indi-vidual readers, we sent the output texts to agroup of readers for viewing and took the aver-age of their outcomes as the measurement.In brief, each object system in our evaluationwork is thought of as having the same individualcomponents, including control and knowledgebases (which are discussed in full in (Yeh95) but112immediate// X~ng?
violating syntactic?onstr~ts?
NY7?
satisfying anF  a, sogmcntY7 ~o YV ~oP N .
satisfying ^ sa~eqce?an'Tcy~iferi?n~e j xX~xoyes ~o/ \ Z satisfying 9P N ani~acy ~ferion.Y7P NFigure 2: A Chinese anaphor generation rule.cannot be presented here for reasons of space),except that the anaphor generation rules usedin the referring expression components are dif-ferent to each other.
In the existing literature,we cannot find other work on the generationof Chinese referring expressions (or indeed onthe full evaluation of anaphor generation for anyother language), which means that we have noreal working systems to compare with.
In prac-tice, we employ our Chinese natural languagegeneration system described in (Yeh95) as thebackbone of the evaluation work because it iseasy for us to control and maintain.
What wehave to do for each generation system is simplyto insert the corresponding generation rule.3 Systems to  Compare  and  the  TestTaskHaving described the framework of the evalu-ation, in this section, we give details about theobject systems to be compared in the evaluationwork and the tasks to be performed in the eval-uation work.3.1 Systems to compareThe anaphor generation rule we obtained inour previous studies (YM94; YM95; Yeh95) isshown in Fig.
2, where the internal nodes repres-ent constraints and the terminal nodes are thedecisions of using a zero (Z), pronominal (P),or nominal (N) form.
The locality constraintchecks whether the anaphor in question occurseither in the immediately previous utterance orat a long distance.
The second constraint de-termines whether an anaphor occurs in a posi-tion violating syntactic onstraints on zero ana-phors.
We adopted the concept of discourse seg-ment structure in (GS86) to build up the con-straint at segment beginning.
It checks whetheran anaphor is at the beginning of a discoursesegment.
The salience constraint says that boththe positions of an anaphor and its antecedentare the topics of their respective utterances.The animacy constraint checks whether the ana-phor in question is animate.
Then the followingrule is used if a nominal form is decided on.If a nominal anaphor, n, is at the be-ginning of a "sentence" 2, or is thefirst mention of the referent in a "sen-tence," then a full description is pre-ferred; otherwise, if n is within a "sen-tence" or has been mentioned previ-ously in the same "sentence" withoutdistracting elements, then a reduceddescription is preferred; otherwise afull description is preferred.The constraints in the anaphor generationrule were established by consulting relevant lin-guistic studies (YM94; YM95; Yeh95).
Con-sequently, subsets of constraints in the aboverule can be thought of as possible rules, ifnot complete, for the generation of anaphors inChinese.
As described previously, the systemsto compare in this evaluation work are assumedto share the same individual components, ex-cept the anaphor generation rules.
In this pa-per, we equipped each system with such a pos-sible anaphor generation rule.We chose three rules, termed TR1 TR2 andTR3, with different complexities among the pos-sible candidates as the targets of the test 3.
The2A "sentence" is in general a meaning-complete unit(Liu84).
A sentential mark is used to indicate the fullstop of a "sentence"; a comma within a "sentence" in-dicates a temporary stop.aThe use of these rules enables us to investigate theeffectiveness of individual constraints.113lt~a~y'?immedi~ngvmlating syntactic N?
ons  nts?
j Esatisl'ying an~n~ britcrmn?
Z'7 3'P NTillimmediay "k~ngvmlating syntactic N' ( Ins I  n~?P N .
salislying .
~ Zanlma~y ?fl~ntln'.P NTR2immcdia~/ ~ngviolating syntactic N"ons  "nts?
'TX' ' YX'P N .
satisfying .
sal~coxcc?<%P NFigure 3: Rules used in the comparison systems.rules are shown in Fig.
3.
The first one uses loc-ality, syntactic constraints and animacy.
Thesecond and the third rules have one additionalconstraint, namely, discourse segment bound-aries and salience, respectively, added to theirpredecessors.
In the following, we use the aboverule names to represent the systems.3.2 The  tes t  taskThe task can be divided into an annotationand a comparison stage.
Each of twelve nativespeakers of Chinese was given a number of testsheets to finish.
On each sheet is a text gener-ated by our generation system.
Each anaphorposition in a generated text was left empty andall candidate forms of the anaphor, includingzero, pronominal, and full, or reduced descrip-tions were put under the empty space.
The taskfor a speaker to perform was to annotate whichform he or she preferred for each anaphor posi-tion on the sheets.We selected five texts generated by our sys-tem for the test.
The numbers of clauses in thetexts are 5, 12, 12, 21 and 34; the numbers ofanaphors in the texts are 4, 11, 11, 20 and 34.See the Appendix for the first three test texts.For convenience, we summarise the occurrenceof anaphors in the test texts in a graphical formin Fig.
4.
In the figure, each box represents aclause and at the right end is the accompany-ing punctuation mark.
Each box is divided intothree parts which represent he topic, the sub-ject and the direct object positions of the clause.The numbers in a box, except for the first occur-rences in the text, are the indices of anaphorsin the corresponding clauses.
Initial referencesare indicated by bold italics.
For example, inText 2, the numbers 1, 2, 3 and 4 occurring inthe first, 5th, 8th and lOth clauses, respectively,are initial references; others are anaphors.After the annotations were collected, we car-ried out comparisons between the speakers' res-ults and the generated texts to investigate theperformance of the test rules.
In each compar-ison, we noted down the number of matchesbetween the computer generated text and thehuman result.
In the following, we use Cij todenote the text indexed j generated by the sys-tem equipped with Rule TRi, where i is 1 to3 and j is 1 to 5; and Hkt to denote the res-ulting text indexed l of speaker k, where k is 1to 12 and l is 1 to 5.
The comparison work issummarised procedurally as below.for each rule TRifor each speaker jfor each text kcompare Cik with Hjk andnote down the number of matchesof anaphors between them4 Resu l tsIn this section, we investigate the result of thecomparisons made in the last section.
The com-parison result is shown in Table 1.
The averagematching rates for all test texts are 72, 74 and76%.This average matching rate, however, is lowerthan the matching rates, about 92%, we ob-tained in the empirical studies described pre-114Text  l1 I t l  I I,2111  I I,3111  I l,4 t l t  \[ l ,51 I I  I IoText  2 Text  31 t t l  I I, 1 I I  I I,2 I I I  I I, 2 ~l  I I,3 I t l  I I, 3 I I  t I,41 I I  I l, 4 I I  l l ,5 I I I I 2 l, 5 I I \[ l ?6 I ?1  I \], 6 I I  i i ,7 I ?1  I J+ 7 11 I I o8 I 9 I I ~ I,,, $ 11 I ?
l ,9 I 41  I I ,  9 ?1 I I ,101  ~1 I ,~ I, 10 '~-F ' - ' r+~- I ,11 I ,11 I I ,  11 I ~ I I ~1~12 I ,1 I I Io 12 I ~ I l IoText  411  / I  I I,21 I I  t t ?3 I wt I ?1 ,4 t ~t  I t,5171 I Io6 I I I  I ?1+71  ~1 I I,8 I a l  141+9141 I I,10 \[ ~ I I I+11 I 3 i I I,12 I ~l I I I+13 I 4 I I Io14 I I I I ?
I,15 I ~ I I I ,161 ~1 I Io171 I I  I +1 ,181F , I  I I ,191  ~1 I I ,201 +I  l l ,21 I ?, I I 1oText  511  t l  t l,21  I I  l I ?3 I i I  I ~,1,4 E 91 I I,5 i71  I I o6 I t i  I ~ i ,7141 I I ,8141 I I,9 I 4 \ ]  t I,10 I ~1 I I,11 I 3 I I I,12 i 3 I I I,13 I ~l I I I ?14 I t I I ?
I~15 I ~ t I I,16 I ~ I f I ,17 I 5 I t6.7.~,181 ~,1 I o1~19t  91 I I ,201 q l  I I ,211 q,t I i ,221 q l  t l ,23 I 7 I I 101,24 I I01 I I~25 \] \]01 I l ,26 I to I  I I ,27 I ~ I I Io28 I I I I H I ,29 \[ I l l  t l ,30 I~t l  I I ,31 I I I I  I 12}~32 t121  \] J,33 1111 1131,34 1131 I \]21oFigure 4: Occurrence of anaphors in the test texts.115Table 1: Match between the results of test systems and native speakers.SystemTR1TR2Speaker101112AverageTotal anaphorsMatching rate7101112AverageTotal anaphorsText 1 Text 24 104 84 74 83 74 84 74 102 64 82 54 97.84 1170%4 104 84 74 83 74 84 74 102 64 82 54 97.84 113.690%3.6Text 3 Text 49 166 165 155 138 147 156 169 177 99 146 105 13142170%8 176 175 165 149 157 167 179 188 9157 116 1414.9216.81162%107.311Text 527242423232825321423202323.83470%26252324242926331424192424.334Matching rate 90% 70% 66% 75% 71%TR3 1 4 7 5 13 182 4 11 9 16 253 4 10 8 19 294 4 9 4 14 245 3 9 9 16 2611 6 14 248 10 13 2314 2520 7 1210 4 9 7 16 2311 2 6 6 12 2112 4 10 9 16 30Average 3.6 8.7 7.1 14.6 24Total anaphors 4 11 11 21 34Matching rate 90% 79% 65% 77% 71%i16Speaker Text 1 I Text2 Text Text 5 3 Text 4Table 2: Agreement of annotations among speakers.1 3.9 8 7.5 14.3 242 3.9 9.5 7.8 16.1 26.53 3.9 9.1 7.8 15.8 26.34 3.9 8.9 6.6 15.4 23.95 3.3 8.5 8.3 15.2 25.46 3.9 9.5 8.3 14.1 26.57 3.9 8.3 7.1 15 26.28 3.9 8.1 7.9 15.8 26.49 2.4 6.8 7 12.1 20.510 3.9 8.6 8.1 14.5 2511 2.3 5.7 7 12.7 21.212 3.9 9.4 7.8 15.3 26.3Average 3.6 8.4 7.6 14.7 24.9Total anaphors 4 11 11 21 3490% 76% 69% 73% 73%viously (YM94; Yeh95).
The problem is partlybecause the test texts used in the former com-parison are human-created, while the test textsused here are machine-generated.
The gram-matical structures of the machine-created textsare simplified; they are not as sophisticated ashuman texts.
In the evaluation work, whenthe speakers were asked to decide their pref-erences for anaphors in the machine-generatedtexts, they may find less complete informationshown in the test texts than what they areused to in creating their own texts and henceit may be difficult for them to make their owndecisions.
In the empirical study, the human-created texts perhaps provided more sufficientinformation for the hypothetical machine to de-cide on an appropriate anaphoric form.A more important reason why the matchingrates are lower than before could be that insome circumstances there may be more thanone acceptable solution and the speakers maynot always choose the same one as the machine.This hypothesis can be investigated by look-ing at the extent to which the speakers agreeamong themselves.
To see how the speakersagree among themselves, we further made acomparison between the speakers' annotations,which is summarised as below.for each speaker ifor each text jcompare i's with the rest of speakers'annotations and note downthe average number of the matchesThe comparison result is shown in Table 2.
Foreach speaker, the number for each test text isthe average number of matches with the othereleven speakers.
For example, Speaker 1 re-ceives, on average, 8 matches for Text 2.
At theend of the table are the average numbers for thespeakers' agreement among themselves.
Thefigures in the table show that the speakers donot achieve an agreement among themselves forthe use of anaphors in this test.
These figuresare further supported by the kappa statistic, astandard measure of agreement between a set ofjudges (SC88).
The overall kappa value for allspeakers is about 0.41, whereas a value of 0.8 orover would normally be required for good evid-ence of agreement.
The measure of agreementgets worse if only the zero/ pronoun/ nominaldistinction is considered or if zero and non-zeropronouns are lumped together.
Only two speak-ers agree with one another with a kappa valueof more than 0.7 (none with a value of greater117than 0.8).
The speakers as a whole agreed withkappa greater than 0.7 on 30 out of the 80 ana-phors, with complete agreement only 14 times.To get an overall agreement of greater than 0.8would require reducing the set of speakers from12 to a carefully selected 3.As shown in Fig.
4, the anaphors in Text 1form a "topic chain" 4 within a single "sen-tence".
These anaphors are all zeroed accordingto the conditions of locality and syntactic on-straints in the three test rules.
All three systemsproduce the same result for Text 1 and, hence,unsurprisingly all three systems have the samematching rate, 90%, as shown in Table 1.Text 2 similarly contains a single "sentence"but has three topic shifts in addition to "topicchains" within the "sentence" as shown inFig.
4.
Since no discourse segment boundariesoccur within the "sentence", the discourse seg-ment boundary constraint in TR2 has no effecton this test text, which means that both TR1and TR2 produce the same output.
However,there are three topic shifts within the "sen-tence", namely, clauses 5 and 6, 8 and 9, and10 and 11, as shown in Fig.
4.
The shiftswould make the rule containing the salienceconstraint, TR3, obtain different output fromthose without this constraint, TR i  and TR2obtain the same matching rate, 70%.
TR3 ob-tains higher matching rates than the other two,79%, which shows the effectiveness of the sali-ence constraint in it.We then examine another middle-sized testtext, Text 3, which is broken into three "sen-tences," as shown in Fig.
4.
The beginning of a"sentence" is the beginning of a discourse seg-ment in our implementation (Yeh95).
Further-more, there are three topic shifts occurring inText 3, i.e., clauses 8 and 9, 10 and 11, and11 and 12.
The constraint of discourse segmentbeginnings in TRP and TR3 and the salienceconstraint in TR3 would therefore have some ef-fects on the output texts.
The matching rates,as shown in Table 1, increase from 62 to 66%for TR2, which shows that the constraint on4A "topic chain" is a situation where a referent is re-ferred to in the first clause, and then several more clausesfollow talking about he same referent, namely, the toi'~c.discourse segment beginnings in TRP is effect-ive.
TR3 obtains 65% matching rate, on av-erage, which is 1% lower than its predecessorTR2.
However, this decrease of average match-ing rate does not deny the effectiveness of thesalience constraint in TR3.
TR2's text differsfrom TR3's in the three topic shifts: TR2 gen-erates zero anaphors for these shifts, while TR3generates full descriptions.
The speakers variedgreatly in choosing anaphoric forms for thesetopic shifts: among twelve speakers, four choseall full descriptions, three used all zero ana-phors, and the other five chose zero, pronom-inal and nominal anaphors.
Thus, four amongthe twelve speakers completely agree with TR3,while three agree with TR2.
This shows thatthe salience constraint in TR3 is still effective.Then we examine the more complicated texts,Texts 4 and 5.
As shown in Table 1, the in-creases of matching rates show the effectivenessof the constraint of discourse segment begin-ning in TR2.
Again, the average matching ratesof TR3 are sightly lower than TR2 for thesetwo texts.
However, similar to the situation inText 3, the speakers have varied agreement onthe choice of anaphors for the topic shiftingsin these two texts.
For Text 4, three and onespeaker completely agree with TRP and TR3,respectively.
As for Text 5, two speakers com-pletely agree with TR2, while the others partlyagree with TR2 and TR3.The above discussions how that the salienceconstraint in TR3 is sometimes effective in get-ting small improvements in the output texts.This shows the difference of concepts of sali-ence used between the speakers and TR3.
Inbrief, the more sophisticated constraints a rulecontains, the better it performs.
Both TR2 andTR3 perform better than TRi .
TR3 performsbetter than TR2 for texts with simple discoursesegment structure.
For the texts having com-plicated discourse segment structures, TR2 isslightly better than TR3 on average matchingrates.
Adding the results of the rules to those ofthe speakers leads to a slight decrease in kappafor TR1 but progressively better (though onlyfrom 0.41 to 0.43) values for kappa for TR2and TR3.
This indicates that the better rules118seem to disagree with the speakers no more thanthe speakers disagree among themselves.
Thereart 9 anaphors where the kappa score includ-ing TR3 is less than that for the speakers alone(in many other cases, the results being better).These seem to involve places where the speakerswere more willing to use a zero pronoun (wherethe system used a reduced nominal anaphor)and where the speakers reduced nominal ana-phors less than the system did.5 Conc lus ionIn this paper, we evaluated the quality of ana-phors in the texts generated by using variousrules.
As shown in the results of comparis-ons between the anaphors created by computersand native speakers of Chinese, the individualconstraints we collected in our previous stud-ies (YM94; YM95; Yeh95), seem to be effect-ive to a large extent in the generation of ana-phors in Chinese.
Also they can be implemen-ted successfully.
The comparison results suggestthat a Chinese natural language generation sys-tem employing the combination of these con-straints might produce more effective anaphorsthan one using individual constraints.
Althoughthe average matching rates between the differ-ent rules and the speakers are lower than thosefrom our previous experiments based on human-generated texts, this at least in part reflectsconsiderable disagreement among the speakersthemselves.Append ixThree test texts, Texts 1, 2 and 3, are shown inFig.
5.Y.
C. Liu.
Zuowen de fangfa (Approaches to Com-position) (in Chinese).
Xuesheng Chubanshe,Taipei, Taiwan, _1984.M.
T Maybury.
Planning Multisentential EnglishText Using Communicative Acts.
PhD thesis,Cambridge University, 1990.M.
Meteer and D. McDonald.
Evaluation for gen-eration.
In J. G Neal and S. M. Wlater, edit-ors, Natural Language Processing Systems Evalu-ation Workshop, pages 127-131, NY, 1991.
RomeLaboratory.S.
Siegel and N. J. Jr. Castellan.
NonparametricStatistics for the Behavioral Sciences.
McGraw-Hill, 1988.C.
L. Yeh.
Generation of Anaphors in Chinese.
PhDthesis, University of Edinburgh, Edinburgh, Scot-land, 1995.C.
L. Yeh and C. Mellish.
An empirical study on thegeneration of zero anaphors in Chinese.
In Proc.of the 15th International Conference on Computa-tional Linguistics, pages 732-736, Kyoto, Japan,1994.C.
L. Yeh and C. Mellish.
An empirical study onthe generation of descriptions for nominal ana-phors in Chinese.
In Prod.
of Recent Advances inNatural Language Processing, Tzigov Chark, Bul-garia, 1995.Re ferencesP.
Chen.
Hanyu lingxin huizhi de huayu fenxi (a dis-course approach to zero anaphora in chinese) (inchinese).
Zhongguo Yuwen (Chinese Linguistics),pages 363-378, 1987.B.
J. Grosz and C. L. Sidner.
Attention, intentions,and the structure of discourse.
ComputationalLinguistics, 12(3):175-204, 1986.119-z/4?;1~1~,,.
: Text  1( z ,~ ,~)  ( z , '~ ,~)(z, '~, ~3~)  (z, 'E.
~ )~{~:~l~,,.
: Text  2~.,._A.Lh ~ '% ,~l,,~J<lJd ' _ _ ,  ~ l~ 977~..~,~, t I.J-I~:~ ~\ [ t~ l \ ]~ t ,( z , ' -~ ,~)  ( z , '~ , iE~)  ( z , '~ ,~)( z , '~ ,~)  ( z .~ ,~z)  ( z ,~ ,~)~J  ~ ___~.l.t, , ~ I~ ,~,  , ~ 880~,~ *( z , '~ ,~)  ( z .
'~ .~)  ( z , '~ ,~)~{~l~, , .
: Text  3?-/~,$,,q, ~.~.
~, , .~,~,  ~ ~%~ ~ 2800zfx~/&,~ , t ~-~ ~(~ 2 /~_~,(z, '~, ~z~)  (z..~.
~?~\ ] )(z, ~ ,  .
'~3~)  (z, "E, ~t~3~) (z, "E. ~ )(z.
'~, ~:ks~) (z.
'~, ~ ,5~)  (z .~,~)( z , 'E ,~5~)  (z,'E, $ ~ )  ( z , '~ ,~?~)Figure 5: Example  of test text for evaluation.120
