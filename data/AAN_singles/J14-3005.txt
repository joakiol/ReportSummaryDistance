Probabilistic Distributional Semantics withLatent Variable ModelsDiarmuid O?
Se?aghdha?University of Cambridge, UKAnna Korhonen?University of Cambridge, UKWe describe a probabilistic framework for acquiring selectional preferences of linguistic predi-cates and for using the acquired representations to model the effects of context on word meaning.Our framework uses Bayesian latent-variable models inspired by, and extending, the well-knownLatent Dirichlet Allocation (LDA) model of topical structure in documents; when applied topredicate?argument data, topic models automatically induce semantic classes of arguments andassign each predicate a distribution over those classes.
We consider LDA and a number ofextensions to the model and evaluate them on a variety of semantic prediction tasks, demon-strating that our approach attains state-of-the-art performance.
More generally, we argue thatprobabilistic methods provide an effective and flexible methodology for distributional semantics.1.
IntroductionComputational models of lexical semantics attempt to represent aspects of word mean-ing.
For example, a model of the meaning of dog may capture the facts that dogs areanimals, that they bark and chase cats, that they are often kept as pets, and so on.
Wordmeaning is a fundamental component of the way language works: Sentences (and largerstructures) consist of words, and their meaning is derived in part from the contributionsof their constituent words?
lexical meanings.
At the same time, words instantiate amapping between conceptual ?world knowledge?
and knowledge of language.The relationship between the meanings of an individual word and the largerlinguistic structure in which it appears is not unidirectional; while the word contributesto the meaning of the structure, the structure also clarifies the meaning of the word.Taken on its own a word may be vague or ambiguous, in the senses of Zwicky andSadock (1975); even when the word?s meaning is relatively clear it may still admitspecification of additional details that affect its interpretation (e.g., what color/breedwas the dog?).
This specification comes through context, which consists of bothlinguistic and extralinguistic factors but shows a strong effect of the immediate lexicaland syntactic environment?the other words surrounding the word of interest andtheir syntactic relations to it.?
15 JJ Thomson Avenue, Cambridge, CB3 0FD, United Kingdom.E-mail: Diarmuid.O?Seaghdha@cl.cam.ac.uk.Submission received: 20 December 2012; revised version received: 14 July 2013; accepted for publication:7 October 2013doi:10.1162/COLI a 00194?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 3These diverse concerns motivate lexical semantic modeling as an important taskfor all computational systems that must tackle problems of meaning.
In this articlewe develop a framework for modeling word meaning and how it is modulated bycontextual effects.1 Our models are distributional in the sense that their parametersare learned from observed co-occurrences between words and contexts in corpus data.More specifically, they are probabilistic models that associate latent variables withautomatically induced classes of distributional behavior and associate each word witha probability distribution over those classes.
This has a natural interpretation as amodel of selectional preference, the semantic phenomenon by which predicates suchas verbs or adjectives more plausibly combine with some classes of arguments thanwith others.
It also has an interpretation as a disambiguation model: The different latentvariable values correspond to different aspects of meaning and a word?s distributionover those values can be modified by information coming from the context it appearsin.
We present a number of specific models within this framework and demonstrate thatthey can give state-of-the-art performance on tasks requiring models of preference anddisambiguation.
More generally, we illustrate that probabilistic modeling is an effectivegeneral-purpose framework for distributional semantics and a useful alternative to thepopular vector-space framework.The main contributions of the article are as follows: We describe the probabilistic approach to distributional semantics,showing how it can be applied as generally as the vector-space approach. We present three novel probabilistic selectional preference models andshow that they outperform a variety of previously proposed models on aplausibility-based evaluation. Furthermore, the representations learned by these models correspond tosemantic classes that are useful for modeling the effect of context onsemantic similarity and disambiguation.Section 2 presents background on distributional semantics and an overview of priorwork on selectional preference learning and on modeling contextual effects.
Section 3introduces the probabilistic latent-variable approach and details the models we use.Section 4 presents our experimental results on four data sets.
Section 5 concludes andsketches promising research directions for the future.2.
Background and Related Work2.1 Distributional SemanticsThe distributional approach to semantics is often traced back to the so-called ?distri-butional hypothesis?
put forward by mid-century linguists such as Zellig Harris andJ.R.
Frith:If we consider words or morphemes A and B to be more different in meaning than Aand C, then we will often find that the distributions of A and B are more different thanthe distributions of A and C. (Harris 1954)1 We build on previous work published in O?
Se?aghdha (2010) and O?
Se?aghdha and Korhonen (2011),adding new models and evaluation experiments as well as a comprehensive exposition.
In Section 4we indicate which experimental results have previously been reported.588O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsYou shall know a word by the company it keeps.
(Frith 1957)In Natural Language Processing (NLP), the term distributional semantics encompassesa broad range of methods that identify the semantic properties of a word or otherlinguistic unit with its patterns of co-occurrence in a corpus of textual data.
Thepotential for learning semantic knowledge from text was recognized very early inthe development of NLP (Spa?rck Jones 1964; Cordier 1965; Harper 1965), but it is withthe technological developments of the past twenty years that this data-driven approachto semantics has become dominant.
Distributional approaches may use a representationbased on vector spaces, on graphs, or (like this article) on probabilistic models, butthey all share the common property of estimating their parameters from empiricallyobserved co-occurrences.The basic unit of distributional semantics is the co-occurrence: an observation ofa word appearing in a particular context.
The definition is a general one: We maybe interested in all kinds of words, or only a particular subset of the vocabulary;we may define the context of interest to be a document, a fixed-size window arounda nearby word, or a syntactic dependency arc incident to a nearby word.
Given adata set of co-occurrence observations we can extract an indexed set of co-occurrencecounts fw for each word of interest w; each entry fwc counts the number of times thatw was observed in context c. Alternatively, we can extract an indexed set fc for eachcontext.The vector-space approach is the best-known methodology for distributionalsemantics; under this conception fw is treated as a vector in R|C|, where C is thevocabulary of contexts.
As such, fw is amenable to computations known from lin-ear algebra.
We can compare co-occurrence vectors for different words with a simi-larity function such as the cosine measure or a dissimilarity function such asEuclidean distance; we can cluster neighboring vectors; we can project a matrixof co-occurrence counts onto a low-dimensional subspace; and so on.
This is per-haps the most popular approach to distributional semantics and there are manygood general overviews covering the possibilities and applications of the vector spacemodel (Curran 2003; Weeds and Weir 2005; Pado?
and Lapata 2007; Turney and Pantel2010).Although it is natural to view the aggregate of co-occurrence counts for a wordas constituting a vector, it is equally natural to view it as defining a probability distri-bution.
When normalized to have unit sum, fw parameterizes a discrete distributiongiving the conditional probability of observing a particular context given that weobserve w. The contents of the vector-space modeler?s toolkit generally have prob-abilistic analogs: similarity and dissimilarity can be computed using measures frominformation theory such as the Kullback?Leibler or Jensen?Shannon divergences (Lee1999); the effects of clustering and dimensionality reduction can be achieved throughthe use of latent variable models (see Section 3.2.2).
Additionally, Bayesian priors onparameter distributions provide a flexible toolbox for performing regularization andincorporating prior information in learning.
A further advantage of the probabilisticframework is that it is often straightforward to extend existing models to accountfor additional structure in the data, or to tie together parameters for shared statis-tical strength, while maintaining guarantees of well-normalized behavior thanks tothe laws of probability.
In this article we focus on selectional preference learning andcontextual disambiguation but we believe that the probabilistic approach exempli-fied here can fruitfully be applied in any scenario involving distributional semanticmodeling.589Computational Linguistics Volume 40, Number 32.2 Selectional Preferences2.2.1 Motivation.
A fundamental concept in linguistic knowledge is the predicate, bywhich we mean a word or other symbol that combines with one or more argumentsto produce a composite representation with a composite meaning (by the principle ofcompositionality).
The archetypal predicate is a verb; for example, transitive drink takestwo noun arguments as subject and object, with which it combines to form a basicsentence.
However, the concept is a general one, encompassing other word classes aswell as more abstract items such as semantic relations (Yao et al.
2011), semantic frames(Erk, Pado?, and Pado?
2010), and inference rules (Pantel et al.
2007).
The asymmetricdistinction between predicate and argument is analogous to that between context andword in the more general distributional framework.It is intuitive that a particular predicate will be more compatible with some semanticargument classes than with others.
For example, the subject of drink is typically ananimate entity (human or animal) and the object of drink is typically a beverage.
Thesubject of eat is also typically an animate entity but its object is typically a foodstuff.The noun modified by the adjective tasty is also typically a foodstuff, whereas thenoun modified by informative is an information-bearing object.
This intuition can beformalized in terms of a predicate?s selectional preference: a function that assigns anumerical score to a combination of a predicate and one or more arguments accordingto the semantic plausibility of that combination.
This score may be a probability, a rank,a real value, or a binary value; in the last case, the usual term is selectional restriction.Models of selectional preference aim to capture conceptual knowledge that alllanguage users are assumed to have.
Speakers of English can readily identify thatexamples such as the following are semantically infelicitous despite being syntacticallywell-formed:1.
The beer drank the man.2.
Quadruplicity drinks procrastination.
(Russell 1940)3.
Colorless green ideas sleep furiously.
(Chomsky 1957)4.
The paint is silent.
(Katz and Fodor 1963)Psycholinguistic experiments have shown that the time course of human sentenceprocessing is sensitive to predicate?argument plausibility (Altmann and Kamide 1999;Rayner et al.
2004; Bicknell et al.
2010): Reading times are faster when participants arepresented with plausible combinations than when they are presented with implausiblecombinations.
It has also been proposed that selectional preference violations are cuesthat trigger metaphorical interpretation.
Wilks (1978) gives the example My car drinksgasoline, which must be understood non-literally since car strongly violates the subjectpreference of drink and gasoline is also an unlikely candidate for something to drink.In NLP, one motivation for modeling predicate?argument plausibility is toinvestigate whether this aspect of human conceptual knowledge can be learnedautomatically from text corpora.
If the predictions of a computational model correlatewith judgments collected from human behavioral data, the assumption is that themodel itself shares some properties with human linguistic knowledge and is in somesense a ?good?
semantic model.
More practically, NLP researchers have shown thatselectional preference knowledge is useful for downstream applications, includingmetaphor detection (Shutova 2010), identification of non-compositional multiword590O?
Se?aghdha and Korhonen Probabilistic Distributional Semanticsexpressions (McCarthy, Venkatapathy, and Joshi 2007), semantic role labeling (Gildeaand Jurafsky 2002; Zapirain, Agirre, and Ma`rquez 2009; Zapirain et al.
2010), wordsense disambiguation (McCarthy and Carroll 2003), and parsing (Zhou et al.
2011).2.2.2 The ?Counting?
Approach.
The simplest way to estimate the plausibility of apredicate?argument combination from a corpus is to count the number of times thatcombination appears, on the assumptions that frequency correlates with plausibilityand that given enough data the resulting estimates will be relatively accurate.
For exam-ple, Keller and Lapata (2003) estimate predicate?argument plausibilities by submittingappropriate queries to a Web search engine and counting the number of ?hits?
returned.To estimate the frequency with which the verb drink takes beer as a direct object,Keller and Lapata?s method uses the query <drink|drinks|drank|drunk|drinking a|the|?beer|beers>; to estimate the frequency with which tasty modifies pizza the query is simply<tasty pizza|pizzas>.
Where desired, these joint frequency counts can be normalized byunigram hit counts to estimate conditional probabilities such as P(pizza|tasty).The main advantages of this approach are its simplicity and its ability to exploitmassive corpora of raw text.
On the other hand, it is hindered by the facts that onlyshallow processing is possible and that even in a Web-scale corpus the probability esti-mates for rare combinations will not be accurate.
At the time of writing, Google returnszero hits for the query <draughtsman|draughtsmen whistle|whistles|whistled|whistling>and 1,570 hits for <onion|onions whistle|whistles|whistled|whistling>, suggesting the im-plausible conclusion that an onion is far more likely to whistle than a draughtsman.2Zhou et al.
(2011) modify the Web query approach to better capture statisticalassociation by using pointwise mutual information (PMI) rather than raw co-occurrencefrequency to quantify selectional preference:PMI(p, a) = logP(p, a)P(p)P(a)(1)The role of the PMI transformation is to correct for the effect of unigram frequency: Acommon word may co-occur often with another word just because it is a common wordrather than because there is a semantic association between them.
However, it does notprovide a way to overcome the problem of inaccurate counts for low-probability co-occurrences.
Zhou et al.
?s goal is to incorporate selectional preference features into aparsing model and they do not perform any evaluation of the semantic quality of theresulting predictions.2.2.3 Similarity-Based Smoothing Methods.
During the 1990s, research on language mod-eling led to the development of various ?smoothing?
methods for overcoming thedata sparsity problem that inevitably arises when estimating co-occurrence counts fromfinite corpora (Chen and Goodman 1999).
The general goal of smoothing algorithmsis to alter the distributional profile of observed counts to better match the knownstatistical properties of linguistic data (e.g., that language exhibits power-law behavior).Some also incorporate semantic information on the assumption that meaning guides thedistribution of words in a text.2 The analogous example given by O?
Se?aghdha (2010) relates to the plausibility of a manservant or acarrot laughing; Google no longer returns zero hits for <a|the manservant|manservants|menservantslaugh|laughs|laughed> but a frequency-based estimate still puts the probability of a carrot laughing at200 times that of a manservant laughing (1,680 hits against 81 hits).591Computational Linguistics Volume 40, Number 3One such class of methods is based on similarity-based smoothing, by whichone can extrapolate from observed co-occurrences by implementing the distributionalhypothesis: ?similar?
words will have similar distributional properties.
A general formfor similarity-based co-occurrence estimates isP(w2|w1) =?w3?S(w1,w2)sim(w2, w3)?w?
?S(w1,w2) sim(w2, w?
)P(w3|w1) (2)sim can be an arbitrarily chosen similarity function; Dagan, Lee, and Pereira (1999)investigate a number of options.
S(w1, w2) is a set of comparison words that may dependon w1 or w2, or neither: Essen and Steinbiss (1992) use the entire vocabulary, whereasDagan, Lee, and Pereira use a fixed number of the most similar words to w2, providedtheir similarity value is above a threshold t.While originally proposed for language modeling?the task of estimating theprobability of a sequence of words?these methods require only trivial alteration toestimate co-occurrence probabilities for predicates and arguments, as was noted earlyon by Grishman and Sterling (1993) and Dagan, Lee, and Pereira (1999).
Erk (2007)and Erk, Pado?, and Pado?
(2010) build on this prior work to develop an ?exemplar-based?
selectional preference model called EPP.
In the EPP model, the set of comparisonwords is the set of words observed for the predicate p in the training corpus, denotedSeenargs(p):SelprefEPP(a|p) =?a?
?Seenargs(p)weight(a?|p)sim(a?, a)?a??
?Seenargs(p) weight(a?
?|p)(3)The co-occurrence strength weight(a|p) may simply be normalized co-occurrence fre-quency; alternatively a statistical association measure such as pointwise mutual in-formation may be used.
As before, sim(a, a?)
may be any similarity measure definedon members of A.
One advantage of this and other similarity-based models is thatthe corpus used to estimate similarity need not be the same as that used to estimatepredicate?argument co-occurrence, which is useful when the corpus labeled with theseco-occurrences is small (e.g., a corpus labeled with FrameNet frames).2.2.4 Discriminative Models.
Bergsma, Lin, and Goebel (2008) cast selectional preferenceacquisition as a supervised learning problem to which a discriminatively trained classi-fier such as a Support Vector Machine (SVM) can be applied.
To produce training datafor a predicate, they pair ?positive?
arguments that were observed for that predicatein the training corpus and have an association with that predicate above a speci-fied threshold (measured by mutual information) with randomly selected ?negative?arguments of similar frequency that do not occur with the predicate or fall belowthe association threshold.
Given this training data, a classifier can be trained in astandard way to predict a positive or negative score for unseen predicate?argumentpairs.An advantage of this approach is that arbitrary sets of features can be used torepresent the training and testing items.
Bergsma, Lin, and Goebel include conditionalprobabilities P(a|p) for all predicates the candidate argument co-occurs with, typo-graphic features of the argument itself (e.g., whether it is capitalized, or contains digits),lists of named entities, and precompiled semantic classes.592O?
Se?aghdha and Korhonen Probabilistic Distributional Semantics2.2.5 WordNet-Based Models.
An alternative approach to preference learning models theargument distribution for a predicate as a distribution over semantic classes providedby a predefined lexical resource.
The most popular such resource is the WordNet lexicalhierarchy (Fellbaum 1998), which provides semantic classes and hypernymic structuresfor nouns, verbs, adjectives, and adverbs.3 Incorporating knowledge about the WordNettaxonomy structure in a preference model enables the use of graph-based regularizationtechniques to complement distributional information, while also expanding the cov-erage of the model to types that are not encountered in the training corpus.
On theother hand, taxonomy-based methods build in an assumption that the lexical hierarchychosen is the universally ?correct?
one and they will not perform as well when facedwith data that violates the hierarchy or contains unknown words.
A further issue facedby these models is that the resources they rely on require significant effort to create andwill not always be available to model data in a new language or a new domain.Resnik (1993) proposes a measure of associational strength between a predicate andWordNet classes based on the empirical distribution of words of each class (and theirhyponyms) in a corpus.
Abney and Light (1999) conceptualize the process of generatingan argument for a predicate in terms of a Markovian random walk from the hierarchy?sroot to a leaf node and choosing the word associated with that leaf node.
Ciaramitaand Johnson (2000) likewise treat WordNet as defining the structure of a probabilisticgraphical model, in this case a Bayesian network.
Li and Abe (1998) and Clark and Weir(2002) both describe models in which a predicate ?cuts?
the hierarchy at an appropriatelevel of generalization, such that all classes below the cut are considered appropriatearguments (whether observed in data or not) and all classes above the cut are consideredinappropriate.In this article we focus on purely distributional models that do not rely onmanually constructed lexical resources; therefore we do not revisit the modelsdescribed in this section subsequently, except as a basis for empirical comparison.O?
Se?aghdha and Korhonen (2012) do investigate a number of Bayesian preferencemodels that incorporate WordNet classes and structure, finding that such modelsoutperform previously proposed WordNet-based models and perform comparably tothe distributional Bayesian models presented here.2.3 Measuring Similarity in Context2.3.1 Motivation.
A fundamental idea in semantics is that the meaning of a word isdisambiguated and modulated by the context in which it appears.
The word body clearlyhas a different sense in each of the following text fragments:1.
Depending on the present position of the planetary body in its orbital path, .
.
.2.
The executive body decided.
.
.3.
The human body is intriguing in all its forms.In a standard word sense disambiguation experiment, the task is to map instances ofambiguous words onto senses from a manually compiled inventory such as WordNet.An alternative experimental method is to have a system rate the suitability of replacingan ambiguous word with an alternative word that is synonymous or semantically3 WordNet also contains many other kinds of semantic relations besides hypernymy but these are nottypically used for selectional preference modeling.593Computational Linguistics Volume 40, Number 3similar in some contexts but not others.
For example, committee is a reasonablesubstitute for body in fragment 2 but less reasonable in fragment 1.
An evaluation ofsemantic models based on this principle was run as the English Lexical SubstitutionTask in SemEval 2007 (McCarthy and Navigli 2009).
The annotated data from theLexical Substitution Task have been used by numerous researchers to evaluate modelsof lexical choice; see Section 4.5 for further details.In this section we formalize the problem of predicting the similarity or substitutabil-ity of a pair of words wo, ws in a given context C = {(r1, w1), (r2, w2), .
.
.
, (rn, wn)}.
Whenthe task is substitution, wo is the original word and ws is the candidate substitute.
Ourgeneral approach is to compute a representation Rep(wo|C) for wo in context C andcompare it with Rep(ws), our representation for wn:sim(wo, ws|C) = sim(Rep(wo|C), Rep(ws)) (4)where sim is a suitable similarity function for comparing the representations.
Thisgeneral framework leaves open the question of what kind of representation we use forRep(wo|C) and Rep(ws); in Section 2.3.2 we describe representations based on vector-space semantics and in Section 3.5 we describe representations based on latent-variablemodels.A complementary perspective on the disambiguatory power of context models isprovided by research on semantic composition, namely, how the syntactic effect ofa grammar rule is accompanied by a combinatory semantic effect.
In this view, thegoal is to represent the combination of a context and an in-context word, not just torepresent the word given the context.
The co-occurrence models described in this articleare not designed to scale up and provide a representation for complex syntactic struc-tures,4 but they are applicable to evaluation scenarios that involve representing binaryco-occurrences.2.3.2 Vector-Space Models.
As described in Section 2.1, the vector-space approach todistributional semantics casts word meanings as vectors of real numbers and uses linearalgebra operations to compare and combine these vectors.
A word w is represented bya vector vw that models aspects of its distribution in the training corpus; the elementsof this vector may be co-occurrence counts (in which case it is the same as the frequencyvector fw) or, more typically, some transformation of the raw counts.Mitchell and Lapata (2008, 2010) present a very general vector-space frameworkin which to consider the problem of combining the semantic representations of co-occurring words.
Given pre-computed word vectors vw, vw?
, their combination p is pro-vided by a function g that may also depend on syntax R and background knowledge K:p = g(vw, vw?
, R, K) (5)Mitchell and Lapata investigate a number of functions that instantiate Equation (5),finding that elementwise multiplication is a simple and consistently effective choice:pi = vwi ?
vw?i (6)4 cf.
Grefenstette and Sadrzadeh (2011), Socher et al.
(2011)594O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsThe motivation for this ?disambiguation by multiplication?
is that lexical vectors aresparse and the multiplication operation has the effect of sending entries not supportedin both vw and vw?
towards zero while boosting entries that have high weights in bothvectors.The elementwise multiplication approach assumes that all word vectors are inthe same space.
For a syntactic co-occurrence model, this is often not the case: Thecontexts for a verb and a noun may have no dependency labels in common and hencemultiplying their vectors will not give useful results.
Erk and Pado?
(2008) propose astructured vector space approach in which each word w is associated with a set of?expectation?
vectors Rw, indexed by dependency label, in addition to its standard co-occurrence vector vw.
The expectation vector Rw(r) for word w and dependency label ris an average over co-occurrence vectors for seen arguments of w and r in the trainingcorpus:Rw(r) =?w?
: f (w,r,w?
)>0f (w, r, w?)
?
vw?
(7)Whereas a standard selectional preference model addresses the question ?which wordsare probable as arguments of predicate (w, r)?
?, the expectation vector (7) addressesthe question ?what does a typical co-occurrence vector for an argument of the pred-icate (w, r) look like??.
To disambiguate the semantics of word w in the context of apredicate (w?, r?
), Erk and Pado?
combine the expectation vector Rw?
(r?)
with the wordvector vw:vw|r?,w?
= Rw?
(r?)
?
vw (8)Thater, Fu?rstenau, and Pinkal (2010, 2011) have built on the idea of using syntacticvector spaces for disambiguation.
The model of Thater, Fu?rstenau, and Pinkal (2011),which is simpler and better-performing, sets the representation of w in the context of(r?, w?)
to bevw|r?,w?
=?w??,r???w?,r?,w??,r??
?
weight(w?
?, r?
?, w) ?
er??,w??
(9)where ?
quantifies the compatibility of the observed predicate (w?, r?)
with the smooth-ing predicate (w?
?, r??
), weight quantifies the co-occurrence strength between (w?
?, r??
)and w, and er??,w??
is a basis vector for (w?
?, r??).
This is a general formulation admit-ting various choices of ?
and weight; the optimal configuration is found to be asfollows:?w?,r?,w??,r??
={sim(vw?
, vw?? )
if r?
= r?
?0 otherwise (10)weight(w?
?, r?
?, w) = PMI((w?
?, r??
), w) (11)This is conceptually very similar to the EPP selectional preference model (3) of Erk,Pado?, and Pado?
(2010); each entry in the vector vw|r?,w?
is a similarity-smoothed estimateof the preference of (w?, r?)
for w. EPP uses seen arguments of (w?, r?)
for smoothing,whereas Thater, Fu?rstenau, and Pinkal (2011) take a complementary approach and595Computational Linguistics Volume 40, Number 3smooth with seen predicates for w. In order to combine the disambiguatory effects ofmultiple predicates, a sum over contextualized vectors is taken:vw|(r1,w1 ),(r2 ,w2 ),...,(rn,wn ) =n?ivw|ri,wi (12)All the models described in this section provide a way of relating a word?s standardco-occurrence vector to a vector representation of the word?s meaning in context.
Thisallows us to calculate the similarity between two in-context words or between a wordand an in-context word using standard vector similarity measures such as the cosine.
Inapplications where the task is to judge the appropriateness of substituting a word ws foran observed word wo in context C = {(r1, w1), (r2, w2), .
.
.
, (rn, wn)}, a common approachis to compute the similarity between the contextualized vector vwo|(r1,w1 ),(r2,w2 ),...,(rn,wn )and the uncontextualized word vector vws .
It has been demonstrated empirically thatthis approach yields better performance than contextualizing both vectors before thesimilarity computation.3.
Probabilistic Latent Variable Models for Lexical Semantics3.1 Notation and TerminologyWe define a co-occurrence as a pair (c, w), where c is a context belonging to the vo-cabulary of contexts C and w is a word belonging to the word vocabulary W .5 Unlessotherwise stated, the contexts considered in this article are head-lexicalized dependencyedges c = (r, wh) where r ?
R is the grammatical relation and wh ?
W is the head lemma.We notate grammatical relations as ph:label:pd, where ph is the head word?s part ofspeech, pd is the dependent word?s part of speech, and label is the dependency label.6We use a coarse set of part-of-speech tags: n (noun), v (verb), j (adjective), r (adverb).The dependency labels are the grammatical relations used by the RASP system (Briscoe2006; Briscoe, Carroll, and Watson 2006), though in principle any dependency formalismcould be used.
The assumption that predicates correspond to head-lexicalized depen-dency edges means that they have arity one.Given a parsed sentence, each word w in the sentence has a syntactic context setC comprising all the dependency edges incident to w. In the sentence fragment Theexecutive body decided.
.
.
, the word body has two incident edges:The:d executive:j body:nn:ncmod:jdecided:vv:ncsubj:n.
.
.5 When specifically discussing selectional preferences, we will also use the terms predicate and argumentto describe a co-occurrence pair; when restricted to syntactic predicates, the former term is synonymouswith our definition of context.6 Strictly speaking, w and wh are drawn from subsets of W that are licensed by r when r is a syntacticrelation, that is, they must have parts of speech pd and ph , respectively.
Our models assume a fixedargument vocabulary, so we can partition the training data according to part of speech; the models areagnostic regarding the predicate vocabulary as these are subsumed by the context vocabulary.
In theinterest of parsimony we leave this detail implicit in our notation.596O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsThe context set for body is C = {(j:ncmod?1:n,executive), (v:ncsubj:n,decide)}, where(v:ncsubj:n,decide) indicates that body is the subject of decide and (j:ncmod?1:n,executive)denotes that it stands in an inverse non-clausal modifier relation to executive (we assumethat nouns are the heads of their adjectival modifiers).To estimate our preference models we will rely on co-occurrence counts extractedfrom a corpus of observations O.
Each observation is a co-occurrence of a predicate andan argument.
The set of observations for context c is denoted O(c).
The co-occurrencefrequency of context c and word w is denoted by fcw, and the total co-occurrencefrequency of c by fc =?w?W fcw.3.2 Modeling Assumptions3.2.1 Bayesian Modeling.
The Bayesian approach to probabilistic modeling (Gelman et al.2003) is characterized by (1) the use of prior distributions over model parameters toencode the modeler?s expectations about the values they will take; and (2) the explicitquantification of uncertainty by maintaining posterior distributions over parametersrather than point estimates.7As is common in NLP, the data we are interested in modeling are drawn from adiscrete sample space (e.g., the vocabulary of words or a set of semantic classes).
Thisleads to the use of a categorical or multinomial distribution for the data likelihood.
Thisdistribution is parameterized by a unit-sum vector ?
with length |K| where K is thesample space.
The probability that an observation o takes value k is then:o ?
Multinomial(?)
(13)P(o = k|?)
= ?k (14)The value of ?
must typically be learned from data.
The maximum likelihood estimate(MLE) sets ?k proportional to the number of times k was observed in a set of observa-tions O, where each observation oi ?
K:?MLEk =fk|O|(15)Although simple, such an approach has significant limitations.
Because a linguisticvocabulary contains a large number of items that individually have low probabilitybut together account for considerable total probability mass, even a large corpus isunlikely to give accurate estimates for low-probability types (Evert 2004).
Items thatdo not appear in the training data will be assigned zero probability of appearing inunseen data, which is rarely if ever a valid assumption.
Sparsity increases further whenthe sample space contains composite items (e.g., context-words pairs).The standard approach to dealing with the shortcomings of MLE estimation inlanguage modeling is to ?smooth?
the distribution by taking probability mass fromfrequent types and giving it to infrequent types.
The Bayesian approach to smoothingis to place an appropriate prior on ?
and apply Bayes?
Theorem:P(?|O) =P(O|?)P(?)?P(O|?)P(?)d?
(16)7 However, the second point is often relaxed in application contexts where the posterior mean is used forinference (e.g., Section 3.4.2).597Computational Linguistics Volume 40, Number 3A standard choice for the prior distribution over the parameters of a discrete distribu-tion is the Dirichlet distribution:?
?
Dirichlet(?)
(17)P(?|?)
=?
(?k ?k)?k ?(?k)?k?
?k?1k (18)Here, ?
is a |K|-length vector where each ?k > 0.
One effect of the Dirichlet prior is thatsetting the sum?k ?k to a small value will encode the expectation that the parametervector ?
is likely to distribute its mass more sparsely.
The Dirichlet distribution is aconjugate prior for multinomial and categorical likelihoods, in the sense that the poste-rior distribution P(?|O) in Equation (16) is also a Dirichlet distribution when P(O|?)
ismultinomial or categorical and P(?)
is Dirichlet:?
?
Dirichlet(fO ??)
(19)where ?
indicates elementwise addition of the observed count vector fO to the Dirichletparameter vector ?.
Furthermore, the conjugacy property allows us to do a number ofimportant computations in an efficient way.
In many applications we are interested inpredicting the distribution over values K for a ?new?
observation given a set of priorobservations O while retaining our uncertainty about the model parameters.
We canaverage over possible values of ?, weighted according to their probability P(?|O,?)
by?integrating out?
the parameter and still retain a simple closed-form expression for theposterior predictive distribution:P(o|O|+1 = k|O,?)
=?P(o|O|+1 = k|?)P(?|O,?)d?
(20)=fk + ?k|O|+?k?
?k?
(21)Expression (21) is central to the implementation of collapsed Gibbs samplers forBayesian models such as latent Dirichlet allocation (Section 3.3).
For mathematicaldetails of these derivations, see Heinrich (2009).Other priors commonly used for discrete distributions in NLP include the Dirichletprocess and the Pitman?Yor process (Goldwater, Griffiths, and Johnson 2011).
TheDirichlet process provides similar behavior to the Dirichlet distribution prior but is?non-parametric?
in the sense of varying the size of its support according to the data;in the context of mixture modeling, a Dirichlet process prior allows the number ofmixture components to be learned rather than fixed in advance.
The Pitman?Yor processis a generalization of the Dirichlet process that is better suited to learning power-law distributions.
This makes it particularly suitable for language modeling where theDirichlet distribution or Dirichlet process would not produce a long enough tail due totheir preference for sparsity (Teh 2006).
On the other hand, Dirichlet-like behavior maybe preferable in semantic modeling, where we expect, for example, predicate?class andclass?argument distributions to be sparse.3.2.2 The Latent Variable Assumption.
In probabilistic modeling, latent variables arerandom variables whose values are not provided by the input data.
As a result, their598O?
Se?aghdha and Korhonen Probabilistic Distributional Semanticsvalues must be inferred at the same time as the model parameters on the basis of thetraining data and model structure.
The latent variable concept is a very general one thatis used across a wide range of probabilistic frameworks, from hidden Markov modelsto neural networks.
One important application is in mixture models, where the datalikelihood is assumed to have the following form:P(x) =?zP(x|z)P(z) (22)Here the latent variables z index mixture components, each of which is associatedwith a distribution over observations x, and the resulting likelihood is an average ofthe component distributions weighted by the mixing weights P(z).
The set of possiblevalues for z is the set of components Z.
When |Z| is small relative to the size of thetraining data, this model has a clustering effect in the sense that the distribution learnedfor P(x|z) is informed by all datapoints assigned to component z.In a model of two-way co-occurrences each observation consists of two discretevariables c and w, drawn from vocabularies C and W , respectively.P(w|c) =?zP(w|z)P(z|c) (23)The idea of compressing the observed co-occurrence data through a small layer oflatent variables shares the same basic motivations as other, not necessarily probabilistic,dimensionality reduction techniques such as Latent Semantic Analysis or Non-negativeMatrix Factorization.
An advantage of probabilistic models is their flexibility, both interms of learning methods and model structures.
For example, the models consideredin this article can potentially be extended to multi-way co-occurrences and to hierarchi-cally defined contexts that cannot easily be expressed in frameworks that require theinput to be a |C| ?
|W| co-occurrence matrix.To the best of our knowledge, latent variable models were first applied to co-occurrence data in the context of noun clustering by Pereira, Tishby, and Lee (1993).They suggest a factorization of a noun n?s distribution over verbs v asP(v|n) =?zP(v|z)P(z|n) (24)which is equivalent to Equation (23) when we take n as the predicate and v as theargument, in effect defining an inverse selectional preference model.
Pereira, Tishby,& Lee also observe that given certain assumptions Equation (24) can be written moresymmetrically asP(v, n) =?zP(v|z)P(n|z)P(z) (25)The distributions P(v|z), P(n|z), and P(z) are estimated by an optimization procedurebased on Maximum Entropy.
Rooth et al.
(1999) propose a much simpler ExpectationMaximization (EM) procedure for estimating the parameters of Equation (25).599Computational Linguistics Volume 40, Number 33.3 Bayesian Models for Binary Co-occurrencesCombining the latent variable co-occurrence model (23) with the use of Dirichlet priorsnaturally leads to Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003).
Oftendescribed as a ?topic model,?
LDA is a model of document content that assumes eachdocument is generated from a mixture of multinomial distributions or ?topics.?
Topicsare shared across documents and correspond to thematically coherent patterns of wordusage.
For example, one topic may assign high probability to the words finance, fund,bank, and invest, whereas another topic may assign high probability to the words football,goal, referee, and header.
LDA has proven to be a very successful model with manyapplications and extensions, and the topic modeling framework remains an area ofactive research in machine learning.Although originally conceived for modeling document content, LDA can be ap-plied to any kind of discrete binary co-occurrence data.
The original application ofLDA is essentially a latent-variable model of document?word co-occurrence.
AdaptingLDA for selectional preference modeling was suggested independently by O?
Se?aghdha(2010) and Ritter, Mausam, and Etzioni (2010).
Conceptually the shift is straightforwardand intuitive: Documents become contexts and words become argument words.
Theselectional preference probability P(w|c) is modeled asP(w|c) =?zP(z|c)P(w|z) (26)Figure 1 sketches the ?generative story?
according to which LDA generatesarguments for predicates and also presents a plate diagram indicating the dependenciesbetween variables in the model.
Table 1 illustrates the semantic representation inducedby a 600-topic LDA model trained on predicate?noun co-occurrences extracted fromthe British National Corpus (for more details of this training data, see Section 4.1).
The?semantic classes?
are actually distributions over all nouns in the vocabulary ratherthan a hard partitioning; therefore we present the eight most probable words for each.We also present the contexts most frequently associated with each class.
Whereas afor topic z ?
{1 .
.
.
|Z|} do(Draw a distribution over words)?z ?
Dirichlet(?
)end forfor context c ?
{1 .
.
.
|C|} do(Draw a distribution over classes)?c ?
Dirichlet(?
)for observation oi ?
O(c) do(Draw a class)zi ?
Multinomial(?c)(Draw a word)wi ?
Multinomial(?zi )end forend for??zw?
?O(c)CZFigure 1Generative story and plate diagram for LDA; descriptive comments (in parentheses) precedeeach sampling step.600O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsTable 1Sample semantic classes learned by an LDA syntactic co-occurrence model with |Z| = 600trained on BNC co-occurrences.Class 1Words: attack, raid, assault, campaign, operation, incident, bombingObject of: launch, carry, follow, suffer, lead, mount, plan, condemnSubject of: happen, come, begin, cause, continue, take, followModifies: raid, furnace, shelter, victim, rifle, warning, aircraftModified by: heart, bomb, air, terrorist, indecent, latest, further, bombingPrepositional: on home, on house, by force, target for, hospital after, die afterClass 2Words: line, axis, section, circle, path, track, arrow, curveObject of: draw, follow, cross, dot, break, trace, use, build, cutSubject of: divide, run, represent, follow, indicate, show, join, connectModifies: manager, number, drawing, management, element, treatmentModified by: straight, railway, long, cell, main, front, production, productPrepositional: on map, by line, for year, line by, point on, in fig, angle toClass 3Words: test, examination, check, testing, exam, scan, assessment, sampleObject of: pass, carry, use, fail, perform, make, sit, write, applySubject of: show, reveal, confirm, prove, consist, come, take, detect, provideModifies: result, examination, score, case, ban, question, board, paper, kitModified by: blood, medical, final, routine, breath, fitness, driving, betaPrepositional: subject to, at end, success in, on part, performance onClass 4Words: university, college, school, polytechnic, institute, institution, libraryObject of: enter, attend, leave, visit, become, found, involve, close, grantSubject of: offer, study, make, become, develop, win, establish, undertakeModifies: college, student, library, course, degree, department, schoolModified by: university, open, technical, city, education, state, technologyPrepositional: student at, course at, study at, lecture at, year atClass 5Words: fund, reserve, eyebrow, revenue, awareness, conservation, alarmObject of: raise, set, use, provide, establish, allocate, administer, createSubject of: raise, rise, shoot, lift, help, remain, set, cover, holdModifies: manager, asset, raiser, statement, management, commissionerModified by: nature, pension, international, monetary, national, social, trustPrepositional: for nature, contribution to, for investment, for developmenttopic model trained on document?word co-occurrences will find topics that reflectbroad thematic commonalities, the model trained on syntactic co-occurrences findssemantic classes that capture a much tighter sense of similarity: Words assigned highprobability in the same topic tend to refer to entities that have similar properties, thatperform similar actions, and have similar actions performed on them.
Thus Class 1 isrepresented by attack, raid, assault, campaign, and so on, forming a coherent semanticgrouping.
Classes 2, 3, and 4 correspond to groups of tests, geometric objects, andpublic/educational institutions, respectively.
Class 5 has been selected to illustrate apotential pitfall of using syntactic co-occurrences for semantic class induction: fund,revenue, eyebrow, and awareness hardly belong together as a coherent conceptual class.The reason, it seems, is that they are all entities that can be (and in the corpus, are)raised.
This class has also conflated different (but related) senses of reserve and as aresult the modifier nature is often associated with it.An alternative approach is suggested by the model used by Pereira, Tishby, andLee (1993) and Rooth et al.
(1999) that is formalized in Equation (25).
This model can601Computational Linguistics Volume 40, Number 3(Draw a distribution over topics)?
?
Dirichlet(?
)for topic z ?
{1 .
.
.
|Z|} do(Draw a distribution over words)?z ?
Dirichlet(?
)(Draw a distribution over contexts)?z ?
Dirichlet(?
)end forfor observation oi ?
O do(Draw a topic)zi ?
Multinomial(?
)(Draw a word)wi ?
Multinomial(?zi )(Draw a context)ci ?
Multinomial(?zi )end for??zwc???
?O ZZFigure 2Generative story and plate diagram for ROOTH-LDA.be ?Bayesianized?
by placing Dirichlet priors on the component distributions; adaptingEquation (25) to our notation, the resulting joint distribution over contexts and words isP(c, w) =?zP(c|z)P(w|z)P(z) (27)The generative story and plate diagram for this model, which was called ROOTH-LDAin O?
Se?aghdha (2010), are given in Figure 2.
Whereas LDA induces classes of arguments,ROOTH-LDA induces classes of predicate?argument interactions.
Table 2 illustratessome classes learned by ROOTH-LDA from BNC verb?object co-occurrences.
One classshows that a cost, number, risk, or expenditure can plausibly be increased, reduced, cut, orinvolved; another shows that a house, building, home, or station can be built, left, visited, orused.
As with LDA, there are some over-generalizations; the fact that an eye or mouth canbe opened, closed, or shut does not necessarily entail that it can be locked or unlocked.For many predicates, the best description of their argument distributions is one thataccounts for general semantic regularities and idiosyncratic lexical patterns.
This sug-gests the idea of combining a distribution over semantic classes and a predicate-specificTable 2Sample semantic classes learned by a Rooth-LDA model with |Z| = 100 trained on BNCverb?object co-occurrences.Class 1 Class 2 Class 3 Class 4increase cost open door build house spend timereduce number close eye leave building work daycut risk shut mouth visit home wait yearinvolve expenditure lock window use station come hourcontrol demand slam gate enter church waste nightestimate pressure unlock shop include school take weeklimit rate keep fire see plant remember monthcover power round book run office end life602O?
Se?aghdha and Korhonen Probabilistic Distributional Semanticsdistribution over arguments.
One way of doing this is through the model depictedin Figure 3, which we call LEX-LDA; this model defines the selectional preferenceprobability P(w|c) asP(w|c) = ?cPlex(w|c) + (1 ?
?c )Pclass(w|c) (28)= ?cPlex(w|c) + (1 ?
?c )?zP(w|z)P(z|c) (29)where ?c is a value between 0 and 1 that can be interpreted as a measure of argumentlexicalization or as the probability that an observation for context c is drawn from thelexical distribution Plex or the class-based distribution Pclass.
Pclass has the same form asthe LDA preference model.
The value of ?c will vary across predicates according to howwell their argument preference can be fit by the class-based models; a predicate withhigh ?c will have idiosyncratic argument patterns that are best learned by observingthat predicate?s co-occurrences in isolation.
In many cases this may reflect idiomatic ornon-compositional usages, though it is also to be expected that ?c will correlate withfrequency; given sufficient data for a context, smoothing becomes less important.
Asan example we trained the LEX-LDA model on BNC verb-object co-occurrences andestimated posterior mean values for ?c for all verbs occurring more than 100 times andtaking at least 10 different object argument types.
The verbs with highest and lowestvalues are listed in Table 3.
Although almost anything can be discussed or highlighted,for topic z ?
{1 .
.
.
|Z|} do(Draw a distribution over words)?z ?
Dirichlet(?
)end forfor context c ?
{1 .
.
.
|C|} do(Draw a distribution over topics)?c ?
Dirichlet(?
)(Draw a distribution over words)?c ?
Dirichlet(?
)(Draw a lexicalization probability)?c ?
Beta(?0, ?1)for observation oi ?
O(c) do(Draw a lexicalization indicator)si ?
Bernoulli(?c )if si = 0 then(Draw a topic)zi ?
Multinomial(?c)(Draw a word)wi ?
Multinomial(?zi )else(Draw a word)wi ?
Multinomial(?c)end ifend forend for?
?z sw?????
?1?0O(c)CZFigure 3Generative story and plate diagram for LEX-LDA.603Computational Linguistics Volume 40, Number 3Table 3BNC verbs with lowest and highest estimated lexicalization values ?c for their object arguments,as well as the arguments with highest Plex(w|c) for high-lexicalization verbs.Lowest ?c Highest ?c Top lexicalized argumentsdiscuss 1.2 ?
10?4 pose 0.872 problem, threat, question, challenge, riskhighlight 4.6 ?
10?4 wreak 0.864 havoc, vengeance, revenge, damageconsume 5.4 ?
10?4 adjourn 0.857 case, hearing, meeting, inquest, trialemphasize 5.8 ?
10?4 reap 0.857 benefit, rewards, harvest, advantageassert 6.5 ?
10?4 exert 0.851 influence, pressure, effect, control, forcecontrast 6.5 ?
10?4 retrace 0.847 step, route, footstep, path, journeyobscure 6.8 ?
10?4 solve 0.847 problem, mystery, equation, crisis, casedocument 6.8 ?
10?4 sip 0.839 coffee, tea, drink, wine, champagnedebate 6.9 ?
10?4 answer 0.826 question, call, phone, door, querysafeguard 8.0 ?
10?4 incur 0.823 cost, expense, loss, expenditure, liabilityverbs such as pose and wreak have very lexicalized argument preferences.
The semanticclasses learned by LEX-LDA are broadly comparable to those learned by LDA, thoughit is less likely to mix classes on the basis of a single argument lexicalization; whereasthe LDA class in row 5 of Table 1 is distracted by the high-frequency collocationsnature reserve and raise eyebrow, LEX-LDA models trained on the same data can explainthese through lexicalization effects and separate out body parts, conservation areas, andinvestments in different classes.3.4 Parameter and Hyperparameter Learning3.4.1 Learning Methods.
A variety of methods are available for parameter learning inBayesian models.
The two standard approaches are variational inference, in whichan approximation to the true distribution over parameters is estimated exactly, andsampling, in which convergence to the true posterior is guaranteed in theory but rarelyverifiable in practice.
In some cases the choice of approach is guided by the model, butoften it is a matter of personal preference; for LDA, there is evidence that equivalentlevels of performance can be achieved through variational learning and sampling givenappropriate parameterization (Asuncion et al.
2009).
In this article we use learningmethods based on Gibbs sampling, following Griffiths and Steyvers (2004).
The basicidea of Gibbs sampling is to iterate through the corpus one observation at a time,updating the latent variable value for each observation according to the conditionalprobability distribution determined by the current observed and latent variable valuesfor all other observations.
Because the likelihoods are multinomials with Dirichletpriors, we can integrate out their parameters using Equation (21).For LDA, the conditional probability that the latent variable for the ith observationis assigned value z is computed asP(zi = z|z?i, ci, wi) ?
( fzci + ?z)fzwi + ?fz + |W|?
(30)where z?i is the set of current assignments for all observations other than the ith, fzis the number of observations in that set assigned latent variable z, fzci is the numberof observations with context ci assigned latent variable z, and fzwi is the number ofobservations with word wi assigned latent variable z.604O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsFor ROOTH-LDA we make a similar calculation:P(zi = z|z?i, ci, wi) ?
( fz + ?z)fzwi + ?fz + |W|?fzci + ?fz + |C|?
(31)For LEX-LDA the lexicalization variables si must also be sampled for each token.We ?block?
the sampling for zi and si to improve convergence.
The Gibbs samplingdistribution isP(si = 0, zi = z|z?i, s?i, ci, wi) ?
( fci,s=0 + ?0)fzci + ?zfci,s=0 +?z?
?z?fzwi + ?fz + |W|?
(32)P(si = 1, zi = ?|z?i, s?i, ci, wi) ?
( fci,s=1 + ?1)fciwi,s=1 + ?fci,s=1 + |W|?
(33)P(si = 0, zi = ?|z?i, s?i, ci, wi) = 0 (34)P(si = 1, zi = ?|z?i, s?i, ci, wi) = 0 (35)where ?
indicates that no topic is assigned.
The fact that topics are not assigned for alltokens means that LEX-LDA is less useful in situations that require representationalpower they afford?for example, the contextual similarity paradigm described inSection 3.5.A naive implementation of the sampler will take time linear in the number of topicsand the number of observations to complete one iteration.
Yao, Mimno, and McCallum(2009) present a new sampling algorithm for LDA that yields a considerable speedup byreformulating Equation (30) to allow caching of intermediate values and an intelligentsorting of topics so that in many cases only a small number of topics need be iteratedthough before assigning a topic to an observation.
In this article we use Yao, Mimno,& McCallum?s algorithm for LDA, as well as a transformation of the ROOTH-LDA andLEX-LDA samplers that can be derived in an analogous fashion.3.4.2 Inference.
As noted previously, the Gibbs sampling procedure is guaranteed toconverge to the true posterior after a finite number of iterations; however, this numberis unknown and it is difficult to detect convergence.
In practice, we run the samplerfor a hopefully sufficient number of iterations and perform inference based on thefinal sampling state (assignments of all z and s variables) and/or a set of intermediatesampling states.In the case of the LDA model, the selectional preference probability P(w|c) isestimated using posterior mean estimates of ?c and ?z:P(w|c) =?zP(z|c)P(w|z) (36)P(z|c) =fzc + ?zfc +?z?
?z?
(37)P(w|z) =fzw + ?fz + |W|?
(38)605Computational Linguistics Volume 40, Number 3For ROOTH-LDA, the joint probability P(c, w) is given byP(c, w) =?zP(c|z)P(w|z)P(z) (39)P(z) =fz + ?z|O|+?z?
?z?
(40)P(w|z) =fzw + ?fz + |W|?
(41)P(c|z) =fzc + ?fz + |C|?
(42)For LEX-LDA, P(w|c) is given byP(w|c) = P(?
= 1|c)Plex(w|c) + P(?
= 0|c)Pclass(w|c) (43)P(?
= 1|c) =fc,s=1 + ?1fc + ?0 + ?1(44)P(?
= 0|c) = 1 ?
P(?
= 1|c) (45)Plex(w|c) =fwc,s=1 + ?fc,s=1 + |W|?
(46)Pclass(w|c) =?zP(z|c)P(w|z) (47)P(z|c) =fzc + ?zfc,s=0 +?z?
?z?
(48)P(w|z) =fzw + ?fz + |W|?
(49)Given a sequence or chain of sampling states S1, .
.
.
, Sn, we can predict a value forP(w|c) or P(c, w) using these equations and the set of latent variable assignments at asingle state Si.
As the sampler is initialized randomly and will take time to find a goodarea of the search space, it is standard to wait until a number of iterations have passedbefore using any samples for prediction.
States S1, .
.
.
, Sb from this burn-in period arediscarded.For predictive stability it can be beneficial to average over predictions computedfrom more than one sampling state; for example, we can produce an averaged estimateof P(w|c) from a set of states S:P(w|c) = 1|S|?Si?SPSi (w|c) (50)It is also possible to average over states drawn from multiple chains.
However,averaging of any kind can only be performed on quantities whose interpretation doesnot depend on the sampling state itself.
For example, we cannot average over estimatesof P(z1|c) drawn from different samples as the topic called z1 in one iteration is notidentical to the topic called z1 in another; even within the same chain, the meaning ofa topic will often change gradually from state to state.606O?
Se?aghdha and Korhonen Probabilistic Distributional Semantics3.4.3 Choosing |Z|.
In the ?parametric?
latent variable models used here the numberof topics or semantic classes, |Z|, must be fixed in advance.
This brings significantefficiency advantages but also the problem of choosing an appropriate value for |Z|.
Themore classes a model has, the greater its capacity to capture fine distinctions betweenentities.
However, this finer granularity inevitably comes at a cost of reduced general-ization.
One approach is to choose a value that works well on training or developmentdata before evaluating held-out test items.
Results in lexical semantics are often reportedover the entirety of a data set, meaning that if we wish to compare those results wecannot hold out any portion.
If the method is relatively insensitive to the parameter itmay be sufficient to choose a default value.
Rooth et al.
(1999) suggest cross-validatingon the training data likelihood (and not on the ultimate evaluation measure).
An alter-native solution is to average the predictions of models trained with different choicesof |Z|; this avoids the need to pick a default and can give better results than any onevalue as it integrates contributions at different levels of granularity.
As mentioned inSection 3.4.2 we must take care when averaging predictions to compute with quan-tities that do not rely on topic identity?for example, estimates of P(a|p) can safely becombined whereas estimates of P(z1|p) cannot.3.4.4 Hyperparameter Estimation.
Although the likelihood parameters can be integratedout, the parameters for the Dirichlet and Beta priors (often referred to as ?hyperparame-ters?)
cannot and must be specified either manually or automatically.
The value of theseparameters affects the sparsity of the learned posterior distributions.
Furthermore, theuse of an asymmetric prior (where not all its parameters have equal value) implementsan assumption that some observation values are more likely than others before anyobservations have been made.
Wallach, Mimno, and McCallum (2009) demonstrate thatthe parameterization of the Dirichlet priors in an LDA model has a material effecton performance, recommending in conclusion a symmetric prior on the ?emission?likelihood P(w|z) and an asymmetric prior on the document topic likelihoods P(z|d).
Inthis article we follow these recommendations and, like Wallach, Mimno, and McCallum,we optimize the relevant hyperparameters using a fixed point iteration to maximizethe log evidence (Minka 2003; Wallach 2008).3.5 Measuring Similarity in Context with Latent-Variable ModelsThe representation induced by latent variable selectional preference models also allowsus to capture the disambiguatory effect of context.
Given an observation of a word ina context, we can infer the most probable semantic classes to appear in that contextand we can also infer the probability that a class generated the observed word.
Wecan also estimate the probability that the semantic classes suggested by the observationwould have licensed an alternative word.
Taken together, these can be used to estimatein-context semantic similarity.
The fundamental intuitions are similar to those behindthe vector-space models in Section 2.3.2, but once again we are viewing them from theperspective of probabilistic modeling.The basic idea is that we identify the similarity between an observed term wo and analternative term ws in context C with the similarity between the probability distributionover latent variables associated with wo and C and the probability distribution overlatent variables associated with ws:sim(wo, ws|C) = sim(P(z|wo, C), P(z|ws)) (51)607Computational Linguistics Volume 40, Number 3This assumes that we can associate a distribution over the same set of latent variableswith each context item c ?
C. As noted in Section 2.3.2, previous research has found thatconditioning the representation of both the observed term and the candidate substituteon the context gives worse performance than conditioning the observed term alone; wealso found a similar effect.
Dinu and Lapata (2010) present a specific version of thisframework, using a window-based definition of context and the assumption that thesimilarity given a set of contexts is the product of the similarity value for each context:simDL10(wo, ws|C) =?c?Csim(P(z|wo, c), P(z|ws)) (52)In this article we generalize to syntactic as well as window-based contexts and alsoderive a well-motivated approach to incorporating multiple contexts inside the prob-ability model; in Section 4.5 we show that both innovations contribute to improvedperformance on a lexical substitution data set.The distributions we use for prediction are as follows.
Given an LDA latent variablepreference model that generates words given a context, it is straightforward to calculatethe distribution over latent variables conditioned on an observed context?word pair:PC?T(z|wo, c) =P(wo|z)P(z|c)?z?
P(wo|z?
)P(z?|c)(53)Given a set of multiple contexts C, each of which has an opinion about the distributionover latent variables, this becomesP(z|wo, C) =P(wo|z)P(z|C)?z?
P(wo|z?
)P(z?|C)(54)P(z|C) =?c?C P(z|c)?z?
?c?C P(z?|c)(55)The uncontextualized distribution P(z|ws) is not given directly by the LDA model.
Itcan be estimated from relative frequencies in the Gibbs sampling state; we use anunsmoothed estimate.8 We denote this model C ?
T to note that the target word isgenerated given the context.Where the context?word relationship is asymmetric (as in the case of syntacticdependency contexts), we can alternatively learn a model that generates contexts givena target word; we denote this model T ?
C:PT?C(z|wo, c) =P(z|wo)P(c|z)?z?
P(z?|wo)P(c|z?
)(56)Again, we can generalize to non-singleton context sets:P(z|wo, C) =P(z|wo)P(C|z)?z?
P(z?|wo)P(C|z?
)(57)8 In the notation of Section 3.4, this estimate is given by fzwsfws.608O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticswhereP(C|z) =?c?CP(c|z) (58)Equation (57) has the form of a ?product of experts?
model (Hinton 2002), thoughunlike many applications of such models we train the experts independently and thusavoid additional complexity in the learning process.
The uncontextualized distributionP(z|ws) is an explicit component of the T ?
C model.An analogous definition of similarity can be derived for the ROOTH-LDA model.Here there is no asymmetry as the context and target are generated jointly.
The distri-bution over topics for a context c and target word wo is given byPROOTH-LDA (z|wo, c) =P(wo, c|z)P(z)?z?
P(wo, c|z?)P(z?
)(59)while calculating the uncontextualized distribution P(z|ws) requires summing over theset of possible contexts C?
:PROOTH-LDA (z|ws) =P(z)?c??C?
P(ws, c|z)?z?
P(z?)?c??C?
P(ws, c|z?
)(60)Because the interaction classes learned by ROOTH-LDA are specific to a relation type,this model is less applicable than LDA to problems that involve a rich context set C.Finally, we must choose a measure of similarity between probability distributions.The information theory literature has provided many such measures; in this article weuse the Bhattacharyya coefficient (Bhattacharyya 1943):simbhatt(Px(z), Py(z)) =?z?Px(z)Py(z) (61)One could alternatively use similarities derived from probabilistic divergences suchas the Jensen?Shannon Divergence or the L1 distance (Lee 1999; O?
Se?aghdha andCopestake 2008).3.6 Related WorkAs related earlier, non-Bayesian mixture or latent-variable approaches to co-occurrencemodeling were proposed by Pereira, Tishby, and Lee (1993) and Rooth et al.
(1999).Blitzer, Globerson, and Pereira (2005) describe a co-occurrence model based on adifferent kind of distributed latent-variable architecture similar to that used in theliterature on neural language models.
Brody and Lapata (2009) use the clustering effectsof LDA to perform word sense induction.
Vlachos, Korhonen, and Ghahramani (2009)use non-parametric Bayesian methods to cluster verbs according to their co-occurrenceswith subcategorization frames.
Reisinger and Mooney (2010, 2011) have alsoinvestigated Bayesian methods for lexical semantics in a spirit similar to that adoptedhere.
Reisinger and Mooney (2010) describe a ?tiered clustering?
model that, like LEX-LDA, mixes a cluster-based preference model with a predicate-specific distribution over609Computational Linguistics Volume 40, Number 3words; however, their model does not encourage sharing of classes between differentpredicates.
Reisinger and Mooney (2011) propose a very interesting variant of the latent-variable approach in which different kinds of contextual behavior can be explainedby different ?views,?
each of which has its own distribution over latent variables; thismodel can give more interpretable classes than LDA for higher settings of |Z|.Some extensions of the LDA topic model incorporate local as well as documentcontext to explain lexical choice.
Griffiths et al.
(2004) combine LDA and a hiddenMarkov model (HMM) in a single model structure, allowing each word to be drawnfrom either the document?s topic distribution or a latent HMM state conditioned on thepreceding word?s state; Moon, Erk, and Baldridge (2010) show that combining HMMand LDA components can improve unsupervised part-of-speech induction.
Wallach(2006) also seeks to capture the influence of the preceding word, while at the same timegenerating every word from inside the LDA model; this is achieved by conditioningthe distribution over words on the preceding word type as well as on the chosentopic.
Boyd-Graber and Blei (2008) propose a ?syntactic topic model?
that makes topicselection conditional on both the document?s topic distribution and on the topic of theword?s parent in a dependency tree.
Although these models do represent a form oflocal context, they either use a very restrictive one-word window or a notion of syntaxthat ignores lexical or dependency-label effects; for example, knowing that the head ofa noun is a verb is far less informative than knowing that the noun is the direct objectof eat.More generally, there is a connection between the models developed here andlatent-variable models used for parsing (e.g., Petrov et al.
2006).
In such modelseach latent state corresponds to a ?splitting?
of a part-of-speech label so as to pro-duce a finer-grained grammar and tease out intricacies of word?rule ?co-occurrence.
?Finkel, Grenager, and Manning (2007) and Liang et al.
(2007) propose a non-parametricBayesian treatment of state splitting.
This is very similar to the motivation behind anLDA-style selectional preference model.
One difference is that the parsing model mustexplain the parse tree structure as well as the choice of lexical items; another is that in theselectional preference models described here each head?dependent relation is treatedas an independent observation (though this could be changed).
These differences allowour selectional preference models to be trained efficiently on large corpora and, by fo-cusing on lexical choice rather than syntax, to home in on purely semantic information.Titov and Klementiev (2011) extend the idea of latent-variable distributional modelingto do ?unsupervised semantic parsing?
and reason about classes of semantically similarlexicalized syntactic fragments.4.
Experiments4.1 Training CorporaIn our experiments we use two training corpora:BNC the written component of the British National Corpus,9 comprising around90 million words.
The corpus was tagged for part of speech, lemmatized, andparsed with the RASP toolkit (Briscoe, Carroll, and Watson 2006).9 http://www.natcorp.ox.ac.uk/.610O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsCOORDINATION:Cats andc:conj:nc:conj:ndogs runv:ncsubj:n?Cats and dogsn:and:nrunv:ncsubj:nPREDICATION:The cat isv:ncsubj:nv:xcomp:jfierce?The catn:ncmod:jis fiercePREPOSITIONS:The catn:ncmod:iini:dobj:nthe hat?The catn:prep in:nin the hatFigure 4Dependency graph preprocessing.WIKI a Wikipedia dump of over 45 million sentences (almost 1 billion words) tagged,lemmatized, and parsed with the C+C toolkit10 and the fast CCG parser describedby Clark et al.
(2009).Although two different parsers were used, they both have the ability to output gram-matical relations in the RASP format and hence they are interoperable for our purposesas downstream users.
This allows us to construct a combined corpus by simply concate-nating the BNC and WIKI corpora.In order to train our selectional preference models, we extracted word?contextobservations from the parsed corpora.
Prior to extraction, the dependency graph foreach sentence was transformed using the preprocessing steps illustrated in Figure 4.We then filtered for semantically discriminative information by ignoring all words withpart of speech other than common noun, verb, adjective, and adverb.
We also ignoredinstances of the verbs be and have and discarded all words containing non-alphabeticcharacters and all words with fewer than three characters.11As mentioned in Section 2.1, the distributional semantics framework admits flex-ibility in how the practitioner defines the context of a word w. We investigate twopossibilities in this article:Syn The context of w is determined by the syntactic relations r and words w?
incidentto it in the sentence?s parse tree, as illustrated in Section 3.1.10 http://svn.ask.it.usyd.edu.au/trac/candc.11 An exception was made for the word PC as it appears in the Keller and Lapata (2003) data set usedfor evaluation.611Computational Linguistics Volume 40, Number 3Win5 The context of w is determined by the words appearing within a window of fivewords on either side of it.
There are no relation labels, so there is essentially justone relation r to consider.Training topic models on a data set with very large ?documents?
leads to tractabilityissues.
The window-based approach is particularly susceptible to an explosion in thenumber of extracted contexts, as each token in the data can contribute 2 ?
W word?context observations, where W is the window size.
We reduced the data by applyinga simple downsampling technique to the training corpora.
For the WIKI/Syn corpus,all word?context counts were divided by 5 and rounded to the nearest integer.
Forthe WIKI/Win5 corpus we divided all counts by 70; this number was suggested byDinu and Lapata (2010), who used the same ratio for downsampling the similarly sizedEnglish Gigaword Corpus.
Being an order of magnitude smaller, the BNC requiredless pruning; we divided all counts in the BNC/Win5 by 5 and left the BNC/Syncorpus unaltered.
Type/token statistics for the resulting sets of observations are givenin Table 4.4.2 Evaluating Selectional Preference ModelsVarious approaches have been suggested in the literature for evaluating selectionalpreference models.
One popular method is ?pseudo-disambiguation,?
in which a sys-tem must distinguish between actually occurring and randomly generated predicate?argument combinations (Pereira, Tishby, and Lee 1993; Chambers and Jurafsky 2010).In a similar vein, probabilistic topic models are often evaluated by measuring theprobability they assign to held-out data; held-out likelihood has also been used forevaluation in a task involving selectional preferences (Schulte im Walde et al.
2008).These two approaches take a ?language modeling?
approach in which model qualityis identified with the ability to predict the distribution of co-occurrences in unseen text.Although this metric should certainly correlate with the semantic quality of the model, itmay also be affected by frequency and other idiosyncratic aspects of language use unlesstightly controlled.
In the context of document topic modeling, Chang et al.
(2009) findthat a model can have better predictive performance on held-out data while inducingtopics that human subjects judge to be less semantically coherent.In this article we choose to evaluate models by comparing system predictionswith semantic judgments elicited from human subjects.
These judgments take variousforms.
In Section 4.3 we use judgments of how plausible it is that a given predicatetakes a given word as its argument.
In Section 4.4 we use judgments of similarityTable 4Type and token counts for the BNC and BNC+WIKI corpora.BNC BNC+WIKITokens Types Contexts Tokens Types ContextsNouns 18,723,082 122,999 316,237 54,145,216 106,448 514,257Verbs 7,893,462 18,494 57,528 20,082,658 16,673 82,580Adjectives 4,385,788 73,684 37,163 11,536,424 88,488 57,531Adverbs 1,976,837 7,124 14,867 3,017,936 4,056 18,510Window5 28,329,238 88,265 102,792 42,828,094 139,640 143,443612O?
Se?aghdha and Korhonen Probabilistic Distributional Semanticsbetween pairs of predicate?argument combinations.
In Section 4.5 we use judgmentsof substitutability for a target word as disambiguated by its sentential context.
Takentogether, these different experimental designs provide a multifaceted analysis of modelquality.4.3 Predicate?Argument Plausibility4.3.1 Data.
For the plausibility-based evaluation we use a data set of human judgmentscollected by Keller and Lapata (2003).
This comprises data for three grammatical re-lations: verb?object, adjective?noun, and noun?noun modification.
For each relation,30 predicates were selected; each predicate was paired with three noun argumentsfrom different predicate?argument frequency bands in the BNC as well as three nounarguments that were not observed for that predicate in the BNC.
In this way twosubsets (Seen and Unseen) of 90 items each were assembled for each predicate.
Humanplausibility judgments were elicited from a large number of subjects; these numeri-cal judgments were then normalized, log-transformed, and averaged in a MagnitudeEstimation procedure.Predicate Seen Unseendredge channel 0.1875 legend ?0.3221dredge canal 0.2388 sheet ?0.2486dredge rubbish ?0.1999 survivor ?0.2077Following Keller and Lapata (2003), we evaluate our models by measuringthe correlation between system predictions and the human judgments.
Keller andLapata use Pearson?s correlation coefficient r; we additionally use Spearman?s rankcorrelation coefficient ?
for a non-parametric evaluation.
Each system prediction islog-transformed before calculating the correlation to improve the linear fit to the goldstandard.4.3.2 Methods.
We evaluate the LDA, ROOTH-LDA, and LEX-LDA latent-variable pref-erence models, trained on predicate?argument pairs (c, w) extracted from the BNC.We use a default setting |Z| = 100 for the number of classes; in our experiments wehave observed that our Bayesian models are relatively robust to the choice of |Z|.
Weaverage predictions of the joint probability P(c, w) over three independent samples, eachof which is obtained by sampling P(c, w) every 50 iterations after a burn-in period of200 iterations.
ROOTH-LDA gives joint probabilities by definition (25), but LDA andLEX-LDA are defined in terms of conditional probabilities (24).
There are two optionsfor training these models:P ?
A: Model the distribution P(w|c) over arguments for each predicate.A ?
P: Model the distribution P(c|w) over predicates for each argument.As the descriptions suggest, the definition of ?predicate?
and ?argument?
is arbitrary;it is equally valid to talk of the selectional preference of a noun for verbs taking it asa direct object as it is to talk of the preference of a verb for nouns taking it as a directobject.
We expect both configurations to perform comparably on average, though there613Computational Linguistics Volume 40, Number 3may be linguistic or conceptual reasons why one configuration is better than the otherfor specific classes of co-occurrence.To convert conditional probabilities to joint probabilities we multiply by a relative-frequency (MLE) estimate of the probability of the conditioning term:PP?A = P(w|c)P(c) (62)PA?P = P(c|w)P(w) (63)As well as evaluating P ?
A and A ?
P implementations of LDA and LEX-LDA,we can evaluate a combined model P ?
A that simply averages the two sets ofpredictions; this removes the arbitrariness involved in choosing one direction or theother.For comparison, we report the performance figures given by Keller and Lapatafor their search-engine method using AltaVista and Google12 as well as a number ofalternative methods that we have reimplemented and trained on identical data:BNC (MLE) A maximum-likelihood estimate proportional to the co-occurrence fre-quency f (c, w) in the parsed BNC.BNC (KN) BNC relative frequencies smoothed with modified Kneser-Ney (Chen andGoodman 1999).Resnik The WordNet-based association strength of Resnik (1993).
We used WordNetversion 2.1 as the method requires multiple roots in the hierarchy for goodperformance.Clark/Weir The WordNet-based method of Clark and Weir (2002), using WordNet 3.0.This method requires that a significance threshold ?
and significance test bechosen; we investigated a variety of settings and report performance for ?
= 0.9and Pearson?s ?2 test, as this combination consistently gave the best results.Rooth-EM Rooth et al.
(1999)?s latent-variable model without priors, trained with EM.As for the Bayesian models, we average the predictions over three iterations.
Thismethod is very sensitive to the number of classes; as proposed by Rooth et al.,we choose the number of classes from the range (20, 25, .
.
.
, 50) through 5-foldcross-validation on a held-out log-likelihood measure.EPP The vector-space method of Erk, Pado?, and Pado?
(2010), as described in Sec-tion 2.2.3.
We used the cosine similarity measure for smoothing as it performedwell in Erk, Pado?, & Pado?
?s experiments.Disc A discriminative model inspired by Bergsma, Lin, and Goebel (2008) (see Sec-tion 2.2.4).
In order to get true probabilistic predictions, we used a logistic regres-sion classifier with L1 regularization rather than a Support Vector Machine.13We train one classifier per predicate in the Keller and Lapata data set.
FollowingBergsma, Lin, and Goebel, we generate pseudonegative instances for eachpredicate by sampling noun arguments that either do not co-occur with it or havea negative PMI association.
Again following Bergsma, Lin, and Goebel, we usea ratio of two pseudonegative instances for each positive instance and requirepseudonegative arguments to be in the same frequency quintile as the matched12 Keller and Lapata only report Pearson?s r correlations; as we do not have their per-item predictions wecannot calculate Spearman?s ?
correlations or statistical significance scores.13 We used the logistic regression implementation provided by LIBLINEAR (Fan et al.
2008), available athttp://www.csie.ntu.edu.tw/~cjlin/liblinear.614O?
Se?aghdha and Korhonen Probabilistic Distributional Semanticsobserved argument.
The features used for each data instance, correspondingto an argument, are: the conditional probability of the argument co-occurringwith each predicate in the training data; and string-based features capturing thelength and initial and final character n-grams of the argument word.14 We alsoinvestigate whether our LDA model can be used to provide additional featuresfor the discriminative model, by giving the index of the most probable classmaxz P(z|c, w); results for this system are labeled Disc+LDA.In order to test statistical significance of performance differences we use a test forcorrelated correlation coefficients proposed by Meng, Rosenthal, and Rubin (1992).
Thisis more appropriate than a standard test for independent correlation coefficients as ittakes into account the strength of correlation between two sets of system outputs aswell as each output?s correlation with the gold standard.
Essentially, if the two sets ofsystem outputs are correlated there is less chance that their difference will be deemedsignificant.
As we have no a priori reason to believe that one model will perform betterthan another, all tests are two-tailed.4.3.3 Results.
Results on the Keller and Lapata (2003) plausibility data set are presentedin Table 5.15 For common combinations (the Seen data) it is clear that relative corpusfrequency is a reliable indicator of plausibility, especially when Web-scale resources areavailable.
The BNC MLE estimate outperforms the best selectional preference modelon three out of six Seen evaluations, and the AltaVista and Google estimates fromKeller and Lapata (2003) outperforms the best selectional preference model on everyapplicable Seen evaluation.
For the rarer Unseen combinations, however, MLE esti-mates are not sufficient and the latent-variable selectional preference models frequentlyoutperform even the Web-based predictions.
The results for BNC(KN) improve on theMLE estimates for the Unseen data but do not match the models that have a semanticcomponent.It is clear from Table 5 that the new Bayesian latent-variable models outperformthe previously proposed selectional preference models under almost every evaluation.Among the latent-variable models there is no one clear winner, and small differencesin performance are as likely to arise through random sampling variation as throughqualitative differences between models.
That said, ROOTH-LDA and LEX-LDA do scorehigher than LDA in a majority of cases.
As expected, the bidirectional P ?
A modelstend to perform at around the midpoint of the P ?
A and A ?
P models, though theycan also exceed both; this suggests that they are a good choice when there is no intuitivereason to choose one direction over the other.Table 6 aggregates comparisons for all combinations of the six data sets and twoevaluation measures.
As before, all the Bayesian latent-variable models achieve aroughly similar level of performance, consistently outperforming the models selectedfrom the literature and frequently reaching statistical significance (p < 0.05).
Theseresults confirm that LDA-style models can be considered the current state of the artfor selectional preference modeling.14 Bergsma, Lin, and Goebel (2008) also use features extracted from gazetteers.
However, they observe thatadditional features only give a small improvement over co-occurrence features alone.
We do not use suchfeatures here but hypothesize that the improvement would be even smaller in our experiments as thedata do not contain proper nouns.15 Results for LDAP?A and ROOTH-LDA were previously published in O?
Se?aghdha (2010).615Computational Linguistics Volume 40, Number 3Table 5Results (Pearson r and Spearman ?
correlations) on Keller and Lapata?s (2003) plausibility data.Asterisks denote performance figures that are taken from the source paper; all other figures aredrawn from our own (re)implementation trained on identical data.Verb?object Noun?noun Adjective?nounSeen Unseen Seen Unseen Seen Unseenr ?
r ?
r ?
r ?
r ?
r ?AltaVista* .641 ?
.551 ?
.700 ?
.578 ?
.650 ?
.480 ?Google* .624 ?
.520 ?
.692 ?
.595 ?
.641 ?
.473 ?BNC (MLE) .620 .614 .196 .222 .544 .604 .114 .125 .543 .622 .135 .102BNC (KN) .615 .614 .327 .350 .543 .594 .485 .523 .510 .619 .179 .173Resnik .384 .473 .469 .470 .242 .187 .152 .037 .309 .388 .311 .280Clark/Weir .489 .546 .312 .365 .441 .521 .543 .576 .440 .476 .271 .242ROOTH-EM .455 .487 .479 .520 .503 .491 .586 .625 .514 .463 .395 .355EPP .541 .562 .403 .436 .382 .465 .377 .398 .401 .400 .260 .195Disc .318 .318 .376 .354 .331 .294 .258 .250 .188 .274 .303 .327Disc+LDA .328 .338 .473 .476 .308 .285 .266 .292 .228 .308 .333 .368LDAP?A .504 .541 .558 .603 .615 .641 .636 .666 .594 .558 .468 .459LDAA?P .514 .555 .448 .469 .623 .652 .648 .688 .547 .583 .465 .458LDAP?A .513 .546 .530 .542 .619 .645 .653 .697 .593 .570 .467 .445ROOTH-LDA .520 .548 .564 .605 .607 .622 .691 .722 .575 .599 .501 .469LEX-LDAP?A .570 .600 .601 .662 .511 .537 .677 .706 .600 .627 .465 .451LEX-LDAA?P .568 .572 .523 .542 .532 .568 .659 .703 .545 .623 .513 .477LEX-LDAP?A .575 .589 .560 .599 .553 .563 .669 .698 .572 .629 .517 .497Human* .604 ?
.640 ?
.641 ?
.570 ?
.630 ?
.550 ?Table 6Aggregate comparisons for the Keller and Lapata (2003) plausibility data set betweenlatent-variable models (rows) and previously proposed selectional preference models (columns).Cell entries give the number of evaluations (out of 12) in which the latent-variable modeloutperformed the alternative method and the number in which the improvement wasstatistically significant.Resnik Clark/Weir ROOTH-EM EPP DiscLDAP?A 12/5 11/8 12/6 10/5 12/4LDAA?P 10/4 12/8 10/5 10/3 12/4LDAP?A 12/4 11/9 12/6 10/5 12/5ROOTH-LDA 12/6 12/7 12/7 10/5 12/5LEX-LDAP?A 12/5 11/8 12/6 12/5 12/5LEX-LDAA?P 10/4 12/8 10/5 12/5 12/6LEX-LDAP?A 12/4 11/9 12/6 12/5 12/5616O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsOne way of performing error analysis for a given result is to decompose the cor-relation coefficient into a sum of per-item ?pseudo-coefficients.?
For Pearson?s r, thecontribution for the ith item isri =(xi ?
?x)(yi ?
?y)?
?j(xj ?
?x)2?
?j(yj ?
?y)2(64)Spearman?s ?
is equivalent to the r correlation between ranks and so a similar quantitycan be computed.
Table 7 illustrates the items with highest and lowest contributionsfor one evaluation (Spearman?s ?
on the Keller and Lapata Unseen data set).
We haveattempted to identify general factors that predict the difficulty of an item by measuringrank correlation between the per-item pseudo-coefficients and various corpus statis-tics.
However, it has proven difficult to isolate reliable patterns.
One finding is thatarguments with high corpus frequency tend to incur larger errors for the P ?
A latent-variable models and ROOTH-LDA, whereas predicates with high corpus frequency tendto incur smaller errors; with the A ?
P the effect is lessened but not reversed, suggestingthat part of the effect may be inherent in the data set rather than in the prediction model.4.4 Predicate?Argument Similarity4.4.1 Data.
Mitchell and Lapata (2008, 2010) collected human judgments of similaritybetween pairs of predicates and arguments corresponding to minimal sentences.Mitchell and Lapata?s explicit aim was to facilitate evaluation of general semanticcompositionality models but their data sets are also suitable for evaluating predicate?argument representations.Mitchell and Lapata (2008) used the BNC to extract 4 attested subject nouns for eachof 15 verbs, yielding 60 reference combinations.
Each verb?noun tuple was matchedwith two verbs that are synonyms of the reference verb in some contexts but not inTable 7Most- and least-accurately predicted items for the LDAP?A models using per-item Spearman?s ?pseudo-coefficients on the unseen data set, with gold and predicted rank values.Item ri Gold Pred Item ri Gold Predinfluence worker 0.030 3 2 spend life ?0.012 63 1originate miner 0.029 89 86 rank pc ?0.012 21 75undergo container 0.027 90 83 deduct stage ?0.011 79 25litter surface 0.027 7 3 sponsor embassy ?0.010 23 73injure pilot 0.026 2 9 spend error ?0.007 80 30desk tomato 0.028 87 87 guitar conviction ?0.012 82 25pupil morale 0.026 3 9 towel fee ?0.011 11 65landlord committee 0.025 12 1 workshop victim ?0.007 18 60restoration specialist 0.025 1 12 opera recommendation ?0.006 6 54cable manager 0.024 7 8 valuation afternoon ?0.005 70 32superb character 0.032 2 1 tremendous newspaper ?0.014 13 72scientific document 0.032 1 2 continuous clinic ?0.012 75 21valid silk 0.031 89 89 lazy promoter ?0.012 24 79naughty protocol 0.026 84 87 unfair coalition ?0.012 20 73exciting can 0.026 87 84 lazy shadow ?0.010 74 24617Computational Linguistics Volume 40, Number 3Table 8Sample items from the Mitchell and Lapata (2008) data set.shoulder slump 6, 7, 5, 5, 6, 5, 5, 7, 5, 5, 7, 5, 6, 6, 5, 6, 6, 6, 7, 5,7, 6, 6, 5, 5, 5, 5, 6, 6, 7, 7, 7, 7, 7shoulder slouchshoulder slump 2, 5, 4, 4, 3, 3, 2, 3, 2, 1, 3, 3, 6, 5, 3, 2, 1, 1, 1, 7,4, 4, 6, 3, 5, 6shoulder declineTable 9Sample items from the Mitchell and Lapata (2010) data set.stress importance 6, 7, 7, 5, 5, 7, 7, 7, 6, 5, 6, 7, 3, 7, 7, 6, 7, 7emphasize needask man 3, 1, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1stretch armfootball club 7, 6, 7, 6, 6, 5, 5, 3, 6, 6, 4, 5, 4, 6, 2, 7, 5, 5league matcheducation course 7, 7, 5, 5, 7, 5, 5, 7, 7, 4, 6, 2, 5, 6, 6, 7, 7, 4training programothers.
In this way, Mitchell and Lapata created a data set of 120 pairs of predicate?argument combinations.
Similarity judgments were obtained from human subjects foreach pair on a Likert scale of 1?7.
Examples of the resulting data items are given inTable 8.
Mitchell and Lapata use six subjects?
ratings as a development data set forsetting model parameters and the remaining 54 subjects?
ratings for testing.
In thisarticle we use the same split.Mitchell and Lapata (2010) adopt a similar approach to data collection with the dif-ference that instead of keeping arguments constant across combinations in a pair, bothpredicates and arguments vary across comparand combinations.
They also consider arange of grammatical relations: verb?object, adjective?noun, and noun?noun modifica-tion.
Human subjects rated similarity between predicate?argument combinations on a1?7 scale as before; examples are given in Table 9.
Inspection of the data suggests thatthe subjects?
annotation may conflate semantic similarity and relatedness; for example,football club and league match are often given a high similarity score.
Mitchell and Lapataagain split the data into development and testing sections, the former comprising 54subjects?
ratings and the latter comprising 108 subjects?
ratings.Turney (2012) reports, on the basis of personal communication, that Mitchell andLapata (2010) used an involved evaluation procedure that is not described in theiroriginal paper; for each grammatical relation, the annotators are partitioned in threegroups and the Spearman?s ?
correlation computed for each group is combined by av-eraging.16 The analogous approach for the Mitchell and Lapata (2008) data set calculatesa single ?
value by pairing of each annotator-item score with the system prediction forthe appropriate item.
Let s be the sequence of system predictions for |I| items and ya16 We do not compare against the system of Turney (2012) as Turney uses a different experimental designbased on partitioning by phrases rather than annotators.618O?
Se?aghdha and Korhonen Probabilistic Distributional Semanticsbe the scores assigned by annotator a ?
A to those |I| items.
Then the ?concatenated?correlation ?cat is calculated as follows:17ycat = (ya1i1 , ya1i2 , .
.
.
, ya1i|I|, ya2i1 .
.
.
, ya|A|i|I| ) (65)scat = (s1, s2, .
.
.
, s|I|, s1, .
.
.
, s|I|) (66)?cat = ?
(scat, ycat) (67)The length of the ycat and scat sequences is equal to the total number of annotator-itemscores.
For the Mitchell and Lapata (2010) data set, a ?cat value is calculated for eachof the three annotator groups and these are then averaged.
As Turney observes, thisapproach seems to have the effect of underestimating model quality relative to the inter-annotator agreement figure, which is calculated as average intersubject correlation.Therefore, in addition to Mitchell and Lapata?s ?cat evaluation, we also perform anevaluation that computes the average correlation ?ave between the system output andeach individual annotator:?ave =1|A|?a?A?
(s, ya) (68)4.4.2 Models.
For the Mitchell and Lapata (2008) data set we train the following modelson the BNC corpus:LDA An LDA selectional preference model of verb?subject co-occurrence with simi-larity computed as described in Section 3.5.
Similarity predictions sim(n, o|c) areaveraged over five runs.
We consider three models of context?target interaction,which in this case corresponds to verb?subject interaction:LDAC?T Target generation is conditioned on the context, as in equation (53).LDAT?C Context generation is conditioned on the target, as in equation (56).LDAC?T An average of the predictions made by LDAC?T and LDAT?C.As before, we consider a default setting of |Z| = 100.
As well as presenting resultsfor an average over all predictors we investigate whether the choice of predictorscan be optimized by using the development data to select the best subset ofpredictors.Mult Pointwise multiplication (6) using Win5 co-occurrences.We also compare against the best figures reported in previous studies; these also usedthe BNC for training and so should be directly comparable:M+L08 The best-performing system of Mitchell and Lapata (2008), combining an addi-tive and a multiplicative model and using window-based co-occurrences.SVS The best-performing system of Erk and Pado?
(2008); the Structured Vector Spacemodel (8), parameterized to use window-based co-occurrences and raising theexpectation vector values (7) to the 20th power (this parameter was optimizedon the development data).17 In practice the sequence of items is not the same for every annotator and the sequence of predictions smust be changed accordingly.619Computational Linguistics Volume 40, Number 3Table 10Results (?ave averaged across annotators) for the Mitchell and Lapata (2008) similarity data set.Model No Optimization Optimized on Dev|Z| = 100LDAC?T 0.34 0.35LDAT?C 0.39 0.41LDAC?T 0.39 0.41Mult 0.15 ?Human* 0.40 ?For the Mitchell and Lapata (2010) data set we train the following models, again onthe BNC corpus:ROOTH-LDA/Syn A ROOTH-LDA model trained on the appropriate set of syntacticco-occurrences (verb?object, noun?noun modification, or adjective?noun), withthe topic distribution calculated as in Equation (59).LDA/Win5 An LDA model trained on the Win5 window-based co-occurrences.
Be-cause all observations are modeled using the same latent classes, the distributionsP(z|o, c) (Equation (53)) for each word in the pair can be combined by taking anormalized product.Combined This model averages the similarity prediction of the ROOTH-LDA/Syn andLDA/Win5 models.Mult Pointwise multiplication (6) using Win5 co-occurrences.We report results for an average over all predictors as well as for the subset that per-forms best on the development data.
We also list results that were reported by Mitchelland Lapata:M+L10/Mult A multiplicative model (6) using a vector space based on window co-occurrences in the BNC.M+L10/Best The best result for each grammatical relation from any of the semanticspaces and combination methods tested by Mitchell and Lapata.
Some of thesemethods require parameters to be set through optimization on the developmentset.184.4.3 Results.
Results for the Mitchell and Lapata (2008) data set are presented inTables 10 and 11.19 The LDA preference models clearly outperform the previous stateof the art of ?cat = 0.27 (Erk and Pado?
2008), with the best simple average of predictorsscoring ?cat = 0.38, ?ave = 0.41, and the best optimized combination scoring ?cat = 0.39,?ave = 0.41.
This is comparable to the average level of agreement between human judgesestimated by Mitchell and Lapata?s to be ?ave = 0.40.
Optimizing on the developmentdata consistently gave better performance than averaging over all predictors, thoughin most cases the differences are small.18 Ultimately, however, none of the combination methods needing optimization outperform theparameter-free methods in Mitchell and Lapata?s results.19 The results in Table 10 were previously published in O?
Se?aghdha and Korhonen (2011).620O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsTable 11Results (?cat) for the Mitchell and Lapata (2008) similarity data set.Model No Optimization Optimized on Dev|Z| = 100LDAC?T 0.28 0.32LDAT?C 0.38 0.39LDAC?T 0.33 0.38Mult 0.13 ?SVS* 0.27 ?M+L08* 0.19 ?Human* 0.40 ?Table 12Results (?ave averaged across annotators) for the Mitchell and Lapata (2010) similarity data set.Model No Optimization Optimized on DevV?Obj N?N Adj?N V?Obj N?N Adj?NLDA/Win5 0.41 0.56 0.46 0.42 0.58 0.49ROOTH-LDA/Syn 0.42 0.46 0.51 0.42 0.47 0.52Combined 0.44 0.56 0.53 0.46 0.58 0.55Mult/Win5 0.34 0.33 0.34 ?
?
?Human* 0.55 0.49 0.52 ?
?
?Results for the Mitchell and Lapata (2010) data set are presented in Tables 12 andTable 13.20 Again the latent-variable models perform well, comfortably outperformingthe Mult baseline, and with just one exception the Combined models surpass Mitchelland Lapata?s reported results.
Combining the syntactic co-occurrence model ROOTH-LDA/Syn and the window-based model LDA/Win5 consistently gives the best perfor-mance, suggesting that the human ratings in this data set are sensitive to both strictsimilarity and a looser sense of relatedness.
As Turney (2012) observes, the average-?cat-per-group approach of Mitchell and Lapata leads to lower performance figures thanaveraging across annotators; with the latter approach (Table 12) the ?ave correlationvalues approach the level of human interannotator agreement for two of the threerelations: noun?noun and adjective?noun modification.4.5 Lexical Substitution4.5.1 Data.
The data set for the English Lexical Substitution Task (McCarthy and Navigli2009) consists of 2,010 sentences sourced from Web pages.
Each sentence features one of205 distinct target words that may be nouns, verbs, adjectives, or adverbs.
The sentenceshave been annotated by human judges to suggest semantically acceptable substitutesfor their target words.
Table 14 gives example sentences and annotations for the targetverb charge.
For the original shared task the data was divided into development and test20 These results were not previously published.621Computational Linguistics Volume 40, Number 3Table 13Results (?cat averaged across groups) for the Mitchell and Lapata (2010) similarity data set.Model No Optimization Optimized on DevV?Obj N?N Adj?N V?Obj N?N Adj?NLDA/Win5 0.37 0.51 0.42 0.37 0.53 0.44ROOTH-LDA/Syn 0.37 0.42 0.45 0.37 0.43 0.47Combined 0.39 0.51 0.47 0.41 0.53 0.48Mult/Win5 0.31 0.30 0.30 ?
?
?M+L10/Mult* 0.37 0.49 0.46 ?
?
?M+L10/Best* 0.40 0.49 0.46 0.40 0.49 0.46Human* 0.55 0.49 0.52 ?
?
?Table 14Example sentences for the verb charge from the English Lexical Substitution Task.Commission is the amount charged to execute a trade.levy (2), impose (1), take (1), demand (1)Annual fees are charged on a pro-rata basis to correspond with the standardized renewal datein December.levy (2), require (1), impose (1), demand (1)Meanwhile, George begins obsessive plans for his funeral.
.
.
George, suspicious, charges toher room to confront them.run (2), rush (2), storm (1), dash (1)Realizing immediately that strangers have come, the animals charge them and the horsesbegan to fight.attack (5), rush at (1)sections; in this article we follow subsequent work using parameter-free models anduse the whole data set for testing.The gold standard substitute annotations contain a number of multiword termssuch as rush at and generate electricity.
As it is impossible for a standard lexical distri-butional model to reason about such terms, we remove these substitutes from the goldstandard.21 We remove entirely the 17 sentences that have only multiword substitutesin the gold standard, as well as 7 sentences for which no gold annotations are provided.This leaves 1,986 sentences.The original Lexical Substitution Task asked systems to propose substitutes froman unrestricted English vocabulary, though in practice all participants used lexicalresources to constrain the set of substitutes considered.
Most subsequent researchersusing the Lexical Substitution data to evaluate models of contextual meaning haveadopted a slightly different experimental design, in which systems are asked to rankonly the list of attested substitutes for the target word in each sentence.
For example,21 Thater, Fu?rstenau, and Pinkal (2010, 2011) and Dinu and Lapata (2010) similarly remove multiwordparaphrases (Georgiana Dinu, p.c.).622O?
Se?aghdha and Korhonen Probabilistic Distributional Semanticsthe list of substitute candidates for an instance of charge is the union of the substitutelists in the gold standard for every sentence containing charge as a target word: levy,impose, take, demand, require, impose, run, rush, storm, dash, attack,.
.
.
.
Evaluation of systempredictions for a given sentence then involves comparing the ranking produced by thesystem with the implicit ranking produced by annotators, assuming that any candidatesnot attested for the sentence appear with frequency 0 at the bottom of the ranking.Dinu and Lapata (2010) use Kendall?s ?b, a standard rank correlation measure that isappropriate for data containing tied ranks.
Thater, Fu?rstenau, and Pinkal (2010, 2011)use Generalized Average Precision (GAP), a precision-like measure originally proposedby Kishida (2005) for information retrieval:GAP =?ni=1 I(xi)?ik=1 xki?Rj=1 I(yj)?jl=1 ylj(69)where x1, .
.
.
, xn are the ranked candidate scores provided by the system, y1, .
.
.
, yR arethe ranked scores in the gold standard and I(x) is an indicator function with value 1 ifx > 0 and 0 otherwise.In this article we report both ?b and GAP scores, calculated individually for eachsentence and averaged.
The open-vocabulary design of the original Lexical SubstitutionTask facilitated the use of other evaluation measures such as ?precision out of ten?
: theproportion of the first 10 words in a system?s ranked substitute list that are containedin the gold standard annotation for that sentence.
This measure is not appropriatein the constrained-vocabulary scenario considered here; when there are fewer than10 candidate substitutes for a target word, the precision will always be 1.4.5.2 Models.
We apply both window-based and syntactic models of similarity in contextto the lexical substitution data set; we expect the latter to give more accurate predictionsbut to have incomplete coverage when a test sentence is not fully and correctly parsedor when the test lexical items were not seen in the appropriate contexts in training.22 Wetherefore also average the predictions of the two model types in the hope of attainingsuperior performance with full coverage.The models we train on the BNC and combined BNC + WIKI corpora are as follows:Win5 An LDA model using 5-word-window contexts (so |C| ?
10) and similarityP(z|o, C) computed according to Equation (54).C ?
T An LDA model using syntactic co-occurrences with similarity computed accord-ing to Equation (54).T ?
C An LDA model using syntactic co-occurrences with similarity computed accord-ing to Equation (57).T ?
C A model averaging the predictions of the C ?
T and T ?
C models.Win5 + C ?
T, Win5 + T ?
C, Win5 + T ?
C A model averaging the predictions ofWin5 and the appropriate syntactic model.TFP11 The vector-space model of Thater, Fu?rstenau, and Pinkal (2011).
We report fig-ures with and without backoff to lexical similarity between target and substitutewords in the absence of a syntax-based prediction.22 An LDA model cannot make an informative prediction of P(z|o, C) if word o was never seen enteringinto at least one (unlexicalized) syntactic relation in C. Other syntactic models such as that of Thater,Fu?rstenau, and Pinkal (2011) face analogous restrictions.623Computational Linguistics Volume 40, Number 3Table 15Results on the English Lexical Substitution Task data set; boldface denotes best performance atfull coverage for each corpus.BNC BNC + WikipediaGAP ?b %Coverage GAP ?b %CoverageWin5 44.5 0.17 100.0 44.8 0.17 100.0C ?
T 46.8 0.20 86.4 48.7 0.21 86.5T ?
C 47.2 0.21 86.4 49.3 0.22 86.5T ?
C 48.2 0.22 86.4 49.1 0.23 86.5Win5 + C ?
T 46.0 0.18 100.0 48.7 0.21 100.0Win5 + T ?
C 48.6 0.21 100.0 49.3 0.22 100.0Win5 + T ?
C 48.1 0.20 100.0 49.5 0.23 100.0Baseline:No Context 43.8 0.16 100.0 43.7 0.15 100.0No Similarity 39.7 0.14 100.0 40.3 0.14 100.0TFP11:No Backoff 46.8 0.20 84.8 47.7 0.22 84.9+Backoff 46.4 0.19 98.1 47.3 0.21 98.2We also consider two baseline LDA models:No Context A model that ranks substitutes n by computing the Bhattacharyya similar-ity between their topic distributions P(z|n) and the target word topic distributionP(z|o).No Similarity A model that ranks substitutes n by their context-conditioned probabil-ity P(n|C) only; this is essentially a language-modeling approach using syntactic?bigrams.
?We report baseline results for the T ?
C syntactic model, but performance is similarwith other co-occurrence types.Predictions for the LDA models are averaged over five runs for each setting of |Z|in the range {600, 800, 1000, 1200}.
In order to test statistical significance of differencesbetween models we use stratified shuffling (Yeh 2000).234.5.3 Results.
Table 15 presents results on the Lexical Substitution Task data set.24 Asexpected, the window-based LDA models attain good coverage but worse performancethan the syntactic models.
The combined model Win5 + T ?
C trained on BNC+WIKIgives the best scores (GAP = 49.5, ?b = 0.23).
Every combined model gives a statisticallysignificant improvement (p < 0.01) over the corresponding window-based Win5 model.Our TFP11 reimplementation of Thater, Fu?rstenau, and Pinkal (2011) has slightly lessthan complete coverage, and performs worse than almost all combined LDA models.To compute statistical significance we only use the sentences for which TFP11 madepredictions; for both the BNC and BNC+WIKI corpora, the Win5 + T ?
C model23 We use the software package provided by Sebastian Pado?
athttp://www.nlpado.de/~sebastian/sigf.html.24 Results for the LDA models were reported in O?
Se?aghdha and Korhonen (2011).624O?
Se?aghdha and Korhonen Probabilistic Distributional Semanticsgives a statistically significant (p < 0.05) improvement over TFP11 for both GAP and?b, while Win5 + T ?
C gives a significant improvement for GAP and ?b on the BNCtraining corpus.
The no-context and no-similarity baselines are clearly worse than thefull models; this difference is statistically significant (p < 0.01) for both training corporaand all models.Table 16 breaks performance down across the four parts of speech used in thedata set.
Verbs appear to present the most difficult substitution questions and alsodemonstrate the greatest beneficial effect of adding syntactic disambiguation to thebasic Win5 model.
The full Win5 + T ?
C outperforms our reimplementation of Thater,Fu?rstenau, and Pinkal (2011) on all parts of speech for the GAP statistic and on verbs andadjectives for ?b, scoring a tie on nouns and adverbs.
Table 16 also lists results reportedby Dinu and Lapata (2010) and Thater, Fu?rstenau, and Pinkal (2010, 2011) for theirmodels trained on the English Gigaword Corpus.
This corpus is of comparable size tothe BNC+WIKI corpus, but we note that the results reported by Thater, Fu?rstenau, andPinkal (2011) are better than those attained by our reimplementation, suggesting thatuncontrolled factors such as choice of corpus, parser, or dependency representation maybe responsible.
Thater, Fu?rstenau, and Pinkal?s (2011) results remain the best reportedfor this data set; our Win5 + T ?
C results are better than Dinu and Lapata (2010) andThater, Fu?rstenau, and Pinkal (2010) in this uncontrolled setting.5.
ConclusionIn this article we have shown that the probabilistic latent-variable framework providesa flexible and effective toolbox for distributional modeling of lexical meaning and givesstate-of-the-art results on a number of semantic prediction tasks.
One useful feature ofthis framework is that it induces a representation of semantic classes at the same timeas it learns about selectional preference distributions.
This can be viewed as a kind ofcoarse-grained sense induction or as a kind of concept induction.
We have demonstratedthat reasoning about these classes leads to an accurate method for calculating semanticsimilarity in context.
By applying our models we attain state-of-the-art performanceon a range of evaluations involving plausibility prediction, in-context similarity, andTable 16Performance by part of speech, with additional results from Thater, Fu?rstenau, and Pinkal (2010,2011) and Dinu and Lapata (2010).Nouns Verbs Adjectives Adverbs OverallGAP ?b GAP ?b GAP ?b GAP ?b GAP ?bWin5/BNC+WIKI 46.0 0.16 38.9 0.14 44.0 0.18 54.0 0.22 44.8 0.17Win5 + T ?
C 50.7 0.22 45.1 0.20 48.8 0.24 55.9 0.24 49.5 0.23TFP11 (+Backoff) 48.9 0.22 42.5 0.17 46.0 0.22 55.2 0.24 47.3 0.21TFP10* (Model 1) 46.4 ?
45.9 ?
39.4 ?
48.2 ?
44.6 ?TFP10* (Model 2) 42.5 ?
?
?
43.2 ?
51.4 ?
?
?TFP11* (+Backoff) 52.9 ?
48.8 ?
51.1 ?
55.3 ?
51.7 ?DL10* (LDA) ?
0.16 ?
0.14 ?
0.17 ?
0.21 ?
0.16DL10* (NMF) ?
0.15 ?
0.14 ?
0.16 ?
0.26 ?
0.16625Computational Linguistics Volume 40, Number 3lexical substitution.
The three models we have investigated?LDA, ROOTH-LDA andLEX-LDA?all perform at a similar level for predicting plausibility, but in other casesthe representation induced by one model may be more suitable than the others.In future work, we anticipate that the same intuitions may lead to similarity accu-rate methods for other tasks where disambiguation is required; an obvious candidatewould be traditional word sense disambiguation, perhaps in combination with theprobabilistic WordNet-based preference models of O?
Se?aghdha and Korhonen (2012).More generally, we expect that latent-variable models will prove useful in applicationswhere other selectional preference models have been applied, for example, metaphorinterpretation and semantic role labeling.A second route for future work is to enrich the semantic representations that arelearned by the model.
As previously mentioned, probabilistic generative models aremodular in the sense that they can be integrated in larger models.
Bayesian methodsfor learning tree structures could be applied to learn taxonomies of semantic classes(Blei, Griffiths, and Jordan 2010; Blundell, Teh, and Heller 2010).
Borrowing ideasfrom Bayesian hierarchical language modeling (Teh 2006), one could build a model ofselectional preference and disambiguation in the context of arbitrarily long dependencypaths, relaxing our current assumption that only the immediate neighbors of a targetword affect its meaning.
Our class-based preference model also suggests an approachto identifying regular polysemy alternation by finding class co-occurrences that repeatacross words, offering a fully data-driven alternative to polysemy models based onWordNet (Boleda, Pado?, and Utt 2012).
In principle, any structure that can be reasonedabout probabilistically, from syntax trees to coreference chains or semantic relations, canbe coupled with a selectional preference model to incorporate disambiguation or lexicalsmoothing in a task-oriented architecture.AcknowledgmentsThe work in this article was funded by theEPSRC (grant EP/G051070/1) and by theRoyal Society.
We are grateful to Frank Kellerand Mirella Lapata for sharing their data setof plausibility judgments; to GeorgianaDinu, Karl Moritz Hermann, Jeff Mitchell,Sebastian Pado?, and Andreas Vlachos foroffering information and advice; and to theanonymous Computational Linguisticsreviewers, whose suggestions havesubstantially improved the quality of thisarticle.ReferencesAbney, Steven and Marc Light.
1999.
Hidinga semantic hierarchy in a Markov model.In Proceedings of the ACL-99 Workshop onUnsupervised Learning in NLP, pages 1?8,College Park, MD.Altmann, Gerry T. M. and Yuki Kamide.1999.
Incremental interpretation ofverbs: Restricting the domain ofsubsequent reference.
Cognition,73(3):247?264.Asuncion, Arthur, Max Welling, PadhraicSmyth, and Yee Whye Teh.
2009.
Onsmoothing and inference for topic models.In Proceedings of the 25th Conference onUncertainty in Artificial Intelligence(UAI-09), pages 27?34, Montreal.Bergsma, Shane, Dekang Lin, and RandyGoebel.
2008.
Discriminative learning ofselectional preferences from unlabeledtext.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP-08), pages 59?68,Honolulu, HI.Bhattacharyya, A.
1943.
On a measureof divergence between two statisticalpopulations defined by their probabilitydistributions.
Bulletin of the CalcuttaMathematical Society, 35:99?110.Bicknell, Klinton, Jeffrey L. Elman, MaryHare, Ken McRae, and Marta Kutas.
2010.Effects of event knowledge in processingverbal arguments.
Journal of Memory andLanguage, 63(4):489?505.Blei, David M., Thomas L. Griffiths, andMichael I. Jordan.
2010.
The nestedChinese restaurant process and Bayesiannonparametric inference of topichierarchies.
Journal of the ACM, 57(2):1?30.Blei, David M., Andrew Y. Ng, and Michael I.Jordan.
2003.
Latent Dirichlet allocation.626O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsJournal of Machine Learning Research,3:993?1,022.Blitzer, John, Amir Globerson, andFernando Pereira.
2005.
Distributed latentvariable models of lexical co-occurrences.In Proceedings of the 10th InternationalWorkshop on Artificial Intelligence andStatistics (AISTATS-05), pages 25?32,Barbados.Blundell, Charles, Yee Whye Teh, andKatherine A. Heller.
2010.
Bayesianrose trees.
In Proceedings of the 26thConference on Uncertainty in ArtificialIntelligence (UAI-10), pages 65?72,Catalina Island, CA.Boleda, Gemma, Sebastian Pado?, andJason Utt.
2012.
Regular polysemy:A distributional model.
In Proceedingsof *SEM-12, pages 151?160, Montreal.Boyd-Graber, Jordan and David M. Blei.2008.
Syntactic topic models.
In Proceedingsof the 22nd Annual Conference on NeuralInformation Processing Systems (NIPS-08),pages 185?192, Vancouver.Briscoe, Ted.
2006.
An introduction to tagsequence grammars and the RASPsystem parser.
Technical Report 662,Computer Laboratory, University ofCambridge.Briscoe, Ted, John Carroll, and RebeccaWatson.
2006.
The second release ofthe RASP system.
In Proceedings of theACL-06 Interactive Presentation Sessions,pages 77?80, Sydney.Brody, Samuel and Mirella Lapata.
2009.Bayesian word sense induction.In Proceedings of EACL-09, pages 103?111,Athens.Chambers, Nathanael and Dan Jurafsky.2010.
Improving the use of pseudo-wordsfor evaluating selectional preferences.In Proceedings of the 48th Annual Meetingof the Association for ComputationalLinguistics (ACL-10), pages 445?453,Uppsala.Chang, Jonathan, Jordan Boyd-Graber, SeanGerrish, Chong Wang, and David M. Blei.2009.
Reading tea leaves: How humansinterpret topic models.
In Proceedingsof the 23rd Annual Conference on NeuralInformation Processing Systems (NIPS-09),pages 288?296, Vancouver.Chen, Stanley F. and Joshua Goodman.1999.
An empirical study of smoothingtechniques for language modeling.Computer Speech and Language,13(4):359?393.Chomsky, Noam.
1957.
Syntactic Structures.Mouton de Gruyter, Berlin.Ciaramita, Massimiliano and Mark Johnson.2000.
Explaining away ambiguity:Learning verb selectional preferencewith Bayesian networks.
In Proceedingsof the 18th International Conference onComputational Linguistics (COLING-00),pages 187?193, Saarbru?cken.Clark, Stephen, Ann Copestake, James R.Curran, Yue Zhang, Aurelie Herbelot,James Haggerty, Byung-Gyu Ahn,Curt Van Wyk, Jessika Roesner, JonathanKummerfeld, and Tim Dawborn.
2009.Large-scale syntactic processing: Parsingthe Web.
Technical report, Final Reportof the 2009 JHU CLSP Workshop.Clark, Stephen and David Weir.
2002.Class-based probability estimation usinga semantic hierarchy.
ComputationalLinguistics, 28(2):187?206.Cordier, Brigitte.
1965.
Factor-analysisof correspondences.
In Proceedings ofthe 1965 International Conference onComputational Linguistics (COLING-65),New York, NY.Curran, James.
2003.
From Distributionalto Semantic Similarity.
Ph.D. thesis,School of Informatics, University ofEdinburgh.Dagan, Ido, Lillian Lee, and Fernando C. N.Pereira.
1999.
Similarity-based models ofword co-occurrence probabilities.
MachineLearning, 34(1):34?69.Dinu, Georgiana and Mirella Lapata.
2010.Measuring distributional similarityin context.
In Proceedings of the 2010Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-10),pages 1,162?1,172, Cambridge, MA.Erk, Katrin.
2007.
A simple, similarity-based model for selectional preferences.In Proceedings of the 45th Annual Meetingof the Association for ComputationalLinguistics (ACL-07), pages 216?223,Prague.Erk, Katrin and Sebastian Pado?.
2008.A structured vector space model for wordmeaning in context.
In Proceedings of the2008 Conference on Empirical Methods inNatural Language Processing (EMNLP-08),pages 897?906, Honolulu, HI.Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.2010.
A flexible, corpus-driven model ofregular and inverse selectional preferences.Computational Linguistics, 36(4):723?763.Essen, Ute and Volker Steinbiss.
1992.Co-occurrence smoothing for stochasticlanguage modeling.
In Proceedings of the1992 IEEE International Conference onAcoustics, Speech, and Signal Processing627Computational Linguistics Volume 40, Number 3(ICASSP-92), pages 161?164,San Francisco, CA.Evert, Stefan.
2004.
The Statistics of WordCo-occurrences: Word Pairs and Collocations.Ph.D.
thesis, Institut fu?r maschinelleSprachverarbeitung, Universita?tStuttgart.Fan, Ron-En, Kai-Wei Chang, Cho-Jui Hsieh,Xiang-Rui Wang, and Chih-Jen Lin.
2008.LIBLINEAR: A library for large linearclassification.
Journal of Machine LearningResearch, 9:1,871?1,874.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Finkel, Jenny Rose, Trond Grenager, andChristopher D. Manning.
2007.
The infinitetree.
In Proceedings of the 45th AnnualMeeting of the Association for ComputationalLinguistics (ACL-07), pages 272?279,Prague.Frith, J. R. 1957.
A Synopsis of LinguisticTheory 1930-1955.
In Studies in LinguisticAnalysis.
Oxford Philological Society,Oxford.Gelman, Andrew, John B. Carlin, Hal S.Stern, and Donald B. Rubin.
2003.
BayesianData Analysis.
Chapman and Hall/CRC, Boca Raton, FL, 2nd edition.Gildea, Daniel and Daniel Jurafsky.
2002.Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.Goldwater, Sharon, Thomas L. Griffiths,and Mark Johnson.
2011.
Producingpower-law distributions and dampingword frequencies with two-stagelanguage models.
Journal of MachineLearning Research, 12:2,335?2,382.Grefenstette, Edward and MehrnooshSadrzadeh.
2011.
Experimental support fora categorical compositional distributionalmodel of meaning.
In Proceedingsof EMNLP-11, pages 1,394?1,404,Edinburgh, UK.Griffiths, Thomas L. and Mark Steyvers.2004.
Finding scientific topics.
Proceedingsof the National Academy of Sciences,101(Suppl.
1):5,228?5,235.Griffiths, Thomas L., Mark Steyvers,David M. Blei, and Joshua B. Tenenbaum.2004.
Integrating topics and syntax.In Proceedings of the 18th AnnualConference on Neural Information ProcessingSystems (NIPS-04), pages 537?544,Vancouver.Grishman, Ralph and John Sterling.
1993.Smoothing of automatically generatedselectional constraints.
In Proceedings ofthe ARPA Human Language TechnologyWorkshop (HLT-93), pages 254?259,Plainsboro, NJ.Harper, Kenneth E. 1965.
Measurement ofsimilarity between nouns.
In Proceedingsof the 1965 International Conference onComputational Linguistics (COLING-65),New York, NY.Harris, Zellig.
1954.
Distributional structure.Word, 10(23):146?162.Heinrich, Gregor.
2009.
Parameter estimationfor text analysis.
Technical report,Fraunhofer IGD.Hinton, Geoffrey E. 2002.
Training productsof experts by minimizing contrastivedivergence.
Neural Computation,14(8):1,771?1,800.Katz, Jerrold J. and Jerry A. Fodor.
1963.
Thestucture of a semantic theory.
Language,39(2):170?210.Keller, Frank and Mirella Lapata.
2003.
Usingthe Web to obtain frequencies for unseenbigrams.
Computational Linguistics,29(3):459?484.Kishida, Kazuaki.
2005.
Property of averageprecision and its generalization: Anexamination of evaluation indicatorfor information retrieval experiments.Technical Report NII-2005-014E, NationalInstitute of Informatics, Tokyo, Japan.Lee, Lillian.
1999.
Measures of distributionalsimilarity.
In Proceedings of the 37th AnnualMeeting of the Association for ComputationalLinguistics (ACL-99), pages 25?32,College Park, MD.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics,24(2):217?244.Liang, Percy, Slav Petrov, Michael Jordan,and Dan Klein.
2007.
The infinite PCFGusing hierarchical Dirichlet processes.In Proceedings of the 2007 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP-07), pages 688?697,Prague.McCarthy, Diana and John Carroll.
2003.Disambiguating nouns, verbs andadjectives using automatically acquiredselectional preferences.
ComputationalLinguistics, 29(4):639?654.McCarthy, Diana and Roberto Navigli.2009.
The English lexical substitutiontask.
Language Resources and Evaluation,43(2):139?159.McCarthy, Diana, Sriram Venkatapathy,and Aravind K. Joshi.
2007.
Detectingcompositionality of verb-objectcombinations using selectionalpreferences.
In Proceedings of the 2007 Joint628O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsConference on Empirical Methods inNatural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL-07), pages 369?379,Prague.Meng, Xiao-Li, Robert Rosenthal, andDonald B. Rubin.
1992.
Comparingcorrelated correlation coefficients.Psychological Bulletin, 111(1):172?175.Minka, Thomas P. 2003.
Estimating aDirichlet distribution.
Available athttp://research.microsoft.com/en-us/um/people/minka/papers/dirichlet/.Mitchell, Jeff and Mirella Lapata.
2008.Vector-based models of semanticcomposition.
In Proceedings of the46th Annual Meeting of the Associationfor Computational Linguistics (ACL-08),pages 236?244, Columbus, OH.Mitchell, Jeff and Mirella Lapata.
2010.Composition in distributional modelsof semantics.
Cognitive Science,34(8):1,388?1,429.Moon, Taesun, Katrin Erk, and JasonBaldridge.
2010.
Crouching Dirichlet,hidden Markov model: Unsupervised POStagging with context local tag generation.In Proceedings of the 2010 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP-10), pages 196?206,Cambridge, MA.O?
Se?aghdha, Diarmuid.
2010.
Latentvariable models of selectional preference.In Proceedings of the 48th Annual Meetingof the Association for ComputationalLinguistics (ACL-10), pages 435?444,Uppsala.O?
Se?aghdha, Diarmuid and Ann Copestake.2008.
Semantic classification withdistributional kernels.
In Proceedingsof the 22nd International Conference onComputational Linguistics (COLING-08),pages 649?655, Manchester.O?
Se?aghdha, Diarmuid and Anna Korhonen.2011.
Probabilistic models of similarity insyntactic context.
In Proceedings of the2011 Conference on Empirical Methods inNatural Language Processing (EMNLP-11),pages 1,047?1,057, Edinburgh.O?
Se?aghdha, Diarmuid and Anna Korhonen.2012.
Modelling selectional preferences ina lexical hierarchy.
In Proceedings of the 1stJoint Conference on Lexical and ComputationalSemantics (*SEM-12), pages 170?179,Montreal.Pado?, Sebastian and Mirella Lapata.
2007.Dependency-based construction ofsemantic space models.
ComputationalLinguistics, 33(2):161?199.Pantel, Patrick, Rahul Bhagat, BonaventuraCoppola, Timothy Chklovski, and EduardHovy.
2007.
ISP: Learning inferentialselectional preferences.
In Proceedings ofNAACL-07, pages 564?571, Rochester, NY.Pereira, Fernando, Naftali Tishby, and LillianLee.
1993.
Distributional clustering ofEnglish words.
In Proceedings of the 31stAnnual Meeting of the Association forComputational Linguistics (ACL-93),pages 183?190, Columbus, OH.Petrov, Slav, Leon Barrett, Romain Thibaux,and Dan Klein.
2006.
Learning accurate,compact, and interpretable treeannotation.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics(COLING-ACL-06), pages 433?440, Sydney.Rayner, Keith, Tessa Warren, Barbara J.Juhasz, and Simon P. Liversedge.
2004.
Theeffect of plausibility on eye movements inreading.
Journal of Experimental Psychology:Learning Memory and Cognition,30(6):1,290?1,301.Reisinger, Joseph and Raymond Mooney.2010.
A mixture model with sharing forlexical semantics.
In Proceedings of the 2010Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-10),pages 1,173?1,182, Cambridge, MA.Reisinger, Joseph and Raymond Mooney.2011.
Cross-cutting models of lexicalsemantics.
In Proceedings of the 2011Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-11),pages 1,405?1,415, Edinburgh.Resnik, Philip.
1993.
Selection and Information:A Class-Based Approach to LexicalRelationships.
Ph.D. thesis, University ofPennsylvania.Ritter, Alan, Mausam, and Oren Etzioni.2010.
A latent Dirichlet allocation methodfor selectional preferences.
In Proceedings ofthe 48th Annual Meeting of the Associationfor Computational Linguistics (ACL-10),pages 424?434, Uppsala.Rooth, Mats, Stefan Riezler, Detlef Prescher,Glenn Carroll, and Franz Beil.
1999.Inducing a semantically annotated lexiconvia EM-based clustering.
In Proceedings ofthe 37th Annual Meeting of the Associationfor Computational Linguistics (ACL-99),pages 104?111, College Park, MD.Russell, Bertrand.
1940.
An Inquiry intoMeaning and Truth.
George Allen andUnwin, London.Schulte im Walde, Sabine, Christian Hying,Christian Scheible, and Helmut Schmid.629Computational Linguistics Volume 40, Number 32008.
Combining EM training and theMDL principle for an automaticverb classification incorporatingselectional preferences.
In Proceedingsof ACL-08: HLT, pages 496?504,Columbus, OH.Shutova, Ekaterina.
2010.
Automaticmetaphor interpretation as a paraphrasingtask.
In Proceedings of Human LanguageTechnologies: The 2010 Annual Conferenceof the North American Chapter of theAssociation for Computational Linguistics(NAACL-HLT-10), pages 1,029?1,037,Los Angeles, CA.Socher, Richard, Eric H. Huang, JeffreyPennington, Andrew Y. Ng, andChristopher D. Manning.
2011.
Dynamicpooling and unfolding recursiveautoencoders for paraphrase detection.In Proceedings of the 25th AnnualConference on Neural Information ProcessingSystems (NIPS-11), pages 801?809,Granada.Spa?rck Jones, Karen.
1964.
Synonymy andSemantic Classification.
Ph.D. thesis,University of Cambridge.Teh, Yee Whye.
2006.
A hierarchical Bayesianlanguage model based on Pitman-Yorprocesses.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics(COLING-ACL-06), pages 985?992,Sydney.Thater, Stefan, Hagen Fu?rstenau, andManfred Pinkal.
2010.
Contextualizingsemantic representations usingsyntactically enriched vector models.In Proceedings of the 48th Annual Meetingof the Association for ComputationalLinguistics (ACL-10), pages 948?957,Uppsala.Thater, Stefan, Hagen Fu?rstenau, andManfred Pinkal.
2011.
Word meaning incontext: A simple and effective vectormodel.
In Proceedings of the 5th InternationalJoint Conference on Natural LanguageProcessing (IJCNLP-11), pages 1,134?1,143,Hyderabad.Titov, Ivan and Alexandre Klementiev.2011.
A Bayesian model for unsupervisedsemantic parsing.
In Proceedings of the49th Annual Meeting of the Associationfor Computational Linguistics,pages 1,445?1,455, Portland, OR.Turney, Peter D. 2012.
Domain and function:A dual-space model of semantic relationsand compositions.
Journal of ArtificialIntelligence Research, 44:533?585.Turney, Peter D. and Patrick Pantel.
2010.From frequency to meaning: Vectorspace models of semantics.
Journalof Artificial Intelligence Research,37:141?188.Vlachos, Andreas, Anna Korhonen, andZoubin Ghahramani.
2009.
Unsupervisedand constrained Dirichlet process mixturemodels for verb clustering.
In Proceedingsof the EACL-09 Workshop on GeometricalModels of Natural Language Semantics(GEMS-09), pages 74?82, Athens.Wallach, Hanna, David Mimno, andAndrew McCallum.
2009.
RethinkingLDA: Why priors matter.
In Proceedingsof the 23rd Annual Conference on NeuralInformation Processing Systems(NIPS-09),pages 1,973?1,981, Vancouver.Wallach, Hanna M. 2006.
Topic modeling:Beyond bag-of-words.
In Proceedings of the23rd International Conference on MachineLearning (ICML-06), pages 977?984,Pittsburgh, PA.Wallach, Hanna M. 2008.
Structured topicmodels for language.
Ph.D. thesis,University of Cambridge.Weeds, Julie and David Weir.
2005.Co-occurrence retrieval: A flexibleframework for lexical distributionalsimilarity.
Computational Linguistics,31(4):439?476.Wilks, Yorick.
1978.
Making preferencesmore active.
Artificial Intelligence,11:197?225.Yao, Limin, Aria Haghighi, Sebastian Riedel,and Andrew McCallum.
2011.
Structuredrelation discovery using generativemodels.
In Proceedings of EMNLP-11,pages 1,456?1,466, Edinburgh.Yao, Limin, David Mimno, and AndrewMcCallum.
2009.
Efficient methods fortopic model inference on streamingdocument collections.
In Proceedings of the15th ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining(KDD-09), pages 937?946, Paris.Yeh, Alexander.
2000.
More accuratetests for the statistical significance ofresult differences.
In Proceedings of the18th Conference on ComputationalLinguistics (COLING-00), pages 947?953,Saarbru?cken.Zapirain, Ben?at, Eneko Agirre, and Llu??sMa`rquez.
2009.
Generalizing over lexicalfeatures: Selectional preferences forsemantic role classification.
In Proceedingsof the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th InternationalJoint Conference on Natural Language630O?
Se?aghdha and Korhonen Probabilistic Distributional SemanticsProcessing of the AFNLP (ACL-IJCNLP-09),pages 73?76, Singapore.Zapirain, Ben?at, Eneko Agirre, Llu?
?sMa`rquez, and Mihai Surdeanu.
2010.Improving semantic role classificationwith selectional preferences.
In Proceedingsof Human Language Technologies: The 2010Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL-HLT-10),pages 373?376, Los Angeles, CA.Zhou, Guangyou, Jun Zhao, Kang Liu,and Li Cai.
2011.
Exploiting Web-derivedselectional preference to improvestatistical dependency parsing.
InProceedings of ACL-11, pages 1,556?1,565,Portland, OR.Zwicky, Arnold M. and Jerrold M. Sadock.1975.
Ambiguity tests and how to failthem.
In John P. Kimball, editor, Syntaxand Semantics 4.
Academic Press,New York, NY.631
