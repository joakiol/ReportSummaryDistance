Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 93?96,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMMAX2 for coreference annotationMateusz Kope?cInstitute of Computer Science, Polish Academy of Sciences,Jana Kazimierza 5, 01-248 Warsaw, Polandm.kopec@ipipan.waw.plAbstractThis article presents major modificationsin the MMAX2 manual annotation tool,which were implemented for the corefer-ence annotation of Polish texts.
Amongother, a new feature of adjudication is de-scribed, as well as some general insightinto the manual annotation tool selectionprocess for the natural language process-ing tasks.1 IntroductionRecently published Polish Coreference Corpus(PCC) (Ogrodniczuk et al., 2013) contains a largenumber of Polish texts annotated manually withcoreference.
During the initial stage of this projectin 2011, a tool had to be selected for the manualtext annotation with coreference.First issue considered during the selection wasthe alternative of desktop versus online annota-tion tool.
Recently, online annotation tools arebecoming increasingly popular (see for exampleBRAT (Stenetorp et al., 2012)), with their advan-tages such as the possibility to monitor the currentstate of annotation and make changes to the anno-tation tool easily, without the need to communi-cate with the annotators.
However, in our opinionthe choice should be made mainly based on theannotators?
preferences, as their work efficiency iscrucial for the cost of the task.10 linguists (which were annotators in previ-ous projects conducted by the authors of this pa-per) were asked in anonymous survey to chooseone of following three options: 1) that they pre-fer an online annotation tool (not requiring anyinstallation), 2) a desktop tool with the possibil-ity to work without constant internet access, 3)that they do not have preference.
Only one per-son chose online tool and one person chose thethird option, leaving no choice for the annota-tion task organizers other than to prepare a desk-top application.
The drawback of this approachwas the need to manage text distribution amongthe annotators, as all the work was done on lo-cal computers.
Distribution was controlled bythe DistSys application, available at the webpagezil.ipipan.waw.pl/DistSys.After some analysis of the features requiredby the project?s scope, the choice was narrowedto only two tools: MMAX2 (M?ller and Strube,2006) and Palinka (Or?asan, 2003).
The problemwith the latter was that is was not error-prone, andlack of publicly available sources did not allowto make project-specific corrections.
ThereforeMMAX2 environment was chosen for the man-ual coreference annotation.
It is a general purposecross-platform desktop annotation tool (written inJava), created to be configurable for many naturallanguage annotation efforts.
It?s source is publiclyavailable at the webpage mmax2.net.
MMAX2interface consists of the main window with thetext being annotated (see for example figure 5) andseveral smaller windows facilitating various tasks(all the other figures).2 Annotation scopeThe annotation scope of the Polish CoreferenceCorpus project consisted of several subtasks foran annotator to perform for each text.
Because theautomatic preannotation was used, annotator?s jobconsisted not only of addition of new markables,but also removal and correction of existing ones.Subtasks to perform were to:?
mark all mentions,?
mark each mention?s semantic head (chooseone word the mention consists of),?
cluster coreferent mentions,?
mark dominant expression of each cluster,93?
mark quasi-identity links.MMAX2 was easy to configure for most of thesubtasks, except the dominant expressions (thereis no possibility to define attributes of clusters,only markables) and the choice of semantic head(available types of attributes did not include a pos-sibility to define a mention-dependent attribute).Because an estimate of inter-annotator agree-ment had to be calculated for the corpus, sometexts were annotated independently by two anno-tators.
Agreement measures were then calculated,but as single gold standard annotation was neededfor the corpus, they also had to be merged into sin-gle version by the adjudicator.
This feature wasalso not present in MMAX2.3 New featuresEven with it?s great number of features, therewas no possibility to use MMAX2 without anychanges in it?s source code.
Some changes werethe result of project?s scope requirements, somewere added in response to annotator requests.
Newimplemented features include:1.
Internationalization ?
the interface of the toolis available in English and Polish and can beeasily translated to other languages.
Polishversion was used by the annotators, but forinternational articles about the tool (such asthis one) the English interface was used.2.
Semantic head selection ?
a dropdown listallows to choose one of the tokens mentionconsists of as it?s semantic head.
This head isalso underlined in markable browser.3.
Storing user setting ?
which windows areopened, where are they located, what is thefont and it?s size ?
these and other user set-tings are saved and automatically restoredwhen the application is reopened.4.
Dominant expressions ?
clusters can havetheir attribute: a dominant expression, whichcan be selected as one of the mentions fromthe cluster or any other expression entered bythe user.5.
Undo button, reverting last action ?
very use-ful feature to revert the last change, regard-less of it?s nature.Figure 1: Adjudication of mentionsFigure 2: Adjudication of clustering6.
Merge two mentions ?
user can merge twomentions into one with a single click, sum-ming their words and merging their clusters.Very useful feature when for example one iscorrecting automatic annotation, which failedto recognize a long named entity name andinstead created two entities, each in it?s sepa-rate cluster.7.
Improved browser operability ?
browsers al-low to operate on mentions, links and clus-ters, not only to view them.8.
Adjudication feature ?
it will be covered indetail in the next section.4 Adjudication featureAdjudication feature of the new Superannotationplugin allows to compare two versions of anno-tation of the same text and merge them into one,adjudicated version.
The design is based on theoriginal MMAX2 Diff plugin, which allowed tosee the differences between two annotations, yetit was not possible to merge them into one.
Thereadability of the differences was also limited andit was improved in our tool.The adjudication process starts with openingone annotation in standard way and then the othervia the menu in Superannotation plugin and con-sist of several steps, each merging particular layer:941.
Mentions ?
first we need to merge mentionannotations.
Differences between the two an-notations are shown in the figure 1.
First col-umn shows the mention content (and this isconstant in all steps of adjudication), secondshows if that mention is in the first annota-tion, third column shows if it is in the secondannotation ("+" if yes, "-" if not).
Single clickat "+" or the first column highlights givenspan in the main window.
Double click at oneof the last two columns selects the clickedversion as the proper one and changes the an-notation in the other file to match the clickedversion.
After such double click, the differ-ence disappears and that row vanishes.
Afterall rows from that step are gone, mention an-notations in both versions are the same andwe can proceed to the next step.2.
Comments ?
this time first column againshows each mention, for which there is adifference in comments in both annotations.Double clicking at 2nd or 3rd column re-solves the difference in given row.3.
Heads ?
similar to comments, by double-clicking we can adjudicate differences inhead annotation.4.
Links ?
analogously as with heads, we mergenear-identity links annotations.5.
Clusters ?
this is the most complex adjudi-cation task.
At this point we surely have thesame set of mentions in both annotations, butthey may be clustered differently.
Figure 2presents how differences in clustering are vi-sualized.
Mentions with the same color inone column are in the same cluster (they havealso the same cluster number).
For example,two occurrences of mention gorzka?
czekolade?are in the same cluster according to the firstannotation, and are singletons according tothe second annotation.
Single click on any ofthese options will show it in the main applica-tion window, while double click will choosethe clicked version as the gold one and updatethe other to match it.6.
Dominating expressions ?
as the clusters arenow the same, the only annotation left con-siders cluster attributes: dominating expres-sions.Figure 3: Mention attributes ?
original MMAX2Figure 4: Mention attributes ?
simplifiedKey point of the adjudication procedure is tomerge all differences at a given level before pro-ceeding to the next one.
This way, after we resolveall differences in the dominating expressions, weare certain that our annotations are fully mergedand in fact the same.5 Removed featuresBecause MMAX2 is a generic tool, the first im-pression is that it is very complicated.
Numer-ous options, many windows and menus are over-whelming and could increase the time of creatinga manual for the annotators (often writing a lot oftext only to inform which options should not bechanged).
Therefore we removed many optionsand simplified the interface to leave only the fea-tures required by our annotation task.
Compare forexample the mention attribute window from theoriginal MMAX2 in figure 4 and in our versionin figure 3.
Removed features included:?
Distinction of multiple annotation levels ?scope of the project considers only one level,and the need to explicitly select it in manyplaces (for example in markable browser) isunnecessary.?
Possibility to edit the base text ?
as we per-95Figure 5: Unnecessary arcsformed the inter-annotator agreement analy-sis, the base text could not be changed.?
Arcs between coreferent mentions in cluster(see figure 5 for original visualization) ?
fromour experience, they decrease the readabilityof the cluster annotation.
As the mentionsin cluster are already highlighted, there is noneed to show the arcs connecting them (thearcs are not clickable as in BRAT).?
MMAX Query Language ?
MMAX2 facili-tates a query language to search for the an-notations fulfilling given properties.
In ouropinion this feature seems more appropriatefor an analyst, not an annotator.
Moreover,results of such querying would be more in-formative for a collection of texts, not a sin-gle document.?
Kappa statistic and coincidence matrix calcu-lation for multiple annotations of a single text?
again, this feature seems more appropriatefor an analyst and for the whole corpus, not asingle text.6 ConclusionEvery unnecessary action, which has to be re-peated numerous times by a human annotator, hasa significant cost in terms of time and money.We claim that annotation efforts are more effi-cient when there is a step of tool customization(or even design and implementation from scratch)beforehand and also during the process, based onthe feedback from the annotators.
Using general-purpose tools has a clear benefit of cheap andfast initialization of the project, but also thereare major drawbacks: a compromise between theproject needs and the tool capabilities.
As we haveseen, even a tool with great customization optionssuch as MMAX2 doesn?t have all the features onewould need.Experience of the PCC project shows, that in-stead of trying to provide a general, configurableannotation tool (which is very complex due to itswide application possibilities), another way to pro-ceed is to create simple, well designed tool fo-cused on specific task.
Such tool can be thencustomized or extended by qualified programmerswithout much effort and then provide great effi-ciency of the annotation process.Presented version of MMAX2 with its sourcecode is available at http://zil.ipipan.waw.pl/MMAX4CORE webpage.
We encourageit?s use and modification for other coreference an-notation projects.AcknowledgmentsThe work reported here was cofounded by theComputer-based methods for coreference resolu-tion in Polish texts project financed by the Pol-ish National Science Centre (contract number6505/B/T02/2011/40) and by the European Unionfrom resources of the European Social Fund.Project PO KL ?Information technologies: Re-search and their interdisciplinary applications?.ReferencesChristoph M?ller and Michael Strube.
2006.
Multi-level annotation of linguistic data with mmax2.
InSabine Braun, Kurt Kohn, and Joybrato Mukher-jee, editors, Corpus Technology and Language Ped-agogy: New Resources, New Tools, New Methods,pages 197?214.
Peter Lang, Frankfurt a.M., Ger-many.Maciej Ogrodniczuk, Katarzyna G?owi?nska, MateuszKope?c, Agata Savary, and Magdalena Zawis?awska.2013.
Polish coreference corpus.
In Zygmunt Ve-tulani, editor, Proceedings of the 6th Language &Technology Conference: Human Language Tech-nologies as a Challenge for Computer Scienceand Linguistics, pages 494?498, Pozna?n, Poland.Wydawnictwo Pozna?nskie, Fundacja Uniwersytetuim.
Adama Mickiewicza.Constantin Or?asan.
2003.
PALinkA: a highly cus-tomizable tool for discourse annotation.
In Proceed-ings of the 4th SIGdial Workshop on Discourse andDialog, pages 39 ?
43, Sapporo, Japan, July, 5 -6.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012. brat: a web-based tool for nlp-assistedtext annotation.
In Proceedings of the Demonstra-tions at the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 102?107, Avignon, France, April.
Associationfor Computational Linguistics.96
