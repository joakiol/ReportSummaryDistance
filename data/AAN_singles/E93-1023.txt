A Probab i l i s t i c  Context - f ree  Grammarfor D isambiguat ion  in Morpho log ica l  Pars ingJos~e S. Heemskerk*Institute of Language Technology and Artificial IntelligenceTilburg UniversityP.O.
Box 90153, 5000 LE TilburgThe NetherlandsE-mail: joseeh@kub.nlAbstractOne of the major problems one is facedwith when decomposing words into theirconstituent parts is ambiguity: the gen-eration of multiple analyses for one inputword, many of which are implausible.
Inorder to deal with ambiguity, the MOR-phological PArser MORPA is providedwith a probabilistic ontext-free grammar(PCFG), i.e.
it combines a "conventional"context-free morphological grammar to fil-ter out ungrammatical segmentations witha probability-based scoring function whichdetermines the likelihood of each success-ful parse.
Consequently, remaining analy-ses can be ordered along a scale of plausi-bility.
Test performance data will show thata PCFG yields good results in morphologi-cal parsing.
MORPA is a fully implementedparser developed for use in a text-to-speechconversion system.1 IntroductionMORPA is a MORphological PArser developed foruse in the text-to-speech conversion system forDutch, SPRAAKMAKER \[van Leeuwen and te Lin-deft, 1993\].
An important step in text-to-speech on-version is the generation of the correct phonemic re-presentation o  the basis of the input text.
As is well-known, phonemic transcriptions can not be derived*This work was carried out at the Phonetics Lab-oratory at Leiden University and supported by theSpeech Technology Foundation, which is funded bythe Netherlands Stimulation Project for InformationSciences, SPIN.directly from orthographic input in Dutch, as thereis no one-to-one correspondence b tween graphemesand phonemes.
Also, stress and the effects of mostphonological rules are not reflected in orthography.A text-to-speech system therefore requires an intel-ligent method to convert the spelled words of theinput sentence into a phonemic representation.As far as the pronunciation of words is concerned,it is impossible to list the entire vocabulary of thelanguage, because language users have the ability tocreate new words and the vocabulary, as such, is in-definitely large.
Daily newspapers, for instance, con-tain a large amount of newly formed words every day.Not all of these survive in the long run, but some ofthem do.
Consider the examples in (1):(1) golfooriog 'gulf war'drugsbaron 'drugs baron'vredesmacht 'peacekeeping force'Because it is unfeasible to give the lexicon a dailyupdate, this approach is not appropriate if the text-to-speech system is to convert unrestricted text.Assuming that newly created words will typicallyconsist of already existing morphemes, and that newmorphemes are added to the language only rarely, wecan, however, use a lexicon in which all Dutch mor-phemes and their pronunciations are listed.
Thencomplex words, such as the ones in (1), have to bedecomposed into their constituent parts before theirpronunciation can be looked up.Since the pronunciation ofa word does not alwaysconsists of the concatenation f the pronunciation ofthe morphemes, because the pronunciation of mor-phemes can be modified in certain contexts, the text-to-speech system also has to be provided with phono-logical rules which adjust the pronunciation of mor-phemes according to their context \[Allen et aL, 1987;183Nunn and van tteuven, 1993\].Dutch phonological rules are in several ways de-pendent on morphemic segmentation a d word classassignment.
As is shown in (2a), for example, thegrapheme d is pronounced voiceless when it occursstem-finally, but voiced when it occurs tem-initially.Final devoicing, the phonological rule which affectsthe pronunciation f the d, depends on syllable struc-ture, and syllabification is sensitive to the morpho-logical structure of a word: compound boundariesare also syllable boundaries.
This has serious con-sequences in Dutch, as Dutch compounds are usu-ally written as one word, i.e.
without spaces or hy-phens in between the parts.
Example (2b) showsthat the stress in compounds differs from the stress inmonomorphemic words.
In (2c) it is shown that thestress in (predicatively used) adjectival compoundsdiffers from the stress in nominal compounds:(2) a hoofdagenthoof\[t\] + agentloofdakIoof + \[d\]akb avonduur'avond + uuravontultravonttuurc onecMon + lecht, Aonrecht%n + recht, NSo to be able to produce high quality speech on un-restricted text, the text- to-speech system SPRAAK-MAKER contains the morpheme l xicon-based mor-phological parser MORPA to recover the morphemicsegmentation a d word class of the input word.
Themodule MORPHON \[Nunn and van Heuven, 1993\]applies phonological rules which derive the pronun-ciation of the word by making use of the morpho-logical information.
Also, the word class providedby MORPA feeds the module for sentence analysiswhich serves sentence prosody \[Dirksen and Quen~,1993\].Our method of morphological nalysis comprises amorpheme l xicon.
Assuming that Dutch word for-mation is concatenative, word or word parts are rec-ognized by dividing the word into substrings thatcorrespond to entries in the lexicon.
The major prob-lem this method poses is ambiguity, i.e.
the gen-eration of alternative segmentations and word classassignments for one input word, many of which areimplausible.
In a text-to-speech system, an incor-rect analysis is unacceptable, because it may lead toa wrong pronunciation \[Nunn and van IIeuven, 1993\].In order to deal with ambiguity, MORPA has beenprovided with a probabilistic ontext-free grammar(PCFG), i.e.
it combines a "conventional" context-free morphological grammar to filter out ungram-'police sergeant''roof of foliage''evening hour''adventure''unreal''injustice'matical segmentations with a probability-based scor-ing function which determines the likelihood of eachsuccessful parse.
Then, aiming at a system that gen-erates the "best" analysis first, the remaining anal-yses are ordered along a scale of plausibility.
In thispaper, I will separately describe the rule-based is-ambiguation techniques and probability-based scor-ing function.
Illustrative performance data obtainedfrom an evaluation will show that a probabilisticcontext-free grammar yields good results in morpho-logical parsing.2 Ru le -based  d i sambiguat ionDecomposition of the input word is carried out intwo successive stages.
First, all the possible seg-mentations of an input word into strings of stemsand affixes are generated.
Secondly, each segmenta-tion is tested for morpho-syntactic well-formedness.While the well-formedness i  tested, word class is de-termined.The task of recovering the morphemic segmentationwith the help of a morpheme l xicon is very muchcomplicated by the fact that a word can be seg-mented in more than one way.
The number of alter-native segmentations for an input word grows withincreasing lexicon size, decreasing average length ofthe lexical elements and increasing average length ofthe input word.
Our lexicon contains 17,087 entries,among which there is a large number of very small in-flectional affixes.
Furthermore, the input words maybe very lengthy, as Dutch compounds are writtenas one word, and because nominal compounding, forinstance, is a highly productive process.
The resultcan be a combinatorial explosion, causing hundredsof segmentations to be generated.In order to restrict ambiguity in the segmentationstage, we employed a number of strategies.
First,we made a pragmatic operalisation ofthe theoreticalnotion "morpheme", which is traditionally defined as"the smallest meaningful nit" in word formation: inour lexicon we only listed words and affixes.
Alongwith all simplex words and productive affixes, welisted all the word formations that belong to closedclasses, i.e.
words which are not formed accordingto productive word formation processes.
Thus, ourparser only has to analyse words formed accordingto productive rules.Secondly, MORPA performs, if available, sometests on phonological nd phonetic restrictions on therecognition of morphemes in a specific context.
Theultimate ffect of these tests is that incorrect recog-nition of highly frequent and very small inflectionalsuffixes, such as -e, -t, -d, -s, -r, -n, -en or -er, canbe prevented in many cases.Finally, MORPA sees to it that words belongingto minor lexical categories ( uch as determiners, pro-nouns, conjunctions, etc.)
are not recognised as wordparts.
They never take part in morphological pro-184cesses.
By rejecting these, we prevent the parserfrom doing work which we know beforehand will bein vain.To illustrate the effect of the segmentation proce-dure, its output for the noun beneveling (intoxica-tion) is shown in (3)z:(3) a be + neef + elingb be+neef+e+l ingc be + nevel + ingd been + e + veel + inge be+n +e+vee l+ ingf be +neef + eel + ingAll of the parts in the segmentations under (3) areDutch morphemes listed in the morpheme lexicon.Because the segmentation procedure analyses the in-put word into all possible strings of morphemes with-out any further grammatical knowledge, it generatesalong with the one and only plausible segmentationbe + nevel + ing (3c), several alternative segmen-tations.
Many of these violate grammatical nd/orsemantic restrictions.In order to filter out ungrammatical segmenta-tions, each segmentation is checked for its morpho-syntactic well-formedness with the help of a cate-gorial grammar.
Consequently, every segmentationthat is not in accordance with the rules of Dutchmorphology is rejected by the parser.
While check-ing, the word class of the grammatical segmentationsis determined.In accordance with the principles of CategorialGrammar, our parser does not make use of a set ofexplicitly represented word formation rules.
Instead,the morphological subcategorisation information isencoded in the form of category assignments in thelexicon.
That is, prefixes have been assigned a cat-egory of type A/B,  which means that they take astem of category A on their right-hand side to yielda word of category B 2.
For instance, the prefix be-with category N/V  requires a nominal stem to theright to form a verb.
Likewise, suffixes of categoryA\B  look for a stem of category A on their left-handside to yield a word of category B.
Thus, the suffix-ing, V \N ,  requires a verbal stem to the left to forma noun.
Free morphemes, uch as nevel, are assignedprimitive categories, uch as V or N 3.1When segmenting, MORPA takes into account hatDutch word stems, when inflected or used as the base of aderivation, may undergo spelling changes.
It would takeus too far to go into the spelling rules here, but in (3) theeffect of rules such as 'vowel gemination' and 'devoicingof stem-final consonants' shows up.
See for more detail\[Heemskerk and van Heuven, 1993\].~Note that in the literature on categorial grammar thenotational variant B/A is frequently used.SSince our parser only accounts for morphological sub-categorisation, the set of lexical categories does not equalthe set of syntactic ategories.
For example, all verbs areIn a strictly bottom-up fashion, the parser itera-tively attempts to combine two adjacent elements,reducing them in accordance with their categorialspecification with the help of three very general re-duction laws:(4) prefixation: A/B  .
A ---.
Bsuffixation: A.
A \B  --* Bcompounding: A.
B --~ BFor pragmatic reasons, MORPA's rule for com-pounding is not a categorial rule, but a categorial-likerule: two adjacent stems AB may, according to theRight-Hand Head Rule be combined into a word ofcategory B4.
In addition to this general rule for com-pounding, the grammar contains a small set of rulesdefining productive compounding.
An analysis failsas soon as a string of categories cannot be reducedto one single category.The examples in (5) illustrate how iterative cat-egorial reduction results in a successful parse.
Thestructures how the derivation and determination fthe output category of (3c).
Also, the examples in (5)illustrate that, while the categorial grammar fltersout many ungrammatical segmentations and derivesthe word class of the input word, parsing introducesa new type of ambiguity: one segmentation can beassigned more than one structure.
The ambiguity in(5) is due to the fact that the morphemes be- (en-)and nevel (mist) can belong to more than one lex-ical category and as a consequence can be reducedin more than one way.
The ambiguity in (5a) and(5b), is spurious in the sense that it does not corre-late with a difference in pronunciation or word classassignment.
The reduction in (5c) results in an in-correct word class assignment.Because the word syntax as such is not restric-tive enough, it was supplemented with a componentwhich heavily restrains the parser in building struc-tures.
This component, which is inspired by LexicalPhonology, imposes an ordering on the attachment ofaffixes and stems.
Consequently, it restricts the typeor the complexity of the stem that an affix or otherstem may attach to.
Rejection of structures can re-sult in avoiding incorrect word class assignment andrejection of incorrect segmentations.In Lexical Phonology, the interaction betweenstress behaviour and affix order is explained.
\[Chore-sky and Halle, 1968\] distinguished two classes ofsuffixes with different stress properties, and \[Siegel,1979\] observed that this distinction correlates withthe order in which the suffixes attach.
Over theyears, theoretical linguists have become scepticalassigned category V, irrespective of (in)transitivity.
Theuse of syntactic categories would complicate the grammarconsiderably.
See \[Dowty, 1979\] and \[Moortgat, 1987\] fora discussion on this matter.4For more principled approaches see \[Hoeksema, 1984;Moortgat, 1987\]185of these "level theories", because of the so-called"bracketing paradoxes", i.e.
constructions in whichtwo distinct constituent structures (for instance amorphological nd a phonological one) have to beassigned to a word 5.
Despite the occurrence of brack-eting paradoxes, however, the claims on level orderedmorphology following from these theories are highlyinteresting: in checking the morphological claimswhich follow from one of the theories that have beendeveloped for Dutch, \[van Beurden, 1987\], againsta large database containing approximately 123,000Dutch words, relatively few counter-examples werefound.
(5) a NV V\NN/V N ingI Ibe nevelb NV V\NV/V V ingbe nevelVN/V Nbe V V\NL .Inevel mgSSee for a recent discussion of this topic \[Spencer,1991\]Van Beurden claims that affix order does not de-pend on stress properties, but on categorial proper-ties.
Thus, the major characteristic of this modelis that each attachment level is associated with aspecific lexical output category.
The model seemsparticularly suitable for use in MORPA, because itis easy to integrate with our categorial parser.
Themodel implemented in MORPA, shown in (6), is anextension of Van Beurden's model in a way which isconsistent with its basic assumptions s.(6)Underived words, affixesUnproductive word formationsLV-morphologyI ,, A-morphology.1N-morphologyOn the basis of this model, the Dutch vocabulary canbe divided into four levels.
Each of the levels in (6)may be viewed as possible successive stages in wordformation.
The first level, or lexical level, comprisesthe lexicon of simplex words, affixes and irregularformations.
This level also contains all (borrowed)Romance words.
The elements of this lexical levelmay be successively developed on the second levelon which V(erbal)-morphology takes place; the thirdlevel on which A(djectival)-morphology takes placeand the fourth level on which N(ominal)-morphologytakes place.
The name of the level indicates the re-sulting word class.
Each of these levels preservesthe possibility for suffixation, compounding and pre-fixation.
On the levels for V-morphology and A-morphology each of these processes may take place6In van Beurden's model each categorial level has aphonological level associated with it.
As we are mainlyinterested in the morphological aspects, we leave thephonological claims for what they are: within SPRAAK-MAKER, MORPA and MORPHON (the phonologicalmodule) are autonomous modules, and as MORPA pre-cedes MORPHON, any interaction between the two sys-tems is one way.186only once.
We assume that only the processes on theN-morphology level are recursive, i.e.
may take placemore than once (see \[Heemskerk, 1989\] for more de-tails).The model correctly predicts the derivation of theword onverdraagzaarnheid ( ntolerance).
As shownin (7), first verbal prefixation yields the verbal stemverdraag (tolerate), then adjectival suffixation yieldsthe adjective verdraagzaam (tolerant), adjectival pre-fixation yields the adjective onverdraagzaam (intoler-ant) and, finally, nominal suffixation yields the nounonverdraagzaamheid ( ntolerance):(7) NA A\NA/A A heidon V V\AV/V V zaamI Iver  draagAlso, the level module rules out the analysis in (5c):the nominal suffix -ing must not be attached beforethe verbal prefix be-.
Therefore the word cannot beanalysed as a verb.
(8)Segmentationsbe + neef + elingbe + neef + e -}- lingbe + nevel .+ ingbe + neef + eel + ingword classassigned bycateg, levelgrammar moduleN NNNV N-NIf we return to the example of beneveling we findthat of the six alternative segmentations in (3), onlyfour are accepted by the categorial component.
Asis shown in (8) one of these segmentations has beenassigned a wrong word class.
In (8) it is also shownthat, as a result of the level ordering, three of the as-signed word classes (and matching structures 7) wererejected.
Consequently, two analyses remain .3 Probability-based scoring functionClearly, the ultimate handling of the remaining am-biguity in (8) demands recourse to semantics andworld knowledge.
For the large-scale domain weare dealing with, however, we considered it unfea-sible to implement semantic and pragmatic con-straints.
Thanks to the availability of a large anno-tated corpus, the alternative of constructing a PCFGcame within reach.
The corpus, being a represen-tative sample of the past or existing vocabulary,is expected to capture implicitly various semanticand pragmatic onstraints.
\[Fujisaki et al, 1989;Liberman, 1991\].
Empirical estimation of the proba-bility of a parse tree on the basis of the corpus enablesus to order the competing analyses along a scale ofplausibility and select he "best" parse out of the setof alternatives.A parse tree, such as (5a), is a series of applied pro-duction rule@.
In a context-free grammar it is as-sumed that the application of a production rule isindependent of previously applied rules.
In a PCFG,each production rule r is assigned an estimated prob-ability of use and the probability of the parse tree tis the product of the constituting production rulesr l ,  r2,  .
.
.
,  rm:(9) P(t)- -P(rz)  x P(r2) x ... x P(rm)The probability of each production rule in the gram-mar has been estimated by means of straightforwardcounting of appearances in the corpus, resulting inrelative frequencies.
Let G be any non-terminal sym-bol of the grammar; n(G) the number of productionsrewriting G and P(ilG ) the probability that the ithof these productions takes place, then(10) P(iIG ) = n(G)It is assumed that for all i -- 1, 2 .
.
.
.
, n(G), P(iIG )is a positive number and that ~iP( i lG)  -- 1.7In (8), I abstract from hierarchical structures, incethey are irrelevant for pronunciation.
Relevant for pro-nunciation are the morphemic segmentation and wordclass assignment.
Consequently, the structures of (5) arerepresented asthe segmentation be + nevel + ing, whichhas been assigned two word classes N and V.Sin this section, I will give a top-down descriptionof a parse tree and discuss production rules of the type"A --, B C a, rather than bottom-up reduction and rulesof the sort "B C --+ A ~ used by the parser.187MORPA's grammar comprises three different ypesof production rules:(11) a w ~ Tb T--~ N1 N2c N----*MIn (11) w is the start symbol for words 9, T any mem-ber of the set of atomic categories which are possi-ble top nodes: 7- = {n, v, a, .
.
.
},  N any member ofthe set of non-terminals containing both atomic andfunctor categories: Af = {n, n/v, v\n, v, .
.
.
}, 7- C .hf,and M any member of the set of terminals: Jvf ={be, nevel, ing, .
.
.}
.The probability of (5a) is then determined as in(12)1?
:(12) P(\[n \[v \[n/v be\]\[v nevell\]\[v\n ing\]\]) =P(w ~ n) xP(n ---, v v \n )  xP(v~ n/v n) xP(n/v  ---* be) xP(n -..-* nevel) xP(v \n  ~ ing)Thus, this simple PCFG provides general informa-tion on how likely a parse tree is going to appear.It is well-known that the accuracy of the empiricalestimate of a probability function depends heavily onthe appropriateness of the training set: for one thing,it must have a reasonable size and be representativeof the domain that is being modelled.
Our trainingset was the CELEX database which contains approx-imately 123,000 Dutch stems provided with syntacticinformation, a morphological decomposition and to-ken frequency information \[van der Wouden, 1988;Burnage, 1990\].
The token frequency informationis based on a 44-million-word corpus.
We collectedfrom this database both type and token frequencies:type frequencies indicate how often a production ruleoccurs in the Dutch vocabulary (i.e.
in the 123,000stems corpus); token frequencies indicate how oftena production rule occurs in Dutch texts (i.e.
in the44-million-word corpus).
The underlying idea wasthat for tests on dictionary samples the empirical es-timate must be based on type frequencies, whereasfor tests on text samples it must be based on tokenfrequencies.Given the information in the database, we ex-pected the collection of frequency data to be a matterof straightforward counting: CELEX's morphologi-cal decomposition consists of hierarchical structureswhich are comparable to MORPA's structures (cf.9Although not in the grammar, this symbol is usedto make it possible to describe the possibility of a wordbeing of a certain category in terms of (5).10 For the reader's convenience, the probabilities denotethe tree (in labelled bracketing) and production rulesinvolved.the examples in (5)), the syntactic information con-sists of the word class, and because each stem inthe stem corpus is provided with a token frequency,type and token frequencies could be collected simul-taneously: every time a production rule was encoun-tered in the stems corpus, 1 was added to its typefrequency, and the token frequency of the word inwhich the rule was attested was added to its tokenfrequency.Unfortunately, however, straightforward countingof all production rules contained in CELEX did notsuffice to provide MORPA with the relevant informa-tion: it turned out that the set of production rulesemployed by MORPA was not contained in the set ofproduction rules given by CELEX.
For a very largepart, the mismatch between the rules is caused bythe fact that CELEX and MOR.PA yield differentanalyses.
For example, because in MORPA all wordsformed according to unproductive rules are entirelylisted in the lexicon, and the Dutch adjectival suffix -elijk '-ly' is considered to be unproductive, all wordsderived by this suffix are listed.
In CELEX, how-ever, these words are decomposed.
Now, in order toanalyse the word vriendelijk (friendly), MORPA willemploy the production rule (13a), whereas CELEXemployed the rules in (13b):(13) a A --~ vriendelijkb A ~ N N\AN ~ vriendN\A  ~ elijkConsequently, straightforward counting of the pro-duction rules in CELEX, would result in overesti-mating the probability of the productions "A ---*N N\A"  and "N --~ vriend", and lack of frequencyinformation for the production "A --* vriendelijk".Amongst the MORPA rules which were not con-tained in the set of CELEX rules, there were alsoall the rules introducing inflectional affixes and in-fleeted stems.
Of course, this is due to the fact thatthe 123,000-entry corpus only contains stems.
AsCELEX stems are considered to be an abstract wayof representing a whole inflectional paradigm, inflec-tional affixes and inflected stems were not includedin the database, and the token frequency associatedwith a stem is the sum of the token frequencies of thestem and all its inflected forms.
However, MORPAalso contains inflectional rules of which the tokenfrequencies should be available.
For obtaining fre-quency information on inflectional affixes and stems,we had to use the CELEX corpus, containing ap-proximately 44 million words.
Unfortunately, themorphological information in this corpus does notcontain any production rules or information on theaffixes.Thus, after all production rules in CELEX hadbeen counted straightforwardly, we were only ableto assign frequency information to a part of theMORPA rules.
Moreover, we knew that some of188these frequencies were overestimated.
Because weexpected these facts to have a negative influence onthe accuracy of the PCFG, we decided to put someeffort in making the empirical estimate more reli-able.
We had to be very creative in finding otherways to provide the rules which are not in CELEXwith frequency information (from CELEX), but wefinally managed to provide almost all productionrules employed by MORPA with frequency informa-tion.
Also, we put some effort into "repairing" theoverestimated frequencies.
Consequently, the datahave become more complete and more reliable, butas a result of these problems, the collection of fre-quency information became a time-consuming anderror-sensitive job: a lot of work had to be done byhand.
Therefore, it is practically almost undoable togo over it all over again.With respect to the reliability of the frequencydata, it turned out that the token frequencies areless reliable than the lexical frequencies.
Most impor-tantly, this was due to the fact that in CELEX, thetoken frequencies were "string" counts, i.e.
they in-dicate how many times each separate string of lettersoccurs in the 44-million-word corpus.
Because someof these "separate strings of letters" may be ambigu-ous in word class, morphemic segmentation r mean-ing, they are assigned different entries in the stemscorpus.
Ideally, the token frequencies in the corpusare disambiguated for the different entries, but atthe time we collected our data they were not 11.
Asa consequence, numerous tems were assigned over-estimated token frequencies.Consider, for example, the string rod, which canbe linked to two entries in the stems database: theentry of the preposition met 'with', and the entryof the noun met 'minced pork'.
Since the individ-ual frequencies of each of these entries have not beensorted out, the rules "P ---* met" and "N ---* met"have the same frequency, i.e.
the frequency of thestring met.
Because the preposition is highly fre-quent and the noun hardly ever occurs, the latterrule has been assigned a frequency which is highlyoverestimated.
Since in addition to that overesti-mation the rule "w ~ N" is more frequent hanthe rule "w --* P",  and to the frequency of the rule"N --* met" is added the frequency of the two com-pounds in which it takes part, MORPA will considerthe noun to be the most likely analysis.
Had the fre-quencies been sorted out, this would not be the case:the high probability of the rule "P ~ met" wouldhave overweighted all other probabilities.The unreliability of token frequencies was bearedout by some preliminary tests, in which we exper-imented using type and token frequencies on bothdictionary and text test samples.
When examining11By now, CELEX has disambiguated the token fre-quencies, but as the collection of reliable data was verytime-consuming, we have not yet "repaired" our tokenfrequencies.MORPA's output on a text test sample (for which to-ken frequencies were used), we discovered that manyof the erroneous elections were indeed attributableto the lack ofdisambiguation of token frequencies.Especially if the sample contained highly frequentstring ambiguous simplex words, such as met, whichdo not take part in derivation or compounding,MORPA's performance got worse.
It turned out thatMORPA's performance was best, when type frequen-cies were used in a dictionary test sample.MORPA first generates all possible parses and theassociated probabilities, ordering them along a scaleof plausibility afterwards.
Thus, as yet, it is not aprobabilistic parser in the sense that it :prunes thelow probability parses in an early stage \[Fujisaki etal., 1989; Jelinek d aL, 1990\].
Adjusting the parserwill speed it up considerably, but also pruning low-ranked analyses may lead to incompleteness.In conclusion, let us return to the example wordbeneveling.
After likelihood determination and or-dering of the two remaining analyses in (8), the cor-rect analysis be + nevel + ing is in topmost position:(14) 1 be -t- nevel 4- ing N2 be 4- neef -t- eling N4 The  per fo rmance  o f  MORPAIn order to evaluate the performance of our systema test was run on a dictionary test sample of 3,077words.
The words contained in this sample were ran-domly taken from texts of the so-called "Bloemendalcorpus" \[Bringmann, 1990\].For a correct interpretation of the results, it is nec-essary to know that a word was considered to becorrectly analysed, if it had been assigned the cor-rect morphemic segmentation and word class.
Theanalysis in (15) is the correct analysis of the wordbeneveling:(15) \[- be\] \[o,.,.
,evel\] \[.-I l i. ins\]\]Thus, in the final output of MORPA, morphologicalinformation which is irrelevant for pronunciation iseliminated: analyses which have the same segmenta-tion, but are ambiguous in their hierarchical struc-ture and/or categorial labelling of the morphemes,such as (5a) and (5b), become one as long as themorphemes have the same morphological c assifica-tion, e.g.
((non)-native) prefix, suffix or stem, andthe word is assigned the same word class.As MORPA combines a conventional grammar witha probability-based scoring function, it is interest-ing to look at the effects of both the rule-based partand the probability-based ordering technique in theirown right: the segmentation procedure and grammardetermine the quality of the analyses and the num-ber of analyses generated, and the probability-based189scoring function enables MORPA to select the mostlikely analysis from a set of alternatives.The results in (16) show how well the segmenta-tion procedure and grammar succeeded in derivingthe correct analysis for the test words:(16)words assigned Numbern = 3,077a correct analysis 2,968no correct analysis 32no analysis at all 77%96MORPA assigned no analysis at all to 3% of the testwords.
For 1% of the test words, one or more anal-yses were generated, but the set of alternatives didnot contain a correct analysis.
In these cases, theword either contains an unknown morpheme, or thegrammar is too restrictive.
96% of the test wordswere assigned a correct analysis.Given the problem of ambiguity, the number ofanalyses generated for one word is remarkably small:considering only the words which were correctly anal-ysed, MORPA assigned a single, correct analysis to46% of the test words.
For 54%, the correct analysiswas among alternatives:(17)words assigned a correctanalysis, which isamong alternativesuniqueNumber %n = 2,9681,612 541,356 46ing analyses along a scale of plausibility, it must beestablished how often MORPA succeeds in select-ing the correct analysis from a set of alternatives.MORPA was able to select the correct analysis asmost likely member of a set of alternatives for 92%of the test words.
For a proper judgement of thisperformance, the percentage must be compared withthe chances of selecting the most likely analysis fromthe set of alternatives.
This chance is determined at40%:(18)words assigned the best.analysis from a set ofalternatives, bythe probability-basedordering techniquechanceNumber %n = 1,6121,483 92645 40It is not easy to tell which factors attributed to thefact that for 8% of the words the correct analysis wasnot selected as best analysis.
The frequency datamay be unreliable or the probability function maynot be appropriate.
Also, the correct analysis doesnot always have to be the most probable one.Most importantly however, is the overall perfor-mance of MORPA's PCFG on the Bloemendal cor-pus: 92% of the test words had been assigned a cor-rect analysis which was also the first analysis yielded.
(19)words assigneda correct analysis intopmost positionNumber %n = 3,0772,835 92Although we did not keep track of the number ofsegmentations a signed to the input words, it canbe generally assumed that the number of alternativesegmentations is very much reduced by the gram-mar.
Also, through converting output that containshierarchical structures and categorial labels (cf.
(5)aand (5)b) to linear structures and morpheme classi-fication (c/.
(15)), a lot of unnecessary ambiguity iseliminated.In order to evaluate the probability-based scoringfunction, which enables MORPA to order compet-For the 8% of the test words which were not assigneda correct analysis in first position, MORPA eithergenerated a correct analysis which was not in firstposition, or no correct analysis or no analysis at all.In order to establish the relevance for word level pro-nunciation, a test was run on a test file containingapproximately 2000 isolated words.
The test wordswere selected from different corpora to make sure thefile contained both newspaper text, dictionary words190and words of frequency 112.
The words of the test filewere analysed by MORPA and the topmost analyseswere used by MORPHON to derive a pronunciationtranscription.
A transcription was considered correctif it had the proper phonemic transcription, whichmeans that all appropriate non-optional phonologi-cal rules must have been applied, and that the wordsmust have the correct syllable structure and stresspattern.Fifteen percent of the words were assigned an er-roneous phonemic transcription 13.
Twenty percentof the errors could be traced back to the phonolog-ical module, the remaining errors, 80%, are due tofaulty morphological nalyses.
Of the errors madeby MORPA, 88% led to an incorrect pronunciationrepresentation.
As expected, segmentation errors al-most always led to an incorrect phonemic transcrip-tion.
Category assignment errors also cause incorrectpronunciations, though less often.
This bears out theimportance of the category a word belongs to.5 ConclusionAs the results how, this fully implemented system,running with a morpheme l xicon of 17,087 entrieson a randomly selected 3,077 words test sample, issuccessful.
This success may to a large extent beput down to the augmentation f the context-freegrammar to a PCFG 14.As mentioned above, the accuracy of a PCFG de-pends heavily on the accuracy of the empirical es-timate of the probability function.
We were luckyto have at our disposal a training set which wasboth large enough and representative, but due to thefacts that, in some cases, MORPA and the trainingset yield different analyses, and token frequencies forstring ambiguous words were not disambiguated, weexpect our estimate to have become less reliable.
Inorder to improve MORPA's performance on text testsamples, we will have to "repair" the token frequen-cies.It is often argued that a PCFG only provides poorestimates ofprobability, and that probabilistic gram-mars require more sensitivity to lexical context.
Af-ter all, PCFGs only provide very general informationon how likely a production rule is going to appearanywhere in a sample of the language, and produc-tion rules are not always context-free \[Magerman da2For reasons I will not go into here, the newspaperand dictionary words did not comprise highly frequentwords \[Nunn and van Heuven, 1993\].13See for a comparison with a data-oriented systemfor Dutch grapheme-to-phoneme transcription \[van denBosch and Daelemans, 1993\].
Note that in this compar-ison syllabification and stress assignment have not beentaken into account.14Before this augmentation, the parser was enrichedwith some preliminary criteria imposing an order on theset of alternatives.
Then, the performance ame up to85%.Marcus, 1991; Resnik, 1992\].
However, most of thework done on context-free probabilistic grammars idone for syntax, and as I hope to have shown that aPCFG yields good results for morphology, it mightbe interesting to find out if, for one reason or another,PCFGs are more successful for morphology than forsyntax.AcknowledgementsI wish to thank my former colleagues of the Phonet-ics Laboratory at Leiden University who contributedto the work on MORPA.
Furthermore, I am greatlyindebted to Louis ten Bosch for his help with proba-bility theory and Emiel Krahmer and Wessel Kraaijfor solving all my IbTEX problems.Re ferences\[Allen et al, 1987\] J. Allen, M.S.
Hunnicutt, andD.
Klatt.
From Text to Speech: the MITalk Sys-tem.
Cambridge University Press, 1987.\[van Beurden, 1987\] L. van Beurden.
Playing levelwith Dutch morphology.
In F. Beukema andP.
Coopmans, editors, Linguistics is the Nether-lands 1987, pages 21-30, 1987.\[van den Bosch and Daelemans, 1993\] A. van denBosch and W. Daelemans.
Data-oriented meth-ods for grapheme-to-phoneme conversion.
In Pro-ceedings of the Sixth Conference of the EuropeanChapter of the Association for Computational Lin-guistics., 1993.\[Bringmann, 1990\] E. Bringmann.
Philip Bloemen-dal corpus.
Internal Report 21, Analysis and Syn-thesis of speech, Utrecht 1990.\[Burnage, 1990\] Gavin Burnage.
CELEX, a guidefor users.
CELEX Centre for Lexical Information,Nijmegen, 1990.\[Chomsky and Halle, 1968\]N. Chomsky and M. Halle.
The sound pattern ofEnglish.
Harper and Row, New York, 1968.\[Dirksen and Quen~, 1993\]A. Dirksen and H. Quen~.
Prosodic analysis: thenext generation.
In V. van Heuven and L. Pols,editors, Analysis and Synthesis of Speech, Strate-gic Research Towards High-Quality Text.to-SpeechGeneration, pages 131-145.
Mouton de Gruyter,Berlin, 1993.\[Dowty, 1979\] D. Dowty.
Word meaning and Mon-tague Grammar.
Foris Dordrecht, 1979.\[Fujisaki et al, 1989\] T. Fujisaki,F.
Jelinek, J. Cocke, E. Black, and T. Nishino.
Aprobabilistic parsing method for sentence disam-biguation.
In International Workshop on ParsingTechnologies, Pittsburgh P.A., 1989.\[Heemskerk and van Heuven, 1993\]J. Heemskerk and V. van Heuven.
MORPA: a191morpheme l xicon-based morphological parser.
InV.
van Heuven and L. Pols, editors, Analysis andSynthesis of'Speech, Strategic Research TowardsHigh-Quality Text-to-Speech Generation.
Moutonde Gruyter, Berlin, 1993.\[Heemskerk, 1989\] J. Heemskerk.
Morphologicalparsing and lexical morphology.
In H. Bennisand A. van Kemenade, ditors, Linguistics in theNetherlands 1989, pages 61-70, 1989.\[Hoeksema, 1984\] J. Hoeksema.
Categorial Morphol-ogy.
PhD thesis, Groningen, 1984.\[Jelinek et al, 1990\] F. Jelinek, J.D.
Lafferty, andR.L.
Mercer.
Basic methods of probabilisticcontext free grammars.
Research Report R..C.16374(72684), IBM, 1990.\[van Leeuwen and te Lindert, 1993\]H.C. van Leeuwen and E. te Lindert.
Speech-Maker: a flexible framework for constructing text-to-speech sytems.
In V. van Heuven and L. Pols,editors, Analysis and Synthesis of Speech, Strate-gic Research Towards High-Quality Text-to-SpeechGeneration.
Mouton de Gruyter, Berlin, 1993.\[Liberman, 1991\] M.J. Liberman.
The trend towardstatistical models in natural anguage processing.In E. Klein and F. Veltman, editors, Natural Lan-guage and Speech.
Springer Verlag:Berlin, 1991.\[Magerman d Marcus, 1991\] D. Magerman andM.
Marcus.
:Pearl: A probabilistic hart parser.In Proceedings of the Fifth Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics, Berlin, 1991.\[Moortgat, 1987\] M. Moortgat.
Compositionalityand the syntax of words.
In :l. Groenendijk,D.
de Jongh, and M. Stokhof, editors, Foundationsof Pragmatics and Lexical semantics, 1987.\[Nunn and van Heuven, 1993\] A. Nunn and V. vanHeuven.
MORPHON, lexicon-based text-to-phoneme conversion and phonological rules.
InV.
van Heuven and L. Pols, editors, Analysis andSynthesis of Speech, Strategic Research TowardsHigh-Quality Text-to-Speech Generation.
Moutonde Gruyter, Berlin, 1993.\[Resnik, 1992\] P. Resnik.
Probabilistic tree-adjoining rammar as a framework for statisticalnatural anguage processing.
In Proceedings In-ternational Conference on Computational Linguis-tics, Nantes, 1992.\[Siegel, 1979\] D. Siegel.
Topics in English Morphol-ogy.
Garland: New York, 1979.\[Spencer, 1991\] A. Spencer.
Morphological Theory.Basil Blackwell, 1991.\[van der Wouden, 1988\] T. van der Wouden.
Celex:?
Building a multifunctional polytheoretical lexicaldatabase.
In Proceedings Budalex, 1988.192
