Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1661?1670,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCross-lingual Models of Word Embeddings: An Empirical ComparisonShyam Upadhyay1Manaal Faruqui2Chris Dyer2Dan Roth11Department of Computer Science, University of Illinois, Urbana-Champaign, IL, USA2School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USAupadhya3@illinois.edu, mfaruqui@cs.cmu.educdyer@cs.cmu.edu, danr@illinois.eduAbstractDespite interest in using cross-lingualknowledge to learn word embeddings forvarious tasks, a systematic comparison ofthe possible approaches is lacking in theliterature.
We perform an extensive eval-uation of four popular approaches of in-ducing cross-lingual embeddings, each re-quiring a different form of supervision,on four typologically different languagepairs.
Our evaluation setup spans four dif-ferent tasks, including intrinsic evaluationon mono-lingual and cross-lingual simi-larity, and extrinsic evaluation on down-stream semantic and syntactic applica-tions.
We show that models which requireexpensive cross-lingual knowledge almostalways perform better, but cheaply super-vised models often prove competitive oncertain tasks.1 IntroductionLearning word vector representations using mono-lingual distributional information is now a ubiqui-tous technique in NLP.
The quality of these wordvectors can be significantly improved by incor-porating cross-lingual distributional information(Klementiev et al, 2012; Zou et al, 2013; Vuli?cand Moens, 2013b; Mikolov et al, 2013b; Faruquiand Dyer, 2014; Hermann and Blunsom, 2014;Chandar et al, 2014, inter alia), with improve-ments observed both on monolingual (Faruqui andDyer, 2014; Rastogi et al, 2015) and cross-lingualtasks (Guo et al, 2015; S?gaard et al, 2015; Guoet al, 2016).Several models for inducing cross-lingual em-beddings have been proposed, each requiring a dif-ferent form of cross-lingual supervision ?
somecan use document-level alignments (Vuli?c andMoens, 2015), others need alignments at the sen-tence (Hermann and Blunsom, 2014; Gouws etal., 2015) or word level (Faruqui and Dyer, 2014;Gouws and S?gaard, 2015), while some requireboth sentence and word alignments (Luong et al,2015).
However, a systematic comparison of thesemodels is missing from the literature, making itdifficult to analyze which approach is suitable for aparticular NLP task.
In this paper, we fill this voidby empirically comparing four cross-lingual wordembedding models each of which require differentform of alignment(s) as supervision, across severaldimensions.
To this end, we train these models onfour different language pairs, and evaluate them onboth monolingual and cross-lingual tasks.1First, we show that different models can beviewed as instances of a more general frame-work for inducing cross-lingual word embeddings.Then, we evaluate these models on both extrin-sic and intrinsic tasks.
Our intrinsic evaluationassesses the quality of the vectors on monolin-gual (?4.2) and cross-lingual (?4.3) word simi-larity tasks, while our extrinsic evaluation spanssemantic (cross-lingual document classification?4.4) and syntactic tasks (cross-lingual depen-dency parsing ?4.5).Our experiments show that word vectors trainedusing expensive cross-lingual supervision (wordalignments or sentence alignments) perform thebest on semantic tasks.
On the other hand, for syn-tactic tasks like cross-lingual dependency parsing,models requiring weaker form of cross-lingual su-pervision (such as context agnostic translation dic-tionary) are competitive to models requiring ex-pensive supervision.
We also show qualitativelyhow the nature of cross-lingual supervision usedto train word vectors affects the proximity oftranslation pairs across languages, and of wordswith similar meaning in the same language in thevector-space.1Instructions and code to reproduce the experimentsavailable at http://cogcomp.cs.illinois.edu/page/publication_view/7941661Algorithm 1 General Algorithm1: InitializeW?W0,V?
V02: (W?,V?)?
argmin?A(W) + ?B(V) +C(W,V)Figure 1: (Above) A general schema for induction of cross-lingual word vector representations.
The word vector modelgenerates embeddings which incorporates distributional in-formation cross-lingually.
(Below) A general algorithm forinducing bilingual word embeddings, where ?, ?,W0,V0are parameters and A,B,C are suitably defined losses.2 Bilingual EmbeddingsA general schema for inducing bilingual embed-dings is shown in Figure 1.
Our comparison fo-cuses on dense, fixed-length distributed embed-dings which are obtained using some form ofcross-lingual supervision.
We briefly describe theembedding induction procedure for each of the se-lected bilingual word vector models, with the aimto provide a unified algorithmic perspective for allmethods, and to facilitate better understanding andcomparison.
Our choice of models spans acrossdifferent forms of supervision required for induc-ing the embeddings, illustrated in Figure 2.Notation.
Let W = {w1, w2, .
.
.
, w|W |} be thevocabulary of a language l1with |W | words, andW ?
R|W |?lbe the corresponding word embed-dings of length l. Let V = {v1, v2, .
.
.
, v|V |} bethe vocabulary of another language l2with |V |words, and V ?
R|V |?mthe corresponding wordembeddings of lengthm.
We denote the word vec-tor for a word w by w.2.1 Bilingual Skip-Gram Model (BiSkip)Luong et al (2015) proposed Bilingual Skip-Gram, a simple extension of the monolingual skip-gram model, which learns bilingual embeddingsby using a parallel corpus along with word align-ments (both sentence and word level alignments).The learning objective is a simple extensionof the skip-gram model, where the context of aword is expanded to include bilingual links ob-tained from word alignments, so that the model istrained to predict words cross-lingually.
In par-ticular, given a word alignment link from wordv ?
V in language l2to w ?
W in language l1,the model predicts the context words of w using vand vice-versa.
Formally, the cross lingual part ofthe objective is,D12(W,V) = ??
(v,w)?Q?wc?NBR1(w)logP (wc| v)(1)where NBR1(w) is the context ofw in language l1,Q is the set of word alignments, and P (wc| v) ?exp(wTcv).
Another similar term D21models theobjective for v and NBR2(v).
The objective can becast into Algorithm 1 as,C(W,V) = D12(W,V) +D21(W,V) (2)A(W) = ?
?w?W?wc?NBR1(w)logP (wc| w) (3)B(V) = ?
?v?V?vc?NBR2(v)logP (vc| v) (4)where A(W) and B(V) are the familiar skip-gram formulation of the monolingual part of theobjective.
?
and ?
are chosen hyper-parameterswhich set the relative importance of the monolin-gual terms.2.2 Bilingual Compositional Model (BiCVM)Hermann and Blunsom (2014) present a methodthat learns bilingual word vectors from a sentencealigned corpus.
Their model leverages the fact thataligned sentences have equivalent meaning, thustheir sentence representations should be similar.We denote two aligned sentences, ~v =?x1, .
.
.
, ?
and ~w = ?y1, .
.
.?
, where xi?V,yi?
W, are vectors corresponding to thewords in the sentences.
Let functions f : ~v ?
Rnand g : ~w ?
Rn, map sentences to their seman-tic representations in Rn.
BiCVM generates wordvectors by minimizing the squared `2norm be-tween the sentence representations of aligned sen-tences.
In order to prevent the degeneracy aris-ing from directly minimizing the `2norm, theyuse a noise-contrastive large-margin update, withrandomly drawn sentence pairs (~v, ~wn) as negativesamples.
The loss for the sentence pairs (~v, ~w) and(~v, ~wn) can be written as,E(~v, ~w, ~wn) = max (?
+ ?E(~v, ~w, ~wn), 0) (5)where,E(~v, ~w) = ?f(~v)?
g(~w)?2(6)1662I Love YouJe t?
aime(a) BiSkipI Love YouJe t?
aime(b) BiCVM(I, Je)(Love, aime)(You, t?
)(c) BiCCAHello!
how areyou?
I Love You.Bonjour!
Je t?
aime.
(d) BiVCDFigure 2: Forms of supervision required by the four models compared in this paper.
From left to right, the cost of the supervisionrequired varies from expensive (BiSkip) to cheap (BiVCD).
BiSkip requires a parallel corpus annotated with word alignments(Fig.
2a), BiCVM requires a sentence-aligned corpus (Fig.
2b), BiCCA only requires a bilingual lexicon (Fig.
2c) and BiVCDrequires comparable documents (Fig.
2d).and,?E(~v, ~w, ~wn) = E(~v, ~w)?
E(~v, ~wn) (7)This can be cast into Algorithm 1 by,C(W,V) =?aligned (~v,~w)random ~wnE(~v, ~w, ~wn) (8)A(W) = ?W?2B(V) = ?V?2(9)with A(W) and B(V) being regularizers, with?
= ?.2.3 Bilingual Correlation Based Embeddings(BiCCA)The BiCCA model, proposed by Faruqui and Dyer(2014), showed that when (independently trained)monolingual vector matrices W,V are projectedusing CCA (Hotelling, 1936) to respect a transla-tion lexicon, their performance improves on wordsimilarity and word analogy tasks.
They first con-struct W??
W,V??
V such that |W?|= |V?|and the corresponding words (wi, vi) in the matri-ces are translations of each other.
The projectionis then computed as:PW,PV= CCA(W?,V?)
(10)W?= WPWV?= VPV(11)where, PV?
Rl?d,PW?
Rm?dare the projec-tion matrices with d ?
min(l,m) and the V?
?R|V |?d,W??
R|W |?dare the word vectors thathave been ?enriched?
using bilingual knowledge.The BiCCA objective can be viewed2as the fol-lowing instantiation of Algorithm 1:W0= W?,V0= V?
(12)C(W,V) = ?W ?V?2+?
(VTW)(13)A(W) = ?W?2?1 B(V) = ?V?2?1 (14)where W = W0PWand V = V0PV, where weset ?
= ?
= ?
=?
to set hard constraints.2described in Section 6.5 of (Hardoon et al, 2004)2.4 Bilingual Vectors from Comparable Data(BiVCD)Another approach of inducing bilingual word vec-tors, which we refer to as BiVCD, was proposedby Vuli?c and Moens (2015).
Their approach isdesigned to use comparable corpus between thesource and target language pair to induce cross-lingual vectors.Let deand dfdenote a pair of comparabledocuments with length in words p and q respec-tively (assume p > q).
BiVCD first merges thesetwo comparable documents into a single pseudo-bilingual document using a deterministic strategybased on length ratio of two documents R = bpqc.Every Rthword of the merged pseudo-bilingualdocument is picked sequentially from df.
Finally,a skip-gram model is trained on the corpus ofpseudo-bilingual documents, to generate vectorsfor all words in W??
V?.
The vectors consti-tuting W?and V?can then be easily identified.Instantiating BiVCD in the general algorithmis obvious: C(W,V) assumes the familiarword2vec skip-gram objective over the pseudo-bilingual document,C(W,V) = ?
?s?W?V?t?NBR(s)logP (t | s)(15)where NBR(s) is defined by the pseudo-bilingualdocument and P (t | s) ?
exp(tTs).
Note thatt, s ?W ?
V .Although BiVCD is designed to use comparablecorpus, we provide it with parallel data in our ex-periments (to ensure comparability), and treat twoaligned sentences as comparable.3 DataWe train cross-lingual embeddings for 4 languagepairs: English-German (en-de), English-French(en-fr), English-Swedish (en-sv) and English-Chinese (en-zh).
For en-de and en-sv we use the1663l1l2#sent #l1-words #l2-wordsende 1.9 53 51fr 2.0 55 61sv 1.7 46 42zh 2.0 58 50Table 1: The size of parallel corpora (in millions) of differentlanguage pairs used for training cross-lingual word vectors.Europarl v7 parallel corpus3(Koehn, 2005).
Foren-fr, we use Europarl combined with the news-commentary and UN-corpus dataset from WMT2015.4For en-zh, we use the FBIS parallel cor-pus from the news domain (LDC2003E14).
Weuse the Stanford Chinese Segmenter (Tseng et al,2005) to preprocess the en-zh parallel corpus.
Cor-pus statistics for all languages is shown in Table 1.4 EvaluationWe measure the quality of the induced cross-lingual word embeddings in terms of their per-formance, when used as features in the followingtasks:?
monolingual word similarity for English?
Cross-lingual dictionary induction?
Cross-lingual document classification?
Cross-lingual syntactic dependency parsingThe first two tasks intrinsically measure howmuch can monolingual and cross-lingual similar-ity benefit from cross-lingual training.
The lasttwo tasks measure the ability of cross-linguallytrained vectors to extrinsically facilitate modeltransfer across languages, for semantic and syn-tactic applications respectively.
These tasks havebeen used in previous works (Klementiev et al,2012; Luong et al, 2015; Vuli?c and Moens, 2013a;Guo et al, 2015) for evaluating cross-lingual em-beddings, but no comparison exists which usesthem in conjunction.To ensure fair comparison, all models aretrained with embeddings of size 200.
We provideall models with parallel corpora, irrespective oftheir requirements.
Whenever possible, we alsoreport statistical significance of our results.3www.statmt.org/europarl/v7/{de,sv}-en.tgz4www.statmt.org/wmt15/translation-task.html4.1 Parameter SelectionWe follow the BestAvg parameter selection strat-egy from Lu et al (2015): we selected the param-eters for all models by tuning on a set of values(described below) and picking the parameter set-ting which did best on an average across all tasks.BiSkip.
All models were trained using a win-dow size of 10 (tuned over {5, 10, 20}), and30 negative samples (tuned over {10, 20, 30}).The cross-lingual weight was set to 4 (tunedover {1, 2, 4, 8}).
The word alignments fortraining the model (available at github.com/lmthang/bivec) were generated usingfast_align (Dyer et al, 2013).
The numberof training iterations was set to 5 (no tuning) andwe set ?
= 1 and ?
= 1 (no tuning).BiCVM.
We use the tool (available at github.com/karlmoritz/bicvm) released by Her-mann and Blunsom (2014) to train all embed-dings.
We train an additive model (that is, f(~x) =g(~x) =?ixi) with hinge loss margin set to200 (no tuning), batch size of 50 (tuned over50, 100, 1000) and noise parameter of 10 (tunedover {10, 20, 30}).
All models are trained for 100iterations (no tuning).BiCCA.
First, monolingual word vectors aretrained using the skip-gram model5with negativesampling (Mikolov et al, 2013a) with windowof size 5 (tuned over {5, 10, 20}).
To generate across-lingual dictionary, word alignments are gen-erated using cdec from the parallel corpus.
Then,word pairs (a, b), a ?
l1, b ?
l2are selected suchthat a is aligned to b the most number of times andvice versa.
This way, we obtained dictionaries ofapproximately 36k, 35k, 30k and 28k word pairsfor en-de, en-fr, en-sv and en-zh respectively.The monolingual vectors are aligned using theabove dictionaries with the tool (available atgithub.com/mfaruqui/eacl14-cca) re-leased by Faruqui and Dyer (2014) to generate thecross-lingual word embeddings.
We use k = 0.5as the number of canonical components (tunedover {0.2, 0.3, 0.5, 1.0}).
Note that this results ina embedding of size 100 after performing CCA.BiVCD.
We use word2vec?s skip gram modelfor training our embeddings, with a window sizeof 5 (tuned on {5, 10, 20, 30}) and negative sam-pling parameter set to 5 (tuned on {5, 10, 25}).Every pair of parallel sentences is treated as a5code.google.com/p/word2vec1664pair of comparable documents, and merging is per-formed using the sentence length ratio strategy de-scribed earlier.64.2 Monolingual EvaluationWe first evaluate if the inclusion of cross-lingualknowledge improves the quality of English em-beddings.Word Similarity.
Word similarity datasets con-tain word pairs which are assigned similarity rat-ings by humans.
The task evaluates how wellthe notion of word similarity according to humansis emulated in the vector space.
Evaluation isbased on the Spearman?s rank correlation coef-ficient (Myers and Well, 1995) between humanrankings and rankings produced by computing co-sine similarity between the vectors of two words.We use the SimLex dataset for English (Hillet al, 2014) which contains 999 pairs of En-glish words, with a balanced set of noun, adjec-tive and verb pairs.
SimLex is claimed to captureword similarity exclusively instead of WordSim-353 (Finkelstein et al, 2001) which captures bothword similarity and relatedness.
We declare sig-nificant improvement if p < 0.1 according toSteiger?s method (Steiger, 1980) for calculatingthe statistical significant differences between twodependent correlation coefficients.Table 2 shows the performance of English em-beddings induced by all the models by training ondifferent language pairs on the SimLex word sim-ilarity task.
The score obtained by monolingualEnglish embeddings trained on the respective En-glish side of each language is shown in columnmarked Mono.
In all cases (except BiCCA on en-sv), the bilingually trained vectors achieve betterscores than the mono-lingually trained vectors.Overall, across all language pairs, BiCVM isthe best performing model in terms of Spearman?scorrelation, but its improvement over BiSkip andBiVCD is often insignificant.
It is notable that 2 ofthe 3 top performing models, BiCVM and BiVCD,need sentence aligned and document-aligned cor-pus only, which are easier to obtain than paralleldata with word alignments required by BiSkip.QVEC.
Tsvetkov et al (2015) proposed an in-trinsic evaluation metric for estimating the qual-ity of English word vectors.
The score producedby QVEC measures how well a given set of wordvectors is able to quantify linguistic properties6We implemented the code for performing the merging aswe could not find a tool provided by the authors.pair Mono BiSkip BiCVM BiCCA BiVCDen-de 0.29 0.34 0.37 0.30 0.32en-fr 0.30 0.35 0.39 0.31 0.36en-sv 0.28 0.32 0.34 0.27 0.32en-zh 0.28 0.34 0.39 0.30 0.31avg.
0.29 0.34 0.37 0.30 0.33Table 2: Word similarity score measured in Spearman?s cor-relation ratio for English on SimLex-999.
The best score foreach language pair is shown in bold.
Scores which are sig-nificantly better (per Steiger?s Method with p < 0.1) thanthe next lower score are underlined.
For example, for en-zh,BiCVM is significantly better than BiSkip, which in turn issignificantly better than BiVCD.pair Mono BiSkip BiCVM BiCCA BiVCDen-de 0.39 0.40 0.31 0.33 0.37en-fr 0.39 0.40 0.31 0.33 0.38en-sv 0.39 0.39 0.31 0.32 0.37en-zh 0.40 0.40 0.32 0.33 0.38avg.
0.39 0.40 0.31 0.33 0.38Table 3: Intrinsic evaluation of English word vectors mea-sured in terms of QVEC score across models.
Best scores foreach language pair is shown in bold.of words, with higher being better.
The metricis shown to have strong correlation with perfor-mance on downstream semantic applications.
Asit can be currently only used for English, we useit to evaluate the English vectors obtained usingcross-lingual training of different models.
Ta-ble 3 shows that on average across language pairs,BiSkip achieves the best score, followed by Mono(mono-lingually trained English vectors), BiVCDand BiCCA.
A possible explanation for why Monoscores are better than those obtained by some ofthe cross-lingual models is that QVEC measuresmonolingual semantic content based on a linguis-tic oracle made for English.
Cross-lingual trainingmight affect these semantic properties arbitrarily.Interestingly, BiCVM which was the best modelaccording to SimLex, ranks last according toQVEC.
The fact that the best models accordingto QVEC and word similarities are different re-inforces observations made in previous work thatperformance on word similarity tasks alone doesnot reflect quantification of linguistic propertiesof words (Tsvetkov et al, 2015; Schnabel et al,2015).1665l1l2BiSkip BiCVM BiCCA BiVCDende 79.7 74.5 72.4 62.5fr 78.9 72.9 70.1 68.8sv 77.1 76.7 74.2 56.9zh 69.4 66.0 59.6 53.2avg.
76.3 72.5 69.1 60.4Table 4: Cross-lingual dictionary induction results (top-10accuracy).
The same trend was also observed across modelswhen computing MRR (mean reciprocal rank).4.3 Cross-lingual Dictionary InductionThe task of cross-lingual dictionary induc-tion (Vuli?c and Moens, 2013a; Gouws et al, 2015;Mikolov et al, 2013b) judges how good cross-lingual embeddings are at detecting word pairsthat are semantically similar across languages.
Wefollow the setup of Vuli?c and Moens (2013a), butinstead of manually creating a gold cross-lingualdictionary, we derived our gold dictionaries usingthe Open Multilingual WordNet data released byBond and Foster (2013).
The data includes synsetalignments across 26 languages with over 90% ac-curacy.
First, we prune out words from each synsetwhose frequency count is less than 1000 in the vo-cabulary of the training data from ?3.
Then, foreach pair of aligned synsets s1= {k1, k2, ?
?
?
}s2= {g1, g2, ?
?
?
}, we include all elements fromthe set {(k, g) | k ?
s1, g ?
s2} into the gold dic-tionary, where k and g are the lemmas.
Using thisapproach we generated dictionaries of sizes 1.5k,1.4k, 1.0k and 1.6k pairs for en-fr, en-de, en-svand en-zh respectively.We report top-10 accuracy, which is the frac-tion of the entries (e, f) in the gold dictionary, forwhich f belongs to the list of top-10 neighborsof the word vector of e, according to the inducedcross-lingual embeddings.
From the results (Ta-ble 4), it can be seen that for dictionary induction,the performance improves with the quality of su-pervision.
As we move from cheaply supervisedmethods (eg.
BiVCD) to more expensive supervi-sion (eg.
BiSkip), the accuracy improves.
Thissuggests that for cross lingual similarity tasks,the more expensive the cross-lingual knowledgeavailable, the better.
Models using weak super-vision like BiVCD perform poorly in comparisonto models like BiSkip and BiCVM, with perfor-mance gaps upwards of 10 pts on an average.l1l2BiSkip BiCVM BiCCA BiVCDende 85.2 85.0 79.1 79.9fr 77.7 71.7 70.7 72.0sv 72.3 69.1 65.3 59.9zh 75.5 73.6 69.4 73.0deen74.9 71.1 64.9 74.1fr 80.4 73.7 75.5 77.6sv 73.4 67.7 67.0 78.2zh 81.1 76.4 77.3 80.9avg.
77.6 73.5 71.2 74.5Table 5: Cross-lingual document classification accuracywhen trained on language l1, and evaluated on language l2.The best score for each language is shown in bold.
Scoreswhich are significantly better (per McNemar?s Test with p <0.05) than the next lower score are underlined.
For example,for sv?en, BiVCD is significantly better than BiSkip, whichin turn is significantly better than BiCVM.4.4 Cross-lingual Document ClassificationWe follow the cross-lingual document classifica-tion (CLDC) setup of Klementiev et al (2012), butextend it to cover all of our language pairs.
We usethe RCV2 Reuters multilingual corpus7for our ex-periments.
In this task, for a language pair (l1, l2),a document classifier is trained using the docu-ment representations derived from word embed-dings in language l1, and then the trained modelis tested on documents from language l2(andvice-versa).
By using supervised training data inone language and evaluating without further su-pervision in another, CLDC assesses whether thelearned cross-lingual representations are semanti-cally coherent across multiple languages.All embeddings are learned on the data de-scribed in ?3, and we only use the RCV2 data tolearn document classification models.
Followingprevious work, we compute document representa-tion by taking the tf-idf weighted average of vec-tors of the words present in it.8A multi-class clas-sifier is trained using an averaged perceptron (Fre-und and Schapire, 1999) for 10 iterations, usingthe document vectors of language l1as features9.Majority baselines for en ?
l2and l1?
en are49.7% and 46.7% respectively, for all languages.Table 5 shows the performance of different mod-els across different language pairs.
We computedconfidence values using the McNemar test (McNe-7http://trec.nist.gov/data/reuters/reuters.html8tf-idf (Salton and Buckley, 1988) was computed using alldocuments for that language in RCV2.9We use the implementation of Klementiev et al (2012).1666mar, 1947) and declare significant improvement ifp < 0.05.Table 5 shows that in almost all cases, BiSkipperforms significantly better than the remainingmodels.
For transferring semantic knowledgeacross languages via embeddings, sentence andword level alignment proves superior to sentenceor word level alignment alone.
This observationis consistent with the trend in cross-lingual dictio-nary induction, where too the most expensive formof supervision performed the best.4.5 Cross-lingual Dependency ParsingUsing cross lingual similarity for direct-transfer ofdependency parsers was first shown in T?ackstr?omet al (2012).
The idea behind direct-transfer isto train a dependency parsing model using em-beddings for language l1and then test the trainedmodel on language l2, replacing embeddings forlanguage l1with those of l2.
The transfer relieson coherence of the embeddings across languagesarising from the cross lingual training.
For our ex-periments, we use the cross lingual transfer setupof Guo et al (2015).10Their framework trains atransition-based dependency parser using nonlin-ear activation function, with the source-side em-beddings as lexical features.
These embeddingscan be replaced by target-side embeddings at testtime.All models are trained for 5000 iterations withfixed word embeddings during training.
Since ourgoal is to determine the utility of word embed-dings in dependency parsing, we turn off otherfeatures that can capture distributional informationlike brown clusters, which were originally used inGuo et al (2015).
We use the universal depen-dency treebank (McDonald et al, 2013) version-2.0 for our evaluation.
For Chinese, we use thetreebank released as part of the CoNLL-X sharedtask (Buchholz and Marsi, 2006).We first evaluate how useful the word embed-dings are in cross-lingual model transfer of depen-dency parsers (Table 6).
On an average, BiCCAdoes better than other models.
BiSkip is a closesecond, with an average performance gap of lessthan 1 point.
BiSkip outperforms BiCVM on Ger-man and French (over 2 point improvement), ow-ing to word alignment information BiSkip?s modeluses during training.
It is not surprising thatEnglish-Chinese transfer scores are low, due to thesignificant difference in syntactic structure of the10github.com/jiangfeng1124/acl15-clnndepl1l2BiSkip BiCVM BiCCA BiVCDende 49.8 47.5 51.3 49.0fr 65.8 63.2 65.9 60.7sv 56.9 56.7 59.4 54.6zh 6.4 6.1 6.4 6.0deen49.7 45.0 50.3 43.6fr 53.3 50.6 54.2 49.5sv 48.2 49.0 49.9 44.6zh 0.17 0.12 0.17 0.15avg.
41.3 39.8 42.2 38.5Table 6: Labeled attachment score (LAS) for cross-lingualdependency parsing when trained on language l1, and eval-uated on language l2.
The best score for each language isshown in bold.two languages.
Surprisingly, unlike the seman-tic tasks considered earlier, the models with ex-pensive supervision requirements like BiSkip andBiCVM could not outperform a cheaply super-vised BiCCA.We also evaluate whether using cross-linguallytrained vectors for learning dependency parsers isbetter than using mono-lingually trained vectorsin Table 7.
We compare against parsing modelstrained using mono-lingually trained word vectors(column marked Mono in Table 7).
These vectorsare the same used as input to the BiCCA model.All other settings remain the same.
On an aver-age across language pairs, improvement over themonolingual embeddings was obtained with theBiSkip and BiCCA models, while BiCVM andBiVCD consistently performed worse.
A possiblereason for this is that BiCVM and BiVCD oper-ate on sentence level contexts to learn the embed-dings, which only captures the semantic meaningof the sentences and ignores the internal syntac-tic structure.
As a result, embedding trained us-ing BiCVM and BiVCD are not informative forsyntactic tasks.
On the other hand, BiSkip andBiCCA both utilize the word alignment informa-tion to train their embeddings and thus do better incapturing some notion of syntax.5 Qualitative AnalysisFigure 3 shows the PCA projection of some of themost frequent words in the English-French corpus.It is clear that BiSkip and BiCVM produce cross-lingual vectors which are the most comparable, theEnglish and French words which are translationsof each other are represented by almost the samepoint in the vector-space.
In BiCCA and BiVCD16670.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.80.40.20.00.20.40.60.8marketworldcountryenergyproblemlawmoneychildrenpeacelifewarpays ch?l?
emondevie?nergieenfantsaixargentguerre loi(a) BiSkip0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.00.60.40.20.00.20.40.60.81.0marketlawcountryproblemenergymoneylifechildrenworldpeacewarpayspr bl?me ch?v e?
r ieargentmondeenfantsaixloiguerre(b) BiCVM0.4 0.2 0.0 0.2 0.4 0.6 0.80.80.60.40.20.00.20.40.6marketworldcountryenergyproblemlawmoneychildrenpeacelifewarpaysmarch?probl?mmondevie?nergieenfa tspaixargentguerreloi(c) BiCCA0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.60.60.40.20.00.20.40.60.8marketworldcountryenergyproblemlawmoneychildrenpeacelifewarpaysmarch?probl?
emondevie?nergieenfantspaixguerreargentloi(d) BiVCDFigure 3: PCA projection of word embeddings of some frequent words present in English-French corpus.
English and Frenchwords are shown in blue and red respectively.l Mono BiSkip BiCVM BiCCA BiVCDde 71.1 72.0 60.4 71.4 58.9fr 78.9 80.4 73.7 80.2 69.5sv 75.5 78.2 70.5 79.0 64.5zh 73.8 73.1 65.8 71.7 67.0avg.
74.8 75.9 67.6 75.6 66.8Table 7: Labeled attachment score (LAS) for dependencyparsing when trained and tested on language l. Mono refersto parser trained with mono-lingually induced embeddings.Scores in bold are better than the Mono scores for each lan-guage, showing improvement from cross-lingual training.the translated words are more distant than BiSkipand BiCVM.
This is not surprising because BiSkipand BiCVM require more expensive supervisionat the sentence level in contrast to the other twomodels.An interesting observation is that BiCCA andBiVCD are better at separating antonyms.
Thewords peace and war, (and their French trans-lations paix and guerre) are well separated inBiCCA and BiVCD.
However, in BiSkip andBiCVM these pairs are very close together.
Thiscan be attributed to the fact that BiSkip andBiCVM are trained on parallel sentences, and iftwo antonyms are present in the same sentence inEnglish, they will also be present together in itsFrench translation.
However, BiCCA uses bilin-gual dictionary and BiVCD use comparable sen-tence context, which helps in pulling apart the syn-onyms and antonyms.6 DiscussionThe goal of this paper was to formulate the taskof learning cross-lingual word vector representa-tions in a unified framework, and conduct exper-iments to compare the performance of existing1668models in a unbiased manner.
We chose exist-ing cross-lingual word vector models that can betrained on two languages at a given time.
In re-cent work, Ammar et al (2016) train multilingualword vectors using more than two languages; ourcomparison does not cover this setting.
It is alsoworth noting that we compare here different cross-lingual word embeddings, which are not to be con-fused with a collection of monolingual word em-beddings trained for different languages individu-ally (Al-Rfou et al, 2013).The paper does not cover all approachesthat generate cross-lingual word embeddings.Some methods do not have publicly availablecode (Coulmance et al, 2015; Zou et al, 2013);for others, like BilBOWA (Gouws et al, 2015), weidentified problems in the available code, whichcaused it to consistently produced results that areinferior even to mono-lingually trained vectors.11However, the models that we included for com-parison in our survey are representative of othercross-lingual models in terms of the form of cross-lingual supervision required by them.
For exam-ple, BilBOWA (Gouws et al, 2015) and cross-lingual Auto-encoder (Chandar et al, 2014) aresimilar to BiCVM in this respect.
Multi-viewCCA (Rastogi et al, 2015) and deep CCA (Lu etal., 2015) can be viewed as extensions of BiCCA.Our choice of models was motivated to com-pare different forms of supervision, and therefore,adding these models, would not provide additionalinsight.7 ConclusionWe presented the first systematic comparativeevaluation of cross-lingual embedding methods onseveral downstream NLP tasks, both intrinsic andextrinsic.
We provided a unified representationfor all approaches, showing them as instances ofa general algorithm.
Our choice of methods spansa diverse range of approaches, in that each requiresa different form of supervision.Our experiments reveal interesting trends.When evaluating on intrinsic tasks such as mono-lingual word similarity, models relying on cheaperforms of supervision (such as BiVCD) perform al-most on par with models requiring expensive su-pervision.
On the other hand, for cross-lingual se-mantic tasks, like cross-lingual document classi-fication and dictionary induction, the model withthe most informative supervision performs best11We contacted the authors of the papers and were unableto resolve the issues in the toolkit.overall.
In contrast, for the syntactic task of de-pendency parsing, models that are supervised ata word alignment level perform slightly better.Overall this suggests that semantic tasks can ben-efit more from richer cross-lingual supervision, ascompared to syntactic tasks.AcknowledgementThis material is based on research sponsored by DARPAunder agreement number FA8750-13-2-0008 and ContractHR0011-15-2-0025.
Approved for Public Release, Distribu-tion Unlimited.
The views expressed are those of the authorsand do not reflect the official policy or position of the Depart-ment of Defense or the U.S. Government.ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual nlp.
In Proc.
of CoNLL.Waleed Ammar, George Mulcaire, Yulia Tsvetkov,Guillaume Lample, Chris Dyer, and Noah A Smith.2016.
Massively multilingual word embeddings.arXiv preprint arXiv:1602.01925.Francis Bond and Ryan Foster.
2013.
Linking and ex-tending an open multilingual wordnet.
In Proc.
ofACL.Sabine Buchholz and Erwin Marsi.
2006.
Conll-Xshared task on multilingual dependency parsing.
InProc.
of CoNLL.Sarath Chandar, Stanislas Lauly, Hugo Larochelle,Mitesh Khapra, Balaraman Ravindran, Vikas CRaykar, and Amrita Saha.
2014.
An autoencoderapproach to learning bilingual word representations.In Proc.
of NIPS.Jocelyn Coulmance, Jean-Marc Marty, GuillaumeWenzek, and Amine Benhalloum.
2015.
Trans-gram, fast cross-lingual word-embeddings.
In Proc.of EMNLP.Chris Dyer, Victor Chahuneau, and Noah A Smith.2013.
A simple, fast, and effective reparameteriza-tion of ibm model 2.
In Proc.
of NAACL.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In Proc.
of EACL.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: theconcept revisited.
In Proc.
of WWW.Yoav Freund and Robert E Schapire.
1999.
Largemargin classification using the perceptron algorithm.Machine learning, 37(3):277?296.1669Stephan Gouws and Anders S?gaard.
2015.
Simpletask-specific bilingual word embeddings.
In Proc.of NAACL.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2015.
Bilbowa: Fast bilingual distributed represen-tations without word alignments.
In Proc.
of ICML.Jiang Guo, Wanxiang Che, David Yarowsky, HaifengWang, and Ting Liu.
2015.
Cross-lingual depen-dency parsing based on distributed representations.In Proc.
of ACL.Jiang Guo, Wanxiang Che, David Yarowsky, HaifengWang, and Ting Liu.
2016.
A representation learn-ing framework for multi-source transfer parsing.
InProc.
of AAAI.David R Hardoon, Sandor Szedmak, and John Shawe-Taylor.
2004.
Canonical correlation analysis:An overview with application to learning methods.Neural computation, 16(12):2639?2664.Karl Moritz Hermann and Phil Blunsom.
2014.
Multi-lingual Models for Compositional Distributional Se-mantics.
In Proc.
of ACL.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377.Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.
In Proc.
of COLING.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proc.
of MT Sum-mit.Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel,and Karen Livescu.
2015.
Deep multilingual corre-lation for improved word embeddings.
In Proc.
ofNAACL.Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015.
Bilingual word representations withmonolingual quality in mind.
In Proc.
of the Work-shop on Vector Space Modeling for NLP.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuz-man Ganchev, Keith Hall, Slav Petrov, HaoZhang, Oscar T?ackstr?om, Claudia Bedini, N?uriaBertomeu Castell?o, and Jungmee Lee.
2013.
Uni-versal dependency annotation for multilingual pars-ing.
In Proc.
of ACL.Quinn McNemar.
1947.
Note on the sampling errorof the difference between correlated proportions orpercentages.
Psychometrika, 12(2):153?157.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.2013b.
Exploiting similarities among lan-guages for machine translation.
arXiv preprintarXiv:1309.4168.Jerome L. Myers and Arnold D. Well.
1995.
ResearchDesign & Statistical Analysis.
Routledge.Pushpendre Rastogi, Benjamin Van Durme, and RamanArora.
2015.
Multiview LSA: Representation learn-ing via generalized CCA.
In Proceedings of NAACL.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.
In-formation Processing and Management.Tobias Schnabel, Igor Labutov, David Mimno, andThorsten Joachims.
2015.
Evaluation methodsfor unsupervised word embeddings.
In Proc.
ofEMNLP.Anders S?gaard,?Zeljko Agi?c, H?ector Mart?
?nez Alonso,Barbara Plank, Bernd Bohnet, and Anders Jo-hannsen.
2015.
Inverted indexing for cross-lingualnlp.
In Proc.
of ACL.James H Steiger.
1980.
Tests for comparing ele-ments of a correlation matrix.
Psychological bul-letin, 87(2):245.O.
T?ackstr?om, R. McDonald, and J. Uszkoreit.
2012.Cross-lingual word clusters for direct transfer of lin-guistic structure.
In NAACL.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proc.
of SIGHAN.Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-laume Lample, and Chris Dyer.
2015.
Evaluation ofword vector representations by subspace alignment.In Proc.
of EMNLP.Ivan Vuli?c and Marie-Francine Moens.
2013a.
Cross-lingual semantic similarity of words as the similar-ity of their semantic word responses.
In Proc.
ofNAACL.Ivan Vuli?c and Marie-Francine Moens.
2013b.
A studyon bootstrapping bilingual vector spaces from non-parallel data (and nothing else).
In Proc.
of EMNLP.Ivan Vuli?c and Marie-Francine Moens.
2015.
Bilin-gual word embeddings from non-parallel document-aligned data applied to bilingual lexicon induction.In Proc.
of ACL.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word embed-dings for phrase-based machine translation.
In Proc.of EMNLP.1670
