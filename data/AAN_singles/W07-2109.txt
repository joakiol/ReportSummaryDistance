Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 488?491,Prague, June 2007. c?2007 Association for Computational LinguisticsXRCE-M: A Hybrid System for Named Entity Metonymy Resolution*Caroline Brun*Maud Ehrmann*Guillaume Jacquet* Xerox Research Centre Europe6, chemin de Maupertuis38240 Meylan France*{Caroline.Brun, Maud.Ehrmann, Guillaume.Jacquet}@xrce.xerox.comAbstractThis paper describes our participation to theMetonymy resolution at SemEval 2007 (task#8).
In order to perform named entity me-tonymy resolution, we developed a hybridsystem based on a robust parser that extractsdeep syntactic relations combined with anon-supervised distributional approach, alsorelying on the relations extracted by theparser.1 Description of our SystemSemEval 2007 introduces a task aiming at resolvingmetonymy for named entities, for location and or-ganization names (Markert and Nissim 2007).
Oursystem addresses this task by combining a symbolicapproach based on robust deep parsing and lexicalsemantic information, with a distributional methodusing syntactic context similarities calculated onlarge corpora.
Our system is completely unsuper-vised, as opposed to state-of-the-art systems (see(Market and Nissim, 2005)).1.1 Robust and Deep Parsing Using XIPWe use the Xerox Incremental Parser (XIP, (A?t etal., 2002)) to perform robust and deep syntacticanalysis.
Deep syntactic analysis consists here in theconstruction of a set of syntactic relations1 from aninput text.
These relations, labeled with deep syn-tactic functions, link lexical units of the input textand/or more complex syntactic domains that areconstructed during the processing (mainly chunks,see (Abney, 1991)).1 inspired from dependency grammars, see (Mel?
?uk,1998), and (Tesni?re, 1959).Moreover, together with surface syntactic relations,the parser calculates more sophisticated relationsusing derivational morphologic properties, deepsyntactic properties2, and some limited lexical se-mantic coding (Levin's verb class alternations, see(Levin, 1993)), and some elements of the Framenet3classification, (Ruppenhofer et al, 2006)).
Thesedeep syntactic relations correspond roughly to theagent-experiencer roles that is subsumed by theSUBJ-N relation and to the patient-theme role sub-sumed by the OBJ-N relation, see (Brun and  Ha-g?ge, 2003).
Not only verbs bear these relations butalso deverbal nouns with their corresponding argu-ments.Here is an example of an output (chunks anddeep syntactic relations):Lebanon still wanted to see the implementation of a UNresolutionTOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{theimplementation} PP{of NP{a UN resolution}} .
}MOD_PRE(wanted,still)MOD_PRE(resolution,UN)MOD_POST(implementation,resolution)COUNTRY(Lebanon)ORGANISATION(UN)EXPERIENCER_PRE(wanted,Lebanon)EXPERIENCER(see,Lebanon)CONTENT(see,implementation)EMBED_INFINIT(see,wanted)OBJ-N(implement,resolution)1.2 Adaptation to the TaskOur parser includes a module for ?standard?named entity recognition, but needs to be adapted tohandle named entity metonymy.
Following theguidelines of the SemEval task #8, we performed a2 Subject and object of infinitives in the context of con-trol verbs.3 http://framenet.icsi.berkeley.edu/488corpus study on the trial data in order to detect lexi-cal and syntactic regularities triggering a metonymy,for both location names and organization names.For example, we examined the subject relation be-tween organizations or locations and verbs and wethen classify the verbs accordingly: we draw hy-pothesis like ?if a location name is the subject of averb referring to an economic action, like import,provide, refund, repay, etc., then it is a place-for-people?.
We adapted our parser by adding dedicatedlexicons that encode the information collected fromthe corpus and develop rules modifying the interpre-tation of the entity, for example:If (LOCATION(#1) & SUBJ-N(#2[v_econ],#1))4?
PLACE-FOR-PEOPLE(#1)We focus our study on relations like subject, object,experiencer, content, modifiers (nominal and prepo-sitional) and attributes.
We also capitalize on thealready-encoded lexical information attached toverbs by the parser, like communication verbs likesay, deny, comment, or categories of the FrameNetExperiencer subject frame, i.e.
verbs like feel, sense,see.
This information was very useful since experi-encers denote persons, therefore all organizations orlocations having an experiencer role can be consid-ered as organization-for-members or place-for-people.
Here is an example of output5, when apply-ing the modified parser on the following sentence:?It was the largest Fiat everyone had ever seen?.ORG-FOR-PRODUCT(Fiat)MOD_PRE(seen,ever)SUBJ-N_PRE(was,It)EXPERIENCER_PRE(seen,everyone)SUBJATTR(It,Fiat)QUALIF(Fiat,largest)Here, the relation QUALIF(Fiat, largest) triggersthe metonymical interpretation of ?Fiat?
as org-for-product.This first development step is the starting point ofour methodology, which is completed by a non-supervised distributional approach described in thenext section.4 Which read as ?if the parser has detected a locationname (#1), which is the subject of a verb (#2) bearing thefeature ?v-econ?, then create a PLACE-FOR-PEOPLEunary predicate on #1.5 Only dependencies are shown.1.3 Hybridizing with a Distributional ApproachThe distributional approach proposes to establish adistance between words depending on there syntac-tic distribution.The distributional hypothesis is that words that ap-pear in similar contexts are semantically similar(Harris, 1951): the more two words have the samedistribution, i.e.
are found in the same syntactic con-texts, the more they are semantically close.We propose to apply this principle for metonymyresolution.
Traditionally, the distributional approachgroups words like USA, Britain, France, Germanybecause there are in the same syntactical contexts:(1) Someone live in Germany.
(2) Someone works in Germany.
(3) Germany declares something.
(4) Germany signs something.The metonymy resolution task implies to distin-guish the literal cases, (1) & (2), from the meto-nymic ones, (3) & (4).
Our method establishes thesedistinctions using the syntactic context distribution.We group contexts occurring with the same words:the syntactic contexts live in and work in are occur-ring with Germany, France, country, city, place,when syntactic contexts subject-of-declare and sub-ject-of-sign are occurring with Germany, France,someone, government, president.For each Named Entity annotation, the hybridmethod consists in using symbolic annotation ifthere is (?1.2), else using distributional annotation(?1.3) as presented below.Method: We constructed a distributional space withthe 100M-word BNC.
We prepared the corpus bylemmatizing and then parsing with the same robustparser than for the symbolic approach (XIP, see sec-tion 3.1).
It allows us to identify triple instances.Each triple have the form w1.R.w2 where w1 andw2 are lexical units and R is a syntactic relation(Lin, 1998; Kilgarriff & al.
2004).Our approach can be distinguished from classicaldistributional approach by different points.First, we use triple occurrences to build a distribu-tional space (one triple implies two contexts andtwo lexical units), but we use the transpose of theclassical space: each point xi of this space is a syn-tactical context (with the form R.w.
), each dimen-sion j is a lexical units, and each value xi(j) is thefrequency of corresponding triple occurrences.
Sec-489ond, our lexical units are words but also complexnominal groups or verbal groups.
Third, contextscan be simple contexts or composed contexts6.We illustrate these three points on the phrase pro-vide Albania with food aid.
The XIP parser givesthe following triples where for example, food aid isconsidered as a lexical unit:OBJ-N('VERB:provide','NOUN: Albania').PREP_WITH('VERB: provide ','NOUN:aid').PREP_WITH('VERB: provide ','NP:food aid').From these triples, we create the following lexicalunits and contexts (in the context 1.VERB: provide.OBJ-N, ?1?
mean that the verb provide is the gov-ernor of the relation OBJ-N):Words: Contexts:VERB:provide 1.VERB: provide.
OBJ-NNOUN:Albania 1.VERB: provide.PREP_WITHNOUN:aid 2.NOUN: Albania.OBJ-NNP:food aid 2.NOUN: aid.
PREP_WITH2.NP: food aid.
PREP_WITH1.VERB:provide.OBJ-N+2.NOUN:aid.
PREP_WITH1.VERB:provide.OBJ-N+2.NP:food aid.
PREP_WITH1.VERB:provide.PREP_WITH +2.NO:Albania.OBJ-NWe use a heuristic to control the high productivityof these lexical units and contexts.
Each lexical unitand each context should appear more than 100 timesin the corpus.
From the 100M-word BNC we ob-tained 60,849 lexical units and 140,634 contexts.Then, our distributional space has 140,634 units and60,849 dimensions.Using the global space to compute distances be-tween each context is too consuming and wouldinduce artificial ambiguity (Jacquet, Venant, 2005).If any named entity can be used in a metonymicreading, in a given corpus each named entity has notthe same distribution of metonymic readings.
Thecountry Vietnam is more frequently used as an eventthan France or Germany, so, knowing that a contextis employed with Vietnam allow to reduce the meto-nymic ambiguity.For this, we construct a singular sub-space de-pending to the context and to the lexical unit (theambiguous named entity):For a given couple context i + lexical unit j weconstruct a subspace as follows:Sub_contexts = list of contexts which are occur-ring with the word i.
If there are more than k con-texts, we take only the k more frequents.Sub_dimension = list of lexical units which areoccurring with at least one of the contexts from the6 For our application, one context can be composed bytwo simple contexts.Sub_contexts list.
If there are more than n words,we take only the n more frequents (relative fre-quency) with the Sub_contexts list (for this applica-tion, k = 100 and n = 1,000).We reduce dimensions of this sub-space to 10dimensions with a PCA (Principal ComponentsAnalysis).In this new reduced space (k*10), we computethe closest context of the context j with the Euclid-ian distance.At this point, we use the results of the symbolicapproach described before as starting point.
We at-tribute to each context of the Sub_contexts list, theannotation, if there is, attributed by symbolic rules.Each kind of annotation (literal, place-for-people,place-for-event, etc) is attributed a score corre-sponding to the sum of the scores obtained by eachcontext annotated with this category.
The score of acontext i  decreases in inverse proportion to its dis-tance from the context j: score(context i) =1/d(context i, context j) where d(i,j) is the Euclidiandistance between i and j.We illustrate this process with the sentence pro-vide Albania with food aid.
The unit Albania isfound in 384 different contexts (|Sub_contexts| =384) and 54,183 lexical units are occurring with atleast one of the contexts from the Sub_contexts list(|Sub_dimension| = 54,183).After reducing dimension with PCA, we obtainthe context list below ordered by closeness with thegiven context (1.VERB:provide.OBJ-N):Contexts   d symb.
annot.1.VERB:provide.OBJ-N  0.001.VERB:allow.OBJ-N  0.76         place-for-people1.VERB:include.OBJ-N  0.962.ADJ:new.MOD_PRE  1.021.VERB:be.SUBJ-N  1.431.VERB:supply.SUBJ-N_PRE 1.47 literal1.VERB:become.SUBJ-N_PRE 1.641.VERB:come.SUBJ-N_PRE  1.691.VERB:support.SUBJ-N_PRE 1.70          place-for-peopleetc.Score for each metonymic annotation of Albania:?
place-for-people 3.11literal  1.23place-for-event  0.00?
0.00The score obtained by each annotation type al-lows annotating this occurrence of Albania as aplace-for-people metonymic reading.
If we can?tchoose only one annotation (all score = 0 or equal-ity between two annotations) we do not annotate.4902 Evaluation and ResultsThe following tables show the results on the testcorpus:type Nb.sampaccuracy coverage BaselineaccuracyBaselinecoverageLoc/coarse 908 0.851 1 0.794 1Loc/medium 908 0.848 1 0.794 1Loc /fine 908 0.841 1 0.794 1Org/coarse 842 0.732 1 0.618 1Org/medium 842 0.711 1 0.618 1Org/fine 842 0.700 1 0.618 1Table 1: Global ResultsNbocc.Prec.
Recall F-scoreLiteral 721 0.867 0.960 0.911Place-for-people 141 0.651 0.490 0.559Place-for-event 10 0.5 0.1 0.166Place-for-product 1 _ 0 0Object-for-name 4 1 0.5 0.666Object-for-representation 0 _ _ _Othermet 11 _ 0 0mixed 20 _ 0 0Table 2: Detailed Results for LocationsNbocc.Prec.
Recall F-scoreLiteral 520 0.730 0.906 0.808Organization-for-members 161 0.622 0.522 0.568Organization-for-event 1 _ 0 0Organization-for-product 67 0.550 0.418 0.475Organization-for-facility 16 0.5 0.125 0.2Organization-for-index 3 _ 0 0Object-for-name 6 1 0.666 0.8Othermet 8 _ 0 0Mixed  60 _ 0 0Table 3: Detailed Results for OrganizationsThe results obtained on the test corpora are abovethe baseline for both location and organizationnames and therefore are very encouraging for themethod we developed.
However, our results on thetest corpora are below the ones we get on the traincorpora, which indicates that there is room for im-provement for our methodology.Identified errors are of different nature:Parsing errors: For example in the sentence ?Manygalleries in the States, England and France de-clined the invitation.
?, because the analysis of thecoordination is not correct, France is calculated assubject of declined, a context triggering a place-for-people interpretation, which is wrong here.Mixed cases: These phenomena, while relativelyfrequent in the corpora, are not properly treated.Uncovered contexts: some of the syntactico-semantic contexts triggering a metonymy are notcovered by the system at the moment.3 ConclusionThis paper describes a system combining a sym-bolic and a non-supervised distributional approach,developed for resolving location and organizationnames metonymy.
We plan to pursue this work inorder to improve the system on the already-coveredphenomenon as well as on different names entities.ReferencesAbney S. 1991.
Parsing by Chunks.
In Robert Berwick, StevenAbney and Carol Teny (eds.).
Principle-based Parsing, Klu-wer Academics Publishers.A?t-Mokhtar S., Chanod, J.P., Roux, C. 2002.
Robustness be-yond Shallowness: Incremental Dependency Parsing.
Spe-cial issue of NLE journal.Brun, C., Hag?ge C., 2003.
Normalization and ParaphrasingUsing Symbolic Methods, Proceeding of the Second Interna-tional Workshop on Paraphrasing.
ACL 2003, Vol.
16, Sap-poro, Japan.Harris Z.
1951.
Structural Linguistics, University of ChicagoPress.Jacquet G.,Venant F. 2003.
Construction automatique de clas-ses de s?lection distributionnelle, In Proc.
TALN 2003,Dourdan.Kilgarriff A., Rychly P., Smrz P., Tugwell D.  2004.
The sketchengine.
In Proc.
EURALEX, pages 105-116.Levin, B.
1993.
English Verb Classes and Alternations ?
Apreliminary Investigation.
The University of Chicago Press.Nissim, M. and Markert, K. 2005.
Learning to buy a Renaultand to talk to a BMW: A supervised approach to conven-tional metonymy.
Proceedings of the 6th International Work-shop on Computational Semantics, Tilburg.Nissim, M. and Markert, K. 2007.
SemEval-2007 Task 08: Me-tonymy Resolution at SemEval-2007.
In Proceedings of Se-mEval-2007.Lin D. 1998.
Automatic retrieval and clustering of similarwords.
In COLING-ACL, pages 768-774.Mel?
?uk I.
1988.
Dependency Syntax.
State University of NewYork, Albany.Ruppenhofer, J. Michael Ellsworth, Miriam R. L. Petruck,Christopher R Johnson and Jan Scheffczyk.
2006.
FramenetII: Extended Theory and Practice.Tesni?re L. 1959.
El?ments de Syntaxe Structurale.
KlincksiekEds.
(Corrected edition Paris 1969).491
