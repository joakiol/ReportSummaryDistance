Proceedings of NAACL-HLT 2013, pages 259?269,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsMeasuring Term Informativeness in ContextZhaohui WuComputer Science and EngineeringThe Pennsylvania State UniversityUniversity Park, PA 16802, USAzzw109@psu.eduC.
Lee GilesInformation Sciences and TechnologyComputer Science and EngineeringThe Pennsylvania State UniversityUniversity Park, PA 16802, USAgiles@ist.psu.eduAbstractMeasuring term informativeness is a funda-mental NLP task.
Existing methods, mostlybased on statistical information in corpora, donot actually measure informativeness of a termwith regard to its semantic context.
This pa-per proposes a new lightweight feature-freeapproach to encode term informativeness incontext by leveraging web knowledge.
Givena term and its context, we model context-aware term informativeness based on semanticsimilarity between the context and the term?smost featured context in a knowledge base,Wikipedia.
We apply our method to three ap-plications: core term extraction from snippets(text segment), scientific keywords extraction(paper), and back-of-the-book index genera-tion (book).
The performance is state-of-the-art or close to it for each application, demon-strating its effectiveness and generality.1 IntroductionComputationally measuring importance of a wordin text, or ?term informativeness?
(Kireyev, 2009;Rennie and Jaakkola, 2005), is fundamental to manyNLP tasks such as keyword extraction, text catego-rization, clustering, and summarization, etc.
Variousfeatures derived from statistical and linguistic infor-mation can be helpful in encoding term informative-ness, whereas practical feature definition and selec-tion are usually ad hoc, data-driven and applicationdependent.
Statistical information based on termfrequency (TF) and document frequency (DF) tendto be more effective in finding keywords in largecorpora, but can have issues with small amounts oftext or small corpora.
Linguistic information suchas POS tag patterns often require manual selectionbased on prior applications.
We contend that fewmethods actually measure the informativeness of aterm to the discourse unit it contains.
For example,given a context such as ?A graph comprises nodes(also called vertices) connected by links (also knownas edges or arcs)?, it is difficult to measure theterm informativeness of ?graph?, ?nodes?, or ?links?based on any statistical or linguistic information.This raises many issues.
Is there a fundamentaland less ad hoc way to measure the term informa-tiveness of a word within a discourse unit?
Can weactually find a general approach based on compre-hensive and high-level ?knowledge?
and not haveto nitpick over features?
Can this new metric beeffectively applied to real world applications?
Toanswer these questions, we develop a new term in-formativeness metric, motivated by query-documentrelevance in information retrieval.
The higher therelevance score a query-document pair is, the moreinformative the query is to the document.
If a sim-ilar principle also exists between word and con-text and there is an effective search engine return-ing ranked contexts for a given word, then we con-tend that word is more informative in the higher rankcontexts.
To see the term informativeness of threewords ?graph?, ?nodes?
and ?links?
in context, wemanually check the search results from Wikipedia,Google, and Bing.
We found that very similar con-texts are among the top 5 ranked results of ?graph?while no such contexts appear in that of the othertwo words.
Thus, we define a context-aware terminformativeness based on the semantic relatedness259between the context and the term?s featured contexts(or the top important contexts that cover most of aterm?s semantics).We apply the context-aware term informativeness(CTI) to three typical NLP applications: core termextraction in snippets, keyword extraction and back-of-the-book index generation.
Experiments showthat the method is effective and efficient.
Moreover,the metric can be easily combined with other meth-ods, or as a feature for learning algorithms.The remainder of this paper is organized as fol-lows.
Section 2 reviews the literature of term infor-mativeness measurements.
Section 3 proposes theformal definition of the context-aware term informa-tiveness as well as its practical implementation usingWeb knowledge.
Section 4 studies the three appli-cations.
Finally, we conclude with discussion andfuture work.2 Related WorkMost known approaches to measure term informa-tiveness fall into basically two categories: statistics-based and semantic-based.Statistics-based methods, such as TFIDF (Saltonand Buckley, 1988), ResidualIDF(RIDF), Variance,Burstiness and Gain, are based on derivations fromterm frequency (TF) and document frequency (DF).Sprck Jones defines IDF or inverse document fre-quency as:IDF (w) = ?log2(dfw/D) (1)where D is the size of the corpus (Jones, 1972;Jones, 1973).
Based on a finding that informativewords tend to have large deviation between IDFand collection frequency fw(the total number of oc-currence of a word), many other informativenessscores have been proposed.
Bookstein and Swan-son (Bookstein and Swanson, 1974) introduced thexI as:XI = fw ?
dfwChurch and Gale (1995) introducedvariance(w) = 1D ?
1D?d=1(tdw ?
t?w) (2)where tdw denotesw?s TF in d and t?w = fw/D indi-cates its mean expected word rate.
Another measuresuggested by them isburstiness(w) = fwdfw(3)which tends to compare collection frequency anddocument frequency directly.
Informative wordswere found to have IDF scores that are larger thanwhat would be expected according to the Poissonmodel; residual IDF (RIDF) was introduced to mea-sure this deviationRIDF (w) = IDF (w)?
?IDF (w) (4)where ?IDF (w) = ?log2(1 ?
e?t?w).
In addition,Papineni (2001) introduced the notion of gain asgain(w) = dfwD(dfwD ?
1?
log(dfwD ))(5)More recently, Rennie and Jaakkola (2005) intro-duced an informativeness score based on the fit ofa word?s frequency to a mixture of 2 Unigram dis-tribution and applied it to named entity detection.
Itis worth noting that term necessity, which measuresthe probability that a term occurs in documents rel-evant to a given query, has been well studied in In-formation Retrieval community (Zhao and Callan,2010; Yang and Callan, 2010).
Though our CIT isnot designed for probabilistic retrieval models, wemay apply it to measure the term necessity in a queryby considering it as a context.Despite extensive research on semantic analysisand understanding of word and text (Deerwester etal., 1990; Budanitsky and Hirst, 2006; Cilibrasi andVitanyi, 2007; Gabrilovich and Markovitch, 2007;Agirre et al 2009; Yazdani and Popescu-Belis,2012), little work studied the measurement of thesemantics of term informativeness.
An exceptionis the LSAspec from Kireyev (2009), based on la-tent semantic analysis (Deerwester et al 1990),which is defined as the ratio of a term?s LSA vec-tor length to its document frequency and thus canbe interpreted as the rate of vector length growth.However, latent semantic models such as LSA arenotoriously hard to interpret since the ?latent con-cepts?
cannot be readily mapped to human knowl-edge (Gabrilovich and Markovitch, 2007).
Our ap-proach explicitly leverages the semantics of wordand text using existing knowledge bases.260Previous methods, all corpus-based, might be ef-fective in identifying informative words at the doc-ument or corpus level, but do not the ability to cap-ture term informativeness in a particular context dueto their absence of semantics and obliviousness ofcontext.
Our method measures the term informative-ness within a context in a semantic-based approach,regardless of the absence of statistical information.3 Context-aware Term Informativeness3.1 ContextA context of a word or phrase may refer to a fewwords nearby (He et al 2010), a sentence or para-graph (Soricut and Marcu, 2003), or even a setof documents containing it (Cilibrasi and Vitanyi,2007).
Here we define context as a syntactic unitof discourse such as a sentence or paragraph, for ex-ample, ?PL/SQL is one of three key programminglanguages embedded in the Oracle Database?, or?There are two types of functions in PL/SQL?.
Theuniversal context set U(t) of a word t is defined asall the contexts containing it in the web.
Differentcontexts vary in their authority just like web pagesvary.
For the two examples, we could argue thatthe first context is much more ?authoritative?
thanthe second.
This can be verified by their popular-ity on Google; (all results from actual search en-gines were at the time of this publication) the firstretrieves approximately 302,000 exact matching re-sults while the second retrieves only one.
We con-sider this as the number of citations of a context,which, to some extent, indicates its ?authority?.
Wedefine the source of a context as the set of all docu-ments citing it.
Here ?citing?
instead of ?containing?is used because some documents may not literallycontain an exact copy of the context.Given a term t, define its universal context setU(t) = {ci} and the source of ci is S(ci) = {dij}.Ideally, the authority of a context will be contributedby every document citing it.
Therefore, we definethe authority score of a context asCA(ci) =?jDA(dij) (6)where DA(dij) denotes the authority contributed bydij .
It is very difficult to acquire the universal con-text set of a term.
Considering that usually we onlycare about the top few results of a query returned bysearch engines and ignore a large faction of less im-portant ones, it is reasonable to assume that a term?ssemantics will be well covered by a few importantcontexts.
We therefore define the featured contextset of term t, or Uf (t), as the top k contexts withthe highest authority scores, where k is an applica-tion dependent parameter.
In our experiments, thedefault k for the Wikipedia based implementation is20.3.2 Term InformativenessWe now consider how to measure the term informa-tiveness in context.
Using the context ?PL/SQL isone of three key programming languages embeddedin the Oracle Database?
(denoted by Cp) as an ex-ample, for its term ?PL/SQL?, the top three contextsreturned by Google are1.
PL/SQL (Procedural Language/Structured Query Language) isOracle Corporation?s procedural extension language for SQL andthe Oracle relational database.2.
PL/SQL is Oracle?s procedural extension to industry-standardSQL.
PL/SQL naturally, efficiently, and safely extends SQL.3.
This Oracle PL SQL tutorial teaches you the basics of program-ming in PL/SQL like cursors, stored procedures, PlSQL func-tions.Those contexts, though being diverse in actualmeaning, all have semantic relatedness to Cp.
Evensomeone who does not completely understand themcan gain some meaning by observing commonwords such as ?Oracle?, ?database?
and ?program-ming?.
However, checking the Google results for?Oracle Database?
or ?programming languages?, wewill find little relatedness between them and Cp.This suggests that if term ta in context c is more in-formative than tb, then most likely the contexts fromta?s featured context set will be more related to cthan will tb.
Thus, given a term t and its featuredcontext set Uf (t) = {c1, ..., ck}, we define the terminformativeness of t in context ci asI(t, ci) =?cj?Uf (t)?
(ci, cj) ?
CA(cj) (7)where ?
(ci, cj) is the semantic relatedness ofci and cj , which can be computed by varioussemantic relatedness metrics such as Wikipediabased (Gabrilovich and Markovitch, 2007; Yazdani261and Popescu-Belis, 2012), Wordnet based (Agirre etal., 2009; Budanitsky and Hirst, 2006), or simple co-sine similarity and Jaccard similarity based (Zobeland Moffat, 1998).The context-aware term informativeness (CTI) in-troduced above is a formal and general definition.As such the definition in Equation (7) includes sev-eral features such as context authority score, fea-tured context set, semantic relatedness, and knowl-edge base, any or all of which could be flexible fordifferent applications.3.3 ImplementationHere, we present a simple practical implementationusing Wikipedia as the knowledge base and the con-text authority estimated by the discounted rank ofthe Wikipedia document.
Note that the problem ishow to compute CA(cj) for each context in Uf .
Werewrite Equation (6) asCA(ci) = DA(di0) +?j ?=0DA(dij) (8)where di0 is the original document of ci and all theothers are further derivatives of ?citing?
ci.
For ex-ample, theWikipedia page of ?PL/SQL?
will be con-sidered as the original document of Cp while allother documents citing Cp are its derivatives.
Intu-itively, the authority of a context will mainly rely onthe authority of its original document.
Here, we sim-ply assume that the context authority depends onlyon its original document, orCA(ci) ?
DA(di0) (9)We then take the top ranked document returned bythe web knowledge base as the original document.We present a practical implementation of CTI inAlgorithm 1.
The discounted rank is used to rep-resent the relative context authority score of eachcontext in Uf .
We use Wikipedia as our knowl-edge base to implement the metric since it is cur-rently one of the largest and most readily availableknowledge repositories and, more importantly, pro-vides free, unlimited and fast query APIs1.
Givenany keyword, the Wikipedia query API will returnthe ranked Wikipedia entries along with the contextscontaining the keyword.
We set the default value 201http://www.mediawiki.org/wiki/API:Queryfor k, or len(Uf ).
Note that there could be othervariations of this implementation.
For example, wecould rule out duplicate or very similar results in theUf .
Search engines such as Google and Bing arealso potential sources since they return high qual-ity web pages along with the contexts containing thequery keyword.In terms of scalability, the proposed method isinherently parallelizable, not only at the documentlevel, but also a the context level, since computingCTI does not depend on any other context in the doc-ument.
In addition, we do not need to issue the samequery more than once.
Our strategy is to locallycache the returned results of every seen query.
For anew term seen in a previous query, we can directlyaccess the local cached file.
If we have built a largelocal pool, the queries will rarely go to a search en-gine or other source.
Given a corpus size N (wordsin total), the number of actual issued queries willbe at most the number of unique terms, which is farless than O(N).
Of course, new terms never seen willhave to be processed, but there will be fewer of theseover time.Algorithm 1: Wikipedia-based I(t, ci)1 Input: t, ci2 Output: I(t, ci)3 begin4 I ??
0;5 Uf ??
queryWikipedia(t);6 for j ?
range(len(Uf )) do7 s??
?
(ci, Uf [j]);8 if j > 0 then9 I ??
I + s/ log(j + 1);10 else I ??
I + s11 return I;4 Applications4.1 Core Terms Extraction from SnippetsWe first investigate CTI in a well defined setting.That is, if we have a collection of terms such thatits most important context is a ?definition,?
e.g.?database?
and ?A database is a structured collec-tion of data, which are typically organized to modelrelevant aspects of reality, in a way that supports262Exemplary snippets of computer science terms Top 5 terms ranked by CTIAcrobat, a document exchange software from Adobe Systems, provides a platform-independent means ofcreating, viewing, and printing documents.
Acrobat can convert a DOS, Windows, UNIX or Macintoshdocuments into a Portable Document Format (PDF) which can be displayed on any computer with anAcrobat reader.
The Acrobat reader can be downloaded free from the Adobe website.Acrobat:3.19Acrobat reader:2.94Portable Document Format:2.08Adobe website:2.03Adobe Systems:1.82Data mining (DM), also known as Knowledge-Discovery in Databases (KDD) or Knowledge-Discoveryand Data Mining (KDD), is the process of automatically searching large volumes of data for patterns.
Datamining uses automated data analysis techniques to uncover previously undetected relationships amongdata items.
Data mining often involves the analysis of data stored in a data warehouse.
Three of the majordata mining techniques are regression, classification and clustering.data mining:3.77data mining techniques:3.64KDD:1.79Knowledge-Discovery:1.66data analysis techniques:1.20Firefox, also known as Mozilla Firefox, is a free, open source, cross-platform, graphical web browserdeveloped by the Mozilla Corporation and hundreds of volunteers.
Firefox includes an integrated pop-upblocker, tabbed browsing, live bookmarks, support for open standards, and an extension mechanism foradding functionality.
Although other browsers have some of these features, Firefox became the first suchbrowser to include them all and achieve wide adoption.Mozilla Firefox:3.89firefox:3.13web browser:2.44browser:2.39graphical web browser:2.35Table 1: Term ranked by CTI from exemplary snippetsprocesses requiring this information?, can CTI iden-tify ?database?
as the most informative term in thiscontext?
To construct the term-context pairs, wecould use the Wikipedia title and the top rankedcontext returned by searching the title using theWikipedia API.
Then we could test our metric basedon other search engines such as Google or Bing.Testing manually, we found the results compare wellto the search engine results, since both Google andBing give top ranks to Wikipedia pages if the querykeyword is a Wikipedia title.
For further analy-sis, we need a collection of term-context pairs fromother sources different from Wikipedia.
Fortunately,we found a list of 1255 computer science termswith its definition snippets manually created by Webusers 2.
The snippets are literally different fromthose contexts in Wikipedia and some of the termsare even not Wikipedia titles, e.g.
bBlog, BetBug,etc.
These can be part of an ?initial?
evaluation.
Thecore term extraction algorithm works in the follow-ing steps for each term-context pair:1.
Extract all n-grams (1 ?
n ?
4) in the contextas candidates2.
For each candidate, calculate its CTI usingWikipedia based implementation3.
Return the top K highest CTI as core termsWe used the top 20 returned Wikipedia contextsas a featured context set Uf and apply the cosinesimilarity for ?.
We show some exemplary snippets2http://www.labautopedia.org/mw/index.php/List ofprogramming and computer science termsK Precision (%) Recall (%) F1(%)1 37.5 37.5 37.52 35.1 55.2 42.93 32.3 64.7 43.14 31.3 72.2 43.75 27.6 76.3 40.510 20.0 88.1 32.6Table 2: Results on computer science term extrac-tion from descriptive snippetswith its top 5 core terms and their CTI scores in Ta-ble 1.
The overall performance is shown in Table 2,in terms of precision, recall and F1 scores based onthe only one titled term of each snippet as the groundtruth.
CTI can correctly find the core term for 37.5%snippets.
If we take the top 5 results, then the recallincrease to 76.3%.Though the algorithm can be easily parallelized,sequentially runtime on all snippets took onlyslightly more than a minute on a 2.35GHz Intel(R)Xeon(R) 4 processors, 23GB of RAM, and Red HatEnterprise Linux Server(5.7) machine.
However, thetime could vary due to network conditions.Though these results look promising, but it couldbe due to the high lexical similarity between thisdataset and Wikipedia content.
To test on a moregeneral corpora, we explore more real world tasks.4.2 Keyword ExtractionThere is a rich literature on keyword extraction prob-lem (Frank et al 1999; Witten et al 1999; Turney,2000; Hulth et al 2003; Tomokiyo and Hurst, 2003;263Wiki20 citeulike180Method P R F P R FTFIDF 13.7 17.8 15.5 14.4 16.0 15.2KEA 18.4 21.5 19.8 20.4 22.3 21.3CTI 19.6 22.7 21.0 18.5 21.4 19.8Table 3: Results on Wiki20 and citeulike180Mihalcea and Tarau, 2004; Medelyan and Witten,2008; Liu, 2010), most of which is treated as a clas-sification or ranking problem with correspondingmachine learning algorithms that use statistical andlinguistic features in a corpus.
Here, we considerthe task as finding the most informative keywords ina document.
Given a document d = {ci}, our key-word extraction algorithm based on CTI works asfollows.1.
For each context ci in a document, compute thesemantic relatedness s(ci, d) between ci and d2.
For each n-gram (1 ?
n ?
4) t in ci, calculateI(t, ci) using Wikipedia based implementation3.
Select the top keywords with the highest?i I(t, ci) ?
s(ci, d)Note that for the last step keywords are selectedbased on a summarized weighted informativenessscore over a document.
Obviously, the pure co-sine or Jaccard similarity is not a good choiceto measure semantic relatedness between two textsegments of very low lexical similarity.
We thususe the Wikipedia based ESA (Gabrilovich andMarkovitch, 2007) to compute the semantic relat-edness s(ci, d) and ?
(ci, cj).
To make the cal-culation more efficient, only the Wikipedia pageswhose title is contained in the dataset are used tobuild the concept space.
We ran the algorithm onseveral datasets including Wiki20 (Medelyan et al2008), citeulike180 (Medelyan et al 2009) and Se-mEval2010 (Kim et al 2010) 3.Though keyword extraction as a research topichas a rich literature, to the best of our knowledgethere is no large scale datasets publicly available.The Wiki20 dataset contains 20 computer sciencearticles each with around 5 terms labeled by 15different teams.
Every term is a Wikipedia title.3http://code.google.com/p/maui-indexer/downloads/listMethod Precision (%) Recall (%) F1(%)TFIDF 14.9 15.3 15.1HUMB 27.2 27.8 27.5CTI 19.3 20.1 19.7CTI+ 25.3 26.2 25.7Table 4: Results on SemEval2010The citeulike180 contains a set of 180 papers eachtagged with around three tags by 332 users.
For eachdataset, the collection of all labeled keywords by dif-ferent taggers are considered as the gold standardfor a document.
We use the set of all keywords forevaluation; otherwise a more complicated evaluationmetrics for each dataset will be needed.
It wouldbe better to investigate other weighting schemes.However, the datasets here are relatively small andthe number of tags on which at least two annota-tors agreed is significantly small; weighting the key-words might not make too much difference.
KEA 4builds a Naive Bayes model using features TFIDF,first occurrence, length of a phrase, and node de-gree (number of candidates that are semantically re-lated to this phrase) (Witten et al 1999).
First oc-currence is computed as the percentage of the doc-ument preceding the first occurrence of the term inthe document.
We compute the node degree as thetextrank (Mihalcea and Tarau, 2004) degree in a doc-ument by simply relating two candidate terms witheach other if they are in the same context.
KEAuses 5 fold cross validation.
All precision P, re-call R and F1 F results are over the top 10 candi-date keywords and the micro-averaged results of thefirst two datasets are shown in Table 3.
The CTI-based algorithm works better than KEA on Wiki20but slightly worse on citeulike180.
We argue thatthe reason might be two-fold.
First, CTI does notuse any inter-document or corpus information whileKEA learns from the corpus.
As such, CTI might notperform as well as supervised learning methods for adomain dependent large corpus.
Second, the labeledkeywords in Wiki20 are all Wikipedia titles whilethose in citeulike are general tags labeled by volun-tary web users.
CTI would give more preference toWikipedia titles since their featured context set re-turned from Wikipedia is more semantically repre-sentative than other non-Wikipedia title words.4http://www.nzdl.org/Kea/264Dataset #Books #Words #Contexts Main domainsGutenberg 55 7,164,463 301,581 History, Art, Psychology, Philosophy, Literature, ZoologyOpen Book 213 22,279,530 1,135,919 Computer Science, Engineering, Information ScienceTable 5: Datasets for book index generation evaluationThe SemEval2010 dataset contains a set of 284scientific papers with 15 keyphrases assigned byreaders and authors.
144 of them are selected astraining set while the other 100 are for testing.
Acomparison of CTI to the results from TFIDF andthe best reported results HUMB (Lopez and Romary,2010) is shown in Table 4.
It achieves 19.8% bymicro-averaged F1 score, ranking 11th out of the 19systems submitted to the competition (Kim et al2010).
However, by adding the structural featuresused by HUMB into CTI, we can improve the per-formance by around 6%, making our results closeto that of HUMB.
The structural information is en-coded as weights for context that is located in ti-tle, abstract, section titles and general content.
Eachweight can be regarded as the prior probability that akeyword will appear in the corresponding location,whose value can be set according to the fraction ofthe number of keyword occurrences of this type oflocation with respect to the number of all keywordoccurrences in the entire training set.
Here they areset to be 0.3, 0.4, 0.25, and 0.05.4.3 Back-of-the-book Index GenerationA back-of-the-book index (or book index) is a col-lection of words or phrases, often alphabetically ar-ranged as an index, created to give readers impor-tant location of important information in a givenbook.
Usually indexing is done by freelancers hiredby authors or publishers, namely professional in-dexers 5.
Csomai and Mihalcea first evaluated theperformance of different informativeness measure-ments for selecting book index terms (2007) andthen investigated automatic book index generationin a supervised learning framework (2008) usingsyntactic features, linguistical features, encyclope-dic features, etc., as a keyword extraction problemrather than building a actual book index.A set of keywords is not a back-of-the-book in-dex.
What really matters for such an index is that5http://www.asindexing.org/an index term or phrase points to its proper loca-tion in the text.
For example, in ?pattern recognitionand machine learning?
by Bishop, ?hidden Markovmodel?
appears in more than 20 pages while theactual index entry has only 2 pages as its locators.Thus the actual problem is to identify a index termwith its context.
As such, learning a robust and ef-ficient model for real book indexes is challenging.First, books from different domains vary in vocabu-lary composition and structure style, requiring vari-ous indexing specialties.
There are different index-ing guides for medicine (Wyman, 1999), psychol-ogy (Hornyak, 2002), and law (Kendrick and Zafran,2001).
Second, book indexing is a highly subjec-tive work and indexes of different books are alwayscreated by different professional indexers who havetheir own preferences and background (Diodato andGandt, 1991; Diodato, 1994).
Third, the trainingset is extremely unbalanced.
As we found in ourdataset, the index size is only 0.42% of the length ofbook on average.
All these motivate us to explore theautomatic creation of index terms that are aware ofthe context at the term?s locations (locators).
To doso we propose the following efficient training-freeand domain independent approach:1.
For each context ci in a book, compute itsweight wi based on structural features2.
For each candidate term t in ci, calculateI(t, ci) using Wikipedia based implementation3.
Select term-context pairs with the highest wi ?I(t, ci) as index entriesThe weight in step 1 represents the relative im-portance of a context in a book.
w(c) = 1 ?cid(c)?cid(titlec)Ntitlecmeasures the weight based on thenormalized distance from the context to its directchapter or sub-chapter title, where cid(c) denotes theid of context c, titlec the title of context c andNtitlecthe number of contexts under titlec.
To select candi-date terms, we first filter the improbable index terms265based on POS patterns using the Standard POS Tag-ger (Toutanova et al 2003).
We then select multi-word keyphrases based on Pointwise Mutual Infor-mation (PMI) (Church and Hanks, 1990), which wasshown to be the best metric to measure word associ-ations (Terra and Clarke, 2003).To evaluate our back-of-the-book index gener-ation method, we conduct extensive experimentson books in various domains, from the Gutenbergdataset and the open book dataset described in Ta-ble 5.
The first one was created by (Csomai andMihalcea, 2006), containing 55 free books collectedfrom Gutenburg6.
Since the dataset does not pro-vide the locators of index terms, we can only servethe evaluation as a keyword extraction task.
The sec-ond dataset was collected from CiteSeer repository,most of which are in computer science and engineer-ing.
We extracted the paged body text and the backindex using Pdfbox7.
Having each index term asso-ciated with its locators (page numbers), we can per-form an evaluation for different methods, not basedsolely on keyword extraction.We first compare CTI with other metrics on bothdatasets for keywords extraction since all other met-rics are context-oblivious.
CTI selects index termsbased on the sum of a term?s CTI scores over all itscontexts, the same as the algorithm used in Section4.2.
The results are shown in Table 6, where the in-dex size = n indicates the number of output terms isn times of the true book index size for each book.The scores are the average recall over a dataset.The CTI outperforms all other 7 metrics in the twodatasets as the output index size increases.
More-over, results show that TF and TFIDF are better thanRIDF in identifying book index terms, which seemscontradictory to previous findings (Church and Gale,1995).
A possible reason is that a book is muchlonger than a regular document thus enhancing TFas a better indicator of keywords but weakening therole of IDF .
We believe this is why Variance, Gain,and Burstiness, which relies on DF , are less effec-tive here.
Wikipedia keyphraseness (Csomai andMi-halcea, 2008) can only find a small fraction of indexterms because it emphasizes Wikipedia titles thathave high in-degree in hyper-link network formed6www.gutenberg.org/7pdfbox.apache.org/1 2 3 4 5510152025303540AverageRecall(%)Index sizeTFIDF+FOTFIDF+CWCI-Indexer(a) TFIDF1 2 3 4 510152025303540AverageRecall(%)Index sizeKEA+FOKEA+CWCI-Indexer(b) KEA1 2 3 4 510152025303540AverageRecall(%)Index sizeSLD+FOSLD+CWCI-Indexer(c) SLD5 10 15 201020304050AverageRecall(%)kIndex size: 1Index size: 2Index size: 3Index size: 4Index size: 5(d) size of featured context setFigure 1: Results for book index generationby Wikipedia terms.
However, a book index coversmuch broader terms not titled in Wikipedia.We then compare with three baselines TFIDF,KEA, and SLD (supervised learning using decisiontree in Csomai?s (2008)) on the second dataset.
ForSLD, we use all the features except the discoursecomprehension based ones which were too com-plicate to implement.
We choose a decision treebecause its training is much faster than the othertwo models while its performance is quite close tothe best.
We follow Csomai?s setting to choose90%(192) books for training and the other 10%(21)for test.
We set two strategies to make the baselinescontext-aware.
First, we select the page of a term?sfirst occurrence as its locator, denoted by ?+FO?
inFigure 1.
Second, we apply the context weight tothem, denoted by ?+CW?.
?CI-Indexer?
denotes ourmethod.
The results are shown in Figure 1a, 1b and1c respectively.
For all the three baselines, addingcontext weight gives better performance than us-ing the simple first occurrence guess, especially forTFIDF.
KEA benefits least from the context weights,suggesting its first occurrence and node degree fea-tures play a similar role as the context weight fea-tures.
SLD outperforms TFIDF and KEA underboth strategies probably because of the new fea-tures of POS pattern and Wikipedia keyphraseness.?SLD+CW?
is the closest to ours.
Finally, we showin Figure 1d that increasing the size of featured con-text set for CTI from 5 to 20 can slightly improve266Dataset Open book dataset Gutenberg datasetIndex size 1 2 3 4 5 1 2 3 4 5Variance 2.4 4.8 7.5 10.4 13.4 1.1 2.9 5.3 8.0 11.0Gain 2.9 6.4 10.2 14.3 18.2 4.9 9.0 14 18.6 23.0Wikipedia keyphraseness 5.3 9.5 13.5 16.4 20.5 9.2 14.1 18.5 21.4 24.3Burstiness 6.0 11.4 16.6 21.4 25.8 10.0 15.8 20.2 23.1 26.2RIDF 8.6 14.5 19.5 23.9 28.0 10.4 15.9 20.1 23.2 26.3TF 9.8 16.9 23.3 29.0 31.7 10.4 17.6 23.5 28.1 30.7TFIDF 10.3 17.3 23.8 29.3 33.6 11.8 19.9 24.7 28.9 32.9CTI 12.4 19.2 25.1 31.5 35.5 14.9 22.3 26.9 29.3 34.5Table 6: Average recall(%) comparisons as the output index size increasesperformance in different index size settings.4.4 DiscussionThe three applications are (incrementally) designedfor different goals.
The first is a toy applica-tion to show the potential capability of this ap-proach, regardless of syntactic or statistical informa-tion.
Clearly, there are simple heuristics that canwork very well for this task, e.g.
the first termof the context.
TF or TFIDF also performs quitewell.
We can rewrite each context (by reordering theterms, changing sentence structures, or substitutingthe core terms with pronouns) to make them inef-fective.
However, this will not effect our method,because what it essentially measures is a term?s in-formativeness among a list of terms appearing in thesame context.
However, for keyword extraction, atopic with a rich literature, to the best of our knowl-edge, has no publicly available large scale datasets,which makes SemEval2010 the best available.
Webelieve our application on back-of-the-book indexgeneration showed how CTI can scale real worldlarge corpora and will scale to millions of bookssince each book can be processed separately.Based on the applications we explored, we cansee that the practical utility of CTI used alone couldbe limited, especially for context-oblivious tasks.It seems reasonable that this method does not out-perform supervised learning methods designed forkeyword extraction.
However, our method showswhat simple but elegant methods can achieve with-out the overhead of machine learning, especially forcontext-aware scenarios such as finding book indexterms.5 Conclusion and Future WorkWe developed a new web knowledge based methodfor encoding informativeness of terms within a unitof discourse.
It is totally feature-free, corpus-free,easy to implement, and inherently parallelizable.Three typical applications on text snippets, scien-tific papers and non-fiction books show its effec-tiveness.
The segmentation of context, the size offeatured context set, the semantic relatedness met-ric ?, and the knowledge base might more or lessaffect the final performance of CTI in terms of ac-curacy or efficiency.
For all applications, we treat aparagraph as an individual context, which is not nec-essary a complete discourse unit.
However, it maynot be fair to set the same number for all contextterms.
In addition, selection of semantic relatednessand knowledge bases need further investigation.
TheWikipedia-based implementation might be a goodchoice for the definitional snippets, scientific arti-cles and text books since they are all ?educational?resources sharing a similar concept space.
However,it is an open question as whether it works for corporasuch as tweets, online reviews, and forum posts.Based on the proposed methods and encouragingresults, it would be interesting to build an online in-dexing tool which automatically finds informativeterms in generic text and generates a back-of-the-book index for a sets of papers, books, theses andother collections.6 AcknowledgmentsWe gratefully acknowledge partial support from theNational Science Foundation and useful commentsfrom referees.267ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pasca,and A. Soroa.
2009.
A study on similarity andrelatedness using distributional and WordNet-basedapproaches.
In Proceedings of NAACL-HLT 2009,pp.
19?27.A.
Bookstein and Don R. Swanson.
1974.
Probabilisticmodels for automatic indexing.
Journal of the Ameri-can Society for Information Science, 25(5):312?316.A.
Budanitsky and G. Hirst.
2012.
Evaluating WordNet-based measures of semantic distance.
ComputationalLinguistics 32:13?47.K.
W. Church and W. A. Gale.
1995.
Inverse documentfrequency(IDF): A measure of deviation from poisson.In Proceedings of the Third Workshop on Very LargeCorpora 1995, pp.
121?130.K.
W. Church and P. Hanks.
1990.
Word associationnorms, mutual information and lexicography.
Compu-tational Linguistics, 16:22?29.Rudi L. Cilibrasi and Paul M.B.
Vitanyi.
2007.
TheGoogle similarity distance.
IEEE Transaction onKnowledge and Data Engineering, 19(3):370?383.A.
Csomai and R. Mihalcea.
2006.
Creating a testbed forthe evaluation of automatically generated back-of-the-book indexes.
In Proceedings of the 7th internationalconference on Computational Linguistics and Intelli-gent Text Processing 2006, pp.
429?440.A.
Csomai and R. Mihalcea.
2007.
Investigations inunsupervised back-of-the-book indexing.
In Proceed-ings of the Florida Artificial Intelligence Research So-ciety 2007, pp.
211?216.A.
Csomai and R. Mihalcea.
2008.
Linguistically Moti-vated Features for Enhanced Back-of-the-Book Index-ing.
In Proceedings of ACL 2008, pp.
932?940.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing by LatentSemantic Analysis.
Journal of the American Societyfor Information Science 41:391-407.V.
Diodato and G. Gandt.
1991.
Back of book indexesand the characteristics of author and nonauthor index-ing: Report of an exploratory study.
Journal of theAmerican Society for Information Science, 42:341?350.V.
Diodato.
1994.
User preferences for features in backof book indexes.
Journal of the American Society forInformation Science, 45:529?536.E.
Frank, G. W. Paynter, I. H. Witten, and C. Gutwin.1999 Domainspecic keyphrase extraction.
In Pro-ceedings IJCAI 1999, pp.
668-673.E.
Gabrilovich and S. Markovitch.
2007.
Computing se-mantic relatedness using Wikipedia-based explicit se-mantic analysis.
In Proceedings of IJCAI 2007, pp.
6?12.Q.
He, J. Pei, D. Kifer, P. Mitra, and C. L. Giles.
2010.Context-aware citation recommendation.
In Proceed-ings of WWW 2010, pp.
421?430.B.
Hornyak.
2002.
Indexing Specialties: Psychology.Medford, NJ : Information Today, Inc.A.
Hulth.
2003.
Improved automatic keyword extractiongiven more linguistic knowledge.
In Proceedings ofEMNLP 2003, pp.
216-223.Karen Sprck Jones.
1972.
A statistical interpretation ofterm specificity and its application in retrieval.
Jour-nal of Documentation, 28(1):11?21.Karen Sprck Jones.
1973.
Index term weighting.
Infor-mation Storage and Retrieval, 9(11):619?633.P.
Kendrick and E. L. Zafran.
2001.
Indexing Specialties:Law.
Medford, NJ : Information Today, Inc.Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Tim-othy Baldwin.
2010.
Semeval-2010 task 5 : Auto-matic keyphrase extraction from scientic articles.
InProceedings of the 5th SIGLEX Workshop on Seman-tic Evaluation.
2010, pp.
21?26.K.
Kireyev.
2009.
Semantic-based Estimation ofTerm Informativeness.
In Proceedings of NAACL-HLT2009, pp.
530?538.Z.
Liu, W. Huang, Y. Zheng, and M. Sun.
2010.
Au-tomatic keyphrase extraction via topic decomposition.In Proceedings of EMNLP 2010, pp.
366?376.P.
Lopez and L. Romary.
2010.
HUMB: Automatic KeyTerm Extraction from Scientic Articles in GROBID.In Proceedings of the 5th SIGLEX Workshop on Se-mantic Evaluation.
2010, pp.
248-251.O.
Medelyan and I. H. Witten.
2008.
Domain-independent automatic keyphrase indexing with smalltraining sets.
J.
Am.
Soc.
Inf.
Sci.
Technol.
59:1026-1040.O.
Medelyan, I. H. Witten, and D. Milne.
2008.
Topicindexing with Wikipedia.
In Proceedings of AAAI08Workshop on Wikipedia and Artificial Intelligence: anEvolving Synergy 2008, pp.
19?24.O.
Medelyan, E. Frank, and I. H. Witten.
2009.
Human-competitive tagging using automatic keyphrase extrac-tion.
In Proceedings of EMNLP 2009, pp.
1318?1327.R.
Mihalcea and P. Tarau.
2004.
TextRank: BringingOrder into Texts.
In Proceedings of EMNLP 2004,pp.
404-411.L.
Page, S. Brin, R. Motwani, and T. Winograd.
1998.The PageRank Citation Ranking: Bringing Order tothe Web.
Technical Report.
Stanford InfoLab.K.
Papineni.
2001.
Why inverse document frequency?In Proceedings of NAACL-HLT 2001, pp.
1?8.J.
D. M. Rennie, and T. Jaakkola.
2005.
Using TermInformativeness for Named Entity Detection.
In Pro-ceedings of SIGIR 2005, pp.
353?360.268G.
Salton, and C. Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
Information Pro-cessing & Management 24(5):513?523.R.
Soricut and D. Marcu.
2003.
Sentence level dis-course parsing using syntactic and lexical information.In Proceedings of NAACL-HLT 2003, pp.
149?156.E.
Terra and C. L. Clarke.
2003.
Frequency estimates forstatistical word similarity measures.
In Proceedings ofNAACL-HLT 2003, pp.
165?172.T.
Tomokiyo and M. Hurst.
2003.
A language model ap-proach to keyphrase extraction.
In Proceedings of theACL 2003 workshop on Multiword expressions: anal-ysis, acquisition and treatment, 2003, pp.
3340.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-Rich Part-of-Speech Tagging with aCyclic Dependency Network.
In Proceedings ofNAACL-HLT 2003, pp.
252-259.P.
D. Turney.
2000.
Learning Algorithms for KeyphraeExtraction.
Information Retrieval 2:303-336.I.
H. Witten, G. W. Paynter, E. Frank, C. Gutwin, andC.
G. Nevill-Manning.
1999.
Kea: practical automatickeyphrase extraction.
In Proceedings of the fourthACM conference on Digital libraries, 1999, pp.
254?255.L.
P. Wyman.
1999.
Indexing Specialities: Medicine.Medford, NJ : Information Today, Inc.M.
Yazdani and A. Popescu-Belis.
2012.
Computingtext semantic relatedness using the contents and linksof a hypertext encyclopedia.
Artificial Intelligence194:176?202.J.
Zobel and A. Moffat.
1998.
Exploring the similarityspace.
ACM SIGIR Forum 32(1):18?34.Le Zhao and Jamie Callan.
2010.
Term necessity predic-tion.
In Proceedings of the 19th ACM internationalconference on Information and knowledge manage-ment, 2010, pp.
259?268.Hui Yang and Jamie Callan.
2009.
A metric-basedframework for automatic taxonomy induction.
In Pro-ceedings of the ACL, 2009, pp.
271?279 .269
