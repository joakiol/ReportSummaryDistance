A Decision-Based Approach to Rhetorical ParsingDaniel MarcuInformation Sciences Institute and Department of Computer  ScienceUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292-6601marcu @ isi.
eduAbstractWe present a shift-reduce rhetorical parsing algo-rithm that learns to construct rhetorical structuresof texts from a corpus of discourse-parse action se-quences.
The algorithm exploits robust lexical, syn-tactic, and semantic knowledge sources.I IntroductionThe application of decision-based learning tech-niques over rich sets of linguistic features hasimproved significantly the coverage and perfor-mance of syntactic (and to various degrees eman-tic) parsers (Simmons and Yu, 1992; Magerman,1995; Hermjakob and Mooney, 1997).
In this pa-per, we apply a similar paradigm to developing arhetorical parser that derives the discourse structureof unrestricted texts.Crucial to our approach is the reliance on a cor-pus of 90 texts which were manually annotated withdiscourse trees and the adoption of a shift-reduceparsing model that is well-suited for learning.
Boththe corpus and the parsing model are used to gener-ate learning cases of how texts should be partitionedinto elementary discourse units and how discourseunits and segments hould be assembled into dis-course trees.2 The CorpusWe used a corpus of 90 rhetorical structure trees,which were built manually using rhetorical rela-tions that were defined informally in the style ofMann and Thompson (1988): 30 trees were builtfor short personal news stories from the MUC7 co-reference corpus (Hirschman and Chinchor, 1997);30 trees for scientific texts from the Brown corpus;and 30 trees for editorials from the Wall Street Jour-nal (WSJ).
The average number of words for eachtext was 405 in the MUC corpus, 2029 in the Browncorpus, and 878 in the WSJ corpus.
Each MUC text365was tagged by three annotators; each Brown andWSJ text was tagged by two annotators.The rhetorical structure assigned to each text is a(possibly non-binary) tree whose leaves correspondto elementary discourse units (edu)s, and whose in-ternal nodes correspond to contiguous text spans.Each internal node is characterized by a rhetori-cal relation, such as ELABORATION and CONTRAST.Each relation holds between two non-overlappingtext spans called NUCLEUS and SATELLITE.
(Thereare a few exceptions to this rule: some relations,such as SEQUENCE and CONTRAST, are multinu-clear.)
The distinction between uclei and satellitescomes from the empirical observation that the nu-cleus expresses what is more essential to the writer'spurpose than the satellite.
Each node in the tree isalso characterized by a promotion set that denotesthe units that are important in the correspondingsubtree.
The promotion sets of leaf nodes are theleaves themselves.
The promotion sets of internalnodes are given by the union of the promotion setsof the immediate nuclei nodes.Edus are defined functionally as clauses orclause-like units that are unequivocally the NU-CLEUS or SATELLITE of a rhetorical relation thatholds between two adjacent spans of text.
For ex-ample, "because of the low atmospheric pressure"in text (1) is not a fully fleshed clause.
However,since it is the SATELLITE of an EXPLANATION rela-tion, we treat it as elementary.\[Only the midday sun at tropical latitudes is warmenough\] \[to thaw ice on occasion,\] \[but any liquid wa-ter formed in this way would evaporate almost instantly\]\[because of the low atmospheric pressure.\](1)Some edus may contain parenthetical units, i.e.,embedded units whose deletion does not affect theunderstanding of the edu to which they belong.
Forexample, the unit shown in italics in (2) is paren-thetic.This book, which I have received from John, is the best (2)book that I have read in a while.The annotation process was carried out using arhetorical tagging tool.
The process consisted in as-signing edu and parenthetical unit boundaries, inas-sembling edus and spans into discourse trees, and inlabeling the relations between edus and spans withrhetorical relation ames from a taxonomy of 71 re-lations.
No explicit distinction was made betweenintentional, informational, and textual relations.
Inaddition, we also marked two constituency relationsthat were ubiquitous in our corpora and that oftensubsumed complex rhetorical constituents.
Theserelations were ATTRIBUTION, which was used to la-bel the relation between a reporting and a reportedclause, and APPOSITION.
Marcu et al (1999) discussin detail the annotation tool and protocol and assessthe inter-judge agreement and the reliability of theannotation.3 The parsing modelWe model the discourse parsing process as a se-quence of shift-reduce operations.
As front-end, theparser uses a discourse segmenter, i.e., an algorithmthat partitions the input text into edus.
The dis-course segmenter, which is also decision-based, ispresented and evaluated in section 4.The input to the parser is an empty stack and aninput list that contains asequence of elementary dis-course trees, edts, one edt for each edu produced bythe discourse segmenter.
The status and rhetoricalrelation associated with each edt is UNDEFINED, andthe promotion set is given by the corresponding edu.At each step, the parser applies a SHIFT or a REDUCEoperation.
Shift operations transfer the first edt ofthe input list to the top of the stack.
Reduce opera-tions pop the two discourse trees located on the topof the stack; combine them into a new tree updatingthe statuses, rhetorical relation names, and promo-tion sets associated with the trees involved in theoperation; and push the new tree on the top of thestack.Assume, for example, that the discourse seg-menter partitions a text given as input as shownin (3).
(Only the edus numbered from 12 to 19 areshown.)
Figure 1 shows the actions taken by a shift-reduce discourse parser starting with step i.
At stepi, the stack contains 4 partial discourse trees, whichspan units \[1,11\], \[12,15\], [16,17\], and \[18\], and the366input list contains the edts that correspond to unitswhose numbers are higher than or equal to 19.... \[Close parallels between tests and practice tests (3)are common, 12\] \[some educators and researcherssay.
13\] \[Test-preparation booklets, software and work-sheets are a booming publishing subindustryJ 4 \] \[Butsome practice products are so similar to the tests them-selves that critics say they represent a form of school-sponsored cheatingJ 5 \]\["If I took these preparation booklets into myclassroom, 16 \] \[I'd have a hard time justifying to my stu-dents and parents that it wasn't cheating, "17 \] \[says JohnKaminsky, TM\] \[a Traverse City, Mich., teacher who hasstudied test coaching.
19 \] ...At step i the parser decides to perform a SHIFT op-eration.
As a result, the edt corresponding to unit19 becomes the top of the stack.
At step i + 1, theparser performs a REDUCE-APPOSITION-NS opera-tion, that combines edts 18 and 19 into a discoursetree whose nucleus is unit 18 and whose satelliteis unit 19.
The rhetorical relation that holds be-tween units 18 and 19 is APPOSITION.
At step i+2,the trees that span over units \[16,17\] and \[18,19\]are combined into a larger tree, using a REDUCE-ATTRIBUTION-NS operation.
As a result, the statusof the tree \[16,17\] becomes NUCLEUS and the statusof the tree \[18,19\] becomes SATELLITE.
The rhetor-ical relation between the two trees is ATTRIBUTION.At step i + 3, the trees at the top of the stack arecombined using a REDUCE-ELABORATION-NS oper-ation.
The effect of the operation is shown at thebottom of figure 1.In order to enable a shift-reduce discourse parserderive any discourse tree, it is sufficient o imple-ment one SHIFT operation and six types of REDUCEoperations, whose operational semantics i  shownin figure 2.
For each possible pair of nuclearityassignments NUCLEUS-SATELLITE (NS), SATELLITE-NUCLEUS (SN), and NUCLEUS-NUCLEUS (NN) thereare two possible ways to attach the tree located atposition top in the stack to the tree located at po-sition top - 1.
If one wants to create a binary treewhose immediate children are the trees at top andtop - 1, an operation of type REDUCE-NS, REDUCE-SN, or REDUCE-NN needs to be employed.
If onewants to attach the tree at top as an extra-childof the tree at top - 1, thus creating or modifyinga non-binary tree, an operation of type REDUCE-BELOW-NS, REDUCE-BELOW-SN, or  REDUCE-BELOW-NN needs to be employed.
Figure 2 illustrates howthe statuses and promotion sets associated with thes ~ l. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I t~UCg-gLAt~A~ON~NS mW~ATIONFigure 1: Example of a sequence of shift-reduce operations that concern the discourse parsing of text (3).trees involved in the reduce operations are affectedin each case.Since the labeled data that we relied uponwas sparse, we grouped the relations that sharedsome rhetorical meaning into clusters of rhetor-ical similarity.
For example, the cluster namedCONTRAST contained the contrast-like rhetoricalrelations of ANTITHESIS, CONTRAST, and CON-CESSION.
The cluster named EVALUATION-INTERPRETATION contained the rhetorical relationsof EVALUATION and INTERPRETATION.
And thecluster named OTHER contained rhetorical rela-tions such as QUESTION-ANSWER, PROPORTION, RE-STATEMENT, and COMPARISON, which were used367Figure 2: The reduce operations upported by ourparsing model.very seldom in the corpus.
The grouping pro-cess yielded 17 clusters, each characterized bya generalized rhetorical relation name.
Thesenames were: APPOSITION-PARENTHETICAL, ATTRI-BUTION, CONTRAST, BACKGROUND-CIRCUMSTANCE,CAUSE-REASON-EXPLANATION, CONDITION, ELABO-RATION, EVALUATION-INTERPRETATION, EVIDENCE,EXAMPLE, MANNER-MEANS, ALTERNATIVE, PUR-POSE, TEMPORAL, LIST, TEXTUAL, and OTHER.In the work described in this paper, we attemptedto automatically derive rhetorical structures treesthat were labeled with relations names that corre-sponded to the 17 clusters of rhetorical similarity.Since there are 6 types of reduce operations andsince each discourse tree in our study uses relation368names that correspond to the 17 clusters of rhetor-ical similarity, it follows that our discourse parserneeds to learn what operation to choose from a setof 6 ?
17 + 1 = 103 operations (the 1 correspondsto the SHXFT operation).4 The discourse segmenter4.1 Generation of learning examplesThe discourse segmenter we implemented processesan input text one lexeme (word or punctuationmark) at a time and recognizes entence and eduboundaries and beginnings and ends of parentheti-cal units.
We used the leaves of the discourse treesthat were built manually in order to derive the learn-ing cases.
To each lexeme in a text, we associatedone learning case, using the features described insection 4.2.
The classes to be learned, which are as-sociated with each lexeme, are sentence-break, edu-break, start-paTen, end-paTen, and none.4.2 Features used for learningTo partition a text into edus and to detect parentheti-cal unit boundaries, we relied on features that modelboth the local and global contexts.The local context consists of a window of size5 that enumerates the Part-Of-Speech (POS) tagsof the lexeme under scrutiny and the two lexemesfound immediately before and after it.
The POStags are determined automatically, using the Brilltagger (1995).
Since discourse markers, such asbecause and and, have been shown to play a ma-jor role in rhetorical parsing (Marcu, 1997), we alsoconsider a list of features that specify whether a lex-eme found within the local contextual window is apotential discourse marker.
The local context alsocontains features that estimate whether the lexemeswithin the window are potential abbreviations.The global context reflects features that pertain tothe boundary identification process.
These featuresspecify whether a discourse marker that introducesexpectations (Cristea and Webber, 1997) (such asalthough) was used in the sentence under consider-ation, whether there are any commas or dashes be-fore the estimated end of the sentence, and whetherthere are any verbs in the unit under consideration.A binary representation f the features that char-acterize both the local and global contexts yieldslearning examples with 2417 features/example.4.3 EvaluationWe used the C4.5 program (Quinlan, 1993) in orderto learn decision trees and rules that classify leT-Corpus # cases BI(%) B2(%) Acc(%)MUC 14362 91.28 93.1 96.244-0.06WSJ 31309 92.39 94.6 97.144-0.10Brown 72092 93.84 96.8 97.874-0.04Table 1: Performance of a discourse segmenter thatuses a decision-tree, non-binary classifier.AceAction (a) (b) (c) (d) (e)sentence-break (a) 272 4edu-break (b) 133 3 84start-parcH (c) 4 26end-paten (d) 20 6none (e) 2 38 1 4 7555Table 2: Confusion matrix for the decision-tree,non-binary classifier (the Brown corpus)./ i/2.00 4.00J/?
cases x 1o 36.00 8.00 I0.00 12.00edu boundaries.
The performance is high with re-spect to recognizing sentence boundaries and endsof parenthetical units.
The performance with re-spect to identifying sentence boundaries appearsto be close to that of systems aimed at identify-ing only sentence boundaries (Palmer and Hearst,1997), whose accuracy is in the range of 99%.Figure 3: Learning curve for discourse segmenter(the MUC corpus).emes as boundaries of sentences, edus, or parenthet-ical units, or as non-boundaries.
We learned bothfrom binary (when we could) and non-binary repre-sentations of the cases.
1 In general the binary rep-resentations yielded slightly better esults than thenon-binary representations and the tree classifierswere slightly better than the rule-based ones.
Dueto space constraints, we show here (in table 1) onlyaccuracy results that concern non-binary, decision-tree classifiers.
The accuracy figures were com-puted using a ten-fold cross-validation procedure.In table 1, B1 corresponds toa majority-based base-line classifier that assigns none to all lexemes, andB2 to a baseline classifier that assigns a sentenceboundary to every DOT lexeme and a non-boundaryto all other lexemes.Figure 3 shows the learning curve that corre-sponds to the MUC corpus.
It suggests that moredata can increase the accuracy of the classifier.The confusion matrix shown in table 2 corre-sponds to a non-binary-based tree classifier thatwas trained on cases derived from 27 Brown textsand that was tested on cases derived from 3 dif-ferent Brown texts, which were selected randomly.The matrix shows that the segmenter has problemsmostly with identifying the beginning of parentheti-cal units and the intra-sentential edu boundaries; forexample, it correctly identifies only 133 of the 220ZLeaming from binary representations of features in theBrown corpus was too computationally expensive to terminate- -  the Brown data file had about 0.5GBytes.5 The shift-reduce action identifier ,5.1 Generation of learning examplesThe learning cases were generated automatically,in the style of Magerman (1995), by traversing in-order the final rhetorical structures built by anno-tators and by generating a sequence of discourseparse actions that used only SHIFT and REDUCE op-erations of the kinds discussed in section 3.
Whena derived sequence is applied as described in theparsing model, it produces a rhetorical tree that isa one-to-one copy of the original tree that was usedto generate the sequence.
For example, the tree atthe bottom of figure 1 - -  the tree found at the topof the stack at step i + 4 - -  can be built if the fol-lowing sequence of operations i performed: {SHIFT12; SHIFT 13; REDUCE-ATTRIBUTION-NS; SHIFT 14;REDUCE-JOINT-NN; SHIFT 15; REDUCE-CONTRAST-SN, SHIFT 16, SHIFT \ ]7 ;  REDUCE-CONDITION-SN; SHIFT 18; SHIFT 19; REDUCE-APPOSITION-NS;REDUCE-ATTRIBUTION-NS; REDUCE-ELABORATION-NS.
}5.2 Features used for learningTo make decisions with respect o parsing actions,the shift-reduce action identifier focuses on the threetop most trees in the stack and the first edt in the in-put list.
We refer to these trees as the trees in focus.The identifier elies on the following classes of fea-tures.Structural features.?
Features that reflect the number of trees in thestack and the number of edts in the input list.?
Features that describe the structure of the trees infocus in terms of the type of textual units that theysubsume (sentences, paragraphs, titles); the number369of immediate children of the root nodes; the rhetor-ical relations that link the immediate children of theroot nodes, etc.
2Lexical (cue-phrase-like) and syntactic features.?
Features that denote the actual words and POStags of the first and last two lexemes of the textspans subsumed by the trees in focus.?
Features that denote whether the first and lastunits of the trees in focus contain potential discoursemarkers and the position of these markers in thecorresponding textual units (beginning, middle, orend).Operational features.?
Features that specify what the last five parsing op-erations performed by the parser were.
3Semantic-similarity-based f atures.?
Features that denote the semantic similarity be-tween the textual segments ubsumed by the treesin focus.
This similarity is computed by applying inthe style of Hearst (1997) a cosine-based metric onthe morphed segments.?
Features that denote Wordnet-based measures ofsimilarity between the bags of words in the promo-tion sets of the trees in focus.
We use 14 Wordnet-based measures of similarity, one for each Word-net relation (Fellbaum, 1998).
Each of these sim-ilarities is computed using a metric similar to thecosine-based metric.
Wordnet-based similarities re-flect the degree of synonymy, antonymy, meronymy,hyponymy, etc.
between the textual segments ub-sumed by the trees in focus.
We also use 14 x 13/2relative Wordnet-based measures of similarity, onefor each possible pair of Wordnet-based relations.For each pair of Wordnet-based measures of simi-larity w~l and wr2, each relative measure (feature)takes the value <, =, or >, depending on whetherthe Wordnet-based similarity w~l between the bagsof words in the promotion sets of the trees in focus islower, equal, or higher that the Wordnet-based sim-ilarity w~2 between the same bags of words.
For ex-ample, if both the synonymy- and meronymy-basedmeasures of similarity are 0, the relative similaritybetween the synonymy and meronymy of the treesin focus will have the value =.2The identifier assumes that each sentence break that endsin a period and is followed by two '\n' characters, for example,is a paragraph break; and that a sentence break that does not endin a punctuation mark and is followed by two '\n' characters isa title.3We could generate hese features because, for learning, weused sequences of shift-reduce operations and not discoursetrees.Corpus # cases B3(%) B4(%) Ace(%)MUC 1996 50.75 26.9 61.124-1.61WSJ 4360 50.34 27.3 61.654-0.41Brown 8242 50.18 28.1 61.814-0.48Table 3: Performance of the tree-based, shift-reduceaction classifiers.Ace60.0058.01356.0054.0~52.0G~0.0~ t ,46.00 /0.5tlS1.00 1.50,,1 c,~es x l0 3Figure 4: Learning curve for the shift-reduce actionidentifier (the MUC corpus).A binary representation f these features yieldslearning examples with 2789 features/example.5.3 EvaluationThe shift-reduce action identifier uses the C4.5 pro-gram in order to learn decision trees and rules thatspecify how discourse segments hould be assem-bled into trees.
In general, the tree-based classifiersperformed slightly better than the rule-based classi-tiers.
Due to space constraints, we present here onlyperformance r sults that concern the tree classifiers.Table 3 displays the accuracy of the shift-reduce ac-tion identifiers, determined for each of the three cor-pora by means of a ten-fold cross-validation proce-dure.
In table 3, the B3 column gives the accuracyof a majority-based classifier, which chooses actionSHIFT in all cases.
Since choosing only the actionSHIFT never produces a discourse tree, in columnB4, we present he accuracy of a baseline classifierthat chooses hift-reduce operations randomly, withprobabilities that reflect he probability distributionof the operations in each corpus.Figure 4 shows the learning curve that corre-sponds to the MUC corpus.
As in the case of thediscourse segmenter, this learning curve also sug-gests that more data can increase the accuracy ofthe shift-reduce action identifier.6 Evaluation of the rhetorical parserObviously, by applying the two classifiers equen-tiaUy, one can derive the rhetorical structure of any370CorpusMUCWSJBrownSeg- Train- Elementary units Hierarchical spans Span nuclearityment- ing Judges \ [  Parser Judges \ [  Parser Judges I Parser Judgeser  corpus R I P R I P R I P R I P R I P R I P R I PDT MUC 88.0 88.0 37.1 100.0 84.4 84.4 38.2 61.0 79.1 83.5 25.5 51.5 78.6 78.6DT All 75.4 96.9 70.9 72.8 58.3 68.9M MUC 100.0 100.0 87.5 82.3 68.8 78.2M All 100.0 100.0 84.8 73.5 71.0 69.3DT WSJ 85.1 86.8 18.1 95.8 79.9 80.1 34.0 65.8 67.6 77.1 21.6 54.0 73.1 73.3DT All 25.1 79.6 40.1 66.3 30.3 58.5M WSJ I00.0 100.0 83.4 84.2 63.7 79.9M All 100.0 100.0 83.0 85.0 69.0 82.4DT Brown 89.5 88.5 60.5 79.4 80.6 79.5 57.3 63.3 67.6 75.8 44.6 57.3 69.7 68.3DT All 44.2 80.3 44.7 59.1 33.2 51.8M Brown 100.0 100.0 81.1 73.4 60.1 67.0M All 100.0 100.0 80.8 77.5 60.0 72.0Rhetorical relationsParserR \] P14.9 28.738.4 45.372.4 62.866.5 53.913.0 34.317.3 36.056.3 57.959.8 63.226.7 35.315.7 25.759.5 45.551.8 44.7Table 4: Performance of the rhetorical parser: labeled (R)ecall and (P)recision.
The segmenter is eitherDecision-Tree-Based (DT) or Manual (M).text.
Unfortunately, the performance results pre-sented in sections 4 and 5 only suggest how wellthe discourse segmenter and the shift-reduce actionidentifier perform with respect o individual cases.They say nothing about he performance of a rhetor-ical parser that relies on these classifiers.In order to evaluate the rhetorical parser as awhole, we partitioned randomly each corpus intotwo sets of texts: 27 texts were used for training andthe last 3 texts were used for testing.
The evalua-tion employs labeled recall and precision measures,which are extensively used to study the performanceof syntactic parsers.
Labeled recall reflects the num-ber of correctly labeled constituents identified bythe rhetorical parser with respect o the number oflabeled constituents in the corresponding manuallybuilt tree.
Labeled precision reflects the numberof correctly labeled constituents identified by therhetorical parser with respect to the total number oflabeled constituents identified by the parser.We computed labeled recall and precision figureswith respect o the ability of our discourse parserto identify elementary units, hierarchical text spans,text span nuclei and satellites, and rhetorical rela-tions.
Table 4 displays results obtained using seg-menters and shift-reduce action identifiers that weretrained either on 27 texts from each corpus andtested on 3 unseen texts from the same corpus; orthat were trained on 27?3 texts from all corporaand tested on 3 unseen texts from each corpus.
Thetraining and test texts were chosen randomly.
Ta-ble 4 also displays results obtained using a man-ual discourse segmenter, which identified correctlyall edus.
Since all texts in our corpora were man-ually annotated by multiple judges, we could also371compute an upper-bound of the performance of therhetorical parser by calculating for each text in thetest corpus and each judge the average labeled recalland precision figures with respect o the discoursetrees built by the other judges.
Table 4 displaysthese upper-bound figures as well.The results in table 4 primarily show that errors inthe discourse segmentation stage affect significantlythe quality of the trees our parser builds.
Whena segmenter is trained only on 27 texts (especiallyfor the MUC and WSJ corpora, which have shortertexts than the Brown corpus), it has very low per-formance.
Many of the intra-sentential edu bound-aries are not identified, and as a consequence, theoverall performance of the parser is low.
Whenthe segmenter is trained on 27 ?
3 texts, its perfor-mance increases ignificantly with respect o theMUC and WSJ corpora, but decreases with respectto the Brown corpus.
This can be explained by thesignificant differences in style and discourse markerusage between the three corpora.
When a perfectsegmenter is used, the rhetorical parser determineshierarchical constituents and assigns them a nucle-arity status at levels of performance that are not farfrom those of humans.
However, the rhetorical la-beling of discourse spans is even in this case about15-20% below human performance.These results uggest that the features that we useare sufficient for determining the hierarchical struc-ture of texts and the nuclearity statuses of discoursesegments.
However, they are insufficient for deter-mining correctly the elementary units of discourseand the rhetorical relations that hold between dis-course segments.7 Related workThe rhetorical parser presented here is the first thatemploys learning methods and a thorough evalua-tion methodology.
All previous parsers aimed atdetermining the rhetorical structure of unrestrictedtexts (Sumita et al, 1992; Kurohashi and Nagao,1994; Marcu, 1997; Corston-Oliver, 1998)em-ployed manually written rules.
Because of the lackof discourse corpora, these parsers did not evaluatethe correctness of the discourse trees they built perse, but rather their adequacy for specific purposes:experiments carded out by Miike et al (1994) andMarcu (1999) showed only that the discourse struc-tures built by rhetorical parsers (Sumita et al, 1992;Marcu, 1997) can be used successfully in order toimprove retrieval performance and summarize t xt.8 ConclusionIn this paper, we presented a shift-reduce rhetori-cal parsing algorithm that learns to construct rhetor-ical structures of texts from tagged ata.
The parserhas two components: a discourse segmenter, whichidentifies the elementary discourse units in a text;and a shift-reduce action identifier, which deter-mines how these units should be assembled intorhetorical structure trees.Our results suggest that a high-performance dis-course segmenter would need to rely on more train-ing data and more elaborate features than the onesdescribed in this paper - -  the learning curves didnot converge to performance limits.
If one's goal is,however, to construct discourse trees whose leavesare sentences (or units that can be identified athigh levels of performance), then the segmenter de-scribed here appears to be adequate.
Our resultsalso suggest that the rich set of features that consti-tute the foundation of the action identifier are suffi-cient for constructing discourse hierarchies and forassigning to discourse segments a rhetorical statusof nucleus or satellite at levels of performance thatare close to those of humans.
However, more re-search is needed in order to approach uman perfor-mance in the task of assigning to segments correctrhetorical relation labels.Acknowledgements.
I am grateful to Ulf Herm-jakob, Kevin Knight, and Eric Breck for commentson previous drafts of this paper.ReferencesEric Brill.
1995.
Transformation-based error-drivenlearning and natural anguage processing: A case372study in part-of-speech tagging.
Computational Lin-guistics, 21 (4):543-565.Simon H. Corston-Oliver.
1998.
Beyond string match-ing and cue phrases: Improving efficiency and cover-age in discourse analysis.
The AAAI Spring Sympo-sium on Intelligent Text Summarization, pages 9-15.Dan Cristea and Bonnie L. Webber.
1997.
Expectationsin incremental discourse processing.
In Proceedingsof ACL/EACL'97, pages 88-95.Christiane Fellbaum, editor.
1998.
Wordnet: An Elec-tronic Lexical Database.
The MIT Press.Marti A. Hearst.
1997.
TextTiling: Segmenting textinto multi-paragraph subtopic passages.
Computa-tional Linguistics, 23(1):33--64.Ulf Hermjakob and Raymond J. Mooney.
1997.
Learn-ing parse and translation decisions from exampleswith rich context.
In Proceedings of ACI_,/EACL'97,pages 482-489.Lynette Hirschman and Nancy Chinchor, 1997.
MUC-7Coreference Task Definition.Sadao Kurohashi and Makoto Nagao.
1994.
Automaticdetection of discourse structure by checking surfaceinformation i  sentences.
In Proceedings of COL-ING'94, volume 2, pages 1123-1127.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of ACL'95, pages276-283.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243-281.Daniel Marcu.
1997.
The rhetorical parsing of natu-ral language texts.
In Proceedings of ACL/EACL'97,pages 96-103.Daniel Marcu.
1999.
Discourse trees are good indica-tors of importance intext.
In Inderjeet Mani and MarkMaybury, editors, Advances in Automatic Text Sum-marization.
The MIT Press.
To appear.Daniel Marcu, Estibaliz Amorrortu, and MagdalenaRomera.
1999.
Experiments in constructing a corpusof discourse trees.
The ACL'99 Workshop on Stan-dards and Tools for Discourse Tagging.Seiji Miike, Etsuo Itoh, Kenji Ono, and Kazuo Sumita.1994.
A full-text retrieval system with a dynamicabstract generation function.
In Proceedings of SI-GIR'94, pages 152-161.David D. Palmer and Marti A. Hearst.
1997.
Adap-tive multilingual sentence boundary disambiguation.Computational Linguistics, 23(2):241-269.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann Publishers.R.F.
Simmons and Yeong-Ho Yu.
1992.
The acquisitionand use of context-depefident grammars for English.Computational Linguistics, 18(4):391-418.K.
Sumita, K. Ono, T. Chino, T. Ukita, and S. Amano.1992.
A discourse structure analyzer for Japanesetext.
In Proceedings of the International Conferenceon Fifth Generation Computer Systems, volume 2,pages 1133-1140.
