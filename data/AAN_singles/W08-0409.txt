Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69?77,ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsImproving Word Alignment Using Syntactic DependenciesYanjun Ma1 Sylwia Ozdowska1 Yanli Sun2 Andy Way11 School of Computing, Dublin City University, Dublin, Ireland{yma,sozdowska,away}@computing.dcu.ie2 School of Applied Language and Intercultural Studies,Dublin City University, Dublin, Irelandyanli.sun2@mail.dcu.ieAbstractWe introduce a word alignment frameworkthat facilitates the incorporation of syntax en-coded in bilingual dependency tree pairs.
Ourmodel consists of two sub-models: an anchorword alignment model which aims to find a setof high-precision anchor links and a syntax-enhanced word alignment model which fo-cuses on aligning the remaining words relyingon dependency information invoked by the ac-quired anchor links.
We show that our syntax-enhanced word alignment approach leads to a10.32% and 5.57% relative decrease in align-ment error rate compared to a generative wordalignment model and a syntax-proof discrim-inative word alignment model respectively.Furthermore, our approach is evaluated ex-trinsically using a phrase-based statistical ma-chine translation system.
The results showthat SMT systems based on our word align-ment approach tend to generate shorter out-puts.
Without length penalty, using our wordalignments yields statistically significant im-provement in Chinese?English machine trans-lation in comparison with the baseline wordalignment.1 IntroductionAutomatic word alignment can be defined as theproblem of determining translational correspon-dences at word level given a parallel corpus ofaligned sentences.
Bilingual word alignment is afundamental component of most approaches to sta-tistical machine translation (SMT).
Dominant ap-proaches to word alignment can be classified intotwo main schools: generative and discriminativeword alignment models.Generative word alignment models, initially de-veloped at IBM (Brown et al, 1993), and thenaugmented by an HMM-based model (Vogel et al,1996), have provided powerful modeling capabilityfor word alignment.
However, it is very difficult toincorporate new features into these models.
Dis-criminative word alignment models, based on dis-criminative training of a set of features (Liu et al,2005; Moore, 2005), on the other hand, are moreflexible to incorporate new features, and feature se-lection is essential to the performance of the system.Syntactic annotation of bilingual corpora, whichcan be obtained more efficiently and accurately withthe advances in monolingual language processing,is a potential information source for word align-ment tasks.
For example, Part-of-Speech (POS) tagsof source and target words can be used to tacklethe data sparseness problem in discriminative wordalignment (Liu et al, 2005; Blunsom and Cohn,2006).
Shallow parsing has also been used to pro-vide relevant information for alignment (Ren et al,2007; Sun et al, 2000).
Deeper syntax, e.g.
phraseor dependency structures, has been shown useful ingenerative models (Wang and Zhou, 2004; Lopezand Resnik, 2005), heuristic-based models (Ayan etal., 2004; Ozdowska, 2004) and even for syntac-tically motivated models such as ITG (Wu, 1997;Cherry and Lin, 2006).In this paper, we introduce an approach to im-prove word alignment by incorporating syntactic de-pendencies.
Our approach is motivated by the factthat words tend to be dependent on each other.
If69we can first obtain a set of reliable anchor links, wecould take advantage of the syntactic dependenciesrelating unaligned words to aligned anchor words toexpand the alignment.
Figure 1 gives an illustratingexample.
Note that the link (2, 4) can be easily iden-tified, but the link involving the fourth Chinese word(a function word denoting ?time?)
(4, 4) is hard.
Insuch cases, we can make use of the dependency re-lationship (?tclause?)
between c2 and c4 to help thealignment process.
Given such an observation, ourmodel is composed of two related alignment models.The first one is an anchor alignment model which isused to find a set of anchor links; the other one is asyntax-enhanced alignment model aiming to processthe words left unaligned after anchoring.Figure 1: How syntactic dependencies can help wordalignment: an exampleThe remainder of this paper is organized as fol-lows.
In Section 2, we introduce our syntax-enhanced discriminative word alignment approach.The feature functions used are described in Sec-tion 3.
Experimental setting and results are pre-sented in Section 4 and 5 respectively.
In Section 6,we compare our approach with other related wordalignment approaches.
Section 7 concludes the pa-per and gives avenues for future work.2 Word Alignment Model2.1 NotationWhile in this paper we focus on Chinese?English,the method proposed is applicable to any languagepair.
The notation will assume Chinese?Englishword alignment and Chinese?English MT.
Here weadopt a notation similar to (Brown et al, 1993).Given a Chinese sentence cJ1 consisting of J words{c1, ..., cJ} and an English sentence eI1 consisting ofI words e1, ..., eI , we define the alignment A be-tween cJ1 and eI1 as a subset of the Cartesian productof the word positions:A ?
{(j, i) : j = 1, ..., J ; i = 1, ..., I}Our alignment representation is restricted so thateach source word can only be aligned to one tar-get word.
The alignment A consists of associationsj ?
i = aj from a source position j to a target po-sition i = aj .
The ?null?
alignment aj = 0 with the?empty?
word e0 is used to account for source wordsthat are not aligned to any target word.We use A?
to denote a subset of A.
The indices ofthe K source words involved in A?
are representedas ?K1 and the corresponding target indices for ?kare represented as a?k .
The unaligned source wordsare represented as ?
?.2.2 General ModelGiven a source sentence cJ1 and target sentence eI1,we seek to find the optimum alignment A?
such that:A?
= argmaxAP (A|cJ1 , eI1) (1)We use a model (2) that directly models the link-age between source and target words similarly to (It-tycheriah and Roukos, 2005).
We decompose thismodel into an anchor alignment model (3) and asyntax-enhanced model (4) by distinguishing the an-chor alignment from the non-anchor alignment.p(A|cJ1 , eI1) =J?j=0p(aj |cJ1 , eI1, aj?11 ) (2)= 1Z ?
p?
(A?|cJ1 , eI1) ?
(3)?j??
?p(aj|cJ1 , eI1, aj?11 , A?)
(4)2.3 Anchor Alignment ModelThe anchor alignment model p?(A?)
aims to find aset of high precision links.
Various approaches canbe used for this purpose.
In this paper we adoptedthe following two approaches.2.3.1 Heuristics-based ApproachThe problem of word alignment is regarded as aprocess of word linkage disambiguation, i.e.
choos-ing the correct links between words from all com-peting hypothesis (Melamed, 2000; Deng and Gao,2007).70We constrain the link probabilities in such a waythat:?i?
?
{1, ..., I}, i?
6= i : p((j, i))p((j, i?))
> ?1 (5)?j?
?
{1, ..., J}, j?
6= j : p((j, i))p((j?, i)) > ?2 (6)Condition (5) implies that for the source word cj ,the link with the target word ei is more probable(with reliability threshold ?1) than the link with anyother target word.
Condition (6) guarantees that forthe target word ei, cj is the only most probable (withthreshold ?2) source word to be linked to.2.3.2 Intersected Generative Word AlignmentModelsWe can use the asymmetric IBM models for bidi-rectional word alignment and get the intersection.2.4 Syntax-Enhanced Word Alignment ModelThe syntax-enhanced model is used to model thealignment of the words left unaligned after anchor-ing.
We directly model the linkage between sourceand target words using a discriminative word align-ment framework where various features can be in-corporated.
Given a source word cj and the targetsentence eI1, we search for the alignment aj suchthat:a?j = argmaxaj{p?M1 (aj |cJ1 , eI1, aj?11 , A?)}
(7)= argmaxaj{?Mm=1 ?mhm(cJ1 , eI1, aj1, A?, Tc, Te)}In this decision rule, we assume that a set of highlyreliable anchor alignments A?
has been obtained,and Tc (resp.
Te) is used to denote the dependencystructure for source (resp.
target) language.
In sucha framework, various machine learning techniquescan be used for parameter estimation.3 Feature Function for Syntax-EnhancedModelThe various features used in our syntax-enhancedmodel can be classified into three groups: statistics-based features, syntactic features and relative distor-tion features.3.1 Statistics-based Features3.1.1 IBM model 1 scoreIBM model 1 is a position-independent wordalignment model which is often used to boot-strap parameters for more complex models.
Model1 models the conditional distribution and uses auniform distribution for the dependencies betweensource word positions and target word positions.Pr(cJ1 , aJ1 |eI1) =p(J |I)(I + 1)JJ?j=1p(cj |eaj ) (8)3.1.2 Log-likelihood ratioThe log-likelihood ratio statistic has been found tobe accurate for modeling the associations betweenrare events (Dunning, 1993).
It has also been suc-cessfully used to measure the associations betweenword pairs (Melamed, 2000; Moore, 2005).
Giventhe following contingency table:cj ?cjei a b?ei c dthe log-likelihood ratio can be defined as:G2(cj , ei) = ?2logB(a|a + b, p1)B(c|c + d, p2)B(a|a+ b, p)B(c|c + d, p)where B(k|n, p) = (nk )pk(1 ?
p)n?k are binomialprobabilities.
The probability parameters can be ob-tained using maximum likelihood estimates:p1 =aa+ b , p2 =cc+ d (9)p = a+ ca + b+ c+ d (10)3.1.3 POS translation probabilityThe POS tags can provide effective informationfor addressing the data sparseness problem using thelexical features (Liu et al, 2005; Blunsom and Cohn,2006).
The POS translation probability can be easilyobtained using maximum likelihood estimation froman annotated corpus:Pr(Tc|Te) =COL(Tc, Te)COF (Te)(11)71where Tc is a Chinese word?s POS tag and Te is anEnglish word?s POS tag.
COL(Tc, Te) is the countof Tc and Te being linked to each other in the corpus,and COF (Te) is the frequency of Te in the corpus.3.2 Syntactic FeaturesThe dependency relation Re (resp.
Rc) between twoEnglish (resp.
Chinese) words ei and ei?
(resp.
cjand cj?)
in the dependency tree of the English sen-tence eI1 (resp.
Chinese sentence cJ1 ) can be repre-sented as a triple <ei, Re, ei?>(resp.
<cj , Rc, ej?>).Given cJ1 , eI1 and their syntactic dependency treesTcJ1 , TeI1 , if ei is aligned to cj and ei?
aligned tocj?
, according to the dependency correspondence as-sumption (Hwa et al, 2002), there exists a triple<cj , Rc, cj?>.While we are not aiming to justify the feasibil-ity of the dependency correspondence assumptionby proving to what extent Re = Rc under the con-dition described above, we do believe that cj and cj?are likely to be dependent on each other.
Given theanchor alignment A?, a candidate link (j, i) and thedependency trees, we can design four classes of fea-ture functions.3.2.1 Agreement featuresThe agreement features can be further classi-fied into dependency agreement features and depen-dency label agreement features.
Given a candidatelink (j, i) and the anchor alignment A?, the depen-dency agreement (DA) feature function is defined asfollows:hDA?1 =????
?1 if ?
<cj, Rc, cj?>, <ei, Re, ei?>and (j?, i?)
?
A?,0 otherwise.
(12)By changing the dependency direction between thewords cj and cj?
, we can derive another dependencyagreement feature:hDA?2 =????
?1 if ?
<cj?
, Rc, cj>, <ei?
, Re, ei>and (j?, i?)
?
A?,0 otherwise.
(13)We can define the dependency label agreement fea-ture1 as follows:hDLA?1 =????
?1 if ?
<cj , Rc, cj?>, <ei, Re, ei?>and (j?, i?)
?
A?,Rc = Re,0 otherwise.
(14)Similarly we can obtain hDLA?2 by changing thedependency direction.3.2.2 Source word dependency featuresGiven a candidate link (j, i) and anchor alignmentA?, source language dependency features are usedto capture the dependency label between a sourceword cj and a source anchor word ck ?
?.
Forexample, a feature function relating to dependencytype ?PRD?
can be defined as:hsrc?1?PRD =????
?1 if ?
<cj, Rc, cj?>and Rc =?PRD?,0 otherwise.
(15)By changing the direction we can obtainhsrc?2?PRD.3.2.3 Target word dependency featuresTarget word dependency features can be definedin a similar way as source word dependency fea-tures.3.2.4 Target anchor featureThe target anchor feature defines whether the tar-get word ei is an anchor word.hsrc?1?PRD ={1 if i ?
a?,0 otherwise.
(16)3.3 Relative distortion featureWe can design features encoding the relative dis-tortion information which can be used to evaluatea candidate link by computing its relative positionchange with respect to the anchor alignment.
Therelative position change of a candidate link l = (j, i)is formally defined as follows:1Note that we used the same dependency parser for sourceand target language parsing.72D(l) = min(|dL|, |dR|) (17)dL = (j ?
jL) ?
(i?
iL) (18)dR = (j ?
jR)?
(i?
iR) (19)where (iL, jL) is the leftmost anchor link of l,(iR, jR) is the rightmost anchor link of l. The lessthe relative position changes, the more likely thecandidate link is.
With a set of anchor alignments,we can obtain the distribution of the relative posi-tion changes from an annotated corpus using maxi-mum likelihood estimation.
In our experiments, weused the following four probabilities: p(D = 0),p(D = 1, 2), p(D = 3, 4) and p(D > 4).4 Experimental Setting4.1 DataThe experiments were carried out using theChinese?English datasets provided within theIWSLT 2007 evaluation campaign (Fordyce, 2007),extracted from the Basic Travel Expression Corpus(BTEC) (Takezawa et al, 2002).
This multilingualspeech corpus contains sentences similar to thosethat are usually found in phrase-books for touristsgoing abroad.We tagged all the sentences in the training and de-vset3 using a maximum entropy-based POS tagger?MXPOST (Ratnaparkhi, 1996), trained on the PennEnglish and Chinese Treebanks.
Both Chinese andEnglish sentences are parsed using the Malt depen-dency parser (Nivre et al, 2007), which achieved84% and 88% labelled attachment scores for Chi-nese and English respectively.4.1.1 Word AlignmentWe manually annotated word alignments on de-vset3.
Since manual word alignment is an ambigu-ous task, we also explicitly allow for ambiguousalignments, i.e.
the links are marked as sure (S) orpossible (P) (Och and Ney, 2003).
IWSLT devset3consists of 502 sentence pairs after cleaning.
Weused the first 300 sentence pairs for training, the fol-lowing 50 sentence pairs as validation set and thelast 152 sentence pairs for testing.4.1.2 Machine TranslationTraining was performed using the default trainingset (39,952 sentence pairs), to which we added theset devset1 (506 sentence pairs).2 We used devset2(506 sentence pairs, 16 references) to tune variousparameters in the MT system and IWSLT 2007 testset (489 sentence pairs, 6 references) for testing.4.2 Alignment Training and SearchIn our experiments, we treated anchor alignment andsyntax-enhanced alignment as separate processes ina pipeline.
The anchor alignments are kept fixed sothat the parameters in the syntax-enhanced modelcan be optimized.3 We used the support vector ma-chine (SVM) toolkit?SVM light4 to optimize theparameters in (7).
Our model is constrained in sucha way that each source word can only be aligned toone target word.
Therefore, in training, we trans-form each possible link involving the words left un-aligned after anchoring into an event.
In testing, thesource words are consumed in sequence and the tar-get words serve as states.
The SVM dual variablewas used to measure the reliability of each candidatelink and the alignment link for each word is madeindependently, which makes the alignment searchmuch easier.
A threshold t was set as the minimalreliability score for each link.
t is optimized accord-ing to alignment error rate (21) on the validation set.4.3 Baselines4.3.1 Word AlignmentWe used the GIZA++ implementation of IBMword alignment model 4 (Brown et al, 1993; Ochand Ney, 2003) for word alignment, and the heuris-tics described in (Och and Ney, 2003) to derive theintersection and refined alignment.4.3.2 Machine TranslationWe use a standard log-linear phrase-based SMT(PB-SMT) model as a baseline: GIZA++ implemen-tation of IBM word alignment model 4,5 the refine-2More specifically, we chose the first English reference fromthe 16 references and the Chinese sentence to construct newsentence pairs.3Note our anchor alignment does not achieve 100% preci-sion.
Since we performed precision-oriented alignment for theanchor alignment model, the errors in anchor alignment will notbring much noise into the syntax-enhanced model.4http://svmlight.joachims.org/5More specifically, we performed 5 iterations of Model 1, 5iterations of HMM, 3 iterations of Model 3, and 3 iterations ofModel 4.73ment and phrase-extraction heuristics described in(Koehn et al, 2003), minimum-error-rate training(Och, 2003), a trigram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002)on the English side of the training data, and Moses(Koehn et al, 2007) to decode.4.4 EvaluationWe evaluate the intrinsic quality of predicted align-ment A with precision, recall and alignment errorrate (AER).
Slightly differently from (Och and Ney,2003), we use possible alignments in computing re-call.recall = |A ?
P ||P | , precision =|A ?
P ||A| (20)AER(S,P ;A) = 1?
|A ?
S|+ |A ?
P ||A| + |S| (21)We also extrinsically measure the word alignmentquality via a Chinese?English translation task.
Thetranslation output is measured using BLEU (Pap-ineni et al, 2002).5 Experimental Results5.1 Word AlignmentWe performed word alignment bidirectionally usingour approach to obtain the union and compared ourresults with two strong baselines based on generativeword alignment models.
The results are shown inTable 1.
We can see that both the syntax-enhancedmodel based on HMM intersection anchors (Syntax-HMM) and on IBM model 4 anchors (Syntax-Model4) are better than the pure generative word alignmentmodels.
Our approach is superior in precision witha disadvantage in recall.
The best result achieved10.32% relative decrease in AER compared to thebaseline when we use IBM model 4 intersection toobtain the set of anchor alignments.model precision recall f-score AERHMM refined 0.8043 0.7592 0.7811 0.2059Syntax-HMM 0.8744 0.7304 0.7959 0.1845Model 4 refined 0.7941 0.7987 0.7964 0.1929Syntax-Model 4 0.8566 0.7685 0.8102 0.1730Table 1: Comparing syntax-enhanced approach with gen-erative word alignment5.1.1 The Influence of Anchor AlignmentQualityAs we can see in Table 2, our precision-orientedapproach to acquire anchor alignments was accom-plished quite well.
All four different anchor align-ment models achieved high precision.
However, therecall differs dramatically, with model 4 achievingthe highest recall and the heuristics-based approachreceiving the lowest.
To investigate the influenceanchor model precision recall f-measure AERHeuristics 0.9774 0.4047 0.5724 0.3947Model 1 0.9509 0.5011 0.6563 0.3157HMM 0.9802 0.5327 0.6903 0.2809Model 4 0.9777 0.5677 0.7179 0.2533Table 2: Performance of anchor alignmentof the anchor alignment model, we first obtainedthe intersection of the words left unaligned after an-choring using each of the anchor alignment models.We evaluate the alignment of these words againstthe gold-standard alignments involving these words.The influence of anchor alignment on the perfor-mance of the syntax-enhanced model can be seenin Table 3.
The performance of the syntax-enhancedmodel is closely related to that of the anchor align-ment method.
As can be seen from Table 2 and3, HMM anchoring achieves the best precision andso does the syntax-enhanced alignment; IBM model4 achieves the best recall and so does the syntax-enhanced alignment.
Finally, the best alignment per-formances are obtained with IBM model 4 anchor-ing, with the difference in recall between HMM andIBM model 4 anchoring being more significant thanthe difference in precision.anchor model precision recall f-score AERHeuristics 0.4505 0.3270 0.3790 0.6210Model 1 0.5538 0.3894 0.4573 0.5427HMM 0.5932 0.3611 0.4489 0.5511Model 4 0.5660 0.4216 0.4832 0.5168Table 3: Influence of anchor alignment in syntax-enhanced model5.1.2 The Influence of Syntactic Dependencieson Word AlignmentThe influence of incorporating syntactic depen-dencies into the word alignment process is shown74in Table 4.
Syntax plays a positive role in all differ-ent anchor alignment configurations.
The influencegrows proportionally to the strength of the anchoralignment model.
With the Model 4 intersectionused as the set of anchor alignments, adding syn-tactic dependency features into the syntax-enhancedalignment model yields a 5.57% relative decrease inAER.model precision recall f-score AERHeuristicsno syntax 0.8362 0.6751 0.7470 0.2302w.
syntax 0.8376 0.6894 0.7563 0.2240Model 1no syntax 0.8759 0.6902 0.7720 0.2045w.
syntax 0.8542 0.7160 0.7790 0.2011HMMno syntax 0.8655 0.7168 0.7841 0.1952w.
syntax 0.8744 0.7304 0.7959 0.1845Model 4no syntax 0.8697 0.7340 0.7961 0.1832w.
syntax 0.8566 0.7685 0.8102 0.1730Table 4: Influence of syntactic dependencies on wordalignment5.1.3 Contribution of Different Feature ClassesWe interpret the contribution of each feature interms of feature weights in SVM model training.The weights for the most discriminative features ineach feature class in Chinese?English word align-ment (using HMM intersection as anchor align-ment) are shown in Table 5.
As we can see, allstatistics-based features are informative.
Two targetdependency features are informative: ?PRD?
denot-ing ?predicative?
dependency, and ?AMOD?
denot-ing ?adjective/adverb modifier?
dependency.weightModel 1 Score 0.1416POS 0.0540Log-likelihood Ratio 0.0856relative distortion 0.0606DA-1 0.0227DLA-2 0.0927tgt-1-PRD 0.0961tgt-2-AMOD 0.0621Table 5: Weights of some informative features5.2 Machine TranslationResearch has shown that an increase in AER doesnot necessarily imply an improvement in translationquality (Liang et al, 2006) and vice-versa (Vilar etal., 2006).
Hereafter, we used a Chinese?EnglishMT task to extrinsically evaluate the quality of ourword alignment.Table 6 shows the influence of our word align-ment approach on MT quality.6 On development set,we achieved statistically significant improvementusing both our syntax-enhanced models?Syntax-HMM (p<0.002) and Syntax-Model 4 (p<0.008).On the test set, we observed that the MT outputbased on our alignment model tends to be shorterthan the reference translations and the BLEU scoreis considerably penalized.
If we ignore the lengthpenalty (?BP?
in Table 6) in significance testing, theimprovement on test set is also statistically signif-icant: p<0.04 for both Syntax-HMM and Syntax-Model 4.
However, an indepth manual analysisneeds to be carried out in order to determine the ex-act nature of the shorter sentences derived.dev.
set test setBaseline 0.5412 0.3510 (BP=0.96)Syntax-HMM 0.6015 0.3409 (BP=0.86)Syntax-Model 4 0.5834 0.3585 (BP=0.91)Table 6: The Influence of Word Alignment on MT6 Comparison with Previous WorkOur syntax-enhanced model is a discriminative wordalignment model.
Certain generative word align-ment models (e.g.
HMM or IBM 4) also takethe first-order dependencies into account.
How-ever, long distance dependencies between words arehard to incorporate into these models because ofthe explosive number of parameters.
On the otherhand, like existing discriminative models, our ap-proach uses a set of informative features based onco-occurrence statistics, e.g.
log-likelihood ratioand DICE score.
The advantage of our approach isthe mechanism by which syntactic features may beincorporated.6Note that the only difference between our MT system andthe baseline PB-SMT system is the word alignment component.75Some previous research also tried to make useof syntax in word alignment.
(Wang and Zhou,2004) investigated the benefit of monolingual pars-ing for alignment.
They learned a generalized wordassociation measure (crosslingual word similarities)based on monolingual dependency structures andimproved alignment performances over IBM model2 and certain heuristic-based models.
(Cherry andLin, 2006) used dependency structures as soft con-straints to improve word alignment in an ITG frame-work.
Compared to these models, our approach di-rectly takes advantage of dependency relations asthey are transformed into feature functions incorpo-rated into a discriminative word alignment frame-work.7 Conclusion and Future WorkIn this paper, we proposed a model that can facili-tate the incorporation of syntax into word alignmentand measured the combination of a set of syntacticfeatures.
Experimental results have shown that syn-tax is useful in word alignment and especially effec-tive in improving the recall.
We have also observedthat in our word alignment framework, the two sub-models are closely related and the quality of the an-chor alignment model plays an important role in thesystem performance.The promising results will lead us to improve ourmodel in the following aspects.
First, the two sub-models in our approach are two separate processesperformed in pipeline.
We plan to jointly optimizethe two models in one go.
Second, some of ourexperiments used complex IBM models, e.g.
IBMModel 4, to obtain anchor alignment.
We plan toboostrap the alignment using simple heuristics with-out relying on complex IBM models.
Third, thealignment searching process assumed the alignmentlink for each word is made independently.
A feasiblemarkovian assumption will be tested for searching.Fourth, a comparison with traditional discriminativeword alignment models is also necessary to justifythe merits of our approach.
Finally, we also plan toadapt our approach to larger data sets and more lan-guage pairs.AcknowledgmentsThis work is supported by Science Foundation Ire-land (grant number OS/IN/1732).
Prof. RebeccaHwa from University of Pittsburgh and Dr. YangLiu from the Institute of Computing Technology,Chinese Academy of Sciences, are kindly acknowl-edged for providing us with their word alignmentguidelines.
We would also like to thank the anony-mous reviewers for their insightful comments.ReferencesNecip Fazil Ayan, Bonnie Dorr, and Nizar Habash.2004.
Multi-align: Combining linguistic and statis-tical techniques to improve alignments for adaptablemt.
In Proceedings of the 6th Conference of the AMTA(AMTA-2004), pages 17?26, Washington DC.Phil Blunsom and Trevor Cohn.
2006.
Discrimina-tive word alignment with conditional random fields.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 65?72, Sydney, Australia.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Colin Cherry and Dekang Lin.
2006.
Soft syntacticconstraints for word alignment through discriminativetraining.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 105?112, Sydney, Australia.Yonggang Deng and Yuqing Gao.
2007.
Guiding sta-tistical word alignment models with prior knowledge.In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 1?8,Prague, Czech Republic.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61?74.Cameron Shaw Fordyce.
2007.
Overview of the IWSLT2007 Evaluation Campaign.
In Proceedings of the In-ternational Workshop on Spoken Language Transla-tion, pages 1?12, Trento, Italy.Rebecca Hwa, Philip Resnik, Amy Weinberg, and OkanKolak.
2002.
Evaluating translational correspondenceusing annotation projection.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 392?399, Philadelphia, PA.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for Arabic-English ma-chine translation.
In Proceedings of Human Language76Technology Conference and Conference on EmpiricalMethods in Natural Language Processing, pages 89?96, Vancouver, British Columbia, Canada.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 48?54, Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of Human Lan-guage Technology Conference and Conference onEmpirical Methods in Natural Language Processing,pages 104?111, New York, NY.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linearmodels for word alignment.
In Proceedings of the43rd Annual Meeting of the Association for Compu-tational Linguistics, pages 459?466, Ann Arbor, MI.Adam Lopez and Philip Resnik.
2005.
Improved HMMalignment models for languages with scarce resources.In Proceedings of the ACL Workshop on Building andUsing Parallel Texts, pages 83?86, Ann Arbor, Michi-gan, June.I.
Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.Robert C. Moore.
2005.
A discriminative framework forbilingual word alignment.
In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing,pages 81?88, Vancouver, British Columbia, Canada.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Ervin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Franz Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Com-putational Linguistics, 29(1):19?51.Franz Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 160?167, Sapporo, Japan.Sylwia Ozdowska.
2004.
Identifying correspondencesbetween words: an approach based on a bilingual syn-tactic analysis of French/English parallel corpora.
InProceedings of the COLING?04 Workshop on Multi-lingual Linguistic Resources, pages 49?56, Geneva,Switzerland.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Eric Brill and Ken-neth Church, editors, Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 133?142, Somerset, NJ.Dengjun Ren, Hua Wu, and Haifeng Wang.
2007.
Im-proving statistical word alignment with various clues.In Machine Translation Summit XI, pages 391?397,Copenhagen, Denmark.Andrea Stolcke.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, CO.Le Sun, Youbing Jin, Lin Du, and Yufang Sun.
2000.Word alignment of English-Chinese bilingual corpusbased on chunks.
In Proceedings of the 2000 JointSIGDAT conference on Empirical Methods in NaturalLanguage Processing and very large corpora, pages110?116.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,Hirofumi Yamamoto, and Seiichi Yamamoto.
2002.Toward a broad-coverage bilingual corpus for speechtranslation of travel conversations in the real world.In Proceedings of Third International Conference onLanguage Resources and Evaluation 2002, pages 147?152, Las Palmas, Spain.David Vilar, Maja Popovic, and Hermann Ney.
2006.AER: Do we need to ?improve?
our alignments?
InProceedings of the International Workshop on SpokenLanguage Translation, pages 205?212, Kyoto, Japan.Stefan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the 16th International Con-ference on Computational Linguistics, pages 836?841,Copenhagen, Denmark.Wei Wang and Ming Zhou.
2004.
Improving word align-ment models using structured monolingual corpora.In Dekang Lin and Dekai Wu, editors, Proceedingsof Conference on Empirical Methods in Natural Lan-guage Processing, pages 198?205, Barcelona, Spain.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.77
