Question Answering on a Case Insensitive CorpusWei Li, Rohini Srihari, Cheng Niu, Xiaoge LiCymfony Inc.600 Essjay RoadWilliamsville, NY 14221, USA{wei, rohini, cniu, xli}@cymfony.comAbstractMost question answering (QA) systemsrely on both keyword index and NamedEntity (NE) tagging.
The corpus fromwhich the QA systems attempt to retrieveanswers is usually mixed case text.However, there are numerous corpora thatconsist of case insensitive documents, e.g.speech recognition results.
This paperpresents a successful approach to QA on acase insensitive corpus, whereby apreprocessing module is designed torestore the case-sensitive form.
Thedocument pool with the restored case thenfeeds the QA system, which remainsunchanged.
The case restorationpreprocessing is implemented as a HiddenMarkov Model trained on a large rawcorpus of case sensitive documents.
It isdemonstrated that this approach leads tovery limited degradation in QAbenchmarking (2.8%), mainly due to thelimited degradation in the underlyinginformation extraction support.1 IntroductionNatural language Question Answering (QA) isrecognized as a capability with great potential.
TheNIST-sponsored Text Retrieval Conference(TREC) has been the driving force for developingthis technology through its QA track since TREC-8[Voorhees 1999] [Voorhees 2000].
There has beensignificant progress and interest in QA research inrecent years [Pasca & Harabagiu.
2001] [Voorhees2000].In real-life QA applications, a system should berobust enough to handle diverse textual mediadegraded to different degrees.
One of thechallenges from degraded text is the treatment ofcase insensitive documents such as speechrecognition results, broadcast transcripts, and theForeign Broadcast Information Service (FBIS)sources.
In the intelligence domain, the majority ofarchives consist of documents in all uppercase.The orthographic case information for writtentext is an important information source.
Inparticular, the basic information extraction (IE)support for QA, namely Named Entity (NE)tagging, relies heavily on the case information forrecognizing proper names.
Almost all NE systems(e.g.
[Bikel et al 1997], [Krupka & Hausman1998]) utilize case-related features.
When thisinformation is not available, if the system is not re-trained or adapted, serious performancedegradation will occur.
In the case of the statisticalNE tagger, without adaptation the system simplydoes not work.
The degradation for proper nameNE tagging is more than 70% based on our testing.The key issue here is how to minimize theperformance degradation by adopting somestrategy for the system adaptation.For search engines, the case information is oftenignored in keyword indexing and retrieval for thesake of efficiency and robustness/recall.
However,QA requires fine-grained text processing beyondkeyword indexing since, instead of a list ofdocuments or URLs, a list of candidate answers atphrase level or sentence level is expected to bereturned in response to a query.
Typically QA issupported by Natural Language Processing (NLP)and IE [Chinchor & Marsh 1998] [Hovy et al2001] [Srihari & Li 2000].
Examples of using NLPand IE in Question Answering include shallowparsing [Kupiec 1993] [Srihari & Li 2000], deepparsing [Li et al 2002] [Litkowski 1999][Voorhees 1999], and IE [Abney et al 2000][Srihari & Li 2000].
Almost all state-of-the-art QAsystems rely on NE in searching for candidateanswers.For a system based on language models, afeature exclusion approach is used to re-train themodels, excluding features related to the caseinformation [Kubala et al 1998] [Miller et al2000] [Palmer et al 2000].
In particular, theDARPA HUB-4   program evaluates NE systemson speech recognizer output in SNOR (StandardNormalized Orthographic Representation) that iscase insensitive and has no punctuations [Chincoret al 1998].
Research on case insensitive text hasso far been restricted to NE and the featureexclusion approach [Chieu & Ng 2002] [Kubala etal.
1998] [Palmer et al 2000] [Robinson et al1999].
When we examine a system beyond theshallow processing of NE, the traditional featureexclusion approach may not be feasible.
Asophisticated QA system usually involves severalcomponents with multiple modules, involvingNLP/IE processing at various levels.
Eachprocessing module may involve some sort of caseinformation as constraints.
It is too costly andsometimes impossible to maintain two versions ofa multi-module QA system for the purpose ofhandling two types of documents, with or withoutcase.This paper presents a case restoration approachto this problem, as applied to QA.
The focus is tostudy the feasibility of QA on a case insensitivecorpus using the presented case restorationapproach.
For this purpose, we use an existing QAsystem as the baseline in experiments; we are notconcerned with enhancing the QA system itself.
Apreprocessing module is designed to restore thecase-sensitive form to feed to this QA system.
Thecase restoration module is based on a HiddenMarkov Model (HMM) trained on a large rawcorpus of case sensitive documents, which aredrawn from a given domain with no need forhuman annotation.
With the plug-in of thispreprocessing module, the entire QA system withits underlying NLP/IE components needs nochange or adaptation in handling the caseinsensitive corpus.
Using the TREC corpus withthe case information artificially removed, thisapproach has been benchmarked with very goodresults, leading to only 2.8% degradation in QAperformance.
In the literature, this is the first timea QA system is applied to case insensitive corpora.Although the artificially-made case insensitivecorpus is an easier case than some real life corporafrom speech recognition, the insight andtechniques gained in this research are helpful infurther exploring solutions of spoken languageQA.
In addition, by using the TREC corpus and theTREC benchmarking standards, the QAdegradation benchmarking is easy to interpret andto compare with other QA systems in thecommunity.The case restoration approach has the followingadvantages: (i) the training corpus is almostlimitless, resulting in a high performance model,with no knowledge bottleneck as faced by manysupervised learning scenarios, (ii) the caserestoration approach is applicable no matterwhether the core system is statistical model, ahand-crafted rule system or a hybrid, (iii) when thecore system consists of multiple modules, as is thecase for the QA system used in the experimentsthat is based on multi-level NLP/IE, the caserestoration approach relieves the burden of havingto re-train or adapt each module in respect of caseinsensitive input, and (iv) the  restoration approachreduces the system complexity: the burden ofhandling degraded text (case in this case) isreduced to a preprocessing module while all othercomponents need no changes.The remaining text is structured as follows.Section 2 presents the QA system.
Section 3describes the language model for case restoration.Section 4  benchmarks the IE engine and Section 5benchmarks the IE-supported QA application.
Inboth benchmarking sections, we compare theperformance degradation from case sensitive inputto case insensitive input.
Section 5 is theConclusion.2 Question Answering Based on IEWe use a QA system supported by increasinglysophisticated levels of IE [Srihari & Li 2000] [Li etal.
2002].
Figure 1 presents the underlying IEengine InfoXtract [Srihari et al 2003] that formsthe basis for the QA system.
The majorinformation objects extracted by InfoXtract includeNEs,1 Correlated Entity (CE) relationships (e.g.Affiliation, Position etc.
), Subject-Verb-Object(SVO) triples, entity profiles, and general orpredefined events.
These information objectscapture the key content of the processed text,preparing a foundation for answering factoidquestions.Document ProcessorKnowledge ResourcesLexiconResourcesGrammarsProcessManagerTokenlistLegendOutputManagerSourceDocumentLinguistic Processor(s)TokenizerTokenlistLexicon LookupPragmaticFilteringPOS TaggingNamed EntityDetectionShallowParsingDeepParsingRelationshipDetectionDocumentpoolNECEEPSVOTimeNormalizationAlias/CoreferenceLinkingProfile/EventLinkingProfile/EventMergeAbbreviationsNE = Named EntityCE = Correlated EntityEP = Entity ProfileSVO=Subject-Verb-ObjectGE = General EventPE = Predefined EventGrammar ModuleProcedure orStatistical ModelHybridModuleGEStatisticalModelsLocationNormalizationli tiPEIEIndexFigure 1: System Architecture of InfoXtractFigure 2 shows the architecture of the QAsystem.
This system consists of three components:(i) Question Processing, (ii) Text Processing, and(iii) Answer Ranking.
In text processing, the caseinsensitive corpus is first pre-processed for caserestoration before being parsed by InfoXtract.
Inaddition, keyword indexing on the corpus isrequired.
For question processing, a special modulefor Asking Point Identification is called for.Linking the two processing components is theAnswer Ranking component that consists of twomodules: Snippet Retrieval and Feature Ranking.21 It is worth noting that there are two types of NE: (i) proper namesNeName (including NePerson, NeOrganization, NeLocation, etc.)
and(ii) non-name NEs (NeItem) such as time NE (NeTimex) andnumerical NE (NeNumex).
Close to 40% of the NE questions targetnon-name NEs.
Proper name NEs are more subject to the case effectbecause recognizing a name in the running text often requires caseinformation.
Non-name NEs generally appear in predictable patterns.Pattern matching rules that perform case-insensitive matching aremost effective in capturing them.2 There is a third, optional module Answer Point Identification in ourQA system [10], which relies on deep parsing for generating phrase-Answer Ranking relies on access to informationfrom both the Keyword Index as well as the IEIndex.IE IndexCaseInsensitiveCorpusMulti-levelTemplateQuestionKeywordIndexKeyword indexingInfoXtract Asking PointIdentificationFeature RankingSnippet Retrieval Snippet-levelAnswerQuestion ProcessingText ProcessingAnswer RankingCase Restoration InfoXtractFigure 2:  Architecture of QA Based on NLP/IESnippet RetrievalSnippet retrieval generates the top n (we chose200) most relevant sentence-level candidateanswer snippets based on the question processingresults.We use two types of evidence for snippetretrieval:  (i) keyword occurrence statistics atsnippet level (with stop words removed), and (ii)the IE results, including NE Asking Points, AskingPoint CE Link, head word of a phrase, etc.If the Question Processing component detects anAsking Point CE Link, the system first attempts toretrieve snippets that contain the corresponding CErelationship.
If it fails, it backs off to thecorresponding NE Asking Point.
This serves as afilter in the sense that only the snippets that containat least one NE that matches the NE Asking Pointare extracted.
For questions that do not contain NEAsking Points, the system backs off to keyword-based snippet retrieval.A synonym lexicon is also constructed for queryexpansion to help snippet retrieval.
This includesirregular verbs (go/went/gone, etc.
), verb-nounconversion (develop/development; satisfy/satisfaction; etc.
), and a human-modifiedlevel answers from snippet-level answers.
This module was not usedin the experiments reported in this paper.conservative synonym list (e.g.
adjust/adapt;adjudicate/judge; etc.
).Factors that contribute to relevancy weighting insnippet retrieval include giving more weight to thehead words of phrases (e.g.
?disaster?
in the nounphrase ?the costliest disaster?
), more weight towords that are linked with question words (e.g.?calories?
in ?How many calories??
and?American?
in ?Who was the first American inspace?
), and discounting the weight for synonymmatching.Feature RankingThe purpose of Feature Ranking is to re-rank thecandidate snippets based on a list of rankingfeatures.Given a list of top n snippets retrieved in theprevious stage, the Feature Ranking module uses aset of re-ranking features to fine-tune relevancymeasures of the initial list of snippets in order togenerate the final top five answer strings that arerequired as output.
Figure 3 gives the rankingmodel for the Feature Ranking module.RankingFeature 1SnippetList QuestionTokenListWeight - Wq1RankingFeature 2SnippetList QuestionTokenListWeight - Wq2RankingFeature mWeight - WqmSnippetList QuestionTokenListRanked List of AnswersFigure 3: Pipeline for Ranking FeaturesFor a given question, Q, let {S1, S2,?,Sn} be theset of candidate answer snippets.
Let {R1, R2, ?,Rk} be the ranking features.
For a snippet Sj, let theranking feature Ri assign a relevancy of rijquantifying the snippet?s relevance to the question.The ranking model is given by==kiijilj rwSQR1),(where l represents the question type of Q and wilgives the weight assigned to the ranking feature.Weights wil vary based on question type.We use both traditional IR ranking features suchas Keyword Proximity and Inverse DocumentFrequency (IDF) as well as the ranking featuressupported by NLP/IE, listed below:?
NE Asking Point?
Asking Point CE Link?
Headword Match for Basic Phrases?
Phrase-Internal Word Order?
Alias (e.g.
?IBM?
and ?InternationalBusiness Machine?)?
NE Hierarchical Match (e.g.
Company vs.Organization)?
Structure-Based Matching (SVO Links,Head-Modifier Link, etc.
)3 Case RestorationThis section presents the case restoration approach[Niu et al 2003] that supports QA on caseinsensitive corpus.
The flowchart for using CaseRestoration as a plug-in preprocessing module toIE is shown in Figure 4.Document InputTokenizationCase DetectionInfoXtractCaseRestorationModuleNoYesCase Sensitive?OutputFigure 4: Case Restoration for IEThe incoming documents first go throughtokenization.
In this process, the case informationis recorded as features for each token.
This token-based case information provides basic evidence forthe optional procedure called Case Detection todecide whether the Case Restoration module needsto be called.A simple bi-gram Hidden Markov Model [Bikelet al 1999] is selected as the choice of languagemodel for this task.
Currently, the system is basedon a bi-gram model trained on a normal, casesensitive raw corpus in the chosen domain.Three orthographic tags are defined in thismodel: (i) initial uppercase followed by at least onelowercase, (ii) all lowercase, and (iii) alluppercase.To handle words with low frequency, each wordis associated with one of five features: (i)PunctuationMark (e.g.
&, ?, !?
), (ii) LetterDot(e.g.
A., J.P., U.S.A.,?
), (iii) Number (e.g.102,?
), (iv) Letters (e.g.
GOOD, MICROSOFT,IBM, ?
), or (v) Other.The HMM is formulated as follows.
Given aword sequence nn00 fwfw W =  (wherejf denotes a single token feature which are definedas above), the goal for the case restoration task isto find the optimal tag sequence n210 tttt T = ,which maximizes the conditional probabilityW)| Pr(T  [Bikel et al 1999].
By Bayesian equality,this is equivalent to maximizing the jointprobability T)Pr(W, .
This joint probability can becomputed by a bi-gram HMM as?
?=i1i1-i1-iiii )t,f,w|t,f,wPr(T)Pr(W, .
Theback-off model is as follows,)t,w|)Pr(tt,t|f,wPr()-(1)t,f,w|t,f,w(P)t,f,w|t,f,wPr(1i1ii1iiii11i1-i1-iiii011i1-i1-iiii?????+=??)t|f,wPr()-(1)t,t|f,w(P)t,t|f,wPr(iii21iiii021iiii??
+=??)w|Pr(t)-(1)t,w|(tP)t,w|Pr(t1-ii31i1-ii031i1-ii??
+=??)t|(f)Pt|(wPr)-(1)t|f,w(P)t|f,wPr(ii0ii4iii04iii??
+=)t(P)-(1)w|(tP)w|Pr(t i051-ii051-ii ??
+=V1)-(1)t|(wP)t|Pr(w 6ii06ii ??
+=where V denotes the size of the vocabulary, theback-off coefficients ?
?s are determined using theWitten-Bell smoothing algorithm, and thequantities)t,f,w|t,f,w(P 1i1i1iiii0 ???
, )t,t|f,w(P 1iiii0 ?
,)t,w|(tP 1i1-ii0 ?
, )t|f,w(P iii0 , )t|(fP ii0 ,)w|(tP 1-ii0 , )(tP i0 , and )t|(wP ii0  are computed bythe maximum likelihood estimation.A separate HMM is trained for bigramsinvolving unknown words.
The training corpus isseparated into two parts, the words occurring inPart I but not in Part II and the words occurring inPart II but not in Part I are all replaced by a specialsymbol #Unknown#.
Then an HMM for unknownwords is trained on this newly marked corpus.
Inthe stage of tagging, the unknown word model isused in case a word beyond the vocabulary occurs.4 IE Engine BenchmarkingA series of benchmarks have been conducted inevaluating the approach presented in this paper.They indicate that this is a simple but veryeffective method to solve the problem of handlingcase insensitive input for NLP, IE and QA.Case RestorationA raw corpus of 7.6 million words in mixed casedrawn from the general news domain is used intraining case restoration.
A separate testing corpusof 0.88 million words drawn from the samedomain is used for benchmarking.
Table 1 givesthe case restoration performance benchmarks.
Theoverall F-measure is 98% (P for Precision, R forRecall and F for F-measure).Table 1: Case Restoration PerformanceP R F0.96 1 0.980.97 0.99 0.980.93 0.84 0.88Initial-Upper Case 0.87 0.84 0.85All-Upper Case 0.77 0.6 0.67OverallLower CaseNon-Lower CaseThe score that is most important for IE is theF-measure of recognizing non-lowercase word.
Wefound that the majority of errors involve missingthe first word in a sentence due to the lack of apowerful sentence final punctuation detectionmodule in the case restoration stage.
But it is foundthat such ?errors?
have almost no negative effect onthe following IE tasks.3There is no doubt that the lack of caseinformation from the input text will impact theNLP/IE/QA performance.
The goal of the caserestoration module is to minimize this impact.
Aseries of degradation tests have been run tomeasure the impact.Degradation Tests on IE and ParsingSince IE is the foundation for our QA system, theIE degradation due to the case insensitive inputdirectly affects the QA performance.The IE degradation benchmarking is designed asfollows.
We start with a testing corpus drawn fromnormal case sensitive text.
We then feed the corpusinto the IE engine for benchmarking.
This isnormal benchmarking for case sensitive text inputas a baseline.
After that, we artificially remove thecase information by transforming the corpus into acorpus in all uppercase.
The case restorationmodule is then plugged in to restore the casebefore feeding the IE engine.
By comparingbenchmarking using case restoration with baselinebenchmarking, we can calculate the level ofperformance degradation from the baseline inhandling case insensitive input.For NE, an annotated testing corpus of 177,000words is used for benchmarking (Table 3), usingan automatic scorer following MessageUnderstanding Conference (MUC) NE standards.Table 2: NE Degradation BenchmarkingType P R FNE on case sensitive input 89.1% 89.7% 89.4%NE on case insensitive input usingcase restoration  86.8% 87.9% 87.3%Degradation  2.3% 1.8% 2.1%The overall F-measure for NE degradation, dueto the loss of case information in the incomingcorpus, is 2.1%.
We have also implemented thetraditional NE-retraining approach proposed by[Kubala et al 1998] [Miller et al 2000] [Palmer etal.
2000] and the re-trained NE model leads to3 In fact, positive effects are observed in some cases.
The normalEnglish orthographic rule that the first word be capitalized canconfuse the NE learning system due to the lack of the usualorthographic distinction between a candidate proper name and acommon word.6.3% degradation in the NE F-measure, a drop ofmore than four percentage points when comparedwith the case restoration two-step approach.
Sincethis comparison between two approaches is basedon the same testing corpus using the same system,the conclusion can be derived that the caserestoration approach is clearly better than theretraining approach for NE.Beyond NE, some fundamental InfoXtractsupport  for QA comes from the CE relationshipsand the SVO parsing results.
We benchmarkedtheir degradation as follows.From a processed corpus drawn from the newsdomain, we randomly picked 250 SVO structurallinks and 60 AFFILIATION and POSITIONrelationships for manual checking (Table 3, CORfor Correct, INC for Incorrect, SPU for Spurious,MIS for Missing, and DEG for Degradation).Surprisingly, there is almost no statisticallysignificant difference in the SVO performance.The degradation due to the case restoration wasonly 0.07%.
This indicates that parsing is lesssubject to the case factor to a degree that theperformance differences between a normal casesensitive input and a case restored input are notobviously detectable.Table 3: SVO/CE Degradation BenchmarkingSVO CEBaselineCaseRestored BaselineCaseRestoredCOR 196 195 48 43INC 13 12 0 1SPU 10 10 2 2MIS 31 33DEG10 14DEGP 89.50% 89.86% -0.36% 96.0% 93.5% 2.5%R 81.67% 81.25% 0.42% 82.8% 74.1% 8.7%F 85.41% 85.34% 0.07% 88.9% 82.7% 6.2%The degradation for CE is about 6%.Considering there is absolutely no adaptation ofthe CE module, this degradation is reasonable.5 QA Degradation BenchmarkingThe QA experiments were conducted following theTREC-8 QA standards in the category of 250-byteanswer strings.
In addition to the TREC-8benchmarking standards Mean Reciprocal Rank(MRR), we also benchmarked precision for the topanswer string (Table 4).Table 4: QA Degradation Benchmarking-1Type Top 1 Precision MRRQA on case sensitive corpus  130/198=65.7% 73.9%QA on case insensitive corpus 124/198=62.6% 71.1%Degradation  3.1% 2.8%Comparing QA benchmarks with benchmarksfor the underlying IE engine shows that the limitedQA degradation is in proportion with the limiteddegradation in NE, CE and SVO.
The followingexamples illustrate the chain effect: caserestoration errors  NE/CE/SVO errors  QAerrors.Q137: ?Who is the mayor of Marbella?
?This is a CE question, the decoded CE askingrelationship is CeHead for the location entity?Marbella?.
In QA on the original case sensitivecorpus, the top answer string has a correspondingCeHead relationship extracted as shown below.Input: Some may want to view the results of themuch-publicised activities of the mayor ofMarbella, Jesus Gil y Gil, in cleaning up thetown [NE tagging]Some may want to view the results of themuch-publicised activities of the mayor of<NeCity>Marbella</NeCity> ,<NeMan>Jesus Gil y Gil</NeMan>, incleaning up the town [CE extraction]CeHead: Marbella  Jesus Gil y GilIn contrast, the case insensitive processing isshown below:Input: SOME MAY WANT TO VIEW THERESULTS OF THE MUCH-PUBLICISEDACTIVITIES OF THE MAYOR OFMARBELLA, JESUS GIL Y GIL, INCLEANING UP THE TOWN [case restoration]some may want to view the results of themuch-publicised activities of the mayor ofmarbella , Jesus Gil y Gil, in cleaning up thetown [NE tagging]some may want to view the results of themuch-publicised activities of the mayor ofmarbella , <NeMan>Jesus Gil yGil</NeMan> , in cleaning up the townThe CE module failed to extract the relationshipfor MARBELLA because this relationship isdefined for the entity type NeOrganization orNeLocation which is absent due to the failed caserestoration for ?MARBELLA?.
The next exampleshows an NE error leading to a problem in QA.Q119: ?What Nobel laureate was expelled fromthe Philippines before the conference on EastTimor?
?In question processing, the NE Asking Point isidentified as NePerson.
Because Mairead Maguirewas successfully tagged as NeWoman, the QAsystem got the correct answer string in thefollowing snippet: Immigration officials at theManila airport on Saturday expelled Irish Nobelpeace prize winner Mairead Maguire.
However,the case insensitive processing fails to tag anyNePerson in this snippet.
As a result the systemmisses this answer string.
The process is illustratedbelow.Input: IMMIGRATION OFFICIALS AT THEMANILA AIRPORT ON SATURDAYEXPELLED IRISH NOBEL PEACE PRIZEWINNER MAIREAD MAGUIRE [case restoration]immigration officials at the Manila airporton Saturday expelled Irish Nobel Peace PrizeWinner Mairead Maguire [NE tagging]immigration officials at the<NeCity>Manila</NeCity> airport on<NeDay>Saturday</NeDay> expelled<NeProduct>Irish Nobel Peace Prize WinnerMairead Maguire </NeProduct>As shown, errors in case restoration causemistakes in the NE grouping and tagging: IrishNobel Peace Prize Winner Mairead Maguire  iswrongly tagged as NeProduct.We also found one interesting case where caserestoration actually leads to QA performanceenhancement over the original case sensitiveprocessing.
A correct answer snippet is promotedfrom the 3rd candidate to the top in answeringQ191 ?Where was Harry Truman born??.
Thisprocess is shown below.Input: HARRY TRUMAN (33RD PRESIDENT):BORN MAY 8, 1884, IN LAMAR, MO. [case restoration]Harry Truman ( 33rd President ) : born May8 , 1884  , in Lamar , MO . [NE tagging]<NeMan>Harry Truman</NeMan> (<NeOrdinal>33rd</NeOrdinal> President ) :born <NeDay>May 8 , 1884</NeDay> , in<NeCity>Lamar , MO</NeCity> .As shown, LAMAR, MO gets correctly tagged asNeCity after case restoration.
But LAMAR is mis-tagged as NeOrg in the original case sensitiveprocessing.
The original case sensitive snippet isHarry Truman (33rd President): Born May 8,1884, in Lamar, Mo.
In our NE system, there issuch a learned pattern as follows:X , TwoLetterUpperCase  NeCity.This rule fails to apply to the original text becausethe US state abbreviation appears in a lessfrequently seen format Mo instead of MO.However, the restoration HMM assigns alluppercase to ?MO?
since this is the most frequentlyseen orthography for this token.
This difference ofthe restored case from the original case enables theNE tagger to tag Lamar, MO as ?NeCity?
whichmeets the NE Asking Point constraint?NeLocation?.QA and Case Insensitive QuestionWe also conducted a test on case insensitivequestions in addition to case insensitive corpus bycalling the same case restoration module.Table 5: QA Degradation Benchmarking-2Type Top 1 Precision MRRQA on case sensitive corpus  130/198=65.7% 73.9%QA on case insensitive corpus,with case insensitive question 111/198=56.1% 64.4%Degradation  9.6% 9.5%This research is useful because, when interfacinga speech recognizer to a QA system to acceptspoken questions, the case information is notavailable in the incoming question.4 We want to4 In addition to missing the case information, there are other aspects ofspoken questions that require treatment, e.g., lack of punctuationmarks, spelling mistakes, repetitions.
Whether the restorationapproach is effective calls for more research.know how the same case restoration techniqueapplies to question processing and gauge thedegradation effect on the QA performance(Table 5).We notice that the question processor missedtwo originally detected NE Asking Points and oneAsking Point CE Link.
There are a number of othererrors due to incorrectly restored case, includingnon-asking-point NEs in the question and groupingerrors in shallow parsing as shown below for Q26 :?What is the name of the ?female?
counterpart toEl Nino, which results in cooling temperatures andvery dry weather??
(Notation: NP for Noun Phrase,VG for Verb Group, PP for Prepositional Phraseand AP for Adjective Phrase).Input: WHAT IS THE NAME OF THE"FEMALE" COUNTERPART TO ELNINO ?
? [case restoration]What is the name of the "Female"counterpart to El Nino, ?
? [question shallow parsing]NP[What] VG[is] NP[the name] PP[of the] "AP[Female] " NP[counterpart] PP[to ElNino] , ?
?In the original mixed-case question, after parsing,we get the following basic phrase grouping:NP[What] VG[is] NP[the name] PP[of the " female" counterpart] PP[to El Nino] , ?
?There is only one difference between the case-restored question and the original mixed-casequestion, i.e.
Female vs. female.
This differencecauses the shallow parsing grouping error for thePP of the "female" counterpart.
This error affectsthe weights of the ranking features HeadwordMatching and Phrase-internal Word Order.
As aresult, the following originally correctly identifiedanswer snippet was dropped: the greenhouse effectand El Nino -- as well as its "female" counterpart,La Nina -- have had a profound effect on weathernationwide.As question processing results are the startingpoint and basis for snippet retrieval and featureranking, an error in question processing seems tolead to greater degradation, as seen in almost 10%drop compared with about 3% drop in the casewhen only the corpus is case insensitive.A related explanation for this degradationcontrast is as follows.
Due to the informationredundancy in a large corpus, processing errors insome potential answer strings in the corpus can becompensated for by correctly processed equivalentanswer strings.
This is due to the fact that the sameanswer may be expressed in numerous ways in thecorpus.
Some of those ways may be less subject tothe case effect than others.
Question processingerrors are fatal in the sense that there is noinformation redundancy for its compensation.Once it is wrong, it directs the search for answerstrings in the wrong direction.
Since questionsconstitute a subset of the natural languagephenomena with their own characteristics, caserestoration needs to adapt to this subset for optimalperformance, e.g.
by including more questions inthe case restoration training corpus.6 ConclusionAn effective approach to perform QA on caseinsensitive corpus is presented with very littledegradation (2.8%).
This approach uses a highperformance case restoration module based onHMM as a preprocessor for the NLP/IE processingof the corpus.
There is no need for any changes onthe QA system and the underlying IE engine whichwere originally designed for handling normal, casesensitive corpora.
It is observed that the limitedQA degradation is due to the limited IEdegradation.An observation from the research of handlingcase insensitive questions is that questionprocessing degradation has more seriousconsequence affecting the QA performance.
Thecurrent case restoration training corpus is drawnfrom the general news articles which rarely containquestions.
As a future effort, we plan to focus onenhancing the case restoration performance byincluding as many mixed-case questions aspossible into the training corpus for caserestoration.AcknowledgmentThis work was partly supported by a grant from theAir Force Research Laboratory?s InformationDirectorate (AFRL/IF), Rome, NY, under contractF30602-03-C-0044.
The authors wish to thankCarrie Pine and Sharon Walter of AFRL forsupporting and reviewing this work.ReferencesAbney, S., Collins, M and Singhal.
2000.
A.Answer Extraction.
Proceedings of ANLP-2000, Seattle.Bikel, D.M.
et al 1997.
Nymble: a High-Performance Learning Name-finder.Proceedings of the Fifth Conference on ANLP,Morgan Kaufmann Publishers,  194-201.Bikel, D.M., R. Schwartz, and R.M.
Weischedel.1999.
An Algorithm that Learns What?s in aName.
Machine Learning, Vol.
1,3, 1999,211-231.Chieu, H.L.
and H.T.
Ng.
2002.
Teaching aWeaker Classifier: Named Entity Recognitionon Upper Case Text.
Proceedings of ACL-2002, Philadelphia.Chinchor N. and E. Marsh.
1998.
MUC-7Information Extraction Task Definition(version 5.1), Proceedings of MUC-7.Hovy, E.H., U. Hermjakob, and Chin-Yew Lin.2001.
The Use of External Knowledge ofFactoid QA.
Proceedings of TREC-10, 2001,Gaithersburg, MD, U.S.A..Krupka, G.R.
and K. Hausman.
1998.
IsoQuestInc.
: Description of the NetOwl (TM)Extractor System as Used for MUC-7,Proceedings of MUC-7.Kubala, F., R. Schwartz, R. Stone and R.Weischedel.
1998.
Named Entity Extractionfrom Speech.
Proceedings of DARPABroadcast News Transcription andUnderstanding Workshop.Kupiec J.
1993.
MURAX: A Robust LinguisticApproach For Question Answering Using AnOn-Line Encyclopaedia.
Proceedings of SIGIRPittsburgh, PA.Li, W, R. Srihari, X. Li, M. Srikanth, X. Zhang andC.
Niu.
2002.
Extracting Exact Answers toQuestions Based on Structural Links.Proceedings of Multilingual Summarizationand Question Answering (COLING-2002Workshop), Taipei, Taiwan.Litkowski, K. C. 1999.
Question-Answering UsingSemantic Relation Triples.
Proceedings ofTREC-8, Gaithersburg, MD.Miller, D., S. Boisen, R. Schwartz, R. Stone, andR.
Weischedel.
2000.
Named Entity Extractionfrom Noisy Input: Speech and OCR.Proceedings of ANLP 2000, Seattle.Niu, C., W. Li, J. Ding and R. Srihari.
2003.Orthographic Case Restoration UsingSupervised Learning Without ManualAnnotation.
Proceedings of the 16thInternational FLAIRS Conference 2003,FloridaChincor, N., P. Robinson and E. Brown.
1998.HUB-4 Named Entity Task Definition Version4.8.
(www.nist.gov/speech/tests/bnr/hub4_98/hub4_98.htm)Palmer, D., M. Ostendorf and J.D.
Burger.
2000.Robust Information Extraction fromAutomatically Generated SpeechTranscriptions.
Speech Communications, Vol.32, 2000, 95-109.Pasca, M. and S.M.
Harabagiu.
2001.
HighPerformance Question/Answering.Proceedings of SIGIR 2001, 366-374.Robinson, P., E. Brown, J. Burger, N. Chinchor, A.Douthat, L. Ferro, and L. Hirschman.
1999.Overview: Information Extraction fromBroadcast News.
Proceedings of The DARPABroadcast News Workshop Herndon, Virginia.Srihari, R and W. Li.
2000.
A Question AnsweringSystem Supported by Information Extraction.Proceedings of ANLP 2000, Seattle.Srihari, R., W. Li, C. Niu and T. Cornell.
2003.InfoXtract: A Customizable IntermediateLevel Information Extraction Engine.
HLT-NAACL03 Workshop on The SoftwareEngineering and Architecture of LanguageTechnology Systems (SEALTS).
Edmonton,CanadaVoorhees, E. 1999.
The TREC-8 QuestionAnswering Track Report.
Proceedings ofTREC-8.
Gaithersburg, MD.Voorhees, E. 2000.
Overview of the TREC-9Question Answering Track.
Proceedings ofTREC-9.
Gaithersburg, MD.
