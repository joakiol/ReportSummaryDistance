Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 153?156,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSegmentation for English-to-Arabic Statistical Machine TranslationIbrahim Badr Rabih ZbibComputer Science and Artificial Intelligence LabMassachusetts Institute of TechnologyCambridge, MA 02139, USA{iab02, rabih, glass}@csail.mit.eduJames GlassAbstractIn this paper, we report on a set of ini-tial results for English-to-Arabic StatisticalMachine Translation (SMT).
We show thatmorphological decomposition of the Arabicsource is beneficial, especially for smaller-sizecorpora, and investigate different recombina-tion techniques.
We also report on the useof Factored Translation Models for English-to-Arabic translation.1 IntroductionArabic has a complex morphology compared toEnglish.
Words are inflected for gender, number,and sometimes grammatical case, and various cli-tics can attach to word stems.
An Arabic corpuswill therefore have more surface forms than an En-glish corpus of the same size, and will also be moresparsely populated.
These factors adversely affectthe performance of Arabic?English Statistical Ma-chine Translation (SMT).
In prior work (Lee, 2004;Habash and Sadat, 2006), it has been shown thatmorphological segmentation of the Arabic sourcebenefits the performance of Arabic-to-English SMT.The use of similar techniques for English-to-ArabicSMT requires recombination of the target side intovalid surface forms, which is not a trivial task.In this paper, we present an initial set of experi-ments on English-to-Arabic SMT.
We report resultsfrom two domains: text news, trained on a large cor-pus, and spoken travel conversation, trained on a sig-nificantly smaller corpus.
We show that segmentingthe Arabic target in training and decoding improvesperformance.
We propose various schemes for re-combining the segmented Arabic, and compare theireffect on translation.
We also report on applyingFactored Translation Models (Koehn and Hoang,2007) for English-to-Arabic translation.2 Previous WorkThe only previous work on English-to-Arabic SMTthat we are aware of is by Sarikaya and Deng (2007).It uses shallow segmentation, and does not makeuse of contextual information.
The emphasis of thatwork is on using Joint Morphological-Lexical Lan-guage Models to rerank the output.Most of the related work, though, is on Arabic-to-English SMT.
Lee (2004) uses a trigram languagemodel to segment Arabic words.
She then pro-ceeds to deleting or merging some of the segmentedmorphemes in order to make the segmented Arabicsource align better with the English target.
Habashand Sadat (2006) use the Arabic morphological an-alyzer MADA (Habash and Rambow, 2005) to seg-ment the Arabic source; they propose various seg-mentation schemes.
Both works show that the im-provements obtained from segmentation decrease asthe corpus size increases.
As will be shown later, weobserve the same trend, which is due to the fact thatthe model becomes less sparse with more trainingdata.There has been work on translating from En-glish to other morphologically complex languages.Koehn and Hoang (2007) present Factored Transla-tion Models as an extension to phrase-based statisti-cal machine translation models.
Factored models al-low the integration of additional morphological fea-153tures, such as POS, gender, number, etc.
at the wordlevel on both source and target sides.
The tighter in-tegration of such features was claimed to allow moreexplicit modeling of the morphology, and is betterthan using pre-processing and post-processing tech-niques.
FactoredModels demonstrate improvementswhen used to translate English to German or Czech.3 Arabic Segmentation andRecombinationAs mentioned in Section 1, Arabic has a relativelyrich morphology.
In addition to being inflected forgender, number, voice and case, words attach to var-ious clitics for conjunction (w+ ?and?
)1, the definitearticle (Al+ ?the?
), prepositions (e.g.
b+ ?by/with?,l+ ?for?, k+ ?as?
), possessive pronouns and objectpronouns (e.g.
+ny ?me/my?, +hm ?their/them?).
Forexample, the verbal form wsnsAEdhm and the nomi-nal formwbsyAratnA can be decomposed as follows:(1) a. w+and+s+will+n+we+sAEdhelp+hm+themb.
w+and+b+with+syArcar+At+PL+nA+ourAlso, Arabic is usually written without the diacriticsthat denote the short vowels, and different sourceswrite a few characters inconsistently.
These issuescreate word-level ambiguity.3.1 Arabic Pre-processingDue to the word-level ambiguity mentioned above,but more generally, because a certain string of char-acters can, in principle, be either an affixed mor-pheme or part of the base word, morphologicaldecomposition requires both word-level linguisticinformation and context analysis; simple patternmatching is not sufficient to detect affixed mor-phemes.
To perform pre-translation morphologi-cal decomposition of the Arabic source, we use themorphological analyzer MADA.
MADA uses SVM-based classifiers for features (such as POS, numberand gender, etc.)
to choose among the different anal-yses of a given word in context.We first normalize the Arabic by changing final?Y?
to ?y?
and the various forms of Alif hamza to bare1In this paper, Arabic text is written using BuckwaltertransliterationAlif.
We also remove diacritics wherever they occur.We then apply one of two morphological decompo-sition schemes before aligning the training data:1.
S1: Decliticization by splitting off each con-junction clitic, particle, definite article andpronominal clitic separately.
Note that pluraland subject pronoun morphemes are not split.2.
S2: Same as S1, except that the split clitics areglued into one prefix and one suffix, such thatany given word is split into at most three parts:prefix+ stem +suffix.For example the word wlAwlAdh (?and for his kids?
)is segmented to w+ l+ AwlAd +P:3MS according toS1, and to wl+ AwlAd +P:3MS according to S2.3.2 Arabic Post-processingAs mentioned above, both training and decoding usesegmented Arabic.
The final output of the decodermust therefore be recombined into a surface form.This proves to be a non-trivial challenge for a num-ber of reasons:1.
Morpho-phonological Rules: For example, thefeminine marker ?p?
at the end of a wordchanges to ?t?
when a suffix is attached to theword.
So syArp +P:1S recombines to syArty(?my car?)2.
Letter Ambiguity: The character ?Y?
(AlfmqSwrp) is normalized to ?y?.
In the recom-bination step we need to be able to decidewhether a final ?y?
was originally a ?Y?.
Forexample, mdy +P:3MS recombines to mdAh?its extent?, since the ?y?
is actually a Y; but fy+P:3MS recombines to fyh ?in it?.3.
Word Ambiguity: In some cases, a word canrecombine into 2 grammatically correct forms.One example is the optional insertion of nwnAlwqAyp (protective ?n?
), so the segmentedword lkn +O:1S can recombine to either lknnyor lkny, both grammatically correct.To address these issues, we propose two recombina-tion techniques:1.
R: Recombination rules defined manually.
Toresolve word ambiguity we pick the grammat-ical form that appears more frequently in the154training data.
To resolve letter ambiguity weuse a unigram language model trained on datawhere the character ?Y?
had not been normal-ized.
We decide on the non-normalized from ofthe ?y?
by comparing the unigram probability ofthe word with ?y?
to its probability with ?Y?.2.
T: Uses a table derived from the training setthat maps the segmented form of the word to itsoriginal form.
If a segmented word has morethan one original form, one of them is pickedat random.
The table is useful in recombin-ing words that are split erroneously.
For ex-ample, qrDAy, a proper noun, gets incorrectlysegmented to qrDAn +P:1S which makes its re-combination without the table difficult.3.3 Factored ModelsFor the Factored Translation Models experiment, thefactors on the English side are the POS tags and thesurface word.
On the Arabic side, we use the sur-face word, the stem and the POS tag concatenatedto the segmented clitics.
For example, for the wordwlAwlAdh (?and for his kids?
), the factored words areAwlAd and w+l+N+P:3MS.
We use two languagemodels: a trigram for surface words and a 7-gramfor the POS+clitic factor.
We also use a genera-tion model to generate the surface form from thestem and POS+clitic, a translation table from POSto POS+clitics and from the English surface word tothe Arabic stem.
If the Arabic surface word cannotbe generated from the stem and POS+clitic, we backoff to translating it from the English surface word.4 ExperimentsThe English source is aligned to the segmented Ara-bic target using GIZA++ (Och and Ney, 2000), andthe decoding is done using the phrase-based SMTsystem MOSES (MOSES, 2007).
We use a max-imum phrase length of 15 to account for the in-crease in length of the segmented Arabic.
Tuningis done using Och?s algorithm (Och, 2003) to op-timize weights for the distortion model, languagemodel, phrase translation model and word penaltyover the BLEU metric (Papineni et al, 2001).
Forour baseline system the tuning reference was non-segmented Arabic.
For the segmented Arabic exper-iments we experiment with 2 tuning schemes: T1Scheme Training Set Tuning SetBaseline 34.6% 36.8%R 4.04% 4.65%T N/A 22.1%T + R N/A 1.9%Table 1: Recombination Results.
Percentage of sentenceswith mis-combined words.uses segmented Arabic for reference, and T2 tuneson non-segmented Arabic.
The Factored TranslationModels experiments uses the MOSES system.4.1 Data UsedWe experiment with two domains: text news andspoken dialogue from the travel domain.
For thenews training data we used corpora from LDC2.
Af-ter filtering out sentences that were too long to beprocessed by GIZA (> 85 words) and duplicate sen-tences, we randomly picked 2000 development sen-tences for tuning and 2000 sentences for testing.
Inaddition to training on the full set of 3 million words,we also experimented with subsets of 1.6 millionand 600K words.
For the language model, we used20 million words from the LDC Arabic Gigawordcorpus plus 3 million words from the training data.After experimenting with different language modelorders, we used 4-grams for the baseline system and6-grams for the segmented Arabic.
The Englishsource is downcased and the punctuations are sepa-rated.
The average sentence length is 33 for English,25 for non-segmented Arabic and 36 for segmentedArabic.For the spoken language domain, we use theIWSLT 2007 Arabic-English (Fordyce, 2007) cor-pus which consists of a 200,000 word training set, a500 sentence tuning set and a 500 sentence test set.We use the Arabic side of the training data to trainthe language model and use trigrams for the baselinesystem and a 4-grams for segmented Arabic.
The av-erage sentence length is 9 for English, 8 for Arabic,and 10 for segmented Arabic.2Since most of the data was originally intended for Arabic-to-English translation our test and tuning sets have only onereference1554.2 Recombination ResultsTo test the different recombination schemes de-scribed in Section 3.2, we run these schemes onthe training and development sets of the news data,and calculate the percentage of sentences with re-combination errors (Note that, on average, thereis one mis-combined word per mis-combined sen-tence).
The scores are presented in Table 1.
Thebaseline approach consists of gluing the prefix andsuffix without processing the stem.
T +Rmeans thatthe words seen in the training set were recombinedusing scheme T and the remainder were recombinedusing scheme R. In the remaining experiments weuse the scheme T + R.4.3 Translation ResultsThe 1-reference BLEU score results for the newscorpus are presented in Table 2; those for IWSLT arein Table 3.
We first note that the scores are generallylower than those of comparable Arabic-to-Englishsystems.
This is expected, since only one refer-ence was used to evaluate translation quality andsince translating to a more morphologically com-plex language is a more difficult task, where thereis a higher chance of translating word inflections in-correctly.
For the news corpus, the segmentation ofArabic helps but the gain diminishes as the trainingdata size increases, since the model becomes lesssparse.
This is consistent with the larger gain ob-tained from segmentation for IWSLT.
The segmen-tation scheme S2 performs slightly better than S1.The tuning scheme T2 performs better for the newscorpus, while T1 is better for the IWSLT corpus.It is worth noting that tuning without segmentationhurts the score for IWSLT, possibly because of thesmall size of the training data.
Factored models per-form better than our approach with the large train-ing corpus, although at a significantly higher cost interms of time and required resources.5 ConclusionIn this paper, we showed that making the Arabicmatch better to the English through segmentation,or by using additional translation model factors thatmodel grammatical information is beneficial, espe-cially for smaller domains.
We also presented sev-eral methods for recombining the segmented ArabicLarge Medium SmallTraining Size 3M 1.6M 0.6MBaseline 26.44 20.51 17.93S1 + T1 tuning 26.46 21.94 20.59S1 + T2 tuning 26.81 21.93 20.87S2 + T1 tuning 26.86 21.99 20.44S2 + T2 tuning 27.02 22.21 20.98Factored Models + tuning 27.30 21.55 19.80Table 2: BLEU (1-reference) scores for the News data.No Tuning T1 T2Baseline 26.39 24.67S1 29.07 29.82S2 29.11 30.10 28.94Table 3: BLEU (1-reference) scores for the IWSLT data.target.
Our results suggest that more sophisticatedtechniques, such as syntactic reordering, should beattempted.AcknowledgmentsWe would like to thank Ali Mohammad, Michael Collins andStephanie Seneff for their valuable comments.ReferencesCameron S. Fordyce 2007.
Overview of the 2007 IWSLT Eval-uation Campaign .
In Proc.
of IWSLT 2007.Nizar Habash and Owen Rambow, 2005.
Arabic Tokenization,Part-of-Speech Tagging and Morphological Disambiguationin One Fell Swoop.
In Proc.
of ACL.Nizar Habash and Fatiha Sadat, 2006.
Arabic PreprocessingSchemes for Statistical Machine Translation.
In Proc.
ofHLT.Philipp Koehn and Hieu Hoang, 2007.
Factored TranslationModels.
In Proc.
of EMNLP/CNLL.Young-Suk Lee, 2004.
Morphological Analysis for StatisticalMachine Translation.
In Proc.
of EMNLP.MOSES, 2007.
A Factored Phrase-based Beam-search Decoder for Machine Translation.
URL:http://www.statmt.org/moses/.Franz Och, 2003.
Minimum Error Rate Training in StatisticalMachine Translation.
In Proc.
of ACL.Franz Och and Hermann Ney, 2000.
Improved StatisticalAlignment Models.
In Proc.
of ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu, 2001.
Bleu: a Method for Automatic Evaluation ofMachine Translation.
In Proc.
of ACL.Ruhi Sarikaya and Yonggang Deng 2007.
JointMorphological-Lexical Language Modeling for MachineTranslation.
In Proc.
of NAACL HLT.156
