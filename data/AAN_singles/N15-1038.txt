Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 345?354,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLexicon-Free Conversational Speech Recognition with Neural NetworksAndrew L.
Maas?, Ziang Xie?, Dan Jurafsky, Andrew Y. NgStanford UniversityStanford, CA 94305, USA{amaas, zxie, ang}@cs.stanford.edu, jurafsky@stanford.eduAbstractWe present an approach to speech recogni-tion that uses only a neural network to mapacoustic input to characters, a character-levellanguage model, and a beam search decodingprocedure.
This approach eliminates much ofthe complex infrastructure of modern speechrecognition systems, making it possible to di-rectly train a speech recognizer using errorsgenerated by spoken language understand-ing tasks.
The system naturally handles outof vocabulary words and spoken word frag-ments.
We demonstrate our approach us-ing the challenging Switchboard telephoneconversation transcription task, achieving aword error rate competitive with existing base-line systems.
To our knowledge, this is thefirst entirely neural-network-based system toachieve strong speech transcription results ona conversational speech task.
We analyzequalitative differences between transcriptionsproduced by our lexicon-free approach andtranscriptions produced by a standard speechrecognition system.
Finally, we evaluate theimpact of large context neural network charac-ter language models as compared to standardn-gram models within our framework.1 IntroductionUsers increasingly interact with natural languageunderstanding systems via conversational speech in-terfaces.
Google Now, Microsoft Cortana, andApple Siri are all systems which rely on spokenlanguage understanding (SLU), where transcribing?Authors contributed equally.speech is a single step within a larger system.
Build-ing such systems is difficult because spontaneous,conversational speech naturally contains repetitions,disfluencies, partial words, and out of vocabulary(OOV) words (De Mori et al, 2008; Huang et al,2001).
Moreover, SLU systems must be robust totranscription errors, which can be quite high depend-ing on the task and domain.Modern systems for large vocabulary continuousspeech recognition (LVCSR) use hidden Markovmodels (HMMs) to handle sequence processing,word-level language models, and a pronunciationlexicon to map words into phonetic pronunciations(Saon and Chien, 2012).
Traditional systems useGaussian mixture models (GMMs) to build a map-ping from sub-phonetic states to audio input fea-tures.
The resulting speech recognition system con-tains many sub-components, linguistic assumptions,and typically over ten thousand lines of source code.Within the past few years LVCSR systems improvedby replacing GMMs with deep neural networks(DNNs) (Dahl et al, 2011; Hinton et al, 2012),drawing on early work on with hybrid GMM-NNarchitectures (Bourlard and Morgan, 1993).
BothHMM-GMM and HMM-DNN systems remain dif-ficult to build, and nearly impossible to efficientlyoptimize for downstream SLU tasks.
As a result,SLU researchers typically operate on an n-best listof possible transcriptions and treat the LVCSR sys-tem as a black box.Recently Graves and Jaitly (2014) demonstratedan approach to LVCSR using a neural networktrained with the connectionist temporal classifica-tion (CTC) loss function (Graves et al, 2006).
Us-345ing the CTC loss function the authors built a neuralnetwork which directly maps audio input features toa sequence of characters.
By re-ranking word-leveln-best lists generated from an HMM-DNN systemthe authors obtained competitive results on the WallStreet Journal corpus.Our work builds upon the foundation introducedby Graves and Jaitly (2014).
Rather than reason-ing at the word level, we train and decode our sys-tem by reasoning entirely at the character-level.
Byreasoning over characters we eliminate the needfor a lexicon, and enable transcribing new words,fragments, and disfluencies.
We train a deep bi-directional recurrent neural network (DBRNN) todirectly map acoustic input to characters using theCTC loss function introduced by Graves and Jaitly(2014).
We are able to efficiently and accuratelyperform transcription using only our DBRNN anda character-level language model (CLM), whereasprevious work relied on n-best lists from a baselineHMM-DNN system.
On the challenging Switch-board telephone conversation transcription task, ourapproach achieves a word error rate competitivewith existing baseline HMM-GMM systems.
To ourknowledge, this is the first entirely neural-network-based system to achieve strong speech transcriptionresults on a conversational speech task.Section 2 reviews the CTC loss function and de-scribes the neural network architecture we use.
Sec-tion 3 presents our approach to efficiently performfirst-pass decoding using a neural network for char-acter probabilities and a character language model.Section 4 presents experiments on the Switchboardcorpus to compare our approach to existing LVCSRsystems, and evaluates the impact of different lan-guage models.
In Section 5, we offer insight on howthe CTC-trained system performs speech recogni-tion as compared to a standard HMM-GMM model,and finally conclude in Section 6.2 ModelWe address the complete LVCSR problem.
Oursystem trains on utterances which are labeled byword-level transcriptions and contain no indicationof when words occur within an utterance.
Our ap-proach consists of two neural networks which weintegrate during a beam search decoding procedure.Our first neural network, a DBRNN, maps acousticinput features to a probability distribution over char-acters at each time step.
Our second system compo-nent is a neural network character language model.Neural network CLMs enable us to leverage high or-der n-gram contexts without dramatically increas-ing the number of free parameters in our languagemodel.
To facilitate further work with our approachwe make our source code publicly available.12.1 Connectionist Temporal ClassificationWe train neural networks using the CTC loss func-tion to do maximum likelihood training of lettersequences given acoustic features as input.
Thisis a direct, discriminative approach to building aspeech recognition system in contrast to the gen-erative, noisy-channel approach which motivatesHMM-based speech recognition systems.
Our ap-plication of the CTC loss function follows the ap-proach introduced by Graves and Jaitly (2014), butwe restate the approach here for completeness.CTC is a generic loss function to train systemson sequence problems where the alignment betweenthe input and output sequence are unknown.
CTCaccounts for time warping of the output sequencerelative to the input sequence, but does not modelpossible re-orderings.
Re-ordering is a problem inmachine translation, but is not an issue when work-ing with speech recognition ?
our transcripts providethe exact ordering in which words occur in the inputaudio.Given an input sequence X of length T , CTC as-sumes the probability of a length T character se-quence C is given by,p(C|X) =T?t=1p(ct|X).
(1)This assumes that character outputs at each timestepare conditionally independent given the input.
Thedistribution p(ct|X) is the output of some predictivemodel.CTC assumes our ground truth transcript is a char-acter sequence W with length ?
where ?
?
T .
Asa result, we need a way to construct possibly shorteroutput sequences from our length T sequence of1Available at: deeplearning.stanford.edu/lexfree346character probabilities.
The CTC collapsing func-tion achieves this by introducing a special blanksymbol, which we denote using ?
?, and collapsingany repeating characters in the original length T out-put.
This output symbol contains the notion of junkor other so as to not produce a character in the fi-nal output hypothesis.
Our transcriptsW come fromsome set of symbols ?
?but we reason over ?
= ???
.We denote the collapsing function by ?(?)
whichtakes an input string and produces the unique col-lapsed version of that string.
As an example, hereare the set of strings Z of length T = 3 such that?
(z) = hi, ?z ?
Z:Z = {hhi,hii, hi,h i,hi }.There are a large number of possible length Tsequences corresponding to a final length ?
tran-script hypothesis.
The CTC objective functionLCTC(X,W ) is a likelihood of the correct final tran-script W which requires integrating over the prob-abilities of all length T character sequences CW={C : ?
(C) = W} consistent with W after applyingthe collapsing function,LCTC(X,W ) =?CWp(C|X)=?CWT?t=1p(ct|X).
(2)Using a dynamic programming approach we can ex-actly compute this loss function efficiently as well asits gradient with respect to our probabilities p(ct|X).2.2 Deep Bi-Directional Recurrent NeuralNetworksOur loss function requires at each time t a probabil-ity distribution p(c|xt) over characters c given in-put features xt.
We model this distribution usinga DBRNN because it provides an expressive modelwhich explicitly accounts for the sequential relation-ships that should exist in our task.
Moreover, theDBRNN is a relatively straightforward neural net-work architecture to specify, and allows us to learnparameters from data rather than more explicitlyspecifying how to convert audio features into char-acters.
Figure 1 shows a DBRNN with two hiddenlayers.W (1) W (1) W (1)W (2) W (2) W (2)W (f) W (f)W (b) W (b)W (s) W (s) W (s)+ + +xh(1)h(f)h(b)p(c|x)t?
1 t t + 1Figure 1: Deep bi-directional recurrent neural net-work to map input audio features X to a distribu-tion p(c|xt) over output characters at each timestept.
The network contains two hidden layers with thesecond layer having bi-directional temporal recur-rence.A DBRNN computes the distribution p(c|xt) us-ing a series of hidden layers followed by an outputlayer.
Given an input vector xtthe first hidden layeractivations are a vector computed as,h(1)= ?
(W(1)Txt+ b(1)), (3)where the matrix W(1)and vector b(1)are theweight matrix and bias vector.
The function ?(?
)is a point-wise nonlinearity.
We use ?
(z) =min(max(z, 0), ?).
This is a rectified linear acti-vation function clipped to a maximum possible ac-tivation of ?
to prevent overflow.
Rectified linearhidden units have been show to work well in gen-eral for deep neural networks, as well as for acousticmodeling of speech data (Glorot et al, 2011; Zeileret al, 2013; Dahl et al, 2013; Maas et al, 2013)We select a single hidden layer j of the networkto have temporal connections.
Our temporal hiddenlayer representation h(j)is the sum of two partialhidden layer representations,h(j)t= h(f)t+ h(b)t.(4)The representation h(f)uses a weight matrix W(f)to propagate information forwards in time.
Sim-ilarly, the representation h(b)propagates informa-tion backwards in time using a weight matrix W(b).These partial hidden representations both take inputfrom the previous hidden layer h(j?1)using a weight347matrix W(j),h(f)t= ?
(W(j)Th(j?1)t+W(f)Th(f)t?1+ b(j)),h(b)t= ?
(W(j)Th(j?1)t+W(b)Th(b)t+1+ b(j)).
(5)Note that the recurrent forward and backward hid-den representations are computed entirely inde-pendently from each other.
As with the otherhidden layers of the network we use ?
(z) =min(max(z, 0), ?
).All hidden layers aside from the first hidden layerand temporal hidden layer use a standard denseweight matrix and bias vector,h(i)= ?
(W(i)Th(i?1)+ b(i)).
(6)DBRNNs can have an arbitrary number of hiddenlayers, but we assume that only one hidden layercontains temporally recurrent connections.The model outputs a distribution p(c|xt) over aset of possible characters ?
using a softmax outputlayer.
We compute the softmax layer as,p(c = ck|xt) =exp(?
(W(s)Tkh(:)+ b(s)k))?|?|j=1exp(?
(W(s)Tjh(:)+ b(s)j)),(7)where W(s)kis the k?th column of the output weightmatrix W(s)and b(s)kis a scalar bias term.
The vec-tor h(:)is the hidden layer representation of the finalhidden layer in our DBRNN.We can directly compute a gradient for all weightsand biases in the DBRNN with respect to the CTCloss function and apply batch gradient descent.3 DecodingOur decoding procedure integrates information fromthe DBRNN and language model to form a sin-gle cohesive estimate of the character sequence ina given utterance.
For an input sequence X oflength T our DBRNN produces a set of probabilitiesp(c|xt), t = 1, .
.
.
, T .
Again, the character proba-bilities are a categorical distribution over the symbolset ?.3.1 Decoding Without a Language ModelAs a baseline, we use a simple, greedy approachto decoding the DBRNN outputs (Graves and Jaitly,2014).
The simplest form of decoding does not em-ploy the language model and instead finds the high-est probability character transcription given only theDBRNN outputs.
This process selects a transcripthypothesis W?by making a greedy approximation,W?= argmaxWp(W |X) ?
?
(argmaxCp(C|X))= ?(argmaxCT?t=1p(ct|X)).
(8)This decoding procedure ignores the issue of manytime-level character sequences mapping to the samefinal hypothesis, and instead considers only the mostprobable character at each point in time.
Becauseour model assumes the character labels for eachtimestep are conditionally independent, C?is sim-ply the most probable character at each timestep inour DBRNN output.
As a result, this decoding pro-cedure is very fast to compute, requiring only timeO(T |?|).3.2 Beam Search DecodingTo decode while taking language model probabili-ties into account, we use a beam search to combinea character language model and the outputs of ourDBRNN.
This search-based decoding method doesnot make a greedy approximation and instead as-signs probability to a final hypothesis by integrat-ing over all character sequences consistent with thehypothesis under our collapsing function ?(?).
Al-gorithm 1 outlines our decoding procedure.We note that our decoding procedure is signifi-cantly simpler, and in practice faster, than previousdecoding procedures applied to CTC models.
Thisis due to reasoning at the character level without alexicon so as to not introduce difficult multi-levelconstraints to obey during the decoding search pro-cedure.
While a softmax over words is typicallythe bottleneck in neural network language models,a softmax over possible characters is comparativelycheap to compute.
Our character language model isapplied at every time step, while word models canonly be applied when we consider adding a space orby computing the likelihood of a sequence being theprefix of a word in the lexicon (Graves and Jaitly,2014).
Additionally, our lexicon-free approach re-348Algorithm 1 Beam Search Decoding: Given the likelihoods from our DBRNN and our character languagemodel, for each time step t and for each string s in our current previous hypothesis set Zt?1, we considerextending s with a new character.
Blanks and repeat characters with no separating blank are handled sep-arately.
For all other character extensions, we apply our character language model when computing theprobability of s. We initialize Z0with the empty string ?.
Notation: ??
: character set excluding ?
?, s+ c:concatenation of character c to string s, |s|: length of s, pb(c|x1:t) and pnb(c|x1:t): probability of s endingand not ending in blank conditioned on input up to time t, ptot(c|x1:t): pb(c|x1:t) + pnb(c|x1:t)Inputs CTC likelihoods pctc(c|xt), character language model pclm(c|s)Parameters language model weight ?, insertion bonus ?, beam width kInitialize Z0?
{?
}, pb(?|x1:0)?
1, pnb(?|x1:0)?
0for t = 1, .
.
.
, T doZt?
{}for s in Zt?1dopb(s|x1:t)?
pctc( |xt)ptot(s|x1:t?1) .
Handle blankspnb(s|x1:t)?
pctc(c|xt)pnb(s|x1:t?1) .
Handle repeat character collapsingAdd s to Ztfor c in ??dos+?
s+ cif c 6= st?1thenpnb(s+|x1:t)?
pctc(c|xt)pclm(c|s)?ptot(c|x1:t?1)elsepnb(s+|x1:t)?
pctc(c|xt)pclm(c|s)?pb(c|x1:t?1) .
Repeat characters have ?
?
betweenend ifAdd s+to Ztend forend forZt?
k most probable s by ptot(s|x1:t)|s|?in Zt.
Apply beamend forReturn argmaxs?Ztptot(s|x1:T)|s|?moves the difficulties of handling OOV words dur-ing decoding, which is typically a troublesome issuein speech recognition systems.4 ExperimentsWe perform LVCSR experiments on the 300 hourSwitchboard conversational telephone speech cor-pus (LDC97S62).
Switchboard utterances are takenfrom approximately 2,400 conversations among 543speakers.
Each pair of speakers had never met, andconverse no more than once about a given topic cho-sen randomly from a set of 50 possible topics.
Ut-terances exhibit many rich, complex phenomena thatmake spoken language understanding difficult.
Ta-ble 2 shows example transcripts from the corpus.For evaluation, we report word error rate (WER)and character error rate (CER) on the HUB5Eval2000 dataset (LDC2002S09).
This test set con-sists of two subsets, Switchboard and CallHome.The CallHome subset represents a mismatched testcondition as it was collected from phone conversa-tions among family and friends rather than strangersdirected to discuss a particular topic.
The mismatchmakes the CallHome subset quite difficult overall.The Switchboard evaluation subset is substantiallyeasier, and represents a better match of test data toour training corpus.
We report WER and CER onthe test set as a whole, and additionally report WERfor each subset individually.4.1 Baseline SystemsWe build two baseline LVCSR systems to compareour approach to standard HMM-based approaches.349Method CER EV CH SWBDHMM-GMM 23.0 29.0 36.1 21.7HMM-DNN 17.6 21.2 27.1 15.1HMM-SHF NR NR NR 12.4CTC no LM 27.7 47.1 56.1 38.0CTC+5-gram 25.7 39.0 47.0 30.8CTC+7-gram 24.7 35.9 43.8 27.8CTC+NN-1 24.5 32.3 41.1 23.4CTC+NN-3 24.0 30.9 39.9 21.8CTC+RNN 24.9 33.0 41.7 24.2CTC+RNN-3 24.7 30.8 40.2 21.4Table 1: Character error rate (CER) and word er-ror rate results on the Eval2000 test set.
We re-port word error rates on the full test set (EV) whichconsists of the Switchboard (SWBD) and CallHome(CH) subsets.
As baseline systems we use an HMM-GMM system and HMM-DNN system.
We evaluateour DBRNN trained using CTC by decoding withseveral character-level language models: 5-gram, 7-gram, densely connected neural networks with 1 and3 hidden layers (NN-1, and NN-3), as well as recur-rent neural networks s with 1 and 3 hidden layers.We additionally include results from a state-of-the-art HMM-based system (HMM-DNN-SHF) whichdoes not report performance on all metrics we eval-uate (NR).First, we build an HMM-GMM system using theKaldi open-source toolkit2(Povey et al, 2011).
Thebaseline recognizer has 8,986 sub-phone states and200K Gaussians trained using maximum likelihood.Input features are speaker-adapted MFCCs.
Overall,the baseline GMM system setup largely follows theexisting s5b Kaldi recipe, and we defer to previouswork for details (Vesely et al, 2013).We additionally built an HMM-DNN systemby training a DNN acoustic model using maxi-mum likelihood on the alignments produced by ourHMM-GMM system.
The DNN consists of five hid-den layers, each with 2,048 hidden units, for a totalof approximately 36 million (M) free parameters inthe acoustic model.Both baseline systems use a bigram language2http://kaldi.sf.netmodel built from the 3M words in the Switch-board transcripts interpolated with a second bi-gram language model built from 11M words on theFisher English Part 1 transcripts (LDC2004T19).Both LMs are trained using interpolated Kneser-Ney smoothing.
For context we also include WERresults from a state-of-the-art HMM-DNN systembuilt with quinphone phonetic context and Hessian-free sequence-discriminative training (Sainath et al,2014).4.2 DBRNN TrainingWe train a DBRNN using the CTC loss function onthe entire 300hr training corpus.
The input featuresto the DBRNN at each timestep are MFCCs withcontext window of ?10 frames.
The DBRNN has5 hidden layers with the third containing recurrentconnections.
All layers have 1824 hidden units, giv-ing about 20M trainable parameters.
In preliminaryexperiments we found that choosing the middle hid-den layer to have recurrent connections led to thebest results.The output symbol set ?
consists of 33 charactersincluding the special blank character.
Note that be-cause speech recognition transcriptions do not con-tain proper casing or punctuation, we exclude capi-tal letters and punctuation marks with the exceptionof ?-?, which denotes a partial word fragment, and??
?, as used in contractions such as ?can?t.
?We train the DBRNN from random initial pa-rameters using the gradient-based Nesterov?s accel-erated gradient (NAG) algorithm as this techniqueis sometimes beneficial as compared with standardstochastic gradient descent for deep recurrent neuralnetwork training (Sutskever et al, 2013).
The NAGalgorithm uses a step size of 10?5and a momentumof 0.95.
After each epoch we divide the learning rateby 1.3.
Training for 10 epochs on a single GTX 570GPU takes approximately one week.4.3 Character Language Model TrainingThe Switchboard corpus transcripts alone are toosmall to build CLMs which accurately model gen-eral orthography in English.
To learn how to spellwords more generally we train our CLMs using acorpus of 31 billion words gathered from the web(Heafield et al, 2013).
Our language models usesentence start and end tokens, <s> and </s>, as350well as a <null> token for cases when our contextwindow extends past the start of a sentence.We build 5-gram and 7-gram CLMs with modifiedKneser-Ney smoothing using the KenLM toolkit(Heafield et al, 2013).
Building traditional n-gramCLMs is for n > 7 becomes increasingly difficult asthe model free parameters and memory footprint be-come unwieldy.
Our 7-gram CLM is already 21GB;we were not able to build higher order n-gram mod-els to compare against our neural network CLMs.Following work illustrating the effectiveness ofneural network CLMs (Sutskever et al, 2011) andword-level LMs for speech recognition (Mikolov etal., 2010), we train and evaluate two variants of neu-ral network CLMs: standard feedfoward deep neu-ral networks (DNNs) and a recurrent neural network(RNN).
The RNN CLM takes one character at a timeas input, while the non-recurrent CLM networks usea context window of 19 characters.
All neural net-work CLMs use the rectified linear activation func-tion, and the layer sizes are selected such that eachhas about 5M parameters (20MB).The DNN models are trained using standard back-propagation using Nesterov?s accelerated gradientwith a learning rate of 0.01 and momentum of 0.95and a batch size of 512.
The RNN is trained usingbackpropagation through time with a learning rate of0.001 and batches of 128 utterances.
For both modeltypes we halve the learning rate after each epoch.The DNN models were trained for 10 epochs, andthe RNN models for 5 epochs.All neural network CLMs were trained using acombination of the Switchboard and Fisher train-ing transcripts which in total contain approximately23M words.
We also performed experiments withCLMs trained from a large corpus of web text,but found these CLMs to perform no better thantranscript-derived CLMs for our task.4.4 ResultsAfter training the DBRNN and CLMs we run de-coding on the Eval2000 test set to obtain CER andWER results.
For all experiments using a CLMwe use our beam search decoding algorithm with?
= 1.25, ?
= 1.5 and a beam width of 100.
Wefound that larger beam widths did not significantlyimprove performance.
Table 1 shows results for theDBRNN as well as baseline systems.The DBRNN performs best with the 3 hiddenlayer DNN CLM.
This DBRNN+NN-3 attains bothCER and WER performance comparable to theHMM-GMM baseline system, albeit substantiallybelow the HMM-DNN system.
Neural networksprovide a clear gain as compared to standard n-grammodels when used for DBRNN decoding, althoughthe RNN CLM does not produce any gain over thebest DNN CLM.Without a language model the greedy DBRNNdecoding procedure loses relatively little in terms ofCER as compared with the DBRNN+NN-3 model.However, this 3% difference in CER translates to a16% gap in WER on the full Eval2000 test set.
Gen-erally, we observe that small CER differences trans-late to large WER differences.
In terms of character-level performance it appears as if the DBRNNalone performs well using only acoustic input data.Adding a CLM yields only a small CER improve-ment, but guides proper spelling of words to producea large reduction in WER.5 AnalysisTo better see how the DBRNN performs transcrip-tion we show the output probabilities p(c|x) for anexample utterance in Figure 2.
The model tends tooutput mostly blank characters and only spike longenough for a character to be the most likely sym-bol for a few frames at a time.
The dominance ofthe blank class is not forced, but rather learned bythe DBRNN during training.
We hypothesize thatthis spiking behavior results in more stable resultsas the DBRNN only produces a character when itsconfidence of seeing that character rises above a cer-tain threshold.
Note that this a dramatic contrast toHMM-based LVCSR systems, which, due to the na-ture of generative models, attempt to explain almostall timesteps as belonging to a phonetic substate.Next, we qualitatively compare the DBRNN andHMM-GMM system outputs to better understandhow the DBRNN approach might interact with SLUsystems.
This comparison is especially interestingbecause our best DBRNN system and the HMM-GMM system have comparable WERs, removingthe confound of overall quality when comparing hy-potheses.
Table 2 shows example test set utterancesalong with transcription hypotheses from the HMM-351# Method Transcription(1)Truth yeah i went into the i do not know what you think of fidelity butHMM-GMM yeah when the i don?t know what you think of fidel it even themCTC+CLM yeah i went to i don?t know what you think of fidelity but um(2)Truth no no speaking of weather do you carry a altimeter slash barometerHMM-GMM no i?m not all being the weather do you uh carry a uh helped emitters lastbrahms herCTC+CLM no no beating of whether do you uh carry a uh a time or less barometer(3)Truth i would ima- well yeah it is i know you are able to stay home with themHMM-GMM i would amount well yeah it is i know um you?re able to stay home with themCTC+CLM i would ima- well yeah it is i know uh you?re able to stay home with themTable 2: Example test set utterances with a ground truth transcription and hypotheses from our method(CTC+CLM) and a baseline HMM-GMM system of comparable overall WER.
The words fidelity andbarometer are not in the lexicon of the HMM-GMM system.0 10 20 30time (t)0.51.0p(c|x t)s:?
(s):_____o__hh__________ ____y_eahh___oh yeahp( |xt)p(?
|xt)Figure 2: DBRNN character probabilities over timefor a single utterance along with the per-frame mostlikely character string s and the collapsed output?(s).
Due to space constraints we only show a dis-tinction in line type between the blank symbol andnon-blank symbols.GMM and DBRNN+NN-3 systems.The DBRNN sometimes correctly transcribesOOV words with respect to our audio training cor-pus.
We find that OOVs tend to trigger clusters oferrors in the HMM-GMM system, an observationthat has been systematically explored in previouswork (Goldwater et al, 2010).
As shown in ex-ample utterance (3), HMM-GMM errors can intro-duce word substitution errors which may alter mean-ing whereas the DBRNN system outputs word frag-ments or non-words which are phonetically similarand may be useful input features for SLU systems.Unfortunately the Eval2000 test set does not offer arich set of utterances containing OOVs or fragmentsto perform a deeper analysis.
The HMM-GMM andbest DBRNN system achieve identical WERs on thesubset of test utterances containing OOVs and thesubset of test utterances containing fragments.Finally, we quantitatively compare how characterprobabilities from the DBRNN align with phoneticsegments from the HMM-GMM system.
We gener-ate HMM-GMM forced alignments on a large sam-ple of the training set, and separate utterances intomonophone segments.
For each monophone, wecompute the average character probabilities from theDBRNN by aligning the beginning of each mono-phone segment, treating it as time 0.
We measuretime using feature frames rather than seconds.
Fig-ure 3 shows character probabilities over time for thephones k, sh, w, and uw.Although the CTC model does not explicitly com-pute a forced alignment as part of training, wesee significant rises in character probabilities corre-sponding to particular phones during HMM-GMM-aligned monophone segments.
This indicates thatthe CTC model automatically learns a reasonablealignment of characters to the audio.
Generally, theCTC model tends to produce character spikes to-wards the beginning of monophone segments.
Thisis especially evident in plosive consonants such ask and t. For liquids and glides (r, l, w, y), the CTCmodel does not produce characters until later in themonophone segment.
For vowels the CTC character352?5 0 5 10 15 20 250.050.100.150.200.25 k cek?5 0 5 10 15 20 250.050.100.150.200.25 sh ihst?5 0 5 10 15 20 250.050.100.150.200.25 w w?5 0 5 10 15 20 250.050.100.150.200.25 uw douyFigure 3: Character probabilities from the CTC-trained neural network averaged over monophone segmentscreated by a forced alignment of the HMM-GMM system.
Time is measured in frames, with 0 indicating thestart of the monophone segment.
The vertical dotted line indicates the average duration of the monophonesegment.
We show only characters with non-trivial probability for each phone while excluding the blankand space symbols.probabilities generally rise slightly later in the phonesegment as compared to consonants.
This may occurto avoid the large contextual variations in vowel pro-nunciations at phone boundaries.
For certain conso-nants we observe CTC probability spikes before themonophone segment begins, as is the case for sh.The probabilities for sh additionally exhibit multiplemodes, suggesting that CTC may learn different be-haviors for the two common spellings of the sibilantsh: the letter sequence ?sh?
and the letter sequence?ti?.6 ConclusionWe presented an LVCSR system consisting of twoneural networks integrated via beam search decod-ing that matches the performance of an HMM-GMMsystem on the challenging Switchboard corpus.
Webuilt on the foundation of Graves and Jaitly (2014)to vastly reduce the overall complexity required forLVCSR systems.
Our method yields a completefirst-pass LVCSR system with about 1,000 lines ofcode ?
roughly an order of magnitude less thanhigh performance HMM-GMM systems.
Operat-ing entirely at the character level yields a systemwhich does not require assumptions about a lexiconor pronunciation dictionary, instead learning orthog-raphy and phonics directly from data.
We hope thesimplicity of our approach will facilitate future re-search in improving LVCSR with CTC-based sys-tems and jointly training LVCSR systems for SLUtasks.
DNNs have already shown great results asacoustic models in HMM-DNN systems.
We freethe neural network from its complex HMM infras-tructure, which we view as the first step towardsthe next wave of advances in speech recognition andlanguage understanding.AcknowledgmentsWe thank Awni Hannun for his contributions to thesoftware used for experiments in this work.
We alsothank Peng Qi and Thang Luong for insightful dis-cussions, and Kenneth Heafield for help with theKenLM toolkit.
Our work with HMM-GMM sys-tems was possible thanks to the Kaldi toolkit and itscontributors.
Some of the GPUs used in this workwere donated by the NVIDIA Corporation.
AM wassupported as an NSF IGERT Traineeship Recipientunder Award 0801700.
ZX was supported by anNDSEG Graduate Fellowship.353ReferencesH.
Bourlard and N. Morgan.
1993.
Connectionist SpeechRecognition: A Hybrid Approach.
Kluwer AcademicPublishers, Norwell, MA.G.
E. Dahl, D. Yu, and L. Deng.
2011.
Large vocabularycontinuous speech recognition with context-dependentDBN-HMMs.
In Proc.
ICASSP.G.
E. Dahl, T. N. Sainath, and G. E. Hinton.
2013.
Im-proving Deep Neural Networks for LVCSR using Rec-tified Linear Units and Dropout.
In ICASSP.R.
De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,G.
Riccardi, and G. Tur.
2008.
Spoken languageunderstanding.
Signal Processing Magazine, IEEE,25(3):50?58.X.
Glorot, A. Bordes, and Y Bengio.
2011.
Deep SparseRectifier Networks.
In AISTATS, pages 315?323.S.
Goldwater, D. Jurafsky, and C. Manning.
2010.Which Words are Hard to Recognize?
Prosodic, Lex-ical, and Disfluency Factors That Increase SpeechRecognition Error Rates.
Speech Communications,52:181?200.A.
Graves and N. Jaitly.
2014.
Towards End-to-EndSpeech Recognition with Recurrent Neural Networks.In ICML.A.
Graves, S. Fern?andez, F. Gomez, and J. Schmid-huber.
2006.
Connectionist temporal classification:Labelling unsegmented sequence data with recurrentneural networks.
In ICML, pages 369?376.
ACM.K.
Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.2013.
Scalable modified Kneser-Ney language modelestimation.
In ACL-HLT, pages 690?696, Sofia, Bul-garia.G.
E. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mo-hamed, N. Jaitly, A.
Senior, V. Vanhoucke, P. Nguyen,T.
Sainath, and B. Kingsbury.
2012.
Deepneural networks for acoustic modeling in speechrecognition.
IEEE Signal Processing Magazine,29(November):82?97.X.
Huang, A. Acero, H.-W. Hon, et al 2001.
Spokenlanguage processing, volume 18.
Prentice Hall Engle-wood Cliffs.A.
Maas, A. Hannun, and A. Ng.
2013.
Rectifier Nonlin-earities Improve Neural Network Acoustic Models.
InICML Workshop on Deep Learning for Audio, Speech,and Language Processing.T.
Mikolov, M. Karafi?at, L. Burget, J. Cernock`y, andS.
Khudanpur.
2010.
Recurrent neural network basedlanguage model.
In INTERSPEECH, pages 1045?1048.D.
Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glem-bek, K. Vesel?y, N. Goel, M. Hannemann, P. Motlicek,Y.
Qian, P. Schwarz, J. Silovsky, and G. Stemmer.2011.
The kaldi speech recognition toolkit.
In ASRU.T.
N. Sainath, I. Chung, B. Ramabhadran, M. Picheny,J.
Gunnels, B. Kingsbury, G. Saon, V. Austel, andU.
Chaudhari.
2014.
Parallel deep neural networktraining for lvcsr tasks using blue gene/q.
In INTER-SPEECH.G.
Saon and J. Chien.
2012.
Large-vocabulary con-tinuous speech recognition systems: A look at somerecent advances.
IEEE Signal Processing Magazine,29(6):18?33.I.
Sutskever, J. Martens, and G. E. Hinton.
2011.
Gen-erating text with recurrent neural networks.
In ICML,pages 1017?1024.I.
Sutskever, J. Martens, G. Dahl, and G. Hinton.
2013.On the Importance of Momentum and Initialization inDeep Learning.
In ICML.K.
Vesely, A. Ghoshal, L. Burget, and D. Povey.
2013.Sequence-discriminative training of deep neural net-works.
In Interspeech.M.
D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang,Q.V.
Le, P. Nguyen, A.
Senior, V. Vanhoucke, J. Dean,and G. E. Hinton.
2013.
On Rectified Linear Units forSpeech Processing.
In ICASSP.354
