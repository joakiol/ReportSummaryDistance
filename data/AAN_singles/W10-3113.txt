Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 78?85,Uppsala, July 2010.Using SVMs with the Command Relation Features toIdentify Negated Events in Biomedical LiteratureFarzaneh SarafrazSchool of Computer ScienceUniversity of ManchesterManchester, United Kingdomsarafraf@cs.man.ac.ukGoran NenadicSchool of Computer ScienceUniversity of ManchesterManchester, United Kingdomg.nenadic@manchester.ac.ukAbstractIn this paper we explore the identification ofnegated molecular events (e.g.
protein binding,gene expressions, regulation, etc.)
in biomedi-cal research abstracts.
We construe the prob-lem as a classification task and apply a ma-chine learning (ML) approach that uses lexi-cal, syntactic, and semantic features associatedwith sentences that represent events.
Lexicalfeatures include negation cues, whereas syn-tactic features are engineered from constitu-ency parse trees and the command relation be-tween constituents.
Semantic features includeevent type and participants.
We also consider arule-based approach that uses only the com-mand relation.
On a test dataset, the ML ap-proach showed significantly better results(51% F-measure) compared to the command-based rules (35-42% F-measure).
Training aseparate classifier for each event class provedto be useful, as the micro-averaged F-scoreimproved to 63% (with 88% precision), dem-onstrating the potential of task-specific MLapproaches to negation detection.1 IntroductionWith almost 2000 new papers published everyday, biomedical knowledge is mainly communi-cated through a growing body of research papers.As the amount of textual information increases,the need for sophisticated information extraction(IE) methods are becoming more than evident.
IEmethods rely on a range of language processingmethods such as named entity recognition andparsing to extract the required information in amore structured form which can be used forknowledge exploration and hypothesis genera-tion (Donaldson et al 2003; Natarajan et al2006).Given the large number of publications, theidentification of conflicting or contradicting factsis critical for systematic mining of biomedicalliterature and knowledge consolidation.
Detec-tion of negations is of particular importance forIE methods, as it often can hugely affect thequality of the extracted information.
For exam-ple, when mining molecular events, a key pieceof information is whether the text states that thetwo proteins are or are not interacting, or that agiven gene is or is not expressed.
In recent years,several challenges and shared tasks have in-cluded the extraction of negations, typically aspart of other tasks (e.g.
the BioNLP?09 SharedTask 3 (Kim et al 2009)).Several systems and methods have aimed tohandle negation detection in order to improve thequality of extracted information (Hakenberg etal.
2009; Morante and Daelemans 2009).
Priorresearch on this topic has primarily focused onfinding negated concepts by negation cues andscopes.
These concepts are usually representedby a set of predefined terms, and negation detec-tion typically aims to determine whether a termfalls within the scope of a negation cue.In this paper we address the task of identifi-cation of negated events.
We present a machinelearning (ML) method that combines a set of fea-tures mainly engineered from a sentence parsetree with lexical cues.
More specifically, parse-based features use the notion of the commandrelation that models the scope affected by anelement (Langacker, 1969).
We use molecularevents as a case study and experiment on theBioNLP?09 data, which comprises a gold-standard corpus of research abstracts manuallyannotated for events and negations (Kim et al2009).
The evaluation shows that, by using theproposed approach, negated events can be identi-fied with precision of 88% and recall of 49%(63% F-measure).
We compare these results withtwo rule-based approaches that achieved themaximum F-measure of 42%.78The rest of this paper is organised as follows.Section 2 summarises and reviews previous re-search on negation extraction.
Section 3 definesthe problem and introduces the data used for thecase study.
Section 4 focuses on the ML-basedmethodology for extracting negated events.
Thefinal sections contain the results and discussions.2 Related WorkThere have been numerous contemplations of theconcept of negation (Lawler, 2010), but no gen-eral agreement so far exists on its definition,form, and function.
We adopt here a definition ofnegation as given by Cambridge Encyclopedia ofLanguage Sciences: ?Negation is a comparisonbetween a ?real?
situation lacking some elementand an ?imaginal?
situation that does not lack it?.The imaginal situation is affirmative comparedwith the negative real situation.
The elementwhose polarity differs between the two situationsis the negation target.Negations in natural language can be ex-pressed by syntactically negative expressions, i.e.with the use of negating words such as no, not,never, etc.
The word or phrase that makes thesentence wholly or partially negative is the nega-tion cue and the part of the sentence that is af-fected by the negation cue and has become nega-tive is the negation scope.We briefly review two classes of approachesto detect negations: those aiming at negated con-cepts and those targeting negated events.2.1 Detecting Negated Concepts andPhrasesThere have been a number of approaches sug-gested for detection of negated targets andscopes.
Most of them rely on task-specific, hand-crafted rules of various complexities.
They differin the size and composition of the list of negationcues, and the way to utilise such a list.
Somemethods use parse trees, whilst others use resultsof shallow parsing.Rule-based methods range from simple co-occurrence based approaches to patterns that relyon shallow parsing.
The ?bag-of-words?
ap-proach, looking for proximate co-occurrences ofnegation cues and terms in the same sentence, isprobably the simplest method for finding nega-tions, and is used by many as a baseline method.Many approaches have targeted the clinicaland biomedical domains.
NegEx (Chapman et al2001), for example, uses two generic regular ex-pressions that are triggered by negation phrasessuch as:<negation cue> * <target term><target term> * <negation cue>where the asterisk (*) represents a string of up tofive tokens.
Target terms represent domain con-cepts that are terms from the Unified MedicalLanguage System (UMLS1).
The cue set com-prises 272 clinically-specific negation cues, in-cluding those such as denial of or absence of.Although simple, the proposed approach showedgood results on clinical data (78% sensitivity(recall), 84% precision, and 94% specificity).In addition to concepts that are explicitly ne-gated by negation phrases, Patrick et al (2006)further consider so-called pre-coordinated nega-tive terms (e.g.
headache) that have been col-lected from SNOMED CT2 medical terminology.Similarly, NegFinder uses hand-crafted rules todetect negated UMLS terms, including simpleconjunctive and disjunctive statements (Mutaliket al 2001).
They used a list of 60 negation cues.Tolentino et al (2006), however, show that usingrules on a small set of only five negation cues(no, neither/nor, ruled out, denies, without) canstill be reasonably successful in detecting nega-tions in medical reports (F-score 91%).Huang and Lowe (2007) introduced a negationgrammar that used regular expressions and de-pendency parse trees to identify negation cuesand their scope in the sentence.
They applied therules to a set of radiology reports and reported aprecision of 99% and a recall of 92%.Not many efforts have been reported on usingmachine learning to detect patterns in sentencesthat contain negative expressions.
Still, Moranteand Daelemans (2009), for example, used vari-ous classifiers (Memory-based Learners, SupportVector Machines, and Conditional RandomFields) to detect negation cues and their scope.An extensive list of features included the token?sstem and part-of-speech, as well as those of theneighbouring tokens.
Separate classifiers wereused for detecting negation cues and negationscopes.
The method was applied to clinical text,biomedical abstracts, and biomedical papers withF-scores of 80%, 77%, and 68% respectively.2.2 Detecting Negated EventsSeveral approaches have recently been suggestedfor the extraction of negated events, particularly1http://www.nlm.nih.gov/research/umls/2http://www.snomed.org79in the biomedical domain.
Events are typicallyrepresented via participants (biomedical entitiesthat take part in an event) and event triggers (to-kens that indicate presence of the event).
VanLandeghem et al (2008) used a rule-based ap-proach based on token distances in sentence andlexical information in event triggers to detectnegated molecular events.
Kilicoglu and Bergler(2009), Hakenberg et al (2009), and Sanchez(2007) used a number of heuristic rules concern-ing the type of the negation cue and the type ofthe dependency relation to detect negated mo-lecular events described in text.
For example, arule can state that if the negation cue is ?lack?
or?absence?, then the trigger has to be in theprepositional phrase of the cue; or that if the cueis ?unable?
or ?fail?, then the trigger has to be inthe clausal complement of the cue (Kilicoglu andBergler 2009).
As expected, such approachessuffer from lower recall.MacKinlay et al (2009), on the other hand,use ML, assigning a vector of complex deepparse features (including syntactic predicates tocapture negation scopes, conjunctions and se-mantically negated verbs) to every event trigger.The system achieved an F-score of 36% on thesame dataset as used in this paper.We note that the methods mentioned abovemainly focus on finding negated triggers in orderto detect negated events.
In this paper we explorenot only negation of triggers but also phrases inwhich participants are negated (consider, for ex-ample, ?SLP-76?
in the sentence ?In contrast,Grb2 can be coimmunoprecipitated with Sos1and Sos2 but not with SLP-76.?
)3 Molecular EventsAs a case study, we look at identification of ne-gated molecular events.
In general, molecularevents include various types of reactions thataffect genes and protein molecules.
Each event isof a particular type (e.g.
binding, phosphoryla-tion, regulation, etc.).
Depending on the type,each event may have one or more participatingproteins (sometimes referred to as themes).Regulatory events are particularly complex, asthey can have a cause (a protein or anotherevent) in addition to a theme, which can be eithera protein or another event.
Table 1 shows exam-ples of five events, where participants are bio-medical entities (events 1-3) or other events(events 4 and 5).
Note that a sentence can ex-press more than one molecular event.Identification of molecular events in the litera-ture is a challenging IE task (Kim et al 2009;Sarafraz et al 2009).
For the task of identifyingnegated events, we assume that events have al-ready been identified in text.
Each event is repre-sented by its type, a textual trigger, and one ormore participants or causes (see Table 1).
Sincethe participants of different event types can varyin both their number and type, we consider threeclasses of events to support our analysis (seeSection 5):?
Class I comprises events with exactly oneentity theme (e.g.
transcription, protein ca-tabolism, localization, gene expression,phosphorylation).?
Class II events include binding events only,which have one or more entity participants.?
Class III contains regulation events, whichhave exactly one theme and possibly onecause.
However, the theme and the cause canbe entities or events of any type.The corpus used in this study is provided bythe BioNLP?09 challenge (Kim et al 2009).
Itcontains two sets of biomedical abstracts: a?training?
set (containing 800 abstracts used fortraining and analysis purposes) and a ?develop-ment?
set (containing 150 abstracts used for test-ing purposes only).
Both document sets aremanually annotated with information about en-tity mentions (e.g.
genes and proteins).
Sentencesthat report molecular events are further annotatedwith the corresponding event type, textual triggerand participants.
In total, nine event types are?The effect of this synergism was perceptible at the level of induction of the IL-2 gene.
?Event Trigger Type Participant (theme) CauseEvent 1 ?induction?
Gene expression IL-2?Overexpression of full-length ALG-4 induced transcription of FasL and, consequently, apoptosis.
?Event Trigger Type Participant (theme) CauseEvent 2 ?transcription?
Transcription FasLEvent 3 ?Overexpression?
Gene expression ALG-4Event 4 ?Overexpression?
Positive regulation Event 3Event 5 ?induced?
Positive regulation Event 2 Event 4Table 1: Examples of how molecular events described in text are characterised.80considered (gene expression, transcription, pro-tein catabolism, localization, phosphorylation,binding, regulation, positive regulation, andnegative regulation).
In addition, every event hasbeen tagged as either affirmative (reporting aspecific interaction) or negative (reporting that aspecific interaction has not been observed).Table 2 provides an overview of the twoBioNLP?09 datasets.
We note that only around6% of events are negated.TrainingdataDevelopmentdata Eventclass total negated total  negatedClass I 2,858 131 559 26Class II 887 44 249 15Class III 4,870 440 987 66Total 9,685 615 1,795 107Table 2: Overview of the total number of events andnegated event annotations in the two datasets.4 MethodologyWe consider two approaches to extract negatedevents.
We first discuss a rule-based approachthat uses constituency parse trees and the com-mand relation to identify negated events.
Then,we introduce a ML method that combines lexi-cal, syntactic and semantic features to identifynegated events.
Note that in all cases, input sen-tences have been pre-annotated for entity men-tions, event triggers, types, and participants.4.1 Negation Detection Using the CommandRelation RulesThe question of which parts of a syntactic struc-ture affect the other parts has been extensivelyinvestigated.
Langacker (1969) introduced theconcept of command to determine the scopewithin a sentence affected by an element.
Moreprecisely, if a and b are nodes in the constituencyparse tree of a sentence, then a X-commands biff the lowest ancestor of a with label X is alsoan ancestor of b.
Note that the command relationis not symmetrical.
Langacker observed thatwhen a S-commands b, then a affects the scopecontaining b.
For simplicity, we say ?command?when we mean S-command.To determine whether token a commands to-ken b, given the parse tree of a sentence, we usea simple algorithm introduced by McCawley(1993): trace up the branches of the constituencyparse tree from a until you hit a node that is la-belled X.
If b is reachable by tracing down thebranches of the tree from that node, then a X-commands b; otherwise, it does not.We hypothesise that if a negation cue com-mands an event trigger or participant, then theassociated event is negated.4.2 Negation Detection Using MachineLearning on Parse Tree FeaturesGiven a sentence that describes an event, we fur-ther construe the negation detection problem as aclassification task: the aim is to classify the eventas affirmative or negative.
We explore both asingle SVM (support vector machine) classifierfor all events and three separate SVMs for eachof the event classes.
The following features havebeen engineered from an event-representing sen-tence:1.
Event type (one of the nine types as definedin BioNLP?09);2.
Whether the sentence contains a negationcue from the cue list;3.
The negation cue itself (if present);4.
The part-of-speech (POS) tag of the negationcue;5.
The POS tag of the event trigger;6.
The POS tag of the participants of the event.If the participant is another event, the POStag of the trigger of that event is used;7.
The parse node type of the lowest commonancestor of the trigger and the cue (i.e.
thetype of the smallest phrase that contains boththe trigger and the cue, e.g.
S, VP, PP, etc.);8.
Whether or not the negation cue commandsany of the participants; nested events (forClass III) are treated as above (i.e.
as beingrepresented by their triggers);9.
Whether or not the negation cue commandsthe trigger;10.
The parse-tree distance between the eventtrigger and the negation cue.We use a default value (null) where none ofthe other values apply (e.g.
when there is no cuein feature 3, 4, 7).
These features have been usedto train four SVMs on the training dataset: onemodelled all events together, and the othersmodelled the three event classes separately.5 ResultsAll the results refer to the methods applied on thedevelopment dataset (see Table 2).
If the nega-tion detection task is regarded as an informationextraction task of finding positive instances (i.e.81negated events), then precision, recall, and F-score would be appropriate measures.
If we con-sider the classification aspect of the task, speci-ficity is more appropriate if true negative hits areconsidered as valuable as true positive ones.
Wetherefore use the following metrics to evaluatethe two methods:Precision= TPTP+FPRecall=Sensitivity= TPTP+FNF1= 2?
Precision?
RecallPrecision+RecallSpecificity= TNTN+FPwhere TP denotes the number of true positives(the number of correctly identified negatedevents), FN is the number of false negatives (thenumber of negated events that have been re-ported as affirmative), with TN and FP definedaccordingly.Two sets of negation cues were used in orderto compare their influence.
A smaller set wasderived from related work, whereas additionalcues were semi-automatically extracted by ex-ploring the training data.
The small negation cueset contains 14 words3, whereas the larger nega-tion cue set contains 32 words4.
As expected, thelarger set resulted in increased recall, but de-creased precision.
However, the effects on the F-score were typically not significant.
The resultsare only shown using the larger cue set.The texts were processed using the GENIAtagger (Tsuruoka and Tsujii 2005).We used con-stituency parse trees automatically produced bytwo different constituency parsers reported in(McClosky et al 2006) and (Bikel 2004).
Nomajor differences were observed in the resultsusing the two parsers.
The data shown in the re-sults are produced by the former.5.1 Baseline ResultsOur baseline method relies on an implementationof the NegEx algorithm as explained in Section2.1.
Event triggers were used as negation targetsfor the algorithm.
An event is then considered tobe negated if the trigger is negated; otherwise it3Negation cues in this set include: no, not, none,negative, without, absence, fail, fails, failed, fail-ure, cannot, lack, lacking, lacked.4Negation cues in this set include the smaller set and18 task-specific words: inactive, neither, nor, in-hibit, unable, blocks, blocking, preventing, pre-vents, absent, never, unaffected, unchanged, im-paired, little, independent, except, and exception.is affirmative.
The results (see Table 3) are sub-stantially lower than those reported for NegEx onclinical data (specificity of 94% and sensitivityof 78%).
For comparison, the table also providesan even simpler baseline approach that tags asnegated any event whose associated sentencecontains any negation cue word.Approach P R F1 Spec.any negation cue present 20% 78% 32% 81%NegEx 36% 37% 36% 93%Table 3: Baseline results.
(NegEx and a ?bag-of-words?
approach)5.2 Rules Based on the Command RelationTable 4 shows the results of applying the S-command relation rule for negation detection.We experimented with three possible ap-proaches: an event is considered negated if- the negation cue commands any eventparticipant in the parse tree;- the negation cue commands the eventtrigger in the tree;- the negation cue commands both.Approach P R F1 Spec.negation cue commandsany participant 23% 76% 35% 84%negation cuecommands trigger 23% 68% 34% 85%negation cuecommands both 23% 68% 35% 86%Table 4: Performance when only the S-commandrelation is used.Compared with the baseline methods, the rulesbased on the command relation did not improvethe performance.
While precision was low(23%), recall was high (around 70%), indicatingthat in the majority of cases there is an S-command relation in particular with the partici-pants (the highest recall).
We also note a signifi-cant drop in specificity, as many affirmativeevents have triggers/participants S-commandedby a negation cue (not ?linked?
to a given event).5.3 Machine Learning ResultsAll SVM classifiers have been trained on thetraining dataset using a Python implementationof SVM Light using the linear kernel and thedefault parameters (Joachims 1999).
Table 5shows the results of the single SVM classifierthat has been trained for all three event classestogether (applied on the development data).82Compared to previous methods, there was sig-nificant improvement in precision, while recallwas relatively low.
Still, the overall F-measurewas significantly better compared with the rule-based methods (51% vs. 35%).Feature set P R F1 Spec.Features 1-7 43% 8% 14% 99.2%Features 1-8 73% 19% 30% 99.3%Features 1-9 71% 38% 49% 99.2%Features 1-10 76% 38% 51% 99.2%Table 5: The results of the single SVM classifier.
Fea-tures 1-7 are lexical and POS tag-based features.
Fea-ture 8 models whether the cue S-commands any of theparticipants.
Feature 9 is related to the cue S-commanding the trigger.
Feature 10 is the parse-treedistance between the cue and trigger.We first experimented with the effect of differ-ent types of feature on the quality of the negationprediction.
Table 5 shows the results of the firstclassifier with an incremental addition of lexicalfeatures, parse tree-related features, and finally acombination of those with the command relationbetween the negation cue and event trigger andparticipants.
It is worth noting that both precisionand recall improved as more features are added.We also separately trained classifiers on thethree classes of events (see Table 6).
This furtherincreased the performance: compared with theresults of the single classifier, the F1 micro-average improved from 51% to 63%, with simi-lar gains for both precision and recall.Event class P R F1 Spec.Class I(559 events) 94% 65% 77% 99.8%Class II(249 events) 100% 33% 50% 100%Class III(987 events) 81% 44% 57% 99.2%Micro Average(1,795 events) 88% 49% 63% 99.4%Macro Average(3 classes) 92% 47% 62% 99.7%Table 6: The results of the separate classifiers on dif-ferent classes using common features.6 DiscussionAs expected, approaches that focus only on eventtriggers and their surface distances from negationcues proved inadequate for biomedical scientificarticles.
Low recall was mainly caused by manyevent triggers being too far from the negation cueto be detected as within the scope.Furthermore, compared to clinical notes, forexample, sentences that describe molecularevents are significantly more complex.
For ex-ample, the event-describing sentences in thetraining data have on average 2.6 event triggers.The number of events per sentence is evenhigher, as the same trigger can indicate multipleevents, sometimes with opposite polarities.
Con-sider for example the sentence?We also demonstrate that the IKK complex,but not p90 (rsk), is responsible for the in vivophosphorylation of I-kappa-B-alpha mediatedby the co-activation of PKC and calcineurin.
?Here, the trigger (phosphorylation) is linked withone affirmative and one negative regulatoryevent by two different molecules, hence trigger-ing two events of opposite polarities.These findings, together with previous work,suggested that for any method to effectively de-tect negations, it should be able to link the nega-tion cue to the specific token, event trigger orentity name in question.
Therefore, more com-plex models are needed to capture the specificstructure of the sentence as well as the composi-tion of the interaction and the arrangement of itstrigger and participants.By combining several feature types (lexical,syntactic and semantic), the machine learningapproach proved to provide significantly betterresults.
In the incremental feature addition explo-ration process, adding the cue-commands-participant feature had the greatest effect on theF-score, suggesting the significance of treatingevent participants.
We note, however, that manyof the previous attempts focus on event triggersonly, although participants do play an importantrole in the detection of negations in biomedicalevents and thus should be used as negation tar-gets instead of or in addition to triggers.
It is in-teresting that adding parse-tree distance betweenthe trigger and negation cue improves precisionby 5%.Differences in event classes (in the numberand type of participants) proved to be important.Significant improvement in performance wasobserved when individual classifiers were trainedfor the three event classes, suggesting that eventswith different numbers or types of participantsare expressed differently in text, at least whennegations are considered.
Class I events are thesimplest (one participant only), so it was ex-pected that negated events in this class would be83the easiest to detect (F-score of 77%).
Class IInegated events (which can have multiple partici-pants), demonstrated the lowest recall (33%).
Alikely reason is that the feature set used is notsuitable for multi-participant events: for exam-ple, feature 8 focuses on the negation cue com-manding any of the participants, and not all ofthem.
It is surprising that negated regulationevents (Class III) were not the most difficult toidentify, given their complexity.We applied the negation detection on thetype, trigger and participants of pre-identifiedevents in order to explore the complexity of ne-gations, unaffected by automatic named entityrecognition, event trigger detection, participantidentification, etc.
As these steps are typicallyperformed before further characterisation ofevents, this assumption is not superficial andsuch information can be used as input to the ne-gation detection module.
MacKinlay et al (2009)also used gold annotations as input for negationdetection, and reported precision, recall, and F-score of 68%, 24%, and 36% respectively on thesame dataset (compared to 88%, 49% and 63%in our case).
The best performing negation detec-tion approach in the BioNLP?09 shared task re-ported recall of up to 15%, but with overall eventdetection sensitivity of 33% (Kilicoglu and Ber-gler 2009) on a ?test?
dataset (different from thatused in this study).
This makes it difficult to di-rectly compare their results to our work, but wecan still provide some rough estimates: had allevents been correctly identified, their negationdetection approach could have reached 45% re-call (compared to 49% in our case).
With preci-sion of around 50%, their projected F-score,again assuming perfect event identification,could have been in the region of 50% (comparedto 63% in our case).The experiments with rules that were basedon the command relations have proven to be ge-neric, providing very high recall (~70%) but withpoor precision.
Although only the results with S-command relations have been reported here (seeTable 4), we examined other types of commandrelation, namely NP-, PP-, SBAR-, and VP-command.
The only variation able to improveprediction accuracy was whether the cue VP-commands any of the participants, with an F-score of 42%, which is higher than the resultsachieved by the S-command (F-score of 35%).The S-command relation was used in the SVMmodules as VP-command did not make the re-sults significantly better.One of the issues we faced was the manage-ment of multi-token and sub-token entities andtriggers (e.g.
alpha B1 and alpha B2 in ?alphaB1/alpha B2 ratio?, which will be typically to-kenised as ?alpha?, ?B1/alpha?, and ?B2?).
Inour approach, we considered all the entities thatare either multi-token or sub-token.
However, ifwe assign participants that are both multi-tokenand sub-token simultaneously to events and ex-tract similar features for the classifier from themas from simple entities, the F-score is reduced byabout 2%.
It would be probably better to assign anew category to those participants and add a newvalue for them specifically in every feature.7 ConclusionsGiven the number of published articles, detectionof negations is of particular importance for bio-medical IE.
Here we explored the identificationof negated molecular events, given their triggers(to characterise event type) and participants.
Weconsidered two approaches: 5  a rule-based ap-proach using constituency parse trees and thecommand relation to identify negation cues andscopes, and a machine learning method thatcombines a set of lexical, syntactic and semanticfeatures engineered from the associated sentence.When compared with a regular-expression-basedbaseline method (NegEx-like), the proposed MLmethod achieved significantly better results: 63%F-score with 88% precision.
The best resultswere obtained when separate classifiers weretrained for each of the three event classes, as dif-ferences between them (in the number and typeof participants) proved to be important.The results presented here were obtained byusing the ?gold?
event annotations as the input.
Itwould be interesting to explore the impact oftypically noisy automatic event extraction onnegation identification.
Furthermore, an immedi-ate future step would be to explore class-specificfeatures (e.g.
type of theme and cause for ClassIII events, and whether the cue S-commands allparticipants for Class II events).
In addition, inthe current approach we used constituency parsetrees.
Our previous attempts to identify molecu-lar events (Sarafraz et al 2009) as well as thosediscussed in Section 2 use dependency parsetrees.
A topic open for future research will be tocombine information from both dependency andconstituency parse trees as features for detectingnegated events.5Available at http://bit.ly/bzBaUX84AcknowledgmentsWe are grateful to the organisers of BioNLP?09for providing the annotated data.ReferencesDaniel Bikel.
2004.
A Distributional Analysis of aLexicalized Statistical Parsing.
Proc.
Conferenceon Empirical Methods in Natural Language.Wendy Chapman.
2001.
A Simple Algorithm forIdentifying Negated Findings and Diseases in Dis-charge Summaries.
Journal of Biomedical Infor-matics, 34(5):301-310.Ian Donaldson, Martin, J., de Bruijn, B., Wolting, C.,Lay, V., Tuekam, B., Zhang, S., Baskin, B., Bader,G.
D., Michalickova, K., Pawson, T. and Hogue, C.W.
2003.
PreBIND and Textomy--mining the bio-medical literature for protein-protein interactionsusing a support vector machine.
BMC Bioinf.
4: 11.J?rg Hakenberg, Ill?s Solt, Domonkos Tikk, LuisTari, Astrid Rheinl?nder, Quang L. Ngyuen,Graciela Gonzalez and Ulf Leser.
2009.
Molecularevent extraction from link grammar parse trees.BioNLP?09: Proceedings of the Workshop onBioNLP.
86-94.Yang Huang and Henry J. Lowe.
2007.
A Novel Hy-brid Approach to Automated Negation Detection inClinical Radiology Reports.
Journal of the Ameri-can Medical Informatics Association, 14(3):304-311.Thorsten Joachims.
1999.
Making large-Scale SVMLearning Practical.
Advances in Kernel Methods -Support Vector Learning, B. Sch?lkopf and C.Burges and A. Smola (ed.)
MIT-Press, MA.Halil Kilicoglu, and Sabine Bergler.
2009.
Syntacticdependency based heuristics for biological eventextraction.
BioNLP?09: Proceedings of the Work-shop on BioNLP.
119-127.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yo-shinobu Kano, Jun?ichi Tsujii.
2009.
Overview ofBioNLP?09 shared task on event extraction.BioNLP?09: Proceedings of the Workshop onBioNLP.
1-9.Sofie Van Landeghem, Yvan Saeys, Bernard DeBaets and Yves Van de Peer.
2008.
Extracting Pro-tein-Protein Interactions from Text using Rich Fea-ture Vectors and Feature Selection.
Proceedings ofthe Third International Symposium on SemanticMining in Biomedicine.
77-84.Ronald Langacker.
1969.
On Pronominalization andthe Chain of Command.
In D. Reibel and S.
Schane(eds.
), Modern Studies in English, Prentice-Hall,Englewood Cliffs, NJ.
160?186.John Lawler.
2010.
Negation and Negative Polarity.The Cambridge Encyclopedia of the Language Sci-ences.
Patrick Colm Hogan (ed.)
Cambridge Uni-versity Press.
Cambridge, UK.Andrew MacKinlay, David Martinez and TimothyBaldwin.
2009.
Biomedical Event Annotation withCRFs and Precision Grammars.
BioNLP?09: Pro-ceedings of the Workshop on BioNLP.
77-85.James McCawley.
1993.
Everything that Linguistshave Always Wanted to Know about Logic ButWere Ashamed to Ask.
2nd edition.
The Universityof Chicago Press.
Chicago, IL.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective Self-Training for Parsing.Proceedings of HLT/NAACL 2006.
152-159.Roser Morante and Walter Daelemans.
2009.
AMetalearning Approach to Processing the Scope ofNegation.
CoNLL ?09: Proceedings of the 13thConference on Computational Natural LanguageLearning.
21-29.Pradeep Mutalik, Aniruddha Deshpande, and PrakashM.
Nadkarni.
2001.
Use of General-purpose Nega-tion Detection to Augment Concept Indexing ofMedical Documents: A Quantitative Study usingthe UMLS.
Journal of the American Medical In-formatics Association : JAMIA.
8(6):598-609.Jeyakumar Natarajan, Berrar, D., Dubitzky, W., Hack,C., Zhang, Y., DeSesa, C., Van Brocklyn, J. R. andBremer, E.G.
2006.
Text mining of full-text journalarticles combined with gene expression analysisreveals a relationship between sphingosine-1-phosphate and invasiveness of a glioblastoma cellline.
BMC Bioinformatics.
7: 373.Jon Patrick, Yefeng Wang, and Peter Budd.
2006.Automatic Mapping Clinical Notes to MedicalTerminologies.
Proc.
Of the 2006 Australian Lan-guage Technology Workshop.
75-82.Olivia Sanchez.
2007.
Text mining applied to biologi-cal texts: beyond the extraction of protein-proteininteractions.
PhD Thesis.Farzaneh Sarafraz, James Eales, Reza Mohammadi,Jonathan Dickerson, David Robertson and GoranNenadic.
2009.
Biomedical Event Detection usingRules, Conditional Random Fields and Parse TreeDistances.
BioNLP?09: Proceedings of the Work-shop on BioNLP.Herman Tolentino, Michael Matters, Wikke Walop,Barbara Law, Wesley Tong, Fang Liu, Paul Fon-telo, Katrin Kohl, and Daniel Payne.
2006.
ConceptNegation in Free Text Components of VaccineSafety Reports.
AMIA Annual Symposium proc.Yoshimasa Tsuruoka, and Jun?ichi Tsujii.
2005.
Bidi-rectional Inference with the Easiest-First Strategyfor Tagging Sequence Data.
Proceedings ofHLT/EMNLP 2005, 467-474.85
