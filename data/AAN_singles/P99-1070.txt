Relating Probabilistic Grammars and AutomataSteven Abney  Dav id  McA l les ter  Fernando Pere i raAT&T Labs-Research180 Park AveFlorham Park NJ 07932{abney, dmac, pereira}@research.att.comAbst ractBoth probabilistic context-free grammars(PCFGs) and shift-reduce probabilistic push-down automata (PPDAs) have been used forlanguage modeling and maximum likelihoodparsing.
We investigate the precise relationshipbetween these two formalisms, showing that,while they define the same classes of probabilis-tic languages, they appear to impose differentinductive biases.1 In t roduct ionCurrent work in stochastic language modelsand maximum likelihood parsers falls into twomain approaches.
The first approach (Collins,1998; Charniak, 1997) uses directly the defini-tion of stochastic grammar, defining the prob-ability of a parse tree as the probability thata certain top-down stochastic generative pro-cess produces that tree.
The second approach(Briscoe and Carroll, 1993; Black et al, 1992;Magerman, 1994; Ratnaparkhi, 1997; Chelbaand Jelinek, 1998) defines the probability of aparse tree as the probability that a certain shift-reduce stochastic parsing automaton outputsthat tree.
These two approaches correspond tothe classical notions of context-free grammarsand nondeterministic pushdown automata re-spectively.
It is well known that these two clas-sical formalisms define the same language class.In this paper, we show that probabilistic ontext-free grammars (PCFGs) and probabilistic push-down automata (PPDAs) define the same classof distributions on strings, thus extending theclassical result to the stochastic ase.
We alsotouch on the perhaps more interesting ques-tion of whether PCFGs and shift-reduce pars-ing models have the same inductive bias withrespect o the automatic learning of model pa-rameters from data.
Though we cannot providea definitive answer, the constructions we use toanswer the equivalence question involve blow-ups in the number of parameters in both direc-tions, suggesting that the two models imposedifferent inductive biases.We are concerned here with probabilisticshift-reduce parsing models that define prob-ability distributions over word sequences, andin particular the model of Chelba and Je-linek (1998).
Most other probabilistic shift-reduce parsing models (Briscoe and Carroll,1993; Black et al, 1992; Magerman, 1994; Rat-naparkhi, 1997) give only the conditional prob-ability of a parse tree given a word sequence.Collins (1998) has argued that those models failto capture the appropriate dependency relationsof natural anguage.
Furthermore, they are notdirectly comparable to PCFGs, which defineprobability distributions over word sequences.To make the discussion somewhat more con-crete, we now present a simplified version of theChelba-Jelinek model.
Consider the followingsentence:The small woman gave the fat man hersandwich.The model under discussion is based on shift-reduce PPDAs.
In such a model, shift transi-tions generate the next word w and its associ-ated syntactic category X and push the pair(X, w) on the stack.
Each shift transitionis followed by zero or more reduce transitionsthat combine topmost stack entries.
For exam-ple the stack elements (Det, the), (hdj, small),(N, woman) can be combined to form the singleentry (NP, woman) representing the phrase "thesmall woman".
In general each stack entry con-sists of a syntactic ategory and a head word.After generating the prefix "The small womangave the fat man" the stack might contain thesequence (NP, woman)<Y, gave)(NP, man).
TheChelba-Jelinek model then executes a shift tran-542S --+ (S, admired)(S, admired) --+ (NP, Mary)(VP, admired)(VP, admired) -+ (V, admired)(Np, oak)(NP, oak) -+ (Det, the)(N, oak)(N, oak) -+ (Adj, towering> (N, oak>(N, oak> -~ (Adj, strong>(N, oak>(N, oak) -+ (hdj, old>(N, oak)(NP, Mary) -+ Mary(N, oak) -+ oakFigure 1: Lexicalized context-free grammarsition by generating the next word.
This isdone in a manner similar to that of a trigrammodel except that, rather than generate thenext word based on the two preceding words, itgenerates the next word based on the two top-most stack entries.
In this example the Chelba-Jelinek model generates the word "her" from(V, gave)(NP, man) while a classical trigrammodel would generate "her" from "fat man".We now contrast Chelba-Jelinek style mod-els with lexicalized PCFG models.
A PCFG isa context-free grammar in which each produc-tion is associated with a weight in the interval\[0, 1\] and such that the weights of the produc-tions from any given nonterminal sum to 1.
Forinstance, the sentenceMary admired the towering strong old oakcan be derived using a lexicalized PCFG basedon the productions in Figure 1.
Productionprobabilities in the PCFG would reflect he like-lihood that a phrase headed by a certain wordcan be expanded in a certain way.
Since it canbe difficult to estimate fully these likelihoods,we might restrict ourselves to models based onbilexical relationships (Eisner, 1997), those be-tween pairs of words.
The simplest bilexical re-lationship is a bigram statistic, the fraction oftimes that "oak" follows "old".
Bilexical rela-tionships for a PCFG include that between thehead-word of a phrase and the head-word of anon-head immediate constituent, for instance.In particular, the generation of the above sen-tence using a PCFG based on Figure 1 wouldexploit a bilexical statistic between "towering"and "oak" contained in the weight of the fifthproduction.
This bilexical relationship between"towering" and "oak" would not be exploited ineither a trigram model or in a Chelba-Jelinekstyle model.
In a Chelba-Jelinek style modelone must generate "towering" before generating"oak" and then "oak" must be generated from(Adj, strong), (Adj, old).
In this example theChelba-Jelinek model behaves more like a clas-sical trigram model than like a PCFG model.This contrast between PPDAs and PCFGsis formalized in theorem 1, which exhibits aPCFG for which no stochastic parameterizationof the corresponding shift-reduce parser yieldsthe same probability distribution over strings.That is, the standard shift-reduce translationfrom CFGs to PDAs cannot be generalized tothe stochastic ase.We give two ways of getting around the abovedifficulty.
The first is to construct a top-downPPDA that mimics directly the process of gen-erating a PCFG derivation from the start sym-bol by repeatedly replacing the leftmost non-terminal in a sentential form by the right-handside of one of its rules.
Theorem 2 statesthat any PCFG can be translated into a top-down PPDA.
Conversely, theorem 3 states thatany PPDA can be translated to a PCFG, notjust those that are top-down PPDAs for somePCFG.
Hence PCFGs and general PPDAs de-fine the same class of stochastic languages.Unfortunately, top-down PPDAs do not al-low the simple left-to-right processing that mo-tivates shift-reduce PPDAs.
A second wayaround the difficulty formalized in theorem 1is to encode additional information about thederivation context with richer stack and statealphabets.
Theorem 7 shows that it is thuspossible to translate an arbitrary PCFG to ashift-reduce PPDA.
The construction requires afair amount of machinery including proofs thatany PCFG can be put in Chomsky normal form,that weights can be renormalized to ensure thatthe result of grammar transformations can bemade into PCFGs, that any PCFG can be putin Greibach normal form, and, finally, that aGreibach normal form PCFG can be convertedto a shift-reduce PPDA.The construction also involves a blow-up inthe size of the shift-reduce parsing automaton.This suggests that some languages that are con-cisely describable by a PCFG are not conciselydescribable by a shift-reduce PPDA, hence thatthe class of PCFGs and the class of shift-reducePPDAs impose different inductive biases on the543CF languages.
In the conversion from shift-reduce PPDAs to PCFGs, there is also a blow-up, if a less dramatic one, leaving open the pos-sibility that the biases are incomparable, andthat neither formalism is inherently more con-cise.Our main conclusion is then that, while thegenerative and shift-reduce parsing approachesare weakly equivalent, they impose different in-ductive biases.2 P robab i l i s t i c  and  WeightedGrammarsFor the remainder of the paper, we fix a terminalalphabet E and a nonterminal alphabet N, towhich we may add auxiliary symbols as needed.A weighted context-free grammar (WCFG)consists of a distinguished start symbol S E Nplus a finite set of weighted productions of theform X -~ a, (alternately, u : X --~ a), whereX E N, a E (Nt2E)* and the weight u is a non-negative real number.
A probabilistic ontext-free grammar (PCFG) is a WCFG such that forall X, )-~u:x-~a u = 1.
Since weights are non-negative, this also implies that u <_ 1 for anyindividual production.A PCFG defines a stochastic process withsentential forms as states, and leftmost rewrit-ing steps as transitions.
In the more generalcase of WCFGs, we can no longer speak ofstochastic processes; but weighted parse treesand sets of weighted parse trees are still well-defined notions.We define a parse tree to be a tree whosenodes are labeled with productions.
Supposenode ~ is labeled X -~ a\[Y1,...,Yn\], where wewrite a\[Y1,... ,Yn\] for a string whose nonter-minal symbols are Y1,...,Y~.
We say that ~'snonterminal label is X and its weight is u. Thesubtree rooted at ~ is said to be rooted in X.
~ iswell-labeled just in case it has n children, whosenonterminal labels are Y1,.
.
.
,  Yn, respectively.Note that a terminal node is well-labeled onlyif a is empty or consists exclusively of terminalsymbols.
We say a WCFG G admits a tree djust in case all nodes of d are well-labeled, andall labels are productions of G. Note that norequirement is placed on the nonterminal of theroot node of d; in particular, it need not be S.We define the weight of a tree d, denotedWa(d), or W(d) if G is clear from context, to bethe product of weights of its nodes.
The depthr(d) of d is the length of the longest path fromroot to leaf in d. The root production it(d) is thelabel of the root node.
The root symbol p(d) isthe left-hand side of ~r(d).
The yield a(d) ofthe tree d is defined in the standard way as thestring of terminal symbols "parsed" by the tree.It is convenient to treat the functions 7r, p,a, and r as random variables over trees.
Wewrite, for example, {p = X} as an abbreviationfor {dip(d)= X}; and WG(p = X) representsthe sum of weights of such trees.
If the sumdiverges, we set WG(p = X) = oo.
We callIIXHG = WG(p = X) the norm of X,  and IIGII =IISlla the norm of the grammar.A WCFG G is called convergent if \[\[G\[\[ < oo.If G is a PCFG then \[\[G\[\[ = WG(p "- S )  < 1,that is, all PCFGs are convergent.
A PCFGG is called consistent if \]\]GII = 1.
A sufficientcondition for the consistency of a PCFG is givenin (Booth and Thompson, 1973).
If (I) and ?
aretwo sets of parse trees such that 0 < WG(~) <co we define PG((I)\]~) to be WG(~Nqt)/WG(kO).For any terminal string y and grammar G suchthat 0 < WG(p -- S) < co we define PG(Y) tobe Pa(a = YIP = S).3 Stochast i c  Push-Down AutomataWe use a somewhat nonstandard efinition ofpushdown automaton for convenience, but allour results hold for a variety of essentially equiv-alent definitions.
In addition to the terminalalphabet ~, we will use sets of stack symbolsand states as needed.
A weighted push-downautomaton (WPDA) consists of a distinguishedstart state q0, a distinguished start stack symbolX0 and a finite set of transitions of the followingform where p and q are states, a E E L.J {e}, Xand Z1, ..., Zn are stack symbols, and w is anonnegative r al weight:x,  pa~ Zl ... Zn, qA WPDA is a probabilistic push-down automa-ton (PPDA) if all weights are in the interval\[0, 1\] and for each pair of a stack symbol X anda state q the sum of the weights of all transitionsof the form X,p ~ Z1 ...Z=, q equals 1.
A ma-chine configuration is a pair (fl, q) of a finitesequence fl of stack symbols (a stack) and a ma-chine state q.
A machine configuration is calledhalting if the stack is empty.
If M is a PPDAcontaining the transition X,p ~ Z1...Zn,qthen any configuration of the form (fiX, p) has544probability w of being transformed into the con-figuration (f~Z1...Zn, q> where this transfor-mation has the effect of "outputting" a if a ?
e.A complete xecution of M is a sequence of tran-sitions between configurations starting in theinitial configuration <X0, q0> and ending in aconfiguration with an empty stack.
The prob-ability of a complete xecution is the productof the probabilities of the individual transitionsbetween configurations in that execution.
Forany PPDA M and y E E* we define PM(Y) tobe the sum of the probabilities of all completeexecutions outputting y.
A PPDA M is calledconsistent if )-~ye~* PM(Y) = 1.We first show that the well known shift-reduce conversion of CFGs into PDAs can notbe made to handle the stochastic ase.
Given a(non-probabilistic) CFG G in Chomsky normalform we define a (non-probabilistic) shift-reducePDA SIt(G) as follows.
The stack symbols ofSIt(G) are taken to be nonterminals of G plusthe special symbols T and ?.
The states ofSR(G) are in one-to-one correspondence withthe stack symbols and we will abuse notationby using the same symbols for both states andstack symbols.
The initial stack symbol is 1and the initial state is (the state correspondingto) _L.
For each production of the form X --+ ain G the PDA SIt(G) contains all shift transi-tions of the following formY,Z-~ YZ, XThe PDA SR(G) also contains the following ter-mination transitions where S is the start symbolof G.E 1, S -+, TI,T -~,TNote that if G consists entirely of productions ofthe form S -+ a these transitions uffice.
Moregenerally, for each production of the form X -+YZ in G the PDA SR(G) contains the followingreduce transitions.Y, Z -~, XAll reachable configurations are in one of thefollowing four forms where the first is the initialconfiguration, the second is a template for allintermediate configurations with a E N*, andthe last two are terminal configurations.<1, 1>, <11., x>, <I,T>, T>Furthermore, a configuration of the form(l_l_a, X) can be reached after outputting y ifand only if aX  :~ y.
In particular, the machinecan reach configuration (?_L, S) outputting yif and only if S :~ y.
So the machine SR(G)generates the same language as G.We now show that the shift-reduce transla-tion of CFGs into PDAs does not generalize tothe stochastic ase.
For any PCFG G we definethe underlying CFG to be the result of erasingall weights from the productions of G.Theorem 1 There exists a consistent PCFG Gin Chomsky normal .form with underlying CFGG' such that no consistent weighting M of thePDA SR(G ~) has the property that PM(Y) =Pa(u) for all U eTo prove the theorem take G to be the fol-lowing grammar.1_ 1_S -~ AX1, S 3+ BY1X, -~ CX2, X2 -~ CAYl Cy2, Y2 A, C BA-~ a, S -~ b, C-~ cNote that G generates acca and bccb eachwith probability ?.
Let M be a consistentPPDA whose transitions consist of some weight-ing of the transitions of SR(G').
We will as-sume that PM(Y) = PG(Y) for all y E E*and derive a contradiction.
Call the nonter-minals A, B, and C preterminals.
Note thatthe only reduce transitions in SR(G ~) com-bining two preterminals are C, A -~,X2 andC, B -~,Y2.
Hence the only machine configu-ration reachable after outputting the sequenceace is (.I__LAC, C>.
If PM(acca) - -  ?
andPM(accb) -- 0 then the machine in configuration(.I_?AC, C> must deterministically move to con-figuration (I?ACC, A>.
But this implies thatconfiguration ( I IBC,  C> also deterministicallymoves to configuration <?
?BCC, A> so we havePM(bccb) -= 0 which violates the assumptionsabout M. ,,Although the standard shift-reduce transla-tion of CFGs into PDAs fails to generalize tothe stochastic ase, the standard top-down con-version easily generalizes.
A top-down PPDAis one in which only ~ transitions can cause thestack to grow and transitions which output aword must pop the stack.545Theorem 2 Any string distribution definableby a consistent PCFG is also definable by a top-down PPDA.Here we consider only PCFGs in Chom-sky normal form--the generalization to arbi-trary PCFGs is straightforward.
Any PCFGin Chomsky normal form can be translated toa top-down PPDA by translating each weightedproduction of the form X --~ YZ  to the set ofexpansion moves of the form W, X ~ WZ, Yand each production of the form X -~ a to theset of pop moves of the form Z, X 72-'~, Z.
?We also have the following converse of theabove theorem.Theorem 3 Any string distribution definableby a consistent PPDA is definable by a PCFG.The proof, omitted here, uses a weighted ver-sion of the standard translation of a PDA intoa CFG followed by a renormalization step usinglemma 5.
We note that  it does in general in-volve an increase in the number of parametersin the derived PCFG.In this paper we are primarily interested inshift-reduce PPDAs which we now define for-mally.
In a shift-reduce PPDA there is a one-to-one correspondence b tween states and stacksymbols and every transition has one of the fol-lowing two forms.Y, Za-~YZ,  X a?EEgWY, Z -~+ ,XTransitions of the first type are called shifttransitions and transitions of the second typeare called reduce transitions.
Shift transitionsoutput a terminal symbol and push a singlesymbol on the stack.
Reduce transitions aree-transitions that combine two stack symbols.The above theorems leave open the question ofwhether shift-reduce PPDAs can express arbi-trary context-free distributions.
Our main the-orem is that they can.
To prove this some ad-ditional machinery is needed.4 Chomsky  Normal  FormA PCFG is in Chomsky normal form (CNF) ifall productions are either of the form X -St a,a E E or X -~ Y1Y2, Y1,Y2 E N. Our nexttheorem states, in essence, that any PCFG canbe converted to Chomsky normal form.Theorem 4 For any consistent PCFG G withPG(e) < 1 there exists a consistent PCFG C(G)in Chomsky normal form such that, for all y EE+:Pa(y) - ea(yly # e)PC(G)(Y)  -- 1 - Pa(e )To prove the theorem, note first that, withoutloss of generality, we can assume that all pro-ductions in G are of one of the forms X --~ YZ,X -5t Y, X -~ a, or X -Y+ e. More specifi-cally, any production ot in one of these formsmust have the form X -5t ?rfl where a and flare nonempty strings.
Such a production canbe replaced by X -~ AB, A -~ a, and B 2+ flwhere A and B are fresh nonterminal symbols.By repeatedly applying this binarization trans-formation we get a grammar in the desired formdefining the same distribution on strings.We now assume that all productions of  Gare in one of the above four forms.
This im-plies that a node in a G-derivation has at mosttwo children.
A node with two children willbe called a branching node.
Branching nodesmust be labeled with a production of the formX -~ YZ.
Because G can contain produc-tions of the form X --~ e there may be ar-bitrarily large G-derivations with empty yield.Even G-derivations with nonempty ield maycontain arbitrarily large subtrees with emptyyield.
A branching node in the G-derivationwill be called ephemeral if either of its chil-dren has empty yield.
Any G-derivation d withla(d)l _ 2 must contain a unique shallowestnon-ephemeral branching node, labeled by someproduction X ~ YZ.
In this case, definefl(d) = YZ.
Otherwise (la(d)l < 2), let fl(d) =a(d).
We say that a nonterminal X is nontrivialin the grammar G if Pa(a # e I P = X) > O.We now define the grammar G' to consist of allproductions of the following form where X, Y,and Z are nontrivial nonterminals ofG and a isa terminal symbol appearing in G.X PG(~=YZ~p=x, ~#~) YZX PG(~=a 12+=x, ~?~) aWe leave it to the reader to verify that G' hasthe property stated in theorem 4.
?The above proof of theorem 4 is non-constructive in that it does not provide any546way of computing the conditional probabilitiesPG(Z = YZ  I p = x ,  # and Pa(Z =a \[ p = X, a ?
e).
However, it is notdifficult to compute probabilities of the formPG(?
\[ p = X,  r <_ t+ 1) from probabili-ties of the form PG((I) \] p = X, v _< t), andPG(?
I P = X)  is the limit as t goes to infinityof Pa((I )\] p= X, r_< t).
We omit the detailshere.from X equals 1:= ~:x-~\[Y1 ..... y.\] u ~--  E .x-,oIv, ..... Y.l II lla= ..... y .
\ ]u l -LwG(p== wo(p=x)Wa(p= X)- 15 Renormal i za t ionA nonterminal X is called reachable in a gram-mar G if either X is S or there is some (re-cursively) reachable nonterminal Y such that Gcontains a production of the form Y -~ a wherecontains X.
A nonterminal X is nonemptyin G if G contains X -~ a where u > 0 and acontains only terminal symbols, or G containsX -~ o~\[Y1, .
.
.
,  Yk\] where u > 0 and each1~ is (recursively) nonempty.
A WCFG G isproper if every nonterminal is both reachableand nonempty.
It is possible to efficiently com-pute the set of reachable and nonempty non-terminals in any grammar.
Furthermore, thesubset of productions involving only nontermi-nals that are both reachable and nonempty de-fines the same weight distribution on strings.So without loss of generality we need only con-sider proper WCFGs.
A reweighting of G is anyWCFG derived from G by changing the weightsof the productions of G.Lemma 5 For any convergent proper WCFGG, there exists a reweighting G t of G such thatG ~ is a consistent PCFG such that for all ter-minal strings y we have PG' (Y) = Pa (Y).Proof."
Since G is convergent, and every non-terminal X is reachable, we must have IIXIla <oo.
We now renormalize all the productionsfrom X as follows.
For each production X -~a\[Y1, .
.
.
,  Yn\] we replace u by?
= II IIGIIXIlaTo show that G' is a PCFG we must showthat the sum of the weights of all productionsFor any parse tree d admitted by G letd ~ be the corresponding tree admitted by G ~,that is, the result of reweighting the pro-ductions in d. One can show by induc-tion on the depth of parse trees that ifp(d) = X then Wc,(d') = \[-~GWG(d).Therefore IIXIIG, = ~~{d\[p(d)=X} WG,(d ' )  -~~ ~{alo(e)=x} Wa(d) = = 1.
In par-ticular, Ilaql = I lS l la,-  1, that is, G' is consis-tent.
This implies that for any terminal stringY we have PG'(Y) = l i -~Wa,(a = y, p = S) =Wa,(a = y, p = S).
Furthermore, for any treed with p(d) = S we have Wa,(d') = ~\[~cWa(d)and so WG,(a = y, p = S) - ~WG(a  =y, p = S) = Pc(Y).
"6 Gre ibach  Normal  FormA PCFG is in Greibach normal form (GNF) ifevery production X -~ a satisfies (~ E EN*.The following holds:Theorem 6 For any consistent PCFG G inCNF there exists a consistent PCFG G ~ in GNFsuch that Pc,(Y) = Pa(Y) for y e E*.Proof: A left corner G-derivation from X toY is a G-derivation from X where the leftmostleaf, rather than being labeled with a produc-tion, is simply labeled with the nonterminalY.
For example, if G contains the productionsX ~ YZ and Z -~ a then we canconstruct  aleft corner G-derivation from X to Y by build-ing a tree with a root labeled by X Z.~ YZ ,  aleft child labeled with Y and a right child la-beled with Z -~ a.
The weight of a left cornerG-derivation is the product of the productionson the nodes.
A tree consisting of a single nodelabeled with X is a left corner G-derivation fromX toX .For each pair of nonterminals X, Y in Gwe introduce a new nonterminal symbol X/Y .547The H-derivations from X/Y  will be in oneto one correspondence with the left-corner G-derivations from X to Y.
For each productionin G of the form X ~ a we include the followingin H where S is the start symbol of G:S --~ a S /XWe also include in H all productions of the fol-lowing form where X is any nonterminal in G:x /xIf G consists only of productions of the formS -~ a these productions uffice.
More gener-ally, for each nonterminal X/Y  of H and eachpair of productions U ~ YZ,  W ~-~ a we in-clude in H the following:X/Y  ~2 a Z /W X/UBecause of the productions X/X  -~ e, WH(# :X /X)  > 1 , and H is not quite in GNF.
Thesetwo issues will be addressed momentarily.Standard arguments can be used to showthat the H-derivations from X/Y  are in one-to-one correspondence with the left corner G-derivations from X to Y.
Furthermore, this one-to-one correspondence preserves weight--if  d isthe H-derivation rooted at X/Y  correspondingto the left corner G-derivation from X to Y thenWH (d) is the product of the weights of the pro-ductions in the G-derivation.The weight-preserving one-to-one correspon-dence between left-corner G-derivations from Xto Y and H-derivations from X/Y  yields thefollowing.WH ( ao~ ): ~'~(S_U+aS/X)EHUWH(~r : Ollp--- S /X)Po(a )Theorem 5 implies that we can reweight theproper subset of H (the reachable and nonemptyproductions of H) so as to construct a consistentPCFG g with Pj((~) = PG(~).
To prove theo-rem 6 it now suffices to show that the produc-tions of the form X/X  -~ e can be eliminatedfrom the PCFG J.
Indeed, we can eliminatethe e productions from J in a manner similarto that used in the proof of theorem 4.
A nodein an J-derivation is ephemeral if it is labeledX -~ e for some X.
We now define a function 7on J-derivations d as follows.
If the root of d islabeled with X -~ aYZ then we have four sub-cases.
If neither child of the root is ephemeralthen 7(d) is the string aYZ.
If only the left childis ephemeral then 7(d) is aZ.
If only the rightchild is ephemeral then 7(d) is aY and if bothchildren are ephemeral then 7(d) is a. Analo-gously, if the root is labeled with X -~ aY, then7(d) is aY if the child is not ephemeral and aotherwise.
If the root is labeled with X -~ ethen 7(d) is e.A nonterminal X in K will be called triviali fP j (7= e I P =X)  = 1.
We now define thefinal grammar G' to consist of all productionsof the following form where X, Y, and Z arenontrivial nonterminals appearing in J and a isa terminal symbol appearing in J .X Pj(a=a I__~=X, "y??)
aX pj(a=aY~_~=X, "yCe) aYX PJ(a=aYZl-~ p=X' ~?)
aYZAs in section 4, for every nontrivial nonterminalX in K and terminal string (~ we have PK (a =(~ I P= X) = P j (a= a I P= X, a ~ e).
Inparticular, since Pj(e) = PG(() = 0, we havethe following:== P j (a=a lp=S )= Pj(a)= Pa( )The PCFG K is the desired PCFG in Greibachnormal form.
?The construction in this proof is essen-tially the standard left-corner transformation(Rosenkrantz and II, 1970), as extended by Sa-lomaa and Soittola (1978, theorem 2.3) to alge-braic formal power series.7 The Main TheoremWe can now prove our main theorem.Theorem 7 For any consistent PCFG G thereexists a shift-reduce PPDA M such thatPM(Y) = PG(Y) for all y E ~*.Let G be an arbitrary consistent PCFG.
Bytheorems 4 and 6~ we can assume that G con-sists of productions of the form S -~ e and548S l~w St plus productions in Greibach normalform not mentioning S. We can then replacethe rule S 1_:+~ S ~ with all rules of the formS 0-__~)~' a where G contains S ~ ~' -+ a.
We nowassume without loss of generality that G con-sists of a single production of the form S -~ eplus productions in Greibach normal form notmentioning S on the right hand side.The stack symbols of M are of the form W~where ce E N* is a proper suffix of the right handside of some production in G. For example, ifG contains the production X -~ aYZ then thesymbols of M include Wyz,  Wy, and We.
Theinitial state is Ws and the initial stack symbol is?.
We have assumed that G contains a uniqueproduction of the form S -~ e. We include thefollowing transition in M corresponding to thisproduction.A_,Ws~,TThen, for each rule of the form X -~ a~ in Gand each symbol of the form Wx,~ we includethe following in M:Z, Wx.
~ ZWx., WzWe also include all "post-processing" rules ofthe following form:Wx~W~ W~~.,1 ?,W~ ~,TI , T  -:+,TNote that all reduction transitions are determin-istic with the single exception of the first rulelisted above.
The nondeterministic shift tran-sitions of M are in one-to-one correspondencewith the productions of G. This yields the prop-erty that PM(Y) = PG(Y).
?8 Conc lus ionsThe relationship between PCFGs and PPDAsis subtler than a direct application of the clas-sical constructions relating general CFGs andPDAs.
Although PCFGs can be concisely trans-lated into top-down PPDAs, we conjecture thatthere is no concise translation of PCFGs intoshift-reduce PPDAs.
Conversely, there appearsto be no concise translation of shift-reduce PP-DAs to PCFGs.
Our main result is that PCFGsand shift-reduce PPDAs are intertranslatable,hence weakly equivalent.
However, the non-conciseness ofour translations i consistent withthe view that stochastic top-down generationmodels are significantly different from shift-reduce stochastic parsing models, affecting theability to learn a model from examples.Re ferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
TheTheory of Parsing, Translation and Compiling,volume I. Prentice-Hall, Englewood Cliffs, NewJersey.Ezra Black, Fred Jelinek, John Lafferty, DavidMagerman, Robert Mercer, and Salim Roukos.1992.
Towards history-based grammars: Usingricher models for probabilistic parsing.
In Pro-ceedings of the 5th DARPA Speech and NaturalLanguage Workshop.Taylor Booth and Richard Thompson.
1973.
Apply-ing probability measures to abstract languages.IEEE Transactions on Computers, C-22(5):442-450.Ted Briscoe and John Carroll.
1993.
Generalizedprobabilistic LR parsing of natural language (cor-pora) with unification-based grammars.
Compu-tational Linguistics, 19(1):25-59.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.In Fourteenth National Conference on ArtificialIntelligence, pages 598-603.
AAAI Press/MITPress.Ciprian Chelba and Fred Jelinek.
1998.
Exploit-ing syntactic structure for language modeling.
InCOLING-ACL '98, pages 225-231.Michael Collins.
1998.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Jason Eisner.
1997.
Bilexical grammars and a cubic-time probabilistic parser.
In Proceedings of theInternational Workshop on Parsing Technologies.David M. Magerman.
1994.
Natural Language Pars-ing as Statistical Pattern Recognition.
Ph.D. the-sis, Department of Computer Science, StanfordUniversity.Adwait Ratnaparkhi.
1997.
A linear oberved timestatistical parser based on maximum entropymodels.
In Claire Cardie and Ralph Weischedel,editors, Second Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-2),Somerset, New Jersey.
Association For Computa-tional Linguistics.Daniel J. Rosenkrantz and Philip M. Lewis II.
1970.Deterministic left corner parser.
In IEEE Con-ference Record of the 11th Annual Symposium onSwitching and Automata Theory, pages 139-152.Arto Salomaa nd Matti Soittola.
1978.
Automata-Theoretic Aspects of Formal Power Series.Springer-Verlag, New York.549
