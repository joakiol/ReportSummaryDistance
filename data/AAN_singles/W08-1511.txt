Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 60?63Manchester, August 2008A Small-Vocabulary Shared Task for Medical Speech TranslationManny Rayner1, Pierrette Bouillon1, Glenn Flores2, Farzad Ehsani3Marianne Starlander1, Beth Ann Hockey4, Jane Brotanek2, Lukas Biewald51 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland{Emmanuel.Rayner,Pierrette.Bouillon}@issco.unige.chMarianne.Starlander@eti.unige.ch2 UT Southwestern Medical Center, Children?s Medical Center of Dallas{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu3 Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089, USAfarzad@fluentialinc.com4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000bahockey@ucsc.edu5 Dolores Labslukeab@gmail.comAbstractWe outline a possible small-vocabularyshared task for the emerging medicalspeech translation community.
Data wouldconsist of about 2000 recorded and tran-scribed utterances collected during an eval-uation of an English ?
Spanish versionof the Open Source MedSLT system; thevocabulary covered consisted of about 450words in English, and 250 in Spanish.
Thekey problem in defining the task is to agreeon a scoring system which is acceptableboth to medical professionals and to thespeech and language community.
We sug-gest a framework for defining and admin-istering a scoring system of this kind.1 IntroductionIn computer science research, a ?shared task?
is acompetition between interested teams, where thegoal is to achieve as good performance as possibleon a well-defined problem that everyone agrees towork on.
The shared task has three main compo-nents: training data, test data, and an evaluationmetric.
Both test and training data are dividedup into sets of items, which are to be processed.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.The evaluation metric defines a score for each pro-cessed item.
Competitors are first given the train-ing data, which they use to construct and/or traintheir systems.
They are then evaluated on the testdata, which they have not previously seen.In many areas of speech and language process-ing, agreement on a shared task has been a majorstep forward.
Often, it has in effect created a newsubfield, since it allows objective comparison ofresults between different groups.
For example, itis very common at speech conference to have spe-cial sessions devoted to recognition within a par-ticular shared task database.
In fact, a conferencewithout at least a couple of such sessions wouldbe an anomaly.
A recent success story in languageprocessing is the Recognizing Textual Entailment(RTE) task1.
Since its inception in 2004, this hasbecome extremely popular; the yearly RTE work-shop now attracts around 40 submissions, and errorrates on the task have more than halved.Automatic medical speech translation wouldclearly benefit from a shared task.
As was madeapparent at the initial 2006 workshop in NewYork2, nearly every group has both a unique ar-chitecture and a unique set of data, essentiallymaking comparisons impossible.
In this note, wewill suggest an initial small-vocabulary medical1http://www.pascal-network.org/Challenges/RTE/2http://www.issco.unige.ch/pub/SLT workshop proceedings book.pdf60shared task.
The aspect of the task that is hard-est to define is the evaluation metric, since thereunfortunately appears to be considerable tensionbetween the preferences of medical professionalsand speech system implementers.
Medical profes-sionals would prefer to carry out a ?deep?
evalu-ation, in terms of possible clinical consequencesfollowing from a mistranslation.
System evalua-tors will on the other hand prefer an evaluationmethod that can be carried out quickly, enablingfrequent evaluations of evolving systems.
The planwe will sketch out is intended to be a compromisebetween these two opposing positions.The rest of the note is organised as follows.Section 2 describes the data we propose to use,and Section 3 discusses our approach to evaluationmetrics.
Section 4 concludes.2 DataThe data we would use in the task is for the English?
Spanish language pair, and was collected us-ing two different versions of the MedSLT system3.In each case, the scenario imagines an English-speaking doctor conducting a verbal examinationof a Spanish-speaking patient, who was assumedto be have visited the doctor because they weredisplaying symptoms which included a sore throat.The doctor?s task was to use the translation sys-tem to determine the likely reason for the patient?ssymptoms.The two versions of the system differed interms of the linguistic coverage offered.
Themore restricted version supported a minimal rangeof English questions (vocabulary size, about 200words), and only allowed the patient to respondusing short phrases (vocabulary size, 100 words).Thus for example the doctor could ask ?How longhave you had a sore throat?
?, and the patient wouldrespond Hace dos d?
?as (?for two days?).
Theless restricted version supported a broader rangeof doctor questions (vocabulary size, about 450words), and allowed the patient to respond usingboth short phrases and complete sentences (vocab-ulary size, about 225 words).
Thus in responseto ?How long have you had a sore throat?
?, thepatient could say either Hace dos d?
?as (?for twodays?)
or Tengo dolor en la garganta hace dos d?
?as(?I have had a sore throat for two days?
).Data was collected in 64 sessions, carried out3http://www.issco.unige.ch/projects/medslt/over two days in February 2008 at the Universityof Texas Medical Center, Dallas.
In each session,the part of the ?doctor?
was played by a real physi-cian, and the part of the ?patient?
by a Spanish-speaking interpreter.
This resulted in 1005 En-glish utterances, and 967 Spanish utterances.
Allspeech data is available in SPHERE-headed form,and totals about 90 MB.
A master file, organised inspreadsheet form, lists metadata for each recordedfile.
This includes a transcription, a possible validtranslation (verified by a bilingual translator), IDsfor the ?doctor?, the ?patient?, the session and thesystem version, and the preceding context.
Con-text is primarily required for short answers, andconsists of the most recent preceding doctor ques-tion.3 Evaluation metricsThe job of the evaluation component in the sharedtask is to assign a score to each translated utter-ance.
Our basic model will be the usual one forshared tasks in speech and language.
Each pro-cessed utterance will be assigned to a category;each category will be associated with a specifiedscore; the score for a complete testset will the sumof the scores for all of its utterances.
We thus havethree sub-problems: deciding what the categoriesare, deciding how to assign a category to a pro-cessing utterance, and deciding what scores to as-sociate with each category.3.1 Defining categoriesIf the system attempts to translate an utterance,there are a priori three things that can happen:it can produce a correct translation, an incorrecttranslation, or no translation.
Medical speechtranslation is a safety-critical problem; a mistrans-lation may have serious consequences, up to andincluding the death of the patient.
This impliesthat the negative score for an incorrect translationshould be high in comparison to the positive scorefor a correct translation.
So a naive scoring func-tion might be ?1 point for a correct translation, 0points for no translation, ?1000 points for an in-correct translation.
?However, since the high negative score for amistranslation is justified by the possible seriousconsequences, not all mistranslations are equal;some are much more likely than others to result inclinical consequences.
For example, consider thepossible consequences of two different mistrans-61lations of the Spanish sentence La penicilina meda alergias.
Ideally, we would like the system totranslate this as ?I am allergic to penicillin?.
If itinstead says ?I am allergic to the penicillin?, thetranslation is slightly imperfect, but it is hard to seeany important misunderstanding arising as a result.In contrast, the translation ?I am not allergic topenicillin?, which might be produced as the resultof a mistake in speech recognition, could have veryserious consequences indeed.
(Note in passing thatboth errors are single-word insertions).
Anothertype of result is a nonsensical translation, perhapsdue to an internal system error.
For instance, sup-pose the translation of our sample sentence were?The allergy penicillin does me?.
In this case, itis not clear what will happen.
Most users willprobably dismiss the output as meaningless; a fewmight be tempted to try and decipher it, with un-predictable results.Examples like these show that it is important forthe scoring metric to differentiate between differ-ent classes of mistranslations, with the differentia-tion based on possible clinical consequences of theerror.
For similar reasons, it is important to thinkabout the clinical consequences when the systemproduces correct translations, or fails to producea translation.
For example, when the system cor-rectly translates ?Hello?
as Buenas d?
?as, there arenot likely to be any clinical consequences, so it isreasonable to reward it with a lower score than theone assigned to a clinically contentful utterance.When no translation is produced, it also seems cor-rect to distinguish the case where the user was ablerecover by a suitably rephrasing the utterance fromthe one where they simply gave up.
For example,if the system failed to translate ?How long has thiscough been troubling you?
?, but correctly handledthe simpler formulation ?How long have you had acough?
?, we would give this a small positive score,rather than a simple zero.Summarising, we propose to classify transla-tions into the following seven categories:1.
Perfect translation, useful clinical conse-quences.2.
Perfect translation, no useful clinical conse-quences.3.
Imperfect translation, but not dangerous interms of clinical consequences.4.
Imperfect translation, potentially dangerous.5.
Nonsense.6.
No translation produced, but later rephrasedin a way the system handled adequately.7.
No translation produced, but not rephrased ina way the system handled adequately.3.2 Assigning utterances to categoriesAt the moment, medical professionals will onlyaccept the validity of category assignments madeby trained physicians.
In the worst case, it isclearly true that a layman, even one who has re-ceived some training, will not be able to determinewhether or not a mistranslation has clinical signif-icance.Physician time is, however, a scarce and valu-able resource, and, as usual, typical case and worstcase may be very different.
Particularly for routinetesting during system development, it is clearly notpossible to rely on expert physician assessments.We consequently suggest a compromise strategy.We will first carry out an evaluation using medicalexperts, in order to establish a gold standard.
Wewill then repeat this evaluation using non-experts,and determine how large the differential is in prac-tice.We initially intend to experiment with two dif-ferent groups of non-experts.
At Geneva Uni-versity, we will use students from the School ofTranslation.
These students will be selected forcompetence in English and Spanish, and will re-ceive a few hours of training on determination ofclinical significance in translation, using guide-lines developed in collaboration with Glenn Floresand his colleagues at the UT Southwestern Medi-cal Center, Texas.
Given that the corpus materialis simple and sterotypical, we think that this ap-proach should yield a useful approximation to ex-pert judgements.Although translation students are far cheaperthan doctors, they are still quite expensive, andevaluation turn-around will be slow.
For these rea-sons, we also propose to investigate the idea of per-forming evaluations using Amazon?s MechanicalTurk4.
This will be done by Dolores Labs, a newstartup specialising in Turk-based crowdsourcing.3.3 Scores for categoriesWe have not yet agreed on exact scores for thedifferent categories, and this is something that is4http://www.mturk.com/mturk/welcome62probably best decided after mutual discussion atthe workshop.
Some basic principles will be evi-dent from the preceding discussion.
The scale willbe normalised so that failure to produce a trans-lation is counted as zero; potentially dangerousmistranslations will be associated with a negativescore large in comparison to the positive score fora useful correct translation.
Inability to communi-cate can certainly be dangerous (this is the point ofhaving a translation system in the first place), butmistakenly believing that one has communicatedis usually much worse.
As Mark Twain put it: ?Itain?t what you don?t know that gets you into trou-ble.
It?s what you know for sure that just ain?t so?.3.4 Discarding uncertain responsesGiven that both speech recognition and machinetranslation are uncertain technologies, a highpenalty for mistranslations means that systemswhich attempt to translate everything may eas-ily end up with an average negative score - inother words, they would score worse than a systemwhich did nothing!
For the shared task to be in-teresting, we must address this problem, and in thedoctor to patient direction there is a natural wayto do so.
Since the doctor can reasonably be as-sumed to be a trained professional who has hadtime to learn to operate the system, we can say thathe has the option of aborting any translation wherethe machine does not appear to have understoodcorrectly.We thus relativise the task with respect to a ?fil-ter?
: for each utterance, we produce both a transla-tion in the target language, and a ?reference trans-lation?
in the source language, which in some waygives information about what the machine has un-derstood.
The simplest way to produce this ?ref-erence translation?
is to show the words producedby speech recognition.
When scoring, we evaluateboth translations, and ignore all examples wherethe reference translation is evaluated as incorrect.To go back to the ?penicillin?
example, supposethat Spanish source-language speech recognitionhas incorrectly recognised La penicilina me daalergias as La penicilina no me da alergias.
Evenif this produces the seriously incorrect translation?I am not allergic to penicillin?, we can score itas a zero rather than a negative, on the groundsthat the speech recognition result already showsthe Spanish-speaking doctor that something hasgone wrong before any translation has happened.The reference translation may also be produced ina more elaborate way; a common approach is totranslate back from the target language result intothe source language.Although the ?filtered?
version of the medicalspeech translation task makes good sense in thedoctor to patient direction, it is less clear howmeaningful it is in the patient to doctor direction.Most patients will not have used the system before,and may be distressed or in pain.
It is consequentlyless reasonable to expect them to be able to pay at-tention to the reference translation when using thesystem.4 Summary and conclusionsThe preceding notes are intended to form a frame-work which will serve as a basis for discussion atthe workshop.
As already indicated, the key chal-lenge here is to arrive at metrics which are ac-ceptable to both the medical and the speech andlanguage community.
This will certainly requiremore negotiation.
We are however encouraged bythe fact that the proposal, as presented here, hasbeen developed jointly by representatives of bothcommunities, and that we appear to be fairly nearagreement.
Another important parameter whichwe have intentionally left blank is the duration ofthe task; we think it will be more productive to de-termine this based on the schedules of interestedparties.Realistically, the initial definition of the metriccan hardly be more than a rough guess.
Experi-mentation during the course of the shared task willprobably show that some adjustment will be desir-able, in order to make it conform more closely tothe requirements of the medical community.
If wedo this, we will, in the interests of fairness, scorecompeting systems using all versions of the metric.63
