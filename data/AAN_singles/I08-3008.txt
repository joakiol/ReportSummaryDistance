Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 35?42,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingCross-Language Parser Adaptation between Related LanguagesDaniel ZemanUniverzita Karlova?stav form?ln?
a aplikovan?
lingvistikyMalostransk?
n?m?st?
25CZ-11800 Prahazeman@ufal.mff.cuni.czPhilip ResnikUniversity of MarylandDepartment of Linguistics andInstitute for Advanced Computer StudiesCollege Park, MD 20742, USAresnik@umd.eduAbstractThe present paper describes an approach toadapting a parser to a new language.Presumably the target language is muchpoorer in linguistic resources than the sourcelanguage.
The technique has been tested ontwo European languages due to test dataavailability; however, it is easily applicableto any pair of sufficiently related languages,including some of the Indic language group.Our adaptation technique using existingannotations in the source language achievesperformance equivalent to that obtained bytraining on 1546 trees in the target language.1 IntroductionNatural language parsing is one of the key areas ofnatural language processing, and its output is usedin numerous end-user applications, e.g.
machinetranslation or question answering.
Unfortunately, itis not easy to build a parser for a resource-poorlanguage.
Either a reasonably-sized syntacticallyannotated corpus (treebank) or a human-designedformal grammar is typically needed.
These types ofresources are costly to build, both in terms of timeand of the expenses on qualified manpower.
Bothalso require, in addition to the actual annotationprocess, a substantial effort on treebank/grammardesign, format specifications, tailoring of annota-tion guidelines etc; the latter costs are rather con-stant no matter how small the resulting corpus is.In this context, there is the intriguing questionwhether we can actually build a parser without atreebank (or a broad-coverage formal grammar) ofthe particular language.
There is some relatedwork that addresses the issue by a variety of means.Klein and Manning (2004) use a hybrid unsuper-vised approach, which combines a constituencyand a dependency model, and achieve an unlabeledF-score of 77.6% on Penn Treebank Wall StreetJournal data (English), 63.9% on Negra Corpus(German), and 46.7% on the Penn Chinese Tree-bank.1 Bod (2006) uses unsupervised data-orientedparsing; the input of his parser contains manuallyassigned gold-standard tags.
He reports 64.2%unlabeled F-score on WSJ sentences up to 40words long.2Hwa et al (2004) explore a different approach toattacking a new language.
They train Collins?s(1997) Model 2 parser on the Penn Treebank WSJdata and use it to parse the English side of a paral-lel corpus.
The resulting parses are converted todependencies, the dependencies are projected to asecond language using automatically obtainedword alignments as a bridge, and the resulting de-pendency trees cleaned up using a limited set oflanguage-specific post-projection transformationrules.
Finally a dependency parser for the targetlanguage is trained on this projected dependencytreebank, and the accuracy of the parser is meas-ured against a gold standard.
Hwa et al report de-pendency accuracy of 72.1 for Spanish, compara-ble to a rule-based commercial parser; accuracy onChinese is 53.9%, the equivalent of a parser trainedon roughly 2000 sentences of the Penn ChineseTreebank (sentences ?40 words, average length20.6).1 Note that in all these experiments they restrict themselves tosentences of 10 words or less.2 On sentences of ?10 words, Bod achieves 78.5% for English(WSJ), 65.4% for German (Negra) and 46.7% for Chinese(CTB).35Our own approach is motivated by McClosky etal.
?s (2006) reranking-and-self-training algorithm,used successfully in adapting a parser to a newdomain.
One can easily imagine viewing two dia-lects of a language or even two related languagesas two domains of one ?super-language?
while thevocabulary will certainly differ (due to independ-ently designed orthographies for the two lan-guages), many morphological and syntactic proper-ties may be shared.
We trained Charniak and John-son?s (2005) reranking parser on one language andapplied it to another closely related language.
Inaddition, we investigated the utility of large butunlabeled data in the target language, and of alarge parallel corpus of the two languages.32 Corpora and Other ResourcesThe selection of our source and target languageswas driven by the need for two closely related lan-guages with associated treebanks.
(In a real-worldapplication we would not assume the existence of atarget-language treebank, but one is needed herefor evaluation.)
Danish served as the source lan-guage and Swedish as target, since these languagesare closely related and there are freely availabletreebanks for both.4The Danish Dependency Treebank (Kromann etal.
2004) contains 5,190 sentences (94,386 tokens).The texts come from the Danish Parole Corpus(1998?2002, mixed domain).
We split the data into4,900 training and 290 test sentences, keeping the276 not exceeding 40 words.The Swedish treebank Talbanken05 (Nivre et al2006) contains 11,042 sentences (191,467 tokens).It was converted at V?xj?
from the much olderTalbanken76 treebank, created at the Lund Univer-sity.
Again, the texts belong to mixed domains.
Wesplit the data to 10,700 training and 342 test sen-tences, out of which 317 do not exceed 40 words.Both treebanks are dependency treebanks, whilethe Charniak-Johnson reranking parser works withphrase structures.
For our experiments, we con-3 There are other approaches to domain adaptation aswell.
For instance, Steedman et al (2003) address do-main adaptation using a weakly supervised methodcalled co-training.
Two parsers, each applying a differ-ent strategy, mutually prepare new training examples foreach other.
We have not tested co-training for cross-language adaptation.4 We used the CoNLL 2006 versions of these treebanks.verted the treebanks from dependencies to phrases,using the ?flattest-possible?
algorithm (Collins etal.
1999; algorithm 2 of Xia and Palmer 2001).
Themorphological annotation of the treebanks helpedus to label the non-terminals.
Although theCharniak?s parser can be taught a new inventory oflabels, we found it easier to map head morpho-tagsdirectly to Penn-Treebank-style non-terminals.Hence the parser can think it?s processing PennTreebank data.
The morphological annotation ofthe treebanks is further discussed in Section 4.We also experimented with a large body of un-annotated Swedish texts.
Such data could theoreti-cally be acquired by crawling the Web; here, how-ever, we used the freely available JRC-Acquis cor-pus of EU legislation (Steinberger et al 2006).5The Acquis corpus is segmented at the paragraphlevel.
We ran a simple procedure to split the para-graphs into sentences and pruned sentences withsuspicious length, contents (sequence of dashes,for instance) or both.
We ended up with 430,808Swedish sentences and 6,154,663 tokens.Since the Acquis texts are available in 21 lan-guages, we can also exploit the Danish Acquis andits alignment with the Swedish one.
We use it tostudy the similarity of the two languages, and forthe ?gloss?
experiment in Section 5.1.
Paragraph-level alignment is provided as part of Acquis andcontains 283,509 aligned segments.
Word-levelalignment, needed for our experiment, was ob-tained using GIZA++ (Och and Ney 2000).The treebanks are manually tagged with parts ofspeech and morphological information.
For someof our experiments, we needed to automatically re-tag the target (Swedish) treebank, and to tag theSwedish Acquis.
For that purpose we used theSwedish tagger of Jan Haji?, a variant of Haji?
?sCzech tagger (Haji?
2004) retrained on Swedishdata.3 Treebank NormalizationThe two treebanks were developed by differentteams, using different annotation styles and guide-lines.
They would be systematically different evenif their texts were in the same language, but it is5 Legislative texts are a specialized domain that cannotbe expected to match the domain of our treebanks, how-ever vaguely defined it is.
But presumably the domainmatching would be even less trustworthy if we acquiredthe unlabeled data from the web.36the impact of the language difference, not annota-tion style differences, that we want to measure;therefore we normalize the treebanks so that theyare as similar as possible.While this may sound suspicious at first glance(?wow, are they refining their test data?!?
), it isimportant to understand why it does notunacceptably bias the results.
If our method wereapplied to a new language, where no treebankexists, trees conforming to the annotation scenarioof a treebank of related language would beperfectly satisfying.
In addition, note that we applyonly systematic changes, mostly reversible.Moreover, the transformations can be done on thetraining data side, instead of test data.Following are examples of the style differencesthat underwent normalization:DET-ADJ-NOUN.
Da: de norske piger.
Sv:6 engammal institution (?an old institution?)
In DDT,the determiner governs the adjective and the noun.The approach of Talbanken (and of a number ofother dependency treebanks) is that both deter-miner and adjective depend on the noun.NUM-NOUN.
Da: 100 procent (?100 percent?
)Sv: tv?
eventuellt tre ?r (?two, possibly threeyears?)
In DDT, the number governs the noun.
InTalbanken, the number depends on the noun.GENITIVE-NOMINATIVE.
Da: Ruslands vej(?Russia?s way?)
Sv: ?rs inkomster (?year?sincome?).
In DDT, the nominative noun (theowned) governs the noun in genitive (the owner).Talbanken goes the opposite way.COORDINATION.
Da: F?r?erne ogGr?nland (?Faroe Islands and Greenland?)
Sv:socialgrupper, nationer och raser (?social groups,nations and races?)
In DDT, the last coordinationmember depends on the conjunction, theconjunction and everything else (punctuation, innermembers) depend on the first member, which is thehead of the coordination.
In Talbanken, everymember depends on the previous member, commasand conjunctions depend on the member followingthem.4 Mapping Tag SetsThe nodes (words) of the Danish DependencyTreebank are tagged with the Parole morphological6 These are separate examples from the two treebanks.They are not translations of each other!tags.
Talbanken is tagged using the much coarserMamba tag set (part of speech, no morphology).The tag inventory of Haji?
?s tagger is quite similarto the Danish Parole tags, but not identical.
Weneed to be able to map tags from one set to theother.
In addition, we also convert pre-terminaltags to the Penn Treebank tag set when convertingdependencies to constituents.Mapping tag sets to each other is obviously aninformation-lossy process, unless both tag setscover identical feature-value spaces.
Apart fromthat, there are numerous considerations that makeany such conversion difficult, especially when thetarget tags have been designed for a differentlanguage.We take an Interlingua-like (or Inter-tag-set)approach.
Every tag set has a driver thatimplements decoding of the tags into a nearlyuniversal feature space that we have defined, andencoding of the feature values by the tags.
Theencoding is (or aims at being) independent ofwhere the feature values come from, and thedecoding does not make any assumptions about thesubsequent encoding.
Hence the effort put inimplementing the drivers is reusable for othertagset pairs.The key function, responsible for theuniversality of the method, is encode().Consider the following example.
There are twofeatures set, POS = ?noun?
and GENDER =?masc?.
The target set is not capable of encodingmasculine nouns.
However, it allows for ?noun?
+?com?
| ?neut?, or ?pronoun?
+ ?masc?
| ?fem?
|?com?
| ?neut?.
An internal rule of encode()indicates that the POS feature has higher prioritythan the GENDER feature.
Therefore the algorithmwill narrow the tag selection to noun tags.
Then thegender will be forced to common (i.e.
?com?
).Even the precise feature mapping does notguarantee that the distribution of the tags in twocorpora will be reasonably close.
All convertedsource tags will now fit in the target tag set.However, some tags of the target tag set may notbe used, although they are quite frequent in thecorpus where the target tags are native.
Someexamples:?
Unlike in Talbanken, there are no deter-miners in DDT.
That does not mean thereare no determiners in Danish ?
but DDTtags them as pronouns.37?
Swedish tags encode a special feature ofpersonal pronouns, ?subject?
vs. ?object?form (the distinction between English heand him).
DDT calls the same paradigm?nominative?
vs. ?unmarked?
case.?
Most noun phrases in both languagesdistinguish just the common and neutergenders.
However, some pronouns could beclassified as masculine or feminine.Swedish tags use the masculine gender,Danish do not.?
DDT does not use special part of speech fornumbers ?
they are tagged as adjectives.All of the above discrepancies are caused bydiffering designs, not by differences in language.The only linguistically grounded difference wewere able to identify is the supine verb form inSwedish, missing from Danish.When not just the tag inventories, but also thetag distributions have to be made compatible(which is the case of our delexicalizationexperiments later in this paper), we can create anew hybrid tag set, omitting any informationspecific for one or the other side.
Tags of bothlanguages can then be converted to this new set,using the universal approach described above.5 Using Related LanguagesThe Figure 1 gives an example of matching Danishand Swedish sentences.
This is a real examplefrom the Acquis corpus.
Even a non-speaker ofthese languages can detect the evident correspon-dence of at least 13 words, out of the total of 16(ignoring final punctuation).
However, due to dif-ferent spelling rules, only 5 word pairs are string-wise identical.
From a parser?s perspective, the restis unknown words, as it cannot be matched againstthe vocabulary learned from training data.We explore two techniques of making unknownwords known.
We call them glosses and delexicali-zation, respectively.5.1 GlossesThis approach needs a Danish-Swedish (da-sv)bitext.
As shown by Resnik and Smith (2003),parallel texts can be acquired from the Web, whichmakes this type of resource more easily availablethan a treebank.
We benefited from the Acquis da-sv alignments.Similarly to phrase-based translation systems,we used GIZA++ (Och and Ney 2000) to obtainone-to-many word alignments in both directions,then combined them into a single set of refinedalignments using the ?final-and?
method of Koehnet al (2003).
The refined alignments provided uswith two-way tables of a source word and all itspossible translations, with weights.
Using thesetables, we glossed each Swedish word by itsDanish, using the translation with the highestweight.The glosses are used to replace Swedish wordsin test data by Danish, making it more likely thatthe parser knows them.
After a parse has beenobtained, the trees are ?restuffed?
with the originalSwedish words, and evaluated.5.2 DelexicalizationA second approach relies on the hypothesis that theinteraction between morphology and syntax in thetwo languages will be very similar.
The basic ideais as follows: Replace Danish words in trainingdata with their morphological (POS) tags.
Simi-larly, replace the Swedish words in test data withtags.
This replacement is called delexicalization.Note that there are now two levels of tags in thetrees: the Danish/Swedish tags in terminal nodes,and the Penn-style tags as pre-terminals.
The ter-minal tags are more descriptive because both Nor-Bestemmelserne i denne aftale kan ?ndres og revideres helt eller delvis efter f?llesBest?mmelserna i detta avtal f?r ?ndras eller revideras helt eller delvis efter gemensamoverenskomst mellem parterne.
?verenskommelse mellan parterna.Figure 1.
Comparison of matching Danish (upper) and Swedish (lower) sentences from Acquis.
De-spite the one-to-one word mapping, only the 5 bold words have identical spelling.38dic languages have a slightly richer morphologythan English, and the conversion to the Penn tagset loses information.The crucial point is that both Danish andSwedish use the same tag set, which helps to dealwith the discrepancy between the training and thetest terminals.Otherwise, the algorithm is similar to that ofglosses: train the parser on delexicalized Danish,run it over delexicalized Swedish, restuff theresulting trees with the original Swedish words(?re-lexicalize?)
and evaluate them.6 Experiments: Part OneWe ran most experiments twice: once withCharniak?s parser alone (?C?)
and once with thereranking parser of Charniak and Johnson, whichwe label simply Brown parser (?B?
).We use the standard evalb program by Sekineand Collins to evaluate the parse trees.
Keepingwith tradition, we report the F-score of the labeledprecision and recall on the sentences of up to 40words.7Language Parser P R FC 77.84 78.48 78.16 daB 78.28 78.20 78.24C 79.50 79.73 79.62 da-hybridB 80.60 79.80 80.20C 77.61 78.00 77.81 svB 79.16 78.33 78.74C 77.54 78.93 78.23 sv-mambaB 79.67 79.26 79.46C 76.10 76.04 76.07 sv-hybridB 78.12 75.93 77.01Table 1.
Monolingual parsing accuracy.To put the experiments in the right context, wefirst ran two monolingual tracks and evaluatedDanish-trained parsers on Danish, and Swedish-trained parsers on Swedish test data.
Bothtreebanks have also been parsed afterdelexicalization into various tag sets: Danish goldstandard converted to the hybrid sv/da tag set,Swedish Mamba gold standard, and Swedishautomatically tagged with hybrid tags.The reranker did not prove useful for lexicalizedSwedish, although it helped with Danish.
(We cur-7 F = 2?P?R / (P+R)rently have no explanation of this.)
On the otherhand, delexicalized reranking parsers outperformedlexicalized parsers for both languages.
This holdsfor delexicalization using the gold standard tags(even though the Mamba tag set encodes much lessinformation than the hybrid tags).
Automaticallyassigned tags perform significantly worse.Our baseline condition is simply to train theparsers on Danish treebank and run them overSwedish test data.
Then we evaluate the twoalgorithms described in the previous section:glosses and delexicalization (hybrid tags).Approach Parser P R FC 44.59 42.04 43.28 baselineB 42.94 40.80 41.84C 61.85 65.03 63.40 glossesB 60.22 62.85 61.50C 63.47 67.67 65.50 delexB 64.74 68.15 66.40Table 2.
Cross-language parsing accuracy.7 Self-TrainingFinally, we explored the self-training baseddomain-adaptation technique of McClosky et al(2006) in this setting.
McClosky et al trained theBrown parser on one domain of English (WSJ),parsed a large corpus of a second domain(NANTC), trained a new Charniak (non-reranking)parser on WSJ plus the parsed NANTC, and testedthe new parser on data from a third domain (BrownCorpus).
They observed improvement overbaseline in spite of the fact that the large corpuswas not in the third domain.Our setting is similar.
We train the Brown parseron Danish treebank and apply it to Swedish Acquis.Then we train new Charniak parser on Danishtreebank and the parsed Swedish Acquis, and testthe parser on the Swedish test data.
The hope isthat the parser will get lexical context for thestructures from the parsed Swedish Acquis.We did not retrain the reranker on the parsedAcquis, as we found it prohibitively expensive inboth time and space.
Instead, we created a newBrown parser by combining the new Charniakparser, and the old reranker trained only on Danish.39A different scenario is used with the gloss anddelex techniques.
In this case, we only use delexi-calization/glosses to parse the Acquis corpus.
Thenew Charniak model is always trained directly onlexicalized Swedish, i.e.
the parsed Acquis is re-stuffed before being handed over to the trainer.Table-3 shows the corresponding application chart.8 Experiments: Part TwoThe following table shows the results of the self-training experiments.
All F-scores outperform thecorresponding results obtained without self-training.Approach Parser P R FC 45.14 43.96 44.54 PlainB 43.12 42.23 42.67C 62.87 66.17 64.48 GlossesB 61.94 64.77 63.32C 55.87 63.86 59.60 DelexB 53.87 61.45 57.41Table 3.
Self-training adaptation results.Not surprisingly, the Danish-trained rerankerdoes not help here.
However, even the first-stageparser failed to outperform the Part One results.Therefore the 66.40% labeled F-score of the del-exicalized Brown parser is our best result.
It im-proves the baseline by 23% absolute, or 41% errorreduction.9 DiscussionAs one way of assessing the usefulness of theresult, we compared it to the learning curve on theSwedish treebank.
This corresponds to the question?How big a treebank would we have to build, sothat the parser trained on the treebank achieves thesame F-score??
We measured the F-scores forSwedish-trained parsers on gradually increasingamounts of training data (50, 100, 250, 500, 1000,2500, 5000 and 10700 sentences).The learning curve is shown in Figure 3.
Usinginterpolation, we see that more than 1500 Swedishparse trees would be required for training, in orderto achieve the performance we obtained by adapt-ing an existing Danish treebank.
This result issimilar in spirit to the results Hwa et al (2004) re-port when training a Chinese parser using depend-ency trees projected from English.
As they observe,creating a treebank of even a few thousand trees isa daunting undertaking ?
consistent annotationtypically requires careful design of guidelines forthe annotators, testing of the guidelines on data,refinement of those guidelines, ramp-up of annota-tors, double-annotation for quality control, and soforth.
As a case in point, the Prague DependencyTreebank (B?hmov?
et al 2003) project began inDanish treebankPARSER 0 RERANKERSwedishAcquis 1PARSER 1Swedish testDELEXGLOSSESSwedishAcquis RESTUFFParsed SwedishAcquisFigure 2.
Scheme of the self-training system.401996, and required almost a year for its first 1000sentences to appear (although things sped upquickly, and over 20000 sentences were availableby fall 1998).
In contrast, if the source and targetlanguage are sufficiently related ?
consider Danishand Swedish, as we have done, or Hindi andUrdu ?
our approach should in principle permit aparser to be constructed in a matter of days.
).9.1 Ways to Improve: Future WorkThe 77.01% F-score of a parser trained ondelexicalized automatically assigned hybridSwedish tags is an upper bound.
Some obviousways of getting closer to it include better treebankand tag-set mapping and better tagging.
In addition,we are interested in seeing to what extentperformance can be further improved by betteriterative self-training.We also want to explore classifier combinationtechniques on glosses, delexicalization, and the N-best outputs of the Charniak parser.
One could alsogo further, and explore a combination of tech-niques, e.g.
taking advantage of the ideas proposedhere in tandem with unsupervised parsing (as inBod 2006) or projection of annotations across aparallel corpus (as in Hwa et al 2004).AcknowledgementsThe authors thank Eugene Charniak and MarkJohnson for making their reranking parseravailable, as well as the creators of the corporaused in this research.
We also thank theanonymous reviewers for useful remarks on whereto focus our workshop presentation.The research reported on in this paper has beensupported by the Fulbright-Masaryk Fellowship(first author), and by Grant No.
N00014-01-1-0685ONR.
Ongoing research (first author) is supportedby the Ministry of Education of the CzechRepublic, project MSM0021620838, and CzechAcademy of Sciences, project No.
1ET101470416.ReferencesRens Bod.
2006a.
Unsupervised Parsing with U-DOP.In: Proceedings of the Conference on NaturalLanguage Learning (CoNLL-2006).
New York, NewYork, USA.Rens Bod.
2006b.
An All-Subtrees Approach to Unsu-pervised Parsing.
In: Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th Annual Meeting of the ACL (COLING-ACL-2006).
Sydney, Australia.0102030405060708050 100 250 500 1000 2500 5000 10700Training sentencesF66.40(delex)~ 1546sentencesFigure 3.
The learning curve on the Swedish training data.41Alena B?hmov?, Jan Haji?, Eva Haji?ov?, BarboraHladk?.
2003.
The Prague Dependency Treebank: AThree-Level Annotation Scenario.
In: Anne Abeill?(ed.
): Treebanks: Building and Using SyntacticallyAnnotated Corpora.
Kluwer Academic Publishers,Dordrecht, The Netherlands.Eugene Charniak, Mark Johnson.
2005.
Coarse-to-FineN-Best Parsing and MaxEnt DiscriminativeReranking.
In: Proceedings of the 43rd AnnualMeeting of the ACL (ACL-2005), pp.
173?180.
AnnArbor, Michigan, USA.Michael Collins.
1997.
Three Generative, LexicalizedModels for Statistical Parsing.
In: Proceedings of the35th Annual Meeting of the ACL, pp.
16?23.
Madrid,Spain.Michael Collins, Jan Haji?, Lance Ramshaw, ChristophTillmann.
1999.
A Statistical Parser for Czech.
In:Proceedings of the 37th Annual Meeting of the ACL(ACL-1999), pp.
505?512.
College Park, Maryland,USA.Jan Haji?.
2004.
Disambiguation of Rich Inflection(Computational Morphology of Czech).
Karolinum,Charles University Press, Praha, Czechia.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, Okan Kolak.
2004.
Bootstrapping Parsersvia Syntactic Projection across Parallel Texts.
In:Natural Language Engineering 1 (1): 1?15.Cambridge University Press, Cambridge, England.Dan Klein, Christopher D. Manning.
2004.
Corpus-Based Induction of Syntactic Structure: Models ofDependency and Constituency.
In: Proceedings of the42nd Annual Meeting of the ACL (ACL-2004).Barcelona, Spain.Philipp Koehn, Franz Josef Och, Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In: Proceedingsof HLT-NAACL 2003, pp.
127?133.
Edmonton,Canada.Matthias T. Kromann, Line Mikkelsen, Stine KernLynge.
2004.
Danish Dependency Treebank.
At:http://www.id.cbs.dk/~mtk/treebank/.
K?benhavn,Denmark.Mitchell P. Marcus, Beatrice Santorini, Mary Ann Mar-cinkiewicz.
1993.
Building a Large Annotated Cor-pus of English: the Penn Treebank.
In: Computa-tional Linguistics, vol.
19, pp.
313?330.David McClosky, Eugene Charniak, Mark Johnson.2006.
Reranking and Self-Training for Parser Adap-tation.
In: Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thAnnual Meeting of the ACL (COLING-ACL-2006).Sydney, Australia.Joakim Nivre, Jens Nilsson, Johan Hall.
2006.Talbanken05: A Swedish Treebank with PhraseStructure and Dependency Annotation.
In:Proceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC-2006).May 24-26.
Genova, Italy.Franz Josef Och, Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In: Proceedings of the38th Annual Meeting of the ACL (ACL-2000), pp.440?447.
Hong Kong, China.Philip Resnik, Noah A. Smith.
2003.
The Web as aParallel Corpus.
In: Computational Linguistics,29(3), pp.
349?380.Mark Steedman, Miles Osborne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, PaulRuhlen, Steven Baker, Jeremiah Crim.
2003.Bootstrapping Statistical Parsers from SmallDatasets.
In: Proceedings of the 11th Conference ofthe European Chapter of the ACL (EACL-2003).Budapest, Hungary.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Toma?
Erjavec, Dan Tufi?, D?nielVarga.
2006.
The JRC-Acquis: A MultilingualAligned Parallel Corpus with 20+ Languages.
In:Proceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC-2006).May 24-26.
Genova, Italy.Fei Xia, Martha Palmer.
2001.
Converting DependencyStructures to Phrase Structures.
In: Proceedings ofthe 1st Human Language Technology Conference(HLT-2001).
San Diego, California, USA.42
