Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1172?1181,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPLarge-Scale Verb Entailment Acquisition from the WebChikara Hashimoto?
Kentaro Torisawa?
Kow Kuroda?Stijn De Saeger?
Masaki Murata?
Jun?ichi Kazama?National Institute of Information and Communications TechnologySorakugun, Kyoto, 619-0289, JAPAN{?
ch,?
torisawa,?
kuroda,?
stijn,?murata,?
kazama}@nict.go.jpAbstractTextual entailment recognition plays afundamental role in tasks that require in-depth natural language understanding.
Inorder to use entailment recognition tech-nologies for real-world applications, alarge-scale entailment knowledge base isindispensable.
This paper proposes a con-ditional probability based directional sim-ilarity measure to acquire verb entailmentpairs on a large scale.
We targeted 52,562verb types that were derived from 108Japanese Web documents, without regardfor whether they were used in daily lifeor only in specific fields.
In an evaluationof the top 20,000 verb entailment pairs ac-quired by previous methods and ours, wefound that our similarity measure outper-formed the previous ones.
Our methodalso worked well for the top 100,000 re-sults.1 IntroductionWe all know that if you snored, you must havebeen sleeping, that if you are divorced, you musthave been married, and that if you won a lawsuit,you must have sued somebody.
These relation-ships between events where one is the logical con-sequence of the other are called entailment.
Suchknowledge plays a fundamental role in tasks thatrequire in-depth natural language understanding,e.g., answering questions and using natural lan-guage interfaces.This paper proposes a novel method for verbentailment acquisition.
Using a Japanese Webcorpus (Kawahara and Kurohashi, 2006a) derivedfrom 108 Japanese Web documents, we automat-ically acquired such verb pairs as snore ?
sleepand divorce ?
marry, where entailment holds be-tween the verbs in the pair.1 Our definition of ?en-tailment?
is the same as that in WordNet3.0; v1entails v2if v1cannot be done unless v2is, or hasbeen, done.2Our method follows the distributional similar-ity hypothesis, i.e., words that occur in the samecontext tend to have similar meanings.
Just as inthe methods of Lin and Pantel (2001) and Szpek-tor and Dagan (2008), we regard the argumentsof verbs as the context in the hypothesis.
How-ever, unlike the previous methods, ours is basedon conditional probability and is augmented witha simple trick that improves the accuracy of verbentailment acquisition.
In an evaluation of the top20,000 verb entailment pairs acquired by the pre-vious methods and ours, we found that our similar-ity measure outperformed the previous ones.
Ourmethod also worked well for the top 100,000 re-sults,Since the scope of Natural Language Process-ing (NLP) has advanced from a formal writingstyle to a colloquial style and from restricted toopen domains, it is necessary for the language re-sources for NLP, including verb entailment knowl-edge bases, to cover a broad range of expressions,regardless of whether they are used in daily lifeor only in specific fields that are highly techni-cal.
As we will discuss later, our method can ac-quire, with reasonable accuracy, verb entailmentpairs that deal not only with common and familiarverbs but also with technical and unfamiliar oneslike podcast ?
download and jibe ?
sail.Note that previous researches on entailment ac-quisition focused on templates with variables orword-lattices (Lin and Pantel, 2001; Szpektor andDagan, 2008; Barzilay and Lee, 2003; Shinyama1Verb entailment pairs are described as v1?
v2(v1isthe entailing verb and v2is the entailed one) henceforth.2WordNet3.0 provides entailment relationships betweensynsets like divorce, split up?marry, get married, wed, con-join, hook up with, get hitched with, espouse.1172et al, 2002).
Certainly these templates or wordlattices are more useful in such NLP applicationsas Q&A than simple entailment relations betweenverbs.
However, our contention is that entailmentcertainly holds for some verb pairs (like snore ?sleep) by themselves, and that such pairs consti-tute the core of a future entailment rule database.Although we focused on verb entailment, ourmethod can also acquire template-level entailmentpairs with a reasonable accuracy.The rest of this paper is organized as follows.In ?2, related works are described.
?3 presents ourproposed method.
After this, an evaluation of ourmethod and the existing methods is presented inSection 4.
Finally, we conclude the paper in ?5.2 Related WorkPrevious studies on entailment, inference rules,and paraphrase acquisition are roughly classi-fied into those that require comparable corpora(Shinyama et al, 2002; Barzilay and Lee, 2003;Ibrahim et al, 2003) and those that do not (Linand Pantel, 2001; Weeds and Weir, 2003; Geffetand Dagan, 2005; Pekar, 2006; Bhagat et al, 2007;Szpektor and Dagan, 2008).Shinyama et al (2002) regarded newspaper arti-cles that describe the same event as a pool of para-phrases, and acquired them by exploiting namedentity recognition.
They assumed that named en-tities are preserved across paraphrases, and thattext fragments in the articles that share severalcomparable named entities should be paraphrases.Barzilay and Lee (2003) also used newspaper ar-ticles on the same event as comparable corporato acquire paraphrases.
They induced paraphras-ing patterns by sentence clustering.
Ibrahim et al(2003) relied on multiple English translations offoreign novels and sentence alignment to acquireparaphrases.
We decided not to take this approachsince using comparable corpora limits the scaleof the acquired paraphrases or entailment knowl-edge bases.
Although obtaining comparable cor-pora has been simplified by the recent explosionof the Web, the availability of plain texts is incom-parably better.Entailment acquisition methods that do not re-quire comparable corpora are mostly based on thedistributional similarity hypothesis and use plaintexts with a syntactic parser.
Basically, they parsetexts to obtain pairs of predicate phrases and theirarguments, which are regarded as features of thepredicates with appropriately assigned weights.Lin and Pantel (2001) proposed a paraphrase ac-quisition method (non-directional similarity mea-sure) called DIRT which acquires pairs of binary-templates (predicate phrases with two argumentslots) that are paraphrases of each other.
DIRT em-ploys the following similarity measure proposedby Lin (1998):Lin(l, r) =?f?Fl?Fr[wl(f) + wr(f)]?f?Flwl(f) +?f?Frwr(f)where l and r are the corresponding slots of twobinary templates, Fsis s?s feature vector (argu-ment nouns), and ws(f) is the weight of f ?
Fs(PMI between s and f ).
The intuition behind thisis that the more nouns two templates share, themore semantically similar they are.
Since we ac-quire verb entailment pairs based on unary tem-plates (Szpektor and Dagan, 2008) we used theLin formula to acquire unary templates directlyrather than using the DIRT formula, which is thearithmetic-geometric mean of Lin?s similarities fortwo slots in a binary template.Bhagat et al (2007) developed an algorithmcalled LEDIR for learning the directionality ofnon-directional inference rules like those pro-duced by DIRT.
LEDIR implements a Direction-ality Hypothesis: when two binary semantic re-lations tend to occur in similar contexts and thefirst one occurs in significantly more contexts thanthe second, then the second most likely implies thefirst and not vice versa.Weeds and Weir (2003) proposed a generalframework for distributional similarity that mainlyconsists of the notions of what they call Precision(defined below) and Recall:Precision(l, r) =?f?Fl?Frwl(f)?f?Flwl(f)where l and r are the targets of a similarity mea-surement, Fsis s?s feature vector, and ws(f) is theweight of f ?
Fs.
The best performing weight isPMI.
Precision is a directional similarity measurethat examines the coverage of l?s features by thoseof r?s, with more coverage indicating more simi-larity.Szpektor and Dagan (2008) proposed a direc-tional similarity measure called BInc (Balanced-Inclusion) that consists of Lin and Precision, asBInc(l, r) =?Lin(l, r) ?
Precision(l, r)1173where l and r are the target templates.
For weight-ing features, they used PMI.
Szpektor and Dagan(2008) also proposed a unary template, which isdefined as a template consisting of one argumentslot and one predicate phrase.
For example, X takea nap ?
X sleep is an entailment pair consistingof two unary templates.
Note that the slot X mustbe shared between templates.
Though most of theprevious entailment acquisition studies focused onbinary templates, unary templates have an obvi-ous advantage over binary ones; they can handleintransitive predicate phrases and those that haveomitted arguments.
The Japanese language, whichwe deal with here, often omits arguments, and thusthe advantage of unary templates is obvious.As shown in ?4, our method outperforms Lin,Precision, and BInc in accuracy.Szpector et al (2004) addressed broad coverageentailment acquisition.
But their method requiresan existing lexicon to start, while ours does not.Apart from the dichotomy of the compara-ble corpora and the distributional similarity ap-proaches, Torisawa (2006) exploited the structureof Japanese coordinated sentences to acquire verbentailment pairs.
Pekar (2006) used the localstructure of coherent text by identifying relatedclauses within a local discourse.
Zanzotto et al(2006) exploited agentive nouns.
For example,they acquired win ?
play from ?the player wins.
?Geffet and Dagan (2005) proposed the Distribu-tional Inclusion Hypotheses, which claimed that ifa word v entails another word w, then all the char-acteristic features of v are expected to appear withw, and vice versa.
They applied this to noun en-tailment pair acquisition, rather than verb pairs.3 Proposed MethodThis section presents our method of verb entail-ment acquisition.
First, the basics of Japanese aredescribed.
Then, we present the directional sim-ilarity measure that we developed in ?3.2.
?3.3describes the structure and acquisition of the web-based data from which entailment pairs are de-rived.
Finally, we show how we acquire verb en-tailment pairs using our proposed similarity mea-sure and the web-based data in ?3.4.3.1 Basics of JapaneseJapanese explicitly marks arguments including thesubject and object by postpositions, and is a head-final language.
Thus, a verb phrase consisting ofan object hon (book) and a verb yomu (read), forexample, is expressed as hon-wo yomu (book-ACCread) ?read a book?
with the accusative postpo-sition wo marking the object.3 Accordingly, werefer to a unary template as ?p, v?
hereafter, withp and v referring to the postposition and a verb.Also, we abbreviate a template-level entailment?pl, vl?
?
?pr, vr?
as l ?
r for simplicity.
Wedefine a unary template as a template consistingof one argument slot and one predicate, followingSzpektor and Dagan (2008).3.2 Directional Similarity Measure based onConditional ProbabilityThe directional similarity measure that we devel-oped and called Score is defined as follows:Score(l, r) = Scorebase(l, r) ?
Scoretrick(l, r)where l and r are unary templates, and Score in-dicates the probability of l ?
r. Scorebase, whichis the base of Score, is defined as follows:Scorebase(l, r) =?f?Fl?FrP (r|f)P (f |l)where Fsis s?s feature vector (nouns includingcompounds).
The intention behind the definitionof Scorebaseis to emulate the conditional proba-bility P (vr|vl)4 in a distributional similarity stylefunction.
Note that P (vr|vl) should be 1 when en-tailment vl?
vrholds (i.e., vris observed when-ever vlis observed) and we have reliable proba-bility values.
Then, if we can directly estimateP (vr|vl), it is reasonable to assume vl?
vrifP (vr|vl) is large enough.
However, we cannot es-timate P (vr|vl) directly since it is unlikely that wewill observe the verbs vrand vlat the same time.
(People do not usually repeat vrand vlin the samedocument to avoid redundancy.)
Thus, instead ofa direct estimation, we substitute Scorebase(l, r)as defined above.
In other words, we assumeP (vr|vl) ?
P (r|l) ?
?f?Fl?FrP (f |l)P (r|f).Actually, Scorebaseoriginally had another mo-tivation, inspired by Torisawa (2005), for which nopostposition but the instrumental postposition dewas relevant.
In this discussion, all of the nouns(fs) that are marked by the instrumental postposi-tion are seen as ?tools,?
and P (f |l) is interpreted3ACC represents an accusative postposition in Japanese.Likewise, NOM, DAT, INS, and TOP are the symbols for thenominative, dative, instrumental, and topic postpositions.4Remember that vland vrare the verbs of unary tem-plates l and r.1174as a measure of how typically the tool f is usedto perform the action denoted by (the vlof) l; ifP (f |l) is large enough, f is a typical tool used inl.
On the other hand, P (r|f) indicates the proba-bility of (the vrof) r being the purpose for usingthe tool f .
See (1) for an example.
(1) konro-de chouri-surucooking.stove-INS cook?cook (something) using a cooking stove.
?The purpose of using a cooking stove is to cook.Torisawa (2005) has pointed out that when r ex-presses the purpose of using a tool f , P (r|f) tendsto be large.
This predicts that P (r|cooking stove)is large, where r is ?de, cook?.According to this observation, if f is a singlepurpose tool and P (f |l), the probability of f be-ing the tool by which l is performed, and P (r|f),the probability of r being the purpose of using thetool f , are large enough, then the typical perfor-mance of the action vlshould contain some ac-tions that can be described by vr, i.e., the pur-pose of using f .
Moreover, if all the typical tools(fs) used in vlare also used for vr, most perfor-mances of the action vlshould contain a part de-scribed by the action vr.
In summary, this meansthat when ?f?Fl?FrP (r|f)P (f |l), Scorebase, hasa large value, we can expect vl?
vr.For example, let vlbe deep-fry and vrbe cook.Note that vl?
vrholds for this example.
Thereare many tools that are used for deep-frying,such as cooking stove, pot, or pan.
This meansthat P (cooking stove|l), P (pot|l), or P (pan|l) arelarge.
On the other hand, the purpose of using allof these tools is cooking, based on common sense.Thus, probabilities such as P (r|cooking stove)and P (r|pan) should have large values.
Accord-ingly, ?f?Fl?FrP (f |l)P (r|f), Scorebase, shouldbe relatively large for deep-fry ?
cook,Actually, we defined Scorebasebased on theabove assumption However, through a series ofpreliminary experiments, we found that the samescore could be applied without losing the preci-sion to the other postpositions.
Thus, we gener-alized the framework so that it could deal withmost postpositions, namely ga (NOM), wo (ACC),ni (DAT), de (INS), and wa (TOP).
Note that thisis a variation of the distributional inclusion hy-pothesis (Geffet and Dagan, 2005), but that we donot use mutual information as in previous works,based on the hypothesis discussed above.
Actu-ally, as shown in ?4, our conditional probabilitybased method outperformed the mutual informa-tion based metrics in our experiments.On the other hand, Scoretrickimplements an-other assumption that if only one feature con-tributes to Scorebaseand the contribution of theother nouns is negligible, if any, the similarity isunreliable.
Accordingly, for Scoretrick, we uni-formly ignore the contribution of the most domi-nant feature from the similarity measurement.Scoretrick(l, r)= Scorebase(l, r) ?
maxf?Fl?FrP (r|f)P (f |l)As shown in ?4, this trick actually improved theentailment acquisition accuracy.We used maximum likelihood estimation to ob-tain P (r|f) and P (f |l) in the above discussion.Bannard and Callison-Burch (2005) and Fujitaand Sato (2008) also proposed directional simi-larity measures based on conditional probability,which are very similar to Scorebase, although ei-ther their method?s prerequisites or the targets ofthe similarity measurements were different fromours.
The method of Bannard and Callison-Burch(2005) requires bilingual parallel corpora, anduses the translations of expressions as its feature.Fujita and Sato (2008) dealt with productive pred-icate phrases, while our target is non-productivelexical units, i.e., verbs.
Thus, this is the firstattempt to apply a conditional probability basedsimilarity measure to verb entailment acquisition.In addition, the trick implemented in Scoretrickisnovel.3.3 Preparing Template-Feature TuplesOur method starts from a dataset called template-feature tuples, which was derived from the Webin the following way: 1) Parse the Japanese Webcorpus (Kawahara and Kurohashi, 2006a) derivedfrom 108 Japanese Web documents with Japanesedependency parser KNP (Kawahara and Kuro-hashi, 2006b).
2) Extract triples ?n, p, v?
consist-ing of nouns (n), postpositions (p), and verbs (v),where an n marked by a p depends on a v fromthe parsed Web text.
3) From the triple database,construct template-feature tuples ?n, ?p, v??
by re-garding ?p, v?
as a unary template and n as one ofits features.
4) Convert the verbs into their canon-ical forms as defined by KNP.
5) Filter out tuplesthat fall into one of the following categories: 5-1) Freq(?p, v?)
< 20.
5-2) Its verb is passivized,1175causativized, or negated.
5-3) Its verb is semanti-cally vague like be, do, or become.
5-4) Its post-position is something other than ga (NOM), wo(ACC), ni (DAT), de (INS), or wa (TOP).The resulting unary template-feature tuples in-cluded 127,808 kinds of templates that consistedof 52,562 verb types and five kinds of postpo-sitions.
The verbs included compound wordslike bosi-kansen-suru (mother.to.child-infection-do) ?infect from mothers to infants.
?3.4 Acquiring Entailment PairsWe acquired verb entailment pairs using the fol-lowing procedure: i) From the template-featuretuples mentioned in ?3.3, acquire unary templatepairs that exhibit an entailment relation betweenthem using the directional similarity measure in?3.2.
ii) Convert the acquired unary templates?p, v?
into naked verbs v by stripping the postpo-sitions p. iii) Remove the duplicated verb pairsresulting from stripping ps.
To be precise, whenwe removed the duplicated pairs, we left the high-est ranked one.
iv) Retrieve N-best verb pairs asthe final output from the result of iii).
That is, wefirst acquired unary template pairs and then trans-formed them into verb pairs.Although this paper focuses on verb entailmentacquisition, we also evaluated the accuracy oftemplate-level entailment acquisition, in order toshow that our similarity measure works well, notonly for verb entailment acquisition, but also fortemplate entailment acquisition (See ?4.4).
wecreated two kinds of unary templates: the ?ScoringSlots?
template and the ?Nom(inative) Slots?
tem-plate.
The first is simply the result of the procedurei); all of the templates have slots that are used forsimilarity scoring.
The second one was obtainedin the following way: 1) Only templates whose pis not a nominative are sampled from the result ofthe procedure i).
2) Their ps are all changed to anominative.
Templates of the second kind are usedto show that the corresponding slots between tem-plates (nominative, in this case) that are not usedfor similarity scoring can be incorporated to re-sulting template-level entailment pairs if the scor-ing function really captures the semantic similaritybetween templates.Note that, for unary template entailment pairslike (2) to be well-formed, the two unary slots (X-wo) between templates must share the same nounas the index i indicates.
This is relevant in ?4.4.
(2) Xi-wo musaborikuu ?
Xi-wo taberuXi-ACC gobble Xi-ACC eat4 EvaluationWe compare the accuracy of our method with thatof the alternative methods in ?4.1.
?4.2 showsthe effectiveness of the trick.
We examine the en-tailment acquisition accuracy for frequent verbs in?4.3, and evaluate the performance of our methodwhen applied to template-level entailment acquisi-tion in ?4.4.
Finally, by showing the accuracy forverb pairs obtained from the top 100,000 results,we claim that our method provides a good start-ing point from which a large-scale verb entailmentresource can be constructed in ?4.5.For the evaluation, three human annotators (notthe authors) checked whether each acquired entail-ment pair was correct.
The average of the threeKappa values for each annotator pair was 0.579for verb entailment pairs and 0.568 for templateentailment pairs, both of which indicate the mid-dling stability of this evaluation annotation.4.1 Experiment 1: Verb PairsWe applied Score, BInc, Lin, and Precision to thetemplate-feature tuples (?3.3), obtained templateentailment pairs, and finally obtained verb entail-ment pairs by removing the postpositions from thetemplates as described in ?3.
As a baseline, wecreated pairs from randomly chosen verbs.Since we targeted all of the verbs that ap-peared on the Web (under the condition ofFreq(?p, v?)
?
20), the annotators were con-fronted with technical terms and slang that theydid not know.
In such cases, they consulted dic-tionaries (either printed or machine readable ones)and the Web.
If they still could not find the mean-ing of a verb, they labeled the pair containing theunknown verb as incorrect.We used the accuracy = # of correct pairs# of acquired pairs asan evaluation measure.
We regarded a pair as cor-rect if it was judged correct by one (Accuracy-1),two (Accuracy-2), or three (Accuracy-3) annota-tors.We evaluated 200 entailment pairs sampledfrom the top 20,000 for each method (# of ac-quired pairs = 200).
For fairness, the evaluationsamples for each method were shuffled and placedin one file from which the annotators worked.
Inthis way, they were unable to know which entail-ment pair came from which method.1176Note that the verb entailment pairs producedby Lin do not provide the directionality of en-tailment.
Thus, the annotators decided the direc-tionality of these entailment pairs as follows: i)Copy 200 original samples and reverse the orderof v1and v2.
ii) Shuffle the 400 Lin samples(the original and reversed samples) with the otherones.
iii) Evaluate all of the shuffled pairs.
EachLin pair was regarded as correct if either directionwas judged correct.
In other words, we evaluatedthe upper bound performance of the LEDIR algo-rithm.Table 1 shows the accuracy of the acquiredverb entailment pairs for each method.
Figure 1Method Acc-1 Acc-2 Acc-3Score 0.770 0.660 0.460BInc 0.450 0.255 0.125Precision 0.725 0.545 0.385Lin 0.590 0.370 0.160Random 0.050 0.010 0.005Table 1: Accuracy of verb entailment pairs.shows the accuracy figures for the N-best entail-ment pairs for each method, with N being 1,000,2,000, .
.
., or 20,000.
We observed the followingpoints from the results.
First, Score outperformedall the other methods.
Second, Score and Pre-cision, which are directional similarity measures,worked well, while Lin, which is a symmetric one,performed poorly even though the directionality ofits output was determined manually.Looking at the evaluated samples, Score suc-cessfully acquired pairs in which the entailedverbs generalized entailing verbs that were techni-cal terms.
(3) shows examples of Score?s outputs.
(3) a. RSS-haisin-suru ?
todokeruRSS-feed-do deliver?feed the RSS data?b.
middosippu-maunto-suru ?
tumumidship-mounting-do mount?have (engine) midship-mounted?The errors made by DIRT (4) and BInc (5) in-cluded pairs consisting of technical terms.
(4) kurakkingu-surusoftware.cracking-do?crack a (security) system??
koutiku-hosyu-surubuilding-maintenance-do?build and maintain a system?Accuracy-100.20.40.60.810  5000  10000  15000  20000ScoreBIncPrecisionLinAccuracy-200.20.40.60.810  5000  10000  15000  20000ScoreBIncPrecisionLinAccuracy-300.20.40.60.810  5000  10000  15000  20000ScoreBIncPrecisionLinFigure 1: Accuracy of verb entailment pairs.
(5) suisou-siiku-surutank-raising-do?raise (fish) in a tank??
siken-houryuu-surutest-discharge-do?stock (with fish) experimentally?These terms are related in some sense, but theyare not entailment pairs.4.2 Experiment 2: Effectiveness of the TrickNext, we investigated the effectiveness of the trickdescribed in ?3.
We evaluated Score, Scoretrick,and Scorebase.
Table 2 shows the accuracy figuresfor each method.
Figure 2 shows the accuracy fig-ures for the N-best outputs for each method.
The1177Method Acc-1 Acc-2 Acc-3Score 0.770 0.660 0.460Scoretrick0.725 0.610 0.395Scorebase0.590 0.465 0.315Table 2: Effectiveness of the trick.results illustrate that introducing the trick signif-icantly improved the performance of Scorebase,and so did multiplying Scoretrickand Scorebase,which is our proposal Score.
(6) shows an example of Scorebase?s errors.
(6) gazou-sakusei-suru ?
henkou-suruimage-making-do change-do?make an image?
?change?This pair has only two shared nouns (f ?
Fl?Fr),and more than 99.99% of the pair?s similarity re-flects only one of the two.
Clearly, the trick wouldhave prevented the pair from being highly ranked.4.3 Experiment 3: Pairs of Frequent VerbsWe found that the errors made by Lin and BIncin Experiment 1 were mostly pairs of infrequentverbs such as technical terms.
Thus, we con-ducted the acquisition of entailment pairs targetingmore frequent verbs to see how their performancechanged.
The experimental conditions were thesame as in Experiment 1, except that the templates(?p, v?)
used were all Freq(?p, v?)
?
200.Table 3 shows the accuracy figures for eachmethod with the changes in accuracy from thoseof the original methods in parentheses.
The re-Method Acc-1 Acc-2 Acc-3Score0.690 0.520 0.335(?0.080) (?0.140) (?0.125)BInc 0.455 0.295 0.160(+0.005) (+0.040) (+0.035)Precision 0.450 0.355 0.205(?0.275) (?0.190) (?0.180)Lin 0.635 0.385 0.205(+0.045) (+0.015) (+0.045)Table 3: Accuracy of frequent verb pairs.sults show that the accuracies of Score and Pre-cision (the two best methods in Experiment 1) de-graded, while the other two improved a little.
Wesuspect that the performance difference betweenthese methods would get smaller if we further re-stricted the target verbs to more frequent ones.Accuracy-100.20.40.60.810  5000  10000  15000  20000ScoreScoretrickScorebaseAccuracy-200.20.40.60.810  5000  10000  15000  20000ScoreScoretrickScorebaseAccuracy-300.20.40.60.810  5000  10000  15000  20000ScoreScoretrickScorebaseFigure 2: Accuracy of verb entailment pairs ac-quired by Score, Scoretrick, and Scorebase.However, we believe that dealing with verbs com-prehensively, including infrequent ones, is impor-tant, since, in the era of information explosion, theimpact on applications is determined not only byfrequent verbs but also infrequent ones that consti-tute the long tail of a verb-frequency graph.
Thus,this tendency does not matter for our purpose.4.4 Experiment 4: Template PairsThis section presents the entailment acquisitionaccuracy for template pairs to show that ourmethod can also perform the entailment acqui-sition of unary templates.
We presented pairsof unary templates, obtained by the procedure in1178?3.4, to the annotators.
In doing so, we restrictedthe correct entailment pairs to those for which en-tailment always held regardless of what argumentfilled the two unary slots, and the two slots had tobe filled with the same argument, as exemplifiedin (2).
We evaluated Score and Precision.Table 4 shows the accuracy of the acquired pairsof unary templates.
Compared to verb entailmentMethod Acc-1 Acc-2 Acc-3Score0.655 0.510 0.300Scoring (?0.115) (?0.150) (?0.160)Slots Precision 0.565 0.430 0.265(?0.160) (?0.115) (?0.120)Score0.665 0.515 0.315Nom (?0.105) (?0.145) (?0.145)Slots Precision 0.490 0.325 0.215(?0.235) (?0.220) (?0.170)Table 4: Accuracy of entailment pairs of templateswhose slots were used for scoring.acquisition, the accuracy of both methods droppedby about 10%.
This was mainly due to the evalua-tion restriction exemplified in (2) which was notintroduced in the previous experiments; the an-notators ignored the argument correspondence be-tween the verb pairs in Experiment 1.
Also notethat Score outperformed Precision in this experi-ment, too.
(7) and (8) are examples of the Scoring Slotstemplate entailment pairs and (9) is that of theNom Slots acquired by our method.
(7) X-wo tatigui-suru ?
X-wo taberuX-ACC standing.up.eating-do X-ACC eat?eat X standing up?
?eat X?
(8) X-de marineedo-suru ?
X-wo ireruX-INS marinade-do X-ACC pour?marinate with X?
?pour X?
(9) X-ga NBA-iri-suru ?
?
?
(was X-de (INS))X-NOM NBA-entering-do?X joins an NBA team??
X-ga nyuudan-suru ?
?
?
(was X-de)X-NOM enrollment-do?X joins a team?4.5 Experiment 5: Verb Pairs form the Top100,000Finally, we examined the accuracy of the top100,000 verb pairs acquired by Score and Preci-sion.
As Table 5 shows, Score outperformed Pre-Method Acc-1 Acc-2 Acc-3Score 0.610 0.480 0.300Precision 0.470 0.295 0.190Table 5: Accuracy of the top 100,000 verb pairs.cision.
Note also that Score kept a reasonable ac-curacy for the top 100,000 results (Acc-2: 48%).The accuracy is encouraging enough to considerhuman annotation for the top 100,000 results toproduce a language resource for verb entailment,which we actually plan to do.Below are correct verb entailment examplesfrom the top 100,000 results of our method.
(10) The 121th pairkaado-kessai-suru ?
siharaucard-payment-do pay?pay by card?
?pay?
(11) The 6,081th pairsaitei-suru ?
sadameruadjudicate-do settle?adjudicate?
?settle?
(12) The 15,464th paireraa-syuuryou-suru ?
jikkou-suruerror-termination-do perform-do?abend?
?execute?
(13) The 30,044th pairribuuto-suru ?
kidou-surureboot-do start-do?reboot?
?boot?
(14) The 57,653th pairrinin-suru ?
syuunin-sururesignation-do accession-do?resign?
?accede?
(15) The 70,103th pairsijou-tounyuu-suru ?
happyou-surumarket-input-do publication-do?bring to the market?
?publicize?Below are examples of erroneous pairs from ourresults.
(16) is a causal relation but not an entail-ment.
(17) is a contradictory pair.
(16) The 5,475th pairjuken-suru ?
goukaku-surutake.an.exam-do acceptance-do?take an exam?
?gain admission?1179(17) The 40,504th pairketujou-suru ?
syutujou-surunot.take.part-do take.part-do?not take part?
?take part?5 ConclusionThis paper addressed verb entailment acquisitionfrom the Web, and proposed a novel directionalsimilarity measure Score.
Through a series of ex-periments, we showed i) that Score outperformsthe previously proposed measures, Lin, Precision,and BInc in large scale verb entailment acquisi-tion, ii) that our proposed trick implemented inScoretricksignificantly improves the accuracy ofverb entailment acquisition despite its simplicity,iii) that Score worked better than the others evenwhen we restricted the target verbs to more fre-quent ones, iv) that our method is also moder-ately successful at producing template-level en-tailment pairs, and v) that our method maintainedreasonable accuracy (in terms of human annota-tion) for the top 100,000 results.
As examples ofthe acquired verb entailment pairs illustrated, ourmethod can acquire from an ocean of information,namely the Web, a variety of verb entailment pairsranging from those that are used in daily life tothose that are used in very specific fields.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL2005),pages 597?604.Regina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: An unsupervised approach us-ing multiple-sequence alignment.
In Proceedings ofHLT-NAACL 2003, pages 16?23.Rahul Bhagat, Patrick Pantel, and Eduard Hovy.
2007.Ledir: An unsupervised algorithm for learning di-rectionality of inference rules.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing (EMNLP2007), pages 161?170.Atsushi Fujita and Satoshi Sato.
2008.
A probabilis-tic model for measuring grammaticality and similar-ity of automatically generated paraphrases of pred-icate phrases.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(COLING2008), pages 225?232.Maayan Geffet and Ido Dagan.
2005.
The dis-tributional inclusion hypotheses and lexical entail-ment.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL2005), pages 107?114.Ali Ibrahim, Boris Katz, and Jimmy Lin.
2003.
Ex-tracting structural paraphrases from aligned mono-lingual corpora.
In Proceedings of the 2nd Interna-tional Workshop on Paraphrasing (IWP2003), pages57?64.Daisuke Kawahara and Sadao Kurohashi.
2006a.Case Frame Compilation from the Web using High-Performance Computing.
In Proceedings of The 5thInternational Conference on Language Resourcesand Evaluation (LREC-06), pages 1344?1347.Daisuke Kawahara and Sadao Kurohashi.
2006b.
AFully-Lexicalized Probabilistic Model for JapaneseSyntactic and Case Structure Analysis.
In Pro-ceedings of the Human Language Technology Con-ference of the North American Chapter of theAssociation for Computational Linguistics (HLT-NAACL2006), pages 176?183.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
Natural Lan-guage Engineering, 7(4):343?360.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics (COLING-ACL1998), pages768?774.Viktor Pekar.
2006.
Acquisition of verb entailmentfrom text.
In Proceedings of the main confer-ence on Human Language Technology Conferenceof the North American Chapter of the Associationof Computational Linguistics (HLT-NAACL2006),pages 49?56.Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic paraphrase acquisition from newsarticles.
In Proceedings of the 2nd internationalConference on Human Language Technology Re-search (HLT2002), pages 313?318.Idan Szpector, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisitionof entailment relations.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP2004), pages 41?48.Idan Szpektor and Ido Dagan.
2008.
Learning en-tailment rules for unary template.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics (COLING2008), pages 849?856.Kentaro Torisawa.
2005.
Automatic acquisition of ex-pressions representing preparation and utilization ofan object.
In Proceedings of the Recent Advancesin Natural Language Processing (RANLP05), pages556?560.1180Kentaro Torisawa.
2006.
Acquiring inference ruleswith temporal constraints by using japanese cood-inated sentences and noun-verb co-occurences.
InProceedings of the Human Language TechnologyConference of the Norh American Chapter of theACL (HLT-NAACL2006), pages 57?64.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP2003), pages 81?88.Fabio Massimo Zanzotto, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs using se-lectional preferences.
In Proceedings of the 44thAnnual Meeting of the Association for Computa-tional Linguistics and 21th InternationalConferenceon Computational Linguistics (COLING-ACL2006),pages 849?856.1181
