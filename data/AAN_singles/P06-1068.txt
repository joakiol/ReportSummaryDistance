Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 537?544,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Study on Automatically Extracted Keywords in Text CategorizationAnette Hulth and Bea?ta B. MegyesiDepartment of Linguistics and PhilologyUppsala University, Swedenanette.hulth@gmail.com bea@stp.lingfil.uu.seAbstractThis paper presents a study on if and howautomatically extracted keywords can beused to improve text categorization.
Insummary we show that a higher perfor-mance ?
as measured by micro-averagedF-measure on a standard text categoriza-tion collection ?
is achieved when thefull-text representation is combined withthe automatically extracted keywords.
Thecombination is obtained by giving higherweights to words in the full-texts thatare also extracted as keywords.
We alsopresent results for experiments in whichthe keywords are the only input to the cat-egorizer, either represented as unigramsor intact.
Of these two experiments, theunigrams have the best performance, al-though neither performs as well as head-lines only.1 IntroductionAutomatic text categorization is the task of assign-ing any of a set of predefined categories to a doc-ument.
The prevailing approach is that of super-vised machine learning, in which an algorithm istrained on documents with known categories.
Be-fore any learning can take place, the documentsmust be represented in a form that is understand-able to the learning algorithm.
A trained predic-tion model is subsequently applied to previouslyunseen documents, to assign the categories.
Inorder to perform a text categorization task, thereare two major decisions to make: how to repre-sent the text, and what learning algorithm to useto create the prediction model.
The decision aboutthe representation is in turn divided into two sub-questions: what features to select as input andwhich type of value to assign to these features.In most studies, the best performing representa-tion consists of the full length text, keeping thetokens in the document separate, that is as uni-grams.
In recent years, however, a number of ex-periments have been performed in which richerrepresentations have been evaluated.
For exam-ple, Caropreso et al (2001) compare unigramsand bigrams; Moschitti et al (2004) add com-plex nominals to their bag-of-words representa-tion, while Kotcz et al (2001), and Mihalcea andHassan (2005) present experiments where auto-matically extracted sentences constitute the inputto the representation.
Of these three examples,only the sentence extraction seems to have had anypositive impact on the performance of the auto-matic text categorization.In this paper, we present experiments in whichkeywords, that have been automatically extracted,are used as input to the learning, both on their ownand in combination with a full-text representation.That the keywords are extracted means that the se-lected terms are present verbatim in the document.A keyword may consist of one or several tokens.In addition, a keyword may well be a whole ex-pression or phrase, such as snakes and ladders.The main goal of the study presented in this pa-per is to investigate if automatically extracted key-words can improve automatic text categorization.We investigate what impact keywords have on thetask by predicting text categories on the basis ofkeywords only, and by combining full-text repre-sentations with automatically extracted keywords.We also experiment with different ways of rep-resenting keywords, either as unigrams or intact.In addition, we investigate the effect of using theheadlines ?
represented as unigrams ?
as input,537to compare their performance to that of the key-words.The outline of the paper is as follows: in Section2, we present the algorithm used to automaticallyextract the keywords.
In Section 3, we present thecorpus, the learning algorithm, and the experimen-tal setup for the performed text categorization ex-periments.
In Section 4, the results are described.An overview of related studies is given in Section5, and Section 6 concludes the paper.2 Selecting the KeywordsThis section describes the method that was used toextract the keywords for the text categorization ex-periments discussed in this paper.
One reason whythis method, developed by Hulth (2003; 2004),was chosen is because it is tuned for short texts(more specifically for scientific journal abstracts).It was thus suitable for the corpus used in the de-scribed text categorization experiments.The approach taken to the automatic keywordextraction is that of supervised machine learning,and the prediction models were trained on man-ually annotated data.
No new training was doneon the text categorization documents, but modelstrained on other data were used.
As a first stepto extract keywords from a document, candidateterms are selected from the document in three dif-ferent manners.
One term selection approach isstatistically oriented.
This approach extracts alluni-, bi-, and trigrams from a document.
The twoother approaches are of a more linguistic charac-ter, utilizing the words?
parts-of-speech (PoS), thatis, the word class assigned to a word.
One ap-proach extracts all noun phrase (NP) chunks, andthe other all terms matching any of a set of empir-ically defined PoS patterns (frequently occurringpatterns of manual keywords).
All candidate termsare stemmed.Four features are calculated for each candi-date term: term frequency; inverse document fre-quency; relative position of the first occurrence;and the PoS tag or tags assigned to the candidateterm.
To make the final selection of keywords,the three predictions models are combined.
Termsthat are subsumed by another keyword selectedfor the document are removed.
For each selectedstem, the most frequently occurring unstemmedform in the document is presented as a keyword.Each document is assigned at the most twelve key-words, provided that the added regression valueAssign.
Corr.mean mean P R F8.6 3.6 41.5 46.9 44.0Table 1: The number of assigned (Assign.)
key-words in mean per document; the number of cor-rect (Corr.)
keywords in mean per document; pre-cision (P); recall (R); and F-measure (F), when 3?12 keywords are extracted per document.
(given by the prediction models) is higher than anempirically defined threshold value.
To avoid thata document gets no keywords, at least three key-words are assigned although the added regressionvalue is below the threshold (provided that thereare at least three candidate terms).In Hulth (2004) an evaluation on 500 abstractsin English is presented.
For the evaluation, key-words assigned to the test documents by profes-sional indexers are used as a gold standard, thatis, the manual keywords are treated as the oneand only truth.
The evaluation measures are preci-sion (how many of the automatically assigned key-words that are also manually assigned keywords)and recall (how many of the manually assignedkeywords that are found by the automatic indexer).The third measure used for the evaluations is theF-measure (the harmonic mean of precision andrecall).
Table 1 shows the result on that particu-lar test set.
This result may be considered to bestate-of-the-art.3 Text Categorization ExperimentsThis section describes in detail the four experi-mental settings for the text categorization exper-iments.3.1 CorpusFor the text categorization experiments we usedthe Reuters-21578 corpus, which contains 20 000newswire articles in English with multiple cate-gories (Lewis, 1997).
More specifically, we usedthe ModApte split, containing 9 603 documents fortraining and 3 299 documents in the fixed test set,and the 90 categories that are present in both train-ing and test sets.As a first pre-processing step, we extracted thetexts contained in the TITLE and BODY tags.
Thepre-processed documents were then given as in-put to the keyword extraction algorithm.
In Ta-ble 2, the number of keywords assigned to the doc-538uments in the training set and the test set are dis-played.
As can be seen in this table, three is thenumber of keywords that is most often extracted.In the training data set, 9 549 documents are as-signed keywords, while 54 are empty, as they haveno text in the TITLE or BODY tags.
Of the 3 299documents in the test set, 3 285 are assigned key-words, and the remaining fourteen are those thatare empty.
The empty documents are included inthe result calculations for the fixed test set, in or-der to enable comparisons with other experiments.The mean number of keyword extracted per docu-ment in the training set is 6.4 and in the test set 6.1(not counting the empty documents).Keywords Training docs Test docs0 54 141 68 362 829 2723 2 016 8384 868 3285 813 2596 770 2527 640 1848 527 1849 486 17710 688 20611 975 31012 869 239Table 2: Number of automatically extracted key-words per document in training set and test set re-spectively.3.2 Learning MethodThe focus of the experiments described in this pa-per was the text representation.
For this reason, weused only one learning algorithm, namely an im-plementation of Linear Support Vector Machines(Joachims, 1999).
This is the learning method thathas obtained the best results in text categorizationexperiments (Dumais et al, 1998; Yang and Liu,1999).3.3 RepresentationsThis section describes in detail the input repre-sentations that we experimented with.
An impor-tant step for the feature selection is the dimen-sionality reduction, that is reducing the numberof features.
This can be done by removing wordsthat are rare (that occur in too few documents orhave too low term frequency), or very common(by applying a stop-word list).
Also, terms maybe stemmed, meaning that they are merged into acommon form.
In addition, any of a number offeature selection metrics may be applied to furtherreduce the space, for example chi-square, or infor-mation gain (see for example Forman (2003) for asurvey).Once that the features have been set, the finaldecision to make is what feature value to assign.There are to this end three common possibilities:a boolean representation (that is, the term exists inthe document or not), term frequency, or tf*idf.Two sets of experiments were run in which theautomatically extracted keywords were the onlyinput to the representation.
In the first set, key-words that contained several tokens were kept in-tact.
For example a keyword such as paradise fruitwas represented as paradise fruit and was?
from the point of view of the classifier ?
just asdistinct from the single token fruit as from meat-packers.
No stemming was performed in this setof experiments.In the second set of keywords-only experiments,the keywords were split up into unigrams, and alsostemmed.
For this purpose, we used Porter?s stem-mer (Porter, 1980).
Thereafter the experimentswere performed identically for the two keywordrepresentations.In a third set of experiments, we extracted onlythe content in the TITLE tags, that is, the head-lines.
The tokens in the headlines were stemmedand represented as unigrams.
The main motiva-tion for the title experiments was to compare theirperformance to that of the keywords.For all of these three feature inputs, we firstevaluated which one of the three possible featurevalues to use (boolean, tf, or tf*idf).
Thereafter,we reduced the space by varying the minimumnumber of occurrences in the training data, for afeature to be kept.The starting point for the fourth set of exper-iments was a full-text representation, where allstemmed unigrams occurring three or more timesin the training data were selected, with the featurevalue tf*idf.
Assuming that extracted keywordsconvey information about a document?s gist, thefeature values in the full-text representation weregiven higher weights if the feature was identical toa keyword token.
This was achieved by adding theterm frequency of a full-text unigram to the term539frequency of an identical keyword unigram.
Notethat this does not mean that the term frequencyvalue was necessarily doubled, as a keyword oftencontains more than one token, and it was the termfrequency of the whole keyword that was added.3.4 Training and ValidationThis section describes the parameter tuning, forwhich we used the training data set.
This setwas divided into five equally sized folds, to de-cide which setting of the following two parametersthat resulted in the best performing classifier: whatfeature value to use, and the threshold for the min-imum number of occurrence in the training data(in this particular order).To obtain a baseline, we made a full-text uni-gram run with boolean as well as with tf*idf fea-ture values, setting the occurrence threshold tothree.As stated previously, in this study, we wereconcerned only with the representation, and morespecifically with the feature input.
As we did nottune any other parameters than the two mentionedabove, the results can be expected to be lower thanthe state-of-the art, even for the full-text run withunigrams.The number of input features for the full-textunigram representation for the whole training setwas 10 676, after stemming and removing all to-kens that contained only digits, as well as thosetokens that occurred less than three times.
Thetotal number of keywords assigned to the 9 603documents in the training data was 61 034.
Ofthese were 29 393 unique.
When splitting up thekeywords into unigrams, the number of uniquestemmed tokens was 11 273.3.5 TestAs a last step, we tested the best performing rep-resentations in the four different experimental set-tings on the independent test set.The number of input features for the full-textunigram representation was 10 676.
The totalnumber of features for the intact keyword repre-sentation was 4 450 with the occurrence thresh-old set to three, while the number of stemmedkeyword unigrams was 6 478, with an occurrencethreshold of two.
The total number of keywordsextracted from the 3 299 documents in the test setwas 19 904.Next, we present the results for the validationand test procedures.4 ResultsTo evaluate the performance, we used precision,recall, and micro-averaged F-measure, and we letthe F-measure be decisive.
The results for the 5-fold cross validation runs are shown in Table 3,where the values given are the average of the fiveruns made for each experiment.
As can be seenin this table, the full-text run with a boolean fea-ture value gave 92.3% precision, 69.4% recall, and79.2% F-measure.
The full-text run with tf*idfgave a better result as it yielded 92.9% precision,71.3% recall, and 80.7% F-measure.
Therefore wedefined the latter as baseline.In the first type of the experiment where eachkeyword was treated as a feature independentlyof the number of tokens contained, the recallrates were considerably lower (between 32.0%and 42.3%) and the precision rates were somewhatlower (between 85.8% and 90.5%) compared tothe baseline.
The best performance was obtainedwhen using a boolean feature value, and setting theminimum number of occurrence in training data tothree (giving an F-measure of 56.9%).In the second type of experiments, wherethe keywords were split up into unigrams andstemmed, recall was higher but still low (between60.2% and 64.8%) and precision was somewhatlower (88.9?90.2%) when compared to the base-line.
The best results were achieved with a booleanrepresentation (similar to the first experiment) andthe minimum number of occurrence in the trainingdata set to two (giving an F-measure of 75.0%)In the third type of experiments, where only thetext in the TITLE tags was used and was repre-sented as unigrams and stemmed, precision ratesincreased above the baseline to 93.3?94.5%.
Here,the best representation was tf*idf with a token oc-curring at least four times in the training data (withan F-measure of 79.9%).In the fourth and last set of experiments, wegave higher weights to full-text tokens if the sametoken was present in an automatically extractedkeyword.
Here we obtained the best results.
Inthese experiments, the term frequency of a key-word unigram was added to the term frequencyfor the full-text features, whenever the stems wereidentical.
For this representation, we experi-mented with setting the minimum number of oc-currence in training data both before and after thatthe term frequency for the keyword token wasadded to the term frequency of the unigram.
The540Input feature Feature value Min.
occurrence Precision Recall F-measurefull-text unigram bool 3 92.31 69.40 79.22full-text unigram tf*idf 3 92.89 71.30 80.67keywords-only intact bool 1 90.54 36.64 52.16keywords-only intact tf 1 88.68 33.74 48.86keywords-only intact tf*idf 1 89.41 32.05 47.18keywords-only intact bool 2 89.27 40.43 55.64keywords-only intact bool 3 87.11 42.28 56.90keywords-only intact bool 4 85.81 41.97 56.35keywords-only unigram bool 1 89.12 64.61 74.91keywords-only unigram tf 1 89.89 60.23 72.13keywords-only unigram tf*idf 1 90.17 60.36 72.31keywords-only unigram bool 2 89.02 64.83 75.02keywords-only unigram bool 3 88.90 64.82 74.97title bool 1 94.17 68.17 79.08title tf 1 94.37 67.89 78.96title tf*idf 1 94.46 68.49 79.40title tf*idf 2 93.92 69.19 79.67title tf*idf 3 93.75 69.65 79.91title tf*idf 4 93.60 69.74 79.92title tf*idf 5 93.31 69.40 79.59keywords+full tf*idf 3 (before adding) 92.73 72.02 81.07keywords+full tf*idf 3 (after adding) 92.75 71.94 81.02Table 3: The average results from 5-fold cross validations for the baseline candidates and the four typesof experiments, with various parameter settings.highest recall (72.0%) and F-measure (81.1%) forall validation runs were achieved when the occur-rence threshold was set before the addition of thekeywords.Next, the results on the fixed test data set forthe four experimental settings with the best per-formance on the validation runs are presented.Table 4 shows the results obtained on the fixedtest data set for the baseline and for those experi-ments that obtained the highest F-measure for eachone of the four experiment types.We can see that the baseline ?
where the full-text is represented as unigrams with tf*idf as fea-ture value ?
yields 93.0% precision, 71.7% re-call, and 81.0% F-measure.
When the intact key-words are used as feature input with a boolean fea-ture value and at least three occurrences in train-ing data, the performance decreases greatly bothconsidering the correctness of predicted categoriesand the number of categories that are found.When the keywords are represented as uni-grams, a better performance is achieved than whenthey are kept intact.
This is in line with the find-ings on n-grams by Caropreso et al (2001).
How-ever, the results are still not satisfactory since boththe precision and recall rates are lower than thebaseline.Titles, on the other hand, represented as uni-grams and stemmed, are shown to be a useful in-formation source when it comes to correctly pre-dicting the text categories.
Here, we achieve thehighest precision rate of 94.2% although the recallrate and the F-measure are lower than the baseline.Full-texts combined with keywords result in thehighest recall value, 72.9%, as well as the highestF-measure, 81.7%, both above the baseline.Our results clearly show that automatically ex-tracted keywords can be a valuable supplement tofull-text representations and that the combinationof them yields the best performance, measured asboth recall and micro-averaged F-measure.
Ourexperiments also show that it is possible to do asatisfactory categorization having only keywords,given that we treat them as unigrams.
Lastly, forhigher precision in text classification, we can usethe stemmed tokens in the headlines as features541Input feature Feature value Min.
occurrence Precision Recall F-measurefull-text unigram tf*idf 3 93.03 71.69 80.98keywords-only intact bool 3 89.56 41.48 56.70keywords-only unigram bool 2 90.23 64.16 74.99title tf*idf 4 94.23 68.43 79.28keywords+full tf*idf 3 92.89 72.94 81.72Table 4: Results on the fixed test set.with tf*idf values.As discussed in Section 2 and also presented inTable 2, the number of keywords assigned per doc-ument varies from zero to twelve.
In Figure 1, wehave plotted how the precision, the recall, and theF-measure for the test set vary with the number ofassigned keywords for the keywords-only unigramrepresentation.1009080706050403012(239)11(310)10(206)9(177)8(184)7(184)6(252)5(259)4(328)3(838)2(272)1(36)Per centNumber of assigned keywords (number of documents)PrecisionF-measureRecallFigure 1: Precision, recall, and F-measure foreach number of assigned keywords.
The valuesin brackets denote the number of documents.We can see that the F-measure and the recall reachtheir highest points when three keywords are ex-tracted.
The highest precision (100%) is obtainedwhen the classification is performed on a singleextracted keyword, but then there are only 36 doc-uments present in this group, and the recall is low.Further experiments are needed in order to estab-lish the optimal number of keywords to extract.5 Related WorkFor the work presented in this paper, there are twoaspects that are of interest in previous work.
Theseare in how the alternative input features (that is, al-ternative from unigrams) are selected and in howthis alternative representation is used in combina-tion with a bag-of-words representation (if it is).An early work on linguistic phrases is done byFu?rnkranz et al (1998), where all noun phrasesmatching any of a number of syntactic heuristicsare used as features.
This approach leads to ahigher precision at the low recall end, when eval-uated on a corpus of Web pages.
Aizawa (2001)extracts PoS-tagged compounds, matching pre-defined PoS patterns.
The representation containsboth the compounds and their constituents, anda small improvement is shown in the results onReuters-21578.
Moschitti and Basili (2004) addcomplex nominals as input features to their bag-of-words representation.
The phrases are extractedby a system for terminology extraction1.
The morecomplex representation leads to a small decreaseon the Reuters corpus.
In these studies, it is un-clear how many phrases that are extracted andadded to the representations.Li et al (2003) map documents (e-mail mes-sages) that are to be classified into a vector spaceof keywords with associated probabilities.
Themapping is based on a training phase requiringboth texts and their corresponding summaries.Another approach to combine different repre-sentations is taken by Sahlgren and Co?ster (2004),where the full-text representation is combinedwith a concept-based representation by selectingone or the other for each category.
They showthat concept-based representations can outperformtraditional word-based representations, and that acombination of the two different types of represen-tations improves the performance of the classifierover all categories.Keywords assigned to a particular text can beseen as a dense summary of the same.
Somereports on how automatic summarization can beused to improve text categorization exist.
For ex-1In terminology extraction all terms describing a domainare to be extracted.
The aim of automatic keyword indexing,on the other hand, is to find a small set of terms that describesa specific document, independently of the domain it belongsto.
Thus, the set of terms must be limited to contain only themost salient ones.542ample, Ko et al (2004) use methods from textsummarization to find the sentences containing theimportant words.
The words in these sentences arethen given a higher weight in the feature vectors,by modifying the term frequency value with thesentence?s score.
The F-measure increases from85.8 to 86.3 on the Newsgroups data set using Sup-port vector machines.Mihalcea and Hassan (2004) use an unsuper-vised method2 to extract summaries, which in turnare used to categorize the documents.
In their ex-periments on a sub-set of Reuters-21578 (amongothers), Mihalcea and Hassan show that the preci-sion is increased when using the summaries ratherthan the full length documents.
?Ozgu?r et al (2005)have shown that limiting the representation to2 000 features leads to a better performance, asevaluated on Reuters-21578.
There is thus evi-dence that using only a sub-set of a document cangive a more accurate classification.
The question,though, is which sub-set to use.In summary, the work presented in this paperhas the most resemblance with the work by Ko etal.
(2004), who also use a more dense version ofa document to alter the feature values of a bag-of-words representation of a full-length document.6 Concluding RemarksIn the experiments described in this paper, weinvestigated if automatically extracted keywordscan improve automatic text categorization.
Morespecifically, we investigated what impact key-words have on the task of text categorization bymaking predictions on the basis of keywords only,represented either as unigrams or intact, and bycombining the full-text representation with auto-matically extracted keywords.
The combinationwas obtained by giving higher weights to words inthe full-texts that were also extracted as keywords.Throughout the study, we were concerned withthe data representation and feature selection pro-cedure.
We investigated what feature value shouldbe used (boolean, tf, or tf*idf) and the minimumnumber of occurrence of the tokens in the trainingdata.We showed that keywords can improve the per-formance of the text categorization.
When key-words were used as a complement to the full-textrepresentation an F-measure of 81.7% was ob-2This method has also been used to extract keywords (Mi-halcea and Tarau, 2004).tained, higher than without the keywords (81.0%).Our results also clearly indicate that keywordsalone can be used for the text categorization taskwhen treated as unigrams, obtaining an F-measureof 75.0%.
Lastly, for higher precision (94.2%) intext classification, we can use the stemmed tokensin the headlines.The results presented in this study are lowerthan the state-of-the-art, even for the full-text runwith unigrams, as we did not tune any other pa-rameters than the feature values (boolean, termfrequency, or tf*idf) and the threshold for the min-imum number of occurrence in the training data.There are, of course, possibilities for furtherimprovements.
One possibility could be to com-bine the tokens in the headlines and keywords inthe same way as the full-text representation wascombined with the keywords.
Another possibleimprovement concerns the automatic keyword ex-traction process.
The keywords are presented inorder of their estimated ?keywordness?, based onthe added regression value given by the three pre-diction models.
This means that one alternativeexperiment would be to give different weights de-pending on which rank the keyword has achievedfrom the keyword extraction system.
Another al-ternative would be to use the actual regressionvalue.We would like to emphasize that the automati-cally extracted keywords used in our experimentsare not statistical phrases, such as bigrams or tri-grams, but meaningful phrases selected by includ-ing linguistic analysis in the extraction procedure.One insight that we can get from these ex-periments is that the automatically extracted key-words, which themselves have an F-measure of44.0, can yield an F-measure of 75.0 in the cat-egorization task.
One reason for this is that thekeywords have been evaluated using manually as-signed keywords as the gold standard, meaningthat paraphrasing and synonyms are severely pun-ished.
Kotcz et al (2001) propose to use text cate-gorization as a way to more objectively judge au-tomatic text summarization techniques, by com-paring how well an automatic summary fares onthe task compared to other automatic summaries(that is, as an extrinsic evaluation method).
Thesame would be valuable for automatic keyword in-dexing.
Also, such an approach would facilitatecomparisons between different systems, as com-mon test-beds are lacking.543In this study, we showed that automatic textcategorization can benefit from automatically ex-tracted keywords, although the bag-of-words rep-resentation is competitive with the best perfor-mance.
Automatic keyword extraction as well asautomatic text categorization are research areaswhere further improvements are needed in order tobe useful for more efficient information retrieval.AcknowledgmentsThe authors are grateful to the anonymous review-ers for their valuable suggestions on how to im-prove the paper.ReferencesAkiko Aizawa.
2001.
Linguistic techniques to im-prove the performance of automatic text categoriza-tion.
In Proceedings of NLPRS-01, 6th NaturalLanguage Processing Pacific Rim Symposium, pages307?314.Maria Fernanda Caropreso, Stan Matwin, and FabrizioSebastiani.
2001.
A learner-independent evaluationof the usefulness of statistical phrases for automatedtext categorization.
In Text Databases and Docu-ment Management: Theory and Practice, pages 78?102.Susan Dumais, John Platt, David Heckerman, andMehran Sahami.
1998.
Inductive learning algo-rithms and representations for text categorization.In Proceedings of the Seventh International Confer-ence on Information and Knowledge Management(CIKM?98), pages 148?155.George Forman.
2003.
An extensive empirical studyof feature selection metrics for text classification.Journal of Machine Learning Research, 3:1289?1305, March.Johannes Fu?rnkranz, Tom Mitchell, and Ellen Riloff.1998.
A case study using linguistic phrases for textcategorization on the WWW.
In AAAI-98 Workshopon Learning for Text Categorization.Anette Hulth.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP 2003),pages 216?223.Anette Hulth.
2004.
Combining Machine Learn-ing and Natural Language Processing for AutomaticKeyword Extraction.
Ph.D. thesis, Department ofComputer and Systems Sciences, Stockholm Uni-versity.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods:Support Vector Learning.
MIT-Press.Youngjoong Ko, Jinwoo Park, and Jungyun Seo.
2004.Improving text categorization using the importanceof sentences.
Information Processing and Manage-ment, 40(1):65?79.Aleksander Kolcz, Vidya Prabakarmurthi, and JugalKalita.
2001.
Summarization as feature selec-tion for text categorization.
In Proceedings of theTenth International Conference on Information andKnowledge Management (CIKM?01), pages 365?370.David D. Lewis.
1997.
Reuters-21578 text categoriza-tion test collection, Distribution 1.0.
AT&T Labs Re-search.Cong Li, Ji-Rong Wen, and Hang Li.
2003.
Text clas-sification using stochastic keyword generation.
InProceedings of the 20th International Conference onMachine Learning (ICML-2003).Rada Mihalcea and Samer Hassan.
2005.
Using theessence of texts to improve document classifica-tion.
In Proceedings of the Conference on RecentAdvances in Natural Language Processing (RANLP2005).Rada Mihalcea and Paul Tarau.
2004.
TextRank:bringing order into texts.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2004).Alessandro Moschitti and Roberto Basili.
2004.
Com-plex linguistic features for text classification: Acomprehensive study.
In Sharon McDonald andJohn Tait, editors, Proceedings of ECIR-04, 26thEuropean Conference on Information Retrieval Re-search, pages 181?196.
Springer-Verlag.Arzucan ?Ozgu?r, Levent ?Ozgu?r, and Tunga Gu?ngo?r.2005.
Text categorization with class-based andcorpus-based keyword selection.
In Proceedingsof the 20th International Symposium on Computerand Information Sciences, volume 3733 of Lec-ture Notes in Computer Science, pages 607?616.Springer-Verlag.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Magnus Sahlgren and Rickard Co?ster.
2004.
Usingbag-of-concepts to improve the performance of sup-port vector machines in text categorization.
In Pro-ceedings of the 20th International Conference onComputational Linguistics (COLING 2004), pages487?493.Yiming Yang and Xin Liu.
1999.
A re-examinationof text categorization methods.
In Proceedings ofthe 22nd Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 42?49.544
