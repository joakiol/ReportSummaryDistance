Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1680?1690,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsDe-Conflated Semantic RepresentationsMohammad Taher Pilehvar and Nigel CollierLanguage Technology LabDepartment of Theoretical and Applied LinguisticsUniversity of CambridgeCambridge, UK{mp792,nhc30}@cam.ac.ukAbstractOne major deficiency of most semantic repre-sentation techniques is that they usually modela word type as a single point in the semanticspace, hence conflating all the meanings thatthe word can have.
Addressing this issue bylearning distinct representations for individ-ual meanings of words has been the subject ofseveral research studies in the past few years.However, the generated sense representationsare either not linked to any sense inventory orare unreliable for infrequent word senses.
Wepropose a technique that tackles these prob-lems by de-conflating the representations ofwords based on the deep knowledge that canbe derived from a semantic network.
Our ap-proach provides multiple advantages in com-parison to the previous approaches, includingits high coverage and the ability to generateaccurate representations even for infrequentword senses.
We carry out evaluations on sixdatasets across two semantic similarity tasksand report state-of-the-art results on most ofthem.1 IntroductionModeling the meanings of linguistic items in amachine-interpretable form, i.e., semantic represen-tation, is one of the oldest, yet most active, areasof research in Natural Language Processing (NLP).The field has recently experienced a resurgence ofinterest with neural network-based models that viewthe representation task as a language modeling prob-lem and learn dense representations (usually re-ferred to as embeddings) by efficiently processingmassive amounts of texts.
However, either in itsconventional count-based form (Turney and Pantel,2010) or the recent predictive approach, the prevail-ing objective of representing each word type as asingle point in the semantic space has a major limita-tion: it ignores the fact that words can have multiplemeanings and conflates all these meanings into a sin-gle representation.
This objective can have negativeimpacts on accurate semantic modeling, e.g., seman-tically unrelated words that are synonymous to dif-ferent senses of a word are pulled towards each otherin the semantic space (Neelakantan et al, 2014).For example, the two semantically-unrelated wordssquirrel and keyboard are pulled towards each otherin the semantic space for their similarities to two dif-ferent senses of mouse, i.e., rodent and computer in-put device.Recently, there has been a growing interest in ad-dressing the meaning conflation deficiency of wordrepresentations.
A series of techniques have beendeveloped to associate a word to multiple pointsin the semantic space by clustering its contextsin a given text corpus and learning distinct rep-resentations for individual clusters (Reisinger andMooney, 2010; Huang et al, 2012).
However, thesetechniques usually assume a fixed number of wordsenses per word type, disregarding the fact that thenumber of senses per word can range from one(monosemy) to dozens.
Neelakantan et al (2014)tackled this issue by allowing the number to be dy-namically adjusted for each word during training.However, the approach and all the other clustering-based techniques still suffer from the fact that thecomputed sense representations are not linked to1680any sense inventory, a linking which would requirelarge amounts of sense-annotated data (Agirre et al,2006).
In addition, because of their dependence onknowledge derived from a text corpus, these tech-niques are generally unable to learn accurate repre-sentations for word senses that are infrequent in theunderlying corpus.Knowledge-based techniques tackle these issuesby deriving sense-specific knowledge from exter-nal sense inventories, such as WordNet (Fellbaum,1998), and learning representations that are linkedto the sense inventory.
These approaches either usesense definitions and employ Word Sense Disam-biguation (WSD) to gather sense-specific contexts(Chen et al, 2014; Iacobacci et al, 2015) or takeadvantage of the properties of WordNet, such assynonymy and direct semantic relations (Rothe andSchu?tze, 2015).
However, the non-optimal WSDtechniques and the shallow utilization of knowl-edge from WordNet do not allow these techniquesto learn accurate and high-coverage semantic repre-sentations for all senses in the inventory.We propose a technique that de-conflates a givenword representation into its constituent sense repre-sentations by exploiting deep knowledge from thesemantic network of WordNet.
Our approach pro-vides the following three main advantages in com-parison to the past work: (1) our representations arelinked to the WordNet sense inventory and, accord-ingly, the number of senses for a word is a dynamicparameter which matches that defined by WordNet;(2) the deep exploitation of WordNet?s semantic net-work allows us to obtain accurate semantic repre-sentations, even for word senses that are infrequentin generic text corpora; and (3) our methodology in-volves only minimal parameter tuning and can be ef-fectively applied to any sense inventory that is view-able as a semantic network and to any word repre-sentation technique.
We evaluate our sense repre-sentations in two tasks: word similarity (both in-context and in-isolation) and cross-level semanticsimilarity.
Experimental results show that the pro-posed technique can provide consistently high per-formance across six datasets, outperforming the re-cent state of the art on most of them.2 De-Conflated RepresentationsPreliminaries.
Our proposed approach takes a setof pre-trained word representations and uses thegraph structure of a semantic lexical resource in or-der to de-conflate the representations into those ofword senses.
Therefore, our approach firstly re-quires a set of pre-trained word representations (e.g.,word embeddings).
Any model that maps a givenword to a fixed-size vector representation (i.e., vec-tor space model) can be used by our approach.
In ourexperiments, we opted for a set of publicly availableword embeddings (cf.
?3.1).Secondly, we require a lexical resource whose se-mantic relations allow us to view it as a graph G =(V,E) where each vertex in the set of vertices V cor-responds to a concept and edges in E denote lexico-semantic relationships among these vertices.
Eachconcept c ?
V is mapped to a set of word senses bya mapping function ?
(c) : c?
{s1, .
.
.
, sl}.
Word-Net, the de facto community standard sense inven-tory, is a suitable resource that satisfies these prop-erties.
WordNet can be readily represented as a se-mantic graph in which vertices are synsets and edgesare the semantic relations that connect these synsets(e.g., hypernymy and meronymy).
The mappingfunction in WordNet maps each synset to the set ofsynonymous words it contains (i.e., word senses).2.1 Overview of the approachOur goal is to compute a semantic representationthat places a given word sense in an existing seman-tic space of words.
We achieve this by leveragingword representations as well as the knowledge de-rived from WordNet.
The gist of our approach liesin its computation of a list of sense biasing words fora given word sense.
To this end, we first analyze thesemantic network of WordNet and extract a list ofmost representative words that can effectively pin-point the semantics of individual synsets (?2.2).
Wethen leverage an effective technique which learns se-mantic representations for individual word senses byplacing the senses in the proximity of their corre-sponding sense biasing words (?2.3).2.2 Determining sense biasing wordsAlgorithm 1 shows the procedure we use to extractfrom WordNet a list of sense biasing words for a1681Algorithm 1 Get sense biasing words for synset ytRequire: Graph G = (V,E) of vertices V = {yi}mi=1(of m synsets) and edges E (semantic relationshipsbetween synsets)Require: Function ?
(yi) that returns for a given synsetyi the words it containsRequire: Target synset yt ?
V for which a sense biasingword sequence is requiredEnsure: The sequence Bt of sense biasing words forsynset yt1: Bt?
()2: for all word w in ?
(yt) do3: Bt?
Bt ?
(w)4: for yi ?
V : yi 6= yt do5: pi?
PERSONALIZEDPAGERANK(yi, yt, G)6: (y?h)m?1h=1 ?
SORT(V \{yt}) according to scores pi7: for h : 1 to m?
1 do8: for all word w in ?
(y?h) do9: if w /?
Bt then10: Bt ?
Bt ?
(w)11: return sequence Btgiven target synset yt.
The algorithm receives as itsinputs the semantic graph of WordNet and the map-ping function ?(?
), and outputs an ordered list of bi-asing words Bt for yt.
The list comprises the mostsemantically-related words to synset yt which canbest represent and pinpoint its meaning.
We lever-age a graph-based algorithm for the computation ofthe sense biasing words.Specifically, we use the Personalized PageRank(Haveliwala, 2002, PPR) algorithm which has beenextensively used by several NLP applications (Yehet al, 2009; Niemann and Gurevych, 2011; Agirreet al, 2014).
To this end, we first represent the se-mantic network of WordNet as a row-stochastic tran-sition matrixM ?
Rm?m wherem is the number ofsynsets in WordNet (|V |).
The cell Mij ofM is setto the inverse of the degree of i if there is a seman-tic relationship between synsets i and j and to zerootherwise.
We compute the PPR distribution for atarget synset yt by using the power iteration methodPt+1 = (1?
?
)P0 + ?MPt, where ?
is the damp-ing factor (usually set to 0.85) and P0 is a one-hotinitialization vector with the corresponding dimen-sion of yt being set to 1.0.
The weight pi in line 5is the value of the ith dimension of the PPR vectorP computed for the synset yt.
This weight can beseen as the importance of the corresponding synset# Sense biasing words1 dactyl, finger, toe, thumb, pollex, body part, nail, minimus,tarsier, webbed, extremity, appendage2 figure, cardinal number, cardinal, integer, whole number,numeration system, number system, system of numeration,large integer, constituent, element, digitalTable 1: The top sense biasing words for the synsets containingthe anatomical (#1) and numerical (#2) senses of the noun digit.of the ith dimension (i.e., yi) to yt.
When appliedto a semantic network, such as the WordNet graph,this importance can be interpreted as semantic rel-evance.
Hence, the value of pi denotes the extentof semantic relatedness between yi and yt.
We usethis notion and retrieve a list of most semantically-related words to yt.To achieve this, we sort the synsets {y?
?
V :y?
6= yt} according to their PPR values {pi}m?1i=1(line 6).
We then iterate (lines 7-10) the sorted list(y?)
and for each synset y?h append the list Bt withall the words in y?h (i.e., ?(y?h)).
However, in order toensure that the words in the target synset yt appear asthe most representative words in Bt, we first assignthese words to the list (line 3).
Finally, the algorithmreturns the ordered list Bt of sense biasing words forthe target synset yt.Table 1 shows a sample of top biasing words ex-tracted for the two senses of the noun digit: thenumerical and the anatomical senses.1 We explainin ?2.3 how we use the sense biasing lists to learnsense-specific representations.
Note that the sizeof the list is equal to the total number of stringsin WordNet.
However, we observed that taking avery small portion of the top-ranking elements in thelists is enough to generate representations that per-form very similarly to those generated when usingthe full-sized lists (please see ?3.1).2.3 Learning sense representationsLet V be the set of pre-trained d-dimensional wordrepresentations.
Our objective here is to computea set V?
= {v?s1 , .
.
.
, v?sn} of representations for nword senses {s1, .
.
.
, sn} in the same d-dimensionalsemantic space of words.
We achieve this for eachsense si by de-conflating the representation vsi ofits corresponding lemma and biasing it towards the1The first and third senses of the noun digit in WordNet 3.0.1682representations of the words in Bi.
Specifically, weobtain a representation v?si for a word sense si bysolving:argminv?si?
d(v?si , vsi) +?bij?Bi?ij d(v?si , vbij ) (1)where vsi and vbij are the respective word repre-sentations (?
V) of the lemma of si and the jthbiasing word in the list of biasing words for si,i.e, Bi.
The distance d(v, v?)
between vectors vand v?
is measured by squared Euclidean distance?v ?
v?
?2= ?k(vk ?
v?k)2.
The first term in For-mula 1 requires the representation of the word sensesi (i.e., v?si) to be similar to that of its correspondinglemma, i.e., vsi , whereas the second term encour-ages v?si to be in the proximity of its biasing wordsin the semantic space.
The above criterion is simi-lar to the frameworks of Das and Smith (2011) andFaruqui et al (2015) which, though being convex, isusually solved for efficiency reasons by an iterativemethod proposed by Bengio et al (2007).
Follow-ing these works, we obtain the below equation forcomputing the representation of a word sense si:v?si =?vsi +?bij?Bi ?ijvbij?+?j ?ij.
(2)We define ?ij as e?
?r(i,j)|Bi| where r(i, j) denotes therank of the word bij in the list Bi.
This is essen-tially an exponential decay function that gives moreimportance to the top-ranking biasing words for si.The hyperparameter ?
denotes the extent to whichv?si is kept close to its corresponding lemma repre-sentation vsi .
Following Faruqui et al (2015), weset ?
to 1.
The only parameter to be tuned in ourexperiments is ?.
We discuss the tuning of this pa-rameter in ?3.1.
The representation of a synset yican be accordingly calculated as the centroid of thevectors of its associated word senses, i.e.,{ vyi?vyi?
: vyi =?s??(yi)v?
?s , v?
?s =v?s?v?s?}.
(3)As a result of this procedure, we obtain the setV?
of n sense representations in the same semanticspace as word representations in V .
In fact, we nowhave a unified semantic space which enables a directcomparison of the two types of linguistic items.
In# Closest words1 crappie, trout, guitar, shad, walleye, bassist, angler, catfish,trombone, percussion, piano, drummer, saxophone, jigs, fish2 baritone, piano, guitar, trombone, saxophone, cello, percussion,tenor, saxophonist, clarinet, pianist, vocals, solos, harmonica3 fish, trout, shrimp, anglers, fishing, bait, guitar, salmon,shark, fisherman, lakes, seafood, drummer, whale, fisheriesTable 2: Ten most similar words to the word bass (#1) and twoof its senses: music (#2) and fish (#3).Figure 1: The illustration of the word digit and two of its com-puted senses in our unified 2-d semantic space.
?3.3 we evaluate our approach in the word to sensesimilarity measurement framework.We show in Table 2 the closest words to the wordbass and two of its senses, music and fish,2 in ourunified semantic space.
We can see in row #1 a mix-ture of both meanings when the word representationis used whereas the closest words to the senses (rows#2 and #3) are mostly in-domain and specific to thecorresponding sense.To exhibit another interesting property of oursense representation approach, we depict in Figure1 the word digit and its numerical and anatomicalsenses (from the example in Table 1) in a 2-d seman-tic space, along with a sample set of words in their2The first and fourth senses in WordNet 3.0, respectivelydefined as ?the lowest part of the musical range?
and ?the leanflesh of a saltwater fish of the family Serranidae.
?1683proximity.3 We can see that the word digit is placedin the semantic space in the neighbourhood of wordsfrom the numerical domain (lower left of the figure),mainly due the dominance (Sanderson and Van Ri-jsbergen, 1999) of this sense in the general-domaincorpus on which the word embeddings in our ex-periments were trained (cf.
?3.1).
However, uponde-conflation, the emerging anatomical sense of theword is shifted towards the region in the semanticspace which is occupied by anatomical words (up-per right of the figure).
A clustering-based senserepresentation technique would have failed in accu-rately representing the infrequent anatomical mean-ing of digit by analyzing a general domain corpus(such as the one used here).
But our sense repre-sentation technique, thanks to its proper usage ofknowledge from a sense inventory, is effective inunveiling and accurately modeling less frequent ordomain-specific senses of a given word.Please note that any vector space model represen-tation technique can be used for the pre-training ofword representations in V .
Also, the list of sensebiasing words can be obtained for larger sense in-ventories, such as FreeBase (Bollacker et al, 2008)or BabelNet (Navigli and Ponzetto, 2012).
We leavethe exploration of further ways of computing sensebiasing words to future work.3 ExperimentsWe benchmarked our sense representation approachagainst several recent techniques on two standardtasks: word similarity (?3.2), for which we eval-uate on both in-isolation and in-context similaritydatasets, and cross-level semantic similarity (?3.3).3.1 Experimental setupPre-trained word representations.
As our wordrepresentations, we used the 300-d Word2vec(Mikolov et al, 2013) word embeddings trained onthe Google News dataset4 mainly for their popular-ity across different NLP applications.
However, ourapproach is equally applicable to any count-basedrepresentation technique (Baroni and Lenci, 2010;Turney and Pantel, 2010) or any other embedding3We used the t-SNE algorithm (van der Maaten and Hinton,2008) for dimensionality reduction.4https://code.google.com/archive/p/word2vec/approach (Pennington et al, 2014; LeCun et al,2015).
We leave the evaluation and comparison ofvarious word representation techniques with differ-ent training approaches, objectives, and dimension-alities to future work.Parameter tuning.
Recall from ?2.3 that our pro-cedure for learning sense representations needs onlyone parameter to be tuned, i.e., ?.
We did not per-form an extensive tuning on the value of this param-eter and set its value to 1/5 after trying four differ-ent values (1, 1/2, 1/5, and 1/10) on a small validationdataset.
We leave the more systematic tuning of theparameter and the choice of alternative decay func-tions (cf.
?2.3) to future work.The size of the sense biasing words lists.
Alsorecall from ?2.2 that the extracted lists of sense bias-ing words were originally as large as the total num-ber of unique strings in WordNet (around 150K inver.
3.0).
But, given that we use an exponential de-cay function in our learning algorithm (cf.
?2.3),the impact of the low-ranking words in the list isnegligible.
In fact, we observed that taking a verysmall portion of the top-ranking words, i.e., the top25, produces similarity scores that are on par withthose generated when the full lists were considered.Therefore, we experimented with the down-sizedlists which enabled us to generate very quickly senserepresentations for all word senses in WordNet.3.2 Word similarityComparison systems.
We compared our resultsagainst nine other sense representation techniques:the WordNet-based approaches of Pilehvar and Nav-igli (2015), Chen et al (2014), Rothe and Schu?tze(2015), Jauhar et al (2015), and Iacobacci etal.
(2015) and the clustering-based approaches ofHuang et al (2012), Tian et al (2014), Neelakan-tan et al (2014), and Liu et al (2015) (please see?4 for more details).
We also compared against theapproach of Faruqui et al (2015) which uses knowl-edge derived from WordNet for improving word rep-resentations.
From the different configurations pre-sented in (Faruqui et al, 2015) we chose the sys-tem that uses GloVe (Pennington et al, 2014) withall WordNet relations which is their best perform-ing monolingual system.
As for the approach ofJauhar et al (2015), we show the results of the1684EM+RETERO system which performs most consis-tently across different datasets.Benchmarks.
As our word similarity benchmark,we considered five datasets: RG-65 (Rubensteinand Goodenough, 1965), YP-130 (Yang and Pow-ers, 2005), MEN-3K (Bruni et al, 2014), SimLex-999 (Hill et al, 2015, SL-999), and Stanford Con-textual Word Similarity (Huang et al, 2012, SCWS).The latter benchmark provides for each word a con-text that triggers a specific meaning of it, making thedataset very suitable for the evaluation of sense rep-resentation.
For each datasets, we list the results thatare reported by any of our comparison systems.Similarity measurement.
For the SCWS dataset,we follow the past works (Reisinger and Mooney,2010; Huang et al, 2012) and report the results ac-cording to two system configurations: (1) AvgSim:where the similarity between two words is computedas the average of all the pairwise similarities be-tween their senses, and (2) AvgSimC: where eachpairwise sense similarity is weighted by the rele-vance of each sense to its corresponding context.
Forall the other datasets, since words are not providedwith any context (they are in isolation), we measurethe similarity between two words as that betweentheir most similar senses.
In all the experiments, weuse the cosine distance as our similarity measure.3.2.1 Experimental resultsTables 4 and 3 show the results of our system, DE-CONF, and the comparison systems on the SCWSand the other four similarity datasets, respectively.In both tables we also report the word vectors base-line, whenever they are available, which is computedby directly comparing the corresponding word rep-resentations of the two words (?
V).
Note that theword-based baseline does not apply to the approachof Pilehvar and Navigli (2015) as it is purely basedon the semantic network of WordNet and does notuse any pre-trained word embeddings.We can see from the tables that our sense rep-resentations obtain considerable improvements overthose of words across the five datasets.
This high-lights the fact that the de-conflation of word rep-resentations into those of their individual meaningshas been highly beneficial.
On the SCWS dataset,DECONF outperforms all the recent state-of-the-artsense representation techniques (in their best set-tings) which proves the effectiveness of our ap-proach in capturing the semantics of specific mean-ings of the words.
The improvement is consistentacross both system configurations (i.e., AvgSim andAvgSimC).
Moreover, the state-of-the-art WordNet-based approach of Rothe and Schu?tze (2015) usesthe same initial word vectors as DECONF does (cf.?3.1).
Hence, the improvement we obtain indicatesthat our approach has made better use of the sense-specific knowledge encoded in WordNet.As seen in Table 3 our approach shows com-petitive performance on the other four datasets.The YP-130 dataset focuses on verb similarity,whereas SimLex-999 contains verbs and adjectivesand MEN-3K has word pairs with different parts ofspeech (e.g., a noun compared to a verb).
The resultswe obtain on these datasets exhibit the reliability ofour approach in modeling non-nominal word senses.3.2.2 DiscussionThe similarity scale of the SimLex-999 dataset isdifferent from our other word similarity benchmarksin that it assigns relatively low scores to antonymouspairs.
For instance, sunset-sunrise and man-womanin this dataset are assigned the respective similari-ties of 2.47 and 3.33 (in a [0, 10] similarity scale)which is in the same range as the similarity betweenword pairs with slight domain relatedness, such ashead-nail (2.47), air-molecule (3.05), or succeed-try(3.98).
In fact, we observed that tweaking the simi-larity scale of our system in a way that it diminishesthe similarity scores between antonyms can resultin a significant performance improvement on thisdataset.
To this end, we performed an experimentin which the similarity of a word pair was simplydivided by 3 whenever the two words belonged tosynsets that were linked by an antonymy relation inWordNet.5 We observed that the performance on theSimLex-999 dataset increased to 61.6 (from 54.2)and 59.1 (from 51.7) according to Pearson (r?
100)and Spearman (?
?
100) correlation scores, respec-tively.5We chose 3 so as to transform a pair with high similar-ity score (around 9.0) to one with slight semantic similarity(around 3.0) in the [0, 10] similarity scale of SimLex-999.
Wealso tested for other values in [2, 6] an observed similar perfor-mance gains.1685Dataset Approach Sense-based score Word-based scorer ?
r ?MEN-3KIacobacci et al (2015) ?
80.5 ?
66.5DECONF 78.0 78.6 72.3 73.2Faruqui et al (2015) ?
75.9 ?
73.7Pilehvar and Navigli (2015) 61.7 66.6 ?
?RG-65DECONF 90.5 89.6 77.2 76.1Iacobacci et al (2015) ?
87.1 ?
73.2Faruqui et al (2015) ?
84.2 ?
76.7Pilehvar and Navigli (2015) 80.2 84.3 ?
?YP-130DECONF 81.6 75.2 58.0 55.9Pilehvar and Navigli (2015) 79.0 71.0 ?
?Iacobacci et al (2015) ?
63.9 ?
34.3SimLex-999 DECONF 54.2 51.7 45.4 44.2Pilehvar and Navigli (2015) 43.4 43.6 ?
?Table 3: Pearson (r ?
100) and Spearman (?
?
100) correlation scores on four standard word similarity benchmarks.
For eachbenchmark, we show the results reported by any of the comparison systems along with the scores for their corresponding initialword representations (word-based).Approach ScoreAvgSim AvgSimCDECONF 70.8 71.5Rothe and Schu?tze (2015) (best) 68.9 69.8Neelakantan et al (2014) (best) 67.3 69.3Chen et al (2014) 66.2 68.9Liu et al (2015) (best) ?
68.1Huang et al (2012) 62.8 65.7Tian et al (2014) (best) ?
65.7Iacobacci et al (2015) 62.4 ?Jauhar et al (2015) ?
58.7Initial word vectors 65.1Table 4: Spearman correlation scores (?
?
100) on the Stan-ford Contextual Word Similarity (SCWS) dataset.
We reportthe AvgSim and AvgSimC scores (cf.
?3.2) for each system,where available.3.3 Cross-Level semantic similarityIn addition to the word similarity benchmark, weevaluated the performance of our representationsin the cross-level semantic similarity measurementframework.
For this, we opted for the SemEval-2014 task on Cross-Level Semantic Similarity (Ju-rgens et al, 2014, CLSS).
The word to sense simi-larity subtask of this task, with 500 instances in itstest set, provides a suitable benchmark for the eval-uation of sense representation techniques.For a word sense s and a word w, we compute thesimilarity score according to four different strate-gies: the similarity of s to the most similar senseof w (MaxSim), the average similarity of s to indi-vidual senses of w (AvgSim), the direct similarity ofs to w when the latter is modeled as its word repre-sentation (Sense-to-Word or S2W) or as the centroidof its senses?
representations (Sense to aggregatedword senses or S2A).
For this task, we can only com-pare against the publicly-available sense representa-tions of Iacobacci et al (2015), Rothe and Schu?tze(2015), Pilehvar and Navigli (2015) and Chen et al(2014) which are linked to the WordNet sense inven-tory.3.3.1 Experimental resultsTable 5 shows the results on the word to sensedataset of the SemEval-2014 CLSS task, accordingto Pearson (r ?
100) and Spearman (?
?
100) cor-relation scores and for the four strategies.
As canbe seen from the low overall performance, the taskis a very challenging benchmark with many Word-Net out-of-vocabulary or slang terms and rare us-ages.
Despite this, DECONF provides consistent im-provement over the comparison sense representationtechniques according to both measures and for allthe strategies.Across the four strategies, S2A proves to be themost effective for DECONF and the representationsof Rothe and Schu?tze (2015).
The representations ofChen et al (2014) perform best with the S2W strat-1686System MaxSim AvgSim S2W S2Ar ?
r ?
r ?
r ?DECONF?
36.4 37.6 36.8 38.8 34.9 35.6 37.5 39.3Rothe and Schu?tze (2015)?
34.0 33.8 34.1 33.6 33.4 32.0 35.4 34.9Iacobacci et al (2015)?
19.1 21.5 21.3 24.2 22.7 21.7 19.5 21.1Chen et al (2014)?
17.7 18.0 17.2 16.8 27.7 26.7 17.9 18.8DECONF 35.5 36.4 36.2 38.0 34.9 35.6 36.8 38.4Pilehvar and Navigli (2015) 19.4 23.8 21.2 26.0 ?
?
?
?Iacobacci et al (2015) 19.0 21.5 20.9 23.2 22.3 20.6 19.2 20.4Table 5: Evaluation results on the word to sense similarity test dataset of the SemEval-14 task on Cross-Level Semantic Similarity,according to Pearson (r ?
100) and Spearman (?
?
100) correlations.
We show results for four similarity computation strategies(see ?3.3).
The best results per strategy are shown in bold whereas they are underlined for the best strategies per system.
Systemsmarked with ?
are evaluated on a slightly smaller dataset (474 of the original 500 pairs) so as to have a fair comparison with Rotheand Schu?tze (2015) and Chen et al (2014) that use older versions of WordNet (1.7.1 and 1.7, respectively).egy whereas those of Iacobacci et al (2015) do notshow a consistent trend with relatively low perfor-mance across the four strategies.
Also, a comparisonof our results across the S2W and S2A strategies re-veals that a word?s aggregated representation, i.e.,the centroid of the representations of its senses, ismore accurate than its original word representation.Our analysis showed that the performance of theapproaches of Rothe and Schu?tze (2015) and Ia-cobacci et al (2015) were hampered partly due totheir limited coverage.
In fact, the former was un-able to model around 35% of the synsets in WordNet1.7.1, mainly for its shallow exploitation of knowl-edge from WordNet, whereas the latter approach didnot cover around 15% of synsets in WordNet 3.0.Chen et al (2014) provide near-full coverage forword senses in WordNet.
However, the relativelylow performance of their system shows that the us-age of glosses in WordNet and the automated dis-ambiguation have not resulted in accurate sense rep-resentations.
Thanks to its deep exploitation of theunderlying resource, our approach provides more re-liable representations and full coverage for all wordsenses and synsets in WordNet.The three best-performing systems in the task areMeerkat Mafia (Kashyap et al, 2014) (r = 37.5,?
= 39.3), SimCompass (Banea et al, 2014) (r =35.4, ?
= 34.9), and SemantiKLUE (Proisl et al,2014) (r = 17.9, ?
= 18.8).
Note that these systemsare specifically designed for the cross-level similar-ity measurement task.
For instance, the best-rankingsystem in the task leverages a compilation of severaldictionaries, including The American Heritage Dic-tionary, Wiktionary and WordNet, in order to handleslang terms and rare usages, which leads to its com-petitive performance (Kashyap et al, 2014).4 Related WorkLearning semantic representations for individualsenses of words has been an active area of researchfor the past few years.
Based on the way they viewthe problem, the recent techniques can be classifiedinto two main branches: (1) those that, similarly toour work, extract knowledge from external sense in-ventories for learning sense representations; and (2)those techniques that cluster the contexts in which aword appears in a given text corpus and learn distinctrepresentations for individual clusters.Examples for the first branch include the ap-proaches of Chen et al (2014), Jauhar et al (2015)and Rothe and Schu?tze (2015), all of which useWordNet as an external resource and obtain senserepresentations for this sense inventory.
Chen etal.
(2014) uses the content words in the definitionof a word sense and WSD.
However, the sole us-age of glosses as sense-distinguishing contexts andthe non-optimal WSD make the approach inaccu-rate, particularly for highly polysemous words withsimilar senses and for word senses with short def-initions.
Similarly, Rothe and Schu?tze (2015) useonly polysemy and synonymy properties of wordsin WordNet alng with a small set of semantic re-1687lations.
This significantly hampers the reliability ofthe technique in providing high coverage (discussedfurther in ?3.3.1).
Our approach improves over theseworks by exploiting deep knowledge from the se-mantic network of WordNet, coupled with an effec-tive training approach.
ADW (Pilehvar and Navigli,2015) is another WordNet-based approach which ex-ploits only the semantic network of this resource andobtains interpretable sense representations.
Otherwork in this branch include SensEmbed (Iacobacciet al, 2015) and Nasari (Camacho-Collados et al,2015; Camacho-Collados et al, 2016) which arebased on the BabelNet sense inventory (Navigliand Ponzetto, 2012).
The former technique firstdisambiguates words in a given corpus with thehelp of a knowledge-based WSD system and thenuses the generated sense-annotated corpus as train-ing data for Word2vec.
Nasari combines structuralknowledge from the semantic network of BabelNetwith corpus statistics derived from Wikipedia forrepresenting BabelNet synsets.
However, the ap-proach falls short of modeling non-nominal sensesas Wikipedia, due to its very encyclopedic nature,does not cover verbs, adjectives, or adverbs.The second branch, which is usually referred toas multi-prototype representation, is often associ-ated with clustering.
Reisinger and Mooney (2010)proposed one of the recent pioneering techniquesin this branch.
Other prominent work in the cate-gory include topical word embeddings (Liu et al,2015) which use latent topic models for assigningtopics to each word in a corpus and learn topic-specific word representations, and the technique pro-posed by Huang et al (2012) which incorporates?global document context.?
Tian et al (2014) mod-ified the Skip-gram model in order to learn multi-ple embeddings for each word type.
Despite the factthat these techniques do not usually take advantageof the knowledge encoded in structured knowledgeresource, they generally suffer from two disadvan-tages.
The first limitation is that they usually makean assumption that a given word has a fixed numberof senses, ignoring the fact that polysemy is highlydynamic across words that can range from monose-mous to highly ambiguous with dozens of associ-ated meanings (McCarthy et al, 2016).
Neelakan-tan et al (2014) tackled this issue by estimating thenumber of senses for a word type during the learn-ing process.
However, all techniques in the secondbranch suffer from another disadvantage that theircomputed sense representations are not linked to anysense inventory, a linking which itself would requirethe existence of high coverage sense-annotated data(Agirre et al, 2006).Another notable line of research incorporatesknowledge from external resources, such as PPDB(Ganitkevitch et al, 2013) and WordNet, to improveword embeddings (Yu and Dredze, 2014; Faruqui etal., 2015).
Neither of the two techniques however,provide representations for word senses.5 ConclusionsWe put forward a sense representation technique,namely DECONF, that provides multiple advantagesin comparison to the recent state of the art: (1) thenumber of word senses in our technique is flexi-ble and the computed representations are linked toword senses in WordNet; (2) DECONF is effectivein providing accurate representation of word senses,even for those senses that do not usually appear fre-quently in generic text corpora; and (3) our approachis general in that it can be readily applied to any setof word representations and any semantic networkwithout the need for extensive parameter tuning.Our experimental results showed that DECONF canoutperform recent state of the art on several datasetsacross two tasks.
The computed representations forword senses in WordNet 3.0 are released at https://pilehvar.github.io/deconf/.
We in-tend to apply our technique to the task of harmo-nizing biomedical terms in the PheneBank project.As future work, we plan to investigate the possibil-ity of using larger semantic networks, such as Free-Base and BabelNet, which would also allow us toapply the technique to languages other than English.We also plan to evaluate the performance of our ap-proach with other decay functions as well as withother initial word representations.AcknowledgmentsThe authors gratefully acknowledge the support ofthe MRC grant No.
MR/M025160/1 for PheneBank.1688References[Agirre et al2006] Eneko Agirre, David Mart?
?nez,Oier Lo?pez de Lacalle, and Aitor Soroa.
2006.Evaluating and optimizing the parameters of an unsu-pervised graph-based wsd algorithm.
In Proceedingsof the First Workshop on Graph Based Methods forNatural Language Processing, TextGraphs-1, pages89?96.
[Agirre et al2014] Eneko Agirre, Oier Lo?pez de Lacalle,and Aitor Soroa.
2014.
Random walks for knowledge-based Word Sense Disambiguation.
ComputationalLinguistics, 40(1):57?84.
[Banea et al2014] Carmen Banea, Di Chen, Rada Mihal-cea, Claire Cardie, and Janyce Wiebe.
2014.
Simcom-pass: Using deep learning word embeddings to assesscross-level similarity.
In Proceedings of the 8th Inter-national Workshop on Semantic Evaluation (SemEval2014), pages 560?565, Dublin, Ireland.
[Baroni and Lenci2010] Marco Baroni and AlessandroLenci.
2010.
Distributional memory: A generalframework for corpus-based semantics.
Computa-tional Linguistics, 36(4):673?721.
[Bengio et al2007] Yoshua Bengio, Olivier Delalleau,and Nicolas Le Roux.
2007.
Semi-Supervised Learn-ing.
MIT Press.
chapter Label Propagation andQuadratic Criterion.
[Bollacker et al2008] Kurt Bollacker, Colin Evans,Praveen Paritosh, Tim Sturge, and Jamie Taylor.2008.
Freebase: A collaboratively created graphdatabase for structuring human knowledge.
InProceedings of the 2008 ACM SIGMOD Interna-tional Conference on Management of Data, pages1247?1250, Vancouver, Canada.
[Bruni et al2014] Elia Bruni, Nam Khanh Tran, andMarco Baroni.
2014.
Multimodal distributional se-mantics.
Journal of Artificial Intelligence Research,49(1):1?47.
[Camacho-Collados et al2015] Jose?
Camacho-Collados,Mohammad Taher Pilehvar, and Roberto Navigli.2015.
NASARI: a Novel Approach to a Semantically-Aware Representation of Items.
In Proceedings ofNAACL, pages 567?577, Denver, USA.
[Camacho-Collados et al2016] Jose?
Camacho-Collados,Mohammad Taher Pilehvar, and Roberto Navigli.2016.
NASARI: Integrating explicit knowledge andcorpus statistics for amultilingual representation ofconcepts and entities.
Artificial Intelligence, 240:36?
64.
[Chen et al2014] Xinxiong Chen, Zhiyuan Liu, andMaosong Sun.
2014.
A unified model for word senserepresentation and disambiguation.
In Proceedings ofEMNLP 2014, pages 1025?1035, Doha, Qatar.
[Das and Smith2011] Dipanjan Das and Noah A. Smith.2011.
Semi-supervised frame-semantic parsing forunknown predicates.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1435?1444, Portland, Oregon, USA.
[Faruqui et al2015] Manaal Faruqui, Jesse Dodge, Su-jay Kumar Jauhar, Chris Dyer, Eduard Hovy, andNoah A. Smith.
2015.
Retrofitting word vectors tosemantic lexicons.
In Proceedings of the 2015 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 1606?1615, Denver, Colorado.
[Fellbaum1998] Christiane Fellbaum, editor.
1998.WordNet: An Electronic Database.
MIT Press, Cam-bridge, MA.
[Ganitkevitch et al2013] Juri Ganitkevitch, BenjaminVan Durme, and Chris Callison-Burch.
2013.PPDB: The paraphrase database.
In Proceedings ofNAACL-HLT, pages 758?764, Atlanta, Georgia.
[Haveliwala2002] Taher H. Haveliwala.
2002.
Topic-sensitive PageRank.
In Proceedings of the 11th Inter-national Conference on World Wide Web, pages 517?526, Honolulu, Hawaii, USA.
[Hill et al2015] Felix Hill, Roi Reichart, and Anna Ko-rhonen.
2015.
SimLex-999: Evaluating semanticmodels with (genuine) similarity estimation.
Compu-tational Linguistics, 41(4):665?695.
[Huang et al2012] Eric H. Huang, Richard Socher,Christopher D. Manning, and Andrew Y. Ng.
2012.Improving word representations via global context andmultiple word prototypes.
In Proceedings of ACL,pages 873?882, Jeju Island, Korea.
[Iacobacci et al2015] Ignacio Iacobacci, Moham-mad Taher Pilehvar, and Roberto Navigli.
2015.SensEmbed: Learning sense embeddings for wordand relational similarity.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 95?105, Beijing, China.
[Jauhar et al2015] Sujay Kumar Jauhar, Chris Dyer, andEduard Hovy.
2015.
Ontologically grounded multi-sense representation learning for semantic vectorspace models.
In Proceedings of the 2015 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 683?693, Denver, Colorado.
[Jurgens et al2014] David Jurgens, Mohammad TaherPilehvar, and Roberto Navigli.
2014.
SemEval-2014task 3: Cross-level semantic similarity.
In Proceed-ings of the 8th International Workshop on SemanticEvaluation (SemEval 2014), pages 17?26, Dublin, Ire-land.1689[Kashyap et al2014] Abhay L. Kashyap, Lushan Han,Roberto Yus, Jennifer Sleeman, Taneeya W. Satya-panich, Sunil R Gandhi, and Tim Finin.
2014.Meerkat Mafia: Multilingual and Cross-Level Seman-tic Textual Similarity systems.
In Proceedings of the8th International Workshop on Semantic Evaluation,pages 416?423.
[LeCun et al2015] Yann LeCun, Yoshua Bengio, and Ge-offrey Hinton.
2015.
Deep learning.
Nature,521(7553):436?444.
[Liu et al2015] Yang Liu, Zhiyuan Liu, Tat-Seng Chua,and Maosong Sun.
2015.
Topical word embeddings.In Proceedings of the Twenty-Ninth AAAI Conferenceon Artificial Intelligence, pages 2418?2424.
[McCarthy et al2016] Diana McCarthy, Marianna Apid-ianaki, and Katrin Erk.
2016.
Word sense cluster-ing and clusterability.
Computational Linguistics, inpress.
[Mikolov et al2013] Tomas Mikolov, Kai Chen, GregCorrado, and Jeffrey Dean.
2013.
Efficient estimationof word representations in vector space.
In Workshopat International Conference on Learning Representa-tions, Scottsdale, Arizona.
[Navigli and Ponzetto2012] Roberto Navigli and Si-mone Paolo Ponzetto.
2012.
BabelNet: Theautomatic construction, evaluation and applicationof a wide-coverage multilingual semantic network.Artificial Intelligence, 193:217?250.
[Neelakantan et al2014] Arvind Neelakantan, JeevanShankar, Alexandre Passos, and Andrew McCallum.2014.
Efficient non-parametric estimation of multipleembeddings per word in vector space.
In Proceedingsof EMNLP 2014, pages 1059?1069, Doha, Qatar.
[Niemann and Gurevych2011] Elisabeth Niemann andIryna Gurevych.
2011.
The people?s Web meetslinguistic knowledge: Automatic sense alignment ofWikipedia and WordNet.
In Proceedings of the NinthInternational Conference on Computational Seman-tics, pages 205?214, Oxford, United Kingdom.
[Pennington et al2014] Jeffrey Pennington, RichardSocher, and Christopher Manning.
2014.
Glove:Global vectors for word representation.
In Pro-ceedings of EMNLP 2014, pages 1532?1543, Doha,Qatar.
[Pilehvar and Navigli2015] Mohammad Taher Pilehvarand Roberto Navigli.
2015.
From senses to texts: Anall-in-one graph-based approach for measuring seman-tic similarity.
Artificial Intelligence, 228:95?128.
[Proisl et al2014] Thomas Proisl, Stefan Evert, PaulGreiner, and Besim Kabashi.
2014.
SemantiK-LUE: Robust semantic similarity at multiple levels us-ing maximum weight matching.
In Proceedings ofSemEval-2014, pages 532?540, Dublin, Ireland.
[Reisinger and Mooney2010] Joseph Reisinger and Ray-mond J. Mooney.
2010.
Multi-prototype vector-spacemodels of word meaning.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 109?117, Los Angeles, Cali-fornia.
[Rothe and Schu?tze2015] Sascha Rothe and HinrichSchu?tze.
2015.
AutoExtend: Extending word em-beddings to embeddings for synsets and lexemes.In Proceedings of ACL, pages 1793?1803, Beijing,China.
[Rubenstein and Goodenough1965] Herbert Rubensteinand John B. Goodenough.
1965.
Contextual cor-relates of synonymy.
Communications of the ACM,8(10):627?633.
[Sanderson and Van Rijsbergen1999] Mark Sandersonand C. J.
Van Rijsbergen.
1999.
The impact onretrieval effectiveness of skewed frequency distri-butions.
ACM Transactions of Infmation Systems,17(4):440?465.
[Tian et al2014] Fei Tian, Hanjun Dai, Jiang Bian, BinGao, Rui Zhang, Enhong Chen, and Tie-Yan Liu.2014.
A probabilistic model for learning multi-prototype word embeddings.
In Proceedings of COL-ING 2014, the 25th International Conference on Com-putational Linguistics: Technical Papers, pages 151?160, Dublin, Ireland.
[Turney and Pantel2010] Peter D. Turney and PatrickPantel.
2010.
From frequency to meaning: Vectorspace models of semantics.
Journal of Artificial Intel-ligence Research, 37(1):141?188.
[van der Maaten and Hinton2008] L.J.P van der Maatenand G.E.
Hinton.
2008.
Visualizing high-dimensionaldata using t-SNE.
Journal of Machine Learning Re-search, 9: 25792605.
[Yang and Powers2005] Dongqiang Yang and DavidM.
W. Powers.
2005.
Measuring semantic similarityin the taxonomy of WordNet.
In Proceedings of theTwenty-eighth Australasian Conference on ComputerScience, volume 38, pages 315?322, Newcastle,Australia.
[Yeh et al2009] Eric Yeh, Daniel Ramage, Christo-pher D. Manning, Eneko Agirre, and Aitor Soroa.2009.
WikiWalk: Random walks on Wikipedia for se-mantic relatedness.
In Proceedings of the 2009 Work-shop on Graph-based Methods for Natural LanguageProcessing, pages 41?49, Suntec, Singapore.
[Yu and Dredze2014] Mo Yu and Mark Dredze.
2014.Improving lexical embeddings with semantic knowl-edge.
In Proceedings of the 52nd Annual Meetingof the Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 545?550, Baltimore,Maryland.1690
