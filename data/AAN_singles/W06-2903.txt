Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 14?20, New York City, June 2006. c?2006 Association for Computational LinguisticsNon-Local Modeling with a Mixture of PCFGsSlav Petrov Leon Barrett Dan KleinComputer Science Division, EECS DepartmentUniversity of California at BerkeleyBerkeley, CA 94720{petrov, lbarrett, klein}@eecs.berkeley.eduAbstractWhile most work on parsing with PCFGshas focused on local correlations betweentree configurations, we attempt to modelnon-local correlations using a finite mix-ture of PCFGs.
A mixture grammar fitwith the EM algorithm shows improve-ment over a single PCFG, both in parsingaccuracy and in test data likelihood.
Weargue that this improvement comes fromthe learning of specialized grammars thatcapture non-local correlations.1 IntroductionThe probabilistic context-free grammar (PCFG) for-malism is the basis of most modern statisticalparsers.
The symbols in a PCFG encode context-freedom assumptions about statistical dependenciesin the derivations of sentences, and the relative con-ditional probabilities of the grammar rules inducescores on trees.
Compared to a basic treebankgrammar (Charniak, 1996), the grammars of high-accuracy parsers weaken independence assumptionsby splitting grammar symbols and rules with ei-ther lexical (Charniak, 2000; Collins, 1999) or non-lexical (Klein and Manning, 2003; Matsuzaki et al,2005) conditioning information.
While such split-ting, or conditioning, can cause problems for sta-tistical estimation, it can dramatically improve theaccuracy of a parser.However, the configurations exploited in PCFGparsers are quite local: rules?
probabilities may de-pend on parents or head words, but do not dependon arbitrarily distant tree configurations.
For exam-ple, it is generally not modeled that if one quantifierphrase (QP in the Penn Treebank) appears in a sen-tence, the likelihood of finding another QP in thatsame sentence is greatly increased.
This kind of ef-fect is neither surprising nor unknown ?
for exam-ple, Bock and Loebell (1990) show experimentallythat human language generation demonstrates prim-ing effects.
The mediating variables can not only in-clude priming effects but also genre or stylistic con-ventions, as well as many other factors which are notadequately modeled by local phrase structure.A reasonable way to add a latent variable to agenerative model is to use a mixture of estimators,in this case a mixture of PCFGs (see Section 3).The general mixture of estimators approach was firstsuggested in the statistics literature by Titteringtonet al (1962) and has since been adopted in machinelearning (Ghahramani and Jordan, 1994).
In a mix-ture approach, we have a new global variable onwhich all PCFG productions for a given sentencecan be conditioned.
In this paper, we experimentwith a finite mixture of PCFGs.
This is similar to thelatent nonterminals used in Matsuzaki et al (2005),but because the latent variable we use is global, ourapproach is more oriented toward learning non-localstructure.
We demonstrate that a mixture fit with theEM algorithm gives improved parsing accuracy andtest data likelihood.
We then investigate what is andis not being learned by the latent mixture variable.While mixture components are difficult to interpret,we demonstrate that the patterns learned are betterthan random splits.2 Empirical MotivationIt is commonly accepted that the context freedomassumptions underlying the PCFG model are too14VPVBDincreasedNPCD11NN%PPTOtoNPQP##CD2.5CDbillionPPINfromNPQP##CD2.25CDbillionRule ScoreQP?
# CD CD 131.6PRN?
-LRB- ADJP -RRB 77.1VP?
VBD NP , PP PP 33.7VP?
VBD NP NP PP 28.4PRN?
-LRB- NP -RRB- 17.3ADJP?
QP 13.3PP?
IN NP ADVP 12.3NP?
NP PRN 12.3VP?
VBN PP PP PP 11.6ADVP?
NP RBR 10.1Figure 1: Self-triggering: QP?
# CD CD.
If one British financial occurs in the sentence, the probability ofseeing a second one in the same sentence is highly inreased.
There is also a similar, but weaker, correlationfor the American financial ($).
On the right hand side we show the ten rules whose likelihoods are mostincreased in a sentence containing this rule.strong and that weakening them results in bettermodels of language (Johnson, 1998; Gildea, 2001;Klein and Manning, 2003).
In particular, certaingrammar productions often cooccur with other pro-ductions, which may be either near or distant in theparse tree.
In general, there exist three types of cor-relations: (i) local (e.g.
parent-child), (ii) non-local,and (iii) self correlations (which may be local ornon-local).In order to quantify the strength of a correlation,we use a likelihood ratio (LR).
For two rules X?
?and Y?
?, we computeLR(X?
?, Y?
?)
= P(?, ?|X,Y )P(?|X,Y )P(?|X,Y )This measures how much more often the rules oc-cur together than they would in the case of indepen-dence.
For rules that are correlated, this score willbe high (?
1); if the rules are independent, it willbe around 1, and if they are anti-correlated, it will benear 0.Among the correlations present in the Penn Tree-bank, the local correlations are the strongest ones;they contribute 65% of the rule pairs with LR scoresabove 90 and 85% of those with scores over 200.Non-local and self correlations are in general com-mon but weaker, with non-local correlations con-tributing approximately 85% of all correlations1 .
Byadding a latent variable conditioning all productions,1Quantifying the amount of non-local correlation is prob-lematic; most pairs of cooccuring rules are non-local and will,due to small sample effects, have LR ratios greater than 1 evenif they were truly independent in the limit.we aim to capture some of this interdependence be-tween rules.Correlations at short distances have been cap-tured effectively in previous work (Johnson, 1998;Klein and Manning, 2003); vertical markovization(annotating nonterminals with their ancestor sym-bols) does this by simply producing a different dis-tribution for each set of ancestors.
This added con-text leads to substantial improvement in parsing ac-curacy.
With local correlations already well cap-tured, our main motivation for introducing a mix-ture of grammars is to capture long-range rule cooc-currences, something that to our knowledge has notbeen done successfully in the past.As an example, the rule QP?
# CD CD, rep-resenting a quantity of British currency, cooc-curs with itself 132 times as often as if oc-currences were independent.
These cooccur-rences appear in cases such as seen in Figure 1.Similarly, the rules VP?
VBD NP PP , S andVP?
VBG NP PP PP cooccur in the Penn Tree-bank 100 times as often as we would expect if theywere independent.
They appear in sentences of avery particular form, telling of an action and thengiving detail about it; an example can be seen in Fig-ure 2.3 Mixtures of PCFGsIn a probabilistic context-free grammar (PCFG),each rule X?
?
is associated with a conditionalprobability P(?|X) (Manning and Schu?tze, 1999).Together, these rules induce a distribution over treesP(T ).
A mixture of PCFGs enriches the basic model15VPVBDhitNPa recordPPin 1998,,SVPVBGrisingNP1.7%PPafter inflation adjustmentPPto $13,120SNPDTNoNXNXNNSlawyersCCorNXNNtapeNNSrecordersVPwere present..(a) (b)SSNPDTTheseNNrateNNSindicationsVPVBPareRBn?tADJPdirectly comparable:;SNPNNlendingNNSpracticesVPVBPvaryADVPwidelyPPby location..XXSYM**ADJPVBNProjected(c) (d)Figure 2: Tree fragments demonstrating coocurrences.
(a) and (c) Repeated formulaic structure in onegrammar: rules VP?
VBD NP PP , S and VP?
VBG NP PP PP and rules VP?
VBP RB ADJPand VP?
VBP ADVP PP.
(b) Sibling effects, though not parallel structure, rules: NX?
NNS andNX?
NN NNS.
(d) A special structure for footnotes has rules ROOT?
X and X?
SYM coocurringwith high probability.by allowing for multiple grammars, Gi, which wecall individual grammars, as opposed to a singlegrammar.
Without loss of generality, we can as-sume that the individual grammars share the sameset of rules.
Therefore, each original rule X?
?is now associated with a vector of probabilities,P(?|X, i).
If, in addition, the individual grammarsare assigned prior probabilities P(i), then the entiremixture induces a joint distribution over derivationsP(T, i) = P(i)P(T |i) from which we recover a dis-tribution over trees by summing over the grammarindex i.As a generative derivation process, we can thinkof this in two ways.
First, we can imagine G to bea latent variable on which all productions are con-ditioned.
This view emphasizes that any otherwiseunmodeled variable or variables can be captured bythe latent variable G. Second, we can imagine se-lecting an individual grammar Gi and then gener-ating a sentence using that grammar.
This view isassociated with the expectation that there are multi-ple grammars for a language, perhaps representingdifferent genres or styles.
Formally, of course, thetwo views are the same.3.1 Hierarchical EstimationSo far, there is nothing in the formal mixture modelto say that rule probabilities in one component haveany relation to those in other components.
However,we have a strong intuition that many rules, such asNP?
DT NN, will be common in all mixture com-ponents.
Moreover, we would like to pool our dataacross components when appropriate to obtain morereliable estimators.This can be accomplished with a hierarchical es-timator for the rule probabilities.
We introduce ashared grammar Gs.
Associated to each rewrite isnow a latent variable L = {S, I} which indicateswhether the used rule was derived from the sharedgrammar Gs or one of the individual grammars Gi:P(?|X, i) =?P(?|X, i, ?= I) + (1?
?
)P(?|X, i, ?= S),where ?
?
P (?
= I) is the probability ofchoosing the individual grammar and can alsobe viewed as a mixing coefficient.
Note thatP(?|X, i, ?= S) = P(?|X, ?= S), since the sharedgrammar is the same for all individual grammars.This kind of hierarchical estimation is analogous tothat used in hierarchical mixtures of naive-Bayes for16text categorization (McCallum et al, 1998).The hierarchical estimator is most easily de-scribed as a generative model.
First, we choose aindividual grammar Gi.
Then, for each nonterminal,we select a level from the back-off hierarchy gram-mar: the individual grammar Gi with probability ?,and the shared grammar Gs with probability 1 ?
?.Finally, we select a rewrite from the chosen level.
Toemphasize: the derivation of a phrase-structure treein a hierarchically-estimated mixture of PCFGs in-volves two kinds of hidden variables: the grammarG used for each sentence, and the level L used ateach tree node.
These hidden variables will impactboth learning and inference in this model.3.2 Inference: ParsingParsing involves inference for a given sentence S.One would generally like to calculate the most prob-able parse ?
that is, the tree T which has the high-est probability P(T |S) ?
?i P(i)P(T |i).
How-ever, this is difficult for mixture models.
For a singlegrammar we have:P(T, i) = P(i)?X??
?TP(?|X, i).This score decomposes into a product and it is sim-ple to construct a dynamic programming algorithmto find the optimal T (Baker, 1979).
However, for amixture of grammars we need to sum over the indi-vidual grammars:?iP(T, i) =?iP(i)?X??
?TP(?|X, i).Because of the outer sum, this expression unfor-tunately does not decompose into a product overscores of subparts.
In particular, a tree which maxi-mizes the sum need not be a top tree for any singlecomponent.As is true for many other grammar formalisms inwhich there is a derivation / parse distinction, an al-ternative to finding the most probable parse is to findthe most probable derivation (Vijay-Shankar andJoshi, 1985; Bod, 1992; Steedman, 2000).
Insteadof finding the tree T which maximizes?i P(T, i),we find both the tree T and component i which max-imize P(T, i).
The most probable derivation can befound by simply doing standard PCFG parsing oncefor each component, then comparing the resultingtrees?
likelihoods.3.3 Learning: TrainingTraining a mixture of PCFGs from a treebank is anincomplete data problem.
We need to decide whichindividual grammar gave rise to a given observedtree.
Moreover, we need to select a generation path(individual grammar or shared grammar) for eachrule in the tree.
To learn estimate parameters, wecan use a standard Expectation-Maximization (EM)approach.In the E-step, we compute the posterior distribu-tions of the latent variables, which are in this caseboth the component G of each sentence and the hier-archy level L of each rewrite.
Note that, unlike dur-ing parsing, there is no uncertainty over the actualrules used, so the E-step does not require summingover possible trees.
Specifically, for the variable Gwe haveP(i|T ) = P(T, i)?j P(T, j).For the hierarchy level L we can writeP(?
= I|X ?
?, i, T ) =?P(?|X, ?= I)?P(?|X, i, ?= I) + (1?
?
)P(?|X, ?= S) ,where we slightly abuse notation since the ruleX ?
?
can occur multiple times in a tree T.In the M-step, we find the maximum-likelihoodmodel parameters given these posterior assign-ments; i.e., we find the best grammars given the waythe training data?s rules are distributed between in-dividual and shared grammars.
This is done exactlyas in the standard single-grammar model using rela-tive expected frequencies.
The updates are shown inFigure 3.3, where T = {T1, T2, .
.
. }
is the trainingset.We initialize the algorithm by setting the assign-ments from sentences to grammars to be uniformbetween all the individual grammars, with a smallrandom perturbation to break symmetry.4 ResultsWe ran our experiments on the Wall Street Jour-nal (WSJ) portion of the Penn Treebank using thestandard setup: We trained on sections 2 to 21,and we used section 22 as a validation set for tun-ing model hyperparameters.
Results are reported17P(i)?
?Tk?T P(i|Tk)?i?Tk?T P(i|Tk)=PTk?TP(i|Tk)kP(l = I)??Tk?T?X??
?Tk P(?
= I|X ?
?
)?Tk?T |Tk|P(?|X, i, ?
= I)??Tk?T?X??
?Tk P(i|Tk)P(?
= I|Tk, i,X ?
?)????Tk?T?X???
?Tk P(i|Tk)P(?
= I|Tk, i,X ?
??
)Figure 3: Parameter updates.
The shared grammar?s parameters are re-estimated in the same manner.on all sentences of 40 words or less from section23.
We use a markovized grammar which was an-notated with parent and sibling information as abaseline (see Section 4.2).
Unsmoothed maximum-likelihood estimates were used for rule probabili-ties as in Charniak (1996).
For the tagging proba-bilities, we used maximum-likelihood estimates forP(tag|word).
Add-one smoothing was applied tounknown and rare (seen ten times or less duringtraining) words before inverting those estimates togive P(word|tag).
Parsing was done with a sim-ple Java implementation of an agenda-based chartparser.4.1 Parsing AccuracyThe EM algorithm is guaranteed to continuously in-crease the likelihood on the training set until conver-gence to a local maximum.
However, the likelihoodon unseen data will start decreasing after a numberof iterations, due to overfitting.
This is demonstratedin Figure 4.
We use the likelihood on the validationset to stop training before overfitting occurs.In order to evaluate the performance of our model,we trained mixture grammars with various numbersof components.
For each configuration, we used EMto obtain twelve estimates, each time with a differentrandom initialization.
We show the F1-score for themodel with highest log-likelihood on the validationset in Figure 4.
The results show that a mixture ofgrammars outperforms a standard, single grammarPCFG parser.24.2 Capturing Rule CorrelationsAs described in Section 2, we hope that the mix-ture model will capture long-range correlations in2This effect is statistically significant.the data.
Since local correlations can be capturedby adding parent annotation, we combine our mix-ture model with a grammar in which node probabil-ities depend on the parent (the last vertical ancestor)and the closest sibling (the last horizontal ancestor).Klein and Manning (2003) refer to this grammar asa markovized grammar of vertical order = 2 and hor-izontal order = 1.
Because many local correlationsare captured by the markovized grammar, there is agreater hope that observed improvements stem fromnon-local correlations.In fact, we find that the mixture does capturenon-local correlations.
We measure the degree towhich a grammar captures correlations by calculat-ing the total squared error between LR scores of thegrammar and corpus, weighted by the probabilityof seeing nonterminals.
This is 39422 for a sin-gle PCFG, but drops to 37125 for a mixture withfive individual grammars, indicating that the mix-ture model better captures the correlations presentin the corpus.
As a concrete example, in the PennTreebank, we often see the rules FRAG?
ADJPand PRN?
, SBAR , cooccurring; their LR is 134.When we learn a single markovized PCFG from thetreebank, that grammar gives a likelihood ratio ofonly 61.
However, when we train with a hierarchi-cal model composed of a shared grammar and fourindividual grammars, we find that the grammar like-lihood ratio for these rules goes up to 126, which isvery similar to that of the empirical ratio.4.3 GenreThe mixture of grammars model can equivalently beviewed as capturing either non-local correlations orvariations in grammar.
The latter view suggests thatthe model might benefit when the syntactic structure180  10  20  30  40  50  60LogLikelihoodIterationTraining dataValidation dataTesting data7979.279.479.679.8801  2  3  4  5  6  7  8  9F1Number of Component GrammarsMixture modelBaseline: 1 grammar(a) (b)Figure 4: (a) Log likelihood of training, validation, and test data during training (transformed to fit on thesame plot).
Note that when overfitting occurs the likelihood on the validation and test data starts decreasing(after 13 iterations).
(b) The accuracy of the mixture of grammars model with ?
= 0.4 versus the number ofgrammars.
Note the improvement over a 1-grammar PCFG model.varies significantly, as between different genres.
Wetested this with the Brown corpus, of which we used8 different genres (f, g, k, l, m, n, p, and r).
We fol-low Gildea (2001) in using the ninth and tenth sen-tences of every block of ten as validation and testdata, respectively, because a contiguous test sectionmight not be representative due to the genre varia-tion.To test the effects of genre variation, we evalu-ated various training schemes on the Brown corpus.The single grammar baseline for this corpus givesF1 = 79.75, with log likelihood (LL) on the testingdata=-242561.
The first test, then, was to estimateeach individual grammar from only one genre.
Wedid this by assigning sentences to individual gram-mars by genre, without using any EM training.
Thisincreases the data likelihood, though it reduces theF1 score (F1 = 79.48, LL=-242332).
The increasein likelihood indicates that there are genre-specificfeatures that our model can represent.
(The lack ofF1 improvement may be attributed to the increaseddifficulty of estimating rule probabilities after divid-ing the already scant data available in the Brown cor-pus.
This small quantity of data makes overfittingalmost certain.
)However, local minima and lack of data cause dif-ficulty in learning genre-specific features.
If we startwith sentences assigned by genre as before, but thentrain with EM, both F1 and test data log likelihooddrop (F1 = 79.37, LL=-242100).
When we useEM with a random initialization, so that sentencesare not assigned directly to grammars, the scores godown even further (F1 = 79.16, LL=-242459).
Thisindicates that the model can capture variation be-tween genres, but that maximum training data likeli-hood does not necessarily give maximum accuracy.Presumably, with more genre-specific data avail-able, learning would generalize better.
So, genre-specific grammar variation is real, but it is difficultto capture via EM.4.4 Smoothing EffectsWhile the mixture of grammars captures rule cor-relations, it may also enhance performance viasmoothing effects.
Splitting the data randomly couldproduce a smoothed shared grammar, Gs, that isa kind of held-out estimate which could be supe-rior to the unsmoothed ML estimates for the single-component grammar.We tested the degree of generalization by eval-uating the shared grammar alone and also a mix-ture of the shared grammar with the known sin-gle grammar.
Those shared grammars were ex-tracted after training the mixture model with four in-dividual grammars.
We found that both the sharedgrammar alone (F1=79.13, LL=-333278) and theshared grammar mixed with the single grammar(F1=79.36, LL=-331546) perform worse than a sin-19gle PCFG (F1=79.37, LL=-327658).
This indicatesthat smoothing is not the primary learning effectcontributing to increased F1.5 ConclusionsWe examined the sorts of rule correlations that maybe found in natural language corpora, discoveringnon-local correlations not captured by traditionalmodels.
We found that using a model capable ofrepresenting these non-local features gives improve-ment in parsing accuracy and data likelihood.
Thisimprovement is modest, however, primarily becauselocal correlations are so much stronger than non-local ones.ReferencesJ.
Baker.
1979.
Trainable grammars for speech recog-nition.
Speech Communication Papers for the 97thMeeting of the Acoustical Society of America, pages547?550.K.
Bock and H. Loebell.
1990.
Framing sentences.
Cog-nition, 35:1?39.R.
Bod.
1992.
A computational model of language per-formance: Data oriented parsing.
International Con-ference on Computational Linguistics (COLING).E.
Charniak.
1996.
Tree-bank grammars.
In Proc.
ofthe 13th National Conference on Artificial Intelligence(AAAI), pages 1031?1036.E.
Charniak.
2000.
A maximum?entropy?inspiredparser.
In Proc.
of the Conference of the North Ameri-can chapter of the Association for Computational Lin-guistics (NAACL), pages 132?139.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Univ.
ofPennsylvania.Z.
Ghahramani and M. I. Jordan.
1994.
Supervisedlearning from incomplete data via an EM approach.
InAdvances in Neural Information Processing Systems(NIPS), pages 120?127.D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).M.
Johnson.
1998.
Pcfg models of linguistic tree repre-sentations.
Computational Linguistics, 24:613?632.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
Proc.
of the 41st Meeting of the Associationfor Computational Linguistics (ACL), pages 423?430.C.
Manning and H. Schu?tze.
1999.
Foundations of Sta-tistical Natural Language Processing.
The MIT Press,Cambridge, Massachusetts.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In Proc.
of the 43rdMeeting of the Association for Computational Linguis-tics (ACL), pages 75?82.A.
McCallum, R. Rosenfeld, T. Mitchell, and A. Ng.1998.
Improving text classification by shrinkage in ahierarchy of classes.
In Int.
Conf.
on Machine Learn-ing (ICML), pages 359?367.M.
Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, Massachusetts.D.
Titterington, A. Smith, and U. Makov.
1962.
Statisti-cal Analysis of Finite Mixture Distributions.
Wiley.K.
Vijay-Shankar and A. Joshi.
1985.
Some computa-tional properties of tree adjoining grammars.
Proc.
ofthe 23th Meeting of the Association for ComputationalLinguistics (ACL), pages 82?93.20
