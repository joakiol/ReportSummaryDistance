Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 201?204,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCorrelation between ROUGE and Human Evaluation of Extractive MeetingSummariesFeifan Liu, Yang LiuThe University of Texas at DallasRichardson, TX 75080, USAffliu,yangl@hlt.utdallas.eduAbstractAutomatic summarization evaluation is critical tothe development of summarization systems.
WhileROUGE has been shown to correlate well with hu-man evaluation for content match in text summa-rization, there are many characteristics in multipartymeeting domain, which may pose potential prob-lems to ROUGE.
In this paper, we carefully exam-ine how well the ROUGE scores correlate with hu-man evaluation for extractive meeting summariza-tion.
Our experiments show that generally the cor-relation is rather low, but a significantly better cor-relation can be obtained by accounting for severalunique meeting characteristics, such as disfluenciesand speaker information, especially when evaluatingsystem-generated summaries.1 IntroductionMeeting summarization has drawn an increasing atten-tion recently; therefore a study on the automatic evalu-ation metrics for this task is timely.
Automatic evalua-tion helps to advance system development and avoids thelabor-intensive and potentially inconsistent human eval-uation.
ROUGE (Lin, 2004) has been widely used forsummarization evaluation.
In the news article domain,ROUGE scores have been shown to be generally highlycorrelated with human evaluation in content match (Lin,2004).
However, there are many differences betweenwritten texts (e.g., news wire) and spoken documents, es-pecially in the meeting domain, for example, the pres-ence of disfluencies and multiple speakers, and the lackof structure in spontaneous utterances.
The question ofwhether ROUGE is a good metric for meeting summa-rization is unclear.
(Murray et al, 2005) have reportedthat ROUGE-1 (unigram match) scores have low correla-tion with human evaluation in meetings.In this paper we investigate the correlation betweenROUGE and human evaluation of extractive meetingsummaries and focus on two issues specific to the meet-ing domain: disfluencies and multiple speakers.
Bothhuman and system generated summaries are used.
Ouranalysis shows that by integrating meeting characteristicsinto ROUGE settings, better correlation can be achievedbetween the ROUGE scores and human evaluation basedon Spearman?s rho in the meeting domain.2 Related workAutomatic summarization evaluation can be broadly clas-sified into two categories (Jones and Galliers, 1996): in-trinsic and extrinsic evaluation.
Intrinsic evaluation, suchas relative utility based metric proposed in (Radev et al,2004), assesses a summarization system in itself (for ex-ample, informativeness, redundancy, and coherence).
Ex-trinsic evaluation (Mani et al, 1998) tests the effective-ness of a summarization system on other tasks.
In thisstudy, we concentrate on the automatic intrinsic summa-rization evaluation.
It has been extensively studied intext summarization.
Different approaches have been pro-posed to measure matches using words or more mean-ingful semantic units, for example, ROUGE (Lin, 2004),factoid analysis (Teufel and Halteren, 2004), pyramidmethod (Nenkova and Passonneau, 2004), and Basic El-ement (BE) (Hovy et al, 2006).With the increasing recent research of summarizationmoving into speech, especially meeting recordings, is-sues related to spoken language are yet to be exploredfor their impact on the evaluation metrics.
Inspired byautomatic speech recognition (ASR) evaluation, (Hori etal., 2003) proposed the summarization accuracy metric(SumACCY) based on a word network created by merg-ing manual summaries.
However (Zhu and Penn, 2005)found a statistically significant difference between theASR-inspired metrics and those taken from text summa-rization (e.g., RU, ROUGE) on a subset of the Switch-board data.
ROUGE has been used in meeting summa-rization evaluation (Murray et al, 2005; Galley, 2006),yet the question remained whether ROUGE is a goodmetric for the meeting domain.
(Murray et al, 2005)showed low correlation of ROUGE and human evalua-tion in meeting summarization evaluation; however, they201simply used ROUGE as is and did not take into accountthe meeting characteristics during evaluation.In this paper, we ask the question of whether ROUGEcorrelates with human evaluation of extractive meetingsummaries and whether we can modify ROUGE to ac-count for the meeting style for a better correlation withhuman evaluation.3 Experimental Setup3.1 DataWe used the ICSI meeting data (Janin et al, 2003) thatcontains naturally-occurring research meetings.
All themeetings have been transcribed and annotated with dialogacts (DA) (Shriberg et al, 2004), topics, and extractivesummaries (Murray et al, 2005).For this study, we used the same 6 test meetings as in(Murray et al, 2005; Galley, 2006).
Each meeting al-ready has 3 human summaries from 3 common annota-tors.
We recruited another 3 human subjects to generate3 more human summaries, in order to create more datapoints for a reliable analysis.
The Kappa statistics forthose 6 different annotators varies from 0.11 to 0.35 fordifferent meetings.
The human summaries have differentlength, containing around 6.5% of the selected DAs and13.5% of the words respectively.
We used four differentsystem summaries for each of the 6 meetings: one basedon the MMR method in MEAD (Carbonell and Gold-stein, 1998; et al, 2003), the other three are the systemoutput from (Galley, 2006; Murray et al, 2005; Xie andLiu, 2008).
All the system generated summaries containaround 5% of the DAs and 16% of the words of the entiremeeting.
Thus, in total we have 36 human summaries and24 system summaries on the 6 test meetings, on whichthe correlation between ROUGE and human evaluationis calculated and investigated.All the experiments in this paper are based on humantranscriptions, with a central interest on whether somecharacteristics of the meeting recordings affect the corre-lation between ROUGE and human evaluations, withoutthe effect from speech recognition or automatic sentencesegmentation errors.3.2 Automatic ROUGE EvaluationROUGE (Lin, 2004) measures the n-grammatch betweensystem generated summaries and human summaries.
Inmost of this study, we used the same options in ROUGEas in the DUC summarization evaluation (NIST, 2007),and modify the input to ROUGE to account for the fol-lowing two phenomena.?
DisfluenciesMeetings contain spontaneous speech with manydisfluencies, such as filled pauses (uh, um), dis-course markers (e.g., I mean, you know), repetitions,corrections, and incomplete sentences.
There havebeen efforts on the study of the impact of disfluen-cies on summarization techniques (Liu et al, 2007;Zhu and Penn, 2006) and human readability (Joneset al, 2003).
However, it is not clear whether dis-fluencies impact automatic evaluation of extractivemeeting summarization.Since we use extractive summarization, summarysentences may contain difluencies.
We hand anno-tated the transcripts for the 6 meetings and markedthe disfluencies such that we can remove them toobtain cleaned up sentences for those selected sum-mary sentences.
To study the impact of disfluencies,we run ROUGE using two different inputs: sum-maries based on the original transcription, and thesummaries with disfluencies removed.?
Speaker informationThe existence of multiple speakers in meetingsraises questions about the evaluation method.
(Gal-ley, 2006) considered some location constrains inmeeting summarization evaluation, which utilizesspeaker information to some extent.
In this studywe use the data in separate channels for each speakerand thus have the speaker information available foreach sentence.
We associate the speaker ID witheach word, treat them together as a new ?word?
inthe input to ROUGE.3.3 Human EvaluationFive human subjects (all undergraduate students in Com-puter Science) participated in human evaluation.
In to-tal, there are 20 different summaries for each of the 6test meetings: 6 human-generated, 4 system-generated,and their corresponding ones with disfluencies removed.We assigned 4 summaries with different configurations toeach human subject: human vs. system generated sum-maries, with or without disfluencies.
Each human evalu-ated 24 summaries in total, for the 6 test meetings.For each summary, the human subjects were asked torate the following statements using a scale of 1-5 accord-ing to the extent of their agreement with them.?
S1: The summary reflects the discussion flow in the meet-ing very well.?
S2: Almost all the important topic points of the meetingare represented.?
S3: Most of the sentences in the summary are relevant tothe original meeting.?
S4: The information in the summary is not redundant.?
S5: The relationship between the importance of each topicin the meeting and the amount of summary space given tothat topic seems appropriate.?
S6: The relationship between the role of each speaker andthe amount of summary speech selected for that speakerseems appropriate.?
S7: Some sentences in the summary convey the samemeaning.?
S8: Some sentences are not necessary (e.g., in terms ofimportance) to be included in the summary.?
S9: The summary is helpful to someone who wants toknow what are discussed in the meeting.202These statements are an extension of those used in(Murray et al, 2005) for human evaluation of meetingsummaries.
The additional ones we added were designedto account for the discussion flow in the meetings.
Someof the statements above are used to measure similar as-pects, but from different perspectives, such as S5 and S6,S4 and S7.
This may reduce some accidental noise in hu-man evaluation.
We grouped these statements into 4 cat-egories: Informative Structure (IS): S1, S5 and S6; Infor-mative Coverage (IC): S2 and S9; Informative Relevance(IRV): S3 and S8; and Informative Redundancy (IRD):S4 and S7.4 Results4.1 Correlation between Human Evaluation andOriginal ROUGE ScoreSimilar to (Murray et al, 2005), we also use Spearman?srank coefficient (rho) to investigate the correlation be-tween ROUGE and human evaluation.
We have 36 hu-man summaries and 24 system summaries for the 6 meet-ings in our study.
For each of the human summaries,the ROUGE scores are generated using the other 5 hu-man summaries as references.
For system generated sum-maries, we calculate the ROUGE score using 5 humanreferences, and then obtain the average from 6 such se-tups.
The correlation results are presented in Table 1.In addition to the overall average for human evaluation(H AVG), we calculated the average score for each evalu-ation category (see Section 3.3).
For ROUGE evaluation,we chose the F-measure for R-1 (unigram) and R-SU4(skip-bigram with maximum gap length of 4), which isbased on our observation that other scores in ROUGE arealways highly correlated (rho>0.9) to either of them forthis task.
We compute the correlation separately for thehuman and system summaries in order to avoid the im-pact due to the inherent difference between the two dif-ferent summaries.Correlation on Human SummariesH AVG H IS H IC H IRV H IRDR-1 0.09 0.22 0.21 0.03 -0.20R-SU4 0.18 0.33 0.38 0.04 -0.30Correlation on System SummariesR-1 -0.07 -0.02 -0.17 -0.27 -0.02R-SU4 0.08 0.05 0.01 -0.15 0.14Table 1: Spearman?s rho between human evaluation (H) andROUGE (R) with basic setting.We can see that R-SU4 obtains a higher correlationwith human evaluation than R-1 on the whole, but stillvery low, which is consistent with the previous conclu-sion from (Murray et al, 2005).
Among the four cat-egories, better correlation is achieved for informationstructure (IS) and information coverage (IC) comparedto the other two categories.
This is consistent with whatROUGE is designed for, ?recall oriented understudy gist-ing evaluation?
?
we expect it to model IS and IC wellby ngram and skip-bigram matching but not relevancy(IRV) and redundancy (IRD) effectively.
In addition, wefound low correlation on system generated summaries,suggesting it is more challenging to evaluate those sum-maries both by humans and the automatic metrics.4.2 Impacts of Disfluencies on CorrelationTable 2 shows the correlation results between ROUGE(R-SU4) and human evaluation on the original andcleaned up summaries respectively.
For human sum-maries, after removing disfluencies, the correlation be-tween ROUGE and human evaluation improves on thewhole, but degrades on information structure (IS) and in-formation coverage (IC) categories.
However, for sys-tem summaries, there is a significant gain of correlationon those two evaluation categories, even though no im-provement on the overall average score.
Our hypothesisfor this is that removing disfluencies helps remove thenoise in the system generated summaries and make themmore easily to be evaluated by human and machines.
Incontrast, the human created summaries have better qual-ity in terms of the information content and may not sufferas much from the disfluencies contained in the summary.Correlation on Human SummariesH AVG H IS H IC H IRV H IRDOriginal 0.18 0.33 0.38 0.04 -0.30Disfluencies 0.21 0.21 0.31 0.19 -0.16removedCorrelation on System SummariesOriginal 0.08 0.05 0.01 -0.15 0.14)Disfluencies 0.08 0.22 0.19 -0.02 -0.07removedTable 2: Effect of disfluencies on the correlation between R-SU4 and human evaluation.4.3 Incorporating Speaker InformationWe further incorporated speaker information in ROUGEsetting using the summaries with disfluencies removed.Table 3 presents the resulting correlation values betweenROUGE SU4 score and human evaluation.
For humansummaries, adding speaker information slightly degradedthe correlation, but it is still better compared to usingthe original transcripts (results in Table 1).
For the sys-tem summaries, the overall correlation is significantly im-proved, with some significant improvement in the infor-mation redundancy (IRD) category.
This suggests thatby leveraging speaker information, ROUGE can assignbetter credits or penalties to system generated summaries(same words from different speakers will not be countedas a match), and thus yield better correlation with humanevaluation; whereas for human summaries, this may nothappen often.
For similar sentences from different speak-ers, human annotators are more likely to agree with each203other in their selection compared to automatic summa-rization.Correlation on Human SummariesSpeaker Info.
H AVG H IS H IC H IRV H IRDNO 0.21 0.21 0.31 0.19 -0.16YES 0.20 0.20 0.27 0.12 -0.09Correlation on System SummariesNO 0.08 0.22 0.19 -0.02 -0.07YES 0.14 0.20 0.16 0.02 0.21Table 3: Effect of speaker information on the correlation be-tween R-SU4 and human evaluation.5 Conclusion and Future WorkIn this paper, we have made a first attempt to system-atically investigate the correlation of automatic ROUGEscores with human evaluation for meeting summariza-tion.
Adaptations on ROUGE setting based on meetingcharacteristics are proposed and evaluated using Spear-man?s rank coefficient.
Our experimental results showthat in general the correlation between ROUGE scoresand human evaluation is low, with ROUGE SU4 scoreshowing better correlation than ROUGE-1 score.
Thereis significant improvement in correlation when disfluen-cies are removed and speaker information is leveraged,especially for evaluating system-generated summaries.
Inaddition, we observe that the correlation is affected differ-ently by those factors for human summaries and system-generated summaries.In our future work we will examine the correlation be-tween each statement and ROUGE scores to better rep-resent human evaluation results instead of using simplythe average over all the statements.
Further studies arealso needed using a larger data set.
Finally, we plan to in-vestigate meeting summarization evaluation using speechrecognition output.AcknowledgmentsThe authors thank University of Edinburgh for providing the an-notated ICSI meeting corpus and Michel Galley for sharing histool to process the annotated data.
We also thank Gabriel Mur-ray and Michel Galley for letting us use their automatic summa-rization system output for this study.
This work is supported byNSF grant IIS-0714132.
Any opinions expressed in this workare those of the authors and do not necessarily reflect the viewsof NSF.ReferencesJ.
Carbonell and J. Goldstein.
1998.
The use of mmr, diversity-based reranking for reordering documents and producingsummaries.
In SIGIR, pages 335?336.M.
Galley.
2006.
A skip-chain conditional random fieldfor ranking meeting utterances by importance.
In EMNLP,pages 364?372.C.
Hori, T. Hori, and S. Furui.
2003.
Evaluation methods forautomatic speech summarization.
In EUROSPEECH, pages2825?2828.E.
Hovy, C. Lin, L. Zhou, and J. Fukumoto.
2006.
Automatedsummarization evaluation with basic elements.
In LREC.A.
Janin, D. Baron, J. Edwards, D. Ellis, G. Gelbart, N. Norgan,B.
Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.2003.
The icsi meeting corpus.
In ICASSP.K.
S. Jones and J. Galliers.
1996.
Evaluating natural languageprocessing systems: An analysis and review.
Lecture Notesin Artificial Intelligence.D.
Jones, F. Wlof, E. Gilbson, E. Williams, E. Fedorenko,D.
Reynolds, and M. Zissman.
2003.
Measuring thereadability of automatic speech-to-text transcripts.
In EU-ROSPEECH, pages 1585?1588.C.
Lin.
2004.
Rouge: A package for automatic evaluation ofsummaries.
In Workshop on Text Summarization BranchesOut at ACL, pages 74?81.Y.
Liu, F. Liu, B. Li, and S. Xie.
2007.
Do disfluencies af-fect meeting summarization?
a pilot study on the impact ofdisfluencies.
In MLMI Workshop, Poster Session.I.
Mani, T. Firmin, D. House, M. Chrzanowski, G. Klein,L.
Hirschman, B. Sundheim, and L. Obrst.
1998.
The tipstersummac text summarization evaluation: Final report.
Tech-nical report, The MITRE Corporation.G.
Murray, S. Renals, J. Carletta, and J. Moore.
2005.
Eval-uating automatic summaries of meeting recordings.
In ACL2005 MTSE Workshop, pages 33?40.A.
Nenkova and R. Passonneau.
2004.
Evaluating con-tent selection in summarization: the pyramid method.
InHLT/NAACL.NIST.
2007.
Document understanding conference (DUC).http://duc.nist.gov/.D.
Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A.
C?elebi,E.
Drabek, W. Lam, D. Liu, H. Qi, H. Saggion, S. Teufel,M.
Topper, and A. Winkel.
2003.
The MEAD Multidocu-ment Summarizer.
http://www.summarization.com/mead/.D.
R. Radev, H. Jing, M. Stys, and T. Daniel.
2004.
Centroid-based summarization of multiple documents.
InformationProcessing and Management, 40:919?938.E.
Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004.The icsi meeting recorder dialog act (mrda) corpus.
In SIG-DAL Workshop, pages 97?100.S.
Teufel and H. Halteren.
2004.
Evaluating information con-tent by factoid analysis: Human annotation and stability.
InEMNLP.S.
Xie and Y. Liu.
2008.
Using corpus and knowledge-basedsimilarity measure in maximummarginal relevance for meet-ing summarization.
In ICASSP.X.
Zhu and G. Penn.
2005.
Evaluation of sentence selection forspeech summarization.
In ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/or Summariza-tion.X.
Zhu and G. Penn.
2006.
Comparing the roles of tex-tual, acoustic and spoken-language features on spontaneous-conversation summarization.
In HLT/NAACL.204
