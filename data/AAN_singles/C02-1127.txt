Location Normalization for Information Extraction*Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei LiCymfony Inc.600 Essjay Road, Williamsville, NY 14221, USA(hli, rohini, cniu, wei)@cymfony.comAbstractAmbiguity is very high for location names.
Forexample, there are 23 cities named ?Buffalo?
inthe U.S.  Country names such as ?Canada?,?Brazil?
and ?China?
are also city names in theUSA.
Almost every city has a Main Street orBroadway.
Such ambiguity needs to be handledbefore we can refer to location names forvisualization of related extracted events.
Thispaper presents a hybrid approach for locationnormalization which combines (i) lexicalgrammar driven by local context constraints, (ii)graph search for maximum spanning tree and(iii) integration of semi-automatically deriveddefault senses.
The focus is on resolvingambiguities for the following types of locationnames: island, town, city, province, and country.The results are promising with 93.8% accuracyon our test collections.1 IntroductionThe task of location normalization is to identifythe correct sense of a possibly ambiguouslocation Named Entity (NE).
Ambiguity is veryserious for location NEs.
For example, there are23 cities named ?Buffalo?, including the city inNew York State and in Alabama State.
Evencountry names such as ?Canada?, ?Brazil?, and?China?
are also city names in the USA.
Almostevery city has a Main Street or Broadway.
Suchambiguity needs to be properly handled beforeconverting location names into some normalform to support entity profile construction, eventmerging and visualization of extracted events on*This work was partly supported by a grant from theAir Force Research Laboratory?s InformationDirectorate (AFRL/IF), Rome, NY, under contractF30602-00-C-0090.a map for an Information Extraction (IE) System.Location normalization is a specialapplication of word sense disambiguation(WSD).
There is considerable research on WSD.Knowledge-based work, such as (Hirst, 1987;McRoy, 1992; Ng and Lee, 1996) usedhand-coded rules or supervised machine learningbased on annotated corpus to perform WSD.Recent work emphasizes corpus-basedunsupervised approach (Dagon and Itai, 1994;Yarowsky, 1992; Yarowsky, 1995) that avoidsthe need for costly truthed training data.
Locationnormalization is different from general WSD inthat the selection restriction often used for WSDin many cases is not sufficient to distinguish thecorrect sense from the other candidates.For example, in the sentence ?The WhiteHouse is located in Washington?, the selectionrestriction from the collocation ?located in?
canonly determine that ?Washington?
should be alocation name, but is not sufficient to decide theactual sense of this location.
Locationnormalization depends heavily on co-occurrenceconstraints of geographically related locationentities mentioned in the same discourse.
Forexample, if ?Buffalo?, ?Albany?
and ?Rochester?are mentioned in the same document, the mostprobable senses of ?Buffalo?, ?Albany?
and?Rochester?
should refer to the cities in NewYork State.
There are certain fixedkeyword-driven patterns from the local context,which decide the sense of location NEs.
Thesepatterns use keywords such as ?city?, ?town?,?province?, ?on?, ?in?
or other location names.
Forexample, the pattern ?X + city?
can determinesense tags for cases like ?New York city?
; and thepattern ?City + comma + State?
can disambiguatecases such as ?Albany, New York?
and?Shanghai, Illinois?.
In the absence of thesepatterns, co-occurring location NEs in the samediscourse can be good evidence for predicting themost probable sense of a location name.Unrestrictedtext TokenizerPOS TaggingShallow ParsingCoreferenceSemantic ParsingPragmatic FilterNE TaggingLocNZProfileEventQuestionAnsweringOutput(IEDatabase)IntelligentBrowsingSummari-zationVisualizationKernel IE Modules Linguistic ModulesNote: NE: name entity tagging; LocNZ: location normalizationApplication ModulesOutput(IEDatabase)Figure 1.
InfoXtract system architectureEvent type: Job changeKeyword: hiredCompany : MicrosoftPerson in: MaryPosition: sales personLocation: BeijingDate: January 1Event type: Job changeKeyword: replacedCompany: MicrosoftPerson out : he(Dick)Position: sales personLocation: BeijingDate: YesterdayEvent 1 Event 2Event type: Job changeKeyword: hiredKeyword: replacedCompany: MicrosoftPerson in: MaryPerson out : he(Dick)Position: salespersonLocation: <LocationProfile101>Date: 2000-01-01Figure 2.
Location verification in Eventmerging.For choosing the best matching sense setwithin a document, we simply construct a graphwhere each node represents a sense of a locationNE, and each edge represents the relationshipbetween two location name senses.
A graphspanning algorithm can be used to select the bestsenses from the graph.
If there exist nodes thatcannot be resolved in this step, we will applydefault location senses that were extractedsemi-automatically by statistical processing.
Thelocation normalization module, or ?LocNZ?, isapplied after the NE tagging module in ourInfoXtract IE system as shown in Figure 1.This paper focuses on how to resolveambiguity for the names of island, town, city,province, and country.
Three applications ofLocNZ in Information Extraction are illustratedin Section 2.
Section 3 presents location senseidentification using local context; Section 4describes disambiguation process usinginformation within a document through graphprocessing; Section 5 shows how tosemi-automatically collect default senses oflocations from a corpus; Section 6 presents analgorithm for location normalization withexperimental results.
The summary andconclusions are given in Section 7.
Sample textand the results of location tagging are given in theAppendix.2   Applications of Location NormalizationSeveral applications are enabled through locationnormalization.?
Event extraction and mergingEvent extraction is an advanced IE task.Extracted events can be merged to provide keycontent in a document.
The merging processconsists of several steps including checkinginformation compatibility such as checkingsynonyms, name aliases and co-reference ofanaphors, time and location normalization.
Twoevents cannot be merged if there is a conflictingcondition such as time and location.
Figure 2shows an example of event merging where theevents occurred in Microsoft at Beijing, not inSeattle.?
Event visua lizationVisualization applications can illustrate where anevent occurred with support of locationnormalization.
Figure 3 demonstrates avisualized event on a map based on thenormalized location names associated with theevents.
The input to visualization consists ofextracted events from a news story pertaining toJulian Hill?s life.
The arrow points to the citywhere the event occurred.?
Entity profile constructionAn entity profile is an information object forentities such as person, organization and location.It is defined as an Attribute Value Matrix (AVM)to represent key aspects of information aboutentities, including their relationships with otherentities.
Each attribute slot embodies someEvent type: <Die: Event 200>Who:       <Julian Werver Hill: PersonProfile 001>When:     1996-01-0 7Where :     <Loca t ionPro f i l e103>Preceding_event:  <hospitalize: Event 260>Subsequent_event:  <bury:  Event  250>Event Visualization;  ;; ;Predicate: DieWho: Julian Werner HillWhen:Where: <LocationProfile 103>Hockessin, Delaware, USA,19707,75.688873,39.776041996-01-07Figure 3.
Event visulization with location.information about the entity in one aspect.
Eachrelationship is represented by an attribute slot inthe Profile AVM.
Sample Profile AVMsinvolving the reference of locations areillustrated below.<PersonProfile 001> ::Name:   Julian Werner HillPosition: Research chemistAge:        91Birth-place: <LocationProfile100>Affiliation:  Du Pont Co.Education:  MIT<LocationProfile 100> ::Name:   St. LouisState:    MissouriCountry: United States of AmericaZipcode:  63101Lattitude : 90.191313Longitude:  38.634616Related_profiles: <PersonProfile 001>Several other applications such as questionanswering and classifying documents by locationareas can also be enabled through LocNZ.3 Lexical Grammar Processing inLocal ContextNamed Entity tagging systems (Krupka andHausman, 1998; Srihari et al, 2000) attempt totag information such as names of people,organizations, locations, time, etc.
in runningtext.
In InfoXtract, we combine MaximumEntropy Model (MaxEnt) and Hidden MarkovModel for NE tagging (Shrihari et al,, 2000).
TheMaximum Entropy Models incorporate localcontextual evidence in handling ambiguity ofinformation from a location gazetteer.
In theTipster Location gazetteer used by InfoXtract,there are a lot of common words, such as I, A,June, Friendship , etc.
Also, there is large overlapbetween person names and location names, suchas Clinton, Jordan, etc.
Using MaxEnt, systemslearn under what situation a word is a locationname, but it is very difficult to determine thecorrect sense of an ambiguous location name.
If aword can represent a city or state at the sametime, such as New York or Washington, it isdifficult to decide if it refers to city or state.
TheNE tagger in InfoXtract only assigns the locationsuper-type tag NeLOC to the identified locationwords and leaves the task of location sub-typetagging such as NeCITY or NeSTATE and itsnormalization to the subsequent module LocNZ.For representation of LocNZ results, we addan unique zip code and position informationthat is longitude and latitude for the cities forevent visualization.The first step of LocNZ is to use local contextthat is the co-occurring words around a locationname.
Local context can be a reliable source indeciding the sense of a location.
The followingare most commonly used patterns for thispurpose.
(1) location+comma+NP(headed by ?city?)e.g.
Chicago, an old city(2) ?city of?
+location1+comma+location2e.g.
city of Albany, New York(3) ?city of?
+location(4) ?state of?+location(5) location1+{,}+location2+{,}+location3e.g.
(i) Williamsville, New York, USA(ii) New York, Buffalo,USA(6) {on, in}+locatione.g.
on Strawberry ?
NeIslandin Key West ?
NeCityPatterns (1) , (3), (4) and (6) can be used to decideif the location is a city, a state or an island, whilepatterns (2) and (5) can be used to determine boththe sub-tag and its sense.
These patterns areimplemented in  our finite state transducerformalism.4 Maximum Spanning TreeCalculation with Global InformationAlthough local context can be reliable evidencefor disambiguating location senses, there are stillmany cases which cannot be captured by theabove patterns.
Information in the entiredocument (i.e.
discourse information) should beconsidered.
Since all location names in adocument have meaning relationships amongthem, a way to represent the best sensecombination within the document is needed.The LocNZ process constructs a weightedgraph where each node represents a locationsense, and each edge represents similarity weightbetween location names.
Apparently there will beno links among the different senses of a locationname, so the graph will be partially complete.
Wecalculate the maximum weight spanning tree(MaxST) using Kruskal?s MinST algorithm(Cormen et al 1990).
The nodes on theresulting MaxST are the most promising sensesof the location names.We define three criteria for similarity weightassignment between two nodes:(1) More weight will be given to the edgebetween a city and the province (or thecountry) to which it belongs.
(2) Distance between location names mentionedin the document is taken into consideration.The shorter the distance, the more we assignthe weight between the nodes.
(3) The number of word occurrences affects theweight calculation.
For multiple mentions ofa location name, only one node will berepresented in the graph.
We assume that allthe same location mentions have the samemeaning in a document following one senseper discourse principle (Gale, Church, andYarowsky, 1992).When calculating the weight between twolocation names, the predefined similarity valuesshown in Table 1, the number of location nameoccurrences and the distance between them in atext are taken into consideration.
After selectingeach edge, the senses that are connected will bechosen, and other senses of the same locationname will be discarded so that they will not beconsidered again in the MaxST calculation.
Aweight value is calculated with equation (1),where sij indicate the jth sense of wordi, a reflectsthe number of location name occurrences in atext, and b refers to the distance between the twolocation names.
Figure 4 shows the graph forcalculating MaxST.
Dots in a circle mean thenumber of senses of a location name.Table 1.
Similarity value sim(si,si) betweenlocation sense pairs.Loc1 Loc2 Relationship  Sim(si,si)C1 P1 P1 includes C1  5IL Ctr1 Ctr1 includes IL 5C1 Ctr1 Ctr1 is direct parent 5C1 C2 C1 and C2 in sameprovince/state3C1 C2 C1 and C2 in samecountry2C1 P1 C1 and P1 are in samecountry but C1 is notin P12C1 Ctr1 Ctr1 is not a directparent of C13P1 Ctr1 P1 is in Ctr1 1P1 P2 P1 and P2 in samecountry1Loc1 Loc2 Loc1 and Loc2 are twosense nodes of thesame location name-?Loc1 Loc2 Other cases 0Note: Ci: city; Pi: province/state; IL: island; Ctri:country; Loci: location.
),(),(/))()((),(/),(),(),(),(jijkijjijkijjkijjkijjkijjkijwwdistssnumAllwnumwnumssnumAllsssssssimssScore=+=-+=baba(1)5 Default Sense ExtractionIn our experiments, we found that the systemperformance suffers greatly from the lack oflexical information on default senses.
Forexample, people refer to ?Los Angeles?
as thecity at California more than the city inPhilippines, Chile, Puerto Rico, or the city inTexas in the USA.
This problem becomes abottleneck in the system performance.
Asmentioned before, a location name usually has adozen senses that need sufficient evidence in adocument for selecting one sense among them.Canada{Kansas,Kentucky,Country}Vancouver{British ColumbiaWashingtonport in USAPort in Canada}New York{Prov in USA,New York City,?
}Toronto(Ontorio ,New South Wales,Illinois,?
}Charlottetown{Prov in USA,New York City,?
}Prince Edward Island{Island in Canada,Island in South Africa,Province in Canada}Quebec(city in Quebec ,Quebec Prov,Connecticut,?
}3*4 l ines2*3 lines4*11 lines11*10 lines3*10 lines8*3 l ines2*8 lines2*43*113*103*33*82*102*38*48*118*108*410*43*11Figure 4.
Graph for calculating maximum weightspanning tree.But in many cases there is no explicit clue in adocument, so the system has to choose the defaultsenses that most people may refer to undercommon sense.The Tipster Gazetteer (http://crl.nmsu.edu/cgi-bin/Tools/CLR/clrcat) used in our system has171,039 location entries with 237,916 totalsenses that cover most location names all over theworld.
Each location in the gazetteer may haveseveral senses.
Among them 30,711 locationnames have more than one sense.
Although it hasranking tags on some location entries, a lot ofthem have no tags attached or the same rank isassigned to the entries of the same name.Manually calculating the default senses for over30,000 location names will be difficult and it issubject to inconsistency due to the differentknowledge background of the human taggers.
Tosolve this problem in calculating the defaultsenses of location names, we propose to extractthe knowledge from a corpus using statisticalprocessing method.With the TREC-8 (Text Retrieval Conference)corpus, we can only extract default senses for1687 location names, which cannot satisfy ourrequirement.
This result shows that the generalcorpus is not sufficient to suit our purpose due tothe serious ?data sparseness?
problem.
Through aseries of experiments, we found that we coulddownload highly useful information from Websearch engines such as Google, Yahoo, andNorthern Light by searching ambiguous locationnames in the Gazetteer.
Web search engines canprovide the closest content by their built-inranking mechanisms.
Among those engines, wefound that the Yahoo search engine is the bestone for our purpose.
We wrote a script todownload web-pages from Yahoo!
using eachambiguous location name as a search string.In order to derive default senses automaticallyfrom the downloaded web-pages, we use thesimilarity features and scoring values betweenlocation-sense pairs described in Section 3.
Forexample, if ?Los Angeles?
co-occurs with?California?
in the same web-page, then its sensewill be most probably set to the city in Californiaby the system.
Suppose a location word w hasseveral city senses si: Sense(w) indicates thedefault sense of w; sim(wi,xjk) means thesimilarity value between two senses of the  wordw and the j th co-occuring word xj; num(w) is thenumber of w in the document, and NumAll is thetotal number of locations.
a  is a parameter thatreflects the importance of the co-occurringlocation names and is determined empirically.The default sense of w is wi that maximizes thesimilarity value with all co-occurring locationnames.
The maximum similarity should be largerthan a threshold to keep meaningful defaultsenses.
The threshold can be determinedempirically through experimentation.
)))(/())((**),((maxmax)(1 11wnumNumAllxnumxssimwSensejnjjkipkmi-= ?= ???
?a(2)For each of 30,282 ambiguous location names,we used the name itself as search term in Yahooto download its corresponding web-page.
Thesystem produced default senses for 18,446location names.
At the same time, it discarded theremaining location names because thecorresponding web-pages do not containsufficient evidence to reach the threshold.
Weobserved that the results reflect the correct sensesin most cases, and found that the discardedlocation names have low references in the searchresults of other Web search engines.
This meansthey will not appear frequently in text, henceminimal impact on system performance.
Wemanually modified some of the default senseresults based on the ranking tags in the TipsterGazetteer and some additional information onpopulation of the locations in order to consolidatethe default senses.6 Algorithm and ExperimentWith the information from local context,discourse context and the knowledge of defaultsenses, the location normalization process turnedout to be very efficient and precise.
Theprocessing flow is divided into 5 steps:Step 1.
Look up the location gazetteer toassociate candidate senses for each location NE;Step 2.
Call the pattern matching sub-module toresolve the ambiguity of the NEs involved inlocal patterns like ?Williamsville, New York,USA?
to retain only one sense for the NE as earlyas possible;Step 3.
Apply the ?one sense per discourse?principle for each disambiguated location nameto propagate the selected sense to its otheroccurrences within a document;Step 4.
Call the global sub-module, which is agraph search algorithm, to resolve the remainingambiguities;Step 5.
If the decision score for a location name islower than a threshold, we choose a default senseof that name as a result.For evaluating the system performance, 53documents from a travel site(http://www.worldtravelguide.net/navigate/region/nam.asp), CNN News and New York Times areused.
Table 2 shows some sample results fromour test collections.
For results shown in Column4, we first applied default senses of locationnames available from the Tipster Gazetteer inaccordance with the rules specified in thegazetteer document.
If there is no ranking valuetagged for a location name, we select the firstsense in the gazetteer as its default.
Thisexperiment showed accuracy of 42%.
ForColumn 5, we tagged the corpus with defaultsenses we derived with the method described insection 5, and found that it can resolve 78%location name ambiguity.
Column 6 in Table 2 isthe result of our LocNZ system using thealgorithm described above as well as defaultsenses we derived.
The system showed promisingresults with 93.8% accuracy.7 ConclusionThis paper presents a method of locationnormalization for information extraction withexperimental results and its applications.
Infuture work, we will integrate a expandedlocation gazetteer including names of landmarks,mountains and lakes such as Holland Tunnel (inNew York, not in Holland) and Hoover Dam (inArizona, not in Alabama), to enlarge the systemcoverage, and adjust the scoring weight given inTable 1 for better normalization results.
Usingcontext information other than location namescan be a subtask for determining specific locationnames such as bridge or area names.Table 2.
Experimental evaluation for location name normalization.Correctly tagged locations Document Type No.
ofAmbigu-ous LocNamesNo.
ofAmbigu-oussensesWith TipsterGazetteerdefault senseand rule onlyWith LocNZdefault sensesonlyLocNZPrecision(%) ofLocNZCalifornia Intro.
26 326 13 18 25 96Canada Intro.
14 75 13 13 14 100Florida Intro 22 221 10 18 20 90Texas Intro.
13 153 9 11 12 93CNN News 1 27 486 10 23 25 92CNN News 2 26 360 10 22 24 92CNN News 3 16 113 4 10 14 87.5New York Times 1 8 140 1 7 8 100New York Times 2 10 119 2 7 10 100New York Times 3 18 218 5 13 17 94Total 180 2211 77 (42%) 142 (78%) 169 (93.8%)  93.88 AcknowledgementThe authors wish to thank Carrie Pine of AFRLfor supporting this work.
Other members ofCymfony?s R&D team, including Sargur N.Srihari, have also contributed in various ways.ReferencesCormen, Thomas H., Charles E. Leiserson, andRonald L. Rivest.
1990.
Introduction toAlgorithm.
The MIT Press, pp.
504-505.Dagon, Ido and Alon Itai.
1994.
Word SenseDisambiguation Using a Second LanguageMonolingual Corpus.
ComputationalLinguistics, Vol.20, pp.
563-596.Gale, W.A., K.W.
Church, and D. Yarowsky.1992.
One Sense Per Discourse.
InProceedings of the 4th DARPA Speech andNatural Language Workshop.
pp.
233-237.Hirst, Graeme.
1987.
Semantic Interpretationand the Resolution of Ambiguity.
CambridgeUniversity Press, Cambridge.Krupka, G.R.
and K. Hausman.
1998.
IsoQuestInc.
: Description of the NetOwl (TM) ExtractorSystem as Used for MUC-7.
Proceedings ofMUC.McRoy, Susan W. 1992.
Using MultipleKnowledge Sources for Word SenseDiscrimination.
Computational Linguistics,18(1): 1-30.Ng, Hwee Tou and Hian Beng Lee.
1996.Integrating Multiple Knowledge Sources toDisambiguate Word Sense: an Exemplar-basedApproach.
In Proceedings of 34th AnnualMeeting of the Association for ComputationalLinguistics, pp.
40-47, California.Srihari, Rohini, Cheng Niu, and Wei Li.
2000.
AHybrid Approach for Named Entity andSub-Type Tagging.
In Proceedings of ANLP2000, Seattle.Yarowsky, David.
1992.
Word-senseDisambiguation Using Statistical Models ofRoget?s Categories Trained on Large Corpora.In Proceedings of the 14 th InternaionalConference on Computational Linguistics(COLING-92), pp.
454-460, Nates, France.Yarowsky, David.
1995.
Unsupervised WordSense Disambiguation Rivaling SupervisedMethods.
In Proceedings of the 33rd AnnualMeeting of the Association for ComputationalLinguistics, Cambridge, Massachusetts.Appendix: Sample text and tagged resultFew countries in the world offer as many choices tothe world traveler as Canada.
Whether your passion isskiing, sailing, museum-combing or indulging inexceptional cuisine, Canada has it all.Western Canada is renowned for its stunninglybeautiful countryside.
Stroll through Vancouver'sPark, overlooking the blue waters of English Bay orski the slopes of world-famous Whistler-Blackcomb,surrounded by thousands of hectares of pristineforestland.
For a cultural experience, you can take anAboriginal nature hike to learn about Canada's FirstNations' history and cuisine, while outdoorsmen canriver-raft, hike or heli-ski the thousands of kilometersof Canada's backcountry, where the memories of goldprospectors and pioneers still flourish today.By contrast, Canada mixes the flavor and charm ofEurope with the bustle of trendy New York.
Torontoboasts an irresistible array of ethnic restaurants,bakeries and shops to tempt the palate, whileCharlottetown, Canada's birthplace, is located amidstthe rolling fields and sandy Atlantic beaches of PrinceEdward Island.
Between the two, ancient Quebec Cityis a world unto itself: the oldest standing citadel inNorth America and the heart of Quebec hospitality.Location City Province CountryCanada - - CanadaVancouver Vancouver BritishColumbiaCanadaNew York New York New York USAToronto Toronto Ontario CanadaCharlotte-townCharlotte-townPrinceEdwardIslandCanadaPrinceEdwardIsland- PrinceEdwardIslandCanadaQuebec Quebec Quebec Canada
