Answering Definition Questions Using Multiple Knowledge SourcesWesley Hildebrandt, Boris Katz, and Jimmy LinMIT Computer Science and Artificial Intelligence Laboratory32 Vassar Street, Cambridge, MA 02139{wes,boris,jimmylin}@csail.mit.eduAbstractDefinition questions represent a largely unex-plored area of question answering?they aredifferent from factoid questions in that thegoal is to return as many relevant ?nuggets?of information about a concept as possible.We describe a multi-strategy approach to an-swering such questions using a database con-structed offline with surface patterns, a Web-based dictionary, and an off-the-shelf docu-ment retriever.
Results are presented fromcomponent-level evaluation and from an end-to-end evaluation of our implemented systemat the TREC 2003 Question Answering Track.1 IntroductionTo date, research in question answering has concentratedon factoid questions such as ?Who was Abraham Lincolnmarried to??
The standard strategy for answering thesequestions using a textual corpus involves a combinationof information retrieval and named-entity extraction tech-nology; see (Voorhees, 2002) for an overview.
Factoidquestions, however, represent but one facet of questionanswering, whose broader goal is to provide humans withintuitive information access using natural language.In contrast to factoid questions, the objective for ?defi-nition?
questions is to produce as many useful ?nuggets?of information as possible.
For example, the answer to?Who is Aaron Copland??
might include the following:American composerwrote ballets and symphoniesborn in Brooklyn, New York, in 1900son of a Jewish immigrantAmerican communistcivil rights advocateUntil recently, definition questions remained a largelyunexplored area of question answering.
Standard factoidquestion answering technology, designed to extract sin-gle answers, cannot be directly applied to this task.
Thesolution to this interesting research challenge will drawfrom related fields such as information extraction, multi-document summarization, and answer fusion.In this paper, we present an approach to answeringdefinition questions that combines knowledge from threesources.
We present results from our own componentanalysis and the TREC 2003 Question Answering Track.2 Answering Definition QuestionsOur first step in answering a definition question is to ex-tract the concept for which information is being sought?called the target term, or simply, the target.
Once the tar-get term has been found, three techniques are employedto retrieve relevant nuggets: lookup in a database createdfrom the AQUAINT corpus1, lookup in a Web dictionaryfollowed by answer projection, and lookup directly in theAQUAINT corpus with an IR engine.
Answers from thethree different sources are then merged to produce the fi-nal system output.
The following subsections describeeach of these modules in greater detail.2.1 Target ExtractionWe have developed a simple pattern-based parser to ex-tract the target term using regular expressions.
If the nat-ural language question does not fit any of our patterns,the parser heuristically extracts the last sequence of capi-talized words in the question as the target.Our simple target extractor was tested on all definitionquestions from the TREC-9 and TREC-10 QA Track test-sets and performed with one hundred percent accuracy onthose questions.
However, there were several instanceswhere the target term was not correctly extracted from1official corpus used for the TREC QA Track, available fromthe Linguistic Data ConsortiumName Pattern BindingsCopular1 (e1 is) NP1 be NP2 [NP1 = t, NP2 = n]Become2 (e1 beca) NP1 become NP2 [NP1 = t, NP2 = n]Verb3 (e1 verb): NP1 v NP2 [where v ?
biography-verb; NP1 = t, NP2 = n]Appositive4 (e1/2 appo) NP1, NP2 [NP1 = t ?
n, NP2 = t ?
n]Occupation5 (e2 occu) NP1 NP2 [where head(NP1) ?
occupation; NP1 = n, NP2 = t]Parenthesis6 (e1 pare) NP1 (NP2) [NP1 = t, NP2 = n]Also-known-as7 (e1/2 aka) NP1, (also) known as NP2 [NP1 = t ?
n, NP2 = t ?
n]Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NP2 = t]Or9 (e1 or) NP1, or NP2 [NP1 = t, NP2 = n]Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NP2 = t]Relative clause11 (e1 wdt) NP (which|that) VP [NP = t, VP = n]1In order to filter out spurious nuggets (e.g., progressive tense), our system discards nuggets that do not begin with a determiner.2The verb become, like be, often yields good nuggets that define a target.3By statistically analyzing a corpus of biographies of famous people, we compiled a list of verbs commonly used to describe peopleand their accomplishments, such as write, invent, and make.4Either NP1 or NP2 can be the target; thus, we index both NPs as the target term.5NPs preceding proper nouns provide information such as occupation or affiliation.
To boost precision, our system discards nuggetsthat do not contain an occupation (e.g., actor, spokesman, leader).
We mined this list from WordNet and the Web.6Parenthetical expressions usually contain interesting nuggets; for persons, they often include a lifespan or job description.7Either NP1 or NP2 can be the target; thus, we index both NPs as the target term.8This and the previous pattern frequently identify hyponymy relations; typically, NP1 is the hypernym of NP2.9This pattern often identifies the discourse function of elaboration.10This pattern typically identifies an exemplification relationship, where NP2 is an instance of NP1.11Relative clauses often provide useful nuggets.Table 1: Description of the surface patterns used in constructing our database.
(t is short for target, n for nugget)the definition questions in TREC 2003, which made itdifficult for downstream modules to find relevant nuggets(see Section 3.2 for a discussion).2.2 Database LookupThe use of surface patterns for answer extraction hasproven to be an effective strategy for factoid questionanswering (Soubbotin and Soubbotin, 2001; Brill et al,2001; Hermjakob et al, 2002).
Typically, surface patternsare applied to a candidate set of documents returned bya document or passage retriever.
Although this strategyoften suffers from low recall, it is generally not a prob-lem for factoid questions, where only a single instanceof the answer is required.
Definition questions, however,require a system to find as many relevant nuggets as pos-sible, making recall very important.To boost recall, we employed an alternative strategy:by applying the set of surface patterns offline, we wereable to ?precompile?
from the AQUAINT corpus a listof nuggets about every entity mentioned within it.
Inessence, we have automatically constructed an immenserelational database containing nuggets distilled from ev-ery article in the corpus.
The task of answering defini-tion questions then becomes a simple lookup for the rel-evant term.
This approach is similar in spirit to the workreported by Fleischman et al (2003) and Mann (2002),except that our system benefits from a greater variety ofpatterns and answers a broader range of questions.Our surface patterns operated both at the word andpart-of-speech level.
Rudimentary chunking, such asmarking the boundaries of noun phrases, was performedby grouping words based on their part-of-speech tags.
Intotal, we applied eleven surface patterns over the entirecorpus?these are detailed in Table 1, with examples inTable 2.Typically, surface patterns identify nuggets on the or-der of a few words.
In answering definition questions,however, we decided to return responses that include ad-ditional context?there is evidence that contextual in-formation results in higher-quality answers (Lin et al,2003).
To accomplish this, all nuggets were expandedaround their center point to encompass one hundred char-acters.
We found that this technique enhances the read-ability of the responses, because many nuggets seem oddand out of place without context.The results of applying our surface patterns to the en-tire AQUAINT corpus?the target, pattern type, nugget,and source sentence?are stored in a relational database.To answer a definition question, the target is used to queryfor all relevant nuggets in the database.2.3 Dictionary LookupAnother component of our system for answeringdefinition questions utilizes an existing Web-baseddictionary?dictionary definitions often supply knowl-edge that can be directly exploited.
Previous factoid ques-Copular A fractal is a pattern that is irregular, but self-similar at all size scalesBecome Althea Gibson became the first black tennis player to win a Wimbledon singles titleVerb Francis Scott Key wrote ?The Star-Spangled Banner?Appositive The Aga Khan, Spiritual Leader of the Ismaili MuslimsOccupation steel magnate Andrew CarnegieParenthesis Alice Rivlin (director of the Office of Management and Budget)Also-known-as special proteins, known as enzymes // amitriptyline, also known as ElavilAlso-called amino acid called phenylalanineOr caldera, or cauldron-like cavity on the summitLike prominent human rights leaders like Desmond TutuRelative clause Solar cells which currently produce less than one percent of global power suppliesTable 2: Example nuggets for each pattern.
(target term in bold, textual landmark in italics, and nugget underlined)tion answering systems have already demonstrated thevalue of semistructured resources on the Web (Lin andKatz, 2003); we believe that some of these resources canbe similarly employed to answer definition questions.The setup of the TREC evaluations requires every an-swer to be paired with a supporting document; therefore,a system cannot simply return the dictionary definitionof a term as its response.
To address this issue, we de-veloped answer projection techniques to ?map?
dictio-nary definitions back onto AQUAINT documents.
Simi-lar techniques have been employed for factoid questions,for example, in (Brill et al, 2001).We have constructed a wrapper around the Merriam-Webster online dictionary.
To answer a question usingthis technique, keywords from the target term?s dictio-nary definition and the target itself are used as the queryto Lucene, a freely-available open-source IR engine.
Oursystem retrieves the top one hundred documents returnedby Lucene and tokenizes them into individual sentences,discarding candidate sentences that do not contain the tar-get term.
The remaining sentences are scored by theirkeyword overlap with the dictionary definition, weightedby the inverse document frequency of each keyword.
Allsentences with a non-zero score are retained and short-ened to one hundred characters centered around the targetterm, if necessary.The following are two examples of results from ourdictionary lookup component:What is the vagus nerve?Dictionary definition: either of the 10th pair ofcranial nerves that arise from the medulla andsupply chiefly the viscera especially with auto-nomic sensory and motor fibersProjected answer: The vagus nerve is some-times called the 10th cranial nerve.
It runs fromthe brain .
.
.What is feng shui?Dictionary definition: a Chinese geomanticpractice in which a structure or site is chosenor configured so as to harmonize with the spir-itual forces that inhabit itProjected answer: In case you?ve missed thefeng shui bandwagon, it is, according to Web-ster?s, ?a Chinese geomantic practice .
.
.This strategy was inspired by query expansiontechniques often employed in document retrieval?essentially, the dictionary definition of a term is used asthe source of expansion terms.
Creative use of Web-based resources combined with proven information re-trieval techniques enables this component to provide highquality responses to definition questions.2.4 Document LookupIf no answers are found by the previous two techniques,as a last resort our system employs traditional documentretrieval to extract relevant nuggets.
The target term isused as a Lucene query to gather a set of one hundred can-didate documents.
These documents are tokenized intoindividual sentences, and all sentences containing the tar-get term are retained as responses (ranked by the Lucene-generated score of the document from which they came).These sentences are also shortened if necessary.2.5 Answer MergingThe answer merging component of our system is re-sponsible for integrating results from all three sources:database lookup, dictionary lookup, and documentlookup.
As previously mentioned, responses extractedusing document lookup are used only if the other twomethods returned no answers.Redundancy presents a major challenge for integratingknowledge from multiple sources.
This problem is espe-cially severe for nuggets stored in our database.
Sincewe precompiled knowledge about every entity instancein the entire AQUAINT corpus, common nuggets are of-ten repeated.
In order to deal with this problem, we ap-plied a simple heuristic to remove duplicate information:if two responses share more than sixty percent of theirkeywords, one of them is randomly discarded.After duplicate removal, all responses are ordered bythe expected accuracy of the technique used to extractthe nugget.
To determine this expected accuracy, we per-formed a fine-grained evaluation for each surface patternas well as the dictionary lookup strategy; we discuss theseresults further in Section 3.1.Finally, the answer merging component decides howmany responses to return.
Given n total responses, wecalculate the final number of responses to return as:n if n ?
10n +?n?
10 if n > 10Having described the architecture of our system, weproceed to present evaluation results.3 EvaluationIn this section we present two separate evaluations ofour system.
The first is a component analysis of ourdatabase and dictionary techniques, and the second in-volves our participation in the TREC 2003 Question An-swering Track.3.1 Component EvaluationWe evaluated the performance of each individual surfacepattern and the dictionary lookup technique on 160 def-inition questions selected from the TREC-9 and TREC-10 QA Track testsets.
Since we primarily generated ourpatterns by directly analyzing the corpus, these questionscan be considered a blind testset.
The performance of oursurface patterns and our dictionary lookup technique isshown in Table 3.Overall, database lookup retrieved approximately eightnuggets per question at an accuracy nearing 40%; dictio-nary lookup retrieved about 1.5 nuggets per question atan accuracy of 45%.
Obviously, recall of our techniquesis extremely hard to measure directly; instead, we use theprevalence of each pattern as a poor substitute.
As shownin Table 3, some patterns occur frequently (e.g., e1 is ande1 appo), but others are relatively rare, such as the rela-tive clause pattern, which yielded only six nuggets for theentire testset.These results represent a baseline for the performanceof each technique.
Our focus was not on perfecting eachindividual pattern and the dictionary matching algorithm,but on building a complete working system.
We will dis-cuss future improvements and refinements in Section 5.3.2 TREC 2003 ResultsOur system for answering definition questions was in-dependently and formally evaluated at the TREC 2003Question Answering Track.
For the first time, TRECevaluated definition questions in addition to factoid andlist questions.
Although our entry handled all three typesPattern accuracy nuggetse2 also 85.71 7e2 aka 80.00 5e2 occu 69.35 62e1 or 67.74 31e1 wdt 66.67 6e2 like 64.60 113e2 appo 60.00 20e1 aka 50.00 2e1 is 35.37 246e1 pare 34.91 106e1 appo 30.40 579e1 verb 26.09 92e1 beca 25.00 8average 38.37 98.2total 1277dictionary 45.23 241Table 3: Performance of each surface pattern and the dic-tionary lookup technique for all 160 test questions.Group Run F-measureMITCSAIL03a 0.309MIT MITCSAIL03b 0.282MITCSAIL03c 0.282best 0.555Overall baseline IR 0.493median 0.192Table 4: Official TREC 2003 results.of questions, we only report the results of the definitionquestions here; see (Katz et al, 2003) for description ofthe other components.Overall, our system performed well, ranking eighth outof twenty-five groups that participated (Voorhees, 2003).Our official results for the definition sub-task are shownin Table 4, along with overall statistics for all groups.
Theformula used to calculate the F-measure is given in Fig-ure 1.
The ?
value of five indicates that recall is consid-ered five times more important than precision, an arbi-trary value set for the purposes of the evaluation.Nugget precision is computed based on a length al-lowance of one hundred non-whitespace characters perrelevant response, because a pilot study demonstrated thatit was impossible for assessors to consistently enumer-ate the total set of ?concepts?
contained in a system re-sponse (Voorhees, 2003).
The assessors?
nugget list (i.e.,the ground truth) was created by considering the unionof all responses returned by all participants.
All rele-vant nuggets are divided into ?vital?
and ?non-vital?
cat-egories, where vital nuggets are items of information thatLet r # of vital nuggets returned in a responsea # of non-vital nuggets returned in a responseR total # of vital nuggets in the assessors?
listl # of non-whitespace characters in the entireanswer stringThenrecall (R) = r/Rallowance (?)
= 100?
(r + a)precision (P) ={1 if l < ?1?
l?
?l otherwiseFinally, the F (?
= 5) = (?2 + 1)?
P ?R?2 ?
P +RFigure 1: Official definition of F-measure.must be in a definition for it to be considered ?good?.Non-vital nuggets may also provide relevant information,but a ?good?
definition does not need to include them.Nugget recall is thus only a function of vital nuggets.The best run, with an F-measure of 0.555, was submit-ted by BBN (Xu et al, 2003).
The system used many ofthe same techniques we described here, with one impor-tant exception?they did not precompile nuggets into adatabase.
In their own error analysis, they cited recall asa major cause of bad performance; this is an issue specif-ically addressed by our approach.Interestingly, Xu et al also reported an IR baselinewhich essentially retrieved the top 1000 sentences in thecorpus that mentioned the target term (subjected to sim-ple heuristics to remove redundant answers).
This base-line technique achieved an F-measure of 0.493, whichbeat all other runs (expect for BBN?s own runs).
Becausethe F-measure heavily favored recall over precision, sim-ple IR techniques worked extremely well.
This issue isdiscussed in Section 4.1.To identify areas for improvement, we analyzed thequestions on which we did poorly and found that manyof the errors can be traced back to problems with targetextraction.
If the target term is not correctly identified,then all subsequent modules have little chance of provid-ing relevant nuggets.
For eight questions, our system didnot identify the correct target.
The presence of stopwordsand special characters in names was not anticipated:What is Bausch & Lomb?Who is Vlad the Impaler?Who is Akbar the Great?Our naive pattern-based parser extracted Lomb, Im-paler, and Great as the target terms for the above ques-tions.
Fortunately, because Lomb and Impaler wererare terms, our system did manage to return relevantnuggets.
However, since Great is a very common word,our nuggets for Akbar the Great were meaningless.The system?s inability to parse certain names is relatedto our simple assumption that the final consecutive se-quence of capitalized words in a question is likely to bethe target.
This simply turned out to be an incorrect as-sumption, as seen in the following questions:Who was Abraham in the Old Testament?What is ETA in Spain?What is Friends of the Earth?Our parser extracted Old Testament, Spain, and Earthas the targets for these questions, which directly resultedin the system?s failure to return relevant nuggets.Our target extractor also had difficulty with apposi-tion.
Given the question ?What is the medical conditionshingles?
?, the extractor incorrectly identified the entirephrase medical condition shingles as the target term.
Fi-nally, our policy of ignoring articles before the target termcaused problems with the question ?What is the Hague?
?Since we extracted Hague as the target term, we returnedanswers about a British politician as well as the city inHolland.
Our experiences show that while target extrac-tion seems relatively straightforward, there are instanceswhere a deeper linguistic understanding is necessary.Overall, our database and dictionary lookup techniquesworked well.
For six questions (out of fifty), however,neither technique found any nuggets, and therefore oursystem resorted to document lookup.4 Evaluation ReconsideredThis section takes a closer look at the setup of the defini-tion question evaluation at TREC 2003.
In particular, weexamine three issues: the scoring metric, error inherent inthe evaluation process, and variations in judgments.4.1 The Scoring MetricAs defined, nugget recall is only a function of the nuggetsconsidered ?vital?.
This, however, leads to a counter-intuitive situation where a system that returned everynon-vital nugget but no vital nuggets would receive ascore of zero.
This certainly does not reflect the informa-tion needs of a real user?even in the absence of ?vital?information, related knowledge might still be useful to auser.
One solution might be to assign a relative weight todistinguish vital and non-vital nuggets.The distinction between vital and non-vital nuggetsis itself somewhat arbitrary.
Consider some relevantnuggets for the question ?What is Bausch & Lomb??
:world?s largest eye care companyabout 12000 employeesin 50 countriesRun Total Relevant RecallNuggets Returnedofficial 407 118 28.99%fixed 407 120 29.48%Table 5: Nugget recall, disregarding the distinction be-tween vital and non-vital nuggets.Figure 2: F-measure as a function of ?.approx.
$1.8 billion annual revenuebased in Rochester, New YorkAccording to the official assessment, the first fournuggets are vital and the fifth is not.
This means thatthe location of Bausch & Lomb?s headquarters is consid-ered less important than employee count and revenue.
Wedisagree and also believe that ?based in Rochester, NewYork?
is more important than ?in 50 countries?.
Sinceit appears that the difference between vital and non-vitalcannot be easily operationalized, there is little hope forsystems to learn and exploit this distinction.As a reference, we decided to reevaluate our sys-tem, ignoring the distinction between vital and non-vitalnuggets.
The overall nugget recall is reported in Table 5.We also report the nugget recall of our system after fixingour target extractor to handle the variety of target termsin the testset (the ?fixed?
run).
Unfortunately, our perfor-mance for the fixed run did not significantly increase be-cause the problem associated with unanticipated targetsextended beyond the target extractor.
Since our surfacepatterns did not handle these special entities, the databasedid not contain relevant entries for those targets.Another important issue in the evaluation concerns thevalue of ?, the relative importance between precisionand recall in calculating the F-measure.
The top entryachieved an F-measure of 0.555, but the response lengthaveraged 2059 non-whitespace characters per question.In contrast, our run with an F-measure of 0.309 averagedonly 620 non-whitespace characters per answer (only twoother runs in the top ten had average response lengthslower than ours; the lowest was 338).
Figure 2 shows F-measure of our system, the top run, and the IR baselineplotted against the value of ?.
As can be seen, if precisionand recall are considered equally important (i.e., ?
= 1),the difference in performance between our system andthat of the top system is virtually indistinguishable (andour system performs significantly better than the IR base-line).
At the level of ?
= 5, it is obvious that standard IRtechnology works very well.
The advantages of surfacepatterns, linguistic processing, answer fusion, and othertechniques become more obvious if the F-measure is notas heavily biased towards recall.What is the proper value of ??
As this was the first for-mal evaluation of definition questions, the value was setarbitrarily.
However, we believe that there is no ?correct?value of ?.
Instead, the relative importance of precisionand recall varies dramatically from application to applica-tion, depending on the user information need.
A collegestudent writing a term paper, for example, would mostlikely value recall highly, whereas the opposite would betrue for a user asking questions on a PDA.
We believe thatthese tradeoffs are worthy of further research.4.2 Evaluation ErrorIn the TREC 2003 evaluation, we submitted three iden-tical runs, but nevertheless received different scores foreach of the runs.
This situation can be viewed as a probeinto the error margin of the evaluation?assessors are hu-man and naturally make mistakes, and to ensure the qual-ity of the evaluation we need to quantify this variation.Voorhees?
analysis (2003) revealed that scores for pairs ofidentical runs differed by as much as 0.043 in F-measure.For the three identical runs we submitted, there wasone nugget missed in our first run that was found in theother two runs, ten nuggets from six questions missedfor our second run that were found in the other runs, andten nuggets from five questions missed in our third run.There were also nine nuggets from seven questions thatwere missed for all three runs, even though they wereclearly present in our answers.Together over our three runs, there were 48 nuggetsfrom 13 questions that were clearly present in our re-sponses but were not consistently recognized by the as-sessors.
The question affected most by these discrepan-cies was ?Who is Alger Hiss?
?, for which we received anF-measure of 0.671 in our first run, while for the secondand third runs we received a score of zero.If the 48 missed nuggets had been recognized by theassessors, our F-measure would be 0.327, 0.045 higherthan the score we actually received for runs b and c. Thissingle-point investigation is not meant to contest the rel-ative rankings of submitted runs, but simply to demon-strate the magnitude of the human error currently presentin the evaluation of definition questions (presumably, allgroups suffered equally from these variations).4.3 Variations in JudgmentThe answers to definition questions were judged by hu-mans, and humans naturally have differing opinions as tothe quality of a response.
These differences of opinion arenot mistakes (unlike the issues discussed in the previoussection), but legitimate variations in what assessors con-sider to be acceptable.
These variations are compoundedby the small size of the testset?only fifty questions.
Ina post-evaluation analysis, Voorhees (2003) determinedthat a score difference of at least 0.1 in F-measure is re-quired in order for two evaluation results to be consid-ered statistically different (at 95% confidence).
A rangeof ?0.1 around our F-measure of 0.309 could either pushour results up to fifth place or down to eleventh place.A major source of variation is whether or not a pas-sage matches a particular nugget in the assessor?s list (theground truth).
Obviously, the assessors are not merelydoing a string comparison, but are instead performing a?semantic match?
of the relevant concepts involved.
Thefollowing passages were rejected as matches to the asses-sors?
nuggets:Who is Al Sharpton?Nugget: Harlem civil rights leaderOur answer: New York civil rights activistWho is Ari Fleischer?Nugget: Elizabeth Dole?s Press SecretaryOur answer: Ari Fleischer, spokesman for .
.
.Elizabeth DoleWhat is the medical condition shingles?Nugget: tropical [sic] capsaicin relieves pain ofshinglesOur answer: Epilepsy drug relieves pain from.
.
.
shinglesConsider the nugget for Al Sharpton: although an ?ac-tivist?
may not be a ?leader?, and someone from NewYork may not necessarily be from Harlem, one might ar-gue that the two nuggets are ?close enough?
to warrant asemantic match.
The same situation is true of the othertwo questions.
The important point here is that differentassessors may judge these nuggets differently, contribut-ing to detectable variations in score.Another important issue is the composition of the as-sessors?
nugget list, which serves as ?ground truth?.
Toinsure proper assessment, each nugget should ideally rep-resent an ?atomic?
concept?which in many cases, itdoes not.
Again consider the nugget for Al Sharpton; ?aHarlem civil rights leader?
includes the concepts that hewas an important civil rights figure and that he did muchof his work in Harlem.
It is entirely conceivable that aresponse would provide one fact but not the other.
Howthen should this situation be scored?
As another example,one of the nuggets for Alexander Pope is ?English poet?,which is clearly two separate facts.Another desirable characteristic of the assessor?snugget list is uniqueness?nuggets should be unique, notonly in their text but also in their meaning.
In the TREC2003 testset, three questions had exact duplicate nuggets.Furthermore, there were also several questions for whichmultiple nuggets are nearly synonymous (or are impliedby other nuggets), such as the following:What is TB?highly infectious lung diseasecontagious respiratory diseasecommon communicable diseaseWho is Allen Iverson?professional basketball playerphiladelphia 76 erWhat is El Shaddai?catholic charismatic groupchristian organizationcatholic sectreligious groupBecause the nuggets overlap greatly with each otherin the concepts they denote, consistent and reproducibleevaluation results are difficult.Another desirable property of the ground truth is com-pleteness, or coverage of the nuggets?which we alsofound to be lacking.
There were many relevant items ofinformation returned by our runs that did not make it ontothe assessors?
nugget list (even as non-vital nuggets).
Forthe question ?Who is Alberto Tomba?
?, the fact that heis Italian was not judged to be relevant.
For ?What arefractals?
?, the ground truth does not contain the idea thatthey can be described by simple formulas, which is oneof their most important characteristics.
Some more ex-amples are shown below:Aga Khan is the founder and principal share-holder of the Nation Media Group.The vagus nerve is the sometimes known as the10th cranial nerve.Alexander Hamilton was an author, a general,and a founding father.Andrew Carnegie established a library systemin Canada.Angela Davis taught at UC Berkeley.This coverage issue also points to a deeper method-ological problem with evaluating definition questions bypooling the results of all participants.
Vital nuggets maybe excluded simply because no system returned them.Unfortunately, there is no easy way to quantify this phe-nomenon.Clearly, evaluating answers to definition questions isa challenging task.
Nevertheless, consistent, repeatable,and meaningful scoring guidelines are critical to drivingthe development of the field.
We believe that lessonslearned from our analysis can lead to a more refined eval-uation in the coming years.5 Future WorkThe results of our work highlight several areas for futureimprovement.
As mentioned earlier, target extraction is akey, non-trivial capability critical to the success of a sys-tem.
Similarly, database lookup works only if the relevanttarget terms are identified and indexed while preprocess-ing the corpus.
Both of these issues point to the need for amore robust named-entity extractor, capable of handlingspecialized names (e.g., ?Bausch & Lomb?, ?Destiny?sChild?, ?Akbar the Great?).
At the same time, the named-entity extractor must not be confused by sentences suchas ?Raytheon & Boeing are defense contractors?
or ?Shegave John the Honda for Christmas?.Another area for improvement is the accuracy of thesurface patterns.
In general, our patterns only used lo-cal information; we expect that expanding the context onwhich these patterns operate will reduce the number offalse matches.
As an example, consider our e1 is pattern;in one test, over 60% of irrelevant nuggets were caseswhere the target is the object of a preposition and notthe subject of the copular verb immediately following it.For example, this pattern matched the question ?What ismold??
to the sentence ?tools you need to look for moldare .
.
.?.
If we endow our patterns with better linguis-tic notions of constituency, we can dramatically improvetheir precision.
Another direction we are pursuing is theuse of machine learning techniques to learn predictorsof good nuggets, much like the work of Fleischman etal.
(2003).
Separating ?good?
from ?bad?
nuggets fitsvery naturally into a binary classification task.6 ConclusionIn this paper, we have described a novel set of strategiesfor answering definition questions from multiple sources:a database of nuggets precompiled offline using surfacepatterns, a Web-based electronic dictionary, and doc-uments retrieved using traditional information retrievaltechnology.
We have also demonstrated how answersderived using multiple strategies can be smoothly inte-grated to produce a final set of answers.
In addition, ouranalyses have shown the difficulty of evaluating defini-tion questions and inability of present metrics to accu-rately capture the information needs of real-world users.We believe that our research makes significant contribu-tions toward the understanding of definition questions, alargely unexplored area of question answering.7 AcknowledgementThis work was supported in part by the ARDA?s Ad-vanced Question Answering for Intelligence (AQUAINT)Program.ReferencesEric Brill, Jimmy Lin, Michele Banko, Susan Dumais,and Andrew Ng.
2001.
Data-intensive question an-swering.
In Proceedings of the Tenth Text REtrievalConference (TREC 2001).Michael Fleischman, Eduard Hovy, and AbdessamadEchihabi.
2003.
Offline strategies for online ques-tion answering: Answering questions before they areasked.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics (ACL2003).Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural language based reformulationresource and Web exploitation for question answering.In Proceedings of the Eleventh Text REtrieval Confer-ence (TREC 2002).Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-des, Gregory Marton, and Federico Mora.
2003.
In-tegrating Web-based and corpus-based techniques forquestion answering.
In Proceedings of the Twelfth TextREtrieval Conference (TREC 2003).Jimmy Lin and Boris Katz.
2003.
Question answeringfrom the Web using knowledge annotation and knowl-edge mining techniques.
In Proceedings of the TwelfthInternational Conference on Information and Knowl-edge Management (CIKM 2003).Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,David Huynh, Boris Katz, and David R. Karger.
2003.What makes a good answer?
The role of context inquestion answering.
In Proceedings of the Ninth IFIPTC13 International Conference on Human-ComputerInteraction (INTERACT 2003).Gideon Mann.
2002.
Fine-grained proper noun ontolo-gies for question answering.
In Proceedings of the Se-maNet?02 Workshop at COLING 2002 on Building andUsing Semantic Networks.Martin M. Soubbotin and Sergei M. Soubbotin.
2001.Patterns of potential answer expressions as clues to theright answers.
In Proceedings of the Tenth Text RE-trieval Conference (TREC 2001).Ellen M. Voorhees.
2002.
Overview of the TREC2002 question answering track.
In Proceedings of theEleventh Text REtrieval Conference (TREC 2002).Ellen M. Voorhees.
2003.
Overview of the TREC2003 question answering track.
In Proceedings of theTwelfth Text REtrieval Conference (TREC 2003).Jinxi Xu, Ana Licuanan, and Ralph Weischedel.
2003.TREC2003 QA at BBN: Answering definitional ques-tions.
In Proceedings of the Twelfth Text REtrievalConference (TREC 2003).
