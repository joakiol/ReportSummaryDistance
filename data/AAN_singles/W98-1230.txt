IIIIIIIIIII!IIIIA Constructivist Approach to Machine TranslationMichael CarlInstitut ffir Angewandte Informationsforschung,Mattin-Luther-Strafle 14,66111 Saatbr~cken, Germany,cad@iai.uni-sb.deIntroductionConstructivist cognitive theories conceptualize memoryas a dynamic process which is directly linked to percep-tion, memories and conclusion/induction (cf.
\[Sch91\]).From this point of view, memory serves to establishstructures that ate relevant o the cognitive system inthe present context of action.
The function of memoryis thus to participate in coherent behavior which makessurvival of the acting cognitive system easier (or pos-sible).
Memories ate similar to perceptions: they ateperceptions without an object.
Perception on the otherhand is an activity (and not a passive process) that isdriven by the memory.In some situations, for example we perceive solid bod-ies where - -  in terms of modern physics - -  there ateno bodies at all, but processes of energy exchange; frompsychoacoustics t is known that we can heat sounds thatphysically do not exist.
Conversely, psychological exper-iments confirm that perception only takes place ff aninterpretation can be assigned to the perceived phenom-enon \[Foe86\], [Foe73\].
In successive steps of abstractionsperception destroys parts of the information which can-not be expressed (or ate of no importance) in the agent'smodel of the situation.
This destruction will increase inproportion to the extent new situations cannot be han-dled.
Confirmation of old solutions, then, will take placeat the expense of new experiences.In this paper, I present a new approach to Machine2~auslation (MT) that - -  ~rnilas to memory - -  doesnot simply 'remember' known solutions of former prob-lems but creates new solutions for new problems.
Likeperception it 'perceives' a problem (i.e.
a text or a sen-tence) to the degree it can handle it.
Both, memory andMT axe successful because they generate useful (e.g.
con-sistent) behavior for an agent in a changing environment.MT has existed since the very beginning of computerscience and there are a number of different systems inresearch and on the market.
However, with respect omany other domains, the problems of MT are due to twopeculiarities of natural anguage: compositionality andhierarchical structure.
Hieraxchical structuring is we\]\]known in natural anguage processing (and MT) and ac-counts for the fact that words can be recursively groupedinto constituents.Composltionality in MT is a more basic phenomenon a drefers to the observation that words or groups of wordsate translated i entically in a different context.
A sourcetext and its translation can thus be considered to con-sist of composed units that ate independently translated.The choice of these units, however, is much discussed inMT literature.
If these units ate too small, the systemmay become unreliable because it may be impossible toproduce a correct arget language text.
If these units atetoo large the system may become too inflexible.In this paper I propose a new MT system a~chitecturewhich can accommodate a number of ditferent ransla-tion units and is thus appropriate for a number of dif-ferent user needs.
In a constant interaction with theMT system, a user can tune 'her' system in order to ob-tain the results she would have expected.
The architec-ture is similar to Example Based Machine Translation 1(EBMT) and owes alot to Case Based Reasoning (CBR).In the next section I will give a brief introduction toCBR.
In order to apply the CBR paradigm to MT, Ishall outline the need for a decomposition componentand the way decomposition is usually treated in MT.Next, I will give a definition of composltionality.
I willthen show that the similarity metric - -  as it is usedin many CBR systems - -  is not an appropriate meansin MT either for the decomposition of the source textor for retrieval.
Similarity, instead, can be based onabstraction: the more abstraction is performed, the moredissimilar two items ate.
In order for a MT system toreact creatively on nnlrnown input a certain degree ofabstraction is required.These considerations imply a number of constraints onthe design of MT systems and offer two degrees of free-dom that cannot analytically be determined.
The choiceof the translation units, their language corresponden-cies and the degree of creativity (abstraction) to whicha MT system may recombine these units is a matter ofuser needs and depends on the goal of the application.Thus, in a very limited domain a MT system is likely tobe different from an all-purpose MT system.
The impli-cations of these insights shall be discussed and relatedto the parameters in a MT system design.The remainder of the paper is dedicated to an implemen-tation of the outlined architecture which has the capacityto accommodate o a number of different user require-ments.
I shaft call this approach Constructivist Machine1 The paradigm of Example Based Machine Translationhas been started only recently, by a number of different au-thors (e.g.
\[SN90\], \[Bro96\], \[CC96\]).Carl 247 A Constructivist Approach to Machine TranslationMichael Carl (1998) A Constructivist Approach to Machine Translation.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98.. NewMethods in Language Processing and Computational Natural Language Learning, ACL, pp 247-256.F igure 1F igure 2The proposal will not now be implemented.
?T~Les propositions ne seront pas raises en application maintenantThe proposel will no~ now be implemented"Les propositions" ne seront ~ raises en application maintenant"T~anslation because similar to memory, the success of aMT outcome - -  in this scenario - -  is solely validated byand dynamically tuned to the needs of the acting agentfo~ whom the translation is being computed.CBR-SystemsCBR systems axe problem-solving systems that heavilyrely on old experiences.
For this reason they can be seenas a generalization of MT systems, too.
CBR systemssolve new problems by adapting (modifying) solutions toold problems that are stored in a case base.According to Richter \[Ric95\] four knowledge containersaxe available in CBR systems:?
In the case base are stored former problems with theirsolutions (cases).?
A similarity metric makes possible retrieval of casesfrom the case base that axe similar to new problems.?
An adaptation mechanism modifies the solutions ofthe retrieved cases according to the requirements ofthe problem.?
The case vocabulary contains the language in whichcases axe written.Retrieval of the appropriate case(s) is based on a.~m~lar-ity metric which is designed such that the retrieved casesrepresent an iustantiation of the problem concept.
Theproblem and the retrieved case(s) axe ~ becausethey are different ~tantist ious of the same concept.In the adaptation step, the solution that is part of theretrieved case is modified where it ditfevs from the prob-lem solution.
Adaptation is one of the most difficultparts in CBR systems because it depends on domainspecific knowledge and on the outcome of the other CBRcomponents.In both, retrieval and adaptation, the original problem isthus destroyed in order to compute a solution: while clas-sifying a problem according to the available conceptualclasses in the case base, some parts of the problem axe ex-pected to be more important han other parts.
In orderto dassify the problem appropriatdy, retrieval in CBR- -  like perception - -  thus evaluates pasts of the informa-tion differently.
The retrieved (set of) case(s) deviates ina number of properties from the original problem thatseem minor to the goal of the agent.
In adaptation,the retrieved solutions axe further modified in order togenerate a coherent outcome of the system.However, in order to apply the CB11 paradigm success-fully to MT, a decomposition component is required thatdivides a sentence into a sequence of chunks.
In the nextsection I shall explain the need for such a componentand outline its importance in MT.Decompos i t ion  in MTIn Machine Translation (MT) compositionality is cru-cial to attain a reasonable coverage.
One of the mainproblems in MT is transfer strategy: how and when totranslate a unit of the source language into a unit of thetarget language.The underlying hypothesis of all MT systems is thatunits derived from the source language string can bemapped onto units from which the target language stringcan be computed.
I use the following English-Frenchtranslation exampleS:1 The proposal will not now be implemented ~ ,Les propositions ne seront pas raises en applicationmainten~ntSeveral possibilities of decomposition shall be consideredin this section.
The corresponding adaptation knowledgewill be discussed.If this sentence (1) consist of only one chunk i.e.
decom-position takes place at a sentence l vel only as in figure 1,no adaptation is requited.
This is typical for translationmemories (TM) (e.g.
TRADOS \[Hey96\], TRANSIT)which have only a case base and a retrieval component s.TMs are likely to have quite long cases tored in the casebase, but have a well informed istance metric to returnslmilar cases from the case base.
However, due to lack ofadaptation, the less the retrieved cases match the sen-tence, the more incomplete and incorrect he translationis.
A major shortcoming is that the increase of coveragedoes not follow the growth of the case base to the sameextent.2The example is taken from the Hansards-corpus and isdiscussed in \[BCDP+90\].
For purpose of illustration I willconsider the translation correct and desirable.~Some TM, however, propose as an extra an interactiveMT system or a batch MT system that is based on a different(e.g.
'traditional') paradigm.Carl 248 A Constructivist Approach to Machine TranslationmRBmmmIIIIIIIIIIIIIIIIIIIIIII!IIFigure 3The proposal?
will not nou beLes propositions "he seront pusimplementedraises en application ma~ nt enanl~Figure 4The proposalLes propositionswill not now be implementedne seront pus raises en application main~enantIn figure 2, the sentence is decomposed into two chunks.Notice, that the English subject (the first chunk) is insingada~, while its French translation is in plural.
Be-cause subject and predicate agree in number and per-son, the (anrillary) verbs in the second chunk has totake the same features as in the first chunk.
The adap-tation mechanism has therefore to be able to reconstructthese agreement reqni~ements.If the sentence is divided into the three chunks/The pro-posal/, ~will not now be/and/implemented/as in figure3 adaptation turns out to be much more compllcsted.
Ithas to reconstruct agreement between the first and thesecond chunk (in the l~ench translation for the thirdchunk too).
Further, adaptation must take into accountthe discontinuity of the second chunk.
Thus, althoughnow and maintenant are translations of each other, theyare separated by the interposed chunk 3 on the Frenchside.
The adaptation mechanism has to be able to inte-grate a continuous chunk into a discontinuous one whentranslating from English to Fronch and the inverse whentranslating from French to English.Example based Machine Translation (EBMT) corre-sponds to a decomposition g~anularity as shown in fig-axes 2 and figure 3.
Some systems (d. \[SNg0\], [CC96\])localize major constituents in the problem sentence bymeans of linguistic analysis.
Adaptation in these sys-tems is essentially a matter of replacing items (words,sequences of words or constituents) in the target lan-guage structure.
Linguistic analysis erves to better de-termine the location and the appropriateness of potentialitems to be replaced in the target language.In the Pangloss EBMT (\[Bro96\], [NBD94\]) cases are se-lected from the case base that contain the problem case(or parts of it) as a substring.
By means of a thesaurusand a bi-lingual exicon the translation of the problemcase (or its respective pa~t) is extracted from the re-trieved cases.
Adaptation of the target language chunksis left to a statistical language model outside the Pan-gloss EBMT system.Many traditional MT systems have an atomic case baseand have no information about the similarity between asentence and some cases i.e.
only exact matching casesare retrieved from the case base.
According to figure 4,sentences are very free-grained and the main translationis carded out by the 'adaptation' mechanism.
All threegenerations of traditional MT systems (cf.
~WK95\]) canbe described by this schema.
The direct approach seeksto map lexical items of the sotuce language onto lexicalitems of the target language and then tries to rearrangethe tazget est.
The interlingual approach (cf.
\[Dor93\])tries to calculate a language independent meaning rep-resentation from which the target ext is generated.
Thetransfer approach (cf.
\[Str96\]) is situated in between thetwo: abstractions of the source language string are com-puted and then transferred (mapped) into target unitsfrom which the target language string is computed.
T~a-ditional MT systems do not systematically make use oflarge chnnks that could facilitate the adaptation mecha-nism.
They thus fail to account for what computers canmost easily do: memorization and retrieval.The main di~erence, however, between traditional andmore recent approaches to MT lies in the fact that thelatter systems pedorm monolingual nalysis and gener-ation while in former systems the analysis (decomposi-tion) of the source language is not independent from theregeneration possibilities in the target language.Compos i t iona l i ty  and  MTWhen decomposing a sentence in a particular way thesentence is dassitied with respect to the context 4.
In theFrench sentence ?e boueher sale la tranche the word salecan be classified as a verb (English: to salt) or as anadjective (English: dirty), la can be an article (English:the) or a pronoun (English: she~her) and t~znche can bea verb (English: to chop) or a noun (English: slice).4This view is of course ot new.
E.g.
in ~BDW96\] morpho-logical analysis (i.e.
morphological classification) is definedas a decomposition task.Carl 249 A Construct\[vist Approach to Machine Translation2 French: (Le boncher) sale (la tranche)English: The butcher salts the slice3 French: (Le botcher sale) (la) traneheEnglish: The dirty butcher chops herIf the sentence is decomposed according to (2) the phraseLe boucheris classified as the subject of the sentence andla tranche as the object.
In (3) Le boncher saleis the sub-ject while his the object.
Passing, for instance, classifiesthe components of a sentence and their relationship bygiving it a structure according to a grammar.However, I will not use a grammar as a basis for de-composition but base the decomposition on examples.This, I shall show, leads to greater flexibility and bettermaintainability of the system.In MT different decompositions become particularly cru-cial if they lead to different ranslations i.e if the sourcelanguage and the target language xpress ambiguities ina different way, which is quite often the case.
A sourcesentence S is compositionally translatable into a targetsentence T, i f?
it is decomposable into a set of chunks.A sentence S is decomposable,  i f  it can be dividedinto a set of chunks Cl.. .
Cn where the intersection ofthe chnnlr~' concepts equals the concept of the case:C(S) = I'k C(~,)?
the case base covers the chunks.A case base CB covers a set of chunks c G S iff thereexists for each c at least one solution case s E CBwhere both c and s are instantistions of the same con-cept: Vc G S 3s E CB : C(c) - C(s)?
the retrieved solutions axe adaptable.A set of solutions l .
.
.
s,~ is adaptable,  i f  it can becomposed into one target sentence T and the intersec-tion of the solutions' concepts equals the concept ofthe result: \[ 'k C(s,~) - C(T)We thus obtain a chain of conceptual equivalences duringall processing steps as shown in equation 1.c(s)_= (,A source sentence S is decomposed into a set of chunksc for each of which a solution s is retrieved from the casebase.
The set of solutions is then composed into a targetsentence T,Similarity and MTIn CBR systems, the similarity metric is a means for clas-sifying the problem according to the empirical data inthe case base.
In this section I shall investigate whethera similarity metric is appropriate for decomposition andclassification in MT.Similarity metrics in CBR are often based on nearestneighbor (NN) algorithms (e.g.
\[WD95\]).
NN algo-rithms make use of a continuous variable w which as-sociates a real value to the attributes ai of a problema.
The similarity between s problem a and a case b isinversely proportional to their distance Ds:5In a symbolic task, the distance d usually is d(a/, bl) = 0if al = b,, else 1.D(a,b) =iThe nearest neighbor in the hyperspace is then retchedas the most similar known instantiation to the problem.Other approaches use symbolic distance metrics.
Onesuch approach (\[Pla95\]) uses anti-uuification e that yieldsfor each case in the case base the intersection (i.e.
whatis common) with the problem.
By means of a prede-fined subsumption ordering these intersections are or-dered.
The cases that are most similes to a problem arethose that field most specific anti-unification results.Other approaches (e.g.
\[Hut97\]) calculate for each casethe minimal number of changes that would have to bedone to transform it into the problem.
The weightedsum of changes then indicates the distance between thecase and the problem.While these metrics may be dynamically adapted inchanging environments (the case base and/or the weightw may be dynamically altered), there axe at least twoproblems when applied to MT.One problem occurs when decomposing a text of arbi-trary length as was shown above.
It is not at all evidenthow to decompose a text into units such that the adap-tation capacity of the system is respected.
Of course,the smaller the units are, the higher the probability ofretrieval success will be.
But we may not necessarily besure whether the retrieved solutions lead to a composi-tionally correct ranslation because it may be impossiblefor the adaptation mechanism to appropriately recom-pose them.Apart from the discontinuity of chunks as shown in fig-exe 3, there are other phenomena oflexical cooccuzrencethat need to be treated by a MT system.
For instance,if we know that German stark transhtes into Englishstrong and German Band translates into English vol-ume we axe likely to translate German starker Bandinto English strong volume by simply concatenating thetranslated units.
This might not always work well be-cause here thick volume would be a better translation.The sequence starker Band should hence be seen as oneundividable unit.
Suitable decomposition ofthe problemis thus a prerequisite for valuable retrieval of cases.A related problem is due to the way in which the solu-tion of a case is related to the matching part.
Partialsimilarity of a phrase and a case does not necessarilyallow one to conclude that there are comparable sim-ilarities between their translations.
For instance if weknow that German starke ErkSltnng translates into Eng-lish bad cold and German Rancher translates into Englishsmoker we axe likely to translate the unknown Germanphrase starker Rancher into English bad smoker becausethe first word of the unknown phrase is just another in-fleeted form of the first word in the known phrase.
Wethus substitute the second word in the translation of6While unification yields least upper bound (hb), anti-unification yields the greatest lower bound (glb) with respectto a subsumptlon ordering.Carl 250 A Constructivist Approach to Machine TranslationIIIIIIIIIIIIIIIIIIIIII!1IIIIthe known phrase (cold) by the translation of the sec-ond word of the unknown phrase (smoker) to obtain theresult.
However, this might not always be a good solu-tion because hea~y smoker (and not bad smoker) wouldbe the appropriate translation.
Of course, one can arguethat stark has different readings according to the contextin which it appears o that starker Raucher and starkeErk~ltung are not similar at all.
However, the knowl-edge concerning the appropriateness of words which canbe replaced needs to be coded in some way 7.
In theproposed architecture xceptions are stored in the casebase and knowledge about replacable words is extracted(induced) from the case base.
This makes possible a dy-namic graduation between regularities subregnlarities anexceptions as it occurs in natural anguages.Merely similarity of an input text and some cases in thecase base does not therefore lead to a satisfactory so-lution because it tells us neither how to decompose atext nor which parts in the retrieved cases are suitablefor substitution.
Further, from a logical point of view,similarity seems a useless notion because, as Goodman\[Goo72\] states, it cannot be measured in terms of, orequated with the possession of common characteristics:Where the number of things in the universe is n, each twothings hare in common ezactly T '-z properties out of thetotal of 2 n -  1 properties; each thing has 2 n-z propertiesthat the other does not, and there are 2 n -z -  1 propertiesthat neither has.
\[pp.
443-444\]The point here is that if two things have some proper-ties in common this is saying nothing more than thatthey have these properties in common i.e.
that they areequal with respect o the common properties.
However,which of the shared properties are more salient is ana-lytically untractable: it remains a matter of who makesthe comparison and when.
For instance in the examplesabove, one might find weighty tome to be a better trans-htion for starker Band than thick volume.
On the otherhand German starker Punk~ compositionally translatesinto English strong point.
It is a decision of the presentsystem architecture not to code these decisions into theprogram or into a grammar, but to leave it in the struc-ture of the case base.Abst rac t ion  as S imi la r i tyInstead of having a similarity metric to classify a sen-tence (or parts of it) decomposition is used as a methodfox classification.
In order to determine the similarityof a complex sentence and some cases in the case baseabstraction by means of decomposition and reductionseems an appropriate means s.rI agree with the comment of an anonymous reviewer thata more thorough linguistic analysis may ~ell yield a betterperformance for direct use of similarity metrics, subvertingthe needs of post hoe adaptations.The problem is how cart you know when you have done suf-ficiently linguistic analyses without reference to the data?SA similar idea can be found in \[CC96\].
However, intheir approach sentences undergo a (rule-drlven) syntacticanalysis.Abstractions are induced from the input sentence basedon cases in the case base.
The less an input sentence isknown to the system, the more abstractions are neededfor the sentence to be matched onto the case base.
How-ever, the more abstractions are performed, the greaterwill be the dissimilarity between the sentence and thematching case(us).
Accordingly, it is stressed in \[BW96\]that the significance of similarity between a problem anda set of cases is more important he hss abstract hecases  are .In the proposed system architecture a sentence is decom-posed into a set of chunks according to the available casesin the case base.
Chunks which share all their proper-ties with a case are reduced and the sequence of reducedand unreduced chunks (i.e.
the abstraction of the origi-nal sentence) is, again, decomposed and matched againstthe case base until no more decomposition and reductionis possible.
In a number of steps, a sentence of length mis thus classified according to the available cases in thecase base into maximal 2m chunks.A sentence is regenerated from an abstraction by specify-ing the reduced chunks and their subsequent refinement.This is repeated until the produced sequence contains nomore reduced chunks.Abstraction (i.e.
decomposition and reduction) and gen-eration (i.e.
specification and refinement) are possible ifthe following criteria hold for the matching chunks inthe abstraction process and the reduced chunks in thegeneration process:?
Chunks are independentwith respect o some 'fixed'features.
The fixed values of one chunk does not affectthe fixed values of another chunk.?
Chunks are adaptable with respect o some 'variable'features.
The set of variabh features for each chunkreflects its inter-chunk dependencies.In order to translate the French sentence Le boncher salela tranche into English The butcher salts the slice accord-ing to the classification 2 above we need the case base tocontain the two concrete cases 4 and 5 and the abstractcase 6:4 le boueher ~ the butcher5 la tranche ~ the slice6 Xsa leY  ~ ,Xsa l tYBased on the examples, the French sentence is decom-posed into the three chunks cx: le boucher, cz: sale andcz: la tranche.
By reducing the (matching) chunks cland c3 the abstraction q salt c3 then matches case 6.The adaptation mechanism subsequently re-specifies andrefines the solutions sl and s3 of the reduced chunks cland c3.
Specification consists in replacing sl in the ab-straction by the butcher and s3 by the slice.
Refinementadapts the variable features uch that the main verb saltagrees in number and person with the chunk inserted inposition X.
Note that the required adaptation complex-ity corresponds to figure 2 above.In this translation the number of decompositions i  3.Another chunking is possible if the case base allows it.For the case base 7 below, the number of decompositionsCarl 251 A Constructivist Approach to Machine Translationequals I: the granularity of decomposition thus relies onthe structure of the case base.7 le boueher sale la tranche, .~the butcher salts the sliceNote that by means of case 4, 5 and 7, the abstract case6 can be induced.Undesirable abstraction is possible and is on the onehand an expression of the creativity of the system but onthe other hand avoidable by adding further cases to thecase base.
Thus, abstraction 10 can be generated basedon the cases 8 and 9.
As outlined in the previous ection,abstraction 10 has the potential to (wrongly) translatestarker Punkt into heavy point.
However, by adding case11 to the case base this is no longer possible.8 Raucher ~ ~ smoker9 starker Raucher 4. , heavy smoker10 starker X ~ heavy X11 starker Punkt ~ .
, strong pointFreedom in MT sys tem des ignIn the equivalence (1) w here reproduced as (2) - -  thedecomposition granularity, the structure of the case baseand the adaptation mechanism depend on each other.c(s) - fqc( .)
=_ NC(s . )
- c(T) (2)To preserve the conceptual equivalence between asourcesentence S and its translation T, all three componentsneed to be synchronized: a certain type of decompositionrequires an adequate case base which covers the decom-posed chunks and an appropriate adaptation mechanismwhich is able to re-combine the retrieved solutions into atarget ranslation.
However, the above equivalence con-rains two degrees of freedom: one degree is related to thenumber n of chunks the other is due to the definition ofthe conceptual equivalence C. Neither can analyticallybe determined because they axe closely related to the re-quirements of a user and his expectations with regard toa MT outcome..
The coverage of the case base increases while thelength of the cases gets shorter.
The coverage of thesystem depends on one hand on the coverage of thecase base and on the other hand on the level of abstrac-tion on which the chunks are matched.
The coverageof the system thus increases with finer decompositiongranularity and a high degree of abstraction.?
The reliability of the results is likely to increase whilethe length of the chunks gets longer and the systemturns into a mere retrieval system of known solutions.Rehability thus increases with coarse decompositiongranularity and low degree of abstraction.?
The creativity of the system combines both, coverageand reliability.
A high degree of creativity can thusbe reached with coarse decomposition granularity andhigh degree of abstraction.Figure 5 shows possible realisations for MT systems.The horizontal axis represents he decomposition granu-larity; the vertical axis represents he degree of abstrac-tion.As the granularity of the decomposition becomes coarser,the system loses coverage but the translation result willbecome more reliable.
The adaptation mechanismbe very simple.
Conversely, finer grmlula~ity implies bet-ter coverage but requires a more complex adaptationmechanism.
Orthogonal to decomposition granularityis the degree of abstraction that a system performs.
Themore abstractions are performed, the less reliable will bethe outcome 9.Creativity is necessary for a MT system unless the do-main is restricted such that retrieval of already knowntranslations i sufficient for the coverage.
To attain acertain degree of creativity, the system needs to disposeof an appropriate degree of abstraction capacity joinedwith an appropriate decomposition granularity.Figure 5Degree of abstraction Chighlowcoverage creativityreliability~ne coarseGranularity of (de)composition nThe more the system design moves into the upper leftarea (high degree of abstraction and fine decompositiongranularity) in figure 5, the more the coverage of thesystem will increase.
The more the design moves intothe lower right area (low degree of abstraction and coarsedecomposition granular/ty) the more the reliability of thesystem increases.To reach reliability for unknown a~bitrazy texts, recentapproaches toNLP prefer shallow analyses that generatefiat representations (i.e.
low degree of abstraction).
Be-cause the number of possible wcong assignments in ananalysis tree grows exponentially with its depth, fiatrepresentations offer fewer possibilities to relate con-stituents and hence offer fewer possibilities to producewrong analyses.To attmn a certain degree of creativity, an appropriatedegree of abstraction and an appropriate degree of de-composition is required.
What degree of creativity auserdesires essentially depends on the variety of text types tobe translated (i.e.
the required coverage of the system)9Thls is supported by the findings in \[BDW96\]: the moreabstraction is performed by a system (degree of eagerness),the worse the generalization performance will be.Carl 252 A Constructivist Approach to Machine Translation||mmmIm|m|F igure 6English Phrase Descriptor of the sentence: The big man ea~s a green appleWD: WDThe I WDbia I ?
WDman I WDea,, I WD, I  WDg .
.
.
.LMA:  theCAT:  artVTP :TNS:NUM:CAS:DEG:WNR:  1bigadjbase2manverb nounf in  infinpres --  - -- -  - -  sing- -  - -  n;a3 3 3eatsverbtinpres4aar t5greenad j  noun- -  n;abase - -i 6 6\] WDa~pleapplenounsingn;a7German Phrase Descriptor of the sentence: Der grosse Mann isst einen gr~nen ApfelWD: WDve, l wDg ..... \[WDM.,,,, I WD,., \[ Izv'D.,,~.. \[ WDg.a,.,, \],, WDap,,,LMA: d_art d_relCAT: art telVTP: I - -  - -TNS:  I - -  - -  NUM: sg sg pin sg --GEN:  f m -- m fCAS: d;g n g n g;dDEG: - -  - -WNR: 1 1 1 1 1grossa~e ?e ?e ?base2mannnounsgmn;d;a3essenverbfinpressg4einartsgma5grinadj verbfin- -  presen* pluen*en* - -base - -6 6apfelnounsgmn;d;a7e* and en* denote the endings (e and en) of a German adjective.
They can be multiplied into a matrix of AVM conta~nl-ginformation on GEN, CAS, NUM and the determination class.LMA:CAT:NUM:VTP :TNS:CAS:DEG:WNR:lemma (basic word form without inflectional information)part-of-speech (syntactic ategory) (adj; adv; art; noun;punct; re1; verb)number (sing; ph)verb type (fro; infin)tense (pres;past)case (n; g; d; a)degree of adjectives (base; comp; sup)word numberand the expected reliability of the results.
In a constantfeed-back process, a user thus has the possibility of de-signing the MT system according to his requirements.In the remainder of this paper I will give a short overviewof the CBAG *?
system.
It can be used as a stand aloneMT system or can be integrated with the Rule BasedMachine Translation CAT2 \[Str96\].
Here, only the prin-cipal functioning shall be considered.
CBAG consistsof three modules: the Case Based Compilation module(CBC), the Case Base Analysis module (CBA) and theCase Base Generation module (CBG).Case Structure in CBAGInstead of simply storing surface strings in the case base,morphological nalysis and lemmatization is carried outand added to the cases n .I?CBAG stands for Case Based Analysis and Generation//We use MPRO (el.
\[Maa96\]) for morphological analysisand lemmatization.
MPRO is a very powerful tool, whichgenerates more than 95% correct analyses for arbitrary Get-Lemmatization yields for a surface string a basic wordform (lemma) that abstracts away from inflectional in-formation which is contained in the surface form.
Inflec-tional information such as person, number and tense forverbs or ease and number for nouns is determined by theuse of the word and is independent of the lemma.
For in-stance, man and men are di~erent instances of the same1emma (man) that only differ with respect o number(singular vs. plural).The part of speech takes an intermediate position be-tween lexical and grammatical information.
On onehand, the part of speech is linked to a lemma, on theother hand each part of speech has typical inflectionalpatterns (e.g.
the examples above).The features of a word are stored in the form of sets ofpairs of attribute/values AVM.
We will refer to a set ofAVM that belong to one word as a word descriptor WD.Note that the contents of each AVM is such that a singlesurface string can be regenerated from it.
Morphologicalman and English texts.Carl 253 A Constructivist Approach to Machine Translationanalysis and lemmatization are thus reversible: asurfacestring can be transformed into a WD (a set of AVM)and a surface form can be generated from a AVM.A phrase descriptor PD is a sequence of word descrip-tors WD1 .. .
WD,,.
A case CASE is a pair of a sourcephrase descriptor PD,o~ce and a target phrase descrip-tor PDtarget that axe considered to be translations ofeach other.
A case base CB is a set of cases.A PD can be represented as a M ?
N matrix wherethe columns describe the words of a phrase and the rowsdescribe a sequence of attribute values.
The figure 612reproduces the CASE of the following translation exam-ple:12 The big man eats a green apple :Der grosse Mann iss~ einen gr~nen ApfelSome words are ambiguous.
For instance, the surfacestring man can be analyzed as a verb or as a noun asshown in the table in figure 6.
The noun has an ac-cusative or a nominative case, the verb can be the in\]i-nlte form or the finite present form.
The WD,na,~ hasthus four interpretations that are melted here togetherinto three AVM.Decomposition i  CBAGDecomposition is example driven dad divides a PD intoa set of chunks.
I distinguish between two ways of de-composition:Hor izontal  decomposit ion divides the PD matrixinto a set of 'fixed' (lexical) features and into a set of'variable' (grammatical) features.
This division accountsfor the distinction between lexical and grammatical in-formation inherent in every sentence.Agreement within a noun phrase is a grammatical phe-nomenon.
The corresponding features are thus part ofthe set of variable features.
In German, for instance,the determiner, the adjective and the noun in a nounphrase have to agree in number, gender and case, whilethe actual lexical fillers of this syntactic schema mayvary.The set of variable features comprises all features thatcan be altered by a di~erent context (e.g.
number andcase for nouns, gender, tense, person for verbs, etc.
).The set of fixed features comprises the lemma and thepart of speech.Vertical decomposi t ion divides the PD matrix intoa sequence of chunks.
Vertical decomposition accountsfor the compositionality oflanguages.
In many contexts,for example, the English noun phrase t/ie man wouldbe translated into German dee Mann.
Most sentencescan be considered as being composed of a sequence ofchunks, that, to a certain extent, can be translated in-dependently.The reasoning behind the double decompositions is asfollows:X~For reasons of space, not all features are given in thematrix.Carl 254?
reduce the size of the case base: Each set of valuesneeds to be stored only once.
Thus man and menare instances of the same lemma, which only needs tobe stored once 13.
Vertical decomposition reduces thesize of the case base more dramatically: if sentencesare considered to consist of words which are groupedinto compositionally translatable chunks, only thosegroups of words must be stored as cases in the casebase which do not allow for compositional translation.?
reduce retrieval time: With a smaller case base theretrieval time of cases should also decrease.?
increase coverage of the system: Cases that can berecomposed tocomplex solutions can occur in differentcontexts.
These cases thus cover several problems.?
make possible case abstraction: While fixed featuresaxe specific for a chunk, variable features axe typicalfor it.
Case abstraction consists in abstracting awayfrom the specifidties of a chunk by keeping track ofthe variable (thus typical) features.Abstraction in CBAGCase abstraction is crucial in attaining a broader cov-erage of the system and to account for interdependen-cies of chunks.
In the translation phase abstractions arecomputed from the input PD, o,,,ce in order to matchabstractions in the case base.In the compilation phase of the case base, abstract casesare computed from the cases that use in the case base.Those chunks that match a case are reduced into a chunkdescriptor CD which consists of a set of vafiabh fea-tures and a chunk index.
To reduce a chunk the headinformation is extracted from it, where head informa-tion is made upon those features that are necessary andsufficient to express inter-chunk dependencies such asagreement.
The index of the chunk is stored with thehead information in the CD.
A sequence of reducedand unreduced CDs (an abstract case) may, again, bedecomposed and matched against he case base.In the following exaanple, the case base contains twocases :13 the big man ~ ~.
der grosse Mann14 a green apple t , ein griner ApfelAs shown in the figure 7, the English sentence The bigman eats a green apple is decomposed into the threechunks/The big man/ /eats /and/ /a  green apple/.
Byabstraction, the sequence COl C02 CD3 is generated,where CDI and CD3 represent reduced chunks that in-dude information on the type of constituent (such asgender, number etc.)
and the index of the matchingcase.
If a WD cannot be integrated into a chunk - -  asis the case for eat in the example - -  it is integrated asan unreduced chunk into the abstract case.Abstract cases are generated in a precompilation step asshown in figure 9.
A new case base CB is incrementallycreated starting from an ordered set of examples.
Basedl~This is even more important for highly inflective lan-guages and paradigms, as for example for Romance verbswhich can have up to 40 different surface forms dependenton person, number, modality, aspect and tense.A Constructivist Approach to Machine TranslationIIII|IIIII!|IIaskkIIIIIIIIIIatIIatIIIIIIIIFigure 7Decomposition and reduction: The English PD the big man eats a 9teen apple is decomposed and reduced into theabstraction CDI CD2 CD3.CDI CD2 CD3T T TAWD~h, WDb~ 9 WD~ WDeot WDa WD~ee.
VfDa~,~Figure 8Specification and refinement: The abstraction CD1 CD2 CD3 is specified and refined into the German PD Dergrosse Mann isst einen #rinen Apfel.CDx CD2 CD3Figure 9Algorithm to induce abstract cases.Sort examples E by number of wordsCB <- emptyfor all examples E /* shortest  first */beginV<-  TRUEwhile V = TRUEbeginA <- reduce(decomp(E,CB))add E to  CBif valid(A)then E <- Aelse V <- FALSEendendon partial matches of the example g and the case baseCB, first an abstraction A is calculated.
Than the originalconcrete xample is added to the case base.
This orderis important because the abstractions should not rely onthe same example.Abstractions are calculated in two steps: decompositiondecomp and reduction reduce.
An abstraction is validif:1. it contains at least one unreduced chunk2.
it contains at least one reduced chunk for both, sourceand target language side3.
decomposition is based on the same cases on both lan-guage sides4.
if it is validated by some other casesThe requirements 1 and 2 express we11-formedness condi-tions of abstractions.
A case that only consists of unre-duced CDs is not an abstract case because it containsno reductions.
An abstraction that only consists of re-duced chunks is not stored in the case base because itwill never be matched by other cases.
The requirement3 accounts for the fact that abstractions are generaliza-tions of regularities contained in the concrete case.
Apair of a source and a target sentence that are equallydecomposed are likely to be regular in that decomposi-tion.
Requirement 4 excludes exceptions to serve as abasis of abstractions as is the case in the heary smokerexample 9.
However, finer grained criteria on this mattercan be found in \[CC96\].Generat ionGeneration performs the reverse task of Abstraction.
Togenerate a target language PD from an abstraction, theabstract case is first specified and then refined into asequence of lower level CD.
This is repeated until thesequence contains no more reduced CD i.e.
the con-crete PDtarget.
Specification extends the chunk descrip-tors index by searching from the case base the solutions(i.e.
translations) of the appropriate cases.
Refinementsubstitutes the variable features in the specified CDsaccording to the head information.In figure 8 the chunk descriptor CDI  is specifiedinto the sequence WDDe, WD~oss WDMan,.
The' chunk descriptor CD2 is specified into the sequenceWDein WD~r~.
WD.4ple~ according to the cases in thecase base given in the last section.
While refining CDhthe corresponding WDs are transformed into nomina-tive singular because the first chunk is the subject of thephrase.
CDz is refined into accusative singular becausethe :Second chunk is the (accusative) object of the phrase.The sequence of WDs can then be sent to a morphologi-cal generation module, which calculates the appropriatesurface strings.ImplementationThe system as described is implemented in C and runsunder gnu-C on sun machine.
It consists of several pro-grams, that are connected via (unix) pipes.
The KURD(cf.
\[CSW98\] in this volume) formalism is a constraint-based shallow parser that is used for chunk reductionand chunk refinement.
Another program is used as adata base system for quick retrieval and decompositionof attribute value matrices as described above.Carl 255 A Constructivist Approach to Machine Translation
