Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 66?70,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTurker-Assisted Paraphrasing for English-Arabic Machine TranslationMichael Denkowski and Hassan Al-Haj and Alon LavieLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15232, USA{mdenkows,hhaj,alavie}@cs.cmu.eduAbstractThis paper describes a semi-automatic para-phrasing task for English-Arabic machinetranslation conducted using Amazon Me-chanical Turk.
The method for automaticallyextracting paraphrases is described, as areseveral human judgment tasks completed byTurkers.
An ideal task type, revised specif-ically to address feedback from Turkers, isshown to be sophisticated enough to identifyand filter problem Turkers while remainingsimple enough for non-experts to complete.The results of this task are discussed alongwith the viability of using this data to combatdata sparsity in MT.1 IntroductionMany language pairs have large amounts of paral-lel text that can be used to build statistical machinetranslation (MT) systems.
For such language pairs,resources for system tuning and evaluation tend tobe disproportionately abundant in the language typ-ically used as the target.
For example, the NISTOpen Machine Translation Evaluation (OpenMT)2009 (Garofolo, 2009) constrained Arabic-Englishdevelopment and evaluation data includes four En-glish translations for each Arabic source sentence,as English is the usual target language.
However,when considering this data to tune and evaluatean English-to-Arabic system, each English sentencehas a single Arabic translation and such translationsare often identical.
With at most one reference trans-lation for each source sentence, standard minimumerror rate training (Och, 2003) to the BLEU met-ric (Papineni et al, 2002) becomes problematic, asBLEU relies on the availability of multiple refer-ences.We describe a semi-automatic paraphrasingtechnique that addresses this problem by identifyingparaphrases that can be used to create new referencetranslations based on valid phrase substitutions onexisting references.
Paraphrases are automaticallyextracted from a large parallel corpus and filtered byquality judgments collected from human annotatorsusing Amazon Mechanical Turk.
As Turkers arenot trained to complete natural language processing(NLP) tasks and can dishonestly submit randomjudgments, we develop a task type that is able tocatch problem Turkers while remaining simpleenough for untrained annotators to understand.2 Data SetThe parallel corpus used for paraphrasing con-sists of all Arabic-English sentence pairs in theNIST OpenMT Evaluation 2009 (Garofolo, 2009)constrained training data.
The target corpus to beparaphrased consists of the 728 Arabic sentencesfrom the OpenMT 2002 (Garofolo, 2002) develop-ment data.2.1 Paraphrase ExtractionWe conduct word alignment and phrase extractionon the parallel data to produce a phrase table con-taining Arabic-English phrase pairs (a, e) with trans-lation probabilities P (a|e) and P (e|a).
Follow-66ing Bannard and Callison-Burch (2005), we iden-tify Arabic phrases (a1) in the target corpus that aretranslated by at least one English phrase (e).
Weidentify paraphrase candidates as alternate Arabicphrases (a2) that translate e. The probability of a2being a paraphrase of a1 given foreign phrases e isdefined:P (a2|a1) =?eP (e|a1)P (a2|e)A language model trained on the Arabic side of theparallel corpus is used to further score the possi-ble paraphrases.
As each original phrase (a1) oc-curs in some sentence (s1) in the target corpus, aparaphrased sentence (s2) can be created by replac-ing a1 with one of its paraphrases (a2).
The finalparaphrase score considers context, scaling the para-phrase probability proportionally to the change inlog-probability of the sentence:F (a2, s2|a1, s1) = P (a2|a1)logP (s1)logP (s2)These scores can be combined for each pair (a1, a2)to obtain overall paraphrase scores, however weuse the F scores directly as our task considers thesentences in which paraphrases occur.3 Turker Paraphrase AssessmentTo determine which paraphrases to use to trans-form the development set references, we elicit bi-nary judgments of quality from human annotators.While collecting this data from experts would be ex-pensive and time consuming, Amazon?s Mechani-cal Turk (MTurk) service facilitates the rapid collec-tion of large amounts of inexpensive data from usersaround the world.
As these users are not trainedto work on natural language processing tasks, anywork posted on MTurk must be designed such thatit can be understood and completed successfully byuntrained annotators.
Further, some Turkers attemptto dishonestly profit from entering random answers,creating a need for tasks to have built-in measuresfor identifying and filtering out problem Turkers.Our original evaluation task consists of elicitingtwo yes/no judgments for each paraphrase and cor-responding sentence.
Shown the original phrase(a1) and the paraphrase (a2), annotators are askedwhether or not these two phrases could have thesame meaning in some possible context.
Annotatorsare then shown the original sentence (s1) and theparaphrased sentence (s2) and asked whether thesetwo sentences have the same meaning.
This task hasthe attractive property that if s1 and s2 have the samemeaning, a1 and a2 can have the same meaning.
An-notators assigning ?yes?
to the sentence pair shouldalways assign ?yes?
to the phrase pair.To collect these judgments from MTurk, we de-sign a human intelligence task (HIT) that presentsTurkers with two instances of the above task alongwith a text area for optional feedback.
The taskdescription asks skilled Arabic speakers to evalu-ate paraphrases of Arabic text.
For each HIT, wepay Turkers $0.01 and Amazon fees of $0.005 fora total label cost of $0.015.
For our initial test,we ask Turkers to evaluate the 400 highest-scoringparaphrases, collecting 3 unique judgments for eachparaphrase in and out of context.
These HITs werecompleted at a rate of 200 per day.Examining the results, we notice that mostTurkers assign ?yes?
to the sentence pairs moreoften than to the phrase pairs, which should not bepossible.
To determine whether quality of Turkersmight be an issue, we run another test for the same400 paraphrases, this time paying Turkers $0.02 perHIT and requiring a worker approval rate of 98% towork on this task.
These HITs, completed by highquality Turkers at a rate of 100 per day, resultedin similarly impossible data.
However, we alsoreceived valuable feedback from one of the Turkers.3.1 Turker FeedbackWe received a comment from one Turker thatour evaluation task was causing confusion.
TheTurker would select ?no?
for some paraphrase inisolation due to missing information.
However, theTurker would then select ?yes?
for the paraphrasedsentence, as the context surrounding the phraserendered the missing information unnecessary.This illustrates the point that untrained annotatorsunderstand the idea of ?possible context?
differentlyfrom experts and allows us to restructure our HITsto be ideal for untrained Turkers.673.2 Revised Main TaskWe simplify our task to eliminate as many sourcesof ambiguity as possible.
Our revised task simplypresents annotators with the original sentence la-beled ?sentence 1?
and the paraphrased sentence la-beled ?sentence 2?, and asks whether or not the twosentences have the same meaning.
Each HIT, titled?Evaluate Arabic Sentences?, presents Turkers with2 such tasks, pays $0.02, and costs $0.005 in Ama-zon fees.Without additional consideration, this task re-mains highly susceptible to random answers fromdishonest or unreliable Turkers.
To ensure that suchTurkers are identified and removed, we intersperseabsolute positive and negative examples with thesentence pairs from our data set.
Absolute posi-tives consist of the same original sentence s1 re-peated twice and should always receive a ?yes?
judg-ment.
Absolute negatives consist of some origi-nal s1 and a different, randomly selected originalsentence s?1 with several words dropped to obscuremeaning.
Absolute negatives should always receivea ?no?
judgment.
Positive and negative control casescan be inserted with a frequency based either on de-sired confidence that enough cases are encounteredfor normalization or on the availability of funds.Inserting either a positive or negative controlcase every 5th task increases the per-label cost to$0.0156.
We use this task type to collect 3 uniquejudgments for each of the 1280 highest-scoringparaphrases at a total cost of $60.00 for 2400 HITs.These HITs were completed substantially faster at arate of 500-1000 per day.
The results of this task arediscussed in section 4.3.3 Editing TaskWe conduct an additional experiment to see if Turk-ers will fix paraphrases judged to be incorrect.
Thetask extends the sentence evaluation task describedin the previous section by asking Turkers who select?no?
to edit the paraphrase text in the second sen-tence such that the sentences have the same mean-ing.
While the binary judgment task is used for fil-tering only, this editing task ensures a usable datapoint for every HIT completed.
As such, fewer totalHITs are required and high quality Turkers can be0 0.25 0.5 0.75 102468101214161820Accuracy of judgments of control casesNumberof TurkersFigure 1: Turker accuracy classifying control casespaid more for each HIT.
We run 3 sequential testsfor this task, offering $0.02, $0.04, and $0.10 perparaphrase approved or edited.Examining the results, we found that regardlessof price, very few paraphrases were actually edited,even when Turkers selected ?no?
for sentenceequality.
While this allows us to easily identify andremove problem Turkers, it does not solve the issuethat honest Turkers either cannot or will not provideusable paraphrase edits for this price range.
A briefexamination by an expert indicates that the $0.02per HIT edits are actually better than the $0.10 perHIT edits.4 ResultsOur main task of 2400 HITs was completed throughthe combined effort of 47 unique Turkers.
As shownFigure 1, these Turkers have varying degrees of ac-curacy classifying the control cases.
The two mostcommon classes of Turkers include (1) those spend-ing 15 or more seconds per judgment and scoringabove 0.9 accuracy on the control cases and (2) thosespending 5-10 seconds per judgment and scoring be-tween 0.4 and 0.6 accuracy as would be expected bychance.
As such, we accept but do not consider thejudgments of Turkers scoring between 0.7 and 0.9accuracy on the control set, and reject all HITs forTurkers scoring below 0.7, republishing them to becompleted by other workers.68Decision Confirm Reject Undec.Paraphrases 726 423 131Table 1: Turker judgments of top 1280 paraphrasesFigure 2: Paraphrases confirmed by TurkersAfter removing judgments from below-thresholdannotators, all remaining judgments are used toconfirm or reject the covered paraphrases.
If aparaphrase has at least 2 remaining judgments, it isconfirmed if at least 2 annotators judge it positivelyand rejected otherwise.
Paraphrases with fewer than2 remaining judgments are considered undecidable.Table 1 shows the distribution of results for the 1280top-scoring paraphrases.
As shown in the table,726 paraphrases are confirmed as legitimate phrasesubstitutions on reference translations, providingan average of almost one paraphrase per reference.Figures 2 and 3 show example Arabic paraphrasesfiltered by Turkers.5 ConclusionsWe have presented a semi-automatic paraphrasingtechnique for creating additional reference transla-tions.
The paraphrase extraction technique providesa ranked list of paraphrases and their contexts whichcan be incrementally filtered by human judgments.Our judgment task is designed to address specificTurker feedback, remaining simple enough fornon-experts while successfully catching problemusers.
The $60.00 worth of judgments collectedproduces enough paraphrases to apply an averageFigure 3: Paraphrases rejected by Turkersof one phrase substitution to each reference.
Ourfuture work includes collecting sufficient data tosubstitute multiple paraphrases into each Arabicreference in our development set, producing a fulladditional set of reference translations for use tuningour English-to-Arabic MT system.
The resultingindividual paraphrases can also be used for othertasks in MT and NLP.AcknowledgementsThis work was supported by a $100 credit fromAmazon.com, Inc. as part of a shared task for theNAACL 2010 workshop ?Creating Speech and Lan-guage Data With Amazon?s Mechanical Turk?.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Proc.
ofACL.John Garofolo.
2002.
NIST OpenMT Eval.
2002.http://www.itl.nist.gov/iad/mig/tests/mt/2002/.John Garofolo.
2009.
NIST OpenMT Eval.
2009.http://www.itl.nist.gov/iad/mig/tests/mt/2009/.Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proc.
of ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proc.
of ACL.69Figure4:ExampleHITasseenbyTurkers70
