Coling 2010: Poster Volume, pages 1274?1282,Beijing, August 2010A Multi-Domain Web-Based Algorithm forPOS Tagging of Unknown WordsShulamit Umansky-PesinInstitute of computer scienceThe Hebrew Universitypesin@cs.huji.ac.ilRoi ReichartICNCThe Hebrew Universityroiri@cs.huji.ac.ilAri RappoportInstitute of computer scienceThe Hebrew Universityarir@cs.huji.ac.ilAbstractWe present a web-based algorithm for thetask of POS tagging of unknown words(words appearing only a small numberof times in the training data of a super-vised POS tagger).
When a sentence scontaining an unknown word u is to betagged by a trained POS tagger, our algo-rithm collects from the web contexts thatare partially similar to the context of u ins, which are then used to compute newtag assignment probabilities for u. Ouralgorithm enables fast multi-domain un-known word tagging, since, unlike pre-vious work, it does not require a corpusfrom the new domain.
We integrate ouralgorithm into the MXPOST POS tagger(Ratnaparkhi, 1996) and experiment withthree languages (English, German andChinese) in seven in-domain and domainadaptation scenarios.
Our algorithm pro-vides an error reduction of up to 15.63%(English), 18.09% (German) and 13.57%(Chinese) over the original tagger.1 IntroductionPart-of-speech (POS) tagging is a fundamentalNLP task that has attracted much research in thelast decades.
While supervised POS taggers haveachieved high accuracy (e.g., (Toutanova et al,2003) report a 97.24% accuracy in the WSJ PennTreebank), tagger performance on words appear-ing a small number of times in their trainingcorpus (unknown words) is substantially lower.This effect is especially pronounced in the do-main adaptation scenario, where the training andtest corpora are from different domains.
For ex-ample, when training the MXPOST POS tagger(Ratnaparkhi, 1996) on sections 2-21 of the WSJPenn Treebank it achieves 97.04% overall accu-racy when tested on WSJ section 24, and 88.81%overall accuracy when tested on the BNC cor-pus, which contains texts from various genres.For unknown words (test corpus words appearing8 times or less in the training corpus), accuracydrops to 89.45% and 70.25% respectively.In this paper we propose an unknown word POStagging algorithm based on web queries.
When anew sentence s containing an unknown word u isto be tagged by a trained POS tagger, our algo-rithm collects from the web contexts that are par-tially similar to the context of u in s. The collectedcontexts are used to compute new tag assignmentprobabilities for u.Our algorithm is particularly suitable for multi-domain tagging, since it requires no informationabout the domain from which the sentence to betagged is drawn.
It does not need domain specificcorpora or external dictionaries, and it requiresno preprocessing step.
The information requiredfor tagging an unknown word is very quickly col-lected from the web.This behavior is unlike previous works for thetask (e.g (Blitzer et al, 2006)), which require atime consuming preprocessing step and a corpuscollected from the target domain.
When the targetdomain is heterogeneous (as is the web itself), acorpus representing it is very hard to assemble.
Tothe best of our knowledge, ours is the first paper toprovide such an on-the-fly unknown word taggingalgorithm.To demonstrate the power of our algorithm as a1274fast multi-domain learner, we experiment in threelanguages (English, German and Chinese) andseveral domains.
We implemented the MXPOSTtagger and integrated it with our algorithm.
Weshow error reduction in unknown word taggingof up to 15.63% (English), 18.09% (German) and13.57% (Chinese) over MXPOST.
The run timeoverhead is less than 0.5 seconds per an unknownword in the English and German experiments, andless than a second per unknown word in the Chi-nese experiments.Section 2 reviews previous work on unknownword Tagging.
Section 3 describes our web-querybased algorithm.
Section 4 and Section 5 describeexperimental setup and results.2 Previous WorkMost supervised POS tagging works address theissue of unknown words.
While the general meth-ods of POS tagging vary from study to study?
Maximum Entropy (Ratnaparkhi, 1996), con-ditional random fields (Lafferty et al, 2001),perceptron (Collins, 2002), Bidirectional Depen-dency Network (Toutanova et al, 2003) ?
thetreatment of unknown words is more homoge-neous and is generally based on additional fea-tures used in the tagging of the unknown word.Brants (2000) used only suffix features.
Rat-naparkhi (1996) used orthographical data such assuffixes, prefixes, capital first letters and hyphens,combined with a local context of the word.
In thispaper we show that we improve upon this method.Toutanova and Manning (2000), Toutanova et al(2003), Lafferty et al (2001) and Vadas and Cur-ran (2005) used additional language-specific mor-phological or syntactic features.
Huihsin et al(2005) combined orthographical and morpholog-ical features with external dictionaries.
Naka-gawa and Matsumoto (2006) used global and localinformation by considering interactions betweenPOS tags of unknown words with the same lexicalform.Unknown word tagging has also been exploredin the context of domain adaptation of POS tag-gers.
In this context two directions were explored:a supervised method that requires a manually an-notated corpus from the target domain (Daume III,2007), and a semi-supervised method that uses anunlabeled corpus from the target domain (Blitzeret al, 2006).Both methods require the preparation of a cor-pus of target domain sentences and re-trainingthe learning algorithm.
Blitzer et al (2006) used100K unlabeled sentences from the WSJ (source)domain as well as 200K unlabeled sentences fromthe biological (target) domain.
Daume III (2007)used an 11K words labeled corpus from the targetdomain.There are two serious problems with these ap-proaches.
First, it is not always realistically pos-sible to prepare a corpus representing the targetdomain, for example when that domain is the web(e.g., when the POS tagger serves an applicationworking on web text).
Second, preparing a cor-pus is time consuming, especially when it needsto be manually annotated.
Our algorithm requiresno corpus from the target data domain, no prepro-cessing step, and it doesn?t even need to know theidentity of the target domain.
Consequently, theproblem we address here is more difficult (and ar-guably more useful) than that addressed in previ-ous work1.The domain adaptation techniques above havenot been applied to languages other than English,while our algorithm is shown to perform well inseven scenarios in three languages.Qiu et al (2008) explored Chinese unknownword POS tagging using internal component andcontextual features.
Their work is not directlycomparable to ours since they did not test a do-main adaptation scenario, and used substantiallydifferent corpora and evaluation measures in theirexperiments.Numerous works utilized web resources forNLP tasks.
Most of them collected corpora us-ing data mining techniques and used them off-line.
For example, Keller et al, (2002) and Kellerand Lapata (2003) described a method to obtainfrequencies for unseen adjective-noun, noun-nounand verb-object bigrams from the web by query-1We did follow their experimental procedure as much aswe could.
Like (Blitzer et al, 2006), we compare our algo-rithm to the performance of the MXPOST tagger trained onsections 2-21 of WSJ.
Like both papers, we experimentedin domain adaptation from WSJ to a biological domain.
Weused the freely available Genia corpus, while they used datafrom the Penn BioIE project (PennBioIE, 2005).1275ing a Web engine.On-line usage of web queries is less frequentand was used mainly in semantic acquisition ap-plications: the discovery of semantic verb rela-tions (Chklovski and Pantel, 2004), the acquisi-tion of entailment relations (Szpektor et al, 2004),and the discovery of concept-specific relation-ships (Davidov et al, 2007).
Chen et al (2007)used web queries to suggest spelling corrections.Our work is related to self-training (McCloskyet al, 2006a; Reichart and Rappoport, 2007) asthe algorithm used its own tagging of the sen-tences collected from the web in order to producea better final tagging.
Unlike most self-trainingworks, our algorithm is not re-trained using thecollected data but utilizes it at test time.
More-over, unlike in these works, in this work the datais collected from the web and is used only dur-ing unknown words tagging.
Interestingly, previ-ous works did not succeed in improving POS tag-ging performance using self-training (Clark et al,2003).3 The AlgorithmOur algorithm utilizes the correlation between thePOS of a word and the contexts in which the wordappears.
When tackling an unknown word, the al-gorithm searches the web to find contexts similarto the one in which the word appears in the sen-tence.
A new tag assignment is then computed forthe unknown word based on the extracted contextsas well as the original ones.We start with a description of the web-basedcontext searching algorithm.
We then describehow we combine the context information col-lected by our algorithm with the statistics of theMXPOST tagger.
While in this paper we imple-mented this tagger and used it in our experiments,the context information collected by our web-query based algorithm can be integrated into anyPOS tagger.3.1 Web-Query Based Context CollectionAn unknown word usually appears in a given sen-tence with other words on its left and on its right.We use three types of contexts.
The first includesall of these neighboring words, the second in-cludes the words on the left, and the third includesthe words on the right.For each context type we define a web query us-ing two common features supported by the majorsearch engines: wild-card search, expressed usingthe ?*?
character, and exact sentence search, ex-pressed by quoted characters.
The retrieved sen-tences contain the parts enclosed in quotes in theexact same place they appear in the query, whilean asterisk can be replaced by any single word.For a word u we execute the following threequeries for each of its test contexts:1.
Replacement: "u?2u?1 ?u+1u+2".
This re-trieves words that appear in the same contextas u.2.
Left-side: "?
?
u u+1 u+2".
This retrievesalternative left-side contexts for the word uand its original right-side context.3.
Right-side: query "u?2 u?1 u ?
?".
Thisretrieves alternative right-side contexts for uand its original left-side context.Query Type Query Matches (Counts)Replacement "irradiation and * heat (15)treatment of" chemical (7)the (6)radiation (1)pressure (1)Left-side "* * H2O2 by an (9)treatment of" indicated that (5)enhanced by (4)familiar with (3)observed after (3)Right-side "irradiation and in comparison (3)H2O2 * *" on Fe (1)treatment by (1)cause an (1)does not (1)Table 1: Top 5 matches of each query type for the word?H2O2?
in the GENIA sentence: ?UV irradiation and H2O2treatment of T lymphocytes induce protein tyrosine phospho-rylation and Ca2+ signals similar to those observed followingbiological stimulation.?.
For each query the matched words(matches) are ranked by the number of times they occur inthe query results (counts).An example is given in Table 1, presenting thetop 5 matches of every query type for the word?H2O2?, which does not appear in the EnglishWSJ corpus, in a sentence taken from the EnglishGenia corpus.
Since matching words can appear1276multiple times in the results, the algorithm main-tains for each match a counter denoting the num-ber of times it appeared in the results, and sortsthe results according to this number.Seeing the table, readers might think of the fol-lowing algorithm: take the leading match in theReplacement query, and tag the unknown word us-ing its most frequent tag (assuming it is a knownword).
We have experimented with this method,and it turned out that its results are worse thanthose given by MXPOST, which we use as a base-line.The web queries are executed by Yahoo!BOSS2, and the resulting XML containing up to a1000 results (a limit set by BOSS) is processed formatches.
A list of matches is extracted from theabstract and title nodes of the web results alongwith counts of the number of times they appear.The matches are filtered to include only knownwords (words that appear in the training data ofthe POS tagger more than a threshold) and to ex-clude the original word or context.Our algorithm uses a positive integer parameterNweb: only the Nweb top-scoring unique resultsof each query type are used for tagging.
If a left-side or right-side query returns less than Nweb re-sults, the algorithm performs a ?reduced?
query:"?
?
u u+1" for left-side and "u?1 u ?
?"
for theright side.
These queries should produce more re-sults than the original ones due to the reduced con-text.
If these reduced queries do not produce Nwebresults, the web query algorithm is not used to as-sist the tagger for the unknown word u at hand.If a replacement query does not produce at leastNweb unique results, only the left-side and right-side queries are used.For Chinese queries, search engines do theirown word segmentation so the semantics of the?*?
operator is supposedly the same as for Englishand German.
However, the answer returned bythe search engine does not provide this segmen-tation.
To obtain the words filling the ?*?
slots inour queries, we take all possible segmentations inwhich the two words appears in our training data.The queries we use in our algorithm are not theonly possible ones.
For example, a possible query2http://developer.yahoo.com/search/boss/we do not use for the word u is "?
?u?1uu+1u+2".The aforementioned set of queries gave the bestresults in our English, German and Chinese de-velopment data and is therefore the one we used.3.2 Final TaggingThe MXPOST Tagger.
We integrated our algo-rithm into the maximum entropy tagger of (Rat-naparkhi, 1996).
The tagger uses a set h of con-texts (?history?)
for each word wi (the index i isused to allow an easy notation of the previous andnext words, whose lexemes and POS tags are usedas features).
For each such word, the tagger com-putes the following conditional probability for thetag tr:p(tr|h) =p(h, tr)?t?r?T p(h, t?r)(1)where T is the tag set, and the denominator is sim-ply p(h).
The joint probability of a history h anda tag t is defined by:p(h, t) = Zk?j=1?fj(h,t)j (2)where ?1, .
.
.
, ?k are the model parameters,f1, .
.
.
, fk are the model?s binary features (indica-tor functions), and Z is a normalization term forensuring that p(h, t) is a probability.In the training phase the algorithm performsmaximum likelihood estimation for the ?
param-eters.
These parameters are then used when themodel tags a new sentence (the test phase).
Forwords that appear 5 times or less in the trainingdata, the tagger extracts special features based onthe morphological properties of the word.Combining Models.
In general, we use thesame equation as MXPOST to compute joint prob-abilities, and our training phase is identical to itstraining phase.
What we change are two things.First, we add new contexts to the ?history?
of aword when it is considered as unknown (so Equa-tion (2) is computed using different histories).Second, we use a different equation for comput-ing the conditional probability (below).When the algorithm encounters an unknownword wi in the context h during tagging, it per-forms the web queries defined in Section 3.1.
For1277each of the Nweb top resulting matches for eachquery, {h?n|n ?
[1, Nweb]}, the algorithm createsits corresponding history representation hn.
Con-verting h?n to hn is required since in MXPOST ahistory consists of an ordered set of words to-gether with their POS tags, while h?n is an orderedset of words without POS tags.
Consequently, wedefine hn to consist of the same ordered set ofwords as h?n, and we tag each word using its mostfrequent POS tag in the training corpus.
If wi?1 orwi?2 are unknown words, we do not tag them, let-ting MXPOST use its back-off technique for such acase (which is simply to compute the features thatit can and ignore those it cannot).For each possible tag t ?
T , its final assign-ment probability to wi is computed as an averagebetween its probability given the various contexts:p?
(tr|h) =porg(tr|h) +?QNwebn=1 pn(tr|hn)QNweb + 1(3)where Q is the number of query types used (1, 2or 3, see Section 3.1).During inference, we use the two search spaceconstraints applied by the original MXPOST.
First,we apply a beam search procedure that consid-ers the 10 most probable different tag sequencesof the tagged sentence at any point in the taggingprocess.
Second, known words are constrained tobe annotated only by tags with which they appearin the training corpus.4 Experimental SetupLanguages and Datasets.
We experimentedwith three languages, English, German and Chi-nese, in various combinations of training and test-ing domains (see Table 2).
For English we usedthe Penn Treebank WSJ corpus (WSJ) (Marcuset al, 1993) from the economics newspapers do-main, the GENIA corpus version 3.02p (GENIA)(Kim et al, 2003) from the biological domainand the British National Corpus version 3 (BNC)(Burnard, 2000) consisting of various genres.
ForGerman we used two different corpora from thenewspapers domain: NEGRA (Brants, 1997) andTIGER (Brants et al, 2002).
For Chinese weused the Penn Chinese Treebank corpus version5.0 (CTB) (Xue et al, 2002).All corpora except of WSJ were split usingrandom sampling.
For the NEGRA and TIGERcorpora we used the Stuttgart-Tuebingen Tagset(STTS).According to the annotation policy of the GE-NIA corpus, only the names of journals, authors,research institutes, and initials of patients are an-notated by the ?NNP?
(Proper Name) tag.
Otherproper names such as general people names, tech-nical terms (e.g.
?Epstein-Barr virus?)
genes, pro-teins, etc.
are tagged by other noun tags (?NN?
or?NNS?).
This is in contrast to the WSJ corpus, inwhich every proper name is tagged by the ?NNP?tag.
We therefore omitted cases where ?NNP?is replaced by another noun tag from the accu-racy computation of the GENIA domain adapta-tion scenario (see analysis in (Lease and Charniak,2005)).In all experimental setups except of WSJ-BNCthe training and test corpora are tagged with thesame POS tag set.
In order to evaluate the WSJ-BNC setup, we converted the BNC tagset to thePenn Treebank tagset using the comparison tableprovided in (Manning and Schuetze, 1999) (pages141?142).Baseline.
As a baseline we implemented theMXPOST tagger.
An executable code for MXPOSTwritten by its author is available on the internet,but we needed to re-implement it in order to in-tegrate our technique.
We made sure that ourimplementation does not degrade results by run-ning it on our WSJ scenario (see Table 2), whichis very close to the scenario reported in (Ratna-parkhi, 1996).
The accuracy of our implementa-tion is 97.04%, a bit better than the numbers re-ported in (Ratnaparkhi, 1996) for a WSJ scenariousing different sections.Parameter Tuning.
We ran experiments withthree values of the unknown word threshold T : 0(only words that do not appear in the training dataare considered unknown), 5 and 8.
That is, the al-gorithm performs the web context queries and uti-lizes the tag probabilities of equation 3 for wordsthat appear up to 0 ,5 or 8 times in the trainingdata.Our algorithm has one free parameter Nweb, thenumber of query results for each context type used1278Language Expe.
name Training Development TestEnglish WSJ sections 2-21 (WSJ) section 22 (WSJ) section 23 (WSJ)(2.4%,6.7%,8.4%)English WSJ-BNC sections 2-21 (WSJ) 2000 BNC sentence 2000 BNC sentences(8.4%,14.9%,17%)English WSJ-GENIA WSJ sections 2-21 2000 GENIA sentences 2000 GENIA sentences(22.7%,30.65%,32.9%)German NEGRA 15689 NEGRA sentences 1746 NEGRA sentences 2096 NEGRA sentences(11.1%,24.7%,28.7%)German NEGRA-TIGER 15689 NEGRA sentences 2000 TIGER sentences 2000 TIGER sentences(16%,27.3%,30.6%)German TIGER-NEGRA 15689 TIGER sentences 1746 NEGRA sentences 2096 NEGRA sentence(16.2%,27.9%,31.6%)Chinese CTB 14903 CTB sentences 1924 CTB sentences 1945 CTB senteces(7.4%,15.7%,18.1%)Table 2: Details of the experimental setups.
In the ?Test?
column the numbers in parentheses are thefraction of the test corpus words that are considered unknown, when the unknown word threshold is setto 0, 5 and 8 respectively.T = 0 T = 5 T = 8WSJ WSJ- WSJ- WSJ WSJ- WSJ- WSJ WSJ- WSJ-BNC GENIA BNC GENIA BNC GENIABaseline 83.56 61.22 80.05 88.79 68.71 80.12 89.45 70.25 80.8Unlimited (-) 84.85 63.51 82.50 89.86 71.12 82.51 90.47 72.77 83.16Top 5 (-) 84.25 64.24 82.75 89.73 71.21 82.78 90.36 72.74 83.46Top 10 (-) 84.42 64.10 83.17 89.70 71.36 83.00 90.29 72.87 83.70Top 10 (+) 84.67 64.47 82.60 89.83 72.12 82.54 90.29 73.53 83.22best imp.
1.19 3.25 3.12 1.07 3.41 2.88 1.02 3.28 2.97.23% 8.38% 15.63% 9.54% 10.89% 14.48% 9.66% 11.02% 15.1%T = 0 T = 5 T = 8NEGRA NEGRA- TIGER- NEGRA NEGRA- TIGER- NEGRA NEGRA- TIGER-TIGER NEGRA TIGER NEGRA TIGER NEGRABaseline 90.26 85.71 87.18 91.06 87.88 87.86 91.45 88.22 88.18Unlimited (-) 91.22 86.60 89.49 91.66 88.22 89.84 92.25 89.08 90.23Top 5 (-) 91.41 86.68 89.32 91.95 89.01 89.72 92.38 89.33 90.26Top 10 (-) 91.06 86.83 89.50 91.25 88.36 89.84 92.33 89.38 90.26Top 10 (+) 90.58 86.86 89.43 91.25 88.36 89.84 91.53 88.35 89.71best imp.
1.15 1.15 2.32 0.89 1.13 1.98 0.93 1.16 2.0811.8% 8.04% 18.09% 9.95% 9.32% 16.3% 10.87% 9.84% 17.59%CTBT = 0 T = 5 T = 8Baseline 74.99 78.03 79.81Unlimited (-) 77.01 80.46 81.94Top 5 (-) 77.58 80.75 82.19Top 10 (-) 77.43 80.68 82.45Top 10 (+) 77.43 80.68 82.35best imp.
2.59 2.72 2.7410.35% 12.28% 13.57%Table 3: Accuracy of unknown word tagging in the English (top table), German (middle table) and Chi-nese (bottom table) experiments.
Results are presented for three values of the unknown word thresholdparameter T : 0, 5 and 8.
For all setups our models improves over the MXPOST baseline of (Ratnaparkhi,1996).
The bottom line of each table (?best imp.?)
presents the improvement (top number) and errorreduction (bottom number) of the best performing model over the baseline.
The best improvement is indomain adaptation scenarios.1279in the probability computation of equation 3.
Foreach setup (Table 2) we ran several combinationsof query types and values of Nweb.
We report re-sults for the four leading combinations:?
Nweb = 5, left-side and right-side queries(Top 5 (-)).?
Nweb = 10, left-side and right-side queries(Top 10 (-)).?
Nweb = 10, replacement, left-side and right-side queries (Top 10 (+)).?
Nweb = Unlimited (in practice, this means1000, the maximum number of results pro-vided by Yahoo!
Boss), left-side and right-side queries (Unlimited (-) ).The order of the models with respect to theirperformance was identical for the developmentand test data.
That is, the best parameter/queriescombination for each scenario can be selected us-ing the development data.
We experimented withother parameter/queries combinations and addi-tional query types but got worse results.5 ResultsThe results of the experiments are shown in Ta-ble 3.
Our algorithm improves the accuracy of theMXPOST tagger for all three languages and for allvalues of the unknown word parameter.Our experimental scenarios consist of three in-domain setups in which the model is trained andtested on the same corpus (the WSJ, NEGRAand CTB experiments), and four domain adap-tation setups: WSJ-GENIA, WSJ-BNC, TIGER-NEGRA and NEGRA-TIGER.Table 3 shows that our model is relativelymore effective in the domain adaptation scenar-ios.
While in the in-domain setups the error reduc-tion values are 7.23% ?
9.66% (English), 9.95% ?11.8% (German) and 10.35% ?
13.57% (Chinese),in the domain adaptation scenarios they are 8.38%?
11.02% (WSJ-BNC), 14.48% ?
15.63% (WSJ-GENIA), 8.04% ?
9.84% (NEGRA-TIGER) and16.3% ?
18.09% (TIGER-NEGRA).Run Time.
As opposed to previous approachesto unknown word tagging (Blitzer et al, 2006;Daume III, 2007), our algorithm does not containa step in which the base tagger is re-trained with acorpus collected from the target domain.
Instead,when an unknown word is tackled at test time, aset of web queries is run.
This is an advantage forflexible multi-domain POS tagging because pre-processing times are minimized, but might causean issue of overhead per test word.To show that the run time overhead created byour algorithm is small, we measured its time per-formance (using an Intel Xeon 3.06GHz, 3GBRAM computer).
The average time it took the bestconfiguration of our algorithm to process an un-known word and the resulting total addition to therun time of the base tagger are given in Table 4.The average time added to an unknown word tag-ging is less than half a second for English, evenless for German, and less than a second for Chi-nese.
This is acceptable for interactive applica-tions that need to examine a given sentence with-out being provided with any knowledge about itsdomain.Error Analysis.
In what follows we try to ana-lyze the cases in which our algorithm is most ef-fective and the cases where further work is stillrequired.
Due to space limitations we focus onlyon the (Top 10 (+), T = 5) parameters setting,and report the patterns for one English setup.
Thecorresponding patterns of the other parameter set-tings, languages and setups are similar.We report the errors of the base tagger that ouralgorithm most usually fixes and the errors thatour algorithm fails to fix.
We describe the basetagger errors of the type ?POS tag ?a?
is replacedwith POS tag ?b?
(denoted by: a -> b)?
usingthe following data: (1) total number of unknownwords whose correct tag is ?a?
that were assigned?b?
by the base tagger; (2) the percentage of un-known words whose correct tag is ?a?
that wereassigned ?b?
by the base tagger; (3) the percentageof unknown words whose correct tag is ?a?
thatwere assigned ?b?
by our algorithm; (4) the per-centage of mistakes of type (1) that were correctedby our algorithm.In the English WSJ-BNC setup, the base taggermistakes that our algorithm handles well (accord-ing to the percentage of corrected mistakes) are:(1) NNS -> VBZ (23, 3.73%, 0.8%, 65.2%); (2)CD -> JJ (19 ,13.2% ,9.7% ,37.5%); (3) NN ->1280WSJ WSJ- WSJ- NEGRA NEGRA- TIGER- CTBBNC GENIA TIGER NEGRATotal addition 00:28:26 00:31:53 1:37:32 00:57:03 00:19:10 00:36:54 2:29:13Avg.
time per word 0.42 0.32 0.33 0.36 0.11 0.21 0.95Table 4: The processing time added by the web based algorithm to the base tagger.
For each setup results are presented forthe best performing model and for the unknown word threshold of 8.
Results for the other models and threshold parametersare very similar.
The top line presents the total time added in the tagging of the full test data (hours:minutes:seconds).
Thebottom line presents the average processing time of an unknown word by the web based algorithm (in seconds).JJ (97, 6.17%, 5.3%, 27.8%); (4) JJ -> NN (69,9.73%, 7.76%, 33.3%).
The errors that were nothandled well by our algorithm are: (1) IN -> JJ(70, 46.36% , 41%, 8.57%); (2) VBP -> NN (25,19.5%, 21.9% , 0%).In this setup, ?CD?
is a cardinal number, ?IN?
isa preposition, ?JJ?
is an adjective, ?NN?
is a noun(singular or mass), ?NNS?
is a plural noun, ?VBP?is a verb in non-third person singular present tenseand ?VBZ?
is a verb in third person, singularpresent tense.We can see that no single factor is responsiblefor the improvement over the baseline.
Rather,it is due to correcting many errors of differenttypes.
The same general behavior is exhibited inthe other setups for all languages.Multiple Unknown Words.
Our method is ca-pable of handling sentences containing several un-known words.
Query results in which ?*?
is re-placed by an unknown word are filtered.
Forqueries in which an unknown word appears as partof the query (when it is one of the two right or leftnon-?*?
words), we let MXPOST invoke its ownunknown word heuristics if needed3.In fact, the relative improvement of our algo-rithm over the baseline is better for adjacent un-known words than for single words.
For ex-ample, consider a sequence of consecutive un-known words as correctly tagged if all of itswords are assigned their correct tag.
In theWSJ-GENIA scenario (Top 10 (+), T = 5), theerror reduction for sequences of length 1 (un-known words surrounded by known words, 8767sequences) is 8.26%, while for 2-words (2620sequences) and 3-words (614 sequences) it is11.26% and 19.11% respectively.
Similarly, forTIGER-NEGRA (same parameters setting) the er-3They are needed only if the word is on the left of theword to be tagged.ror reduction is 6.85%, 8.07% and 18.18% for se-quences of length 1 (4819) ,2 (1126) and 3 (223)respectively.6 Conclusions and Future WorkWe presented a web-based algorithm for POS tag-ging of unknown words.
When an unknown wordis tackled at test time, our algorithm collects webcontexts of this word that are then used to improvethe tag probability computations of the POS tag-ger.In our experiments we used our algorithm to en-hance the unknown word tagging quality of theMXPOST tagger (Ratnaparkhi, 1996), a leadingstate-of-the-art tagger, which we implemented forthis purpose.
We showed significant improvement(error reduction of up to 18.09%) for three lan-guages (English, German and Chinese) in sevenexperimental setups.
Our algorithm is especiallyeffective in domain-adaptation scenarios wherethe training and test data are from different do-mains.Our algorithm is fast (requires less than a sec-ond for processing an unknown word) and canhandle test sentences coming from any desired un-known domain without the costs involved in col-lecting domain-specific corpora and retraining thetagger.
These properties makes it particularly ap-propriate for applications that work on the web,which is highly heterogeneous.In future work we intend to integrate our al-gorithm with additional POS taggers, experimentwith additional corpora and domains, and improveour context extraction mechanism so that our al-gorithm will be able to fix more error types.ReferencesBlitzer, John, Ryan McDonald, and Fernando Pereira,2006.
Domain adaptation with structural correspon-1281dence learning.
EMNLP ?06.Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius and George Smith, 2002.
The TIGERTreebank.
Proceedings of the Workshop on Tree-banks and Linguistic Theories.Brants, Thorsten, 1997.
The NEGRA Export Format.CLAUS Report, Saarland University.Brants, Thorsten, 2000.
Tnt: a statistical part-of-speech tagger.
In The Sixth Conference on AppliedNatural Language Processing.Burnard, Lou, 2000.
The British National CorpusUser Reference Guide.
Technical Report, OxfordUniversity.Chen, Qing, Mu Li, and Ming Zhou.
2007.
Improvingquery spelling correction using web search results.In EMNLP-CoNLL ?07.Chklovski, Timothy and Patrick Pantel.
2004.
Ver-bocean: Mining the web for fine-grained semanticverb relations.
EMNLP ?04.Clark, Stephen, James Curran and Miles Osborne.2003.
Bootstrapping POS-taggers using unlabeleddata.
CoNLL ?03.Collins, Michael.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
EMNLP ?02.Daume III, Hal.
2007.
Frustratingly easy domainadaptation.
ACL ?07.Davidov, Dmitry, Ari Rappoport, and Moshe Koppel.2007.
Fully unsupervised discovery of concept-specific relationships by web mining.
ACL ?07.Huihsin, Tseng, Daniel Jurafsky, and ChristopherManning.
2005.
Morphological features help postagging of unknown words across language vari-eties.
The Fourth SIGHAN Workshop on ChineseLanguage Processing.Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi andJun?ichi Tsujii, 2003.
GENIA corpus ?
a seman-tically annotated corpus for bio-textmining.
Bioin-formatics, 19:i180?i182, Oxford University Press,2003.Lafferty, John D., Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
The Eighteenth International Con-ference on Machine Learning.Keller, Frank, Mirella Lapata, and Olga Ourioupina.2002.
Using the Web to Overcome Data Sparseness.EMNLP ?02.Keller, Frank, Mirella Lapata.
2003. .
ComputationalLinguistics, 29(3):459?484.Lease, Matthew and Eugene Charniak.
2005.
Pars-ing Biomedical Literature.
Proceedings of the Sec-ond International Joint Conference on Natural Lan-guage Processing..Manning Chris and Hinrich Schuetze.
1999.
Foun-dations of Statistical Natural Language Processing.MIT Press..Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.McClosky, David, Eugene Charniak, and Mark John-son, 2006a.
Effective self-training for parsing.HLT-NAACL ?06.Nakagawa, Tetsuji and Yuji Matsumoto.
2006.
Guess-ing parts-of-speech of unknown words using globalinformation.
ACL-COLING ?06 .PennBioIE.
2005.
Mining the Bibliome Project..http://bioie.ldc.upenn.edu.Qiu, Likun, Changjian Hu and Kai Zhao.
2008.
Amethod for automatic POS guessing of Chinese un-known words.
COLING ?08.Ratnaparkhi, Adwait.
1996.
A maximum entropymodel for part-of-speech tagging.
EMNLP ?96.Reichart, Roi and Ari Rappoport.
2007.
Self-Trainingfor Enhancement and Domain Adaptation of Statis-tical Parsers Trained on Small Datasets.
ACL ?07.Reynolds, Sheila M. and Jeff A. Bilmes.
2005.
Part-of-speech tagging using virtual evidence and nega-tive training.
EMNLP ?06.Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisitionof entailment relations.
EMNLP ?04.Toutanova, Kristina and Christopher D. Manning.2000.
Enriching the knowledge sources used in amaximum entropy part-of-speech tagger.
EMNLP?00.Toutanova, Kristina, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.NAACL ?03.Vadas, David and James R. Curran.
2005.
Tagging un-known words with raw text features.
AustralasianLanguage Technology Workshop 2005.Nianwen Xue, Fu-Dong Chiou and Martha Palmer,2002.
Building a large?scale annotated Chinesecorpus.
ACL ?02.1282
