The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 251?256,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsKorea University System in the HOO 2012 Shared TaskJieun Lee?
and Jung-Tae Lee?
and Hae-Chang Rim??Dept.
of Computer & Radio Communications Engineering, Korea University, Seoul, Korea?Research Institute of Computer Information & Communication, Korea University, Seoul, Korea{jelee,jtlee,rim}@nlp.korea.ac.krAbstractIn this paper, we describe the Korea Univer-sity system that participated in the HOO 2012Shared Task on the correction of prepositionand determiner errors in non-native speakertexts.
We focus our work on training the sys-tem on a large collection of error-tagged textsprovided by the HOO 2012 Shared Task or-ganizers and incrementally applying severalmethods to achieve better performance.1 IntroductionIn the literature, there have been efforts aimed at de-veloping grammar correction systems designed es-pecially for non-native English speakers.
A typi-cal approach is to train statistical models on well-formed texts written by native English speakers andapply the learned models to non-native speaker textsto correct textual errors based on given context.
Thisapproach, however, fails to model the types of errorsthat non-native speakers usually make.
Recent stud-ies demonstrate that it is possible to improve the per-formance of error correction systems by training themodels on error-annotated non-native speaker texts(Han et al, 2010; Dahlmeier and Ng, 2011; Gamon,2010).
Most recently, a large collection of trainingdata consisting of preposition and determiner errorsmade by non-native English speakers has been re-leased in the HOO (Helping Our Own) 2012 SharedTask, which aims at promoting the research and de-velopment of automated tools for assisting authorsin writing (Dale et al, 2012).In this paper, we introduce our error correctionsystem that participated in the HOO 2012 SharedTask, where the goal is to correct errors in the use ofprepositions and determiners by non-native speakersof English.
We mainly focus our efforts on trainingthe system using the non-native speaker texts pro-vided in the HOO 2012 Shared Task.
We also shareour experience in handling some issues that emergedwhile exclusively using the non-native speaker textsfor training our system.
In the following sections,we will describe the system in detail.2 System ArchitectureThe goal of our system is to detect and correct prepo-sition and determiner errors in a given text.
Our sys-tem consists of two types of classifiers, namely editand insertion classifiers.
Inputs for the two typesof classifiers are noun phrases (NP), verb phrases(VP), and prepositional phrases (PP); we initiallypre-process the text given for training/testing by us-ing the Illinois Chunker1 and the Stanford Part-of-Speech Tagger (Toutanova et al, 2003).
For learn-ing the classifiers, we use maximum entropy models,which have been successfully applied to many tasksin natural language processing.
We particularly useLe Zhang?s Maximum Entropy Modeling Toolkit2for implementation.2.1 Edit ClassifiersThe role of an edit classifier is to check the sourcepreposition/determiner word originally chosen bythe author in a given text.
If the source wordis incorrect, the classifier replaces it with a bet-ter choice.
For every preposition/determiner word,1Available at http://cogcomp.cs.illinois.edu2Available at http://homepages.inf.ed.ac.uk/lzhang10/251we train a classifier using examples that are ob-served in training data.
The choice for preposi-tions is limited to eleven prepositions (about, at,as, by, for, from, in, of, on , to, with) that mostfrequently occur in the training data, and the can-didates for determiner choice are the and a/an.
Insummary, we train a total of thirteen edit classifiers,one for each source preposition or determiner.
Foreach edit classifier, the set of candidate outputs con-sists of the source preposition/determiner word it-self, other confusable preposition/determiner words,and no preposition/determiner in case the sourceword should be deleted.
Note that the number ofconfusable words for each source preposition is de-cided flexibly, depending on examples observed inthe training data; a similar approach has been pro-posed earlier by Rozovskaya and Roth (2010a).
Fora particular source preposition/determiner word inthe test data, the system decides whether to correctit or not based on the output of the classifier for thatsource word.2.2 Insertion ClassifierAlthough the edit classifiers described above arecapable of deciding whether a source preposi-tion/determiner word that appears in the test datashould be replaced or removed, a large proportionof common mistakes for non-native English writersconsists of missing prepositions/determiners (i.e.,leaving them out by mistake).
To deal with thosetypes of errors, we train a special classifier for inser-tions.
A training or testing event for this particularclassifier is any whitespace before or after a wordin a noun or verb phrase that is a potential loca-tion for a preposition or determiner.
Table 1 showsthe five simple heuristic patterns based on part-of-speech tags that the system uses in order to locatepotential sites for prepositions/determiners.
Notethat s is a whitespace to be examined, an asterisk (*)means wildcard, and NN includes the tags that startwith NN, such as NNS, NNP, and NNPS.
VB is alsotreated in the same manner as NN.
The set of can-didate outputs consists of the eleven prepositions,the two determiners, and no preposition/determinerclass.
Once a candidate position for insertion is de-tected in the test data, the system decides whether tomake an insertion or not based on the output of theinsertion classifier.Pattern Examples+NN I?ll give you all informations+*+NN I need few dayss+VB It may seem relaxing at beginnings+*+VB Buy new colored clothesVB+s I?m looking forward your replyTable 1: Patterns of candidates for insertion2.3 FeaturesBoth edit and insertion classifiers can be trained us-ing three types of features described below.?
LEX/POS/HEAD This feature set refers to thecontextual features from a window of n tokensto the right and left that are practically used inerror correction studies (Rozovskaya and Roth,2010b; Han et al, 2010; Gamon, 2010).
Suchfeatures include lexical features, part-of-speechtags, and head words of the preceding and thefollowing chunks of the source word.
In thiswork, we set n to be 3.?
HAN This represents the set of features specifi-cally used in the work of Han et al (2010); theydemonstrate that a model trained on non-nativespeaker texts can outperform one trained solelyon well-formed texts.?
L13 L1 refers to the first language of the au-thor.
There have been some efforts to leverageL1 information for improving error correctionperformance.
For example, Rozovskaya andRoth (2011) propose an algorithm for adaptinga learned model to the L1 of the author.
Therehave been many studies leveraging writers?
L1.In this work, we propose to directly utilize L1information of the authors as features.
We alsoleverage additional features by combining L1and individual head words that govern or aregoverned by VP or NP.3 Additional Methods for ImprovementThe training data provided in the HOO 2012 SharedTask consists of exam scripts drawn from the pub-licly available FCE dataset (Yannakoudakis et al,3L1 information was provided in the training data but not inthe test data.
Therefore, the benefits of using L1 remain incon-clusive in this paper.252a/an the NULL6028 114 203Table 2: Training data distribution for a/an classifierabout as at by for from0 3 2510 1 2 3in of on to with NULL75 7 20 30 3 41Table 3: Training data distribution for at classifier2011) with textual errors annotated in HOO dataformat.
From this data, we extract examples fortraining our classifiers.
For example, let w be asource word that we specifically want our classifierto learn.
Every use of w that appears in the train-ing data may be an example that the classifier canlearn from.
However, it is revealed that for all w,there are always many more examples where w isused correctly than examples where w is replaced orremoved.
Table 2 and Table 3 respectively show theclass distributions of all examples for source wordsa/an and at that are observable from the whole train-ing data for training a/an- and at-specific classifiers.We can see that various classes among the trainingdata are unevenly represented.
When training data ishighly skewed as shown in the two tables, construct-ing a useful classifier becomes a challenging task.We observed from our preliminary experiments thatclassifiers learned on highly unbalanced data hardlytend to correct the incorrect choices made by non-native speakers.
Therefore, we investigate two sim-ple ways to alleviate this problem.3.1 Filtering Examples Less Likely to beIncorrectAs mentioned above, there are many more exam-ples where the source preposition/determiner is usedwithout any error.
One straightforward way to ad-just the training data distribution is to reduce thenumber of examples where the source word is lesslikely to be replaced or removed by using languagemodel probabilities.
If a language model learned ona very large collection of well-formed texts returnsa very high language model probability for a sourceword surrounded by its context, it may be reason-Class Initial After AfterDistribution Filtering Addingabout 0 0 528as 3 3 275at 2510 2367 2367by 1 1 207for 2 2 1159from 3 3 550in 75 75 1521of 7 7 1454on 20 20 541to 30 30 2309with 3 3 727NULL 41 41 41Table 4: Refined data distribution for at classifierable to assume that the source word is used correctly.Therefore we build a language model trained on theEnglish Gigaword corpus by utilizing trigrams.
Be-fore providing examples to the classifiers for train-ing or testing, we filter out those that have very highlanguage model probabilities above a pre-definedthreshold value.3.2 Adding Artificial ErrorsOur second approach is to introduce more artificialexamples to the training data, so that the class dis-tribution of all training examples becomes more bal-anced.
For example, if we aim at adding more train-ing examples for a/an classifier, we would extractcorrect phrases such as ?the different actor?
from thetraining data and artificially convert it into ?a differ-ent actor?
so that an example of a/an being correctedto the is also provided to a/an classifier for training.When adding aritificial examples into the trainingdata, we avoid the number of examples belongingto each class exceeding the number of cases wherethe source word is not replaced or removed.
Table4 demonstrates the results of both the filtering andadding approaches for training the a/an classifier.4 Experiments4.1 RunsThis section describes individual runs that we sub-mitted to the HOO 2012 Shared Task organizers.
Ta-ble 5 represents the setting of each run.253Runs Models Features Filtering AddingThresholdRun0 LM n/a n/aRun1 ME LEX/POS/HEAD X XRun2 ME HAN X XRun3 ME LEX/POS/HEAD -2 XRun4 ME LEX/POS/HEAD -2 ORun5 ME LEX/POS/HEAD, L1 -2 ORun6 ME LEX/POS/HEAD, L1, age -2 ORun7 ME Insertion: POS/HEAD X XOther: LEX/POS/HEADRun8 ME LEX/POS/HEAD -3 XTable 5: The explanation of each runs?
Run0 This is a baseline run that represents thelanguage model approach proposed by Gamon(2010).
We train our language model on Giga-word corpus, utilizing trigrams with interpola-tion and Kneser-Ney discount smoothing.?
Run1, 2 Run1 and 2 represent our system us-ing the LEX/POS/HEAD feature sets and HANfeature sets respectively.
Neither additionalmethod described in Section 3 is applied.?
Run3, 8 These runs represent our system us-ing LEX/POS/HEAD features (Run1), whereexamples that are less likely to be incorrect arefiltered out by consulting our language model.The threshold value is set to ?2 and ?3 forRuns 3 and 8 respectively.?
Run4 This particular run is one where we intro-duce additional errors in order to make the classdistribution of the training data for the classi-fiers more balanced.
This step is incrementallyapplied in the setting of Run3.?
Run5, 6 Run5 and 6 are when we consider L1information and age respectively as additionalfeatures for training the classifiers.
The basicsetup is same as Run4.?
Run7 This run represents our system withits insertion classifier trained using POS andHEAD features only.
No LEX features areused.Runs Precision Recall F-scoreRun0 1.45 15.45 2.65Run1 1.35 10.82 2.39Run2 1.23 11.48 2.22Run3 1.33 10.6 2.36Run4 1.19 11.26 2.15Run5 1.02 10.38 1.87Run6 0.99 9.93 1.79Run7 1.16 11.26 2.1Run8 1.46 11.04 2.58Table 6: Correction before test data revisions5 ResultsTable 6 shows the correction scores of the individualruns that we originally submitted.
Unfortunately, weshould confess that we made a vital mistake whilegenerating the runs from 1-8; the modules imple-mented for learning the insertion classifier had somebugs that we could not notice during the submissiontime.
Because of this, our system was unable to han-dle MD and MT type errors properly.
This is thereason why the performance figures of our runs arevery low.
For reference, we include Tables 7-10 thatillustrate the performance of our individual runs thatwe calculated by ourselves using the test data andthe evaluation tool provided by the organizers.We can observe that Run3 outperforms Run1 andRun4 performs better than Run3, which demon-strates that our attempts to improve the system per-formance by adjusting training data for classifiers254Runs Precision Recall F-scoreRun1 42.67 7.06 12.12Run2 49.28 7.51 13.03Run3 47.62 6.62 11.63Run4 45.45 7.73 13.21Run5 33.82 10.15 15.62Run6 8.68 18.54 11.82Run7 33.33 10.82 16.33Run8 50.0 7.28 12.72Table 7: Recognition before test data revisions (systemrevised)Runs Precision Recall F-scoreRun1 32.0 5.3 9.09Run2 42.03 6.4 11.11Run3 34.92 4.86 8.53Run4 37.66 6.4 10.94Run5 26.47 7.94 12.22Run6 5.68 12.14 7.74Run7 24.49 7.95 12.0Run8 42.42 7.28 10.79Table 8: Correction before test data revisions (system re-vised)help.
Moreover, we can also see that L1 informa-tion helps when directly used for training features.6 ConclusionThis was our first attempt to participate in a sharedtask that involves the automatic correction of gram-matical errors made by non-native speakers of En-glish.
In this work, we tried to focus on investigatingsimple ways to improve the error correction systemlearned on non-native speaker texts.
While we hadmade some critical mistakes on the submitted runs,we were able to observe that our method can poten-tially improve error correction systems.AcknowledgmentsWe would like to thank Hyoung-Gyu Lee for histechnical assistance.ReferencesDaniel Dahlmeier and Hwee Tou Ng.
2011.
Gram-matical error correction with alternating structure op-Runs Precision Recall F-scoreRun1 49.33 7.82 13.50Run2 52.17 7.61 13.28Run3 52.38 6.98 12.31Run4 51.95 8.46 14.55Run5 37.5 10.78 16.75Run6 9.29 19.03 12.5Run7 36.73 11.42 17.42Run8 51.52 7.18 12.62Table 9: Recognition after test data revisions (system re-vised)Runs Precision Recall F-scoreRun1 34.67 5.5 9.49Run2 42.02 6.13 10.7Run3 36.51 4.86 8.58Run4 38.96 6.34 10.91Run5 29.42 8.45 13.13Run6 6.40 13.11 8.61Run7 25.85 8.03 12.26Run8 42.42 5.92 10.39Table 10: Correction after test data revisions (system re-vised)timization.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies - Volume 1, ACL-HLT?11, pages 915?923, Portland, Oregon.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
Hoo 2012: A report on the preposition anddeterminer error correction shared task.
In Proceed-ings of the 7th Workshop on Innovative Use of NLP forBuilding Educational Applications, HOO ?12, Mon-treal, Canada.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners?
writing: a meta-classifierapproach.
In Human Language Technologies: Pro-ceedings of the 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, NAACL-HLT ?10, pages 163?171,Los Angeles, California.Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-Young Ha.
2010.
Using an error-annotated learnercorpus to develop an esl/efl error correction system.In Proceedings of the 7th International Conferenceon Language Resources and Evaluation, LREC ?10,pages 763?770, Malta.Alla Rozovskaya and Dan Roth.
2010a.
Generatingconfusion sets for context-sensitive error correction.255In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 961?970, Cambridge, Massachusetts.Alla Rozovskaya and Dan Roth.
2010b.
Trainingparadigms for correcting errors in grammar and usage.In Human Language Technologies: Proceedings of the2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL-HLT ?10, pages 154?162, Los Angeles, Cali-fornia.Alla Rozovskaya and Dan Roth.
2011.
Algorithm se-lection and model adaptation for esl correction tasks.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, ACL-HLT ?11, pages924?933, Portland, Oregon.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 173?180, Edmonton, Canada.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, ACL-HLT ?11, pages 180?189, Portland, Oregon.256
