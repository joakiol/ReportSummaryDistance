Evolving Questions in Text PlanningMick O?DonnellEscuela Politecnica Superior,Universidad Aut?noma de Madrid,Cantoblanco, Spainmichael.odonnell@uam.esAbstractThis paper explores how the main questionaddressed in Text Planning has evolved overthe last twenty years.
Earlier approaches totext planning asked the question: How do wewrite a good text?, and whatever answerswere found were programmed directly intocode.
With the introduction of search-basedtext planning in recent years, the focus shiftedto the evaluation function, and thus the ques-tion became: How do we tell if a text is good?This paper will explore these evolving ques-tions, and subsequent refinements of them asthe field matures.IntroductionGiven the growing interest in the application ofsearch-based planning methods in NLG, we candescribe how text planning has evolved from itsbeginning in terms of the kinds of questions thathave been addressed.1 How do we compose a good text?The object in text planning is to write a programwhich can compose a text to meet some inputspecification.
Traditionally, the programs them-selves embody decision making procedures codedto make textual decisions, such as whether or notto include some content, how to rhetorically link itto the rest of the text, and in what order should thecontent nodes appear.How one writes a good text has many answers,and depends partly on the type of text one is writ-ing.
For some texts, a schematic approach is possi-ble: a text can be composed by using predefinedschemas, which nominate what information shouldappear, and in what order (McKeown 1985).Sometimes the system makes the inclusion of aschema element conditional on the context of thetext, for instance, deciding to include or excludecontent based on the type of user that the messageis being generated for (DiMarco et al 1997).For other types of text, the application of sche-mas is less appropriate.
Some text planners take astheir goal the delivery of a single fact, or a set offacts, but take into account that other informationmay need to be given first for each message to beunderstood, other information may need to begiven after to counter misconceptions, and exam-ples may be given to aide assimilation (for in-stance, the RST text planners, such as Hovy 1988).The system thus composes a text which deliversthe facts required of it, and any other facts whichwill facilitate the uptake of these facts.In another text genre, the goal is not to deliverspecific facts, but rather to describe an entity, forinstance, museum artefacts (e.g., ILEX: O?Donnellet al 2001), or animals (Peba: Milosavljevic1999).
A basic strategy to generate a reasonabletext of this kind is to list the different attributes ofthe target entity, possibly sorted into relevant top-ics.
Good systems are aware of which informationis already known to the intended audience, whatshould be interesting, etc.
In ILEX, the system fol-lowed various strategies to generate better texts,such as allowing digressions (describing secondaryentities), generalisations, defeating misconcep-tions, aggregation, etc.Systems have increasingly tried to address lar-ger sets of the issues needed to produce good qual-ity text.
We not only need to ensure thatappropriate content is selected, and that adequategrounding is given to ensure the audience can un-121derstand the content.
Increasingly issues of coher-ence dominate.
Entities should be referred to in acontextually appropriate form, related sentencesshould be aggregated to avoid repetition, the se-quence of sentence themes should be structured toexpress a logical development of messages, etc.As systems address more and more of these is-sues, the planning algorithms become more com-plex.
Systems which need to produce texts meetingcontent and coherence constraints are reaching thelimit of what they can do, because the program-ming needs to address too many issues.2 What texts can be produced?The text-planning problem gets more complexwhen text planners need to address global as wellas local goals.
In Ilex, one constraint on the plan-ner was that the generated text needed to fit in aspecified space on a web page.
During the genera-tion process, the decision of whether or not to in-clude a given fact in the page was difficult tomake, since while the inclusion of the fact mighthelp achieve the communicative goals, it also in-creases the size of the text.
However, the systemcannot know until the whole text is producedwhether including this fact would push the text sizeover the limit.
One can only calculate the real costof including extra content when the text is finished.In the Ilex case, we selected an incremental textplanning approach, adding facts into the text struc-ture in order of their relevance, until the space limitis reached.
But this approach is not compatiblewith all generation goals.In the 1990s, several approaches started to re-characterise the text planning process as one ofsearching the space of possible text structures forthose structures which were optimal when seen asa completed whole.
These approaches split thequestion: how do we produce a good text?
into twoparts:?
How can we search the space of texts whichcould be produced for given content??
How can we select the text in this space whichoptimally meets our goals?In this section, we will discuss the first of thesequestions.
Marcu (1997) characterised the textplanning process as follows.
He assumed that theknowledge base (KB) consists of a set of semanticunits to express, and that an additional resourceprovided a set of rhetorical relations, which canhold between two semantic units in the KB.
A pairof semantic units may have no relation betweenthem, one relation, or multiple.The set of units to express is finite, and the rela-tions that can hold between them is finite, so if weassume that each semantic unit can only appearonce in the text tree, then it is clear that the set oftext structures that can realise the KB is also finite.We can model the space of possible text structuresas a lattice, where each point in the lattice repre-sents a given set of included facts, with a given setof relationships between them, and a given se-quencing of these facts in relation to each other.Seen in this light, one might naively proposethe following approach to text planning: generateall possible text plans for the given input, evaluateeach tree as a whole, and choose the one with thehighest value.
However, even for a small numberof facts, the number of trees is huge, and this ap-proach is not feasible.A more feasible approach is to treat this as asearch problem.
Each text plan represents onepoint in the space of all possible plans.
Adjacentpoints in this lattice represent similar text plansdiffering in one detail (the insertion of a fact, thedeletion of a subtree, the reordering of nucleus andsatellite, etc.).
Given a starting text plan, we canmove from point to point in the space, searchingfor a text plan which is optimal for our goals.The earliest of work to take this approach wasthat of Zukerman and McConachy (1994).
Theywere concerned with generating texts which con-veyed content adequately but concisely.
A textadequately conveys the facts it is given to conveyif these facts are understood by the addressee.Where a given fact requires other facts to be givenfor it to be understood, these facts need also to beincluded.
If a given fact or set of facts may causethe reader to make an unintended inference, factsare included to prevent this.
Additionally, the in-clusion of a fact sometimes made the inclusion ofother facts unnecessary (e.g., including an examplemight clarify a concept better than a longer expla-nation).
The conciseness goal required that as fewfacts as possible are used to convey the informa-tion that needs to be conveyed.Given these constraints, traditional planningmethods which made decisions locally were notappropriate.
The conciseness and communicativeadequacy of the text is a property of a set of factsas a whole, and thus text plans need to be evalu-122ated as a whole.
They thus rephrased the text plan-ning issue as optimisation at a global text level.They only addressed part of text planning as anoptimisation problem: content selection.
They takeas a starting point the set of facts that are categori-cally required to be expressed.
They then use anincremental graph search algorithm to search foroptimal sets of facts to include in a text: on eachiteration, facts are added to or deleted from eachsolution to produce new solutions, until the pro-gram finds one solution which adequately conveysthe designated facts, with the fewest facts possible.Marcu (1997) experimented with several differentsearch methods.
Firstly, he assumed that all theunits in the KB need to be included in the text plan(the problem of content selection was thus notrelevant).
His goal was thus to find RST treeswhich optimally covered all facts.In his first algorithm, he used a chart mecha-nism to construct all possible text plans for thegiven content and the set of possible relations.However, as he notes, for any non-trivial numberof facts, with multiple possible relations betweenthem, this approach is computationally expensive.
Ialso note that Marcu assumed all facts in the KBwere to be expressed.
If content selection is in-cluded in the process, meaning all facts are op-tional, the solution is even less tractable.His second proposal restricts the chart mecha-nism to a greedy algorithm, which at each level,selects only the best tree to take to the next level.However, this basically amounts to make decisionslocally, not globally, and thus is not relevant to thissection.His third algorithm is more relevant here.
Hesplits the problem into two parts.
Firstly, determinean ordering of content units such that as many aspossible of the ordering and adjacency constraintsin the potential relations are satisfied.
Secondly,derive an RST tree for this ordering of contentunits.He uses a constraint satisfaction algorithm forthe first step.
A small corpus study was used toderive, for each relation, the relative probability ofnucleus-satellite vs. satellite-nucleus ordering, andalso the degree to which the connected text seg-ments are required to be linearly adjacent.Given a set of facts, for each possible relationbetween a pair of facts, the process asserts a con-straint stating what the nucleus-satellite ordershould be, and another constraint asserting that theconnected facts should be adjacent.
The strength ofthe constraint depends on the values obtained inthe corpus study, for instance, if a relation allowsrelatively free ordering of a satellite, the constraintstrength would be low.The constraint satisfaction algorithm then de-rives the ordering of facts which maximises theconstraints.
Marcu elsewhere provides an algo-rithm to build a text tree on top of a given orderingof facts, again using adjacency and order propensi-ties.Mellish et al (1998) made the point that eventhis restricted approach would soon become intrac-table with more than a small set of facts when oneallows weak RST relations such as Joint andElaboration into the model.An alternative approach, which I haven?t seenimplemented (although Mellish mentions the pos-sibility), would be to apply hill climbing tech-niques to the problem of finding a text whichoptimally satisfies a set of locally and globallystated text constraints.
The idea would be to startwith a single text structure, generated in a simplemanner.
The system then tries each possible singlemutation on this text structure, (e.g., adding a factin each location in the tree this is possible; deletingeach subtree in the tree; grafting a subtree fromone location to another, etc.).
Each of the resultingtext structures is then evaluated, and the one whichscores higher is taken as the text structure for thenext cycle.
When no one-step change to the treeresults in an improvement, the process stops.The problem with all hill-climbing applicationsis that we might reach a local maximum, a pointwhere no simple change can produce an improve-ment, but radical restructuring could produce betterstructures.
One partial solution is to repeat theprocess a number of times from different startingstructures, and select the best structure produced inall the trials.
But the problem remains.Genetic algorithms offer an alternative to hill-climbing methods, and are less susceptible to stop-ping at local maxima.
In the late 1990s, Chris Mel-lish implemented the first stochastic text planner(Mellish et al 1998).
He replaced the proceduraltext planner of the ILEX system, with one based ona genetic approach.Like the above approaches, a genetic algorithmcan be seen as a means to search the space of pos-sible texts.
The system starts with a small popula-tion of randomly produced text structures, and, on123each iteration (each generation), randomly mutatessome or all of the structures.
Each new text struc-ture is then evaluated by an ?evaluation function?.In each generation, those text structures withthe highest ?fitness?
are more likely to produce off-spring, while those under a certain threshold aredropped from the population.
In this way, muta-tions which improve the text structure should bepreserved in the population, while those whichweaken the text should disappear.
After a certainnumber of iterations, the process is stopped, andthe highest scoring text-structure is selected as theone to give the user.The advantage of a genetic algorithm over ahill-climbing approach is that mutations whichmay by themselves lower the text value can sur-vive in the population, and later combine withother mutations to produce a high-value text struc-ture.
The evolutionary approach has its cost, how-ever, in that far more processing time is required.For this reason, the text mutation stage is usuallykept fairly simple.Like Marcu, Mellish also assumes that thesearch algorithm is given a set of facts, all ofwhich need to be included.
He proposes 3 algo-rithms.
The first mutates a selected tree by ran-domly swapping two subtrees, and then ?repairing?any relation which is no longer valid (where therelation which links a nucleus and satellite nolonger holds between these facts, select anotherrelation which does, or if none exist, use the ge-neric relation, Joint).Mellish noted that, often, different trees in thepopulation develop good structure for differentsubsets of facts.
He thus introduced a cross-overmechanism, allowing the grafting of structure fromone tree to another.
He simplifies the processsomewhat in a manner similar to Marcu, assumingthat the genetic algorithm only manipulates thesurface ordering of facts, and that an RST tree canbe derived from that ordering (he assumes satel-lites always follow the nucleus).
He differs fromMarcu in that evaluation is of the final text tree,rather than the fact sequence.
Mellish thus allowsfor two forms of mutation: moving a fact from oneplace to another within a single fact sequence, andinserting a sequence of facts from one fact-sequence into a random place in another (then de-leting any facts which are repeated).I have two problems with the approach above.Firstly, I do not believe that a given sequencing offacts so deterministically relates to a particular treestructuring of those facts.
Two trees with the samesurface ordering of facts could be fundamentallydifferent in structure and in terms of the relationsused, and thus will be evaluated very differently.By not including the text structure in the searchprocess, the process cannot search for an optimaltext structure.My second problem with this approach is thatthe mutations used are fairly destructive of the textplans.
When as a human writer I cut and paste textfrom one document to another, I generally find theamount of repair needed out-weighs the time savedby reusing text.
And even the simpler mutation,randomly moving a text node within a sequence,will have far more chance of fracturing coherencethan of creating it.For these two reasons, I believe it is better touse a genetic text planner which firstly operatesdirectly on text trees, and secondly, only allowsmutations which preserve the legality of the tree.
Ialso assume the genetic algorithm is used to deter-mine selected content as well as structuring andordering it.
In this hypothetical text planner, nocross-over is used, only operations on a target tree.We start with a population of trees of one fact each(all related to the entity being described).
The mu-tations include:?
Insert Fact: select a fact not in the tree andattach it as satellite to a fact which permitssuch a relation.?
Delete Subtree: randomly select a subtree anddelete it.?
Move Subtree: randomly select a subtree andmove it to another location where it can legallyattach.?
Change Top Nucleus: break off a subtree fromthe main tree, make its nucleus the top of thetree, and graft the original tree into this tree ata legal point.?
Switch Order: change the order of a satellite inrelation to the nucleus, or in regards to othersatellites of the same nucleus.?
Switch Focus: the assessment of text trees in-cludes the evaluation of the focus movementthroughout the tree, each fact in the tree nomi-nates a focus.
This operation changes that fo-cus for another of the entities in the fact, whichwill affect which entity is realised as subject inthe final text.124All of these operations preserve the legality ofthe tree, but may change the global evaluation ofthe tree, given that suboptimal focus movementsmay result, information prerequisite for under-standing may occur late, or not at all, etc.By only allowing legal mutations, the systemneeds to do less work repairing the illegalities in-troduced by mutations.
Good solutions should bereached quicker, although as said above, this ap-proach is still in an early stage of development,To a degree, this approach mirrors the way hu-mans write texts.
We start off with a rough draftwith the core of what we want to say, and reviseparts, sometimes adding in an explanation, some-times adding a digression to provide backgroundmaterial.
We may erase material because a changein another part of the document made the materialredundant.
Or we might cut/paste material fromone part of the document to another.Another approach to the problem of satisfyingglobal constraints in text planning has been the?generate and revise?
approach, where a text isgenerated with only partial regard to the globalconstraints, and the resulting text is then revised tofit the global constraints (e.g., Robin and McKe-own, 1996).
In the STOP system (Reiter 2000),texts are constrained to fit a certain length.
Thesystem generates texts of approximately the rightlength, and then prunes the text until the size re-quirement is met.
Piwek and van Deemter (2007)expand this approach to handle more than a singleglobal constraint, exemplifying using both globallength and communicative effectiveness as (some-times) contradictory goals.
Their approach is togenerate a single starting text, and then apply revi-sion operations to work towards an optimal text.If one sees a revision operation as similar to thetext mutations applied above, then it seems the re-vision approach is not so different to the geneticalgorithm approach above.
One difference is thatthe revision approach uses procedural means togenerate a reasonable starting text, while the sto-chastic approaches generally start with easy-to-generate texts and mutate these towards highercomplexity and optimality.
Another important dif-ference, at least in the work of Piwek and vanDeemter, is that they apply their revisions exhaus-tively, looking at all possible combinations of revi-sions to locate the optimal one.
For more complextext planning tasks, this may become less tractable.3 How do we tell if a text is good?One of the consequences of using genetic algo-rithms for text planning is that much of one?s effortis spent on developing the evaluation function, de-fining the formulas used to decide how good a par-ticular text is.
The main focus in building a textplanner thus moves from deciding how do we com-pose the text?
to one of deciding: how do we tell ifa text is good?The important point here is that with proceduralplanners, we need to take a cognitive approach,trying to perceive how a text is constructed fromscratch, which decisions are made, and in whatorder.
Using an evolutionary approach, we can ig-nore the process of writing, and focus on the prod-ucts of writing.
We can then function as linguists:our concern is to decide what makes a text functionwell, and what interferes with its success.
Theseare more important, more abstract questions to ad-dress.Of the systems discussed above, the evaluationfunctions vary.
The system of Zukerman andMcConachy (1993) introduced the idea of globalcriteria for evaluating text, such as conciseness(meeting the informational goals of the system inthe fewest facts), and ILEX used text length as aglobal criteria.
Marcu (1997) set the goal of maxi-mising ?global coherence?
of the text, which basi-cally amounts to maximising the sum of localdecisions made throughout the tree.
Marcu used acorpus study to determine how flexible the adja-cency and ordering constraints of each relationwere, and penalised instances of the relation whichdid not meet the constraint, the size of the penaltydepending on the observation in the corpus.He does however mention that his approachcould be set up to ?ensure that the resulting planssatisfy multiple high-level communicative goals?
(p629).The evaluation function of Mellish et al (1998)also was calculated over a sum of local features ofthe tree, although a wider set of features were in-volved.
These included: rewarding ?interesting?relations and penalising the Joint relation; penalis-ing separation between a nucleus and satellite; pe-nalising unsatisfied precondition of a relation;rewarding good focus movements and penalisingbad ones.While it is clear each of these criteria is con-tributory to good text, the numbers used were125made up.
Basing these numbers on a corpus studyof good texts may be useful for future work.Following on from Mellish et al, some worktook place focusing on improving the evaluationfunction, for instance Cheng and Mellish (2000)expanded on the criteria for evaluating focusmovement (there called ?discourse topic move-ment?
), and also allowed for embedding of facts inothers, providing criteria for evaluating how goodan embedding is.
Karamanis also examined focusmovement (or as he calls it, entity coherence) intext evaluation (Karamanis and Manurung 2002;Karamanis 2003; Karamanis et al 2004; Kara-manis and Mellish 2005).
He proposes droppingthe use of rhetorical structure, and taking the goalof text planning as sequencing facts so as toachieve smooth focus movement.
Mutations thuschange the ordering of facts in a given sequence(as in the Mellish work), but evaluation is applieddirectly to the fact sequence, penalising discon-tinuous focus movements.In summary of this section, the movement tosearch-based text planning allowed the researcherto move away from issues regarding how to pro-gram a system to generate texts, focusing insteadon the problems of deciding how to evaluate thequality of a text.4 How good is this text?A major problem with the search-based systemsdescribed above is that the system-maker needs toformulate the evaluation function: to specify thecriteria used to determine how good a text is.
Thisis a difficult problem, with no simple answers.In some cases, corpus studies have been per-formed to find the patterns used in good texts, andbase the evaluation metric on these results.
How-ever, this approach requires significant amounts oftime spent on corpus analysis.One way around this problem is the use of ma-chine learning (ML).
The basic idea in ML is toprovide the computer with lots of example texts,along with the classification of the text.
The sys-tem then builds a classification model from thetraining data.
This classification model can then beapplied to previously unseen examples to assign aclass.Applying ML to text planning, we provide thecomputer with a corpus of finished texts, eachrated from 0 to 10 in terms of quality, and leave thecomputer to work out an evaluation function on itsown.
This step would move the work of the inves-tigator from deciding how to evaluate a text to a farsimpler one, of just saying if each text is good ornot, something we all can do with or without lin-guistic training.The problem here is that ML techniques canonly function on the range of features that theyhave access to.
In the worst case, only the surfacetext is provided to the system.
Some approacheswork simply with the n-gram sequences of words,for instance, to select between alternative sen-tences generated to express some content (e.g.,Langkilde and Knight 1998).
N-grams have evenbeen used to assess the global quality of texts, notfor text planning, but to assess student exam ques-tions (P?rez et al 2004).
However, the sequence ofwords within a sentence cannot tell us anythingabout the quality of the text structure.In recent years, there has been substantial workthat assumes that the quality of a text can bejudged in relation to the degree to which the se-quence of sentences in the generated text corre-sponds to a ?golden standard?
(Lapata 2003;Dimitromanolaki and Androutsopoulos 2003;Karamanis et al 2004).
This work, referred to as?Input Ordering?, assumes that content selection isdone as a separate task, and the role of text plan-ning can be seen as simply ordering the facts out-put from content planning.
A corpus of human-written texts, (in most approaches tagged by pro-positional content), is provided.
These represent a?golden standard?.
Text plans generated by thecomputer are evaluated in terms of the degree towhich the terminals of the text structures occur inthe same order as the golden standard.For me, this approach is problematic: two textsmay have the same surface ordering of facts, yetvery different rhetorical structure.
One text may bewell structured, and the other badly structured.
Thesurface ordering of facts is by itself a poor indica-tor of the overall quality of the text.This recent focus on input ordering resultspartly from the fact that it is an aspect of text thatcan easily be recognised.
Focusing on input orderjust because it is easy is something like looking foryour watch under the street-light, even though youlost it in the dark alley.
The looking might be eas-ier, but if the answers are in the dark, then that iswhere we should be looking.126Rather than explore surface features of lan-guage which are easier to recognise, I believe weshould be either:?
building discourse-level tree-banks of realtexts, to provide real information to inform theautomatic derivation of evaluation functions,or,?
building tools to automatically recognise dis-course structure of text, (RST, focus move-ment, etc.
)Daniel Marcu has been a leader in both directions,working both on building an RST-tagged corpus,and also exploring the automatic recognition ofrhetorical structure (Marcu 2000).
In regards to thelatter, unfortunately, the results have not so farbeen useful for real applications.
If one wants torhetorically annotate a corpus, one needs to do itby hand, which involves a substantial investmentof time.
The case is similar for annotation of otherdiscourse structures, such as focus movement.Within the context of NLG, another approach ispossible.
For each generated text, we have at handthe deeper structure which was produced in con-structing the text.
In the case of ILEX for instance,a side-product of generating a text is the rhetoricalstructure of the text, information regarding co-identity of the entities mentioned in the text, etc.The machine-learning program can thus be pro-vided with all of this information as input, as wellas derived focus-movement, etc.
We can produce acorpus of generated texts, tagged with structuralinformation, and ask a human, to assign a level ofgoodness to each text.
The program can then de-rive an evaluation function of its own.The work of the human in the process thus be-comes simply to assign a level of goodness as-sessment to each of the texts in the training set.5 What features of a text are critical inevaluating worth?The effectiveness of any machine leaning programis very limited by the value of the text features itreceives.
While the previous point started out witha focal question How good is this text?, we sawthat the real question becomes: What factors do wefeed the system to optimise classification?In a sense, this is a movement back towards ques-tion 3: How do we tell if a text is good?
However,note the difference: question 3 concerns the inves-tigator assigning values to particular text features,typically making them up as they go.With this new question however, the investiga-tor is not assigning values to text features.
Rather,the investigator need only include the feature in thedata, and it is up to the learning algorithm to de-cide to what degree that feature improves or wors-ens the text quality.
The investigator only needs tohypothesise whether the text feature could be in-fluential in this decision.The main advantage here is the movement awayfrom investigators making up evaluation functionin their heads, or performing corpus studies.
In themachine learning approach, we need only includethe feature in the corpus.
For instance, the evalua-tion function of Mellish et al (1998) assigned +3for each instance of subject-repetition.
In an MLsystem, the human just decides that subject-repetition should be considered, and the systemderives an appropriate parameter for this feature.It is possible that many of the features we pro-vide to the system are not actually used by the sys-tem.
If we use an ML approach which allowsaccess to its evaluation model in an understandableform, then the approach has the added value ofrevealing to the investigator which of the featuresprovided to the system are important to text value,and which are not.
One product of this approach,apart from an improved text generator, will be animproved understanding of what it is that makes atext good or bad.Early approaches to applying ML in this wayare calculating the relative frequency of particulardiscourse features (e.g., of types of focus move-ments, of types of rhetorical relations and their or-dering).
Each generated text can then be evaluatedin terms of the relative frequency of its discoursefeatures in comparison with the tagged corpus.One problem with this approach is that manyaspects of text structure cannot be said to be goodor bad per se, it is only in relation to the context ofuse that such can be judged.
Almost any RST rela-tion can appear in a text, and the value of the useneeds to be judged in the context of its appearance.However, it is difficult for a machine learning sys-tem to infer in which contexts the use of a particu-lar relation is good and in which it is bad.We have assumed a supervised learning ap-proach, where a human evaluated the quality ofeach text in the corpus.
Because there is a cost toproducing such a corpus (human evaluation of127each text), some prefer an unsupervised approach,whereby the input at the training phase is not val-ued: it is assumed that all of the texts in the train-ing corpus are good representatives of the genre.6 SummaryWe have represented recent developments in textplanning in terms of an evolution in the principlequestion addressed.
While earlier work in textplanning addressed the question of how we com-pose texts, the movement into search-based plan-ning split the question into two parts: how wesearch the space of possible texts, and how weevaluate a text.The second of these questions has been increas-ingly in focus, and the need to move away frommade-up evaluation functions leads to the corpus-based derivation of text metrics.
At first glance,this seems to leave the analyst with the simple taskof evaluating the overall quality of a text, and leav-ing it to the machine to derive the evaluation func-tion (the question thus becomes: how good is thistext?
).However, on deeper analysis, the learning sys-tem needs to be fed structured input to be able toderive intelligent evaluation functions.
The focalquestion thus needs to shift to considering whatfeatures of discourse structure need to be providedto the ML system.The field of search-based text planning is stillyoung, and evolving quickly.
In the last 10 years,the focal questions have evolved rapidly.
The ques-tion is, what will we be asking in another 10 years?ReferencesH.
Cheng and C. Mellish.
2000.
Capturing the Interac-tion between Aggregation and Text Planning in TwoGeneration Systems.
Proceedings of the 1st Int.
NLGConference, Mitzpe Ramon, Israel, 108-115.A.
Dimitromanolaki and I. Androutsopoulos.
2003.Learning to order facts for discourse planning innatural language generation.
Proceedings of the 9thEuropean Workshop on NLG.C.
DiMarco, G. Hirst and E. Hovy.
1997.
Generation byselection and repair as a method for adapting text forthe individual reader.
Proceedings Workshop onFlexible Hypertext, 8th ACM International HypertextConference, Southampton, U.K., April 1997, 20-23.E.
Hovy.
1988.
Planning coherent multisentential text.Proceedings of the 26th Annual Meeting of ACL,163-169.N.
Karamanis.
2003.
Entity Coherence for DescriptiveText Structuring.
Ph.D. thesis, Informatics, Univer-sity of Edinburgh.N.
Karamanis and H. Manurung.
2002.
Stochastic TextStructuring using the Principle of Continuity.
Pro-ceedings of the 2nd Int.
NLG Conference.N.
Karamanis, C. Mellish, J. Oberlander and M. Poesio.2004.
A corpus-based methodology for evaluatingmetrics of coherence for text structuring.
Proceed-ings of INLG04, Brockenhurst, 90-99.I.
Langkilde and K. Knight.
1998.
The Practical Valueof N-Grams in Generation.
Proceedings of the 9thInt.
Workshop on NLG, Ontario, Canada, 1998.M.
Lapata.
2003.
Probabilistic text structuring: Experi-ments with sentence ordering.
Proceedings of ACL2003, 545-552.D.
Marcu.
1997.
From local to global coherence: A bot-tom-up approach to text planning.
Proceedings of theNational Conference on Artificial Intelligence(AAAI'97), 629-635.D.
Marcu.
2000.
The Rhetorical Parsing of UnrestrictedTexts: a Surface-Based Approach.
ComputationalLinguistics, 26 (3), 395-448.K.
McKeown.
1985.
Text Generation.
Cambridge Uni-versity Press, Cambridge.C.
Mellish, A. Knott, J. Oberlander and M. O'Donnell.1998.
Experiments using stochastic search for textplanning.
Proceedings of the 9th Int.
Workshop onNLG, Ontario, Canada, 98-107.M.
Milosavljevic.
1999.
The automatic generation ofcomparisons in descriptions of entities.
PhD Thesis.Department of Computing, Macquarie University,Australia.M.
O'Donnell, C. Mellish, J. Oberlander and A. Knott.2001.
ILEX: An architecture for a dynamic hypertextgeneration system.
Natural Language Engineering 7,225-250.D.
P?rez, E. Alfonseca, and P. Rodr?guez.
2004.
Appli-cation of the BLEU method for evaluating free-textanswers in an e-learning environment.
Proceedingsof the Language Resources and Evaluation Confer-ence (LREC-2004), Portugal.P.
Piwek and K. van Deemter.
2007.
Generating underGlobal Constraints: the Case of Scripted Dialogue.Journal of Research on Language and Computation,5(2):237-263.E.
Reiter.
(2000).
Pipelines and Size Constraints.
Com-putational Linguistics.
26:251-259.J.
Robin and K. McKeown.
1996.
Empirically designingand evaluating a new revision-based model for sum-mary generation?, Artificial Intelligence.
85(1-2).I.
Zukerman and R. McConachy.
1994.
Discourse Plan-ning as an Optimization Process.
Proceedings of the7th Int.
Workshop on NLG, Kennebunkport, Maine,37-44.128
