Performance Measures for the Next Generation of SpokenNatural Language Dialog SystemsRonn ie  W.  Smi thDepartment  of Mathemat icsComputer  Science SubareaEast Carol ina UniversityGreenvil le, NC 27858, USArws?cs,  ecu.
edu1 Improved  Per fo rmance  in SpokenNatura l  Language D ia log  SystemsSince approximately the mid 1980's, technologyhas been adequate (if not ideal) for researchers toconstruct spoken natural anguage dialog systems(SNLDS) in order to test theories of natural an-guage processing and to see what machines were ca-pable of based on current echnological limits.
Overthe course of time, a few systems have been con-structed in sufficient detail and robustness to enablesome evaluation of the systems.
For the most part,these systems were greatly limited by the availablespeech recognition technology.
Continuous peechsystems required speaker dependent training and re-stricted vocabularies, but still had such a large num-ber of misrecognitions that this tended to be thelimiting factor in the success of the system.
For ex-ample, testing in 1991 of the Circuit Fix-It Shop of(Smith, Hipp, and Biermann, 1995) required an ex-perimenter to remain in the room in order to notifythe user when misrecognitions occurred.Fortunately, speech recognition capabilities areimproving, and systems are being constructed thatallow individuals to walk up and use them after abrief orientation.
One example is the TRAINS sys-tem of (Allen et al, 1995) that was demonstratedat the 1995 ACL conference, where people just satdown and used the system after a brief set of in-structions were given to them by the demonstrator.Another example is the current system under devel-opment at Duke University that serves as a tutorfor liberal arts students learning the basics of Pascalprogramming.
In this system, the machine itself ex-plains how to use it.
More thorough and challengingmethods of evaluation are now feasible.
This paperproposes ome measures for evaluation based on aretrospective look at measures used in the past, an-alyzing their relevance in today's environment.For the future, expect measurements of speechrecognition performance and basic utterance under-standing to remain important, but there should alsobe more emphasis on measuring robustness and mea-suring the utility of domain-independent k owledgeabout dialog.
Furthermore, we should expect real-time response from evaluated systems, a sharp re-duction in the amount of specialized training for us-ing systems, and the use of longitudinal studies tosee how user behavior evolves.2 Fundamenta ls  in  Eva luat ion2.1 Linguistic CoverageA forward looking view of evaluation is offered by(Whittaker and Stenton, 1989).
It is forward look-ing in the sense that they investigated issues in eval-uation independent ofbuilding a system.
Their per-spective was not based on a specific SNLDS, buta general analysis of the issue of evaluation.
Theirmain point was that evaluation eeded to be placedwithin the context of a system's use.
Consequently,they used a Wizard of Oz study in an informationretrieval environment (e.g., database query) in or-der to identify the types of natural anguage inputsa typical user would use in order to gain access toneeded information.
Their analysis identified the fol-lowing requirements for the linguistic overage of adialog system in the information retrieval environ-ment: (1) operators for specifying the properties ofthe set of objects for which information would berequested; (2) contextual references; and (3) refer-ences to the actual source of information (e.g., thedatabase).
In general, linguistic overage of SNLDSin the past has been limited, and to the extentthat limitations will exist in the next generation ofSNLDS, such limitations need to be measured anddescribed.2.2 Early System Performance Measures(Biermann, Fineman, and Heidlage, 1992) report onthe results of testing their VIPX system of the mid1980's which offers users the ability to display and37manipulate text on a computer terminal using spo-ken English supported by touches of the screen.
Themain dimensions which they evaluated were: (1)Learnability, (2) Correctness, (3) Timing, and (4)User Response.
Learnability measures how easilysubjects could learn to communicate with the ma-chine.
Correctness measures whether or not therewas successful completion of the task.
Timing de-scribes the rate at which work was completed.
Userresponse measures how users felt about using thesystem.These general categories of performance measurescan be broken down into more precisely defined andquantifiable measures.
Information on learnabilityand user response can be elicited via a subject sur-vey and through comparison to alternative forms ofuser interface for completing the same task (e.g., dis-crete speech versus continuous speech, keyboard vs.speech input, and speech vs. multimodal input).
Inthe next section we examine some of the measuresrelevant o correctness and timing and discuss theirrelevance for future evaluation of SNLDS.3 Measures  o f  Cor rec tness  andT iming :  Pas t ,  P resent ,  and  Future3.1 Recognit ion RateThis measure has been expressed in a variety ofways.
Its purpose is to describe the performance ofthe speech recognition component in terms of howaccurately it converts the speech signal into the ac-tual words uttered.
Recognition rate and the overallsuccess rate of the interaction are invariably highlycorrelated.
This measure is still relevant.
Whilerecognition technology is improving, it is not per-fect.
In particular, telephone interactions providea very challenging environment for speech recogni-tion equipment.
For the Dialogos ystem which an-swers inquiries about Italian Railway train sched-ules, (Billi, Castagneri, and Danieli, 1996) reportonly 68.2% word accuracy for the system in 96 di-alogs.
In spite of this Dialogos still understood81.6% of all sentences, a promising result.
As sys-tems are tested in more challenging environments,the base level accuracy of the input signal remainsan important benchmark in measuring system per-formance.3.2 Perplexi tyThis measure is used to describe the amount ofsearch that a speech recognition component mustdo in translating the input signal.
The MINDSsystem of (Young et al, 1989) and the TINA sys-tem of (Seneff, 1992) represent speech systems thatmade use of various techniques for reducing the per-plexity faced by the speech recognition component.TINA used probabilistic networks and semantic fil-tering to reduce perplexity.
MINDS used predictionsbased on dialog context o reduce perplexity.
Whilenot a specific measure of a dialog system, an inte-grated dialog system such as MINDS can provideinformation that can reduce the perplexity that thespeech recognition component must deal with.
Con-sequently, comparative measures of perplexity withand without context-dependent predictions remaina valid measure for evaluating the performance ofa dialog system, particularly in a complex linguisticenvironment where reduction of perplexity is essen-tial for good speech recognizer performance.3.3 Correct ly UnderstoodUt terances /Correct ly  ProcessedQueriesThis measures how well a system processed utter-ances in isolation, but does not give the completepicture of system performance in a dialog where ut-terances are related through context.
As the envi-ronments in which systems are tested become morechallenging, the ability to handle partially under-stood utterances will be important.
Measures forcapturing the rate of success in situations where ut-terances are partially understood or perhaps evencompletely misunderstood are needed.
Such mea-sures must take the overall dialog context into ac-count.
One such measure has been proposed by(Danieli and Gerbino, 1995).
They define the no-tion of "Implicit Recovery" (IR) as a measure ofthe ability of a system to filter the output of theparser and interpret it using contextual knowledge.In particular, an implicit recovery occurs when thesystem only partially understands an utterance, butstill responds in an appropriate fashion, They alsodefine what it means for a response to be appropri-ate within the context of an information retrievalsituation.
There is still a need for such definitionsin a task assistance environment.3.4 System Response TimesThis measure was used in order to demonstratethe practical viability of systems/techniques when"the hardware gets faster."
For the most part,near real-time performance was the best result ob-tained.
However, as (Oviatt and Cohen, 1989) cau-tion, speakers expect fast response times in a systemthat provides poken interaction.
If one expects toevaluate human-computer spoken language interac-tion, one will need a system that can give the quickresponses that people normally expect in spoken in-38teraction.
It is hoped in the next generation ofmea-suring SNLDS, system response time will no longerbe a required measure, as systems will perform withreal-time speed and not continually have awkwarddelays that break up the flow of the dialog.3.5 Durat ion of  the Interact ionAn overall measure specifying how long it takes auser to complete the interaction, it provides a grossmeasure that can indicate interactional differencesunder different conditions, such as the level of sys-tem initiative ((Smith, Hipp, and Biermann, 1995)).Another way in which this is used is in compar-ing the efficiency of natural language interaction toother modes of communication that could be usedfor the given task.
For example, (Biermann, Fine-man, and Heidlage, 1992) as part of their overallevaluation of their voice and touch-driven text edi-tor compare the time it takes to execute commandswith their system to the time it takes people to com-plete the commands using the vi text editor.
Com-paring the speed at which someone can obtain infor-mation over the telephone by using a speech-basedinterface as opposed to the ubiquitous touch-tone in-terface with exhausting menu hierarchies that mostbusinesses have (this seems to be true of businessesin the United States) might be very illuminating in-deed!3.6 Overall Interact ion SuccessThis measures whether or not the interaction wassuccessful (i.e., was the desired information ob-tained, or the required task completed?).
Giventhe unfortunate circumstance that for the foresee-able future, some interactions will fail, this measureremains necessary.
And if all interactions were suc-cessful, we might believe that the task was simplynot challenging enough!3.7 Frequency of System Fai lure/ErrorThe earliest systems were prone to frequent hard-ware and software failures.
Robustness was mea-sured in terms of how infrequently a system crashed.In other circumstances, system failure might be castas "user error", because the user did not follow theallowed syntax or else spoke a word that was not inthe recognizers vocabulary.
As the state of the artprogresses ystem errors are evolving into inappro-priate responses rather than total system failure.
Itis hoped that system failure will disappear and bereplaced by system robustness, that is, a measure ofhow well a system responds in error situations, eitherbecause of misunderstandings by itself, or because ofmisstatements by the human user.4 New I ssues  in Eva luat ion4.1 A Reduction in the Training RegimenDue to their brittle nature and the limits of speechrecognition technology, rigorous experimental evalu-ation of systems required extensive training by sub-jects before testing began.
This training involvedrecording of voice patterns for speaker-dependentspeech recognition as well as training on the re-stricted vocabulary and syntax that systems re-quired.
Thus, as reported in (Smith and Hipp,1994) for the Circuit Fix-It Shop much care had tobe taken to get users to speak somewhat naturally,while still remaining within the linguistic coveragecapabilities of the system.
In the future, we hopethat such restrictions will not be necessary, or atthe very least, be greatly lessened.Speaker-independent continuous peech recogni-tion technology is now available, so the amount oftime required to enable a person to interact with anSNLDS is much less.
As mentioned previously, theTRAINS system demoed at ACL 1995 did not re-quire any particular training other than being toldthe task you were trying to complete, being given abrief description of the screen layout on the consoleyou were viewing, and the encouragement to alk tothe machine like you would talk to a human assis-tant.
On the other hand, they were using the systemin a "data collection" mode at that point rather thanin a formal experimental evaluation of the system.Depending on the nature of the task, the amount oftraining required will be varied and still needs to bereported.
Care must be taken in any training notto overly bias the type of linguistic behavior thatusers will exhibit, if claims of general capability androbustness are to be validated.4.2 Measuring the Uti l i ty ofDomain- Independent  I format ionSNLDS cannot succeed without a strong base of do-main knowledge.
Nevertheless, if our main researchfocus is on our theories of natural language process-ing, we would like to justify our theory by showinghow well it performs.
One way to capture this wouldbe the development of measures that show the util-ity of domain-independent dialog knowledge as com-pared to domain-specific information which a systemcontains.
For example, some inputs to the systemwill be contextually self-contained (e.g., "The redswitch is in the off position" when there is only onered switch in the domain), while other inputs requirethe use of dialog knowledge to be understood.
Whenreporting the percentage ofutterances correctly un-derstood, it may be illuminating to report the cause39of the utterances not understood--is t because ofa lack of domain knowledge, a lack of vocabulary,or a lack of ability at doing contextual interpreta-tion?
Such measures can be helpful in determiningthe usefulness of a theory of dialog processing as wellas determining future directions for research.5 An  Idea l i zed  V iew o f  Eva luat ionFor the future testing of systems, I hope to see thefollowing: systems that (1) interact with users ina complex problem-solving domain where both theuser and system have knowledge about what prob-lem is being solved; (2) do not need experimentersto act as intermediaries between system and user asthe system and user will be able to collaborate viaspoken natural anguage to a successful conclusion;and (3) allow users to be ready to use them afterless than five minutes of instruction.Due to the wide ranging motivations of fundingagencies and the world-wide interest in SNLDS, itis not likely that we will find a common task forwhich everyone will implement their model of dia-log processing and then be able to test them all ona common set of problems to see which one per-forms better.
Consequently, when reporting evalu-ations, a variety of measures will be needed in or-der to allow ones colleagues to gain an idea of theeffectiveness of the system.
These measures shouldinclude (1) speech recognition accuracy; (2) the util-ity of domain-independent k owledge about dialog;(3) the nature and effectiveness of system error han-dling; and (4) comparisons of effectiveness for mul-tiple interaction styles.Furthermore, public access to transcripts and theproduction of videotapes ofsubjects in the actual ex-perimental situation should also be part of the eval-uation framework.
In environments where one mayencounter novices, experts, or individuals with inter-mediate xpertise, the ability to interact in a vari-ety of styles becomes essential.
Longitudinal studieswith subjects in such environments are the only wayto gain an idea of a system's success in dealing withsuch a situation.
Only through careful evaluationand full reporting of the results can the communityof researchers as well as the general public gain anunderstanding of the current abilities and the futurepotential of SNLDS.6 AcknowledgmentsThis material is based upon work supported by theNational Science Foundation under Grant No.
IRI-9501571.Re ferencesAllen, J.F., L.K.
Schubert, G. Ferguson, P. Hee-man, C.H.
Hwang, T. Kato, M. Light, N. Mar-tin, B. Miller, M. Poesio, and D.R.
Traum.
1995.The TRAINS project: a case study in buildinga conversational planning agent.
Journal of Ex-perimental and Theoretical Artificial Intelligence,7:7-48.Biermann, Alan W., Linda Fineman, and J. FrancisHeidlage.
1992.
A voice- and touch-driven atu-ral language ditor and its performance.
Interna-tional Journal of Man-Machine Studies, 37:1-21.Billi, R., G. Castagneri, and M. Danieli.
1996.Field trial evaluations of two different informa-tion inquiry systems.
In Proceeding s of the ThirdIEEE Workshop on Interactive Voice TechnologiesTelecommunications Applications.Danieli, Morena and Elisabetta Gerbino.
1995.Metrics for evaluating dialogue strategies in a spo-ken language system.
In Proceedings of the 1995AAAI Spring Symposium on Empirical Methodsin Discourse Interpretation and Generation, pages34-39.Oviatt, Sharon L. and Philip R. Cohen.
1989.
Theeffects of interaction on spoken discourse.
In Pro-ceedings of the 27th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 126-134.Seneff, S. 1992.
TINA: A natural anguage systemfor spoken language applications.
ComputationalLinguistics, pages 61-86, March.Smith, R.W.
and D.R.
Hipp.
1994.
Spoken NaturalLanguage Dialog Systems: A Practical Approach.Oxford University Press, New York.Smith, R.W., D.R.
Hipp, and A.W.
Biermann.
1995.An architecture for voice dialog systems basedon Prolog-style theorem-proving.
ComputationalLinguistics , pages 281-320.Whittaker, Steve and Phil Stenton.
1989.
User stud-ies and the design of natural language systems.
InProceedings of the Fourth Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics, pages 116-123.Young, S.R., A.G. Hauptmann, W.H.
Ward, E.T.Smith, and P. Werner.
1989.
High level knowl-edge sources in usable speech recognition sys-tems.
Communications of the ACM, pages 183-194, February.40
