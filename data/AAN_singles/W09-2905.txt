Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 31?39,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPA Re-examination of Lexical Association MeasuresAbstractWe review lexical Association Measures(AMs) that have been employed by pastwork in extracting multiword expressions.Our work contributes to the understandingof these AMs by categorizing them intotwo groups and suggesting the use of rankequivalence to group AMs with the sameranking performance.
We also examinehow existing AMs can be adapted to betterrank English verb particle constructionsand light verb constructions.
Specifically,we suggest normalizing (Pointwise)Mutual Information and using marginalfrequencies to construct penalizationterms.
We empirically validate theeffectiveness of these modified AMs indetection tasks in English, performed onthe Penn Treebank, which showssignificant improvement over the originalAMs.1 IntroductionRecently, the NLP community has witnessed arenewed interest in the use of lexical associationmeasures in extracting Multiword Expressions(MWEs).
Lexical Association Measures(hereafter, AMs) are mathematical formulaswhich can be used to capture the degree ofconnection or association between constituentsof a given phrase.
Well-known AMs includePointwise Mutual Information (PMI),Pearson?s 2?
and the Odds Ratio.
These AMshave been applied in many different fields ofstudy, from information retrieval to hypothesistesting.
In the context of MWE extraction, manypublished works have been devoted to comparingtheir effectiveness.
Krenn and Evert (2001)evaluate Mutual Information (MI), Dice,Pearson?s 2?
, log-likelihoodratio and the T score.
In Pearce (2002), AMssuch as Z score, Pointwise MI, cost reduction,left and right context entropy, odds ratio areevaluated.
Evert (2004) discussed a wide rangeof AMs, including exact hypothesis tests such asthe binomial test and Fisher?s exact tests, variouscoefficients such as Dice and Jaccard.
Later,Ramisch et al (2008) evaluated MI,Pearson?s 2?
and Permutation Entropy.
Probablythe most comprehensive evaluation of AMs waspresented in Pecina and Schlesinger (2006),where 82 AMs were assembled and evaluatedover Czech collocations.
These collocationscontained a mix of idiomatic expressions,technical terms, light verb constructions andstock phrases.
In their work, the bestcombination of AMs was selected using machinelearning.While the previous works have evaluated AMs,there have been few details on why the AMsperform as they do.
A detailed analysis of whythese AMs perform as they do is needed in orderto explain their identification performance, andto help us recommend AMs for future tasks.
Thisweakness of previous works motivated us toaddress this issue.
In this work, we contribute tofurther understanding of association measures,using two different MWE extraction tasks tomotivate and concretize our discussion.
Our goalis to be able to predict, a priori, what types ofAMs are likely to perform well for a particularMWE class.We focus on the extraction of two commontypes of English MWEs that can be captured bybigram model: Verb Particle Constructions(VPCs) and Light Verb Constructions (LVCs).VPCs consist of a verb and one or more particles,which can be prepositions (e.g.
put on, bolsterup), adjectives (cut short) or verbs (make do).For simplicity, we focus only on bigram VPCsthat take prepositional particles, the mostcommon class of VPCs.
A special characteristicof VPCs that affects their extraction is theHung Huu HoangDept.
of Computer ScienceNational Universityof  Singaporehoanghuu@comp.nus.edu.sgSu Nam KimDept.
of Computer Scienceand Software EngineeringUniversity of Melbournesnkim@csse.unimelb.edu.auMin-Yen KanDept.
of Computer ScienceNational Universityof  Singaporekanmy@comp.nus.edu.sg31mobility of noun phrase complements intransitive VPCs.
They can appear after theparticle (Take off your hat) or between the verband the particle (Take your hat off).
However, apronominal complement can only appear in thelatter configuration (Take it off).In comparison, LVCs comprise of a verb and acomplement, which is usually a noun phrase(make a presentation, give a demonstration).Their meanings come mostly from theircomplements and, as such, verbs in LVCs aretermed semantically light, hence the name lightverb.
This explains why modifiers of LVCsmodify the complement instead of the verb(make a serious mistake vs. *make a mistakeseriously).
This phenomenon also shows that anLVC?s constituents may not occur contiguously.2 Classification of Association MeasuresAlthough different AMs have differentapproaches to measuring association, weobserved that they can effectively be classifiedinto two broad classes.
Class I AMs look at thedegree of institutionalization; i.e., the extent towhich the phrase is a semantic unit rather than afree combination of words.
Some of the AMs inthis class directly measure this associationbetween constituents using various combinationsof co-occurrence and marginal frequencies.Examples include MI, PMI and their variants aswell as most of the association coefficients suchas Jaccard, Hamann, Brawn-Blanquet, andothers.
Other Class I AMs estimate a phrase?sMWE-hood by judging the significance of thedifference between observed and expectedfrequencies.
These AMs include, among others,statistical hypothesis tests such as T score, Zscore and Pearson?s 2?
test.Class II AMs feature the use of context tomeasure non-compositionality, a peculiarcharacteristic of many types of MWEs, includingVPCs and idioms.
This is commonly done in oneof the following two ways.
First, non-compositionality can be modeled through thediversity of contexts, measured using entropy.The underlying assumption of this approach isthat non-compositional phrases appear in a morerestricted set of contexts than compositional ones.Second, non-compositionality can also bemeasured through context similarity between thephrase and its constituents.
The observation hereis that non-compositional phrases have differentsemantics from those of their constituents.
It thenfollows that contexts in which the phrase and itsconstituents appear would be different (Zhai,1997).
Some VPC examples include carry out,give up.
A close approximation stipulates thatcontexts of a non-compositional phrase?sconstituents are also different.
For instance,phrases such as hot dog and Dutch courage arecomprised of constituents that have unrelatedmeanings.
Metrics that are commonly used tocompute context similarity include cosine anddice similarity; distance metrics such asEuclidean and Manhattan norm; and probabilitydistribution measures such as Kullback-Leiblerdivergence and Jensen-Shannon divergence.Table 1 lists all AMs used in our discussion.The lower left legend defines the variables a, b, c,and d with respect to the raw co-occurrencestatistics observed in the corpus data.
When anAM is introduced, it is prefixed with its indexgiven in Table 1(e.g., [M2] Mutual Information)for the reader?s convenience.3 EvaluationWe will first present how VPC and LVCcandidates are extracted and used to form ourevaluation data set.
Second, we will discuss howperformances of AMs are measured in ourexperiments.3.1 Evaluation DataIn this study, we employ the Wall Street Journal(WSJ) section of one million words in the PennTree Bank.
To create the evaluation data set, wefirst extract the VPC and LVC candidates fromour corpus as described below.
We note here thatthe mobility property of both VPC and LVCconstituents have been used in the extractionprocess.For VPCs, we first identify particles using apre-compiled set of 38 particles based onBaldwin (2005) and Quirk et al (1985)(Appendix A).
Here we do not use the WSJparticle tag to avoid possible inconsistenciespointed out in Baldwin (2005).
Next, we searchto the left of the located particle for the nearestverb.
As verbs and particles in transitive VPCsmay not occur contiguously, we allow anintervening NP of up to 5 words, similar toBaldwin and Villavicencio (2002) and Smadja(1993), since longer NPs tend to be located afterparticles.32Extraction of LVCs is carried out in a similarfashion.
First, occurrences of light verbs arelocated based on the following set of sevenfrequently used English light verbs: do, get, give,have, make, put and take.
Next, we search to theright of the light verbs for the nearest noun,AM Name Formula AM Name FormulaM1.
Joint Probability ( ) /f xy N  M2.
Mutual  Information,1log?ijiji j ijffN f?M3.
Log likelihoodratio,2 log?ijiji j ijfff?
M4.
Pointwise MI (PMI) ( )log( ) ( )P xyP x P y?
?M5.
Local-PMI ( ) PMIf xy ?
M6.
PMIk ( )log( ) ( )kNf xyf x f y?
?M7.
PMI2 2( )log( ) ( )Nf xyf x f y?
?M8.
Mutual Dependency 2( )log( *) (* )P xyP x P yM9.
Driver-Kroeber( )( )aa b a c+ +M10.
Normalizedexpectation22aa b c+ +M11.
Jaccard aa b c+ +M12.
First Kulczynski ab c+M13.
SecondSokal-Sneath 2( )aa b c+ +M14.
ThirdSokal-Sneatha db c++M15.
Sokal-Michiner a da b c d++ + +M16.
Rogers-Tanimoto2 2a da b c d++ + +M17.
Hamann ( ) ( )a d b ca b c d+ ?
++ + +M18.
Odds ratio adbcM19.
Yule?s ?
ad bcad bc?+M20.
Yule?s Q ad bcad bc?+M21.
Brawn-Blanquet max( , )aa b a c+ +M22.
Simpsonmin( , )aa b a c+ +M23.
S cost 12min( , )log(1 )1b ca?++M24*.
Adjusted S Cost 12max( , )log(1 )1b ca?++M25.
Laplace 1min( ,  ) 2aa b c++ +M26*.
Adjusted Laplace 1max( ,  ) 2aa b c++ +M27.
Fager[M9]1max( , )2b c?M28*.
Adjusted Fager[M9]1max( , )b caN?M29*.
NormalizedPMIsPMI / NF( )?PMI / NFMaxM30*.
Simplifiednormalized PMI forVPCslog( )(1 )adb c?
??
+ ?
?M31*.
NormalizedMIsMI / NF( )?MI / NFMaxNF( )?
= ( )P x?
?
+ (1 ) ( )P y??
?
[0,  1]?
?NFMax = max( ( ),  ( ))P x P y?
?11 ( )a f f xy= =   12 ( )b f f xy= =21 ( )c f f xy= =  22 ( )d f f xy= =( )f x?
( )f x?
( )f y?
( )f y?
NTable 1.
Association measures discussed in this paper.
Starred AMs (*) are developed in this work.Contingency table of a bigram (x y), recording co-occurrence and marginal frequencies; w  stands for allwords except w; * stands for all words; N is totalnumber of bigrams.
The expected frequency under theindependence assumption is ?
( ) ( ) ( ) / .f xy f x f y N= ?
?33permitting a maximum of 4 intervening words toallow for quantifiers (a/an, the, many, etc.
),adjectival and adverbial modifiers, etc.
If thissearch fails to find a noun, as when LVCs areused in the passive (e.g.
the presentation wasmade), we search to the right of the light verb,also allowing a maximum of 4 intervening words.The above extraction process produced a total of8,652 VPC and 11,465 LVC candidates whenrun on the corpus.
We then filter out candidateswith observed frequencies less than 6, assuggested in Pecina and Schlesinger (2006), toobtain a set of 1,629 VPCs and 1,254 LVCs.Separately, we use the following two availablesources of annotations: 3,078 VPC candidatesextracted and annotated in (Baldwin, 2005) and464 annotated LVC candidates used in (Tan etal., 2006).
Both sets of annotations give bothpositive and negative examples.Our final VPC and LVC evaluation datasetswere then constructed by intersecting the gold-standard datasets with our corresponding sets ofextracted candidates.
We also concatenated bothsets of evaluation data for composite evaluation.This set is referred to as ?Mixed?.
Statistics ofour three evaluation datasets are summarized inTable 2.VPC data LVC data MixedTotal(freq  ?
6)413 100 513Positiveinstances117(28.33%)28(28%)145(23.26%)Table 2.
Evaluation data sizes (type count, not token).While these datasets are small, our primarygoal in this work is to establish initialcomparable baselines and describe interestingphenomena that we plan to investigate overlarger datasets in future work.3.2 Evaluation MetricTo evaluate the performance of AMs, we can usethe standard precision and recall measures, as inmuch past work.
We note that the ranked list ofcandidates generated by an AM is often used as aclassifier by setting a threshold.
However, settinga threshold is problematic and optimal thresholdvalues vary for different AMs.
Additionally,using the list of ranked candidates directly as aclassifier does not consider the confidenceindicated by actual scores.
Another way to avoidsetting threshold values is to measure precisionand recall of only the n most likely candidates(the n-best method).
However, as discussed inEvert and Krenn (2001), this method dependsheavily on the choice of n. In this paper, we optfor average precision (AP), which is the averageof precisions at all possible recall values.
Thischoice also makes our results comparable tothose of Pecina and Schlesinger (2006).3.3 Evaluation ResultsFigure 1(a, b) gives the two average precisionprofiles of the 82 AMs presented in Pecina andSchlesinger (2006) when we replicated theirexperiments over our English VPC and LVCdatasets.
We observe that the average precisionprofile for VPCs is slightly concave while theone for LVCs is more convex.
This can beinterpreted as VPCs being more sensitive to thechoice of AM than LVCs.
Another point weobserved is that a vast majority of Class I AMs,including PMI, its variants and associationcoefficients (excluding hypothesis tests), performreasonably well in our application.
In contrast,the performances of most of context-based andhypothesis test AMs are very modest.
Theirmediocre performance indicates theirinapplicability to our VPC and LVC tasks.
Inparticular, the high frequencies of particles inVPCs and light verbs in LVCs both underminetheir contexts?
discriminative power and skewthe difference between observed and expectedfrequencies that are relied on in hypothesis tests.4 Rank EquivalenceWe note that some AMs, although notmathematically equivalent (i.e., assigningidentical scores to input candidates) produce thesame lists of ranked candidates on our datasets.Hence, they achieve the same average precision.The ability to identify such groups of AMs ishelpful in simplifying their formulas, which inturn assisting in analyzing their meanings.Definition: Association measures M1 and M2 arerank equivalent over a set C, denoted by M1rC?M2, if and only if M1(cj) > M1(ck) ?
M2(cj) >M2(ck) and M1(cj) = M1(ck) ?
M2(cj) = M2(ck) forall cj, ck belongs to C where Mk(ci) denotes thescore assigned to ci by the measure Mk.As a corollary, the following also holds for rankequivalent AMs:34Corollary: If M1rC?
M2 then APC(M1) = APC(M2)where APC(Mi) stands for the average precisionof the AM Mi over the data set C.Essentially, M1 and M2 are rank equivalent overa set C if their ranked lists of all candidates takenfrom C are the same, ignoring the actualcalculated scores1.
As an example, the following3 AMs: Odds ratio, Yule?s ?
and Yule?s Q (Table3, row 5), though not mathematically equivalent,can be shown to be rank equivalent.
Five groupsof rank equivalent AMs that we have found arelisted in Table 3.
This allows us to replace thebelow 15 AMs with their (most simple)representatives from each rank equivalent group.1 Two AMs may be rank equivalent with the exception ofsome candidates where one AM is undefined due to a zeroin the denominator while the other AM is still well-defined.We call these cases weakly rank equivalent.
With areasonably large corpus, such candidates are rare for ourVPC and LVC types.
Hence, we still consider such AMpairs to be rank equivalent.1) [M2] Mutual Information,[M3] Log likelihood ratio2) [M7] PMI2, [M8] Mutual Dependency,[M9] Driver-Kroeber (a.k.a.
Ochiai)3) [M10] Normalized expectation,[M11] Jaccard, [M12] First Kulczynski,[M13]Second Sokal-Sneath(a.k.a.
Anderberg)4) [M14] Third Sokal-Sneath,[M15] Sokal-Michiner,[M16] Rogers-Tanimoto, [M17] Hamann5) [M18] Odds ratio, [M19] Yule?s ,?
[M20] Yule?s QTable 3.
Five groups of rank equivalent AMs.5 Examination of Association MeasuresWe highlight two important findings in ouranalysis of the AMs over our English datasets.Section 5.1 focuses on MI and PMI and Section5.2 discusses penalization terms.5.1 Mutual Information and PointwiseMutual InformationIn Figure 1, over 82 AMs, PMI ranks 11th inidentifying VPCs while MI ranks 35th in0.10.20.30.40.50.6APFigure 1a.
AP profile of AMs examined over our VPC data set.0.10.20.30.40.50.6APFigure 1b.
AP profile of AMs examined over our LVC data set.Figure 1.
Average precision (AP) performance of the 82 AMs from Pecina and Schlesinger (2006), on ourEnglish VPC and LVC datasets.
Bold points indicate AMs discussed in this paper.?
Hypothesis test AMs     ?
Class I AMs, excluding hypothesis test AMs     + Context-based AMs.35identifying LVCs.
In this section, we show howtheir performances can be improved significantly.Mutual Information (MI) measures thecommon information between two variables orthe reduction in uncertainty of one variable givenknowledge of theother.,( )MI( ; ) ( )log( ) ( )u vp uvU V p uvp u p v=?
??
.
In thecontext of bigrams, the above formula can besimplified to [M2] MI =,1log?Nijiji jijfff?
.
While MIholds between random variables, [M4] PointwiseMI (PMI) holds between specific values: PMI(x,y) =( )log( ) ( )P xyP x P y?
?
( )log( ) ( )Nf xyf x f y=?
?.
It has longbeen pointed out that PMI favors bigrams withlow-frequency constituents, as evidenced by theproduct of two marginal frequencies in itsdenominator.
To reduce this bias, a commonsolution is to assign more weight to the co-occurrence frequency ( )f xy in the numerator byeither raising it to some power k (Daille, 1994) ormultiplying PMI with ( )f xy .
Table 4 lists theseadjusted versions of PMI and their performanceover our datasets.
We can see from Table 4 thatthe best performance of PMIk is obtained at kvalues less than one, indicating that it is better torely less on ( )f xy .
Similarly, multiplying( )f xy directly to PMI reduces the performance ofPMI.
As such, assigning more weight to ( )f xydoes not improve the AP performance of PMI.AM VPCs LVCs MixedBest [M6] PMIk .547(k = .13).573(k = .85).544(k = .32)[M4] PMI .510 .566 .515[M5] Local-PMI  .259 .393 .272[M1] Joint Prob.
.170 .28 .175Table 4.
AP performance of PMI and its variants.
Bestalpha settings shown in parentheses.Another shortcoming of (P)MI is that bothgrow not only with the degree of dependence butalso with frequency (Manning and Schutze,&& 1999,p.
66).
In particular, we can show that MI(X; Y) ?min(H(X), H(Y)), where H(.)
denotes entropy,and PMI(x,y) ?
min( log ( ),P x?
?
log ( )P y?
?
).These two inequalities suggest that theallowed score ranges of different candidates varyand consequently, MI and PMI scores are notdirectly comparable.
Furthermore, in the case ofVPCs and LVCs, the differences among scoreranges of different candidates are large, due tohigh frequencies of particles and light verbs.
Thishas motivated us to normalize these scoresbefore using them for comparison.
We suggestMI and PMI be divided by one of the followingtwo normalization factors: NF( )?
= ( )P x?
?
+(1 ) ( )P y??
?
with [0,  1]?
?
and NFmax= max( ( ),  ( ))P x P y?
?
.
NF( )?
, being dependent onalpha, can be optimized by setting an appropriatealpha value, which is inevitably affected by theMWE type and the corpus statistics.
On the otherhand, NFmax is independent of alpha and isrecommended when one needs to applynormalized (P)MI to a mixed set of differentMWE types or when sufficient data forparameter tuning is unavailable.
As shown inTable 5, normalized MI and PMI showconsiderable improvements of up to 80%.
Also,PMI and MI, after being normalized with NFmax,rank number one in VPC and LVC task,respectively.
If one re-writes MI as = (1/N) ij iji, jPMIf ??
, it is easy to see the heavydependence of MI on direct frequenciescompared with PMI and this explains whynormalization is a pressing need for MI.AM VPCs LVCs MixedMI / NF( )?
.508(?
= .48).583(?
= .47).516(?
= .5)MI / NFmax .508 .584 .518[M2] MI .273 .435 .289PMI / NF( )?
.592(?
= .8).554(?
= .48).588(?
= .77)PMI / NFmax .565 .517 .556[M4] PMI .510 .566 .515Table 5.
AP performance of normalized (P)MI versusstandard (P)MI.
Best alpha settings shown inparentheses.5.2 Penalization TermsIt can be seen that given equal co-occurrencefrequencies, higher marginal frequencies reducethe likelihood of being MWEs.
This motivates usto use marginal frequencies to synthesizepenalization terms which are formulae whosevalues are inversely proportional to thelikelihood of being MWEs.
We hypothesize thatincorporating such penalization terms canimprove the respective AMs detection AP.Take as an example, the AMs [M21] Brawn-Blanquet (a.k.a.
Minimum Sensitivity) and [M22]Simpson.
These two AMs are identical, except36for one difference in the denominator: Brawn-Blanquet uses max(b, c); Simpson uses min(b, c).It is intuitive and confirmed by our experimentsthat penalizing against the more frequentconstituent by choosing max(b, c) is moreeffective.
This is further attested in AMs [M23]S Cost and [M25] Laplace, where we tried toreplace the min(b, c) term with max(b, c).
Table6 shows the average precision on our datasets forall these AMs.AM VPCs LVCs Mixed[M21]Brawn-Blanquet.478 .578 .486[M22] Simpson .249 .382 .260[M24] AdjustedS Cost.485 .577 .492[M23] S cost .249 .388 .260[M26] AdjustedLaplace.486 .577 .493[M25] Laplace .241 .388 .254Table 6.
Replacing min() with max() in selected AMs.In the [M27] Fager AM, the penalization termmax(b, c) is subtracted from the first term, whichis no stranger but rank equivalent to [M7] PMI2.In our application, this AM is not good since thesecond term is far larger than the first term,which is less than 1.
As such, Fager is largelyequivalent to just ??
max(b, c).
In order to makeuse of the first term, we need to replace theconstant ?
by a scaled down version of max(b,c).
We have approximately derived 1/ aN as alower bound estimate of max(b, c) using theindependence assumption, producing [M28]Adjusted Fager.
We can see from Table 7 thatthis adjustment improves Fager on both datasets.AM VPCs LVCs Mixed[M28] AdjustedFager.564 .543 .554[M27] Fager .552 .439 .525Table 7.
Performance of Fager and its adjustedversion.The next experiment involves [M14] ThirdSokal Sneath, which can be shown to be rankequivalent to ?b ?c.
We further notice thatfrequencies c of particles are normally muchlarger than frequencies b of verbs.
Thus, this AMruns the risk of ranking VPC candidates based ononly frequencies of particles.
So, it is necessarythat we scale b and c properly as in[M14'] b??
?
?
(1 ) c??
?
.
Having scaled theconstituents properly, we still see that [M14'] byitself is not a good measure as it uses onlyconstituent frequencies and does not take intoconsideration the co-occurrence frequency of thetwo constituents.
This has led us to experimentwith [MR14'']PMI(1 )b c?
??
+ ?
?.
Thedenominator of [MR14''] is obtained byremoving the minus sign from [MR14'] so that itcan be used as a penalization term.
The choice ofPMI in the numerator is due to the fact that thedenominator of [MR14''] is in essence similar toNF( )?
= ( )P x?
?
+ (1 ) ( )P y??
?
, which hasbeen successfully used to divide PMI in thenormalized PMI experiment.
We heuristicallytried to simplify [MR14''] to the following AM[M30]log( )(1 )adb c?
??
+ ?
?.
The setting of alpha inTable 8 below is taken from the best alphasetting obtained the experiment on thenormalized PMI (Table 5).
It can be observedfrom Table 8 that [MR14'''], beingcomputationally simpler than normalized PMI,performs as well as normalized PMI and betterthan Third Sokal-Sneath over the VPC data set.AM VPCs LVCs MixedPMI / NF( )?
.592(?
=.8).554(?
=.48).588(?
=.77)[M30]log( )(1 )adb c?
??
+ ?
?.600(?
=.8).484(?
=.48).588(?
=.77)[M14] ThirdSokal Sneath.565 .453 .546Table 8.
AP performance of suggested VPCs?penalization terms and AMs.With the same intention and method, we havefound that while addition of marginal frequenciesis a good penalization term for VPCs, theproduct of marginal frequencies is more suitablefor LVCs (rows 1 and 2, Table 9).
As with thelinear combination, the product bc should also beweighted accordingly as (1 )b c?
??
.
The best alphavalue is also taken from the normalized PMIexperiments (Table 5), which is nearly .5.
Underthis setting, this penalization term is exactly thedenominator of the [M18] Odds Ratio.
Table 9below show our experiment results in derivingthe penalization term for LVCs.37AM VPCs LVCs Mixed?b ?c .565 .453 .5461/bc .502 .532 .502[M18] Odds ratio .443 .567 .456Table 9.
AP performance of suggested LVCs?penalization terms and AMs.6 ConclusionsWe have conducted an analysis of the 82 AMsassembled in Pecina and Schlesinger (2006) forthe tasks of English VPC and LVC extractionover the Wall Street Journal Penn Treebank data.In our work, we have observed that AMs can bedivided into two classes: ones that do not usecontext (Class I) and ones that do (Class II), andfind that the latter is not suitable for our VPC andLVC detection tasks as the size of our corpus istoo small to rely on the frequency of candidates?contexts.
This phenomenon also revealed theinappropriateness of hypothesis tests for ourdetection task.
We have also introduced thenovel notion of rank equivalence to MWEdetection, in which we show that complex AMsmay be replaced by simpler AMs that yield thesame average precision performance.We further observed that certain modificationsto some AMs are necessary.
First, in the contextof ranking, we have proposed normalizing scoresproduced by MI and PMI in cases where thedistributions of the two events are markedlydifferent, as is the case for light verbs andparticles.
While our claims are limited to thedatasets analyzed, they show clearimprovements: normalized PMI produces betterperformance over our mixed MWE dataset,yielding an average precision of 58.8%compared to 51.5% when using standard PMI, asignificant improvement as judged by paired Ttest.
Normalized MI also yields the bestperformance over our LVC dataset with asignificantly improved AP of 58.3%.We also show that marginal frequencies canbe used to form effective penalization terms.
Inparticular, we find that (1 )b c?
??
+ ?
?
is a goodpenalization term for VPCs, while (1 )b c?
??
issuitable for LVCs.
Our introduced alpha tuningparameter should be set to properly scale thevalues b and c, and should be optimized perMWE type.
In cases where a common factor isapplied to different MWE types, max(b, c) is abetter choice than min(b, c).
In future work, weplan to expand our investigations over larger,web-based datasets of English, to verify theperformance gains of our modified AMs.AcknowledgementThis work was partially supported by a NationalResearch Foundation grant ?Interactive MediaSearch?
(grant # R 252 000 325 279).ReferencesBaldwin, Timothy (2005).
The deep lexicalacquisition of English verb-particle constructions.Computer Speech and Language, SpecialIssue on Multiword Expressions, 19(4):398?414.Baldwin, Timothy and Villavicencio, Aline (2002).Extracting the unextractable: A case study on verb-particles.
In Proceedings of the 6th Conferenceon Natural Language Learning (CoNLL-2002),pages 98?104, Taipei, Taiwan.Daille, B?atrice (1994).
Approche mixte pourl'extraction automatique de terminologie:statistiques lexicales et filtres linguistiques.PhD thesis, Universit?
Paris 7.Evert, Stefan (2004).
Online repository of associationmeasures http://www.collocations.de/, acompanion to The Statistics of WordCooccurrences: Word Pairs and Collocations.Ph.D.
dissertation, University of Stuttgart.Evert, Stefan and Krenn, Brigitte (2001) Methods forqualitative evaluation of lexical associationmeasures.
In Proceedings of the 39th AnnualMeeting of the Association of ComputationalLinguistics, pages 188-915, Toulouse, France.Katz, Graham and Giesbrecht, Eugenie (2006).Automatic identification of non-compositionalmulti-word expressions using latent semanticanalysis.
In Proceedings of the ACL Workshopon Multiword Expressions: Identifying andExploiting Underlying Properties, pages 12-19,Sydney, Australia.Krenn, Brigitte and Evert, Stefan (2001).
Can we dobetter than frequency?
A case study on extractingPP-verb collocations.
In Proceedings of theACL/EACL 2001 Workshop on theComputational Extraction, Analysis andExploitation of Collocations, pages 39?46,Toulouse, France.Manning D. Christopher and Schutze, && Hinrich(1999).
Foundations of Statistical NaturalLanguage Processing.
The MIT Press, Cambridge,Massachusetts.Pearce, Darren (2002).
A comparative evaluation ofcollocation extraction techniques.
In Proc.
of the383rd International Conference on LanguageResources and Evaluation (LREC 2002), LasPalmas, pages 1530-1536, Canary Islands.Pecina, Pavel and Schlesinger, Pavel (2006).Combining association measures for collocationextraction.
In Proceedings of the 21thInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics(COLING/ACL 2006), pages 651-658, Sydney,Australia.Quirk Randolph, Greenbaum Sidney, Leech Geoffreyand Svartvik Jan (1985).
A ComprehensiveGrammar of the English Language.
Longman,London, UK.Ramisch Carlos, Schreiner Paulo, Idiart Marco andVillavicencio Aline (2008).
An Evaluation ofMethods for the extraction of MultiwordExpressions.
In Proceedings of the LREC-2008Workshop on Multiword Expressions:Towards a Shared Task for MultiwordExpressions, pages 50-53, Marrakech, Morocco.Smadja, Frank (1993).
Retrieving collocations fromtext: Xtract.
Computational Linguistics 19(1):143?77.Tan, Y.
Fan, Kan M. Yen and Cui, Hang (2006).Extending corpus-based identification of light verbconstructions using a supervised learningframework.
In Proceedings of the EACL 2006Workshop on Multi-word-expressions in amultilingual context, pages 49?56, Trento, Italy.Zhai, Chengxiang (1997).
Exploiting context toidentify lexical atoms ?
A statistical view oflinguistic context.
In International andInterdisciplinary Conference on Modellingand Using Context (CONTEXT-97), pages 119-129, Rio de Janeiro, Brazil.Appendix A.
List of particles used inidentifying verb particle constructions.about,  aback,  aboard,  above,  abroad,  across,  adrift,ahead,  along,  apart,  around,  aside,  astray,  away,back,  backward,  backwards,  behind, by, down,forth,  forward, forwards, in,  into,  off,  on,  out,  over,past,  round,  through, to, together, under, up,  upon,without.39
