Proceedings of NAACL-HLT 2013, pages 938?946,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsAdaptation of Reordering Models for Statistical Machine TranslationBoxing Chen, George Foster and Roland KuhnNational Research Council Canadafirst.last@nrc-cnrc.gc.caAbstractPrevious research on domain adaptation (DA)for statistical machine translation (SMT) hasmainly focused on the translation model (TM)and the language model (LM).
To the best ofour knowledge, there is no previous work onreordering model (RM) adaptation for phrase-based SMT.
In this paper, we demonstratethat mixture model adaptation of a lexical-ized RM can significantly improve SMT per-formance, even when the system already con-tains a domain-adapted TM and LM.
We findthat, surprisingly, different training corporacan vary widely in their reordering character-istics for particular phrase pairs.
Furthermore,particular training corpora may be highly suit-able for training the TM or the LM, but unsuit-able for training the RM, or vice versa, so mix-ture weights for these models should be esti-mated separately.
An additional contributionof the paper is to propose two improvementsto mixture model adaptation: smoothing thein-domain sample, and weighting instancesby document frequency.
Applied to mixtureRMs in our experiments, these techniques (es-pecially smoothing) yield significant perfor-mance improvements.1 IntroductionA phrase-based statistical machine translation(SMT) system typically has three main components:a translation model (TM) that contains informationabout how to translate word sequences (phrases)from the source language to the target language,a language model (LM) that contains informationabout probable word sequences in the target lan-guage, and a reordering model (RM) that indicateshow the order of words in the source sentence islikely to influence the order of words in the targetsentence.
The TM and the RM are trained on paralleldata, and the LM is trained on target-language data.Usage of language and therefore the best translationpractice differs widely across genres, topics, and di-alects, and even depends on a particular author?s orpublication?s style; the word ?domain?
is often usedto indicate a particular combination of all these fac-tors.
Unless there is a perfect match between thetraining data domain and the (test) domain in whichthe SMT system will be used, one can often get bet-ter performance by adapting the system to the testdomain.In offline domain adaptation, the system is pro-vided with a sample of translated sentences fromthe test domain prior to deployment.
In a popularvariant of offline adaptation, linear mixture modeladaptation, each training corpus is used to gener-ate a separate model component that forms part ofa linear combination, and the sample is used to as-sign a weight to each component (Foster and Kuhn,2007).
If the sample resembles some of the corporamore than others, those corpora will receive higherweights in the combination.Previous research on domain adaptation for SMThas focused on the TM and the LM.
Such researchis easily motivated: translations across domains areunreliable.
For example, the Chinese translationof the English word ?mouse?
would most likely be?laoshu ???
if the topic is the animal; if the topicis computer hardware, its translation would most938likely be ?shubiao???.
However, when the trans-lation is for people in Taiwan, even when the topicis computer hardware, its translation would morelikely be ?huashu ???.
It is intuitively obviouswhy TM and LM adaptation would be helpful here.By contrast, it is not at all obvious that RM modeladaptation will improve SMT performace.
Onewould expect reordering behaviour to be characteris-tic of a particular language pair, but not of particulardomains.
At most, one might think that reorderingis lexicalized?perhaps, (for instance) in translatingfrom Chinese to English, or from Arabic to English,there are certain words whose English translationstend to undergo long-distance movement from theiroriginal positions, while others stay close to theiroriginal positions.
However, one would not expecta particular Chinese adverb or a particular Arabicnoun to undergo long-distance movement when be-ing translated into English in one domain, but not inothers.
Nevertheless, that is what we observe: seesection 5 below.This paper shows that RM adaptation improvesthe performance of our phrase-based SMT system.In our implementation, the RM is adapted by meansof a linear mixture model, but it is likely that otherforms of RM adaptation would also work.
We ob-tain even more effective RM adaptation by smooth-ing the in-domain sample and by weighting orienta-tion counts by the document frequency of the phrasepair.
Both improvements could be applied to the TMor the LM as well, though we have not done so.Finally, the paper analyzes reordering to see whyRM adaptation works.
There seem to be two fac-tors at work.
First, the reordering behaviour ofwords and phrases often differs dramatically fromone bilingual corpus to another.
Second, there arecorpora (for instance, comparable corpora and bilin-gual lexicons) which may contain very valuable in-formation for the TM, but which are poor sourcesof RM information; RM adaptation downweights in-formation from these corpora significantly, and thusimproves the overall quality of the RM.2 Reordering ModelIn early SMT systems, such as (Koehn, 2004),changes in word order when a sentence is trans-lated were modeled by means of a penalty that is in-curred when the decoder chooses, as the next sourcephrase to be translated, a phrase that does not imme-diately follow the previously translated source sen-tence.
Thus, the system penalizes deviations frommonotone order, with the magnitude of the penaltybeing proportional to distance in the source sentencebetween the end of the previously translated sourcephrase and the start of the newly chosen sourcephrase.Many SMT systems, including our own, still usethis distance-based penalty as a feature.
However,starting with (Tillmann and Zhang, 2005; Koehnet al 2005), a more sophisticated type of reorder-ing model has often been adopted as well, and hasyielded consistent performance gains.
This type ofRM typically identifies three possible orientationsfor a newly chosen source phrase: monotone (M),swap (S), and discontinuous (D).
The M orientationoccurs when the newly chosen phrase is immedi-ately to the right of the previously translated phrasein the source sentence, the S orientation occurs whenthe new phrase is immediately to the left of the pre-vious phrase, and the D orientation covers all othercases.1 This type of RM is lexicalized: the estimatedprobabilities of M, S and D depend on the source-language and target-language words in both the pre-vious phrase pair and the newly chosen one.Galley and Manning (2008) proposed a ?hierar-chical?
lexicalized RM in which the orientation (M,S, or D) is determined not by individual phrase pairs,but by blocks.
A block is the largest contiguous se-quence of phrase pairs that satisfies the phrase pairconsistency requirement of having no external links.Thus, classification of the orientation of a newlychosen phrase as M, S, or D is carried out as if thedecoder always chose the longest possible sourcephrase in the past, and will choose the longest pos-sible source phrase in the future.The RM used in this paper is hierarchical and lex-icalized.
For a given phrase pair (f , e), we estimatethe probabilities that it will be in an M, S, or D ori-entation o with respect to the previous phrase pairand the following phrase pair (two separate distri-butions).
Orientation counts c(o, f, e) are obtainedfrom a word-aligned corpus using the method de-1Some researchers have distinguished between left and rightversions of the D orientation, but this 4-orientation scheme hasnot yielded significant gains over the 3-orientation one.939scribed in (Cherry et al 2012), and correspondingprobabilities p(o|f, e) are estimated using recursiveMAP smoothing:p(o|f, e) =c(o, f, e) + ?f p(o|f) + ?e p(o|e)c(f, e) + ?f + ?ep(o|f) =c(o, f) + ?g p(o)c(f) + ?gp(o) =c(o) + ?u/3c(?)
+ ?u, (1)where p(o|e) is defined analogously to p(o|f), andthe four smoothing parameters ?e, ?f , ?g, and ?uare set to values that minimize the perplexity of theresulting model on held-out data.During decoding, orientations with respect to theprevious context are obtained from a shift-reduceparser, and orientations with respect to followingcontext are approximated using the coverage vector(Cherry et al 2012).3 RM Adaptation3.1 Linear mixture modelFollowing previous work (Foster and Kuhn, 2007;Foster et al 2010), we adopt the linear mixturemodel technique for RM adaptation.
This techniquetrains separate models for each training corpus, thenlearns weights for each of the models and combinesthe weighted component models into a single model.If we have N sub-corpora, the global reorderingmodel probabilities p(o|f, e) are computed as in (2):p(o|f, e) =N?i=1?i pi(o|f, e) (2)where pi(o|f, e) is the reordering model trained onsub-corpus i, and ?i is its weight.Following (Foster et al 2010), we use the EMalgorithm to learn the weights that maximize theprobability of phrase-pair orientations in the devel-opment set (in-domain data):??
= argmax??o,f,ep?
(o, f, e) logN?i=1?i pi(o|f, e)(3)where p?
(o, f, e) is the empirical distribution ofcounts in the dev set (proportional to c(o, f, e)).
Twoseparate sets of mixing weights are learned: one forthe distribution with respect to the previous phrasepair, and one for the next phrase pair.3.2 Development set smoothingIn Equation 3, p?
(o, f, e) is extracted from the in-domain development set.
Since dev sets for SMTsystems are typically small (1,000-3,000 sentences),we apply smoothing to this RM.
We first obtaina smoothed conditional distribution p(o|f, e) usingthe MAP technique described above, then multiplyby the empirical marginal p?
(e, f) to obtain a finalsmoothed joint distribution p(o, f, e).There is nothing about this idea that limits it tothe RM: smoothing could be applied to the statisticsin the dev that are used to estimate a mixture TMor LM, in order to mitigate over-fitting.
However,we note that, compared to the TM, the over-fittingproblem is likely to be more acute for the RM, sinceit splits counts for each phrase pair into three cate-gories.3.3 Document-frequency weightingMixture models, like the RM in this paper, dependon the existence of multiple training corpora, witheach sub-corpus nominally representing a domain.A recent paper suggests that some phrase pairs be-long to general language, while others are domain-specific (Foster et al 2010).
If a phrase pair existsin all training corpora, it probably belongs to generallanguage; on the other hand, if it appears in onlyone or two training corpora, it is more likely to bedomain-specific.We were interested in seeing whether informationabout domain-specificity could improve the estima-tion of mixture RM weights.
The intuition is thatphrase pairs that belong to general language shouldcontribute more to determining sub-corpus weights,since they are the ones whose reordering behaviouris most likely to shift with domain.
To capture thisintuition, we multiplied the empirical distribution in(3) by the following factor, inspired by the standarddocument-frequency formula:D(f, e) = log(DF (f, e) +K), (4)where DF (f, e) is the number of sub-corporathat (f, e) appears in, and K is an empirically-determined smoothing term.940corpus # segs # en tok % genresfbis 250K 10.5M 3.7 nwfinancial 90K 2.5M 0.9 financialgale bc 79K 1.3M 0.5 bcgale bn 75K 1.8M 0.6 bn nggale nw 25K 696K 0.2 nwgale wl 24K 596K 0.2 wlhkh 1.3M 39.5M 14.0 Hansardhkl 400K 9.3M 3.3 legalhkn 702K 16.6M 5.9 nwisi 558K 18.0M 6.4 nwlex&ne 1.3M 2.0M 0.7 lexiconothers nw 146K 5.2M 1.8 nwsinorama 282K 10.0M 3.5 nwun 5.0M 164M 58.2 unTOTAL 10.1M 283M 100.0 (all)devtesttune 1,506 161K nw wlNIST06 1,664 189K nw bn ngNIST08 1,357 164K nw wlTable 1: NIST Chinese-English data.
In the gen-res column: nw=newswire, bc=broadcast conversa-tion, bn=broadcast news, wl=weblog, ng=newsgroup,un=United Nations proceedings.4 Experiments4.1 Data settingWe carried out experiments in two different settings,both involving data from NIST Open MT 2012.2The first setting uses data from the Chinese to En-glish constrained track, comprising 283M Englishtokens.
We manually identified 14 sub-corpora onthe basis of genres and origins.
Table 1 summarizesthe statistics and genres of all the training corporaand the development and test sets; for the trainingcorpora, we show their size in number of words asa percentage of all training data.
Most training cor-pora consist of parallel sentence pairs.
The isi andlex&ne corpora are exceptions: the former is ex-tracted from comparable data, while the latter is alexicon that includes many named entities.
The de-velopment set (tune) was taken from the NIST 2005evaluation set, augmented with some web-genre ma-terial reserved from other NIST corpora.2http://www.nist.gov/itl/iad/mig/openmt12.cfmcorpus # segs # en toks % genresgale bc 57K 1.6M 3.3 bcgale bn 45K 1.2M 2.5 bngale ng 21K 491K 1.0 nggale nw 17K 659K 1.4 nwgale wl 24K 590K 1.2 wlisi 1,124K 34.7M 72.6 nwother nw 224K 8.7M 18.2 nwTOTAL 1,512K 47.8M 100.0 (all)devtestNIST06 1,664 202K nw wlNIST08 1,360 205K nw wlNIST09 1,313 187K nw wlTable 2: NIST Arabic-English data.
In the gen-res column: nw=newswire, bc=broadcast conversation,bn=broadcase news, ng=newsgroup, wl=weblog.The second setting uses NIST 2012 Arabic to En-glish data, but excluding the UN data.
There areabout 47.8 million English running words in thesetraining data.
We manually grouped the training datainto 7 groups according to genre and origin.
Ta-ble 2 summarizes the statistics and genres of all thetraining corpora and the development and test sets.Note that for this language pair, the comparable isidata represent a large proportion of the training data:72% of the English words.
We use the evaluationsets from NIST 2006, 2008, and 2009 as our devel-opment set and two test sets, respectively.4.2 SystemExperiments were carried out with an in-housephrase-based system similar to Moses (Koehn et al2007).
The corpus was word-aligned using IBM2,HMM, and IBM4 models, and the phrase table wasthe union of phrase pairs extracted from these sepa-rate alignments, with a length limit of 7.
The trans-lation model was smoothed in both directions withKN smoothing (Chen et al 2011).
The DF smooth-ing term K in equation 4 was set to 0.1 using held-out optimization.
We use the hierarchical lexical-ized RM described above, with a distortion limit of7.
Other features include lexical weighting in bothdirections, word count, a distance-based RM, a 4-gram LM trained on the target side of the paralleldata, and a 6-gram English Gigaword LM.
The sys-941system Chinese Arabicbaseline 31.7 46.8baseline+loglin 29.6 45.9RMA 31.8 47.7**RMA+DF 32.2* 47.9**RMA+dev smoothing 32.3* 48.3**RMA+dev smoothing+DF 32.8** 48.2**Table 3: Results for variants of RM adaptation.system Chinese ArabicLM+TM adaptation 33.2 47.7+RMA+dev-smoothing+DF 33.5 48.4**Table 4: RM adaptation improves over a baseline con-taining adapted LMs and TMs.tem was tuned with batch lattice MIRA (Cherry andFoster, 2012).4.3 ResultsFor our main baseline, we simply concatenate alltraining data.
We also tried augmenting this withseparate log-linear features corresponding to sub-corpus-specific RMs.
Our metric is case-insensitvieIBM BLEU-4 (Papineni et al 2002); we reportBLEU scores averaged across both test sets.
Follow-ing (Koehn, 2004), we use the bootstrap-resamplingtest to do significance testing.
In tables 3 to 5, *and ** denote significant gains over the baseline atp < 0.05 and p < 0.01 levels, respectively.Table 3 shows that reordering model adaptationhelps in both data settings.
Adding either document-frequency weighting (equation 4) or dev-set smooth-ing makes the improvement significant in both set-tings.
Using both techniques together yields highlysignificant improvements.Our second experiment measures the improve-ment from RM adaptation over a baseline thatincludes adapted LMs and TMs.
We use thesame technique?linear mixtures with EM-tunedweights?to adapt these models.
Table 4 shows thatadapting the RM gives gains over this strong base-line for both language pairs; improvements are sig-nificant in the case of Arabic to English.The third experiment breaks down the gains in thelast line of table 4 by individual adapted model.
Asshown in table 5, RM adaptation yielded the largestsystem Chinese Arabicbaseline 31.7 46.8LM adaptation 32.1* 47.0TM adaptation 33.0** 47.5**RM adaptation 32.8** 48.2**Table 5: Comparison of LM, TM, and RM adaptation.improvement on Arabic, while TM adaptation didbest on Chinese.
Surprisingly, both methods sig-nificantly outperformed LM adaptation, which onlyachieved significant gains over the baseline for Chi-nese.5 AnalysisWhy does RM adaptation work?
Intuitively, onewould think that reordering behaviour for a givenphrase pair should not be much affected by domain,making RM adaptation pointless.
That is probablywhy (as far as we know) no-one has tried it before.In this section, we describe three factors that accountfor at least part of the observed gains.5.1 Weighting by corpus qualityOne answer to the above question is that some cor-pora are better for training RMs than others.
Fur-thermore, corpora that are good for training the LMor TM are not necessarily good for training the RM,and vice versa.
Tables 6 and 7 illustrate this.
Theselist the weights assigned to various sub-corpora forLM, TM, and RM mixture models.The weights assigned to the isi sub-corpus in par-ticular exhibit a striking pattern.
These are high inthe LM mixtures, moderate in the TM mixtures, andvery low in the RM mixtures.
When one considersthat isi contains 72.6% of the English words in theArabic training data (see table 2), its weight of 0.01in the RM mixture is remarkable.On reflection, it makes sense that EM would as-sign weights in the order it does.
The isi corpusconsists of comparable data: sentence pairs whosesource- and target-language sides are similar, but of-ten not mutual translations.
These are a valuablesource of in-domain n-grams for the LM; a some-what noisy source of in-domain phrase pairs for theTM; and an unreliable source of re-ordering patternsfor the RM.
Figure 1 shows this.
Although the two942LM TM RMisi (0.23) un (0.29) un (0.21)gale nw (0.11) fbis (0.15) gale nw (0.13)un (0.11) hkh (0.10) lex&ne (0.12)sino.
(0.09) gale nw (0.09) hkh (0.08)fbis (0.08) gale bn (0.07) fbis (0.08)fin.
(0.07) oth nw (0.06) gale bn (0.08)oth nw (0.07) sino.
(0.06) gale wl (0.06)gale bn (0.07) isi (0.05) gale bc (0.06)gale wl (0.06) hkn (0.04) hkn (0.04)hkh (0.06) fin.
(0.04) fin.
(0.04)hkn (0.03) gale bc (0.03) oth nw (0.03)gale bc (0.02) gale wl (0.02) hkl (0.03)lex&ne (0.00) lex&ne (0.00) isi (0.01)hkl (0.00) hkl (0.00) sino.
(0.01)Table 6: Chinese-English sub-corpora for LM, TM, andRM mixture models, ordered by mixture weight.LM TM RMisi (0.41) isi (0.35) gale bc (0.21)oth nw (0.19) oth nw (0.29) gale ng (0.20)gale ng (0.15) gale bc (0.10) gale nw (0.20)gale wl (0.09) gale ng (0.08) oth nw (0.13)gale nw (0.07) gale bn (0.07) gale ng (0.12)gale bc (0.05) gale nw (0.07) gale wb (0.11)gale bn (0.02) gale wl (0.05) isi (0.01)Table 7: Arabic-English sub-corpora for LM, TM, andRM mixture models, ordered by mixture weight.sides of the comparable data are similar, they givethe misleading impression that the phrases labeled1, 2, 3 in the Chinese source should be reordered as2, 3, 1 in English.
We show a reference translationof the Chinese source (not found in the comparabledata) that reorders the phrases as 1, 3, 2.Thus, RM adaptation allows the RM to learn thatcertain corpora whose reordering information is oflower quality corpora should have lower weights.The optimal weights for corpora inside an RM maybe different from the optimal weights inside a TM orLM.5.2 Weighting by domain matchSo is this all that RM adaptation does: downweightpoor-quality data?
We believe there is more toRM adaptation than that.
Specifically, even if oneREF: The American list of goods that would incur tariffs in retaliation would certainly not be accepted by the Chinese government.
SRC: ??
(1) ?
??
???
??
(2) ??
??
?
??
?(3)?
TGT: And the Chinese(2) side would certainly not accept(3)  the unreasonable demands put forward by the Americans(1) concerning the protection of intellectual property rights .Figure 1: Example of sentence pair from comparabledata; underlined words with the same number are trans-lations of each otherCorpus M S D Countfbis 0.50 0.07 0.43 685financial 0.32 0.28 0.41 65gale bc 0.60 0.10 0.31 50gale bn 0.47 0.15 0.37 109gale nw 0.51 0.05 0.44 326gale wl 0.42 0.26 0.32 52hkh 0.29 0.23 0.48 130hkl 0.28 0.16 0.56 263hkn 0.30 0.27 0.43 241isi 0.24 0.16 0.60 240lex&ne 0.94 0.03 0.02 1others nw 0.29 0.16 0.55 23sinorama 0.44 0.07 0.49 110un 0.37 0.10 0.53 15dev 0.46 0.24 0.31 11Table 8: Orientation frequencies for the phrase pair ???
immediately?, with respect to the previous phrase.considers only high-quality data for training RMs(ignoring comparable data, etc.)
one sees differ-ences in reordering behaviour between different do-mains.
This isn?t just because of differences in wordfrequencies between domains, because we observedomain-dependent differences in reordering for thesame phrase pair.
Two examples are given below:one Chinese-English, one Arabic-English.Table 8 shows reordering data for the phrasepair ???
immediately?
in various corpora.
No-tice the strong difference in behaviour between thethree Hong Kong corpora?hkh, hkl and hkn?andsome of the other corpora, for instance fbis.
In the943Corpus M S D Countgale bc 0.50 0.27 0.22 233gale bn 0.56 0.21 0.23 226gale ng 0.51 0.13 0.37 295gale nw 0.47 0.20 0.33 167gale wl 0.56 0.18 0.26 127isi 0.50 0.06 0.44 5502other nw 0.50 0.16 0.34 1450dev 0.75 0.12 0.13 52Table 9: Orientation frequencies for the phrase pair?work AlEml?
with respect to the previous phrase.Hong Kong corpora, immediately is much less likely(probability of around 0.3) to be associated with amonotone (M) orientation than it is in fbis (proba-bility of 0.5).
This phrase pair is relatively frequentin both corpora, so this disparity seems too great tobe due to chance.Table 9 shows reordering behaviour for the phrasepair ?work AlEml?3 across different sub-corpora.As in the Chinese example, there appear to be sig-nificant differences in reordering patterns for cer-tain corpora.
For instance, gale bc swaps this well-attested phrase pair twice as often (probability of0.27) as gale ng (probability of 0.13).For Chinese, it is possible that dialect plays a rolein reordering behaviour.
In theory, Mandarin Chi-nese is a single language which is quite different,especially in spoken form, from other languages ofChina such as Cantonese, Hokkien, Shanghainese,and so on.
In practice, many speakers of Mandarinmay be unconsciously influenced by other languagesthat they speak, or by other languages that they don?tspeak but that have an influence over people they in-teract with frequently.
Word order can be affectedby this: the Mandarin of Mainland China, HongKong and Taiwan sometimes has slightly differentword order.
Hong Kong Mandarin can be somewhatinfluenced by Cantonese, and Taiwan Mandarin byHokkien.
For instance, if a verb is modified by anadverb in Mandarin, the standard word order is ?ad-verb verb?.
However, since in Cantonese, ?verb ad-verb?
is a more common word order, speakers andwriters of Mandarin in Hong Kong may adopt the3We represent the Arabic word AlEml in its Buckwaltertransliteration.   Figure 2: An example of different word ordering in Man-darin from different area.
?verb adverb?
order in that language as well.
Figure2 shows how a different word order in the Mandarinsource affects reordering when translating into En-glish.
Perhaps in situations where different trainingcorpora represent different dialects, RM adaptationinvolves an element of dialect adaptation.
We are ea-ger to test this hypothesis for Arabic?different di-alects of Arabic are much more different from eachother than dialects of Mandarin, and reordering isoften one of the differences?but we do not have ac-cess to Arabic training, dev, and test data in whichthe dialects are clearly separated.It is possible that RM adaptation also has an el-ement of genre adaptation.
We have not yet beenable to confirm or refute this hypothesis.
However,whatever is causing the corpus-dependent reorder-ing patterns for particular phrase pairs shown in thetwo tables above, it is clear that they may explainthe performance improvements we observe for RMadaptation in our experiments.5.3 Penalizing highly-specific phrase pairsIn section 3.3 we described our strategy for givinggeneral (high document-frequency) phrase pairs thatoccur in the dev set more influence in determiningmixing weights.
An artifact of our implementationapplies a similar strategy to the probability estimatesfor all phrase pairs in the model.
This is that 0 prob-abilities are assigned to all orientations whenever aphrase pair is absent from a particular sub-corpus.Thus, for example, a pair (f, e) that occurs onlyin sub-corpus iwill receive a probability p(o|f, e) =?i pi(o|f, e) in the mixture model (equation 2).Since ?i ?
1, this amounts to a penalty on pairsthat occur in few sub-corpora, especially ones withlow mixture weights.The resulting mixture model is deficient (non-944normalized), but easy to fix by backing off to aglobal distribution such as p(o) in equation 1.
How-ever, we found that this ?fix?
caused large drops inperformance, for instance from the Arabic BLEUscore of 48.3 reported in table 3 to 46.0.
We there-fore retained the original strategy, which can be seenas a form of instance weighting.
Moreover, it is onethat is particularly effective in the RM, since, com-pared to a similar strategy in the TM (which we alsoemploy), it applies to whole phrase pairs and resultsin much larger penalties.6 Related workDomain adaptation is an active topic in the NLP re-search community.
Its application to SMT systemshas recently received considerable attention.
Previ-ous work on SMT adaptation has mainly focusedon translation model (TM) and language model(LM) adaptation.
Approaches that have been triedfor SMT model adaptation include mixture models,transductive learning, data selection, data weighting,and phrase sense disambiguation.Research on mixture models has considered bothlinear and log-linear mixtures.
Both were studiedin (Foster and Kuhn, 2007), which concluded thatthe best approach was to combine sub-models ofthe same type (for instance, several different TMsor several different LMs) linearly, while combiningmodels of different types (for instance, a mixtureTM with a mixture LM) log-linearly.
(Koehn andSchroeder, 2007), instead, opted for combining thesub-models directly in the SMT log-linear frame-work.In transductive learning, an MT system trained ongeneral domain data is used to translate in-domainmonolingual data.
The resulting bilingual sentencepairs are then used as additional training data (Ueff-ing et al 2007; Chen et al 2008; Schwenk, 2008;Bertoldi and Federico, 2009).Data selection approaches (Zhao et al 2004; Lu?et al 2007; Moore and Lewis, 2010; Axelrod etal., 2011) search for bilingual sentence pairs that aresimilar to the in-domain ?dev?
data, then add themto the training data.
The selection criteria are typi-cally related to the TM, though the newly found datawill be used for training not only the TM but also theLM and RM.Data weighting approaches (Matsoukas et al2009; Foster et al 2010; Huang and Xiang, 2010;Phillips and Brown, 2011; Sennrich, 2012) use arich feature set to decide on weights for the train-ing data, at the sentence or phrase pair level.
Forinstance, a sentence from a corpus whose domain isfar from that of the dev set would typically receivea low weight, but sentences in this corpus that ap-pear to be of a general nature might receive higherweights.The 2012 JHU workshop on Domain Adapta-tion for MT 4 proposed phrase sense disambiguation(PSD) for translation model adaptation.
In this ap-proach, the context of a phrase helps the system tofind the appropriate translation.All of the above work focuses on either TM orLM domain adaptation.7 ConclusionsIn this paper, we adapt the lexicalized reorderingmodel (RM) of an SMT system to the domain inwhich the system will operate using a mixture modelapproach.
Domain adaptation of translation mod-els (TMs) and language models (LMs) has becomecommon for SMT systems, but to our knowledgethis is the first attempt in the literature to adapt theRM.
Our experiments demonstrate that RM adap-tation can significantly improve translation quality,even when the system already has TM and LM adap-tation.
We also experimented with two modifica-tions to linear mixture model adaptation: dev setsmoothing and weighting orientation counts withdocument frequency of phrase pairs.
Both ideasare potentially applicable to TM and LM adaptation.Dev set smoothing, in particular, seems to improvethe performance of RM adaptation significantly.
Fi-nally, we investigate why RM adaptation helps SMTperformance.
Three factors seem to be important:downweighting information from corpora that areless suitable for modeling reordering (such as com-parable corpora), dialect/genre effects, and implicitinstance weighting.4http://www.clsp.jhu.edu/workshops/archive/ws-12/groups/dasmt945ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011.Domain adaptation via pseudo in-domain data selec-tion.
In EMNLP 2011.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translation withmonolingual resources.
In Proceedings of the 4thWorkshop on Statistical Machine Translation, Athens,March.
WMT.Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.2008.
Exploiting n-best hypotheses for smt self-enhancement.
In ACL 2008.Boxing Chen, Roland Kuhn, George Foster, and HowardJohnson.
2011.
Unpacking and transforming featurefunctions: New ways to smooth phrase tables.
In MTSummit 2011.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InNAACL 2012.Colin Cherry, Robert C. Moore, and Chris Quirk.
2012.On hierarchical re-ordering and permutation parsingfor phrase-based decoding.
In WMT 2012.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the ACL Work-shop on Statistical Machine Translation, Prague, June.WMT.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adapta-tion in statistical machine translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), Boston.Michel Galley and C. D. Manning.
2008.
A simpleand effective hierarchical phrase reordering model.
InEMNLP 2008, pages 848?856, Hawaii, October.Fei Huang and Bing Xiang.
2010.
Feature-rich discrimi-native phrase rescoring for SMT.
In COLING 2010.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine transla-tion.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, pages 224?227, Prague,Czech Republic, June.
Association for ComputationalLinguistics.P.
Koehn, A. Axelrod, A.
B. Mayne, C. Callison-Burch,M.
Osborne, D. Talbot, and M. White.
2005.
Edin-burgh system description for the 2005 NIST MT eval-uation.
In Proceedings of Machine Translation Evalu-ation Workshop.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In ACL 2007, Demon-stration Session.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of the 6th Conference of the As-sociation for Machine Translation in the Americas,Georgetown University, Washington D.C., October.Springer-Verlag.Yajuan Lu?, Jin Huang, and Qun Liu.
2007.
ImprovingStatistical Machine Translation Performance by Train-ing Data Selection and Optimization.
In Proceedingsof the 2007 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), Prague, CzechRepublic.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight estima-tion for machine translation.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), Singapore.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In ACL2010.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of Machine Translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 311?318, Philadel-phia, July.
ACL.Aaron B. Phillips and Ralf D. Brown.
2011.
Trainingmachine translation with a second-order taylor approx-imation of weighted translation instances.
In MT Sum-mit 2011.Holger Schwenk.
2008.
Investigations on large-scale lightly-supervised training for statistical machinetranslation.
In IWSLT 2008.Rico Sennrich.
2012.
Mixture-modeling with unsuper-vised clusters for domain adaptation in statistical ma-chine translation.
In EACL 2012.Christoph Tillmann and Tong Zhang.
2005.
A localizedprediction model for statistical machine translation.
InProceedings of the 43th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), Ann Ar-bor, Michigan, July.
ACL.Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.2007.
Transductive learning for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL), Prague, Czech Republic, June.
ACL.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Proceed-ings of the International Conference on ComputationalLinguistics (COLING) 2004, Geneva, August.946
