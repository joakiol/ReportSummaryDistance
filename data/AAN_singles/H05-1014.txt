Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 105?112, Vancouver, October 2005. c?2005 Association for Computational LinguisticsNovelty Detection: The TREC ExperienceIan Soboroff and Donna HarmanNational Institute of Standards and TechnologyGaithersburg, MD(ian.soboroff,donna.harman)@nist.govAbstractA challenge for search systems is to de-tect not only when an item is relevant tothe user?s information need, but also whenit contains something new which the userhas not seen before.
In the TREC noveltytrack, the task was to highlight sentencescontaining relevant and new information ina short, topical document stream.
Thisis analogous to highlighting key parts of adocument for another person to read, andthis kind of output can be useful as input toa summarization system.
Search topics in-volved both news events and reported opin-ions on hot-button subjects.
When peo-ple performed this task, they tended to se-lect small blocks of consecutive sentences,whereas current systems identified manyrelevant and novel passages.
We also foundthat opinions are much harder to track thanevents.1 IntroductionThe problem of novelty detection has long been a sig-nificant one for retrieval systems.
The ?selective dis-semination of information?
(SDI) paradigm assumedthat the people wanted to be able to track new in-formation relating to known topics as their primarysearch task.
While most SDI and information filter-ing systems have focused on similarity to a topicalprofile (Robertson, 2002) or to a community of userswith a shared interest (Belkin and Croft, 1992), re-cent efforts (Carbonell and Goldstein, 1998; Allan etal., 2000; Kumaran et al, 2003) have looked at theretrieval of specifically novel information.The TREC novelty track experiments were con-ducted from 2002 to 2004 (Harman, 2002; Soboroffand Harman, 2003; Soboroff, 2004).
The basic taskwas defined as follows: given a topic and an orderedset of documents related to that topic, segmentedinto sentences, return those sentences that are bothrelevant to the topic and novel given what has al-ready been seen previously in that document set.This task models an application where a user is skim-ming a set of documents, and the system highlightsnew, on-topic information.There are two problems that participants mustsolve in this task.
The first is identifying relevantsentences, which is essentially a passage retrievaltask.
Sentence retrieval differs from document re-trieval because there is much less text to work with,and identifying a relevant sentence may involve ex-amining the sentence in the context of those sur-rounding it.
The sentence was specified as the unitof retrieval in order to standardize the task across avariety of passage retrieval approaches, as well as tosimplify the evaluation.The second problem is that of identifying those rel-evant sentences that contain new information.
Theoperational definition of ?new?
here is informationthat has not appeared previously in this topic?s setof documents.
In other words, we allow the system toassume that the user is most concerned about find-ing new information in this particular set of docu-ments, and is tolerant of reading information he al-ready knows because of his background knowledge.Since each sentence adds to the user?s knowledge,and later sentences are to be retrieved only if theycontain new information, novelty retrieval resemblesa filtering task.Novelty is an inherently difficult phenomenon tooperationalize.
Document-level novelty detection,while intuitive, is rarely useful because nearly ev-ery document contains something new, particularlywhen the domain is news.
Hence, our decision touse sentences as the unit of retrieval.
Moreover, de-termining ground truth for a novelty detection taskis more difficult than for topical relevance, becauseone is forced not only to face the idiosyncratic na-105ture of relevance, but also to rely all the more onthe memory and organizational skills of the assessor,who must try and remember everything he has read.We wanted to determine if people could accomplishthis task to any reasonable level of agreement, as wellas to see what computational approaches best solvethis problem.2 Input DataThe first year of the novelty track (Harman, 2002)was a trial run in several ways.
First, this was a newtask for the community and participating groups hadno training data or experience.
But second, it wasunclear how humans would perform this task andtherefore creating the ?truth?
data was in itself alarge experiment.
NIST decided to minimize the costby using 50 old topics from TRECs 6, 7, and 8.The truth data was created by asking NIST asses-sors (the humans performing this task) to identifythe set of relevant sentences from each relevant doc-ument and then from that set of relevant sentences,mark those that were novel.
Specifically, the asses-sors were instructed to identify a list of sentencesthat were:1. relevant to the question or request made in thedescription section of the topic,2.
their relevance was independent of any sur-rounding sentences,3.
they provided new information that had notbeen found in any previously picked sentences.Most of the NIST assessors who worked on thistask were not the ones who created the original top-ics, nor had they selected the relevant documents.This turned out to be a major problem.
The as-sessors?
judgments for the topics were remarkablein that only a median of 2% of the sentences werejudged to be relevant, despite the documents them-selves being relevant.
As a consequence, nearly ev-ery relevant sentence (median of 93%) was declarednovel.
This was due in large part to assessor dis-agreement as to relevancy, but also that fact thatthis was a new task to the assessors.
Additionally,there was an encouragement not to select consecu-tive sentences, because the goal was to identify rel-evant and novel sentences minimally, rather than totry and capture coherent blocks of text which couldstand alone.
Unfortunately, this last instruction onlyserved to confuse the assessors.
Data from 2002 hasnot been included in the rest of this paper, nor aregroups encouraged to use that data for further ex-periments because of these problems.In the second year of the novelty track (Soboroffand Harman, 2003), the assessors created their ownnew topics on the AQUAINT collection of three con-temporaneous newswires.
For each topic, the asses-sor composed the topic and selected twenty-five rele-vant documents by searching the collection.
Once se-lected, the documents were ordered chronologically,and the assessor marked the relevant sentences andthose relevant sentences that were novel.
No instruc-tion or limitation was given to the assessors concern-ing selection of consecutive sentences, although theywere told that they did not need to choose an other-wise irrelevant sentence in order to resolve a pronounreference in a relevant sentence.
Each topic was in-dependently judged by two different assessors, thetopic author and a ?secondary?
assessor, so that theeffects of different human judgments could be mea-sured.
The judgments of the primary assessor wereused as ground truth for evaluation, and the sec-ondary assessor?s judgments were taken to representa ceiling for system performance in this task.Another new feature of the 2003 data set was a di-vision of the topics into two types.
Twenty-eight ofthe fifty topics concerned events such as the bombingat the 1996 Olympics in Atlanta, while the remain-ing topics focused on opinions about controversialsubjects such as cloning, gun control, and same-sexmarriages.
The topic type was indicated in the topicdescription by a <toptype> tag.This pattern was repeated for TREC 2004 (Sobo-roff, 2004), with fifty new topics (twenty-five eventsand twenty-five opinion) created in a similar man-ner and with the same document collection.
For2004, assessors also labeled some documents as irrel-evant, and irrelevant documents up through the firsttwenty-five relevant documents were included in thedocument sets distributed to the participants.
Theseirrelevant documents were included to increase the?noise?
in the data set.
However, the assessors onlyjudged sentences in the relevant documents, since,by the TREC standard of relevance, a document isconsidered relevant if it contains any relevant infor-mation.3 Task DefinitionThere were four tasks in the novelty track:Task 1.
Given the set of documents for the topic,identify all relevant and novel sentences.Task 2.
Given the relevant sentences in all docu-ments, identify all novel sentences.Task 3.
Given the relevant and novel sentences inthe first 5 documents only, find the relevant106and novel sentences in the remaining documents.Note that since some documents are irrelevant,there may not be any relevant or novel sentencesin the first 5 documents for some topics.Task 4.
Given the relevant sentences from all doc-uments, and the novel sentences from the first5 documents, find the novel sentences in the re-maining documents.These four tasks allowed the participants to testtheir approaches to novelty detection given differentlevels of training: none, partial, or complete rele-vance information, and none or partial novelty infor-mation.The test data for a topic consisted of the topicstatement, the set of sentence-segmented documents,and the chronological order for those documents.
Fortasks 2-4, training data in the form of relevant andnovel ?sentence qrels?
were also given.
The data wasreleased and results were submitted in stages to limit?leakage?
of training data between tasks.
Dependingon the task, the system was to output the identifiersof sentences which the system determined to containrelevant and/or novel relevant information.4 EvaluationBecause novelty track runs report their relevant andnovel sentences as an unranked set, traditional mea-sures of ranked retrieval effectiveness such as meanaverage precision can?t be used.
One alternative isto use set-based recall and precision.
Let M be thenumber of matched sentences, i.e., the number ofsentences selected by both the assessor and the sys-tem, A be the number of sentences selected by theassessor, and S be the number of sentences selectedby the system.
Then sentence set recall is R = M/Aand precision is P = M/S.However, set-based recall and precision do not av-erage well, especially when the assessor set sizes Avary widely across topics.
Consider the following ex-ample as an illustration of the problems.
One topichas hundreds of relevant sentences and the systemretrieves 1 relevant sentence.
The second topic has 1relevant sentence and the system retrieves hundredsof sentences.
The average for both recall and preci-sion over these two topics is approximately .5 (thescores on the first topic are 1.0 for precision and es-sentially 0.0 for recall, and the scores for the secondtopic are the reverse), even though the system didprecisely the wrong thing.
While most real systemswouldn?t exhibit this extreme behavior, the fact re-mains that set recall and set precision averaged overa set of topics is not a robust diagnostic indicator0.0 0.2 0.4 0.6 0.8 1.0Recall0.00.20.40.60.81.0Precision0.10.20.30.40.50.60.70.80.9F Value5152535455565758596061626364656667686970 717273747576777879 80818283848586878889909192939495 96979899100Figure 1: The F measure, plotted according to itsprecision and recall components.
The lines show con-tours at intervals of 0.1 points of F. The black num-bers are per-topic scores for one TREC system.of system performance.
There is also the problemof how to define precision when the system returnsno sentences (S = 0).
Leaving that topic out of theevaluation for that run would mean that differentsystems would be evaluated over different numbersof topics.
The standard procedure is to define preci-sion to be 0 when S = 0.To avoid these problems, the primary measureused in the novelty track was the F measure.
TheF measure (which is itself derived from van Rijsber-gen?s E measure (van Rijsbergen, 1979)) is a functionof set recall and precision, together with a parameter?
which determines the relative importance of recalland precision:F = (?2 + 1)PR?2P + RA ?
value of 1, indicating equal weight, is used inthe novelty track:F?=1 =2PRP + RAlternatively, this can be formulated asF?=1 =2?
(# relevant retrieved)(# retrieved) + (# relevant)For any choice of ?, F lies in the range [0, 1], andthe average of the F measure is meaningful even whenthe judgment sets sizes vary widely.
For example,the F measure in the scenario above is essentially0, an intuitively appropriate score for such behavior.Using the F measure also deals with the problem of107what to do when the system returns no sentencessince recall is 0 and the F measure is legitimately 0regardless of what precision is defined to be.Note, however, that two runs with equal F scoresdo not indicate equal precision and recall.
The con-tour lines in Figure 1 illustrate the shape of the Fmeasure in recall-precision space.
An F score of 0.5,for example, can describe a range of precision and re-call scores.
Figure 1 also shows the per-topic scoresfor a particular TREC run.
It is easy to see thattopics 98, 83, 82, and 67 exhibit a wide range of per-formance, but all have an F score of close to 0.6.Thus, two runs with equal F scores may be perform-ing quite differently, and a difference in F scores canbe due to changes in precision, recall, or both.
Inpractice, if F is used, precision and recall should alsobe examined, and we do so in the analysis which fol-lows.5 Analysis5.1 Analysis of truth dataSince the novelty task requires systems to automat-ically select the same sentences that were selectedmanually by the assessors, it is important to ana-lyze the characteristics of the manually-created truthdata in order to better understand the system re-sults.
Note that the novelty task is both a passageretrieval task, i.e., retrieve relevant sentences, anda novelty task, i.e., retrieve only relevant sentencesthat contain new information.In terms of the passage retrieval part, the TRECnovelty track was the first major investigation intohow users select relevant parts of documents.
Thisleads to several obvious questions, such as what per-centage of the sentences are selected as relevant, anddo these sentences tend to be adjacent/consecutive?Additionally, what kinds of variation appear, bothacross users and across topics.
Table 1 shows themedian percentage of sentences that were selectedas relevant, and what percentage of these sentenceswere consecutive.
Since each topic was judged by twoassessors, it also shows the percentage of sentencesselected by assessor 1 (the ?official?
assessor used inscoring) that were also selected by assessor 2.
Thetable gives these percentages for all topics and alsobroken out into the two types of topics (events andopinions).First, the table shows a large variation across thetwo years.
The group in 2003 selected more rele-vant sentences (almost 40% of the sentences were se-lected as relevant), and in particular selected manyconsecutive sentences (over 90% of the relevant sen-tences were adjacent).
The median length of a stringof consecutive sentences was 2; the mean was 4.252sentences.
The following year, a different group ofassessors selected only about half as many relevantsentences (20%), with fewer consecutive sentences.This variation across years may reflect the group ofassessors in that the 2004 set were TREC ?veterans?and were more likely to be very selective in terms ofwhat was considered relevant.The table also shows a variation across topics, inparticular between topics asking about events versusthose asking about opinions.
The event topics, forboth years, had more relevant sentences, and moreconsecutive sentences (this effect is more apparent in2004).Agreement between assessors on which sentenceswere relevant was fairly close to what is seen in docu-ment relevance tasks.
There was slightly more agree-ment in 2003, but there were also many more relevantsentences so the likelihood of a match was higher.There is more agreement on events than on opinions,partially for the same reason, but also because thereis generally less agreement on what constitutes anopinion.
These medians hide a wide range of judgingbehavior across the assessors, particularly in 2003.The final two rows of data in the table show themedians for novelty.
There are similar patterns tothose seen in the relevant sentence data, with the2003 assessors clearly being more liberal in judging.However, the pattern is reversed for topic types, withmore sentences being considered relevant and novelfor the opinion topics than for the event topics.
Theagreement on novelty is less than on relevance, par-ticularly in 2004 where there were smaller numbersof novel and relevant sentences selected.Another way to look at agreement is with thekappa statistic (Cohen, 1960).
Kappa computeswhether two assessors disagree, with a correction for?chance agreement?
which we would expect to occurrandomly.
Kappa is often interpreted as the degreeof agreement between assessors, although this inter-pretation is not well-defined and varies from fieldto field (Di Eugenio, 2000).
For relevant sentencesacross all topics in the 2004 data set, the kappa valueis 0.549, indicating statistically significant agreementbetween the assessors but a rather low-to-moderatedegree of agreement by most scales of interpretation.Given that agreement is usually not very high forrelevance judgments (Voorhees, 1998), this is as ex-pected.5.2 Analysis of participants resultsMost groups participating in the 2004 novelty trackemployed a common approach, namely to measurerelevance as similarity to the topic and novelty as1082003 2004Relevant all topics 0.39 0.20events only 0.47 0.25opinions only 0.38 0.15Consecutive all topics 0.91 0.70events only 0.93 0.85opinions only 0.91 0.65Relevant all topics 0.69 0.60agreement events only 0.82 0.68opinions only 0.63 0.50Novelty all topics 0.68 0.40events only 0.61 0.38opinions only 0.73 0.42Novelty all topics 0.56 0.35agreement events only 0.65 0.45opinions only 0.48 0.29Table 1: Median fraction of sentences which wererelevant and novel, fraction of consecutive relevantsentences, and proportion of agreement by the sec-ondary assessor.dissimilarity to past sentences.
On top of this frame-work the participants used a wide assortment ofmethods which may be broadly categorized into sta-tistical and linguistic methods.
Statistical methodsincluded using traditional retrieval models such astf.idf and Okapi coupled with a threshold for retriev-ing a relevant or novel sentence, expansion of thetopic and/or document sentences using dictionariesor corpus-based methods, and using named entitiesas features.
Some groups also used machine learningalgorithms such as SVMs in parts of their detectionprocess.
Semantic methods included deep parsing,matching discourse entities, looking for particularverbs and verb phrases in opinion topics, coreferenceresolution, normalization of named entities, and inone case manual construction of ontology?s for topic-specific concepts.Figure 2 shows the Task 1 results for the top runfrom each group in TREC 2004.
Groups employingstatistical approaches include UIowa, CIIR, UMich,and CDVP.
Groups employing more linguistic meth-ods include CLR, CCS, and LRI.
THU and ICT tooka sort of kitchen-sink approach where each of theirruns in each task tried different techniques, mostlystatistical.The F scores for both relevance and novelty re-trieval are fairly uniform, and they are dominated bythe precision component.
The top scoring systems byF score are largely statistical in nature; for example,see (Abdul-Jaleel et al, 2004) (CIIR) and (Eichmannet al, 2004) (UIowa).
CLR (Litkowski, 2004) andTask 1scoreUIowa04Nov11THUIRnv0411ICTOKAPIOVLPumich0411cdvp4QePnD2IRITT3CIIRT1R2clr04n1h2ISIRUN404HIL10ccs3fqrt1NTU11LRIaze20.20.40.60.8o o o o o o o o o o o o o+ + + + + + + + + + + + +* *** * * * ******newFPrecisionRecallHuman performance, FPrecisionRecallo+*0.20.40.60.8o o o o o o o o o o o o o+ + + + + + + + + + + ++* * *** *** * ****relevantrel new0.6 0.410.6 0.40.67 0.54Figure 2: Task 1 precision, recall, and F scores forthe top run from each group in TREC 2004LRI (Amrani et al, 2004), which use much strongerlinguistic processing, achieve the highest precision atthe expense of recall.
Overall, precision is quite lowand recall is high, implying that most systems areerring in favor of retrieving many sentences.A closer comparison of the runs among them-selves and to the truth data confirms this hypothe-sis.
While the 2004 assessors were rather selective inchoosing relevant and novel sentences, often selectingjust a handful of sentences from each document, thesystems were not.
The systems retrieved an averageof 49.5% of all sentences per topic as relevant, com-pared to 19.2% chosen by the assessor.
Furthermore,the runs chose 41% of all sentences (79% of their ownrelevant sentences) as novel, compared to the asses-sor who selected only 8.4%.
While these numbersare a very coarse average that ignores differences be-tween the topics and between the documents in eachset, it is a fair summary of the data.
Most of the sys-tems called nearly every sentence relevant and novel.By comparison, the person attempting this task (thesecond assessor, scored as a run and shown as hor-izontal lines in Figure 2) was much more effectivethan the systems.The lowest scoring run in this set, LRIaze2, actu-ally has the highest precision for both relevant and109novel sentences.
The linguistics-driven approach ofthis group included standardizing acronyms, build-ing a named-entity lexicon, deep parsing, resolvingcoreferences, and matching concepts to manually-built, topic-specific ontologies (Amrani et al, 2004).A close examination of this run?s pattern shows thatthey retrieved very few sentences, in line with theamounts chosen by the assessor.
They were not of-ten the correct sentences, which accounts for the lowrecall, but by not retrieving too many false alarms,they managed to achieve a high precision.Our hypothesis here is that the statistical systems,which are essentially using algorithms designed fordocument retrieval, approached the sentences withan overly-broad term model.
The majority of thedocuments in the data set are relevant, and so manyof the topic terms are present throughout the docu-ments.
However, the assessor was often looking fora finer-grained level of information than what ex-ists at the document level.
For example, topic N51is concerned with Augusto Pinochet?s arrest in Lon-don.
High-quality content terms such as Pinochet,Chile, dictator, torture, etc appear in nearly everysentence, but the key relevant ones ?
which are veryfew ?
are those which specifically talk about the ar-rest.
Most systems flagged nearly every sentence asrelevant, when the topic was much narrower than thedocuments themselves.One explanation for this may be in how thresholdswere learned for this task.
Since task 1 provides nodata beyond the topic statement and the documentsthemselves, it is possible that systems were tunedto the 2003 data set where there are more relevantsentences.
However, this isn?t the whole story, sincethe difference in relevant sentences between 2003 and2004 is not so huge that it can explain the rates of re-trieval seen here.
Additionally, in task 3 some topic-specific training data was provided, and yet the ef-fectiveness of the systems was essentially the same.Of those systems that tried a more fine-grainedapproach, it appears that it is complicated to learnexactly which sentences contain the relevant informa-tion.
For example, nearly every system had troubleidentifying relevant opinion sentences.
One mightexpect that those systems which analyzed sentencestructure more closely would have done better here,but there is essentially no difference.
Identifying rel-evant information at the sentence level is a very hardproblem.We see very similar results for novel sentence re-trieval.
Rather than looking at task 1, where systemsretrieved novel from their own selection of relevantsentences, it?s better to look at runs in task 2 (Fig-ure 3).
Since in this task the systems are given all rel-   fffiflffiffififfifi!""ffffi# $fi%&#'fi%ffi!
'fi (#)fi%fifi*+ff,--(fi.-/'fifi"'!fi021 3021 4021 5021 677 7 777 777 7777888888889888:88;;;;;;;;;;;;9;<=?>A@CBED FED7HGIJ@CBLKEM MIJ@CNPOQ>GKEM ML>P@EM @SREKGNUTQ<WVXEY ZL[L[IJ@CNPOQ>GKEM ML>P@EM @SREKGNUTE=\V]XEY ^`_78;Figure 3: Task 2 scores for the top run from eachgroup in TREC 2004evant sentences and just search for novelty, the base-line performance for comparison is just labeling allthe sentences as novel.
Most systems, surprisingly in-cluding the LRI run, essentially do retrieve nearly ev-ery sentence as novel.
The horizontal lines show thebaseline performance; the baseline recall is 1.0 and isat the top of the Y axis.
All the runs except clr04n2are just above this baseline, with cdvp4NTerFr1 andnovcolrcl the most discriminating.The approach of Dublin City University(cdvp4NTerFr1) is essentially to set a thresh-old on the tf.idf value of the unique words in thegiven sentence, but their other methods which incor-porate the history of unique terms and the differencein sentence frequencies between the current andpast sentences perform comparably (Blott et al,2004).
Similarly, Columbia University (novcolrcl)focuses on previously unseen words in the currentsentence as the main evidence of novelty (Schiffmanand McKeown, 2004).
As opposed to the ad hocthreshold in the DCU system, Columbia employsa hill-climbing approach to learning the threshold.This particular run is optimized for recall; anotheroptimized for precision achieved the highest preci-sion of all task 2 runs, but with very low recall.
Ingeneral, we conclude that most systems achievinghigh scores in novelty detection are recall-orientedand as a result still provide the user with too muchinformation.As was mentioned above, opinion topics provedmuch harder than events.
Every system but one didbetter on event topics than on opinions in task 1110 	  fffiflffi!
"#$%"&ffi !'
()*' +%*!!,-./0&0%12fl#/!.
""!$.34&0"".ff 51&0/ff4"",6 7fi8"94:0;fl*3<#=fi&03">#?
@A>&0""BDC EBDC FBDC GBDC HIKJLDMDI BC EBC FBC GBC HN KMIOII PFigure 4: F scores for event and opinion topics intask 1.
(Figure 4).
In task 2, where all relevant sentenceswere provided, many runs do as well or better onopinion topics than events.
Thus, the complexity foropinions is more in finding which sentences containthem, than determining which opinions are novel.6 ConclusionThe novelty track in TREC examined a particularkind of novelty detection, that is, finding novel, on-topic sentences within documents that the user isreading.
Both statistical and linguistic techniques, aswell as filtering and learning approaches can be ap-plied to detecting novel relevant information withindocuments, but nevertheless it is a hard problem forseveral reasons.
First, because the unit of interestis a sentence, there is not a lot of data in each uniton which to base the decision.
When the documentas a whole is relevant, techniques designed for docu-ment retrieval seem unable to make fine distinctionsabout which sentences within the document containthe relevant information.
Initial threshold setting iscritical and difficult.When we examined human performance on thistask, it is clear that users do make very fine distinc-tions.
Looking particularly at the 2004 set of relevantand novel sentences, less than 20% of the sentencesin relevant documents were marked as relevant, andonly 40% of those (or 8% of the total sentences) weremarked as both relevant and novel.The TREC novelty data sets themselves supportsome interesting uses outside of the novelty track.Whereas the data from 2002 is clearly flawed andshould not be used, the data from 2003 and 2004can be regarded as valid samples of user input interms of relevant sentence selection, and further re-duction of those sentences to those presenting newinformation.
One obvious use is in the passage re-trieval arena, e.g., using the relevant sentences fortesting passage retrieval, either at the single sentencelevel or using the consecutive sentences to test whento retrieve multiple sentences.
A second use is forsummarization, where the relevant AND novel sen-tences can serve as the truth data for the extractionphase (and then compressed in some manner).
Otheruses of the data include manual analysis of user be-havior when processing documents in response to aquestion, or looking further into the user agreementissues, particularly in the summarization area.The novelty data is also unique in that it delib-erately contains a mix of topics on events and onopinions regarding controversial subjects.
The opin-ions topics are quite different in this regard thanother TREC topics, which have historically focusedon events or narrative information on a subject orperson.
This exploration has been an interesting andfruitful one.
By mixing the two topic types withineach task, we see that identifying opinions withindocuments is hard, even with training data, whiledetecting new opinions (given relevance) seems anal-ogous to detecting new information about an event.ReferencesNasreen Abdul-Jaleel, James Allan, W. Bruce Croft,Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D.Smucker, and Courtney Wade.
2004.
UMassat TREC 2004: Novelty and HARD.
In Pro-ceedings of the Thirteenth Text REtrieval Confer-ence (TREC 2004), http://trec.nist.gov/pub/trec13/papers/umass.novelty.hard.pdf.James Allan, Victor Lavrenko, and Hubert Jin.2000.
First story detection in TDT is hard.
InProceedings of the Ninth International Confer-ence on Information and Knowledge Management(CIKM 2000), pages 374?381.Ahmed Amrani, Je?ro?me Aze?, Thomas Heitz, YvesKodratoff, and Mathieu Roche.
2004.
Fromthe texts to the concepts they contain: achain of linguistic treatments.
In Proceed-ings of the Thirteenth Text REtrieval Confer-111ence (TREC 2004), http://trec.nist.gov/pub/trec13/papers/uparis.novelty2.pdf.Nicholas J. Belkin and W. Bruce Croft.
1992.
In-formation filtering and information retrieval: Twosides of the same coin?
Communications of theACM, 35(12):29?38, December.Stephen Blott, Oisin Boydell, Fabrice Camous,Paul Ferguson, Georgina Gauhan, Cathal Gur-rin, Gareth J. F. Jones, Noel Murphy, NoelO?Connor, Alan F. Smeaton, Barry Smyth,and Peter Wilkins.
2004.
Experiments interabyte searching, genomic retrieval and nov-elty detection for TREC-2004.
In Proceed-ings of the Thirteenth Text REtrieval Confer-ence (TREC 2004), http://trec.nist.gov/pub/trec13/papers/dcu.tera.geo.novelty.pdf.Jaime G. Carbonell and Jade Goldstein.
1998.
Theuse of MMR, diversity-based reranking for reorder-ing documents and producing summaries.
In Pro-ceedings of the 21st Annual International Confer-ence on Research and Development in InformationRetrieval (SIGIR ?98), Melbourne, Australia, Au-gust, pages 335?336.
ACM Press.J.
A. Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20:37?46.Barbara Di Eugenio.
2000.
On the usage of kappato evaluate agreement on coding tasks.
In Pro-ceedings of the Second International Conferenceon Language Resources and Evaluatation (LREC2000), Athens, Greece, pages 441?446.David Eichmann, Yi Zhang, Shannon Bradshaw,Xin Ying Qui, Li Zhou, Padmini Srinivasan,Aditya Kumar Sehgal, and Hudon Wong.
2004.Novelty, question answering and genomics: theUniversity of Iowa response.
In Proceed-ings of the Thirteenth Text REtrieval Confer-ence (TREC 2004), http://trec.nist.gov/pub/trec13/papers/uiowa.novelty.qa.geo.pdf.Donna Harman.
2002.
Overview of the TREC 2002novelty track.
In Proceedings of the Eleventh TextREtrieval Conference (TREC 2002), NIST SpecialPublication 500-251, pages 46?55, Gaithersburg,MD, November.Girindhar Kumaran, James Allan, and Andrew Mc-Callum.
2003.
Classification models for new eventdetection.
Technical Report IR-362, CIIR, Uni-versity of Massachusetts, Amherst.Kenneth C. Litkowski.
2004.
Evolving XMLand dictionary strategies for question answeringand novelty tasks.
In Proceedings of the Thir-teenth Text REtrieval Conference (TREC 2004),http://trec.nist.gov/pub/trec13/papers/clresearch.qa.novelty.pdf.Stephen E. Robertson.
2002.
Introduction to thespecial issue: Overview of the TREC routing andfiltering tasks.
Information Retrieval, 5:127?137.Barry Schiffman and Kathleen R. McKeown.2004.
Columbia university in the novelty trackat TREC 2004.
In Proceedings of the Thir-teenth Text REtrieval Conference (TREC 2004),http://trec.nist.gov/pub/trec13/papers/columbiau.novelty.pdf.Ian Soboroff and Donna Harman.
2003.
Overviewof the TREC 2003 novelty track.
In Proceed-ings of the Twelfth Text REtrieval Conference(TREC 2003), NIST Special Publication 500-255,Gaithersburg, MD, November.Ian Soboroff.
2004.
Overview of the TREC2004 novelty track.
In Proceedings of the Thir-teenth Text REtrieval Conference (TREC 2004),http://trec.nist.gov/pub/trec13/papers/NOVELTY.OVERVIEW.pdf.C.
J. van Rijsbergen.
1979.
Information Retrieval.Butterworths.Ellen M. Voorhees.
1998.
Variations in relevancejudgments and the measurement of retrieval effec-tiveness.
In Proceedings of the 21st Annual Inter-national Conference on Research and Developmentin Information Retrieval (SIGIR ?98), Melbourne,Australia, August, pages 315?323.
ACM Press.112
