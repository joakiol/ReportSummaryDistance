Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 55,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsClueless: Explorations in unsupervised, knowledge-lean extraction oflexical-semantic informationInvited TalkLillian LeeDepartment of Computer Science, Cornell Universityllee@cs.cornell.eduI will discuss two current projects on automatically extracting certain types of lexical-semanticinformation in settings wherein we can rely neither on annotations nor existing knowledge resourcesto provide us with clues.
The name of the game in such settings is to find and leverage auxiliary sourcesof information.Why is it that if you know I?ll give a silly talk, it follows that you know I?ll give a talk, whereas if youdoubt I?ll give a good talk, it doesn?t follow that you doubt I?ll give a talk?
This pair of examplesshows that the word ?doubt?
exhibits a special but prevalent kind of behavior known as downwardentailingness ?
the licensing of reasoning from supersets to subsets, so to speak, but not vice versa.
Thefirst project I?ll describe is to identify words that are downward entailing, a task that promises to enhancethe performance of systems that engage in textual inference, and one that is quite challenging since it isdifficult to characterize these items as a class and no corpus with downward-entailingness annotationsexists.
We are able to surmount these challenges by utilizing some insights from the linguistics literatureregarding the relationship between downward entailing operators and what are known as negative polarityitems ?
words such as ?ever?
or the idiom ?have a clue?
that tend to occur only in negative contexts.A cross-linguistic analysis indicates some potentially interesting connections to findings in linguistictypology.That previous paragraph was quite a mouthful, wasn?t it?
Wouldn?t it be nice if it were written in plainEnglish that was easier to understand?
The second project I?ll talk about, which has the eventual aim tomake it possible to automatically simplify text, aims to learn lexical-level simplifications, such as ?worktogether?
for ?collaborate?.
(This represents a complement to prior work, which focused on syntactictransformations, such as passive to active voice.)
We exploit edit histories in Simple English Wikipediafor this task.
This isn?t as simple (ahem) as it might at first seem because Simple English Wikipedia andthe usual Wikipedia are far from a perfect parallel corpus and because many edits in Simple Wikipediado not constitute simplifications.
We consider both explicitly modeling different kinds of operations andvarious types of bootstrapping, including as clues the comments Wikipedians sometimes leave when theyedit.Joint work with Cristian Danescu-Niculescu-Mizil, Bo Pang, and Mark Yatskar.55
