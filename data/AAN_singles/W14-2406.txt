Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 28?32,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsCombining Formal and Distributional Models of Temporal andIntensional SemanticsMike LewisSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKmike.lewis@ed.ac.ukMark SteedmanSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKsteedman@inf.ed.ac.ukAbstractWe outline a vision for computational se-mantics in which formal compositionalsemantics is combined with a powerful,structured lexical semantics derived fromdistributional statistics.
We consider howexisting work (Lewis and Steedman, 2013)could be extended with a much richerlexical semantics using recent techniquesfor modelling processes (Scaria et al.,2013)?for example, learning that visit-ing events start with arriving and end withleaving.
We show how to closely inte-grate this information with theories of for-mal semantics, allowing complex compo-sitional inferences such as is visiting?hasarrived in but will leave, which requiresinterpreting both the function and contentwords.
This will allow machine readingsystems to understand not just what hashappened, but when.1 Combined Distributional and LogicalSemanticsDistributional semantics aims to induce the mean-ing of language from unlabelled text.
Traditionalapproaches to distributional semantics have repre-sented semantics in vector spaces (Baroni et al.,2013).
Words are assigned vectors based on col-locations in large corpora, and then these vectorsa composed into vectors representing longer utter-ances.
However, so far there is relatively limitedempirical evidence that composed vectors provideuseful representations for whole sentences, and itis unclear how to represent logical operators (suchas universal quantifiers) in a vector space.
Whilefuture breakthroughs may overcome these limita-tions, there are already well developed solutions inthe formal semantics literature using logical rep-resentations.
On the other hand, standard for-mal semantic approaches such as Bos (2008) havefound that hand-built ontologies such as Word-Net (Miller, 1995) provide an insufficient modelof lexical semantics, leading to low recall on appli-cations.
The complementary strengths and weak-nesses of formal and distributional semantics mo-tivate combining them into a single model.In Lewis and Steedman (2013), we proposeda solution to these problems which uses CCG(Steedman, 2012) as a model of formal semantics,making it straightforward to build wide-coveragelogical forms.
Hand built representations areadded for a small number of function words suchas negatives and quantifiers?but the lexical se-mantics is represented by first clustering predi-cates (based on their usage in large corpora), andthen using the cluster-identifiers as symbols in thelogical form.
For example, the induced CCG lexi-con might contain entries such as the following1:write ` (S\NP)/NP: ?y?x?e.rel43(x, y, e)author `N/PPof: ?y?x?e.rel43(x, y, e)Equivalent sentences like Shakespeare wroteMacbeth and Shakespeare is the author ofMacbeth can then both be mapped to arel43(shakespeare,macbeth) logical form, us-ing derivations such as:Shakespeare wrote MacbethNP (S\NP)/NP NPshakespeare ?y?x?e.rel43(x, y, e) macbeth>S\NP?x?e.rel43(x,macbeth, e)<S?e.rel43(shakespeare,macbeth, e)This approach interacts seamlessly with stan-dard formal semantics?for example modellingnegation by mapping Francis Bacon didn?t writeMacbeth to ?rel43(francis bacon,macbeth).Their method has shown good performance on adataset of multi-sentence textual inference prob-lems involving quantifiers, by using first-order the-1The e variables are Davidsonian event variables.28orem proving.
Ambiguity is handled by a proba-bilistic model, based on the types of the nouns.Beltagy et al.
(2013) use an alternative approachwith similar goals, in which every word instanceexpresses a unique semantic primitive, but is con-nected to the meanings of other word instances us-ing distributionally-derived probabilistic inferencerules.
This approach risks requiring very largenumber of inference rules, which may make infer-ence inefficient.
Our approach avoid this problemby attempting to fully represent lexical semanticsin the lexicon.2 ProposalWe propose how our previous model could be ex-tended to make more sophisticated inferences.
Wewill demonstrate how many interesting problemsin semantics could be solved with a system basedon three components:?
A CCG syntactic parse for modelling com-position.
Using CCG allows us to handle in-teresting forms of composition, such as co-ordination, extraction, questions, right noderaising, etc.
CCG also has both a developedtheory of operator semantics and a transpar-ent interface to the underlying predicate ar-gument structure.?
A small hand built lexicon for wordswith complex semantics?such as negatives,quantifiers, modals, and implicative verbs.?
A rich model of lexical semantics de-rived from distributionally-induced entail-ment graphs (Berant et al., 2011), extendedwith subcategories of entailment relations ina similar way to Scaria et al.
(2013).
We showhow such graphs can be converted into a CCGlexicon.2.1 Directional InferenceA major limitation of our previous model isthat it uses a flat clustering to model themeaning of content words.
This method en-ables them to model synonymy relations be-tween words, but not relations where the en-tailment only holds in one direction?for ex-ample, conquers?invades, but not vice-versa.This problem can be addressed using the en-tailment graph framework introduced by Berantet al.
(2011), which learns globally consistentgraphs over predicates in which directed edgesindicate entailment relations.
Exactly the samemethods can be used to build entailment graphsover the predicates derived from a CCG parse:1attackarg0,arg12invadearg0,arg1invasionposs,of3conquerarg0,arg1annexarg0,arg14bombarg0,arg1The graph can then be converted to a CCG lexi-con by making the semantics of a word be the con-junction of all the relation identifiers it implies inthe graph.
For example, the above graph is equiv-alent to the following lexicon:attack ` (S\NP)/NP: ?x?y?e.rel1(x, y, e)bomb ` (S\NP)/NP: ?x?y?e.rel1(x, y, e)?rel4(x, y, e)invade ` (S\NP)/NP: ?x?y?e.rel1(x, y, e)?rel2(x, y, e)conquer` (S\NP)/NP: ?x?y?e.rel1(x, y, e) ?rel2(x, y, e) ?
rel3(x, y, e)This lexicon supports the correct infer-ences, such as conquers?attacks and didn?tinvade?didn?t conquer.2.2 Temporal SemanticsOne case where combining formal and distribu-tional semantics may be particularly helpful is ingiving a detailed model of temporal semantics.
Arich understanding of time would allow us to un-derstand when events took place, or when stateswere true.
Most existing work ignores tense, andwould treat the expressions used to be presidentand is president either as equivalent or completelyunrelated.
Failing to model tense would lead to in-correct inferences when answering questions suchas Who is the president of the USA?Another motivation for considering a detailedmodel of temporal semantics is that understandingthe time of events should improve the quality ofthe distributional clustering.
It has recently beenshown that such information is extremely usefulfor learning equivalences between predicates, bydetermining which sentences describe the same29events using date-stamped text and simple tenseheuristics (Zhang and Weld, 2013).
Such meth-ods escape common problems with traditional ap-proaches to distributional similarity, such as con-flating causes with effects, and may prove veryuseful for building entailment graphs.Temporal information is conveyed by both byauxiliary verbs such as will or used to, and inthe semantics of content words.
For example, thestatement John is visiting Baltimore licences en-tailments such as John has arrived in Baltimoreand John will leave Baltimore, which can only beunderstood through both knowledge of tense andlexical semantic relations.The requisite information about lexical seman-tics could be represented by labelling edges in theentailment graphs, along the lines of Scaria et al.(2013).
Instead of edges simply representing en-tailment, they should represent different kinds oflexical relations, such as precondition or conse-quence.
Building such graphs requires trainingclassifiers that predict fine-grained semantic rela-tions between predicates, and defining transitivityproperties of the relations (e.g.
a precondition of aprecondition is a precondition).
For example, thesystem might learn the following graph:1visitarg0,arg13leavearg0,arg1exitarg0,arg1departarg0,from2arrivearg0,inreacharg0,arg1initiated by terminated byBy defining a simple mapping between edge la-bels and logical forms, this graph can be convertedto CCG lexical entries such as:visit ` (S\NP)/NP: ?y?x?e.rel1(x, y, e) ??e?
[rel2(x, y, e?)?
before(e, e?)]??e??
[rel3(x, y, e??)
?
after(e, e??
)]arrive ` (S\NP)/PPin: ?y?x?e.rel2(x, y, e)leave ` (S\NP)/NP: ?y?x?e.rel3(x, y, e)These lexical entries could be complementedwith hand-built interpretations for a small set ofcommon auxiliary verbs:has ` (S\NP)/(Sb\NP): ?p?x?e.before(r, e) ?
p(x, e)will ` (S\NP)/(Sb\NP): ?p?x?e.after(r, e) ?
p(x, e)is ` (S\NP)/(Sng\NP): ?p?x?e.during(r, e) ?
p(x, e)used ` (S\NP)/(Sto\NP): ?p?x?e.before(r, e) ?
p(x, e) ???e?
[during(r) ?
p(x, e?
)]Here, r is the reference time (e.g.
the time thatthe news article was written).
It is easy to verifythat such a lexicon supports inferences such as isvisiting?will leave, has visited?has arrived in,or used to be president?is not president.The model described here only discusses tense,not aspect?so does not distinguish between Johnarrived in Baltimore and John has arrived in Bal-timore (the latter says that the consequences of hisarrival still hold?i.e.
that he is still in Baltimore).Going further, we could implement the much moredetailed proposal of Moens and Steedman (1988).Building this model would require distinguishingstates from events?for example, the semantics ofarrive, visit and leave could all be expressed interms of the times that an is in state holds.2.3 Intensional SemanticsSimilar work could be done by subcatego-rizing edges in the graph with other lexi-cal relations.
For example, we could ex-tend the graph with goal relations betweenwords, such as between set out for and ar-rive in, search and find, or invade and conquer:1set outarg0,forheadarg0,to2arrivearg0,inreacharg0,arg1goalThe corresponding lexicon contains entries suchas:set out ` (S\NP)/PPfor: ?y?x?e.rel1(x, y, e) ??e?
[goal(e, e?)
?
rel2(x, y, e?
)]The modal logic  operator is used to mark thatthe goal event is a hypothetical proposition, thatis not asserted to be true in the real world?soColumbus set out for India6?Columbus reachedIndia.
The same mechanism allows us to handleMontague (1973)?s example that John seeks a uni-corn does not imply the existence of a unicorn.Just as temporal information can be expressedby auxiliary verbs, relations such as goals can30Columbus failed to reach India<Sdcl/(Sdcl\NP ) (Sdcl\NP )/(Sto\NP ) (Sto\NP )/(Sb\NP ) Sb\NP?p.p(Columbus) ?p?x?e.
 ?e?
[p(x, e?)
?
goal(e?, e)] ?
??e??
[p(x, e??)]
?p?x?e.p(x, e) ?x?e.rel2(x, India, e)>Sto\NP?x?e.rel2(x, India, e)>Sdcl\NP?x?e.
 ?e?
[rel2(x, India, e?)
?
goal(e?, e)] ?
??e??
[rel2(x, India, e??)]>Sdcl?e.
 ?e?
[rel2(Columbus, India, e?)
?
goal(e?, e)] ?
??e??
[rel2(Columbus, India, e??
)]Figure 1: Output from our system for the sentence Columbus failed to reach Indiabe expressed using implicative verbs like try orfail.
As the semantics of implicative verbs is of-ten complex (Karttunen, 1971), we propose hand-coding their lexical entries:try ` (S\NP)/(Sto\NP): ?p?x?e.?e?
[goal(e, e?
)?p(x, e?
)]fail ` (S\NP)/(Sto\NP): ?p?x?e.?e?
[goal(e, e?
)?p(x, e?)]???e??
[goal(e, e??)
?
p(x, e??
)]The  operator is used to assert that the comple-ment of try is a hypothetical proposition (so try toreach 6?reach).
Our semantics for fail is the sameas that for try, except that it asserts that the goalevent did not occur in the real world.These lexical entries allow us to make complexcompositional inferences, for example Columbusfailed to reach India now entails Columbus setout for India, Columbus tried to reach India andColumbus didn?t arrive in India.Again, we expect that the improved model offormal semantics should increase the quality ofthe entailment graphs, by allowing us to only clus-ter predicates based on their real-world arguments(ignoring hypothetical events).3 ConclusionWe have argued that several promising recentthreads of research in semantics can be combinedinto a single model.
The model we have describedwould enable wide-coverage mapping of open-domain text onto rich logical forms that modelcomplex aspects of semantics such as negation,quantification, modality and tense?whilst alsousing a robust distributional model of lexical se-mantics that captures the structure of events.
Con-sidering these interwined issues would allow com-plex compositional inferences which are far be-yond the current state of the art, and would givea more powerful model for natural language un-derstanding.AcknowledgementsWe thank Omri Abend, Michael Roth and theanonymous reviewers for their helpful comments.This work was funded by ERC Advanced Fellow-ship 249520 GRAMPLUS and IP EC-FP7-270273Xperience.ReferencesM.
Baroni, R. Bernardi, and R. Zamparelli.
2013.Frege in space: A program for compositional dis-tributional semantics.
Linguistic Issues in LanguageTechnologies.Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.Montague meets markov: Deep semantics withprobabilistic logical form.
pages 11?21, June.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies - Volume 1, HLT ?11, pages610?619.
Association for Computational Linguis-tics.Johan Bos.
2008.
Wide-coverage semantic analy-sis with boxer.
In Johan Bos and Rodolfo Del-monte, editors, Semantics in Text Processing.
STEP2008 Conference Proceedings, Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.L.
Karttunen.
1971.
The Logic of English Predi-cate Complement Constructions.
Linguistics ClubBloomington, Ind: IU Linguistics Club.
IndianaUniversity Linguistics Club.Mike Lewis and Mark Steedman.
2013.
CombinedDistributional and Logical Semantics.
Transactionsof the Association for Computational Linguistics,1:179?192.G.A.
Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.31Marc Moens and Mark Steedman.
1988.
Temporal on-tology and temporal reference.
Computational lin-guistics, 14(2):15?28.Richard Montague.
1973.
The proper treatment ofquantification in ordinary english.
In Approaches tonatural language, pages 221?242.
Springer.Aju Thalappillil Scaria, Jonathan Berant, MengqiuWang, Peter Clark, Justin Lewis, Brittany Harding,and Christopher D. Manning.
2013.
Learning bi-ological processes with global constraints.
In Pro-ceedings of EMNLP.Mark Steedman.
2012.
Taking Scope: The NaturalSemantics of Quantifiers.
MIT Press.Congle Zhang and Daniel S Weld.
2013.
Harvest-ing parallel news streams to generate paraphrases ofevent relations.32
