NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 55?63,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsNudging the Envelope of Direct Transfer Methodsfor Multilingual Named Entity RecognitionOscar Ta?ckstro?mSICS / Uppsala UniversitySwedenoscar@sics.seAbstractIn this paper, we study direct transfer meth-ods for multilingual named entity recognition.Specifically, we extend the method recentlyproposed by Ta?ckstro?m et al (2012), which isbased on cross-lingual word cluster features.First, we show that by using multiple sourcelanguages, combined with self-training for tar-get language adaptation, we can achieve sig-nificant improvements compared to using onlysingle source direct transfer.
Second, we in-vestigate how the direct transfer system faresagainst a supervised target language systemand conclude that between 8,000 and 16,000word tokens need to be annotated in each tar-get language to match the best direct transfersystem.
Finally, we show that we can signif-icantly improve target language performance,even after annotating up to 64,000 tokens inthe target language, by simply concatenatingsource and target language annotations.1 IntroductionRecognition of named entities in natural languagetext is an important subtask of information extrac-tion and thus bears importance for modern text min-ing and information retrieval applications.
The needto identify named entities such as persons, loca-tions, organizations and places, arises both in ap-plications where the entities are first class objects ofinterest, such as in Wikification of documents (Rati-nov et al, 2011), and in applications where knowl-edge of named entities is helpful in boosting perfor-mance, e.g., machine translation (Babych and Hart-ley, 2003) and question answering (Leidner et al,2003).
The advent of massive machine readable fac-tual databases, such as Freebase1 and the proposed1http://www.freebase.comWikidata2, will likely push the need for automaticextraction tools further.
While these databases storeinformation about entity types and the relationshipsbetween those types, the named entity recognition(NER) task concerns finding occurrences of namedentities in context.
This view originated with the Mes-sage Understanding Conferences (MUC) (Grishmanand Sundheim, 1996).As with the majority of tasks in contemporary nat-ural language processing, most approaches to NERhave been based on supervised machine learning.However, although resources for a handful of lan-guages have been created, through initiatives suchas MUC, the Multilingual Entity Task (Merchantet al, 1996) and the CoNLL shared tasks (TjongKim Sang, 2002; Tjong Kim Sang and De Meul-der, 2003), coverage is still very limited in terms ofboth domains and languages.
With fine-grained en-tity taxonomies such as that proposed by Sekine andNobata (2004), who define over two hundred cate-gories, we can expect an increase in the amount ofannotated data required for acceptable performance,as well as an increased annotation cost for each entityoccurrence.
Although semi-supervised approacheshave been shown to reduce the need for manual an-notation (Freitag, 2004; Miller et al, 2004; Andoand Zhang, 2005; Suzuki and Isozaki, 2008; Lin andWu, 2009; Turian et al, 2010; Dhillon et al, 2011;Ta?ckstro?m et al, 2012), these methods still require asubstantial amount of manual annotation for each tar-get language.
Manually creating a sufficient amountof annotated resources for all entity types in all lan-guages thus seems like an Herculean task.In this study, we turn to direct transfer methods(McDonald et al, 2011; Ta?ckstro?m et al, 2012) as2http://meta.wikimedia.org/wiki/Wikidata55a way to combat the need for annotated resourcesin all languages.
These methods allow one to traina system for a target language, using only annota-tions in some source language, as long as all sourcelanguage features also have support in the target lan-guages.
Specifically, we extend the direct transfermethod proposed by Ta?ckstro?m et al (2012) in twoways.
First, in ?3, we use multiple source languagesfor training.
We then propose a self-training algo-rithm, which allows for the inclusion of additionaltarget language specific features, in ?4.
By com-bining these extensions, we achieve significant errorreductions on all tested languages.
Finally, in ?5,we assess the viability of the different direct transfersystems compared to a supervised system trained ontarget language annotations, and conclude that directtransfer methods may be useful even in this scenario.2 Direct Transfer for Cross-lingual NERRather than starting from scratch when creating sys-tems that predict linguistic structure in one language,we should be able to take advantage of any cor-responding annotations that are available in otherlanguages.
This idea is at the heart of both directtransfer methods (McDonald et al, 2011; Ta?ckstro?met al, 2012) and of annotation projection methods(Yarowsky et al, 2001; Diab and Resnik, 2002; Hwaet al, 2005).
While the aim of the latter is to transferannotations across languages, direct transfer meth-ods instead aim to transfer systems, trained on somesource language, directly to other languages.
In thispaper, we focus on direct transfer methods, however,we briefly discuss the relationship between these ap-proaches in ?6.Considering the substantial differences betweenlanguages at the grammatical and lexical level, theprospect of directly applying a system trained onone language to another language may seem bleak.However, McDonald et al (2011) showed that a lan-guage independent dependency parser can indeed becreated by training on a delexicalized treebank andby only incorporating features defined on universalpart-of-speech tags (Das and Petrov, 2011).Recently, Ta?ckstro?m et al (2012) developed an al-gorithm for inducing cross-lingual word clusters andproposed to use these clusters to enrich the featurespace of direct transfer systems.
The richer set ofcross-lingual features was shown to substantially im-prove on direct transfer of both dependency parsingand NER from English to other languages.Cross-lingual word clusters are clusterings ofwords in two (or more) languages, such that the clus-ters are adequate in each language and at the sametime consistent across languages.
For cross-lingualword clusters to be useful in direct transfer of lin-guistic structure, the clusters should capture cross-lingual properties on both the semantic and syntac-tic level.
Ta?ckstro?m et al (2012) showed that thisis, at least to some degree, achievable by couplingmonolingual class-based language models, via wordalignments.
The basic building block is the follow-ing simple monolingual class-based language model(Saul and Pereira, 1997; Uszkoreit and Brants, 2008):L(w; C) =m?i=1p(wi|C(wi))p(C(wi)|wi?1) ,where L(w; C) is the likelihood of a sequence ofwords, w, and C is a (hard) clustering function, whichmaps words to cluster identities.
These monolingualmodels are coupled through word alignments, whichconstrains the clusterings to be consistent across lan-guages, and optimized by approximately maximizingthe joint likelihood across languages.
Just as monolin-gual word clusters are broadly applicable as featuresin monolingual models for linguistic structure predic-tion (Turian et al, 2010), the resulting cross-lingualword clusters can be used as features in various cross-lingual direct transfer models.
We believe that theextensions that we propose are likely to be useful forother tasks as well, e.g., direct transfer dependencyparsing, in this paper, we focus solely on discrimina-tive direct transfer models for NER.3 Multi-source Direct TransferLearning from multiple languages have been shownto be of benefit both in unsupervised learning of syn-tax and part-of-speech (Snyder et al, 2009; Berg-Kirkpatrick and Klein, 2010) and in transfer learningof dependency syntax (Cohen et al, 2011; McDonaldet al, 2011).
Here we perform a set of experimentswhere we investigate the potential of multi-sourcetransfer for NER, in German (DE), English (EN),Spanish (ES) and Dutch (NL), using cross-lingualword clusters.
For all experiments, we use the same56Source DE ES NLEN 39.7 62.0 63.7EN + DE ?
61.8 65.5EN + ES 39.3 ?
65.6EN + NL 41.0 62.5 ?ALL 41.0 63.6 66.4?
DEVELOPMENT SET ?
TEST SETEN 37.8 59.1 57.2EN + DE ?
59.4 57.9EN + ES 35.9 ?
59.1EN + NL 38.1 59.7 ?ALL 36.4 61.9 59.9Table 1: Results of multi-source direct transfer, measuredwith F1-score on the CoNLL 2002/2003 development andtest sets.
ALL: all languages except the target languageare used as source languages.256 cross-lingual word clusters and the same featuretemplates as Ta?ckstro?m et al (2012), with the ex-ception that the transition factors are not conditionedon the input.3 The features used are similar to thoseused by Turian et al (2010), but include cross-lingualrather than monolingual word clusters.
We removethe capitalization features when transferring to Ger-man, but keep them in all other cases, even when Ger-man is included in the set of source languages.
Weuse the training, development and test data sets pro-vided by the CoNLL 2002/2003 shared tasks (TjongKim Sang, 2002; Tjong Kim Sang and De Meul-der, 2003).
The multi-source training sets are cre-ated by concatenating each of the source languages?training sets.
In order to have equivalent label setsacross languages, we use the IO (inside/outside) en-coding, rather than the BIO (begin/inside/outside) en-coding, since the latter is available only for Spanishand Dutch.
The models are trained using CRFSuite0.12 (Okazaki, 2007), by running stochastic gradientdescent for a maximum of 100 iterations.Table 1 shows the result of using different sourcelanguages for different target languages.
We see thatmulti-source transfer is somewhat helpful in general,but that the results are sensitive to the combinationof source and target languages.
On average, using allsource languages only give a relative error reductionof about 3% on the test set.
However, results for3This is due to limitations in the sequence labeling softwareused and gives slightly lower results, across the board, than thosereported by Ta?ckstro?m et al (2012).DE ES NL AVGNATIVE CLUSTERS 71.2 80.7 82.5 78.1X-LING CLUSTERS 68.9 78.8 80.9 76.2NATIVE & X-LING CLUST.
72.5 81.2 83.6 79.1?
DEVELOPMENT SET ?
TEST SETNATIVE CLUSTERS 72.2 81.0 83.0 78.7X-LING CLUSTERS 71.0 80.2 80.7 77.3NATIVE & X-LING CLUST.
73.5 81.8 83.7 79.7Table 2: The impact of different word clusters in thesupervised monolingual setting.
Results are measuredwith F1-score on the CoNLL 2002/2003 developmentand test sets.
NATIVE/X-LING CLUSTERS: The cross-lingual/monolingual clusters from Ta?ckstro?m et al (2012).Spanish and Dutch are more promising, with relativereductions of 7% and 6%, respectively, when usingall source languages.
Using all available source lan-guages gives the best results for both Spanish andDutch, but slightly worse results for German.
Whentransferring to Dutch, using more source languagesconsistently help, while Spanish and German aremore sensitive to the choice of source languages.Based on the characteristics of these languages, thisis not too surprising: while Dutch and German hasthe most similar vocabularies, Dutch uses similar cap-italization rules to English and Spanish.
Dutch shouldthus benefit from all the other languages, while Span-ish may not bring much to the table for German andvice versa, given their lexical differences.
Knowl-edge of such relationships between the languages,could potentially be used to give different weights todifferent source languages in the training objective,as was shown effective by Cohen et al (2011) in thecontext of direct transfer of generative dependencyparsing models.
Although better results could beachieved by cherry-picking language combinations,since we do not have any general principled way ofchoosing/weighting source languages in discrimina-tive models, we include all source languages withequal weight in all subsequent experiments wheremultiple source languages are used.4 Domain Adaptation via Self-TrainingThus far, we have not made use of any informationspecific to the target language, except when inducingthe cross-lingual word clusters.
However, as shownin Table 2, which lists the results of experiments on57Algorithm 1 Self-Training for Domain AdaptationDls: Labeled source domain dataDlt: Labeled target domain data (possibly empty)Dut : Unlabeled target domain data?
: Dominance thresholdT : Number of iterationsprocedure SELFTRAIN(Dls,Dlt,Dut , ?, T )?0 ?
LEARN(Dls ?
Dlt) .
Train supervised modelfor i?
1 to T doP i ?
PREDICT(Dut , ?i?1) .
Predict w/ curr.
mod.F i ?
FILTER(P i, ?)
.
Filter p?i?1(y?|x) ?
?Si ?
SAMPLE(F i) .
Pick ?
p?i?1(y|x).
(?
)?i ?
LEARN(Dls ?
Dlt ?
Si) .
Retrainend forreturn ?T .
Return adapted modelend procedure?
If LEARN(?)
supports instance weighting, we could weighteach instance (x,y?)
?
F i by p?i?1(y?|x) in the trainingobjective, rather than performing sampling according to thesame distribution.supervised target language models trained with differ-ent cluster features,4 these clusters are not optimallyadapted to the target language, compared to the mono-lingual native clusters that are induced solely on thetarget language, without any cross-lingual constraints.This is to be expected, as the probabilistic model usedto learn the cross-lingual clusters strikes a balancebetween two language specific models.
On the otherhand, this suggests an opportunity for adapting to tar-get language specific features through self-training.In fact, since the direct transfer models are trainedusing cross-lingual features, the target language canbe viewed as simply representing a different domainfrom the source language.Self-training has previously been shown to be asimple and effective way to perform domain adapta-tion for syntactic parsers and other tasks (McCloskyet al, 2006; Chen et al, 2011).
The idea of self-training for domain adaptation is to first train a su-pervised predictor on labeled instances from a sourcedomain.
This predictor is then used to label instancesfrom some unlabeled target domain.
Those instancesfor which the predictor is confident are added to thesource training set, and the process is repeated untilsome stopping criterion is met.
Recently, Daume?et al (2010) and Chen et al (2011) proposed more4For these experiments, the same settings were used as in themulti-source transfer experiments in ?3, with the difference thatonly target language training data was used.complex domain adaptation techniques, based on co-training.
In this work, however, we stick with the sim-ple single-view self-training approach just outlined.In the self-training for domain adaptation method, de-scribed by Chen et al (2011), the top-k instances forwhich the predictor is most confident are added to thetraining set in each iteration.
We instead propose toweight the target instances selected for self-trainingin each iteration proportional to the confidence of theclassifier trained in the previous iteration.In short, let x ?
Dut be an unlabeled target lan-guage input sequence (in our case a sentence) andy?
?
Yt(x) its top-ranked label sequence (in ourcase an IO sequence).
In the first iteration, a predictoris trained on the labeled source language data, Dls.
Ineach subsequent iteration the sequences are scoredaccording to the probabilities assigned by the pre-dictor trained in the previous iteration, p?i?1(y?|x).When constructing the training set for the next it-eration, we first filter out all instances for whichthe top-ranked label sequence is not ?-dominating.That is, we filter out all instances x ?
Dtu such thatp?i?1(y?|x) < ?, for some user-specified ?.
In thiswork, we set ?
= 0.5, since this guarantees that theoutput associated with each instance that is kept isassigned the majority of the probability mass.
This isimportant, as we only consider the most likely outputy?
for each input x, so that sampling low-confidenceinstances will result in a highly biased sample.
Afterfiltering, we sample from the remaining instances,i.e.
from the set of instances x ?
Dtu such thatp?i?1(y?|x) ?
?, adding each instance (x,y?)
tothe training set with probability p?i?1(y?|x).
Thisprocedure is repeated for T iterations as outlinedin Algorithm 1.
By using instance weighting ratherthan a top-k list, we remove the need to heuristicallyset the number of instances to be selected for self-training in each iteration.
Further, although we havenot verified this empirically, we hypothesize that us-ing instance weighting is more robust than pickingonly the most confident instances, as it maintains di-versity in the training set in the face of uncertainty.Note also that when we have access to target languagetest data during training, we can perform transduc-tive learning by including the test set in the pool ofunlabeled data.
This gives the model the opportunityto adapt to the characteristics of the test domain.Our use of self-training for exploiting features na-58DE ES NL AVGSINGLE 39.7 62.0 63.7 55.2MULTI 41.0 63.6 66.4 57.0SINGLE + SELF 42.6 65.7 64.0 57.4SINGLE + SELF/NATIVE 44.5 66.5 65.9 59.0MULTI + SELF 48.4 64.7 68.1 60.4MULTI + SELF/NATIVE 49.5 66.5 69.7 61.9?
DEVELOPMENT SET ?
TEST SETSINGLE 37.8 59.1 57.2 51.4MULTI 36.4 61.9 59.9 52.8SINGLE + SELF 41.3 61.0 57.8 53.3SINGLE + SELF/NATIVE 43.0 62.5 58.9 54.8MULTI + SELF 45.3 62.3 61.9 56.5MULTI + SELF/NATIVE 47.2 64.8 63.1 58.4Table 3: Results of different extensions to direct trans-fer as measured with F1-score on the CoNLL 2002/2003development and test sets.
SINGLE: single-source trans-fer, MULTI: multi-source transfer, SELF: self-trainingwith only cross-lingual word clusters, SELF/NATIVE: self-training with cross-lingual and native word clusters.tive to the target language resembles the way McDon-ald et al (2011) re-lexicalize a delexicalized directtransfer parser.
Both methods allow the model tomove weights from shared parameters to more pre-dictive target language specific parameters.
However,rather than using the direct transfer parser?s own pre-dictions through self-training, these authors projecthead-modifier relations to the target language throughloss-augmented learning (Hall et al, 2011).
The boot-strapping methods for language independent NER ofCucerzan and Yarowsky (1999) have a similar effect.Our self-training approach is largely orthogonal tothese approaches.
We therefore believe that combin-ing these methods could be fruitful.4.1 ExperimentsIn these experiments we combine direct transfer withself-training using unlabeled target data.
This is thetransductive setting, as we include the test data (withlabels removed, of course) in the unlabeled targetdata.
We investigate the effect of adding self-training(SELF) to the single-source and multi-source transfersettings of ?3, where only cross-lingual features areused (SINGLE and MULTI, respectively).
We furtherstudy the effect of including native monolingual wordcluster features in addition to the cross-lingual fea-tures (SELF/NATVE).
The experimental settings anddatasets used are the same as those described in ?3.We performed self-training for T = 5 iterations forall languages, as preliminary experiments indicatedthat the procedure converges to a stable solution af-ter this number of iterations.
CRFSuite was used tocompute all the required probabilities for the filteringand sampling steps.The results of these experiments are shown in Ta-ble 3.
By itself, self-training without target specificfeatures result in an average relative error reductionof less than 4%, compared to the baseline directtransfer system.
This is only slightly better thanthe improvement achieved with multi-source transfer.However, when adding target specific features, self-training works better, with a 7% reduction.
Combin-ing multi-source transfer with self-training, withouttarget specific features, performs even better witha 10% reduction.
Finally, combining multi-sourcetransfer and self-training with target specific features,gives the best result across all three languages, withan average relative error reduction of more than 14%.The results for German are particularly interest-ing, in that they highlight a rather surprising generaltrend.
The relative improvement achieved by com-bining multi-source transfer and self training with na-tive clusters is almost twice as large as that achievedwhen using only self-training with native clusters,despite the fact that multi-source transfer is not veryeffective on its own ?
in the case of German, multi-source transfer actually hurts results when used inisolation.
One explanation for this behavior could bethat the regularization imposed by the use of multi-ple source languages is beneficial to self-training, inthat it generates better confidence estimates.
Another,perhaps more speculative, explanation could be thateach source language shares different characteristicswith the target language.
Even though the predictionson the target language are not much better on aver-age in this case, as long as a large enough subset ofthe confident predictions are better than with single-source transfer, these predictions can be exploitedduring self-training.In addition to using self-training with native wordcluster features, we also experimented with creatingtarget language specific versions of the cross-lingualfeatures by means of the feature duplication trick(Daume?, 2007).
However, preliminary experimentssuggested that this is not an effective strategy in the59010203040506070800 125 250 500 1k 2k 4k 8k 16k 32k 64k 128kF 1Number of annotated target tokenssupervisedsinglemultisingle + self/nativemulti + self/nativeFigure 1: Learning curves for German.01020304050607080900 125 250 500 1k 2k 4k 8k 16k 32k 64k 128kF 1Number of annotated target tokenssupervisedsinglemultisingle + self/nativemulti + self/nativeFigure 2: Learning curves for Spanish.cross-lingual direct transfer scenario.
It thus seemslikely that the significant improvements that we ob-serve are at least in part explained by the fact thatthe native features are distinct from the cross-lingualfeatures and not mere duplicates.5 Direct Transfer vs.
Supervised LearningFinally, we look at the relative performance of the dif-ferent direct transfer methods and a target languagespecific supervised system trained with native andcross-lingual word cluster features.
For these experi-ments we use the same settings as for the experimentsin ?3 and ?4.1.Figures 1?3 show the learning curves for the su-pervised system, as more and more target languageannotations, selected by picking sentences at randomfrom the full training set, are added to the trainingset, compared to the same system when combinedwith different direct transfer methods.
From thesecurves, we can see that the purely supervised model01020304050607080900 125 250 500 1k 2k 4k 8k 16k 32k 64k 128kF 1Number of annotated target tokenssupervisedsinglemultisingle + self/nativemulti + self/nativeFigure 3: Learning curves for Dutch.requires between 8,000 and 16,000 annotated wordtokens (roughly corresponding to between 430 and860 sentences) in each target language to match thebest direct transfer system.
The learning curves alsoshow that adding source language data improves per-formance with as many as 64,000 annotated targetlanguage tokens.Although we believe that the results on combin-ing source and target data are interesting, in practicethe marginal cost of annotation is typically quite lowcompared to the initial cost.
Therefore, the cost ofgoing from 125 to 64,000 annotated tokens is likelynot too high, so that the benefit of cross-lingual trans-fer is small on the margin in this scenario.
However,we believe that direct transfer methods can reducethe initial cost as well, especially when a larger labelset is used, since a larger label set implies a largercognitive load throughout annotation, but especiallyin the initial phase of the annotation.Another aspect, which we were unable to investi-gate is the relative performance of these methods ondomains other than news text.
It is well known thatthe performance of supervised NER systems drop sig-nificantly when applied to data outside of the trainingdomain (Nothman et al, 2008).
Although the directtransfer systems in these experiments are also trainedon news data, we suspect that the advantage of thesemethods will be more pronounced when applied toother domains, since the supervised target systemruns a higher risk of overfitting to the characteristicsof the target language training domain compared tothe direct transfer system, which has already to somedegree overfitted to the source language.606 DiscussionWe have focused on direct transfer methods that ex-ploit cross-lingual word clusters, which are inducedwith the help of word alignments.
A more com-mon use of word alignments for cross-lingual linguis-tic structure prediction is for projecting annotationsacross languages (Yarowsky et al, 2001; Diab andResnik, 2002; Hwa et al, 2005).Apart from the algorithmic differences betweenthese approaches, there are more fundamental differ-ences in terms of the assumptions they make.
An-notation projection relies on the construction of amapping from structures in the source language tostructures in the target language, Ys 7?
Y ?t.
Basedon the direct correspondence assumption (Diab andResnik, 2002; Hwa et al, 2005), word alignments areassumed to be a good basis for this mapping.
Whenprojecting annotations, no consideration is taken tothe source language input space, Xs, nor to the targetlanguage input space, Xt, except implicitly in theconstruction of the word alignments.
The learning al-gorithm is thus free to use any parameters when train-ing on instances from Xt ?
Y ?t, but can at the sametime not exploit any additional information that maybe present in Xs ?
Ys about Xt ?
Yt.
Furthermore,word alignments are noisy and often only providepartial information about the target side annotations.Direct transfer, on the other hand, makes a strongerassumption, as it relies on a mapping from the jointspace of source inputs and output structures to thetarget language, Xs ?
Ys 7?
X ?t ?
Y ?t.
Actually,the assumption is even stronger, since in order toachieve low error on the target language with a dis-criminative model, we must further assume that theconditional distribution P (Y ?t|X ?t ) does not divergetoo much from P (Yt|Xt) in regions where P (Xt)is large.
This suggests that direct transfer might bepreferable when source and target languages are suffi-ciently similar so that a good mapping can be found.These differences suggest that it may be fruitfulto combine direct transfer with annotation projec-tion.
For example, direct transfer could be usedto first map Xs ?
Ys 7?
X ?t ?
Y ?t, while annota-tion projection could be used to derive constraintson the target output space by means of a mappingYs 7?
Y ?
?t .
These constraints could perhaps be ex-ploited in self-training, e.g., through posterior reg-ularization (Ganchev et al, 2010), or be used forco-training (Blum and Mitchell, 1998).7 ConclusionsWe investigated several open questions regarding theuse of cross-lingual word clusters for direct transfernamed entity recognition.
First, we looked at the sce-nario where no annotated resources are available inthe target language.
We showed that multi-source di-rect transfer and self-training with additional features,exclusive to the target language, both bring benefitsin this setting, but that combining these methodsprovide an even larger advantage.
We then exam-ined the rate with which a supervised system, trainedwith cross-lingual and native word cluster features,approaches the performance of the direct transfersystem.
We found that on average between 8,000and 16,000 word tokens need to be annotated in eachtarget language to match our best direct transfer sys-tem.
We also found that combining native and cross-lingual word clusters leads to improved results acrossthe board.
Finally, we showed that direct transfermethods can aid even in the supervised target lan-guage scenario.
By simply mixing annotated sourcelanguage data with target language data, we can sig-nificantly reduce the annotation burden required toreach a given level of performance in the target lan-guage, even with up to 64,000 tokens annotated in thetarget language.
We hypothesize that more elaboratedomain adaptation techniques, such as that proposedby Chen et al (2011), can lead to further improve-ments in these scenarios.Our use of cross-lingual word clusters is orthog-onal to several other approaches discussed in thispaper.
We therefore suggest that such clusters couldbe of general use in multilingual learning of lin-guistic structure, in the same way that monolingualword clusters have been shown to be a robust way tobring improvements in many monolingual applica-tions (Turian et al, 2010; Ta?ckstro?m et al, 2012).AcknowledgmentsThis work benefited from discussions with Ryan Mc-Donald and from comments by Joakim Nivre andthree anonymous reviewers.
The author is gratefulfor the financial support of the Swedish NationalGraduate School of Language Technology (GSLT).61ReferencesRie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method for textchunking.
In Proceedings of ACL.Bogdan Babych and Anthony Hartley.
2003.
Improvingmachine translation quality with automatic named en-tity recognition.
In Proceedings of the EAMT workshopon Improving MT through other Language TechnologyTools: Resources and Tools for Building MT.Taylor Berg-Kirkpatrick and Dan Klein.
2010.
Phyloge-netic grammar induction.
In Proceedings of ACL.Avrim Blum and Tom Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Proceedings ofCOLT, COLT?
98, New York, NY, USA.
ACM.Minmin Chen, John Blitzer, and Kilian Q. Weinberger.2011.
Co-training for domain adaptation.
In Proceed-ings of NIPS.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011.Unsupervised structure prediction with non-parallelmultilingual guidance.
In Proceedings of EMNLP.Silviu Cucerzan and David Yarowsky.
1999.
Languageindependent named entity recognition combining mor-phological and contextual evidence.
In Proceedings ofEMNLP-Very Large Corpora.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of ACL-HLT.Hal Daume?, III, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domain adap-tation.
In Proceedings of the 2010 Workshop on Do-main Adaptation for Natural Language Processing.Hal Daume?, III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of ACL.Paramveer Dhillon, Dean Foster, and Lyle Dean.
2011.Multi-view learning of word embeddings via cca.
InProceedings of NIPS.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proceedings of ACL.Dayne Freitag.
2004.
Trained named entity recogni-tion using distributional clusters.
In Proceedings ofEMNLP.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of Machine Learn-ing Research.Ralph Grishman and Beth Sundheim.
1996.
Messageunderstanding conference-6: a brief history.
In Pro-ceedings of the 16th conference on Computational lin-guistics - Volume 1.Keith Hall, Ryan McDonald, Jason Katz-Brown, andMichael Ringgaard.
2011.
Training dependencyparsers by jointly optimizing multiple objectives.
InProceedings of EMNLP.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(03):311?325.Jochen L. Leidner, Gail Sinclair, and Bonnie Webber.2003.
Grounding spatial named entities for informationextraction and question answering.
In Proceedings ofHLT-NAACL-GEOREF.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of ACL-IJCNLP, pages 1030?1038.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of NAACL-HLT.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of EMNLP.Roberta Merchant, Mary Ellen Okurowski, and NancyChinchor.
1996.
The multilingual entity task (met)overview.
In Proceedings of a workshop on held atVienna, Virginia: May 6-8, 1996.Scott Miller, Jethran Guinness, and Alex Zamanian.
2004.Name tagging with word clusters and discriminativetraining.
In Proceedings of HLT-NAACL.Joel Nothman, James R Curran, and Tara Murphy.
2008.Transforming wikipedia into named entity training data.In Proceedings of the Australasian Language Technol-ogy Association Workshop 2008, pages 124?132, Ho-bart, Australia, December.Naoaki Okazaki.
2007.
Crfsuite: a fast implementationof conditional random fields (crfs).Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-son.
2011.
Local and global algorithms for disam-biguation to wikipedia.
In Proceedings of ACL-HLT.Lawrence Saul and Fernando Pereira.
1997.
Aggregateand mixed-order markov models for statistical languageprocessing.
In Proceedings of EMNLP, pages 81?89.Satoshi Sekine and Chikashi Nobata.
2004.
Definition,dictionaries and tagger for extended named entity hier-archy.
In Proceedings of LREC.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, andRegina Barzilay.
2009.
Adding more languages im-proves unsupervised multilingual part-of-speech tag-ging: A bayesian non-parametric approach.
In Pro-ceedings of NAACL.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-wordscale unlabeled data.
In Proceedings of ACL-HLT.Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.2012.
Cross-lingual word clusters for direct transfer oflinguistic structure.
In Proceedings of NAACL-HLT.62Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: Language-independent named entity recognition.
In Proceedingsof CoNLL.Erik F. Tjong Kim Sang.
2002.
Introduction to the conll-2002 shared task: Language-independent named entityrecognition.
In Proceedings of CoNLL.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceedingsof ACL.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributedword clustering for large scale class-based languagemodeling in machine translation.
In Proceedings ofACL-HLT.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In Proceedingsof HLT.63
