Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 329?334,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsTarget-Centric Features for Translation Quality EstimationChris Hokamp and Iacer Calixto and Joachim Wagner and Jian ZhangCNGL Centre for Global Intelligent ContentDublin City UniversitySchool of ComputingDublin, Ireland{chokamp|icalixto|jwagner|zhangj}@computing.dcu.ieAbstractWe describe the DCU-MIXED and DCU-SVR submissions to the WMT-14 QualityEstimation task 1.1, predicting sentence-level perceived post-editing effort.
Fea-ture design focuses on target-side featuresas we hypothesise that the source side haslittle effect on the quality of human trans-lations, which are included in task 1.1of this year?s WMT Quality Estimationshared task.
We experiment with featuresof the QuEst framework, features of ourpast work, and three novel feature sets.Despite these efforts, our two systems per-form poorly in the competition.
Follow upexperiments indicate that the poor perfor-mance is due to improperly optimised pa-rameters.1 IntroductionTranslation quality estimation tries to predict thequality of a translation given the source and targettext but no reference translations.
Different fromprevious years (Callison-Burch et al., 2012; Bo-jar et al., 2013), the WMT 2014 Quality Estima-tion shared task is MT system-independent, i. e. noglass-box features are available and translations inthe training and test sets are produced by differentMT systems and also by human translators.This paper describes the CNGL@DCU teamsubmission to task 1.1 of the WMT 2014 QualityEstimation shared task.1The task is to predict theperceived post-editing effort given a source sen-tence and its raw translation.
Due to the inclusionof human translation in the task, we focus our ef-forts on target-side features as we expect that thequality of a translation produced by a human trans-lator is much less affected by features of the source1A CNGL system based on referential translation ma-chines is submitted separately (Bic?ici and Way, 2014).than by extrinsic factors such as time pressure andfamiliarity with the domain.To build our quality estimation system, we useand extend the QuEst framework for translationquality estimation2(Shah et al., 2013; Speciaet al., 2013).
QuEst provides modules for fea-ture extraction and machine learning.
We modifyboth the feature extraction framework and the ma-chine learning components to add functionality toQuEst.The novel features we add to our systems are(a) a language model on a combination of stopwords and POS tags, (b) inverse glass-box fea-tures for translating the translation, and (c) ran-dom indexing (Sahlgren, 2005) for measuring thesemantic similarity of source and target side acrosslanguages.
Furthermore, we integrated (d) source-side pseudo-reference features (Soricut and Echi-habi, 2010) and (e) error grammar features (Wag-ner, 2012), which were used first in MT qualityestimation by (Rubino et al., 2012; Rubino et al.,2013).The remaining sections are organised as fol-lows.
Section 2 gives details on the features weuse.
Section 3 describes how we set up our ex-periments.
Results are presented in Section 4 andconclusions are drawn in Section 5 together withpointers to future work.2 FeaturesThis section describes the features we extract fromsource and target sentences in order to train predic-tion models and to make predictions in addition tothe baseline features provided for the task.We focus on the target side as we assume thatthe quality of the source side has little predictivepower for human translations, which are includedin task 1.1.2http://www.quest.dcs.shef.ac.uk/3292.1 QuEst Black-Box Features and BaselineFeaturesWe use the QuEst framework to extract 47 ba-sic black-box features from both source and tar-get side, such as the ratio of the number of to-kens, punctuation statistics, number if mismatchedbrackets and quotes, language model perplexity,n-gram frequency quartile statistics (n = 1, 2, 3),and coarse-grained POS frequency ratios.
17 ofthe 47 features are identical to the baseline fea-tures from the shared task website, i. e. 30 fea-tures are new.
To train the language models andto extract frequency information, we use the NewsCommentary corpus (Bojar et al., 2013).2.2 POS and Stop Word Language ModelFeaturesFor all languages, we extract probability and per-plexity features from language models trained onPOS tagged corpora.
POS tagging is performedusing the IMS Tree Tagger (Schmid, 1994).We also experiment with language models builtfrom a combination of stop words3and POS tags.Starting with a tokenised corpus, and its POS-tagged counterpart, we create a new representationof the corpus by replacing POS tags for stop wordswith the literal stop word that occurred in the orig-inal corpus, leaving non-stop word tags intact.4The intuition behind the approach is that the com-bined POS and stop word model should encodethe distributional tendencies of the most commonwords in the language.The log-probability and the perplexity of thetarget side are used as features.
The developmentof these features was motivated by manual exam-ination of the common error types in the train-ing data.
We noted that stop word errors (omis-sion, mistranslation, mis-translation of idiom), areprevalent in all language pairs, indicating that fea-tures which focus on stop word usage could beuseful for predicting the quality of machine trans-lation.
We implement POS and stop word lan-guage models inside the QuEst framework.2.3 Source-Side Pseudo-Reference FeaturesWe extract source-side pseudo-reference features(Albrecht and Hwa, 2008; Soricut and Echihabi,3We use the stop word lists from Apache Lucene (McCan-dless et al., 2010).4The News Commentary corpus from WMT13 was usedto build these models, same as for the black-box features(Section 2.1).2010; Rubino et al., 2012), for English to Germanquality prediction using a highly-tuned German toEnglish translation system (Li et al., 2014) work-ing in the reverse direction.
The MT system trans-lates the German target side, the quality of whichis to be predicted, back into English, and we ex-tract pseudo-reference features on the source side:?
BLEU score (Papineni et al., 2002) be-tween back-translation and original sourcesentence, and?
TER score (Snover et al., 2006).For the 5th English to German test set item, forexample, the translation(1) Und belasse sie dort eine Woche.is translated back to English as(2) and leave it there for a week .and compared to the original source sentence(3) Leave for a week.producing a BLEU score of 0.077 using thePython interface to the cdec toolkit (Chahuneau etal., 2012).2.4 Inverse Glass-Box Features forTranslating the TranslationIn the absence of direct glass-box features, we ob-tain glass-box features from translating the rawtranslation back to the source language using thesame MT system that we use for the source-sidepseudo-reference features.
We extract featuresfrom the following components of the Moses de-coder: distortion model, language model, lexi-cal reordering, lexical translation probability, op-erational sequence model (Durrani et al., 2013),phrase translation probability, and the decoderscore.The intuition for this set of features is that back-translating an incorrect translation will give lowsystem-internal scores, e. g. a low phrase transla-tion score, and produce poor output with low lan-guage model scores (garbage in, garbage out).We are not aware of any previous work usinginverse glass-box features of translating the targetside to another language for quality estimation.3302.5 Semantic Similarity Using RandomIndexingThese features try to measure the semantic sim-ilarity of source and target side of a translationunit for quality estimation using random index-ing (Sahlgren, 2005).
We experiment with addingthe similarity score of the source and target ran-dom vectors.For each source and target pair in the English-Spanish portion of the Europarl corpus (Koehn,2005), we initialize a sparse random vector.
Wethen create token vectors for each source and tar-get token by summing the vectors for all of thesegments where the token occurs.
To extract thesimilarity feature for new source and target pairs,we map them into the vector space by taking thecentroid of the token vectors for the source sideand the target side, and computing their cosinesimilarity.2.6 Error Grammar ParsingWe obtain features from monolingual parsing withthree grammars:1. the vanilla grammar shipped with the Blippparser (Charniak, 2000; Charniak and John-son, 2005) induced from the Penn-Treebank(Marcus et al., 1994),2. an error grammar induced from Penn-Tree-bank trees distorted according to an errormodel (Foster, 2007), and3.
a grammar induced from the union of theabove two treebanks.Features include the log-ratios between the prob-ability of the best parse obtained with each gram-mar and structural differences measured with Par-seval (Black et al., 1991) and leaf-ancestor (Samp-son and Babarczy, 2003) metrics.
These featureshave been shown to be useful for judging thegrammaticality of sentences (Wagner et al., 2009;Wagner, 2012) and have been used in MT qualityestimation before (Rubino et al., 2012; Rubino etal., 2013).3 Experimental SetupThis section describes how we set up our experi-ments.3.1 Cross-ValidationDecisions about parameters are made in 10-foldcross-validation on the training data provided forthe task.
As the datasets for task 1.1 includethree to four translations for each source segment,we group segments by their source side and splitthe data for cross-validation between segments toensure that a source segment does not occur inboth training and test data for any of the cross-validation runs.We implement these modifications to cross-validation and randomisation in the QuEst frame-work.3.2 TrainingWe use the QuEst framework to train our models.Support vector regression (SVR) meta-parametersare optimised using QuEst?s default settings, ex-ploring RBF kernels with two possible values foreach of the three meta-parameters C, ?
and .5The two final models are trained on thefull training set with the meta-parameters thatachieved the best average cross-validation score.3.3 Classifier CombinationWe experiment with combining logistic regression(LR) and support vector regression (SVR) by firstchoosing the instances where LR classification isconfident and using the LR class label (1, 2, or3) as predicted perceived post-editing effort, andfalling back to SVR for all other instances.We employ several heuristics to decide whetherto use the output of LR or SVR.
As the LR classi-fier learns a decision function for each of the threeclasses, we can exploit the scores of the classes tomeasure the confidence of the LR classifier aboutits decision.
If the LR classifier is confident, weuse its prediction directly, otherwise we use theSVR prediction.For the cases where one of the three decisionfunctions for the LR classifier is positive, we selectthe prediction directly, falling back to SVR whenthe classifier is not confident about any of the threeclasses.
We implement the LR+SVR classifiercombination inside the QuEst framework.4 ResultsTable 1 shows cross-validation results for the 17baseline features, the combination of all featuresand target-side features only.
We do not showcombinations of individual feature sets and base-line features that do not improve over the base-5We only discovered this limitation of the default config-uration after the system submission, see Sections 4 and 5.331Features Classifier RMSE MAEBasel.17 LR+SVR 0.75 0.62ALL LR+SVR 0.74 0.59ALL LR> 0.5+SVR 0.75 0.58Target LR+SVR 0.75 0.59ALL LR> 0.5+SVR-r 0.78 0.55Table 1: Cross-validation results for English toGerman.
LR > 0.5 indicates that we require theLR decision function to be > 0.5.
SVR-r roundsthe output to the nearest natural number.line.
Several experiments, including those with thesemantic similarity feature sets, are thus omitted.Furthermore, we only exemplify one language pair(English to German), as the other language pairsshow similar patterns.
The feature set target con-tains the subset of the QuEst black-box features(Section 2.1) which only examine the target side.Our best results for English to German in thecross-validation experiments are achieved by com-bining a logistic regression (LR) classifier withsupport vector regression (SVR).
Furthermore,performance on the cross-validation is slightly im-proved for the mean absolute error (MAE) byrounding SVR scores to the nearest integer.
Forthe root-mean-square error (RMSE), rounding hasthe opposite effect.Performing a more fine-grained grid search forthe meta-parameters C, ?
and  after system sub-mission, we were able to match the scores forthe baseline features published on the shared taskwebsite.4.1 Parameters for the Final ModelsThe final two models for system submission aretrained on the full data set.
We submit our best sys-tem according to MAE in cross-validation com-bining LR, SVR and rounding with all features(ALL) as DCU-MIXED.
For our second submis-sion, we choose SVR on its own (system DCU-SVR).
For English-Spanish, we only submit DCU-SVR.5 Conclusions and Future WorkWe identified improperly optimised parameters ofthe SVR component as the cause, or at least as acontributing factor, for the placement of our sys-tems below the official baseline system.
Other po-tential factors may be an error in our experimen-tal setup or over-fitting.
Therefore, we plan to re-peat the experiments with a more fine-grained gridsearch for optimal parameters and/or will try an-other machine learning toolkit.Unfortunately, due to the above problems withour system so far, we cannot draw conclusionsabout the effectiveness of our novel feature sets.A substantial gain is achieved on the MAE met-ric with the rounding method, indicating that themajority of prediction errors are below 0.5.6Fu-ture work should account for this effect.
Two ideasare: (a) round all predictions before evaluationand (b) use more fine-grained gold values, e. g. the(weighted) average over multiple annotations as inthe WMT 2012 quality estimation task (Callison-Burch et al., 2012).For the error grammar method, the next stepwill be to adjust the error model to errors found intranslations.
It may be possible to do this without atime-consuming analysis of errors: Wagner (2012)suggests to use parallel data of authentic errors andcorrections to build the error grammar, first pars-ing the corrections and then guiding the error cre-ation procedure with the edit operations inverse tothe corrections.
Post-editing corpora can play thisrole and have recently become available (Potet etal., 2012).Furthermore, future work should explore theinverse glass-box feature idea with arbitrary tar-get languages for the MT system.
(There is norequirement that the glass-box system translatesback to the original source language).Finally, we would like to integrate referentialtranslation machines (Bic?ici, 2013; Bic?ici andWay, 2014) into our system as they performed wellin the WMT quality estimation tasks this and lastyear.AcknowledgmentsThis research is supported by the European Com-mission under the 7th Framework Programme,specifically its Marie Curie Programme 317471,and by the Science Foundation Ireland (Grant12/CE/I2267) as part of CNGL (www.cngl.ie) atDublin City University.
We thank the anonymousreviewers and Jennifer Foster for their commentson earlier versions of this paper.6The simultaneous increase on RMSE can be explained ifthere is a sufficient number of errors above 0.5: After squar-ing, these errors are still quite small, e. g. 0.36 for an error of0.6, but after rounding, the square error becomes 1.0 or 4.0.332ReferencesJoshua Albrecht and Rebecca Hwa.
2008.
The roleof pseudo references in MT evaluation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 187?190, Columbus, Ohio, June.Association for Computational Linguistics.Ergun Bic?ici and Andy Way.
2014.
Referential trans-lation machines for predicting translation quality.
InProceedings of the Nineth Workshop on StatisticalMachine Translation, Baltimore, USA, June.
Asso-ciation for Computational Linguistics.Ergun Bic?ici.
2013.
Referential translation machinesfor quality estimation.
In Proceedings of the EighthWorkshop on Statistical Machine Translation, pages343?351, Sofia, Bulgaria, August.
Association forComputational Linguistics.Ezra Black, Steve Abney, Dan Flickinger, ClaudiaGdaniec, Robert Grishman, Philip Harrison, DonaldHindle, Robert Ingria, Fred Jelinek, Judith Klavans,Mark Liberman, Mitchell Marcus, Salim Roukos,Beatrice Santorini, and Tomek Strzalkowski.
1991.A procedure for quantitatively comparing the syn-tactic coverage of English grammars.
In E. Black,editor, Proceedings of the HLT Workshop on Speechand Natural Language, pages 306?311, Morristown,NJ, USA.
Association for Computational Linguis-tics.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Victor Chahuneau, Noah A. Smith, and Chris Dyer.2012.
pycdec: A python interface to cdec.
PragueBull.
Math.
Linguistics, 98:51?62.Eugene Charniak and Mark Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the ACL (ACL-05), pages 173?180, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Eugene Charniak.
2000.
A maximum entropy inspiredparser.
In Proceedings of the First Annual Meetingof the North American Chapter of the Associationfor Computational Linguistics (NAACL-00), pages132?139, Seattle, WA.Nadir Durrani, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013.
Edinburgh?s machine trans-lation systems for European language pairs.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 114?121, Sofia, Bulgaria,August.
Association for Computational Linguistics.Jennifer Foster.
2007.
Treebanks gone bad: Parserevaluation and retraining using a treebank of un-grammatical sentences.
International Journal onDocument Analysis and Recognition, 10(3-4):129?145.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT summit, vol-ume 5, pages 79?86.Liangyou Li, Xiaofeng Wu, Santiago Cort?es Va?
?llo, JunXie, Jia Xu, Andy Way, and Qun Liu.
2014.
TheDCU-ICTCAS-Tsinghua MT system at WMT 2014on German-English translation task.
In Proceed-ings of the Nineth Workshop on Statistical MachineTranslation, Baltimore, USA, June.
Association forComputational Linguistics.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre, Ann Bies,Mark Ferguson, Karen Katz, and Britta Schas-berger.
1994.
The Penn Treebank: Annotatingpredicate argument structure.
In Proceedings ofthe 1994 ARPA Speech and Natural LanguageWorkshop, pages 114?119.Michael McCandless, Erik Hatcher, and Otis Gospod-netic.
2010.
Lucene in Action, Second Edition:Covers Apache Lucene 3.0.
Manning PublicationsCo., Greenwich, CT, USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics (ACL02), pages 311?318, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Marion Potet, Emmanuelle Esperanc?a-Rodier, LaurentBesacier, and Herv?e Blanchon.
2012.
Collectionof a large database of French-English SMT outputcorrections.
In Proceedings of the 8th InternationalConference on Language Resources and Evaluation(LREC).Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-hann Roturier, Rasul Samad Zadeh Kaljahi, andFred Hollowood.
2012.
Dcu-symantec submis-sion for the wmt 2012 quality estimation task.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 138?144, Montr?eal,Canada, June.
Association for Computational Lin-guistics.Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-hann Roturier, Rasoul Samad Zadeh Kaljahi, andFred Hollowood.
2013.
DCU-Symantec at the333WMT 2013 quality estimation shared task.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 392?397, Sofia, Bulgaria,August.
Association for Computational Linguistics.Magnus Sahlgren.
2005.
An introduction to randomindexing.
In Methods and Applications of Seman-tic Indexing Workshop at the 7th International Con-ference on Terminology and Knowledge Engineering(TKE), volume 5, Copenhagen, Denmark.Geoffrey Sampson and Anna Babarczy.
2003.
A test ofthe leaf-ancestor metric for parse accuracy.
NaturalLanguage Engineering, 9(4):365?380.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, Manchester, United Kingdom.Kashif Shah, Eleftherios Avramidis, Ergun Bic?ici, andLucia Specia.
2013.
QuEst - design, implemen-tation and extensions of a framework for machinetranslation quality estimation.
The Prague Bulletinof Mathematical Linguistics, 100.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of association for machine transla-tion in the Americas, pages 223?231.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translationsvia ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 612?621, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Lucia Specia, Kashif Shah, Jose G.C.
de Souza, andTrevor Cohn.
2013.
QuEst - a translation quality es-timation framework.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics: System Demonstrations, pages 79?84,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Joachim Wagner, Jennifer Foster, and Josef van Gen-abith.
2009.
Judging grammaticality: Experimentsin sentence classification.
CALICO Journal (SpecialIssue of the 2008 CALICO Workshop on AutomaticAnalysis of Learner Language), 26(3):474?490.Joachim Wagner.
2012.
Detecting grammatical errorswith treebank-induced, probabilistic parsers.
Ph.D.thesis, Dublin City University, Dublin, Ireland.334
