Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 609?617,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsGood Question!
Statistical Ranking for Question GenerationMichael Heilman Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{mheilman,nasmith}@cs.cmu.eduAbstractWe address the challenge of automaticallygenerating questions from reading materialsfor educational practice and assessment.
Ourapproach is to overgenerate questions, thenrank them.
We use manually written rules toperform a sequence of general purpose syn-tactic transformations (e.g., subject-auxiliaryinversion) to turn declarative sentences intoquestions.
These questions are then ranked bya logistic regression model trained on a small,tailored dataset consisting of labeled outputfrom our system.
Experimental results showthat ranking nearly doubles the percentage ofquestions rated as acceptable by annotators,from 27% of all questions to 52% of the topranked 20% of questions.1 IntroductionIn this paper, we focus on question generation (QG)for the creation of educational materials for read-ing practice and assessment.
Our goal is to gener-ate fact-based questions about the content of a givenarticle.
The top-ranked questions could be filteredand revised by educators, or given directly to stu-dents for practice.
Here we restrict our investigationto questions about factual information in texts.We begin with a motivating example.
Considerthe following sentence from the Wikipedia article onthe history of Los Angeles:1 During the Gold Rushyears in northern California, Los Angeles becameknown as the ?Queen of the Cow Counties?
for itsrole in supplying beef and other foodstuffs to hungryminers in the north.Consider generating the following question fromthat sentence: What did Los Angeles become known1?History of Los Angeles.?
Wikipedia.
2009.
WikimediaFoundation, Inc. Retrieved Nov. 17, 2009 from: http://en.wikipedia.org/wiki/History_of_Los_Angeles.as the ?Queen of the Cow Counties?
for?We observe that the QG process can be viewedas a two-step process that essentially ?factors?
theproblem into simpler components.2 Rather than si-multaneously trying to remove extraneous informa-tion and transform a declarative sentence into an in-terrogative one, we first transform the input sentenceinto a simpler sentence such as Los Angeles becomeknown as the ?Queen of the Cow Counties?
for itsrole in supplying beef and other foodstuffs to hungryminers in the north, which we then can then trans-form into a more succinct question.Question transformation involves complex longdistance dependencies.
For example, in the ques-tion about Los Angeles, the word what at the begin-ning of the sentence is a semantic argument of theverb phrase known as .
.
.
at the end of the ques-tion.
The characteristics of such phenomena are (ar-guably) difficult to learn from corpora, but they havebeen studied extensively in linguistics (Ross, 1967;Chomsky, 1973).
We take a rule-based approach inorder to leverage this linguistic knowledge.However, since many phenomena pertaining toquestion generation are not so easily encoded withrules, we include statistical ranking as an integralcomponent.
Thus, we employ an overgenerate-and-rank approach, which has been applied successfullyin areas such as generation (Walker et al, 2001;Langkilde and Knight, 1998) and syntactic parsing(Collins, 2000).
Since large datasets of the appro-priate domain, style, and form of questions are notavailable to train our ranking model, we learn to rankfrom a relatively small, tailored dataset of human-labeled output from our rule-based system.The remainder of the paper is organized as fol-2The motivating example does not exhibit lexical semanticvariations such as synonymy.
In this work, we do not modelcomplex paraphrasing, but believe that paraphrase generationtechniques could be incorporated into our approach.609lows.
?2 clarifies connections to prior work and enu-merates our contributions.
?3 discusses particularterms and conventions we will employ.
?4 discussesrule-based question transformation.
?5 describes thedata used to learn and to evaluate our question rank-ing model, and ?6 then follows with details on theranking approach itself.
We then present and dis-cuss results from an evaluation of ranked questionoutput in ?7 and conclude in ?8.2 Connections with Prior WorkThe generation of questions by humans has long mo-tivated theoretical work in linguistics (e.g., Ross,1967), particularly work that portrays questions astransformations of canonical declarative sentences(Chomsky, 1973).Questions have also been a major topic of studyin computational linguistics, but primarily with thegoal of answering questions (Dang et al, 2008).While much of the question answering research hasfocused on retrieval or extraction (e.g., Ravichan-dran and Hovy, 2001; Hovy et al, 2001), mod-els of the transformation from answers to questionshave also been developed (Echihabi and Marcu,2003) with the goal of finding correct answers givena question (e.g., in a source-channel framework).Also, Harabagiu et al (2005) present a system thatautomatically generates questions from texts to pre-dict which user-generated questions the text mightanswer.
In such work on question answering, ques-tion generation models are typically not evaluatedfor their intrinsic quality, but rather with respect totheir utility as an intermediate step in the questionanswering process.QG is very different from many natural languagegeneration problems because the input is natural lan-guage rather than a formal representation (cf.
Reiterand Dale, 1997).
It is also different from some othertasks related to generation: unlike machine transla-tion (e.g., Brown et al, 1990), the input and outputfor QG are in the same language, and their lengthratio is often far from one to one; and unlike sen-tence compression (e.g., Knight and Marcu, 2000),QG may involve substantial changes to words andtheir ordering, beyond simple removal of words.Some previous research has directly approachedthe topic of generating questions for educationalpurposes (Mitkov and Ha, 2003; Kunichika et al,2004; Gates, 2008; Rus and Graessar, 2009; Rus andLester, 2009), but to our knowledge, none has in-volved statistical models for choosing among outputcandidates.
Mitkov et al (2006) demonstrated thatautomatic generation and manual correction of ques-tions can be more time-efficient than manual author-ing alone.
Much of the prior QG research has evalu-ated systems in specific domains (e.g., introductorylinguistics, English as a Second Language), and thuswe do not attempt empirical comparisons.
Exist-ing QG systems model their transformations fromsource text to questions with many complex rulesfor specific question types (e.g., a rule for creatinga question Who did the Subject Verb?
from asentence with SVO word order and an object refer-ring to a person), rather than with sets of generalrules.This paper?s contributions are as follows:?
We apply statistical ranking to the task of gen-erating natural language questions.
In doing so,we show that question rankings are improved byconsidering features beyond surface characteris-tics such as sentence lengths.?
We model QG as a two-step process of firstsimplifying declarative input sentences and thentransforming them into questions, the latter stepbeing achieved by a sequence of general rules.?
We incorporate linguistic knowledge to explic-itly model well-studied phenomena related to longdistance dependencies in WH questions, such asnoun phrase island constraints.?
We develop a QG evaluation methodology, in-cluding the use of broad-domain corpora.3 Definitions and ConventionsThe term ?source sentence?
refers to a sentencetaken directly from the input document, from whicha question will be generated (e.g., Kenya is locatedin Africa.).
The term ?answer phrase?
refers tophrases in declarative sentences which may serveas targets for WH-movement, and therefore as possi-ble answers to generated questions (e.g., in Africa).The term ?question phrase?
refers to the phrase con-taining the WH word that replaces an answer phrase(e.g., Where in Where is Kenya located?
).610To represent the syntactic structure of sentences,we use simplified Penn Treebank-style phrase struc-ture trees, including POS and category labels, asproduced by the Stanford Parser (Klein and Man-ning, 2003).
Noun phrase heads are selected usingCollins?
rules (Collins, 1999).To implement the rules for transforming sourcesentences into questions, we use Tregex, a treequery language, and Tsurgeon, a tree manipula-tion language built on top of Tregex (Levy and An-drew, 2006).
The Tregex language includes vari-ous relational operators based on the primitive re-lations of immediate dominance (denoted ?<?)
andimmediate precedence (denoted ?.?).
Tsurgeonadds the ability to modify trees by relabeling, delet-ing, moving, and inserting nodes.4 Rule-based OvergenerationMany useful questions can be viewed as lexical, syn-tactic, or semantic transformations of the declarativesentences in a text.
We describe how to model thisprocess in two steps, as proposed in ?1.34.1 Sentence SimplificationIn the first step for transforming sentences into ques-tions, each of the sentences from the source text isexpanded into a set of derived declarative sentences(which also includes the original sentence) by al-tering lexical items, syntactic structure, and seman-tics.
Many existing NLP transformations could po-tentially be exploited in this step, including sentencecompression, paraphrase generation, or lexical se-mantics for word substitution.In our implementation, a set of transformationsderive a simpler form of the source sentence byremoving phrase types such as leading conjunc-tions, sentence-level modifying phrases, and apposi-tives.
Tregex expressions identify the constituentsto move, alter, or delete.
Similar transformationshave been utilized in previous work on headline gen-eration (Dorr and Zajic, 2003) and summarization(Toutanova et al, 2007).To enable questions about syntactically embeddedcontent, our implementation also extracts a set ofdeclarative sentences from any finite clauses, rela-3See Heilman and Smith (2009) for details on the rule-basedcomponent.tive clauses, appositives, and participial phrases thatappear in the source sentence.
For example, it trans-forms the sentence Selling snowballed because ofwaves of automatic stop-loss orders, which are trig-gered by computer when prices fall to certain lev-els into Automatic stop-loss orders are triggered bycomputer when prices fall to certain levels, fromwhich the next step will produce What are triggeredby computer when prices fall to certain levels?.4.2 Question TransformationIn the second step, the declarative sentences de-rived in step 1 are transformed into sets of ques-tions by a sequence of well-defined syntactic andlexical transformations (subject-auxiliary inversion,WH-movement, etc.).
It identifies the answer phraseswhich may be targets for WH-movement and con-verts them into question phrases.4In the current implementation, answer phrases canbe noun phrases or prepositional phrases, which en-ables who, what, where, when, and how much ques-tions.
The system could be extended to transformother types of phrases into other types of questions(e.g., how, why, and what kind of ).
It should benoted that the transformation from answer to ques-tion is achieved by applying a series of general-purpose rules.
This would allow, for example, theaddition of a rule to generate why questions thatbuilds off of the existing rules for subject-auxiliaryinversion, verb decomposition, etc.
In contrast, pre-vious QG approaches have employed separate rulesfor specific sentence types (e.g., Mitkov and Ha,2003; Gates, 2008).For each sentence, many questions may be pro-duced: there are often multiple possible answerphrases, and multiple question phrases for each an-swer phrase.
Hence many candidates may resultfrom the transformations.These rules encode a substantial amount of lin-guistic knowledge about the long distance depen-dencies prevalent in questions, which would be chal-lenging to learn from existing corpora of questionsand answers consisting typically of only thousandsof examples (e.g., Voorhees, 2003).Specifically, the following sequence of transfor-4We leave the generation of correct answers and distractorsto future work.611During the Gold Rush years in northern California,Los Angeles became known as the "Queen of theCow Counties" for its role in supplying beef andother foodstuffs to hungry miners in the north.Los Angeles became known as the "Queen of theCow Counties" for its role in supplying beef andother foodstuffs to hungry miners in the north.Los Angeles became known as the "QotCC" forLos Angeles did become known as the "QotCC" fordid Los Angeles become known as the "QotCC" forWhat did Los Angeles become known as the  "QotCC" for?Source SentenceAnswer Phrase: its role...(other possibilities)(other possibilities)(other possibilities)Sentence SimplificationAnswer Phrase SelectionSubject-AuxiliaryInversionMain Verb DecompositionMovement and Insertion of Question PhraseStatistical Ranking(other possibilities)1.
What became known as ...?2.
What did Los Angeles become known...for?3.
What did Los Angeles become known...as?4.
During the Gold Rush years... ?5.
Whose role in supplying beef...?...NPSVBDVPPPINVBNVPPPIN NPVBVPPPINVBNVPPPIN NPNPVBDWPWHNPSQSBARQFigure 1: An illustration of the sequence of steps for generating questions.
For clarity, trees are not shown for all steps.Also, while many questions may be generated from a single source sentence, only one path is shown.mations is performed, as illustrated in Figure 1:mark phrases that cannot be answer phrases due toconstraints on WH movement (?4.2.1, not in figure);select an answer phrase, remove it, and generate pos-sible question phrases for it (?4.2.2); decompose themain verb; invert the subject and auxiliary verb; andinsert one of the possible question phrases.Some of these steps do not apply in all cases.
Forexample, no answer phrases are removed when gen-erating yes-no questions.4.2.1 Marking Unmovable PhrasesIn English, various constraints determine whetherphrases can be involved in WH-movement and otherphenomena involving long distance dependencies.In a seminal dissertation, Ross (1967) describedmany of these phenomena.
Goldberg (2006) pro-vides a concise summary of them.For example, noun phrases are ?islands?
tomovement, meaning that constituents dominatedby a noun phrase typically cannot undergo WH-movement.
Thus, from John liked the book that Igave him, we generate What did John like?
but not*Who did John like the book that gave him?.We operationalize this linguistic knowledge to ap-propriately restrict the set of questions produced.Eight Tregex expressions mark phrases that cannotbe answer phrases due to WH-movement constraints.For example, the following expression encodesthe noun phrase island constraint described above,where unmv indicates unmovable noun phrases:NP << NP=unmv.4.2.2 Generating Possible Question PhrasesAfter marking unmovable phrases, we iterativelyremove each possible answer phrase and generatepossible question phrases from it.
The system an-notates the source sentence with a set of entity typestaken from the BBN Identifinder Text Suite (Bikelet al, 1999) and then uses these entity labels alongwith the syntactic structure of a given answer phraseto generate zero or more question phrases, each ofwhich is used to generate a final question.
(This stepis skipped for yes-no questions.
)5 Rating Questions for Evaluation andLearning to RankSince different sentences from the input text, as wellas different transformations of those sentences, maybe more or less likely to lead to high-quality ques-tions, each question is scored according to featuresof the source sentence, the input sentence, the ques-tion, and the transformations used in its generation.The scores are used to rank the questions.
This is612an example of an ?overgenerate-and-rank?
strategy(Walker et al, 2001; Langkilde and Knight, 1998).This section describes the acquisition of a setof rated questions produced by the steps describedabove.
Separate portions of these labeled data willbe used to develop a discriminative question ranker(?6), and to evaluate ranked lists of questions (?7).Fifteen native English-speaking university stu-dents rated a set of questions produced from steps1 and 2, indicating whether each question exhibitedany of the deficiencies listed in Table 1.5 If a ques-tion exhibited no deficiencies, raters were asked tolabel it ?acceptable.?
Annotators were asked to readthe text of a newswire or encyclopedia article (?5.1describes the corpora used), and then rate approxi-mately 100 questions generated from that text.
Theywere asked to consider each question independently,such that similar questions about the same informa-tion would receive similar ratings.For a predefined training set, each question wasrated by a single annotator (not the same for eachquestion), leading to a large number of diverse ex-amples.
For the test set, each question was rated bythree people (again, not the same for each question)to provide a more reliable gold standard.
To assignfinal labels to the test data, a question was labeled asacceptable only if a majority of the three raters ratedit as acceptable (i.e., without deficiencies).6An inter-rater agreement of Fleiss?s ?
= 0.42was computed from the test set?s acceptability rat-ings.
This value corresponds to ?moderate agree-ment?
(Landis and Koch, 1977) and is somewhatlower than for other rating schemes.75.1 CorporaThe training and test datasets consisted of 2,807and 428 questions, respectively.
The questions were5The ratings from one person were excluded due to an ex-tremely high rate of accepting questions as error-free and otherirregularities.6The percentages in Table 1 do not add up to 100% for tworeasons: first, questions are labeled acceptable in the test setonly if the majority of raters labeled them as having no defi-ciencies, rather than the less strict criterion of requiring no de-ficiencies to be identified by a majority of raters; second, thecategories are not mutually exclusive.7E.g., Dolan and Brockett (2005) and Glickman et al (2005)report ?
values around 0.6 for paraphrase identification and tex-tual entailment, respectively.generated from three corpora.The first corpus was a random sample from thefeatured articles in the English Wikipedia8 with be-tween 250 and 2,000 word tokens.
This EnglishWikipedia corpus provides expository texts writtenat an adult reading level from a variety of domains,which roughly approximates the prose that a sec-ondary or post-secondary student would encounter.By choosing from the featured articles, we intendedto select well-edited articles on topics of general in-terest.
The training set included 1,328 questionsabout 12 articles, and the test set included 120 ques-tions about 2 articles from this corpus.The second corpus was a random sample from thearticles in the Simple English Wikipedia of simi-lar length.
This corpus provides similar text but ata reading level corresponding to elementary educa-tion or intermediate second language learning.9 Thetraining set included 1,195 questions about 16 arti-cles, and the test set included 118 questions about 2articles from this corpus.The third corpus was Section 23 of the Wall StreetJournal data in the Penn Treebank (Marcus et al,1993).10 The training set included 284 questionsabout 8 articles, and the test set included 190 ques-tions about 2 articles from this corpus.6 RankingWe use a discriminative ranker to rank questions,similar to the approach described by Collins (2000)for ranking syntactic parses.
Questions are rankedby the predictions of a logistic regression model ofquestion acceptability.
Given the question q andsource text t, the model defines a binomial distribu-tion p(R | q, t), with binary random variableR rang-ing over a (?acceptable?)
and u (?unacceptable?
).We estimate the parameters by optimizing the reg-ularized log-likelihood of the training data (cf.
?5.1)with a variant of Newton?s method (le Cessie and8The English and Simple English Wikipedia data weredownloaded on December 16, 2008 from http://en.wikipedia.org and http://simple.wikipedia.org, respectively.9The subject matter of the articles in the two Wikipedia cor-pora was not matched.10In separate experiments with the Penn Treebank, gold-standard parses led to an absolute increase of 15% in the per-centage of acceptable questions (Heilman and Smith, 2009).613Question Deficiency Description %Ungrammatical The question is not a valid English sentence.
(e.g., In what were nests excavated exposed to thesun?
from .
.
.
eggs are usually laid .
.
.
, in nests excavated in pockets of earth exposed to thesun..
This error results from the incorrect attachment by the parser of exposed to the sun to theverb phrase headed by excavated)14.0Does not make sense The question is grammatical but indecipherable.
(e.g., Who was the investment?)
20.6Vague The question is too vague to know exactly what it is asking about, even after reading the article(e.g., What do modern cities also have?
from .
.
.
, but modern cities also have many problems).19.6Obvious answer The correct answer would be obvious even to someone who has not read the article (e.g., aquestion where the answer is obviously the subject of the article).0.9Missing answer The answer to the question is not in the article.
1.4Wrong WH word The question would be acceptable if the WH phrase were different (e.g., a what question with aperson?s name as the answer).4.9Formatting There are minor formatting errors (e.g., with respect to capitalization, punctuation).
8.9Other The question was unacceptable for other reasons.
1.2None The question exhibits none of the above deficiencies and is thus acceptable.
27.3Table 1: Deficiencies a question may exhibit, and the percentages of test set questions labeled with them.van Houwelingen, 1997).
In our experiments, theregularization constant was selected through cross-validation on the training data.The features used by the ranker can be organizedinto several groups described in this section.
Thisfeature set was developed by an analysis of ques-tions generated from the training set.
The num-bers of distinct features for each type are denoted inparentheses, with the second number, after the ad-dition symbol, indicating the number of histogramfeatures (explained below) for that type.Length Features (3 + 24) The set includes integerfeatures for the numbers of tokens in the question,the source sentence, and the answer phrase fromwhich the WH phrase was generated.
These num-bers of tokens will also be used for computing thehistogram features discussed below.WH Words (9 + 0) The set includes boolean fea-tures for the presence of each possible WH word inthe question.Negation (1 + 0) This is a boolean feature for thepresence of not, never, or no in the question.N -Gram Language Model Features (6 + 0) Theset includes real valued features for the log like-lihoods and length-normalized log likelihoods ofthe question, the source sentence, and the answerphrase.
Separate likelihood features are included forunigram and trigram language models.
These lan-guage models were estimated from the written por-tion of the American National Corpus Second Re-lease (Ide and Suderman, 2004), which consists ofapproximately 20 million tokens, using Kneser andNey (1995) smoothing.Grammatical Features (23 + 95) The set includesinteger features for the numbers of proper nouns,pronouns, adjectives, adverbs, conjunctions, num-bers, noun phrases, prepositional phrases, and sub-ordinate clauses in the phrase structure parse treesfor the question and answer phrase.
It also includesone integer feature for the number of modifyingphrases at the start of the question (e.g., as in Atthe end of the Civil War, who led the Union Army?
);three boolean features for whether the main verb isin past, present, or future tense; and one boolean fea-ture for whether the main verb is a form of be.Transformations (8 + 0) The set includes bi-nary features for the possible syntactic transforma-tions (e.g., removal of appositives and parentheti-cals, choosing the subject of source sentence as theanswer phrase).Vagueness (3 + 15) The set includes integer fea-tures for the numbers of noun phrases in the ques-tion, source sentence, and answer phrase that arepotentially vague.
We define this set to include pro-nouns as well as common nouns that are not speci-fied by a subordinate clause, prepositional phrase, orpossessive.
In the training data, we observed manyvague questions resulting from such noun phrases(e.g., What is the bridge named for?
).614Histograms In addition to the integer features forlengths, counts of grammatical types, and counts ofvague noun phrases, the set includes binary ?his-togram?
features for each length or count.
Thesefeatures indicate whether a count or length exceedsvarious thresholds: 0, 1, 2, 3, and 4 for counts; 0,4, 8, 12, 16, 20, 24, and 28 for lengths.
We aim toaccount for potentially non-linear relationships be-tween question quality and these values (e.g., mostgood questions are neither very long nor very short).7 Evaluation and DiscussionThis section describes the results of experiments toevaluate the quality of generated questions beforeand after ranking.
Results are aggregated across the3 corpora (?5.1).
The evaluation metric we employis the percentage of test set questions labeled as ac-ceptable.
For rankings, our metric is the percentageof the top N% labeled as acceptable, for various N .7.1 Results for Unranked QuestionsFirst, we present results for the unranked questionsproduced by the rule-based overgenerator.
As shownin Table 1, 27.3% of test set questions were labeledacceptable (i.e., having no deficiencies) by a major-ity of raters.11The most frequent deficiencies were ungrammati-cality (14.0%), vagueness (19.6%), and semantic er-rors labeled with the ?Does not make sense?
cate-gory (20.6%).
Formatting errors (8.9%) were dueto both straightforward issues with pre-processingand more challenging issues such as failing to iden-tifying named entities (e.g., Who was nixon?s secondvice president?
).While Table 1 provides data on how often badquestions were generated, a measure of how oftengood questions were not generated would requireknowing the number of possible valid questions.
In-stead, we provide a measure of productivity: the sys-tem produced an average of 6.0 acceptable questionsper 250 words (i.e., the approximate average numberof words on a single page in a printed book).7.2 Configurations and BaselinesFor ranking experiments, we present results for thefollowing configurations of features:1112.1% of test set questions were unanimously acceptable.All This configuration includes the entire set offeatures described in ?6.Surface Features This configuration includesonly features that can be computed from the sur-face form of the question, source sentence, andanswer phrase?that is, without hidden linguisticstructures such as parts of speech or syntactic struc-tures.
Specifically, it includes features for lengths,length histograms, WH words, negation, and lan-guage model likelihoods.Question Only This configuration includes allfeatures of questions, but no features involving thesource sentence or answer phrase (e.g., it does notinclude source sentence part of speech counts).
Itdoes not include transformation features.We also present two baselines for comparison:Random The expectation of the performance ifquestions were ranked randomly.Oracle The expected performance if all questionsthat were labeled acceptable were ranked higherthan all questions that were labeled unacceptable.7.3 Ranking ResultsFigure 2 shows that the percentage of questionsrated as acceptable generally increases as the setof questions is restricted from the full 428 ques-tions in the test set to only the top ranked questions.While 27.3% of all test set questions were accept-able, 52.3% of the top 20% of ranked questions wereacceptable.
Thus, the quality of the top fifth wasnearly doubled by ranking with all the features.Ranking with surface features also improvedquestion quality, but to a lesser extent.
Thus, unob-served linguistic features such as parts of speech andsyntax appear to add value for ranking questions.12The ranker seems to have focused on the ?Doesnot make sense?
and ?Vague?
categories.
Thepercentage of nonsensical questions dropped from20.6% to 4.7%, and vagueness dropped from 19.6%12Ranking with all features was statistically significantly bet-ter (p < .05) in terms of the percentage of acceptable questionsin the top ranked 20% than ranking with the ?question only?or ?surface?
configurations, or the random baseline, as verifiedby computing 95% confidence intervals with the BCa Bootstrap(Efron and Tibshirani, 1993).61550%60%70%Pct.
Rated AcceptableOracle All Features Question OnlySurface FeaturesRandom20%30%40%50%60%70% 0100200300400Number of Top-Ranked QuestionsOracle All Features Question OnlySurface FeaturesRandom20%30%40%50%60%70% 0100200300400Number of Top-Ranked QuestionsOracle All Features Question OnlySurface FeaturesRandomFigure 2: A graph of the percentage of acceptable ques-tions in the top-N questions in the test set, using variousrankings, for N varying from 0 to the size of the test set.The percentages become increasingly unstable when re-stricted to very few questions (e.g., < 50).to 7.0%, while ungrammaticality dropped from14.0% to 10.5%, and the other, less prevalent, cat-egories changed very little.137.4 Ablation StudyAblation experiments were also conducted to studythe effects of removing each of the different types offeatures.
Table 2 presents the percentages of accept-able test set questions in the top 20% and top 40%when they are scored by rankers trained with vari-ous feature sets that are defined by removing variousfeature types from the set of all possible features.Grammatical features appear to be the most im-portant: removing them from the feature set resultedin a 9.0% absolute drop in acceptability in the top20% of questions, from 52.3% to 43.3%.Some of the features did not appear to be partic-ularly helpful, notably the N -gram language modelfeatures.
We speculate that they might improve re-sults when used with a larger, less noisy training set.Performance did not drop precipitously upon theremoval of any particular feature type, indicating ahigh amount of shared variance among the features.However, removing several types of features at onceled to somewhat larger drops in performance.
Forexample, using only surface features led to a 12.8%13We speculate that improvements in syntactic parsing andentity recognition would reduce the proportion of ungrammati-cal questions and incorrect WH words, respectively.Features # Top 20% Top 40%All 187 52.3 40.8All ?
Length 160 52.3 42.1All ?
WH 178 50.6 39.8All ?
Negation 186 51.7 39.3All ?
Lang.
Model 181 51.2 39.9All ?
Grammatical 69 43.2 38.7All ?
Transforms 179 46.5 39.0All ?
Vagueness 169 48.3 41.5All ?
Histograms 53 49.4 39.8Surface 43 39.5 37.6Question Only 91 41.9 39.5Random - 27.3 27.3Oracle - 100.0 87.3Table 2: The total numbers of features (#) and the per-centages of the top 20% and 40% of ranked test set ques-tions labeled acceptable, for rankers built from variationsof the complete set of features (?All?).
E.g., ?All ?
WH?is the set of all features except WH word features.drop in acceptability in the top 20%, and using onlyfeatures of questions led to a 10.4% drop.8 ConclusionBy ranking the output of rule-based natural lan-guage generation system, existing knowledge aboutWH-movement from linguistics can be leveraged tomodel the complex transformations and long dis-tance dependencies present in questions.
Also, inthis overgenerate-and-rank framework, a statisticalranker trained from a small set of annotated ques-tions can capture trends related to question qualitythat are not easily encoded with rules.
In our exper-iments, we found that ranking approximately dou-bled the acceptability of the top-ranked questionsgenerated by our approach.AcknowledgmentsWe acknowledge partial support from the Instituteof Education Sciences, U.S. Department of Educa-tion, through Grant R305B040063 to Carnegie Mel-lon University; and from the National Science Foun-dation through a Graduate Research Fellowship forthe first author and grant IIS-0915187 to the secondauthor.
We thank the anonymous reviewers for theircomments.616ReferencesD.
M. Bikel, R. Schwartz, and R. M. Weischedel.
1999.An algorithm that learns what?s in a name.
MachineLearning, 34(1-3).P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2).N.
Chomsky.
1973.
Conditions on transformations.
AFestschrift for Morris Halle.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proc.
of ICML.H.
T. Dang, D. Kelly, and J. Lin.
2008.
Overview ofthe TREC 2007 question answering track.
In Proc.
ofTREC.W.
B. Dolan and C. Brockett.
2005.
Automatically con-structing a corpus of sentential paraphrases.
In Proc.of IWP.B.
Dorr and D. Zajic.
2003.
Hedge Trimmer: A parse-and-trim approach to headline generation.
In Proc.
ofWorkshop on Automatic Summarization.A.
Echihabi and D. Marcu.
2003.
A noisy-channel ap-proach to question answering.
In Proc.
of ACL.B.
Efron and R. Tibshirani.
1993.
An Introduction to theBootstrap.
Chapman & Hall/CRC.D.
M. Gates.
2008.
Generating reading comprehensionlook-back strategy questions from expository texts.Master?s thesis, Carnegie Mellon University.O.
Glickman, I. Dagan, and M. Koppel.
2005.
A prob-abilistic classification approach for lexical textual en-tailment.
In Proc.
of AAAI.A.
Goldberg.
2006.
Constructions at Work: The Na-ture of Generalization in Language.
Oxford Univer-sity Press, New York.S.
Harabagiu, A. Hickl, J. Lehmann, and D. Moldovan.2005.
Experiments with interactive question-answering.
In Proc.
of ACL.Michael Heilman and Noah A. Smith.
2009.
Ques-tion generation via overgenerating transformations andranking.
Technical Report CMU-LTI-09-013, Lan-guage Technologies Institute, Carnegie Mellon Uni-versity.E.
Hovy, U. Hermjakob, and C. Lin.
2001.
The use ofexternal knowledge in factoid QA.
In Proc.
of TREC.N.
Ide and K. Suderman.
2004.
The american nationalcorpus first release.
In Proc.
of LREC.D.
Klein and C. D. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.
InAdvances in NIPS 15.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proc.
of IEEE Int.Conf.
Acoustics, Speech and Signal Processing.K.
Knight and D. Marcu.
2000.
Statistics-based summa-rization - step one: Sentence compression.
In Proc.
ofthe Seventeenth National Conference on Artificial In-telligence and Twelfth Conference on Innovative Ap-plications of Artificial Intelligence.H.
Kunichika, T. Katayama, T. Hirashima, andA.
Takeuchi.
2004.
Automated question generationmethods for intelligent English learning systems andits evaluation.
In Proc.
of ICCE.J.
R. Landis and G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33.I.
Langkilde and Kevin Knight.
1998.
Generation thatexploits corpus-based statistical knowledge.
In Proc.of ACL.S.
le Cessie and J. C. van Houwelingen.
1997.
Ridge es-timators in logistic regression.
Applied Statistics, 41.R.
Levy and G. Andrew.
2006.
Tregex and Tsurgeon:tools for querying and manipulating tree data struc-tures.
In Proc.
of LREC.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of English:The Penn Treebank.
Computational Linguistics, 19.R.
Mitkov and L. A. Ha.
2003.
Computer-aided gen-eration of multiple-choice tests.
In Proc.
of the HLT-NAACL 03 workshop on Building educational appli-cations using natural language processing.R.
Mitkov, L. A. Ha, and N. Karamanis.
2006.
Acomputer-aided environment for generating multiple-choice test items.
Natural Language Engineering,12(2).D.
Ravichandran and E. Hovy.
2001.
Learning surfacetext patterns for a question answering system.
In Proc.of ACL.E.
Reiter and R. Dale.
1997.
Building applied naturallanguage generation systems.
Nat.
Lang.
Eng., 3(1).J.
R. Ross.
1967.
Constraints on Variables in Syntax.Phd dissertation, MIT, Cambridge, MA.V.
Rus and A. Graessar, editors.
2009.
The QuestionGeneration Shared Task and Evaluation Challenge.http://www.questiongeneration.org.V.
Rus and J. Lester, editors.
2009.
Proc.
of the 2ndWorkshop on Question Generation.
IOS Press.K.
Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,H.
Suzuki, and L. Vanderwende.
2007.
The PYTHYsummarization system: Microsoft research at duc2007.
In Proc.
of DUC.E.
M. Voorhees.
2004.
Overview of the TREC 2003question answering track.
In Proc.
of TREC 2003.M.
A. Walker, O. Rambow, and M. Rogati.
2001.
Spot:a trainable sentence planner.
In Proc.
of NAACL.617
