Flex ib le  Pars ing iPhilip J. HayesGeorge V. MouradianComputer  Sc ience  Depar tmentCarneg ie -Me l lon  Un ivers i tyP i t t sburgh ,  Pennsy lvan ia  15213When people use natural language in natural settings, they often use it ungrammatical-ly, leaving out or repeating words, breaking off  and restarting, speaking in fragments, etc.Their human listeners are usually able to cope with these deviations with little difficulty.
Ifa computer system is to accept natural language input from its users on a routine basis, itshould be similarly robust.
In this paper, we outline a set of parsing flexibilities that such asystem should provide.
We go on to describe FlexP, a bottom-up pattern matching parserthat we have designed and implemented to provide many of these flexibilities for restrictednatural language input to a limited-domain computer system.1.
The Importance of Flexible ParsingWhen people use natural language in natural con-versation, they often do not respect grammatical nice-ties.
Instead of speaking sequences of grammaticallywell-formed and complete sentences, people oftenleave out or repeat words or phrases, break off whatthey are saying and rephrase or replace it, speak infragments, or use otherwise incorrect grammar.
Thefollowing example conversation involves a number ofthese grammatical deviations:A: I want ... can you send a memo a messageto to SmithB: Is that John or John Smith or Jim SmithA: JimInstead of being unable or refusing to parse such un-grammatical utterances, human listeners are generallyunperturbed by them.
Neither participant in the abovedialogue, for instance, would have any difficulty.When computers attempt to interact with peopleusing natural language, they face a very similar situa-tion; the people will still tend to deviate from whatev-er grammar the computer system is using.
The factthat the input is typed rather than spoken makes littledifference; grammatical deviations eem to be inherentin spontaneous human use of language whatever themodality.
So, if computers are ever to converse natu-rally with humans, they must be able to parse their1 This research was sponsored by the Air Force Office ofScientific Research under Contract F49620-79-C-0143.inputs as flexibly and robustly as humans do.
Whileconsiderable advances have been made in recent yearsin applied natural anguage processing, few of the sys-tems that have been constructed have paid sufficientattention to the kinds of deviation that will inevitablyoccur in their input if they are used in a natural envi-ronment.
In many cases, if the user's input does notconform to the system's grammar, an indication ofincomprehension followed by a request to rephrasemay be the best he can expect.
We believe that suchinflexibility in parsing severely limits the practicality ofnatural language computer interfaces, and is a majorreason why natural language has yet to find wide ac-ceptance in such applications as database retrieval orinteractive command languages.In this paper, we report on a flexible parser, calledFlexP, suitable for use with a restricted natural lan-guage interface to a limited-domain computer system.We describe first the kinds of grammatical deviationswe are trying to deal with, then the basic design char-acteristics of FlexP with justification for them basedon the kinds of problem to be solved, and finally moredetails of our parsing system with worked examples ofits operation.
These examples, and most of the othersin the paper, represent natural language input to anelectronic mail system that we and others \[2\] are con-structing as part of our research on user interfaces.This system employs FlexP to parse its input.Copyright 1981 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial dvantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires afee and/or specific permission.0362-613X/81/040232-11501.00232 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981Philip J. Hayes and George V. Mouradian Flexible Parsing2.
Types of Grammatical DeviationThere are a number of distinct types of grammati-cal deviation, and not all types are found in all typesof communication situation.
In this section, we firstdefine the restricted type of communication situationthat we will be concerned with, that of a limited-domain computer system and its user communicatingvia a keyboard and display screen.
We then present ataxonomy of grammatical deviations common in thiscontext, and by implication a set of parsing flexibilitiesneeded to deal with them.2.1 Communicat ion  w i th  a L imi ted-Domain  SystemIn the remainder of this paper, we will focus on arestricted type of communication situation, that be-tween a limited-domain system and its user, and onthe parsing flexibilities needed by such a system tocope with the user's inevitable grammatical deviations.Examples of the type of system we have in mind aredatabase retrieval systems, electronic mail systems,medical diagnosis systems, or any systems operating ina domain so restricted that they can completely under-stand any relevant input a user might provide.
Thereare several points to be made.First, although such systems can be expected toparse and understand anything relevant to their do-main, their users cannot be expected to confine them-selves to relevant input.
As Bobrow et.
al.
\[3\] note,users often explain their underlying motivations orotherwise justify their requests in terms quite irrele-vant to the domain of the system.
The result is thatsuch systems cannot expect to parse all their inputseven with the use of flexible parsing techniques.Secondly, a flexible parser is just part of the con-versational component of such a system, and cannotsolve all parsing problems by itself.
For example, if aparser can extract two coherent fragments from anotherwise incomprehensible input, the decisions aboutwhat the system should do next must be made by an-other component of the system.
A decision on wheth-er to jump to a conclusion about what the user intend-ed, to present him with a set of alternative interpreta-tions, or to profess total confusion, can only be madewith information about the history of the conversation,beliefs about the user's goals, and measures of plausi-bility for any given action by the user.
(See \[10\] formore discussion of this broader view of graceful inter-action in man-machine communication.)
Suffice it tosay that we assume a flexible parser is just one compo-nent of a larger system, and that any incomprehen-sions or ambiguities that it finds are passed on to an-other component of the system with access to higher-level information, putting it in a better position todecide what to do next.Finally, we assume that, as is usual for such sys-tems, input is typed, rather than spoken as is normal inhuman conversations.
This simplifies low-level proc-essing tremendously because key-strokes, unlikespeech wave-forms, are unambiguous.
On the otherhand, problems like misspelling arise, and a flexibleparser cannot even assume that segmentation intowords by spaces and carriage returns will always becorrect.
However, such input is still one side of aconversation, rather than a polished text in the mannerof most written material.
As such, it is likely to con-tain many of the same type of errors normally foundin spoken conversations.2.2 MisspellingMisspelling is perhaps the most common class oferror in written language.
Accordingly, it is the formof ungrammaticality hat has been dealt with the mostby language processing systems.
PARRY \[14\], L IFER\[11\], and numerous other systems have tried to correctmisspelt input from their users.An ability to correct spelling implies the existenceof a dictionary of correctly spelled words (possiblyaugmented by a set of morphological rules to producederived forms).
An input word not found in or deriva-ble from the dictionary is assumed tO be misspelt andis compared against each of the dictionary words andtheir derivations.
If one of these words comes closeenough to the input word according to some criteria oflexical matching, it is used in place of the input word.Spelling correction may be attempted in or out ofcontext.
For instance, there is only one reasonablecorrection for "relavent" or for "seperate",  but for aninput like "un"  some kind of context is typically nec-essary as in "I'11 see you un April" or "he was shotwith the stolen un."
In effect, context can be used toreduce the size of the dictionary to be searched forcorrect words.
This both makes the search more effi-cient and reduces the possibility of multiple matches ofthe input against the dictionary.
The L IFER \[11\]system uses the strong constraints typically providedby its semantic grammar in this way to reduce therange of possibilities for spelling correction.A particularly troublesome kind of spelling errorresults in a valid word different from the one intended,as in "show me on of the messages".
Clearly, such anerror can only be corrected through comparisonagainst a contextually restricted subset of a system'svocabulary.2.3 Novel  WordsEven accomplished users of a language will some-times encounter words they do not know.
Such situa-tions are a test of their language learning skills.
If onedid not know the word " fawn" ,  one could at leastAmerican Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 233Philip J. Hayes and George V. Mouradian Flexible Parsingdecide it was a colour from "a fawn colouredsweater".
If one just knew the word as referring to ayoung deer, one might conclude that it was being usedto mean the colour of a young deer.
In general, be-yond making direct inferences about the role of un-known words from their immediate context, vocabu-lary learning can require arbitrary amounts of real-world knowledge and inference, and this is certainlybeyond the capabilities of present day artificial intelli-gence techniques (though see Carbonell \[5\] for workin this direction).There is, however, a very common special subclassof novel words that is well within the capabilities ofpresent day systems: unknown proper names.
Givenan appropriate context, either sentential or discourse,it is relatively straightforward to classify unknownwords as the names of people, places, etc..
Thus in"send copies to Moledeski Chiselov" it is reasonableto conclude from the local context hat "Moledeski" isa first name, "Chiselov" is a surname, and togetherthey identify a person (the intended recipient of thecopies).
Strategies like this were used in the POLITI-CS \[6\], FRUMP \[8\], and PARRY \[14\] systems.Since novel words are by definition not in theknown vocabulary, how can a parsing system distin-guish them from misspellings?
In most cases, the nov-el words will not be close enough to known words toallow successful correction, as in the above example,but this is not always true; an unknown first name of"AI"  could easily be corrected to "all".
Conversely, itis not safe to assume that unknown words in contextswhich allow proper names are really proper names asin: "send copies to al managers".
In this example,"al" probably should be corrected to "all".
In orderto resolve such cases it may be necessary to checkagainst a list of referents for proper names, if this isknown, or otherwise to consider such factors aswhether the initial letters of the words are capitalized.As far as we know, no systems yet constructedhave integrated their handling of misspelt words andunknown proper names to the degree outlined above.However, several applied natural language processingsystems, including the COOP \[12\] system, allow sys-tematic access to a database containing proper nameswithout the need for inclusion of the words in thesystem's parsing vocabulary.2.4 Erroneous segmenting markersWritten text is segmented into words by spaces andnew lines, and into higher level units by commas, peri-ods and other punctuation marks.
Both classes, espe-cially the second, may be omitted or inserted specious-ly.
Spoken language is also segmented, but by thequite different markers of stress, intonation and noisewords and phrases, which we will not consider here.Incorrect segmentation at the lexical level results intwo or more words being run together, as in"runtogether", or a single word being split up into twoor more segments, as in "tog ether" or(inconveniently) "to get her", or combinations ofthese effects as in "runto geth er".
In all cases, itseems natural to deal with such errors by extendingthe spelling correction mechanism to be able to recog-nize target words as initial segments of unknownwords, and vice-versa.
As far as we know, no currentsystems deal with incorrect segmentation i to words.The other type of segmenting error, incorrect punc-tuation, has a much broader impact on parsing metho-dology.
Current parsers typically work one sentenceat a time, and assume that each sentence is terminatedby an explicit end-of-sentence marker.
A flexibleparser must be able to deal with the potential absenceof such a marker, and recognize that the sentence isbeing terminated implicitly by the start of the nextsentence.
In general, a flexible parser should be ableto take advantage of the information provided bypunctuation if it is used correctly, and ignore it if it isused incorrectly.Instead of punctuation, many interactive systemsuse carriage-return to indicate sentence termination.Missing sentence terminators in this case correspondto two sentences on one line, or to the typing of asentence without the terminating return, while spe-cious terminators correspond to typing a sentence onmore than one line.2.5 Broken-Of f  and Restarted Ut terancesIn spoken language, it is very common to break offand restart all or part of an utterance:I want to - -  Could you tell me the name?Was the man -e r -  the official here yesterday?Usually, such restarts are signalled in some way, by"um" or "er", or more explicitly by "let's back up" orsome similar phrase.In written language, such restarts do not normallyoccur because they are erased by the writer before thereader sees them.
Interactive computer systems typi-cally provide facilities for their users to delete the lastcharacter, word, or current line as though it had neverbeen typed, for the very purpose of allowing suchrestarts.
Given these signals, the restarts are easy todetect and interpret.
However, users sometimes fail tomake use of these signals.
Input not containing acarriage-return can be spread over several lines byintermixing of input and output, or a user may simplyfail to type the "kill" character that deletes the inputline he has typed so far, as in:delete the show me all the messages from Smith234 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981Philip J. Hayes and George V. Mouradian Flexible Parsingwhere the user probably intended to erase the first twowords and start over.
A flexible parser should be ableto deal with such non-signalled restarts.2.6 Fragmentary and Otherwise Elliptical InputNaturally occurring language often involves utter-ances that are not complete sentences.
Often the ap-propriateness of such fragmentary utterances dependson conversational or physical context as in:A: Do you mean Jim Smith or Fred Smith?B: JimA: Send a message to SmithB: OKA: with copies to JonesA flexible parser must be able to parse such fragmentsgiven the appropriate context.There is a question here of what such fragmentsshould be parsed into.
Parsing systems that have dealtwith the problem have typically assumed that suchinputs are ellipses of complete sentences, and thattheir parsing involves finding that complete sentence,and parsing it.
Thus the sentence corresponding to" J im" in the example above would be " I  mean J im".Essentially this view has been taken by the L IFER\[11\] and GUS \[3\] systems.
An alternative view is thatsuch fragments are not ellipses of more complete sen-tences, but are themselves complete utterances giventhe context in which they occur, and should be parsedas such.
Certain speech parsers, including HEAR-SAY-II  \[9\] and HWIM \[20\], are oriented towards thismore bottom-up view of fragments, and this view isalso the basis of our approach to fragmentary input, aswe will explain more fully below.
Carbonell \[personalcommunication\] suggests a third view appropriate forsome fragments: that of an extended case frame.
Inthe second example above, for instance, A's "withcopies to Jones"  forms a natural part of the caseframe established by "send a message to Smith".
Yetanother approach to fragment parsing is taken in thePLANES system \[15\] which always parses in terms ofmajor fragments rather than complete utterances.
Thistechnique relies on there being only one way to com-bine the fragments thus obtained, which may be areasonable assumption for many limited domain sys-tems.Ellipses can also occur without regard to context.A type that interactive systems are particularly likelyto face is crypticness in which articles and other non-essential words are omitted as in "show messages afterJune 17" instead of the more complete "show me allmessages dated after June 17".
Again, there is aquestion of whether to consider the cryptic input com-plete, which would mean modifying the system's gram-mar, or to consider it elliptical, and complete it byusing flexible techniques to parse it against the com-plete version as it exists in the standard grammar.Other common forms of ellipsis are associated withconjunction as in:John got up and \[John\] brushed his teeth.Mary saw Bill and Bill \[saw\] Mary.Fred recognized \[the building\] and \[Fred\] walkedtowards the building.
ISince conjunctions can support such a wide range ofellipsis, it is generally impractical to recognize suchutterances by appropriate grammar extensions.
Ef-forts to deal with conjunction have therefore dependedon general mechanisms that supplement the basic pars-ing strategy, as in the LUNAR system \[19\], or thatmodify the grammar temporari ly, as in the work ofKwasny and Sondheimer \[13\].
We have not attemptedto deal with this type of ellipsis in our parsing system,and will not discuss further the type of flexibility itrequires.2.7 Interjected Phrases, Omission, and Subst i tut ionSometimes people interject noise or other qualify-ing phrases into what is otherwise a normal grammati-cal flow as in:I want the message dated I think June 17Such interjections can be inserted at almost any pointin an utterance, and so must be dealt with as theyarise by flexible techniques.It is relatively straightforward for a system of limit-ed comprehension to screen out and ignore standardnoise phrases such as " I  think" or "as far as I cantell".
More troublesome are interjections that cannotbe recognized by the system, as might for instance bethe case inDisplay \[just to refresh my memory\] the messagedated June 17.I want to see the message \[as I forgot what itsaid\] dated June 17.where the unrecognized interjections are bracketed.
Aflexible parser should be able to ignore such interjec-tions.
There is always the chance that the unrecog-nized part was an important part of what the user wastrying to say, but clearly, the problems that arise fromthis cannot be handled by a parser.Omissions of words (or phrases) from the input areclosely related to cryptic input as discussed above, andone way of dealing with cryptic input is to treat it as aset of omissions.
However, in cryptic input only ines-sential information is left out, while it is conceivablethat one could also omit essential information as in:Display the message June 17Here it is unclear whether the speaker means a mes-sage dated on June 17 or before June 17 or after JuneAmerican Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 235Philip J. Hayes and George V. Mouradian Flexible Parsing17.
(We assume that the system addressed can displaythings immediately, or not at all.)
If an omission canbe narrowed down in this way, the parser should beable to generate all the alternatives (for contextualresolution of the ambiguity or for the basis of a ques-tion to the user).
If the omission can be narrowed toone alternative then the input was merely cryptic.Besides omitting words and phrases, people some-times substitute incorrect or unintended ones.
Oftensuch substitutions are spelling errors and should becaught by the spelling correction mechanism, butsometimes they are inadvertent substitutions or uses ofequivalent vocabulary not known to the system.
Thistype of substitution is just like an omission except thatthere is an unrecognized word or phrase in the placewhere the omitted input should have been.
For in-stance, in "the message over June 17", "over"  takesthe place of "dated" or "sent after" or whatever elseis appropriate at that point.
If the substitution is ofvocabulary that is appropriate but unknown to thesystem, parsing of substituted words can provide thebasis of vocabulary extension.
Wilks \[17\] has devel-oped techniques for relaxing semantic constraintswhen they are apparently violated by relations impliedby the input, as in the previous example, where " June17" and "the message" do not fit the normal semanticconstraints of the "over"  relation.2.8 Agreement FailureIt is not uncommon for people to fail to make theappropriate agreement between the various parts of anoun or verb phrase as in :I wants to send a messages to Jim Smith.The appropriate action is to ignore the lack of agree-ment, and Weischedel and Black \[16\] describe a me-thod for relaxing the predicates in an ATN grammarwhich typically check for such agreements.
However,it is generally not possible to conclude locally whichvalue of the marker (number or person) for which theclash occurs is actually intended.2.9 Id iomsIdioms are phrases whose interpretation is not whatwould be obtained by parsing and interpreting themconstructively in the normal way.
They may also notadhere to the standard syntactic rules.
Idioms mustthus be parsed as a whole in a pattern matching kindof mode.
Parsers based purely on pattern matching,like that of PARRY \[14\], are able to parse idiomsnaturally, while others must either add a preprocessingphase of pattern matching as in the LUNAR system\[19\], or mix specific patterns in with more generalrules, as in the work of Kwasny and Sondheimer \[13\].Semantic grammars \[4, 11\] provide a relatively naturalway of mixing idiomatic and more general patterns.2.10 User-Supplied ChangesIn normal human conversation, once something issaid, it cannot be changed, except indirectly by morewords that refer back to the original ones.
In interac-tively typed input, there is always the possibility that auser may notice an error he has made and go back andcorrect it himself, without waiting for the system topursue its own, possibly slow and ineffective, methodsof correction.
With appropriate diting facilities, theuser may do this without erasing intervening words,and, if the system is processing his input on a word-by-word basis, may thus alter a word that the systemhas already processed.
A flexible parser must be ableto take advantage of such user-provided corrections tounknown words, and to prefer them over its own cor-rections.
It must also be prepared to change its parseif the user changes a valid word to another differentbut equally valid word.3.
An  Approach  to Flexible ParsingMost current parsing systems are unable to copewith most of the kinds of grammatical deviation out-lined above.
This is because typical parsing systemsattempt to apply their grammars to their inputs in arigid way, and since deviant input, by definition, doesnot conform to the grammar, they are unable to prod-uce any kind of parse for it at all.
Attempts to parsemore flexibly have typically involved parsing strategiesto be used after a top-down parse using an ATN \[18\]or similar transition net has failed.
Such efforts in-elude the ellipsis and paraphrase mechanisms of LI-FER \[11\], the predicate relaxation techniques of Weis-chedel and Black \[16\], and several of the devices forextending ATN's  proposed by Kwasny  and Sondheim-er \[13\].
An important exception to this observation isthe case of parsers, including HEARSAY- I I  \[9\] andHWIM \[20\], designed for spoken input with its inher-ent low-level uncertainty.
HWIM, in particular, incor-porates techniques to apply an ATN in a bottom-upstyle, and thus can capitalize on whatever points ofcertainty it can find in the input.
Fragmentary input,however, is typically the only type of ungrammaticalitydealt with by speech parsers.In the remainder of this paper, we outline an ap-proach to parsing that was designed with ungrammati-cal input specifically in mind.
We have embodied thisapproach in a working parser, called FlexP, which canapply its grammar to its input flexibly enough to dealwith most of the grammatical deviations discussed inthe previous section.
We should emphasize, however,that FlexP is designed to be used in the interface to arestricted-domain system.
As such, it is intended towork from a domain-specific semantic grammar, ratherthan one suitable for broader classes of input.
FlexPthus does not embody a solution for flexible parsing ofnatural language in general.
In describing FlexP, we236 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981Philip J. Hayes and George V. Mouradian Flexible Parsingwill note those of its techniques that seem unlikely toscale up to use with more complex grammars withwider coverage.We have adopted in FlexP an approach to flexibleparsing based not on ATN's,  but closer to the patternmatching techniques used in the PARRY system \[14\],possibly the most robust natural language processingsystem yet constructed.
At the highest level, the de-sign of FlexP can be characterized by the followingthree features:?
pattern matching: This provides a conven-ient way to recognize idioms, and alsoaids in the detection of omissions andsubstitutions in non-idiomatic phrases.?
bottom-up rather than top-down parsing:This aids in the parsing of f ragmentaryutterances, and in the recognition of in-terjections and restarts.?
parse suspension and continuation: This isimportant for dealing with interjections,restarts, and implicit terminations.In the rest of this section, we examine and justifythese design characteristics in more detail, and in theprocess, give an outline of FlexP's parsing algorithm.3.1 Pattern Match ingWe have chosen to use a grammar of linear pat-terns rather than a transition network because patternmatching meshes well with bottom-up arsing, becauseit facilitates recognition of utterances with omissionsand substitutions, and because it is ideal for the recog-nition of idiomatic phrases.The grammar of the parser is a set of rewrite orproduction rules; the left-hand side of one of theserules is a linear pattern of constituents (lexical or high-er level) and the right-hand side defines a result con-stituent.
Elements of the pattern may be labelled op-tional or allow for repeated matches.
We make theassumption, certainly true for the grammar we arepresently working with, that the grammar will be se-mantic rather than syntactic, with patterns correspond-ing to idiomatic phrases or to object and event de-scriptions meaningful in some limited domain, ratherthan to general syntactic structures.Linear patterns fit well with bottom-up parsingbecause they can be indexed by any of their compo-nents, and because, once indexed, it is straightforwardto confirm whether a pattern matches input alreadyprocessed in a way consistent with the way the patternwas indexed.Patterns help with the detection of omissions andsubstitutions because in either case the relevant pat-tern can still be indexed by the remaining elementsthat appear correctly in the input, and thus the patternas a whole can be recognized even if some of its ele-ments are missing or incorrect.
In the case of substi-tutions, such a technique can actually help focus thespell ing-correction, proper-name-recognit ion,  or vo-cabulary-learning techniques, whichever is appropriate,by isolating the substituted input and the pattern con-stituent that it should have matched.
The (often high-ly selective) restrictions on what can match the con-stituent can then be used to reduce the number ofpossibilities considered by these relatively expensivelexical correction techniques.3.2 Bot tom-Up ParsingOur choice of a bottom-up strategy is based on ourneed to recognize isolated sentence fragments.
If anutterance that would normally be considered only afragment of a complete sentence is to be recognizedtop-down, there are two straightforward approaches totake.
First, the grammar can be altered so that thefragment is recognized as a complete utterance in itsown right.
This is undesirable because it can causeenormous expansion of the grammar, and because itbecomes difficult to decide whether a fragment ap-pears in isolation or as part of a larger utterance, espe-cially if there is the possibility of missing end-of-sentence markers.
The second option is for the parserto infer from the conversational context what gram-matical category (or sequence of sub-categories) thefragment might fit into, and then to do a top-downparse from that sub-category.
This essentially is thetactic used in the GUS \[3\] and L IFER \[11\] systems.This strategy is clearly better than the first one, buthas two problems: first, of predicting all possible sub-categories which might come next, and secondly, ofinefficiency if a large number are predicted.
Kwasnyand Sondheimer \[13\] use a combination of the twostrategies by temporari ly modifying an ATN grammarto accept fragment categories as complete utterancesat the times they are contextually predicted.Bottom-up parsing avoids the problem of trying topredict what grammatical sub-category a sentencefragment should be parsed into.
The data-driven a-ture of bottom-up parsing means that any sub-category can be parsed as an isolated unit in exactlythe same way as a complete sentence, so that no pre-diction about what sub-category to expect is neces-sary.
On the other hand, if a given input can beparsed as more than one sub-category, a bottom-upapproach has no good way of distinguishing betweenthem, even if only one would be predicted top-down.In a system of limited comprehension, fragmentaryrecognition is sometimes necessary because not all ofan input can be recognized, rather than because ofintentional ellipsis.
Here, it may not be possible tomake predictions, and bottom-up parsing has a clearadvantage.
As described below, bottom-up strategies,American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 237Philip J. Hayes and George V. Mouradian Flexible Parsingcoupled with suspended parses, are also helpful inrecognizing interjections and restarts.While well suited to parsing fragmentary input,pure bottom-up arsing (as, for instance, described byChester \[7\]) can result in the generation of an unnec-essarily large number of intermediate structures thatcannot form part of a completed parse.
To reducethese inefficiencies, FlexP does not require all ele-ments of a pattern to be present before including thepattern in the parse structure, and is thus similar to theleft-corner algorithm also described by Chester.
(Forgreater detail see Aho and Ullman \[1\].)
In fact, FlexPis more restrictive than the left-corner algorithm.
Innormal left-to-right processing, if a new word is ac-counted for by a pattern that has already been partial-ly matched by previous input, other patterns indexedby the new word are not considered as possiblematches for that word.
This is a heuristic designed tolimit the number of partial parses that need to be in-vestigated, and could lead to missed parses, but inpractical experience, we have not found this to be aproblem.
Exactly how this heuristic operates will bemade clear by the description of the parsing algorithmin the following section.3.3 Parse Suspension and ContinuationFlexP employs the technique of suspending a parsewith the possibility of later continuation to help withthe recognition of interjections, restarts, and implicitterminations.
To make clear what this means, it isfirst necessary to sketch the operation of the parsingalgorithm as a whole.
This will also serve to clarifythe discussion of bottom-up parsing in the previoussection.
Examples of the algorithm in action are givenin Section 4.FlexP's parsing algorithm maintains a set of partialparses, each of which accounts for the input alreadyprocessed but not yet accounted for by a completedparse.
The parser attempts to incorporate each newinput word into each of the partial parses by one ofthe following methods:1. fitting the word directly into one of thepattern slots available for matching at theright-hand edge of the partial parse;2. finding a chain of non-terminal grammarsub-categories that allow the word to fitindirectly at the right-hand edge of thepartial parse;3. finding a common super-category of theinput word and the sub-category at thetop of the partial parse, so that the partialparse can be extended upwards and theinput word will then fit into it by eithermethod 1 or 2 above;4. same as 1, but based on flexible patternmatching;5. same as 2, but based on flexible patternmatching;6. same as 3, but based on flexible patternmatching.Flexible pattern matching is explained in Section 4.As the description of method 3 implies, FlexP does notbuild the partial parses any higher than is necessary toaccount for the input already processed.
In particular,FlexP does not try to build each partial parse up to acomplete utterance unless a complete utterance isneeded to account for the input seen so far.Which of the six methods is used to incorporate thenew word into the existing set of partial parses is de-termined in the following way.
The parser first triesmethod 1 on each of the partial parses.
For each ofthem, it may succeed in one or more than one way orit may fail.
If it succeeds on any of them, the ones onwhich it fails are discarded, the ones on which it suc-ceeds are extended in all the ways that are possible,the extensions become the new set of partial parses forpossible extension by the next input word, and theother five methods are not tried.
If method 1 fails forall partial parses, the same procedure is repeated formethod 2, and so on.
If no partial parse can be ex-tended by any of the six methods, the entire set ofpartial parses is saved as a suspended parse, and theinput is used either to start a completely new set ofpartial parses, or to extend a previously suspendedparse.
Clearly, the policy of not attempting the morecomplicated methods if the simpler methods succeedcan result in some parses being missed.
However,  inpractice, we have found it heuristically adequate forthe small domain-specific grammar we have been us-ing, and much more efficient than trying all methodsregardless of the outcomes of the others.
On the otherhand, if completeness became important, it would besimple to change FlexP always to try all methods.There are several possible explanations for inputmismatch, i.e.
the failure of an input to extend thecurrently active set of partial parses.?
The input could be an implicit termina-tion, i.e.
the start of a new top-level ut-terance, in which case the previous utter-ance should be assumed complete,?
The input could be a restart, in whichcase the active parse should be abandonedand a new parse started from that point.?
The input could be the start of an inter-jection, in which case the active parseshould be temporari ly suspended, and anew parse started for the interjection.238 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981Philip J. Hayes and George V. Mouradian Flexible ParsingIt is not possible, in general, to distinguish betweenthese cases at the time the mismatch occurs.
If theactive parse is not at a possible termination point, theninput mismatch cannot indicate implicit termination,but may indicate either restart or interjection.
It isnecessary to suspend the active parse and wait to seeif it is continued at the next input mismatch.
On theother hand, if the active parse is at a possible termina-tion point, input mismatch does not rule out interjec-tion or even restart.
In this situation, our algorithmtentatively assumes that there has been an implicittermination, but suspends the active parse anyway forsubsequent potential continuation.Finally, it may be worthwhile to note why we im-plemented FlexP to operate in a breadth-first mode,carrying ambiguous alternative parses along in parallel,rather than investigating them individually depth-first.This choice follows naturally from a decision to parseeach input token immediately after it is typed, whichin turn follows from our desire to deal with implicittermination of input strings (see Section 2.4).
Abreadth-first approach allows the parser to make bestuse of the time during which the user is typing.
Adepth-first implementation could postpone considera-tion of some alternatives until the input had been ter-minated by the user.
In such cases, unacceptably longdelays might result.
Note that the possibility of im-plicit termination also provides justification for thestrategy of parsing each input word immediately afterit is typed.
If the input signals an implicit termination,then the user may well expect the system to respondimmediately to the input thus terminated.4.
FlexP in OperationThis section describes through a series of examplesof gradually increasing complexity how FlexP's parsingalgorithm operates, and how it achieves the flexibilitiesdiscussed earlier.
The implementation used to run theexamples has been used as the parser for an intelligentinterface to an electronic mail system \[2\].
The intelli-gence in this interface is concentrated in a User Agentthat mediates between the user and the underlyingmail system to ensure that the interaction goessmoothly.
The Agent does this by, among otherthings, checking that the user specifies the operationshe wants performed and their parameters correctly andunambiguously, conducting a dialogue with the user iferrors or ambiguities arise.
The role of FlexP as theAgent's parser is to transform the user's input into theinternal representations employed by the Agent, re-solving as many of the errors or potential ambiguitiesthat it can, so as to minimize the amount of interac-tion between Agent and user necessary to arrive at acorrect and unambiguous version of the input.
Usuallythe user's input is a request for action by the mailsystem or a description of objects known to the mailsystem.
Our examples are drawn from that context.4.1 Pre l iminary ExampleSuppose the user typesdisplay new messagesParsing begins as soon as any input is available.
Thefirst word is used as an index into the store of rewriterules.
Each rule gives a pattern and a structure to beproduced when the pattern is matched.
The compo-nents of the structure are built from the structures orwords that match the elements of the pattern.
Theword "display" indexes the rule:(pattern: (Display MessageDescription)result: (StructureType: OperationRequestOperation: DisplayMessage: (Filler MessageDescription) ) )Note that the non-terminals in the pattern of this andsubsequent rules are specific to the message systemdomain, so that.
the grammar being used is semanticrather than syntactic.
Using this rule the parser con-structs the partial parse tree(Display MessageDescription)IIdi splayWe call the partially instantiated pattern that labelsthe upper node a hypothesis.
It represents a possibleinterpretation for a segment of input.The next word "new"  does not directly match thehypothesis, but since "new"  is a MsgAdj (an adjectivethat can modify a description of a message), it indexesthe rule:(pattern: (?Det *MsgAdj MsgHead *MsgCase)result: (StructureType: MessageDescriptionComponents : .
.
.
.
.
.
.
.
.
.
.
.  )
)Here, "?"
means optional, and "*"  means repeatable.For the sake of clarity, we have omitted other prefixesthat distinguish between terminal and non-terminalpattern elements.
The result of this rule is a structureof type MessageDescription that fits the current hy-pothesis, and so extends the parse as follows:(Display MessageO~scription)i l1 (?Det *MsgAdj MsgHead *MsgCase)f IIdi splay newThe top-level hypothesis is not yet fully confirmedeven though all of its own elements are matched.
Itssecond element matches another lower level hypothesisthat is only incompletely matched.
This lower patternbecomes the current hypothesis because it predicts whatshould come next in the input stream.American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981 239Philip J. Hayes and George V. Mouradian Flexible ParsingThe third input matches the category MsgHead(head noun of a message description) and so fits thecurrent hypothesis.
This match fills the last non-optional slot in that pattern.
By doing so it makes thecurrent hypothesis and its parent pattern potentiallycomplete.
When the parser finds a potentially com-plete phrase whose result is of interest to the Agent(and the parent phrase in this example is in that cate-gory), the result is constructed and sent to the Agent.
2However, since the parser has not seen a terminationsignal, this parse is kept active.
The input seen so farmay be only a prefix for some longer utterance such as"display new messages about ADA" .
In this case"about  ADA"  would be recognized as a match forMsgCase (a prepositional phrase that can be part of amessage description), the parse would be extended,and a revision of the previous structure sent to theAgent.4.2 Unrecognized WordsWhen an input word cannot be found in the dic-tionary, FlexP tries to spelling-correct the input wordagainst a list of possibilities derived from the currenthypothesis.
For example:display the new messaegsproduces the partial parse(DisplaYl MessageD~scription)I Il (?Det *MsgAdj MsgHead *MsgCase)I I II I Idisplay the newThe lower pattern is the current hypothesis and hastwo elements eligible to match the next input.
Anoth-er MsgAdj could be matched.
A match for MsgHeadwould also fit.
Both elements have associated lists ofwords that match them or occur in phrases that matchthem.
The one for MsgHead includes the word"messages",  and the spelling corrector passes this backto the main part of the parser as the most likely inter-pretation.In some cases the spelling corrector produces ever-al likely alternatives.
The parser handles such ambigu-ous words using the same mechanisms that accommo-date phrases with ambiguous interpretations; that is,alternative interpretations are carried along until thereis enough input to discriminate those that are plausiblefrom those that are not.
The details are given in thenext section.2 What happens to the result when the Agent receives it isbeyond the scope of this paper.
However, we should note that theAgent is not obliged to act on the result right away.
One strategyis for the Agent to perform immediately "safe" actions, such as theidentification or display of a set of messages, but to wait for explicittermination of "unsafe" commands, such as those to send or deletemessages.The user may also correct the input text himself.These changes are handled in much the same way asthose proposed by the spelling corrector.
Of course,these user-supplied changes are given priority, andparses built using the former version must be modifiedor discarded.4.3 Ambiguous InputIn the first example there was only one hypothesisabout the structure of the input.
More generally, theremay be several hypotheses that provide competinginterpretations about what has already been seen andwhat will appear next.
Until these partial parses arefound to be inconsistent with the actual input, they arecarried along as part of the active parse.
Thereforethe active parse is a set of partial parse trees each witha top-level hypothesis about the overall structure ofthe input so far and a current hypothesis concerningthe next input.
The actual implementat ion allowssharing of common structure among competing hy-potheses and so is more efficient than this descriptionsuggests.The inputwere there any messages on .......could be completed by giving a date (".. .on Tuesday")or a topic (".
.
.on ADA") .
Consequently,  the sub-phrase "any messages on" results in two partial pars-(?Detl *MsgAdj MsgHead *MsgCase)I I Iany messages (On Date) IIones:(?Detl *MsgAdj MsgHead *Msg~ase)I I Iany messages (On Topic)IIonIf the next input were "Tuesday"  it would be consist-ent with the first parse, but not the second.
"Since oneof the alternatives does account for the input, thosethat do not may be discarded.
On the other hand, ifall the partial parses fail to match the input, otheraction is taken.
We consider such situations in thesection on suspended parses.4.4 Flexible MatchingThe only flexibility described so far is that allowedby the optional elements of patterns.
If omissions canbe anticipated, allowances may be built into the gram-mar.
In this section we show how other omissionsmay be handled and other flexibilities achieved byallowing additional freedom in the way an item is al-lowed to match a pattern.
There are two ways inwhich the matching criteria may be relaxed, namely240 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981Phi l ip J. Hayes  and George V. Mourad ian  Flexible Pars ing?
relax consistency constraints, e.g.
numberagreement?
allow out-of-order matchesConsistency constraints are predicates that areattached to rules.
They assert relationships that musthold among the items which fill the pattern, e.g.
num-ber agreement.
Although such relationships can usual-ly (it depends on the particular relation) also be ex-pressed through context-free rewrite rules, they can beexpressed much more compactly through consistencyconstraints.
The compactness that can be achieved inthis way has often been exploited by augmentingcontext-free parsers to deal with consistency con-straints on their context-free rules.
The flexibilityachieved by relaxing such constraints in ATN parsers\[18\] has been explored by Weischedel and Black \[16\]and by Kwasny and Sondheimer \[13\].
This techniquewould fit smoothly into FlexP but has not actuallybeen needed or used in our current application.On the other hand, out-of-order matching is essen-tial for the parser's approach to errors of omission,transposition, and substitution.
Even when strictlyinterpreted, several elements of a pattern may be eligi-ble to match the next input item.
For example, in thepattern for a MessageDescription(?Det *MsgAdj MsgHead *MsgCase)each of the first three elements is initially eligible butthe last is not.
On the other hand, once MsgHead hasbeen matched, only the last element is eligible underthe strict interpretation of the pattern.Consider the inputdisplay new about ADAThe first two words parse normally to produce(DisplaYl MessageDesc~iption)I II (?Det *MsgAdj MsgHead *MsgCase)I II Ldisplay newThe next word does not fit that hypothesis.
The twoeligible elements predict either another message adjec-tive or a MsgHead.
The word "about"  does not matcheither of these, nor can the parser construct any pathto them using intermediate hypotheses.
Since thereare no other partial parses available to account for thisinput, and since normal matching fails, flexible match-ing is tried.First, previously skipped elements are compared tothe input.
In this example, the element ?Det is consid-ered but does not match.
Next, elements to the rightof the eligible elements are considered.
Thus MsgCaseis considered even though the non-optional elementMsgHead has not been matched.
This succeeds andallows the partial parse to be extended to(DisplayI(?Det *MsgAdjIIIdisplay newMessageDesc~iption)MsgHead *Msg~ase)I(About topic)Iaboutwhich correctly predicts the final input item.
3 As al-ready described, flexible matching is applied using thesame three methods used for the initial non-flexiblematching, i.e.
first for direct matches with patternelements in the current partial parses, then for indirectmatches, and then for matches that involve extendingthe partial parses upwards.Unrecognizable substitutions are also handled bythis flexible matching mechanism.
In the phrasedisplay the new stuff about ADAthe word "stuf f"  is not found in the dictionary, sospelling correction is tried but does not produce anyplausible alternatives.
However, the remaining inputscan be parsed by simply omitting "stuf f"  and using theflexible matching procedure.
Transpositions are han-dled through one application of flexible matching ifthe first element of the transposed pair is optional,two applications if not.4.5 Suspended ParsesInterjections are more common in spoken than inwritten language but do occur in typed input some-times.
To deal with such input, our design allows forblocked parses to be suspended rather than merelydiscarded.Users, especially novices, may embellish their inputwith words and phrases that do not provide essentialinformation and cannot be specifically anticipated.Consider two examples:display please messages dated June 17display for me messages dated June 17In the first case, the interjected word "please" couldbe recognized as a common noise phrase that meansnothing to the Agent except possibly to suggest thatthe user is a novice.
The second example is moredifficult.
Both words of the interjected phrase canappear in a number of legitimate and meaningful con-structions; they cannot be ignored so easily.3 This technique can, of course, produce parses in whichrequired pattern elements have no match.
Whether these matchfailures are important enough to warrant interrogation of the user isdetermined in our example system by the intelligent User Agentwhich interprets the input.
In the case above, failure to match theMsgHead element would not require further interaction because themeaning is completely determined by the non-terminal e ement, butinteraction would be required if, for instance, the topic elementfailed to match.Amer ican  Journal  of Computat iona l  L inguist ics,  Vo lume 7, Number  4, October -December  1981 241Philip J. Hayes and George V. Mouradian Flexible ParsingFor  the latter example, parse suspension works asfollows.
After the first word, the active parse containsa single partial parse:(DisplayldisplayMessageDescription)The next word does not  fit this hypothesis,  so it issuspended.
In its place, a new active parse is con-structed.
It contains everal partial parses including(For Person) and (For Timelnterval)I II Ifor forThe next word confirms the first of these, but  thefourth word "messages"  does not.
When the parserf inds that it cannot  extend the active parse, it consid-ers the suspended parse.
Since "messages" fits, theactive and suspended parses are exchanged and theremainder  of the input processed normal ly,  so that theparser recognizes "display messages dated June 17" asif it did not conta in " for  me".5.
Conclus ionWhen people use language natural ly,  they makemistakes and employ economies  of express ion thatoften result in language that is ungrammatica l  by strictstandards.
In particular, such grammatical  deviat ionswill inevitably occur in the input of a computer  systemthat allows its users to employ natural  anguage.
Sucha computer  system must,  therefore,  be prepared toparse its input flexibly, if it is to avoid frustrat ion forits users.In this paper, we have out l ined the main kinds off lexibi l ity that should be provided by a natura l  lan-guage parser intended for natural  use.
We have alsodescribed a bot tom-up attern matching parser, FlexP,which exhibits many of these flexibilities, and which issuitable for restr icted natura l  language input  to al imited-domain system.AcknowledgementsThe authors thank the referees for their extremelydetai led comments and Michael McCord  and GeorgeHeidorn for their helpful edit ing suggestions.References1.
Aho, A. V. and Ullman, J. D. The Theory of Parsing, Transla-tion, and Compiling.
Prentice-Hall, Englewood Cliffs, 1972.2.
Ball, J. E. and Hayes, P. J.
Representation of Task-Independent Knowledge in a Gracefully Interacting User Inter-face.
Proc.
1st Ann.
Mtg.
of the AAAI, Stanford University,August, 1980, pp.
116-120.3.
Bobrow, D. G., Kaplan, R. M., Kay, M., Norman D. A.,Thompson, H., and Winograd, T. "GUS: a Frame-DrivenDialogue System."
Artif.
Intell.
8 (1977), 155-173.4.
Burton, R. R. Semantic Grammar: An Engineering Techniquefor Constructing Natural Language Understanding Systems.
TR3453, Bolt Beranek and Newman, Inc., Cambridge, Mass.,December, 1976.5.
Carbonell, J. G. Towards a Self-Extending Parser.
Proc.
17thAnn.
Mtg.
of the ACL, La Jolla, Ca., August, 1979, pp.
3-7.6.
Carbonell, J. G. Subjective Understanding: Computer Models ofBelief Systems.
Ph.D.
Thesis., Yale University, 1979.7.
Chester, D. "A Parsing Algorithm That Extends Phrases."Am.
J. Comp.
Ling.
6, 2 (1980), 87-96.8.
DeJong, G. Skimming Stories in Real-Time.
Ph.D. Thesis.,Computer Science Dept., Yale University, 1979.9.
Erman, L. D., and Lesser, V. R. HEARSAY-II: Tutorial Intro-duction and Retrospective View.
Tech.
Report, ComputerScience Department, Carnegie-Mellon University, 1978.10.
Hayes, P. J., and Reddy, R. Graceful Interaction in Man-Machine Communication.
Proc.
6th IJCAI, Tokyo, 1979, pp.372-374.11.
Hendrix, G. G. Human Engineering for Applied Natural Lan-guage Processing.
Proc.
5th IJCAI, MIT, 1977, pp.
183-191.12.
Kaplan, S. J.
Cooperative Responses from a Portable NaturalLanguage Data Base Query System.
Ph.D. Thesis, Dept.
ofComp.
and Inform.
Science, University of Pennsylvania, 1979.13.
Kwasny, S. C. and Sondheimer, N. K. "Relaxation Techniquesfor Parsing Grammatically Ill-Formed Input in Natural Lan-guage Understanding Systems."
Am.
J. Comp.
Ling.
7, 2 (1981),99-108.14.
Parkison, R. C., Colby, K. M., and Faught, W. S."Conversational Language Comprehension Using IntegratedPattern-Matching and Parsing."
Artif.
Intell.
9 (1977), 111-134.15.
Waltz, D. L. "An English Language Question AnsweringSystem for a Large Relational Data Base."
Comm.
ACM 21, 7(1978), 526-539.16.
Weischedel, R. M. and Black, J.
"Responding Intelligently toUnparsable Inputs."
Am.
J. Comp.
Ling.
6, 2, (1980), 97-109.17.
Wilks, Y.
A.
Preference Semantics.
In Formal Semantics ofNatural Language, Keenan, Ed., Cambridge Univ.
Press, 1975.18.
Woods, W. A.
"Transition Network Grammars for NaturalLanguage Analysis."
Comm.
ACM 13, 10 (Oct. 1970), 591-606.19.
Woods, W. A., Kaplan, R. M., and Nash-Webber, B. TheLunar Sciences Language System: Final Report.
TR 2378, BoltBeranek and Newman, Inc., Cambridge, Mass., 1972.20.
Woods, W. A., Bates, M., Brown, G., Bruce, B., Cook, C.,Klovstad, J., Makhoul, J., Nash-Webber, B., Schwartz, R.,Wolf, J., and Zue, V. Speech Understanding Systems - FinalTechnical Report.
TR 3438, Bolt Beranek and Newman, Inc.,Cambridge, Mass., 1976.Phil ip J. Hayes is a research computer scientist inthe Department o f  Computer Science at Carnegie-MellonUniversity.
He received the D.Sc.
degree in computerscience f rom the Ecole polytechnique federale de Lau-sanne in 1977.George V. Mouradian is a research programmer inthe Department o f  Computer Science at Carnegie-MellonUniversity.
He received the M.Ph.
degree in computerscience f rom Syracuse University in 1978.242 American Journal of Computational Linguistics, Volume 7, Number 4, October-December 1981
