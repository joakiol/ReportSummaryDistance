Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583?593,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsData-Driven Response Generation in Social MediaAlan RitterComputer Sci.
& Eng.University of WashingtonSeattle, WA 98195aritter@cs.washington.eduColin CherryNational Research Council CanadaOttawa, Ontario, K1A 0R6Colin.Cherry@nrc-cnrc.gc.caWilliam B. DolanMicrosoft ResearchRedmond, WA 98052billdol@microsoft.comAbstractWe present a data-driven approach to generat-ing responses to Twitter status posts, based onphrase-based Statistical Machine Translation.We find that mapping conversational stimulionto responses is more difficult than translat-ing between languages, due to the wider rangeof possible responses, the larger fraction ofunaligned words/phrases, and the presence oflarge phrase pairs whose alignment cannot befurther decomposed.
After addressing thesechallenges, we compare approaches based onSMT and Information Retrieval in a humanevaluation.
We show that SMT outperformsIR on this task, and its output is preferred overactual human responses in 15% of cases.
Asfar as we are aware, this is the first work toinvestigate the use of phrase-based SMT to di-rectly translate a linguistic stimulus into an ap-propriate response.1 IntroductionRecently there has been an explosion in the numberof people having informal, public conversations onsocial media websites such as Facebook and Twit-ter.
This presents a unique opportunity to buildcollections of naturally occurring conversations thatare orders of magnitude larger than those previouslyavailable.
These corpora, in turn, present new op-portunities to apply data-driven techniques to con-versational tasks.We investigate the problem of response genera-tion: given a conversational stimulus, generate anappropriate response.
Specifically, we employ alarge corpus of status-response pairs found on Twit-ter to create a system that responds to Twitter statusposts.
Note that we make no mention of context, in-tent or dialogue state; our goal is to generate any re-sponse that fits the provided stimulus; however, wedo so without employing rules or templates, with thehope of creating a system that is both flexible andextensible when operating in an open domain.Success in open domain response generationcould be immediately useful to social media plat-forms, providing a list of suggested responses to atarget status, or providing conversation-aware auto-complete for responses in progress.
These featuresare especially important on hand-held devices (Has-selgren et al, 2003).
Response generation shouldalso be beneficial in building ?chatterbots?
(Weizen-baum, 1966) for entertainment purposes or compan-ionship (Wilks, 2006).
However, we are most ex-cited by the future potential of data-driven responsegeneration when used inside larger dialogue sys-tems, where direct consideration of the user?s utter-ance could be combined with dialogue state (Wongand Mooney, 2007; Langner et al, 2010) to generatelocally coherent, purposeful dialogue.In this work, we investigate statistical machinetranslation as an approach for response generation.We are motivated by the following observation: Innaturally occurring discourse, there is often a strongstructural relationship between adjacent utterances(Hobbs, 1985).
For example, consider the stimulus-response pair from the data:Stimulus: I?m slowly making this soup...... and it smells gorgeous!583Response: I?ll bet it looks delicious too!HahaHere ?it?
in the response refers to ?this soup?
inthe status by co-reference; however, there is also amore subtle relationship between the ?smells?
and?looks?, as well as ?gorgeous?
and ?delicious?.
Par-allelisms such as these are frequent in naturally oc-curring conversations, leading us to ask whether itmight be possible to translate a stimulus into an ap-propriate response.
We apply SMT to this problem,treating Twitter as our parallel corpus, with statusposts as our source language and their responses asour target language.
However, the established SMTpipeline cannot simply be applied out of the box.We identify two key challenges in adapting SMTto the response generation task.
First, unlike bilin-gual text, stimulus-response pairs are not semanti-cally equivalent, leading to a wider range of possibleresponses for a given stimulus phrase.
Furthermore,both sides of our parallel text are written in the samelanguage.
Thus, the most strongly associated wordor phrase pairs found by off-the-shelf word align-ment and phrase extraction tools are identical pairs.We address this issue with constraints and features tolimit lexical overlap.
Secondly, in stimulus-responsepairs, there are far more unaligned words than inbilingual pairs; it is often the case that large portionsof the stimulus are not referenced in the responseand vice versa.
Also, there are more large phrase-pairs that cannot be easily decomposed (for examplesee figure 2).
These difficult cases confuse the IBMword alignment models.
Instead of relying on thesealignments to extract phrase-pairs, we consider allpossible phrase-pairs in our parallel text, and applyan association-based filter.We compare our approach to response genera-tion against two Information Retrieval or nearestneighbour approaches, which use the input stimu-lus to select a response directly from the trainingdata.
We analyze the advantages and disadvantagesof each approach, and perform an evaluation usinghuman annotators from Amazon?s Mechanical Turk.Along the way, we investigate the utility of SMT?sBLEU evaluation metric when applied to this do-main.
We show that SMT-based solutions outper-form IR-based solutions, and are chosen over actualhuman responses in our data in 15% of cases.
As faras we are aware, this is the first work to investigatethe feasibility of SMT?s application to generating re-sponses to open-domain linguistic stimuli.2 Related WorkThere has been a long history of ?chatterbots?
(Weizenbaum, 1966; Isbell et al, 2000; Shaikh etal., 2010), which attempt to engage users, typicallyleading the topic of conversation.
They usually limitinteractions to a specific scenario (e.g.
a Rogerianpsychotherapist), and use a set of template rules forgenerating responses.
In contrast, we focus on thesimpler task of generating an appropriate responseto a single utterance.
We leverage large amounts ofconversational training data to scale to our SocialMedia domain, where conversations can be on justabout any topic.Additionally, there has been work on generat-ing more natural utterances in goal-directed dia-logue systems (Ratnaparkhi, 2000; Rambow et al,2001).
Currently, most dialogue systems rely on ei-ther canned responses or templates for generation,which can result in utterances which sound veryunnatural in context (Chambers and Allen, 2004).Recent work has investigated the use of SMT intranslating internal dialogue state into natural lan-guage (Langner et al, 2010).
In addition to dialoguestate, we believe it may be beneficial to considerthe user?s utterance when generating responses in or-der to generate locally coherent discourse (Barzilayand Lapata, 2005).
Data-driven generation based onusers?
utterances might also be a useful way to fill inknowledge gaps in the system (Galley et al, 2001;Knight and Hatzivassiloglou, 1995).Statistical machine translation has been applied toa smo?rga?sbord of NLP problems, including questionanswering (Echihabi and Marcu, 2003), semanticparsing and generation (Wong and Mooney, 2006;Wong and Mooney, 2007), summarization (Daume?III and Marcu, 2009), generating bid-phrases in on-line advertising (Ravi et al, 2010), spelling correc-tion (Sun et al, 2010), paraphrase (Dolan et al,2004; Quirk et al, 2004) and query expansion (Rie-zler et al, 2007).
Most relevant to our efforts is thework by Soricut and Marcu (2006), who applied theIBM word alignment models to a discourse order-ing task, exploiting the same intuition investigated584in this paper: certain words (or phrases) tend to trig-ger the usage of other words in subsequent discourseunits.
As far as we are aware, ours is the first workto explore the use of phrase-based translation in gen-erating responses to open-domain linguistic stimuli,although the analogy between translation and dia-logue has been drawn (Leuski and Traum, 2010).3 DataFor learning response-generation models, we usea corpus of roughly 1.3 million conversationsscraped from Twitter (Ritter et al, 2010; Danescu-Niculescu-Mizil et al, 2011).
Twitter conversationsdon?t occur in real-time as in IRC; rather as in email,users typically take turns responding to each other.Twitter?s 140 character limit, however, keeps con-versations chat-like.
In addition, the Twitter APImaintains a reference from each reply to the postit responds to, so unlike IRC, there is no need forconversation disentanglement (Elsner and Charniak,2008; Wang and Oard, 2009).
The first message of aconversation is typically unique, not directed at anyparticular user but instead broadcast to the author?sfollowers (a status message).
For the purposes ofthis paper, we limit the data set to only the first twoutterances from each conversation.
As a result ofthis constraint, any system trained with this data willbe specialized for responding to Twitter status posts.4 Response Generation as TranslationWhen applied to conversations, SMT models theprobability of a response r given the input status-post s using a log-linear combination of featurefunctions.
Most prominent among these featuresare the conditional phrase-translation probabilitiesin both directions, P (s|r) and P (r|s), which ensurer is an appropriate response to s, and the languagemodel P (r), which ensures r is a well-formed re-sponse.
As in translation, the response models areestimated from counts of phrase pairs observed inthe training bitext, and the language model is builtusing n-gram statistics from a large set of observedresponses.
To find the best response to a given inputstatus-post, we employ the Moses phrase-based de-coder (Koehn et al, 2007), which conducts a beamsearch for the best response given the input, accord-ing to the log-linear model.what .
.
.
 time .
.
.
 u  .
.
.
.get .
 .
.
.out .
.
 .
.?
.
.
.
.
.i getoff at 5Figure 1: Example from the data where word alignmentis easy.
There is a clear correspondence between wordsin the status and the response.4.1 Challenge: Lexical RepetitionWhen applied directly to conversation data, off-the-shelf MT systems simply learn to parrot back theinput, sometimes with slight modification.
For ex-ample, directly applying Moses with default settingsto the conversation data produces a system whichyields the following (typical) output on the aboveexample:Stimulus: I?m slowly making this soup...... and it smells gorgeous!Response: i?m slowly making this soup...... and you smell gorgeous!This ?paraphrasing?
phenomenon occurs becauseidentical word pairs are frequently observed togetherin the training data.
Because there is a wide rangeof acceptable responses to any status, these identicalpairs have the strongest associations in the data, andtherefore dominate the phrase table.
In order to dis-courage lexically similar translations, we filter outall phrase-pairs where one phrase is a substring ofthe other, and introduce a novel feature to penalizelexical similarity:?lex(s, t) = J(s, t)Where J(s, t) is the Jaccard similarity between theset of words in s and t.4.2 Challenge: Word AlignmentAlignment is more difficult in conversational datathan bilingual data (Brown et al, 1990), or textualentailment data (Brockett, 2006; MacCartney et al,2008).
In conversational data, there are some casesin which there is a decomposable alignment between585if .
.
.
.anyones .
.
.
.still .
.
.
.awake .
.
.
.lets .
.
.
.play .
.
.
.a .
.
.
.game.
.
.
.
.name    .3    .kevin    .costner    .movies    .that    .dont    .suck    .. .
.
.
easierquestionplease.Figure 2: Example from the data where word alignmentis difficult (requires alignment between large phrases inthe status and response).words, as seen in figure 1, and some difficult caseswhere alignment between large phrases is required,for example figure 2.
These difficult sentence pairsconfuse the IBM word alignment models which haveno way to distinguish between the easy and hardcases.We aligned words in our parallel data using thewidely used tool GIZA++ (Och and Ney, 2003);however, the standard growing heuristic resulted invery noisy alignments.
Precision could be improvedconsiderably by using the intersection of GIZA++trained in two directions (s?
r, and r ?
s), but thealignment also became extremely sparse.
The aver-age number of alignments-per status/response pairin our data was only 1.7, as compared to a datasetof aligned French-English sentence pairs (the WMT08 news commentary data) where the average num-ber of intersection alignments is 14.Direct Phrase Pair ExtractionBecause word alignment in status/response pairs isa difficult problem, instead of relying on local align-ments for extracting phrase pairs, we exploit infor-mation from all occurrences of the pair in determin-C(s, t) C(s,?t) C(s)C(?s, t) C(?s,?t) N ?
C(s)C(t) N ?
C(t) NFigure 3: Contingency table for phrase pair (s,t).
Fisher?sExact Test estimates the probability of seeing this event,or one more extreme assuming s and t are independent.ing whether its phrases form a valid mapping.We consider all possible phrase-pairs in the train-ing data,1 then use Fisher?s Exact Test to filter outpairs with low correlation (Johnson et al, 2007).Given a source and target phrase s and t, we considerthe contingency table illustrated in figure 3, whichincludes co-occurrence counts for s and t, the num-ber of sentence-pairs containing s, but not t and viceversa, in addition to the number of pairs containingneither s nor t. Fisher?s Exact Test provides us withan estimate of the probability of observing this table,or one more extreme, assuming s and t are indepen-dent; in other words it gives us a measure of howstrongly associated they are.
In contrast to statisticaltests such as ?2, or the G2 Log Likelihood Ratio,Fisher?s Exact Test produces accurate p-values evenwhen the expected counts are small (as is extremelycommon in our case).In Fisher?s Exact Test, the hypergeometric proba-bility distribution is used to compute the exact prob-ability of a particular joint frequency assuming amodel of independence:C(s)!C(?s)!C(t)!C(?t)!N !C(s, t)!C(?s, t)!C(s,?t)!C(?s,?t)!The statistic is computed by summing the prob-ability for the joint frequency in Table 3, and ev-ery more extreme joint frequency consistent with themarginal frequencies.
Moore (2004) illustrates sev-eral tricks which make this computation feasible inpractice.We found that this approach generates phrase-table entries which appear quite reasonable uponmanual inspection.
The top 20 phrase-pairs (after fil-tering out identical source/target phrases, substrings,1We define a possible phrase-pair as any pair of phrasesfound in a sentence-pair from our training corpus, where bothphrases consist of 4 tokens or fewer.
The total number of phrasepairs in a sentence pair (s, r) is O(|s| ?
|r|).586Source Targetrt [retweet] thanks for thepotter harryice creamhow are you you ?good morningchuck norriswatching moviei miss miss you tooare you i ?mmy birthday happy birthdaywish me luck good luckhow was it wasmiss you i missswine flui love you love you toohow are are you ?did you i didjackson michaelhow are you i ?m goodmichael mjTable 1: Top 20 Phrase Pairs ranked by the Fisher ExactTest statistic.
Slight variations (substrings or symmetricpairs) were removed to show more variety.
See the sup-plementary materials for the top 10k (unfiltered) pairs.and symmetric pairs) are listed in Table 1.2 Our ex-periments in ?6 show that using the phrase table pro-duced by Fisher?s Exact Test outperforms one gen-erated based on the poor quality IBM word align-ments.4.3 System DetailsFor the phrase-table used in the experiments (?6) weused the 5M phrases with highest association ac-cording the Fisher Exact Test statistic.3 To buildthe language model, we used all of the 1.3M re-sponses from the training data, along with roughly1M replies collected using Twitter?s streaming API.2See the supplementary materials for the top 10k (unfiltered)phrase pairs.3Note that this includes an arbitrary subset of the (1,1,1)pairs (phrase pairs where both phrases were only observed oncein the data).
Excluding these (1,1,1) pairs yields a rather smallphrase table, 201K phrase-pairs after filtering, while includingall of them led to a table which was too large for the memory ofthe machine used to conduct the experiments.We do not use any form of SMT reorderingmodel, as the position of the phrase in the responsedoes not seem to be very correlated with the corre-sponding position in the status.
Instead we let thelanguage model drive reordering.We used the default feature weights provided byMoses.4 Because automatic evaluation of responsegeneration is an open problem, we avoided the use ofdiscriminative training algorithms such as MinimumError-Rate Training (Och, 2003).5 Information RetrievalOne straightforward data-driven approach to re-sponse generation is nearest neighbour, or informa-tion retrieval.
This general approach has been ap-plied previously by several authors (Isbell et al,2000; Swanson and Gordon, 2008; Jafarpour andBurges, 2010), and is used as a point of compari-son in our experiments.
Given a novel status s and atraining corpus of status/response pairs, two retrievalstrategies can be used to return a best response r?
:IR-STATUS [rargmaxi sim(s,si)] Retrieve the re-sponse ri whose associated status message siis most similar to the user?s input s.IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the re-sponse ri which has highest similarity when di-rectly compared to s.At first glance, IR-STATUS may appear to be themost promising option; intuitively, if an input statusis very similar to a training status, we might expectthe corresponding training response to pair well withthe input.
However, as we describe in ?6, it turnsout that directly retrieving the most similar response(IR-RESPONSE) tends to return acceptable repliesmore reliably, as judged by human annotators.
Toimplement our two IR response generators, we relyon the default similarity measure implemented in theLucene5 Information Retrieval Library, which is anIDF-weighted Vector-Space similarity.6 ExperimentsIn order to compare various approaches to auto-mated response generation, we used human evalu-4The language model weight was set to 0.5, the translationmodel weights in both directions were both set to 0.2, the lexicalsimilarity weight was set to -0.2.5http://lucene.apache.org/587ators from Amazon?s Mechanical Turk (Snow et al,2008).
Human evaluation also provides us with datafor a preliminary investigation into the feasibilityof automatic evaluation metrics.
While automatedevaluation has been investigated in the area of spo-ken dialogue systems (Jung et al, 2009), it is unclearhow well it will correlate with human judgment inopen-domain conversations where the range of pos-sible responses is very large.6.1 Experimental ConditionsWe performed pairwise comparisons of severalresponse-generation systems.
Similar work on eval-uating MT output (Bloodgood and Callison-Burch,2010) has asked Turkers to rank more than twochoices, but in order to keep our evaluation asstraightforward as possible, we limited our experi-ments to pairwise comparisons.For each experiment comparing 2 systems (a andb), we built a test set by selecting a random sam-ple of 200 tweets which had received responses,and which had a length between 4 and 20 words.These tweets were selected from conversations col-lected from a later, non-overlapping time-periodfrom those used in training.
Each experiment useda different random sample of 200 tweets.
For eachof the 200 statuses, we generated a response usingmethod a and b, then showed the status and both re-sponses to the Turkers, asking them to choose thebest response.
The order of the systems used togenerate a response was randomized, and each ofthe 200 HITs was submitted to 3 different Turkers.Turkers were paid 1?
per judgment.The Turkers were instructed that an appropriateresponse should be on the same topic as the sta-tus, and should also ?make sense?
in response to it.While this is an inherently subjective task, from in-specting the results, we found Turkers to be quitecompetent in judging between two responses.The systems used in these pairwise comparisonsare summarized in table 2, and example output gen-erated by each system is presented in Table 3.6.2 ResultsThe results of the experiments are summarized inTable 4.
For each experiment we show the fractionof HITs where the majority of annotators agreed sys-tem a was better.
We also show the p-value from anSystem Name DescriptionRND-BASELINE Picks randomly from the set ofresponses which are observed atleast twice in the training data.The assumption is these arelikely very general responsesIR-STATUS rargmaxi sim(s,si) as describedin ?5IR-RESPONSE rargmaxi sim(s,ri) as describedin ?5MT-CHAT Phrase-based translation systemas described in ?4MT-BASELINE Exactly the same as MT-CHAT,except using a phrase table ex-tracted based on word align-ments from GIZA++HUMAN Actual responses from the testdata.Table 2: Summary of systems compared experimentallyexact Binomial significance test; note that all dif-ferences are significant with above 95% confidence.Table 4 also shows the S coefficient (Bennett et al,1954) as a measure of agreement between annotatorsfor each experiment.
S is equivalent the commonlyused Kappa, except it assumes the uniform distribu-tion when computing expected or chance agreement(Artstein and Poesio, 2008).
The uniform distribu-tion is appropriate in our setup, since annotators arenot told which system generated each output, and theorder of choices is randomized.
If annotators wereindeed choosing randomly, they would be expectedto agree in 50% of cases.
In most comparisons thevalue of S is in the range of 0.2 to 0.4, which canbe considered ?Fair agreement?
(Landis and Koch,1977).Note that agreement between annotators is lowerthan typically reported in corpus annotation tasks.When annotating which of two automatically gen-erated outputs is better, there is not always a clearanswer; both responses might be good or bad.
Wecan expect strong agreement only in cases whereone response is clearly better.
Strong agreement isnot required, however, as we are using many anno-tations to compare each pair of systems, and the hu-man judgments are not intended to be used as train-ing data.
Similar agreement was reported in an eval-uation of automatically generated MT output as part588Status MT-CHAT MT-BASELINE IR-STATUS HUMAN RND-BASELINE IR-RESPONSEriding to thearc on my bike.Hopefully I won?tmelt into thepavement.good luck on yourbike ride arci ca n?t wait for it!
i ?m arc is a year.grab some re-freshing drinks... and a towel ...lol ... then eat icecream and suikalater ...make sure to ridein the middle ofthe streetlmfaoooo !
u sure the cup won?t melt ?I really hatewatching moviesin the livingroom.me too , whatmovies are youdoing in room ?me too !
i lovethose ? )
in mymouth .
:( cheer up !
youneed to invest inone of those turbofansWhy?
no comfyspots or just tooopen to publictas fazendo oq ?
someone waswatching in theliving room kasi e. :) i was too lazyto go up to myroom .
:) )Hello Did I missDavid Gettinghis teen choiceaward?
Or didthey not show it :(hello miss ?
*he was getting hiswill probably win.it ?s just like whatyou just did theexact date hello ?it did n?t show yet-ha- -lol-nothing yet...hepresented thoughhe is so darn cuteyou wish .
are they gon nashow teen choiceawards again ?
imissed itI can think the im-possible is possi-ble at any givenmoment.i think you cando it , anythingis given impossi-ble dreams reallyspecsi ca n?t wait tosee if there is nopoint in the endbut if you do afew hours ...i hope you usethose powers forevil .
i mean good!
good .
that ?swhat i meant .for sure Eric, andthat makes it not?impossible?
anylonger.no -user- ?
nobody withwhom i mightapply the bluecobalt ?
rabbit ?at the moment .given the econ-omy , perhaps-url- ( any of em )I want to enter myparents house intoa home makeovercontest.
Any helpout there?me too !
thanksfor the win , par-ents are so smart, just got homemakeoveroh man , i don?t want to comeback to my momis getting thati put troy on thepotty everyday atlunch and b4 uknow it he wasgoing on his own... only took aweek .check TLC I?mpretty sure it wasthem who wererecently postingabout lookingfor houses to benominated!good job you want to dolaundry with meat my parentshouse after i getoff ?
maybe get afree meal out of it?
!Table 3: Example responses from each system.
We tried to pick examples where most (or all) systems generatereasonable responses for illustration purposes.System A System B Fraction A p-value Agreement(S)System ABLEUSystem BBLEUMT-CHAT?
IR-STATUS 0.645 5.0e-05 0.347 1.15 0.57MT-CHAT?
IR-RESPONSE 0.593 1.0e-02 0.333 0.84 1.53IR-STATUS IR-RESPONSE?
0.422 3.3e-02 0.330 0.40 1.59MT-CHAT?
MT-BASELINE 0.577 3.8e-02 0.160 1.23 1.14MT-CHAT HUMAN?
0.145 2.2e-16 0.433 N/A N/AMT-CHAT?
RND-BASELINE 0.880 2.2e-16 0.383 1.17 0.10Table 4: Results of pairwise comparisons between various response-generation methods.
Each row presents a com-parison between systems a and b on 200 randomly selected tweets.
The column Fraction A lists the fraction of HITswhere the majority of annotators agreed System A?s response was better.
The winning system is indicated with anasterisk?.
All differences are significant.589of the WMT09 shared tasks (Callison-Burch et al,2009).6The results of the paired evaluations provide aclear ordering on the automatic systems: IR-STATUSis outperformed by IR-RESPONSE, which is in turnoutperformed by MT-CHAT.
These results aresomewhat surprising.
We had expected that match-ing status to status would create a more natural andeffective IR system, but in practice, it appears thatthe additional level of indirection employed by IR-STATUS created only more opportunity for confu-sion and error.
Also, we did not necessarily expectMT-CHAT?s output to be preferred by human anno-tators: the SMT system is the only one that generatesa completely novel response, and is therefore thesystem most likely to make fluency errors.
We hadexpected human annotators to pick up on these flu-ency errors, giving the the advantage to the IR sys-tems.
However, it appears that MT-CHAT?s abilityto tailor its response to the status on a fine-grainedscale overcame the disadvantage of occasionally in-troducing fluency errors.7Given MT-CHAT?s success over the IR systems,we conducted further experiments to validate its out-put.
In order to test how close MT-CHAT?s responsescome to human-level abilities, we compared its out-put to actual human responses from our dataset.
Insome cases the human responses change the topic ofconversation, and completely ignore the initial sta-tus.
For instance, one frequent type of response wenoticed in the data was a greeting: ?How have youbeen?
I haven?t talked to you in a while.?
For thepurposes of this evaluation, we manually filtered outcases where the human response was completely off-topic from the status, selecting 200 pairs at randomthat met our criteria and using the actual responsesas the HUMAN output.When compared to the actual human-generatedresponse, MT-CHAT loses.
However, its output ispreferred over the human responses 15% of the time,a fact that is particularly surprising given the verysmall ?
by MT standards ?
amount of data used totrain the model.
A few examples where MT-CHAT?soutput were selected over the human response are6See inter annotator agreement in table 4.7Also, as one can see from the example exchanges in Ta-ble 3, fluency errors are rampant across all systems, includingthe gold-standard human responses.listed in Table 5.We also evaluated the effect of filtering all possi-ble phrase pairs using Fisher?s Exact Test, which wedid instead of conducting phrase extraction accord-ing to the very noisy word alignments.
We alteredour MT-CHAT system to use the standard Mosesphrase-extraction pipeline, creating the system de-noted as MT-BASELINE.
We compared this to thecomplete MT-CHAT system.
Note that both systemsaccount for lexical repetition as described in ?4.1.MT-CHAT?s output is preferred 58% of the time overMT-BASELINE, indicating that direct phrase extrac-tion is useful in this conversational setting.Finally, as an additional baseline, we comparedMT-CHAT?s output to random responses selectedfrom those observed 2 or more times in the train-ing data.
One might argue that short, common re-sponses are very general, and that a reply like ?lol?could be considered a good response to almost anystatus.
However, the human evaluation shows a clearpreference for MT-CHAT?s output: raters favour re-sponses that are tailored to the stimulus.6.3 Automatic EvaluationThe field of SMT has benefited greatly fromthe existence of an automatic evaluation metric,BLEU (Papineni et al, 2002), which grades an out-put candidate according to n-gram matches to one ormore reference outputs.
To evaluate whether BLEUis an appropriate automatic evaluation measure forresponse generation, we attempted to measure itsagreement with the human judgments.
We calcu-late BLEU using a single reference derived from ourparallel corpus.
We show the smoothed BLEU 1-4scores for each system on each dataset evaluated inTable 4.
Although these scores are extremely low,the overall BLEU scores agree with overall annota-tor judgments in all cases except when comparingMT-CHAT and IR-RESPONSE.
It would seem thatBLEU has some agreement with human judgmentson this task, but perhaps not enough to be immedi-ately useful.7 ConclusionsWe have presented several data-driven approaches,based on information retrial and phrase-based SMT,to the task of automatically responding to open-590Status MT-CHAT HUMANi want to go to a bonfire right aboutnow.that does sound fun bonfire i wan nago !did my tweet bring that upon you ?heheI?m the invisible man.
man , you could never be invisible i can still see your face surroundedin blue !put ya number in my phone ..andima call you up !call me when u do that in your phonecall :) for what ?s up ?o well hw kum u dnt eva call or txtme :) ?
!
?
!
lolz jpn hw r ya tho :)?
!
?
!Gray Maynard with hair?
I did notapprove this.maynard with grey hair !
i take it back, the hair isn?t workingfor maynard.Table 5: Examples where MT-CHAT output was preferred over HUMAN response by Turker annotatorsdomain linguistic stimuli.Our experiments show that SMT techniques arebetter-suited than IR approaches on the task of re-sponse generation.
Our system, MT-CHAT, pro-duced responses which were preferred by human an-notators over actual human responses 15% of thetime.
Although this is still far from human-levelperformance, we believe there is much room forimprovement: from designing appropriate word-alignment and decoding algorithms that account forthe selective nature of response in dialogue, to sim-ply adding more training data.We described the many challenges posed byadapting phrase-based SMT to dialogue, and pre-sented initial solutions to several, including directphrasal alignment, and phrase-table scores discour-aging responses that are lexically similar to the sta-tus.
Finally, we have provided results from an initialexperiment to evaluate the BLEU metric when ap-plied to response generation, showing that thoughthe metric as is does not work well, there is suffi-cient correlation to suggest that a similar, dialogue-focused approach may be feasible.By generating responses to Tweets out of context,we have demonstrated that the models underlyingphrase-based SMT are capable of guiding the con-struction of appropriate responses.
In the future, weare excited about the role these models could po-tentially play in guiding response construction forconversationally-aware chat input schemes, as wellas goal-directed dialogue systems.AcknowledgmentsWe would like to thank Oren Etzioni, MichaelGamon, Jerry Hobbs, Dirk Hovy, Yun-Cheng Ju,Kristina Toutanova, Saif Mohammad, Patrick Pan-tel, and Luke Zettlemoyer, in addition to the anony-mous reviewers for helpful discussions and com-ments on a previous draft.
The first author is sup-pored by a National Defense Science and Engineer-ing Graduate (NDSEG) Fellowship 32 CFR 168a.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Comput.
Lin-guist., 34:555?596, December.Regina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: an entity-based approach.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05.E.
M. Bennett, R. Alpert, and A. C. Goldstein.
1954.Communications through limited-response question-ing.
Public Opinion Quarterly, 18(3):303?308.Michael Bloodgood and Chris Callison-Burch.
2010.Using mechanical turk to build machine translationevaluation sets.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, CSLDAMT?10, pages 208?211, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Chris Brockett.
2006.
Aligning the rte 2006 corpus.
InMicrosoft Research Techincal report MSR-TR-2007-77.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Comput.Linguist., 16:79?85, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009 work-shop on statistical machine translation.
In Proceedingsof the Fourth Workshop on Statistical Machine Trans-lation, StatMT ?09.591Nathanael Chambers and James Allen.
2004.
Stochas-tic language generation in a dialogue system: Towarda domain independent generator.
In Michael Strubeand Candy Sidner, editors, Proceedings of the 5th SIG-dial Workshop on Discourse and Dialogue, pages 9?18, Cambridge, Massachusetts, USA, April 30 - May1.
Association for Computational Linguistics.Cristian Danescu-Niculescu-Mizil, Michael Gamon, andSusan Dumais.
2011.
Mark my words!
Linguisticstyle accommodation in social media.
In Proceedingsof WWW.Hal Daume?
III and Daniel Marcu.
2009.
Induction ofword and phrase alignments for automatic documentsummarization.
CoRR, abs/0907.0804.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:exploiting massively parallel news sources.
In Pro-ceedings of the 20th international conference on Com-putational Linguistics, COLING ?04, Morristown, NJ,USA.
Association for Computational Linguistics.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.
InProceedings of the 41st Annual Meeting on Associa-tion for Computational Linguistics - Volume 1, ACL?03, pages 16?23, Morristown, NJ, USA.
Associationfor Computational Linguistics.Micha Elsner and Eugene Charniak.
2008.
You talkingto me?
a corpus and algorithm for conversation disen-tanglement.
In Proceedings of ACL-08: HLT, June.Michel Galley, Eric Fosler-Lussier, and AlexandrosPotamianos.
2001.
Hybrid natural language gener-ation for spoken dialogue systems.
In Proceedingsof the 7th European Conference on Speech Commu-nication and Technology (EUROSPEECH?01), pages1735?1738, Aalborg, Denmark, September.Jon Hasselgren, Erik Montnemery, Pierre Nugues, andMarkus Svensson.
2003.
Hms: a predictive text entrymethod using bigrams.
In Proceedings of the 2003EACL Workshop on Language Modeling for Text EntryMethods, TextEntry ?03.Jerry R. Hobbs.
1985.
On the coherence and structure ofdiscourse.Charles Lee Isbell, Jr., Michael J. Kearns, Dave Ko-rmann, Satinder P. Singh, and Peter Stone.
2000.Cobot in lambdamoo: A social statistics agent.
In Pro-ceedings of the Seventeenth National Conference onArtificial Intelligence and Twelfth Conference on In-novative Applications of Artificial Intelligence, pages36?41.
AAAI Press.Sina Jafarpour and Christopher J. C. Burges.
2010.
Fil-ter, rank, and transfer the knowledge: Learning to chat.Howard Johnson, Joel Martin, George Foster, and RolandKuhn.
2007.
Improving translation quality by dis-carding most of the phrasetable.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 967?975, Prague, Czech Republic, June.
Association forComputational Linguistics.Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-woo Jeong, and Gary Geunbae Lee.
2009.
Data-driven user simulation for automated evaluation ofspoken dialog systems.
Comput.
Speech Lang.,23:479?509, October.Kevin Knight and Vasileios Hatzivassiloglou.
1995.Two-level, many-paths generation.
In Proceedings ofthe 33rd annual meeting on Association for Computa-tional Linguistics, ACL ?95.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL.
TheAssociation for Computer Linguistics.J R Landis and G G Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics.Brian Langner, Stephan Vogel, and Alan W. Black.
2010.Evaluating a dialog language generation system: com-paring the mountain system to other nlg approaches.In INTERSPEECH.Anton Leuski and David R. Traum.
2010.
Practicallanguage processing for virtual humans.
In Twenty-Second Annual Conference on Innovative Applicationsof Artificial Intelligence (IAAI-10).Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment model fornatural language inference.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 802?811, Morristown,NJ, USA.
Association for Computational Linguistics.Robert C. Moore.
2004.
On log-likelihood-ratios and thesignificance of rare events.
In EMNLP.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.F.
J. Och.
2003.
Minimum error rate training for statisti-cal machine translation.
In ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In ACL, pages 311?318.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Processing,pages 142?149.592Owen Rambow, Srinivas Bangalore, and Marilyn Walker.2001.
Natural language generation in dialog systems.In Proceedings of the first international conference onHuman language technology research, HLT ?01, pages1?4, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Adwait Ratnaparkhi.
2000.
Trainable methods for sur-face natural language generation.
In Proceedings ofthe 1st North American chapter of the Association forComputational Linguistics conference.Sujith Ravi, Andrei Broder, Evgeniy Gabrilovich, VanjaJosifovski, Sandeep Pandey, and Bo Pang.
2010.
Au-tomatic generation of bid phrases for online advertis-ing.
In Proceedings of the third ACM internationalconference on Web search and data mining, WSDM?10.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages464?471, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 172?180,Morristown, NJ, USA.
Association for ComputationalLinguistics.Samira Shaikh, Tomek Strzalkowski, Sarah Taylor, andNick Webb.
2010.
Vca: an experiment with a mul-tiparty virtual chat agent.
In Proceedings of the 2010Workshop on Companionable Dialogue Systems.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In Pro-ceedings of the COLING/ACL on Main conferenceposter sessions, COLING-ACL ?06.Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.2010.
Learning phrase-based spelling error modelsfrom clickthrough data.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 266?274, Morristown,NJ, USA.
Association for Computational Linguistics.Reid Swanson and Andrew S. Gordon.
2008.
Say any-thing: A massively collaborative open domain storywriting companion.
In Proceedings of the 1st JointInternational Conference on Interactive Digital Story-telling: Interactive Storytelling, ICIDS ?08, pages 32?40, Berlin, Heidelberg.
Springer-Verlag.Lidan Wang and Douglas W. Oard.
2009.
Context-basedmessage expansion for disentanglement of interleavedtext conversations.
In HLT-NAACL.Joseph Weizenbaum.
1966.
Eliza: a computer programfor the study of natural language communication be-tween man and machine.
Commun.
ACM, 9:36?45,January.Yorick Wilks.
2006.
Artificial companions as a new kindof interface to the future internet.
In OII Research Re-port No.
13.Yuk Wah Wong and Raymond Mooney.
2006.
Learningfor semantic parsing with statistical machine transla-tion.
In Proceedings of the Human Language Technol-ogy Conference of the NAACL, Main Conference.Yuk Wah Wong and Raymond Mooney.
2007.
Gener-ation by inverting a semantic parser that uses statis-tical machine translation.
In Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics; Proceedings of the Main Conference.593
