Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 161?171, Dublin, Ireland, August 23-29 2014.Learning Task-specific Bilexical EmbeddingsPranava Swaroop Madhyastha Xavier Carreras Ariadna QuattoniTALP Research CenterUniversitat Polit`ecnica de CatalunyaCampus Nord UPC, Barcelonapranava,carreras,aquattoni@lsi.upc.eduAbstractWe present a method that learns bilexical operators over distributional representations of wordsand leverages supervised data for a linguistic relation.
The learning algorithm exploits low-rank bilinear forms and induces low-dimensional embeddings of the lexical space tailored forthe target linguistic relation.
An advantage of imposing low-rank constraints is that predictionis expressed as the inner-product between low-dimensional embeddings, which can have greatcomputational benefits.
In experiments with multiple linguistic bilexical relations we show thatour method effectively learns using embeddings of a few dimensions.1 IntroductionWe address the task of learning functions that compute compatibility scores between pairs of lexicalitems under some linguistic relation.
We refer to these functions as bilexical operators.
As an instance ofthis problem, consider learning a model that predicts the probability that an adjective modifies a noun ina sentence.
In this case, we would like the bilexical operator to capture the fact that some adjectives aremore compatible with some nouns than others.
For example, a bilexical operator should predict that theadjective electronic has high probability of modifying the noun device but little probability of modifyingthe noun case.Bilexical operators can be useful for multiple NLP applications.
For example, they can be used toreduce ambiguity in a parsing task.
Consider the following sentence extracted from a weblog: Vynilcan be applied to electronic devices and cases, wooden doors and furniture and walls.
If we want topredict the dependency structure of this sentence we need to make several decisions.
In particular, theparser would need to decide (1) Does electronic modify devices?
(2) Does electronic modify cases?
(3)Does wooden modify doors?
(4) Does wooden modify furniture?
Now imagine that in the corpus used totrain the parser none of these nouns have been observed, then it is unlikely that these attachments can beresolved correctly.
However, if an accurate noun-adjective bilexical operator were available most of theuncertainty could be resolved.
This is because a good bilinear operator would give high probability to thepairs electronic-device, wooden-door, wooden-furniture and low probability to the pair electronic-case.The simplest way of inducing a bilexical operator is to learn it from a training corpus.
That is, assumingthat we are given some data annotated with a linguistic relation between a modifier and a head (e.g.adjective and noun) we can simply build a maximum likelihood estimator for Pr(m | h) by counting theoccurrences of modifiers and heads under the target relation.
For example, we could consider learningbilexical operators from sentences annotated with dependency structures.
Clearly, this model can notgeneralize to head words not present in the training data.To mitigate this we could consider bilexical operators that can exploit lexical embeddings, such asa distributional vector-space representation of words.
In this case, we assume that for every word wecan compute an n-dimensional vector space representation ?
(w) ?
Rn.
This representation typicallycaptures distributional features of the context in which the lexical item can occur.
The key point is thatThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/161we do not need a supervised corpus to compute the representation.
All we need is a large textual corpusto compute the relevant statistics.
Once we have the representation we can exploit operations in theinduced vector space to define lexical compatibility operators.
For example we could define a bilexicaloperator as:Pr(m | h) =exp {??
(m), ?(h)?
}?m?exp {??(m?
), ?(h)?
}(1)where ??
(x), ?(y)?
denotes the inner-product.
Alternatively, given an initial high-dimensional distribu-tional representation computed from a large textual corpus we could first induce a projection to a lowerk dimensional space by performing truncated singular value decomposition.
The idea is that the lowerdimensional representation will be more efficient and it will better capture the relevant dimensions of thedistributional representation.
The bilexical operator would then take the form of:Pr(m|h) =exp {?U?
(m), U?(h)?
}?m?exp {?U?(m?
), U?(h)?
}(2)where U ?
Rk?nis the projection matrix obtained via SVD.
The advantage of this approach is that aslong as we can estimate the distribution of contexts of words we can compute the value of the bilexicaloperator.
However, this approach has a clear limitation: to design a bilinear operator for a target linguisticrelation we must design the appropriate distributional representation.
Moreover, there is no clear way ofexploiting a supervised training corpus.In this paper we combine both the supervised and distributional approaches and present a learningalgorithm for inducing bilexical operators from a combination of supervised and unsupervised trainingdata.
The main idea is to define bilexical operators using bilinear forms over distributional representa-tions: ?(x)>W?
(y), where W ?
Rn?nis a matrix of parameters.
We can then train our model on thesupervised training corpus via conditional maximum-likelihood estimation.
To induce a low-dimensionalrepresentation, we first observe that the implicit dimensionality of the bilinear form is given by the rankofW .
In practice controlling the rank ofW can result in important computational savings in cases whereone evaluates a target word x against a large number of candidate words y: this is because we can projectthe representations ?
(x) and ?
(y) down to the low-dimensional space where evaluating the function issimply an inner-product.
This setting is in fact usual, for example for lexical retrieval applications (e.g.given a noun, sort all adjectives in the vocabulary according to their compatibility), or for parsing (whereone typically evaluates the compatibility between all pairs of words in a sentence).Consequently with these ideas, we propose to regularize the maximum-likelihood estimation usinga nuclear norm regularizer that serves as a convex relaxation to the rank function.
To minimize theregularized objective we make use of an efficient iterative proximal method that involves computing thegradient of the function and performing singular value decompositions.We test the proposed algorithm on several linguistic relations and show that it can predict modifiersfor unknown words more accurately than the unsupervised approach.
Furthermore, we compare differenttypes of regularizers for the bilexical operatorW , and observe that indeed the low-rank regularizer resultsin the most efficient technique at prediction time.In summary, the main contributions of this paper are:?
We propose a supervised framework for learning bilexical operators over distributional representa-tions, based on learning bilinear forms W .?
We show that we can obtain low-dimensional compressions of the distributional representation byimposing low-rank constraints to the bilinear form.
Combined with supervision, this results inlexical embeddings tailored for a specific bilexical task.?
In experiments, we show that our models generalize well to unseen word pairs, using only a fewdimensions, and outperforming standard unsupervised distributional approaches.
We also presentan application to prepositional phrase attachment.1622 Bilinear Models for Bilexical Predictions2.1 DefinitionsLet V be a vocabulary, and let x ?
V denote a word.
Let H ?
V be a set of head words, andM?
V bea set of modifier words.
In the noun-adjective relation example, H is the set of nouns andM is the setof adjectives.The task is as follows.
We are given a training set of l tuples D = {(m,h)1, .
.
.
, (m,h)l}, wherem ?
M and h ?
H and we want to learn a model of the conditional distribution Pr(m | h).
We wantthis model to perform well on all head-modifier pairs.
In particular we will test the performance of themodel on heads that do not appear in D.We assume that we are given access to a distributional representation function ?
: V ?
Rn, where?
(x) is the n-dimensional representation of x.
Typically, this function is computed from an unsupervisedcorpus.
We use ?
(x)[i]to refer to the i-th coordinate of the vector.2.2 Bilinear ModelOur model makes use of the bilinear form W : Rn?
Rn?
R, where W ?
Rn?n, and evaluates as?(m)>W?(h).
We define the bilexical operator as:Pr(m | h) =exp{?(m)>W?(h)}?m?
?Mexp {?(m?)>W?
(h)}(3)Note that the above model is nothing more than a conditional log-linear model defined over n2fea-tures fi,j(m,h) = ?(m)[i]?
(h)[j](this can be seen clearly when we write the bilinear form as?ni=1?nj=1fi,j(m,h)Wi,j.
The reason why it is useful to regard W as a matrix will become evident inthe next section.Before moving to the next section, let us note that the unsupervised SVD model in Eq.
(2) is also abilinear model as defined here.
This can be seen if we set W = UU>, which is a bilinear form of rankk.
The key difference is in the way W is learned using supervision.3 Learning Low-rank Bilexical Operators3.1 Low-rank OptimizationGiven a training set D and a feature function ?
(x) we can do standard conditional max-likelihood opti-mization and minimize the negative of the log-likelihood function, log Pr(D):?(m,h)?D?(m)>W?(h)?
log?m??Mexp{?(m?)>W?
(h)}(4)We would like to control the complexity of the learned model by including some regularization penalty.Moreover, like in the low-dimensional unsupervised approach we want our model to induce a low-dimensional representation of the lexical space.
The first observation is that the bilinear form computesa weighted inner product in some space.
Consider the singular value decomposition: W = U?V .
Wecan write the bilinear form as: [?
(m)>U ] ?
[V ?
(h)], thus we can regard m?
= ?
(m)>U as a projectionof m and?h = V ?
(h) as a projection of h. Then the bilinear form can be written as:?ni=1?[i,i]m?
[i]?h[i].The rank of W defines the dimensionality of the induced space.
It is easy to see that if W has rank k itcan be factorized as U?V where U ?
Rn?kand V ?
Rk?n.Since the rank of W determines the dimensionality of the induced space, it would be reasonable toadd a rank minimization penalty in the objective in (4).
Unfortunately this would lead to a non-convexregularized objective.
Instead, we propose to use as a regularizer a convex relaxation of the rank function,the nuclear norm ?W??
(the `1norm of the singular values of W ).
Putting it all together, our learningalgorithm minimizes:?(m,h)?D?
log Pr(m | h)) + ??W??
(5)163Here ?
is a constant that controls the trade-off between fitting the data and the complexity of the model.This objective is clearly convex since both the objective and the regularizer are convex.
To minimize itwe use the a proximal gradient algorithm which is described next.3.2 A Proximal Algorithm for Bilexical OperatorsWe now describe the learning algorithm that we use to induce the bilexical operators from training data.We are interested in minimizing the objective (5), or in fact a more general version where we can replacethe regularizer ?W?
?by standard `1or `2penalties.
For any convex regularizer r(W ) (namely `1, `2orthe nuclear norm) the objective in (5) is convex.
Our learning algorithm is based on a simple optimizationscheme known as forward-backward splitting (FOBOS) (Duchi and Singer, 2009).This algorithm has convergence rates in the order of 1/2, which we found sufficiently fast for ourapplication.
Many other optimization approaches are possible, for example one could express the regu-larizer as a convex constraint and utilize a projected gradient method which has a similar convergencerate.
Proximal methods are slightly more simple to implement and we chose the proximal approach.The FOBOS algorithm works as follows.
In a series of iterations t = 1 .
.
.
T compute parametermatrices Wtas follows:1.
Compute the gradient of the negative log-likelihood, and update the parametersWt+0.5= Wt?
?tg(Wt)where ?t=c?tis a step size and g(Wt) is the gradient of the loss at Wt.2.
Update Wt+0.5to take into account the regularization penalty r(W ), by solvingWt+1= argminW||Wt+0.5?W ||22+ ?t?r(W )For the regularizers we consider, this step is solved using the proximal operator associated with theregularizer.
Specifically:?
For `1it is a simple thresholding:Wt+1(i, j) = sign(Wt+0.5(i, j)) ?max(Wt+0.5(i, j)?
?t?, 0)?
For `2it is a simple scaling:Wt+1=11 + ?t?Wt+0.5?
For nuclear-norm, perform SVD thresholding.
Compute the SVD to write Wt+0.5= USV>with S a diagonal matrix and U, V orthogonal matrices.
Denote by ?ithe i-th element on thediagonal of S. Define a new matrix?S with diagonal elements ?
?i= max(?i?
?t?, 0).
ThensetWt+1= U?SV>Optimizing a bilinear model using nuclear-norm regularization involves the extra cost of performingSVD of W at each iteration.
In our experiments the dimension of W was 2, 000?
2, 000 and computingSVD was fast, much faster than computing the gradient, which dominates the cost of the algorithm.
Theoptimization parameters of the method are the regularization constant ?, the step size constant c and thenumber of iterations T .
In our experiments we ran a range of ?
and c values for 200 iterations, and useda validation set to pick the best configuration.1644 Related WorkResearch in learning representations for natural language processing can be broadly classified intotwo different paradigms based on the learning setting: unsupervised representation learning and semi-supervised representation learning.
Unsupervised representation learning does not require any supervisedtraining data, while semi-supervised representation learning requires the presence of supervised trainingdata with the potential advantage that it can adapt the representation to the task at hand.Unsupervised approaches to learning representations mainly involve representations that are learnednot for a specific task, rather a variety of tasks.
These representations rely more on the property ofabstractness and generalization.
Further, unsupervised approaches can be roughly categorized into (a)clustering-based approaches that make use of clusters induced using a notion of distributed similarity,such as the method by Brown et al.
(1992); (b) neural-network-based representations that focus on learn-ing multilayer neural network in a way to extract features from the data (Morin and Bengio, 2005; Mnihand Hinton, 2007; Bengio and S?en?ecal, 2008; Mnih and Hinton, 2009); (c) pure distributional approachesthat principally follow the distributional assumption that the words which share a set of contexts are sim-ilar (Sahlgren, 2006; Turney and Pantel, 2010; Dumais et al., 1988; Landauer et al., 1998; Lund et al.,1995; V?ayrynen et al., 2007).We also induce lexical embeddings, but in our case we employ supervision.
That is, we follow asemi-supervised paradigm for learning representations.
Semi-supervised approaches initially learn rep-resentations typically in an unsupervised setting and then induce a representation that is jointly learnedfor the task with a labeled corpus.
A high-dimensional representation is extracted from unlabeled data,while the supervised step compresses the representation to be low-dimensional in a way that favors thethe task at hand.Collobert and Weston (2008) present a neural network language model, where given a sentence, itperforms a set of language processing tasks (from part of speech tagging, chunking, extracting namedentity, extracting semantic roles and decisions on the correctness of the sentence) by using the learnedrepresentations.
The representation itself is extracted from unlabeled corpora, while all the other tasksare jointly trained on labeled corpus.Socher et al.
(2011) present a model based on recursive neural networks that learns vector space rep-resentations for words, multi-word phrases and sentences.
Given a sentence with its syntactic structure,their model assings vector representations to each of the lexical tokens of the sentence, and then traversesthe syntactic tree bottom-up, such that at each node a vector representation of the corresponding phraseis obtained by composing the vectors associated with the children.Bai et al.
(2010) use a technique similar to ours, using bilinear forms with low-rank constraints.
Intheir case, they explicitly look for a low-rank factorization of the matrix, making their optimizationnon-convex.
As far as we know, ours is the first convex formulation, where we employ a relaxationof the rank (i.e.
the nuclear norm) to make the objective convex.
They apply the method to documentranking, and thus optimize a max-margin ranking loss.
In our application to bilexical models, we performconditional max-likelihood estimation.
Hutchinson et al.
(2013) propose an explicitly sparse and low-rank maximum-entropy language model.
The sparse plus low rank setting is learned in such a way thatthe low rank component learns the regularities in the training data and the sparse component learns theexceptions like multiword expressions etc.Chechik et al.
(2010) also learned bilinear operators using max-margin techniques, with pairwisesimilarity as supervision, but they did not consider low-rank constraints.One related area where bilinear operators are used to induce embeddings is distance metric learning.Weinberger and Saul (2009) used large-margin nearest neighbor methods to learn a non-sparse embed-ding, but these are computationally intensive and might not be suitable for large-scale tasks in NLP.5 Experiments on Syntactic RelationsWe conducted a set of experiments to test the ability of our algorithm to learn bilexical operators forseveral linguistic relations.
As supervised training data we use the gold standard dependencies of theWSJ training section of the Penn Treebank (Marcus et al., 1993).
We consider the following relations:1655055606570758085901e3 1e4 1e5 1e6 1e7 1e8pairwiseaccuracynumber of operationsAdjectives given NoununsupervisedNNL1L266687072747678801e3 1e4 1e5 1e6 1e7 1e8pairwiseaccuracynumber of operationsNouns given AdjectiveunsupervisedNNL1L24648505254565860621e3 1e4 1e5 1e6 1e7 1e8pairwiseaccuracynumber of operationsObjects given VerbunsupervisedNNL1L260657075801e3 1e4 1e5 1e6 1e7 1e8pairwiseaccuracynumber of operationsVerbs given ObjectunsupervisedNNL1L2Figure 1: Pairwise accuracy with respect to the number of double operations required to compute thedistribution over modifiers for a head word.
Plots for noun-adjective and verb-object relations, in bothdirections.?
Noun-Adjective: we model the distribution of adjectives given a noun; and a separate distributionof nouns given an adjective.?
Verb-Object: we model the distribution of object nouns given a verb; and a separate distribution ofverbs given an object.?
Prepositions: in this case we consider bilexical operators associated with a preposition, which modelthe probability of a head noun or verb above the preposition given the noun below the preposition.We present results for prepositional relations given by ?with?, ?for?, ?in?
and ?on?.The distributional representation ?
(x) was computed using the BLLIP corpus (Charniak et al., 2000).We compute a bag-of-words representation for the context of each lexical item, that is ?
(w)[i]corre-sponds to the frequency of word i appearning in the context of w. We use a context window of size 10and restrict our bag-of-words vocabulary to contain only the 2,000 most frequent words present in thecorpus.
Vectors were normalized.1665055606570751e3 1e4 1e5 1e6 1e7 1e8pairwiseaccuracynumber of operationsWithunsupervisedNNL1L2545658606264666870721e3 1e4 1e5 1e6 1e7 1e8pairwiseaccuracynumber of operationsForunsupervisedNNL1L2464850525456581e3 1e4 1e5 1e6 1e7 1e8pairwiseaccuracynumber of operationsInunsupervisedNNL1L260626466687072747678801e3 1e4 1e5 1e6 1e7 1e8pairwiseaccuracynumber of operationsOnunsupervisedNNL1L2Figure 2: Pairwise accuracy with respect to the number of double operations required to compute thedistribution over modifiers for a head word.
Plots for four prepositional relations: with, for, in, on.
Thedistributions are of verbs and objects above the preposition given the noun below the preposition.To test the performance of our algorithm for each relation we partition the set of heads into a trainingand a test set, 60% of the heads are use for training, 10% of the heads are used for validation and 30% ofthe heads are used for testing.
Then, we consider all observed modifiers in the data to form a vocabularyof modifier words.
The goal of this task is to learn conditional distribution over all these modifers givena head word without context.
In our experiments, the number of modifiers per relation ranges from 2,500to 7,500 words.
For each head word, we create a list of compatible modifiers from the annotated data, bytaking all modifiers that occur at least once with the head.
Hence, for each head the set of all modifiersis partitioned into compatible and non-compatible.
For testing, we measure a pairwise accuracy, thepercentage of compatible/non-compatible pairs of modifiers where the former obtains higher probability.Let us stress that none of the test head words has been observed in training, while the list of modifiers isthe same for training, validation and testing.We compare the performance of the bilexical model trained with nuclear norm regularization (NN)with other regularization penalties (L1 and L2).
We also compare these supervised methods with an167Noun Predicted Adjectivespresident executive, senior, chief, frank, former, international, marketing,assistant, annual, financialwife former, executive, new, financial, own, senior, old, other, deputy,majorshares annual, due, net, convertible, average, new, high-yield, initial,tax-exempt, subordinatedmortgages annualized, annual, three-month, one-year, average, six-month,conventional, short-term, higher, lowermonth last, next, fiscal, first, past, latest, early, previous, new, currentproblem new, good, major, tough, bad, big, first, financial, long, federalholiday new, major, special, fourth-quarter, joint, quarterly, third-quarter,small, strong, ownTable 1: 10 most likely adjectives for some test nouns.unsupervised model: a low-dimensional SVD model as in Eq.
(2), which corresponds to an inner productas in Eq.
(1) when all dimensions are considered.To report performance, we measure pairwise accuracy with respect to the capacity of the model interms of number of active parameters.
To measure the capacity of a model we consider the number ofdouble operations that are needed to compute, given a head, the scores for all modifiers in the vocabulary(we exclude the exponentiations and normalization needed to compute the distribution of modifiers givena head, since this is a constant cost for all the models we compare, and is not needed if we only want torank modifiers).
Recall that the dimension of ?
(x) is n, and assume that there are m total modifiers inthe vocabulary.
In our experiments n = 2, 000 and m ranges from 2, 500 to 7, 500.
The correspondanceswith operations are:?
Assume that the L1 and L2 models have k non-zero weights in W .
Then the number of operationsto compute a distribution is km.?
Assume that the NN and the unsupervised models have rank k. We assume that the modifier vectorsare alredy projected down to k dimensions.
For a new head, one needs to project it and perform minner products, hence the number of operations is kn+ km.Figure 1 shows the performance of models for noun-adjective and verb-object relations, while Figure 2shows plots for prepositional relations.1The first observation is that supervised approaches outperformthe unsupervised approach.
In cases such as noun-adjetive relations the unsupervised approach performsclose to the supervised approaches, suggesting that the pure distributional approach can sometimes work.But in most relations the improvement obtained by using supervision is very large.
When comparing thetype of regularizer, we see that if the capacity of the model is unrestricted (right part of the curves), allmodels tend to perform similarly.
However, when restricting the size, the nuclear-norm model performsmuch better.
Roughly, 20 hidden dimensions are enough to obtain the most accurate performances(which result in?
140, 000 operations for initial representaions of 2, 000 dimensions and 5, 000 modifiercandidates).
As an example of the type of predictions, Table 1 shows the most likely adjectives for sometest nouns.6 Experiments on PP AttachmentWe now switch to a standard classification task, prepositional phrase attachment, that we frame as abilexical prediction task.
We start from the formulation of the task as a binary classification problem byRatnaparkhi et al.
(1994): given a tuple x = ?v, o, p, n?
consisting of a verb v, noun object o, preposition1To obtain curves for each model type with respect to a range of number of operations, we first obtained the best model onvalidation data and then forced it to have at most k non-zero features or rank k by projecting, for a range of k values.168556065707580for from withattachment accuracybilinear L1bilinear L2bilinear NNlinearinterpolated L1interpolated L2interpolated NNFigure 3: Attachment accuracies of linear, bilinear and interpolated models for three prepositions.p and noun n, decide if the prepositional phrase p-n attaches to v (y = V) or to o (y = O).
For example,in ?
meet,demand,for,products?
the correct attachment is O.Ratnaparkhi et al.
(1994) define a linear maximum likelihood model of the formPr(y | x) = exp{?w, f(x, y)?}
?
Z(x)?1, where f(x, y) is a vector of d features, w is a parame-ter vector in Rd, and Z(x) is the normalizer summing over y = {V, O}.
Here we define a bilexicalmodel of the form that uses a distributional representation ?
:Pr(V|?v, o, p, n?)
=exp{?(v)>WpV?
(n)}Z(x)Pr(O|?v, o, p, n?)
=exp{?(o)>WpO?
(n)}Z(x)(6)The bilinear model is parameterized by two matricesWVandWOper preposition, each of which capturesthe compatibility between nouns below a certain preposition and heads of V or O prepositional relations,respectively.
Again Z(x) is the normalizer summing over y = {V, O}, but now using the bilinear form.It is straighforward to modify the learning algorithm in Section 3 such that the loss is a negative log-likelihood for binary classification, and the regularizer considers the sum of norms of the model matrices.We ran experiments using the data by Ratnaparkhi et al.
(1994).
We trained separate models fordifferent prepositions, focusing on the prepositions that are more ambiguous: for, from, with.We compare to a linear ?maxent?
model following Ratnaparkhi et al.
(1994) that uses the same featureset.
Figure 3 shows the test results for the linear model, and bilinear models trained with L1, L2, NNregularization penalties.
The results of the bilinear models are significantly below the accuracy of thelinear model, suggesting that some of the non-lexical features of the linear model (such as prior weightingof the two classes) might be difficult to capture by the bilinear model over lexical representations.
Tocheck if the bilinear model might complement the linear model or just be worse than it, we tested simplecombinations based on linear interpolations.
For a constant ?
?
[0, 1] we define:Pr(y | x) = ?
PrL(y | x) + (1?
?)
PrB(y | x) .
(7)We search for the best ?
on the validation set, and report results of combining the linear model witheach of the three bilinear models.
Results are shown also in Figure 3.
Interpolation models improve overlinear models, though only the improvement for for is significant (2.6%).
Future work should exploitfiner combinations between standard linear features and distributional bilinear forms.7 ConclusionsWe have presented a model for learning bilexical operators that can leverage both supervised and unsu-pervised data.
The model is based on exploiting bilinear forms over distributional representations.
The169learning algorithm induces a low-dimensional representation of the lexical space by imposing low-rankconstraints on the parameters of the bilinear form.
By means of supervision, our model induces twolow-dimensional lexical embeddings, one on each side of the bilexical linguistic relation, and compu-tations can be expressed as an inner-product between the two embeddings.
This factorized form of themodel can have great computational advantages: in many applications one needs to evaluate the functionmultiple times for a fixed set of lexical items, for example in dependency parsing.
Hence, one can firstproject the lexical items to their embeddings, and then compute all pairwise scores as inner-products.
Inexperiments, we have shown that the embeddings we obtain in a number of linguistic relations can bemodeled with a few hidden dimensions.As future work, we would like to apply the low-rank approach to other model forms that can employlexical embeddings, specially when supervision is available.
For example, dependency parsing models,or models of predicate-argument structures representing semantic roles, exploit bilexical relations.
Inthese applications, being able to generalize to word pairs that are not observed during training is essential.We would also like to study how to combine low-rank bilexical operators, which in essence inducea task-specific representation of words, with other forms of features that capture class or contextualinformation.
One desires that such combinations can preserve the computational advantages behindlow-rank embeddings.AcknowledgementsWe thank the reviewers for their helpful comments.
This work was supported by projects XLike (FP7-288342), ERA-Net CHISTERA VISEN and TACARDI (TIN2012-38523-C02-00).
Xavier Carreras wassupported by the Ram?on y Cajal program of the Spanish Government (RYC-2008-02223).ReferencesBing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, andKilian Weinberger.
2010.
Learning to rank with (a lot of) word features.
Information Retrieval, 13(3):291?314,June.Yoshua Bengio and Jean-S?ebastien S?en?ecal.
2008.
Adaptive importance sampling to accelerate training of a neuralprobabilistic language model.
IEEE Transactions on Neural Networks, 19(4):713?722.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-basedn-gram models of natural language.
Computational Linguistics, 18:467?479.Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, and Mark Johnson.
2000.
BLLIP 1987?89 WSJ CorpusRelease 1, LDC No.
LDC2000T43.
Linguistic Data Consortium.Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio.
2010.
Large scale online learning of image similaritythrough ranking.
Journal of Machine Learning Research, pages 1109?1135.Ronan Collobert and Jason Weston.
2008.
A unified architecture for natural language processing: Deep neuralnetworks with multitask learning.
In Proceedings of the 25th International Conference on Machine Learning,ICML ?08, pages 160?167, New York, NY, USA.
ACM.John Duchi and Yoram Singer.
2009.
Efficient online and batch learning using forward backward splitting.
Journalof Machine Learning Research, 10:2899?2934.Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Scott Deerwester, and Richard Harshman.
1988.
Usinglatent semantic analysis to improve access to textual information.
In SIGCHI Conference on Human Factors inComputing Systems, pages 281?285.
ACM.Brian Hutchinson, Mari Ostendorf, and Maryam Fazel.
2013.
Exceptions in language as learned by the multi-factor sparse plus low-rank language model.
In ICASSP, pages 8580?8584.Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998.
An introduction to latent semantic analysis.Discourse Processes, 25:259?284.Kevin Lund, Curt Burgess, and Ruth A. Atchley.
1995.
Semantic and associative priming in high-dimensionalsemantic space.
In Cognitive Science Proceedings, LEA, pages 660?665.170Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz.
1993.
Building a Large Annotated Corpus ofEnglish: The Penn Treebank.
Computational Linguistics, 19(2):313?330.Andriy Mnih and Geoffrey E. Hinton.
2007.
Three new graphical models for statistical language modelling.
InProceedings of the 24th International Conference on Machine Learning, pages 641?648.Andriy Mnih and Geoffrey E. Hinton.
2009.
A scalable hierarchical distributed language model.
In Advances inNeural Information Processing Systems, pages 1081?1088.Frederic Morin and Yoshua Bengio.
2005.
Hierarchical probabilistic neural network language model.
In AIS-TATS05, pages 246?252.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994.
A maximum entropy model for prepositional phraseattachment.
In Proceedings of the workshop on Human Language Technology, HLT ?94, pages 250?255,Stroudsburg, PA, USA.
Association for Computational Linguistics.Magnus Sahlgren.
2006.
The Word-Space Model: Using distributional analysis to represent syntagmatic andparadigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, Stockholm University.Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning.
2011.
Semi-supervised recursive autoencoders for predicting sentiment distributions.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing, pages 151?161.
Association for Computational Linguis-tics.Peter D. Turney and Patrick Pantel.
2010.
From frequency to meaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37(1):141?188, January.Jaakko J. V?ayrynen, Timo Honkela, and Lasse Lindqvist.
2007.
Towards explicit semantic features using indepen-dent component analysis.
In Proceedings of the Workshop Semantic Content Acquisition and Representation(SCAR), Stockholm, Sweden.
Swedish Institute of Computer Science.
SICS Technical Report T2007-06.Kilian Q. Weinberger and Lawrence K. Saul.
2009.
Distance metric learning for large margin nearest neighborclassification.
Journal of Machine Learning Research, 10:207?244, June.171
