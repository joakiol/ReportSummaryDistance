Tagging Speech RepairsPeter A. Heeman and James AllenDepartment of Computer ScienceUniversity of RochesterRochester, New York, 14627{heeman, james}@cs, rochester, eduABSTRACTThis paper describes a method of detecting speechrepairs that uses apart-of-speech tagger.
The tagger is given knowledge about categorytransitions for speechrepairs, and so is able to mark a transition eitheras a likely repair or as fluent speech.
Other contextual clues, such asediting terms, word fragments, and word matchings, are also factoredin by modifying the transition probabilities.1.
IntroductionInteractive spoken dialog provides many new challenges for spokenlanguage systems.
One of the most critical is the prevalence ofspeech repairs.
Speech repairs are dysfluencies where some of thewords that he speaker utters need to be removed in order to correctlyunderstand the speaker's meaning.
These repairs can be divided intothree types: flesh starts, modifications, and abridged.
A fresh startis where the speaker abandons what she was saying and starts again.the current plan is we take - okay let's say we start withthe bananas (d91-2.2 uttl05)A modification repair is where the speech repair modifies what wassaid before.after the orange juice is at - the oranges are at the OJfactory (d93-193 utt59)An abridged repair is where the repair consists olely of a fragmentand/or editing terms.we need to -um manage to get the bananas to Dansvillemore quickly (d93-14.3 utt50)In these examples, the "-"  marks the interruption point, the pointthat marks the end of the removed text (including word fragments).and precedes the editing terms, if present.
In our corpus of problemsolving dialogs, 25% of turns contain at least one repair, 67% ofrepairs occur with at least one other epair in the ram, and repairsin the same turn occur on average within 6 words of each other.
Asa result, no spoken language system will perform well without aneffective way to detect and correct speech repairs.We propose that speechrepairs can be detected and corrected withinthe local context of the repair.
So, clues are needed for detectingrepairs that do not depend on such global properties as the syntac-tic or semantic well-formedness of the entire utterance.
But thisdoes not mean that syntactic lues cannot be used.
One power-fig predictor of modification repairs is the presence of a syntacticanomaly (c.f.
Bear, Dowding and Shriberg, 1992) at the interruptionpoint.
The anomaly occurs because the text after the interruptionpoint is not intended to follow the text before the interruption, but toreplace it, so there is no reason why the text before and the text afterneed to be syntactically well-furmed.
In this paper, we describe howthe syntactic anomalies of modification repairs can be detected bya part-of-speech tagger, augmented with category transition probe-bilities for modification repairs.
Because we use a statistical model,other clues, such as the presence of editing terms, word fragments,and word conespondence can be factored in by appropriately modi-fying the transition probabilities.Focusing on the detection of modification repairs does not mean weare ignoring abridged repairs.
Assuming that word fragments andediting terms can be detected, abridged repairs are easy to detect andcorrect.
What is not trivial about hese repairs is differentiating themfrom modification repairs, especially where there are incidental wordcorrespondences.
It isthis distinction that makes uch repairs easy todetect, but potentially difficult o correcL Since our approach looksfor syntactic anomalies, other than those caused by word fragmentsand editing terms, it can distinguish abridged repairs from modifi-cation repairs, which should make both types of repairs easier tocorrecLAn ulterior motive for not using higher level syntactic or se-mantic knowledge is that the coverage of parsers and semanticinterpreters is not sufficient for unrestricted ialogs.
Recently,Dowding et al (1993) reported syntactic and semantic overage of86% for the Darpa Airline reservation corpus.
Unrestricted dialogswill present even more difficulties; not only will the speech be moreungrammatical, but there is also the problem of segmenting the dia-log into utterance units (c.f.
Wang and Hirachberg, 1992).
If speechrepairs can be detected and corrected before parsing and semanticinterpretation, this should simplify those modules as well as makethem more robust.2.
Previous WorkSeveral different strategies have been discussed in the literature fordetecting and correcting speech repairs.
One way to compare theeffectiveness of these approaches is to look at their recall and pre-cision rates.
For detecting repairs, the recall rate is the number ofcorrectly detected repairs compared to the number of repairs, andthe precision rate is the number of detected repairs compared tothe number of detections (including false positives).
But the truemeasures of success are the correction rates.
Correction recall isthe number of repairs that were properly corrected compared to thenumber of repairs.
Correction precision is the number of repairs thatwere properly corrected compared tothe total number of corrections.One of the first computational pproaches was that taken by187Hindle (I 983), who used a deterministic parser augmented with rulesto look for matching categories and matching strings of words.
Hin-die achieved a correction recall rate of 97% on his corpus; however,this was olbtalned by assuming that speech repairs were marked byan explicit "edit signal" and with part-of-speech tags externally sup-plied.The SRI g~up (Bear, Dowding and Shn%erg, 1992) removed theassumptiml of an explicit edit signal, and employed simple patternmatching techniques for detecting and correcting modification re-pairs (they removed all utterances with abridged repairs from theircorpus).
For detection, they were able to achieve a recall rate of76%, and a precision of 62%, and they were able to find the correctrepair 57% of the time, leading to an overall correction recall of 43%and correetion precision of 50%.
They also tried combining syntac-tic and semantic knowledge in a "parser-first" approach--first try toparse the input and if that fails, invoke repair strategies based on theirpattern matehing technique.
In a test set of 756 utterances containing26 repairs (Dowding et al, 1993), they obtained a detection recallrate of 42% and a precision of 84.6%; for correction, they obtaineda recall rate of 30% and a precision rate of 62%.Nakatani and Hirschberg (1993) investigated using acoustic infor-marion to detect he interruption point of speech repairs.
In theircorpus, 74% of all repairs are marked by a word fragment.
Usinghand-transcribed prosodic annotations, they trained a classifier ona 172 utterance training set to identify the interruption point (eachutterance contained at least one repair).
On a test set of 186 utter-antes containing 223 repairs, they obtained a recall rate of 83.4%and a precision of 93.9% in detecting speech repairs.
The clues thatthey found relevant were duration of pause between words, pres-ence of fragments, and lexical matching within a window of threewords.
However, they do not address the problem of determiningthe correction or distinguishing modification repairs from abridgedrepairs.3.
The CorpusAs part of the TRAINS project (Allen and Schubert, 1991), which isa long term research pmjeet o build a conversationally proficientplanning assistant, we are collecting a corpus of problem solvingdialogs.
The dialogs involve two participants, one who is playing therole of a user and has a certain task to accomplish, and another, who isplaying the role of the system by acting as a planning assistant (Gross,Allen and Traum, 1992).
The entire corpus consists of 112 dialogstotaling almost eight hours in length and containing about 62,000words and 6300 speaker turns.
These dialogs have been segmentedinto utterance files (c.f.
Heeman and Allen, 1994?
); words have beentranscribed and the speech repairs have been annotated.
For a trainingset, we use 40 of the dialogs, consisting of 24,000 words; and fortesting, 7 of the dialogs, consisting of 5800 words.In order to provide a large training corpus for the statistical model,we use a tagged version of the Brown corpus, from the Penn Tree-bank (Marcus, Santorini and Marcinklewicz, 1993).
We removedall punch~on in order to more closely approximate unsegmentedspoken speech.
This corpus provides us with category transitionprobabilities fur fluent speech.
These probabilities have also beenused to bootstrap our algorithm in order to determine the categoryprobabilities for speechrepalrs from our training corpus.
** We found that he tagset used in the Penn Treebank did not always providea fine onongh distinction for detecting syntactic anomalies.
We have madeTotalModification Repair 450Word Repetition 179Larger Repetition 58Word Replacement 72Other 141Abridged Repair i 267Total 1717withFrag.14.7%16.2%17.2%4.2%17.0%46A%26.5%with EditTerm19.3%16.2%19.0%13.9%26.2%54.3%32.4%Table 1: Oecunence of Types of RepalrsSpeech repairs can be divided into three intervals (c.f.
Levelt, 1983),the removed text, editing terms, and the resumed texL The removedtext and the editing terms are what need to be deleted in order todetermine what the speaker intended to say.
2 There is typically acorrespondence between the removed text and the resumed text, andfollowing Bear, Dowding and Shriberg (1992), we annotate this us-ing the labels m for word matching and r for word replacements(words of the same syntactic category).
Each pair is given a uniqueindex.
Other words in the removed text and resumed text are anno-tated with an x.
Also, editing terms (filled pauses and clue words)are labeled with et, and the interruption point with Int, which willbe before any editing terms associated with the repair, and after thefragment, if present.
(Further details of our annotation scheme canbe found in (Heeman and Allen, 1994a).)
Below is a sample an-notation, with removed text "go ~ oran-", editing term "urn", andresumed text "go to".gol tol oran-I uml gol toi Corningmll m2l xl intl etl ml\[ m2lTable 1 gives a breakdown of the modification speech repairs (thatdo not interfere with other epairs) and the abridged repairs, basedon hand-annotations.
Modification repairs are broken down intofour groups, word repetitious, larger epetitions, one word replacinganother, and others.
Also, the percentage of repairs that includefragments and editing terms is also given.
Two trends emerge fromthis data.
First, fragments and editing terms mark less than 34% of allmodification repairs.
Second, the presence of a fragment or editingterm does not give conclusive vidence as to whether the repair is amodification or an abridged repair.4.
Part.of-Speech TaggingPart-of-speech tagging is the process of assigning to a word thecategory that is most probable given the sentential context (Church,1988).
The sentential context is typically approximated by only aset number of previous categories, usually one or two.
Since thecontext is limited, we are making the Markov assumption, that thenext transition depends only on the input, which is the word that wethe following changes: (1) we ~-parated Weposifiom from subordinatingconjunctions; (2) we separated uses of "to" as a preposition from in me aspart of a to-infinilive; (3) rather than classify verbs by tense, we classifiedthem into four groups, conjugations of "be", conjugations of "have", verbsthat are followed by a to-infinitive, and verbs that are followed immediatelyby another verb.2The l~noved text and editing terms might still contain pragmatical in-formation, as the following example displays, "Peter was.., well...he wasfired/'188are currently uying to tag and the previous categories.
Good part-of-speech results can be obtained using only the preceding category(Weischedel t al., 1993), which is what we will be using.
In thiscase, the number of states of the Markov model will be N, whereN is the number of tags.
By making the Markov assumption, wecan use the Viterbi Algorithrn to find a maximum probability path inlinear time.Figure 1 gives a simplied view of a Markov model for part-of-speechtagging, where Ci is a possible category for the ith word, wi, andCi+~ is a possible category for word wi?t.
The category transitionprobability is simply the probability of category Ci+, followingcategory Ci, which is written as P(Ci+tICi), and the probabilityof word wi+, given category C~+, is P(wi+xlCi+~).
The categoryassignment that maximizes the product of these probabilities is takento be the best category assignment.P(~,~IC~) P(~+~IC~+,)Q P(C~+,IC,) "~Figure h Markov Model of Part-of-Speech Taggingthe next word.
(R~-l is independent of RiCi+1, givenC~.)
So P(RiC~+iIRi-iCi) = P(R~Ci+i ICi).One manipulation we can do is to use the definition of con-ditional probabilities to rewrite P(R~C~+,\[Ci) as P(RdCi) *P ( C~+ 1\[C~ R~).
This manipulation allows us to view the problem astagging null tokens between words as either the interreption point ofa modification repair, R~ = n,  or as fluent speech, R~ = ~b~.
Theresulting Markov model is shown in Figure 2.
Note that he contextfor category Ci+l is both C~ and R~.
So, R~ depends (indirectly) onthe joint context of Ci and Ci+l.
thus allowing syntactic anomaliesto be detected.
4 (~+~lCi+l)Figure 2: Markov Model of Repairs5.
A S imple  Mode l  o f  Speech  Repa i rsModification repairs are often accompanied bya syntactic anomalyacross the interruption point.
Consider the following example, "soit takes two hours to go to - from Elmira to Coming" (d93-17.4utt57), which contains a "to" followed by a "from".
Both shouldbe classified as prepositions, but the event of a preposition followedby another preposition is very rare in well-formed speech, so thereis a good chance that one of the prepositions might get erroneouslytagged as some other part of speech.
Since the category transitionsacross interruption points tend to be rare events in fluent speech, wesimply give the tagger the category transition probabilities aroundinterruption points of modification repairs.
By keeping track ofwhen this information is used, we not only have a way of detectingmodification repairs, but part-of-speech tagging is also improved.To incorporate knowledge about modification repairs, we let R/bea variable that indicates whether the transition from word wi towi+l contains the interruption point of a modification repair, andrather than tag each word, wl, with just a category, Ci, we willtag it with Ri-l Ci, the category and the presence of a modificationrepair.
3 This effectively multiplies the size of the tagsetby two.
FromFigure 1, we see that we will now need the following probabilities,P(RiCi+l iR,-I Ci) and P(wi\[R,-i C~).To keep the model simple, and ease problems with sparse data, wemake several independence assumptions.
(1) Given the category of a word, a repair before h is in-dependent of the word.
(Ri-i and wi are independent.given Ci.)
So P(wi\[Ri-lCl) = P(wdC~).
(2) Given the category of a word, a repair before that word isindependent of a repair following it and the category of3Changing each tag to CiRi would result in the same model.Table 3 (Section 6.4) gives results for this simple model running onour training corpus.
In order to remove ffects due to editing termsand word fragments, we temporarily eliminate them from the corpus.Also, for fresh starts and change-of-turn, the algorithm is reset, as ifit was an end of sentence.
To eliminate problems due to overlappingrepairs, we include only ~ points in which the next word is notintended to be removed (based on our hand annotations).
This givesus a total of 19587 data points, 384 were modification repairs, and thestatistical model found 169 of these, and a ftmher 204 false positives.This gives us a recall rate of 44.2% and a precision of 45.3%.
In thetest corpus, there are 98 modification repairs, of which the modelfound 30, and a further 23 false positives; giving a recall rate of30.6% and a precision rate of 56.6%.From Table 1, we can see that the recall rate of fragments as apredictor of a modification repair is 14.7% and their precision is34.7%.
s So, the method of statistically tagging modification repairshas more predictive power, and so can be used as a clue for detectingthem.
Furthermore, this method is doing something mere powerfulthan just detecting word repetitions or category repetitions.
Of the169 repairs that it found, 109 were word repetitions and an additional28 were category repetitions.
So, 32 of the repairs that were foundwere fIom less obvious yntactic anomalies.6.
Add ing  Addit ional  CluesIn the preceding section we built a model for detecting modificationrepairs by simply using category transitions.
However, there are othersources of infonnation that can be exploited, such as the presence offragments, editing terms, and word matehings.
The problem is that4pro~b\[lides for fluent ransitions are from the Brown coipus nd prob.-abflilies for repair transitions are from the Uaining ,~I~=5The precision rate was calculated by taking the number of fragmentsin a modification ~pair (450 * 14.7%) over the total number of fragments(450 * 14.7% + 267 * 46.4%).189these clues do not always ignal a modification repair.
For instance,a fragment is twice as likely to be part of an abridged repair than it isto be part of a modification repair.
One way to exploit hese clues isto aT to learn how to combine them, using a technique such as CART(Bfiemen, Friedman and Olsherh 1984).
However, a more intuitiveapproach is to adjust he transition probabilities for a modificationrepair to better eflect he more specific information that is known.Thus, we combine the information such that he individual pieces donot  have to give a 'yes' or a 'no', but rather, all can contribute to thedecision.6.1.
FragmentsAssuming; that fragments can be detected automatically(c.f.
Nakatani and Hirschberg, 1993), the question arises as to whatthe tagger should do with them.
If the tagger treats them as lexicalitems, the words on either side of the fragment will be separated.
Thiswill cause two problems.
First' if the fragment ispart of an abridgedrepair, category assignment to these words will be hindered.
Second,and more important to our work, is that the fragment will preventthe statistical model from judging the syntactic well-formedness ofthe word before the fragment and the word after, preventing it fromdistinguishing a modification repair from an abridged repair.
So, thetagger needs to skip over fragments.
However, the fragment can beviewed as the "word" that gets tagged as a modification repair ornot.
(The 'not' in this case means that the fragment is part of anabridged repair.)
When no fragment is present between words, weview the interval as a null word.
So, we augment the model picturedin Figure 2 with the probability of the presence of a fragment' Fi,given the presence of a repair, Rh as is pictured in Figure 3.P(Fd~'d@ ".-...?
(c,+, Ic, T,) P( -dc 'd~ r ~.........,.~ ~)c,+, ) ,Figure 3: Incorporating FragmentsSince there are two alternatives for F i - -a  fragment, fi, or not, 7 i - -and two alternatives for Ri---a repair or not, we need four statistics.From our training corpus, we have found that if a fragment ispresent,a modification repair is favored--P(filrl)/P(fd?i)---by a factorof 28.9.
If a fragment is not present' fluent speech is favored--P ( f i  kbi)/P(7, \[Ti), by a factor of 1.17.6.2.
Editing TermsEditing tenus, like fragments, give information as to the presenceof a modification repair.
So, we incorporate hem into the statisticalmodel by viewing them as part of the "word" that gets tagged withRi, thus changing the probability on the repair state from P(  Fi \[Ri)to P(  F~ E~ IRk), where E~ indicates the presence of editing terms.
Tosimplify the probabilities, and reduce problems due to sparse data,we make the fonowing independence assumption.O) Given that there is a modification repair, the presenceof a fragment or editing terms is independent.
(F/andE~ are independent, given Ri.)
So P(F~EdRi ) =P(F, IR,) * P(E, IR,).An additional complexity is that different editing terms do not havethe same predictive power.
So far we have investigated "urn" and"uh".
The presence of an "urn" favors a repair by a factor of 2.7,while for "uh" it is favored by a factor of 9.4.
If no editing term ispresent, fluent speech is favored by a factor of 1.2.6.3.
Word MatchingsIn a modification repair, there is often a coxrespondence between thetext that must be removed and the text that follows the interruptionpoint.
The simplest type of correspondence is word matchings.
Infact, in our test corpus, 80% of modification repairs have at leastone matching.
This information can be incorporated into the sta-tistical model in the same way that editing terms and fragments arehandled.
So, we change the probability of the repair state to beP(F~EiM~\[R~), where M~ indicates a word matching.
Again, weassume that the clues are independent of each other, allowing us totreat his clue separately from the others.Just as with editing terms, not all matches make the samepredictions about the occurrence of a modification repair.Bear, Dowding and Shriberg (1992) looked at the number of match-ing words versus the number of intervening words.
However, thisignores the category of the word matches.
For instance, a matchingverb (with some intervening words) is more likely to indicate arepairthan say a matching preposition or determiner.
So, we classify wordmatchings by category and number of intervening words.
Further-more, if there are multiple matches in a repair, we only use one, theone that most predicts arepair.
For instance in the following repair,the matching instances of "take" would be used over the matchinginstances of "will", since main verbs were found to more stronglysignal a modification repair than do modals.how long will that take - will it take for engine one atDansvifie (d93-183 utt43)Since the statistical model only uses one matching per repair, thesame is done in collecting the statistics.
So, our collection involvestwo steps.
In the first we collect statistics on all word matches, andin the second, for each repair, we count only the matching that moststrongly signals the repair.
Table 2 gives a partial list of how mucheach matching favors a repair broken down by category and numberof intervening words.
Entries that are marked with"-" do not containany datapoints andenlaies that are blank are below the baseline rate of0.209, the rate at which a modification repair is favored (or actuallydisfavored) when there is no matching at all.The problem with using word matching isthat it depends on identify-ing the removed text and its correspondences to the text that followsthe intermpdon point.
However, agood estimate can be obtained byusing all word matches with at most eight intervening words.6.4.
ResultsTable 3 summarizes the results of incorporating R~lifional cluesinto the Markov model.
The first column gives the results withoutany clues, the second with fragments, the third with editing terms,190Number of Intervening WordsCat 0 1 2 3 4 5DT 935.5 38.5 2.7 2.2 0.7 0.8IN - 171.7 59.6 22.9 10.4 6.3IS 490.0 55.8 5.9 3.2MD - 6706.5 199.8 37.1 12.4 2.4NN - 68.0 32.2 10.4 0.3 0.2NNP 144.3 9.2 6.2 6.7 3.3 2.8PREP 16433.6 2.8PRP 8242.3 15.2 2.9 1.2 0.5RB 25.2 19.4 6.9 6.4 3.9 3.6TO 5170.7 i 1.6 0.5 0.4VB 5170.6 216.3 71.5 31.2 18.1 7.0the test corpus, it achieved a recall rate of 83.0% and a precision of80.2%.The true measure of success is the overall detection and correctionrates.
On 721 repairs in the training corpus, which includes over-lapping repairs, the combined approach made the right correctionsfor 637, it made incorrect corrections for 19 more, and it falselydetected (and falsely corrected) 30 more.
This gives an overall cor-rection recall rate of 88.3% and a precision of 92.9%.
On the testcorpus consisting of 142 repairs, it made the right correction for 114of them, it incorrectly corrected 4 more, and it falsely detected 14more, for a correction recall rate of 80.3% end a precision of 86.4%.Table 4 summarizes the overall results for both the pattern builderand statistical model on the training corpus and on the test set.Table 2: Factor by which a repair is favoredthe fourth with word matches, and the fifth, with all of these cluesincorporated.
Of the 384 modification repairs in the training corpus,the full model predicts 305 of them versus 169 by the simple model.As for the false positives, the full model incorrectly predicted 207versus the simple model at 204.
So, we see that by incorporating~didonal clues, the statistical model can better identify modificationrepairs.TrainingCorpusDetectionRecall 91%Precision 96%CorrectionRecall 88%Precision 93%TestCorpus83%89%80%86%Table 4: Overall ResultsSimple Frag- Edit WordModel ments Terms Match  FullTraining: \[Recall 44.0% 50.0% 45.1% 76.5% 79.4%Precision 45.3% 47.8% 46.5% 54.9% 59.6%Testing:Recall 30.6% 43.9% 32.7% 74.5% 76.5%Precision 56.6% 62.3% 59.3% 58.4% 62.0%Table 3: Results of Markov Models7.
Correcting RepairsThe actual goal of detecting speech repairs is to be able to correctthem, so that the speaker's utterance can be understood.
We haveargued for the need to distinguish modification repairs from abridgedrepairs, because this distinction would be useful in determining thecorrection.
We have implemented a pattern builder (Heeman andAllen, 1994b), which builds potential repair patterns based on wordmatches and word replacements.
However, the pattom builder hasonly limited knowledge which it can use to decide which patterns arelikely repairs.
For instance, given the utterance "pick up uh fill upthe boxcars" (d93-17.4 utt40), it will postulate that there is a singlerepair, in which "pick up" is replaced by "fill up".
However, for anuuerance like "we need to urn manage to get the bananas" (d93-14-3uttS0), it will postulate that "manage to" replaces "need to".
So,we use the statistical model to filter repairs found by the patternbuilder.
This also removes alot of the false positives of the statisticalmodel, since no potential repair pattern would be found for them.On the training set, the model was queried by the pattern builder on961 potential modification repairs, of which 397 contained repairs.The model predicted 365 of these, and incorrectly detected 33 more,giving a detection recall rate of 91.9% and a precision of 91.7%.
ForThe results that we obtained are better than others reported in theliterature.
However, such compmisens are limited ue to differencesin both the type of repairs that ~ being studied ~ in the da~setsused for drawing results.
Bear, Dowding, and Shn'berg (1992) use theATIS corpus, which is a collection of queries made to an automatedairline reservation system.
As stated earlier, they removed all utter-ances that contained abridged repaY.
For detection they obtained arecall rate of 76% and a precision of 62%, and for correction, a recallrate of 43% and a precision of 50%.
It is not clear whether theirresults would be better or worse ff abridged repairs were included.Dowding et al (1993) used a similar setup for their d~t~= As part ofa complete system, they obtained a detection recall rate of 42% anda precision of 85%; and for correction, a recall rate of 30% and aprecision of 62%.
Lastly, Nakatani and Hirschberg (1993) also usedthe ATIS corpus, but in this case, focused only on detection, butdetection of all three types of repairs.
However, their test corpusconsisted entirely of utterances that contained at least one repair.This makes it hard to evaluate their results, reporting a detectionrecall rate of 83 % and precision of 94%.
Testing on an entire corpuswould clearly decrease their precision.
As for our own data, we useda corpus of natural dialogues that were segmented only by speakertarns, not by individual utterances, and we focused on modificationrepairs and abridged repairs, with fresh starts being marked in theinput so as not to cause interference in detecting the other two types.8.
DiscussionWe have described a statistical model for detecting speech repairs.The model detects repairs by using category trausifion probabilitiesaround repair intervals and for fluent speech~ By training on actualexamples of repairs, we can detect hem without having to set ar-bitrary cutoffs for category transidous that might be insensitive torarely used constructs.
If people actually use syntactic anomalies asa clue in detecting speech repairs, then training on examples of them191makes ense.In doing this work, we were faced with a lack of training dam.
Theeventual answer is to have a large corpus of tagged ialogs withthe speech repairs annotated.
Since this was not available, we usedthe Brown corpus for the fluent category-transition probabilities.As well, these transition probabilities were used to 'bootstrap' ourtagger in determining the part-of-speech tags for our training corpus.The tags of the 450 or so hand-annotated modification repairs werethen used for setting the transition probabilities around modificationrepairs.Another problem that we encountered was interference between ad-jacent utte,~ances in the same turn.
Subsequentutterances often buildon, or even repeat what was previously said (Walker, 1993).
Considerthe following utterance.that's all you needyou only need one tanker (d93-83 uu79)The tagger incorrectly hypothesized that this was a modificationrepair with an interruption point after the first occurrence of theword "need".
Even a relatively simple segmentation f the dialogsinto utterances would remove some of the false positives and improveperformance.Speech repairs do interact negatively with part-of-speech tagging,and even with statistical modeling of repairs, inappropriate ags arestill sometimes assigned.
In the following example, the secondoccurrence of the word "load" was categorized as a noun, and thespeech repair went undetected.it'll be seven a.m. by the time we load in - load thebananas (d93-12.4 utt53)9.
ConclusionsThis paper described a method of detecting repairs that uses a part-of-speech tagger.
Our work shows that a large percentage of speechrepairs can be detected, and corrected prior to parsing.
Prosodic cluescan be easily incorporated into our statistical model, and we arecurrently investigating methods of automatically extracting simpleprosodic features in order to further improve the performance of thealgorithm.Our algorithm assumes that the speech recognizer produces a se-quence of words and identifies the presertce ofword fragments.
Withthe exception of identifying fresh starts, all other processing is au-tomatic and does not require additional hand-tailored transcription.We will be incorporating this method of detecting and correctingspeech repairs into the next version of the TRAINS system, whichwill use spoken input.10.
AcknowledgmentsWe wish to thank Bin Li, Greg Mitchell, and Mia Stern for their helpin both transcribing dialogs and giving us useful comments on theannotation scheme.
We also wish to thank Hannah Blau, ElizabethShriberg, and David Traum for enlightening conversations.
Fund-ing gratefully received from the Natural Sciences and EngineeringResearch Council of Canada, fiom N SF under Grant IRI-90-13160,and from ONR/DARPA under Grant N00014-92 -J- 1512.ReferencesAllen, J. F. and Schubert, L. K. (1991).
The TRAINS project.
Tech-nical Report 382, Department of Computer Science, Universityof Rochester.Bear, J., Dowding, J., and Sire'berg, E. (1992).
Integrating mul-tiple knowledge sources for detection and correction of repairsin human-computer dialog.
In Proceedings ofthe 30 th AnnualMeeting of the Association for Computational Linguistics, pages56-63.Brieman, L., Friedman, J. H., and Olshen, R. A.
(1984).
Classi-fication and RegressionTrees.
Wadsworth & Brooks, Monterrey,CA.Chureh, K. (1988).
A stochastic parts program and noun phraseparser for unrestricted text.
In Preceedingsofthe 2nd Conferenceon Applied Natural Language Processing, pages 136---I 43.Dowding, J., Gawron, J. M., Appelt, D., Bear, J., Cherny, L.,Moore, R., and Moran, D. (1993).
Gemini: A natural languagesystem for spoken-language understanding.
In Proceedings ofthe 31 th Annual Meeting of the Association for ComputationalLinguistics, pages 54--61.Gross, D., Allen, J., and Traum, D. (1992).
The TRAINS 91dialogues.
Trains Technical Note 92-1, Department of ComputerScience, University of Rochester.Heeman, P. A. and Allen, J.
(1994a).
Annotating speech repairs.unpublished manuscript.Heeman, P. A. and Allen, J.
(1994b).
Detecting and correctingspeech repairs.
To appear in the 3 lth Meeting of the Associationfor Computational Linguistics.Heeman, P. A. andAllen, J.
(1994c).
Dialogue transcription tools.unpublished manuscript.Hindle, D. (1983).
Deterministic parsing of syntactic non-fluencies.
In Proceedings of the 21 ?t Annual Meeting of theAssociation for Computational Linguistics, pages 12.3-128.Levelt, W. J. M. (1983).
Monitoring and self-repair in speech.Cognition, 14:41-104.Marcus, M. P., Santorini, B., and Mareinkiewicz, M. A.
0993).Building a large annotated corpus of english: The Penn Treebank.Computational Linguistics, 19(2):313-330.Nakatani, C. and Hirschberg, J.
(1993).
A speech-first modelfor repair detection and correction.
In Proceedings ofthe 31 thAnnual Meeting of the Association for Computational Linguistics,pages 46--53.Walker, M. A.
(1993).
Informational redundancy and resourcebounds in dialogue.
Doctoral dissertion, Institute for Research inCognitive Science report IRCS-93-45, University of Pennsylva-nia.Wang, M. Q. and Hirschberg, J.
(1992).
Automatic lassifica-tion of intonational phrase boundaries.
Computer Speech andLanguage, 6:175-196.Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L., andPalmucci, J.
(1993).
Coping with ambiguity and unknownwords through probabilistic models.
Computational Linguistics,19(2):359-382.192
