Proceedings of the 8th International Natural Language Generation Conference, pages 99?102,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsTwo-Stage Stochastic Email SynthesizerYun-Nung Chen and Alexander I. RudnickySchool of Computer Science, Carnegie Mellon University5000 Forbes Ave., Pittsburgh, PA 15213-3891, USA{yvchen, air}@cs.cmu.eduAbstractThis paper presents the design and im-plementation details of an email synthe-sizer using two-stage stochastic naturallanguage generation, where the first stagestructures the emails according to senderstyle and topic structure, and the secondstage synthesizes text content based on theparticulars of an email structure elementand the goals of a given communicationfor surface realization.
The synthesizedemails reflect sender style and the intent ofcommunication, which can be further usedas synthetic evidence for developing otherapplications.1 IntroductionThis paper focuses on synthesizing emails that re-flect sender style and the intent of the communica-tion.
Such a process might be used for the gener-ation of common messages (for example a requestfor a meeting without direct intervention from thesender).
It can also be used in situations where nat-uralistic emails are needed for other applications.For instance, our email synthesizer was developedto provide emails to be used as part of syntheticevidence of insider threats for purposes of train-ing, prototyping, and evaluating anomaly detec-tors (Hershkop et al., 2011).Oh and Rudnicky (2002) showed that stochas-tic generation benefits from two factors: 1) ittakes advantage of the practical language of a do-main expert instead of the developer and 2) it re-states the problem in terms of classification andlabeling, where expertise is not required for de-veloping a rule-based generation system.
In thepresent work we investigate the use of stochastictechniques for generation of a different class ofcommunications and whether global structures canbe convincingly created.
Specifically we inves-tigate whether stochastic techniques can be usedto acceptably model longer texts and individualsender characteristics in the email domain, both ofwhich may require higher cohesion to be accept-able (Chen and Rudnicky, 2014).Our proposed system involves two-stagestochastic generation, shown in Figure 1, in whichthe first stage models email structures accordingto sender style and topic structure (high-levelgeneration), and the second stage synthesizestext content based on the particulars of a givencommunication (surface-level generation).2 The Proposed SystemThe whole architecture of the proposed system isshown in left part of Figure 1, which is composedof preprocessing, first-stage generation for emailorganization, and second-stage generation for sur-face realization.In preprocessing, we perform sentence segmen-tation for each email, and then manually anno-tate each sentence with a structure element, whichis used to create a structural label sequence foreach email and then to model sender style andtopic structure for email organization (1st stage inthe figure).
The defined structural labels includegreeting, inform, request, suggestion, question,answer, regard, acknowledgement, sorry, and sig-nature.
We also annotate content slots, includinggeneral classes automatically created by namedentity recognition (NER) (Finkel et al., 2005) andhand-crafted topic classes, to model text contentfor surface realization (2nd stage in the figure).The content slots include person, organization, lo-cation, time, money, percent, and date (generalclasses), and meeting, issue, and discussion (topicclasses).2.1 Modeling Sender Style and TopicStructure for Email OrganizationIn the first stage, given the sender and the fo-cused topic from the input, we generate the emailstructures by predicted sender-topic-specific mix-ture models, where the detailed is illustrated as be-99PredictingMixtureModelsEmail ArchiveBuildingStructureLMStructural Label AnnotationStructural Label SequencesGeneratingEmailStructuresGenerated StructuralLabel Sequences<greeting><inform>?Slot-Value PairsSlot AnnotationEmails w/ SlotsBuildingContentLMGeneratingTextContentScoringEmailCandidatesFillingSlotsSynthesized EmailsHi PeterToday?s ...1st-Stage Generation2nd-Stage GenerationSenderModelTopicModelSender TopicSystem Input PreprocessingSynthesized Emailswith SlotsHi [person]Today?s ...Request is generated atthe narrative level.Form filling:?
Topic Model?
Sender Model?
Slot fillersFigure 1: The system architecture (left) and the demo synthesizer (right).low.2.1.1 Building Structure Language ModelsBased on the annotation of structural labels, eachemail can be transformed into a structural labelsequence.
Then we train a sender-specific struc-ture model using the emails from each sender anda topic-specific model using the emails related toeach topic.
Here the structure models are tri-gram models with Good-Turing smoothing (Good,1953).2.1.2 Predicting Mixture ModelsWith sender-specific and topic-specific structuremodels, we predict the sender-topic-specific mix-ture models by interpolating the probabilities oftwo models.2.1.3 Generating Email StructuresWe generate structural label sequences randomlyaccording to the distribution from sender-topic-specific models.
Smoothed trigram models maygenerate any unseen trigrams based on back-offmethods, resulting in more randomness.
In ad-dition, we exclude unreasonable emails that don?tfollow two simple rules.1.
The structural label ?greeting?
only occurs atthe beginning of the email.2.
The structural label ?signature?
only occursat the end of the email.2.2 Surface RealizationIn the second stage, our surface realizer consistsof four aspects: building content language models,generating text content, scoring email candidates,and filling slots.2.2.1 Building Content Language ModelsAfter replacing the tokens with the slots, for eachstructural label, we train an unsmoothed 5-gramlanguage model using all sentences belonging tothe structural label.
Here we assume that the usageof within-sentence language is independent acrosssenders and topics, so generating the text contentonly considers the structural labels.
Unsmoothed5-gram language models introduce some variabil-ity in the output sentences while preventing non-sense sentences.2.2.2 Generating Text ContentThe input to surface realization is the generatedstructural label sequences.
We use the correspond-ing content language model for the given struc-tural label to generate word sequences randomlyaccording to distribution from the language model.Using unsmoothed 5-grams will not generateany unseen 5-grams (or smaller n-grams at thebeginning and end of a sentence), avoiding gen-eration of nonsense sentences within the 5-wordwindow.
With a structural label sequence, we cangenerate multiple sentences to form a synthesizedemail.1002.3 Scoring Email CandidatesThe input to the system contains the required in-formation that should be included in the synthe-sized result.
For each synthesized email, we penal-ize it if the email 1) contains slots for which thereis no provided valid value, or 2) does not havethe required slots.
The content generation enginestochastically generates a candidate email, scoresit, and outputs it when the synthesized email witha zero penalty score.2.4 Filling SlotsThe last step is to fill slots with the appropriatevalues.
For example, the sentence ?Tomorrow?s[meeting] is at [location].?
becomes ?Tomorrow?sspeech seminar is at Gates building.?
The rightpart of Figure 1 shows the process of the demo sys-tem,where based on a specific topic, a sender, andan interpolation weight, the system synthesizes anemail with structural labels first and then fills slotswith given slot fillers.3 ExperimentsWe conduct a preliminary experiment to evaluatethe proposed system.
The corpus used for our ex-periments is the Enron Email Dataset1, which con-tains a total of about 0.5M messages.
We selectedthe data related to daily business for our use.
Thisincludes data from about 150 users, and we ran-domly picked 3 senders, ones who wrote manyemails, and define additional 3 topic classes (meet-ing, discussion, issue) as topic-specific entitiesfor the task.
Each sender-specific model (acrosstopics) or topic-specific model (across senders) istrained on 30 emails.3.1 Evaluation of Sender Style ModelingTo evaluate the performance of sender style, 7 sub-jects were given 5 real emails from each senderand then 9 synthesized emails.
They were askedto rate each synthesized email for each sender ona scale between 1 to 5.With higher weight for sender-specific modelwhen predicting mixture models, average normal-ized scores the corresponding senders receives ac-count for 45%, which is above chance (33%).
Thissuggests that sender style can be noticed by sub-jects.
In a follow-up questionnaire, subjects indi-cated that their ratings were based on greeting us-age, politeness, the length of email and other char-acteristics.1https://www.cs.cmu.edu/?enron/3.2 Evaluation of Surface RealizationWe conduct a comparative evaluation of twodifferent generation algorithms, template-basedgeneration and stochastic generation, on thesame email structures.
Given a structural label,template-based generation consisted of randomlyselecting an intact whole sentence with the targetstructural label.
This could be termed sentence-level NLG, while stochastic generation is word-level NLG.We presented 30 pairs of (sentence-, word-)synthesized emails, and 7 subjects were asked tocompare the overall coherence of an email, itssentence fluency and naturalness; then select theirpreference.
The experiments showed that word-based stochastic generation outperforms or per-forms as well as the template-based algorithmfor all criteria (coherence, fluency, naturalness,and preference).
Some subjects noted that nei-ther email seemed human-written, perhaps an ar-tifact of our experimental design.
Nevertheless,we believe that this stochastic approach would re-quire less effort compared to most rule-based ortemplate-based systems in terms of knowledge en-gineering.In the future, we plan to develop an automaticemail structural label annotator in order to buildbetter language models (structure language mod-els and content language models) by increasingtraining data, and then improve the naturalness ofsynthesized emails.4 ConclusionThis paper illustrates a design and implementationof an email synthesizer with two-stage stochasticNLG: first a structure is generated, and then text isgenerated for each structure element.
Here senderstyle and topic structure can be modeled.
We be-lieve that this system can be applied to create re-alistic emails and could be carried out using mix-tures containing additional models based on othercharacteristics.
The proposed system shows thatemails can be synthesized using a small corpus oflabeled data, and the performance seems accept-able; however these models could be used to boot-strap the labeling of a larger corpus which in turncould be used to create more robust models.AcknowledgmentsThe authors wish to thank Brian Lindauer and KurtWallnau from the Software Engineering Instituteof Carnegie Mellon University for their guidance,advice, and help.101ReferencesYun-Nung Chen and Alexander I. Rudnicky.
2014.Two-stage stochastic natural language generation foremail synthesis by modeling sender style and topicstructure.
In Proceedings of the 8th InternationalNatural Language Generation Conference.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Irving J Good.
1953.
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40(3-4):237?264.Shlomo Hershkop, Salvatore J Stolfo, Angelos DKeromytis, and Hugh Thompson.
2011.
Anomalydetection at multiple scales (ADAMS).Alice H Oh and Alexander I Rudnicky.
2002.Stochastic natural language generation for spokendialog systems.
Computer Speech & Language,16(3):387?407.102
