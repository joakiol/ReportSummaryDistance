Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1?11,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEfficient Third-order Dependency ParsersTerry Koo and Michael CollinsMIT CSAIL, Cambridge, MA, 02139, USA{maestro,mcollins}@csail.mit.eduAbstractWe present algorithms for higher-order de-pendency parsing that are ?third-order?in the sense that they can evaluate sub-structures containing three dependencies,and ?efficient?
in the sense that they re-quire only O(n4) time.
Importantly, ournew parsers can utilize both sibling-styleand grandchild-style interactions.
Weevaluate our parsers on the Penn Tree-bank and Prague Dependency Treebank,achieving unlabeled attachment scores of93.04% and 87.38%, respectively.1 IntroductionDependency grammar has proven to be a very use-ful syntactic formalism, due in no small part to thedevelopment of efficient parsing algorithms (Eis-ner, 2000; McDonald et al, 2005b; McDonaldand Pereira, 2006; Carreras, 2007), which can beleveraged for a wide variety of learning methods,such as feature-rich discriminative models (Laf-ferty et al, 2001; Collins, 2002; Taskar et al,2003).
These parsing algorithms share an impor-tant characteristic: they factor dependency treesinto sets of parts that have limited interactions.
Byexploiting the additional constraints arising fromthe factorization, maximizations or summationsover the set of possible dependency trees can beperformed efficiently and exactly.A crucial limitation of factored parsing algo-rithms is that the associated parts are typicallyquite small, losing much of the contextual in-formation within the dependency tree.
For thepurposes of improving parsing performance, it isdesirable to increase the size and variety of theparts used by the factorization.1 At the sametime, the need for more expressive factorizations1For examples of how performance varies with the degreeof the parser?s factorization see, e.g., McDonald and Pereira(2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al(2008, Tables 2 and 4), or Suzuki et al (2009, Tables 3?6).must be balanced against any resulting increase inthe computational cost of the parsing algorithm.Consequently, recent work in dependency pars-ing has been restricted to applications of second-order parsers, the most powerful of which (Car-reras, 2007) requires O(n4) time and O(n3) space,while being limited to second-order parts.In this paper, we present new third-order pars-ing algorithms that increase both the size and vari-ety of the parts participating in the factorization,while simultaneously maintaining computationalrequirements of O(n4) time and O(n3) space.
Weevaluate our parsers on the Penn WSJ Treebank(Marcus et al, 1993) and Prague DependencyTreebank (Hajic?
et al, 2001), achieving unlabeledattachment scores of 93.04% and 87.38%.
In sum-mary, we make three main contributions:1.
Efficient new third-order parsing algorithms.2.
Empirical evaluations of these parsers.3.
A free distribution of our implementation.2The remainder of this paper is divided as follows:Sections 2 and 3 give background, Sections 4 and5 describe our new parsing algorithms, Section 6discusses related work, Section 7 presents our ex-perimental results, and Section 8 concludes.2 Dependency parsingIn dependency grammar, syntactic relationshipsare represented as head-modifier dependencies:directed arcs between a head, which is the more?essential?
word in the relationship, and a modi-fier, which supplements the meaning of the head.For example, Figure 1 contains a dependency be-tween the verb ?report?
(the head) and its object?sales?
(the modifier).
A complete analysis of asentence is given by a dependency tree: a set of de-pendencies that forms a rooted, directed tree span-ning the words of the sentence.
Every dependencytree is rooted at a special ?*?
token, allowing the2http://groups.csail.mit.edu/nlp/dpo3/1Insiders must report purchases and immediatelysales*Figure 1: An example dependency structure.selection of the sentential head to be modeled as ifit were a dependency.For a sentence x, we define dependency parsingas a search for the highest-scoring analysis of x:y?
(x) = argmaxy?Y(x)SCORE(x, y) (1)Here, Y(x) is the set of all trees compatible withx and SCORE(x, y) evaluates the event that tree yis the analysis of sentence x.
Since the cardinal-ity of Y(x) grows exponentially with the length ofthe sentence, directly solving Eq.
1 is impractical.A common strategy, and one which forms the fo-cus of this paper, is to factor each dependency treeinto small parts, which can be scored in isolation.Factored parsing can be formalized as follows:SCORE(x, y) =?p?ySCOREPART(x, p)That is, we treat the dependency tree y as a setof parts p, each of which makes a separate contri-bution to the score of y.
For certain factorizations,efficient parsing algorithms exist for solving Eq.
1.We define the order of a part according to thenumber of dependencies it contains, with analo-gous terminology for factorizations and parsing al-gorithms.
In the remainder of this paper, we focuson factorizations utilizing the following parts:gghhh h hmmm mmssstdependency sibling grandchildtri-siblinggrand-siblingSpecifically, Sections 4.1, 4.2, and 4.3 describeparsers that, respectively, factor trees into grand-child parts, grand-sibling parts, and a mixture ofgrand-sibling and tri-sibling parts.3 Existing parsing algorithmsOur new third-order dependency parsers build onideas from existing parsing algorithms.
In thissection, we provide background on two relevantparsers from previous work.
(a) +=h h mm ee(b) +=h h mm r r+1Figure 2: The dynamic-programming structuresand derivations of the Eisner (2000) algorithm.Complete spans are depicted as triangles and in-complete spans as trapezoids.
For brevity, we elidethe symmetric right-headed versions.3.1 First-order factorizationThe first type of parser we describe uses a ?first-order?
factorization, which decomposes a depen-dency tree into its individual dependencies.
Eis-ner (2000) introduced a widely-used dynamic-programming algorithm for first-order parsing; asit is the basis for many parsers, including our newalgorithms, we summarize its design here.The Eisner (2000) algorithm is based on twointerrelated types of dynamic-programming struc-tures: complete spans, which consist of a head-word and its descendents on one side, and incom-plete spans, which consist of a dependency and theregion between the head and modifier.Formally, we denote a complete span as Ch,ewhere h and e are the indices of the span?s head-word and endpoint.
An incomplete span is de-noted as Ih,m where h and m are the index of thehead and modifier of a dependency.
Intuitively,a complete span represents a ?half-constituent?headed by h, whereas an incomplete span is onlya partial half-constituent, since the constituent canbe extended by adding more modifiers to m.Each type of span is created by recursivelycombining two smaller, adjacent spans; the con-structions are specified graphically in Figure 2.An incomplete span is constructed from a pairof complete spans, indicating the division of therange [h,m] into constituents headed by h andm.
A complete span is created by ?complet-ing?
an incomplete span with the other half ofm?s constituent.
The point of concatenation ineach construction?m in Figure 2(a) or r in Fig-ure 2(b)?is the split point, a free index that mustbe enumerated to find the optimal construction.In order to parse a sentence x, it suffices tofind optimal constructions for all complete andincomplete spans defined on x.
This can be2(a) +=h h mm ee(b) +=h h mm ss(c) +=mms s r r+1Figure 3: The dynamic-programming structuresand derivations of the second-order sibling parser;sibling spans are depicted as boxes.
For brevity,we elide the right-headed versions.accomplished by adapting standard chart-parsingtechniques (Cocke and Schwartz, 1970; Younger,1967; Kasami, 1965) to the recursive derivationsdefined in Figure 2.
Since each derivation is de-fined by two fixed indices (the boundaries of thespan) and a third free index (the split point), theparsing algorithm requires O(n3) time and O(n2)space (Eisner, 1996; McAllester, 1999).3.2 Second-order sibling factorizationAs remarked by Eisner (1996) and McDonaldand Pereira (2006), it is possible to rearrange thedynamic-programming structures to conform to animproved factorization that decomposes each treeinto sibling parts?pairs of dependencies with ashared head.
Specifically, a sibling part consistsof a triple of indices (h,m, s) where (h,m) and(h, s) are dependencies, and where s and m aresuccessive modifiers to the same side of h.In order to parse this factorization, the second-order parser introduces a third type of dynamic-programming structure: sibling spans, which rep-resent the region between successive modifiers ofsome head.
Formally, we denote a sibling spanas Ss,m where s and m are a pair of modifiers in-volved in a sibling relationship.
Modified versionsof sibling spans will play an important role in thenew parsing algorithms described in Section 4.Figure 3 provides a graphical specification ofthe second-order parsing algorithm.
Note that in-complete spans are constructed in a new way: thesecond-order parser combines a smaller incom-plete span, representing the next-innermost depen-dency, with a sibling span that covers the regionbetween the two modifiers.
Sibling parts (h,m, s)can thus be obtained from Figure 3(b).
Despitethe use of second-order parts, each derivation is(a) = +gg hhh mm ee(b) = +g gh h hm mr r+1(c) = +gg hh hm me e(d) = +gg hh hm mr r+1Figure 4: The dynamic-programming structuresand derivations of Model 0.
For brevity, we elidethe right-headed versions.
Note that (c) and (d)differ from (a) and (b) only in the position of g.still defined by a span and split point, so the parserrequires O(n3) time and O(n2) space.4 New third-order parsing algorithmsIn this section we describe our new third-order de-pendency parsing algorithms.
Our overall methodis characterized by the augmentation of each spanwith a ?grandparent?
index: an index external tothe span whose role will be made clear below.
Thissection presents three parsing algorithms based onthis idea: Model 0, a second-order parser, andModels 1 and 2, which are third-order parsers.4.1 Model 0: all grandchildrenThe first parser, Model 0, factors each dependencytree into a set of grandchild parts?pairs of de-pendencies connected head-to-tail.
Specifically,a grandchild part is a triple of indices (g, h,m)where (g, h) and (h,m) are dependencies.3In order to parse this factorization, we augmentboth complete and incomplete spans with grand-parent indices; for brevity, we refer to these aug-mented structures as g-spans.
Formally, we denotea complete g-span as Cgh,e, where Ch,e is a normalcomplete span and g is an index lying outside therange [h, e], with the implication that (g, h) is adependency.
Incomplete g-spans are defined anal-ogously and are denoted as Igh,m.Figure 4 depicts complete and incomplete g-spans and provides a graphical specification of the3The Carreras (2007) parser also uses grandchild parts butonly in restricted cases; see Section 6 for details.3OPTIMIZEALLSPANS(x)1. ?
g, i Cgi,i = 0 / base case2.
for w = 1 .
.
.
(n?
1) / span width3.
for i = 1 .
.
.
(n?
w) / span start index4.
j = i + w / span end index5.
for g < i or g > j / grandparent index6.
Igi,j = max i?r<j {Cgi,r + Cij,r+1} +SCOREG(x, g, i, j)7.
Igj,i = max i?r<j {Cgj,r+1 + Cji,r} +SCOREG(x, g, j, i)8.
Cgi,j = max i<m?j {Igi,m + Cim,j}9.
Cgj,i = max i?m<j {Igj,m + Cjm,i}10. endfor11.
endfor12.
endforFigure 5: A bottom-up chart parser for Model 0.SCOREG is the scoring function for grandchildparts.
We use the g-span identities as shorthandfor their chart entries (e.g., Igi,j refers to the entrycontaining the maximum score of that g-span).Model 0 dynamic-programming algorithm.
Thealgorithm resembles the first-order parser, exceptthat every recursive construction must also set thegrandparent indices of the smaller g-spans; for-tunately, this can be done deterministically in allcases.
For example, Figure 4(a) depicts the de-composition of Cgh,e into an incomplete half anda complete half.
The grandparent of the incom-plete half is copied from Cgh,e while the grandpar-ent of the complete half is set to h, the head of mas defined by the construction.
Clearly, grandchildparts (g, h,m) can be read off of the incompleteg-spans in Figure 4(b,d).
Moreover, since eachderivation copies the grandparent index g into suc-cessively smaller g-spans, grandchild parts will beproduced for all grandchildren of g.Model 0 can be parsed by adapting standardtop-down or bottom-up chart parsing techniques.For concreteness, Figure 5 provides a pseudocodesketch of a bottom-up chart parser for Model 0;although the sketch omits many details, it suf-fices for the purposes of illustration.
The algo-rithm progresses from small widths to large inthe usual manner, but after defining the endpoints(i, j) there is an additional loop that enumeratesall possible grandparents.
Since each derivation isdefined by three fixed indices (the g-span) and onefree index (the split point), the complexity of thealgorithm is O(n4) time and O(n3) space.Note that the grandparent indices cause each g-(a) = +gg hhh mm ee(b) = +g gh h hm mss(c) = +hh hm mss r r+1Figure 6: The dynamic-programming structuresand derivations of Model 1.
Right-headed andright-grandparented versions are omitted.span to have non-contiguous structure.
For ex-ample, in Figure 4(a) the words between g and hwill be controlled by some other g-span.
Due tothese discontinuities, the correctness of the Model0 dynamic-programming algorithm may not beimmediately obvious.
While a full proof of cor-rectness is beyond the scope of this paper, we notethat each structure on the right-hand side of Fig-ure 4 lies completely within the structure on theleft-hand side.
This nesting of structures implies,in turn, that the usual properties required to ensurethe correctness of dynamic programming hold.4.2 Model 1: all grand-siblingsWe now describe our first third-order parsing al-gorithm.
Model 1 decomposes each tree into aset of grand-sibling parts?combinations of sib-ling parts and grandchild parts.
Specifically, agrand-sibling is a 4-tuple of indices (g, h,m, s)where (h,m, s) is a sibling part and (g, h,m) and(g, h, s) are grandchild parts.
For example, in Fig-ure 1, the words ?must,?
?report,?
?sales,?
and?immediately?
form a grand-sibling part.In order to parse this factorization, we intro-duce sibling g-spans Shm,s, which are composed ofa normal sibling span Sm,s and an external indexh, with the implication that (h,m, s) forms a validsibling part.
Figure 6 provides a graphical specifi-cation of the dynamic-programming algorithm forModel 1.
The overall structure of the algorithm re-sembles the second-order sibling parser, with theaddition of grandparent indices; as in Model 0, thegrandparent indices can be set deterministically inall cases.
Note that the sibling g-spans are crucial:they allow grand-sibling parts (g, h,m, s) to beread off of Figure 6(b), while simultaneously prop-agating grandparent indices to smaller g-spans.4(a) = +gg hhh mm ee(b) =g hh mm s(c) = +hh hm ms sst(d) = +hh hm mss r r+1Figure 7: The dynamic-programming structuresand derivations of Model 2.
Right-headed andright-grandparented versions are omitted.Like Model 0, Model 1 can be parsed via adap-tations of standard chart-parsing techniques; weomit the details for brevity.
Despite the move tothird-order parts, each derivation is still defined bya g-span and a split point, so that parsing requiresonly O(n4) time and O(n3) space.4.3 Model 2: grand-siblings and tri-siblingsHigher-order parsing algorithms have been pro-posed which extend the second-order sibling fac-torization to parts containing multiple siblings(McDonald and Pereira, 2006, also see Section 6for discussion).
In this section, we show how ourg-span-based techniques can be combined with athird-order sibling parser, resulting in a parser thatcaptures both grand-sibling parts and tri-siblingparts?4-tuples of indices (h,m, s, t) such thatboth (h,m, s) and (h, s, t) are sibling parts.In order to parse this factorization, we intro-duce a new type of dynamic-programming struc-ture: sibling-augmented spans, or s-spans.
For-mally, we denote an incomplete s-span as Ih,m,swhere Ih,m is a normal incomplete span and s is anindex lying in the strict interior of the range [h,m],such that (h,m, s) forms a valid sibling part.Figure 7 provides a graphical specification ofthe Model 2 parsing algorithm.
An incompletes-span is constructed by combining a smaller in-complete s-span, representing the next-innermostpair of modifiers, with a sibling g-span, coveringthe region between the outer two modifiers.
Asin Model 1, sibling g-spans are crucial for propa-gating grandparent indices, while allowing the re-covery of tri-sibling parts (h,m, s, t).
Figure 7(b)shows how an incomplete s-span can be convertedinto an incomplete g-span by exchanging the in-ternal sibling index for an external grandparent in-dex; in the process, grand-sibling parts (g, h,m, s)are enumerated.
Since every derivation is definedby an augmented span and a split point, Model 2can be parsed in O(n4) time and O(n3) space.It should be noted that unlike Model 1, Model2 produces grand-sibling parts only for the outer-most pair of grandchildren,4 similar to the behav-ior of the Carreras (2007) parser.
In fact, the re-semblance is more than passing, as Model 2 canemulate the Carreras (2007) algorithm by ?demot-ing?
each third-order part into a second-order part:SCOREGS(x, g, h,m, s) = SCOREG(x, g, h,m)SCORETS(x, h,m, s, t) = SCORES(x, h,m, s)where SCOREG, SCORES, SCOREGS andSCORETS are the scoring functions for grand-children, siblings, grand-siblings and tri-siblings,respectively.
The emulated version has the samecomputational complexity as the original, so thereis no practical reason to prefer it over the original.Nevertheless, the relationship illustrated abovehighlights the efficiency of our approach: weare able to recover third-order parts in place ofsecond-order parts, at no additional cost.4.4 DiscussionThe technique of grandparent-index augmentationhas proven fruitful, as it allows us to parse ex-pressive third-order factorizations while retainingan efficient O(n4) runtime.
In fact, our third-order parsing algorithms are ?optimally?
efficientin an asymptotic sense.
Since each third-order partis composed of four separate indices, there are?
(n4) distinct parts.
Any third-order parsing al-gorithm must at least consider the score of eachpart, hence third-order parsing is ?
(n4) and it fol-lows that the asymptotic complexity of Models 1and 2 cannot be improved.The key to the efficiency of our approach is afundamental asymmetry in the structure of a di-rected tree: a head can have any number of mod-ifiers, while a modifier always has exactly onehead.
Factorizations like that of Carreras (2007)obtain grandchild parts by augmenting spans withthe indices of modifiers, leading to limitations on4The reason for the restriction is that in Model 2, grand-siblings can only be derived via Figure 7(b), which does notrecursively copy the grandparent index for reuse in smallerg-spans as Model 1 does in Figure 6(b).5the grandchildren that can participate in the fac-torization.
Our method, by ?inverting?
the modi-fier indices into grandparent indices, exploits thestructural asymmetry.As a final note, the parsing algorithms describedin this section fall into the category of projectivedependency parsers, which forbid crossing depen-dencies.
If crossing dependencies are allowed, itis possible to parse a first-order factorization byfinding the maximum directed spanning tree (Chuand Liu, 1965; Edmonds, 1967; McDonald et al,2005b).
Unfortunately, designing efficient higher-order non-projective parsers is likely to be chal-lenging, based on recent hardness results (McDon-ald and Pereira, 2006; McDonald and Satta, 2007).5 ExtensionsWe briefly outline a few extensions to our algo-rithms; we hope to explore these in future work.5.1 Probabilistic inferenceMany statistical modeling techniques are based onpartition functions and marginals?summationsover the set of possible trees Y(x).
Straightfor-ward adaptations of the inside-outside algorithm(Baker, 1979) to our dynamic-programming struc-tures would suffice to compute these quantities.5.2 Labeled parsingOur parsers are easily extended to labeled depen-dencies.
Direct integration of labels into Models 1and 2 would result in third-order parts composedof three labeled dependencies, at the cost of in-creasing the time and space complexities by fac-tors of O(L3) and O(L2), respectively, where Lbounds the number of labels per dependency.5.3 Word sensesIf each word in x has a set of possible ?senses,?our parsers can be modified to recover the bestjoint assignment of syntax and senses for x, byadapting methods in Eisner (2000).
Complex-ity would increase by factors of O(S4) time andO(S3) space, where S bounds the number ofsenses per word.5.4 Increased contextIf more vertical context is desired, the dynamic-programming structures can be extended with ad-ditional ancestor indices, resulting in a ?spine?
ofancestors above each span.
Each additional an-cestor lengthens the vertical scope of the factor-ization (e.g., from grand-siblings to ?great-grand-siblings?
), while increasing complexity by a factorof O(n).
Horizontal context can also be increasedby adding internal sibling indices; each additionalsibling widens the scope of the factorization (e.g.,from grand-siblings to ?grand-tri-siblings?
), whileincreasing complexity by a factor of O(n).6 Related workOur method augments each span with the indexof the head that governs that span, in a mannersuperficially similar to parent annotation in CFGs(Johnson, 1998).
However, parent annotation isa grammar transformation that is independent ofany particular sentence, whereas our method an-notates spans with indices into the current sen-tence.
These indices allow the use of arbitrary fea-tures predicated on the position of the grandparent(e.g., word identity, POS tag, contextual POS tags)without affecting the asymptotic complexity of theparsing algorithm.
Efficiently encoding this kindof information into a sentence-independent gram-mar transformation would be challenging at best.Eisner (2000) defines dependency parsing mod-els where each word has a set of possible ?senses?and the parser recovers the best joint assignmentof syntax and senses.
Our new parsing algorithmscould be implemented by defining the ?sense?
ofeach word as the index of its head.
However, whenparsing with senses, the complexity of the Eisner(2000) parser increases by factors of O(S3) timeand O(S2) space (ibid., Section 4.2).
Since eachword has n potential heads, a direct applicationof the word-sense parser leads to time and spacecomplexities of O(n6) and O(n4), respectively, incontrast to our O(n4) and O(n3).5Eisner (2000) also uses head automata to scoreor recognize the dependents of each head.
An in-teresting question is whether these automata couldbe coerced into modeling the grandparent indicesused in our parsing algorithms.
However, notethat the head automata are defined in a sentence-independent manner, with two automata per wordin the vocabulary (ibid., Section 2).
The automataare thus analogous to the rules of a CFG and at-5In brief, the reason for the inefficiency is that the word-sense parser is unable to exploit certain constraints, such asthe fact that the endpoints of a sibling g-span must have thesame head.
The word-sense parser would needlessly enumer-ate all possible pairs of heads in this case.6tempts to use them to model grandparent indiceswould face difficulties similar to those already de-scribed for grammar transformations in CFGs.It should be noted that third-order parsershave previously been proposed by McDonald andPereira (2006), who remarked that their second-order sibling parser (see Figure 3) could easilybe extended to capture m > 1 successive modi-fiers in O(nm+1) time (ibid., Section 2.2).
To ourknowledge, however, Models 1 and 2 are the firstthird-order parsing algorithms capable of model-ing grandchild parts.
In our experiments, we findthat grandchild interactions make important con-tributions to parsing performance (see Table 3).Carreras (2007) presents a second-order parserthat can score both sibling and grandchild parts,with complexities of O(n4) time and O(n3) space.An important limitation of the parser?s factoriza-tion is that it only defines grandchild parts foroutermost grandchildren: (g, h,m) is scored onlywhen m is the outermost modifier of h in some di-rection.
Note that Models 1 and 2 have the samecomplexity as Carreras (2007), but strictly greaterexpressiveness: for each sibling or grandchild partused in the Carreras (2007) factorization, Model 1defines an enclosing grand-sibling, while Model 2defines an enclosing tri-sibling or grand-sibling.The factored parsing approach we focus on issometimes referred to as ?graph-based?
parsing;a popular alternative is ?transition-based?
parsing,in which trees are constructed by making a se-ries of incremental decisions (Yamada and Mat-sumoto, 2003; Attardi, 2006; Nivre et al, 2006;McDonald and Nivre, 2007).
Transition-basedparsers do not impose factorizations, so they candefine arbitrary features on the tree as it is beingbuilt.
As a result, however, they rely on greedy orapproximate search algorithms to solve Eq.
1.7 Parsing experimentsIn order to evaluate the effectiveness of our parsersin practice, we apply them to the Penn WSJ Tree-bank (Marcus et al, 1993) and the Prague De-pendency Treebank (Hajic?
et al, 2001; Hajic?,1998).6 We use standard training, validation, andtest splits7 to facilitate comparisons.
Accuracy is6For English, we extracted dependencies using JoakimNivre?s Penn2Malt tool with standard head rules (Yamadaand Matsumoto, 2003); for Czech, we ?projectivized?
thetraining data by finding best-match projective trees.7For Czech, the PDT has a predefined split; for English,we split the Sections as: 2?21 training, 22 validation, 23 test.measured with unlabeled attachment score (UAS):the percentage of words with the correct head.87.1 Features for third-order parsingOur parsing algorithms can be applied to scoresoriginating from any source, but in our experi-ments we chose to use the framework of structuredlinear models, deriving our scores as:SCOREPART(x, p) = w ?
f(x, p)Here, f is a feature-vector mapping and w is avector of associated parameters.
Following stan-dard practice for higher-order dependency parsing(McDonald and Pereira, 2006; Carreras, 2007),Models 1 and 2 evaluate not only the relevantthird-order parts, but also the lower-order partsthat are implicit in their third-order factoriza-tions.
For example, Model 1 defines feature map-pings for dependencies, siblings, grandchildren,and grand-siblings, so that the score of a depen-dency parse is given by:MODEL1SCORE(x, y) =?
(h,m)?ywdep ?
fdep(x, h,m)?
(h,m,s)?ywsib ?
fsib(x, h,m, s)?
(g,h,m)?ywgch ?
fgch(x, g, h,m)?
(g,h,m,s)?ywgsib ?
fgsib(x, g, h,m, s)Above, y is simultaneously decomposed into sev-eral different types of parts; trivial modificationsto the Model 1 parser allow it to evaluate all ofthe necessary parts in an interleaved fashion.
Asimilar treatment of Model 2 yields five featuremappings: the four above plus ftsib(x, h,m, s, t),which represents tri-sibling parts.The lower-order feature mappings fdep, fsib, andfgch are based on feature sets from previous work(McDonald et al, 2005a; McDonald and Pereira,2006; Carreras, 2007), to which we added lexical-ized versions of several features.
For example, fdepcontains lexicalized ?in-between?
features that de-pend on the head and modifier words as well as aword lying in between the two; in contrast, pre-vious work has generally defined in-between fea-tures for POS tags only.
As another example, our8As in previous work, English evaluation ignores any to-ken whose gold-standard POS tag is one of {??
??
: , .
}.7second-order mappings fsib and fgch define lexicaltrigram features, while previous work has gener-ally used POS trigrams only.Our third-order feature mappings fgsib and ftsibconsist of four types of features.
First, we define4-gram features that characterize the four relevantindices using words and POS tags; examples in-clude POS 4-grams and mixed 4-grams with oneword and three POS tags.
Second, we define 4-gram context features consisting of POS 4-gramsaugmented with adjacent POS tags: for exam-ple, fgsib(x, g, h,m, s) includes POS 7-grams forthe tags at positions (g, h,m, s, g+1, h+1,m+1).Third, we define backed-off features that track bi-gram and trigram interactions which are absentin the lower-order feature mappings: for exam-ple, ftsib(x, h,m, s, t) contains features predicatedon the trigram (m, s, t) and the bigram (m, t),neither of which exist in any lower-order part.Fourth, noting that coordinations are typically an-notated as grand-siblings (e.g., ?report purchasesand sales?
in Figure 1), we define coordinationfeatures for certain grand-sibling parts.
For exam-ple, fgsib(x, g, h,m, s) contains features examin-ing the implicit head-modifier relationship (g,m)that are only activated when the POS tag of s is acoordinating conjunction.Finally, we make two brief remarks regardingthe use of POS tags.
First, we assume that inputsentences have been automatically tagged in a pre-processing step.9 Second, for any feature that de-pends on POS tags, we include two copies of thefeature: one using normal POS tags and anotherusing coarsened versions10 of the POS tags.7.2 Averaged perceptron trainingThere are a wide variety of parameter estima-tion methods for structured linear models, suchas log-linear models (Lafferty et al, 2001) andmax-margin models (Taskar et al, 2003).
Wechose the averaged structured perceptron (Freundand Schapire, 1999; Collins, 2002) as it combineshighly competitive performance with fast trainingtimes, typically converging in 5?10 iterations.
Wetrain each parser for 10 iterations and select pa-9For Czech, the PDT provides automatic tags; for English,we used MXPOST (Ratnaparkhi, 1996) to tag validation andtest data, with 10-fold cross-validation on the training set.Note that the reliance on POS-tagged input can be relaxedslightly by treating POS tags as word senses; see Section 5.3and McDonald (2006, Table 6.1).10For Czech, we used the first character of the tag; for En-glish, we used the first two characters, except PRP and PRP$.Beam Pass Orac Acc1 Acc2 Time1 Time20.0001 26.5 99.92 93.49 93.49 49.6m 73.5m0.001 16.7 99.72 93.37 93.29 25.9m 24.2m0.01 9.1 99.19 93.26 93.16 6.7m 7.9mTable 1: Effect of the marginal-probability beamon English parsing.
For each beam value, parserswere trained on the English training set and evalu-ated on the English validation set; the same beamvalue was applied to both training and validationdata.
Pass = %dependencies surviving the beam intraining data, Orac = maximum achievable UASon validation data, Acc1/Acc2 = UAS of Models1/2 on validation data, and Time1/Time2 = min-utes per perceptron training iteration for Models1/2, averaged over all 10 iterations.
For perspec-tive, the English training set has a total of 39,832sentences and 950,028 words.
A beam of 0.0001was used in all experiments outside this table.rameters from the iteration that achieves the bestscore on the validation set.7.3 Coarse-to-fine pruningIn order to decrease training times, we followCarreras et al (2008) and eliminate unlikely de-pendencies using a form of coarse-to-fine pruning(Charniak and Johnson, 2005; Petrov and Klein,2007).
In brief, we train a log-linear first-orderparser11 and for every sentence x in training, val-idation, and test data we compute the marginalprobability P (h,m |x) of each dependency.
Ourparsers are then modified to ignore any depen-dency (h,m) whose marginal probability is below0.0001?maxh?
P (h?,m |x).
Table 1 provides in-formation on the behavior of the pruning method.7.4 Main resultsTable 2 lists the accuracy of Models 1 and 2 on theEnglish and Czech test sets, together with somerelevant results from related work.12 The mod-els marked ???
are not directly comparable to ourwork as they depend on additional sources of in-formation that our models are trained without?unlabeled data in the case of Koo et al (2008) and11For English, we generate marginals using a projectiveparser (Baker, 1979; Eisner, 2000); for Czech, we generatemarginals using a non-projective parser (Smith and Smith,2007; McDonald and Satta, 2007; Koo et al, 2007).
Param-eters for these models are obtained by running exponentiatedgradient training for 10 iterations (Collins et al, 2008).12Model 0 was not tested as its factorization is a strict sub-set of the factorization of Model 1.8Parser Eng CzeMcDonald et al (2005a,2005b) 90.9 84.4McDonald and Pereira (2006) 91.5 85.2Koo et al (2008), standard 92.02 86.13Model 1 93.04 87.38Model 2 92.93 87.37Koo et al (2008), semi-sup?
93.16 87.13Suzuki et al (2009)?
93.79 88.05Carreras et al (2008)?
93.5Table 2: UAS of Models 1 and 2 on test data, withrelevant results from related work.
Note that Kooet al (2008) is listed with standard features andsemi-supervised features.
?
: see main text.Suzuki et al (2009) and phrase-structure annota-tions in the case of Carreras et al (2008).
All threeof the ???
models are based on versions of the Car-reras (2007) parser, so modifying these methods towork with our new third-order parsing algorithmswould be an interesting topic for future research.For example, Models 1 and 2 obtain results com-parable to the semi-supervised parsers of Koo etal.
(2008), and additive gains might be realized byapplying their cluster-based feature sets to our en-riched factorizations.7.5 Ablation studiesIn order to better understand the contributions ofthe various feature types, we ran additional abla-tion experiments; the results are listed in Table 3,in addition to the scores of Model 0 and the emu-lated Carreras (2007) parser (see Section 4.3).
In-terestingly, grandchild interactions appear to pro-vide important information: for example, whenModel 2 is used without grandchild-based features(?Model 2, no-G?
in Table 3), its accuracy suffersnoticeably.
In addition, it seems that grandchildinteractions are particularly useful in Czech, whilesibling interactions are less important: considerthat Model 0, a second-order grandchild parserwith no sibling-based features, can easily outper-form ?Model 2, no-G,?
a third-order sibling parserwith no grandchild-based features.8 ConclusionWe have presented new parsing algorithms that arecapable of efficiently parsing third-order factoriza-tions, including both grandchild and sibling inter-actions.
Due to space restrictions, we have beennecessarily brief at some points in this paper; someadditional details can be found in Koo (2010).Parser Eng CzeModel 0 93.07 87.39Carreras (2007) emulation 93.14 87.25Model 1 93.49 87.64Model 1, no-3rd 93.17 87.57Model 2 93.49 87.46Model 2, no-3rd 93.20 87.43Model 2, no-G 92.92 86.76Table 3: UAS for modified versions of our parserson validation data.
The term no-3rd indicates aparser that was trained and tested with the third-order feature mappings fgsib and ftsib deactivated,though lower-order features were retained; notethat ?Model 2, no-3rd?
is not identical to the Car-reras (2007) parser as it defines grandchild partsfor the pair of grandchildren.
The term no-G indi-cates a parser that was trained and tested with thegrandchild-based feature mappings fgch and fgsibdeactivated; note that ?Model 2, no-G?
emulatesthe third-order sibling parser proposed by McDon-ald and Pereira (2006).There are several possibilities for further re-search involving our third-order parsing algo-rithms.
One idea would be to consider extensionsand modifications of our parsers, some of whichhave been suggested in Sections 5 and 7.4.
A sec-ond area for future work lies in applications of de-pendency parsing.
While we have evaluated ournew algorithms on standard parsing benchmarks,there are a wide variety of tasks that may bene-fit from the extended context offered by our third-order factorizations; for example, the 4-gram sub-structures enabled by our approach may be usefulfor dependency-based language modeling in ma-chine translation (Shen et al, 2008).
Finally, inthe hopes that others in the NLP community mayfind our parsers useful, we provide a free distribu-tion of our implementation.2AcknowledgmentsWe would like to thank the anonymous review-ers for their helpful comments and suggestions.We also thank Regina Barzilay and AlexanderRush for their much-appreciated input during thewriting process.
The authors gratefully acknowl-edge the following sources of support: TerryKoo and Michael Collins were both funded bya DARPA subcontract under SRI (#27-001343),and Michael Collins was additionally supportedby NTT (Agmt.
dtd.
06/21/98).9ReferencesGiuseppe Attardi.
2006.
Experiments with a Multilan-guage Non-Projective Dependency Parser.
In Pro-ceedings of the 10th CoNLL, pages 166?170.
Asso-ciation for Computational Linguistics.James Baker.
1979.
Trainable Grammars for SpeechRecognition.
In Proceedings of the 97th meeting ofthe Acoustical Society of America.Xavier Carreras, Michael Collins, and Terry Koo.2008.
TAG, Dynamic Programming, and the Per-ceptron for Efficient, Feature-rich Parsing.
In Pro-ceedings of the 12th CoNLL, pages 9?16.
Associa-tion for Computational Linguistics.Xavier Carreras.
2007.
Experiments with a Higher-Order Projective Dependency Parser.
In Proceed-ings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 957?961.
Association for Computa-tional Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine N -best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of the 43rd ACL.Y.J.
Chu and T.H.
Liu.
1965.
On the Shortest Ar-borescence of a Directed Graph.
Science Sinica,14:1396?1400.John Cocke and Jacob T. Schwartz.
1970.
Program-ming Languages and Their Compilers: PreliminaryNotes.
Technical report, New York University.Michael Collins, Amir Globerson, Terry Koo, XavierCarreras, and Peter L. Bartlett.
2008.
Exponenti-ated Gradient Algorithms for Conditional RandomFields and Max-Margin Markov Networks.
Journalof Machine Learning Research, 9:1775?1822, Aug.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Exper-iments with Perceptron Algorithms.
In Proceedingsof the 7th EMNLP, pages 1?8.
Association for Com-putational Linguistics.Jack R. Edmonds.
1967.
Optimum Branchings.
Jour-nal of Research of the National Bureau of Standards,71B:233?240.Jason Eisner.
1996.
Three New Probabilistic Modelsfor Dependency Parsing: An Exploration.
In Pro-ceedings of the 16th COLING, pages 340?345.
As-sociation for Computational Linguistics.Jason Eisner.
2000.
Bilexical Grammars and TheirCubic-Time Parsing Algorithms.
In Harry Buntand Anton Nijholt, editors, Advances in Probabilis-tic and Other Parsing Technologies, pages 29?62.Kluwer Academic Publishers.Yoav Freund and Robert E. Schapire.
1999.
LargeMargin Classification Using the Perceptron Algo-rithm.
Machine Learning, 37(3):277?296.Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila Panevova,and Petr Sgall.
2001.
The Prague Dependency Tree-bank 1.0, LDC No.
LDC2001T10.
Linguistics DataConsortium.Jan Hajic?.
1998.
Building a Syntactically AnnotatedCorpus: The Prague Dependency Treebank.
In EvaHajic?ova?, editor, Issues of Valency and Meaning.Studies in Honor of Jarmila Panevova?, pages 12?19.Mark Johnson.
1998.
PCFG Models of LinguisticTree Representations.
Computational Linguistics,24(4):613?632.Tadao Kasami.
1965.
An Efficient Recognition andSyntax-analysis Algorithm for Context-free Lan-guages.
Technical Report AFCRL-65-758, AirForce Cambridge Research Lab.Terry Koo, Amir Globerson, Xavier Carreras, andMichael Collins.
2007.
Structured Prediction Mod-els via the Matrix-Tree Theorem.
In Proceedingsof EMNLP-CoNLL, pages 141?150.
Association forComputational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple Semi-supervised Dependency Pars-ing.
In Proceedings of the 46th ACL, pages 595?603.Association for Computational Linguistics.Terry Koo.
2010.
Advances in Discriminative Depen-dency Parsing.
Ph.D. thesis, Massachusetts Instituteof Technology, Cambridge, MA, USA, June.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
In Proceedings of the 18th ICML,pages 282?289.
Morgan Kaufmann.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.David A. McAllester.
1999.
On the ComplexityAnalysis of Static Analyses.
In Proceedings ofthe 6th Static Analysis Symposium, pages 312?329.Springer-Verlag.Ryan McDonald and Joakim Nivre.
2007.
Character-izing the Errors of Data-Driven Dependency Parsers.In Proceedings of EMNLP-CoNLL, pages 122?131.Association for Computational Linguistics.Ryan McDonald and Fernando Pereira.
2006.
OnlineLearning of Approximate Dependency Parsing Al-gorithms.
In Proceedings of the 11th EACL, pages81?88.
Association for Computational Linguistics.Ryan McDonald and Giorgio Satta.
2007.
On theComplexity of Non-Projective Data-Driven Depen-dency Parsing.
In Proceedings of IWPT.10Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online Large-Margin Training ofDependency Parsers.
In Proceedings of the 43rdACL, pages 91?98.
Association for ComputationalLinguistics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-Projective DependencyParsing using Spanning Tree Algorithms.
In Pro-ceedings of HLT-EMNLP, pages 523?530.
Associa-tion for Computational Linguistics.Ryan McDonald.
2006.
Discriminative Training andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania, Philadel-phia, PA, USA, July.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?enEryig?it, and Svetoslav Marinov.
2006.
LabeledPseudo-Projective Dependency Parsing with Sup-port Vector Machines.
In Proceedings of the 10thCoNLL, pages 221?225.
Association for Computa-tional Linguistics.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
In Proceedings of HLT-NAACL, pages 404?411.
Association for Computa-tional Linguistics.Adwait Ratnaparkhi.
1996.
A Maximum EntropyModel for Part-Of-Speech Tagging.
In Proceedingsof the 1st EMNLP, pages 133?142.
Association forComputational Linguistics.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A New String-to-Dependency Machine TranslationAlgorithm with a Target Dependency LanguageModel.
In Proceedings of the 46th ACL, pages 577?585.
Association for Computational Linguistics.David A. Smith and Noah A. Smith.
2007.
Proba-bilistic Models of Nonprojective Dependency Trees.In Proceedings of EMNLP-CoNLL, pages 132?140.Association for Computational Linguistics.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael Collins.
2009.
An Empirical Study ofSemi-supervised Structured Conditional Models forDependency Parsing.
In Proceedings of EMNLP,pages 551?560.
Association for Computational Lin-guistics.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003.Max margin markov networks.
In Sebastian Thrun,Lawrence K. Saul, and Bernhard Scho?lkopf, editors,NIPS.
MIT Press.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical Dependency Analysis with Support Vector Ma-chines.
In Proceedings of the 8th IWPT, pages 195?206.
Association for Computational Linguistics.David H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2):189?208.11
