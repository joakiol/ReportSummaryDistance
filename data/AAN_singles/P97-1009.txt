Using Syntactic Dependency as Local Context to Resolve WordSense AmbiguityDekang LinDepar tment  of  Computer  ScienceUn ivers i ty  of Man i tobaWinn ipeg ,  Man i toba ,  Canada R3T  2N2l indek@cs .umani toba .caAbst rac tMost previous corpus-based algorithms dis-ambiguate a word with a classifier trainedfrom previous usages of the same word.Separate classifiers have to be trained fordifferent words.
We present an algorithmthat uses the same knowledge sources todisambiguate different words.
The algo-rithm does not require a sense-tagged cor-pus and exploits the fact that two differentwords are likely to have similar meanings ifthey occur in identical ocal contexts.1 In t roduct ionGiven a word, its context and its possible meanings,the problem of word sense disambiguation (WSD) isto determine the meaning of the word in that con-text.
WSD is useful in many natural anguage tasks,such as choosing the correct word in machine trans-lation and coreference r solution.In several recent proposals (Hearst, 1991; Bruceand Wiebe, 1994; Leacock, Towwell, and Voorhees,1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky,1994), statistical and machine learning techniqueswere used to extract classifiers from hand-taggedcorpus.
Yarowsky (Yarowsky, 1995) proposed anunsupervised method that used heuristics to obtainseed classifications and expanded the results to theother parts of the corpus, thus avoided the need tohand-annotate any examples.Most previous corpus-based WSD algorithms de-termine the meanings of polysemous words by ex-ploiting their local contexts.
A basic intuition thatunderlies those algorithms is the following:(i) Two occurrences of the same word haveidentical meanings if they have similar localcontexts.In other words, most previous corpus-based WSD al-gorithms learn to disambiguate a polysemous wordfrom previous usages of the same word.
This has sev-eral undesirable consequences.
Firstly, a word mustoccur thousands of times before a good classifier canbe learned.
In Yarowsky's experiment (Yarowsky,1995), an average of 3936 examples were used todisambiguate between two senses.
In Ng and Lee'sexperiment, 192,800 occurrences of 191 words wereused as training examples.
There are thousands ofpolysemous words, e.g., there are 11,562 polysemousnouns in WordNet.
For every polysemous word tooccur thousands of times each, the corpus must con-tain billions of words.
Secondly, learning to disam-biguate a word from the previous usages of the sameword means that whatever was learned for one wordis not used on other words, which obviously missedgenerality in natural anguages.
Thirdly, these algo-rithms cannot deal with words for which classifiershave not been learned.In this paper, we present a WSD algorithm thatrelies on a different intuition:(2) Two different words are likely to have similarmeanings if they occur in identical ocalcontexts.Consider the sentence:(3) The new facility will employ 500 of theexisting 600 employeesThe word "facility" has 5 possible meanings inWordNet 1.5 (Miller, 1990): (a) installation, (b)proficiency/technique, (c) adeptness, (d) readiness,(e) toilet/bathroom.
To disambiguate the word, weconsider other words that appeared in an identicallocal context as "facility" in (3).
Table 1 is a listof words that have also been used as the subject of"employ" in a 25-million-word Wall Street Journalcorpus.
The "freq" column are the number of timesthese words were used as the subject of "employ".64Table 1: Subjects of "employ" with highest likelihood ratioword freq logA word freq logAbRG 64 50.4plant 14 31.0company 27 28.6operation 8 23.0industry 9 14.6firm 8 13.5pirate 2 12.1unit 9 9.32shift 3 8.48postal service 2 7.73machine 3 6.56corporation 3 6.47manufacturer 3 6.21insurance company 2 6.06aerospace 2 5.81memory device 1 5.79department 3 5.55foreign office 1 5.41enterprise 2 5.39pilot 2 5.37*ORG includes all proper names recognized as organizationsThe logA column are their likelihood ratios (Dun-ning, 1993).
The meaning of "facility" in (3) canbe determined by choosing one of its 5 senses thatis most similar 1 to the meanings of words in Table1.
This way, a polysemous word is disambiguatedwith past usages of other words.
Whether or not itappears in the corpus is irrelevant.Our approach offers several advantages:?
The same knowledge sources are used for allwords, as opposed to using a separate classifierfor each individual word.?
It requires a much smaller corpus that needs notbe sense-tagged.?
It is able to deal with words that are infrequentor do not even appear in the corpus.?
The same mechanism can also be used to inferthe semantic ategories of unknown words.The required resources of the algorithm includethe following: (a) an untagged text corpus, (b) abroad-coverage parser, (c) a concept hierarchy, suchas the WordNet (Miller, 1990) or Roget's Thesaurus,and (d) a similarity measure between concepts.In the next section, we introduce our definition oflocal contexts and the database of local contexts.
Adescription of the disambiguation algorithm is pre-sented in Section 3.
Section 4 discusses the evalua-tion results.2 Loca l  ContextPsychological experiments how that humans areable to resolve word sense ambiguities given a narrowwindow of surrounding words (Choueka and Lusig-nan, 1985).
Most WSD algorithms take as input?
to be defined in Section 3.1a polysemous word and its local context.
Differentsystems have different definitions of local contexts.In (Leacock, Towwell, and Voorhees, 1996), the lo-cal context of a word is an unordered set of words inthe sentence containing the word and the precedingsentence.
In (Ng and Lee.
1996), a local context of aword consists of an ordered sequence of 6 surround-ing part-of-speech tags, its morphological features,and a set of collocations.In our approach, a local context of a word is de-fined in terms of the syntactic dependencies betweenthe word and other words in the same sentence.A dependency relationship (Hudson, 1984;Mel'~uk, 1987) is an asymmetric binary relation-ship between a word called head (or governor, par-ent), and another word called modi f ier  (or depen-dent, daughter).
Dependency grammars representsentence structures as a set of dependency relation-ships.
Normally the dependency relationships forma tree that connects all the words in a sentence.
Anexample dependency structure is shown in (4).
(4)spec subj /-'~ //the boy chased a brown dogThe local context of a word W is a triple thatcorresponds to a dependency relationship in whichW is the head or the modifier:(type word position)where type is the type of the dependency relation-ship, such as subj (subject), adjn (adjunct), compl(first complement), etc.
; word is the word related toW via the dependency relationship; and pos i t ioncan either be head or rood.
The pos i t ion  indicateswhether word is the head or the modifier in depen-65dency relation.
Since a word may be involved in sev-eral dependency relationships, each occurrence of aword may have multiple local contexts.The local contexts of the two nouns "boy" and"dog" in (4) are as follows (the dependency relationsbetween ouns and their determiners are ignored):(5)Word Local Contextsboy (subj chase head)dog (adjn brown rood) (compl chase head)Using a broad coverage parser to parse a corpus,we construct a Local Context Database.
An en-try in the database is a pair:(6) (tc, C(tc))where Ic is a local context and C(lc) is a set of (wordf requency l ikel ihood)-tr iples.
Each triple speci-fies how often word occurred in lc and the likelihoodratio of lc and word.
The likelihood ratio is obtainedby treating word and Ic as a bigram and computedwith the formula in (Dunning, 1993).
The databaseentry corresponding to Table 1 is as follows:C(/c)  -- ((ORG 64 50.4) (p lant  14 31.0).
.
.
.
.
.
(p i lo t  2 5.37))3 The  ApproachThe polysemous words in the input text are disam-biguated in the following steps:Step A. Parse the input text and extract local con-texts of each word.
Let LCw denote the set oflocal contexts of all occurrences of w in the in-put text.S tep B.
Search the local context database and findwords that appeared in an identical ocal con-text as w. They are called selectors of w:Selectorsw = (\[JlceLC,~ C(Ic) ) - {w}.S tep  C. Select a sense s of w that maximizes thesimilarity between w and Selectors~.S tep  D. The sense s is assigned to all occurrencesof w in the input text.
This implements the"one sense per discourse" heuristic advocatedin (Gale, Church, and Yarowsky, 1992).Step C. needs further explanation.
In the next sub-section, we define the similarity between two wordsenses (or concepts).
We then explain how the simi-larity between a word and its selectors i  maximized.3.1 Similarity between Two ConceptsThere have been several proposed measures for sim-ilarity between two concepts (Lee, Kim, and Lee,1989; Kada et al, 1989; Resnik, 1995b; Wu andPalmer, 1994).
All of those similarity measuresare defined directly by a formula.
We use insteadan information-theoretic definition of similarity thatcan be derived from the following assumptions:Assumpt ion  1: The commonality between A andB is measured byI(common(A, B))where common(A, B) is a proposition that states thecommonalities between A and B; I(s) is the amountof information contained in the proposition s.Assumpt ion  2: The differences between A and Bis measured byI ( describe( A, B) ) - I ( common( A, B ) )where describe(A, B) is a proposition that describeswhat A and B are.Assumpt ion  3: The similarity between A and B,sire(A, B), is a function of their commonality anddifferences.
That is,sire(A, B) = f( I (common(d, B)), I(describe(A, B)))Whedomainof f (x,y)  is {(x,y)lx > O,y > O,y > x}.Assumpt ion  4: Similarity is independent of theunit used in the information measure.According to Information Theory (Cover andThomas, 1991), I(s) = -logbP(S), where P(s) isthe probability of s and b is the unit.
When b = 2,I(s) is the number of bits needed to encode s. Sincelog~,, Assumption 4 means that the func- l ogbx  = logb, b ,tion f must satisfy the following condition:Vc > O, f(x, y) = f(cz, cy)Assumpt ion  5: Similarity is additive with respectto commonality.If common(A,B) consists of two independentparts, then the sim(A,B) is the sum of the simi-larities computed when each part of the commonal-ity is considered.
In other words: f (x l  + x2,y) =f (x l ,y )  + f(x2,y).A corollary of Assumption 5 is that Vy, f(0, y) =f (x  + O,y) - f (x ,y )  = O, which means that whenthere is no commonality between A and B, theirsimilarity is 0, no matter how different they are.For example, the similarity between "depth-firstsearch" and "leather sofa" is neither higher nor lowerthan the similarity between "rectangle" and "inter-est rate".66Assumpt ion  6: The similarity between a pair ofidentical objects is 1.When A and B are identical, knowning theircommonalities means knowing what they are, i.e.,I ( comrnon(.4, B ) ) = I ( describe( A.
B ) ) .
Therefore,the function f must have the following property:vz , / ( z ,  z) = 1.Assumpt ion  7: The function f (x,y)  is continu-ous.S imi lar i ty  Theorem:  The similarity between Aand B is measured by the ratio between the amountof information eededto state the commonality of Aand B and the information eeded to fully describewhat A and B are:sirn( A.
B) = logP(common( A, B) )logP( describe(.4, B) )Proof."
To prove the theorem, we need to showf ( z ,y )  = ~.
Since f(z,V) = f (~, l )  (due to As-sumption 4), we only need to show that when ~ is arational number f (z ,  y) = -~.
The result can be gen- yeralized to all real numbers because f is continuousand for any real number, there are rational numbersthat are infinitely close to it.Suppose m and n are positive integers.f (nz ,  y) = f ( (n  - 1)z, V) + f(z,  V) = nf(z,  V)(due to Assumption 5).
Thus.
f (z ,  y) = ?f(nx, y).Substituting ~ for x in this equation:f(z,v)Since z is rational, there exist m and n such that~- -- --nu Therefore, Y m"m yQ.E.D.For example.
Figure 1 is a fragment of the Word-Net.
The nodes are concepts (or synsets as they arecalled in the WordNet).
The links represent IS-Arelationships.
The number attached to a node C isthe probability P(C) that a randomly selected nounrefers to an instance of C. The probabilities areestimated by the frequency of concepts in SemCor(Miller et al, 1994), a sense-tagged subset of theBrown corpus.If x is a Hill and y is a Coast, the commonalitybetween x and y is that "z is a GeoForm and yis a GeoForm".
The information contained in this0.0001130.0000189entity 0.395inanima\[e-object 0.167/natural-~bject 0.0163/ ,eyi a, 000,70natural-?levation shire 0.0000836hill coast 0.0000216Figure 1: A fragment of WordNetstatement is -2 x logP(GeoForm).
The similaritybetween the concepts Hill and Coast is:2 x logP(GeoForm) sim(HiU, Coast) = = 0.59 logP(Hill) + logP(Coast)Generally speaking,2xlogP(N i Ci )(7) $irlz(C, C') "- iogP(C)+logP(C,)where P(fqi Ci) is the probability of that an objectbelongs to all the maximally specific super classes(Cis) of both C and C'.3.2 Disambiguation by MaximizingSimilarityWe now provide the details of Step C in our algo-rithm.
The input to this step consists of a polyse-mous word W0 and its selectors {l,I,'l, I, V2 .
.
.
.
.
IVy}.The word Wi has ni senses: {sa, .
.
.
,  sin, }.Step C.I :  Construct a similarity matrix (8).
Therows and columns represent word senses.
Thematrix is divided into (k + 1) x (k + 1) blocks.The blocks on the diagonal are all 0s.
The el-ements in block Sij are the similarity measuresbetween the senses of Wi and the senses of II~.Similarity measures lower than a threshold 0areconsidered to be noise and are ignored.
In ourexperiments, 0 = 0.2 was used.sire(sit.
Sjm) if i ?
j andSij(l ,m) = sim(sa.
Sjm) >__ O0 otherwise67(8)80180n 081181~ 18kl8kn~801 ?
- .
80no$10Sk08kl...Skn~SokS~koStep  C.2: Let A be the set of polysemous words in{Wo,...,wk):A = {Witn~ > 1}Step C.3: Find a sense of words in ,4 that gets thehighest total support from other words.
Callthis sense si,,~,t,,~, :ksi.,a,l.,~ = argmaxs, ~ support(sit, Wj)j=0where sit is a word sense such that W/E A and1 6 \[1, n/\] and support(su,Wj) is the supportsa gets from Wj:support(sil, Wj) = max Sij(l,m)mE\[1 ,n j \ ]Step C.4: The sense of Wi~,,~ is chosen to be8i~.~lm,a,.
Remove Wi,.,,,, from A.A ( A -  {W/.,., }Step C.5: Modify the similarity matrix to removethe similarity values between other senses ofW/~,  and senses of other words.
For all l, j ,m, such that l E \[1,ni.~.,\] and l ~ lmaz andj # imax and m E \[1, nj\]:Si.~o~j (/, m) e---- 0S tep  C.6: Repeat from Step C.3 unless im,~z = O.3.3 Walk  Through ExamplesLet's consider again the word "facility" in (3).
Ithas two local contexts: subject of "employ" (subjemploy head) and modifiee of "new" (adjn newrood).
Table 1 lists words that appeared in the firstlocal context.
Table 2 lists words that appeared inthe second local context.
Only words with top-20likelihood ratio were used in our experiments.The two groups of words are merged and used asthe selectors of "facility".
The words "facility" has5 senses in the WordNet.Table 2: Modifiees of "new" with the highest likeli-hood ratiosword freq logA word freq logApost 432 952.9issue 805 902.8product 675 888.6rule 459 875.8law 356 541.5technology 237 382.7generation 150 323.2model 207 319.3job 260 269.2system 318 251.8bonds 223 245.4capital 178 241.8order 228 236.5version 158 223.7position 236 207.3high 152 201.2contract 279 198.1bill 208 194.9venture 123 193.7program 283 183.81. something created to provide a particular ser-vice;2. proficiency, technique;3. adeptness, deftness, quickness;4. readiness, effortlessness;5. toilet, lavatory.Senses 1 and 5 are subclasses of artifact.
Senses 2and 3 are kinds of state.
Sense 4 is a kind of ab-straction.
Many of the selectors in Tables 1 andTable 2 have artifact senses, such as "post", "prod-uct", "system", "unit", "memory device", "ma-chine", "plant", "model", "program", etc.
There-fore, Senses 1 and 5 of "facility" received muchmore support, 5.37 and 2.42 respectively, than othersenses.
Sense 1 is selected.Consider another example that involves an un-known proper name:(9) DreamLand employed 20 programmers.We treat unknown proper nouns as a polysemousword which could refer to a person, an organization,or a location.
Since "DreamLand" is the subject of"employed", its meaning is determined by maximiz-ing the similarity between one of {person, organiza-tion, locaton} and the words in Table 1.
Since Table1 contains many "organization" words, the supportfor the "organization" sense is nmch higher than theothers.4 Eva luat ionWe used a subset of the SemCor (Miller et al, 1994)to evaluate our algorithm.684.1 Eva luat ion  Cr i ter iaGeneral-purpose l xical resources, such as Word-Net, Longman Dictionary of Contemporary English(LDOCE), and Roget's Thesaurus, strive to achievecompleteness.
They often make subtle distinctionsbetween word senses.
As a result, when the WSDtask is defined as choosing a sense out of a list ofsenses in a general-purpose lexical resource, even hu-mans may frequently disagree with one another onwhat the correct sense should be.The subtle distinctions between different wordsenses are often unnecessary.
Therefore, we relaxedthe correctness criterion.
A selected sense 8answeris correct if it is "similar enough" to the sense tagskeu in SemCor.
We experimented with three in-terpretations of "similar enough".
The strictest in-terpretation is sim(sanswer,Ske~)=l, which is trueonly when 8answer~Skey.
The most relaxed inter-pretation is sim(s~nsw~, Skey) >0, which is true if8answer and 8key are  the descendents of the sametop-level concepts in WordNet (e.g., entity, group,location, etc.).
A compromise between these two issim(Sans~er, Skew) >_ 0.27, where 0.27 is the averagesimilarity of 50,000 randomly generated pairs (w, w')in which w and w ~ belong to the same Roget's cate-gory.We use three words "duty", "interest" and "line"as examples to provide a rough idea about whatsirn( s~nswer, Skew) >_ 0.27 means.The word "duty" has three senses in WordNet 1.5.The similarity between the three senses are all below0.27, although the similarity between Senses 1 (re-sponsibility) and 2 (assignment, chore) is very close(0.26) to the threshold.The word "interest" has 8 senses.
Senses 1 (sake,benefit) and 7 (interestingness) are merged.
2 Senses3 (fixed charge for borrowing money), 4 (a right orlegal share of something), and 5 (financial interestin something) are merged.
The word "interest" isreduced to a 5-way ambiguous word.
The otherthree senses are 2 (curiosity), 6 (interest group) and8 (pastime, hobby).The word "line" has 27 senses.
The similaritythreshold 0.27 reduces the number of senses to 14.The reduced senses are?
Senses 1, 5, 17 and 24: something that is com-municated between people or groups.1: a mark that is long relative to its width5: a linear string of words expressing someidea')The similarities between senses of the same word arecomputed uring scoring.
We do not actually change theWordNet hierarchy17: a mark indicating positions or bounds ofthe playing area24: as in "drop me a line when you get there"?
Senses 2, 3, 9, 14, 18: group2: a formation of people or things beside oneanother3: a formation of people or things one afteranother9: a connected series of events or actions ordevelopments14: the descendants of one individual18: common carrier?
Sense 4: a single frequency (or very narrowband) of radiation in a spectrum?
Senses 6 and 25: cognitive process6: line of reasoning25: a conceptual separation or demarcation?
Senses 7, 15, and 26: instrumentation7: electrical cable15: telephone line26: assembly line?
Senses 8 and 10: shape8: a length (straight or curved) withoutbreadth or thickness10: wrinkle, furrow, crease, crinkle, seam, line?
Senses 11 and 16: any road or path affordingpassage from one place to another;11: pipeline16: railway?
Sense 12: location, a spatial ocation defined bya real or imaginary unidimensional extent;?
Senses 13 and 27: human action13: acting in conformity27: occupation, line of work;?
Sense 19: something long and thin and flexible?
Sense 20: product line, line of products?
Sense 21: space for one line of print (one col-umn wide and 1/14 inch deep) used to measureadvertising?
Sense 22: credit line, line of credit?
Sense 23: a succession of notes forming a dis-tinctived sequencewhere each group is a reduced sense and the numbersare original WordNet sense numbers.694.2 Resul tsWe used a 25-million-word Wall Street Journal cor-pus (part of LDC/DCI 3 CDROM) to construct helocal context database.
The text was parsed in126 hours on a SPARC-Ultra 1/140 with 96MBof memory.
We then extracted from the parsetrees 8,665,362 dependency relationships in whichthe head or the modifier is a noun.
We then fil-tered out (lc, word) pairs with a likelihood ratiolower than 5 (an arbitrary threshold).
The resultingdatabase contains 354,670 local contexts with a to-tal of 1,067,451 words in them (Table 1 is countedas one local context with 20 words in it).Since the local context database is constructedfrom WSJ corpus which are mostly business news,we only used the "press reportage" part of Sem-Cor which consists of 7 files with about 2000 wordseach.
Furthermore, we only applied our algorithmto nouns.
Table 3 shows the results on 2,832 polyse-mous nouns in SemCor.
This number also includesproper nouns that do not contain simple markers(e.g., Mr., Inc.) to indicate its category.
Such aproper noun is treated as a 3-way ambiguous word:person, organization, or location.
We also showedas a baseline the performance of the simple strategyof always choosing the first sense of a word in theWordNet.
Since the WordNet senses are ordered ac-cording to their frequency in SemCor, choosing thefirst sense is roughly the same as choosing the sensewith highest prior probability, except that we arenot using all the files in SemCor.It can be seen from Table 3 that our algorithmperformed slightly worse than the baseline whenthe strictest correctness criterion is used.
However,when the condition is relaxed, its performance gainis much lager than the baseline.
This means thatwhen the algorithm makes mistakes, the mistakestend to be close to the correct answer.5 D iscuss ion5.1 Re la ted  WorkThe Step C in Section 3.2 is similar to Resnik's noungroup disambiguation (Resnik, 1995a), although hedid not address the question of the creation of noungroups.The earlier work on WSD that is most similar toours is (Li, Szpakowicz, and Matwin, 1995).
Theyproposed a set of heuristic rules that are based onthe idea that objects of the same or similar verbs aresimilar.3http://www.ldc.upenn.edu/5.2 Weak  ContextsOur algorithm treats all local contexts equally inits decision-making.
However, some local contextshardly provide any constraint on the meaning of aword.
For example, the object of "get" can practi-cally be anything.
This type of contexts hould befiltered out or discounted in decision-making.5.3 Id iomat ic  UsagesOur assumption that similar words appear in iden-tical context does not always hold.
For example,(10) ... the condition in which the hear t  beatsbetween 150 and 200 beats a minuteThe most frequent subjects of "beat" (according toour local context database) are the following:(11) PER, badge, bidder, bunch, challenger,democrat, Dewey, grass, mummification, pimp,police, return, semi.
and soldier.where PER refers to proper names recognized as per-sons.
None of these is similar to the "body part"meaning of "heart".
In fact, "heart" is the only bodypart that beats.6 Conc lus ionWe have presented a new algorithm for word sensedisambiguation.
Unlike most previous corpus-based WSD algorithm where separate classifiers aretrained for different words, we use the same lo-cal context database and a concept hierarchy asthe knowledge sources for disambiguating all words.This allows our algorithm to deal with infrequentwords or unknown proper nouns.Unnecessarily subtle distinction between wordsenses is a well-known problem for evaluating WSDalgorithms with general-purpose l xical resources.Our use of similarity measure to relax the correct-ness criterion provides a possible solution to thisproblem.AcknowledgementThis research as also been partially supported byNSERC Research Grant 0GP121338 and by the In-stitute for Robotics and Intelligent Systems.Re ferencesBruce, Rebecca and Janyce Wiebe.
1994.
Word-sense disambiguation using decomposable models.In Proceedings of the 32nd Annual Meeting o/theAssociations/or Computational Linguistics, pages139-145, Las Cruces, New Mexico.70Table 3: Performance on polysemous nouns in 7 SemCor filescorrectness criterion our algorithm first sense in WordNetsim(Sanswer, Skey) > 0 73.6% 67.2%sim(sanswe~,Skey) >_ 0.27 68.5% 64.2%sim(Sanswer, Skey) = 1 56.1% 58.9%Choueka, Y. and S. Lusignan.
1985.
Disambigua-tion by short contexts.
Computer and the Hu-manities, 19:147-157.Cover, Thomas M. and Joy A. Thomas.
1991.
El-ements of information theory.
Wiley series intelecommunications.
Wiley, New York.Dunning, Ted.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Computa-tional Linguistics, 19(1):61-74, March.Gale, W., K. Church, and D. Yarowsky.
1992.
Amethod for disambiguating word senses in a largecorpus.
Computers and the Humannities, 26:415-439.Hearst, Marti.
1991. noun homograph disambigua-tion using local context in large text corpora.
InConference on Research and Development in In-formation Retrieval ACM/SIGIR, pages 36-47,Pittsburgh, PA.Hudson, Richard.
1984.
Word Grammar.
BasilBlackwell Publishers Limited., Oxford, England.Leacock, Claudia, Goeffrey Towwell, and Ellen M.Voorhees.
1996.
Towards building contextual rep-resentations of word senses using statistical mod-els.
In Corpus Processing for Lexical Acquisition.The MIT Press, chapter 6, pages 97-113.Lee, Joon Ho, Myoung Ho Kim, and Yoon Joon Lee.1989.
Information retrieval based on conceptualdistance in is-a hierarchies.
Journal of Documen-tation, 49(2):188-207, June.Li, Xiaobin, Stan Szpakowicz, and Stan Matwin.1995.
A wordnet-based algorithm for word sensedisambiguation.
In Proceedings of IJCAI-95,pages 1368-1374, Montreal, Canada, August.Mel'~uk, Igor A.
1987.
Dependency syntax: theoryand practice.
State University of New York Press,Albany.Miller, George A.
1990.
WordNet: An on-line lexi-cal database.
International Journal of Lexicogra-phy, 3(4):235-312.Miller, George A., Martin Chodorow, Shari Landes,Claudia Leacock, and robert G. Thomas.
1994.Using a semantic oncordance for sense identifi-cation.
In Proceedings of the ARPA Human Lan-guage Technology Workshop.Ng, Hwee Tow and Hian Beng Lee.
1996.
Integrat-ing multiple knowledge sources to disambiguateword sense: An examplar-based approach.
In Pro-ceedings of 34th Annual Meeting of the Associa-tion for Computational Linguistics, pages 40-47,Santa Cruz, California.Rada, Roy, Hafedh Mili, Ellen Bicknell, and MariaBlettner.
1989.
Development and applicationof a metric on semantic nets.
IEEE Transactionon Systems, Man, and Cybernetics, 19(1):17-30,February.Resnik, Philip.
1995a.
Disambiguating oun group-ings with respect o wordnet senses.
In ThirdWorkshop on Very Large Corpora.
Association forComputational Linguistics.Resnik, Philip.
1995b.
Using information contentto evaluate semantic similarity in a taxonomy.In Proceedings of IJCAI-95, pages 448-453, Mon-treal, Canada, August.Wu, Zhibiao and Martha Palmer.
1994.
Verb se-mantics and lexical selection.
In Proceedings ofthe 32nd Annual Meeting of the Associations forComputational Linguistics, pages 133-138, LasCruces, New Mexico.Yarowsky, David.
1992.
Word-sense disambigua-tion using statistical models of Roget's cate-gories trained on large corpora.
In Proceedingsof COLING-92, Nantes, France.Yarowsky, David.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restora-tion in spanish and french.
In Proceedings of 32ndAnnual Meeting of the Association for Computa-tional Linguistics, pages 88-95, Las Cruces, NM,June.Yarowsky, David.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of 33rd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 189-196, Cambridge, Massachusetts, June.71
