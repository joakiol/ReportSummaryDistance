Relational Learning of Pattern-Match Rules forInformation ExtractionMary  E la ine  Ca l i f f  and  Raymond J .
MooneyDepar tment  of Computer  SciencesUnivers i ty  of Texas at Aust inAust in,  TX  78712{mecaliff ,mooney}@cs.
utexas, eduAbst ractInformation extraction systems processnatural language documents and locatea specific set of relevant items.
Giventhe recent success of empirical or corpus-based approaches in other areas of natu-ral language processing, machine learninghas the potential to significantly aid thedevelopment of these knowledge-intensivesystems.
This paper presents a system,RAPmrt, that takes pairs of documents andfilled templates and induces pattern-matchrules that directly extract fillers for theslots in the template.
The learning al-gorithm incorporates techniques from sev-eral inductive logic programming systemsand learns unbounded patterns that in-clude constraints on the words and part-of-speech tags surrounding the filler.
En-couraging results are presented on learn-ing to extract information from com-puter job postings from the newsgroupmisc.
jobs.
offered.1 In t roduct ionAn increasing amount of information is available inthe form of electronic documents.
The need to in-telligently process uch texts makes information ex-traction (IE), the task of locating specific pieces ofdata from a natural anguage document, a particu-larly useful sub-area of natural anguage processing(NLP).
In recognition of their significance, IE sys-tems have been the focus of DARPA's MUC program(Lehnert and Sundheim, 1991).
Unfortunately, IEsystems are difficult and time-consuming to buildand the resulting systems generally contain highlydomain-specific components, making them difficultto port to new domains.Recently, several researchers have begun to ap-ply learning methods to the construction of IE sys-tems (McCarthy and Lehnert, 1995, Soderland etah, 1995, Soderland et al, 1996, Riloff, 1993, Riloff,1996, Kim and Moldovan, 1995, Huffman, 1996).Several symbolic and statistical methods have beenemployed, but learning is generally used to constructonly part of a larger IE system.
Our system, RAPIER(Robust Automated Production of Information Ex-traction Rules), learns rules for the complete IEtask.
The resulting rules extract he desired itemsdirectly from documents without prior parsing orsubsequent processing.
Using only a corpus of doc-uments paired with filled templates, RAPIER learnsunbounded Eliza-like patterns (Weizenbaum, 1966)that utilize limited syntactic information, such asthe output of a part-of-speech tagger.
Induced pat-terns can also easily incorporate semantic lass infor-mation, such as that provided by WordNet (Milleret al, 1993).
The learning algorithm was inspiredby several Inductive Logic Programming (ILP) sys-tems and primarily consists of a specific-to-general(bottom-up) search for patterns that characterizeslot-fillers and their surrounding context.The remainder of the paper is organized as follows.Section 2 presents background material on IE and re-lational earning.
Section 3 describes RAPIEK's rulerepresentation and learning algorithm.
Section 4presents and analyzes results obtained on extractinginformation from messages posted to the newsgroupmist.jobs.offered.
Section 5 discusses relatedwork in applying learning to IE, Section 6 suggestsareas for future research, and Section 7 presents ourconclusions.2 Background2.1 In fo rmat ion  Ext rac t ionIn information extraction, the data to be extractedfrom a natural anguage text is given by a templatespecifying a list of slots to be filled.
The slot fillersCaliff 8J Mooney 9 Relational LearningMary Elaine Califf and Raymond J. Mooney (1997) Relat ional  Learning of Pattern-Match Rules for Infor-mation Extract ion.
In T.M.
Ellison (ed.)
CoNLLPT: Computational Natural Language Learning, ACL pp 9-15.
@ 1997 Association for Computational LinguisticsPosting from NewsgroupTelecommunications.
SOLARIS SystemsAdministrator.
38-44K.
Immediate needLeading telecommunications firm in needof an energetic individual to fill thefollowing position in the Atlantaoffice:SOLARIS SYSTEMS ADMINISTRATORSalary: 38-44K with full benefitsLocation: Atlanta Georgia, norelocation assistance providedFilled Templatecomputer_science_jobtitle: SOLARIS Systems Administratorsalary: 38-44Kstate: Georgiacity: Atlantaplatform: SOLARISarea :  telecommunicationsFigure 1: Sample Message and Filled Templatemay be either one of a set of specified values orstrings taken directly from the document.
For ex-ample, Figure 1 shows part of a job posting, and thecorresponding slots of the filled computer-science jobtemplate.IE can be useful in a variety of domains.
The var-ious MUC's have focused on domains uch as LatinAmerican terrorism, joint ventures, rnicroelectron-ics, and company management changes.
Others haveused IE to track medical patient records (Soderlandet al, 1995) or company mergers (Huffman, 1996).A general task considered in this paper is extractinginformation from postings to USENET newsgroups,such as job announcements.
Our overall goal is toextract a database from all the messages in a news-group and then use learned query parsers (Zelle andMooney, 1996) to answer natural language questionssuch as "What jobs are available in Austin for C++programmers with only one year of experience?
".Numerous other Internet applications are possible,such as extracting information from product webpages for a shopping agent (Doorenbos, Etzioni, andWeld, 1997).2.2 Relat ional  LearningMost empirical natural-language research as em-ployed statistical techniques that base decisions onvery limited contexts, or symbolic techniques suchas decision trees that require the developer to spec-ify a manageable, finite set of features for use inmaking decisions.
Inductive logic programming andother relational learning methods (Birnbaum andCollins, 1991) allow induction over structured exam-ples that can include first-order logical predicatesand functions and unbounded ata structures uchas lists, strings, and trees.
Detailed experimen-tal comparisons of ILP and feature-based inductionhave demonstrated the advantages ofrelational rep-resentations in two language related tasks, text cat-egorization (Cohen, 1995) and generating the pasttense of an English verb (Mooney and Califf, 1995).While RAPIEa is not strictly an ILP system, its rela-tional earning algorithm was inspired by ideas fromthe following ILP systems.GOLEM (Muggleton and Feng, 1992) is a bottom-up (specific to general) ILP algorithm based on theconstruction ofrelative least-general generalizations,rlggs (Plotkin, 1970).
The idea of least-general gen-eralizations (LGGs) is, given two items (in ILP, twoclauses), finding the least general item that coversthe original pair.
This is usually a fairly simple com-putation.
Rlggs are the LGGs relative to a set ofbackground relations.
Because of the difficulties in-troduced by non-finite rlggs, background predicatesmust be defined extensionally.
The algorithm op-erates by randomly selecting several pairs of posi-tive examples and computing the determinate rlggsof each pair.
Determinacy constrains the clause tohave for each example no more than one possiblevalid substitution for each variable in the body of theclause.
The resulting clause with the greatest cover-age of positive xamples i selected, and that clauseis further generalized by computing the rlggs of theselected clause with new randomly chosen positiveexamples.
The generalization process stops whenthe coverage of the best clause no longer increases.The CHILLIN (Zelle and Mooney, 1994) systemcombines top-down (general to specific) and bottom-up ILP techniques.
The algorithm starts with a mostspecific definition (the set of positive examples) andintroduces generalizations which make the definitionmore compact.
Generalizations are created by se-lecting pairs of clauses in the definition and com-puting LGGs.
If the resulting clause covers negativeexamples, it is specialized by adding antecedent li -erals in a top-down fashion.
The search for new liter-als is carried out in a hill-climbing fashion, using aninformation gain metric for evaluating literals.
Thisis similar to the search employed by FOIL (Quin-lan, 1990).
In cases where a correct clause cannotbe learned with the existing background relations,CHILLIN attempts o construct new predicates whichwill distinguish the covered negative xamples fromthe covered positives.
At each step, a number ofpossible generalizations are considered; the one pro-ducing the greatest compaction of the theory is im-Califf 8J Mooney 10 Relational Learningplemented, and the process repeats.
CHILLIN usesthe notion of empirical subsumption, which meansthat as new, more general clauses are added, all ofthe clauses which are not needed to prove positiveexamples are removed from the definition.PROGOL (Muggleton, 1995) also combinesbottom-up and top-down search.
Using mode decla-rations provided for both the background predicatesand the predicate being learned, it constructs amost specific clause for a random seed example.
Themode declarations specify for each argument of eachpredicate both the argument's type and whetherit should be a constant, a variable bound beforethe predicate is called, or a variable bound by thepredicate.
Given this most specific clause, PROGOLemploys a A*-like search through the set of clausescontaining up to k literals from that clause in orderto find the simplest consistent generalization to addto the definition.
Advantages of PROGOL are thatthe constraints on the search make it fairly efficient,especially on some types of tasks for which top-downapproaches are particularly inefficient, and that itssearch is guaranteed to find the simplest consistentgeneralization if such a clause exists with no morethan k literals.
The primary problems with thesystem are its need for the mode declarations andthe fact that too small a k may prevent PROGOLfrom learning correct clauses while too large a kmay allow the search to explode.3 RAPIER System3.1 Ru le  Representat ionI:LAPIER's rule representation uses patterns thatmake use of limited syntactic and semantic informa-tion, using freely available, robust knowledge sourcessuch as a part-of-speech tagger and a lexicon with se-mantic classes, such as the hypernym links in Word-Net (Miller et al, 1993).
The initial implementationdoes not use a parser, primarily because of the dif-ficulty of producing a robust parser for unrestrictedtext and because simpler patterns of the type we pro-pose can represent useful extraction rules for at leastsome domains.
The extraction rules are indexed bytemplate name and slot name and consist of threeparts: 1) a pre-filler pattern that must match thetext immediately preceding the filler, 2) a patternthat must match the actual slot filler, and 3) a post-filler pattern that must match the text immediatelyfollowing the filler.
Each pattern is a sequence (pos-sibly of length zero in the case of pre- and post-fillerpatterns) of pattern items or pattern lists.
A patternitem matches exactly one word or symbol from thedocument that meets the item's constraints.
A pat-Pre-filler Pattern: Filler Pattern: Post-filler Pattern:1) word: leading 1) list: len: 2 1) word: \[firm, company\]tags: Inn, nns\]Figure 2: A Rule Extracting an Area Filler from theExample Documenttern list specifies a maximum length N and matches0 to N words or symbols from the document hateach must match the list's constraints.
Possible con-straints are: a list of words, one of which must matchthe document item; a list of part-of-speech (POS)tags, one of which must match the document item'sPOS tag; a list of semantic classes, one of whichmust be a class that the document item belongs to.Figure 2 shows a rule created by hand that extractsthe area filler from the example document in fig-ure reftemplate.
This rule assumes that the docu-ment has been tagged with the POS tagger of (Brill,1994).3.2 The  Learn ing  A lgor i thmAs noted above, RAPIER is inspired by ILP meth-ods, and primarily consists of a specific to gen-eral (bottom-up) search.
First, for each slot, most-specific patterns are created for each example, speci-fying word and tag for the filler and its complete con-text.
Thus, the pre-filler pattern contains an itemfor each word from the beginning of the document tothe word immediately preceding the filler with con-straints on the item consisting of the word and itsassigned POS tag.
Likewise, the filler pattern hasone item from each word in the filler, and the post-filler pattern has one item for each word from theend of the filler to the end of the document.Given this maximally specific rule-base, R~APIER.attempts to compress and generalize the rules foreach slot.
New rules are created by selecting twoexisting rules and creating a generalization.
Theaim is to make small generalization steps, coveringmore positive examples without generating supriousfillers, so a standard approach would be to generatethe least general generalization (LGG) of the pairof rules.
However, in this particular epresentationwhich allows for unconstrained disjunction, the LGGmay be overly specific.
Therefore, in cases where theLGG of two constraints i their disjunction, we wantto create two generalizations: one would be the dis-junction and the other the removal of the constraint.Thus, we often want to consider multiple generaliza-tion of a pair of items.
This, combined with the factthat patterns are of varying length, making the num-ber of possible generalizations of two long patternsextremely large, makes the computational cost ofCaliff ~ Mooney 11 Relational LearningFor each slot, S in the template being learnedSlotRules = most specific rules from documents for Swhile compression has failed fewer than lira timesrandomly select 2 rules, R1 and R2, from Sfind the set L of generalizations of the fillers of R1and R2create rules from L, evaluate, and initializeRulesListlet n -- 0while best rule in RuleList produces puriousfillers and the weighted information valueof the best rule is improvingincrement nspecialize ach rule in RuleList with general-izations of the last n items of thepre-filler patterns of R1 and R2 andadd specializations to RuleListspecialize ach rule in RuleList with general-izations of the first n item of thepost-filler patterns of R1 and R2 andadd specializations of RuleListif best rule in RuleList produces only valid fillersAdd it to SlotRules and remove mpiricallysubsumed rulesFigure 3: RAPIER Algorithm for Inducing IE Rulesproducing all interesting eneralizations of two com-plete rules prohibitive.
But, while we do not wantto arbitrarily limit the length of a pre-filler or post-filler pattern, it is likely that the important parts ofthe pattern will be close to the filler.
Therefore, westart by computing the generalizations of the fillerpatterns of the two rules and create rules from thosegeneralizations.
We maintain a list of the best nrules created and specialize the rules under consid-eration by adding pieces of the generalizations of thepre- and post-filler patterns of the two seed rules,working outward from the fillers.
The rules are or-dered using an information value metric (Quinlan,1990) weighted by the size of the rule (preferringsmaller rules).
When the best rule under consider-ation produces no negative xamples, specializationceases; that rule is added to the rule base, and allrules empirically subsumed by it are removed.
Spe-cialization will be abandoned if the value of the bestrule does not improve across k specialization itera-tions.
Compression of the rule base for each slot isabandoned when the number of successive iterationsof the compression algorithm which fail to producea compressing rule exceed either a pre-defined limitor the number of rules for that slot.
An outline ofthe algorithm appears in Figure 3 where RuleList isa prioritized list of no more than Beam- Width rules.The search is somewhat similar to a beam search inthat a limited number of rules is kept for considera-tion, but all rules in RuleList are expanded at eachiteration, rather than only the best.As an example of the creation of a new rule, con-sider generalizing the rules based on the phrases "lo-cated in Atlanta, Georgia."
and "offices in KansasCity, Missouri."
The rules created from thesephrases for the city slot would bePre-fdler Pattern: Filler Pattern: Post-filler Pattern:1) word: located 1) word: atlanta 1) word: ,tag: vbn tag: nnp tag: ,2) word: in 2) word: georgiatag: in tag: nnp3) word: .tag: .andPre-filler Pattern: Filler Pattern: Post-filler Pattern:1) word: offices 1) word: kansas 1) word: ,tag: nns tag: imp tag: ,2) word: in 2) word: city 2) word: missouritag: in tag: imp tag: nnp3) word: .tag: .The fillers are generalized to produce two possiblerules with empty pre-filler and post-filler patterns.Because one filler has two items and the other onlyone, they generalize to a list of no more than twowords.
The word constraints generalize to either adisjunction of all the words or no constraint.
The tagconstraints on all of the items are the same, so theLGG's tag constraints are also the same.
Since thethree words do not belong to a single semantic lassin the lexicon, the semantics remain unconstrained.The fillers produced are:Pre-filler Pattern: Filler Pattern: Post-filler Pattern:1) list: len: 2word: \[atlanta, kansas, city\]tag: nnpandPre-filler Pattern: Filler Pattern: Post-filler Pattern:1) list: len: 2tag: nnpEither of these rules is likely to cover spurious exam-ples, so we add pre-filler and post-filler LGGs.
Theitems produced from the "in" 's and the commas areidentical and, therefore, unchanged.
Assuming thatour lexicon contains a semantic lass for states, gen-eralizing the state names produces a semantic on-straint o f that  class along with a tag constraint nnpand either no word constraint or the disjunction ofthe two states.
Thus, a final best rule would be:Pre-filler Pattern: Filler Pattern: Post-filler Pattern:1) word: in 1) list: len: 2 1) word: ,tag: in tag: nnp tag: ,2) tag: nnpsemantic: state4 EvaluationThe task we have chosen for initial tests of RAPIERis to extract information from computer-related jobCaliff ~ Mooney 12 Relational LearningtO0/ Preel~on -e--/ Rel~all1o 2o ~o ,'o io ooTraining ExamplesFigure 4: Performance on job postingspostings that could be used to create a databaseof available jobs.
The computer-related job post-ing template contains 17 slots, including informa-tion about the employer, the location, the salary,and job requirements.
Several of the slots, such asthe languages and platforms used, can take multiplevalues.
The current results do not employ semanticcategories, only words and the results of Brill's POStagger.The results presented here use a data set of 100documents paired with filled templates.
We dida ten-fold cross-validation, and also ran tests withsmaller subsets of the training examples for each testset in order to produce learning curves.
We use threemeasures: precision, the percentage of slot fillersproduced which are correct; recall, the percentageof slot fillers in the correct emplates which are pro-duced by the system; and an F-measure, which isthe average of the recall and the precision.Figure 4 shows the learning curves generated.At 90 training examples, the average precision was83.7% and the average recall was 53.1%.
These num-bers look quite promising when compared to themeasured performance of other information extrac-tion systems on various domains.
This performanceis comparable to that of CRYSTAL on a medical do-main task (Soderland et al, 1996), and better thanthat of AuTOSLOG and AUTOSLOG-TS on part ofthe MUC4 terrorism task (Riloff, 1996).
It also com-pares favorably with the typical system performanceon the MUC tasks (ARPA, 1992, ARPA, 1993).
Allof these comparisons are only general, since the tasksare different, but they do indicate that RAPIER is do-ing relatively well.
The relatively high precision is anespecially positive result, because it is highly likelythat recall will continue to improve as the numberof training examples increases.The rules RAPIER, learns are of several differenttypes.
Some are fairly simple memorizations ofwords or phrases that consistently appear in par-ticular slots: these include things like programminglanguages and operating systems.
Others learn thecontext of the filler, usually also constraining theparts of speech of the filler: for example, a rule forthe language slot where the prefix is constrained to"familiarity with", the suffix is "programming" andthe filler is a list of up to three items which must beproper nouns or symbols.5 Re la ted  WorkPrevious researchers have generally applied machinelearning only to parts of the IE task and their sys-tems have typically required more human interactionthan just providing texts with filled templates.
RE-SOLVE uses decision trees to handle coreference deci-sions for an IE system and requires annotated coref-erence examples (McCarthy and Lehnert, 1995).CRYSTAL USeS a form of clustering to create a dictio-nary of extraction patterns by generalizing patternsidentified in the text by an expert (Soderland et al,1995, Soderland et al, 1996).
AUTOSLOG createsa dictionary of extraction patterns by specializing aset of general syntactic patterns (Riloff, 1993, Riloff,1996).
It assumes that an expert will later examinethe patterns it produces.
PALKA learns extractionpatterns relying on a concept hierarchy to guide gen-eralization and specialization (Kim and Moldovan,1995).
AUTOSLOG, CRYSTAL, and PALKA all relyon prior sentence analysis to identify syntactic ele-ments and their relationships, and their output re--quires further processing to produce the final filledtemplates.
LIEP also learns IE patterns (Huffman,1996).
Line's primary limitations are that it also re-quires a sentence analyzer to identify noun groups,verbs, subjects, etc.
; it makes no real use of semanticinformation; it assumes that all information it needsis between two entities it identifies as "interesting";and it has been applied to only one domain in whichthe texts are quite short (1-3 sentences).6 Future  ResearchCurrently, RAPIER, assumes lot values are stringstaken directly from the document; however, MUCtemplates also include slots whose values are takenfrom a pre-specified set.
We plan to extend the sys-tem to learn rules for such slots.
Also, the currentsystem attempts to extract he same set of slots fromevery document.
RAPIER must be extended to learnpatterns that first categorize the text to determinewhich set of slots, if any, should be extracted from aCaliff 8J Mooney 13 Relational Learninggiven document.
Finally, the same pattern learningalgorithm ay prove applicable to other natural lan-guage processing tasks such as identifying the senseof an ambiguous word based on its surrounding con-text.7 ConclusionThe ability to extract desired pieces of informationfrom natural anguage texts is an important askwith a growing number of potential applications.Tasks requiring locating specific data in newsgroupmessages or web pages are particularly promisingapplications.
Manually constructing such informa-tion extraction systems i a laborious task; however,learning methods have the potential to help auto-mate the development process.
The RAPIER systemdescribed in this paper uses relational learning toconstruct unbounded pattern-match rules for infor-mation extraction given only a database of texts andfilled templates.
The learned patterns employ lim-ited syntactic and semantic information to identifypotential slot fillers and their surrounding context.Results on extracting information from newsgroupjobs postings have shown that for one realistic ap-plication, fairly accurate rules can be learned fromrelatively small sets of examples.
Future researchwill hopefully demonstrate that similar techiqueswill prove useful in a wide variety of interesting ap-plications.8 AcknowledgementsThis research was supported by a fellowship fromAT&T awarded to the first author and by the Na-tional Science Foundation under grant IRI-9310819.Re ferencesARPA, editor.
1992.
Proceedings of the FourthDARPA Message Understanding Evaluationand Conference, San Mateo, CA.
MorganKaufman.ARPA, editor.
1993.
Proceedings of the FifthDARPA Message Understanding Evaluationand Conference, San Mateo, CA.
MorganKaufman.Birnbaum, L. A. and G. C. Collins, editors.
1991.Proceedings of the Eighth International Work-shop on Machine Learning: Part VI LearningRelations, Evanston, IL, June.Brill, Eric.
1994.
Some advances inrule-based part ofspeech tagging.
In Proceedings of the TwelfthNational Conference on Artificial Intelligence,pages 722-727.Cohen, W. W. 1995.
Text categorization a d rela-tional learning.
In Proceedings of the TwelfthInternational Conference on Machine Learn-ing, pages 124-132, San Francisco, CA.
Mor-gan Kaufman.Doorenbos, R. B., O. Etzioni, and D. S. Weld.1997.
A scalable comparison-shopping agentfor the world-wide web.
In Proceedings ofthe First International Conference on Au-tonomous Agents.Huffman, S. B.
1996.
Learning information extrac-tion patterns from examples.
In S. Wermter,E.
Riloff, and G. Scheler, editors, Connec-tionist, Statistical, and Symbolic Approachesto Learning for Natural Language Processing.Springer, Berlin, pages 246-260.Kim, Jun-Tae and Dan I. Moldovan.
1995.
Acquisi-tion of linguistic patterns for knowledge-basedinformation extraction.
IEEE Transactionson Knowledge and DataEngineering, 7(5):713-724, October.Lehnert, Wendy and Beth Sundheim.
1991.
A per-formance valuation of text-analysis technolo-gies.
AI Magazine, 12(3):81-94.McCarthy, J. and W. Lehnert.
1995.
Using decisiontrees for coreference resolution.
In Proceedingsof the Fourteenth International Joint Confer-ence on Artificial Intelligence, pages 1050-1055.Miller, G., R. Beckwith, C. Fellbaum, D. Gross, andK.
Miller.
1993.
Introduction to WordNet:An on-line lexical database.
Available by ftpto clarity.princeton.edu.Mooney, R. J. and M. E. Califf.
1995.
Induction offirst-order decision lists: Results on learningthe past tense of English verbs.
Journal ofArtificial Intelligence Research, 3:1-24.Muggleton, S. and C. Feng.
1992.
Efficient inductionof logic programs.
In S. Muggleton, editor, In-ductive Logic Programming.
Academic Press,New York, pages 281-297.Muggleton, Steve.
1995.
Inverse entailment andProgol.
New Generation Computing Journal,13:245-286.Califf 8J Mooney 14 Relational LearningPlotkin, G. D. 1970.
A note on inductive general-ization.
In B. Meltzer and D. Michie, editors,Machine Intelligence (Vol.
5).
Elsevier North-Holland, New York.Quinlan, J.R. 1990.
Learning logical definitions fromrelations.
Machine Learning, 5(3):239-266.Riloff, E. 1993.
Automatically constructing a dictio-nary for information extraction tasks.
In Pro-ceedings of the Eleventh National Conferenceon Artificial Intelligence, pages 811-816.Riloff, Ellen.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proceed-ings of the Thirteenth National Conference onArtificial Intelligence, pages 1044-1049.Soderland, Stephen, D. Fisher, J. Aseltine, andW.
Lehnert.
1995.
Crystal: Inducing a con-ceptual dictionary.
In Proceedings of the Four-teenth International Joint Conference on Ar-tificial Intelligence, pages 1314-1319.Soderland, Stephen, David Fisher, Jonathan Asel-tine, and Wendy Lehnert.
1996.
Issues ininductive learning of domain-specific text ex-traction rules.
In Stefan Wermter, Ellen Riloff,and Gabriele Scheller, editors, Connectionist,Statistical, and Symbolic Approaches to Learn-ing for Natural Language Processing, LectureNotes in Artificial Intelligence.
Springer, pages290-301.Weizenbaum, J.
1966.
EL IZA-  A computer pro-gram for the study of natural language com-munications between men and machines.
Com-munications of the Association for ComputingMachinery, 9:36-45.Zelle, J. M. and R. J. Mooney.
1994.
Combiningtop-down and bottom-up methods in induc-tive logic programming.
In Proceedings of theEleventh International Conference on MachineLearning, pages 343-351, New Brunswick, N J,July.Zelle, J. M. and R. J. Mooney.
1996.
Learningto parse database queries using inductive logicprogramming.
In Proceedings of the ThirteenthNational Conference on Artificial Intelligence,Portland, OR, August.Califf 8J Mooney 15 Relational Learning
