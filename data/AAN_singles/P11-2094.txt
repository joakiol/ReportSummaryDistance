Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534?539,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsNonparametric Bayesian Machine Transliteration with SynchronousAdaptor GrammarsYun Huang1,2 Min Zhang1 Chew Lim Tan2huangyun@comp.nus.edu.sg mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg1Human Language Department 2Department of Computer ScienceInstitute for Infocomm Research National University of Singapore1 Fusionopolis Way, Singapore 13 Computing Drive, SingaporeAbstractMachine transliteration is defined as auto-matic phonetic translation of names acrosslanguages.
In this paper, we propose syn-chronous adaptor grammar, a novel nonpara-metric Bayesian learning approach, for ma-chine transliteration.
This model providesa general framework without heuristic or re-striction to automatically learn syllable equiv-alents between languages.
The proposedmodel outperforms the state-of-the-art EM-based model in the English to Chinese translit-eration task.1 IntroductionProper names are one source of OOV words in manyNLP tasks, such as machine translation and cross-lingual information retrieval.
They are often trans-lated through transliteration, i.e.
translation by pre-serving how words sound in both languages.
Ingeneral, machine transliteration is often modelledas monotonic machine translation (Rama and Gali,2009; Finch and Sumita, 2009; Finch and Sumita,2010), the joint source-channel models (Li et al,2004; Yang et al, 2009), or the sequential label-ing problems (Reddy and Waxmonsky, 2009; Ab-dul Hamid and Darwish, 2010).Syllable equivalents acquisition is a critical phasefor all these models.
Traditional learning approachesaim to maximize the likelihood of training databy the Expectation-Maximization (EM) algorithm.However, the EM algorithm may over-fit the trainingdata by memorizing the whole training instances.
Toavoid this problem, some approaches restrict that asingle character in one language could be alignedto many characters of the other, but not vice versa(Li et al, 2004; Yang et al, 2009).
Heuristics areintroduced to obtain many-to-many alignments bycombining two directional one-to-many alignments(Rama and Gali, 2009).
Compared to maximumlikelihood approaches, Bayesian models provide asystemic way to encode knowledges and infer com-pact structures.
They have been successfully appliedto many machine learning tasks (Liu and Gildea,2009; Zhang et al, 2008; Blunsom et al, 2009).Among these models, Adaptor Grammars (AGs)provide a framework for defining nonparametricBayesian models based on PCFGs (Johnson et al,2007).
They introduce additional stochastic pro-cesses (named adaptors) allowing the expansion ofan adapted symbol to depend on the expansion his-tory.
Since many existing models could be viewedas special kinds of PCFG, adaptor grammars givegeneral Bayesian extension to them.
AGs have beenused in various NLP tasks such as topic modeling(Johnson, 2010), perspective modeling (Hardisty etal., 2010), morphology analysis and word segmenta-tion (Johnson and Goldwater, 2009; Johnson, 2008).In this paper, we extend AGs to SynchronousAdaptor Grammars (SAGs), and describe the in-ference algorithm based on the Pitman-Yor process(Pitman and Yor, 1997).
We also describe howtransliteration could be modelled under this formal-ism.
It should be emphasized that the proposedmethod is language independent and heuristic-free.Experiments show the proposed approach outper-forms the strong EM-based baseline in the Englishto Chinese transliteration task.5342 Synchronous Adaptor Grammars2.1 ModelA Pitman-Yor Synchronous Adaptor Grammar(PYSAG) is a tuple G = (Gs,Na,a, b,?
), whereGs = (N ,Ts,Tt,R, S,?)
is a SynchronousContext-Free Grammar (SCFG) (Chiang, 2007),N is a set of nonterminal symbols, Ts/Tt aresource/target terminal symbols, R is a set of rewriterules, S ?
N is the start symbol, ?
is the distri-bution of rule probabilities, Na ?
N is the set ofadapted nonterminals, a ?
[0, 1], b ?
0 are vec-tors of discount and concentration parameters bothindexed by adapted nonterminals, and ?
are Dirich-let prior parameters.Algorithm 1 Generative Process1: draw ?A ?
Dir(?A) for all A ?
N2: for each yield pair ?s / t?
do3: SAMPLE(S) .
Sample from root4: return5: function SAMPLE(A) .
For A ?
N6: if A ?
Na then7: return SAMPLESAG(A)8: else9: return SAMPLESCFG(A)10: function SAMPLESCFG(A) .
For A /?
Na11: draw rule r = ??
/ ??
?
Multi(?A)12: tree tB ?SAMPLE(B) for nonterminalB ?
??
?13: return BUILDTREE(r, tB1 , tB2 , .
.
.
)14: function SAMPLESAG(A) .
For A ?
Na15: draw cache index zn+1 ?
P (z|zi<n), whereP (z|zi<n) ={ma+bn+b if zn+1 = m+ 1nk?an+b if zn+1 = k ?
{1, ?
?
?
,m}16: if zn+1 = m+ 1 then .
New entry17: tree t?
SAMPLESCFG(A)18: m?
m+ 1; nm = 1 .
Update counts19: INSERTTOCACHE(CA, t).20: else .
Old entry21: nk ?
nk + 122: tree t?
FINDINCACHE(CA, zn+1)23: return tThe generative process of a synchronous tree setT is described in Algorithm 1.
First, rule probabil-ities are sampled for each nonterminal A ?
N (line1) according to the Dirichlet distribution.
Then syn-chronous trees are generated in the top-down fashionfrom the start symbol S (line 3) for each yield pair.For nonterminals that are not adapted, the grammarexpands it just as the original synchronous grammar(function SAMPLESCFG).
For each adapted non-terminal A ?
Na, the grammar maintains a cacheCA to store previously generated subtrees under A.Let zi be the subtree index in CA, denoting the syn-chronous subtree generated at the ith expansion ofA.
At some particular time, assuming n subtreesrooted at A have been generated with m differenttypes in the cache of A, each of which has been gen-erated for n1, .
.
.
, nm times respectively1 .
Then thegrammar either generates the (n+1)th synchronoussubtree as SCFG (line 17) or chooses an existingsubtree (line 22), according to the conditional prob-ability P (z|zi<n).The above generative process demonstrates ?richget richer?
dynamics, i.e.
previous sampled subtreesunder adapted nonterminals would more likely besampled again in following procedures.
This is suit-able for many learning tasks since they prefer sparsesolutions to avoid the over-fitting problems.
If weintegrate out the adaptors, the joint probability of aparticular sequence of indexes z with cached counts(n1, .
.
.
, nm) under the Pitman-Yor process isPY (z|a, b) =?mk=1(a(k ?
1) + b)?nk?1j=1 (j ?
a)?n?1i=0 (i+ b).
(1)Given synchronous tree set T , the joint probabilityunder the PYSAG isP (T |?,a, b) =?A?NB(?A + fA)B(?A)PY (z(T )|a, b)(2)where fA is the vector containing the number oftimes that rules r ?
RA are used in the T , and Bis the Beta function.2.2 Inference for PYSAGsDirectly drawing samples from Equation (2) isintractable, so we extend the component-wiseMetropolis-Hastings algorithm (Johnson et al,2007) to the synchronous case.
In detail, wedraw sample T ?i from some proposal distributionQ(Ti|yi,T?i)2, then accept the new sampled syn-1Obviously, n =?mk=1 nk.2T?i means the set of sampled trees except the ith one.535chronous tree T ?i with probabilityA(Ti, T ?i ) = min{1, P (T?|?,a, b)Q(Ti|yi,T?i)P (T |?,a, b)Q(T ?i |yi,T?i)}.
(3)In theory, Q could be any distribution if it neverassigns zero probability.
For efficiency reason, wechoose the probabilistic SCFG as the proposal dis-tribution.
We pre-parse the training instances3 be-fore inference and save the structure of synchronousparsing forests.
During the inference, we onlychange rule probabilities in parsing forests withoutchanging the forest structures.
The probability ofrule r ?
RA in Q is estimated by relative frequency?r = [fr]?i?r??RA[fr?
]?i, where RA is the set of rulesrooted at A, and [fr]?i is the number of times thatrule r is used in the tree set T?i.
We use the sam-pling algorithm described in (Blunsom and Osborne,2008) to draw a synchronous tree from the parsingforest according to the proposal Q.Following (Johnson and Goldwater, 2009), we putan uninformative Beta(1,1) prior on a and a ?vague?Gamma(10, 0.1) prior on b to model the uncertaintyof hyperparameters.3 Machine Transliteration3.1 GrammarsFor machine transliteration, we design the followinggrammar to learn syllable mappings4:Name ?
?Syl / Syl?+Syl ?
?NECs / NECs?Syl ?
?NECs SECs / NECs SECs?Syl ?
?NECs TECs / NECs TECs?NECs ?
?NEC / NEC?+SECs ?
?SEC / SEC?+TECs ?
?TEC / TEC?+NEC ?
?si / tj?SEC ?
??
/ tj?TEC ?
?si / ?
?3We implement the CKY-like bottom up parsing algorithmdescribed in (Wu, 1997).
The complexity is O(|s|3|t|3).4Similar to (Johnson, 2008), the adapted nonterminal are un-derlined.
Similarly, we also use rules in the regular expressionstyle X?
?A / A?+ to denote the following three rules:X ?
?As / As?As ?
?A / A?As ?
?A As / A As?where the adapted nonterminal Syl is designed tocapture the syllable equivalents between two lan-guages, and the nonterminal NEC, SEC and TEC cap-ture the character pairs with no empty character,empty source and empty target respectively.
Notethat this grammar restricts the leftmost characters onboth sides must be aligned one-by-one.
Since ourgoal is to learn the syllable equivalents, we are notinterested in the subtree tree inside the syllables.
Werefer this grammar as syllable grammar.The above grammar could capture inner-syllabledependencies.
However, the selection of the targetcharacters also depend on the context.
For example,the following three instances are found in the train-ing set:?a a b y e / c[ao] '[bi]?
?a a g a a r d / D[ai] ?
[ge] [de]?
?a a l t o / C[a] [er] ?
[tuo]?where the same English syllable ?a a?
are translit-erated to ?c[ao]?, ?D[ai]?
and ?C[a]?
respec-tively, depending on the following syllables.
Tomodel these contextual dependencies, we proposethe hierarchical SAG.
The two-layer word grammaris obtained by adding following rules:Name ?
?Word / Word?+Word ?
?Syl / Syl?+We might further add a new adapted nonterminalCol to learn the word collocations.
The followingrules appear in the collocation grammar:Name ?
?Col / Col?+Col ?
?Word / Word?+Word ?
?Syl / Syl?+Figure 1 gives one synchronous parsing treesunder the collocation grammar of the example?m a x / ?
[mai] ?
[ke] d[si]?.3.2 Translation ModelAfter sampling, we need a translation model totransliterate new source string to target string.Following (Li et al, 2004), we use the n-gramtranslation model to estimate the joint distributionP (s, t) = ?Kk=1 P (pk|pk?11 ), where pk is the kthsyllable pair of the string pair ?s / t?.The first step is to construct joint segmentationlattice for each training instance.
We first generate amerged grammar G?
using collected subtrees underadapted nonterminals, then use synchronous parsing536NameColsColWordsWordSylsSylNECsNECTECsTECSylsSylNECsNECSECsSECm/?
a/?
x/?
?/dFigure 1: An example of parse tree.to obtain probabilities in the segmentation lattice.Specifically, we ?flatten?
the collected subtrees un-der Syl, i.e.
removing internal nodes, to constructnew synchronous rules.
For example, we could gettwo rules from the tree in Figure 1:Syl ?
?m a / ?
?Syl ?
?x / ?d?If multiple subtrees are flattened to the same syn-chronous rule, we sum up the counts of these sub-trees.
For rules with non-adapted nonterminal asparent, we assign the probability as the same of thesampled rule probability, i.e.
let ?
?r = ?r.
Forthe adapted nonterminal Syl, there are two kindsof rules: (1) the rules in the original probabilisticSCFG, and (2) the rules flattened from subtrees.
Weassign the rule probability as?
?r ={ma+bn+b ?
?r if r is original SCFG rulenr?an+b if r is flatten from subtree(4)where a and b are the parameters associated withSyl, m is the number of types of different rules flat-ten from subtrees, nr is the count of rule r, and n isthe total number of flatten rules.
One may verify thatthe rule probabilities are well normalized.
Basedon this merged grammar G?, we parse the trainingstring pairs, then encode the parsed forest into thelattice.
Figure 2 show a lattice example for the stringpair ?a a l t o / C[a] [er] ?[tuo]?.
Thetransition probabilities in the lattice are the ?inside?probabilities of corresponding Syl node in the pars-ing forest.starta/Caa/Caal/Caalto/C??
a/C ??
aa/C ??
al/ ??
l/ ??
lto/?
??
to/?
?Figure 2: Lattice example.After building the segmentation lattice, we train3-order language model from the lattice using theSRILM5.
In decoding, given a new source string, weuse the Viterbi algorithm with beam search (Li et al,2004) to find the best transliteration candidate.4 Experiments4.1 Data and SettingsWe conduct experiments on the English-Chinesedata in the ACL Named Entities Workshop (NEWS2009) 6.
Table 1 gives some statistics of the data.
Forevaluation, we report the word accuracy and meanF-score metrics defined in (Li et al, 2009).Train Dev Test# Entry 31,961 2,896 2,896# En Char 218,073 19,755 19,864# Ch Char 101,205 9,160 9,246# Ch Type 370 275 283Table 1: Transliteration data statisticsIn the inference step, we first run sampler throughthe whole training corpus for 10 iterations, then col-lect adapted subtree statistics for every 10 iterations,and finally stop after 20 collections.
After each it-eration, we resample each of hyperparameters fromthe posterior distribution of hyperparameters using aslice sampler (Neal, 2003).4.2 ResultsWe implement the joint source-channel model (Li etal., 2004) as the baseline system, in which the ortho-graphic syllable alignment is automatically derivedby the Expectation-Maximization (EM) algorithm.5http://www.speech.sri.com/projects/srilm/6http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/537Since EM tends to memorize the training instanceas a whole, Li et al (2004) restrict the Chinese sideto be single character in syllable equivalents.
Ourmethod can be viewed as the Bayesian extension ofthe EM-based baseline.
Since PYSAGs could learnaccurate and compact transliteration units, we do notneed the restriction any more.Grammar Dev (%) Test (%)Baseline 67.8/86.9 66.6/85.7Syl 66.6/87.0 66.6/86.6Word 67.1/87.2 67.0/86.7Col 67.2/87.1 66.9/86.7Table 2: Transliteration results, in the format of word ac-curacy / mean F-score.
?Syl?,?Word?
and ?Col?
denotethe syllable, word and collocation grammar respectively.Table 2 presents the results of all experiments.From this table, we draw following conclusions:1.
The best results of our model are 67.1%/87.2%on development set and corresponding67.0%/86.7% on test set, achieved by wordgrammars.
The results on test set outperformthe EM-based baseline system on both wordaccuracy and mean F-score.2.
Comparing grammars of different layers, wefind that the word grammars perform consis-tently better than the syllable grammars.
Thesesupport the assumption that the context infor-mation are helpful to identify syllable equiva-lents.
However, the collocation grammars donot further improve performance.
We guessthe reason is that the instances in transliter-ation are very short, so two-layer grammarsare good enough while the collocations becomevery sparse, which results in unreliable proba-bility estimation.4.3 DiscussionTable 3 shows some examples of learned syllablemappings in the final sampled tree of the syllablegrammar.
We can see that the PYSAGs could findgood syllable mappings from the raw name pairswithout any heuristic or restriction.
In this point ofview, the proposed method is language independent.Specifically, we are interested in the English to-ken ?x?, which is the only one that has two corre-s/d[si]/1669 k/?
[ke]/408 ri/p[li]/342t/A[te]/728 ma/?
[ma]/390 ra/.[la]/339man/?
[man]/703 co/?
[ke]/387 ca/k[ka]/333d/[de]/579 ll/[er]/383 m/0[mu]/323ck/?
[ke]/564 la/.
[la]/382 li/|[li]/314de/[de]/564 tt/A[te]/380 ber/?[bo]/311ro/?
[luo]/531 l/[er]/367 ley/|[li]/310son/?
[sen]/442 ton/?
[dun]/360 na/B[na]/302x/?d[ke si]/40 x/?
[ke]/3 x/d[si]/1Table 3: Examples of learned syllable mappings.
ChinesePinyin are given in the square bracket.
The counts of syl-lable mappings in the final sampled tree are also given.sponding Chinese characters (?
?d[ke si]?).
Ta-ble 3 demonstrates that nearly all these correct map-pings are discovered by PYSAGs.
Note that thesekinds of mapping can not be learned if we restrict theChinese side to be only one character (the heuristicused in (Li et al, 2004)).
We will conduct experi-ments on other language pairs in the future.5 ConclusionThis paper proposes synchronous adaptor gram-mars, a nonparametric Bayesian model, for machinetransliteration.
Based on the sampling, the PYSAGscould automatically discover syllable equivalentswithout any heuristic or restriction.
In this pointof view, the proposed model is language indepen-dent.
The joint source-channel model is then usedfor training and decoding.
Experimental results onthe English-Chinese transliteration task show thatthe proposed method outperforms the strong EM-based baseline system.
We also compare grammarsin different layers and find that the two-layer gram-mars are suitable for the transliteration task.
Weplan to carry out more transliteration experiments onother language pairs in the future.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir helpful comments and suggestions.
We alsothank Zhixiang Ren, Zhenghua Li, and Jun Sun forinsightful discussions.
Special thanks to ProfessorMark Johnson for his open-source codes7.7Available from http://web.science.mq.edu.au/~mjohnson/Software.htm538ReferencesAhmed Abdul Hamid and Kareem Darwish.
2010.
Sim-plified feature set for arabic named entity recognition.In Proceedings of the 2010 Named Entities Workshop,pages 110?115, Uppsala, Sweden, July.Phil Blunsom and Miles Osborne.
2008.
Probabilisticinference for machine translation.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 215?223, Honolulu,Hawaii, October.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 782?790, Sun-tec, Singapore, August.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228,June.Andrew Finch and Eiichiro Sumita.
2009.
Transliter-ation by bidirectional statistical machine translation.In Proceedings of the 2009 Named Entities Workshop:Shared Task on Transliteration (NEWS 2009), pages52?56, Suntec, Singapore, August.Andrew Finch and Eiichiro Sumita.
2010.
A BayesianModel of Bilingual Segmentation for Transliteration.In Proceedings of the 7th International Workshop onSpoken Language Translation (IWSLT), pages 259?266, Paris, France, December.Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik.2010.
Modeling perspective using adaptor grammars.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 284?292, Cambridge, MA, October.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 317?325, Boulder, Colorado, June.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Adaptor grammars: A framework for spec-ifying compositional nonparametric bayesian models.In B. Sch?lkopf, J. Platt, and T. Hoffman, editors, Ad-vances in Neural Information Processing Systems 19,pages 641?648.
Cambridge, MA.Mark Johnson.
2008.
Using adaptor grammars to iden-tify synergies in the unsupervised acquisition of lin-guistic structure.
In Proceedings of ACL-08: HLT,pages 398?406, Columbus, Ohio, June.Mark Johnson.
2010.
Pcfgs, topic models, adaptor gram-mars and learning topical collocations and the struc-ture of proper names.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1148?1157, Uppsala, Sweden, July.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.
InProceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL?04), Main Vol-ume, pages 159?166, Barcelona, Spain, July.Haizhou Li, A Kumaran, Vladimir Pervouchine, and MinZhang.
2009.
Report of news 2009 machine transliter-ation shared task.
In Proceedings of the 2009 NamedEntities Workshop: Shared Task on Transliteration(NEWS 2009), pages 1?18, Suntec, Singapore, August.Ding Liu and Daniel Gildea.
2009.
Bayesian learningof phrasal tree-to-string templates.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 1308?1317, Singapore,August.Radford M. Neal.
2003.
Slice sampling.
Annals ofStatistics, 31(3):705?767.J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Dirichlet distribution derived from a stable subordina-tor.
Annals of Probability, 25:855?900.Taraka Rama and Karthik Gali.
2009.
Modeling ma-chine transliteration as a phrase based statistical ma-chine translation problem.
In Proceedings of the 2009Named Entities Workshop: Shared Task on Translit-eration (NEWS 2009), pages 124?127, Suntec, Singa-pore, August.Sravana Reddy and Sonjia Waxmonsky.
2009.Substring-based transliteration with conditional ran-dom fields.
In Proceedings of the 2009 Named Enti-ties Workshop: Shared Task on Transliteration (NEWS2009), pages 92?95, Suntec, Singapore, August.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403, Septem-ber.Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-ishi, Masanobu Nakamura, and Sadaoki Furui.
2009.Combining a two-step conditional random field modeland a joint source channel model for machine translit-eration.
In Proceedings of the 2009 Named Enti-ties Workshop: Shared Task on Transliteration (NEWS2009), pages 72?75, Suntec, Singapore, August.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InProceedings of ACL-08: HLT, pages 97?105, Colum-bus, Ohio, June.539
