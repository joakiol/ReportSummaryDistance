Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114?121,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsEdinburgh?s Machine Translation Systems for European Language PairsNadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp KoehnSchool of InformaticsUniversity of EdinburghScotland, United Kingdom{dnadir,bhaddow,kheafiel,pkoehn}@inf.ed.ac.ukAbstractWe validated various novel and recentlyproposed methods for statistical machinetranslation on 10 language pairs, usinglarge data resources.
We saw gainsfrom optimizing parameters, training withsparse features, the operation sequencemodel, and domain adaptation techniques.We also report on utilizing a huge lan-guage model trained on 126 billion tokens.The annual machine translation evaluation cam-paign for European languages organized aroundthe ACL Workshop on Statistical Machine Trans-lation offers the opportunity to test recent advance-ments in machine translation in large data condi-tion across several diverse language pairs.Building on our own developments and externalcontributions to the Moses open source toolkit, wecarried out extensive experiments that, by early in-dications, led to a strong showing in the evaluationcampaign.We would like to stress especially two contri-butions: the use of the new operation sequencemodel (Section 3) within Moses, and ?
in a sepa-rate unconstraint track submission ?
the use of ahuge language model trained on 126 billion tokenswith a new training tool (Section 4).1 Initial System DevelopmentWe start with systems (Haddow and Koehn, 2012)that we developed for the 2012 Workshop onStatistical Machine Translation (Callison-Burchet al 2012).
The notable features of these systemsare:?
Moses phrase-based models with mostly de-fault settings?
training on all available parallel data, includ-ing the large UN parallel data, the French-English 109 parallel data and the LDC Giga-word data?
very large tuning set consisting of the test setsfrom 2008-2010, with a total of 7,567 sen-tences per language?
German?English with syntactic pre-reordering (Collins et al 2005), compoundsplitting (Koehn and Knight, 2003) and useof factored representation for a POS targetsequence model (Koehn and Hoang, 2007)?
English?German with morphological targetsequence modelNote that while our final 2012 systems in-cluded subsampling of training data with modifiedMoore-Lewis filtering (Axelrod et al 2011), wedid not use such filtering at the starting point ofour development.
We will report on such filteringin Section 2.Moreover, our system development initiallyused the WMT 2012 data condition, since it tookplace throughout 2012, and we switched to WMT2013 training data at a later stage.
In this sec-tion, we report cased BLEU scores (Papineni et al2001) on newstest2011.1.1 Factored Backoff (German?English)We have consistently used factored models in pastWMT systems for the German?English languagepairs to include POS and morphological target se-quence models.
But we did not use the factoreddecomposition of translation options into multi-ple mapping steps, since this usually lead to muchslower systems with usually worse results.A good place, however, for factored decompo-sition is the handling of rare and unknown sourcewords which have more frequent morphologicalvariants (Koehn and Haddow, 2012a).
Here, weused only factored backoff for unknown words,giving gains in BLEU of +.12 for German?English.1.2 Tuning with k-best MIRAIn preparation for training with sparse features, wemoved away from MERT which is known to fall114apart with many more than a couple of dozen fea-tures.
Instead, we used k-best MIRA (Cherry andFoster, 2012).
For the different language pairs, wesaw improvements in BLEU of -.05 to +.39, with anaverage of +.09.
There was only a minimal changein the length ratio (Table 1)MERT k-best MIRA ?de-en 22.11 (1.010) 22.10 (1.008) ?.01 (+.002)fr-en 30.00 (1.023) 30.11 (1.026) +.11 (?.003)es-en 30.42 (1.021) 30.63 (1.020) +.21 (?.001)cs-en 25.54 (1.022) 25.49 (1.024) ?.05 (?.002)en-de 16.08 (0.995) 16.04 (1.001) ?.04 (?.006)en-fr 29.26 (0.980) 29.65 (0.982) +.39 (?.002)en-es 31.92 (0.985) 31.95 (0.985) +.03 (?.000)en-cs 17.38 (0.967) 17.42 (0.974) +.04 (?.007)avg ?
?
+.09Table 1: Tuning with k-best MIRA instead of MERT(cased BLEU scores with length ratio)1.3 Translation Table Smoothing withKneser-Ney DiscountingPreviously, we smoothed counts for the phrasalconditional probability distributions in the trans-lation model with Good Turing discounting.
Weexplored the use of Kneser-Ney discounting, butresults are mixed (no difference on average, seeTable 2), so we did not pursue this further.Good Turing Kneser Ney ?de-en 22.10 22.15 +.05fr-en 30.11 30.13 +.02es-en 30.63 30.64 +.01cs-en 25.49 25.56 +.07en-de 16.04 15.93 ?.11en-fr 29.65 29.75 +.10en-es 31.95 31.98 +.03en-cs 17.42 17.26 ?.16avg ?
?
?.00Table 2: Translation model smoothing with Kneser-Ney1.4 Sparse FeaturesA significant extension of the Moses system overthe last couple of years was the support for largenumbers of sparse features.
This year, we testedthis capability on our big WMT systems.
First, weused features proposed by Chiang et al(2009):?
phrase pair count bin features (bins 1, 2, 3,4?5, 6?9, 10+)?
target word insertion features?
source word deletion features?
word translation features?
phrase length feature (source, target, both)The lexical features were restricted to the 50 mostfrequent words.
All these features together onlygave minor improvements (Table 3).baseline sparse ?de-en 22.10 22.02 ?.08fr-en 30.11 30.24 +.13es-en 30.63 30.61 ?.02cs-en 25.49 25.49 ?.00en-de 16.04 15.93 ?.09en-fr 29.65 29.81 +.16en-es 31.95 32.02 +.07en-cs 17.42 17.28 ?.14avg ?
?
+.04Table 3: Sparse featuresWe also explored domain features in the sparsefeature framework, in three different variations.Assume that we have three domains, and a phrasepair occurs in domain A 15 times, in domain B 5times, and in domain C never.We compute three types of domain features:?
binary indicator, if phrase-pairs occurs in do-main (example: indA = 1, indB = 1, indC = 0)?
ratio how frequent the phrase pairs occurs indomain (example: ratioA = 1515+5 = .75, ratioB =515+5 = .25, ratioC = 0)?
subset of domains in which phrase pair oc-curs (example: subsetAB = 1, other subsets 0)We tested all three feature types, and foundthe biggest gain with the domain indicator feature(+.11, Table 4).
Note that we define as domain thedifferent corpora (Europarl, etc.).
The number ofdomains ranges from 2 to 9 (see column #d).1#d base.
indicator ratio subsetde-en 2 22.10 22.14 +.04 22.07 ?.03 22.12 +.02fr-en 4 30.11 30.34 +.23 30.29 +.18 30.15 +.04es-en 3 30.63 30.88 +.25 30.64 +.01 30.82 +.19cs-en 9 25.49 25.58 +.09 25.58 +.09 25.46 ?.03en-de 2 16.122 16.14 +.02 15.96 ?.16 16.01 ?.11en-fr 4 29.65 29.75 +.10 29.71 +.05 29.70 +.05en-es 3 31.95 32.06 +.11 32.13 +.18 32.02 +.07en-cs 9 17.42 17.45 +.03 17.35 ?.07 17.44 +.02avg.
- ?
+.11 +.03 +.03Table 4: Sparse domain featuresWhen combining the domain features and theother sparse features, we see roughly additivegains (Table 5).
We use the domain indicator fea-ture and the other sparse features in subsequent ex-periments.1In the final experiments on the 2013 data condition, onedomain (commoncrawl) was added for all language pairs.115baseline indicator ratio subsetde-en 22.10 22.18 +.08 22.10 ?.00 22.16 +.06fr-en 30.11 30.41 +.30 30.49 +.38 30.36 +.25es-en 30.63 30.75 +.12 30.56 ?.07 30.85 +.22cs-en 25.49 25.56 +.07 25.63 +.14 25.43 ?.06en-de 16.12 15.95 ?.17 15.96 ?.16 16.05 ?.07en-fr 29.65 29.96 +.31 29.88 +.23 29.92 +.27en-es 31.95 32.12 +.17 32.16 +.21 32.08 +.23en-cs 17.42 17.38 ?.04 17.35 ?.07 17.40 ?.02avg.
?
+.11 +.09 +.11Table 5: Combining domain and other sparse features1.5 Tuning SettingsGiven the opportunity to explore the parametertuning of models with sparse features across manylanguage pairs, we investigated a number of set-tings.
We expect tuning to work better with moreiterations, longer n-best lists and bigger cube prun-ing pop limits.
Our baseline settings are 10 itera-tions with 100-best lists (accumulating) and a poplimit of 1000 for tuning and 5000 for testing.base 25 it.
25it+1k-best 25it+pop5kde-en 22.18 22.16 ?.02 22.14 ?.04 22.17 ?.01fr-en 30.41 30.40 ?.01 30.44 +.03 30.49 +.08es-en 30.75 30.91 +.16 30.86 +.11 30.81 +.06cs-en 25.56 25.60 +.04 25.64 +.08 25.56 ?.00en-de 15.96 15.99 +.03 16.05 +.09 15.96 ?.00en-fr 29.96 29.90 ?.06 29.95 ?.01 29.92 ?.04en-es 32.12 32.17 +.05 32.11 ?.01 32.19 +.07en-cs 17.38 17.43 +.05 17.50 +.12 17.38 ?.00avg ?
+.03 +.05 +.02Table 6: Tuning settings (number of iterations, size of n-bestlist, and cube pruning pop limit)Results support running tuning for 25 iterationsbut we see no gains for 5000 pops.
There is ev-idence that an n-best list size of 1000 is better intuning but we did not adopt this since these largelists take up a lot of disk space and slow down theMIRA optimization step (Table 6).1.6 Smaller PhrasesGiven the very large corpus sizes (up to a billionwords of parallel data for French?English), thesize of translation model and lexicalized reorder-ing model becomes a challenge.
Hence, we wantto examine if restriction to smaller phrases is fea-sible without loss in translation quality.
Resultsin Table 7 suggest that a maximum phrase lengthof 5 gives almost identical results, and only witha phrase length limit of 4 significant losses occur.We adopted the limit of 5.max 7 max 6 max 5 max 4de-en 22.16 22.03 ?.13 22.05 ?.11 22.17 +.01fr-en 30.40 30.30 ?.10 30.39 ?.01 30.23 ?.17es-en 30.91 30.80 ?.09 30.86 ?.05 30.81 ?.10cs-en 25.60 25.55 ?.05 25.53 ?.07 25.48 ?.12en-de 15.99 15.94 ?.05 15.97 ?.02 16.03 +.04en-fr 29.90 29.97 +.07 29.89 ?.01 29.77 ?.13en-es 32.17 32.13 ?.04 32.27 +.10 31.93 ?.24en-cs 17.43 17.46 +.03 17.41 ?.02 17.41 ?.02avg ?
?.05 ?.03 ?.09Table 7: Maximum phrase length, reduced from baseline1.7 Unpruned Language ModelsPreviously, we trained 5-gram language modelsusing the default settings of the SRILM toolkit interms of singleton pruning.
Thus, training throwsout all singletons n-grams of order 3 and higher.We explored whether unpruned language modelscould give better performance, even if we are onlyable to train 4-gram models due to memory con-straints.
At the time, we were not able to build un-pruned 4-gram language models for English, butfor the other language pairs we did see improve-ments of -.07 to +.13 (Table 8).
We adopted suchmodels for these language pairs.5g pruned 4g unpruned ?en-fr 29.89 29.83 ?.07en-es 32.27 32.34 +.07en-cs 17.41 17.54 +.13Table 8: Language models without singleton pruning1.8 Translations per Input PhraseFinally, we explored one more parameter: the limiton how many translation options are consideredper input phrase.
The default for this setting is 20.However, our experiments (Table 9) show that wecan get better results with a translation table limitof 100, so we adopted this.ttl 20 ttl 30 ttl 50 ttl 100de-en 21.05 +.06 +.09 +.01fr-en 30.39 ?.02 +.05 +.07es-en 30.86 ?.00 ?.03 ?.07cs-en 25.53 +.24 +.13 +.20en-de 15.97 +.03 +.07 +.11en-fr 29.83 +.14 +.19 +.13en-es 32.34 +.08 +.10 +.07en-cs 17.54 ?.05 ?.02 +.01avg ?
+.06 +.07 +.07Table 9: Maximal number translations per input phrase1.9 Other ExperimentsWe explored a number of other settings and fea-tures, but did not observe any gains.116?
Using HMM alignment instead of IBMModel 4 leads to losses of ?.01 to ?.27.?
An earlier check of modified Moore?Lewisfiltering (see also below in Section 3) gavevery inconsistent results.?
Filtering the phrase table with significancefiltering (Johnson et al 2007) leads to lossesof ?.19 to ?.63.?
Throwing out phrase pairs with direct transla-tion probability ?(e?|f?)
of less than 10?5 hasalmost no effect.?
Double-checking the contribution of thesparse lexical features in the final setup, weobserve an average losses of ?.07 when drop-ping these features.?
For the German?English language pairs wesaw some benefits to using sparse lexical fea-tures over POS tags instead of words, so weused this in the final system.1.10 SummaryWe adopted a number of changes that improvedour baseline system by an average of +.30, see Ta-ble 10 for a breakdown.avg.
method+.01 factored backoff+.09 kbest MIRA+.11 sparse features and domain indicator+.03 tuning with 25 iterations?.03 maximum phrase length 5+.02 unpruned 4-gram LM+.07 translation table limit 100+.30 totalTable 10: Summary of impact of changesMinor improvements that we did not adopt wasavoiding reducing maximum phrase length to 5(average +.03) and tuning with 1000-best lists(+.02).The improvements differed significantly by lan-guage pair, as detailed in Table 11, with thebiggest gains for English?French (+.70), no gainfor English?German and no gain for English?German.1.11 New DataThe final experiment of the initial system devel-opment phase was to train the systems on the newdata, adding newstest2011 to the tuning set (now10,068 sentences).
Table 12 reports the gains onnewstest2012 due to added data, indicating veryclearly that valuable new data resources becameavailable this year.baseline improved ?de-en 21.99 22.09 +.10fr-en 30.00 30.46 +.46es-en 30.42 30.79 +.37cs-en 25.54 25.73 +.19en-de 16.08 16.08 ?.00en-fr 29.26 29.96 +.70en-es 31.92 32.41 +.49en-cs 17.38 17.55 +.17Table 11: Overall improvements per language pairWMT 2012 WMT 2013 ?de-en 23.11 24.01 +0.90fr-en 29.25 30.77 +1.52es-en 32.80 33.99 +1.19cs-en 22.53 22.86 +0.33ru-en ?
31.67 ?en-de 16.78 17.95 +1.17en-fr 27.92 28.76 +0.84en-es 33.41 34.00 +0.59en-cs 15.51 15.78 +0.27en-ru ?
23.78 ?Table 12: Training with new data (newstest2012 scores)2 Domain Adaptation TechniquesWe explored two additional domain adaptationtechniques: phrase table interpolation and modi-fied Moore-Lewis filtering.2.1 Phrase Table InterpolationWe experimented with phrase-table interpolationusing perplexity minimisation (Foster et al 2010;Sennrich, 2012).
In particular, we used the im-plementation released with Sennrich (2012) andavailable in Moses, comparing both the naive andmodified interpolation methods from that paper.For each language pair, we took the alignmentscreated from all the data concatenated, built sepa-rate phrase tables from each of the individual cor-pora, and interpolated using each method.
The re-sults are shown in Table 13baseline naive modifiedfr-en 30.77 30.63 ?.14 ?es-en?
33.98 33.83 ?.15 34.03 +.05cs-en?
23.19 22.77 ?.42 23.03 ?.17ru-en 31.67 31.42 ?.25 31.59 ?.08en-fr 28.76 28.88 +.12 ?en-es 34.00 34.07 +.07 34.31 +.31en-cs 15.78 15.88 +.10 15.87 +.09en-ru 23.78 23.84 +.06 23.68 ?.10Table 13: Comparison of phrase-table interpolation (twomethods) with baseline (on newstest2012).
The baselines areas Table 12 except for the starred rows where tuning withPRO was found to be better.
The modified interpolation wasnot possible in fr?en as it uses to much RAM.The results from the phrase-table interpolationare quite mixed, and we only used the technique117for the final system in en-es.
An interpolationbased on PRO has recently been shown (Haddow,2013) to improve on perplexity minimisation issome cases, but the current implementation of thismethod is limited to 2 phrase-tables, so we did notuse it in this evaluation.2.2 Modified Moore-Lewis FilteringIn last year?s evaluation (Koehn and Haddow,2012b) we had some success with modifiedMoore-Lewis filtering (Moore and Lewis, 2010;Axelrod et al 2011) of the training data.
Thisyear we conducted experiments in most of the lan-guage pairs using MML filtering, and also exper-imented using instance weighting (Mansour andNey, 2012) using the (exponential of) the MMLweights.
The results are show in Table 14base MML Inst.
Wt Inst.
Wtline 20% (scale)fr-en 30.77 ?
?
?es-en?
33.98 34.26 +.28 33.85 ?.13 33.98 ?.00cs-en?
23.19 22.62 ?.57 23.17 ?.02 23.13 ?.06ru-en 31.67 31.58 ?.09 31.57 ?.10 31.62 ?.05en-fr 28.67 28.74 +.07 28.81 +.17 28.63 ?.04en-es 34.00 34.07 +.07 34.27 +.27 34.03 +.03en-cs 15.78 15.37 ?.41 15.87 +.09 15.89 +.11en-ru 23.78 22.90 ?.88 23.82 +.05 23.72 ?.06Table 14: Comparison of MML filtering and weighting withbaseline.
The MML uses monolingual news as in-domain,and selects from all training data after alignment.The weight-ing uses the MML weights, optionally downscaled by 10,then exponentiated.
Baselines are as Table 13.As with phrase-table interpolation, MML filter-ing and weighting shows a very mixed picture, andnot the consistent improvements these techniquesoffer on IWSLT data.
In the final systems, we usedMML filtering only for es-en.3 Operation Sequence Model (OSM)We enhanced the phrase segmentation and re-ordering mechanism by integrating OSM: an op-eration sequence N-gram-based translation and re-ordering model (Durrani et al 2011) into theMoses phrase-based decoder.
The model is basedon minimal translation units (MTUs) and Markovchains over sequences of operations.
An opera-tion can be (a) to jointly generate a bi-languageMTU, composed from source and target words, or(b) to perform reordering by inserting gaps and do-ing jumps.Model: Given a bilingual sentence pair <F,E > and its alignment A, we transform it toFigure 1: Bilingual Sentence with Alignmentssequence of operations (o1, o2, .
.
.
, oJ ) and learna Markov model over this sequence as:posm(F,E,A) = p(oJ1 ) =J?j=1p(oj |oj?n+1, ..., oj?1)By coupling reordering with lexical generation,each (translation or reordering) decision condi-tions on n ?
1 previous (translation and reorder-ing) decisions spanning across phrasal boundariesthus overcoming the problematic phrasal indepen-dence assumption in the phrase-based model.
Inthe OSM model, the reordering decisions influ-ence lexical selection and vice versa.
Lexical gen-eration is strongly coupled with reordering thusimproving the overall reordering mechanism.We used the modified version of the OSMmodel (Durrani et al 2013b) that addition-ally handles discontinuous and unaligned targetMTUs3.
We borrow 4 count-based supportive fea-tures, the Gap, Open Gap, Gap-width and Dele-tion penalties from Durrani et al(2011).Training: During training, each bilingual sen-tence pair is deterministically converted to aunique sequence of operations.
Please refer toDurrani et al(2011) for a list of operations andthe conversion algorithm and see Figure 1 and Ta-ble 15 for a sample bilingual sentence pair andits step-wise conversion into a sequence of oper-ation.
A 9-gram Kneser-Ney smoothed operationsequence model is trained with SRILM.Search: Although the OSM model is based onminimal units, phrase-based search on top of OSMmodel was found to be superior to the MTU-baseddecoding in Durrani et al(2013a).
Following thisframework allows us to use OSM model in tandemwith phrase-based models.
We integrated the gen-erative story of the OSM model into the hypothe-sis extension of the phrase-based Moses decoder.Please refer to (Durrani et al 2013b) for details.Results: Table 16 shows case-sensitive BLEUscores on newstest2012 and newstest2013 for fi-3In the original OSM model these are removed from thealignments through a post-processing heuristic which hurts insome language pairs.
See Durrani et al(2013b) for detailedexperiments.118Operation Sequence GenerationGenerate(Ich, I) Ich ?IGenerate Target Only (do) Ich ?I doInsert Gap Ich nicht ?Generate (nicht, not) I do notJump Back (1) Ich gehe ?
nichtGenerate (gehe, go) I do not goGenerate Source Only (ja) Ich gehe ja ?
nichtI do not goJump Forward Ich gehe ja nicht ?I do not goGenerate (zum, to the) .
.
.
gehe ja nicht zum ?.
.
.
not go to theGenerate (haus, house) .
.
.
ja nicht zum haus ?.
.
.
go to the houseTable 15: Step-wise Generation of Figure 1LP Baseline +OSMnewstest 2012 2013 2012 2013de-en 23.85 26.54 24.11 +.26 26.83 +.29fr-en 30.77 31.09 30.96 +.19 31.46 +.37es-en 34.02 30.04 34.51 +.49 30.94 +.90cs-en 22.70 25.70 23.03 +.33 25.79 +.09ru-en 31.87 24.00 32.33 +.46 24.33 +.33en-de 17.95 20.06 18.02 +.07 20.26 +.20en-fr 28.76 30.03 29.36 +.60 30.39 +.36en-es 33.87 29.66 34.44 +.57 30.10 +.44en-cs 15.81 18.35 16.16 +.35 18.62 +.27en-ru 23.75 18.44 24.05 +.30 18.84 +.40Table 16: Results using the OSM Featurenal systems from Section 1 and these systems aug-mented with the operation sequence model.
Themodel gives gains for all language pairs (BLEU+.09 to +.90, average +.37, on newstest2013).4 Huge Language ModelsTo overcome the memory limitations of SRILM,we implemented modified Kneser-Ney (Kneserand Ney, 1995; Chen and Goodman, 1998)smoothing from scratch using disk-based stream-ing algorithms.
This open-source4 tool is de-scribed fully by Heafield et al(2013).
We used itto estimate an unpruned 5?gram language modelon web pages from ClueWeb09.5 The corpus waspreprocessed by removing spam (Cormack et al2011), selecting English documents, splitting sen-tences, deduplicating, tokenizing, and truecasing.Estimation on the remaining 126 billion tokenstook 2.8 days on a single machine with 140 GBRAM (of which 123 GB was used at peak) and sixhard drives in a RAID5 configuration.
Statisticsabout the resulting model are shown in Table 17.4http://kheafield.com/code/5http://lemurproject.org/clueweb09/1 2 3 4 5393m 3,775m 17,629m 39,919m 59,794mTable 17: Counts of unique n-grams (m for millions) for the5 orders in the unconstrained language modelThe large language model was then quantizedto 10 bits and compressed to 643 GB with KenLM(Heafield, 2011), loaded onto a machine with 1TB RAM, and used as an additional feature inunconstrained French?English, Spanish?English,and Czech?English submissions.
This additionallanguage model is the only difference between ourfinal constrained and unconstrained submissions;no additional parallel data was used.
Results areshown in Table 18.
Improvement from large lan-guage models is not a new result (Brants et al2007); the primary contribution is estimating on asingle machine.Constrained Unconstrained ?fr-en 31.46 32.24 +.78es-en 30.59 31.37 +.78cs-en 27.38 28.16 +.78ru-en 24.33 25.14 +.81Table 18: Gain on newstest2013 from the unconstrained lan-guage model.
Our time on shared machines with 1 TB islimited so Russian?English was run after the deadline andGerman?English was not ready in time.5 SummaryTable 19 breaks down the gains over the final sys-tem from Section 1 from using the operation se-quence models (OSM), modified Moore-Lewis fil-tering (MML), fixing a bug with the sparse lex-ical features (Sparse-Lex Bugfix), and instanceweighting (Instance Wt.
), translation model com-bination (TM-Combine), and use of the huge lan-guage model (ClueWeb09 LM).AcknowledgmentsThanks to Miles Osborne for preprocessing the ClueWeb09corpus.
The research leading to these results has re-ceived funding from the European Union SeventhFramework Programme (FP7/2007-2013) under grantagreement 287658 (EU BRIDGE) and grant agreement288487(MosesCore).This work made use of the resourcesprovided by the Edinburgh Compute and Data Facility6.The ECDF is partially supported by the eDIKT initia-tive7.
This work also used the Extreme Science andEngineering Discovery Environment (XSEDE), which issupported by National Science Foundation grant numberOCI-1053575.
Specifically, Stampede was used underallocation TG-CCR110017.6http://www.ecdf.ed.ac.uk/7http://www.edikt.org.uk/119System 2012 2013Spanish-English1.
Baseline 34.02 30.042.
1+OSM 34.51 +.49 30.94 +.903.
1+MML (20%) 34.38 +.36 30.38 +.344.
1+Sparse-Lex Bugfix 34.17 +.15 30.33 +.295.
1+2+3: OSM+MML 34.65 +.63 30.51 +.476.
1+2+3+4 34.68 +.66 30.59 +.557.
6+ClueWeb09 LM 31.37 +1.33English-Spanish1.
Baseline 33.87 29.662.
1+OSM 34.44 +.57 30.10 +.443.
1+TM-Combine 34.31 +44 29.76 +.104.
1+Instance Wt.
34.27 +.40 29.63 ?.035.
1+Sparse-Lex Bugfix 34.20 +.33 29.86 +.206.
1+2+3: OSM+TM-Cmb.
34.63 +.76 30.21 +.557.
1+2+4: OSM+Inst.
Wt.
34.58 +.71 30.11 +.458.
1+2+3+5 34.78 +.91 30.43 +.77Czech-English1.
Baseline 22.70 25.702.
1+OSM 23.03 +.33 25.79 +.093.
1+with PRO 23.19 +.49 26.08 +.384.
1+Sparse-Lex Bugfix 22.86 +.16 25.74 +.045.
1+OSM+PRO 23.42 +.72 26.23 +.536.
1+2+3+4 23.16 +.46 25.94 +.247.
5+ClueWeb09 LM 27.06 +.36English-Czech1.
Baseline 15.85 18.352.
1+OSM 16.16 +.31 18.62 +.27French-English1.
Baseline 30.77 31.092.
1+OSM 30.96 +.19 31.46 +.373.
2+ClueWeb09 LM 32.24 +1.15English-French1.
Baseline 28.76 30.032.
1+OSM 29.36 +.60 30.39 +.363.
1+Sparse-Lex Bugfix 28.97 +.21 30.08 +.054.
1+2+3 29.37 +.61 30.58 +.55German-English1.
Baseline 23.85 26.542.
1+OSM 24.11 +.26 26.83 +.29English-German1.
Baseline 17.95 20.062.
1+OSM 18.02 +.07 20.26 +.20Russian-English1.
Baseline 31.87 24.002.
1+OSM 32.33 +.46 24.33 +.33English-Russian1.
Baseline 23.75 18.442.
1+OSM 24.05 +.40 18.84 +.40Table 19: Summary of methods with BLEU scores on news-test2012 and newstest2013.
Bold systems were submitted,with the ClueWeb09 LM systems submitted in the uncon-straint track.
The German?English and English?GermanOSM systems did not complete in time for the official sub-mission.ReferencesAxelrod, A., He, X., and Gao, J.
(2011).
Domain adaptationvia pseudo in-domain data selection.
In Proceedings of the2011 Conference on Empirical Methods in Natural Lan-guage Processing, pages 355?362, Edinburgh, Scotland,UK.
Association for Computational Linguistics.Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.(2007).
Large language models in machine translation.In Proceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL),pages 858?867.Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut,R., and Specia, L. (2012).
Findings of the 2012 work-shop on statistical machine translation.
In Proceedings ofthe Seventh Workshop on Statistical Machine Translation,pages 10?48, Montreal, Canada.
Association for Compu-tational Linguistics.Chen, S. and Goodman, J.
(1998).
An empirical study ofsmoothing techniques for language modeling.
TechnicalReport TR-10-98, Harvard University.Cherry, C. and Foster, G. (2012).
Batch tuning strategies forstatistical machine translation.
In Proceedings of the 2012Conference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human LanguageTechnologies, pages 427?436, Montre?al, Canada.
Associ-ation for Computational Linguistics.Chiang, D., Knight, K., and Wang, W. (2009).
11,001 newfeatures for statistical machine translation.
In Proceedingsof Human Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associationfor Computational Linguistics, pages 218?226, Boulder,Colorado.
Association for Computational Linguistics.Collins, M., Koehn, P., and Kucerova, I.
(2005).
Clauserestructuring for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL?05), pages 531?540,Ann Arbor, Michigan.
Association for Computational Lin-guistics.Cormack, G. V., Smucker, M. D., and Clarke, C. L. (2011).Efficient and effective spam filtering and re-ranking forlarge web datasets.
Information retrieval, 14(5):441?465.Durrani, N., Fraser, A., and Schmid, H. (2013a).
Model WithMinimal Translation Units, But Decode With Phrases.
InThe 2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: HumanLanguage Technologies, Atlanta, Georgia, USA.
Associ-ation for Computational Linguistics.Durrani, N., Fraser, A., Schmid, H., Hoang, H., and Koehn,P.
(2013b).
Can Markov Models Over Minimal Transla-tion Units Help Phrase-Based SMT?
In Proceedings ofthe 51st Annual Meeting of the Association for Computa-tional Linguistics, Sofia, Bulgaria.
Association for Com-putational Linguistics.Durrani, N., Schmid, H., and Fraser, A.
(2011).
A Joint Se-quence Translation Model with Integrated Reordering.
InProceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human Language Tech-nologies, pages 1045?1054, Portland, Oregon, USA.Foster, G., Goutte, C., and Kuhn, R. (2010).
Discriminativeinstance weighting for domain adaptation in statistical ma-chine translation.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Processing,pages 451?459, Cambridge, MA.
Association for Compu-tational Linguistics.120Haddow, B.
(2013).
Applying pairwise ranked optimisationto improve the interpolation of translation models.
In Pro-ceedings of the 2013 Conference of the North AmericanChapter of the Association for Computational Linguistics:Human Language Technologies, pages 342?347, Atlanta,Georgia.
Association for Computational Linguistics.Haddow, B. and Koehn, P. (2012).
Analysing the effect ofout-of-domain data on smt systems.
In Proceedings ofthe Seventh Workshop on Statistical Machine Translation,pages 175?185, Montreal, Canada.
Association for Com-putational Linguistics.Heafield, K. (2011).
KenLM: Faster and smaller languagemodel queries.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 187?197, Edin-burgh, Scotland.
Association for Computational Linguis-tics.Heafield, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.(2013).
Scalable modified Kneser-Ney language modelestimation.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics, Sofia, Bul-garia.Johnson, H., Martin, J., Foster, G., and Kuhn, R. (2007).Improving translation quality by discarding most of thephrasetable.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learning(EMNLP-CoNLL), pages 967?975.Kneser, R. and Ney, H. (1995).
Improved backing-off form-gram language modeling.
In Proceedings of the IEEEInternational Conference on Acoustics, Speech and SignalProcessing, pages 181?184.Koehn, P. and Haddow, B.
(2012a).
Interpolated backoff forfactored translation models.
In Proceedings of the TenthConference of the Association for Machine Translation inthe Americas (AMTA).Koehn, P. and Haddow, B.
(2012b).
Towards Effective Use ofTraining Data in Statistical Machine Translation.
In Pro-ceedings of the Seventh Workshop on Statistical MachineTranslation, pages 317?321, Montre?al, Canada.
Associa-tion for Computational Linguistics.Koehn, P. and Hoang, H. (2007).
Factored TranslationModels.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learning(EMNLP-CoNLL), pages 868?876, Prague, Czech Repub-lic.
Association for Computational Linguistics.Koehn, P. and Knight, K. (2003).
Empirical methods for com-pound splitting.
In Proceedings of Meeting of the Euro-pean Chapter of the Association of Computational Lin-guistics (EACL).Mansour, S. and Ney, H. (2012).
A Simple and Effec-tive Weighted Phrase Extraction for Machine TranslationAdaptation.
In Proceedings of IWSLT.Moore, R. C. and Lewis, W. (2010).
Intelligent selection oflanguage model training data.
In Proceedings of the ACL2010 Conference Short Papers, pages 220?224, Uppsala,Sweden.
Association for Computational Linguistics.Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2001).BLEU: a method for automatic evaluation of machinetranslation.
Technical Report RC22176(W0109-022),IBM Research Report.Sennrich, R. (2012).
Perplexity minimization for translationmodel domain adaptation in statistical machine transla-tion.
In Proceedings of the 13th Conference of the Eu-ropean Chapter of the Association for Computational Lin-guistics, pages 539?549, Avignon, France.
Association forComputational Linguistics.121
