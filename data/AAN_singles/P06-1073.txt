Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 577?584,Sydney, July 2006. c?2006 Association for Computational LinguisticsMaximum Entropy Based Restoration of Arabic DiacriticsImed Zitouni, Jeffrey S. Sorensen, Ruhi SarikayaIBM T.J. Watson Research Center1101 Kitchawan Rd, Yorktown Heights, NY 10598{izitouni, sorenj, sarikaya}@us.ibm.comAbstractShort vowels and other diacritics are notpart of written Arabic scripts.
Exceptionsare made for important political and reli-gious texts and in scripts for beginning stu-dents of Arabic.
Script without diacriticshave considerable ambiguity because manywords with different diacritic patterns ap-pear identical in a diacritic-less setting.
Wepropose in this paper a maximum entropyapproach for restoring diacritics in a doc-ument.
The approach can easily integrateand make effective use of diverse types ofinformation; the model we propose inte-grates a wide array of lexical, segment-based and part-of-speech tag features.
Thecombination of these feature types leadsto a state-of-the-art diacritization model.Using a publicly available corpus (LDC?sArabic Treebank Part 3), we achieve a di-acritic error rate of 5.1%, a segment errorrate 8.5%, and a word error rate of 17.3%.In case-ending-less setting, we obtain a di-acritic error rate of 2.2%, a segment errorrate 4.0%, and a word error rate of 7.2%.1 IntroductionModern Arabic written texts are composed ofscripts without short vowels and other diacriticmarks.
This often leads to considerable ambigu-ity since several words that have different diacriticpatterns may appear identical in a diacritic-lesssetting.
Educated modern Arabic speakers are ableto accurately restore diacritics in a document.
Thisis based on the context and their knowledge of thegrammar and the lexicon of Arabic.
However, atext without diacritics becomes a source of confu-sion for beginning readers and people with learningdisabilities.
A text without diacritics is also prob-lematic for applications such as text-to-speech orspeech-to-text, where the lack of diacritics addsanother layer of ambiguity when processing thedata.
As an example, full vocalization of text isrequired for text-to-speech applications, where themapping from graphemes to phonemes is simplecompared to languages such as English and French;where there is, in most cases, one-to-one relation-ship.
Also, using data with diacritics shows animprovement in the accuracy of speech-recognitionapplications (Afify et al, 2004).
Currently, text-to-speech, speech-to-text, and other applications usedata where diacritics are placed manually, whichis a tedious and time consuming excercise.
A di-acritization system that restores the diacritics ofscripts, i.e.
supply the full diacritical markings,would be of interest to these applications.
It alsowould greatly benefit nonnative speakers, sufferersof dyslexia and could assist in restoring diacriticsof children?s and poetry books, a task that is cur-rently done manually.We propose in this paper a statistical approachthat restores diacritics in a text document.
Theproposed approach is based on the maximum en-tropy framework where several diverse sources ofinformation are employed.
The model implicitlylearns the correlation between these types of infor-mation and the output diacritics.In the next section, we present the set of diacrit-ics to be restored and the ambiguity we face whenprocessing a non-diacritized text.
Section 3 givesa brief summary of previous related works.
Sec-tion 4 presents our diacritization model; we ex-plain the training and decoding process as well asthe different feature categories employed to restorethe diacritics.
Section 5 describes a clearly definedand replicable split of the LDC?s Arabic TreebankPart 3 corpus, used to built and evaluate the sys-tem, so that the reproduction of the results andfuture comparison can accurately be established.Section 6 presents the experimental results.
Sec-tion 7 reports a comparison of our approach tothe finite state machine modeling technique thatshowed promissing results in (Nelken and Shieber,2005).
Finally, section 8 concludes the paper anddiscusses future directions.2 Arabic DiacriticsThe Arabic alphabet consists of 28 letters that canbe extended to a set of 90 by additional shapes,marks, and vowels (Tayli and Al-Salamah, 1990).The 28 letters represent the consonants and long577vowels such as A, ?
 (both pronounced as /a:/),? (pronounced as /i:/), and ? (pronounced as/u:/).
Long vowels are constructed by combin-ing A, ?
, ?, and ? with the short vowels.
Theshort vowels and certain other phonetic informa-tion such as consonant doubling (shadda) are notrepresented by letters, but by diacritics.
A dia-critic is a short stroke placed above or below theconsonant.
Table 1 shows the complete set of Ara-Diacritic Name Meaning/on ?
PronunciationShort vowels?
fatha /a/?
damma /u/?
kasra /i/Doubled case ending (?tanween?)?
tanween al-fatha /an/?
tanween al-damma /un/? tanween al-kasra /in/Syllabification marks?
shadda consonantdoubling?
sukuun vowelabsenceTable 1: Arabic diacritics on the letter ?
consonant?
?
(pronounced as /t/).bic diacritics.
We split the Arabic diacritics intothree sets: short vowels, doubled case endings, andsyllabification marks.
Short vowels are written assymbols either above or below the letter in textwith diacritics, and dropped all together in textwithout diacritics.
We find three short vowels:?
fatha: it represents the /a/ sound and is anoblique dash over a consonant as in?
(c.f.fourth row of Table 1).?
damma: it represents the /u/ sound and isa loop over a consonant that resembles theshape of a comma (c.f.
fifth row of Table 1).?
kasra: it represents the /i/ sound and is anoblique dash under a consonant (c.f.
sixth rowof Table 1).The doubled case ending diacritics are vowels usedat the end of the words to mark case distinction,which can be considered as a double short vowels;the term ?tanween?
is used to express this phe-nomenon.
Similar to short vowels, there are threedifferent diacritics for tanween: tanween al-fatha,tanween al-damma, and tanween al-kasra.
Theyare placed on the last letter of the word and havethe phonetic effect of placing an ?N?
at the endof the word.
Text with diacritics contains also twosyllabification marks:?
shadda: it is a gemination mark placed abovethe Arabic letters as in?.
It denotes the dou-bling of the consonant.
The shadda is usuallycombined with a short vowel such as in?.?
sukuun: written as a small circle as in?.
It isused to indicate that the letter doesn?t containvowels.Figure 1 shows an Arabic sentence transcribed withand without diacritics.
In modern Arabic, writingscripts without diacritics is the most natural way.Because many words with different vowel patternsmay appear identical in a diacritic-less setting,considerable ambiguity exists at the word level.The word I.
J?, for example, has 21 possible formsthat have valid interpretations when adding dia-critics (Kirchhoff and Vergyri, 2005).
It may havethe interpretation of the verb ?to write?
in I.J?
(pronounced /kataba/).
Also, it can be interpretedas ?books?
in the noun form I.J?
(pronounced /ku-tubun/).
A study made by (Debili et al, 2002)shows that there is an average of 11.6 possible di-acritizations for every non-diacritized word whenanalyzing a text of 23,000 script forms.. ?Q?
Y??
@ ?KQ?
@ I.
J?.
?
Q?Y??
@ ?K Q?
@ I.J?Figure 1: The same Arabic sentence without (up-per row) and with (lower row) diacritics.
The En-glish translation is ?the president wrote the docu-ment.
?Arabic diacritic restoration is a non-trivial task asexpressed in (El-Imam, 2003).
Native speakers ofArabic are able, in most cases, to accurately vo-calize words in text based on their context, thespeaker?s knowledge of the grammar, and the lex-icon of Arabic.
Our goal is to convert knowledgeused by native speakers into features and incor-porate them into a maximum entropy model.
Weassume that the input text does not contain anydiacritics.3 Previous WorkDiacritic restoration has been receiving increas-ing attention and has been the focus of severalstudies.
In (El-Sadany and Hashish, 1988), a rulebased method that uses morphological analyzer for578vowelization was proposed.
Another, rule-basedgrapheme to sound conversion approach was ap-peared in 2003 by Y. El-Imam (El-Imam, 2003).The main drawbacks of these rule based methods isthat it is difficult to maintain the rules up-to-dateand extend them to other Arabic dialects.
Also,new rules are required due to the changing natureof any ?living?
language.More recently, there have been several new stud-ies that use alternative approaches for the diacriti-zation problem.
In (Emam and Fisher, 2004) anexample based hierarchical top-down approach isproposed.
First, the training data is searched hi-erarchically for a matching sentence.
If there isa matching sentence, the whole utterance is used.Otherwise they search for matching phrases, thenwords to restore diacritics.
If there is no match atall, character n-gram models are used to diacritizeeach word in the utterance.In (Vergyri and Kirchhoff, 2004), diacritics inconversational Arabic are restored by combiningmorphological and contextual information with anacoustic signal.
Diacritization is treated as an un-supervised tagging problem where each word istagged as one of the many possible forms providedby the Buckwalter?s morphological analyzer (Buck-walter, 2002).
The Expectation Maximization(EM) algorithm is used to learn the tag sequences.Y.
Gal in (Gal, 2002) used a HMM-based diacriti-zation approach.
This method is a white-spacedelimited word based approach that restores onlyvowels (a subset of all diacritics).Most recently, a weighted finite state machinebased algorithm is proposed (Nelken and Shieber,2005).
This method employs characters and largermorphological units in addition to words.
Amongall the previous studies this one is more sophisti-cated in terms of integrating multiple informationsources and formulating the problem as a searchtask within a unified framework.
This approachalso shows competitive results in terms of accuracywhen compared to previous studies.
In their algo-rithm, a character based generative diacritizationscheme is enabled only for words that do not occurin the training data.
It is not clearly stated in thepaper whether their method predict the diacriticsshedda and sukuun.Even though the methods proposed for diacriticrestoration have been maturing and improving overtime, they are still limited in terms of coverage andaccuracy.
In the approach we present in this paper,we propose to restore the most comprehensive listof the diacritics that are used in any Arabic text.Our method differs from the previous approachesin the way the diacritization problem is formulatedand because multiple information sources are inte-grated.
We view the diacritic restoration problemas sequence classification, where given a sequenceof characters our goal is to assign diacritics to eachcharacter.
Our appoach is based on MaximumEntropy (MaxEnt henceforth) technique (Bergeret al, 1996).
MaxEnt can be used for sequenceclassification, by converting the activation scoresinto probabilities (through the soft-max function,for instance) and using the standard dynamic pro-gramming search algorithm (also known as Viterbisearch).
We find in the literature several otherapproaches of sequence classification such as (Mc-Callum et al, 2000) and (Lafferty et al, 2001).The conditional random fields method presentedin (Lafferty et al, 2001) is essentially a MaxEntmodel over the entire sequence: it differs from theMaxent in that it models the sequence informa-tion, whereas the Maxent makes a decision for eachstate independently of the other states.
The ap-proach presented in (McCallum et al, 2000) com-bines Maxent with Hidden Markov models to allowobservations to be presented as arbitrary overlap-ping features, and define the probability of statesequences given observation sequences.We report in section 7 a comparative study be-tween our approach and the most competitive dia-critic restoration method that uses finite state ma-chine algorithm (Nelken and Shieber, 2005).
TheMaxEnt framework was successfully used to com-bine a diverse collection of information sources andyielded a highly competitive model that achieves a5.1% DER.4 Automatic DiacritizationThe performance of many natural language pro-cessing tasks, such as shallow parsing (Zhang etal., 2002) and named entity recognition (Florianet al, 2004), has been shown to depend on inte-grating many sources of information.
Given thestated focus of integrating many feature types, weselected the MaxEnt classifier.
MaxEnt has theability to integrate arbitrary types of informationand make a classification decision by aggregatingall information available for a given classification.4.1 Maximum Entropy ClassifiersWe formulate the task of restoring diacritics asa classification problem, where we assign to eachcharacter in the text a label (i.e., diacritic).
Be-fore formally describing the method1, we introducesome notations: let Y = {y1, .
.
.
, yn} be the set ofdiacritics to predict or restore, X be the examplespace and F = {0, 1}m be a feature space.
Each ex-ample x ?
X has associated a vector of binary fea-tures f (x) = (f1 (x) , .
.
.
, fm (x)).
In a supervisedframework, like the one we are considering here, wehave access to a set of training examples togetherwith their classifications: {(x1, y1) , .
.
.
, (xk, yk)}.1This is not meant to be an in-depth introductionto the method, but a brief overview to familiarize thereader with them.579The MaxEnt algorithm associates a set of weights(?ij)i=1...nj=1...m with the features, which are estimatedduring the training phase to maximize the likeli-hood of the data (Berger et al, 1996).
Given theseweights, the model computes the probability dis-tribution over labels for a particular example x asfollows:P (y|x) = 1Z(x)m?j=1?fj (x)ij , Z(x) =?i?j?fj (x)ijwhere Z(X ) is a normalization factor.
To esti-mate the optimal ?j values, we train our Max-Ent model using the sequential conditional gener-alized iterative scaling (SCGIS) technique (Good-man, 2002).
While the MaxEnt method can nicelyintegrate multiple feature types seamlessly, in cer-tain cases it is known to overestimate its confidencein especially low-frequency features.
To overcomethis problem, we use the regularization methodbased on adding Gaussian priors as described in(Chen and Rosenfeld, 2000).
After computing theclass probability distribution, the chosen diacriticis the one with the most aposteriori probability.The decoding algorithm, described in section 4.2,performs sequence classification, through dynamicprogramming.4.2 Search to Restore DiacriticsWe are interested in finding the diacritics of allcharacters in a script or a sentence.
These dia-critics have strong interdependencies which can-not be properly modeled if the classification is per-formed independently for each character.
We viewthis problem as sequence classification, as con-trasted with an example-based classification prob-lem: given a sequence of characters in a sentencex1x2 .
.
.
xL, our goal is to assign diacritics (labels)to each character, resulting in a sequence of diacrit-ics y1y2 .
.
.
yL.
We make an assumption that dia-critics can be modeled as a limited order Markovsequence: the diacritic associated with the char-acter i depends only on the diacritics associatedwith the k previous diacritics, where k is usuallyequal to 3.
Given this assumption, and the nota-tion xL1 = x1 .
.
.
xL, the conditional probability ofassigning the diacritic sequence yL1 to the charactersequence xL1 becomesp(yL1 |xL1)=p(y1|xL1)p(y2|xL1 , y1).
.
.
p(yL|xL1 , yL?1L?k+1)(1)and our goal is to find the sequence that maximizesthis conditional probabilityy?L1 = arg maxyL1p(yL1 |xL1)(2)While we restricted the conditioning on the classi-fication tag sequence to the previous k diacritics,we do not impose any restrictions on the condition-ing on the characters ?
the probability is computedusing the entire character sequence xL1 .To obtain the sequence in Equation (2), we createa classification tag lattice (also called trellis), asfollows:?
Let xL1 be the input sequence of character andS = {s1, s2, .
.
.
, sm} be an enumeration of Yk(m = |Y|k) - we will call an element sj a state.Every such state corresponds to the labelingof k successive characters.
We find it usefulto think of an element si as a vector with kelements.
We use the notations si [j] for jthelement of such a vector (the label associatedwith the token xi?k+j+1) and si [j1 .
.
.
j2] forthe sequence of elements between indices j1and j2.?
We conceptually associate every characterxi, i = 1, .
.
.
, L with a copy of S, Si ={si1, .
.
.
, sim}; this set represents all the possi-ble labelings of characters xii?k+1 at the stagewhere xi is examined.?
We then create links from the set Si to theSi+1, for all i = 1 .
.
.
L?
1, with the propertythatw(sij1 , si+1j2)=??
?p(si+1j1 [k] |xL1 , si+1j2 [1..k ?
1])if sij1 [2..k] = si+1j2 [1..k ?
1]0 otherwiseThese weights correspond to probability of atransition from the state sij1 to the state si+1j2 .?
For every character xi, we compute recur-sively2?0 (sj) = 0, j = 1, .
.
.
, k?i (sj) = maxj1=1,...,M?i?1 (sj1 ) + log w(si?1j1 , sij)?i (sj) =arg maxj1=1,...,M?i?1 (sj1 ) + log w(si?1j1 , sij)Intuitively, ?i (sj) represents the log-probability of the most probable path throughthe lattice that ends in state sj after i steps,and ?i (sj) represents the state just before sjon that particular path.?
Having computed the (?i)i values, the algo-rithm for finding the best path, which corre-sponds to the solution of Equation (2) is1.
Identify s?LL = arg maxj=1...L ?L (sj)2.
For i = L ?
1 .
.
.
1, computes?ii = ?i+1(s?i+1i+1)2For convenience, the index i associated with statesij is moved to ?
; the function ?i (sj) is in fact ?(sij).5803.
The solution for Equation (2) is given byy?
={s?11[k], s?22[k], .
.
.
, s?LL [k]}The runtime of the algorithm is ?
(|Y|k ?
L), linearin the size of the sentence L but exponential in thesize of the Markov dependency, k. To reduce thesearch space, we use beam-search.4.3 Features EmployedWithin the MaxEnt framework, any type of fea-tures can be used, enabling the system designer toexperiment with interesting feature types, ratherthan worry about specific feature interactions.
Incontrast, with a rule based system, the system de-signer would have to consider how, for instance,lexical derived information for a particular exam-ple interacts with character context information.That is not to say, ultimately, that rule-based sys-tems are in some way inferior to statistical mod-els ?
they are built using valuable insight whichis hard to obtain from a statistical-model-only ap-proach.
Instead, we are merely suggesting that theoutput of such a rule-based system can be easilyintegrated into the MaxEnt framework as one ofthe input features, most likely leading to improvedperformance.Features employed in our system can be dividedinto three different categories: lexical, segment-based, and part-of-speech tag (POS) features.
Wealso use the previously assigned two diacritics asadditional features.In the following, we briefly describe the differentcategories of features:?
Lexical Features: we include the charac-ter n-gram spanning the curent character xi,both preceding and following it in a win-dow of 7: {xi?3, .
.
.
, xi+3}.
We use the cur-rent word wi and its word context in a win-dow of 5 (forward and backward trigram):{wi?2, .
.
.
, wi+2}.
We specify if the characterof analysis is at the beginning or at the endof a word.
We also add joint features betweenthe above source of information.?
Segment-Based Features : Arabic blank-delimited words are composed of zero or moreprefixes, followed by a stem and zero or moresuffixes.
Each prefix, stem or suffix will becalled a segment in this paper.
Segments areoften the subject of analysis when processingArabic (Zitouni et al, 2005).
Syntactic in-formation such as POS or parse informationis usually computed on segments rather thanwords.
As an example, the Arabic white-spacedelimited word ??
D?K.
A?
contains a verb ?K.
A?, athird-person feminine singular subject-markerH (she), and a pronoun suffix ??
(them); itis also a complete sentence meaning ?she metthem.?
To separate the Arabic white-spacedelimited words into segments, we use a seg-mentation model similar to the one presentedby (Lee et al, 2003).
The model obtains anaccuracy of about 98%.
In order to simulatereal applications, we only use segments gener-ated by the model rather than true segments.In the diacritization system, we include thecurrent segment ai and its word segment con-text in a window of 5 (forward and backwardtrigram): {ai?2, .
.
.
, ai+2}.
We specify if thecharacter of analysis is at the beginning or atthe end of a segment.
We also add joint infor-mation with lexical features.?
POS Features : we attach to the segmentai of the current character, its POS: POS(ai).This is combined with joint features that in-clude the lexical and segment-based informa-tion.
We use a statistical POS tagging systembuilt on Arabic Treebank data with MaxEntframework (Ratnaparkhi, 1996).
The modelhas an accuracy of about 96%.
We did notwant to use the true POS tags because wewould not have access to such information inreal applications.5 DataThe diacritization system we present here istrained and evaluated on the LDC?s Arabic Tree-bank of diacritized news stories ?
Part 3 v1.0: cata-log number LDC2004T11 and ISBN 1-58563-298-8.The corpus includes complete vocalization (includ-ing case-endings).
We introduce here a clearly de-fined and replicable split of the corpus, so that thereproduction of the results or future investigationscan accurately and correctly be established.
Thiscorpus includes 600 documents from the An NaharNews Text.
There are a total of 340,281 words.
Wesplit the corpus into two sets: training data and de-velopment test (devtest) data.
The training datacontains 288,000 words approximately, whereas thedevtest contains close to 52,000 words.
The 90documents of the devtest data are created by tak-ing the last (in chronological order) 15% of docu-ments dating from ?20021015 0101?
(i.e., October15, 2002) to ?20021215 0045?
(i.e., December 15,2002).
The time span of the devtest is intention-ally non-overlapping with that of the training set,as this models how the system will perform in thereal world.Previously published papers use proprietary cor-pus or lack clear description of the training/devtestdata split, which make the comparison to othertechniques difficult.
By clearly reporting the splitof the publicly available LDC?s Arabic Treebank581corpus in this section, we want future comparisonsto be correctly established.6 ExperimentsExperiments are reported in terms of word errorrate (WER), segment error rate (SER), and di-acritization error rate (DER).
The DER is theproportion of incorrectly restored diacritics.
TheWER is the percentage of incorrectly diacritizedwhite-space delimited words: in order to becounted as incorrect, at least one character in theword must have a diacritization error.
The SERis similar to WER but indicates the proportion ofincorrectly diacritized segments.
A segment canbe a prefix, a stem, or a suffix.
Segments are oftenthe subject of analysis when processing Arabic (Zi-touni et al, 2005).
Syntactic information such asPOS or parse information is based on segmentsrather than words.
Consequently, it is importantto know the SER in cases where the diacritizationsystem may be used to help disambiguate syntacticinformation.Several modern Arabic scripts contains the con-sonant doubling ?shadda?
; it is common for na-tive speakers to write without diacritics except theshadda.
In this case the role of the diacritizationsystem will be to restore the short vowels, doubledcase ending, and the vowel absence ?sukuun?.
Werun two batches of experiments: a first experimentwhere documents contain the original shadda anda second one where documents don?t contain anydiacritics including the shadda.
The diacritizationsystem proceeds in two steps when it has to pre-dict the shadda: a first step where only shadda isrestored and a second step where other diacritics(excluding shadda) are predicted.To assess the performance of the system under dif-ferent conditions, we consider three cases based onthe kind of features employed:1. system that has access to lexical features only;2. system that has access to lexical and segment-based features;3. system that has access to lexical, segment-based and POS features.The different system types described above use thetwo previously assigned diacritics as additional fea-ture.
The DER of the shadda restoration step isequal to 5% when we use lexical features only, 0.4%when we add segment-based information, and 0.3%when we employ lexical, POS, and segment-basedfeatures.Table 2 reports experimental results of the diacriti-zation system with different feature sets.
Usingonly lexical features, we observe a DER of 8.2%and a WER of 25.1% which is competitive to aTrue shadda Predicted shaddaWER SER DER WER SER DERLexical features24.8 12.6 7.9 25.1 13.0 8.2Lexical + segment-based features18.2 9.0 5.5 18.8 9.4 5.8Lexical + segment-based + POS features17.3 8.5 5.1 18.0 8.9 5.5Table 2: The impact of features on the diacriti-zation system performance.
The columns markedwith ?True shadda?
represent results on docu-ments containing the original consonant doubling?shadda?
while columns marked with ?Predictedshadda?
represent results where the system re-stored all diacritics including shadda.state-of-the-art system evaluated on Arabic Tree-bank Part 2: in (Nelken and Shieber, 2005) a DERof 12.79% and a WER of 23.61% are reported.The system they described in (Nelken and Shieber,2005) uses lexical, segment-based, and morpholog-ical information.
Table 2 also shows that, whensegment-based information is added to our sys-tem, a significant improvement is achieved: 25%for WER (18.8 vs. 25.1), 38% for SER (9.4 vs.13.0), and 41% for DER (5.8 vs. 8.2).
Similar be-havior is observed when the documents contain theoriginal shadda.
POS features are also helpful inimproving the performance of the system.
Theyimproved the WER by 4% (18.0 vs. 18.8), SER by5% (8.9 vs. 9.4), and DER by 5% (5.5 vs. 5.8).Case-ending in Arabic documents consists of thediacritic attributed to the last character in a white-space delimited word.
Restoring them is the mostdifficult part in the diacritization of a document.Case endings are only present in formal or highlyliterary scripts.
Only educated speakers of mod-ern standard Arabic master their use.
Technically,every noun has such an ending, although at theend of a sentence no inflection is pronounced, evenin formal speech, because of the rules of ?pause?.For this reason, we conduct another experiment inwhich case-endings were stripped throughout thetraining and testing data without the attempt torestore them.We present in Table 3 the performance of the di-acritization system on documents without case-endings.
Results clearly show that when case-endings are omitted, the WER declines by 58%(7.2% vs. 17.3%), SER is decreased by 52% (4.0%vs.
8.5%), and DER is reduced by 56% (2.2% vs.5.1%).
Also, Table 3 shows again that a richerset of features results in a better performance;compared to a system using lexical features only,adding POS and segment-based features improvedthe WER by 38% (7.2% vs. 11.8%), the SER by39% (4.0% vs. 6.6%), and DER by 38% (2.2% vs.582True shadda Predicted shaddaWER SER DER WER SER DERLexical features11.8 6.6 3.6 12.4 7.0 3.9Lexical + segment-based features7.8 4.4 2.4 8.6 4.8 2.7Lexical + segment-based + POS features7.2 4.0 2.2 7.9 4.4 2.5Table 3: Performance of the diacritization systembased on employed features.
System is trainedand evaluated on documents without case-ending.Columns marked with ?True shadda?
represent re-sults on documents containing the original con-sonant doubling ?shadda?
while columns markedwith ?Predicted shadda?
represent results wherethe system restored all diacritics including shadda.3.6%).
Similar to the results reported in Table 2,we show that the performance of the system aresimilar whether the document contains the origi-nal shadda or not.
A system like this trained onnon case-ending documents can be of interest toapplications such as speech recognition, where thelast state of a word HMM model can be defined toabsorb all possible vowels (Afify et al, 2004).7 Comparison to other approachesAs stated in section 3, the most recent and ad-vanced approach to diacritic restoration is the onepresented in (Nelken and Shieber, 2005): theyshowed a DER of 12.79% and a WER of 23.61% onArabic Treebank corpus using finite state transduc-ers (FST) with a Katz language modeling (LM) asdescribed in (Chen and Goodman, 1999).
Becausethey didn?t describe how they split their corpusinto training/test sets, we were not able to use thesame data for comparison purpose.In this section, we want essentially to duplicatethe aforementioned FST result for comparison us-ing the identical training and testing set we use forour experiments.
We also propose some new vari-ations on the finite state machine modeling tech-nique which improve performance considerably.The algorithm for FST based vowel restorationcould not be simpler: between every pair of char-acters we insert diacritics if doing so improvesthe likelihood of the sequence as scored by a sta-tistical n-gram model trained upon the trainingcorpus.
Thus, in between every pair of charac-ters we propose and score all possible diacriticalinsertions.
Results reported in Table 4 indicatethe error rates of diacritic restoration (includingshadda).
We show performance using both Kneser-Ney and Katz LMs (Chen and Goodman, 1999)with increasingly large n-grams.
It is our opinionthat large n-grams effectively duplicate the use ofa lexicon.
It is unfortunate but true that, even fora rich resource like the Arabic Treebank, the choiceof modeling heuristic and the effects of small sam-ple size are considerable.
Using the finite state ma-chine modeling technique, we obtain similar resultsto those reported in (Nelken and Shieber, 2005): aWER of 23% and a DER of 15%.
Better perfor-mance is reached with the use of Kneser-Ney LM.These results still under-perform those obtainedby MaxEnt approach presented in Table 2.
Whenall sources of information are included, the Max-Ent technique outperforms the FST model by 21%(22% vs. 18%) in terms of WER and 39% (9% vs.5.5%) in terms of DER.The SER reported on Table 2 and Table 3 are basedon the Arabic segmentation system we use in theMaxEnt approach.
Since, the FST model doesn?tuse such a system, we found inappropriate to re-port SER in this section.Katz LM Kneser-Ney LMn-gram size WER DER WER DER3 63 31 55 284 54 25 38 195 51 21 28 136 44 18 24 117 39 16 23 118 37 15 23 10Table 4: Error Rate in % for n-gram diacriticrestoration using FST.We propose in the following an extension to theaforementioned FST model, where we jointly de-termines not only diacritics but segmentation intoaffixes as described in (Lee et al, 2003).
Table 5gives the performance of the extended FST modelwhere Kneser-Ney LM is used, since it producesbetter results.
This should be a much more dif-ficult task, as there are more than twice as manypossible insertions.
However, the choice of diacrit-ics is related to and dependent upon the choice ofsegmentation.
Thus, we demonstrate that a richerinternal representation produces a more powerfulmodel.8 ConclusionWe presented in this paper a statistical model forArabic diacritic restoration.
The approach we pro-pose is based on the Maximum entropy framework,which gives the system the ability to integrate dif-ferent sources of knowledge.
Our model has the ad-vantage of successfully combining diverse sourcesof information ranging from lexical, segment-basedand POS features.
Both POS and segment-basedfeatures are generated by separate statistical sys-tems ?
not extracted manually ?
in order to sim-ulate real world applications.
The segment-basedfeatures are extracted from a statistical morpho-logical analysis system using WFST approach andthe POS features are generated by a parsing model583True Shadda Predicted Shaddan-gram size Kneser-Ney Kneser-NeyWER DER WER DER3 49 23 52 274 34 14 35 175 26 11 26 126 23 10 23 107 23 9 22 108 23 9 22 10Table 5: Error Rate in % for n-gram dia-critic restoration and segmentation using FSTand Kneser-Ney LM.
Columns marked with ?Trueshadda?
represent results on documents contain-ing the original consonant doubling ?shadda?
whilecolumns marked with ?Predicted shadda?
repre-sent results where the system restored all diacriticsincluding shadda.that also uses Maximum entropy framework.
Eval-uation results show that combining these sources ofinformation lead to state-of-the-art performance.As future work, we plan to incorporate Buckwaltermorphological analyzer information to extract newfeatures that reduce the search space.
One idea willbe to reduce the search to the number of hypothe-ses, if any, proposed by the morphological analyzer.We also plan to investigate additional conjunctionfeatures to improve the accuracy of the model.AcknowledgmentsGrateful thanks are extended to Radu Florian forhis constructive comments regarding the maximumentropy classifier.ReferencesM.
Afify, S. Abdou, J. Makhoul, L. Nguyen, and B. Xi-ang.
2004.
The BBN RT04 BN Arabic System.
InRT04 Workshop, Palisades NY.A.
Berger, S. Della Pietra, and V. Della Pietra.
1996.A maximum entropy approach to natural language pro-cessing.
Computational Linguistics, 22(1):39?71.T.
Buckwalter.
2002.
Buckwalter Arabic morpholog-ical analyzer version 1.0.
Technical report, Linguis-tic Data Consortium, LDC2002L49 and ISBN 1-58563-257-0.Stanley F. Chen and Joshua Goodman.
1999.
Anempirical study of smoothing techniques for languagemodeling.
computer speech and language.
ComputerSpeech and Language, 4(13):359?393.Stanley Chen and Ronald Rosenfeld.
2000.
A surveyof smoothing techniques for me models.
IEEE Trans.on Speech and Audio Processing.F.
Debili, H. Achour, and E. Souissi.
2002.
Del?etiquetage grammatical a?
la voyellation automatiquede l?arabe.
Technical report, Correspondances del?Institut de Recherche sur le Maghreb Contemporain17.Y.
El-Imam.
2003.
Phonetization of arabic: rules andalgorithms.
Computer Speech and Language, 18:339?373.T.
El-Sadany and M. Hashish.
1988.
Semi-automaticvowelization of Arabic verbs.
In 10th NC Conference,Jeddah, Saudi Arabia.O.
Emam and V. Fisher.
2004.
A hierarchical ap-proach for the statistical vowelization of Arabic text.Technical report, IBM patent filed, DE9-2004-0006, USpatent application US2005/0192809 A1.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing,N.
Kambhatla, X. Luo, N Nicolov, and S Roukos.
2004.A statistical model for multilingual entity detectionand tracking.
In Proceedings of HLT-NAACL 2004,pages 1?8.Y.
Gal.
2002.
An HMM approach to vowel restora-tion in Arabic and Hebrew.
In ACL-02 Workshop onComputational Approaches to Semitic Languages.Joshua Goodman.
2002.
Sequential conditional gener-alized iterative scaling.
In Proceedings of ACL?02.K.
Kirchhoff and D. Vergyri.
2005.
Cross-dialectaldata sharing for acoustic modeling in Arabic speechrecognition.
Speech Communication, 46(1):37?51, May.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML.Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, andH.
Hassan.
2003.
Language model based Arabic wordsegmentation.
In Proceedings of the ACL?03, pages399?406.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy markov models forinformation extraction and segmentation.
In ICML.Rani Nelken and Stuart M. Shieber.
2005.
Arabicdiacritization using weighted finite-state transducers.In ACL-05 Workshop on Computational Approaches toSemitic Languages, pages 79?86, Ann Arbor, Michigan.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Conference onEmpirical Methods in Natural Language Processing.M.
Tayli and A. Al-Salamah.
1990.
Building bilingualmicrocomputer systems.
Communications of the ACM,33(5):495?505.D.
Vergyri and K. Kirchhoff.
2004.
Automatic dia-critization of Arabic for acoustic modeling in speechrecognition.
In COLING Workshop on Arabic-scriptBased Languages, Geneva, Switzerland.Tong Zhang, Fred Damerau, and David E. Johnson.2002.
Text chunking based on a generalization of Win-now.
Journal of Machine Learning Research, 2:615?637.Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and RaduFlorian.
2005.
The impact of morphological stemmingon Arabic mention detection and coreference resolu-tion.
In Proceedings of the ACL Workshop on Compu-tational Approaches to Semitic Languages, pages 63?70, Ann Arbor, June.584
