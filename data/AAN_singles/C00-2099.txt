A Statistical Theory of Dependency SyntaxChr i s ter  Sa lnue lssonXerox  Resem:ch Cent re  Europe6, chemin  de Mauper tu is38240 Mey lan ,  FRANCEChr i s ' cer .
Samue: l .
sson?xrce .
x rox ,  comAbst ractA generative statistical model of dependency syntaxis proposed based on 'l'csniSre's classical theory.
Itprovides a stochastic formalization of the originalmodel of syntactic structure and augments it witha model of the string realization process, the latterwhich is lacking in TesniSre's original work.
Theresulting theory models crossing dependency links,discontinuous nuclei and string merging, and it hasbeen given an etIicient computational rendering.1 I n t roduct ionThe theory of dependency grammar culminated inthe seminal book by Lncien TesniSre, (Tesnihre,1959), to which also today's leading scholars payhomage, see, e.g., (Mel'enk, 1987).
Unfortunately,Tesnibre's book is only available in French, with apartial translation into German, and subsequent de-scriptions of his work reported in English, (Hays,196/1), (Gaifinan, 1965), (Robinson, 1970), ,etc.,stray increasingly t'urther fi:om the original, see (En-gel, 1996) or (,15~rvinen, 1998) for an account of this.The first step when assigning a dependency de-scription to an input string is to segment he inputstring into nuclei.
A nucleus can he a word, a partof a word, or a sequence of words and subwords,and these need not appear eontiguonsly in the inputstring.
The best way to visualize this is perhaps thefollowing: the string is tokenized into a sequence oftokens and each lmcleus consists of a subsequence ofthese tokens.
Alternative readings may imply differ-ent ways of dividing the token sequence into nuclei,and segmenting the input string into nuclei is there-fore in general a nondeterministic process.The next step is to relate the nuclei to each otherthrough dependency links, which are directed andtyped.
If there is a dependency link froln one nu-cleus to another, the former is called a dependentof the latter, and the latter a regent of the former.Theories of dependency syntax typically require thateach nucleus, save a single root nucleus, is assigneda unique regent, and that there is no chain of de-pendency links that constitutes a cycle.
This meansthat the dependency links establish a tree structure,main mainate ate~bj~d~j  ~bj~su~iJohn beans beans JohnFignre 1: Dependency trees for John ate beans.where each node is labeled by a nucleus.
Thus, thelabel assigned to a node is a dependent of the labelassigned to its parent node, and conversely, the labelassigned to a node is the regent of the labels assignedto its child nodes.
Figure 1 shows two dependencytrees tbr the sentence John ate beans.In Tesni~re's dependency syntax, only the depen-dency strncture, not the order of the dependents, isrepresented by a dependency tree.
This means thatdependency trees are unordered, a.nd thus that thetwo trees of Figure 1 are equivalent.
This also mea.nsthat specitying the sm'face-string realization of a de-pendency description becomes a separate issue.We will model dependency desc,:iptions as twoseparate stochastic processes: one top-down processgenerating the tree structure T and one bottom-upprocess generating the surt3.ce string(s) S given thetree structure:1)(7, s) = \] '(7).
p(s 1 7)This can be viewed as a variant of Shannon's noisychannel model, consisting of a language model oftree structures and a signal model converting treesto surface strings.
In Section 2 we describe the top-down process generating tree structures and in Sec-tion 3 we propose a series of increasingly more so-phisticated bottom-up roccsses generating surl~cestrings, which resnlt in grammars with increasinglygreater expressive power.
Section el describes howthe proposed stochastic model of dependency syn-tax was realized as a probabilistic hart parser.2 Generat ing  Dependency  TreesTo describe a tree structure T, we will use a stringnotation, introduced in (Gorn, 1962), for the nodes684?/emainlate/I~bv d0blJohn/ll beans/12l!
'igure 2: Corn's tree notation tbr John ate beans.H ?
S Se \[main\] s (1) .\] ate \ [subj ,dobj \ ]  s ( l l )  ate s(12)1:1 John ~ John12 beans 0 beansI,'igure 3: l)ependency encoding of John ale beans.of Clio tree, where the node name sl)ecifi0s the pathfi'om the root no(le ~ to the node in question,I f  (Jj is a node of the tree T,with j C N+ and (/J E N~,then  q5 is also a node of the trc'e Tand ()j is a child of 4.ltere, N+ denotes the set of positive integers{1,2,. .
.}
and N~_ is the set of strings over N+.
'l'hislncans that the label assigned to node ()j is a de-pendent of the label assigned to node (J.
The firstdependency tree of Figure 1 is shown in l!
'igure 2using this notation.We introduce three basic random variables, whichincrementally generate the tree strucl;ure:?
?
(4)) = l assigns the \]al)el l to node 4), wherel is a i it lc|etis, i.e., it; is drawn frol,-I the  set ofs t r ings  over  t, he so.t of  i, okens.?
"D(Oj) = d indicates t.h(~ dep(:ndency t pe d link-ing the label of node OJ to its regent, the labelOf node 4'"?
V(?,) = v indica.tes that node 7, has exactly vchild nodes.Note the use of ~(~/J) = 0, rather than a partitioningof the labels into terlninal and nonterminal syml)ols,to indicate that ~ is a leaf node.l,et D be the (finite) set of l)ossible dependencytypes.
We next introduce the composite variables.T(()) ranging over the power bag* N D, indicatingthe bag of dependency t pes of dJ's children:m(4,) = f = \[d,,...,dl,\] *>e~O P(c/,) = v A V:ic{i ..... v} D(6 j )  = djFigure 3 encodes the dependency ti;ee ()1' Figure 2accordingly.
We will ignore the last cohunn \['or now.1 A bag (mull 'set) can contain several tokens of the Smlmtype.
We denote sets {...}, \]Jags \[...\] and ordered tuples {...),\]Jill, over\]o+'id O~ (~> etc,We introduce the probabilitiess'~(~) -: P (~(0  : l)P~ if, j )  == l ' (10 j )  = (~ I ~(4,) = t,~D(4,J) = 4dPT(0  == P( f (0  = f l~(0  = l)Pm(~) = { tbr4J~-e }= s,(m(4,) = S I ?
(4>) -- l, 9(40 -- d)These l~robabilities are tyl)ically model parameters,o1' further decomposed into such.
lJ~(@j) is the prob-ability of the label ?
(4~J) of a node given the label?
(4') of its regent and the dependency type "D(0j)linking them.
l{eh~ting Eft, j) and ?
(0) yiekls lexicalcollocation statistics and including D((~j) makes thecollocation statistics lexical-fimetional.
Pm(~0 is theprobability of the bag of' dependency types Y(0) ofa ,,ode Rive,, its label ?
(4J) and its relation D(#)) toits regent.
This retleets the probability of the label'svM oncy, or lexieal-fimctional eoml)lement , and of op-1.ional adjuncts.
Including D(q)) makes this proba-bility situa.ted in taking its current role into accounl..These allow us to define the tree probal)ility~,(m) = I I  ~ '~0/ , ) - s>(4 , ),',,EArwiiere the 1)roduct is taken over the set.
of nodes .A/"of the tree.\?e generate the random variables ?
and S usinga top-down s tochast i c  process,  where  ?
(( ) )  is goner-ated I)efore Y(O).
The probal)ility of the condition-ing material of l~(Oj) is then known from Pc(O) and19((,), and that of Sg(4,j) is known froln \]'?
(OJ)and lJ:n(O).
F'igure 3 shows the process generatingthe dependency tree' of Figure 2 by reading the ?and .7:- colunms downwards in parallel, ?
before Y:~,(~) =.
,  m(O = r~,~\ ] ,?
(1) = ate, Y(1)= \ [subj ,dobj \ ] ,~;(~)  = Job,,, ro l l : i )=  ~,~(~)  = /,~,,,,~, m(12) = 0Consider calculating the l)robabilities at node 1:IJ~ (1) == s'(/_;(l) = <,t~ I?
(0  = .,'D(1) = main)P~(~)  == P lY ( l )=  \[subj,dobj\] \[I C(I) = .t~,'D(~) = m~n)3 St r ing  Rea l i za t ion'\]'he string realization cannot be uniquely deter-mined from the tree structure.
'lb model the string-realization process, we introduce another fundamen-tal random w~riable $(()), which denotes the string685associated with node 0 and which should not be con-fused with the node label ?(()).
We will introduceyet another fundamental randoln variable \]v4(~)) inSection 3.2, when we accommodate crossing depen-dency links.
In Section 3.1, we present a projectivcstochastic dependency gralnlnar with an expressivepower not exceeding that of stochastic ontext-freegrammars.3.1 P ro jec t ive  Dependency  GrammarsWe let the stochastic process generating the ?
and .7-vtu'iM)les be as described above.
We then define tilestochastic string-realization process by letting tile8(~5) variables, given ?
's label 1(40 and the bag ofstrings s(()j) of ~5's child nodes, randomly permuteand concatenate them according to the probabilitydistributions of the modehPs(0 == s,(s(<.)
= 401  c(O, 7 (0 ,  c(O)~'~(4,) = { fo r?
?~ }= s'(s(?,) = 4?')
I >frO, eft,), 7(?)
,  c(?,))wherec(4,) = 0 \[~(<bJ)\]j= l8(~$) = adjoin(C(g,),l(?
))adjoin(A,/3) = eoncat(permute(A U \[/3\]))The latter equations hould be interpreted as defin-ing the randorn variable 8, rather than specifying itsprobability distribution or some possible outcome.This means that each dependent is realized adjacentto its regent, where wc allow intervening siblings,and that we thus stay within the expressive powerof stochastic ontext-free grammars.We define the string-realization probability~beArand the tree-string probability asP(7, s) = I"(7-)./,(s I 7)The stochastic process generating the tree struc-ture is as described above.
We then generate thestring variables S using a bottom-up stochastic pro-cess.
Figure 3 also shows the process realizing thesurface string John ate beans fl-om the dependencytree of Figure 2 by reading the S column upwards:,9(12) = bca,,s, S (11)  = Job,,,,9(1) = s(11)ate s(12), S(e) = s ( l ) .Consider cMeulating tile striug probability at node1.
Ps is the probability of the particular permut~t-tion observed of the strings of the children and the?ledid say/1~bj/'%sc0njMao,/l 1 that ate/l 2subj/"~ol0bjJohnll21 Whatbeans/122Figure 4: Del)endency tree for What beans did Marysay lhat John ate?1M)el of the node.
To overcome the sparse-data prob-lem, we will generalize over the actual strings of tilechildren to their dependency types.
For example,s(subj) denotes the string of the subject child, re-gardless of what it actually might be.J'~(1) = P(S(1) = s(subj) ate s(dobj) II "D(1) = m~n,  s:(1) = <,re,C(I) = \[s(subj), s(dobj)\])This is the probability of the permutation(s(subj), ate, s(dobj))of the bag\[s(subj), aic, s(dobj)\]given this bag and the fact that we wish to tbrm amain, declarative clause.
This example highlightsthe relationship between the node strings and bothSallssure's notion of constituency and tile l)ositiolmlschemata of, amongst others, l)idrichsen.3.2 Cross ing Dependency  LinksTo accommodate long-distance dependencies, we al-low a dependent to be realized adjacent o the la-bel of rely node that dominates it, immediately ornot.
For example, consider the dependency tree ofFigure 4 tbr the sentence l/Vhat beans did Ma'Jw saythat John ate?
as encoded in Figure 5.
Ilere, Whatbeans is a dependent of that arc, which in turn is adependent of did say, and What beans is realized be-tween did and sag.
This phenomenon is called move-ment in conjunction with phrase-structure gram-m~rs.
It makes the dependency grammar non-projective, since it creates crossing dependency linksif the dependency trees also depict the word order.We introduce variables A//(~) that randomly se-lect from C(4)) a, subbag CM(4,) of strings passed upto ()'s regent:C(?)
= O(\[s(4)J)\] UCM(?j))j= lGd</,) c_ c(4))s'~(4,) = P(M(4)) = c'~,,(e) II ~(<t,), s:(?
), f i fO, c(6))686N" ?
bec v \[whq\]1 did say \ [subj ,  sconj \ ]11 Mary (~:12 thai ate \[subj ,dobj\]1.21 John122 What beans 0Figure 5: l)ependency encoding of What beans didMary say that John ate?A/ M Sdi(/411)11 ~ Mary12 \[.s(122)\] that s(121) ate121 ~J John122 0 W/tat bem~sFi.?
;ure 6: Process generating What beans did Mary.sag that dohn ate?q'he rest of the strings, Cs(?
), are realized here:c:,;(?)
= c(?)
\= = I be(C), c;,(?))s'(?)
=3,3 D iscont inuous Nuch;iWe generalize the scheme to discontinuous nuclei byallowing 8(?)
to i,mert the strings of C~.
(~5) any-where in 1(?
): eadjoin(A, fl) == V @mj= lfl = b~ ...b,,,Tllis means that strings can only l)e inserted into an-cestor labels, ,lot into other strings, which enforcesa.
type of reverse islaml constraint.
Note how in Fig-ure 6 John is inserted between that and ate to formthe subordina, te clause that John atc.We define tile string-realization probability,b6 arand again define the tree-string prol)ability~ ' ( r ,S )  = l ' (T ) .
l ' (a  I T)2x -~ y indicates that x precedes y in the resulting per-mutation, q~snihre's original implicit definition of a nucleusactually does not require that the order be preserved whenrealizing it; if has catch is a nucleus, so is eaten h.as.
This isobviously a useflfl feature for nlodeling verb chains in G erln&nsubordinate clauses.
'lb avoid derivational ambiguity when generating atree-string pair, i.e., have more than one derivationgenerate tile same tree-string pair, we require thatno string be realized adjacent o the string of anynode it was passed u 1) through.
This introduces thel)raetica.l problem of ensuring that zero probabilitymass is assigned to all derivations violating this con-straint.
Otherwise, the result will be approxima.tingthe parse probabi\]ity with a derivation probability,as described in detail in (Samuelsson, 2000) based onthe seminal work of (Sima'an, 1996).
Schemes like(Alshawi, 1996) tacitly make this approximation.The tree-structure variables ?
and be are gener-ated just as before.
~?e then generate the string vari-ables 8 and Ad using a bottom-up stochastic process,where M(?)
i s  generated before 8(?).
'l.
'he proba-bility of the eonditkming material of \]o~ (?)
is thenknown either from the top-down process or fromI 'M(?j)  and Pa(?j) ,  and that of INTO)is knowneither from the top-down process, or from 15v4(?
),\[)dgq(4)j) and 1~(?j).
The coherence of S(~) a.ndf14(~/)) is enforced by explicit conditioning.Figure 5 shows a top-down process generating thedependency tree of Figure <1; the columns ?
andbe should be read downwards in parallel, L; beforeb e. Figure 6 shows a bottom-up rocess generatingthe string l/Vhat beans did Mary say that dohn at(:?from the dependency description of Figure 5.
Thecolltlll,lS d~v4 and S should be read upwards in paral-lel, 2t4 before $.3.4 St r ing Merg ingWe have increased the expressive power of our de-pendency gramma.rs by nlodifying tile S variables,i.e., by extending the adjoin opera.lion.
In tile firstversion, the adjoin operation randomly permutes thenode label and the strings of the child nodes, andconcatenates the result.
In the second version, itrandondy inserts the strings of the child nodes, andany moved strings to be rea.lized at tile current node,into the node label.The adjoin operation can be fln:ther refined to al-low handling an even wider range of phenomena,such as negation in French.
Here, the dependentstring is merged with the label of the regent, as ne .
.
.pas is wrapped around portions of the verb phrase,e.g., Ne me quitte pas!, see (Brel, 195.(t).
Figure 7shows a dependency tree h)r this.
In addition to this,the node labels may be linguistic abstractions, e.g.
"negation", calling on the S variables also for theirsurface-string realization.Note that the expressive power of the grammardepends on the possible distributions of the stringprobabilities IN.
Since each node label can be movedand realized at the root node, any language can berecognized to which the string probabilities allow as-signing the entire probablity mass, and the gralnmarwill possess at least this expressive power.687!leimnlquitte/l~g>~do~Ne pas/l l me~12Figure 7: Dependency tree for Ne me quitte pas t4 A Computational RenderingA close approximation of the described stochasticmodel of dependency syntax has been realized as atype of prohabilistic bottom-up chart parser.4.1 Model SpecializationThe following modifications, which are really justspecializations, were made to the proposed modelfor efficiency reasons and to cope with sparse data.According to Tesni6re, a nucleus is a unit thatcontains both tile syntactic and semantic head andthat does not exhihit any internal syntactic struc-ture.
We take the view that a nucleus consists of acontent word, i.e., an open-class word, and all flmc-tion words adding information to it that could just aswell have been realized morphologically.
For exam-ple, the definite article associates definiteness with a.word, which conld just has well have been manifestedin the word form, as it is done in North-Germaniclanguages; a preposition could be realized as a loca.-tional or temporal inflection, as is done in Finnish.The longest nuclei we currently allow are verb chainsof the form that have been eoten, as in John knowsthat lhe beans have been eaten.The 5 r variables were decomposed into generatingthe set of obligatory arguments, i.e., the valency orlexical complement, atonce, as in the original model.Optional modifiers (adjuncts) are attached throughone memory-less process tbr each modifier type, re-suiting in geometric distributions for these.
This isthe same separation of arguments and adjuncts asthat employed by (Collins, 1997).
However, the ?variables remained as described above, thus leavingthe lexieal collocation statistics intact.The movement probability was divided into threeparts: the probability of moving the string of a par-ticular argument dependent from its regent, that ofa moved dependency type passing through a par-ticular other dependency type, and that of a de-pendency type landing beneath a particular otherdependency type.
The one type of movement thatis not yet properly handled is assigning argumentsand adjuncts to dislocated heads, as in What bookdid John read by Chomsky?The string-realization probability is a straight-forward generalization of that given at the end ofSection 3.1, and they m:e defined through regu-lar expressions.
Basically, each unmoved depen-dent string, each moved string landed at.
the cur-?/e ?/eynql ynqlDid xxx/I Did eat/1s/uu b j /~ 'do .~ s/ubj/ '~dobiJohn/l I beaus~12 Johull I xxx/12Figure 8: Dependency trees for Did John xxm beans?and Did John eat xxm?rent node, and each token of the nucleus labeling thecurrent node are treated as units that are randomlypermuted.
Whenever possible, strings are general-ized to their dependency t pes, but accurately mod-elling dependent order in French requires inspectingtile actual strings of dependent clitics.
Open-classwords are typically generalized to their word class.String merging only applies to a small class of nuclei,where we treat tile individual tokens of the depen-dent string, which is typically its label, as separateunits when perfornfing tile permutation.4.2 The  Char t  ParserThe parsing algorithm, which draws on the Co&e-t(asanli-Younger (CI(Y) algorithm, see (Younger,1967), is formulated as a prohabilistic deductionscheme, which in turn is realized as an agenda-drivenchart-pa.rser.
The top-level control is similar to thatof (Pereira and Shieher, 1987), pp.
196-210.
Theparser is implemented in Prolog, and it relies heav-ily on using set and bag operations as primitives,utilizing and extending existing SICStus libraries.The parser first nondeterministically segments theinput string into nuclei, using a lexicon, and eachpossible lmcleus spawns edges tbr the initial chart.Due to discontinuous nuclei, each edge spans not asingle pair of string positions, indicating its startand end position, \])tit a set of such string-positionpairs, and we call this set an index.
If the indexis a singleton set, then it is continuous.
We extendthe notion of adjacent indices to be any two non-overlapping indices where one has a start positionthat equals an end position of the other.The lexicon contains intbrmation about the roles(dependency t pes linking it to its regent) and va-lencies (sets of types of argument dependents) thatare possible for each nucleus.
These are hard con-straints.
Unknown words are included in nuclei in ajudicious way and the resulting nuclei are assignedall reasonable role/valency pairs in the lexicon.
Forexample, the parser "correctly" analyzes tile sen-tences Did John xxx beans?
and Did John eat xxx?as shown in Figure 8, where xxx' is not in the lexicon.For each edge added to the initial chart, the lexi-con predicts a single valency, but a set of alternativeroles.
Edges arc added to cover all possible valen-al)ue to the uniqueness principle of arguments, these aresets, rather than bags.688ties for each nucleus.
The roles correspond to tim"goal" of dotted items used ill traditional cha.rt pars-ing, and the unfilled valency slots play the part ofthe "l)ody", i.e., the i)ortion of the \]{IlS \['ol\]owingthe dot that renlailis to I)e found.
If an argunl_ent isattached to the edge, the corresponding valency slotis filled in the resulting new odg(;; no arg~llnlont ea.llbe atta.ched to a.n edge llnless tllere is a (;orrespon(l-ing unfilled witency slot for it, or it is licensed by alnoved arguln0nt, l,'or obvions reasons, the lexiconca.nnot predict all possible combinations of adjunctsfor each nuehms, and in fact predicts none at all.There will in general be nmltiple derivations of anyedge with more than ()no del)endent , but the parseravoids adding dul)licate edges to tlt(?
chart in thesame way as a. traditional chart l)arser does.The l>arser enll)loys a. l)a(:ked l)arse foresl.
(PI)I!
')to represent he set of all possible analyses and thei)robalfility of each analysis is recoverable I\]:om thePPI!'
entries.
Since optional inodifiers are not 1)re -dieted by the lexicon, the chart does not (:onl, a.ii~any edges that ('orrespon(t directly to passive edgesill traditional chart parsing; at any point, an ad.lun('tC~ll always be added to an existing edge to form anew edge.
In sonic sense, though, tile 1)1)1 '' nodesplay tlie role all' passive edges, since the l)arser neverattempts to combine two edges, only Olle ('xlgc andone I)l)l! '
lio(le, and the la.tter will a.lways 1)e a. de-pendent el'the fornier, directly, or indirectly tliroughthe lists of n:iovcd dependents.
'l'he edge and l)l)l i'node to be Colnl)ined ai'e required to \]lave adjacentindices, and their lnlion is the index of tile now edge.The lnain point in using a l)acked parse forest, is topo'rI'orni local ~tiiil)iguity packing, which lneans a.bstracting over difl);ren(-es ill intc'rnal stlFlletlllye thatdo not lllalL, t(;r for fllrth(~,r \])arsilig.
\?hen attching aI)PF no(-l(~' to SOlllo edgc ;_is a direct or indirect de-pendent, the only relewuit teatnres are its index, itsnucleus, its role a.nd its moved dependents.
Oilierfeatures necessary for recovering the comph;tc" anal-ysis are recorded in the P1)F entries of the node, bntarc not used for parsing.To indicate the alternative that no more depen-dents are added to an edge, it is converted into aset of PPF updates, where each alternative role ofthe edge adds or updates one PPF entry.
Whendoing this, any unfilled valency slots are added tothe edge's set of moved arguments, which in turn isinherited by the resulting PPF update.
'.\['lie edgesare actually not assigned probabilities, since theycontain enough information to derive the appropri-ate l)robabilities once they are converted into I)I)Fentries.
'1'o avoid the combinator ia l  explosion el' un-restricted string merging, we only allow edges withcontinuous indices to be converted into PI)I! '
0ntries,with the exception of a very limited class of lexicallysignMed nnelei, snch as the nc pas, nc jamais, etc.,scheme of French negation.4.3 P r in t ingAs Ot)l)osed to traditional chart parsing, meaningfulupper and lower 1)ounds of the supply and demandfor the dependency types C" the "goal" (roles) and"body" (wdency) of each edge can 1)e determinedFrom the initial chart, which allows performing so-phis(,icated pruning.
The basic idea is that if someedge is proposed with a role that is not sought out-side its index, this role can safely be removed.
Forexample, the word me could potentially be an in-direct object, but if there is no other word in theinl)ut string that can have a.n indirect object as anargument, this alternative can be discarded.
'Phis idea is generalized to a varia.nt of pigeonholereasoning, in the v(;in ofIf wc select this role or edge, then ~here areby necessity too few or too many of somedel)endcncy tyl)e sought or Cl'ered in thechart.or alternativelyIf wc select this nucleus or edge, then wecannot span the entire input string.Pruning is currently only al)plied to the initial chartto remove logically inq>ossible alternatives and usedto filter out impossible dges produced in the predic-tion step.
Nonetheless, it reduces parsing times byan order of magnitude or more tbr many of the testexamples.
\]t would however be possible to applysimilar ideas to interniittently reinove alternativesthat are known 1:o be suboptimal, or to \]leuristicallyprtllie unlik(;ly searcll branches.DiscussionWe have proposed a generative, statistical t.iieoryof dependency syntax, based on TesniSrc's classicaltheory, that models crossing dependency links, dis-continuous nuclei and string merging.
The key in-sight was to separate the tree-generation a d string-realization processes.
The model has been real-ized as a type of probabilistie chart parser.
Theonly other high-fidelity computational rendering ofTesnitre's dependency syntax that we are aware ofis that of (rl.
'apanainen and J fi.rvinen, 1997), which isneither generative nor statistical.The stochastic model generating dependency treesis very similar to other statistical dependency mod-els, e.g., to that of (Alshawi, 1996).
Formulating itusing Gorn's notation and the L; and 2" variables,though, is concise, elegant; and novel.
Nothing pre-vents conditioning the random variables on arbitraryportions of Clio 1)artial tree generated this far, using,e.g., maximum-entrol)y or decision-tree models toextract relevant ~atnres of it; there is no difference689in principle between our model and history-basedparsing, see (Black el; al., 1993; Magerman, 1995).The proposed treatment of string realizationthrough the use of the ,5 and A4 variables is also bothtruly novel and important.
While phrase-structuregrammars overemphasize word order by making theprocesses generating the S variables deterministic,Tesni6re treats string realization as a secondary is-sue.
We tind a middle ground by nsing stochasticprocesses to generate the S and Ad variables, thusreinstating word order as a parameter of equal im-portance as, say, lexical collocation statistics.
It ishowever not elevated to the hard-constraint s atusit enjoys in phrase-structure grammars.Due to the subordinate role of string realization inclassical dependency grammar, the technical prob-lems related to incorporating movement into thestring-realization process have not been investigatedin great detail.
Our use of the 54 variables is moti-vated partly by practical considerations, and partlyby linguistic ones.
The former in the sense thatthis allows designing efficient parsing algorithms forhandling also crossing dependency links.
The lat-ter as this gives us a quantitative handle on theempirically observed resistance against crossing de-pendency links.
As TesniSre points out, there islocality in string realization in the sense that de-pendents tend to be realized adjacent o their re-gents.
This fact is reflected by the model parame-ters, which also model, probabilistically, barrier con-straints, constraints on landing sites, etc.
It is note-worthy that treating movelnent as in GPSG, withthe use of the "slash" l~ature, see (Gazdar et al,1985), pp.
137-168, or as is done in (Collins, \]997),is the converse of that proposed here for dependencygrammars: the tbrmer pass constituents down thetree, the 54 variables pass strings up the tree.The relationship between the proposed stochasticmodel of dependency syntax and a number of otherprominent stochastic grammars i explored in detailin (Samuelsson, 2000).ReferencesItiyan Alshawi.
1996.
IIead automata nd bilingualtiling: Translation with minimal representations.Procs.
3~th Annual Meeting of the Association forComputational Linguistics, pages 167-176.Ezra Black, Fred Jelinek, John Lafferty, DavidMagerman, l~obert Mercer, and Salim Roukos.1993.
Towards history-based grammars: Usingricher models tbr probabilistic parsing.
Proes.28th Annual Meeting of the Association for Com-putational Linguistics, pages 31 37.Jacques Brel.
1959.
Ne mc quitte pas.
La Valse hMille Temps (PIII 6325.205).Micha.el Collins.
1997.
Three generative, lexical-ized models for statistical parsing.
Procs.
35thAnnual Meeting of the Association fog" Computa-tional Linguistics, pages 16-23.Ulrich Engel, 1996.
Tcsni~rc Miflvcrstanden: Lu-cicn Tcsni~re - Syntaxc Structuralc et Opera-tion Mcntalcs.
Aktcn des deulscl,-franzSsischenKolloquiums anliifllich dcr 100 Wiedcrkchr seinesGebursttagcs, 5'trasbourg 1993, volume 348 of Lin-guistische Arbeiten, pages 53-61.
Niedermcyer,Tiibingen.ltaim Gaiflnan.
1965. l)ependency systems andphrase-structure systems.
Information and Con-trol, 8:304-337.Gerald Gazdar, Ewan Klein, Geoffrey K. Pullnm,and Ivan A.
Sag.
1985.
Generalized Phrase Struc-turc Grammar.
Basil Blackwell l?ul)lishing, Ox-ford, England.
Also published by Harvard Uni-versity Press, Cambridge, MA.Saul Gorn.
1962.
Processors for infinite codes ofshannon-fa.no type.
Syrup.
Math.
Theory of Au-tomata.David Ilays.
1964. l)ependency theory: A formal-ism and some observations.
Languagc, 40(4):511-525.Timo J'a.rvinen.
1998.
Tcsnidre's 5'tg'uctural SyntaxRcworl?ed.
University of llelsinki, Itelsinki.David Ma.german.
1995.
Statistical decision-treemodels for parsing.
Ib'ocs.
33rd Annual Meetingof the Association fog-" Computational Linguistics,pages 276 283.Igor Mel'Snk.
1987.
Dependency 5'ynta.r:.
State Uni-versity of New York Press, All)any.l?crnando Pereira and Stuart Shieber.
1987.
Pro-log and Natural-Language Analysis.
CSLI LectureNote 10.Jane Robinson.
1970. l)ependency structures andtransfornm.tional rules.
Lcmguage, 46:259 285.Christer Samuelsson.
2000.
A theory of stochasticgrammars.
In Proceedings oJNLI)-200(\], l)ages 92105.
Springer Verlag.Khalil Sima'an.
1996.
Computational complexity ofprobabilistic disambigua.tions by means of tree-grammars.
Procs.
16lh International Confcrenccon Computational Linguistics, at the very end.Pasi Tapanainen and Timo JSrvinen.
1997.
A non-projective dependency parser.
Pgvcs.
5th Con-fercnce on Applied Natural Language Processing,pages 6zl 71.Lucien Tesnihre.
1959. lOldmcnts de Syntaxc Sh'uc-turalc.
Libraire C. Klincksieck, Paris.l)avid 1I.
Younger.
1967.
R.ecognition and parsingof context-fi'ee languages in time n a. Informationand Control, 10(2):189 208.690
