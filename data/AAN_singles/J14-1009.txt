On the Problem of Theoretical Termsin Empirical Computational LinguisticsStefan Riezler?Computational LinguisticsHeidelberg University, GermanyPhilosophy of science has pointed out a problem of theoretical terms in empirical sciences.
Thisproblem arises if all known measuring procedures for a quantity of a theory presuppose thevalidity of this very theory, because then statements containing theoretical terms are circular.We argue that a similar circularity can happen in empirical computational linguistics, especiallyin cases where data are manually annotated by experts.
We define a criterion of T-non-theoreticalgrounding as guidance to avoid such circularities, and exemplify how this criterion can be metby crowdsourcing, by task-related data annotation, or by data in the wild.
We argue that thiscriterion should be considered as a necessary condition for an empirical science, in addition tomeasures for reliability of data annotation.1.
IntroductionThe recent history of computational linguistics (CL) shows a trend towards encodingnatural language processing (NLP) problems as machine learning tasks, with the goalof applying task-specific learning machines to solve the encoded NLP problems.
In thefollowing we will refer to such approaches as empirical CL approaches.Machine learning tools and statistical learning theory play an important enablingand guiding role for research in empirical CL.
A recent discussion in the machine learn-ing community claims an even stronger and more general role of machine learning.
Weallude here to a discussion concerning the relation of machine learning and philosophyof science.
For example, Corfield, Scho?lkopf, and Vapnik (2009) compare Popper?s ideasof falsifiability of a scientific theory with ?similar notions?
from statistical learning the-ory regarding Vapnik-Chervonenkis theory.
A recent NIPS workshop on ?Philosophyand Machine Learning?1 presented a collection of papers investigating similar problemsand concepts in the two fields.
Korb (2004) sums up the essence of the discussion bydirectly advertising ?Machine Learning as Philosophy of Science.
?In this article we argue that adopting machine learning theory as philosophy ofscience for empirical CL has to be done with great care.
A problem arises in the applica-tion of machine learning methods to natural language data under the assumption thatinput?output pairs are given and do not have to be questioned.
In contrast to machinelearning, in empirical CL neither a representation of instances nor an association of?
Department of Computational Linguistics, Heidelberg University, Im Neuenheimer Feld 325, 69120Heidelberg, Germany.
E-mail: riezler@cl.uni-heidelberg.de.1 http://www.dsi.unive.it/PhiMaLe2011/.doi:10.1162/COLI a 00182?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 1instances and labels is always ?given.?
We show that especially in cases where dataare manually annotated by expert coders, a problem of circularity arises if one and thesame theory of measurement is used in data annotation and in feature construction.
Inthis article, we use insights from philosophy of science to understand this problem.
Weparticularly point to the ?problem of theoretical terms,?
introduced by Sneed (1971),that shows how circularities can make empirical statements in sciences such as physicsimpossible.In the following, we will explain the problem of theoretical terms with the helpof a miniature physical theory used in philosophy of science (Section 2).
We will thenexemplify this concept on examples from empirical CL (Section 3).
We also make anattempt at proposing solutions to this problem by using crowdsourcing techniques,task-related annotation, or data in the wild (Section 4).2.
The Problem of Theoretical Terms in Philosophy of ScienceIn order to characterize the logical structure of empirical science, philosophy of sciencehas extensively discussed the notions of ?theoretical?
and ?observational?
language.Sneed (1971)2 was the first to suggest a distinction between ?theoretical?
and ?non-theoretical?
terms of a given theory by means of the roles they play in that theory.Balzer (1996, page 140) gives a general definition that states that a term is ?theoreticalin theory T iff every determination of (a realization of) that term presupposes that Thas been successfully applied beforehand.?
Because there are no theory-independentterms in this view, an explicit reference to a theory T is always carried along whencharacterizing terms as theoretical with respect to T (T-theoretical) or non-theoreticalwith respect to T (T-non-theoretical).
Stegmu?ller (1979) makes the notions of ?determina-tion?
or ?realization?
more concrete by referring to procedures for measuring values ofquantities or functions in empirical science:What does it mean to say that a quantity (function) f of a physical theory T isT-theoretical?...
In order to perform an empirical test of an empirical claim containingthe T-theoretical quantity f , we have to measure values of the function f .
But all knownmeasuring procedures (or, if you like, all known theories of measurement of f -values)presuppose the validity of this very theory T. (page 17)The ?problem of theoretical terms?
can then be stated as follows (see Stegmu?ller1979): Suppose a statement of the formx is a P (1)where x is an entity and P is a set-theoretic predicate by which a physical theoryis axiomatized.
If this theory contains P-theoretic terms, then (1) is not an empiricalstatement because another sentence of exactly the same form and with exactly the samepredicate is presupposed.
An illustration of this concept can be given by Stegmu?ller(1986)?s miniature theory of an Archimedian Statics.
Let us assume that this miniaturetheory is formalized by the set-theoretic predicate AS.
The intended applications of thetheory AS are objects a1, .
.
.
, an that are in balance around a pivot point.
The theory uses2 The following discussion of concepts of the ?structuralist?
or ?non-statement view of theories?
is basedon works by Stegmu?ller (1979, 1986) and Balzer and Moulines (1996) that are more accessible than theoriginal book by Sneed (1971).
All translations from German are by the author.236Riezler On the Problem of Theoretical Terms in Empirical CLtwo functions that measure the distance d of the objects from the pivot point, and theweight g. The central axiom of the theory states that the sum of the products d(ai)g(ai)is the same for the objects on either side of the pivot point.
The theory AS can then bedefined as follows:x is an AS iff there is an A, d, g such that:1. x = ?A, d, g?,2.
A = {a1, .
.
.
, an},3. d : A ?
IR,4.
g : A ?
IR,5.
?a ?
A : g(a) > 0,6.?ni=1 d(ai)g(ai) = 0.Entities that satisfy conditions (1) to (5) are called potential models of the theory.
Enti-ties that also satisfy the central axiom (6) are called models of the theory.
An empiricalstatement is a statement that a certain entity is a model of the theory.Stegmu?ller (1986) uses the miniature theory AS to explain the problem of theoreticalterms as follows: Suppose we observe children sitting on a seesaw board.
Supposefurther that the board is in balance.
Translating this observation into the set-theoreticlanguage, we could denote by y the balanced seesaw including the children, and wewould be tempted to make the empirical statement thaty is an AS (2)In order to verify the central axiom, we need to measure distance and weight of thechildren.
Suppose that we have a measuring tape available to measure distance, andsuppose further that our only method to measure weight is the use of beam balancescales.
Let us denote by z the entity consisting of the balanced beam scale, the child,and the counterbalancing measuring weight; then the validity of our measuring resultdepends on a statementz is an AS (3)Thus, in order to check statement (2), we have to presuppose statement (3), whichis of the very same form and uses the very same predicate.
That means, in order tomeasure the weight of the children, we have to presuppose successful applications ofthe theory AS.
But in order to decide for successful applications of AS, we need to beable to measure the weight of the objects in such application.
This epistemological circleprevents us from claiming that our original statement (2) is an empirical statement.The crux of the problem of theoretical terms for the miniature theory AS is themeasuring procedure for the function g that presupposes the validity of the theory AS.The term g is thus AS-theoretical.
There are two solutions to this problem:1.
In order to avoid the use of AS-theoretic terms such as g, we could discardthe assumption that our weight-measuring procedure uses beam balancescales.
Instead we could use AS-non-theoretic measuring procedures suchas spring scales.
The miniature theory AS would no longer contain237Computational Linguistics Volume 40, Number 1AS-theoretic terms.
Thus we would be able to make empirical statementsof the form (2), that is, statements about certain entities being models ofthe theory AS.2.
In complex physical theories such as particle mechanics there are nosimplified assumptions on measuring procedures that can be droppedeasily.
Sneed (1971) proposed the so-called Ramsey solution3 that inessence avoids AS-theoretical terms by existentially quantifying over them.Solution (1), where T-theoretical terms are measured by applications of a theoryT?, thus is the standard case in empirical sciences.
Solution (2) is a special case wherewe need theory T in order to measure some terms in theory T. Gadenne (1985) arguesthat this case can be understood as a tentative assumption of theory T that still makesempirical testing possible.4The important point for our discussion is that in both solutions to the problemof theoretical terms, whether we refer to another theory T?
(solution (1)) or whetherwe tentatively assume theory T (solution (2)), we require an explicit dichotomy betweenT-theoretical and T-non-theoretical terms.
This insight is crucial in the following analysisof possible circularities in the methodology of empirical CL.3.
The Problem of Theoretical Terms in Empirical CLMost machine-learning approaches can be characterized as identifying a learningproblem as a problem of estimating a prediction function f (x) for given identicallyand independently distributed (i.i.d.)
data {(xi, yi)}Ni=1 of instances and labels.
Formost approaches in empirical CL, this prediction function can be characterized by adiscriminant form of a function f wheref (x;w,?)
= argmaxyF(x, y;w,?
)and where w ?
IRD denotes a D-dimensional parameter vector, ?
(x, y) ?
IRD is aD-dimensional vector of features (also called attributes or covariates) jointly represent-ing input patterns x and outputs y (denoting categorical, scalar, or structured variables),3 For the miniature theory AS, this is done by firstly stripping out statements (4)?
(6) containing theoreticalterms, achieving a partial potential model.
Secondly statements (4) and (5) are replaced by a so-calledtheoretical extension that existentially quantifies over measuring procedures for terms like g. Theresulting Ramsey claim applies a theoretical extension to a partial potential model that also satisfiescondition (6).
Because such a statement does not contain theoretical terms we can make empiricalstatements about entities being models of the theory AS.4 Critics of the structuralist theory of science have remarked that both of the solutions are instances of amore general problem, the so-called Duhem-Quine problem, thus the focus of the structuralist programon solution (2) seems to be an exaggeration of the actual problem (von Kutschera 1982; Gadenne 1985).The Duhem-Quine thesis states that theoretical assumptions cannot be tested in isolation, but ratherwhole systems of theoretical assumptions and auxiliary assumptions are subjected to empirical testing.That is, if our predictions are not in accordance with our theory, we can only conclude that one of ourmany theoretical assumptions must be wrong, but we cannot know which one, and we can alwaysmodify our system of assumptions, leading to various ways of immunity of theories (Stegmu?ller 1986).This problem arises in Solution (1) as well as in Solution (2)238Riezler On the Problem of Theoretical Terms in Empirical CLand F measures the compatibility of pairs (x, y), for example, in the form of a lineardiscriminant function (Taskar et al.
2004; Tsochantaridis et al.
2005).5The problem of theoretical terms arises in empirical CL in cases where a singletheoretical tier is used both in manual data annotation (i.e., in the assignment of labelsy to patterns x via the encoding of data pairs (x, y)), and in feature construction (i.e., inthe association of labels y to patterns x via features ?
(x, y)).This problem can be illustrated by looking at automatic methods for data an-notation.
For example, information retrieval (IR) in the patent domain uses citationsof patents in other patents to automatically create relevance judgments for ranking(Graf and Azzopardi 2008).
Learning-to-rank models such as that of Guo and Gomes(2009) define domain knowledge features on patent pairs (e.g., same patent class in theInternational Patent Classification [IPC], same inventor, same assignee company) and IRscore features (e.g., tf-idf, cosine similarity) to represent data in a structured predictionframework.
Clearly, one could have just as well used IPC classes to create automaticrelevance judgments, and patent citations as features in the structured predictionmodel.It should also be evident that using the same criterion to automatically create relevancelabels and as feature representation would be circular.
In terms of the philosophical con-cepts introduced earlier, the theory of measurement of relevance used in data labelingcannot be the same as the theory expressed by the features of the structured predictionmodel; otherwise we exhibit the problem of theoretical terms.This problem can also arise in scenarios of manual data annotation.
One example isdata annotation by expert coders: The expert coder?s decisions of which labels to assignto which types of patterns may be guided by implicit or tacit knowledge that is sharedamong the community of experts.
These experts may apply the very same knowledge todesign features for their machine learning models.
For example, in attempts to constructsemantic annotations for machine learning purposes, the same criteria such as negationtests might be used to distinguish presupposition from entailment in the labeling ofdata, and in the construction of feature functions for a classifier to be trained and testedon these data.
Similar to the example of automatic data annotation in patent retrieval,we exhibit the problem of theoretical terms in manual data annotation by experts inthat the theory of measurement used in data annotation and feature construction isthe same.
This problem is exacerbated in the situation where a single expert annotatorcodes the data and later assumes the role of a feature designer using the ?given?
data.For example, in constructing a treebank for the purpose of learning a statistical disam-biguation model for parsing with a hand-written grammar, the same person might act indifferent roles as grammar writer, as manual annotator using the grammar?s analyses ascandidate annotations, and as feature designer for the statistical disambiguation model.The sketched scenarios are inherently circular in the sense of the problem of the-oretical terms described previously.
Thus in all cases, we are prevented from makingempirical statements.
High prediction accuracy of machine learning in such scenariosindicates high consistency in the application of implicit knowledge in different roles ofa single expert or of groups of experts, but not more.This problem of circularity in expert coding is related to the problem of reliability indata annotation, a solution to which is sought by methods for measuring and enhancinginter-annotator agreement.
A seminal paper by Carletta (1996) and a follow-up survey5 In this article, we concentrate on supervised machine learning.
Semisupervised, transductive, active,or unsupervised learning deal with machine learning from incomplete or missing labelings where thegeneral assumption of i.i.d.
data is not questioned.
See Dundar et al.
(2007) for an approach of machinelearning from non-i.i.d.
data.239Computational Linguistics Volume 40, Number 1paper by Artstein and Poesio (2008) have discussed this issue at length.
Both papersrefer to Krippendorff (2004, 1980a, page 428) who recommends that reliability data?have to be generated by coders that are widely available, follow explicit and commu-nicable instructions (a data language), and work independently of each other.
.
.
.
[T]hemore coders participate in the process and the more common they are, the more likelythey can ensure the reliability of data.?
Ironically, it seems as if the best inter-annotatoragreement is achieved by techniques that are in conflict with these recommendations,namely, by using experts (Kilgarriff 1999) or intensively trained coders (Hovy et al.
2006)for data annotation.
Artstein and Poesio (2008) state explicitly thatexperts as coders, particularly long-term collaborators, [.
.
. ]
may agree not because theyare carefully following written instructions, but because they know the purpose of theresearch very well?which makes it virtually impossible for others to reproduce theresults on the basis of the same coding scheme .
.
.
.
Practices which violate the thirdrequirement (independence) include asking the coders to discuss their judgments witheach other and reach their decisions by majority vote, or to consult with each otherwhen problems not foreseen in the coding instructions arise.
Any of these practicesmake the resulting data unusable for measuring reproducibility.
(page 575)Reidsma and Carletta (2007) and Beigman Klebanov and Beigman (2009) reach theconclusion that high inter-annotator agreement is neither sufficient nor necessary toachieve high reliability in data annotation.
The problem lies in the implicit or tacitknowledge that is shared among the community of experts.
This implicit knowledgeis responsible for the high inter-annotator agreement, but hinders reproducibility.
Ina similar way, implicit knowledge of expert coders can lead to a circularity in dataannotation and feature modeling.4.
Breaking the CircularityFinke (1979), in attempting to establish criteria for an empirical theory of linguistics,demands that the use of a single theoretical strategy to identify and describe the entitiesof interest shall be excluded from empirical analyses.
He recommends that the possibilityof using T-non-theoretic strategies to identify observations be made the defining crite-rion for empirical sciences.
That is, in order tomake an empirical statement, the two tiersof a T-theoretical and a T-non-theoretical level are necessary because the use of a singletheoretical tier prevents distinguishing empirical statements from those that are not.Let us call Finke?s requirement the criterion of T-non-theoretical grounding.6Moulines (see Balzer 1996, page 141) gives a pragmatic condition for T-non-theoreticitythat can be used as a guideline: ?Term t?
is T-non-theoretical if there exists and acknowl-edged method of determination of t?
in some theory T?
different from T plus some linkfrom T?
to T which permits the transfer of realizations of t?
from T?
into T.?Balzer (1996) discusses a variety of more formal characterizations of the notion ofT-(non-)theoretical terms.
Although the pragmatic definition cited here is rather infor-mal, it is sufficient as a guideline in discussing concrete examples and strategies to breakthe circlularity in the methodology of empirical CL.
In the following, we will exemplifyhow this criterion can be met by manual data annotation by using naive coders, or by6 Note that our criterion of T-non-theoretical grounding is related to the more specific concept ofoperationalization in social sciences (Friedrichs 1973).
Operationalization refers to the process ofdeveloping indicators of the form ?X is an a if Y is a b (at time t)?
to connect T-theoretical andT-non-theoretical levels.
We will stick with the more general criterion in the rest of this article.240Riezler On the Problem of Theoretical Terms in Empirical CLembedding data annotation into a task extrinsic to the theory to be tested, or by usingindependently created language data that are available in the wild.4.1 T-non-theoretical Grounding by Naive Coders and CrowdsourcingNow that we have defined the criterion of T-non-theoretical grounding, we see thatKrippendorff?s (2004) request for ?coders that are widely available, follow explicitand communicable instructions (a data language), and work independently of eachother?
can be regarded as a concrete strategy to satisfy our criterion.
The key is therequirement for coders to be ?widely available?
and to work on the basis of ?explicitand communicable instructions.?
The need to communicate the annotation task to non-experts serves two purposes: On the one hand, the goal of reproducibility is supportedby having to communicate the annotation task explicitly in written form.
Furthermore,the ?naive?
nature of annotators requires a verbalization in words comprehensible tonon-experts, without the option of relying on implicit or tacit knowledge that is sharedamong expert annotators.
The researcher will thus be forced to describe the annotationtask without using technical terms that are common to experts, but are not known tonaive coders.Annotation by naive coders can be achieved by using crowdsourcing services suchas Amazon?s Mechanical Turk,7 or alternatively, by creating games with a purpose (vonAhn and Dabbish 2004; Poesio et al.
2013).8 Non-expert annotations created by crowd-sourcing have been shown to provide expert-level quality if certain recommendationson experiment design and quality control are met (Snow et al.
2008).
Successful exam-ples of the use of crowdsourcing techniques for data annotation and system evaluationcan be found throughout all areas of NLP (see Callison-Burch and Dredze [2010] for arecent overview).
The main advantage of these techniques lies in the ability to achievehigh-quality annotations at a fraction of the time and the expense of expert annotation.However, a less apparent advantage is the need for researchers to provide succinctand comprehensible descriptions of Human Intelligence Tasks, and the need to breakcomplex annotation tasks down to simpler basic units of work for annotators.
Receivinghigh-quality annotations with sufficient inter-worker agreement from crowdsourcingcan be seen as a possible litmus test for a successful T-non-theoretical grounding ofcomplex annotation tasks.
Circularity issues will vanish because T-theoretical termscannot be communicated directly to naive coders.4.2 Grounding by Extrinsic Evaluation and Task-Related AnnotationAnother way to achieve T-non-theoretical grounding is extrinsic evaluation of NLPsystems.
This type of evaluation assesses ?the effect of a system on something thatis external to it, for example, the effect on human performance at a given task orthe value added to an application?
(Belz 2009) and has been demanded for at least20 years (Spa?rck Jones 1994).
Extrinsic evaluation is advertised as a remedy against?closed problem?
approaches (Spa?rck Jones 1994) or against ?closed circles?
in intrinsicevaluation where system rankings produced by automatic measures are compared withhuman rankings which are themselves unfalsifiable (Belz 2009).7 http://www.mturk.com.8 See Fort, Adda, and Cohen (2011) for a discussion of the ethical dimensions of crowdsourcing servicesand their alternatives.241Computational Linguistics Volume 40, Number 1An example of an extrinsic evaluation in NLP is the evaluation of the effect ofsyntactic parsers on retrieval quality in a biomedical IR task (Miyao et al.
2008).
In-terestingly, the extrinsic set-up revealed a different system ranking than the standardintrinsic evaluation, according to F-scores on the Penn WSJ corpus.
Another exampleis the area of clustering.
Deficiencies in current intrinsic clustering evaluation methodshave led von Luxburg, Williamson, and Guyon (2012) to pose the question ?Clustering:Science or Art??.
They recommend to measure the usefulness of a clustering method fora particular task under consideration, that is, to always study clustering in the contextof its end use.Extrinsic scenarios are not only useful for the purpose of evaluation.
Rather, everyextrinsic evaluation creates data that can be used as training data for another learningtask (e.g., rankings of system outputs with respect to an extrinsic task can be used totrain discriminative (re)ranking models).
For example, Kim and Mooney (2013) usethe successful completion of navigation tasks to create training data for rerankingin grounded language learning.
Nikoulina et al.
(2012) use retrieval performance oftranslated queries to create data for reranking in statistical machine translation.
Clarkeet al.
(2010) use the correct response for a query to a database of geographical facts toselect data for structured learning of a semantic parser.
Thus the extrinsic set-up canbe seen as a general technique for T-non-theoretical grounding in training as well asin testing scenarios.
Circularity issues will not arise in extrinsic set-ups because theextrinsic task is by definition external to the system outputs to be tested or ranked.4.3 Grounded Data in the WildHalevy, Norvig, and Pereira (2009, page 8) mention statistical speech recognitionand statistical machine translation as ?the biggest successes in natural-language-relatedmachine learning.?
This success is due to the fact that ?a large training set of the input?output behavior that we seek to automate is available to us in the wild.?
While they em-phasize the large size of the training set, we think that the aspect that the training dataare given as a ?natural task routinely done every day for a real human need?
(Halevy,Norvig, and Pereira 2009), is just as important as the size of the training set.
This isbecause a real-world task that is extrinsic and independent of any scientific theoryavoids any methodological circularity in data annotation and enforces an application-based evaluation.Speech and translation are not the only lucky areas where data are available in thewild.
Other data sets that have been ?found?
by NLP researchers are IMDb moviereviews (exploited for sentiment analysis by Pang, Lee, and Vaithyanathan [2002]),Amazon product reviews (used for multi-domain sentiment analysis by Blitzer, Dredze,and Pereira [2007]), Yahoo!
Answers (used for answer ranking by Surdeanu, Ciaramita,and Zaragoza [2008]), reading comprehension tests (used for automated reading com-prehension by Hirschman et al.
[1999]), or Wikipedia (with too many uses to cite).
Mostof these data were created by community-based efforts.
This means that the data setsare freely available and naturally increasing.The extrinsic and independent aspect of data in the wild can also be created incrowdsourcing approaches that enforce a distinction between data annotation tasksand scientific modeling.
For example, Denkowski, Al-Haj, and Lavie (2010) usedAmazon?s Mechanical Turk to create reference translations for statistical machine trans-lation by monolingual phrase substitutions on existing references.
?Translations?
cre-ated by workers that paraphrase given references without knowing the source cannever lead to the circularity that data annotation by experts is susceptible to.
In a242Riezler On the Problem of Theoretical Terms in Empirical CLscenario of monolingual paraphrasing for reference translations even inter-annotatoragreement is not an issue anymore.
Data created by single annotators (e.g., monolingualmeaning equivalents created for bilingual purposes [Dreyer and Marcu 2012]), can betreated as ?given?
data for machine learning purposes, even if each network of meaningequivalences is created by a single annotator.5.
ConclusionIn this article, we have argued that the problem of theoretical terms as identified fortheoretical physics can occur in empirical CL in cases where data are not ?given?
ascommonly assumed in machine learning.
We exemplified this problem on the exampleof manual data annotation by experts, where the task of relating instances to labels inmanual data annotation and the task of relating instances to labels via modeling fea-ture functions are intertwined.
Inspired by the structuralist theory of science, we havedefined a criterion of T-non-theoretical grounding and exemplified how this criterioncan be met by manual data annotation by using naive coders, or by embedding dataannotation into a task extrinsic to the theory to be tested, or by using independentlycreated language data that are available in the wild.Our suggestions for T-non-theoretical grounding are related to work on groundedlanguage learning that is based on weak supervision in the form of the use of sentencesin naturally occurring contexts.
For example, the meaning of natural language express-sions can be grounded in visual scenes (Roy 2002; Yu and Ballard 2004; Yu and Siskind2013) or actions in games or navigation tasks (Chen and Mooney 2008, 2011).
Becauseof the ambiguous supervision, most such approaches work with latent representationsand use unsupervised techniques in learning.
Our suggestions for T-non-theoreticalgrounding can be used to avoid circularities in standard supervised learning.
We thinkthat this criterion should be considered a necessary condition for an empirical science,in addition to ensuring reliability of measurements.
Our negligence of related issuessuch as validity of measurements (see Krippendorff 1980b) shows that there is a vastmethodological area to be explored, perhaps with further opportunity for guidance byphilosophy of science.AcknowledgmentsWe are grateful for feedback on earlierversions of this work from Sebastian Pado?,Artem Sokolov, and Katharina Wa?schle.Furthermore, we would like to thank PaolaMerlo for her suggestions andencouragement.ReferencesArtstein, Ron and Massimo Poesio.
2008.Inter-coder agreement for computationallinguistics.
Computational Linguistics,34(4):555?596.Balzer, Wolfgang.
1996.
Theoretical terms:Recent developments.
In WolfgangBalzer and C. Ulises Moulines, editors,Structuralist Theory of Science.
FocalIssues, New Results.
de Gruyter,pages 139?166.Balzer, Wolfgang and C. Ulises Moulines,editors.
1996.
Structuralist Theory of Science.Focal Issues, New Results.
de Gruyter.Beigman Klebanov, Beata and Eyal Beigman.2009.
From annotator agreement to noisemodels.
Computational Linguistics,35(4):495?503.Belz, Anja.
2009.
That?s nice ... what can youdo with it?
Computational Linguistics,35(1):111?118.Blitzer, John, Mark Dredze, and FernandoPereira.
2007.
Biographies, Bollywood,boom-boxes and blenders: Domainadaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics(ACL?07), pages 440?447, Prague.Callison-Burch, Chris and Mark Dredze.2010.
Creating speech and languagedata with Amazon?s Mechanical Turk.In Proceedings of the NAACL-HLT 2010243Computational Linguistics Volume 40, Number 1Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk,pages 1?12, Los Angeles, CA.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22(2):1?6.Chen, David L. and Raymond J. Mooney.2008.
Learning to sportscast: A testof grounded language learning.In Proceedings of the 25th InternationalConference on Machine Learning (ICML?08),pages 128?135, Helsinki.Chen, David L. and Raymond J. Mooney.2011.
Learning to interpret naturallanguage navigation instructions fromobservations.
In Proceedings of the25th AAAI Conference on ArtificialIntelligence (AAAI?11), pages 859?866,San Francisco, CA.Clarke, James, Dan Goldwasser, Wing-WeiChang, and Dan Roth.
2010.
Drivingsemantic parsing from the world?sresponse.
In Proceedings of the 14thConference on Natural Language Learning(CoNLL?10), pages 18?27, Uppsala.Corfield, David, Bernhard Scho?lkopf, andVladimir Vapnik.
2009.
Falsificationismand statistical learning theory: Comparingthe Popper and Vapnik-Chervonenkisdimensions.
Journal for General Philosophyof Science, 40:51?58.Denkowski, Michael, Hassan Al-Haj,and Alon Lavie.
2010.
Turker-assistedparaphrasing for English-Arabicmachine translation.
In Proceedings ofthe NAACL-HLT 2010 Workshop onCreating Speech and Language Data withAmazon?s Mechanical Turk, pages 66?70,Los Angeles, CA.Dreyer, Markus and Daniel Marcu.
2012.HyTER: Meaning-equivalent semanticsfor translation evaluation.
In Proceedings of2012 Conference of the North AmericanChapter of the Association for ComputationalLinguistics: Human Language Technologies(NAACL-HLT 2012), pages 162?171,Montreal.Dundar, Murat, Balaji Krishnapuram, JinboBi, and R. Bharat Rao.
2007.
Learningclassifiers when the training data is notIID.
In Proceedings of the 20th InternationalJoint Conference on Artifical Intelligence(IJCAI?07), pages 756?761, Hyderabad.Finke, Peter.
1979.
Grundlagen einerlinguistischen Theorie.
Empirie undBegru?ndung in der Sprachwissenschaft.Vieweg.Fort, Kare?n, Gilles Adda, and K. BretonnelCohen.
2011.
Amazon Mechanical Turk:Gold mine or coal mine?
ComputationalLinguistics, 37(2):413?420.Friedrichs, Ju?rgen.
1973.
Methoden empirischerSozialforschung.
Opladen, WestdeutscherVerlag, 14th (1990) edition.Gadenne, Volker.
1985.
Theoretische Begriffeund die Pru?fbarkeit von Theorien.Zeitschrift fu?r allgemeineWissenschaftstheorie, XVI(1):19?24.Graf, Erik and Leif Azzopardi.
2008.A methodology for building a patenttest collection for prior art search.
InProceedings of the 2nd International Workshopon Evaluating Information Access (EVIA),pages 60?71, Tokyo.Guo, Yunsong and Carla Gomes.
2009.Ranking structured documents: A largemargin based approach for patentprior art search.
In Proceedings of theInternational Joint Conference on ArtificialIntelligence (IJCAI?09), pages 1,058?1,064,Pasadena, CA.Halevy, Alon, Peter Norvig, and FernandoPereira.
2009.
The unreasonableeffectiveness of data.
IEEE IntelligentSystems, 24:8?12.Hirschman, Lynette, Marc Light, Eric Breck,and John D. Burger.
1999.
Deep read:A reading comprehension system.In Proceedings of the 37th Annual Meetingof the Association for ComputationalLinguistics (ACL?99), pages 325?332,College Park, MD.Hovy, Eduard, Mitchell Marcus, MarthaPalmer, Lance Ramshaw, and RalphWeischedel.
2006.
Ontonotes: The 90%solution.
In Proceedings of the HumanLanguage Technology Conference of theNorth American Chapter of the ACL(HLT-NAACL?06), pages 57?60,New York, NY.Kilgarriff, Adam.
1999.
95% replicability formanual word sense tagging.
In Proceedingsof the Ninth Conference of the EuropeanChapter of the Association for ComputationalLinguistics (EACL?99), pages 277?278,Bergen.Kim, Joohyun and Raymond J. Mooney.2013.
Adapting discriminative rerankingto grounded language learning.In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics(ACL?13), pages 218?277, Sofia.Korb, Kevin.
2004.
Introduction: Machinelearning as philosophy of science.
Mindsand Machines, 14(4):1?7.Krippendorff, Klaus.
1980a.
Content Analysis.An Introduction to Its Methodology.
Sage,third (2013) edition.244Riezler On the Problem of Theoretical Terms in Empirical CLKrippendorff, Klaus.
1980b.
Validityin content analysis.
In EkkehardMochmann, editor, Computerstrategienfu?r die Kommunikationsanalyse.
Campus,pages 69?112.Krippendorff, Klaus.
2004.
Reliabilityin content analysis: Some commonmisconceptions and recommendations.Human Communication Research,30(3):411?433.Miyao, Yusuke, Rune Saetre, Kenji Sagae,Takuya Matsuzaki, and Jun?ichi Tsujii.2008.
Task-oriented evaluation ofsyntactic parsers and their representations.In Proceedings of the 46th AnnualMeeting of the Association forComputational Linguistics: HumanLanguage Technologies (ACL-HLT?08),pages 46?54, Columbus, OH.Nikoulina, Vassilina, Bogomil Kovachev,Nikolaos Lagos, and Christof Monz.
2012.Adaptation of statistical machinetranslation model for cross-lingualinformation retrieval in a service context.In Proceedings of the 13th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL?12),pages 109?119, Avignon.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?Sentiment classification using machinelearning techniques.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing (EMNLP?02),pages 79?86, Philadelphia, PA.Poesio, Massimo, Jon Chamberlain, UdoKruschwitz, Livio Robaldo, and LucaDucceschi.
2013.
Phrase detectives:Utilizing collective intelligence forInternet-scale language resource creation.ACM Transactions on Interactive IntelligentSystems, 3(1):Article 3.Reidsma, Dennis and Jean Carletta.
2007.Reliability measurements without limits.Computational Linguistics, 34(3):319?326.Roy, Deb K. 2002.
Learning visuallygrounded words and syntax for a scenedescription task.
Computer Speech andLanguage, 16:353?385.Sneed, Joseph D. 1971.
The Logical Structureof Mathematical Physics.
D. Reidel.Snow, Rion, Brendan O?Connor, DanielJurafsky, and Andrew Y. Ng.
2008.
Cheapand fast?but is it good?
Evaluatingnon-expert annotations for naturallanguage tasks.
In Proceedings of theConference on Empirical Methods inNatural Language Processing (EMNLP?08),pages 254?263, Edinburgh.Spa?rck Jones, Karen.
1994.
Towards betterNLP system evaluation.
In Proceedings ofthe Workshop on Human Language Technology(HLT?94), pages 102?107, Plainsboro, NJ.Stegmu?ller, Wolfgang.
1979.
The StructuralistView of Theories.
A Possible Analogue of theBourbaki Programme in Physical Science.Springer.Stegmu?ller, Wolfgang.
1986.
Probleme undResultate der Wissenschaftstheorie undAnalytischen Philosophie.
Band II: Theorieund Erfahrung.
Springer.Surdeanu, Mihai, Massimiliano Ciaramita,and Hugo Zaragoza.
2008.
Learning torank answers on large online QAcollections.
In Proceedings of the 46th AnnualMeeting of the Association for ComputationalLinguistics (ACL?08), pages 719?727,Columbus, OH.Taskar, Ben, Dan Klein, Michael Collins,Daphne Koller, and Christopher Manning.2004.
Max-margin parsing.
In Proceedingsof the 2004 Conference on Empirical Methodsin Natural Language Processing (EMNLP?04),pages 1?8, Barcelona.Tsochantaridis, Ioannis, Thorsten Joachims,Thomas Hofmann, and Yasemin Altun.2005.
Large margin methods for structuredand interdependent output variables.Journal of Machine Learning Research,5:1453?1484.von Ahn, Luis and Laura Dabbish.
2004.Labeling images with a computer game.In Proceedings of the Conference on HumanFactors in Computing Systems (CHI?04),pages 319?326, Vienna.von Kutschera, Franz.
1982.
Grundfragen derErkenntnistheorie.
de Gruyter.von Luxburg, Ulrike, Robert C. Williamson,and Isabelle Guyon.
2012.
Clustering:Science or art?
In Proceedings of the ICML2011 Workshop on Unsupervised and TransferLearning, pages 1?12, Bellevue, WA.Yu, Chen and Dana H. Ballard.
2004.
Onthe integration of grounding languageand learning objects.
In Proceedings of the19th National Conference on ArtificialIntelligence (AAAI?04), pages 488?493,San Jose, CA.Yu, Haonan and Jeffrey Mark Siskind.
2013.Grounded language learning from videodescribed with sentences.
In Proceedings ofthe 51st Annual Meeting of the Associationfor Computational Linguistics (ACL?13),pages 53?63, Sofia.245
