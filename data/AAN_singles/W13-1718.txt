Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 140?145,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsCognate and Misspelling Features for Natural Language IdentificationGarrett Nicolai, Bradley Hauer, Mohammad Salameh, Lei Yao, Grzegorz KondrakDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, Canada{nicolai,bmhauer,msalameh,lyao1,gkondrak}@ualberta.caAbstractWe apply Support Vector Machines to differ-entiate between 11 native languages in the2013 Native Language Identification SharedTask.
We expand a set of common languageidentification features to include cognate inter-ference and spelling mistakes.
Our best resultsare obtained with a classifier which includesboth the cognate and the misspelling features,as well as word unigrams, word bigrams, char-acter bigrams, and syntax production rules.1 IntroductionAs the world becomes more inter-connected, an in-creasing number of people devote effort to learn-ing one of the languages that are dominant in theglobal community.
English, in particular, is stud-ied in many countries across the globe.
The goal isoften related to increasing one?s chances to obtainemployment and succeed professionally.
The lan-guage of work-place communication is often not aspeaker?s native language (L1) but their second lan-guage (L2).
Speakers and writers of the same L1can sometimes be identified by similar L2 errors.The weak Contrastive Analysis Hypothesis (Jarvisand Crossley, 2012) suggests that these errors maybe a result of L1 causing linguistic interference; thatis, common tendencies of a speaker?s L1 are super-imposed onto their L2.
Native Language Identifi-cation, or NLI, is an attempt to exploit these errorsin order to identify the L1 of the speaker from textswritten in L2.Our group at the University of Alberta was unfa-miliar with the NLI research prior to the announce-ment of a shared task (Tetreault et al 2013).
How-ever, we saw it as an opportunity to apply our exper-tise in character-level NLP to a new task.
Our goalwas to propose novel features, and to combine themwith other features that have been previously shownto work well for language identification.In the end, we managed to define two feature setsthat are based on spelling errors made by L2 writers.Cognate features relate a spelling mistake to cognateinterference with the writer?s L1.
Misspelling fea-tures identify common mistakes that may be indica-tive of the writer?s L1.
Both feature sets are meantto exploit the Contrastive Analysis Hypothesis, andbenefit from the writer?s L1 influence on their L2writing.2 Related WorkKoppel et al(2005b) approach the NLI task usingSupport Vector Machines (SVMs).
They experi-ment with features such as function-word unigrams,rare part-of-speech bigrams, character bigrams, andspelling and syntax errors.
They report 80% accu-racy across 5 languages.
We further investigate therole of word unigrams and spelling errors in nativelanguage identification.
We consider not only func-tion words, but also content words, as well as wordbigrams.
We also process spell-checking errors witha text aligner to find common spelling errors amongwriters with the same L1.Tsur and Rappoport (2007) also use SVMs on theNLI task, but limit their feature set to character bi-grams.
They report 65% accuracy on 5 languages,and hypothesize that the choice of words when writ-ing in L2 is strongly affected by the phonology of140their L1.
We also consider character bigrams in ourfeature set, but combine them with a number of otherfeatures.Wong and Dras (2011) opt for a maximum en-tropy classifier, and focus more on syntax errors thanlexical errors.
They find that syntax tree productionrules help their classifier in a seven language clas-sification task.
They only consider non-lexicalizedrules, and rules with function words.
In contrast, weconsider both lexicalized and non-lexicalized pro-duction rules, and we include content words.Bergsma et al(2012) consider the NLI task as asub-task of the authorship attribution task.
They fo-cus on the following three questions: (1) whether thenative language of the writer of a paper is English,(2) what is the gender of the writer, and (3) whethera paper is a conference or workshop paper.
The au-thors conclude that syntax aids the native languageclassification task, further motivating our decision touse part-of-speech n-grams and production rules asfeatures for our classifier.
Furthermore, the authorssuggest normalizing text to reduce sparsity, and im-plement several meta-features that they claim aid theclassification.3 ClassifierFollowing Koppel et al(2005b) and others, weperform classification with SVMs.
We chose theSVM-Multiclass package, a version of the SVM-light package(Joachims, 1999) specifically modifiedfor multi-class classification problems.
We use a lin-ear kernel, and two hyperparameters that were tunedon the development set: the c soft-margin regular-ization parameter, which measures the tradeoff be-tween training error and the size of the margin, and, which is used as a stopping criterion for the SVM.C was tuned to a value of 5000, and epsilon to avalue of 0.1.4 FeaturesAs features for our SVM, we used a combination offeatures common in the literature and new featuresdeveloped specifically for this task.
The features arelisted in the following section.4.1 Word n-gramsFollowing previous work, we use word n-grams asthe primary feature set.
We normalize the text beforeselecting n-grams using the method of Bergsma etal.
(2012).
In particular, all digits are replaced witha representative ?0?
character; for example, ?22?
and?97?
are both represented as ?00?.
However, unlikeKoppel et al(2005b), we incorporate word bigramsin addition to word unigrams, and utilize both func-tion words and content words.4.1.1 Function WordsUsing a list of 295 common function words, wereduce each document to a vector of values repre-senting their presence or absence in a document.
Allother tokens in the document are ignored.
Whenconstructing vectors of bigrams, any word that is noton the list of function words is converted to a place-holder token.
Thus, most of our function-word bi-grams consist of a single function word preceded orfollowed by a placeholder token.4.1.2 Content WordsOther than the normalization mentioned in Sec-tion 4.1, all tokens in the documents are allowed aspossible word unigrams.
No spelling correction isused for reducing the number of word n-grams.
Fur-thermore, we consider all token unigrams that occurin the training data, regardless of their frequency.An early concern with token bigrams was thatthey were both large in number, and sparse.
In anattempt to reduce the number of bigrams, we con-ducted experiments on the development set with dif-ferent numbers of bigrams that exhibited the highestinformation gain.
It was found that using all combi-nations of word bigrams improved predictive accu-racy the most, and did not lead to a significant costto the SVM.
Thus, for experiments on the test set, alltoken bigrams that were encountered in the trainingset were used as features.4.2 Character n-gramsFollowing Tetreault et al(2012), we utilize all char-acter bigrams that occur in the training data, ratherthan only the most frequent ones.
However, wherethe literature uses either binary indicators or relativefrequency of bigrams as features, we use a modi-fied form of the relative frequency in our classifier.141In a pre-processing step, we calculate the averagefrequency of each character bigram across all train-ing documents.
Then, during feature extraction, weagain determine the relative frequency of each char-acter bigram across documents.
We then use bi-nary features to indicate if the frequency of a bigramis higher than the average frequency.
Experimentsconducted on the development set showed that al-though this modified frequency was out-performedby the original relative frequency on its own, ourmethod performed better when further features wereincorporated into the classifier.4.3 Part-of-speech n-gramsAll documents are tagged with POS tags using theStanford parser (Klein and Manning, 2003), Fromthe documents in the training data, a list of all POSbigrams was generated, and documents were repre-sented by binary indicators of the presence or ab-sence of a bigram in the document.
As with char-acter bigrams, we did not simply use the most com-mon bigrams, but rather considered all bigrams thatappeared in the training data.4.4 Syntax Production RulesAfter generating syntactic parse trees with the Stan-ford Parser.
we extract all possible production rulesfrom each document, including lexicalized rules.The features are binary; if a production rule occursin an essay, its value is set to 1, and 0 otherwise.
Foreach language, we use information gain for featureselection to select the most informative productionrules as suggested by Wong and Dras (2011).
Ex-periments on the development set indicated that theinformation gain is superior to raw frequency for thepurpose of syntax feature selection.
Since the accu-racy increased as we added more production rules,the feature set for final testing includes all produc-tion rules encountered in the training set.
The ma-jority of the rules are of the form POS?
terminal.We hypothesized that most of the information con-tained in these rules may be already captured by theword unigram features.
However, experiments onthe development set suggested that the lexicalizedrules contain information that is not captured by theunigrams, as they led to an increase in predictive ac-curacy.4.5 Spelling ErrorsKoppel et al(2005a) suggested spelling errorscould be helpful as writers might be affected bythe spelling convention in their native languages.Moreover, spelling errors also reflect the pronun-ciation characteristics of the writers?
native lan-guages.
They identified 8 types of spelling errorsand collected the statistics of each error type astheir features.
Unlike their approach, we focus onthe specific spelling errors made by the writers be-cause 8 types may be insufficient to distinguish thespelling characteristics of writers from 11 differ-ent languages.
We extract the spelling error fea-tures from character-level alignments between themisspelled word and the intended word.
For ex-ample, if the word abstract is identified as the in-tended spelling of a misspelling abustruct, the char-acter alignments are as follows:a bu s t ru ct| | | | | |a b s t ra ctOnly the alignments of the misspelled parts, i.e.
(bu,b) and (ru,ra) in this case, are used as fea-tures.
The spell-checker we use is aspell1, and thecharacter-level alignments are generated by m2m-aligner (Jiampojamarn et al 2007).4.6 Cognate InterferenceCognates are words that share their linguistic origin.For example, English become and German bekom-men have evolved from the same word in a com-mon ancestor language.
Other cognates are wordsthat have been transfered between languages; for ex-ample, English system comes from the Greek word???????
via Latin and French.
On average, pairsof cognates exhibit higher orthographic similaritythan unrelated translation pairs (Kondrak, 2013).Cognate interference may cause an L1-speakerto use a cognate word instead of a correct Englishtranslation (for example, become instead of get).Another instance of cognate interference is mis-spelling of an English word under the influence ofthe L1 spelling (Table 1).We aim to detect cognate interference by identi-fying the cases where the cognate word is closer to1http://aspell.net142Misspelling Intended Cognatedevelopped developed developpe?
(Fre)exemple example exemple (Fre)organisation organization organisation (Ger)conzentrated concentrated konzentrierte (Ger)comercial commercial comercial (Spa)sistem system sistema (Spa)Table 1: Examples of cognate interference in the data.the misspelling than to the intended word (Figure 1).We define one feature to represent each language L,for which we could find a downloadable bilingualEnglish-L dictionary.
We use the following algo-rithm:1.
For each misspelled English word m found ina document, identify the most likely intendedword e using a spell-checking program.2.
For each language L:(a) Look up the translation f of the intendedword e in language L.(b) Compute the orthographic edit distance Dbetween the words.
(c) If D(e, f) < t then f is assumed to be acognate of e.(d) If f is a cognate and D(m, f) < D(e, f)then we consider it as a clue that L = L1.We use a simple method of computing ortho-graphic distance with threshold t = 0.58 definedas the baseline method by Bergsma and Kondrak(2007).
However, more accurate methods of cog-nate identification discussed in that paper could alsobe used.Misspellings can betray cognate interference evenif the misspelled word has no direct cognate inlanguage L1.
For example, a Spanish speakermight spell the word quick as cuick because ofthe existence of numerous cognates such as ques-tion/cuestio?n.
Our misspelling features can detectsuch phenomena at the character level; in this case,qu:cu corresponds to an individual misspelling fea-ture.4.7 Meta-featuresWe included a number of document-specific meta-features as suggested by Bergsma et al(2012): theconzentratedconcentrated konzentrierte0.30.4Figure 1: A cognate word influencing the spelling.average number of words per sentence, the averageword length, as well as the total number of char-acters, words, and sentences in a document.
Wereasoned that writers from certain linguistic back-grounds may prefer many short sentences, whileother writers may prefer fewer but longer sentences.Similarly, a particular linguistic background may in-fluence the preference for shorter or longer words.5 ResultsThe dataset used for experiments was the TOEFL11Non-Native English Corpus (Blanchard et al 2013).The dataset was split into three smaller datasets: theTraining set, consisting of 9900 essays evenly dis-tributed across 9 languages, the Development set,which contained a further 1100 essays, and the Testset, which also contained 1100 essays.
As the datahad a staggered release, we used the data differently.We further split the Training set, with a split of 80%for training, and 10% for development and testing.We then used the Development set as a held-out testset.
For held-out testing, the classifier was trained onall data in the Training set, and for final testing, theclassifier was trained on all data in both the Trainingand Development sets.We used four different combinations of featuresfor our task submissions.
The results are shown inTable 2.
We include the following accuracy values:(1) the results that we obtained on the Developmentset before the Test data release, (2) the official Testset results provided by the organizers (Tetreault etal., 2013), (3) the actual Test set results, and (4) themean cross-validation results (for submissions 1 and3).
The difference between the official and the ac-tual Test set results is attributed to two mistakes inour submissions.
In submission 1, the feature listsused for training and testing did not match.
In sub-missions 3 and 4, only non-lexicalized syntax pro-duction rules were used, whereas our intention wasto use all of them.143No.
Features Dev Org Test CV1 Base 82.0 61.2 80.4 58.22 ?
cont.
words 67.4 68.7 68.7 ?3 + char 81.4 80.3 81.7 58.54 + char + meta 81.2 80.0 80.8 ?Table 2: Accuracy of our submissions.All four submissions used the following basecombination of features:?
word unigrams?
word bigrams?
error alignments?
syntax production rules?
word-level cognate interference featuresIn addition, submission 3 includes character bi-grams, while submission 4 includes both characterbigrams and meta-features.
In submission 2, onlyfunction words are used, with the exclusion of con-tent words.Our best submission, which achieves 81.73% ac-curacy on the Test set, includes all features discussedin Section 4 except POS bigrams.
Early tests in-dicated that any gains obtained with POS bigramswere absorbed by the production rules, so they wereexcluded form the final experiments.
Character bi-grams help on the Test set but not on the Devel-opment set.
The meta-features decrease accuracyon both sets.
Finally, the content words dramati-cally improve accuracy.
The reason we included asubmission which did not use content words is thatit is a common practice in previous work.
In ouranalysis of the data, we found content words thatwere highly indicative of the language of the writer.Particularly, words and phrases which contained thespeaker?s home country were useful in predicting thelanguage.
It should be noted that this correspon-dence may be dependent upon the prompt given tothe writer.
Furthermore, it may lead to false posi-tives for L1 speakers who live in multi-lingual coun-tries.5.1 Confusion MatrixWe present the confusion matrix for our best submis-sion in Table 5.1.
The highest number of incorrectA C F G H I J K S T TuARA 83 0 0 0 2 2 2 1 4 5 1CHI 1 81 2 0 1 0 8 6 1 0 0FRE 6 0 82 2 1 3 0 0 1 0 5GER 1 0 0 90 1 1 1 0 2 0 4HIN 1 2 2 0 76 1 0 0 0 16 2ITA 1 1 0 1 0 89 1 0 5 1 1JPN 2 1 1 1 0 1 86 6 0 0 2KOR 1 8 0 0 0 0 11 78 0 1 1SPA 2 2 7 0 3 5 0 2 75 0 4TEL 2 0 0 2 15 0 0 0 1 80 0TUR 4 3 2 1 0 1 1 5 2 2 79Table 3: Confusion Matrix for our best classifier.Features TestFull system 81.7w/o error alignments 81.3w/o word unigrams 81.1w/o cognate features 81.0w/o production rules 80.6w/o character bigrams 80.4w/o word bigrams 76.7Table 4: Accuracy of various feature combinations.classifications are between languages that are eitherlinguistically or culturally related (Jarvis and Cross-ley, 2012).
For example, Korean is often misclassi-fied as Japanese or Chinese.
The two languages arenot linguistically related to Korean, but both havehistorically had cultural ties with Korean.
Likewise,while Hindi and Telugu are not related linguistically,they are both spoken in the same geographic area,and speakers are likely to have contact with eachother.5.2 Ablation StudyTable 4 shows the results of an ablation experimenton our best-performing submission.
The word bi-grams contribute the most to the classification; theirremoval increases the relative error rate by 27%.
Theword unigrams contribute much less., This is un-surprising, as much of the information contained inthe word unigrams is also contained in the bigrams.The remaining features are also useful.
In particu-lar, our cognate interference features, despite apply-ing to only 4 of 11 languages, reduce errors by about4%.1446 Conclusions and Future WorkWe have described the system that we have devel-oped for the NLI 2013 Shared Task.
The systemcombines features that are prevalent in the litera-ture with our own novel character-level spelling fea-tures and word cognate interference features.
Mostof the features that we experimented with appearto increase the overall accuracy, which contradictsthe view that simple bag-of-words usually performbetter than more complex feature sets (Sebastiani,2002).Our cognate features can be expanded by includ-ing languages that do not use the Latin script, suchas Russian and Greek, as demonstrated by Bergsmaand Kondrak (2007).
We utilized bilingual dictio-naries representing only four of the eleven languagesin this task2; yet our cognate interference featuresstill improved classifier accuracy.
With more re-sources and with better methods of cognate identi-fication, the cognate features have the potential tofurther contribute to native language identification.Our error-alignment features can likewise be fur-ther investigated in the future.
Currently, after ana-lyzing texts with a spell-checker, we automaticallyaccept the first suggestion as the correct one.
Inmany cases, this leads to faulty corrections, and mis-leading alignments.
By using context sensitive spell-checking, we can choose better corrections, and ob-tain information which improves classification.This shared task was a wonderful introductionto Native Language Identification, and an excellentlearning experience for members of our group,ReferencesShane Bergsma and Grzegorz Kondrak.
2007.Alignment-based discriminative string similarity.
InProceedings of the 45th Annual Meeting of the Associ-ation for Computational Linguistics, pages 656?663.Shane Bergsma, Matt Post, and David Yarowsky.
2012.Stylometric analysis of scientific articles.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 327?337,Montre?al, Canada.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: A2French, Spanish, German, and Italian.Corpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Scott Jarvis and Scott Crossley, editors.
2012.
Approach-ing Language Transfer Through Text Classification:Explorations in the Detection-based Approach, vol-ume 64.
Multilingual Matters Limited, Bristol, UK.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand HMMs to letter-to-phoneme conversion.
In Pro-ceedings of NAACL-HLT, pages 372?379.Thorsten Joachims.
1999.
Making large-scale supportvector machine learning practical.
In Advances in ker-nel methods, pages 169?184.
MIT Press.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.Grzegorz Kondrak.
2013.
Word similarity, cognation,and translational equivalence.
To appear.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005a.Automatically determining an anonymous author?s na-tive language.
Intelligence and Security Informatics,pages 41?76.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005b.Determining an author?s native language by mining atext for errors.
In Proceedings of the eleventh ACMSIGKDD international conference on Knowledge dis-covery in data mining, pages 624?628, Chicago, IL.ACM.Fabrizio Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM computing surveys(CSUR), 34(1):1?47.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost andfound: Resources and empirical evaluations in nativelanguage identification.
In Proceedings of COLING2012, pages 2585?2602, Mumbai, India.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A report on the first native language identificationshared task.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications, Atlanta, GA, USA.Oren Tsur and Ari Rappoport.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Proceed-ings of the Workshop on Cognitive Aspects of Com-putational Language Acquisition, pages 9?16, Prague,Czech Republic.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploit-ing parse structures for native language identification.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, UK.145
