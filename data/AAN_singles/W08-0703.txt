Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 12?19,Columbus, Ohio, USA June 2008. c?2008 Association for Computational LinguisticsA Bayesian model of natural language phonology:generating alternations from underlying formsDavid Ellisde@cs.brown.eduBrown UniversityProvidence, RI 02912AbstractA stochastic approach to learning phonology.The model presented captures 7-15% morephonologically plausible underlying formsthan a simple majority solution, because itprefers ?pure?
alternations.
It could be use-ful in cases where an approximate solution isneeded, or as a seed for more complex mod-els.
A similar process could be involved insome stages of child language acquisition; inparticular, early learning of phonotactics.1 IntroductionSound changes in natural language, such as stemvariation in inflected forms, can be described asphonological processes.
These are governed by aconstraint hierarchy as in Optimality Theory (OT),or by a set of ordered rules.
Both rely on a sin-gle lexical representation of each morpheme (i.e., itsunderlying form), and context-sensitive transforma-tions to surface forms.
Phonological changes oftenaffect segments near morpheme boundaries, but canalso apply over an entire prosodic word, as in vowelharmony.It does not seem straightforward to incorporatecontext into a Bayesian model of phonology, al-though a clever solution may yet be found.
Astandard way of incorporating conditioning envi-ronments is to treat them as factors in a Gibbsmodel (Liang and Klein, 2007), but such modelsrequire an explicit calculation of the partition func-tion.
Unless the rule contexts possess some kind oflocality, we don?t know how to compute this par-tition function efficiently.
Some context could becaptured by generating underlying phonemes froman n-gram model, or by annotating surface formswith neighborhood features.
However, the effects ofautosegmental phonology and other long-range de-pendencies (like vowel harmony) cannot be easilyBayesianized.1.1 Related WorkIn the last decade, finite-state approaches to phonol-ogy (Gildea and Jurafsky, 1996; Beesley and Kart-tunen, 2000) have effectively brought theoretical lin-guistic work on rewrite rules into the computationalrealm.
A finite-state approximation of optimalitytheory (Karttunen, 1998) was later refined into acompact treatment of gradient constraints (Gerde-mann and van Noord, 2000).Recent work on Bayesian models of morpholog-ical segmentation (Johnson et al, 2007) could becombined with phonological rule induction (Gold-water and Johnson, 2004) in a variety of ways,some of which will be explored in our discussionof future work.
Also, the Hierarchical Bayes Com-piler (Daume III, 2007) could be used to generate amodel similar to the one presented here, but less con-strained1 which makes correspondingly more ran-dom, less accurate predictions.1.2 DatasetAs we describe the model and its implementation inthis and subsequent sections, we will refer to a sam-1Recent updates to HBC, inspired by discussions with theauthor, have addressed some of these limitations.12ple dataset (in Figure 1), consisting of a paradigm2of verb stems and person/number suffixes.
Thehead of each row or column is an /underlying/ form,which in 3rd person singular is a phonologically nullsegment (represented as /?/).
In [surface] forms, therealization of each morpheme is affected by phono-logical processes.
For example, in the combinationof /tieta?/ + /vat/, the result is [tieta?+va?t], where the3rd person plural /a/ becomes [a?]
due to vowel har-mony.1.3 Bayesian ApproachAs a baseline model, we select the most frequentlyoccurring allophone as the underlying form.
Ourgoal is to outperform this baseline using a Bayesianmodel.
In other words, what patterns in phonologi-cal processes can be inferred with such a statisticalmodel?
This simple framework begins learning withthe assumption that the underlying forms are faithfulto the surface (i.e., without considering markednessor phonotactics).We model the generation of surface forms fromunderlying ones on the segmental (character) level.Input is an inflectional paradigm, with tokens of theform stem+suffix.
Morphology is limited to asingle suffix (no agglutination), and is already iden-tified.
Each character of an underlying stem or suf-fix (ui) generates surface characters (sij) in an entirerow or column of the input.To capture the phonology of a variety of lan-guages with a single model, we need constraintsfrom linguistically plausible priors (universal gram-mar).
We prefer that underlying characters be pre-served in surface forms, especially when there is noalternation.
It is also reasonable that there be fewerunderlying forms (phonemes) than surface forms(phones, phonetic inventory), to account for allo-phones.
We expect to be able to capture a signifi-cant subset of phonological processes using a simplemodel (only faithfulness constraints).1.4 Pure GeneratorsOur model has an advantage over the baseline in itspreference for ?purity?
in underlying forms.
Eachunderlying segment should generate as few distinct2The paradigm format lends itself to analysis of word types,but if supplemented with surface counts, can also handle tokens.surface segments as possible: if it generates non-alternating (identical) segments, it will be less likelyto generate an alternation in addition.
This meansthat when two segments alternate, the underlyingform should be the one that appears less frequentlyin other contexts, irrespective of the majority withinthe alternation.In the first stem of our Finnish verb conjugation(Figure 1), we see a [t,d] alternation (a case of con-sonant gradation), as well as unalternating [t].
If weisolate three of the surface forms where /tieta?/ is in-flected (1st person singular, and 3rd person singularand plural), and consider only the dental segments inthe stem of each, we have two underlying segments.Here, we use question marks to indicate unknownunderlying segments./?
?/ [dt] [tt] [tt]In this subset of the data, the reasonable candidateunderlying forms are /t/ and /d/.
These two competeto explain the observed data (surface forms).
The na-ture of the prior probability distribution determineswhether the majority is hypothesized for each under-lying form, so /t/ produces both alternating and unal-ternating surface segments, or /d/ is hypothesized asthe source of the alternation (and /t/ remains ?pure?
).In a Bayesian setting, we impose a sparse prior overunderlying forms conditioned on the surface formsthey generate.If u2 is hypothesized to be /t/, the posterior prob-ability of u1 being /t/ goes down:P (u1 = /t/|u2 = /t/) < P (u1 = /t/)The probability of u1 being the competitor, /d/, cor-respondingly increases:P (u1 = /d/|u2 = /t/) > P (u1 = /d/)Even though the majority in this case would be /t/,the favored candidate for the alternating form was/d/.
This happened because of how we defined themodel?s prior, in combination with the evidence that/t/ (assigned to u2) generated the sequence of [t].
Soselection bias prefers /d/ as the source of an ambigu-ous segment, leaving /t/ to always generate itself.A similar effect can occur if there are both unal-ternating [t]?s and [d]?s on the surface, in addition tothe [t,d] alternation.
The candidate (/t/ or /d/) that is13aaaaaaStemSuffix /n/ (1s) /t/ (2s) /?/ (3s) /mme/ (1p) /tte/ (2p) /vat/ (3p)/tieta?/ [tieda?+n] [tieda?+t] [tieta?+a?]
[tieda?+mme] [tieda?+tte] [tieta?+va?t]/aiko/ [ai?o+n] [ai?o+t] [aiko+o] [ai?o+mme] [ai?o+tte] [aiko+vat]/luke/ [lu?e+n] [lu?e+t] [luke+e] [lu?e+mme] [lu?e+tte] [luke+vat]/puhu/ [puhu+n] [puhu+t] [puhu+u] [puhu+mme] [puhu+tte] [puhu+vat]/saa/ [saa+n] [saa+t] [saa+?]
[saa+mme] [saa+tte] [saa+vat]/tule/ [tule+n] [tule+t] [tule+e] [tule+mme] [tule+tte] [tule+vat]/pelka?a?/ [pelka?a?+n] [pelka?a?+t] [pelka?a?+?]
[pelka?a?+mme] [pelka?a?+tte] [pelka?a?+va?t]Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.generating fewer unalternating segments is preferredto explain the alternation.
For example, if there were1000 cases of [t], 500 [d] and 500 [t,d], we would ex-pect the following hypotheses: /t/ ?
[t], /d/ ?
[d]and /d/ ?
[t, d].
This is because one of the twocandidates must be responsible for both unalternat-ing and alternating segments, but we prefer to haveas much ??purity?
as possible, to minimize ambigu-ity.With this solution, we still have 1000 pure /t/ ?
[t], and only the 500 /d/ ?
[d] are now indistinctfrom /d/ ?
[t, d].
If we had selected /t/ as thesource of the alternation, there would be only 500remaining ?pure?
(/d/) segments, and 1500 ambigu-ous /t/.
Our Bayesian model should prefer the lessambiguous (?purer?)
solution, given an appropriateprior.2 ModelWe will use boldface to indicate vectors, and sub-scripts to identify an element from a vector or ma-trix.
The variable N(u) is a vector of observedcounts with the current underlying form hypothe-ses.
The notation we use for a vector u with oneelement i removed is u?i, so we can exclude thecounts associated with a particular underlying formby indicating that in the parenthesized variable (i.e.,N(u?4) is all the counts except those associated withthe fourth underlying form).
Ni(u) is the number oftimes character i is used as an underlying form, andNij(u) is the number of times character i generatedsurface character j.The priors over surface s and underlying u seg-ments in Figure 2 are captured by Dirichlet priors?
and ?, which generate the multinomial distribu-tions ?
and ?, respectively (see Figure 3).
Theprior over underlying form encourages sparse solu-tions, so ?u < 1 for all u.
The prior over surfaceform given underlying encourages identity mapping,/x/ ?
[x], so ?xx > 1, and discourages differentsegments, /x/ ?
[y], so ?xy < 1 for all x 6= y.nc??numnusu?
?Figure 2: Bayesian network: ?
and ?
are vectors of hy-perparameters, and ?i (for i ?
{1, .
.
.
, nc}) and ?
aredistributions.
u is a vector of underlying forms, generatedfrom ?, and si (for i ?
nu) is a set of observed surfaceforms generated from the hidden variable ui according to?iPhones and phonemes are drawn from a set ofcharacters (e.g., IPA, unicode) C used to representthem.
?i is the probability of a character (Ci fori ?
nc) being an underlying form, irrespective ofcurrent alignments or its position in the paradigm.
?ij is the conditional probability of a surface char-14?c | ?
?
DIR(?
), c = 1, .
.
.
, nc?
| ?
?
DIR(?
)ui | ?i ?
MULTI(?i), i = 1, .
.
.
, nusij | ui,?ui ?
MULTI(?ui), i = 1, .
.
.
, nu,j = 1, .
.
.
,miFigure 3: Model parameters: nc is # different segments,nu is # underlying segmentsacter (skn = Cj for j ?
nc, n ?
mk) given theunderlying character it is generated from (uk = Cifor i ?
nc, k ?
nu), which is determined by its po-sition in the paradigm.In our Finnish example (Figure 1), if k = 1, weare looking at the first underlying character, whichis /t/ (from /tieta?/), so assuming our character set isthe Finnish alphabet, of which ?t?
is the 20th char-acter, u1 = C20 = t. It generates the first characterof each inflected form (1st, 2nd, 3rd person, singu-lar and plural) of that stem, so m1 = 6, and sincethere is no alternation s1n = t (for n ?
{1, .
.
.
, 6}).Given the phonologically plausible (gold) underly-ing forms, the probability of /t/ is ?20 = 7/41.On the other hand, k = 33 identifies the 3rd per-son singular /?/, which inflects each of the sevenstems, so m33 = 7.
Since we need our alpha-bet to identify a null character, we?ll give it in-dex zero (i.e., u33 = C0 = ?).
For each of the(underlying, surface) alignments in this alternation(caused by vowel gemination), we can identify theprobability in ?.
For 3rd person singular [tieta?+a?
],where s33,1 = C28 = a?, the conditional probability?0,28 = 1/7.The prior hyperparameters can be understood asfollows.
As ?i gets smaller, an underlying form ukis less likely to be Ci.
As ?ij gets smaller, an un-derlying uk = Ci is less likely to generate a surfacesegment skn = Cj ?n ?
mk.
In our experiments,we will vary ?i=j (prior over identity map from un-derlying to surface) and ?i6=j .Our implementation of this model uses Gibbssampling (c.f., (Bishop, 2006), pp 542-8), an algo-rithm that produces samples from the posterior dis-tribution.
Each sample is an assignment of the hid-den variables, u (i.e., a set of hypothesized underly-ing forms).
Our sampler initializes u from a uniformdistribution over segments in the training data, andresamples underlying forms in a fixed order, as in-put in the paradigm.
Rather than reestimate ?
and?
at each iteration before sampling from u, we canmarginalize these intermediate probability distribu-tions in order to ease implementation and speed con-vergence.Our search procedure tries to sample from theposterior probability, according to Bayes?
rule.posterior ?
likelihood ?
priorP (u, s|?,?)
?
P (u|?
)P (s, u|?
)Each of these probabilities is drawn from a Dirichletdistribution, which is defined in terms of the multi-variate Beta function, C .
The prior ?
added to un-derlying counts N(u) forms the posterior Dirichletcorresponding to P (u|?).
In P (s|u,?
), each ?ivector is supplemented by the observed counts of(underlying, surface) pairs N(si).P (u, s|?,?)
= C(?
+ N(u))C(?
)nc?c=1C(?c +?i:ui=c N(si))C(?
)The collapsed update procedure consists of re-sampling each underlying form, u, incorporating theprior hyperparameters ?,?
and counts N over therest of the dataset.
The relevant counts for a can-didate k being the underlying form ui are Nk(u?i)and Nksij (u?i) for j ?
mi.
P (ui = k|u?i,?,?)
isproportional to the probability of generating ui = k,given the other u?i and all sij (for j ?
mi), givens?i and u?i.P (ui = c|u?i,?,?)
?
Nc(u?i) + ?cn?
1 + ??C(?
+ ?i?
6=i:ui?=c N(s?i) + N(si))C(?
+ ?i?
6=i:ui?=c N(s?i))Suppose we were updating this sampler runningon the Finnish verb inflections.
If we had all seg-ments as in Figure 1, but wanted to resample u31 (1stperson singular /n/), we would consider the countsN excluding that form (i.e., under u?31).
The priorfor /n/, ?14, is fixed, and there are no other occur-rences, so N14(u?31) = 0.
Another potential un-derlying form, like /t/, would have higher uncondi-tioned posterior probability, because of the counts15(7, in this case) added to its prior from ?.
Then, wehave to multiply by the probability of each generatedsurface segment (all are [n], so 7 ?
P ([n]|c,?)
for agiven hypothesis u31 = c).We select a given character c ?
C for u31 from adistribution at random.
Depending on the prior, /n/will be the most likely choice, but other values arestill possible with smaller probability.
The countsused for the next resampling, N(u?31), are affectedby this choice, because the new identity of u31 hascontributed to the posterior distribution.
After un-bounded iterations, Gibbs sampling is guaranteed toconverge and produce samples from the true poste-rior (Geman and Geman, 1984).3 EvaluationThis model provides a language agnostic solution toa subset of phonological problems.
We will firstexamine performance on the sample Finnish data(from Figure 1), and then look more closely at the is-sue of convergence.
Finally, we present results fromlarger corpora 3.3.1 FinnishOutput from a trial run on Finnish verbs (from Fig-ure 1) follows, with hyperparameters ?ij{100 ?
?i = j, 0.05 ??
i 6= j} and ?i = {0.1}.In the paradigm (a sample after 1000 iterations),each [sur+face] form is followed by its hypothesized/under/ + /lying/ morphemes.
[tieda?+n] : /tieda?/ + /n/[tieda?+t] : /tieda?/ + /t/[tieta?+a?]
: /tieda?/ + /a?/[tieda?+mme] : /tieda?/ + /mme/[tieda?+tte] : /tieda?/ + /tte/[tieta?+va?t] : /tieda?/ + /va?t/[ai?o+n] : /ai?o/ + /n/...[pelka?a?+va?t] : /pelka?a?/ + /vat/With strong enough priors (faithfulness con-straints), our sampler often selects the most com-mon surface form aligned with an underlying seg-ment.
Although [vat] is more common than [va?t],we choose the latter as the purer underlying form.So /a/ is always [a], but /a?/ can be either [a?]
or [a].32.8 million word types from Morphochallenge2007 (Ku-rimo et al, 2007)3.2 ConvergenceTesting convergence, we run again on the sampledata (Figure 1), using ?ij = 0.1 when i 6= j and10 when i = j and ?
= 0.1, starting from differentinitializations, we get the same solution.0 10 20 30 40 50 60 70 80 90 1002.972.982.9933.013.023.033.043.05 x 106Iteration?logLikelihoodFigure 4: Posterior likelihood at each of the first 100 iter-ations, from 4 runs (with different random seeds) on 10%of the Morphochallenge dataset (?i6=j = 0.001, ?i=j =100, ?
= 0.1), indicating convergence within the first 15iterations.To confirm that the sampler has converged, weoutput and plot trace statistics at each iteration, in-cluding marginal probability, log likelihood, andchanges in underlying forms (i.e., variables resam-pled).
If the sampler has converged, there should nolonger be a trend (consistent slope) in any of thesestatistics (as in Figure 4).Examining the posterior probability of each se-lected underlying form reveals interesting patternsthat also help explain the variation.
In the above run,the ambiguous segments (with surface alternations)were drawn from the distributions (with improbablesegments elided) in Figure 5.We expect this model to maximize the probabil-ity of either the ?majority?
solution or a solutiondemonstrating selection bias.
We compare likeli-hood of the posterior sample with that of a ?phono-logically plausible?
solution (in which underlyingforms are determined by referring to formal lin-guistic accounts of phonological derivation) and a?majority solution?
(see Figure 6 for a log-log plot,where lower is more likely).The posterior sample has optimal likelihood witheach parameter setting, as expected.
The majorityparse is selected with ?i6=j = 0.5 With lower val-ues of ?i6=j , the ?phonologically plausible?
parse is16u4=/d/ s4=[d,d,t,d,d,t]P (ui = c) ?d 0.99968t 0.00014u8=/k/ s8=[?,?,k,?,?,k](same behavior as u12)P (ui = c) ??
0.642k 0.124u33=/e/ s33=[a?,o,e,u,?,e,?
]P (ui = c) ?a?,o,u 0.0029?
0.215a 0.0003e 0.297Figure 5: Resampling probabilities for alternations, after1000 iterations.10?2 10?1 100104.24104.25104.26alpha?loglikelihoodposterior samplemajority solutionphonologically plausibleFigure 6: Parse likelihoodmore likely than the majority.
However, the sam-pler does not converge to this solution, because inthis [t,d] alternation, the ?phonologically plausible?solution identifies /t/, but neither selection bias normajority rules would lead to that with the given data.3.3 Morphologically segmented corporaIn our search for appropriate data for additional,larger-scale experiments, we found several vi-able alternatives.
The correct morphological seg-mentations for Finnish data used in Morphochal-lenge2007 (Kurimo et al, 2007) provide a rich andvaried set of words, and are readily analyzable byour sampler.
Rather than associating each surfaceform with a position in the paradigm, we use the an-Majority Bayesiantypes 50.84 69.53tokens 65.23 72.11Figure 7: Accuracy of underlying segment hypotheses.notated morphemes.For example, the word ajavalle is listed in the cor-pus as follows:ajavalle aja:ajaa|V va:PCP1 lle:ALL Theword is segmented into a verb stem, ?aja?
(drive),a present participle marker ?va?, and the allative suf-fix (?for?).
Each surface realization of a given mor-pheme is identified by the same tag (e.g., PCP1).However, in this corpus, insertion and deletion arenot explicitly marked (as they were in the paradigm,by ?).
Rather than introduce another componentto determine which segments in the form weredropped, we ignore these cases.The sampling algorithm proceeds as described insection 2.
To run on tokens (as opposed to types), weincorporate another input file that contains countsfrom the original text (ajavalle appeared 8 times).The counts of each morpheme?s surface forms thenreflect the number of times that form appeared in anyword in the corpus.3.3.1 Type or TokenIn Finnish verb conjugation, 3rd person (esp.
sin-gular) forms have high frequency and tend to be un-marked (i.e., closer to underlying).
In types, un-marked is a minority (one third), but incorporat-ing token frequency shifts that balance, benefitingthe ?majority learner.?
Among noun inflections, un-marked has higher frequency in speech, but markedtokens may still dominate in text.
We might expectthat it is easier to learn from tokens than types, inpart because more data is often helpful.Testing on half of the Morphochallenge 2007Finnish data (1M word types, 5M morph types,17.5M word tokens, 48M morph tokens), we ranboth our Bayesian model and a majority solver onthe morphological analyses, and compared againstphonologically plausible (gold) underlying forms.Results are reported in Figure 7.The Bayesian estimate consistently outperformedthe majority solution, and cases where the two differcould often be ascribed to the preference for ?pure?17analyses.4 ConclusionWe have described a model where surface formsare generated from underlying representations seg-ment by segment.
Taking this approach allowed usto investigate the properties of a Bayesian statisticallearner, and how these can be useful in the contextof sound systems, a basic component of language.Experiments with our implementation of a collapsedsampler have produced results largely confirmingour hypotheses.Without context, we can often learn about 60 to 80percent of the mapping from underlying phonemesto surface phones.
Especially with lower values of?i6=j , closer to 0, our model does prefer pure alter-nations.
Gibbs sampling tends to select the major-ity underlying form, particularly with ?i6=j relativelyhigh, closer to 1.
So, a sparser prior leads us furtherfrom the baseline, and often closer to a phonologi-cally plausible solution.4.1 DirectionsIn future research, we hope to integrate morpholog-ical analysis into this sort of a treatment of phonol-ogy.
This is a natural approach for children learn-ing their first language.
They intuitively discoverphonotactics, and how it affects the prosodic shapeof each word, as they learn meaningful units andcompose them together.
It is clear that many lay-ers of linguistic information interact in the earlystages of child language acquisition (Demuth andEllis, 2005 in press), so they should also interactin our models.
As discussed above, the presentmodel should be applicable to analysis of language-learners?
speech errors, and this connection shouldbe explored in greater depth.It might be interesting to predispose the samplerto select underlying forms from open syllables.
Thatis, set ?
to increase the probability of matchingone of the surface segments if its context (featureannotations) includes a vocalic segment or a wordboundary immediately following.
The probabilityof phonological processes like assimilation could besimilarly modeled, with the prior higher for choos-ing a segment that appears on the surface in a con-trastive context (where it shares few features withneighboring segments).If we define a MaxEnt distribution over Optimal-ity Theoretic constraints, we might use that to in-form our selection of underlying forms.
In (Gold-water and Johnson, 2003), the learning algorithmwas given a set of candidate surface forms asso-ciated with an underlying form, and tried to opti-mize the constraint weights.
In addition to the con-straint weights, we must also optimize the underly-ing form, since our goal is to take as input only ob-servable data.
Sampling from this type of complexdistribution is quite difficult, but some approaches(e.g., (Murray et al, 2006)) may help reduce the in-tractability.ReferencesKenneth R. Beesley and Lauri Karttunen.
2000.
Finite-state non-concatenative morphotactics.
In Lauri Kart-tunen, Jason Eisner, and Alain The?riault, editors, SIG-PHON2000, August 6 2000.
Proceedings of the FifthWorkshop of the ACL Special Interest Group in Com-putational Phonology., pages 1?12.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning (Information Science and Statis-tics).
Springer, August.Hal Daume III.
2007.
Hbc: Hierarchical bayes compiler.Katherine Demuth and David Ellis, 2005 (in press).
Re-visiting the acquisition of Sesotho noun class prefixes.Lawrence Erlbaum.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, gibbs distributions, and the bayesian restora-tion of images.
IEEE Trans.
Pattern Anal.
MachineIntell., 6(6):721?741, Nov.Dale Gerdemann and Gertjan van Noord.
2000.
Approx-imation and exactness in finite state optimality theory.Daniel Gildea and Daniel Jurafsky.
1996.
Learning biasand phonological-rule induction.
Computational Lin-guistics, 22(4):497?530.Sharon Goldwater and Mark Johnson.
2003.
Learning otconstraint rankings using a maximum entropy model.Sharon Goldwater and Mark Johnson.
2004.
Priors inBayesian learning of phonological rules.
In Proceed-ings of the Seventh Meeting of the ACL Special Inter-est Group in Computational Phonology, pages 35?42,Barcelona, Spain, July.
Association for ComputationalLinguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Adaptor grammars: A framework for spec-ifying compositional nonparametric Bayesian models.In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-vances in Neural Information Processing Systems 19,pages 641?648.
MIT Press, Cambridge, MA.18Lauri Karttunen.
1998.
The proper treatment of optimal-ity in computational phonology.
In Lauri Karttunen,editor, FSMNLP?98: International Workshop on Fi-nite State Methods in Natural Language Processing,pages 1?12.
Association for Computational Linguis-tics, Somerset, New Jersey.Mikko Kurimo, Mathias Creutz, and Ville Turunen.2007.
Overview of morpho challenge in clef 2007.In Working Notes for the CLEF 2007 Workshop, Bu-dapest, Hungary.Percy Liang and Dan Klein.
2007.
Tutorial 1: Bayesiannonparametric structured models, June.Iain Murray, Zoubin Ghahramani, and David MacKay.2006.
MCMC for doubly-intractable distributions.
InUAI.
AUAI Press.19
