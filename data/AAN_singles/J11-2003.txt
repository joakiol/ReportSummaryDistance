Learning to Rank Answers to Non-FactoidQuestions fromWeb CollectionsMihai Surdeanu?Stanford UniversityMassimiliano Ciaramita?
?Google Inc.Hugo Zaragoza?Yahoo!
ResearchThis work investigates the use of linguistically motivated features to improve search, in par-ticular for ranking answers to non-factoid questions.
We show that it is possible to exploitexisting large collections of question?answer pairs (from online social Question Answering sites)to extract such features and train ranking models which combine them effectively.
We investigatea wide range of feature types, some exploiting natural language processing such as coarse wordsense disambiguation, named-entity identification, syntactic parsing, and semantic role label-ing.
Our experiments demonstrate that linguistic features, in combination, yield considerableimprovements in accuracy.
Depending on the system settings we measure relative improvementsof 14% to 21% in Mean Reciprocal Rank and Precision@1, providing one of the most compellingevidence to date that complex linguistic features such as word senses and semantic roles can havea significant impact on large-scale information retrieval tasks.1.
IntroductionThe problem of Question Answering (QA) has received considerable attention in thepast few years.
Nevertheless, most of the work has focused on the task of factoidQA, where questions match short answers, usually in the form of named or numericalentities.
Thanks to international evaluations organized by conferences such as the TextREtrieval Conference (TREC) and the Cross Language Evaluation Forum (CLEF) Work-shop, annotated corpora of questions and answers have become available for severallanguages, which has facilitated the development of robust machine learning modelsfor the task.1?
Stanford University, 353 Serra Mall, Stanford, CA 94305?9010.
E-mail: mihais@stanford.edu.??
Google Inc., Brandschenkestrasse 110, CH?8002 Zu?rich, Switzerland.
E-mail: massi@google.com.?
Yahoo!
Research, Avinguda Diagonal 177, 8th Floor, 08018 Barcelona, Spain.E-mail: hugoz@yahoo-inc.com.1 TREC: http://trec.nist.gov; CLEF: http://www.clef-campaign.org.The primary part of this work was carried out while all authors were working at Yahoo!
Research.Submission received: 1 April 2010; revised submission received: 11 September 2010; accepted for publication:23 November 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 2Table 1Sample content from Yahoo!
Answers.High Q: How do you quiet a squeaky door?Quality A: Spray WD-40 directly onto the hinges of the door.
Open and close the doorseveral times.
Remove hinges if the door still squeaks.
Remove any rust,dirt or loose paint.
Apply WD-40 to removed hinges.
Put the hinges back,open and close door several times again.High Q: How does a helicopter fly?Quality A: A helicopter gets its power from rotors or blades.
So as the rotors turn,air flows more quickly over the tops of the blades than it does below.This creates enough lift for flight.Low Q: How to extract html tags from an html documents with c++?Quality A: very carefullyThe situation is different once one moves beyond the task of factoid QA.
Com-paratively little research has focused on QA models for non-factoid questions suchas causation, manner, or reason questions.
Because virtually no training data is avail-able for this problem, most automated systems train either on small hand-annotatedcorpora built in-house (Higashinaka and Isozaki 2008) or on question?answer pairsharvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricutand Brill 2006; Riezler et al 2007; Agichtein et al 2008).
None of these situations isideal: The cost of building the training corpus in the former setup is high; in the latterscenario the data tend to be domain-specific, hence unsuitable for the learning of open-domain models, and for drawing general conclusions about the underlying scientificproblems.On the other hand, recent years have seen an explosion of user-generated content(or social media).
Of particular interest in our context are community-driven question-answering sites, such as Yahoo!
Answers, where users answer questions posed by otherusers and best answers are selected manually either by the asker or by all the partici-pants in the thread.2 The data generated by these sites have significant advantages overother Web resources: (a) they have a high growth rate and they are already abundant;(b) they cover a large number of topics, hence they offer a better approximation of open-domain content; and (c) they are available for many languages.
Community QA sites,similar to FAQs, provide a large number of question?answer pairs.
Nevertheless, thesedata have a significant drawback: they have high variance of quality (i.e., questionsand answers range from very informative to completely irrelevant or even abusive).Table 1 shows some examples of both high and low quality content from the Yahoo!Answers site.In this article we investigate two important aspects of non-factoid QA:1.
Is it possible to learn an answer-ranking model for non-factoid questions, in acompletely automated manner, using data available in on-line social QA sites?This is an interesting question because a positive answer indicates that aplethora of training data are readily available to researchers and system2 http://answers.yahoo.com.352Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collectionsdevelopers working on natural language processing, information retrieval,and machine learning.2.
Which features and models are more useful in this context, that is, ample butnoisy data?
For example: Are similarity models as effective as models thatlearn question-to-answer transformations?
Does syntactic and semanticinformation help?Social QA sites are the ideal vehicle to investigate such questions.
Questions postedon these sites typically have a correct answer that is selected manually by users.
As-suming that all the other candidate answers are incorrect (we discuss this assumptionin Section 5), it is trivial to automatically organize these data into a format ready fordiscriminative learning, namely, the pair question?correct answer generates one posi-tive example and all other answers for the same question are used to generate negativeexamples.
This allows one to use the collection in a completely automated manner tolearn answer ranking models.The contributions of our investigation are the following:1.
We introduce and evaluate many linguistic features for answer re-ranking.Although several of these features have been introduced in previous work,some are novel in the QA context, for example, syntactic dependenciesand semantic role dependencies with words generalized to semantic tags.Most importantly, to the best of our knowledge this is the first work thatcombines all these features into a single framework.
This allows us toinvestigate their comparative performance in a formal setting.2.
We propose a simple yet powerful representation for complex linguisticfeatures, that is, we model syntactic and semantic information as bags ofsyntactic dependencies or semantic role dependencies and build similarityand translation models over these representations.
To address sparsity,we incorporate a back-off approach by adding additional models wherelexical elements in these structures are generalized to semantic tags.These models are not only simple to build, but, as our experimentsindicate, they perform at least as well as complex, dedicated models suchas tree kernels.3.
We are the first to evaluate the impact of such linguistic features in alarge-scale setting that uses real-world noisy data.
The impact on QA ofsome of the features we propose has been evaluated before, but theseexperiments were either on editorialized data enhanced with goldsemantic structures (e.g., the Wall Street Journal corpus with semanticroles from PropBank [Bilotti et al 2007]), or on very few questions(e.g., 413 questions from TREC 12 [Cui et al 2005]).
On the other hand,we evaluate on over 25,000 questions, and each question has up to100 candidate answers from Yahoo!
Answers.
All our data are processedwith off-the-shelf natural language (NL) processors.The article is organized as follows.
We describe our approach, including all thefeatures explored for answer modeling, in Section 2.
We introduce the corpus used inour empirical analysis in Section 3.
We detail our experiments and analyze the results353Computational Linguistics Volume 37, Number 2in Section 4.
Section 5 discusses current shortcomings of our system and proposessolutions.
We overview related work in Section 6 and conclude the article in Section 7.2.
ApproachFigure 1 illustrates our QA architecture.
The processing flow is the following.
First, theanswer retrieval component extracts a set of candidate answers A for a question Qfrom a large collection of answers, C, provided by a community-generated question-answering site.
The retrieval component uses a state-of-the-art information retrieval(IR) model to extract A given Q.
The second component, answer ranking, assigns toeach answer Ai ?
A a score that represents the likelihood that Ai is a correct answerfor Q, and ranks all answers in descending order of these scores.
In our experiments,the collection C contains all answers previously selected by users of a social QA siteas best answers for non-factoid questions of a certain type (e.g., ?How to?
questions).The entire collection of questions, Q, is split into a training set and two held-out sets:a development one used for parameter tuning, and a testing one used for the formalevaluation.Our architecture follows closely the architectures proposed in the TREC QA track(see, e.g., Voorhees 2001).
For efficiency reasons, most participating systems split theanswer extraction phase into a retrieval phase that selected likely answer snippetsusing shallow techniques, followed by a (usually expensive) answer ranking phasethat processes only the candidates proposed by the retrieval component.
Due to thisseparation, such architectures can scale to collections of any size.
We discuss in Section 6how related work has improved this architecture further?for example, by addingquery expansion terms from the translation models back to answer retrieval (Riezleret al 2007).The focus of this work, however, is on the re-ranking model implemented in theanswer ranking component.
We call this model FMIX?from feature mix?because theproposed scoring function is a linear combination of four different classes of featuresFigure 1Architecture of our QA framework.354Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections(detailed in Section 2.2).
To accommodate and combine all these feature classes, ourQA approach combines three types of machine learning methodologies (as highlightedin Figure 1): the answer retrieval component uses unsupervised IR models, the an-swer ranking is implemented using discriminative learning, and finally, some of theranking features are produced by question-to-answer translation models, which useclass-conditional generative learning.
To our knowledge, this combined approach isnovel in the context of QA.
In the remainder of the article, we will use the FMIXfunction to answer the research objectives outlined in the Introduction.
To answer thefirst research objective we will compare the quality of the rankings provided by thiscomponent against the rankings generated by the IRmodel used for answer retrieval.
Toanswer the second research objective we will analyze the contribution of the proposedfeature set to this function.We make some simplifying assumptions in this study.
First, we will consider onlymanner questions, and in particular only ?How to?
questions.
This makes the corpusmore homogeneous and more focused on truly informational questions (as opposedto social questions such as ?Why don?t girls like me?
?, or opinion questions such as?Who will win the next election?
?, both of which are very frequent in Yahoo!
Answers).Second, we concentrate on the task of answer-re-ranking, and ignore all other modulesneeded in a complete on-line social QA system.
For example, we ignore the problemof matching questions to questions, very useful when retrieving answers in a FAQ ora QA collection (Jeon, Croft, and Lee 2005), and we ignore all ?social?
features such asthe authority of users (Jeon et al 2006; Agichtein et al 2008).
Instead, we concentrate onmatching answers and on the different textual features.
Hence, the document collectionused in our experiments contains only answers, without the corresponding questionsanswered.
Furthermore, we concentrate on the re-ranking phase and we do not exploretechniques to improve the recall of the initial retrieval phase (by methods of queryexpansion, for example).
Such aspects are complementary to our work, and can beinvestigated separately.2.1 Representations of ContentOne of our main interests in using very large data sets was to show that complex lin-guistic features can improve rankingmodels if they are correctly combinedwith simplerfeatures, in particular using discriminative learning methods on a particular task.
Forthis reason we explore several forms of textual representation going beyond the bagof words.
In particular, we generate our features over four different representationsof text:Words (W): This is the traditional IR view where the text is seen as a bag of words.n-grams (N): The text is represented as a bag of word n-grams, where n ranges from twoup to a given length (we discuss structure parameters in the following).Dependencies (D): The text is converted to a bag of syntactic dependency chains.
Weextract syntactic dependencies in the style of the CoNLL-2007 shared task using thesyntactic processor described in Section 3.3 From the tree of syntactic dependencies weextract all the paths up to a given length following modifier-to-head links.
The top part3 http://depparse.uvt.nl/depparse-wiki/SharedTaskWebsite.355Computational Linguistics Volume 37, Number 2Figure 2Sample syntactic dependencies and semantic tags.Figure 3Sample semantic proposition.of Figure 2 shows a sample corpus sentence with the actual syntactic dependencies ex-tracted by our syntactic processor.
The figure indicates that this representation capturesimportant syntactic relations, such as subject?verb (e.g., helicopterSBJ???
gets) or object-verb (e.g., powerOBJ???
gets).Semantic Roles (R): The text is represented as a bag of predicate?argument rela-tions extracted using the semantic parser described in Section 3.
The parser follows thePropBank notations (Palmer, Gildea, and Kingsbury 2005), that is, it assigns semanticargument labels to nodes in a constituent-based syntactic tree.
Figure 3 shows an exam-ple.
The figure shows that the semantic proposition corresponding to the predicate getsincludes A helicopter as the Arg0 argument (Arg0 stands for agent), its power as the Arg1argument (or patient), and from rotors or blades as Arg2 (or instrument).
Semantic roleshave the advantage that they extract meaning beyond syntactic representations (e.g., asyntactic subject may be either an agent or a patient in the actual proposition).
We con-vert the semantic propositions detected by our parser into semantic dependencies usingthe same approach as Surdeanu et al (2008), that is, we create a semantic dependencybetween each predicate and the syntactic head of every one of its arguments.
Thesedependencies are labeled with the label of the corresponding argument.
For example,the semantic dependency that includes the Arg0 argument in Figure 3 is represented asgetsArg0???
helicopter.
If the syntactic constituent corresponding to a semantic argument is356Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collectionsa prepositional phrase (PP), we convert it to a bigram that includes the preposition andthe head word of the attached phrase.
For example, the tuple for Arg2 in the example isrepresented as getsArg2???
from-rotors.In all representations we remove structures where either one of the elements is astop word and convert the remaining words to their WordNet lemmas.4The structures we propose are highly configurable.
In this research, we investigatethis issue along three dimensions:Degree of lexicalization:We reduce the sparsity of the proposed structures by replacingthe lexical elements with semantic tags which might provide better generalization.
Inthis article we use two sets of tags, the first consisting of coarse WordNet senses, orsupersenses (WNSS) (Ciaramita and Johnson 2003), and the second of named-entitylabels extracted from the Wall Street Journal corpus.
We present in detail the tag setsand the processors used to extract them in Section 3.
For an overview, we show a sampleannotated sentence in the bottom part of Figure 2.Labels of relations: Both dependency and predicate?argument relations can be labeledor unlabeled (e.g., getsArg0???
helicopter versus gets?
helicopter).
We make this distinctionin our experiments for two reasons: (a) removing relation labels reduces the modelsparsity because fewer elements are created, and (b) performing relation recognitionwithout classification is simpler than performing the two tasks, so the correspondingNL processors might be more robust in the unlabeled-relation setup.Structure size: This parameter controls the size of the generated structures, namely,number of words in n-grams or dependency chains, or number of elements in thepredicate?argument tuples.
Nevertheless, in our experiments we did not see any im-provements from structure sizes larger than two.
In the experiments reported in thisarticle, all the structures considered are of size two, that is, we use bigrams, dependencychains of two elements, and tuples of one predicate and one semantic argument.2.2 FeaturesWe explore a rich set of features inspired by several state-of-the-art QA systems(Harabagiu et al 2000; Magnini et al 2002; Cui et al 2005; Soricut and Brill 2006; Bilottiet al 2007; Ko, Mitamura, and Nyberg 2007).
To the best of our knowledge this is thefirst work that: (a) adapts all these features for non-factoid answer ranking, (b) combinesthem in a single scoring model, and (c) performs an empirical evaluation of the differentfeature families and their combinations.For clarity, we group the features into four sets: features that model the similaritybetween questions and answers (FG1), features that encode question-to-answer trans-formations using a translation model (FG2), features that measure keyword density andfrequency (FG3), and features that measure the correlation between question?answerpairs and other collections (FG4).
Wherever applicable, we explore different syntacticand semantic representations of the textual content, as introduced previously.
We nextexplain in detail each of these feature groups.4 http://wordnet.princeton.edu.357Computational Linguistics Volume 37, Number 2FG1: Similarity Features.
We measure the similarity between a question Q and ananswer A using the length-normalized BM25 formula (Robertson and Walker 1997),which computes the score of the answer A as follows:BM25(A) =|Q|?i=0(k1 + 1)tfAi (k3 + 1)tfQi(K+ tf Ai )(k3 + tfQi )log(idfi) (1)where tf Ai and tfQi are the frequencies of the question term i in A and Q, and idfi isthe inverse document frequency of term i in the answer collection.
K is the length-normalization factor:K = k1((1?
b)+ b|A|/avg len)where avg len is the average answer length in the collection.
For all the constants in theformula (b, k1, and k3) we use values reported optimal for other IR collections (b = 0.75,k1 = 1.2, and k3 = 1, 000).We chose this similarity formula because, of all the IR models we tried, it providedthe best ranking at the output of the answer retrieval component.
For completenesswe also include in the feature set the value of the tf ?
idf similarity measure.
For bothformulas we use the implementations available in the Terrier IR platform with thedefault parameters.5To understand the contribution of our syntactic and semantic processors we com-pute the similarity features for different representations of the question and answercontent, ranging from bag of words to semantic roles.
We detail these representations inSection 2.1.FG2: Translation Features.
Berger et al (2000) showed that similarity-based modelsare doomed to perform poorly for QA because they fail to ?bridge the lexical chasm?between questions and answers.
One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al 2000; Echihabi andMarcu 2003; Soricut and Brill 2006; Riezler et al 2007).
In our model, we incorporate thisapproach by adding the probability that the question Q is a translation of the answerA, P(Q|A), as a feature.
This probability is computed using IBM?s Model 1 (Brown et al1993):P(Q|A) =?q?QP(q|A) (2)P(q|A) = (1?
?
)Pml(q|A)+ ?Pml(q|C) (3)Pml(q|A) =?a?A(T(q|a)Pml(a|A)) (4)where the probability that the question term q is generated from answer A, P(q|A),is smoothed using the prior probability that the term q is generated from the entirecollection of answers C, Pml(q|C).
?
is the smoothing parameter.
Pml(q|C) is computed5 http://ir.dcs.gla.ac.uk/terrier.358Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collectionsusing the maximum likelihood estimator.
To mitigate sparsity, we set Pml(q|C) to a smallvalue for out-of-vocabulary words.6 Pml(q|A) is computed as the sum of the probabilitiesthat the question term q is a translation of an answer term a, T(q|a), weighted by theprobability that a is generated fromA.
The translation table for T(q|a) is computed usingthe EM algorithm implemented in the GIZA++ toolkit.7Translation models have one important limitation when used for retrieval tasks:They do not guarantee that the probability of translating a word to itself, that is, T(w|w),is high (Murdock and Croft 2005).
This is a problem for QA, where word overlapbetween question and answer is a good indicator of relevance (Moldovan et al 1999).We address this limitation with a simple algorithm: we set T(w|w) = 0.5 and re-scalethe other T(w?|w) probabilities for all other words w?
in the vocabulary to sum to 0.5, toguarantee that?w?
T(w?|w) = 1.
This has the desired effect that T(w|w) becomes largerthan any other T(w?|w).
Our initial experiments proved empirically that this is essentialfor good performance.As prior work indicates, tuning the smoothing parameter ?
is also crucial for theperformance of translation models, especially in the context of QA (Xue, Jeon, andCroft 2008).
We tuned the ?
parameter independently for each of the translation modelsintroduced as follows: (a) for a smaller subset of the development corpus introducedin Section 3 (1,500 questions) we retrieved candidate answers using our best retrievalmodel (BM25); (b) we implemented a simple re-ranking model using as the only featurethe translation model probability; and (c) we explored a large range of values for ?and selected the one that maximizes the mean reciprocal rank (MRR) of the re-rankingmodel.
This process selected a wide range of values for the ?
parameter for the differenttranslation models (e.g., 0.09 for the translation model over labeled syntactic depen-dencies, and 0.43 for the translation model over labeled semantic role dependencies).Similarly to the previous feature group, we add translation-based features for thedifferent text representations detailed in Section 2.1.
By moving beyond the bag-of-words representation we hope to learn relevant transformations of structures, for ex-ample, from the squeaky?
door dependency to spray?WD-40 in the Table 1 example.FG3: Density and Frequency Features.
These features measure the density and fre-quency of question terms in the answer text.
Variants of these features were usedpreviously for either answer or passage ranking in factoid QA (Moldovan et al 1999;Harabagiu et al 2000).
Tao and Zhai (2007) evaluate a series of proximity-based mea-sures in the context of information retrieval.Same word sequence: Computes the number of non-stop question words that arerecognized in the same order in the answer.Answer span: The largest distance (in words) between two non-stop question words inthe answer.
We compute multiple variants of this feature, where we count: (a) the totalnumber of non-stop words in the span, or (b) the number of non-stop nouns.Informativeness: Number of non-stop nouns, verbs, and adjectives in the answer textthat do not appear in the question.6 We used 1E-9 for the experiments in this article.7 http://www.fjoch.com/GIZA++.html.359Computational Linguistics Volume 37, Number 2Same sentencematch:Number of non-stop question termsmatched in a single sentencein the answer.
This feature is added both unnormalized and normalized by the questionlength.Overall match: Number of non-stop question terms matched in the complete answer.All these features are computed as raw counts and as normalized counts (dividingthe count by the question length, or by the answer length in the case of Answer span).The last two features (Same sentence match and Overall match) are computed for alltext representations introduced, including syntactic and semantic dependencies (seeSection 2.1).Note that counting the number of matched syntactic dependencies is essentiallya simplified tree kernel for QA (e.g., see Moschitti et al 2007) matching only trees ofdepth 2.
We also include in this feature group the following tree-kernel features.Tree kernels: Tomodel larger syntactic structures that are shared between questions andanswers we compute the tree kernel values between all question and answer sentences.We implemented a dependency-tree kernel based on the convolution kernels proposedby Collins and Duffy (2001).
We add as features the largest value measured betweenany two individual sentences, as well as the average of all computed kernel values fora given question and answer.
We compute tree kernels for both labeled and unlabeleddependencies, and for both lexicalized trees and for trees where words are generalizedto their predicted WNSS or named-entity tags (when available).FG4: Web Correlation Features.
Previous work has shown that the redundancy of alarge collection (e.g., the Web) can be used for answer validation (Brill et al 2001;Magnini et al 2002).
In the same spirit, we add features that measure the correlationbetween question?answer pairs and large external collections:Web correlation:Wemeasure the correlation between the question?answer pair and theWeb using the Corrected Conditional Probability (CCP) formula of Magnini et al (2002):CCP(Q,A) = hits(Q+ A)/(hits(Q) hits(A)2/3) (5)where hits returns the number of page hits from a search engine.
The hits procedureconstructs a Boolean query from the given set of terms, represented as a conjunction ofall the corresponding keywords.
For example, for the second question in Table 1, hits(Q)uses the Boolean query: helicopter AND fly.It is notable that this formula is designed for Web-based QA, that is, the conditionalprobability is adjusted with 1/hits(A)2/3 to reduce the number of cases when snippetscontaining high-frequency words are marked as relevant answers.
This formula wasshown to perform best for the task of QA (Magnini et al 2002).
Nevertheless, thisformula was designed for factoid QA, where both the question and the exact answerhave a small number of terms.
This is no longer true for non-factoid QA.
In this contextit is likely that the number of hits returned forQ, A, orQ+ A is zero given the large sizeof the typical question and answer.
To address this issue, wemodified the hits procedureto include a simple iterative query relaxation algorithm:1.
Assign keyword priorities using a set of heuristics inspired byMoldovan et al (1999).
The complete priority detection algorithmis listed in Table 2.360Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb CollectionsTable 2Keyword priority heuristics.Step Keyword type Priority(a) Non-stop keywords within quotes 8(b) Non-stop keywords tagged as proper nouns 7(c) Contiguous sequences of 2+ adjectives as nouns 6(d) Contiguous sequences of 2+ nouns 5(e) Adjectives not assigned in step (c) 4(f) Nouns not assigned in steps (c) or (d) 3(g) Verbs and adverbs 2(h) Non-stop keywords not assigned in the previous steps 12.
Fetch the number of page hits using the current query.3.
If the number of hits is larger than zero, stop; otherwise discard the set ofkeywords with the smallest priority in the current query and repeat fromstep 2.Query-log correlation: As in Ciaramita, Murdock, and Plachouras (2008), we also com-pute the correlation between question?answer pairs from a search-engine query-logcorpus of more than 7.5 million queries, which shares roughly the same time stampwith the community-generated question?answer corpus.
Using the query-log correla-tion between two snippets of text was shown to improve performance for contextualadvertising, that is, linking a user?s query to the description of an ad (Ciaramita,Murdock, and Plachouras 2008).
In this work, we adapt this idea to the task of QA.However, because it is not clear which correlation metric performs best in this context,we compute both the Pointwise Mutual Information (PMI) and chi square (?2) associ-ation measures between each question?answer word pair in the query-log corpus.
Thelargest and the average values are included as features, as well as the number of QAword pairs which appear in the top 10, 5, and 1 percentile of the PMI and ?2 word pairrankings.We replicate all features that can be computed for different content representationsusing every independent representation and parameter combination introduced inSection 2.1.
For example, we compute similarity scores (FG1) for 16 different repre-sentations of question/answer content, produced by different parametrizations of thefour different generic representations (W, N, D, R).
One important exception to thisstrategy are the translation-model features (FG2).
Because our translation models aimto learn both lexical and structural transformations between questions and answers,it is important to allow structural variations in the question/answer representations.In this article, we implement a simple and robust approximation for this purpose: Fortranslation models we concatenate all instances of structured representations (N, D, R)with the corresponding bag-of-words representation (W).
This allows the translationmodels to learn some combined lexical and structural transformation (e.g., from thedependency squeaky?
door dependency to the tokenWD-40).
All in all, replicating ourfeatures for all the different content representations yields 137 actual features to be usedfor learning.361Computational Linguistics Volume 37, Number 22.3 Ranking ModelsOur approach is agnostic with respect to the actual learning model.
To emphasize this,we experimented with two learning algorithms.
First, we implemented a variant of theranking Perceptron proposed by Shen and Joshi (2005).
In this framework the rankingproblem is reduced to a binary classification problem.
The general idea is to exploit thepairwise preferences induced from the data by training on pairs of patterns, rather thanindependently on each pattern.
Given a weight vector ?, the score for a pattern x (acandidate answer) is given by the inner product between the pattern and the weightvector:f?
(x) = ?x,??
(6)However, the error function depends on pairwise scores.
In training, for each pair(xi, xj) ?
A, the score f?
(xi ?
xj) is computed; note that if f is an inner product f?
(xi ?xj) = f?(xi)?
f?(xj).
In this framework one can define suitable margin functions thattake into account different levels of relevance; for example, Shen and Joshi (2005)propose g(i, j) = ( 1i ?1j ), where i and j are the rank positions of xi and xj.
Because inour case there are only two relevance levels we use a simpler sign function yi,j, whichis negative if i > j and positive otherwise; yi,j is then scaled by a positive rate ?
foundempirically on the development data.
In the presence of numbers of possible rank levelsappropriate margin functions can be defined.
During training, if f?
(xi ?
xj) ?
yi,j?, anupdate is performed as follows:?t+1 = ?t + (xi ?
xj)yi,j?
(7)We notice, in passing, that variants of the perceptron including margins have beeninvestigated before; for example, in the context of uneven class distributions (see Li et al2002).
It is interesting to notice that such variants have been found to be competitivewith SVMs in terms of performance, while being more efficient (Li et al 2002; Surdeanuand Ciaramita 2007).
The comparative evaluation from our experiments are consistentwith these findings.
For regularization purposes, we use as a final model the average ofall Perceptron models posited during training (Freund and Schapire 1999).We also experimented with SVM-rank (Joachims 2006), which is an instance ofstructural SVM?a family of Support Vector Machine algorithms that model structuredoutputs (Tsochantaridis et al 2004)?specifically tailored for ranking problems.8 SVM-rank optimizes the area under a ROC curve.
The ROC curve is determined by the truepositive rate vs. the false positive rate for varying values of the prediction threshold,thus providing a metric closely related to Mean Average Precision (MAP).3.
The CorpusThe corpus is extracted from a sample of the U.S. Yahoo!
Answers questions andanswers.
We focus on the subset of advice or ?how to?
questions due to their fre-quency, quality, and importance in social communities.
Nevertheless, our approach8 http://www.cs.cornell.edu/People/tj/svm light/svm rank.html.362Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collectionsis independent of the question type.
To construct our corpus, we implemented thefollowing successive filtering steps:Step 1: From the full corpus we keep only questions that match the regularexpression:how (to|do|did|does|can|would|could|should)and have an answer selected as best either by the asker or by theparticipants in the thread.
The outcome of this step is a set of364,419 question?answer pairs.Step 2: From this corpus we remove the questions and answers of dubiousquality.
We implement this filter with a simple heuristic by keepingonly questions and answers that have at least four words each, outof which at least one is a noun and at least one is a verb.
Therationale for this step is that genuine answers to ?how to?
questionsshould have a minimal amount of structure, approximated by theheuristic.
This step filters out questions like How to be excellent?
andanswers such as I don?t know.
The outcome of this step forms ouranswer collection C. C contains 142,627 question?answer pairs.This corpus is freely available through the Yahoo!
Webscopeprogram.9Arguably, all these filters could be improved.
For example, the first step can bereplaced by a question classifier (Li and Roth 2006).
Similarly, the second step can beimplemented with a statistical classifier that ranks the quality of the content usingboth the textual and non-textual information available in the database (Jeon et al 2006;Agichtein et al 2008).
We plan to further investigate these issues, which are not themain object of this work.The data was processed as follows.
The text was split at the sentence level, token-ized and POS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus,Santorini, and Marcinkiewicz 1993).
Each word was morphologically simplified usingthe morphological functions of the WordNet library.
Sentences were annotated withWNSS categories, using the tagger of Ciaramita and Altun (2006), which annotatestext with a 46-label tagset.10 These tags, defined by WordNet lexicographers, providea broad semantic categorization for nouns and verbs and include labels for nouns suchas food, animal, body, and feeling, and for verbs labels such as communication, contact,and possession.
We chose to annotate the data with this tagset because it is less biasedtowards a specific domain or set of semantic categories than, for example, a named-entity tagger.
Using the same tagger as before we also annotated the text with a named-entity tagger trained on the BBNWall Street Journal (WSJ) Entity Corpus which defines105 categories for entities, nominal concepts, and numerical types.11 See Figure 2 for asample sentence annotated with these tags.Next, we parsed all sentences with the dependency parser of Attardi et al (2007).12We chose this parser because it is fast and it performed very well in the domain adap-tation shared task of CoNLL 2007.
Finally, we extracted semantic propositions using9 You can request the corpus by email at research-data-requests@yahoo-inc.com.
More informationabout this corpus can be found at: http://www.yr-bcn.es/MannerYahooAnswers.10 http://sourceforge.net/projects/supersensetag.11 LDC catalog number LDC2005T33.12 http://sourceforge.net/projects/desr.363Computational Linguistics Volume 37, Number 2the SwiRL semantic parser of Surdeanu et al (2007).13 SwiRL starts by syntacticallyanalyzing the text using a constituent-based full parser (Charniak 2000) followed by asemantic layer, which extracts PropBank-style semantic roles for all verbal predicates ineach sentence.It is important to realize that the output of all mentioned processing steps is noisyand contains plenty of mistakes, because the data have huge variability in terms ofquality, style, genres, domains, and so forth.
In terms of processing speed, both thesemantic tagger of Ciaramita and Altun and the Attardi et al parser process 100+sentences/second.
The SwiRL system is significantly slower: On average, it parses lessthan two sentences per second.
However, recent research showed that this latter taskcan be significantly sped up without loss of accuracy (Ciaramita et al 2008).We used 60% of the questions for training, 20% for development, and 20% for test-ing.
Our ranking model was tuned strictly on the development set for feature selection(described later) and the ?
parameter of the translation models.
The candidate answerset for a given question is composed of one positive example, that is, its correspondingbest answer, and as negative examples all the other answers retrieved in the top N bythe retrieval component.4.
ExperimentsWe used several measures to evaluate our models.
Recall that we are using an initialretrieval engine to select a pool of N answer candidates (Figure 1), which are then re-ranked.
This couples the performance of the initial retrieval engine and the re-rankers.We tried to de-couple them in our performance measures, as follows.
We note that ifthe initial retrieval engine does not rank the correct answer in the pool of top N results,it is impossible for any re-ranker to do well.
We therefore follow the approach of Koet al (2007) and define performance measures only with respect to the subset of poolswhich contain the correct answer for a given N.This complicates slightly the typical notions of recall and precision.
Let us callQ theset of all queries in the collection and QN the subset of queries for which the retrievedanswer pool of size N contains the correct answer.
We will then use the followingperformance measure definitions:Retrieval Recall@N: The usual recall definition:|QN||Q| .
This is equal for all re-rankers.Re-ranking Precision@1: Average Precision@1 over the QN set, where the Precision@1of a query is defined as 1 if the correct answer is re-ranked into the first position,0 otherwise.Re-ranking MRR: MRR over theQN set, where the reciprocal rank is the inverse of therank of the correct answer.Note that as N gets larger, QN grows in size, increasing the Retrieval Recall@N butalso increasing the difficulty of the task for the re-ranker, and therefore decreasing Re-ranking Precision@1 and Re-ranking MRR.During training of the FMIX re-ranker, the presentation of the training instances israndomized, which defines a randomized training protocol producing different modelswith each permutation of the data.
We exploit this property to estimate the variance onthe experimental results by reporting the average performance of 10 different models,together with an estimate of the standard deviation.13 http://swirl-parser.sourceforge.net.364Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb CollectionsTable 3Re-ranking evaluation for Perceptron and SVM-rank.
Improvement indicates relativeimprovement over the baseline.N = 15 N = 25 N = 50 N = 100Retrieval Recall@N 29.04% 32.81% 38.09% 43.42%Re-ranking Precision@1Baseline 41.48 36.74 31.66 27.75FMIX (Perceptron) 49.87?0.03 44.48?0.03 38.53?0.11 33.72?0.05FMIX (SVM-rank) 49.48 44.10 38.18 33.52Improvement (Perceptron) +20.22% +21.06% +21.69% +21.51%Improvement (SVM-rank) +19.28% +20.03% +20.59% +20.79%Re-ranking MRRBaseline 56.12 50.31 43.74 38.53FMIX (Perceptron) 64.16?0.01 58.20?0.01 51.19?0.07 45.29?0.05FMIX (SVM-rank) 63.81 57.89 50.93 45.12Improvement (Perceptron) +14.32% +15.68% +17.03% +17.54%Improvement (SVM-rank) +13.70% +15.06% +16.43% +17.10%The initial retrieval engine used to select the pool of candidate answers is the BM25score as described earlier.
This is also our baseline re-ranker.
We will compare this to theFMIX re-ranker using all features or using subsets of features.4.1 Overall ResultsTable 3 and Figure 4 show the results obtained using FMIX and the baseline for in-creasing values of N. We report results for Perceptron and SVM-rank using the optimalfeature set for each (we discuss feature selection in the next sub-section).Looking at the first column in Table 3 we see that a good bag-of-words representa-tion alone (BM25 in this case) can achieve 41.5% Precision@1 (for the 29.0% of queries forFigure 4Re-ranking evaluation; precision-recall curve.365Computational Linguistics Volume 37, Number 2which the retrieval engine can find an answer in the top N = 15 results).
These baselineresults are interesting because they indicate that the problem is not hopelessly hard, butit is far from trivial.
In principle, we see much room for improvement over bag-of-wordsmethods.
Indeed, the FMIX re-ranker greatly improves over the baseline.
For example,the FMIX approach using Perceptron yields a Precision@1 of 49.9%, a 20.2% relativeincrease.SettingN to a higher valuewe see recall increase at the expense of precision.
Becauserecall depends only on the retrieval engine and not on the re-ranker, what we areinterested in is the relative performance of our re-rankers for increasing numbers ofN.
For example, setting N = 100 we observe that the BM25 re-ranker baseline obtains27.7% Precision@1 (for the 43.4% of queries for which the best answer is found in thetop N = 100).
For this same subset, the FMIX re-ranker using Perceptron obtains 33.7%Precision@1, a 21.5% relative improvement over the baseline model.The FMIX system yields a consistent and significant improvement for all valuesof N, regardless of the type of learning algorithm used.
As expected, as N grows theprecision of both re-rankers decreases, but the relative improvement holds or increases.This can be seen most clearly in Figure 4 where re-ranking Precision and MRR areplotted against retrieval Recall.
Recalling that the FMIX model was trained only once,using pools of N = 15, we can note that the training framework is stable at increasingsizes of N.Table 3 and Figure 4 show that the two FMIX variants (Perceptron and SVM-rank)yield scores that are close (e.g., Precision@1 scores are within 0.5% of each other).
Wehypothesize that the small difference between the two different learning models iscaused by our greedy tuning procedures (described in the next section), which convergeto slightly different solutions due to the different learning algorithms.
Most importantly,the fact that we obtain analogous results with two different learningmodels underscoresthe robustness of our approach and of our feature set.These overall results provide strong evidence that: (a) readily available and scalableNLP technology can be used to improve lexical matching and translation models forretrieval and QA tasks, (b) we can use publicly available online QA collections toinvestigate features for answer ranking without the need for costly human evaluation,and (c) we can exploit large and noisy on-line QA collections to improve the accuracy ofanswer ranking systems.
In the remainder of this section we analyze the performanceof the different features.4.2 Contribution of Feature GroupsIn order to gain some insights about the effectiveness of the different features groups,we carried out a greedy feature selection procedure.
We implemented similar processesfor Perceptron and SVM-rank, to guarantee that our conclusions are not biased by aparticular learning model.4.2.1 Perceptron.
We initialized the feature selection process with a single feature thatreplicates the baseline model (BM25 applied to the bag-of-words [W] representation).Then the algorithm incrementally adds to the feature set the single feature that providesthe highest MRR improvement in the development partition.
The process stops whenno features yield any improvement.
Note that this is only a heuristic process, and needsto be interpreted with care.
For example, if two features were extremely correlated, thealgorithm would choose one at random and discard the other.
Therefore, if a feature is366Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collectionsmissing from the selection process it means that it is either useless, or strongly correlatedwith other features in the list.Table 4 summarizes the outcome of this feature selection process.
Where applicable,we show within parentheses the text representation for the corresponding feature: Wfor words, N for n-grams, D for syntactic dependencies, and R for semantic roles.
Weuse subscripts to indicate if the corresponding representation is fully lexicalized (nosubscript), or its elements are replaced by WordNet supersenses (WNSS) or named-entity tags (WSJ).
Where applicable, we use the l superscript to indicate if the cor-responding structures are labeled.
No superscript indicates unlabeled structures.
Forexample, DWNSS stands for unlabeled syntactic dependencies where the participatingtokens are replaced by their WordNet supersense; RlWSJ stands for semantic tuples ofpredicates and labeled arguments with the words replaced with the corresponding WSJnamed-entity tags.The table shows that, although the features selected span all the four feature groupsintroduced, the lion?s share is taken by the translation features (FG2): 75% of the MRRimprovement is achieved by these features.
The frequency/density features (FG3) areresponsible for approximately 16% of the improvement.
The rest is due to the query-logcorrelation features (FG4).
This indicates that, even though translation models are themost useful, it is worth exploring approaches that combine several strategies for answerranking.As we noted before, many features may be missing from this list simply becausethey are strongly correlated with others.
For example most similarity features (FG1) arecorrelated with BM25(W); for this reason the selection process does not choose a FG1feature until iteration 9.
On the other hand, some features do not provide a useful signalTable 4Summary of the model selection process using Perceptron.Iteration Feature Set Group MRR P@1 (%)0 BM25(W) FG1 56.09 41.141 + translation(R) FG2 61.18 46.332 + translation(N) FG2 62.49 47.973 + overall match(DWNSS) FG3 63.07 48.934 + translation(W) FG2 63.27 49.125 + query-log avg(PMI) FG4 63.57 49.566 + overall match(W) FG3 63.72 49.747 + overall match(W), normalized by Q size FG3 63.82 49.898 + same word sequence, normalized by Q size FG3 63.90 49.949 + BM25(N) FG1 63.98 50.0010 + informativeness: verb count FG3 64.16 49.9711 + query-log max(PMI) FG4 64.37 50.2812 + same sentence match(W) FG3 64.42 50.4013 + overall match(NWSJ) FG3 64.49 50.5114 + query-log max(?2) FG4 64.56 50.5915 + same word sequence FG3 64.66 50.7216 + BM25(RWSJ) FG1 64.68 50.7817 + translation(RlWSJ) FG2 64.71 50.7518 + answer span, normalized by A size FG3 64.76 50.8019 + query-log top10(?2) FG4 64.89 51.0620 + tree kernel(DWSJ) FG3 64.93 51.0721 + translation(RWNSS) FG2 64.95 51.16367Computational Linguistics Volume 37, Number 2at all.
A notable example in this class is theWeb-based CCP feature, which was designedoriginally for factoid answer validation and does not adapt well to our problem.
To testthis, we learned a model with BM25 and the Web-based CCP feature only, and thismodel did not improve over the baseline model at all.
We hypothesize that because thelength of non-factoid answers is typically significantly larger than in the factoid QAtask, we have to discard a large part of the query when computing hits(Q+ A) to reachnon-zero counts.
This means that the final hit counts, hence the CCP value, are generallyuncorrelated with the original (Q,A) tuple.One interesting observation is that two out of the first three features chosen byour model selection process use information from the NLP processors.
The first featureselected is the translation probability computed between the R representation (unla-beled semantic roles) of the question and the answer.
This feature alone accounts for57% of the measured MRR improvement.
This is noteworthy: Semantic roles have beenshown to improve factoid QA, but to the best of our knowledge this is the first resultdemonstrating that semantic roles can improve ad hoc retrieval (on a large set of non-factoid open-domain questions).
We also find noteworthy that the third feature chosenmeasures the number of unlabeled syntactic dependencies with words replaced by theirWNSS labels that are matched in the answer.
Overall, the features that use the output ofNL processors account for 68% of the improvement produced by our model over the IRbaseline.
These results provide empirical evidence that natural language analysis (e.g.,coarse word sense disambiguation, syntactic parsing, and semantic role labeling) has apositive contribution to non-factoid QA, even in broad-coverage noisy settings basedon Web data.
To our knowledge, this had not been shown before.Finally, we note that tree kernels provide minimal improvement: A tree kernelfeature is selected only in iteration 20 and the MRR improvement is only 0.04 points.One conjecture is that, due to the sparsity and the noise of the data, matching trees ofdepth higher than 2 is highly uncommon.
Hence matching immediate dependenciesis a valid approximation of kernels in this setup.
Another possible explanation is thatbecause the syntactic trees produced by the parser contain several mistakes, the treekernel, which considers matches between an exponential number of candidate sub-trees, might be particularly unreliable on noisy data.4.2.2 SVM-rank.
For SVM-rank we employed a tuning procedure similar to the one usedfor the Perceptron that implements both feature selection and tuning of the regularizerparameter C. We started with the baseline feature alone and greedily added one featureat a time.
In each iteration we added the feature that provided the best improvement.The procedure continues to evaluate all available features, until no improvement isobserved.
For this step we set the regularizer parameter to 1.0, a value which provideda good tradeoff between accuracy and speed as evaluated in an initial experiment.The selection procedure generated 12 additional features.
At this point, using only theselected features, we fine-tuned the regularization parameter C across a wide spectrumof possible values.
This can be useful because in SVM-rank the interpretation of C isslightly different than in standard SVM, specifically Csvm = Crank/m, where m is thenumber of queries, or questions in our case.
Therefore, an optimal value can dependcrucially on the target data.
The final value selected by this search procedure was equalto 290, although performance is relatively stable with values between 1 and 100,000.
Asa final optimization step, we continued the feature selection routine, starting from the13 features already chosen and C = 290.
This last step selected six additional features.A further attempt at fine-tuning the C parameter did not provide any improvements.368Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb CollectionsThis process is summarized in Table 5, using the same notations as Table 4.
Al-though the features selected by SVM-rank are slightly different than the ones chosenby the Perceptron, the conclusions drawn are the same as before: Features generatedby NL processors provide a significant boost on top of the IR model.
Similarly to thePerceptron, the first feature chosen by the selection procedure is a translation probabil-ity computed over semantic role dependencies (labeled, unlike the Perceptron, whichprefers unlabeled dependencies).
This feature alone accounts for 33.3% of the measuredMRR improvement.
This further enforces our observation that semantic roles improveretrieval performance for complex tasks such as our non-factoid QA exercise.
All in all,13 out of the 18 selected features, responsible for 70% of the total MRR improvement,use information from the NL processors.4.3 Contribution of Natural Language StructuresOne of the conclusions of the previous analysis is that features based on natural lan-guage processing are important for the problem of QA.
This observation deserves amore detailed analysis.
Table 6 shows the performance of our first three feature groupswhen they are applied to each of the content representations and incremental combina-tions of representations.
In this table, for simplicity we merge features from labeledand unlabeled representations.
For example, R indicates that features are extractedfrom both labeled (Rl) and unlabeled (R) semantic role representations.
The g subscriptindicates that the lexical terms in the corresponding representation are separately gener-alized toWNSS andWSJ labels.
For example,Dgmerges features generated fromDWNSS,Table 5Summary of the model selection process using SVM-rank.Iteration Feature Set Group MRR P@1 (%)0 BM25(W) FG1 56.09 41.121 + translation(Rl) FG2 59.02 43.992 + answer span FG3 60.31 45.053 + translation(W) FG2 61.16 46.134 + translation(R) FG2 61.65 46.775 + overall match(D) FG3 62.85 48.576 + translation(RlWSJ) FG2 63.05 48.787 + translation(NWSJ) FG2 63.23 48.888 + translation(DlWSJ) FG2 63.47 49.219 + query-log max(?2) FG4 63.64 49.3510 + translation(D) FG2 63.77 49.5311 + translation(N) FG2 63.85 49.6612 + overall match(NWSJ) FG3 64.03 49.93+ C fine tuning 64.49 50.4313 + BM25(DWSJ) FG1 64.49 50.4314 + BM25(N) FG1 64.74 50.7115 + tf ?
idf(DWNSS) FG1 64.74 50.6016 + answer span in nouns FG3 64.74 50.6017 + tf ?
idf(Rl) FG1 64.84 50.8918 + translation(Dl) FG2 64.88 50.91369Computational Linguistics Volume 37, Number 2Table 6Contribution of natural language structures in each feature group.
Scores are MRR changes ofthe Perceptron on the development set over the baseline model (FG1 with W), for N = 15.
Thebest scores for each feature group (i.e., column in the table), are marked in bold.FG1 FG2 FG3W ?
+4.18 ?6.80N ?13.97 +2.49 ?13.63Ng ?18.65 +3.63 ?15.57D ?15.15 +1.48 ?15.39Dg ?19.31 +3.41 ?18.18R ?27.61 +0.33 ?27.82Rg ?28.29 +3.46 ?26.74W +N +1.46 +5.20 ?4.36W +N +Ng +1.51 +5.33 ?4.31W +N +Ng +D +1.56 +5.78 ?4.31W +N +Ng +D +Dg +1.56 +5.85 ?4.21W +N +Ng +D +Dg + R +1.58 +6.12 ?4.28W +N +Ng +D +Dg + R + Rg +1.65 +6.29 ?4.28DWSJ, DlWNSS, and DlWSJ.
For each cell in the table, we use only the features from thecorresponding feature group and representation to avoid the correlation with featuresfrom other groups.
We generate each best model using the same feature selectionprocess described above.The top part of the table indicates that all individual representations perform worsethan the bag-of-words representation (W) in every feature group.
The differences rangefrom less than one MRR point (e.g., FG2[Rg] versus FG2[W]), to over 28 MRR points(e.g., FG1[Rg] versus FG1[W]).
Such a large difference is justified by the fact that forfeature groups FG1 and FG3 we compute feature values using only the correspondingstructures (e.g., only semantic roles), which could be very sparse.
For example, thereare questions in our corpus where our SRL system does not detect any semantic propo-sition.
Because translation models merge all structured representations with the bag-of-word representation, they do not suffer from this sparsity problem.
Furthermore, ontheir own, FG3 features are significantly less powerful than FG1 or FG2 features.
Thisexplains why models using FG3 features fail to improve over the baseline.
Regardlessof these differences, the analysis indicates that in our noisy setting the bag-of-wordsrepresentation outperforms any individual structured representation.However, the bottom part of the table tells a more interesting story: The secondpart of our analysis indicates that structured representations provide complementaryinformation to the bag-of-words representation.
Even the combination of bag of wordswith the simplest n-gram structures (W + N) always outperforms the bag-of-wordsrepresentation alone.
But the best results are always obtained when the combinationincludes more natural language structures.
The improvements are relatively small, butremarkable (e.g., see FG2) if we take into account the significant scale and settings of theevaluation.
The improvements yielded by natural language structures are statisticallysignificant for all feature groups.
This observation correlates well with the analysisshown in Tables 4 and 5, which shows that features using semantic (R) and syntactic(D) representations contribute the most on top of the IR model (BM25(W)).370Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections5.
Error Analysis and DiscussionSimilar to most re-ranking systems, our system improves the answer quality for somequestions while decreasing it for others.
Table 7 lists the percentage of questions fromour test set that are improved (i.e., the correct answer is ranked higher after re-ranking),worsened (i.e., the correct answer is ranked lower), and unchanged (i.e., the position ofthe correct answer does not change after re-ranking).
The table indicates that, regard-less of the number of candidate answers for re-ranking (N), the number of improvedquestions is approximately twice the number of worsened questions.
This explains theconsistent improvements in P@1 and MRR measured for various values of N. As Nincreases, the number of questions that are improved also grows, which is an expectedconsequence of having more candidate answers to re-rank.
However, the percentageof improved questions grows at a slightly lower rate than the percentage of worsenedquestions.
This indicates that choosing the ideal number of candidate answers to re-rank requires a trade-off: On the one hand, having more candidate answers increasesthe probability of capturing the correct answer in the set; on the other hand, it alsoincreases the probability of choosing an incorrect answer due to the larger numberof additional candidates.
For our problem, it seems that re-ranking using values of Nmuch larger than 100 would not yield significant benefits over smaller values of N.This analysis is consistent with the experiments reported in Table 3 where we did notmeasure significant growth in P@1 or MRR for N larger than 50.Although Table 7 gives the big picture of the behavior of our system, it is importantto look at actual questions that are improved or worsened by the re-ranking model inorder to understand the strengths and weaknesses of our system.
Table 8 lists somerepresentative questions where the re-ranking model brings the correct answer to thetop position.
For every question we list: (a) the correct answer and its position as givenby the baseline IR Model (?Baseline?)
and the re-ranking model (?Re-ranking?
); and (b)the answer that was ranked by the baseline model in the first position and its positionafter re-ranking.Generally, Table 8 indicates that our model performs considerably better than thebag-of-words IRmodel.
For example, we boost the rank of answers that share structureswith the question: for example, the cook ?
grouse syntactico-semantic dependency forthe second sample question or make ?
call and see ?
number for the third example.Modeling structures is important especially for questions with minimal context, that is,short length and common terms, like the third sample question.
Due to the structure-based translation models and/or the generalizations to supersenses or named-entitylabels, our model can match structures even when they are not identical.
For example,Table 7Percentage of questions in the test set that are improved/worsened/unchanged after re-ranking.This experiment used the Perceptron model.Better (%) Worse (%) Unchanged (%)N = 10 33.98 16.81 49.21N = 15 36.76 18.56 44.68N = 25 39.64 20.68 39.68N = 50 42.95 23.30 33.75N = 100 45.18 25.28 29.54371Computational Linguistics Volume 37, Number 2Table 8Examples of questions improved by our re-ranking model.
URLs were replaced with <URL> inanswer texts.
Some non-relevant text was replaced with <...> to save space.
The remaining textmaintains the original capitalization and spelling.
Non-stop question terms are emphasized inthe answers.How would you rank the top 5 NFL teams?
Do your rankings depend on the outcomeof the Colts Vs. Pats Game?Baseline Re-ranking Correct?
Answer Text2 1 yes Ok. How can you think the Chargers are better than the Colts, Ravens, andthe Broncos?
As for the Rankings, this is what I got: 1.
Colts (they actually beata good team) 2.
Bears (for now, they are going to lose to the Giants) 3.
Patriots(the winner of Cotls vs Patriots should be the next #1) 4.
Broncos (They loston a field goal) 5.
Ravens (all about the D)1 2 no Basically in sport rankings you will look at the how they perform theirwins against their loss or their stats.
Ranking is the process of positioningindividuals, groups or businesses on a ordinal scale in relation to others.A list arranged in this way is said to be in rank order.
Some examples: Inmany sports, individuals or teams are given rankings, generally by the sport?sgoverning body.
In football (soccer), national teams are ranked in the FIFAWorld Rankings.
In snooker, players are ranked using the Snooker worldrankings.
In ice hockey, national teams are ranked in the IIHF World Ranking.In golf, the top male golfers are ranked using the Official World Golf Rankingshow can i cook grouse quick with normal household spices w/o going out to buy stuff?Baseline Re-ranking Correct?
Answer Text10 1 yes I?ve never cooked a grouse, but poultry is poultry... You could salt it and pepperit, put some flour on a plate and roll it in the flour to coat it lightly, then heata few tablespoons of olive oil in a skillet and pan-fry it.
(If you have no oliveoil, use a little vegetable oil plus a pat of butter ?
the oil is to keep the butterfrom burning.)
Squeeze a few drops of lemon juice over it if you want.
Or:Skip the flour.
Salt and pepper the grouse.
Pan-fry it in a little olive oil.
Whenit?s looking close to ready, pour in a little white wine which will sizzle andreduce down to a sauce.
If you?ve got some fresh or dried herbs (rosemary,thyme, parsley) you could sprinkle a pinch of that in.1 2 no Well, a grouse is a prey animal.
If there was a decline in the populationof grouse, then the animals that usually prey on the grouse - coyotes, owls,etc - would probably start eating other prey animals, like the pheasants andsquirrels.how did I do for make a call and that the other dont see my number?Baseline Re-ranking Correct?
Answer Text2 1 yes to make a call so that the other person cant see the number... dial *67 and waitfor the three beeps.. then dial the number1 2 no Oneday out of the blue call her.
If u dont have her number, when u see her askher if she wanted to go out oneday then get her number.
When u talk on thephone get to know her.
But dont ask her out too soon because she may notfeel the same way.
After a couple of days or weeks taking to her let her knowhow u felt about her since the first time u met her.372Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb CollectionsTable 8(continued)how can i find a veterinary college with dorms?Baseline Re-ranking Correct?
Answer Text14 1 yes <...> I would say not to look for a specific school of veterinarianmedicine butrather find a creditable University that offers a degree such as Pre-Vet.
Thenfrom there you can attend graduate school to finish up to become a doctor inthat field.
Most major universities will have this degree along with dorms.
Inmy sources you can see that this is just one of many major universities thatoffer Pre-vet medicine.1 7 no Hi there... here?s an instructional video by Cornell University Feline HealthCenter - College of VeterinaryMedicine on how to pill cats: <URL>how to handle commission splits with partners in Real estate?Baseline Re-ranking Correct?
Answer Text5 1 yes My company splits the commissions all evenly.
However many variousagents/brokers are involved (or even think they are involved), it gets splitfurther.
Keeps everyone happy.
No one complains that someone gets ?more?.1 3 no You will find information regarding obtaining a real estate license in Okla-homa at the Oklahoma Real Estate Commission?s website (<URL>) Good luck!for the fourth question, find?
college can bematched to look?
school if the structures aregeneralized toWordNet supersenses.
Translation models are crucial to fetching answersrich in terms related to question concepts.
For example, for the first question, our modelboosts the position of the correct answer due to the large numbers of concepts thatare related to NFL, Colts, and Pats: Ravens, Broncos, Patriots, and so forth.
In the secondexample, our model ranks on the first position the answer containing many conceptsrelated to cook: salt, pepper, flour, tablespoons, oil, skillet, and so on.
In the last example,our model is capable of associating the bigram real estate to agent and broker.
Withoutthese associationsmany answers are lost to false positives provided by the bag-of-wordssimilarity models.
For example, in the first and last examples in the table, the answersselected by the baseline model contain more matches of the questions terms than thecorrect answers extracted by our model.All in all, this analysis proves that non-factoid QA is a complex problem wheremany phenomena must be addressed.
The key for success does not seem to be a uniquemodel, but rather a combination of approaches each capable of addressing differentfacets of the problem.
Our model makes a step forward towards this goal, mainlythrough concept expansion and the exploration of syntactico-semantic structures.
Nev-ertheless, our model is not perfect.
To understand where FMIX fails we performeda manual error analysis on 50 questions where FMIX performs worse than the IRbaseline and we identified seven error classes.
Table 9 lists the distribution of these errorclasses and Table 10 lists sample questions and answers from each class.
Note that thepercentage values listed in Table 9 sum up to more than 100% because the error classesare not exclusive.
We now detail each of these error classes.373Computational Linguistics Volume 37, Number 2Table 9Distribution of error classes in questions where FMIX (Perceptron) performs worse.COMPLEX INFERENCE 38%ELLIPSIS 36%ALSO GOOD 18%REDIRECTION 10%ANSWER QUALITY 4%SPELLING 2%CLARIFICATION 2%COMPLEX INFERENCE: This is the most common class of errors (38%).
Questions in thisclass could theoretically be answered by an automated system but such a system wouldrequire complex reasoning mechanisms, large amounts of world knowledge, and dis-course understanding.
For example, to answer the first question in Table 10, a systemwould have to understand that confronting or being supportive are forms of dealing witha person.
To answer the second question, the system would have to know that creatinga CD at what resolution you need supersedes making a low resolution CD.
Our approachcaptures some simple inference rules through translationmodels but fails to understandcomplex implications such as these.ELLIPSIS: This class of errors is not necessarily a fault of our approach but is rathercaused by the problem setting.
Because in a social QA site each answer responds to aspecific question, discourse ellipsis (i.e., omitting the context set by the question in theanswer text) is common.
This makes some answers (e.g., the third answer in Table 10)ambiguous, hence hard to retrieve automatically.
This affects 36% of the questionsanalyzed.ALSO GOOD: It is a common phenomenon in Yahoo!
Answers that a question is askedseveral times by different users, possibly in a slightly different formulation.
To enableour large scale automatic evaluation, we considered an answer as correct only if it waschosen as the ?best answer?
for the corresponding question.
So in our setting, ?bestanswers?
from equivalent questions are marked as incorrect.
This causes 18% of the?errors?
of the re-ranking model.
One example is the fourth question in Table 10, wherethe answer selected by our re-ranking model is obviously also correct.
It is importantto note that at testing time we do not have access to the questions that generated thecandidate answers for the current test question, that is, the system does not knowwhich questions are answered by the answers in the ALSO GOOD section of Table 10.So the answers in the ALSO GOOD category are not selected based on the similarity ofthe corresponding queries, but rather, based on better semantic matching between testquestion and candidate answer.REDIRECTION: Some answers (10% of the questions analyzed) do not directly answer aquestion but rather redirect the user to relevant URLs (see the fifth question in Table 10).Because we do not extract the text behind URLs in the answer content, such questionsare virtually impossible to answer using our approach.ANSWER QUALITY: For a small number of the questions analyzed (4%) the choice of ?bestanswer?
is dubious (see the sixth example in Table 10).
This is to be expected in asocial QA site, where the selection of best answers is not guaranteed to be optimal.Nevertheless, the relatively small number of such cases is unlikely to influence thequality of the evaluation.374Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb CollectionsTable 10Examples of questions in each error class.
The corresponding error class is listed on the left sideof the question text.
We list the answer ranked at the top position by FMIX only where relevant(e.g., the ALSO GOOD category).
URLs were replaced with <URL> in answer texts.
Somenon-relevant text was replaced with <...> to save space.
The remaining text maintains theoriginal capitalization and spelling.COMPLEX INFERENCE how to deal with a person in denial with M.P.D.
?Baseline Re-ranking Correct?
Answer Text1 6 yes First, i would find out if MPD has been diagnosed by a pro-fessional.
In current terminology, MPD is considered a partof Dissociative Personality Disorder.
In any case, it would beup to the professionals to help this person because you couldcause further problems by confronting this person with whatyou think the problem is.
If this person is a family member,you could ask for a consultation with the psychiatric profes-sional who is treating him/her.
Please, please, just do youbest to be supportive without being confrontational since thatmight make things even worse for that person.COMPLEX INFERENCE How do I make a low resolution CD of 100 phtos that were shot at8 megapixels?Baseline Re-ranking Correct?
Answer Text1 4 yes you can use picasa2 from google: <URL> is free.
In picasayou can do ?GiftCD?
create a CD with you picture at whatresolution you need (including original size)ELLIPSIS How do mineral ions affect the biology and survival of a pond organism?Baseline Re-ranking Correct?
Answer Text1 3 yes Some mineral ions are fertilizer and will increase vegetativegrowth while others are poisons.ALSO GOOD How to learn the British accent?Baseline Re-ranking Correct?
Answer Text4 5 yes Get a dictionary where there is a pronunciation guide whichgives the pronunciation in British English.
Watch britishmovies and imitate what you can.
Then just practice, practicepractice.
But before you go about learning accents, slangs ordialects, make sure you brush up on your basic grammar.<...>3 1 no You can do one of two things: first, go to a local bookstore, likeBarnes and Noble.
They sell cd?s with different accents fromaround the world, accompanied by a book that phoneticallyspells the words.
This is designed for actors/actresses whoneed to learn different accents.
Also, go rent a bunch of britishmovies, or watch british television.
Continually pause andrepeat common phrases and words.375Computational Linguistics Volume 37, Number 2Table 10(continued)REDIRECTION How can I build an easy lean-to shed out of scrap wood and skids?Baseline Re-ranking Correct?
Answer Text6 15 yes the pallet shed... <URL> building a wood shed from pallets...<URL> good ideas from those who?ve been there...<URL> pics.
ofthe shed... <URL> nice pics.
<URL> taking pallets apart... and othertips... <URL> <...>ANSWER QUALITY How do make a Naruto AMV?
Can you show me how?
I need thewebsite or program and the exact directions.
?Baseline Re-ranking Correct?
Answer Text2 94 yes i?m not an expert.
but i sure do like Naruto.
i?ll wait for answerstooSPELLING how does aliquid expansion boiler thrrmosstat work?Baseline Re-ranking Correct?
Answer Text2 4 yes the liquid expands inside the thermostat when the liquid reachesthe shutoff temp or pressure it will shut off the boiler preventingboiler explosionsCLARIFICATION how could you combine your styles and personalities effectively toproduce the best paper?Baseline Re-ranking Correct?
Answer Text29 1 yes Your question is not clear.
Are you asking about writing styles?it also depends on what kind of paper you are writing?
Yourquestion cannot be answered without more info.SPELLING: Two percent (2%) of the error cases analyzed are caused by spelling errors(e.g., the seventh example in Table 10).
Because these errors are relatively infrequent,they are not captured by our translation models, and our current system does notinclude any other form of spelling correction.CLARIFICATION: Another 2% of the questions inspected manually had answers thatpointed to errors or ambiguities in the question text rather than responding to the givenquestion (see the last example in Table 10).
These answers are essentially correct butthey require different techniques to be extracted: Our assumption is that questions arealways correct and sufficient for answer extraction.6.
Related WorkThere is a considerable amount of previous work in several related areas.
First, we willdiscuss related work with respect to the features and models used in this research; mostof this work is to be found in the factoid QA community, where the most sophisticated376Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb CollectionsQA selection and re-ranking algorithms have been developed.
We then review existingwork in non-factoid QA; we will see that in this area there is much less work, and theemphasis has been so far in query re-writing and scalability using relatively simplefeatures andmodels.
Finally wewill discuss relatedwork in the area of community-built(social) QA sites.
Although we do not exploit the social aspect of our QA collection, thisis complementary to our work and would be a natural extension.
Table 11 summarizesaspects of the different approaches discussed in this section, highlighting the differencesand similarities with our current work.Our work borrows ideas from many of the papers mentioned in this section, es-pecially for feature development; indeed our work includes matching features as wellas translation and retrieval models, and operates at the lexical level, the parse treeTable 11Comparison of some of the characteristics of the related work cited.
Task: Document Retrieval(DRet), Answer Extraction (Ex) or Answer Re-ranking or Selection (Sel).Queries: factoid (Fact)or non-factoid (NonFact).
Features: lexical (L), n-grams (Ngr), collocations (Coll), paraphrases(Para), POS, syntactic dependency tree (DT), syntactic constituent tree (CT), named entities (NE),WordNet Relations (WNR), WordNet supersenses (WNSS), semantic role labeling (SRL), causalrelations (CR), query classes (QC), query-log co-ocurrences (QLCoOcc).Models: bag-of-wordsscoring (BOW), tree matching (TreeMatch), linear (LM), log-linear (LLM), statistical learning(kernel) (SL), probabilistic grammar (PG), statistical machine translation (SMT), query likelihoodlanguage model (QLLM).Development and Evaluation: data sizes used, expressed as numberof queries/number of query?answer pairs (i.e., sum of all candidate answers per question).Data: type of source used for feature construction, training and/or evaluation.
Question marksare place holders for information not available or not applicable in the corresponding work.Publication Task Queries Features Models Devel Eval DataAgichtein et al DRet NonFact L, Ngr, Coll BOW, LM ?/10K 50/100 WWW(2001)Echihabi and Sel Fact, L, Ngr, Coll, SMT 4.6K/100K 1K/300K?
TREC, KM,Marcu (2003) NonFact DT, NE, WWWWNHigashinaka and Sel NonFact L, WN, SRL, SL 1K/500K 1K/500K WHYQAIsozaki (2008) (Why) CRPunyakanok et al Sel Fact L, POS, DT, TreeMatch ?/400 TREC13(2004) NE, QCRiezler et al DRet NonFact L, Ngr, Para SMT 10M/10M 60/1.2K WWW, FAQ(2007)Soricut and Brill DRet, NonFact L, Ngr, Coll BOW, SMT 1M/?
100/?
WWW, FAQ(2006) Sel,ExVerberne et al Sel NonFact CT, WN, BOW, LLM same as eval 186/28K Webclopedia,(2010) (Why) Para WikipediaWang et al (2007) Sel Fact L, POS, DT, LLM, PG 100/1.7K 200/1.7K TREC13NE, WNR,HybXue et al (2008) DRet, NonFact, L, Coll SMT, QLLM 1M/1M 50/?
SocQA,Sel Fact TREC9This work Sel NonFact L, Ngr, POS, TreeMatch, 112K/1.6M 28K/up to SocQA,(How) DT, SRL, BOW, SMT, 2.8M QLogNE, WN, SLWNSS,Hyb,QLCoOcc377Computational Linguistics Volume 37, Number 2level, as well as the level of semantic roles, named entities, and lexical semantic classes.However, to the best of our knowledge no previous work in QA has evaluated the useof so many types of features concurrently, nor has it built so many combinations of thesefeatures at different levels.
Furthermore, we employ unsupervised methods, generativemethods, and supervised learning methods.
This is made possible by the choice of thetask and the data collection, another novelty of our work which should enable futureresearch in complex linguistic features for QA and ranking.Factoid QA.
Within the statistical machine translation community there has beenmuch research on the issue of automatically learning transformations (at the lexical,syntactical, and semantical level).
Some of this work has been applied to automatedQA systems, mostly for factoid questions.
For example, Echihabi and Marcu (2003)presented a noisy-channel approach (IBM model 4) adapted for the task of QA.
Thefeatures used included lexical and parse-tree elements as well as some named entities(such as dates).
They use a dozen heuristic rules to heavily reduce the feature space andchoose a single representation mode for each of the tokens in the queries (for example:?terms overlapping with the question are preserved as surface text?)
and learn languagemodels on the resulting representation.
We extend Echihabi and Marcu by consideringdeeper semantic representations (such as SRL andWNSS), but instead of using selectionheuristics we learn models from each of the full representations (as well as from somehybrid representations) and then combine them using discriminant learning techniques.Punyakanok, Roth, and Yih (2004) attempted a more comprehensive use of theparse tree information, computing a similarity score between question and answerparse trees (using a distance function based on approximate tree matching algorithms).This is an unsupervised approach, which is interesting especially when coupled withappropriate distances.
Shen and Joshi (2005) extend this idea with a supervised learningapproach, training dependency tree kernels to compute the similarity.
In our work wealso used this type of feature, although we show that, in our context, features based ondependency tree kernels are subsumed by simpler features that measure the overlapof binary dependencies.
Another alternative is proposed by Cui et al (2005), wheresignificant words are aligned and similarity measures (based on mutual information ofcorrelations) are then computed on the resulting dependency paths.
Shen and Klakow(2006) extend this using a dynamic time warping algorithm to improve the alignmentfor approximate question phrase mapping, and learn a Maximum Entropy model tocombine the obtained scores for re-ranking.
Wang, Smith, andMitamura (2007) proposeto use a probabilistic quasi-synchronous grammar to learn the syntactic transformationsbetween questions and answers.
We extend the work of Cui et al by considering pathswithin and across different representations beyond dependency trees, although we donot investigate the issue of alignment specifically?instead we use standard statisticaltranslation models for this.Non-factoid QA.
The previous works dealt with the problem of selection, that is,finding the single sentence that correctly answers the question out of a set of candidatedocuments.
A related problem in QA is that of retrieval: selecting potentially relevantdocuments or sentences prior to the selection phase.
This problem is closer to gene-ral document retrieval and it is therefore easier to generalize to the non-factoid domain.Retrieval algorithms tend to be much simpler than selection algorithms, however, inpart due to the need for speed, but also because there has been little previous evidencethat complex algorithms or deeper linguistic analysis helps at this stage, especially inthe context of non-factoid questions.378Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb CollectionsPrevious work addressed the task by learning transformations between questionsand answers and using them to improve retrieval.
All these works use only lexicalfeatures.
For example, Agichtein et al (2001) learned lexical transformations (from theoriginal question to a set of Web search queries, from ?what is a?
to ?the term?, ?standsfor?, etc.)
which are likely to retrieve good candidate documents in commercial Websearch engines; they applied this successfully to large-scale factoid and non-factoid QAtasks.
Murdock and Croft (2005) study the problem of candidate sentence retrieval forQA and show that a lexical translation model can be exploited to improve factoid QA.Xue, Jeon, and Croft (2008) show that a linear interpolation of translation models anda query likelihood language model outperforms each individual model for a QA taskthat is independent of the question type.
In the same space, Riezler et al (2007) developSMT-based query expansionmethods and use them for retrieval from FAQpages.
In ourwork we did not address the issue of query expansion and re-writing directly: Whileour re-ranking approach is limited to the recall of the retrieval model, these methods ofquery transformation could be used in a complementary manner to improve the recall.Even more interesting would be to couple the two approaches in an efficient manner;this remains as future work.There has also been some work in the problem of selection for non-factoid ques-tions.
Girju (2003) extracts non-factoid answers by searching for certain semantic struc-tures (e.g., causation relations as answers to causation questions).
We generalized thismethodology (in the form of semantic roles) and evaluated it systematically.
Soricutand Brill (2006) develop a statistical model by extracting (in an unsupervised manner)QA pairs from one million FAQs obtained from the Web.
They show how differentstatistical models may be used for the problems of ranking, selection, and extractionof non-factoid QAs on the Web; due to the scale of their problem they only consider lex-ical n-grams and collocations, however.
More recent work has showed that structuredretrieval improves answer ranking for factoid questions: Bilotti et al (2007) showed thatmatching predicate?argument frames constructed from the question and the expectedanswer types improves answer ranking.
Cui et al (2005) learned transformations ofdependency paths from questions to answers to improve passage ranking.
All theseapproaches use similarity models at their core because they require the matching ofthe lexical elements in the search structures, however.
On the other hand, our approachallows the learning of full transformations from question structures to answer structuresusing translation models applied to different text representations.The closest work to ours is that of Higashinaka and Isozaki (2008) and Verberneet al (2010), both on Why questions.
Higashinaka et al consider a wide range ofsemantic features by exploiting WordNet and gazetteers, semantic role labeling, andextracted causal relations.
Verberne et al exploit syntactic information from constituenttrees, WordNet synonymy sets and relatedness measures, and paraphrases.
As in ourmodels, both these works combine these features using discriminative learning tech-niques and apply the learned models to re-rank answers to non-factoid questions (Whytype questions).
Their features, however, are based on counting matches or eventsdefined heuristically.
We have extended this approach in several ways.
First, we use amuch larger feature set that includes correlation and transformation-based features andfive different content representations.
Second, we use generative (translation) modelsto learn transformation functions before they are combined by the discriminant learner.Finally, we carry out training and evaluation at a much larger scale.Content from community-built question?answer sites can be retrieved by searchingfor similar questions already answered (Jeon, Croft, and Lee 2005) and ranked usingmeta-data information like answerer authority (Jeon et al 2006; Agichtein et al 2008).379Computational Linguistics Volume 37, Number 2Here we show that the answer text can be successfully used to improve answer rankingquality.
Our method is complementary to the earlier approaches.
It is likely that anoptimal retrieval engine from social media would combine all three methodologies.Moreover, our approach might have applications outside of social media (e.g., for open-domainWeb-based QA), because the rankingmodel built is based only on open-domainknowledge and the analysis of textual content.7.
ConclusionsIn this work we describe an answer ranking system for non-factoid questions builtusing a large community-generated question?answer collection.
We show that the bestranking performance is obtained when several strategies are combined into a singlemodel.
We obtain the best results when similarity models are aggregated with featuresthat model question-to-answer transformations, frequency and density of content, andcorrelation of QA pairs with external collections.
Although the features that modelquestion-to-answer transformations provide the most benefits, we show that the com-bination is crucial for improvement.
Further, we show that complex linguistic features,most notably semantic role dependencies and semantic labels derived from WordNetsenses, yield a statistically significant performance increase on top of the traditionalbag-of-words and n-gram representations.
We obtain these results using only off-the-shelf NL processors that were not adapted in any way for our task.
As a side effect, ourexperiments prove that we can effectively exploit large amounts of availableWeb data todo research on NLP for non-factoid QA systems, without any annotation or evaluationcost.
This provides an excellent framework for large-scale experimentation with variousmodels that otherwise might be hard to understand or evaluate.As implications of our work, we expect the outcome of our investigation to helpseveral applications, such as retrieval from social media and open-domain QA on theWeb.
On social media, for example, our system should be combined with a componentthat searches for similar questions already answered; the output of this ensemble canpossibly be filtered further by a content-quality module that explores ?social?
featuressuch as the authority of users, and so on.
Although we do not experiment on Wikipediaor news sites in this work, one can view our data as a ?worse-case scenario,?
given itsungrammaticality and annotation quality.
It seems reasonable to expect that training ourmodel on cleaner data (e.g., fromWikipedia or news), would yield even better results.This work can be extended in several directions.
First, answers that were not se-lected as best, but were marked as good by a minority of voters, could be incorporatedin the training data, possibly introducing a graded notion of relevance.
This wouldmakethe learning problemmore interesting andwould also provide valuable insights into thepossible pitfalls of user-annotated data.
It is not clear if more data, but of questionablequality, is beneficial.
Another interesting problem concerns the adaptation of the re-ranking model trained on social media to collections from other genres and/or domains(news, blogs, etc.).
To our knowledge, this domain adaptation problem for QA has notbeen investigated yet.ReferencesAgichtein, Eugene, Carlos Castillo, DeboraDonato, Aristides Gionis, and Gilad Mishne.2008.
Finding high-quality content in socialmedia, with an application to community-based question answering.
In Proceedings ofthe Web Search and Data Mining Conference(WSDM), pages 183?194, Stanford, CA.Agichtein, Eugene, Steve Lawrence, and LuisGravano.
2001.
Learning search enginespecific query transformations for questionanswering.
In Proceedings of the World380Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb CollectionsWide Web Conference, pages 169?178,Hong Kong.Attardi, Giuseppe, Felice Dell?Orletta, MariaSimi, Atanas Chanev, and MassimilianoCiaramita.
2007.
Multilingual dependencyparsing and domain adaptation usingDeSR.
In Proceedings of the Shared Taskof the Conference on ComputationalNatural Language Learning (CoNLL),pages 1112?1118, Prague.Berger, Adam, Rich Caruana, David Cohn,Dayne Freytag, and Vibhu Mittal.
2000.Bridging the lexical chasm: Statisticalapproaches to answer finding.
InProceedings of the 23rd Annual InternationalACM SIGIR Conference on Research &Development on Information Retrieval,pages 192?199, Athens, Greece.Bilotti, Matthew W., Paul Ogilvie, JamieCallan, and Eric Nyberg.
2007.
Structuredretrieval for question answering.
InProceedings of the 30th Annual InternationalACM SIGIR Conference on Research &Development on Information Retrieval,pages 351?358, Amsterdam.Brill, Eric, Jimmy Lin, Michele Banko,Susan Dumais, and Andrew Ng.
2001.Data-intensive question answering.In Proceedings of the Text REtrievalConference (TREC), pages 393?400,Gaithersburg, MD, USA.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings ofthe North American Chapter of the Associationfor Computational Linguistics (NAACL),pages 132?139, Seattle, WA.Ciaramita, Massimiliano and YaseminAltun.
2006.
Broad coverage sensedisambiguation and informationextraction with a supersense sequencetagger.
In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 594?602,Sidney.Ciaramita, Massimiliano, Giuseppe Attardi,Felice Dell?Orletta, and Mihai Surdeanu.2008.
Desrl: A linear-time semantic rolelabeling system.
In Proceedings of the SharedTask of the 12th Conference on ComputationalNatural Language Learning (CoNLL-2008),pages 258?262, Manchester.Ciaramita, Massimiliano and Mark Johnson.2003.
Supersense tagging of unknownnouns in WordNet.
In Proceedings of the2003 Conference on Empirical Methods inNatural Language Processing,pages 168?175, Sapporo.Ciaramita, Massimiliano, Vanessa Murdock,and Vassilis Plachouras.
2008.
Semanticassociations for contextual advertising.Journal of Electronic CommerceResearch?Special Issue on OnlineAdvertising and Sponsored Search, 9(1):1?15.Collins, Michael and Nigel Duffy.
2001.Convolution kernels for natural language.In Proceedings of the Neural InformationProcessing Systems Conference (NIPS),pages 625?632, Vancouver, Canada.Cui, Hang, Renxu Sun, Keya Li, Min-YenKan, and Tat-Seng Chua.
2005.
Questionanswering passage retrieval usingdependency relations.
In Proceedings of the28th Annual International ACM SIGIRConference on Research & Development inInformation Retrieval, pages 400?407,Salvador.Echihabi, Abdessamad and Daniel Marcu.2003.
A noisy-channel approach toquestion answering.
In Proceedings of the41st Annual Meeting of the Association forComputational Linguistics (ACL),pages 16?23, Sapporo.Freund, Yoav and Robert E. Schapire.
1999.Large margin classification using theperceptron algorithm.Machine Learning,37:277?296.Girju, Roxana.
2003.
Automatic detection ofcausal relations for question answering.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics(ACL), Workshop on MultilingualSummarization and Question Answering,pages 76?83, Sapporo.Harabagiu, Sanda, Dan Moldovan, MariusPasca, Rada Mihalcea, Mihai Surdeanu,Razvan Bunescu, Roxana Girju, Vasile Rus,and Paul Morarescu.
2000.
Falcon:Boosting knowledge for answer engines.In Proceedings of the Text REtrievalConference (TREC), pages 479?487,Gaithersburg, MD.Higashinaka, Ryuichiro and Hideki Isozaki.2008.
Corpus-based question answeringfor why-questions.
In Proceedings of theThird International Joint Conference onNatural Language Processing (IJCNLP),pages 418?425, Hyderabad.Jeon, Jiwoon, W. Bruce Croft, andJoon Hoo Lee.
2005.
Finding similarquestions in large question and answerarchives.
In Proceedings of the ACMConference on Information and Knowledge381Computational Linguistics Volume 37, Number 2Management (CIKM), pages 84?90,Bremen.Jeon, Jiwoon, W. Bruce Croft, Joon Hoo Lee,and Soyeon Park.
2006.
A framework topredict the quality of answers withnon-textual features.
In Proceedings of the29th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 228?235,Seattle, WA.Joachims, Thorsten.
2006.
Training linearsvms in linear time.
In KDD ?06:Proceedings of the 12th ACM SIGKDDInternational Conference on KnowledgeDiscovery and Data Mining, pages 217?226,New York, NY.Ko, Jeongwoo, Teruko Mitamura, and EricNyberg.
2007.
Language-independentprobabilistic answer ranking for questionanswering.
In Proceedings of the 45thAnnual Meeting of the Association forComputational Linguistics, pages 784?791,Prague.Li, Xin and Dan Roth.
2006.
Learningquestion classifiers: The role of semanticinformation.
Natural Language Engineering,12:229?249.Li, Yaoyong, Hugo Zaragoza, Ralf Herbrich,John Shawe-Taylor, and Jaz S. Kandola.2002.
The perceptron algorithm withuneven margins.
In Proceedings of theNineteenth International Conference onMachine Learning, pages 379?386,Sidney.Magnini, Bernardo, Matteo Negri, RobertoPrevete, and Hristo Tanev.
2002.Comparing statistical and content-basedtechniques for answer validation on theweb.
In Proceedings of the VIII ConvegnoAI*IA, Siena, Italy.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Moldovan, Dan, Sanda Harabagiu, MariusPasca, Rada Mihalcea, Richard Goodrum,Roxana Girju, and Vasile Rus.
1999.Lasso?a tool for surfing the answer net.
InProceedings of the Text REtrieval Conference(TREC), pages 175?183, Gaithersburg, MD.Moschitti, Alessandro, Silvia Quarteroni,Roberto Basili, and Suresh Manandhar.2007.
Exploiting syntactic and shallowsemantic kernels for question/answerclassification.
In Proceedings of the 45thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 776?783, Prague.Murdock, Vanessa and W. Bruce Croft.
2005.A translation model for sentence retrieval.In Proceedings of the Conference on HumanLanguage Technology and Empirical Methodsin Natural Language Processing,pages 684?691, Vancouver.Palmer, Martha, Daniel Gildea, and PaulKingsbury.
2005.
The proposition bank: Anannotated corpus of semantic roles.Computational Linguistics, 31(1):71?106Punyakanok, Vasin, Dan Roth, and Wen-tauYih.
2004.
Mapping dependencies trees:An application to question answering.Proceedings of AI&Math 2004, pages 1?10,Fort Lauderdale, FL.Riezler, Stefan, Alexander Vasserman,Ioannis Tsochantaridis, Vibhu Mittal,and Yi Liu.
2007.
Statistical machinetranslation for query expansion inanswer retrieval.
In Proceedings of the45th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 464?471, Prague.Robertson, Stephen and Stephen G. Walker.1997.
On relevance weights with littlerelevance information.
In Proceedings ofthe Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 16?24,New York, NY.Shen, Dan and Dietrich Klakow.
2006.Exploring correlation of dependencyrelation paths for answer extraction.
InProceedings of the 21st InternationalConference on Computational Linguistics andthe 44th Annual Meeting of the Association forComputational Linguistics, pages 889?896,Sydney.Shen, Libin and Aravind K. Joshi.
2005.Ranking and reranking with perceptron.Machine Learning.
Special Issue on Learningin Speech and Language Technologies,60(1):73?96.Soricut, Radu and Eric Brill.
2006.
Automaticquestion answering using the Web:Beyond the factoid.
Journal of InformationRetrieval?Special Issue on Web InformationRetrieval, 9(2):191?206.Surdeanu, Mihai and MassimilianoCiaramita.
2007.
Robust informationextraction with perceptrons.
In Proceedingsof the NIST 2007 Automatic ContentExtraction Workshop (ACE07), College Park,MD.
Available at: http://www.surdeanu.name/mihai/papers/ace07a.pdf.Surdeanu, Mihai, Richard Johansson, AdamMeyers, Lluis Marquez, and Joakim Nivre.2008.
The CoNLL-2008 shared task on jointparsing of syntactic and semantic382Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collectionsdependencies.
In Proceedings of theConference on Computational NaturalLanguage Learning (CoNLL), pages 159?177,Manchester.Surdeanu, Mihai, Lluis Marquez, XavierCarreras, and Pere R. Comas.
2007.Combination strategies for semantic rolelabeling.
Journal of Artificial IntelligenceResearch, 29:105?151.Tao, Tao and ChengXiang Zhai.
2007.
Anexploration of proximity measures ininformation retrieval.
In Proceedings of the30th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 259?302,Amsterdam.Tsochantaridis, Ioannis, Thomas Hofmann,Thorsten Joachims, and Yasemin Altun.2004.
Support vector machine learning forinterdependent and structured outputspaces.
In ICML ?04: Proceedings of theTwenty-First International Conference onMachine Learning, pages 104?111,New York, NY.Verberne, Suzan, Lou Boves, NellekeOostdijk, and Peter-Arno Coppen.
2010.What is not in the bag of words forwhy-qa?
Computational Linguistics,36(2):229?245.Voorhees, Ellen M. 2001.
Overview of theTREC-9 question answering track.
InProceedings of the Text REtrieval Conference(TREC) TREC-9 Proceedings, pages 1?15,Gaithersburg, MD.Wang, Mengqiu, Noah A. Smith, and TerukoMitamura.
2007.
What is the Jeopardymodel?
A quasi-synchronous grammar forQA.
In Proceedings of the 2007 JointConference on Empirical Methods in NaturalLanguage Processing and ComputationalNatural Language Learning, pages 22?32,Prague.Xue, Xiaobing, Jiwoon Jeon, and W. BruceCroft.
2008.
Retrieval models for questionand answer archives.
In Proceedings of theAnnual ACM SIGIR Conference on Researchand Development in Information Retrieval,pages 475?482, Singapore.383
