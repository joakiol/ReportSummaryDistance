Automatic error detection in the Japanese learners?
English spoken dataEmi IZUMI?
?emi@crl.go.jpKiyotaka UCHIMOTO?uchimoto@crl.go.jpToyomi SAIGA?hoshi@karl.tis.co.jpThepchai Supnithi*thepchai@nectec.or.thHitoshi ISAHARA?
?isahara@crl.go.jpAbstractThis paper describes a method ofdetecting grammatical and lexical errorsmade by Japanese learners of Englishand other techniques that improve theaccuracy of error detection with a limitedamount of training data.
In this paper, wedemonstrate to what extent the proposedmethods hold promise by conductingexperiments using our learner corpus,which contains information on learners?errors.1 IntroductionOne of the most important things in keeping upwith our current information-driven society is theacquisition of foreign languages, especiallyEnglish for international communications.
Indeveloping a computer-assisted language teachingand learning environment, we have compiled alarge-scale speech corpus of Japanese learnerEnglish, which provides a great deal of usefulinformation on the construction of a model for thedevelopmental stages of Japanese learners?speaking abilities.In the support system for language learning,we have assumed that learners must be informedof what kind of errors they have made, and inwhich part of their utterances.
To do this, we needto have a framework that will allow us to detectlearners?
errors automatically.In this paper, we introduce a method of detect-ing learners?
errors, and we examine to what ex-tent this could be accomplished using our learnercorpus data including error tags that are labeledwith the learners?
errors.2 SST CorpusThe corpus data was based entirely on audio-recorded data extracted from an interview test, the?Standard Speaking Test (SST)?.
The SST is aface-to-face interview between an examiner andthe test-taker.
In most cases, the examiner is anative speaker of Japanese who is officiallycertified to be an SST examiner.
All theinterviews are audio-recorded, and judged by twoor three raters based on an SST evaluation scheme(SST levels 1 to 9).
We recorded 300 hours ofdata, totaling one million words, and transcribedthis.2.1 Error tagsWe designed an original error tagset forlearners?
grammatical and lexical errors, whichwere relatively easy to categorize.
Our error tagscontained three pieces of information, i.e., the partof speech, the grammatical/lexical system and thecorrected form.
We prepared special tags for someerrors that cannot be categorized into any wordclass, such as the misordering of words.
Our errortagset currently consists of 45 tags.
The followingexample is a sentence with an error tag.
*I lived in <atcrr="">the</at> New Jersey.at indicates that it is an article error, andcrr=??
means that the corrected form does not?Computational Linguistics Group, Communications Research Laboratory,3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan?Graduate School of Science and Technology, Kobe University, 1-1 Rokkodai, Nada-ku, Kobe, Japan?TIS Inc., 9-1 Toyotsu, Suita, Osaka, Japan*National Electronics and Computer Technology Center,112 Pahonyothin Road, Klong 1, Klong Luang, Pathumthani, 12120, Thailandneed an article.
By referring to information on thecorrected form indicated in an error tag, the sys-tem can convert erroneous parts into correctedequivalents.3 Error detection methodIn this section, we would like to describe howwe proceeded with error detection in the learnercorpus.3.1 Types of errorsWe first divided errors into two groups de-pending on how their surface structures were dif-ferent from those of the correct ones.
The first wasan ?omission?-type error, where the necessaryword was missing, and an error tag was inserted tointerpolate it.
The second was a ?replacement?-type error, where the erroneous word was en-closed in an error tag to be replaced by the cor-rected version.
We applied different methods todetecting these two kinds of errors.3.2 Detection of omission-type errorsOmission-type errors were detected by estimat-ing whether or not a necessary word string wasmissing in front of each word, including delimit-ers.
We also estimated to which category the errorbelonged during this process.
What we call ?errorcategories?
here means the 45 error categories thatare defined in our error tagset.
(e.g.
article andtense errors) These are different from ?errortypes?
(omission or replacement).
As we can seefrom Fig.
1, when more than one error category isgiven, we have two ways of choosing the best one.Method A allows us to estimate whether there is amissing word or not for each error category.
Thiscan be considered the same as deciding which ofthe two labels (E: ?There is a missing word.?
or C:?There is no missing word.?)
should be inserted infront of each word.
Here, there is an article miss-ing in front of ?telephone?, so this can be consid-ered an omission-type error, which is categorizedas an article error (?at?
is a label that indicates thatthis is an article error.).
In Method B, if N errorcategories come up, we need to choose the mostappropriate error category ?k?
from among N+1categories, which means we have added one morecategory (+1) of ?There is no missing word.?
(la-beled with ?C?)
to the N error categories.
This canbe considered the same as putting one of the N+1labels in front of each word.
If there is more thanone error tag inserted at the same location, theyare combined to form a new error tag.As we can see from Fig.
2, we referred to 23pieces of information to estimate the error cate-gory: two preceding and following words, theirword classes, their root forms, three combinationsof these (one preceding word and one followingword/two preceding words and one followingword/one preceding word and two followingwords), and the first and last letter of the wordimmediately following.
(In Fig.
2, ?t?
and ?e?
in?telephone?.)
The word classes and root formswere acquired with ?TreeTagger?.
(Shmid 1994)3.3 Detection of replacement-type errorsReplacement-type errors were detected by es-timating whether or not each word should be de-leted or replaced with another word string.
Theerror category was also estimated during thisprocess.
As we did in detecting omission-type er-rors, if more than one error category was given,we use two methods of detection.
Method C wasused to estimate whether or not the word shouldbe replaced with another word for each error cate-gory, and if it was to be replaced, the model esti-mated whether the word was located at thebeginning, middle or end of the erroneous part.
Aswe can see from Fig.
3, this can be considered the Figure 2.
Features used for detecting omission-type errorsWord   POS    Root formthere     EX       thereis      VBZ       betelephone     NN       telephoneand      CC       andthe  DT       thebooks NNS       books.
SENT       .
t e?
:feature combination      :single feature?ErroneouspartFigure 1.
Detection of omission-type errors whenthere are more than one (N) error categories.Method A* there is telephone and the books .E: There is a missing wordC: There is no missing word (=correct)Mehod B* there is telephone and the books .Ek: There is a missing word and the related errorcategory is k (1?k?N)C: There is no missing word (=correct)?C?C?Ek?C?C?C?C?C?C?E?C?C?C?Csame as deciding which of the three labels (Eb:?The word is at the beginning of the erroneouspart.
?, Ee: ?The word is in the middle or end.?
orC: ?The word is correct.?)
must be applied to eachword.
Method D was used if N error categoriescame up and we chose an appropriate one for theword from among 2N+1 categories.
?2N+1 cate-gories?
means that we divided N categories intotwo groups, i.e., where the word was at the begin-ning of the erroneous part and where the word wasnot at the beginning, and we added one morewhere the word neither needed to be deleted norreplaced.
This can be considered the same as at-taching one of the 2N+1 labels to each word.
Todo this, we applied Ramshaw?s IOB scheme(Lance 1995).
If there was more than one error tagattached to the same word, we only referred to thetag that covered the highest number of words.As Fig.
4 reveals, 32 pieces of information arereferenced to estimate an error category, i.e., thetargeted word and the two preceding and follow-ing words, their word classes, their root forms,five combinations of these (the targeted word, theone preceding and one following/ the targetedword and the one preceding/ the targeted wordand the one following/ the targeted word and thetwo preceding/ the targeted word and the two fol-lowing), and the first and last letters of the word.3.4 Use of machine learning modelThe Maximum Entropy (ME) model (Jaynes1957) is a general technique that is used to esti-mate the probability distributions of data.
Theover-riding principle in ME is that when nothingis known, the distribution should be as uniform aspossible, i.e., maximum entropy.
We calculatedthe distribution of probabilities p(a,b) with thismethod when Eq.
1 was satisfied and Eq.
2 wasmaximized.
We then selected the category withmaximum probability, as calculated from this dis-tribution of probabilities, to be the correct cate-gory.
(2)   )),(log(),(             )()1((1)          ),(),(~       ),(),(,, ,??
?????
???=??
?=BbAajBbAa BbAajjbapbappHkjfforbagbapbagbapWe assumed that the constraint of feature setsfi (i?j?k) was defined by Eq.
1.
This is where Ais a set of categories and B is a set of contexts,and gj(a,b) is a binary function that returns value 1when feature fj exists in context b and the categoryis a.
Otherwise, gj(a,b) returns value 0. p~ (a,b) isthe occurrence rate of the pair (a,b) in the trainingdata.4 Experiment4.1 Targeted error categoriesWe selected 13 error categories for detection.Table 1.
Error categories to be detectedNoun Number error, Lexical errorVerb Erroneous subject-verb agreement, Tense error,Compliment errorAdjective Lexical errorAdverb Lexical errorPreposition Lexical error on normal and dependent prepositionArticle Lexical errorPronoun Lexical errorOthers Collocation errorFigure 4.
The features used for detecting replace-ment-type errors?
:feature combination      :single featureWord     POS         Root formthere     EX         thereis      VBZ         betelephone     NN         telephoneand      CC         andthe      DT         thebooks     NNS         bookon      IN         onthe      DT         thedesk      NN         NN.
SENT         .t e?ErroneouspartFigure 3.
Detection of replacement-type errorswhen there are more than one (N) error categories.Method C* there is telephone and the books on the desk.Eb: The word in the beginning of the part whichshould be replaced.Ee: The word in the middle or the end of the partwhich should be replaced.C: no need to be replaced (=correct)Mehod D* there is telephone and the books on the desk.Ebk: The word in the beginning of the part whichshould be replaced and which error category is k.Eek: The word in the middle or the end of the partwhich should be replaced and which error categoryis k. (1?k?N)C: no need to be replaced (=correct)?C?C?C?Eb?C?C?C?C?C?C?C?C?Ebk?C?C?C?C?C4.2 Experiment based on tagged dataWe obtained data from 56 learners?
with errortags.
We used 50 files (5599 sentences) as thetraining data, and 6 files (617 sentences) as thetest data.We tried to detect each error category using themethods discussed in Sections 3.2 and 3.3.
Therewere some error categories that could not be de-tected because of the lack of training data, but wehave obtained the following results for article er-rors which occurred most frequently.Article errorsOmission- Recall rate 8/71 * 100 = 32.39(%)type errors Precision rate 8/11 * 100 = 52.27(%)Replacement- Recall rate 0/43 * 100 =  9.30(%)type errors Precision rate 0/ 1 * 100 =  22.22(%)Results for 13 errors were as follows.All errorsOmission- Recall rate 21/ 93 * 100 = 22.58(%)type errors Precision rate 21/ 38 * 100 = 55.26(%)Replacement- Recall rate 5/224 * 100 =  2.23(%)type errors Precision rate 5/ 56 * 100 =  8.93(%)We assumed that the results were inadequatebecause we did not have sufficient training data.To overcome this, we added the correct sentencesto see how this would affect the results.4.3 Addition of corrected sentencesAs discussed in Section 2.1, our error tags pro-vided a corrected form for each error.
If the erro-neous parts were replaced with the correctedforms indicated in the error tags one-by-one, ill-formed sentences could be converted into cor-rected equivalents.
We did this with the 50 itemsof training data to extract the correct sentencesand then added them to the training data.
We alsoadded the interviewers?
utterances in the entirecorpus data (totaling 1202 files, excluding 6 thatwere used as the test data) to the training data ascorrect sentences.
We added a total of 104925correct new sentences.
The results we obtained bydetecting article errors with the new data were asfollows.Article errorsOmission- Recall rate 8/71 * 100 = 11.27(%)type errors Precision rate 8/11 * 100 = 72.73(%)Replacement- Recall rate 0/43 * 100 =  0.00(%)type errors Precision rate 0/ 1 * 100 =  0.00(%)We found that although the recall rate de-creased, the precision rate went up through addingcorrect sentences to the training data.We then determined how we could improvethe results by adding the artificially made errors tothe training data.4.4 Addition of sentences with artificiallymade errorsWe did this only for article errors.
We first ex-amined what kind of errors had been made witharticles and found that ?a?, ?an?, ?the?
and theabsence of articles were often confused.
We madeup pseudo-errors just by replacing the correctlyused articles with one of the others.
The results ofdetecting article errors using the new training data,including the new corrected sentences describedin Section 4.2, and 7558 sentences that containedartificially made errors were as follows.Article errorsOmission- Recall rate 24/71 * 100 = 33.80(%)type errors Precision rate 24/30 * 100 = 80.00(%)Replacement- Recall rate 2/43 * 100 =  4.65(%)type errors Precision rate 2/ 9 * 100 = 22.22(%)We obtained a better recall and precision ratefor omission-type errors.There were no improvements for replacement-type errors.
Since some more detailed contextmight be necessary to decide whether ?a?
or ?the?must be used, the features we used here might beinsufficient.5 ConclusionIn this paper, we explained how errors inlearners?
spoken data could be detected and in theexperiment, using the corpus as it was, the recallrate was about 30% and the precision rate wasabout 50%.
By adding corrected sentences andartificially made errors, the precision rate rose to80% while the recall rate remained the same.ReferencesHelmut  Schmid  Probabilistic  part-of-Speechtagging using decision trees.
In Proceedings of In-ternational Conference on New Methods in Lan-guage Processing.
pp.
44-49, 1994.Lance A. Ramshaw and Mitchell P. Marcus.
Textchunking using transformation-based learning.
InProceedings of the Third ACL Workshop on VeryLarge Corpora, pp.
82-94, 1995.Jaynes, E. T. ?Information Theory and Statistical Me-chanics?
Physical Review, 106, pp.
620-630, 1957.
