Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597?603,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsNonparametric Bayesian Inference and Efficient Parsing forTree-adjoining GrammarsElif Yamangil and Stuart M. ShieberHarvard UniversityCambridge, Massachusetts, USA{elif, shieber}@seas.harvard.eduAbstractIn the line of research extending statis-tical parsing to more expressive gram-mar formalisms, we demonstrate for thefirst time the use of tree-adjoining gram-mars (TAG).
We present a Bayesian non-parametric model for estimating a proba-bilistic TAG from a parsed corpus, alongwith novel block sampling methods andapproximation transformations for TAGthat allow efficient parsing.
Our workshows performance improvements on thePenn Treebank and finds more compactyet linguistically rich representations ofthe data, but more importantly providestechniques in grammar transformation andstatistical inference that make practicalthe use of these more expressive systems,thereby enabling further experimentationalong these lines.1 IntroductionThere is a deep tension in statistical modeling ofgrammatical structure between providing good ex-pressivity ?
to allow accurate modeling of thedata with sparse grammars ?
and low complexity?
making induction of the grammars (say, froma treebank) and parsing of novel sentences com-putationally practical.
Tree-substitution grammars(TSG), by expanding the domain of locality ofcontext-free grammars (CFG), can achieve betterexpressivity, and the ability to model more con-textual dependencies; the payoff would be bettermodeling of the data or smaller (sparser) modelsor both.
For instance, constructions that go acrosslevels, like the predicate-argument structure of averb and its arguments can be modeled by TSGs(Goodman, 2003).Recent work that incorporated Dirichlet pro-cess (DP) nonparametric models into TSGs hasprovided an efficient solution to the dauntingmodel selection problem of segmenting trainingdata trees into appropriate elementary fragmentsto form the grammar (Cohn et al, 2009; Post andGildea, 2009).
The elementary trees combined ina TSG are, intuitively, primitives of the language,yet certain linguistic phenomena (notably variousforms of modification) ?split them up?, preventingtheir reuse, leading to less sparse grammars thanmight be ideal (Yamangil and Shieber, 2012; Chi-ang, 2000; Resnik, 1992).TSGs are a special case of the more flexiblegrammar formalism of tree adjoining grammar(TAG) (Joshi et al, 1975).
TAG augments TSGwith an adjunction operator and a set of auxil-iary trees in addition to the substitution operatorand initial trees of TSG, allowing for ?splicing in?of syntactic fragments within trees.
This func-tionality allows for better modeling of linguisticphenomena such as the distinction between modi-fiers and arguments (Joshi et al, 1975; XTAG Re-search Group, 2001).
Unfortunately, TAG?s ex-pressivity comes at the cost of greatly increasedcomplexity.
Parsing complexity for unconstrainedTAG scales as O(n6), impractical as compared toCFG and TSG?s O(n3).
In addition, the modelselection problem for TAG is significantly morecomplicated than for TSG since one must reasonabout many more combinatorial options with twotypes of derivation operators.
This has led re-searchers to resort to manual (Doran et al, 1997)or heuristic techniques.
For example, one can con-sider ?outsourcing?
the auxiliary trees (Shieber,2007), use template rules and a very small num-ber of grammar categories (Hwa, 1998), or relyon head-words and force lexicalization in order toconstrain the problem (Xia et al, 2001; Chiang,5972000; Carreras et al, 2008).
However a solutionhas not been put forward by which a model thatmaximizes a principled probabilistic objective issought after.Recent work by Cohn and Blunsom (2010) ar-gued that under highly expressive grammars suchas TSGs where exponentially many derivationsmay be hypothesized of the data, local Gibbs sam-pling is insufficient for effective inference andglobal blocked sampling strategies will be nec-essary.
For TAG, this problem is only more se-vere due to its mild context-sensitivity and evenricher combinatorial nature.
Therefore in previ-ous work, Shindo et al (2011) and Yamangil andShieber (2012) used tree-insertion grammar (TIG)as a kind of expressive compromise between TSGand TAG, as a substrate on which to build nonpara-metric inference.
However TIG has the constraintof disallowing wrapping adjunction (coordinationbetween material that falls to the left and rightof the point of adjunction, such as parentheticalsand quotations) as well as left adjunction along thespine of a right auxiliary tree and vice versa.In this work we formulate a blocked samplingstrategy for TAG that is effective and efficient, andprove its superiority against the local Gibbs sam-pling approach.
We show via nonparametric in-ference that TAG, which contains TSG as a sub-set, is a better model for treebank data than TSGand leads to improved parsing performance.
TAGachieves this by using more compact grammarsthan TSG and by providing the ability to makefiner-grained linguistic distinctions.
We explainhow our parameter refinement scheme for TAGallows for cubic-time CFG parsing, which is justas efficient as TSG parsing.
Our presentation as-sumes familiarity with prior work on block sam-pling of TSG and TIG (Cohn and Blunsom, 2010;Shindo et al, 2011; Yamangil and Shieber, 2012).2 Probabilistic ModelIn the basic nonparametric TSG model, there isan independent DP for every grammar category(such as c = NP), each of which uses a base dis-tribution P0 that generates an initial tree by mak-ing stepwise decisions and concentration parame-ter ?c that controls the level of sparsity (size) ofthe generated grammars: Gc ?
DP(?c, P0(?
| c))We extend this model by adding specialized DPsfor auxiliary trees Gauxc ?
DP(?auxc , P aux0 (?
| c))Therefore, we have an exchangeable process forgenerating auxiliary tree aj given j ?
1 auxiliarytrees previously generatedp(aj | a<j) =nc,aj + ?auxc P aux0 (aj | c)j ?
1 + ?auxc(1)as for initial trees in TSG (Cohn et al, 2009).We must define base distributions for initialtrees and auxiliary trees.
P0 generates an initialtree with root label c by sampling rules from aCFG P?
and making a binary decision at everynode generated whether to leave it as a frontiernode or further expand (with probability ?c) (Cohnet al, 2009).
Similarly, our P aux0 generates an aux-iliary tree with root label c by sampling a CFG rulefrom P?
, flipping an unbiased coin to decide the di-rection of the spine (if more than a unique childwas generated), making a binary decision at thespine whether to leave it as a foot node or furtherexpand (with probability ?c), and recurring into P0or P aux0 appropriately for the off-spine and spinalchildren respectively.We glue these two processes together via a setof adjunction parameters ?c.
In any derivation forevery node labeled c that is not a frontier nodeor the root or foot node of an auxiliary tree, wedetermine the number (perhaps zero) of simulta-neous adjunctions (Schabes and Shieber, 1994)by sampling a Geometric(?c) variable; thus k si-multaneous adjunctions would have probability(?c)k(1 ?
?c).
Since we already provide simul-taneous adjunction we disallow adjunction at theroot of auxiliary trees.3 InferenceGiven this model, our inference task is to ex-plore posterior derivations underlying the data.Since TAG derivations are highly structured ob-jects, we design a blocked Metropolis-Hastingssampler that samples derivations per entire parsetrees all at once in a joint fashion (Cohn and Blun-som, 2010; Shindo et al, 2011; Yamangil andShieber, 2012).
As in previous work, we use aGoodman-transformed TAG as our proposal dis-tribution (Goodman, 2003) that incorporates ad-ditional CFG rules to account for the possibil-ity of backing off to the infinite base distributionP aux0 , and use the parsing algorithm described byShieber et al (1995) for computing inside proba-bilities under this TAG model.The algorithm is illustrated in Table 1 alongwith Figure 1.
Inside probabilities are computedin a bottom-up fashion and a TAG derivation issampled top-down (Johnson et al, 2007).
The598N?
N?
NNNi.
.
.??N0?
N1?
N2N3N4.
.
.??Nj?
NkN?
?Nl?
NmN?
?Figure 1: Example used for illustrating blockedsampling with TAG.
On the left hand side we havea partial training tree where we highlight the par-ticular nodes (with node labels 0, 1, 2, 3, 4) that thesampling algorithm traverses in post-order.
On theright hand side is the TAG grammar fragment thatis used to parse these particular nodes: one initialtree and two wrapping auxiliary trees where oneadjoins into the spine of the other for full general-ity of our illustration.
Grammar nodes are labeledwith their Goodman indices (letters i, j, k, l,m).Greek letters ?, ?, ?, ?
denote entire subtrees.
Weassume that a subtree in an auxiliary tree (e.g., ?
)parses the same subtree in a training tree.sampler visits every node of the tree in post-order(O(n) operations, n being the number of nodes),visits every node below it as a potential foot (an-other O(n) operations), visits every mid-node inthe path between the original node and the poten-tial foot (if spine-adjunction is allowed) (O(log n)operations), and forms the appropriate chart items.The complexity is O(n2 log n) if spine-adjunctionis allowed, O(n2) otherwise.4 Parameter RefinementDuring inference, adjunction probabilities aretreated simplistically to facilitate convergence.Only two parameters guide adjunction: ?c, theprobability of adjunction; and p(aj | a<j , c) (seeEquation 1), the probability of the particular aux-iliary tree being adjoined given that there is anadjunction.
In all of this treatment, c, the con-text of an adjunction, is the grammar category la-bel such as S or NP, instead of a unique identi-fier for the node at which the adjunction occurs aswas originally the case in probabilistic TAG liter-ature.
However it is possible to experiment withfurther refinement schemes at parsing time.
Oncethe sampler converges on a grammar, we can re-estimate its adjunction probabilities.
Using theO(n6) parsing algorithm (Shieber et al, 1995) weexperimented with various refinements schemes?
ranging from full node identifiers, to GoodmanChart item Why made?
Inside probabilityNi[4] By assumption.
?Nk[3-4] N?
[4] and ?
(1 ?
?c) ?
pi(?
)Nm[2-3] N?
[3] and ?
(1 ?
?c) ?
pi(?
)Nl[1-3] ?
and Nm[2-3] (1 ?
?c) ?
pi(?
)?pi(Nm[2-3])Naux[1-3] Nl[1-3] nc,al/(nc + ?auxc )?pi(Nl[1-3])Nk[1-4] Naux[1-3] and Nk[3-4] ?c ?
pi(Naux[1-3])?pi(Nk[3-4])Nj [0-4] ?
and Nk[1-4] (1 ?
?c) ?
pi(?
)?pi(Nk[1-4])Naux[0-4] Nj [0-4] nc,aj /(nc + ?auxc )?pi(Nj [0-4])Ni[0] Naux[0-4] and Ni[4] ?c ?
pi(Naux[0-4])?pi(Ni[4])Table 1: Computation of inside probabilities forTAG sampling.
We create two types of chartitems: (1) per-node, e.g., Ni[?]
denoting theprobability of starting at an initial subtree thathas Goodman index i and generating the subtreerooted at node ?, and (2) per-path, e.g., Nj[?-?
]denoting the probability of starting at an auxiliarysubtree that has Goodman index j and generatingthe subtree rooted at ?
minus the subtree rootedat ?.
Above, c denotes the context of adjunction,which is the nonterminal label of the node of ad-junction (here, N), ?c is the probability of adjunc-tion, nc,a is the count of the auxiliary tree a, andnc =?a nc,a is total number of adjunctions atcontext c. The function pi(?)
retrieves the insideprobability corresponding to an item.index identifiers of the subtree below the adjunc-tion (Hwa, 1998), to simple grammar category la-bels ?
and find that using Goodman index identi-fiers as c is the best performing option.Interestingly, this particular refinement schemealso allows for fast cubic-time parsing, which weachieve by approximating the TAG by a TSG withlittle loss of coverage (no loss of coverage underspecial conditions which we find that are often sat-isfied) and negligible increase in grammar size, asdiscussed in the next section.5 Cubic-time parsingMCMC training results in a list of sufficient statis-tics of the final derivation that the TAG samplerconverges upon after a number of iterations.
Basi-cally, these are the list of initial and auxiliary trees,their cumulative counts over the training data, andtheir adjunction statistics.
An adjunction statisticis listed as follows.
If ?
is any elementary tree, and?
is an auxiliary tree that adjoins n times at node ?of ?
that is uniquely reachable at path p, we write?
p?
?
(n times).
We denote ?
alternatively as?
[p].599*q!p"nmk# *p"iiiq!i k# *mi"i#iii#jjjq!ijij!iji(1) (2) (3)Figure 2: TAG to TSG transformation algorithm.
By removing adjunctions in the correct order we endup with a larger yet adjunction-free TSG.Now imagine that we end up with a small gram-mar that consists of one initial tree ?
and two aux-iliary trees ?
and ?, and the following adjunctionsoccurring between them?
p?
?
(n times)?
p?
?
(m times)?
q?
?
(k times)as shown in Figure 2.
Assume that ?
itself occursl > n +m times in total so that there is nonzeroprobability of no adjunction anywhere within ?.Also assume that the node uniquely identified by?
[p] has Goodman index i, which we denote asi = G(?
[p]).The general idea of this TAG-TSG approxima-tion is that, for any auxiliary tree that adjoins at anode ?
with Goodman index i, we create an ini-tial tree out of it where the root and foot nodes ofthe auxiliary tree are both replaced by i. Further,we split the subtree rooted at ?
from its parent andrename the substitution site that is newly createdat ?
as i as well.
(See Figure 2.)
We can sep-arate the foot subtree from the rest of the initialtree since it is completely remembered by any ad-joined auxiliary trees due to the nature of our re-finement scheme.
However this method fails foradjunctions that occur at spinal nodes of auxiliarytrees that have foot nodes below them since wewould not know in which order to do the initialtree creation.
However when the spine-adjunctionrelation is amenable to a topological sort (as is thecase in Figure 2), we can apply the method by go-ing in this order and doing some extra bookkeep-ing: updating the list of Goodman indices and re-directing adjunctions as we go along.
When thereis no such topological sort, we can approximatethe TAG by heuristically dropping low-frequencyadjunctions that introduce cycles.1The algorithm is illustrated in Figure 2.
In (1)we see the original TAG grammar and its adjunc-tions (n,m, k are adjunction counts).
Note thatthe adjunction relation has a topological sort of?, ?, ?.
We process auxiliary trees in this orderand iteratively remove their adjunctions by creat-ing specialized initial tree duplicates.
In (2) wefirst visit ?, which has adjunctions into ?
at thenode denoted ?
[p] where p is the unique path fromthe root to this node.
We retrieve the Goodman in-dex of this node i = G(?
[p]), split the subtreerooted at this node as a new initial tree ?i, relabelits root as i, and rename the newly-created sub-stitution site at ?
[p] as i.
Since ?
has only thisadjunction, we replace it with initial tree version?i where root/foot labels of ?
are replaced withi, and update all adjunctions into ?
as being into?i.
In (3) we visit ?
which now has adjunctionsinto ?
and ?i.
For the ?
[p] adjunction we create ?ithe same way we created ?i but this time we can-not remove ?
as it still has an adjunction into ?i.We retrieve the Goodman index of the node of ad-junction j = G(?i[q]), split the subtree rooted atthis node as new initial tree ?ij , relabel its rootas j, and rename the newly-created substitutionsite at ?i[q] as j.
Since ?
now has only this ad-junction left, we remove it by also creating initialtree version ?j where root/foot labels of ?
are re-placed with j.
At this point we have an adjunction-free TSG with elementary trees (and counts)?
(l), ?i(l), ?i(n), ?ij(n), ?i(m), ?j(k) where l isthe count of initial tree ?.
These counts, when theyare normalized, lead to the appropriate adjunc-1We found that, on average, about half of our grammarshave a topological sort of their spine-adjunctions.
(On aver-age fewer than 100 spine adjunctions even exist.)
When nosuch sort exists, only a few low-frequency adjunctions haveto be removed to eliminate cycles.60005101520253035400  10  20  30  40  50  60Parsingtime(seconds)Sentence length (#tokens)Figure 3: Nonparametric TAG (blue) parsing is ef-ficient and incurs only a small increase in parsingtime compared to nonparametric TSG (red).tion probability refinement scheme of ?c ?
p(aj |a<j , c) where c is the Goodman index.Although this algorithm increases grammarsize, the sparsity of the nonparametric solutionensures that the increase is almost negligible: onaverage the final Goodman-transformed CFG has173.9K rules for TSG, 189.2K for TAG.
Figure 3demonstrates the comparable Viterbi parsing timesfor TSG and TAG.6 EvaluationWe use the standard Penn treebank methodologyof training on sections 2?21 and testing on section23.
All our data is head-binarized, all hyperpa-rameters are resampled under appropriate vaguegamma and beta priors.
Samplers are run 1000iterations each; all reported numbers are aver-ages over 5 runs.
For simplicity, parsing resultsare based on the maximum probability derivation(Viterbi algorithm).In Table 4, we compare TAG inferenceschemes and TSG.
TAGGibbs operates by locallyadding/removing potential adjunctions, similar toCohn et al (2009).
TAG?
is the O(n2) algorithmthat disallows spine adjunction.
We see that TAG?has the best parsing performance, while TAG pro-vides the most compact representation.model F measure # initial trees # auxiliary treesTSG 84.15 69.5K -TAGGibbs 82.47 69.9K 1.7KTAG?
84.87 66.4K 1.5KTAG 84.82 66.4K 1.4KFigure 4: EVALB results.
Note that the Gibbssampler for TAG has poor performance and pro-vides no grammar compaction due to its lack ofconvergence.label #adj ave. #lex.
#left #right #wrap(spine adj) depth trees trees trees treesVP 4532 (23) 1.06 45 22 65 0NP 2891 (46) 1.71 68 94 13 1NN 2160 (3) 1.08 85 16 110 0NNP 1478 (2) 1.12 90 19 90 0NNS 1217 (1) 1.10 43 9 60 0VBN 1121 (1) 1.05 6 18 0 0VBD 976 (0) 1.0 16 25 0 0NP 937 (0) 3.0 1 5 0 0VB 870 (0) 1.02 14 31 4 0S 823 (11) 1.48 42 36 35 3total 23320 (118) 1.25 824 743 683 9Table 2: Grammar analysis for an estimated TAG,categorized by label.
Only the most common top10 are shown, binarization variables are denotedwith overline.
A total number of 98 wrappingadjunctions (9 unique wrapping trees) and 118spine adjunctions occur.ADJP?
?ADJPADJP* ?
?NP-LRB- NPNP* -RRB-S-LRB--LRB-SS* -RRB--RRB-S?
?SS* ?
?NP-LRB--LRB-NPNP* -RRB--RRB-NNP,,NNPNNPNNP* CC&NNPNP?
?NPNP* ?
?NPNPNP :NPNP* PPFigure 5: Example wrapping trees from estimatedTAGs.7 ConclusionWe described a nonparametric Bayesian inferencescheme for estimating TAG grammars and showedthe power of TAG formalism over TSG for return-ing rich, generalizable, yet compact representa-tions of data.
The nonparametric inference schemepresents a principled way of addressing the diffi-cult model selection problem with TAG.
Our sam-pler has near quadratic-time efficiency, and ourparsing approach remains context-free allowingfor fast cubic-time parsing, so that our overallparsing framework is highly scalable.2There are a number of extensions of thiswork: Experimenting with automatically in-duced adjunction refinements as well as in-corporating substitution refinements can benefitBayesian TAG (Shindo et al, 2012; Petrov et al,2006).
We are also planning to investigate TAGfor more context-sensitive languages, and syn-chronous TAG for machine translation.2An extensive report of our algorithms and experimentswill be provided in the PhD thesis of the first author (Ya-mangil, 2013).
Our code will be made publicly available atcode.seas.harvard.edu/?elif.601ReferencesXavier Carreras, Michael Collins, and Terry Koo.2008.
TAG, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In Proceed-ings of the Twelfth Conference on ComputationalNatural Language Learning, CoNLL ?08, pages 9?16, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of the 38th Annual Meeting on Associa-tion for Computational Linguistics, ACL ?00, pages456?463, Morristown, NJ, USA.
Association forComputational Linguistics.Trevor Cohn and Phil Blunsom.
2010.
Blocked in-ference in Bayesian tree substitution grammars.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 225?230, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Trevor Cohn, Sharon Goldwater, and Phil Blun-som.
2009.
Inducing compact but accurate tree-substitution grammars.
In NAACL ?09: Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 548?556, Morristown, NJ, USA.
Associationfor Computational Linguistics.Christine Doran, Beth Hockey, Philip Hopely, JosephRosenzweig, Anoop Sarkar, B. Srinivas, Fei Xia,Alexis Nasr, and Owen Rambow.
1997.
Maintain-ing the forest and burning out the underbrush in xtag.In Proceedings of the ENVGRAM Workshop.Joshua Goodman.
2003.
Efficient parsing of DOPwith PCFG-reductions.
In Rens Bod, Remko Scha,and Khalil Sima?an, editors, Data-Oriented Parsing.CSLI Publications, Stanford, CA.Rebecca Hwa.
1998.
An empirical evaluation ofprobabilistic lexicalized tree insertion grammars.
InProceedings of the 17th international conference onComputational linguistics - Volume 1, pages 557?563, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Mark Johnson, Thomas Griffiths, and Sharon Gold-water.
2007.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 139?146, Rochester, New York, April.Association for Computational Linguistics.Aravind K. Joshi, Leon S. Levy, and Masako Taka-hashi.
1975.
Tree adjunct grammars.
Journal ofComputer and System Sciences, 10(1):136?163.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440,Sydney, Australia, July.
Association for Computa-tional Linguistics.Matt Post and Daniel Gildea.
2009.
Bayesian learningof a tree substitution grammar.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages45?48, Suntec, Singapore, August.
Association forComputational Linguistics.Philip Resnik.
1992.
Probabilistic tree-adjoininggrammar as a framework for statistical natural lan-guage processing.
In Proceedings of the 14th con-ference on Computational linguistics - Volume 2,COLING ?92, pages 418?424, Stroudsburg, PA,USA.
Association for Computational Linguistics.Yves Schabes and Stuart M. Shieber.
1994.
Analternative conception of tree-adjoining derivation.Computational Linguistics, 20(1):91?124.
Alsoavailable as cmp-lg/9404001.Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementation of de-ductive parsing.
J. Log.
Program., 24(1&2):3?36.Stuart M. Shieber.
2007.
Probabilistic synchronoustree-adjoining grammars for machine translation:The argument from bilingual dictionaries.
In DekaiWu and David Chiang, editors, Proceedings of theWorkshop on Syntax and Structure in StatisticalTranslation, Rochester, New York, 26 April.Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.2011.
Insertion operator for Bayesian tree substitu-tion grammars.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: short pa-pers - Volume 2, HLT ?11, pages 206?211, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, andMasaaki Nagata.
2012.
Bayesian symbol-refinedtree substitution grammars for syntactic parsing.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 440?448, Jeju Island, Korea,July.
Association for Computational Linguistics.Fei Xia, Chung-hye Han, Martha Palmer, and AravindJoshi.
2001.
Automatically extracting and compar-ing lexicalized grammars for different languages.
InProceedings of the 17th international joint confer-ence on Artificial intelligence - Volume 2, IJCAI?01,pages 1321?1326, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.XTAG Research Group.
2001.
A lexicalized treeadjoining grammar for English.
Technical ReportIRCS-01-03, IRCS, University of Pennsylvania.602Elif Yamangil and Stuart Shieber.
2012.
Estimatingcompact yet rich tree insertion grammars.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 110?114, Jeju Island, Korea, July.Association for Computational Linguistics.Elif Yamangil.
2013.
Rich Linguistic Structure fromLarge-Scale Web Data.
Ph.D. thesis, Harvard Uni-versity.
Forthcoming.603
