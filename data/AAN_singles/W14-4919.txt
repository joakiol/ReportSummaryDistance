LAW VIII - The 8th Linguistic Annotation Workshop, pages 129?138,Dublin, Ireland, August 23-24 2014.Towards Automatic Annotation of Clinical Decision-Making StyleLimor Hochberg1Cecilia O. Alm1Esa M. Rantanen1Qi Yu2Caroline M. DeLong1Anne Haake21 College of Liberal Arts 2 College of Computing & Information SciencesRochester Institute of Technologylxh6513|coagla|emrgsh|qi.yu|cmdgsh|anne.haake@rit.eduAbstractClinical decision-making has high-stakes outcomes for both physicians and patients, yet littleresearch has attempted to model and automatically annotate such decision-making.
The dualprocess model (Evans, 2008) posits two types of decision-making, which may be ordered ona continuum from intuitive to analytical (Hammond, 1981).
Training clinicians to recognizedecision-making style and select the most appropriate mode of reasoning for a particular contextmay help reduce diagnostic error (Norman, 2009).
This study makes preliminary steps towardsdetection of decision style, based on an annotated dataset of image-based clinical reasoning inwhich speech data were collected from physicians as they inspected images of dermatologicalcases and moved towards diagnosis (Hochberg et al., 2014).
A classifier was developed based onlexical, speech, disfluency, physician demographic, cognitive, and diagnostic difficulty features.Using random forests for binary classification of intuitive vs. analytical decision style in physi-cians?
diagnostic descriptions, the model improved on the baseline by over 30%.
The introducedcomputational model provides construct validity for decision styles, as well as insights into thelinguistic expression of decision-making.
Eventually, such modeling may be incorporated intoinstructional systems that teach clinicians to become more effective decision makers.1 IntroductionDiagnostic accuracy is critical for both physicians and patients, but there is insufficient training on clini-cal decision-making strategy in medical schools, towards avoiding diagnostic error (Graber et al., 2012;Croskerry & Norman, 2008).
Berner and Graber (2008) estimate that diagnostic error in medicine occursat a rate of 5-15%, and that two-thirds of diagnostic errors involve cognitive root causes.The dual process model distinguishes between intuitive and analytic modes of reasoning (Kahneman& Frederick, 2002; Evans, 1989).
Use of the intuitive system, while efficient, may lead to cognitiveerrors based on heuristics and biases (Graber, 2009).
Croskerry (2003) distinguished over 30 such biasesand heuristics that underlie diagnostic error, including anchoring, base-rate neglect, and hindsight bias.Hammond?s (1981) Cognitive Continuum Theory proposes that decision-making lies on a continuumfrom intuitive to analytical reasoning.
Intuitive reasoning is described as rapid, unconscious, moderatelyaccurate, and employing simultaneous use of cues and pattern recognition (Hammond, 1981).
Analyticaldecision-making is described as slow, conscious, task-specific, more accurate, making sequential use ofcues, and applying logical rules (Hammond, 1996).
Much reasoning is quasirational: between the twopoles of purely intuitive and purely analytical decision-making (Hamm, 1988; Hammond, 1981).Cader et al.
(2005) suggested that cognitive continuum theory is appropriate for the evaluation ofdecision-making in medical contexts.
The current study links to another work (Hochberg et al., 2014),where the cognitive continuum was applied to physician decision-making in dermatology.
Decision stylewas manually assessed in physician verbalizations during medical image inspection.
Figure 1 shows the4-point annotation scheme, ranging from intuitive to analytical; the two intermediate points on the scalereflect the presence of both styles, with intuitive (BI) or analytical (BA) reasoning more prevalent.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/129Figure 1: Four narratives along the intuitive-analytical decision-making continuum, for which annotatorsagreed on their labels, where I=Intuitive, BI=Both-Intuitive, BA=Both-Analytical, A=Analytical.
Thenarratives were produced by different physicians for the same image case (left, used with permissionfrom Logical Images, Inc.), and all four physicians were correct in their final diagnosis.
(Confidencementions were removed in narratives presented to annotators, to avoid any potential bias.
)This work describes computational modeling for automatic annotation of decision style using thisannotated dataset, on the basis of linguistic, speaker, and image case features.1.1 ContributionsTo date, this appears to be the first study attempting to computationally predict physician decision style.Similar to the case of affect, automatic annotation of decision style can be characterized as a subjectivenatural language processing problem (Alm, 2011).
This adds special challenges to the modeling process.Accordingly, this work details a thorough process for moving from manual to automatic annotation.This study contributes to cognitive psychology, annotation methodology, and clinical computationallinguistic analysis.
Methodologically, the study details a careful process for selecting and labeling manu-ally annotated data for modeling in the realm of subjective natural language phenomena, thus addressingthe need for their characterization (Alm, 2011).
Theoretically, acceptable annotator reliability on deci-sion style, along with successful computational modeling, will lend construct validity to the dual processmodel.
From a linguistic perspective, the identification of discriminative features for intuitive and analyt-ical reasoning provides a springboard for further studying decision-making using language as a cognitivesensor.Practically, prediction of decision style would also be useful for determining whether individuals areusing the appropriate style for a particular task, based on analyses linking decision style to task perfor-mance.
Importantly, detection of decision style from observable linguistic behaviors allows for objectivemeasurement that avoids biases present in self-report surveys (Sj?oberg, 2003; Allinson & Hayes, 1996).1302 Data and Manual Decision Style AnnotationThe annotated corpus used in this study was introduced in Hochberg et al.
(2014), which also discussesthe manual annotation scheme and annotator strategies in greater detail.
For clarity, the dataset andannotation scheme are described here briefly.The dataset consisted of spoken narratives collected from 29 physicians as they examined 30 clinicalimages of dermatological cases, for a total of 8671narratives.
Physicians described their reasoningprocess as they advanced towards a diagnosis, and they also estimated their confidence2in their finaldiagnosis.
Narratives were assessed for correctness (based on final diagnoses) and image cases wereevaluated for difficulty by a practicing dermatologist.3For the manual annotation of decision style, anonymized text transcripts of the narratives were pre-sented to two annotators with graduate training in cognitive psychology.4Analytical reasoning considersmore alternatives in greater detail.
Thus, it was expected to be associated with longer narratives, asFigure 1 illustrates.
Therefore, annotators were asked not to use length as a proxy for decision style.Narratives were randomized to ensure high-quality annotation, and 10% of narratives were duplicatedto measure intra-annotator reliability.
For analysis, primary ratings were used, and secondary ratings (onduplicated narratives) were used to measure intra-annotator consistency.
The kappa scores and proportionagreement, detailed below, motivate the labeling and data selection process used for classification andmodeling in this work.Figure 2 shows the distribution of annotation labels for both annotators, respectively, for the wholedataset, on the original 4-point scale.
In comparison, Figure 3 shows the annotators?
distributions acrossa collapsed 2-point scale of intuitive vs. analytical, where, for each annotator, narratives labeled BI wereassigned to I and those labeled BA assigned to A.Figure 2: The distribution of ratings among thedecision-making spectrum, on a 4-point scale.Figure 3: The distribution of ratings among thedecision-making spectrum, on a 2-point scale.Annotator agreement was well above chance for both the 4-point (Figure 4) and 2-point (Figure 5)scales.
Notably, the annotators were in full agreement or agreed within one rating for over 90% of nar-ratives on the original 4-point scale.
This pattern of variation reveals both the fuzziness of the categoriesand also that the subjective perception of decision-making style is systematic.Annotator agreement was also assessed via linear weighted kappa scores (Cohen, 1968).
As shown inFigure 6, inter-annotator reliability was moderate, and intra-annotator reliability was moderate (Annota-tor 2) to good (Annotator 1); see Landis and Koch (1977) and Altman (1991).Since both proportion agreement and kappa scores were slightly higher for the 2-point scale, theautomatic annotation modeling discussed below used this binary scale.
In addition, the distribution of1One narrative was excluded due to extreme brevity, and two physicians each skipped an image during data collection.2For consistency, this paper uses the term confidence, treated as interchangeable with certainty and similar synonymousexpressions used by clinicians in the medical narratives, such as sure, certain, confident, just certainty percentages, etc.3Some imperfections may occur in the data, e.g., in transcriptions, difficulty ratings, or annotations (or in extracted features).4Annotator instructions included decision style definitions, a description of the 4-point scale and example narratives.
Anno-tators were asked to focus on decision style as present in the text rather than speculate beyond it.131Figure 4: Inter- and intra-annotator reliability forthe 4-point scheme, by proportion agreement.
Thereference line shows chance agreement (25%).
(A1=Annotator 1; A2=Annotator 2).Figure 5: Inter- and intra-annotator reliability forthe 2-point scheme, by proportion agreement.
Thereference line shows chance agreement (50%).
(A1=Annotator 1; A2=Annotator 2).Figure 6: Annotator reliability, as measured by linear weighted kappa scores on the 2-pt and 4-pt scales.data across binary classes was more balanced compared to the 4-point scale, as shown by the contrastbetween Figures 2 and 3, further making it a suitable starting point for computational modeling.2.1 Data Selection and Labeling for Computational ModelingThis section details the systematic method used to select data for model development.
The goal of thework was to develop a computational model that could automatically annotate narratives as intuitiveor analytical, based on lexical, speech, disfluency, physician demographic, cognitive, and diagnosticdifficulty features.
The study employed a supervised learning approach, and since no real ground truthwas available, it relied on manual annotation of each narrative for decision style.
However, annotators didnot always agree on the labels, as discussed above.
Thus, strategies were developed to label narratives,including in the case of disagreement (Figure 7).The dataset used for modeling consisted of 672 narratives.5Annotators were in full agreement for 614ratings on the binary scale of intuitive vs. analytical (Figure 8).6Next, 49 narratives were assigned abinary label based on the center of gravity of both annotators?
primary ratings (Figure 9).
For example,if a narrative was rated as Intuitive and Both-Analytical by Annotators 1 and 2, respectively, the center ofgravity was at Both-Intuitive, resulting in an Intuitive label.
Finally, 9 narratives were labeled using theannotators?
secondary ratings,7available for 10% of narratives, to resolve annotator disagreement.85Within a reasonable time frame, the text data are expected to be made publicly available.6Excluding also narratives lacking confidence or correctness information.7Collected to measure intra-annotator reliability.8For example, if the primary ratings of Annotator 1 and Annotator 2 were Both-Analytical and Both-Intuitive, respectively,but both annotators?
secondary ratings were intuitive (e.g., Both-Intuitive or Intuitive), the narrative was labeled Intuitive.132Narratives with disagreements that could not be resolved in these ways were excluded.
As perceptionof decision-making style is subject to variation in human judgment, this work focused on an initialmodeling of data which represent the clearer-cut cases of decision style (rather than the disagreementgray zone on this gradient perception continuum).
From the perspective of dealing with a subjectiveproblem, this approach enables an approximation of ground truth, as a validation concept.9Figure 7: Narrative labeling pipeline.
614 narratives were labeled due to full binary agreement, andcenter-of-gravity and secondary rating strategies were used to label an additional 58 narratives for whichannotators were not in agreement.Figure 8: Demonstration of initial corpus labeling,in which 614 narratives were labeled on the basisof binary agreement.Figure 9: Demonstration of center-of-gravitystrategy, used to label an additional 49 narratives.2.2 Relationship Between Physicians?
Diagnostic Correctness and Decision StyleUsing the 672 narratives selected for modeling, Table 1 shows the relationship of physicians?
diagnosticcorrectness by decision style (intuitive vs. analytical on a binary scale).Correct Incorrect TotalIntuitive 158 186 344Analytical 106 222 328Total 264 408 672Table 1: Distribution of diagnostic correctness by decision style.Overall, there was a slightly higher prevalence of intuitive reasoning, and there were more incorrectthan correct diagnoses.10Table 1 also suggests a relationship between correctness and decision-makingstyle, where for correct diagnoses, intuitive reasoning was more dominant.
The opposite trend heldfor incorrect diagnoses: analytical reasoning was more frequent.
Indeed, a chi-square test revealed asignificant relationship between correctness and decision style, ?2(1, N = 672) = 13.05, p < 0.01.This pattern is in line with claims that intuitive reasoning is linked to better performance when muchinformation is to be processed; mechanisms of intuitive reasoning and pattern recognition allow individ-uals to overcome the limitations of their working memory (Evans, 2008).
However, others have linkedintuitive reasoning to decreased diagnostic accuracy, as intuitive reasoning may be prey to inappropriate9Modeling of fuzzier, hard to label data, is left to future work.
One possible approach is to learn the labels by using ak-nearest neighbor classifier, which identifies the most similar narratives and uses their labels to make the prediction.10Contributing factors to the proportion of incorrect diagnoses might include case difficulty levels in the experimental sce-nario, and that physicians did not have access to additional information, such as patient history or follow-up tests.133heuristics and biases (Croskerry, 2003).
Viewed from the perspective of cognitive continuum theory, thehigher prevalence of incorrect diagnoses may be due to the use of decision styles that were not suited tothe task demands of the particular case (Hammond, 1981).
Finally, it might be the case that diagnosticdifficulty was a moderating variable, where physicians preferred intuitive reasoning for less challengingcases, and analytical reasoning for more difficult cases.3 MethodsA model was developed for the binary prediction case (intuitive vs. analytical), since the 2-point ratingscheme had slightly higher annotator agreement (see Section 2).
Model development and analysis wereperformed using the WEKA data mining software package (Hall et al., 2009).
The dataset was split into80% development and 20% final test sets (Table 2).11Parameter tuning was performed using 10-foldcross-validation on the best features in the development set.1280% Development Set 20% Final Test SetIntuitive 276 (51%) 68 (51%)Analytical 263 (49%) 65 (49%)Total 539 133Table 2: Class label statistics.3.1 FeaturesThree feature types were derived from the spoken narratives to study the linguistic link to decision-making style: lexical (37), speech (13), and disfluency (3) features.
Three other feature types relevant todecision-making were demographic (2), cognitive (2), and difficulty (2) features (Table 3).Type Feature Description / ExamplesLexicalexclusion but, withoutinclusion both, withinsight think, knowtentative maybe, perhapscause because, thereforecognitive process know, whether.
.
.Speechspeech length number of tokenspitch min, max, mean, st.
dev., time of min/maxintensity min, max, mean, st.
dev., time of min/maxDisfluencysilent pauses number offillers like, blahnonfluencies uh, umDemographicgender male, femalestatus resident, attendingCognitiveconfidence percentagecorrectness binaryDifficultyexpert rating ordinal ranking% correctness/image percentageTable 3: Six feature types.
The listed lexical features are a sub-sample of the total set.Relevant lexical features were extracted with the Linguistic Inquiry and Word Count (LIWC) software,which calculates the relative frequency of syntactic and semantic classes in text samples based on val-11This split rests on the assumption that physicians may share common styles.
Thus, the testing data will represent differentphysicians, but the styles themselves have been captured by the training data so that they can be correctly classified; the samerationale can be applied to image cases.
To further investigate the phenomenon and identify the degree of inter- and intra-individual variation in decision style, future work could experiment with holding out particular images and physicians.12In Section 4.1, parameters were tuned for each case of feature combinations in a similar way.134idated, researched dictionaries (Tausczik & Pennebaker, 2010).
Disfluency features were silent pauses,and the frequency of fillers and nonfluencies as computed by LIWC.
Speech features are in Table 3.Besides linguistic features, three additional groups of features were included, with an eye towardsapplication.
Demographic features were gender and professional status, while cognitive features werephysician confidence in diagnosis and correctness of the final diagnosis.
Difficulty features consistedof an expert-assigned rank of diagnostic case difficulty, and the percent of correct diagnoses given byphysicians for each image, calculated on the development data only.
In an instructional system, a traineecould input a demographic profile, and the system could also collect performance data over time, whilealso taking into account stored information on case difficulty when available.
This information couldthen be used in modeling of decision style in spoken or written diagnostic narratives.3.2 Feature SelectionWEKA?s CfsSubsetEval, an attribute evaluator, was used for feature selection,13using 10-fold cross-validation on the development set only.
Features selected by the evaluator in at least 5 of 10 folds wereconsidered best features.
The best features from the entire feature set were: 2nd person pronouns, con-junctions, cognitive process, insight, cause, bio, and time words, plus silent pauses, speech length, time ofmin.
pitch, standard deviation of pitch, time of min.
intensity, and difficulty: percent correctness/image.Feature selection, using the same attribute evaluator, was also performed on only the lexical fea-tures, which could be a starting point for analysis of decision-making style in text-only data.
The bestlexical features14included conjunctions, cause, cognitive process, inclusion, exclusion, and perceptionwords.
These lexical items seem associated with careful examination and reasoning, which might bemore present in analytical decision-making and less present in intuitive decision-making.
Some cate-gories, especially inclusion (e.g., with, and), exclusion (e.g., but, either, unless), and cause words (e.g.,affect, cause, depend, therefore), seem particularly good representatives of logical reasoning and justifi-cation, a key feature of analytical reasoning.
But as shown in the next section, when available, speechand disfluency information is useful, and potentially more so than some lexical features.154 Results and DiscussionTable 4 lists the results for the Random Forest (Breiman, 2001) and Logistic Regression (Cox, 1972)classifiers on the best features (as selected from all features) on the final test set, after training on thedevelopment set.
These results suggest that decision style can be quantified and classified on a binaryscale; the percent error reduction (compared to baseline performance) for both classifiers is substantial.Classifier %Acc %ER Pr ReRandom Forest 88 76 88 88Logistic Regression 84 67 84 84Majority Class Baseline 51 ?
?
?Table 4: Performance on final test set; reduction in error is calculated relative to majority class baseline.Precision and recall are macro-averages of the two classes.4.1 Feature Combination ExplorationA study of feature combinations was performed on the final test set with Random Forest (Table 5) toexplore the contribution of each feature type towards automatic annotation.
The best performance wasachieved after applying feature selection on all features.
Lexical and disfluency features were useful fordetermining decision style, and the best linguistic features (chosen with feature selection) were slightlymore useful.
These latter feature types improve on the performance achieved when considering only13With BestFirst search method.14Best lexical features were: function words, singular pronouns, prepositions, conjunctions, quantifiers, and cognitive pro-cess, cause, discrepancy, tentative, inclusion, exclusion, perception, see, bio, motion, time, and assent words.15Feature selection was also performed only on the linguistic (lexical, speech, and disfluency) features as a group.
The bestfeatures of these types were: second personal pronouns, conjunctions, cognitive process, insight, cause, bio, and time words;silent pauses; and speech length, time of minimum pitch, standard deviation of pitch, and time of minimum intensity.
Theycould represent a starting for point for analyzing speech data not enhanced by additional speaker and task information.135speech length and silent pauses, which were apparent characteristics to the human annotators and amongthe best features (see Section 3.2.
).Demographic features improved somewhat over the baseline, indicating an association between gen-der, professional status, and decision-making, and adding cognitive features increased performance.
Im-portantly, overall these findings hint at linguistic markers as key indicators of decision style.Features AccuracyAll* 88All 85(Lexical + Speech + Disfluency)* 86Lexical + Speech + Disfluency 84Lexical + Disfluency 84Only speech length and silent pauses 81Disfluency 79Lexical 77Demographic + Cognitive 68Demographic 64Majority Class Baseline 51Table 5: Performance on final test set.
Star (*) indicates the use of feature selection (see Section 3.2.
)4.2 LimitationsIn this study, doctors diagnosed solely on the basis of visual information (e.g., without tests or follow-up), so their speech may reflect only part of the clinical reasoning process.
In addition, most decisionstyle ratings on the 4-point scale were in the distribution center (Figure 2), so the binary labels used inthe study only partially reflect purely intuitive or purely analytical reasoning.
However, since clinicianreasoning in the current dataset can be reliably measured by human and computational classification,linguistic features of decision style must be present.
Finally, the LIWC software used for lexical featuresmatches surface strings rather than senses; future work might operate on the sense rather than token level.5 Related WorkLauri et al.
(2001) asked nurses in five countries to rate statements representative of intuitive or analyticaldecision-making on a 5-point scale.
They found that reasoning varies with context and that styles in themiddle of the cognitive continuum predominate.
In this work, annotation ratings were prevalent in themiddle of the spectrum.
Thus, both studies endorse that most decision-making occurs in the central partof the continuum (Hamm, 1988; Hammond, 1981).
Womack et al.
(2012) proposed that silent pauses inphysician narration may indicate cognitive processing.
Here, silent pauses were also important, perhapsbecause analytical decision-making may recruit more cognitive resources than intuitive decision-making.6 ConclusionThis work suggests that decision style is revealed in language use, in line with claims that linguisticdata reflect speakers?
cognitive processes (Pennebaker & King, 1999; Tausczik & Pennebaker, 2010).Theoretically, the study adds validity to the dual process and cognitive continuum theories.
Methodolog-ically, it articulates a method of transitioning from manual to automatic annotation of fuzzy semanticphenomena, including label adjudication and data selection for computational modeling.
Future workmay investigate modeling of the 4-point decision scale, as well as whether particular variables, such asdifficulty or expertise, mediate the relationship between diagnostic correctness and decision style.Practically, automatic detection of decision style is useful for both clinical educational systems andmission-critical environments.
Clinical instructional systems can assess whether trainees are using theappropriate style for a particular task (Hammond, 1981), and they can help users determine and attend totheir own decision styles, towards improving diagnostic skill (Norman, 2009).
Finally, in mission-criticalenvironments, linguistic markers of decision-making style may be used to determine the optimal modesof reasoning for a particular task in high-stakes human factors domains.136AcknowledgementsThis work was supported by a COLA Faculty Development grant, Xerox award, and NIH award R21LM01002901.
Many thanks to the annotators and reviewers.
This content is solely the responsibility ofthe authors and does not necessarily represent the official views of the National Institutes of Health.ReferencesAllinson, C. W., & Hayes, J.
(1996).
The cognitive style index: A measure of intuition-analysis for organizationalresearch.
Journal of Management Studies, 33(1), 119-135.Alm, C. O.
(2011, June).
Subjective natural language problems: Motivations, applications, characterizations, andimplications.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: HumanLanguage Technologies: Short papers-Volume 2 (pp.
107-112).
Association for Computational Linguistics.Altman, D. (1991).
Practical statistics for medical research.
London: Chapman and Hall.Berner, E. S., & Graber, M. L. (2008).
Overconfidence as a cause of diagnostic error in medicine.
AmericanJournal of Medicine, 121, S2-S23.Breiman, L. (2001).
Random forests.
Machine Learning, 45(1), 5-32.Cader, R., Campbell, S., & Watson, D. (2005).
Cognitive continuum theory in nursing decision-making.
Journalof Advanced Nursing, 49(4), 397-405.Cohen, J.
(1968).
Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit.Psychological Bulletin, 70(4), 213-220.Cox, D. R. (1972).
Regression models and life tables.
Journal of the Royal Statistical Society, Series B, 34(2),187-220.Croskerry, P. (2003).
The importance of cognitive errors in diagnosis and strategies to minimize them.
AcademicMedicine, 78, 775-780.Croskerry, P., & Norman, G. (2008).
Overconfidence in clinical decision making.
The American Journal ofMedicine, 121(5), S24-S29.Evans, J.
(1989).
Bias in human reasoning: Causes and consequences.
Hillsdale, NJ: Erlbaum.Evans, J.
(2008).
Dual-processing accounts of reasoning, judgment and social cognition.
Annual Review of Psy-chology, 59, 255-278.Graber, M. (2009).
Educational strategies to reduce diagnostic error: Can you teach this stuff?
Advances in HealthSciences Education, 14, 63-69.Graber, M. L., Kissam, S., Payne, V. L., Meyer, A. N., Sorensen, A., Lenfestey, N., ... & Singh, H. (2012).Cognitive interventions to reduce diagnostic error: A narrative review.
BMJ Quality & Safety, 2(7), 535-557.Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009).
The WEKA data miningsoftware: An update.
ACM SIGKDD Explorations Newsletter, 11(1), 10-18.Hamm, R. M. (1988).
Clinical intuition and clinical analysis: Expertise and the cognitive continuum.
In J. Dowie& A.S. Elstein (Eds.
), Professional judgment: A reader in clinical decision making (pp.
78-105).
Cambridge,England: Cambridge University Press.Hammond, K. R. (1981).
Principles of organization in intuitive and analytical cognition (Report #231).
Boulder,CO: University of Colorado, Center for Research on Judgment & Policy.Hammond, K. R. (1996).
Human judgement and social policy: Irreducible uncertainty, inevitable error, unavoid-able injustice.
New York, NY: Oxford University Press.Hochberg, L., Alm, C. O., Rantanen, E. M., DeLong, C.M., & Haake, A.
(2014).
Decision style in a clinicalreasoning corpus.
In Proceedings of the BioNLP Workshop (pp.
83-87).
Baltimore, MD: Association for Com-putational Linguistics.Kahneman, D., & Frederick, S. (2002).
Representativeness revisited: Attribute substitution in intuitive judgment.In T. Gilovich, D. Griffin, & D. Kahneman (Eds.
), Heuristics of intuitive judgment: Extensions and applications(pp.
49-81).
New York, NY: Cambridge University Press.137Lauri, S., Salanter?a, S., Chalmers, K., Ekman, S. L., Kim, H. S., K?appeli, S., & MacLeod, M. (2001).
Anexploratory study of clinical decision-making in five countries.
Journal of Nursing Scholarship, 33(1), 83-90.Landis, J. R., & Koch, G. G. (1977).
The measurement of observer agreement for categorical data.
Biometrics,33(1), 159-174.Norman, G. (2009).
Dual processing and diagnostic errors.
Advances in Health Sciences Education, 14(1), 37-49.Pennebaker, J. W., & King, L. A.
(1999).
Linguistic styles: Language use as an individual difference.
Journal ofPersonality and Social Psychology, 77(6), 1296-1312.Sj?oberg, L. (2003).
Intuitive vs. analytical decision making: Which is preferred?
Scandinavian Journal of Man-agement, 19(1), 17-29.Tausczik, Y. R., & Pennebaker, J. W. (2010).
The psychological meaning of words: LIWC and computerized textanalysis methods.
Journal of Language and Social Psychology, 29(1), 24-54.Womack, K., McCoy, W., Alm, C. O., Calvelli, C., Pelz, J.
B., Shi, P., & Haake, A.
(2012, July).
Disfluenciesas extra-propositional indicators of cognitive processing.
Proceedings of the Workshop on Extra-PropositionalAspects of Meaning in Computational Linguistics (pp.
1-9).
Association for Computational Linguistics.138
