Proceedings of the ACL 2014 Student Research Workshop, pages 34?40,Baltimore, Maryland USA, June 22-27 2014. c?2014 Association for Computational LinguisticsA Mapping-Based Approach for General FormalHuman Computer Interaction Using Natural LanguageVincent LetardLIMSI CNRSletard@limsi.frSophie RossetLIMSI CNRSrosset@limsi.frGabriel IllouzLIMSI CNRSillouz@limsi.frAbstractWe consider the problem of mapping nat-ural language written utterances express-ing operational instructions1 to formal lan-guage expressions, applied to French andthe R programming language.
Developinga learning operational assistant requiresthe means to train and evaluate it, that is,a baseline system able to interact with theuser.
After presenting the guidelines ofour work, we propose a model to repre-sent the problem and discuss the fit of di-rect mapping methods to our task.
Finally,we show that, while not resulting in excel-lent scores, a simple approach seems to besufficient to provide a baseline for an in-teractive learning system.1 IntroductionTechnical and theoretical advances allow achiev-ing more and more powerful and efficient opera-tions with the help of computers.
However, thisdoes not necessarily make it easier to work withthe machine.
Recent supervised learning work(Allen et al, 2007; Volkova et al, 2013) exploitedthe richness of human-computer interaction forimproving the efficiency of a human performedtask with the help of the computer.Contrary to most of what was proposed so far,our long term goal is to build an assistant systemlearning from interaction to construct a correct for-mal language (FL) command for a given naturallanguage (NL) utterance, see Table 1.
However,designing such a system requires data collection,and early attempts highlighted the importance ofusability for the learning process: a system that ishard to use (eg.
having very poor performance)1We call operational instruction the natural language ex-pression of a command in any programming language.would prevent from extracting useful learning ex-amples from the interaction.
We thus need to pro-vide the system with a basis of abilities and knowl-edge to allow both incremental design and to keepthe interest of the users, without which data turnto be way more tedious to collect.
We assume thatmaking the system usable requires the ability toprovide help to the user more often than it needshelp from him/her, that is an accuracy over 50%.We hypothesize that a parametrized directmapping between the NL utterances and the FLcommands can reach that score.
A knowledge setK is built from parametrized versions of the asso-ciations shown in Table 1.
The NL utterance Ubestfrom K that is the closest to the request-utteranceaccording to a similarity measure is chosen and itsassociated command C(Ubest) is adapted to theparameters of the request-utterance and returned.For example, given the request-utterance Ureq:?Load the file data.csv?, the system should rankthe utterances of K by similarity with Ureq.
Con-sidering the associations represented in Table 1,the first utterance should be the best ranked, andthe system should return the command:?var1 <- read.csv("data.csv")?.Note that several commands can be proposed atthe same time to give the user alternate choices.We use Jaccard, tf-idf, and BLEU similaritymeasures, and consider different selection strate-gies.
We highlight that the examined similaritymeasures show enough complementarity to permitthe use of combination methods, like vote or sta-tistical classification, to improve a posteriori theefficiency of the retrieval.2 Related Work2.1 Mapping Natural Language to FormalLanguageRelated problems have been previously processedusing different learning methods.
Branavan (2009,34NL utterances FL commands (in R)1 Charge les donne?es depuis ?res.csv?
var1=read.csv("res.csv")Load the data from ?res.csv?2 Trace l?histogramme de la colonne 2 de tab plot(hist(tab[[2]]))Draw a bar chart with column 2 of tab3 Dessine la re?partition de la colonne 3 de tab plot(hist(tab[[3]]))Draw the distribution of column 3 of tab4 Somme les colonnes 3 et 4 de tab var2=c(sum(tab[3]),sum(tab[4]))Compute the sum of columns 3 and 4 of tab5 Somme les colonnes 3 et 4 de tab var3=sum(c(tab[[3]],tab[[4]]))Compute the sum of columns 3 and 4 of tabTable 1: A sample of NL utterances to FL commands mappingThese examples specify the expected command to be returned for each utterance.
The tokens in boldfont are linked with the commands parameters, cf.
section 4.
Note that the relation between utterancesand commands is a n to n. Several utterances can be associated to the same command and conversely.2010) uses reinforcement learning to map En-glish NL instructions to a sequence of FL com-mands.
The mapping takes high-level instructionsand their constitution into account.
The scopeof usable commands is yet limited to graphicalinteraction possibilities.
As a result, the learn-ing does not produce highly abstract schemes.
Inthe problematic of interactive continuous learning,Artzi and Zettlemoyer (2011) build by learning asemantic NL parser based on combinatory cate-gorial grammars (CCG).
Kushman and Barzilay(2013) also use CCG in order to generate regu-lar expressions corresponding to their NL descrip-tions.
This constructive approach by translationallows to generalize over learning examples, whilethe expressive power of regular expressions cor-respond to the type-3 grammars of the Chomskyhierarchy.
This is not the case for the program-ming languages since they are at least of type-2.Yu and Siskind (2013) use hidden Markov mod-els to learn a mapping between object tracks froma video sequence and predicates extracted froma NL description.
The goal of their approach isdifferent from ours but the underlying problem offinding a map between objects can be compared.The matched objects constitute here a FL expres-sion instead of a video sequence track.2.2 Machine TranslationMachine translation usually refers to transforminga NL sentence from a source language to anothersentence of the same significance in another natu-ral language, called target language.
This task isachieved by building an intermediary representa-tion of the sentence structure at a given level ofabstraction, and then encoding the obtained objectinto the target language.
While following a dif-ferent goal, one of the tasks of the XLike project(Marko Tadic?
et al, 2012) was to examine thepossibility of translating statements from NL (En-glish) to FL (Cycl).
Adapting such an approachto operational formal target language can be inter-esting to investigate, but we will not focus on thattrack for our early goal.2.3 Information RetrievalThe issue of information retrieval systems can becompared with the operational assistant?s (OA),when browsing its knowledge.
Question an-swering systems in particular (Hirschman andGaizauskas, 2001), turn out to be similar to OAsince both types of systems have to respond to aNL utterance of the user by generating an accu-rate reaction (which is respectively a NL utterancecontaining the wanted information, or the execu-tion of a piece of FL code).
However, as in (Toneyet al, 2008), questions answering systems usuallyrely on text mining to retrieve the right informa-tion.
Such a method demands large sets of anno-tated textual data (either by hand or using an au-tomatic annotator).
Yet, tutorials, courses or man-uals which could be used in order to look for re-sponses for operational assistant systems are het-erogeneous and include complex or implicit ref-erences to operational knowledge.
This makesthe annotation of such data difficult.
Text min-ing methods are thus not yet applicable to oper-ational assistant systems but could be consideredonce some annotated data is collected.353 Problem FormulationAs we introduced in the first section, we representthe knowledge K as a set of examples of a binaryrelation R : NL ?
FL associating a NL utter-ance to a FL command.
If we consider the simplecase of a functional and injective relation, eachutterance is associated to exactly one command.This is not realistic since it is possible to reformu-late nearly any NL sentence.
The case of a non in-jective relation covers better the usual cases: eachcommand can be associated with one or more ut-terances, this situation is illustrated by the secondand third examples of Table 1.
Yet, the real-lifecase should be a non injective nor functional rela-tion.
Not only multiple utterances can refer to asame command, but one single utterance can alsostand for several distinct commands (see the fourthand fifth examples2 in Table 1).
We must considerall these associations when matching a request-utterance Ureqfor command retrieval in K .At this point, several strategies can be used todetermine what to return, with the help of the sim-ilarity measure ?
: NL ?
NL ?
R between twoNL utterances.
Basically, we must determine ifa response should be given, and if so how manycommands to return.
To do this, two potentialstrategies can be considered for selecting the as-sociated utterances in K .The first choice focuses on the number of re-sponses that are given for each request-utterance.The n first commands according to the rankings oftheir associated utterances in K are returned.
Therank r of a given utterance U is computed with:r(U |Ureq) =???U??
K : ?
(Ureq, U?)
> ?
(Ureq, U)???
(1)The second strategy choice can be done by de-termining an absolute similarity threshold belowwhich the candidate utterances from K and theirassociated sets of commands are considered toodifferent to match.
The resulting set of commandsis given by:Res = {C ?
FL : (U,C) ?
K,?
(Ureq, U) < t} (2)with t the selected threshold.
Once selected theset of commands to be given as response, if thereare more than one, the choice of the one to executecan be done interactively with the help of the user.2The command 4 returns a vector of the sums of each col-umn, while the command 5 returns the sum of the columns asa single integer.4 ApproachWe are given a simple parsing result of both the ut-terance and the command.
The first step to addressis the acquisition of examples and the way to up-date the knowledge.
Then we examine the meth-ods for retrieving a command from the knowledgeand a given request-utterance.Correctly mapping utterances to commands re-quires at least to take their respective parametersinto account (variable names, numeric values, andquoted strings).
We build generic representationsof utterances and commands by identifying the pa-rameters in the knowledge example pair (see Ta-ble 1), and use them to reconstruct the commandwith the parameters of the request-utterance.4.1 Retrieving the CommandsWe applied three textual similarity measures toour model in order to compare their strengths andweaknesses on our task: the Jaccard similarity co-efficient (Jaccard index), a tf-idf (Term frequency-inverse document frequency) aggregation, and theBLEU (Bilingual Evaluation Understudy) mea-sure.4.1.1 Jaccard indexThe Jaccard index measures a similarity betweentwo sets valued in the same superset.
For thepresent case, we compare the set of words of theinput NL instruction and the one of the comparedcandidate instruction, valued in the set of possibletokens.
The adapted formula for two sentences S1and S2results in:J(s1, s2) =|W (s1) ?W (s2)||W (s1) ?W (s2)|(3)where W (S) stands for the set of words of thesentence S. The Jaccard index is a baseline tocompare co-occurences of unigrams, and shouldbe efficient mainly with corpora containing fewambiguous examples.4.1.2 tf-idfThe tf-idf measure permits, given a word, to clas-sify documents on its importance in each one, re-garding its importance in the whole set.
This mea-sure should be helpful to avoid noise bias when itcomes from frequent terms in the corpus.
Here,the documents are the NL utterances from K , andthey are classified regarding the whole request-utterance, or input sentence si.
We then use the36following aggregation of the tf-idf values for eachword of si.tfidfS(si, sc) =1|W (si)|Xw?W (si)tfidf(w, sc, S) (4)with S = {s|(s, com) ?
K}, where siis the inputsentence, sc?
S is the compared sentence, andwhere the tf-idf is given by:tfidf(w, sc, S) = f(w, sc)idf(w, S) (5)idf(w,S) = log?|S||{s ?
S|w ?
s}|?
(6)where at last f(w, s) is the frequency of the wordw in the sentence s. As we did for the Jaccard in-dex, we performed the measures on both raw andlemmatized words.
On the other hand, getting ridof the function words and closed class words is nothere mandatory since the tf-idf measure alreadytakes the global word frequency into account.4.1.3 The BLEU measureThe bilingual evaluation understudy algorithm(Papineni et al, 2002) focuses on n-grams co-occurrences.
This algorithm can be used to dis-card examples where the words ordering is too farfrom the candidate.
It computes a modified pre-cision based on the ratio of the co-occurring n-grams within candidate and reference sentences,on the total size of the candidate normalized by n.PBLEU(si, S) =Xgrn?simaxsc?Socc(grn, sc)grams(si, n)(7)where grams(s, n) = |s| ?
(n?
1) is the numberof n-grams in the sentence s and occ(grn, s) =?grn?
?s[grn= grn?]
is the number of occur-rences of the n-gram grnin s. BLEU also usesa brevity penalty to prevent long sentences frombeing too disadvantaged by the n-gram based pre-cision formula.
Yet, the scale of the length of theinstructions in our corpus is sufficiently reducednot to require its use.4.2 Optimizing the similarity measureWe applied several combinations of filters to theutterances compared before evaluating their sim-ilarity.
We can change the set of words takeninto account, discarding or not the non open-classwords3.
Identified non-lexical references such as3Open-class words include nouns, verbs, adjectives, ad-verbs and interjections.variable names, quoted character strings and nu-meric values can also be discarded or transformedto standard substitutes.
Finally, we can apply ornot a lemmatization4 on lexical tokens.By discard-ing non open-class words, keeping non-lexical ref-erences and applying the lemmatization, the sec-ond utterance of Table 1 would then become:draw bar chart column xxVALxx xxVARxx5 Experimental Setup5.1 ParsingThe NL utterances first pass through an arith-metic expression finder to completely tag them be-fore the NL analyzer.
They are then parsed us-ing WMATCH, a generic rule-based engine forlanguage analysis developed by Olivier Galibert(2009).
This system is modular and dispose ofrules sets for both French and English.
As an ex-ample, the simplified parsing result of the first ut-terance of Table 1 looks like:<_operation><_action> charge|_?V </_action><_det> les </_det><_subs> donne?es|_?N </_subs><_prep> depuis </_prep><_unk> "res.csv" </_unk></_operation>Words tagged as unknown are considered as po-tential variable or function names.
We also addeda preliminary rule to identify character strings andcount them among the possibly linked features ofthe utterance.
The commands are normalized byinserting spaces between every non semanticallylinked character pair and we identify numeric val-ues, variable/function names and character stringsas features.Only generative forms of the commands areassociated to utterances in the knowledge.
Thisform consists in a normalized command with unre-solved references for every parameter linked withthe learning utterance.
These references are re-solved at the retrieving phase by matching with thetokens of the request-utterance.5.2 Corpus ConstitutionOur initial corpus consists in 605 associations be-tween 553 unique NL utterances in French and240 unique R commands.4Lemmatization is the process of transforming a word toits canonical form, or lemma, ignoring the inflections.
It canbe performed with a set of rules or with a dictionary.
Thedeveloped system uses a dictionary.37The low number of documents describing amajority of R commands and their heterogeneitymake automatic example gathering not yet achiev-able.
These documentations are written for humanreaders having global references on the task.
Thus,we added each example pair manually, makingsure that the element render all the example infor-mation and that the format correspond to the cor-pus specifications.
Those specifications are meantto be the least restrictive, that is: a NL utterancemust be written as to ask for the execution of theassociated R task.
It therefore should be mostlyin the imperative form and reflect, for experiencedpeople, a usual way they would express the con-cerned operation for non specialists.5.3 Evaluation MetricsThe measures that can contribute to a relevantevaluation of the system depend on its purpose.Precision and recall values of information retrievalsystems are computed as follows:P =# correct responses# responses given (8)R =# correct responses# responses in K (9)Note that the recall value is not as important as forinformation retrieval: assuming that the situationshowed by the fourth and fifth associations of Ta-ble 1 are not usual5, there should be few differentvalid commands for a given request-utterance, andmost of them should be equivalent.
Moreover, thenumber of responses given is fixed (so is the num-ber of responses in K), the recall thus gives thesame information as the precision, with a linearcoefficient variation.These formulae can be applied to the ?commandlevel?, that is measuring the accuracy of the sys-tem in terms of its good command ratio.
However,the user satisfaction can be better measured at the?utterance level?
since it represents the finest gran-ularity for the user experience.
We define the ut-terance precision uP as:uP =# correct utterances# responses given (10)where ?# correct utterances?
stands for the num-ber of request-utterances for which the system pro-vided at least one good command.5Increasing the tasks covering of the corpus will makethese collisions more frequent, but this hypothesis seems rea-sonable for a first approach.6 Results and DiscussionThe system was tested on 10% of the corpus (61associations).
The set of known associations Kcontains 85% of the corpus (514 associations), in-stead of 90% in order to allow several distinctdrawings (40 were tested), and thus avoid toomuch noise.6.1 Comparing similarity measuresAs shown in Table 2 the tf-idf measure outper-forms the Jaccard and BLEU measures, whicheverfilter combination is applied.
The form of the ut-terances in the corpus causes indeed the repetitionof a small set of words across the associations.This can explain why the inverse document fre-quency is that better.non-lexical included not includedlemmatize yes no yes noJaccard 36.5 36.5 21.2 23.0tf-idf 48.0 51.9 36.5 40.4BLEU 30.8 32.7 26.9 30.8chance 1.9Table 2: Scores of precision by utterance (uP ),providing 3 responses for each request-utterance.The lemmatization and the inclusion of nonopen-class words (not shown here) does not seemto have a clear influence on uP , whereas includingthe non-lexical tokens allows a real improvement.This behaviour must result from the low length av-erage (7.5 words) of the utterances in the corpus.0.30.40.50.60.71 2 3 4 5 6 7 8 9Number of responsesUtteranceprecision(uP)measuretfidftfidf_inlFigure 1: Utterance precision (uP ) for a fixednumber of responses by utterance.
The tfidf inlcurve includes the non-lexical tokens.Note that uP is obtained with Equation 10, whichexplains the increase of the precision along thenumber of responses.38Figure 1 shows the precision obtained with tfidfwhile increasing the number of commands givenfor each request-utterance.
It comes out that itis useful to propose at least 3 commands to theuser.
It would not be interesting, though, to offer achoice of more than 5 items, because the gain onuP would be offset by the time penalty for retriev-ing the good command among the proposals.6.2 Allowing silenceWe also tested the strategy of fixing an absolutethreshold to decide between response and silence.Given a request-utterance and an associated order-ing of K according to ?, the system will remainsilent if the similarity of the best example in K isbelow the defined threshold.Surprisingly, it turned out that for every mea-sure, the 6 best similar responses at least were allwrong.
This result seems to be caused by the ex-istence, in the test set of commands uncovered byK , of some very short utterances that contain onlyone or two lexical tokens.6.3 Combinations0.00.20.40.60.81 2 3 4 5 6 7 8 9Number of responsesUtteranceprecision(uP)methodvotetfidf_inlvote_oraclelearningFigure 2: Comparison of the combinations withthe tf-idf inl method.
Oracle and actual vote aredone using tf-idf, Jaccard, and BLEU, with andwithout non-lexical tokens.
The training set forlearning is the result of a run on K .Having tested several methods giving differ-ent results, combining these methods can be veryinteresting depending on their complementarity.The oracle vote using the best response amongthe 6 best methods shows an encouraging progres-sion margin (cf.
Figure 2).
The actual vote it-self outperforms the best method for giving up to3 responses (reaching 50% for only 2 responses).However, the curve position is less clear for moreresponses, and tests must be performed on otherdrawings of K to measure the noise influence.The complementarity of the methods can alsobe exploited by training a classification model toidentify when a method is better than the others.We used the similarity values as features and themeasure that gave a good response as the refer-ence class label (best similarity if multiple, and?none?
class if no good response).
This setup wastested with the support vector machines using lib-svm (Chang and Lin, 2011) and results are shownin Figure 2.
As expected, machine learning per-forms poorly on our tiny corpus.
The accuracyis under 20% and the system only learned whento use the best method, and when to give no re-sponse.
Still, it manages to be competitive withthe best method and should be tested again withmore data and multiple drawings of K .7 Conclusion and Future WorkThe simple mapping methods based on similar-ity ranking showed up to 60% of utterance pre-cision6 remaining below a reasonable level of usersollicitation, which validate our prior hypothesis.A lot of approaches can enhance that score, suchas adding or developing more suitable similaritymeasures (Achananuparp et al, 2008), combininglearning and vote or learning to rerank utterances.However, while usable as a baseline, thesemethods only allow poor generalization and reallyneed more corpus to perform well.
As we pointedout, the non-functionality of the mapping relationalso introduces ambiguities that cannot be solvedusing the only knowledge of the system.Thanks to this baseline method, we are now ableto collect more data by developing an interactiveagent that can be both an intelligent assistant anda crowdsourcing platform.
We are currently de-veloping a web interface for this purpose.
Finally,situated human computer interaction will allow thereal-time resolving of ambiguities met in the re-trieval with the help of the user or with the use ofcontextual information from the dialogue.AknowledgementsThe authors are grateful to every internal and ex-ternal reviewer for their valuable advices.
We alsowould like to thank Google for the financial sup-port for the authors participation to the conference.6The corpus will soon be made available.39ReferencesPalakorn Achananuparp, Xiaohua Hu, and XiajiongShen.
2008.
The Evaluation of Sentence Similar-ity Measures.
In Data Warehousing and KnowledgeDiscovery, Springer.James Allen, Nathanael Chambers, George Ferguson,Lucian Galescu, Hyuckchul Jung, Mary Swift, andWilliam Tayson.
2007.
PLOW: A CollaborativeTask Learning Agent.
In Proceedings of the 22ndNational Conference on Artificial Intelligence.Yoav Artzi, and Luke S. Zettlemoyer.
2011.
Boot-strapping semantic parsers from conversations.
Pro-ceedings of the conference on empirical methods innatural language processing.S.R.K.
Branavan, Luke S. Zettlemoyer, and ReginaBarzilay.
2010.
Reading Between the Lines: Learn-ing to Map High-level Instructions to Commands.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics.S.R.K.
Branavan, Harr Chen, Luke S. Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement Learning forMapping Instructions to Actions.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP.Chih-Chung Chang, and Chih-Jen Lin.
2011.
LIB-SVM: A Library for Support Vector Machines.
ACMTransactions on Intelligent Systems and TechnologyOlivier Galibert.
2009.
Approches et me?thodologiespour la re?ponse automatique a` des questionsadapte?es a` un cadre interactif en domaine ouvert.Doctoral dissertation, Universite?
Paris Sud XI.Lynette Hirschman, and Robert Gaizauskas.
2001.Natural language question answering: The viewfrom here.
Natural Language Engineering 7.
Cam-bridge University Press.Nate Kushman, and Regina Barzilay.
2013.
Using Se-mantic Unification to Generate Regular Expressionsfrom Natural Language.
In Proceedings of the Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics.Marko Tadic?, Boz?o Bekavac, ?Zeljko Agic?, MateaSrebac?ic?, Das?a Berovic?, and Danijela Merkler.2012.
Early machine translation based semantic an-notation prototype XLike project www.xlike.org .Dave Toney, Sophie Rosset, Aure?lien Max, OlivierGalibert, and e?ric Billinski.
2008.
An Evaluation ofSpoken and Textual Interaction on the RITEL Inter-active Question Answering System In Proceedingsof the Sixth International Conference on LanguageResources and Evaluation.Svitlana Volkova, Pallavi Choudhury, Chris Quirk, BillDolan, and Luke Zettlemoyer.
2013.
Lightly Su-pervised Learning of Procedural Dialog System InProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics.Haonan Yu, and Jeffrey Mark Siskind.
2013.Grounded Language Learning from Video De-scribed with Sentences.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics.40
