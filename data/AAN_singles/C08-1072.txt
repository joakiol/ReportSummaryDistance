Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 569?576Manchester, August 2008A Syntactic Time-Series Model for Parsing Fluent and Disfluent Speech ?Tim MillerDepartment of Computer Scienceand EngineeringUniversity of Minnesotatmill@cs.umn.eduWilliam SchulerDepartment of Computer Scienceand EngineeringUniversity of Minnesotaschuler@cs.umn.eduAbstractThis paper describes an incremental ap-proach to parsing transcribed spontaneousspeech containing disfluencies with a Hier-archical Hidden Markov Model (HHMM).This model makes use of the right-cornertransform, which has been shown to in-crease non-incremental parsing accuracyon transcribed spontaneous speech (Millerand Schuler, 2008), using trees trans-formed in this manner to train the HHMMparser.
Not only do the representationsused in this model align with structure inspeech repairs, but as an HMM-like time-series model, it can be directly integratedinto conventional speech recognition sys-tems run on continuous streams of audio.A system implementing this model is eval-uated on the standard task of parsing theSwitchboard corpus, and achieves an im-provement over the standard baseline prob-abilistic CYK parser.1 IntroductionDisfluency is one obstacle preventing speechrecognition systems from being able to recog-nize spontaneous speech.
Perhaps the most chal-lenging aspect of disfluency recognition is thephenomenon of speech repair, which involves aspeaker realizing a mistake, cutting off the flowof speech, and then continuing on, possibly re-tracing and replacing part of the utterance to thatpoint.
This paper will describe a system which ap-plies a syntactic model of speech repair to a time-?The authors would like to thank the anonymous review-ers for their input.
This research was supported by NationalScience Foundation CAREER/PECASE award 0447685.
Theviews expressed are not necessarily endorsed by the sponsors.?c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.series parsing model, and evaluate that system onthe standard Switchboard corpus parsing task.The speech repair terminology used here fol-lows that of Shriberg (1994).
A speech repair con-sists of a reparandum, an interruption point, andthe alteration.
The reparandum contains the wordsthat the speaker means to replace, including bothwords that are in error and words that will be re-traced.
The interruption point is the point in timewhere the stream of speech is actually stopped, andthe repairing of the mistake can begin.
The alter-ation contains the words that are meant to replacethe words in the reparandum.2 BackgroundHistorically, research in speech repair has focusedon acoustic cues such as pauses and prosodic con-tours for detecting repairs, which could then be ex-cised from the text for improved transcription.
Re-cent work has also looked at the possible contribu-tion of higher-level cues, including syntactic struc-ture, in the detection of speech repair.
Some of thiswork is inspired by Levelt?s (1983) investigation ofthe syntactic and semantic variables in speech re-pairs, particularly his well-formedness rule, whichstates that the reparandum and alteration of a repairtypically have the same consitituent label, similarto coordination.Hale et al (2006) use Levelt?s well-formednessrule to justify an annotation scheme where unfin-ished categories (marked X-UNF) have the UNFlabel appended to all ancestral category labelsall the way up to the top-most constituent be-neath an EDITED label (EDITED labels denotinga reparandum).
They reason that this should pre-vent grammar rules of finished constituents in thecorpus from corrupting the grammar of unfinishedconstituents.
While this annotation proves helpful,it also leads to the unfortunate result that a largereparandum requires several special repair rules tobe applied, even though the actual error is only569happening at one point.Intuitively, though, it seems that an error is onlyoccurring at the end of the reparandum, and thatuntil that point only fluent grammar rules are be-ing applied by the speaker.
This intuition has alsobeen confirmed by empirical studies (Nakatani andHirschberg, 1994), which show that there is no ob-vious error signal in speech up until the moment ofinterruption.
Although speakers may retrace muchof the reparandum for clarity or other reasons, ide-ally the reparandum contains nothing but standardgrammatical rules until the speech is interrupted.1Another recent approach to this problem (John-son and Charniak, 2004) uses a tree-adjoininggrammar (TAG) approach to define a mapping be-tween a source sentence possibly containing a re-pair, and a target, fluent sentence.
The use ofthe TAG channel model is justified by the putativecrossing dependencies seen in repairs like .
.
.
aflight to Boston, uh, I mean, to Denver.
.
.
wherethere is repetition from the reparandum to the re-pair.
Essentially, this model is building up thereparandum and alteration in tandem, based onthese crossing dependencies.
While this is an inter-esting model, it focuses on detection and removalof EDITED sections, and subsequent parsing ofcleaned up speech.
As such, it introduces chal-lenges for integrating the system into a real-timespeech recognizer.Recent work by Miller and Schuler (2008)showed how a probabilistic grammar trained ontrees modified by use of the right-corner transformcan improve parsing accuracy over an unmodifiedgrammar when tested on the Switchboard corpus.The approach described here builds on that work inusing right-corner transformed trees, and extends itby mapping them to a time-series model to do pars-ing directly in a model of the sort used in speechrecognition.
This system is shown to be more ac-curate than a baseline CYK parser when used toparse the Switchboard corpus.
The remainder ofthis section will review the right-corner transform,followed by Section 3, which will step through anextended example giving details about the trans-form process and its applicability to the problemof processing speech repairs.1One objection to this claim is the case of multiple nestedrepairs.
In this case, though, we presume that all reparandawere originally intended by the speaker to be fluent at the timeof generation.2.1 Right-corner transformThe right-corner transform rewrites syntax trees,turning all right branching structure into leftbranching structure, and leaving left branchingstructure as is.
As a result, constituent structurecan be explicitly built from the bottom up dur-ing incremental recognition.
This arrangement iswell-suited to recognition of speech with errors,because it allows constituent structure to be builtup using fluent speech rules until the moment ofinterruption, at which point a special repair rulemay be applied.Before transforming the trees in the gram-mar into right-corner trees, trees are binarized inthe same manner as Johnson (1998b) and Kleinand Manning (2003).2 Binarized trees are thentransformed into right-corner trees using trans-form rules similar to those described by John-son (1998a).
In this transform, all right branch-ing sequences in each tree are transformed intoleft branching sequences of symbols of the formA1/A2, denoting an incomplete instance of cate-gory A1lacking an instance of category A2to theright.3Rewrite rules for the right-corner transform areshown below, first flattening right-branching struc-ture:A1?1A2?2A3a3?A1A1/A2?1A2/A3?2A3a3A1?1A2A2/A3?2.
.
.?A1A1/A2?1A2/A3?2.
.
.then replacing it with left-branching structure:A1A1/A2:?1A2/A3?2.
.
.?A1A1/A3A1/A2:?1?2.
.
.Here, the first two rewrite rules are applied iter-atively (bottom-up on the tree) to flatten all rightbranching structure, using incomplete constituents2For the details of the particular binarization process usedhere, see Miller and Schuler (2008).3Here, all Aidenote nonterminal symbols, and all ?ide-note subtrees; the notation A1:?1indicates a subtree ?1withlabel A1; and all rewrites are applied recursively, from leavesto root.570to record the original nonterminal ordering.
Thethird rule is then applied to generate left branchingstructure, preserving this ordering.Because this process turns right expansion intoleft expansion (leaving center expansion as theonly stack consumer), right-corner transformedtrees also require less stack memory than ordi-nary phrase structure trees.
This key property ofthe right-corner transform is exploited in the map-ping of transformed training trees to a time-seriesmodel.
This property will be examined further inSection 5.3 Speech Repair ExampleA substantial example of a speech repair from theSwitchboard corpus can be seen in Figures 1 and 2,in which the same repair and surrounding contextis shown after the preliminary binarization pro-cess, and after the right-corner transform.
Fig-ure 1 also shows, in brackets, the augmented an-notation described above from Hale et al (2006).This scheme consisted of adding -X to an EDITEDlabel which produced a category X (daughter an-notation), as well as propagating the -UNF label atthe right corner of the tree up through every parentbelow the EDITED root.3.1 Re-annotation of speech repairBefore applying the right-corner transform, somecorpus pre-processing steps are applied ?
Fig-ure 1 shows an example tree after these changes.These steps aim to improve on the default annota-tion of repair in the Switchboard corpus by mak-ing the representation more in line with linguisticas well as processing desiderata.The standard annotation practice of having theEDITED label as the category at the root of areparandum does not represent any proposed lin-guistic phenomenon, and it conflates speech re-pairs of different categories.
As a result, the parseris unable to make use of information about whichsyntactic category the reparandum was originallyintended to be.
This information would be usefulin the example discussed here, since an unfinishedNP is followed by a completed NP.
The daughterannotation used by Hale et al fixes this annota-tion to allow the parser to have access to this infor-mation.
The annotation scheme in this paper alsomakes use of the daughter annotation.In addition, trees are modified to reduce the ar-ity of speech repair rules, and to change the modelof how repairs occur.
The default Switchboardhas, e.g.
an NP reparandum (headed by EDITED)and the NP alteration represented as siblings in thesyntax tree.
This annotation seems to implicitlymodel all repairs as stopping the process of pro-ducing the current constituent, then starting a newone.
In contrast, the representation here models re-pairs of this type as a speaker generating a partialnoun phrase, realizing an error, and then continu-ing to generate the same noun phrase.
This rep-resentation scheme thus represents the beginningof an effort to distinguish between repairs that arechanging the direction of the utterance and thosethat are just fixing a local mistake.3.2 Right-corner transform model of speechrepairThe right corner transform is then applied to thisexample in Figure 2.
The resulting tree represen-tation is especially valuable because it models hu-man production of speech repairs well, by not ap-plying any special rule until the moment of inter-ruption.In the example in Figure 1, there is an unfinishedconstituent (PP-UNF) at the end of the reparan-dum.
This standard annotation is deficient becauseeven if an unfinished consituent like PP-UNF iscorrectly recognized, and the speaker is essentiallyin an error state, there may be several partiallycompleted constituents above ?
in Figure 1, theNP, PP, and NP above the PP-UNF.
These con-stituents need to be completed, but using the stan-dard annotation there is only one chance to makeuse of the information about the error that has oc-curred ?
the ?NP ?
NP PP-UNF?
rule.
Thus, bythe time the error section is completed, there is noinformation by which a parsing algorithm couldchoose to reduce the topmost NP to EDITED (orEDITED-NP) other than independent rule proba-bilities.The approach used by Hale et al (2006) worksbecause the information about the transition to an?error state?
is propagated up the tree, in the formof the -UNF tags.
As the parsing chart is filledin from the bottom up, each rule applied is essen-tially coming out of a special repair rule set, andso at the top of the tree the EDITED hypothesis ismuch more likely.
However, this requires that sev-eral fluent speech rules from the data set be modi-fied for use in a special repair grammar, which notonly reduces the amount of available training data,571SNPCCandNPEDITED-NPNPDTtheNNJJfirstNNkindPP[-UNF]INofNP[-UNF]NPinvasionPP-UNFofNPNPDTtheNNJJfirstNNtypePPINofNPprivacyVP.
.
.Figure 1: Binarized tree repair structure, with the -UNF propagation as in Hale et al (2006) shown inbrackets.S/VPS/VPS/SCCandNPNP/NPNP/PPNP/NPEDITED-NPNP/PPNP/NPNP/PPNPNP/NNNP/NNDTtheJJfirstNNkindINofNPinvasionPP-UNFofNPNP/NNNP/NNDTtheJJfirstNNtypeINofNPprivacyVBD.
.
.Figure 2: Right-corner transformed tree with repair structurebut violates our intuition that reparanda are usuallyfluent up until the actual edit occurs.The right corner transform works in a differentway, by building up constituent structure from leftto right.
In Figure 2, the same repair is shownas it appears in the training data for this system.With this representation, the problem noticed byHale and colleagues has been solved in a differentway, by incrementally building up left-branchingrather than right-branching structure, so that onlya single special error rule is required at the end ofthe constituent.
As seen in the figure, all of thestructure beneath the EDITED-NP label is built us-ing rules from the fluent grammar.
It is only atone point, when the PP-UNF is found, that a re-pair rule is applied and the EDITED-NP section isfound.
The next step in the process is that the NPessentially restarts (the NP/NP label), and the sub-sequent words start to build up what will be the NPalteration in a fluent manner.To summarize, while the -UNF propagationscheme often requires the entire reparandum tobe generated from a speech repair rule set, thisscheme only requires one special rule, where themoment of interruption actually occurred.
This re-duces the number of special speech repair rulesthat need to be learned and saves more potentialexamples of fluent speech rules, and therefore po-tentially makes better use of limited data.4 Mapping to an HHMMThis section describes how a corpus of trees trans-formed as above can be mapped to a time-seriesmodel called a Hierarchical Hidden Markov Model(HHMM) in order to incorporate parsing intospeech decoding.
This suggests that this approachcan be used in applications using streaming speechinput, unlike other parsing approaches which arecubic time on input length at best, and require in-put to be pre-segmented.This section will begin by showing how HH-MMs can model linguistic structure by extendingstandard Hidden Markov Models (HMMs) used inspeech recognition, and will follow with a descrip-tion of how right-corner transformed trees can bemapped to this model topology.5724.1 Hierarchical HMMsIn general, the hidden state in an HMM can be assimple or complex as necessary.
This can includefactorizing the hidden state into any number of in-terdependent random variables modeling the sub-states of the complex hidden state.
A Hierarchi-cal Hidden Markov Model is essentially an HMMwith a specific factorization that is useful in manydomains ?
the hidden state at each time step isfactored into d random variables which function asa stack, and d additional boolean random variableswhich regulate the operations of the stack throughtime.
The boolean random variables are typicallymarginalized out when performing inference on asequence.While the vertical direction of the hidden sub-states (at a fixed t) represents a stack at a sin-gle point in time, the horizontal direction of thehidden sub-states (at a fixed d) can be viewed assimple HMMs at depth d, taking direction fromthe HMM above them and controlling those be-low them.
This interpretation will be useful whenformally defining the transitions between the stackelements at different time steps below.Formally, HMMs characterize speech or text asa sequence of hidden states qt(which may con-sist of speech sounds, words, and/or other hypoth-esized syntactic or semantic information), and ob-served states otat corresponding time steps t (typ-ically short, overlapping frames of an audio sig-nal, or words or characters in a text processingapplication).
A most likely sequence of hiddenstates q?1..Tcan then be hypothesized given any se-quence of observed states o1..T, using Bayes?
Law(Equation 2) and Markov independence assump-tions (Equation 3) to define a full P(q1..T| o1..T)probability as the product of a Language Model(?L) prior probability and an Observation Model(?O) likelihood probability:q?1..T= argmaxq1..TP(q1..T| o1..T) (1)= argmaxq1..TP(q1..T) ?
P(o1..T| q1..T) (2)def= argmaxq1..TT?t=1P?L(qt| qt-1)?P?O(ot| qt) (3)Language model transitions P?L(qt| qt?1) overcomplex hidden states qtcan be modeled us-ing synchronized levels of stacked-up compo-nent HMMs in a Hierarchic Hidden MarkovModel (HHMM) (Murphy and Paskin, 2001).HHMM transition probabilities are calculated intwo phases: a ?reduce?
phase (resulting in an in-termediate, marginalized state ft), in which com-ponent HMMs may terminate; and a ?shift?
phase(resulting in a modeled state qt), in which untermi-nated HMMs transition, and terminated HMMs arere-initialized from their parent HMMs.
Variablesover intermediate ftand modeled qtstates are fac-tored into sequences of depth-specific variables ?one for each of D levels in the HMM hierarchy:ft= ?f1t.
.
.
fDt?
(4)qt= ?q1t.
.
.
qDt?
(5)Transition probabilities are then calculated as aproduct of transition probabilities at each level, us-ing level-specific ?reduce?
?F and ?shift?
?Q mod-els:P?L(qt|qt-1) =?ftP(ft|qt-1)?P(qt|ftqt-1) (6)def=?f1..DtD?d=1P?F(fdt| fd+1tqdt-1qd-1t-1)?P?Q(qdt|fd+1tfdtqdt-1qd-1t) (7)with fD+1tand q0tdefined as constants.Shift and reduce probabilities are now definedin terms of finitely recursive FSAs with probabil-ity distributions over transition, recursive expan-sion, and final-state status of states at each hier-archy level.
In simple HHMMs, each interme-diate state variable is a boolean switching vari-able fdt?
{0,1} and each modeled state variableis a syntactic, lexical, or phonetic state qdt.
Theintermediate variable fdtis true (equal to 1) withprobability 1 if there is a transition at the level im-mediately below d and the stack element qdt?1is afinal state, and false (equal to 0) with probability 1otherwise:4P?F(fdt| fd+1tqdt?1qd?1t?1)def={if fd+1t=0 : [fdt=0]if fd+1t=1 : P?F-Reduce(fdt| qdt?1, qd?1t?1)(8)where fD+1 = 1 and q0t= ROOT.Shift probabilities at each level are defined us-ing level-specific transition ?Q-Trans and expan-4Here [?]
is an indicator function: [?]
= 1 if ?
is true, 0otherwise.573d=1d=2d=3wordt=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13and thefirstkind ofinvasionof thefirsttype ofprivacy.
.
.?
?
?
?
?
?
?
?NP/NNNP/NN?
??
?NP/NNNP/NNNP/PPNP/NPNP/PPNP/NPNP/NPNP/NPNP/PPNP/NP?S/S S/S S/S S/S S/S S/S S/S S/S S/S S/S S/SS/VPFigure 3: Sample tree from Figure 2 mapped to qdtvariable positions of an HHMM at each stack depth d(vertical) and time step t (horizontal).
Values for final-state variables fdtare not shown.
Note that somenonterminal labels have been omitted; labels for these nodes can be reconstructed from their children.This includes the EDITED-NP nonterminal that occurs as a child of the NP/NP at t=8, d=2, indicated inboldface.sion ?Q-Expand models:P?Q(qdt| fd+1tfdtqdt?1qd?1t)def=??
?if fd+1t=0, fdt=0 : [qdt= qdt?1]if fd+1t=1, fdt=0 : P?Q-Trans(qdt| qdt?1qd?1t)if fd+1t=1, fdt=1 : P?Q-Expand(qdt| qd?1t)(9)where fD+1 = 1 and q0t= ROOT.
This modelis conditioned on final-state switching variables atand immediately below the current HHMM level.If there is no final state immediately below thecurrent level (the first case above), it determinis-tically copies the current HHMM state forward tothe next time step.
If there is a final state imme-diately below the current level (the second caseabove), it transitions the HHMM state at the cur-rent level, according to the distribution ?Q-Trans.And if the state at the current level is final (thethird case above), it re-initializes this state giventhe state at the level above, according to the distri-bution ?Q-Expand.
The overall effect is that higher-level HMMs are allowed to transition only whenlower-level HMMs terminate.
An HHMM there-fore behaves like a probabilistic implementation ofa pushdown automaton (or ?shift-reduce?
parser)with a finite stack, where the maximum stack depthis equal to the number of levels in the HHMM hi-erarchy.4.2 Mapping trees to HHMM derivationsAny tree can now be mapped to an HHMM deriva-tion by aligning the nonterminals with qdtcate-gories.
First, it is necessary to define rightwarddepth d, right index position t, and final (right)child status f , for every nonterminal node A in atree, where:?
d is defined to be the number of right branchesbetween node A and the root,?
t is defined to be the number of words beneathor to the left of node A, and?
f is defined to be 0 if node A is a left (orunary) child, 1 otherwise.Any binary-branching tree can then be annotatedwith these values and rewritten to define labels andfinal-state values for every combination of d and tcovered by the tree.
This process simply copiesstacked up constituents over multiple time steps,while other constituents are being recognized.
Co-ordinates d, t ?
D,T that are not covered by thetree are assigned label ??
?, and f = 1.
The result-ing label and final-state values at each node nowdefine a value of qdtand fdt+1for each depth d andtime step t of the HHMM (see Figure 3).
Prob-abilities for HHMM models ?Q-Expand, ?Q-Trans,and ?F-Reduce can then be estimated from these val-ues directly.
Like the right-corner transform, thismapping is reversible, so q and f values can betaken from a hypothesized most likely sequenceand mapped back to trees (which can then undergothe reverse of the right-corner transform to becomeordinary phrase structure trees).5 HHMM Application to Speech RepairWhile the HHMM parser described above can pro-duce the same output as a standard probablisticCYK parser (the most likely parse tree), the differ-ent parsing strategy of an HHMM parser and theclose connection of this system with a probabilis-tic model of semantics present potential benefits tothe recognition of disfluent speech.574First, by using a depth-limited stack, this modelbetter adheres to psycholinguistically observedshort term memory limits that the human parserand generator are likely to obey (Cowan, 2001;Miller, 1956).
The use of a depth-limited stackis enabled by the right-corner transform?s prop-erty of transforming right expansions to left ex-pansions, which minimizes stack usage.
Corpusstudies (Schuler et al, 2008) suggest that broadcoverage parsing can be achieved via this trans-form using only four stack elements.
In practicalterms this means that the model is less likely thana standard CYK parser to spend time and probabil-ity mass on analyses that conflict with the memorylimits humans appear to be constrained by whengenerating and understanding speech.Second, this model is part of a more generalframework that incorporates a model of referentialsemantics into the parsing model of the HHMM(Schuler et al, in press).
While the frameworkevaluated in this paper models only the syntacticcontribution to speech repair, there are some caseswhere syntactic cues are not sufficient to distin-guish disfluent from fluent utterances.
In manyof these cases, semantic information is the onlyway to tell that an utterance contains a repair.5 Arecognition system that incorporates referential se-mantics with syntax should improve the accuracyof speech repair recognition as it has been shownto increase recognition of entities in fluent speechrecognition.Finally, the semantic model just described, aswell as the mechanics of the HHMM parser ona right-corner transformed grammar, combine toform a model that accounts for two previouslydistant aspects of speech processing: referentialsemantics and speech repair.
From the genera-tive view of language processing, the model startswith a desired referent, and based on that refer-ent selects the appropriate syntactic structures, andwithin those it selects the appropriate lexical itemsto unambiguously describe the referent.
In the se-mantic sense, then, the model is operating in atop-down fashion, with the referent being the driv-ing force for the generation of syntax and words.However, since the model is also working in a left-5For example, the sentence ?The red.
.
.
uh.
.
.
blue box?
ismore likely to be considered a repair in a context with sin-gle colored boxes, whereas the sentence ?The big.
.
.
uh.
.
.
bluebox?
is less likely to be considered a repair in the same con-text, although the two sentences have the same syntactic struc-ture.to-right fashion on a highly left-branching gram-mar, there is also a bottom-up composition of con-stituents, which models the phenomenon of speechrepair naturally and accurately.6 EvaluationThe evaluation of this system was performed onthe Switchboard corpus of transcribed conversa-tional speech, using the mrg annotations in directo-ries 2 and 3 for training, and the files sw4004.mrgto sw4153.mrg in directory 4 for evaluation, fol-lowing Johnson and Charniak (2004).
In additionto testing the HHMM parser on the Switchboardcorpus, the experiment testing a CYK parser fromMiller and Schuler (2008) was replicated, withslightly better results due to a change in the evalu-ation script6 and small changes in the binarizationprocess (both of these changes affect the baselineand test systems).The input to the system consists of the terminalsymbols from the trees in the corpus section men-tioned above.
The terminal symbol strings are firstpre-processed by stripping punctuation and emptycategories, which could not be expected from theoutput of a speech recognizer.
In addition, any in-formation about repair is stripped from the input,including partial words, repair symbols,7 and in-terruption point information.
While an integratedsystem for processing and parsing speech may useboth acoustic and syntactic information to find re-pairs, and thus may have access to some of thisinformation about where interruptions occur, thistesting paradigm is intended to evaluate the use ofthe right-corner transform in a time-series modelon parsing speech repair.
To make a fair compari-son to the CYK baseline of Hale et al (2006), therecognizer was given correct part-of-speech tags asinput along with words.The results presented here use two standard met-rics for assessing accuracy of transcribed speechwith repairs.
The first metric, Parseval F-measure,takes into account precision and recall of all non-terminal (and non pre-terminal) constituents in ahypothesized tree relative to the gold standard.
Thesecond metric, EDIT-finding F, measures precisionand recall of the words tagged as EDITED in thehypothesized tree relative to those tagged EDITED6Specifically, we switched to using the evalb tool createdby Sekine and Collins (1997).7The Switchboard corpus has special terminal symbols in-dicating e.g.
the start and end of the reparandum.575in the gold standard.
F score is defined as usual,2pr/(p + r) for precision p and recall r.Table 1 below shows the results of experimentsusing the model of speech repair described in thispaper.
The ?Baseline?
result shows the accuracy ofthe binarized grammar at parsing the Switchboardtest set.
The ?RCT?
result shows the accuracyof parsing when the right-corner transform is per-formed on the trees in the training set prior to train-ing.
Finally, the ?HHMM+RCT?
results shows theaccuracy of the HHMM parser system describedin this paper, trained on right-corner trees mappedto the random variables at each time step.
?CYK?and ?TAG?
lines show relevant results from relatedwork.System Parseval F EDIT FBaseline 63.43 41.82CYK (H06) 71.16 41.7RCT 73.21 61.03HHMM+RCT 77.15 68.03TAG-based model (JC04) ?
79.7Table 1: Summary of results.These results show an improvement over thestandard CYK parsing algorithm, in both overallparsing accuracy and EDIT-finding accuracy.
Thisshows that the HHMM parser, which is more ap-plicable to speech input due to its asymptotic lineartime complexity, does not need to sacrifice any ac-curacy to do so, and indeed improves on accuracyfor both metrics under consideration.7 ConclusionThe work described here has extended previouswork for recognizing disfluent speech to an incre-mental model, moving in a direction that holdspromise for eventual direct implementation in aspeech recognizer.Extending this model to actual speech addssome complexity, since disfluency phenomena aredifficult to detect in an audio signal.
However,there are also advantages in this extension, sincethe extra phonological variables and acoustic ob-servations contain information that can be usefulin the recognition of disfluency phenomena.ReferencesCowan, Nelson.
2001.
The magical number 4 in short-term memory: A reconsideration of mental storagecapacity.
Behavioral and Brain Sciences, 24:87?185.Hale, John, Izhak Shafran, Lisa Yung, Bonnie Dorr,Mary Harper, Anna Krasnyanskaya, Matthew Lease,Yang Liu, Brian Roark, Matthew Snover, and RobinStewart.
2006.
PCFGs with syntactic and prosodicindicators of speech repairs.
In Proceedings of the45th Annual Conference of the Association for Com-putational Linguistics (COLING-ACL).Johnson, Mark and Eugene Charniak.
2004.
A tag-based noisy channel model of speech repairs.
In Pro-ceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL ?04), pages33?39, Barcelona, Spain.Johnson, Mark.
1998a.
Finite state approximation ofconstraint-based grammars using left-corner gram-mar transforms.
In Proceedings of COLING/ACL,pages 619?623.Johnson, Mark.
1998b.
PCFG models of linguistic treerepresentation.
Computational Linguistics, 24:613?632.Klein, Dan and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 423?430.Levelt, William J.M.
1983.
Monitoring and self-repairin speech.
Cognition, 14:41?104.Miller, Tim and William Schuler.
2008.
A unified syn-tactic model for parsing fluent and disfluent speech.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics (ACL ?08).Miller, George A.
1956.
The magical number seven,plus or minus two: Some limits on our capacityfor processing information.
Psychological Review,63:81?97.Murphy, Kevin P. and Mark A. Paskin.
2001.
Lin-ear time inference in hierarchical HMMs.
In Proc.NIPS, pages 833?840.Nakatani, C. and J. Hirschberg.
1994.
A corpus-basedstudy of repair cues in spontaneous speech.
TheJournal of the Acoustic Society of America, 95:1603?1616.Schuler, William, Samir AbdelRahman, TimMiller, and Lane Schwartz.
2008.
Toward apsycholinguistically-motivated model of language.In Proceedings of COLING, Manchester, UK.Schuler, William, Stephen Wu, and Lane Schwartz.
inpress.
A framework for fast incremental interpre-tation during speech decoding.
Computational Lin-guistics.Sekine, Satoshi and Michael Collins.
1997.
Evalbbracket scoring program.Shriberg, Elizabeth.
1994.
Preliminaries to a Theoryof Speech Disfluencies.
Ph.D. thesis, University ofCalifornia at Berkeley.576
