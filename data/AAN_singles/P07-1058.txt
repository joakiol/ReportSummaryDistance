Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456?463,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsInstance-based Evaluation of Entailment Rule AcquisitionIdan Szpektor, Eyal Shnarch, Ido DaganDept.
of Computer ScienceBar Ilan UniversityRamat Gan, Israel{szpekti,shey,dagan}@cs.biu.ac.ilAbstractObtaining large volumes of inference knowl-edge, such as entailment rules, has becomea major factor in achieving robust seman-tic processing.
While there has been sub-stantial research on learning algorithms forsuch knowledge, their evaluation method-ology has been problematic, hindering fur-ther research.
We propose a novel evalua-tion methodology for entailment rules whichexplicitly addresses their semantic proper-ties and yields satisfactory human agreementlevels.
The methodology is used to comparetwo state of the art learning algorithms, ex-posing critical issues for future progress.1 IntroductionIn many NLP applications, such as Question An-swering (QA) and Information Extraction (IE), it iscrucial to recognize that a particular target mean-ing can be inferred from different text variants.
Forexample, a QA system needs to identify that ?As-pirin lowers the risk of heart attacks?
can be inferredfrom ?Aspirin prevents heart attacks?
in order to an-swer the question ?What lowers the risk of heart at-tacks??.
This type of reasoning has been recognizedas a core semantic inference task by the generic tex-tual entailment framework (Dagan et al, 2006).A major obstacle for further progress in seman-tic inference is the lack of broad-scale knowledge-bases for semantic variability patterns (Bar-Haim etal., 2006).
One prominent type of inference knowl-edge representation is inference rules such as para-phrases and entailment rules.
We define an entail-ment rule to be a directional relation between twotemplates, text patterns with variables, e.g.
?X pre-vent Y ?
X lower the risk of Y ?.
The left-hand-side template is assumed to entail the right-hand-side template in certain contexts, under the samevariable instantiation.
Paraphrases can be viewedas bidirectional entailment rules.
Such rules capturebasic inferences and are used as building blocks formore complex entailment inference.
For example,given the above rule, the answer ?Aspirin?
can beidentified in the example above.The need for large-scale inference knowledge-bases triggered extensive research on automatic ac-quisition of paraphrase and entailment rules.
Yet thecurrent precision of acquisition algorithms is typ-ically still mediocre, as illustrated in Table 1 forDIRT (Lin and Pantel, 2001) and TEASE (Szpek-tor et al, 2004), two prominent acquisition algo-rithms whose outputs are publicly available.
Thecurrent performance level only stresses the obviousneed for satisfactory evaluation methodologies thatwould drive future research.The prominent approach in the literature for eval-uating rules, termed here the rule-based approach, isto present the rules to human judges asking whethereach rule is correct or not.
However, it is difficult toexplicitly define when a learned rule should be con-sidered correct under this methodology, and this wasmainly left undefined in previous works.
As the cri-terion for evaluating a rule is not well defined, usingthis approach often caused low agreement betweenhuman judges.
Indeed, the standards for evaluationin this field are lower than other fields: many papers456don?t report on human agreement at all and thosethat do report rather low agreement levels.
Yet itis crucial to reliably assess rule correctness in or-der to measure and compare the performance of dif-ferent algorithms in a replicable manner.
Lacking agood evaluation methodology has become a barrierfor further advances in the field.In order to provide a well-defined evaluationmethodology we first explicitly specify when entail-ment rules should be considered correct, followingthe spirit of their usage in applications.
We thenpropose a new instance-based evaluation approach.Under this scheme, judges are not presented onlywith the rule but rather with a sample of sentencesthat match its left hand side.
The judges then assesswhether the rule holds under each specific example.A rule is considered correct only if the percentage ofexamples assessed as correct is sufficiently high.We have experimented with a sample of inputverbs for both DIRT and TEASE.
Our results showsignificant improvement in human agreement overthe rule-based approach.
It is also the first compar-ison between such two state-of-the-art algorithms,which showed that they are comparable in precisionbut largely complementary in their coverage.Additionally, the evaluation showed that both al-gorithms learn mostly one-directional rules ratherthan (symmetric) paraphrases.
While most NLP ap-plications need directional inference, previous ac-quisition works typically expected that the learnedrules would be paraphrases.
Under such an expec-tation, unidirectional rules were assessed as incor-rect, underestimating the true potential of these algo-rithms.
In addition, we observed that many learnedrules are context sensitive, stressing the need to learncontextual constraints for rule applications.2 Background: Entailment Rules and theirEvaluation2.1 Entailment RulesAn entailment rule ?L ?
R?
is a directional rela-tion between two templates, L and R. For exam-ple, ?X acquire Y ?
X own Y ?
or ?X beat Y ?X play against Y ?.
Templates correspond to textfragments with variables, and are typically either lin-ear phrases or parse sub-trees.The goal of entailment rules is to help applica-Input Correct Incorrect(?)
X modify Y X adopt YX change Y (?)
X amend Y X create Y(DIRT) (?)
X revise Y X stick to Y(?)
X alter Y X maintain YX change Y (?)
X affect Y X follow Y(TEASE) (?)
X extend Y X use YTable 1: Examples of templates suggested by DIRTand TEASE as having an entailment relation, insome direction, with the input template ?X changeY ?.
The entailment direction arrows were judgedmanually and added for readability.tions infer one text variant from another.
A rule canbe applied to a given text only when L can be in-ferred from it, with appropriate variable instantia-tion.
Then, using the rule, the application deducesthat R can also be inferred from the text under thesame variable instantiation.
For example, the rule?X lose to Y ?Y beat X?
can be used to infer ?Liv-erpool beat Chelsea?
from ?Chelsea lost to Liver-pool in the semifinals?.Entailment rules should typically be applied onlyin specific contexts, which we term relevant con-texts.
For example, the rule ?X acquire Y ?X buy Y ?
can be used in the context of ?buying?events.
However, it shouldn?t be applied for ?Stu-dents acquired a new language?.
In the same man-ner, the rule ?X acquire Y ?X learn Y ?
should beapplied only when Y corresponds to some sort ofknowledge, as in the latter example.Some existing entailment acquisition algorithmscan add contextual constraints to the learned rules(Sekine, 2005), but most don?t.
However, NLP ap-plications usually implicitly incorporate some con-textual constraints when applying a rule.
For ex-ample, when answering the question ?Which com-panies did IBM buy??
a QA system would applythe rule ?X acquire Y ?X buy Y ?
correctly, sincethe phrase ?IBM acquire X?
is likely to be foundmostly in relevant economic contexts.
We thus ex-pect that an evaluation methodology should considercontext relevance for entailment rules.
For example,we would like both ?X acquire Y ?X buy Y ?
and?X acquire Y ?X learn Y ?
to be assessed as cor-rect (the second rule should not be deemed incorrect457just because it is not applicable in frequent economiccontexts).Finally, we highlight that the common notion of?paraphrase rules?
can be viewed as a special caseof entailment rules: a paraphrase ?L?
R?
holds ifboth templates entail each other.
Following the tex-tual entailment formulation, we observe that manyapplied inference settings require only directionalentailment, and a requirement for symmetric para-phrase is usually unnecessary.
For example, in or-der to answer the question ?Who owns Overture?
?it suffices to use a directional entailment rule whoseright hand side is ?X own Y ?, such as ?X acquireY ?X own Y ?, which is clearly not a paraphrase.2.2 Evaluation of Acquisition AlgorithmsMany methods for automatic acquisition of ruleshave been suggested in recent years, ranging fromdistributional similarity to finding shared contexts(Lin and Pantel, 2001; Ravichandran and Hovy,2002; Shinyama et al, 2002; Barzilay and Lee,2003; Szpektor et al, 2004; Sekine, 2005).
How-ever, there is still no common accepted frameworkfor their evaluation.
Furthermore, all these methodslearn rules as pairs of templates {L,R} in a sym-metric manner, without addressing rule directional-ity.
Accordingly, previous works (except (Szpektoret al, 2004)) evaluated the learned rules under theparaphrase criterion, which underestimates the prac-tical utility of the learned rules (see Section 2.1).One approach which was used for evaluating au-tomatically acquired rules is to measure their contri-bution to the performance of specific systems, suchas QA (Ravichandran and Hovy, 2002) or IE (Sudoet al, 2003; Romano et al, 2006).
While measuringthe impact of learned rules on applications is highlyimportant, it cannot serve as the primary approachfor evaluating acquisition algorithms for several rea-sons.
First, developers of acquisition algorithms of-ten do not have access to the different applicationsthat will later use the learned rules as generic mod-ules.
Second, the learned rules may affect individualsystems differently, thus making observations thatare based on different systems incomparable.
Third,within a complex system it is difficult to assess theexact quality of entailment rules independently ofeffects of other system components.Thus, as in many other NLP learning settings,a direct evaluation is needed.
Indeed, the promi-nent approach for evaluating the quality of rule ac-quisition algorithms is by human judgment of thelearned rules (Lin and Pantel, 2001; Shinyama etal., 2002; Barzilay and Lee, 2003; Pang et al, 2003;Szpektor et al, 2004; Sekine, 2005).
In this evalua-tion scheme, termed here the rule-based approach, asample of the learned rules is presented to the judgeswho evaluate whether each rule is correct or not.
Thecriterion for correctness is not explicitly described inmost previous works.
By the common view of con-text relevance for rules (see Section 2.1), a rule wasconsidered correct if the judge could think of rea-sonable contexts under which it holds.We have replicated the rule-based methodologybut did not manage to reach a 0.6 Kappa agree-ment level between pairs of judges.
This approachturns out to be problematic because the rule correct-ness criterion is not sufficiently well defined and ishard to apply.
While some rules might obviouslybe judged as correct or incorrect (see Table 1), judg-ment is often more difficult due to context relevance.One judge might come up with a certain contextthat, to her opinion, justifies the rule, while anotherjudge might not imagine that context or think thatit doesn?t sufficiently support rule correctness.
Forexample, in our experiments one of the judges didnot identify the valid ?religious holidays?
contextfor the correct rule ?X observe Y ?X celebrate Y ?.Indeed, only few earlier works reported inter-judgeagreement level, and those that did reported ratherlow Kappa values, such as 0.54 (Barzilay and Lee,2003) and 0.55 - 0.63 (Szpektor et al, 2004).To conclude, the prominent rule-based methodol-ogy for entailment rule evaluation is not sufficientlywell defined.
It results in low inter-judge agreementwhich prevents reliable and consistent assessmentsof different algorithms.3 Instance-based Evaluation MethodologyAs discussed in Section 2.1, an evaluation methodol-ogy for entailment rules should reflect the expectedvalidity of their application within NLP systems.Following that line, an entailment rule ?L ?
R?should be regarded as correct if in all (or at leastmost) relevant contexts in which the instantiatedtemplate L is inferred from the given text, the instan-458Rule Sentence Judgment1 X seek Y ?X disclose Y If he is arrested, he can immediately seek bail.
Left not entailed2 X clarify Y ?X prepare Y He didn?t clarify his position on the subject.
Left not entailed3 X hit Y ?X approach Y Other earthquakes have hit Lebanon since ?82.
Irrelevant context4 X lose Y ?X surrender Y Bread has recently lost its subsidy.
Irrelevant context5 X regulate Y ?X reform Y The SRA regulates the sale of sugar.
No entailment6 X resign Y ?X share Y Lopez resigned his post at VW last week.
No entailment7 X set Y ?X allow Y The committee set the following refunds.
Entailment holds8 X stress Y ?X state Y Ben Yahia also stressed the need for action.
Entailment holdsTable 2: Rule evaluation examples and their judgment.tiated template R is also inferred from the text.
Thisreasoning corresponds to the common definition ofentailment in semantics, which specifies that a textL entails another text R if R is true in every circum-stance (possible world) in which L is true (Chierchiaand McConnell-Ginet, 2000).It follows that in order to assess if a rule is cor-rect we should judge whether R is typically en-tailed from those sentences that entail L (within rel-evant contexts for the rule).
We thus present a newevaluation scheme for entailment rules, termed theinstance-based approach.
At the heart of this ap-proach, human judges are presented not only witha rule but rather with a sample of examples of therule?s usage.
Instead of thinking up valid contextsfor the rule the judges need to assess the rule?s va-lidity under the given context in each example.
Theessence of our proposal is a (apparently non-trivial)protocol of a sequence of questions, which deter-mines rule validity in a given sentence.We shall next describe how we collect a sample ofexamples for evaluation and the evaluation process.3.1 Sampling ExamplesGiven a rule ?L?R?, our goal is to generate evalua-tion examples by finding a sample of sentences fromwhich L is entailed.
We do that by automatically re-trieving, from a given corpus, sentences that matchL and are thus likely to entail it, as explained below.For each example sentence, we automatically ex-tract the arguments that instantiate L and generatetwo phrases, termed left phrase and right phrase,which are constructed by instantiating the left tem-plate L and the right template R with the extractedarguments.
For example, the left and right phrasesgenerated for example 1 in Table 2 are ?he seek bail?and ?he disclose bail?, respectively.Finding sentences that match L can be performedat different levels.
In this paper we match lexical-syntactic templates by finding a sub-tree of the sen-tence parse that is identical to the template structure.Of course, this matching method is not perfect andwill sometimes retrieve sentences that do not entailthe left phrase for various reasons, such as incorrectsentence analysis or semantic aspects like negation,modality and conditionals.
See examples 1-2 in Ta-ble 2 for sentences that syntactically match L butdo not entail the instantiated left phrase.
Since weshould assess R?s entailment only from sentencesthat entail L, such sentences should be ignored bythe evaluation process.3.2 Judgment QuestionsFor each example generated for a rule, the judges arepresented with the given sentence and the left andright phrases.
They primarily answer two questionsthat assess whether entailment holds in this example,following the semantics of entailment rule applica-tion as discussed above:Qle: Is the left phrase entailed from the sentence?A positive/negative answer corresponds to a?Left entailed/not entailed?
judgment.Qre: Is the right phrase entailed from the sentence?A positive/negative answer corresponds to an?Entailment holds/No entailment?
judgment.The first question identifies sentences that do not en-tail the left phrase, and thus should be ignored whenevaluating the rule?s correctness.
While inappropri-ate matches of the rule left-hand-side may happen459and harm an overall system precision, such errorsshould be accounted for a system?s rule matchingmodule rather than for the rules?
precision.
The sec-ond question assesses whether the rule application isvalid or not for the current example.
See examples5-8 in Table 2 for cases where entailment does ordoesn?t hold.Thus, the judges focus only on the given sentencein each example, so the task is actually to evaluatewhether textual entailment holds between the sen-tence (text) and each of the left and right phrases(hypotheses).
Following past experience in textualentailment evaluation (Dagan et al, 2006) we expecta reasonable agreement level between judges.As discussed in Section 2.1, we may want to ig-nore examples whose context is irrelevant for therule.
To optionally capture this distinction, thejudges are asked another question:Qrc: Is the right phrase a likely phrase in English?A positive/negative answer corresponds to a?Relevant/Irrelevant context?
evaluation.If the right phrase is not likely in English then thegiven context is probably irrelevant for the rule, be-cause it seems inherently incorrect to infer an im-plausible phrase.
Examples 3-4 in Table 2 demon-strate cases of irrelevant contexts, which we maychoose to ignore when assessing rule correctness.3.3 Evaluation ProcessFor each example, the judges are presented with thethree questions above in the following order: (1) Qle(2) Qrc (3) Qre.
If the answer to a certain questionis negative then we do not need to present the nextquestions to the judge: if the left phrase is not en-tailed then we ignore the sentence altogether; and ifthe context is irrelevant then the right phrase cannotbe entailed from the sentence and so the answer toQre is already known as negative.The above entailment judgments assume that wecan actually ask whether the left or right phrasesare correct given the sentence, that is, we assumethat a truth value can be assigned to both phrases.This is the case when the left and right templatescorrespond, as expected, to semantic relations.
Yetsometimes learned templates are (erroneously) notrelational, e.g.
?X , Y , IBM?
(representing a list).We therefore let the judges initially mark rules thatinclude such templates as non-relational, in whichcase their examples are not evaluated at all.3.4 Rule PrecisionWe compute the precision of a rule by the percent-age of examples for which entailment holds outof all ?relevant?
examples.
We can calculate theprecision in two ways, as defined below, dependingon whether we ignore irrelevant contexts or not(obtaining lower precision if we don?t).
Whensystems answer an information need, such as aquery or question, irrelevant contexts are sometimesnot encountered thanks to additional context whichis present in the given input (see Section 2.1).
Thus,the following two measures can be viewed as upperand lower bounds for the expected precision of therule applications in actual systems:upper bound precision: #Entailment holds#Relevant contextlower bound precision: #Entailment holds#Left entailedwhere # denotes the number of examples withthe corresponding judgment.Finally, we consider a rule to be correct only ifits precision is at least 80%, which seems sensiblefor typical applied settings.
This yields two alterna-tive sets of correct rules, corresponding to the upperbound and lower bound precision measures.
Eventhough judges may disagree on specific examples fora rule, their judgments may still agree overall on therule?s correctness.
We therefore expect the agree-ment level on rule correctness to be higher than theagreement on individual examples.4 Experimental SettingsWe applied the instance-based methodology to eval-uate two state-of-the-art unsupervised acquisition al-gorithms, DIRT (Lin and Pantel, 2001) and TEASE(Szpektor et al, 2004), whose output is publiclyavailable.
DIRT identifies semantically related tem-plates in a local corpus using distributional sim-ilarity over the templates?
variable instantiations.TEASE acquires entailment relations from the Webfor a given input template I by identifying charac-teristic variable instantiations shared by I and othertemplates.460For the experiment we used the published DIRTand TEASE knowledge-bases1.
For every given in-put template I , each knowledge-base provides a listof learned output templates {Oj}nI1 , where nI is thenumber of output templates learned for I .
Each out-put template is suggested as holding an entailmentrelation with the input template I , but the algorithmsdo not specify the entailment direction(s).
Thus,each pair {I,Oj} induces two candidate directionalentailment rules: ?I?Oj?
and ?Oj?I?.4.1 Test Set ConstructionThe test set construction consists of three samplingsteps: selecting a set of input templates for the twoalgorithms, selecting a sample of output rules to beevaluated, and selecting a sample of sentences to bejudged for each rule.First, we randomly selected 30 transitive verbsout of the 1000 most frequent verbs in the ReutersRCV1 corpus2.
For each verb we manuallyconstructed a lexical-syntactic input template byadding subject and object variables.
For exam-ple, for the verb ?seek?
we constructed the template?X subj???
seek obj???
Y ?.Next, for each input template I we consideredthe learned templates {Oj}nI1 from each knowledge-base.
Since DIRT has a long tail of templates witha low score and very low precision, DIRT templateswhose score is below a threshold of 0.1 were filteredout3.
We then sampled 10% of the templates in eachoutput list, limiting the sample size to be between5-20 templates for each list (thus balancing betweensufficient evaluation data and judgment load).
Foreach sampled template O we evaluated both direc-tional rules, ?I?O?
and ?O?I?.
In total, we sam-pled 380 templates, inducing 760 directional rulesout of which 754 rules were unique.Last, we randomly extracted a sample of examplesentences for each rule ?L?R?
by utilizing a searchengine over the first CD of Reuters RCV1.
First, weretrieved all sentences containing all lexical termswithin L. The retrieved sentences were parsed usingthe Minipar dependency parser (Lin, 1998), keep-ing only sentences that syntactically match L (as1Available at http://aclweb.org/aclwiki/index.php?title=Te-xtual Entailment Resource Pool2http://about.reuters.com/researchandstandards/corpus/3Following advice by Patrick Pantel, DIRT?s co-author.explained in Section 3.1).
A sample of 15 match-ing sentences was randomly selected, or all match-ing sentences if less than 15 were found.
Finally,an example for judgment was generated from eachsampled sentence and its left and right phrases (seeSection 3.1).
We did not find sentences for 108rules, and thus we ended up with 646 unique rulesthat could be evaluated (with 8945 examples to bejudged).4.2 Evaluating the Test-SetTwo human judges evaluated the examples.
Werandomly split the examples between the judges.100 rules (1287 examples) were cross annotated foragreement measurement.
The judges followed theprocedure in Section 3.3 and the correctness of eachrule was assessed based on both its upper and lowerbound precision values (Section 3.4).5 Methodology Evaluation ResultsWe assessed the instance-based methodology bymeasuring the agreement level between judges.
Thejudges agreed on 75% of the 1287 shared exam-ples, corresponding to a reasonable Kappa value of0.64.
A similar kappa value of 0.65 was obtainedfor the examples that were judged as either entail-ment holds/no entailment by both judges.
Yet, ourevaluation target is to assess rules, and the Kappavalues for the final correctness judgments of theshared rules were 0.74 and 0.68 for the lower andupper bound evaluations.
These Kappa scores areregarded as ?substantial agreement?
and are substan-tially higher than published agreement scores andthose we managed to obtain using the standard rule-based approach.
As expected, the agreement onrules is higher than on examples, since judges maydisagree on a certain example but their judgementswould still yield the same rule assessment.Table 3 illustrates some disagreements that werestill exhibited within the instance-based evaluation.The primary reason for disagreements was the dif-ficulty to decide whether a context is relevant fora rule or not, resulting in some confusion between?Irrelevant context?
and ?No entailment?.
This mayexplain the lower agreement for the upper boundprecision, for which examples judged as ?Irrelevantcontext?
are ignored, while for the lower bound both461Rule Sentence Judge 1 Judge 2X sign Y ?X set Y Iraq and Turkey sign agreementto increase trade cooperationEntailment holds Irrelevant contextX worsen Y ?X slow Y News of the strike worsened thesituationIrrelevant context No entailmentX get Y ?X want Y He will get his parade on Tuesday Entailment holds No entailmentTable 3: Examples for disagreement between the two judges.judgments are conflated and represent no entailment.Our findings suggest that better ways for distin-guishing relevant contexts may be sought in futureresearch for further refinement of the instance-basedevaluation methodology.About 43% of all examples were judged as ?Leftnot entailed?.
The relatively low matching precision(57%) made us collect more examples than needed,since ?Left not entailed?
examples are ignored.
Bet-ter matching capabilities will allow collecting andjudging fewer examples, thus improving the effi-ciency of the evaluation process.6 DIRT and TEASE Evaluation ResultsDIRT TEASEP Y P YRules:Upper Bound 30.5% 33.5 28.4% 40.3Lower Bound 18.6% 20.4 17% 24.1Templates:Upper Bound 44% 22.6 38% 26.9Lower Bound 27.3% 14.1 23.6% 16.8Table 4: Average Precision (P) and Yield (Y) at therule and template levels.We evaluated the quality of the entailment rulesproduced by each algorithm using two scores: (1)micro average Precision, the percentage of correctrules out of all learned rules, and (2) average Yield,the average number of correct rules learned for eachinput template I , as extrapolated based on the sam-ple4.
Since DIRT and TEASE do not identify ruledirectionality, we also measured these scores at the4Since the rules are matched against the full corpus (as in IRevaluations), it is difficult to evaluate their true recall.template level, where an output template O is con-sidered correct if at least one of the rules ?I?O?
or?O?
I?
is correct.
The results are presented in Ta-ble 4.
The major finding is that the overall quality ofDIRT and TEASE is very similar.
Under the specificDIRT cutoff threshold chosen, DIRT exhibits some-what higher Precision while TEASE has somewhathigher Yield (recall that there is no particular naturalcutoff point for DIRT?s output).Since applications typically apply rules in a spe-cific direction, the Precision for rules reflects theirexpected performance better than the Precision fortemplates.
Obviously, future improvement in pre-cision is needed for rule learning algorithms.
Mean-while, manual filtering of the learned rules can proveeffective within limited domains, where our evalua-tion approach can be utilized for reliable filtering aswell.
The substantial yield obtained by these algo-rithms suggest that they are indeed likely to be valu-able for recall increase in semantic applications.In addition, we found that only about 15% of thecorrect templates were learned by both algorithms,which implies that the two algorithms largely com-plement each other in terms of coverage.
One ex-planation may be that DIRT is focused on the do-main of the local corpus used (news articles for thepublished DIRT knowledge-base), whereas TEASElearns from the Web, extracting rules from multipledomains.
Since Precision is comparable it may bebest to use both algorithms in tandem.We also measured whether O is a paraphrase ofI , i.e.
whether both ?I ?O?
and ?O?
I?
are cor-rect.
Only 20-25% of all correct templates were as-sessed as paraphrases.
This stresses the significanceof evaluating directional rules rather than only para-phrases.
Furthermore, it shows that in order to im-prove precision, acquisition algorithms must iden-tify rule directionality.462About 28% of all ?Left entailed?
examples wereevaluated as ?Irrelevant context?, yielding the largedifference in precision between the upper and lowerprecision bounds.
This result shows that in orderto get closer to the upper bound precision, learningalgorithms and applications need to identify the rel-evant contexts in which a rule should be applied.Last, we note that the instance-based quality as-sessment corresponds to the corpus from which theexample sentences were taken.
It is therefore best toevaluate the rules using a corpus of the same domainfrom which they were learned, or the target applica-tion domain for which the rules will be applied.7 ConclusionsAccurate learning of inference knowledge, such asentailment rules, has become critical for furtherprogress of applied semantic systems.
However,evaluation of such knowledge has been problematic,hindering further developments.
The instance-basedevaluation approach proposed in this paper obtainedacceptable agreement levels, which are substantiallyhigher than those obtained for the common rule-based approach.We also conducted the first comparison betweentwo state-of-the-art acquisition algorithms, DIRTand TEASE, using the new methodology.
We foundthat their quality is comparable but they effectivelycomplement each other in terms of rule coverage.Also, we found that most learned rules are not para-phrases but rather one-directional entailment rules,and that many of the rules are context sensitive.These findings suggest interesting directions for fu-ture research, in particular learning rule direction-ality and relevant contexts, issues that were hardlyexplored till now.
Such developments can be thenevaluated by the instance-based methodology, whichwas designed to capture these two important aspectsof entailment rules.AcknowledgementsThe authors would like to thank Ephi Sachs andIddo Greental for their evaluation.
This work waspartially supported by ISF grant 1095/05, the ISTProgramme of the European Community under thePASCAL Network of Excellence IST-2002-506778,and the ITC-irst/University of Haifa collaboration.ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The second pascal recognising textual entail-ment challenge.
In Second PASCAL Challenge Work-shop for Recognizing Textual Entailment.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of NAACL-HLT.Gennaro Chierchia and Sally McConnell-Ginet.
2000.Meaning and Grammar (2nd ed.
): an introduction tosemantics.
MIT Press, Cambridge, MA.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailment chal-lenge.
Lecture Notes in Computer Science, 3944:177?190.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proceedings of the Workshop on Evalu-ation of Parsing Systems at LREC.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProceedings of HLT-NAACL.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of ACL.Lorenza Romano, Milen Kouylekov, Idan Szpektor, IdoDagan, and Alberto Lavelli.
2006.
Investigating ageneric paraphrase-based approach for relation extrac-tion.
In Proceedings of EACL.Satoshi Sekine.
2005.
Automatic paraphrase discoverybased on context and keywords between ne pairs.
InProceedings of IWP.Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, andRalph Grishman.
2002.
Automatic paraphrase acqui-sition from news articles.
In Proceedings of HLT.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representationmodel for automatic IE pattern acquisition.
In Pro-ceedings of ACL.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisition ofentailment relations.
In Proceedings of EMNLP.463
