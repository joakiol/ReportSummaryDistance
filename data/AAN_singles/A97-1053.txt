Learning Probabil istic Subcategorization Preferenceby Identifying Case Dependencies andOptimal Noun Class General ization Level*Takehito Utsuro Yuji MatsumotoGraduate School of Information Science, Nara Institute of Science and Technology8916-5, Takayama-cho, Ikoma-shi, Nara, 630-01, JAPAN{ut  suro  ,matsu} l~ is .
a i s t -nara ,  ac .
jpAbst ractThis paper proposes a novel method of learningprobabilistic subcategorization preference.
In themethod, for the purpose of coping with the ambi-guities of case dependencies and noun class gen-eralization of argument/adjunct nouns, we intro-duce a data structure which represents a tupleof independent partial subcategorization frames.Each collocation of a verb and argument/adjunctnouns is assumed to be generated from one of thepossible tuples of independent partial subcatego-rization frames.
Parameters of subcategorizationpreference are then estimated so as to maximizethe subcategorization preference function for eachcollocation of a verb and argument/adjunct nounsin the training corpus.
We also describe the resultsof the experiments on learning probabilistic sub-categorization preference from the EDR Japanesebracketed corpus, as well as those on evaluatingthe performance of subcategorization preference.1 IntroductionIn corpus-based NLP, extraction of linguistic knowl-edge such as lexical/semantic collocation is one of themost important issues and has been intensively stud-ied in recent years.
In those research, extracted lex-ical/semantic collocation is especially useful in termsof ranking parses in syntactic analysis as well as au-tomatic construction of lexicon for NLP.For example, in the context of syntactic disam-biguation, Black (1993) and Magerman (1995) pro-posed statistical parsing models based-on decision-tree learning techniques, which incorporated notonly syntactic but also lexical/semantic informationin the decision-trees.
As lexical/semantic nforma-tion, Black (1993) used about 50 semantic ategories,while Magerman (1995) used lexicai forms of words.Collins (1996) proposed a statistical parser whichis based on probabilities of dependencies betweenhead-words in the parse tree.
In those works, lexi-cal/semantic ollocation are used for ranking parsesin syntactic analysis.
*The authors would like to thank Dr.
Hang Li of NECC&:C Research Laboratories, Dr. Kentaro Inui of TokyoInstitute of Technology, Dr. Koiti Hasida of Electrotech-nical Laboratory, Dr. Tak_ashi Miyata of Nara Institute ofScience and Technology, and also anonymous reviewers ofANLP97 for valuable comments on this work.On the other hand, in the context of automatic lex-icon construction, the emphasis is mainly on the ex-traction of lexical/semantic collocational knowledge ofspecific words rather than its use in sentence parsing.For example, Haruno (1995) applied an information-theoretic data compression technique to corpus-basedcase frame learning, and proposed a method of find-ing case frames of verbs as compressed representationof verb-noun collocational data in corpus.
The workconcentrated on the extraction of declarative repre-sentation of case frames and did not consider theirperformance in sentence parsing.This paper focuses on extracting lexical/semanticcollocational knowledge of verbs for the purpose of ap-plying it to ranking parses in syntactic analysis.
Morespecifically, we propose a novel method for learningparameters for calculating subcategorization prefer-ence functions of verbs.
In general, when learning lex-ical/semantic ollocational knowledge of verbs fromcorpus, it is necessary to cope with the following twotypes of ambiguities:1) The ambiguity of case dependencies2) The ambiguity of noun class generalization1) is caused by the fact that, only by observing eachverb-noun collocation in corpus, it is not decidablewhich cases are dependent on each other and whichcases are optional and independent of other cases.
2)is caused by the fact that, only by observing each verb-noun collocation in corpus, it is not decidable whichsuperordinate class generates each observed leaf classin the verb-noun collocation.So far, there exist several researches which workedon these two issues in learning collocational knowl-edge of verbs and also evaluated the results interms of syntactic disambiguation.
Resnik (1993)and Li and Abe (1995) studied how to find an opti-mal abstraction level of an argument noun in a tree-structured thesaurus.
Although they evaluated theobtained abstraction level of the argument noun by itsperformance in syntactic disambiguation, their worksare limited to only one argument.
Li and Abe (1996)also studied a method for learning dependencies be-tween case slots and evaluated the discovered epen-dencies in the syntactic disambiguation task.
Theyfirst obtained optimal abstraction levels of the argu-ment nouns by the method in Li and Abe (1995), andthen tried to discover dependencies between the class-based case slots.
They reported that dependencies364were discovered only at the slot-level and not at theclass-level.Compared with those previous works, this paperproposes to cope with the above two ambiguities ina uniform way.
First, we introduce a data structurewhich represents a tuple of independent partial sub-categorization frames.
Each collocation of a verb andargument/adjunct  nouns is assumed to be generatedfrom one of the possible tuples of independent par-tim subcategorization frames.
Then, parameters ofsubcategorization preference are estimated so as tomaximize the subcategorization preference functionfor each collocation of a verb and argument /ad junctnouns in the training corpus.
We describe the resultsof the experiments on learning probabilistic subcate-gorization preference from the EDR Japanese brack-eted corpus (EDR, 1995), as well as those on evaluat-ing the performance of subcategorization preference.2 Data  S t ruc ture2.1 Verb -Noun Col locat ionVerb-noun collocation is a data structure for the collo-cation of a verb and all of its argument/adjunct  nouns.A verb-noun collocation e is represented by a featurestructure which consists of the verb v and all the pairsof co-occurring case-markers p and thesaurus classesc of case-marked nouns: 1epred : vpl : clp~ : ck(1)We assume that  a thesaurus is a tree-structured typehierarchy in which each node represents a semanticclass, and each thesaurus class c l , .
.
.
,  Ck in a verb-noun collocation is a leaf class.
We also introduce __cas the superordinate-subordinate relation of classes ina thesaurus: Cl ~c c2 means that cl is subordinate toC2.2.2 Subcategor i za t ion  F rameA subcategorization frame f is represented by a featurestructure which consists of a verb v and the pairs ofcase-markers p and sense restriction c of case-markedargument/adjunct  nouns:pred : vpl : C1/ =Pt : elSense restriction Cl, .
.
.
,ct  of case-marked argu-ment/adjunct  nouns are represented by classes at ar-bitrary levels of the thesaurus.
A subcategorizationframe f can be divided into two parts: one is theverbal part fv containing the verb v while the otheris the nominal part fp containing all the pairs of1Although we ignore sense ambiguities of case-markednouns in this definition, in section 5.2, we briefly mentionhow we deM with sense ambiguities of case-marked nounsin the current implementation.case-markers p and sense restriction c of case-markednouns.f=fvAfp - - - - \ [p red :v \ ]A \ [  plpl ctCl \]2.3 Subsumpt ion  Relat ionWe introduce subsumption relation -~ y of a verb-nouncollocation e and a subcategorization frame f:e ___i f iff.
for each case-marker Pi in f andits noun class cii, there exists thesame case-marker pi in e and itsnoun class cie is subordinate toelf, i.e.
cle ~c clyThe subsumption relation ___y is applicable also as asubsumption relation of two subcategorization frames.3 A Model of Generating Verb-NounCollocationIn this section, we introduce a model of generat-ing a verb-noun collocation from subcategorizationframe(s).
In order to cope with the ambiguities ofcase dependencies and noun class generalization inthis model, we introduce a data structure which repre-sents a tuple of independent partial subcategorizationframes.3.1 Generat ing a Verb-Noun Col locat ionfrom Independent  PartialSubcategor izat ion FramesFirst, we describe the idea of generating a verb-nouncollocation from a subcategorization frame, or a tupleof partial subcategorization frames.Generat ion from a Subcategor izat ion FrameSuppose a verb-noun collocation e is given as:pred : vpl : CleePk : CkeThen, let us consider a subcategorization frame fwhich can generate .
We assume that  f has exactlythe same case-markers as e has, 2 and each semanticclass ci.f of a case-marked noun of f is superordinateto the corresponding leaf semantic lass eie of e:pred : vpl ::clf/ = , cie _~c ciy ( i= l , .
.
.
, k )  (2)L pk : Ckf JThen, we denote the generation of the verb-nouncollocation e from the subcategorization frame f as:f ~ eNext, we describe the idea of generating a verb-nouncollocation from a tuple of partial subcategorizationframes which are independent of each other.2Since we do not consider ellipsis of argument nounswhen generating a verb-noun collocation from a subcate-gorization frame, the subcategorization frame f is requiredto have exactly the same case-markers as e.365Par t ia l  Subcategor i za t ion  F rameFirst, we define a partial subcategorization frame fiof f as a subcategorization frame which has the sameverb v as f as well as some of the case-markers of f andtheir semantic lasses.
Then, we can find a divisionof f into a tuple (fl, - .
- ,  f~) of partial subcategoriza-tion frames of f ,  where any pair fi and fi' (i ~?
i')do not have common case-markers and the unificationf~ A.. ?
A f~ of all the partial subcategorization framesequals to f:f = f zA .
.
.A f .
(3)pred : vVjYj' pij # Pi,j,fl---- pij:cij ' ( i , i '= l , .
.
.
,n ,  iC i ' )  (4)Independence of Partial Subcategorizat ionFramesWe allow the division of f into a tuple (f\], .
.
.
,  fn)of partial subcategorization frames as in the equation(3) only when the partial subcategorization frames f l ,?
.
.
,  fn can be regarded as events occurring indepen-dently of each other.
With some corpus, usually wecan estimate the conditional probabilities p( f  I v) andp(\], I v) of the (partial) subcategorization frames \]and fi (i = 1 , .
.
.
,n )  given the verb v. Accordingto the estimated probabilities, we can judge whetherf l , .
.
.
,  fn are independent of each other as follows.First, we estimate the conditional probability p( f  Iv) of a (partial) subcategorization frame f by sum-ming up the conditional probabilities p(e \[ v) of allthe verb-noun collocations e given the verb v, wheree is subsumed by f (e _ / f )3p( f lv )  ~ ~p(c l~)  (5)e~_ffThe conditional joint probabilityp(fl,..., f ,  I v) isalso estimated by summing up p(e I v) where e issubsumed by all of f l , .
.
.
,  fn (e _y f l , .
.
.
,  f , ) :p(fl .
.
.
.  '
f"  Iv) ~" E p(e l v) (6)e~_ffl ..... fnThen, we give a formal definition of independenceof partial subcategorization frames according to theestimated conditional probabilities:partial subcategorization frames f l , -  - ' ,  fn are in-dependent if, any pair fi and f j  (i # j)  do nothave common case-markers, and for every sub-set f i~, .
.
.
,  fir of j of these partial subcategoriza-tion frames (j = 2 , .
.
.
,  n), the following equationholds:P( f i l , ' ' ' , f i  r I v ) = P( f i l  l v) " ' 'p ( f i  r I v) (7)Since it is too strict to judge the independence ofpartial subcategorization frames by the equation (7),3The probability p(e I v) can be estimated asfreq(e)/freq(v) by M.L.E.
(maximum likelihood estima-tion) directly from the training corpus.we relax the constraint of independence using a re-laxation parameter c~ (0 < a < 1).
Partial subcatego-rization frames f l , .
.
.
,  fn are judged as independentif, for every subset f i l , - - .
,  fit of j of these partialsubcategorization frames (j = 2 , .
.
.
,  n), the followinginequalities hold:< P( / ' l , - .
.
, f ' r  Iv) < 1 (s)- p(l,1 Iv) - -P(Y~J  Iv) -Generat ion  f rom Independent  Par t ia lSubcategor i za t ion  FramesNow, as in the case of the generation from a sub-categorization frame f ,  we denote the generation of efrom a tuple (fl, .
.
.
,  fn) of independent partial sub-categorization frames of f as below:( f l , .
.
, f , )  , e3.2 The  Ambigu i ty  of  Case DependenciesThis section describes the problem of the ambiguityof case dependencies when observing verb-noun collo-cation in corpus.
This problem is caused by the factthat, only by observing each verb-noun collocation incorpus, it is not decidable which cases are dependenton each other and which cases are optional and inde-pendent of other cases.For example, consider the following example:Example 1Kodomo-ga kouen-de juusu-wo nomu.child-NOM park-at juice-ACC drink(A child drinks juice at the park.
)The verb-noun collocation is represented as a featurestructure below:e = ga : Ccwo : cjde : %In this feature structure e, co, cp, and cj repre-sent the leaf classes (in the thesaurus) of the nouns"kodomo(child)", "kouen(park)", and "juusu(juice)".Next, we assume that the concepts "human", "place", and "beverage" are superordinate o "kodomo(child)", "kouen(park)", and '~uusu(juice)", respectively,and introduce the corresponding classes Chum, Cplc,and Cbe~.
Then, the following superordinate-subordinate relations hold:Cc "~c Churn~ Cp "~c Cplc~ Cj "~c CbevAllowing these superordinate classes as sense restric-tion in subcategorization frames, let us consider theseveral patterns of subcategorization frames which cangenerate the verb-noun collocation e. Those patternsof subcategorization frames vary according to the de-pendencies of cases within them.If the three cases "ga(NOM)", "wo(ACC)", and"de(at)" are dependent on each other and it is notpossible to find any division into a tuple of several in-dependent partial subcategorization frames, e can beregarded as generated from a subcategorization framecontaining all of the three cases:ga : ch,m ~ e (9)WO : Obeyde : epic366Otherwise, if only the two cases "ga(NOM)" and"wo(ACC)" are dependent on each other and the"de(at)" case is independent of those two cases, e canbe regarded as generated from the following tuple ofindependent partial subcategorization frames:ga : Chum ~ de : CplcWO : Cbev?
(10)Otherwise, if all the three cases "ga(NOM)","wo(ACC)", and "de(at)" are independent of eachother, e can be regarded as generated from the fol-lowing tuple of independent partial subcategorizationframes, each of which contains only one case:ga : Churn ' L wo : Cbev ~ de  : Cplce(11)3.3 The  Ambigu i ty  of Noun ClassGenera l i zat ionThis section describes the problem of the ambiguityof noun class generalization when observing verb-nouncollocation in corpus.
This problem is caused by thefact that, only by observing each verb-noun colloca-tion in corpus, it is not decidable which superordinateclass generates each observed leaf class in the verb-noun collocation.For example, let us again consider Example 1.We assume that the concepts "animal" and "liquid"are superordinate to "human" and "beverage", re-spectively, and introduce the corresponding classesCani and ctiq.
Then, the following superordinate-subordinate relations hold:Chum "~c Cani~ Cbtv -~c CliqIf we additionally allow these superordinate classesas sense restriction in subcategorization frames, wecan consider several additional patterns of subcate-gorization frames which can generate the verb-nouncollocation e, along with those patterns described inthe previous section.Suppose that only the two cases "ga(NOM)" and"wo(ACC)" are dependent on each other and the"de(aQ" case is independent of those two cases as inthe formula (10).
Since the leaf class Cc ("child") canbe generated from either Chum or Cani, and also theleaf class cj ("juice") can be generated from eitherCbe v or  e l iq ,  e can  be regarded as generated accordingto either of the four formulas (10) and (12),~(14):ga:ca~i ~ e (12)WO : Cbe vga:ch,m ~ e (13)WO : Cliqga:c~.i , e (14)WO : Cliq3.4 A Mode l  of  Generat ing  Verb -NounCo l locat ionWhen observing each verb-noun collocation e, as wedescribed in the previous two sections, the ambiguitiesof case dependencies and noun class generalization re-main, and it is necessary to consider every possibletuple of independent partial subcategorization frameswhich can generate the observed verb-noun collocatione.
In order to cope with these ambiguities, we intro-duce two sets: one is a set F of tuples ~f l , .
.
.
, fn>of independent partial subcategorization irames andthe other is a set E of verb-noun collocations e. Thegeneration of a verb-noun collocation from a tuple ofindependent partial subcategorization frames can beregarded as a mapping ~r from F to E:r : F ~ E (15)Usually, for each given verb-noun collocation inE, there exist several possible tuples of independentpartial subcategorization frames in F. Thus, lr is amany-to-one mapping.
The mapping from a tuple( f \ ] , - .
- ,  fn) of independent partial subcategorizationframes to a verb-noun collocation e can be denotedalso as follows:( f l , .
.
.
, f , )  ~ e (16)When observing a verb-noun collocation e, we as-sume this many-to-one mapping Ir and consider everypossible tuple of independent partial subcategoriza-tion frames which can generate , according to theambiguities of case dependencies and noun class gen-eralization.3.5 Parameters  of  Generat ing  Verb -NounCo l locat ionBefore we give definitions of subcategorization prefer-ence functions in the next section, we introduce theparameter q(fk \] v) of generating verb-noun colloca-tion, which is used in the calculation of the subcate-gorization preference.
The parameter q(fk \] v) can beregarded as the conditional probability of the partialsubcategorization frame fk and could be estimated inthe similar way as the p(f \[ v) in the formula (5).However, it is the parameter of generating verb-nouncollocation and have to be estimated so as to maxi-mize the subcategorization preference function for thetraining corpus.One solution of this parameter estimation processmight be to regard the model of generating verb-nouncollocation as a probabilistic model and then to applythe maximum likelihood estimation method.
Whenestimating the parameters from the training sample,we have to note that each verb-noun collocation isambiguous ince it could be interpreted in several dif-ferent ways according to case dependencies and opti-mal noun class generalization levels.
As for param-eter estimation of probabilistic models from ambigu-ous training sample, EM algorithm(Baum, 1972) is awell-known solution and has been studied for years.In EM algorithm, parameters are assigned to events,and it is required that parameters sum up to 1.
How-ever, since two subcategorization frames could havethe same case and a subsumption relation could hold367between their sense restrictions, they may have over-lap and the requirement that parameters sum up to1 is not satisfiable.
Therefore, it is not so straightfor-ward to apply EM algorithm to the task of parameterestimation of generating verb-noun collocation.Instead of introducing a probabilistic model of gen-erating verb-noun collocation 4, in this paper, we em-ploy more general framework which is applicable tovarious measures of subcategorization preference in-cluding the probability of generating verb-noun collo-cation.
In the framework, the process of parameterestimation is regarded as a general optimization prob-lem of maximizing the subcategorization preferencefunction for the training corpus.In order to describe the framework, first we intro-duce the probability P ( ( f l , .
.
.
,  fn)j " ' *  el \[ el) of gen-erating a verb-noun collocation ei in the set E from atuple ( fx , .
.
.
,  f~)j in the set F, given ei, and denoteit as a conditional probability P ( ( f l , .
- .
, fn ) j  \[ ei).Then, for each ei in E, we can consider a probabilitydistribution P ( ( f l , .
.
.
,  f , ) j  \[ ei) over the set F of tu-pies of independent partial subcategorization frames:Eel " " " e l( f l , .
.
.
, fn') l  "'"F p((fl, ..-, fn)i led(fl .
.
.
.
.
fn" )m "'"Each probability distribution P( ( f l , .
.
.
,  fn)j I ei) sat-isfies the following axiom of the probability:Y2v( f f l , - .
.
, f ,b  I ei) = 1 for all i?
J .
.
.
.
.
According to the probabxhty distribution P(( f l , .
.
.
,fn)j  \[ ei) of generating ei from (A, , fn)-, we esti- ?
:" .
$mate the frequency of the subcategonzatlon frame fkand then estimate the parameter q(fk I v) as below:E 1 ?
p((fl .
.
.
.
, f k , .
.
.
, f , ) i l e i )freq(fk) i,j q(fk Iv) ~ - -  freq(v) freq(v)(17)When learning probabilistic subcategorization pref-erence (section 5), we estimate the probability distri-bution P ( ( f l , .
.
.
,  fn)j \[ ei) for each ei so as to maxi-mize the subcategorization preference function for el.4 Subcategorization PreferenceFunctionsThis section introduces a function ?
which measuresthe subcategorization preference when generating averb-noun collocation e from a tuple ( f l , .
.
- , f~)  ofindependent partial subcategorization frames:?
( ( /a  .
.
.
.
.
f . )
---* e) (18)In this paper, we introduce a subcategorization pref-erence function which is based-on the idea of KullbackLeibler distance.
54Another alternative of solving the problem of learn-ing probabilistic subcategorization preference based-on aprobabilistic model is to .regard the problem as the con-struction of probabilistic models from the training sample.We will discuss this issue in section 7.5In Utsuro and Matsumoto (1997), we defined anothersubcategorization preference function ev which is based-4.1 Nomina l  Par ts  of  (Par t ia l )Subcategor i za t ion  F ramesFirst, let fp, fp l , ' " , fpn  be the nominal parts of(partial) subcategorization fl'ames f, f l , - .
.
,  fn in theequations (2) and (4), respectively:fp = -Pk : Ck$fP = fPl A"" Afp.fPi = pij : cij VjVj' PO # Pc j, ' (i, i '= 1 .
.
.
.
,n, i# i ' )As in the case of the parameters q(fi \[ v) of figiven the verb v, we estimate the probability P(fpi)of the nominal part fpi in the whole corpus and call itthe parameter q(fpi) of fpi in the whole training cor-pus.
We estimate the frequency of fpi throughout thewhole training corpus and then estimate the parame-ter q(hi)  of fpl as below:E freq(fk)q( f ,p  ~ N~ 1. p(ffl .... ,h , .
.
.
,S ,b  I ~,)i,j (19) N4.2 ?kt: Ku l lback  Le ib ler  D is tanceRather than the simple conditional probability, thispreference function is intended to measure theinformation-theoretic association of the verb v and thenominal part of the subcategorization frame.The Kullback Leibler (KL) distance is a measureof the distance between two probability distribution.Given a random variable X and two probability dis-tributions p(X) and q(X), the KL distance D(p\[\[q) ofp(X) and q(X) is defined as below(Cover and Thomas,1991), where each term can be regarded as the dis-tance of two probabilities p(z) and q(x) of an event x:D(Pllq) Y2 p(x)" p(x) = log qxEXIn order to apply the idea of the KL distance to mea-suring the association of the verb v and the nominalpart fp of f ,  we introduce a random variable Fp  whichtakes fp as its value?
We also introduce the probabilitydistribution p(Fp)  of Fp  and the conditional proba-bility distribution p(Fp \[ v) of Fp  given the verb v.Then, the KL distance of p(Fp \[ v) and p(Fp)  isdenoted as D(p(Fp \[ v)Hp(Fp) and each term of itcan be regarded as the distance of two probabilitiesp(fp \] v) and p(fp).
We assume that the larger thisdistance is, the stronger the association of fp and vis, and measure the association of fp and v with thison the probability of generating the verb-noun collocationand described experimental results of applying ep to thetask of learning probabilistic subcategorization.368distance of the two probabilities P(fv \[ t') and P(fv)"With this idea.
the subcategorization preference func-tion okt is now formally defined as below: 6 7oktf(fl .
.
.
.
.
f . )
- ' *  e)= P(fp I t,)log p(fp Jr) (20)P( fv)n1-I P(fpi l V) n,~ r ip(fpi  \[ t,) ?
log i=1, (21)~=1 H p(fvl )i=1n1-Iq(fpi I t') n"~ 1-I q(fP' t v) ?
log i=1~ (22)i=1 YI  q(fPi)i=1(21) is derived from the independence of the partialsubcategorization frames f l , .
.
.
,  fn.
In (22), we usethe parameters q(fvi I v) and q(fvi) as an approxima-tion of the probabilities P(fpi I r) and P(fpi)"5 Learning ProbabilisticSubcategorization PreferenceThe problem of learning subcategorization preferencecan be formalized as an optimization problem of es-timating the probability distribution P ( ( f l , .
.
.
,  fn)j \[el) (in section 3.5) of generating ei from ( fx , .
.
.
,  fn)j(and then the parameters q(fpk Iv) and q(fPk)) so asto maximize the value of the subcategorization pref-erence function for the whole training corpus.
Inthis paper, we give only an approximate solution tothis problem: we estimate the probability distribu-tion P ( ( f l , .
- .
,  fn)j \[ el) for each ei so as to maximizethe value of the subcategorization preference functiononly for el, not for the whole training corpus.5.1 P rob lem Set t ingLet the training corpus C be the set of verb-noun col-location e. We define the subcategorization preference?
(e) of a verb-noun collocation e as the maximum ofthe subcategorization preference function ?
(the for-mula (18)) of generating e from a tuple ( fa , .
.
.
,  fn).
(~(e) = max ?
((fl .
.
.
.
,f~) ~ e) (23)(11 ..... Y~)Now, the problem of learning probabilistic subcate-gorization preference is stated as:for every verb-noun collocation e in C, es-timating the probability distribution P((fl,6Resnik (1993) applys the idea of the KL distance tomeasuring the association of a verb v and its object nounclass c. Our definition of ekt corresponds toan extension ofResnik's association score, which considers dependencies ofmore than one case-markers in a subcategorization frame.7Another related measure is Dunning (1993)'s likeli-hood ratio tests for binomial and multinomial distribu-tions, which are claimed to be effective ven with verymuch smaller volumes of text than is necessary for othertests based on assumed normal distributions.. .
.
.
f~)j I e) of generating e from (fl.. .
.
.
f~)j. under the constraint that the valueof the subcategorization preference o(e) ismaximized.5.2 Learn ing  A lgor i thmFirst.
we identi~, independent partial subcategoriza-tion frames according to the condition of (8).
Then,let E(t') be the set of verb-noun collocations contain-ing the verb v in the training corpus ~.
Let F(e) be theset of tuples (fl .
.
.
.
.
fn) of independent partial sub-categorization frames which can generate and satisfythe independence condition of (8).
8F(e) = {( f , , .
.
.
, f . )
(fl .
.
.
.
.
f , ) - -e}  (24)F(e) contains a tuple (f)  consisting of only onesubcategorization frame f only if f can not be di-vided into several independent partial subcategoriza-tion frames.Then, we assume that each element of F(e) occursevenly and estimate the initial conditional probabilitydistribution P ( ( f l , .
.
.
,  f , ) j  I e) of generating e from( f l , .
.
.
,  fn)j as an approximation below:1P((fl,... ,fn)j \[e) ,~ IF(e)l (25)5.2.1 Approx imate  Es t imat ion  o fVerb - Independent  ParametersUsing the initial conditional probability distributionof P ( ( f i , .
.
.
,  fn)j I e) as in the formula (25), the ini-tial values of the verb-independent parameters q(fPk)are estimated by the formulas (19).
In the current im-plementation of the learning algorithm, we use theseinitial values as approximate estimation of those verb-independent parameters and probabilities throughoutthe learning process.5.2.2 I te ra t ive  Reest imat ion  o fVerb -Dependent  ParametersVerb-dependent parameters q(fk I v)(= q(fpk \[ v))are iteratively estimated so as to maximize the sub-categorization preference ?
(e) for every verb-noun col-location e in the training corpus C. As a learning al-gorithm, we employ the following stingy algorithm:1.
In i t ia l i za t ionAs with the case of the verb-independent param-eters, for each verb-noun collocatoin e in C, the setF(e) is initially constructed according to the defini-tion in (24).
Then, the initial conditional probabilitydistribution of P(( f l , .
.
.
,  fn)j \[e) and the initial val-ues of the verb-dependent parameters q(fk \[ v) areestimated as (25) and (17), respectively.Sin the current implementation, wedeal with sense am-biguities of case-marked nouns and case ambiguities ofJapanese topic-marking post-positional particles such as"ha(TOPIC)", "too(ALSO)", and "dake(ONLY)".
Whenconstructing the set F(e), we consider all the possible com-bination of senses of semantically ambiguous nouns andcases of topic-marking post-positional particles.
Theseambiguities can be resolved by maximizing the subcate-gorization preference function (section 5.2.2).369Table 1: The Result of Learning Probabilistic Sub-categorization Preference for "kau(buy, incur)" (?kl,a=0.9)I II .
.
.
.
,sp./}/Eg) I '~,,, leg s. I1 \[wo(AOO):14(Products)\] 1.88 1582 \[wo(ACC):13721-8(kabu(s~ock))\] 0.27 153 \[ga(NOM):12(Human)\] 0.27 404 \[wo(ACC):lh(Nature)\] 0.21 255 \[kara(from):12(Human)\] 0.19 146 \[de(at): 12(Shop,Place)\] 0.17 187 \[ga(NOM):12(Human), 0.16 6wo( ACC):13721-8( kabu(stock))\]8 \[wo(ACC):13OlO(hukyou(disgust))\] 0.12 69 \[wo(ACC): 11961-1(Currency)\] 0.10 610 \[ga(NOM):12(Human),wo(ACC): 0.09 41456(Musical Instruments)\](llth,~lh0th) - -  1962.
Iterative ReestimationThe subcategorization preference ?
(e) are maxi-mized by repeatedly searching the set F(e)  for tuples( f l , .
.
.
,  fn) which give the maximum subcategoriza-tion preference and removing other tuples from F(e).The following two steps are repeated until the valuesof the parameters q(fk I v) converge.
(2a) For each verb-noun collocatoin e in ?, set -~(e)as the set of tuples ( f l , .
.
.
,  fn) of independentpartial subcategorization frames which can gen-erate e and give the maximum subcategorizationpreference in the equation (23).
(2b) Set the values of the conditional probabilitiesP( ( f l , .
.
.
,  fn) j  \[ e) as below and the parametersq(fk Iv) as (17), respectively: 1P((fl,...,f,~)j I~) ' -  IF(e)l6 Experiments and Evaluation6.1 Corpus  and  ThesaurusAs the training and test corpus, we used the EDRJapanese bracketed corpus (EDR, 1995), which con-thins about 210,000 sentences collected from newspa-per and magazine articles.
From the EDR corpus,we extracted 153,014 verb-noun collocations of 835verbs which appear more than 50 times in the cor-pus.
These verb-noun collocations contain about 270case-markers.
We constructed the training set C fromthese 153,014 verb-noun collocations.We used 'Bunrui Goi Hyou'(BGH) (NLRI, 1993)as the Japanese thesaurus.
BGH has a six-layeredabstraction hierarchy and more than 60,000 words areassigned at the leaves and its nominal part containsabout 45,000 words.
Five classes are allocated at thenext level from the root node.6.2 Experiments and Resu l tsFrom the training set C, we first estimated the valuesof verb-independent parameters as in section 5.2.1,and then iteratively reestimated verb-dependent pa-rameters of the subcategorization preference functionOkl for 10 verbs as in section 5.2.2.
For each of the10 verbs, the numbers of verb-noun collocations are100 ~ 500.
We made experiments with the indepen-dence parameter a = 0.5/0.7/0.9.
In the iterative rees-timation procedure, the values of the verb-dependentparameters converged after 2 -~ 5 iterations.For the 10 verbs, about 75% of the verb-noun collo-cations have only one case-marked noun.
The ratethat tuples of partial subcategorization frames arejudged as independent increases as the value of theindependence parameter a decreases.
This rate in-creases from 1.4% (a=0.9)  to 12.1% (a=0.5) .As an example, for the verb "kau(buy, incur)", Ta-ble 1 shows the set _~(e) of tuples of independentpartial subcategorization frames which give maximumsubcategorization preference.
The table lists the setsF(e) with 10 highest preference values of Ckl, alongwith the numbers (the column 'Egs.')
of verb-nouncollocations for each F(e), which are judged as gen-erated from it 9.
Since about 75% of the verb-nouncollocations have only one case-marked noun, mostof the 10 high-scored sets have only one case-markednoun.
However, the 10 high-scored sets cover about60% of the verb-noun collocations in the training set,and they can be regarded as typical subcategorizationframes of the verb "kau(buy, incur)".6.3 Evaluation of SubcategorizationPre ferenceWe evaluate the performance of the estimated param-eters of the subcategorization preference as follows.Suppose that the following word sequence repre-sents a verb-final Japanese sentence with a subordi-nate clause, where Nx , .
.
.
,  N2k are nouns, p~, .
.
.
,P2kare case-marking post-positional particles, Vl, v2 areverbs, and the first verb vl is the head verb of thesubordinate clause.N~-p=- N11-p11 .. .
.
.
N11-pl t-vl- N21-p21 .
.
.
.
.
N2k-p2k-V2We consider the subcategorization ambiguity of thepost-positional phrase N=-p,: i.e, whether N, -p ,  issubcategorized by Vl or v2.We use held-out verb-noun collocations of the verbsvl and v2 which are not used in the training.
Theyare like those verb-noun collocations %1 and %2 inthe left side below.
Next, we generate rroneous verb-noun collocations e~l of vl and %2 of v2 as those in theright side below, by choosing a case element p,  : N ,  atrandom and moving it from vl to v2.pred : vl wed : vlp l l  :N l l  pll : Nllee l  =ecl =p l l  : N i l  ~ p i t  : N i lp~ : N= pred : v2pred : v2 p21 : N21P21 : N21 ee2 = "ec2 =p2k : N2kt)21 : N2k Px : Nx9In each subcategorization frame, .Japanese nounclasses of BGH thesaurus are represented as numericalcodes, in which each digit denotes the choice of the branchin the thesaurus.370Table 2:with tkl (%)Accuracies of Subcategorization PreferenceIndependent Anyc~=0.5 a=O.9 ~=0.5 (~=0.9Optimal + 81.7 70.7 65.8 68.6- 2.2 3.3 27.1 6.0Initial + 16.1 25.6 7.1 25.0- 0 O.4 0 0.4I Accuracy 97.8 96.3 72.9 93.6 tApplicability 83.9 74.0 92.9 74.6Then, we compare the sum ?
(ecl) + q~(ec2) of themaximums (in the definition (23)) of ?kl for the cor-rect pair with the sum ?
(eel )+ ?
(ee2) of those for theerroneous pair, and calculate the rate that the correctpair has the greater value.For the purpose of evaluating the effectivenessof factors of learning probabilistic subcategorizationpreference, we perform experiments with different set-tings and compare their results.
The following twooptions are examined:* Whether the subcategorization preference func-tion uses tuples of partial subcategorizationframes judged as independent ("Independent"), orany tuples ("Any").
* The independence parameter c~=0.5/0.9.For three Japanese verbs "kau (buy, incur)", "nomu(drink)", and "kasaneru (pile up, repeat)", we ex-tracted pairs of correct verb-noun collocations andevaluated the performance of subcategorization pref-erence.
Table 2 gives the results averaged over ex-tracted pairs, including the accuracies of subcat-egorization preference.
The difference of "Opti-mal' /" Init ial" means that initial values of the pa-rameters are used instead of optimized values (section5.2.2) when the subcategorization preference functionis not applicable to the given verb-noun collocationand returns zero.
The line "Accuracy" lists the sumsof both "Optimal" and "Initial" accuracies, while theline "Applicability" lists the percentages of positivevalues of the subcategorization preference functionwith optimized parameters.It is natural that the settings with more weak con-ditions on the independence judgment of partial sub-categorization frames result in higher applicabilities.The setting with independent tuples of partial subcat-egorization frames achieves higher accuracy than thatwith any tuples, and this result claims that the resultof the independence judgment is effective when apply-ing the estimated parameters to the task of subcate-gorization preference.
Even in the case of the settingwith any tuples, the setting with c~=0.5 gives pooreraccuracy than that of ce = 0.9.
In this case, the differ-ence of the independence parameter ~ affects only theparameter estimation stage.
This result claims thatthe independence judgment process is effective alsowhen estimating parameters from the training corpus.7 ConclusionThis paper proposed a novel method of learningprobabilistic subcategorization preference of verbs.We described a part of the results of the exper-iments on learning probabilistic subcategorizationpreference from the EDR Japanese bracketed cor-pus, as well as those on evaluating the performanceof subcategorization preference.
Although the scaleof the evaluation experiment was relatively small,we achieved accuracies higher than 96%.
The de-tails of the experimental results are available inUtsuro and Matsumoto (1997).
As we mentioned insection 3.5, probabilistic model construction methodsmight be also applicable to the task of learning prob-abilistic subcategorization preference.
We have al-ready applied the maximum entropy methods(Pietra,Pietra, and Lafferty, 1995; Berger, Pietra, and Pietra,1996) to this task(Utsuro, Miyata, and Matsumoto,1997) and are also planning to evaluate the effective-ness of the MDL principle(Rissanen, 1989) when com-bining with the maximum entropy method.
Their re-sults will be compared with those of the method pro-posed in this paper and reported in the near future.Re ferencesBaum, L. E. 1972.
An inequality and associated maximizationtechnique in statistical estimation for probabilistic functions ofmarkov processes.
Inequalities, 3:1-8.Better, A. L., S. A. D. Pietra, and V. J. D. Pietra.
1996.
A maxi-mum entropy approach to natural anguage processing.
Compu-tational Linguistics, 22(1):39-71.Black, E. 1993.
Towards history-based grammars: Using richermodels for probabilistic parsing.
In Proceedings of the 31st An-nual Meeting of ACL, pages 31-37.Collins, M. 1996.
A new statistical parser based on bigram lexicaldependencies.
In Proceedings of the 3,~th Annual Meeting ofACL, pages 184-191.Cover, T. M. and J.
A. Thomas.
1991.
Elements of InformationTheory.
John Wiley and Sons, Inc.Dunning, T. 1993.
Accurate methods for the statistics of surpriseand coincidence.
Computational Linguistics, 19(1):61-74.EDR, (Japan Electronic Dictionary Research Institute, Ltd), 1995.EDR Electronic Dictionary Technical Guide.Haruno, M. 1995.
Verbal case frame acquisition as data compres-sion.
In Proceedings of the 5th International Workshop on Nat-ural Language Understanding and Logic Programming.Li, H. and N. Abe.
1995.
Generalizing case frames using a the-saurus and the MDL principle.
In Proceedings of InternationalConference on Recent Advances in Natural Language Process-ing, pages 239-248.Li, H. and N. Abe.
1996.
Learning dependencies between caseframe slots.
In Proceedings of the 16th COLING, pages 10-15.Magerman, D. M. 1995.
Statistical decision-tree models for pars-ing.
In Proceedings of the 3Jrd Annual Meeting of ACL, pages276-283.NLRI, (National Language Research Institute), 1993.
Word Listby Semantic Principles.
Syuei Syuppan.
(in Japanese).Pietra, S. D., V. D. Pietra, and J. Lafferty.
1995.
Inducing fea-tures of random fields.
CMU Technical Report CMU-CS-95-144,School of Computer Science, Carnegie Mellon University.Resnik, P. 1993.
Semantic lasses and syntactic ambiguity.
In Pro-ceedings of the Human Language Technology Workshop, pages278-283.Rissanen, J.
1989.
Stochastic Complexity in Statistical Inquiry,volume 15 of Series in Computer Science.
World Scientific Pub-lishing Company.Utsuro, T. and Y. Matsumoto.
1997.
Learning proba-bilistic subcategorization preference and its application tosyntactic disambiguation.
Information Science TechnicalReport NAIST-IS-TR97006, Nara Institute of Science andTechnology.
(htt p://www.aist-nara.ac.j p/IS/TechRepor t /  re-port-gz/97006.ps.gz).Utsuro, T., T. Miyata, and Y. Matsumoto.
1997.
Maximum en-tropy parameter learning of subcategorization preference.
(sub-mitted to the 35th Annual Meeting of ACL).371
