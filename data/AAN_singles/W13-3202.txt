Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 11?19,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsLearning from errors: Using vector-based compositional semantics forparse rerankingPhong Le, Willem Zuidema, Remko SchaInstitute for Logic, Language, and ComputationUniversity of Amsterdam, the Netherlands{p.le,zuidema,scha}@uva.nlAbstractIn this paper, we address the problem ofhow to use semantics to improve syntac-tic parsing, by using a hybrid rerankingmethod: a k-best list generated by a sym-bolic parser is reranked based on parse-correctness scores given by a composi-tional, connectionist classifier.
This classi-fier uses a recursive neural network to con-struct vector representations for phrases ina candidate parse tree in order to classifyit as syntactically correct or not.
Tested onthe WSJ23, our method achieved a statisti-cally significant improvement of 0.20% onF-score (2% error reduction) and 0.95% onexact match, compared with the state-of-the-art Berkeley parser.
This result showsthat vector-based compositional semanticscan be usefully applied in syntactic pars-ing, and demonstrates the benefits of com-bining the symbolic and connectionist ap-proaches.1 IntroductionFollowing the idea of compositionality in formalsemantics, compositionality in vector-based se-mantics is also based on the principle of composi-tionality, which says that ?The meaning of a wholeis a function of the meanings of the parts and ofthe way they are syntactically combined?
(Partee,1995).
According to this principle, composing themeaning of a phrase or sentence requires a syntac-tic parse tree, which is, in most current systems,given by a statistical parser.
This parser, in turn, istrained on syntactically annotated corpora.However, there are good reasons to also con-sider information flowing in the opposite direc-tion: from semantics to syntactic parsing.
Per-formance of parsers trained and evaluated on thePenn WSJ treebank has reached a plateau, as manyambiguities cannot be resolved by syntactic infor-mation alone.
Further improvements in parsingmay depend on the use of additional sources of in-formation, including semantics.
In this paper, westudy the use of semantics for syntactic parsing.The currently dominant approach to syntacticparsing is based on extracting symbolic grammarsfrom a treebank and defining appropriate proba-bility distributions over the parse trees that theylicense (Charniak, 2000; Collins, 2003; Kleinand Manning, 2003; Petrov et al 2006; Bod etal., 2003; Sangati and Zuidema, 2011; van Cra-nenburgh et al 2011).
An alternative approach,with promising recent developments (Socher etal., 2010; Collobert, 2011), is based on us-ing neural networks.
In the present paper, wecombine the ?symbolic?
and ?connectionist?
ap-proaches through reranking: a symbolic parseris used to generate a k-best list which is thenreranked based on parse-correctness scores givenby a connectionist compositional-semantics-basedclassifier.The idea of reranking is motivated by anal-yses of the results of state-of-the-art symbolicparsers such as the Brown and Berkeley parsers,which have shown that there is still considerableroom for improvement: oracle results on 50-bestlists display a dramatic improvement in accuracy(96.08% vs. 90.12% on F-score and 65.56% vs.37.22% on exact match with the Berkeley parser).This suggests that parsers that rely on syntacticcorpus-statistics, though not sufficient by them-selves, may very well serve as a basis for sys-tems that integrate other sources of information bymeans of reranking.One important complementary source of infor-mation is the semantic plausibility of the con-stituents of the syntactically viable parses.
The ex-ploitation of that kind of information is the topicof the research we report here.
In this work,we follow up on a proposal by Mark Steedman11(1999), who suggested that the realm of seman-tics lacks the clearcut hierarchical structures thatcharacterise syntax, and that semantic informationmay therefore be profitably treated by the clas-sificatory mechanisms of neural nets?while thetreatment of syntactic structures is best left to sym-bolic parsers.
We thus developed a hybrid system,which parses its input sentences on the basis of asymbolic probabilistic grammar, and reranks thecandidate parses based on scores given by a neuralnetwork.Our work is inspired by the work of Socher andcolleagues (2010; 2011).
They proposed a parserusing a recursive neural network (RNN) for en-coding parse trees, representing phrases in a vec-tor space, and scoring them.
Their experimentalresult (only 1.92% lower than the Stanford parseron unlabelled bracket F-score for sentences up to alength of 15 words) shows that an RNN is expres-sive enough for syntactic parsing.
Additionally,their qualitative analysis indicates that the learntphrase features capture some aspects of phrasal se-mantics, which could be useful to resolve semanticambiguity that syntactical information alone cannot.
Our work in this paper differs from their workin that we replace the parsing task by a rerankingtask, and thus reduce the object space significantlyto a set of parses generated by a symbolic parserrather than the space of all parse trees.
As a result,we can apply our method to sentences which aremuch longer than 15 words.Reranking a k-best list is not a new idea.Collins (2000), Charniak and Johnson (2005), andJohnson and Ural (2010) have built reranking sys-tems with performances that are state-of-the-art.In order to achieve such high F-scores, thosererankers rely on a very large number of featuresselected on the basis of expert knowledge.
Unlikethem, our feature set is selected automatically, yetthe reranker achieved a statistically significant im-provement on both F-score and exact match.Closest to our work is Menchetti et al(2005)and Socher et al(2013): both also rely on sym-bolic parsers to reduce the search space and useRNNs to score candidate parses.
However, ourwork differs in the way the feature set for rerank-ing is selected.
In their methods, only the score atthe tree root is considered whereas in our methodthe scores at all internal nodes are taken into ac-count.
Selecting the feature set like that gives us aflexible way to deal with errors accumulated fromthe leaves to the root.Figure 1 shows a diagram of our method.
First,a parser (in this paper: the Berkeley parser) is usedto generate k-best lists of the Wall Street Jour-nal (WSJ) sections 02-21.
Then, all parse trees inthese lists and the WSJ02-21 are preprocessed bymarking head words, binarising, and performingerror-annotation (Section 2).
After that, we usethe annotated trees to train our parse-correctnessclassifier (Section 3).
Finally, those trees and theclassifier are used to train the reranker (Section 4).2 Experimental SetupThe experiments presented in this paper have thefollowing setting.
We use the WSJ corpus withthe standard splits: sections 2-21 for training, sec-tion 22 for development, and section 23 for test-ing.
The latest implementation (version 1.7) of theBerkeley parser1 (Petrov et al 2006) is used forgenerating 50-best lists.
We mark head words andbinarise all trees in the WSJ and the 50-best listsas in Subsection 2.1, and annotate them as in Sub-section 2.2 (see Figure 2).2.1 Preprocessing TreesWe preprocess trees by marking head words andbinarising the trees.
For head word marking,we used the head finding rules of Collins (1999)which are implemented in the Stanford parser.To binarise a k-ary branching, e.g.
P ?C1 ... H ... Ck where H is the top label of thehead constituent, we use the following method.
IfH is not the left-most child, thenP ?
C1 @P ; @P ?
C2 ... H ... Ckotherwise,P ?
@P Ck ; @P ?
H ... Ck?1where @P , which is called extra-P , now is thehead of P .
We then apply this transformationagain on the children until we reach terminalnodes.
In this way, we ensure that every internalnode has one head word.2.2 Error AnnotationWe annotate nodes (as correct or incorrect) as fol-lows.
Given a parse tree T in a 50-best list anda corresponding gold-standard tree G in the WSJ,1https://code.google.com/p/berkeleyparser12Figure 1: An overview of our method.Figure 2: Example for preprocessing trees.
Nodes marked with (*) are labelled incorrect whereas theother nodes are labelled correct.we first attempt to align their terminal nodes ac-cording to the following criterion: a terminal nodet is aligned to a terminal node g if they are atthe same position counting from-left-to-right andthey have the same label.
Then, a non-terminalnode P [wh] with children C1, ..., Ck is aligned toa gold-standard non-terminal node P ?
[w?h] withchildren C?1 , ..., C?l (1 ?
k, l ?
2 in our case)if they have the same word head, the same syn-tactical category, and their children are all alignedin the right order.
In other words, the followingconditions have to be satisfiedP = P ?
; wh = w?h ; k = lCi is aligned to C?i , for all i = 1..kAligned nodes are annotated as correct whereasthe other nodes are annotated as incorrect.3 Parse-Correctness ClassificationThis section describes how a neural networkis used to construct vector representations forphrases given parse trees and to identify if thosetrees are syntactically correct or not.
In order toencode tree structures, we use an RNN2 (see Fig-ure 3 and Figure 4) which is similar to the oneproposed by Socher and colleagues (2010).
How-ever, unlike their RNN, our RNN can handle unarybranchings, and also takes head words and syntac-tic tags as input.
It is worth noting that, althoughwe can use some transformation to remove unarybranchings, handling them is helpful in our casebecause the system avoids dealing with so manysyntactic tags that would result from the transfor-2The first neural-network approach attempting to operateand represent compositional, recursive structure is the Recur-sive Auto-Associative Memory network (RAAM), which wasproposed by Pollack (1988).
In order to encode a binary tree,the RAAM network contains three layers: an input layer fortwo daughter nodes, a hidden layer for their parent node, andan output layer for their reconstruction.
Training the networkis to minimise the reconstruction error such that we can de-code the information captured in the hidden layer to the orig-inal tree form.
Our RNN differs from the RAAM network inthat its output layer is not for reconstruction but for classifi-cation.13mation.
In addition, using a new set of weight ma-trices for unary branchings makes our RNN moreexpressive without facing the problem of sparsitythanks to a large number of unary branchings inthe treebank.Figure 3: An RNN attached to the parse treeshown in the top-right of Figure 2.
All unarybranchings share a set of weight matrices, and allbinary branchings share another set of weight ma-trices (see Figure 4).An RNN processes a tree structure by repeat-edly applying itself at each internal node.
Thus,walking bottom-up from the leaves of the tree tothe root, we compute for every node a vector basedon the vectors of its children.
Because of thisprocess, those vectors have to have the same di-mension.
It is worth noting that, because informa-tion at leaves, i.e.
lexical semantics, is composedaccording to a given syntactic parse, what a vec-tor at each internal node captures is some aspectsof compositional semantics of the correspondingphrase.
In the remainder of this subsection, wedescribe in more detail how to construct composi-tional vector-based semantics geared towards theparse-correctness classification task.Similar to Socher et al(2010), and Col-lobert (2011), given a string of words (w1, ..., wl),we first compute a string of vectors (x1, ..., xl)representing those words by using a look-up table(i.e., word embeddings) L ?
Rn?|V |, where |V | isthe size of the vocabulary and n is the dimension-ality of the vectors.
This look-up table L couldbe seen as a storage of lexical semantics whereeach column is a vector representation of a word.Hence, let bi be the binary representation of wordwi (i.e., all of the entries of bi are zero except theone corresponding to the index of the word in thedictionary), thenxi = Lbi ?
Rn (1)We also encode syntactic tags by binary vectorsbut put an extra bit at the end of each vector tomark if the corresponding tag is extra or not (i.e.,@P or P ).Figure 4: Details about our RNN for a unarybranching (top) and a binary branching (bottom).The bias is not shown for the simplicity.Then, given a unary branching P [wh]?
C, wecan compute the vector at the node P by (see Fig-ure 4-top)p = f(Wuc+Whxh +W?1x?1 +W+1x+1 +Wttp + bu)where c, xh are vectors representing the child Cand the head word, x?1, x+1 are the left and rightneighbouring words of P , tp encodes the syn-tactic tag of P , Wu,Wh,W?1,W+1 ?
Rn?n,Wt ?
Rn?
(|T |+1), |T | is the size of the set ofsyntactic tags, bu ?
Rn, and f can be any acti-vation function (tanh is used in this case).
Witha binary branching P [wh] ?
C1 C2, we simplychange the way the children?s vectors added (seeFigure 4-bottom)p = f(Wb1c1 +Wb2c2 +Whxh +W?1x?1 +W+1x+1 +Wttp + bb)Finally, we put a sigmoid neural unit on thetop of each internal node (except pre-terminalnodes because we are not concerned with POS-tagging) to detect the correctness of the subparsetree rooted at that nodey = sigmoid(Wcatp+ bcat) (2)where Wcat ?
R1?n, bcat ?
R.143.1 LearningThe error on a parse tree is computed as the sumof classification errors of all subparses.
Hence, thelearning is to minimise the objectiveJ(?)
=1N?T?(y(?),t)?T12(t?
y(?
))2 + ???
?2(3)where ?
are the model parameters, N is the num-ber of trees, ?
is a regularisation hyperparameter,T is a parse tree, y(?)
is given by Equation 2, andt is the class of the corresponding subparse (t = 1means correct).
The gradient ?J??
is computed ef-ficiently thanks to backpropagation through thestructure (Goller and Kuchler, 1996).
L-BFGS(Liu and Nocedal, 1989) is used to minimise theobjective function.3.2 ExperimentsWe implemented our classifier in Torch73 (Col-lobert et al 2011a), which is a powerful Matlab-like environment for machine learning.
In order tosave time, we only trained the classifier on 10-bestparses of WSJ02-21.
The training phase took sixdays on a computer with 16 800MHz CPU-coresand 256GB RAM.
The word embeddings given byCollobert et al(2011b)4 were used as L in Equa-tion 1.
Note that these embeddings, which are theresult of training a language model neural networkon the English Wikipedia and Reuters, have beenshown to capture many interesting semantic simi-larities between words.We tested the classifier on the developmentset WSJ22, which contains 1, 700 sentences, andmeasured the performance in positive rate andnegative ratepos-rate =#true pos#true pos +#false negneg-rate =#true neg#true neg +#false posThe positive/negative rate tells us the rate at whichpositive/negative examples are correctly labelledpositive/negative.
In order to achieve high per-formance in the reranking task, the classifier musthave a high positive rate as well as a high nega-tive rate.
In addition, percentage of positive exam-ples is also interesting because it shows the unbal-ancedness of the data.
Because the accuracy is not3http://www.torch.ch/4http://ronan.collobert.com/senna/a reliable measurement when the dataset is highlyunbalanced, we do not show it here.
Table 1, Fig-ure 5, and Figure 6 show the classification results.pos-rate (%) neg-rate (%) %-Posgold-std 75.31 - 11-best 90.58 64.05 71.6110-best 93.68 71.24 61.3250-best 95.00 73.76 56.43Table 1: Classification results on the WSJ22 andthe k-best lists.Figure 5: Positive rate, negative rate, and percent-age of positive examples w.r.t.
subtree depth.3.3 DiscussionTable 1 shows the classification results on thegold-standard, 1-best, 10-best, and 50-best lists.The positive rate on the gold-standard parses,75.31%, gives us the upper bound of %-pos whenthis classifier is used to yield 1-best lists.
On the 1-best data, the classifier missed less than one tenthpositive subtrees and correctly found nearly twothird of the negative ones.
That is, our classi-fier might be useful for avoiding many of the mis-takes made by the Berkeley parser, whilst not in-troducing too many new mistakes of its own.
Thisfact gave us hope to improve parsing performancewhen using this classifier for reranking.Figure 5 shows positive rate, negative rate, andpercentage of positive examples w.r.t.
subtreedepth on the 50-best data.
We can see that the pos-itive rate is inversely proportional to the subtreedepth, unlike the negative rate.
That is because the15Figure 6: Positive rate, negative rate, and percentage of positive samples w.r.t.
syntactic categories(excluding POS tags).deeper a subtree is, the lower the a priori likeli-hood that the subtree is positive (we can see thisin the percentage-of-positive-example curve).
Inaddition, deep subtrees are difficult to classify be-cause uncertainty is accumulated when propagat-ing from bottom to top.4 RerankingIn this section, we describe how we use the aboveclassifier for the reranking task.
First, we need torepresent trees in one vector space, i.e., ?
(T ) =(?1(T ), ..., ?v(T ))for an arbitrary parse tree T .Collins (2000), Charniak and Johnson (2005), andJohnson and Ural (2010) set the first entry to themodel score and the other entries to the number ofoccurrences of specific discrete hand-chosen prop-erties (e.g., how many times the word pizza comesafter the word eat) of trees.
We here do the samewith a trick to discretize results from the classifier:we use a 2D histogram to store predicted scoresw.r.t.
subtree depth.
This gives us a flexible way topenalise low score subtrees and reward high scoresubtrees w.r.t.
the performance of the classifier atdifferent depths (see Subsection 3.3).
However,unlike the approaches just mentioned, we do notuse any expert knowledge for feature selection; in-stead, this process is fully automatic.Formally speaking, a vector feature ?
(T ) iscomputed as following.
?1(T ) is the model score(i.e., max-rule-sum score) given by the parser,(?2(T ), ..., ?v(T ))is the histogram of a set of(y, h) where y is given by Equation 2 and h is thedepth of the corresponding subtree.
The domainof y (i.e., [0, 1]) is split into ?y equal bins whereasthe domain of h (i.e., {1, 2, 3, ...}) is split into ?hbins such that the i-th (i < ?h) bin corresponds tosubtrees of depth i and the ?h-th bin correspondsto subtrees of depth equal or greater than ?h.
Theparameters ?y and ?h are then estimated on the de-velopment set.After extracting feature vectors for parse trees,we then find a linear ranking functionf(T ) = w>?
(T )such thatf(T1) > f(T2) iff fscore(T1) > fscore(T2)where fscore(.)
is the function giving F-score, andw ?
Rv is a weight vector, which is efficientlyestimated by SVM ranking (Yu and Kim, 2012).SVM was initially used for binary classification.Its goal is to find the hyperplane which has thelargest margin to best separate two example sets.
Itwas then proved to be efficient in solving the rank-ing task in information retrieval, and in syntacticparsing (Shen and Joshi, 2003; Titov and Hender-son, 2006).
In our experiments, we used SVM-16Rank5 (Joachims, 2006), which runs extremelyfast (less than two minutes with about 38, 000 10-best lists).4.1 ExperimentsUsing the classifier in Section 3, we implementedthe reranker in Torch7, trained it on WSJ02-21.We used WSJ22 to estimate the parameters ?y and?h by the grid search and found that ?y = 9 and?h = 4 yielded the best F-score.Table 2 shows the results of our reranker on50-best WSJ23 given by the Berkeley parser, us-ing the standard evalb.
Our method improves0.20% on F-score for sentences with all length,and 0.22% for sentences with ?
40 words.These differences are statistically significant6 withp-value < 0.003.
Our method also improves ex-act match (0.95% for all sentences as well as forsentences with ?
40 words).Parser LR LP LF EXallBerkeley parser 89.98 90.25 90.12 37.22This paper 90.10 90.54 90.32 38.17Oracle 95.94 96.21 96.08 65.56?
40 wordsBerkeley parser 90.43 90.70 90.56 39.65This paper 90.57 91.01 90.78 40.50Oracle 96.47 96.73 96.60 68.51Table 2: Reranking results on 50-best lists onWSJ23 (LR is labelled recall, LP is labelled pre-cision, LF is labelled F-score, and EX is exactmatch.
)Table 3 shows the comparison of the threeparsers that use the same hybrid reranking ap-proach.
On F-score, our method performed 0.1%lower than Socher et al(2013), and 1.5% betterthan Menchetti et al(2005).
However, our methodachieved the least improvement on F-score over itscorresponding baseline.
That could be because ourbaseline parser (i.e., the Berkeley parser) performsmuch better than the other two baseline parsers;and hence, detecting errors it makes on candidateparse trees is more difficult.5www.cs.cornell.edu/people/tj/svm light/svm rank.html6We used the ?Significance testing for evalua-tion statistics?
software (http://www.nlpado.de/ sebas-tian/software/sigf.shtml) given by Pado?
(2006).Parser LF (all) K-bestparserMenchetti etal.
(2005)88.8 (0.6) Collins(1999)Socher etal.
(2013)90.4 (3.8) PCFG Stan-ford parserThis paper 90.3 (0.2) BerkeleyparserTable 3: Comparison of parsers using the same hy-brid reranking approach.
The numbers in the blan-kets indicate the improvements on F-score over thecorresponding baselines (i.e., the k-best parsers).5 ConclusionsThis paper described a new reranking methodwhich uses semantics in syntactic parsing: a sym-bolic parser is used to generate a k-best list whichis later reranked thanks to parse-correctness scoresgiven by a connectionist compositional-semantics-based classifier.
Our classifier uses a recursiveneural network, like Socher et al (2010; 2011), tonot only represent phrases in a vector space givenparse trees, but also identify if these parse trees aregrammatically correct or not.Tested on WSJ23, our method achieved astatistically significant improvement on F-score(0.20%) as well as on exact match (0.95%).This result, although not comparable to the re-sults reported by Collins (2000), Charniak andJohnson (2005), and Johnson and Ural (2010),shows an advantage of using vector-based com-positional semantics to support available state-of-the-art parsers.One of the limitations of the current paper is thelack of a qualitative analysis of how learnt vector-based semantics has affected the reranking results.Therefore, the need for ?compositional seman-tics?
in syntactical parsing may still be doubted.In future work, we will use vector-based seman-tics together with non-semantic features (e.g., theones of Charniak and Johnson (2005)) to find outwhether the semantic features are truly helpful orthey just resemble non-semantic features.AcknowledgmentsWe thank two anonymous reviewers for helpfulcomments.17ReferencesRens Bod, Remko Scha, and Khalil Sima?an.
2003.Data-Oriented Parsing.
CSLI Publications, Stan-ford, CA.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 173?180.
Association for Computational Lin-guistics.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st NorthAmerican chapter of the Association for Computa-tional Linguistics, pages 132?139.
Association forComputational Linguistics.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of the In-ternational Workshop on Machine Learning (thenConference), pages 175?182.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
Computational lin-guistics, 29(4):589?637.Ronan Collobert, Koray Kavukcuoglu, and Cle?mentFarabet.
2011a.
Torch7: A matlab-like environmentfor machine learning.
In BigLearn, NIPS Workshop.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011b.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Ronan Collobert.
2011.
Deep learning for efficientdiscriminative parsing.
In International Conferenceon Artificial Intelligence and Statistics (AISTATS).Christoph Goller and Andreas Kuchler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In IEEE Inter-national Conference on Neural Networks, volume 1,pages 347?352.
IEEE.Thorsten Joachims.
2006.
Training linear SVMs in lin-ear time.
In Proceedings of the 12th ACM SIGKDDinternational conference on Knowledge discoveryand data mining, pages 217?226.
ACM.Mark Johnson and Ahmet Engin Ural.
2010.
Rerank-ing the Berkeley and Brown parsers.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 665?668.
As-sociation for Computational Linguistics.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics, Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Dong C Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical programming, 45(1-3):503?528.Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, andMassimiliano Pontil.
2005.
Wide coverage natu-ral language processing using kernel methods andneural networks for structured data.
Pattern Recogn.Lett., 26(12):1896?1906, September.Sebastian Pado?, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Barbara Partee.
1995.
Lexical semantics and compo-sitionality.
In L. R. Gleitman and M. Liberman, ed-itors, Language.
An Invitation to Cognitive Science,volume 1, pages 311?360.
MIT Press, Cambridge,MA.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 433?440.
Association for Computational Linguistics.Jordan B Pollack.
1988.
Recursive auto-associativememory.
Neural Networks, 1:122.Federico Sangati and Willem Zuidema.
2011.
Ac-curate parsing with compact tree-substitution gram-mars: Double-DOP.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 84?95.
Association for Computa-tional Linguistics.Libin Shen and Aravind K Joshi.
2003.
An SVM basedvoting algorithm with application to parse reranking.In Proceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003-Volume 4,pages 9?16.
Association for Computational Linguis-tics.Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop.Richard Socher, Cliff C Lin, Andrew Y Ng, andChristopher D Manning.
2011.
Parsing naturalscenes and natural language with recursive neu-ral networks.
In Proceedings of the 26th Inter-national Conference on Machine Learning (ICML),volume 2.18Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing With Composi-tional Vector Grammars.
In Proceedings of the ACLconference (to appear).Mark Steedman.
1999.
Connectionist sentenceprocessing in perspective.
Cognitive Science,23(4):615?634.Ivan Titov and James Henderson.
2006.
Loss mini-mization in parse reranking.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing, pages 560?567.
Associationfor Computational Linguistics.Andreas van Cranenburgh, Remko Scha, and FedericoSangati.
2011.
Discontinuous Data-Oriented Pars-ing: A mildly context-sensitive all-fragments gram-mar.
In Proceedings of the Second Workshop on Sta-tistical Parsing of Morphologically Rich Languages,pages 34?44.
Association for Computational Lin-guistics.Hwanjo Yu and Sungchul Kim.
2012.
SVM tutorial:Classification, regression, and ranking.
In GrzegorzRozenberg, Thomas Ba?ck, and Joost N. Kok, ed-itors, Handbook of Natural Computing, volume 1,pages 479?506.
Springer.19
