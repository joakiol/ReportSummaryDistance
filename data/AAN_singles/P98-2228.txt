Word Sense Disambiguation using Optimised Combinations ofKnowledge SourcesYor ick  Wi lks  and  Mark  S tevensonDepar tment  of  Computer  Science,Un ivers i ty  of  Sheff ield,Regent  Cour t ,  211 Por tobe l lo  S t reet ,Sheff ield, S1 4DPUn i ted  K ingdom{yorick, marks}@dcs, shef.
ac .ukAbst ractWord sense disambiguation algorithms, with few ex-ceptions, have made use of only one lexical know-ledge source.
We describe a system which performsword sense disambiguation on all content words infree text by combining different knowledge sources:semantic preferences, dictionary definitions and sub-ject/domain codes along with part-of-speech tags,optimised by means of a learning algorithm.
We alsodescribe the creation of a new sense tagged corpus bycombining existing resources.
Tested accuracy of ourapproach on this corpus exceeds 92%, demonstrat-ing the viability of all-word disambiguation ratherthan restricting oneself to a small sample.1 IntroductionThis paper describes a system that integrates a num-ber of partial sources of information to perform wordsense disambiguation (WSD) of content words ingeneral text at a high level of accuracy.The methodology and evaluation of WSD aresomewhat different from those of other NLP mod-ules, and one can distinguish three aspects of thisdifference, all of which come down to evaluationproblems, as does so much in NLP these days.
First,researchers are divided between a general method(that attempts to apply WSD to all the contentwords of texts, the option taken in this paper) andone that is applied only to a small trial selection oftexts words (for example (Schiitze, 1992) (Yarowsky,1995)).
These researchers have obtained very highlevels of success, in excess of 95%, close to the fig-ures for other "solved" NLP modules, the issue beingwhether these small word sample methods and tech-niques will transfer to general WSD over all contentwords.Others, (eg.
(Mahesh et al, 1997) (Harley andGlennon, 1997)) have pursued the general optionon the grounds that it is the real task and shouldbe tackled directly, but with rather lower successrates.
The division between the approaches prob-ably comes down to no more than the availabilityof gold standard text in sufficient quantities, whichis more costly to obtain for WSD than other tasks.In this paper we describe a method we have used forobtaining more test material by transforming one re-source into another, an advance we believe is uniqueand helpful in this impasse.However, there have also been deeper problemsabout evaluation, which has led sceptics like (Kil-garriff, 1993) to question the whole WSD enterprise,for example that it is harder for subjects to assignone and only one sense to a word in context (andhence the produce the test material itself) than toperform other NLP related tasks.
One of the presentauthors has discussed Kilgarriff's figures elsewhere(Wilks, 1997) and argued that they are not, in fact,as gloomy as he suggests.
Again, this is probablyan area where there is an "expertise ffect": somesubjects can almost certainly make finer, more inter-subjective, sense distinctions than others in a reli-able way, just as lexicographers do.But there is another, quite different, source of un-ease about the evaluation base: everyone agrees thatnew senses appear in corpora that cannot be as-signed to any existing dictionary sense, and this isan issue of novelty, not just one of the difficulty ofdiscrimination.
If that is the case, it tends to under-mine the standard mark-up-model-and-test method-ology of most recent NLP, since it will not then bepossible to mark up sense assignment in advanceagainst a dictionary if new senses are present.
Weshall not tackle this difficult issue further here, butpress on towards experiment.2 Knowledge Sources and WordSense  Disambiguat ionOne further issue must be mentioned, because itis unique to WSD as a task and is at the core ofour approach.
Unlike other well-known NLP mod-ules, WSD seems to be implementable by a numberof apparently different information sources.
All thefollowing have been implemented as the basis of ex-perimental WSD at various times: part-of-speech,semantic preferences, collocating items or classes,thesaural or subject areas, dictionary definitions,synonym lists, among others (such as bilingual equi-valents in parallel texts).
These phenomena seem1398different, so how can they all be, separately or incombination, informational clues to a single phe-nomenon, WSD?
This is a situation quite unlike syn-tactic parsing or part-of-speech tagging: in the lat-ter case, for example, one can write a Cherry-stylerule tagger or an HMM learning model, but there isno reason the believe these represent different ypesof information, just different ways of conceptualisingand coding it.
That seems not to be the case, at firstsight, with the many forms of information for WSD.It is odd that this has not been much discussed inthe field.In this work, we shall adopt the methodologyfirst explicitly noted in connection with WSD by(McRoy, 1992), and more recently (Ng and Lee,1996), namely that of bringing together a number ofpartial sources of information about a phenomenonand combining them in a principled manner.
This isin the AI tradition of combining "weak" methods forstrong results (usually ascribed to Newell (Newell,1973)) and used in the CRL-NMSU lexical work onthe Eighties (Wilks et al, 1990).
We shall, in thispaper, offer a system that combines the three typesof information listed above (plus part-of-speech fil-tering) and, more importantly, applies a learningalgorithm to determine the optimal combination ofsuch modules for a given word distribution; it beingobvious, for example, that thesaural methods workfor nouns better than for verbs, and so on.3 The  Sense  TaggerWe describe a system which is designed to assignsense tags from a lexicon to general text.
We usethe Longman Dictionary of Contemporary English(LODCE)(Procter, 1978), which contains two levelsof sense distinction: the broad homograph level andthe more fine-grained level of sense distinction.Our tagger makes use of several modules whichperform disambiguation and these are of two types:filters and partial taggers.
A filter removes ensesfrom consideration, thereby reducing the complex-ity of the disambiguation task.
Each partial taggermakes use of a different knowledge source from thelexicon and uses it to suggest a set of possible sensesfor each ambiguous word in context.
None of thesemodules performs the disambiguation alone but theyare combined to make use of all of their results.3.1 PreprocessingBefore the filters or partial taggers are applied thetext is tokenised, lemmatised, split into sentencesand part-of-speech tagged using the Brill part-of-speech tagger (Brill, 1992).Our system disambiguates only the content wordsin the text 1 (the part-of-speech tags assigned by1We define content words as nouns, verbs, adjectives andadverbs, prepositions are not included in this class.Brill's tagger are used to decide which are contentwords).3.2 Par t -o f - speechPrevious work (Wilks and Stevenson, 1998) showedthat part-of-speech tags can play an important rolein the disambiguation ofword senses.
A small exper-imentwas carried out on a 1700 word corpus takenfrom the Wall Street Journal and, using only part-of-speech tags, an attempt was made to find the correctLDOCE homograph for each of the content wordsin the corpus.
The text was part-of-speech taggedusing Brill's tagger and homographs whose part-of-speech category did not agree with the tags assignedby Brill's system were removed from consideration.The most frequently occuring of the remaining ho-mographs was chosen as the sense of each word.
Wefound that 92% of content words were assigned thecorrect homograph compared with manual disam-biguation of the same texts.While this method will not help us disambiguatewithin the homograph, since all senses which com-bine to form an LDOCE homograph ave the samepart-of-speech, it will help us to identify the sensescompletely innapropriate for a given context (whenthe homograph's part-of-speech disagrees with thatassigned by a tagger).It could be reasonably argued that this is a dan-gerous strategy since, if the part-of-speech taggermade an error, the correct sense could be removedfrom consideration.
As a precaution against his wehave designed our system so that if none of the dic-tionary senses for a given word agree with the part-of-speech tag then they are all kept (none removedfrom consideration).There is also good evidence from our earlier WSDsystem (Wilks and Stevenson, 1997) that this ap-proach works well despite the part-of-speech taggingerrors, that system's results improved by 14% usingthis strategy, achieved 88% correct disambiguationto the LDOCE homograph using this strategy butonly 74% without it.3.3 D ic t ionary  Def in i t ions(Cowie et al, 1992) used simulated annealing to op-timise the choice of senses for a text, based upontheir textual definition in a dictionary.
The optim-isation was over a simple count of words in commonin definitions, however, this meant hat longer defin-itions were preferred over short ones, since they havemore words which can contribute to the overlap, andshort definitions or definitions by synonym were cor-respondingly penalised.
We attempted to solve thisproblem as follows.
Instead of each word contribut-ing one we normalise its contribution by the numberof words in the definition it came from.
The Cowieet.
al.
implementation returned one sense for eachambiguous word in the sentence, without any indic-1399ation of the system's confidence in its choice, but, wehave adapted the system to return a set of sugges-ted senses for each ambiguous word in the sentence.We found that the new evaluation function led to animprovement in the algorithm's effectiveness.3.4 P ragmat ic  CodesOur next partial tagger makes use of the hierarchyof LDOCE pragmatic odes which indicate the likelysubject area for a sense.
Disambiguation is carriedout using a modified version of the simulated anneal-ing algorithm, and attempts to optimise the num-ber of pragmatic odes of the same type in the sen-tence.
Rather than processing over single sentenceswe optimise over entire paragraphs and only for thesense of nouns.
We chose this strategy since thereis good evidence (Gale et al, 1992) that nouns arebest disambiguated by broad contextual considera-tions, while other parts of speech are resolved bymore local factors.3.5 Se lect ional  Rest r i c t ionsLDOCE senses contain simple selectional restric-tions for each content word in the dictionary.
Aset of 35 semantic lasses are used, such as S = Hu-man, M = Human male, P = Plant, S -- Solid and soon.
Each word sense for a noun is given one of thesesemantic types, senses for adjectives list the typewhich they expect for the noun they modify, sensesfor adverbs the type they expect of their modifierand verbs list between one and three types (depend-ing on their transitivity) which are the expected se-mantic types of the verb's subject, direct object andindirect object.
Grammatical links between verbs,adjectives and adverbs and the head noun of theirarguments arer identified using a specially construc-ted shallow syntactic analyser (Stevenson, 1998).The semantic lasses in LDOCE are not providedwith a hierarchy, but, Bruce and Guthrie (Bruce andGuthrie, 1992) manually identified hierarchical re-lations between the semantic classes, constructingthem into a hierarchy which we use to resolve therestrictions.
We resolve the restrictions by return-ing, for each word, the set of sense which do notbreak them (that is, those whose semantic ategoryis at the same, or a lower, level in the hierarchy).4 Combin ing  Knowledge SourcesSince each of our partial taggers uggests only pos-sible senses for each word it is necessary to have somemethod to combine their results.
We trained de-cision lists (Clark and Niblett, 1989) using a super-vised learning approach.
Decision lists have alreadybeen successfully applied to lexical ambiguity res-olution by (Yarowsky, 1995) where they perfromedwell.We present he decision list system with a num-ber of training words for which the correct senseis known.
For each of the words we supplyeach of its possible senses (apart from those re-moved from consideration by the part-of-speechfilter (Section 3.2)) within a context consistingof the results from each of the partial taggers,frequency information and 10 simple collocations(first noun/verb/preposition to the left/right andfirst/second word to the left/right).
Each sense ismarked as either appropr ia te  (if it is the correctsense given the context) or inappropr ia te .
A learn-ing algorithm infers a decision list which classifiessenses as appropriate or inappropriate in con-text.
The partial taggers and filters can then be runover new text and the decision list applied to theresults, so as to identify the appropriate senses forwords in novel contexts.Although the decision lists are trained on a fixedvocabulary of words this does not limit the decisionlists produced to those words, and our system canassign a sense to any word, provided it has a defini-tion in LDOCE.
The decision list produced consistsof rules such as "if the part-of-speech is a noun andthe pragmatic odes partial tagger eturned a confid-ent value for that word then that sense is appropriatefor the context".5 P roduc ing  an  Eva luat ion  CorpusRather than expend a vast amount of effort onmanual tagging we decided to adapt two existingresources to our purposes.
We took SEMCOR, a200,000 word corpus with the content words manu-ally tagged as part of the WordNet project.
Thesemantic tagging was carried out under disciplinedconditions using trained lexicographers with tag-ging inconsistencies between manual annotators con-trolled.
SENSUS (Knight and Luk, 1994) is a large-scale ontology designed for machine-translation a dwas produced by merging the ontological hierarch-ies of WordNet and LDOCE (Bruce and Guthrie,1992).
To facilitate this merging it was necessaryto derive a mapping between the senses in the twolexical resources.
We used this mapping to translatethe WordNet-tagged content words in SEMCOR toLDOCE tags.The mapping is not one-to-one, and some Word-Net senses are mapped onto two or three LDOCEsenses when the WordNet sense does not distinguishbetween them.
The mapping also contained signific-ant gaps (words and senses not in the translation).SEMCOR contains 91,808 words tagged with Word-Net synsets, 6,071 of which are proper names whichwe ignore, leaving 85,737 words which could poten-tially be translated.
The translation contains only36,869 words tagged with LDOCE senses, althoughthis is a reasonable size for an evaluation corpusgiven this type of task (it is several orders of mag-nitude larger than those used by (Cowie et al, 1992)1400(Harley and Glennon, 1997) (Mahesh et al, 1997)).This corpus was also constructed without the ex-cessive cost of additional hand-tagging and does notintroduce any inconsistencies which may occur witha poorly controlled tagging strategy.6 Resu l tsTo date we have tested our system on only a por-tion of the text we derived from SEMCOR, whichconsisted of 2021 words tagged with LDOCE senses(and 12,208 words in total).
The 2021 word occur-ances are made up from 1068 different ypes, withan average polysemy of 7.65.
As a baseline againstwhich to compare results we computed the percent-age of words which are correctly tagged if we chosethe first sense for each, which resulted in 49.8% cor-rect disambiguation.We trained a decision list using 1821 of the occur-ances (containing 1000 different types) and kept 200(129 types) as held-back training data.
When thedecision list was applied to the held-back data wefound 70% of the first senses correctly tagged.
Wealso found that the system correctly identified oneof the correct senses 83.4% of the time.
Assumingthat our tagger will perform to a similar level over allcontent words in our corpus if test data was avilable,and we have no evidence to the contrary, this figureequates to 92.8% correct agging over all words intext (since, in our corpus, 42% of words tokens areambiguous in LDOCE).Comparative valuation is generally difficult inword sense disambiguation due to the variation inapproach and the evaluation corpora.
However, it isfair to compare our work against other approacheswhich have attempted to disambiguate all contentwords in a text against some standard lexical re-source, such as (Cowie et al, 1992), (Harley andGlennon, 1997), (McRoy, 1992), (Veronis and Ide,1990) and (Mahesh et al, 1997).
Neither McRoynor Veronis & Ide provide a quantative evaluation oftheir system and so our performance annot be eas-ily compared with theirs.
Mahesh et.
al.
claim highlevels of sense tagging accuracy (about 89%), but ourresults are not directly comparable since its authorsexplicitly reject the conventional markup-training-test method used here.
Cowie et.
al.
used LDOCEand so we can compare results using the same set ofsenses.
Harley and Glennon used the Cambridge In-ternational Dictionary of English which is a compar-able resource containing similar lexical informationand levels of semantic distinction to LDOCE.
Ourresult of 83% compares well with the two systemsabove who report 47% and 73% correct disambig-uation for their most detailed level of semantic dis-tinction.
Our result is also higher than both systemsat their most rough grained level of distinction (72%and 78%).
These results are summarised in Table 1.In order to compare the contribution of the separ-ate taggers we implemented a simple voting system.By comparing the results obtained from the votingsystem with those from the decision list we get someidea of the advantage gained by optimising the com-bination of knowledge sources.
The voting systemprovided 59% correct disambiguation, at identify-ing the first of the possible senses, which is littlemore than each knowledge source used separately(see Table 2).
This provides a clear indication thatthere is a considerable benefit to be gained fromcombining disambiguation evidence in an optimalway.
In future work we plan to investigate whetherthe apparently orthogonal, independent, sources ofinformation are in fact so.7 Conc lus ionThese experimental results show that it is possibleto disambiguate all content word in a text to a highlevel of accuracy (92%).
Our system uses an optim-ised combination of lexical knowledge sources whichappears to be a sucessful strategyu for this prob-lem.
The results reported here are slightly lowerthan those for system which concentrate on smallsets of words.
Our future research aims to reducethis gap further.AcknowledgmentsThe work described in this paper has been supportedby the European Union Language Engineering project"ECRAN - Extraction of Content: Research at Near-market" (LE-2110).Re ferencesE.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proceeding of the Third Conference onApplied Natural Language Processing, pages 152-155, Trento, Italy.R.
Bruce and L. Guthrie.
1992.
Genus disambigu-ation: A study in weighted preference.
In Proceed-ings of COLING-92, pages 1187-1191, Nantes,France.P.
Clark and T. Niblett.
1989.
The CN2 InductionAlgorithm.
Machine Learning Journal, 3(4):261-283.J.
Cowie, L. Guthrie, and J. Guthrie.
1992.
Lex-ical disambiguation using simulated annealing.In Proceedings of COLING-92, pages 359-365,Nantes, France.W.
Gale, K. Church, and D. Yarowsky.
1992.
Onesense per discourse.
In Proceedings ofthe DARPASpeech and Natural Language Workshop, pages233-237, Harriman, NY, February.A.
Harley and D. Glennon.
1997.
Sense tagging inaction: Combining different ests with additiveweights.
In Proceedings ofthe SIGLEX Workshop1401System Resource Ambiguity level(Cowie et al, 1992)(Harley and Glennon, 1997)Reported systemLDOCECIDELDOCEhomographsense'coarse' level'fine' levelsenseTable 1: Comparison of tagger with similar systemsResult72%47%78%73%83%Knowledge SourcesDictionary definitionsPragmatic odesSelectional RestrictionsAll58.1%55.1%57%59%Table 2: Results from different knowledge sources"Tagging Text with Lexical Semantics", pages 74-78, Washington, D.C., April.A.
Kilgarriff.
1993.
Dictionary word sense distinc-tions: An enquiry into their nature.
Computersand the Humanities, 26:356-387.K.
Knight and S. Luk.
1994.
Building a large know-ledge base for machine tanslation.
In Proceedingsof AAAI-94, pages 185-109, Seattle, WA.K.
Mahesh, S. Nirenburg, S. Beale, E. Viegas,V.
Raskin, and B. Onyshkevych.
1997.
Wordsense disambiguation: Why have statistics whenwe have these numbers?
In Proceedings ofthe 7th International Conference on Theoreticaland Methodological Issues in Machine Transla-tion, pages 151-159, Santa Fe, NM, June.S.
McRoy.
1992.
Using multiple knowledge sourcesfor word sense disambiguation.
ComputationalLinguistics, 18(1):1-30.A.
Newell.
1973.
Computer models of thought andlanguage.
In Schank and Colby, editors, ArtificialIntelligence and the Concept of Mind.
Freeman,San Francisco.H.
T. Ng and H. B. Lee.
1996.
Integrating multipleknowldge sources to disambiguate word sense: Anexemplar-based approach.
In Proceedings of A CL-96, pages 40-47, Santa Cruze, CA.P.
Procter, editor.
1978.
Longman Dictionary ofContemporary English.
Longman Group, Essex,England.H.
Sch/itze.
1992.
Dimensions of meaning.
In Pro-ceedings of Supercomputing '92, pages 787-796,Minneapolis, MN.M.
Stevenson.
1998.
Extracting syntactic relationsusing heuristics.
In Proceedings of the EuropeanSummer School on Logic, Language and Informa-tion '98, Saarbr/icken, Germany.
(to appear).J.
Veronis and N. Ide.
1990.
Word sense disambig-uation with very large neural networks extractedfrom machine readable dictionaries.
In Proceed-ings of COLING-90, pages 389-394, Helsinki, Fin-land.Y.
Wilks and M. Stevenson.
1997.
Combining in-dependent knowledge sources for word sense dis-ambiguation.
In Proceedings of the Third Con-ference on Recent Advances in Natural LangaugeProcessing Conference (RANLP-97), pages 1-7,Tzigov Chark, Bulgaria.Y.
Wilks and M. Stevenson.
1998.
The grammarof sense: Using part-of-speech tags as a first stepin semantic disambiguation.
Journal of NaturalLanguage Engineering, 4(1):1-9.Y.
Wilks, D. Fass, CM.
Guo, J. McDonald, T. Plate,and B. Slator.
1990.
A tractable machine dic-tionary as a basis for computational semantics.Journal of Machine Translation, 5:99-154.Y.
Wilks.
1997.
Senses and Texts.
Computers andthe Humanities.D.
Yarowsky.
1995.
Unsupervised word-sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of ACL-95, pages 189-196, Cambridge,MA.1402
