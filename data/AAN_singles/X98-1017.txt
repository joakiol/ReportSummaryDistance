The Smart/Empire TIPSTER IR SystemChris  Buckley,  Janet WalzSabir  Research,  Gai thersburg,  MDchr isb,walz  @ sabi r .comClaire Cardie,  Scott  Mardis ,  Mandar  Mitra,  Dav id  Pierce, Kir i  Wagsta f fDepar tment  o f  Computer  Sc ienceCornel l  University,  I thaca,  NY  14853card ie ,mard is ,mit ra ,p ierce,wki r i  @cs.cornel l .edu1 INTRODUCTIONThe primary goal of the Cornell/Sabir TIPSTER Phase IIIproject is to develop techniques to improve the end-userefficiency of information retrieval (IR) systems.
We havefocused our investigations in four related research areas:1.
High Precision Information Retrieval.
The goalof our research in this area is to increase the accu-racy of the set of documents given to the user.2.
Near-Duplicate Detection.
The goal of our workin near-duplicate detection is to develop methodsfor delineating or removing from the set of retrieveddocuments any information that the user has alreadyseen.3.
Context-Dependent Document Summarization.The goal of our research in this area is to providefor each document a short summary that includesonly those portions of the document relevant to thequery.4.
Context-Dependent Multi-Document Summari-zation.
The goal of our research in this area isto provide a short summary for an entire group ofrelated ocuments that includes only query-relatedportions.Taken as a whole, our research aims to increase nd-userefficiency in each of the above tasks by reducing the a-mount of text that the user must peruse in order to get thedesired useful information.We attack each task through a combination of statis-tical and linguistic approaches.
The proposed statisticalapproaches extend existing methods in IR by perform-ing statistical computations within the context of anotherquery or document.
The proposed linguistic approachesbuild on existing work in information extraction and relyon a new technique for trainable partial parsing.
In short,our integrated approach uses both statistical and linguisticsources to identify selected relationships among importantterms in a query or text.
The relationships are encoded asTIPSTER annotations \[7\].
We then use the extracted re-lationships: (1) to discard or reorder etrieved texts (forhigh-precision text retrieval); (2) to locate redundant in-formation (for near-duplicate document detection); and(3) to generate coherent synopses (for context-dependenttext summarization).An end-user scenario that takes advantage of the ef-ficiency opportunities offered by our research might pro-ceed as follows:1.
The user submits a natural anguage query to the re-trieval system, asking for a high-precision search.
Thissearch will attempt o retrieve fewer documents than anormal search, but at a higher quality, so many fewer non-useful documents will need to be examined.2.
The documents in the result set will be clustered so thatclosely related ocuments are grouped.Duplicate documents will be clearly marked so theuser will not have to look at them at all.Near-duplicate documents will also be clearly mar-ked.
When the user examines a document markedas a near-duplicate o a document previously ex-amined, the new material in this document is em-phasized in color so that it can be quickly perused,while the duplicate material can be ignored.3.
Long documents can be automatically summarized,within the context of the query, so that perhaps only 20%of the document will be presented.
This 20% summary107would include the material that made the system decidethe document was useful, as well as other material de-signed to set the context for the query-related material.4.
If the user wishes, an entire cluster of documents canbe summarized.
The user can then decide whether to lookat any of the individual documents.
This multi-documentsummary will once again be query-related.One key result of our TIPSTER efforts is the devel-opment of TRUESmart, a Toolbox for Research in UserEfficiency.
TRUESmart is a set of tools and data sup-porting researchers in the development of methods for im-proving user efficiency for state-of-the-art information re-trieval systems.
TRUESmart allows the integration of sys-tem components for high-precision retrieval, duplicate de-tection, and context-dependent summarization; it includesa simple graphical user interface (GUI) that supports eachof these tasks in the context of the end-user scenario de-scribed above.
In addition, TRUESmart aids system eval-uation and analysis by highlighting important term rela-tionships identified by the underlying statistical and lin-guistic language processing algorithms.The rest of the paper presents TRUESmart and its un-derlying IR and NLP components.
Section 2 first pro-vides an overview of the Smart IR system and the EmpireNatural Language Processing (NLP) system.
Section 3describes the TRUESmart oolbox.
To date, we have usedTRUESmart o support our work in high-precision retriev-al and context-dependent document summarization.
Wedescribe our results in these areas in Sections 4-5 usingthe TRUESmart interface to illustrate the algorithms de-veloped and their contribution to the end-user scenario de-scribed above.
Section 6 summarizes our work in dupli-cate detection and describes how the TRUESmart inter-face will easily be extended to support his task and in-clude linguistic term relationships in addition to statisticalterm relationships.
We conclude with a summary of thepotential advantages ofour overall approach.2 THE UNDERLYING SYSTEMS:SMART AND EMPIREThe two main foundations of our research are the Smartsystem for Information Retrieval and the Empire systemfor Natural Language Processing.
Both are large systemsrunning in the UNIX environment a Comell University.2.1 SmartSmart Version 13 is the latest in a long line of experi-mental information retrieval systems, dating back over 30years, developed under the guidance of G. Salton.
Thenew version is approximately 50,000 lines of C code anddocumentation.Smart Version 13 offers a basic framework for investi-gations of the vector space and related models of informa-tion retrieval.
Documents are fully automatically indexed,with each document representation being a weighted vec-tor of concepts, the weight indicating the importance of aconcept to that particular document.
The document rep-resentatives are stored on disk as an inverted file.
Naturallanguage queries undergo the same indexing process.
Thequery representative ctor is then compared with the in-dexed document representatives to arrive at a similarityand the documents are then fully ranked by similarity.Smart Version 13 is highly flexible (i.e., its algorithmscan be easily adapted for a variety of IR tasks) and veryfast, thus providing an ideal platform for information re-trieval experimentation.
Documents are indexed at a rateof almost two gigabytes an hour, on systems currentlycosting under $5,000 (for example, a dual Pentium Pro200 Mhz with 512 megabytes memory and disk).
Re-trieval speed is similarly fast, with basic simple searchestaking much less than a second aquery.2.2 The Empire System: A Trainable Par-tial ParserStated simply, the goal of the natural anguage process-ing (NLP) component for the selected text retrieval tasksis to locate linguistic relationships between query terms.For this, we have developed Empire 1, a trainable partialparser.
The remainder of this section describes the as-sumptions of our approach and the general architecture ofthe system.For the TIPSTER project, we are investigating the roleof linguistic relationships in information retrieval tasks.
Alinguistic relationship between two terms is any relation-ship that can be determined through syntactic or semanticinterpretation of the text that contains the terms.
We arefocusing on three classes of linguistic relationships thatwe believe will aid the information retrieval tasks:1. noun phrase relationships.
E g., determine wheth-er two query terms appear in the same (simple) nounphrase; find all places where a query term appearsas the head of a noun phrase.1 The name refers to our focus on empirical methods for developmentand evaluation fthe system.1082.
TrainingCorpus.subject-verb-object relationships, including theidentification of subjects and objects in gap con-structions.
These relationships help to identify thefunctional structure of a sentence, i.e., who did whatto whom.
Once identified, Smart can assign higherweights to query terms that appear in these topic-indicating verb, object, and especially subject posi-tions.noun phrase coreference.
Coreference resolutionis the identification of all strings in a document thatrefer to the same entity.
Noun phrase coreferencewill allow Smart to create more coherent summaries,e.g., by replacing pronouns with their referents asidentified by Empire.
In addition, Smart can usecoreference relationships to modify its term weight-ing function to reflect he implied equality betweenall elements of a noun phrase equivalence class.Once identified, the linguistic relationships can be em-ployed in a number of ways to improve the efficiency ofend-users: they can be used (1) to prefer the retrieval ofdocuments that also exhibit the relationships; (2) to indi-cate the presence of redundant information; or (3) to es-tablish the necessary context in automatically generatedsummaries.
Our approach to locating linguistic relation-ships is based on the following assumptions:?
The NLP system need recognize only those relation-ships that are useful for the specific text retrievalapplication.
There may be no need for full-blownsyntactic and semantic analysis of queries and doc-uments.?
The NLP system must recognize these relationshipsboth quickly and accurately.
The speed requirementargues for a shallow linguistic analysis; the accu-racy requirement argues for algorithms that focuson precision rather than recall.?
The NLP component need only provide a compar-ative linguistic analysis between a document and aquery.
This should simplify the NLP task becauseindividual documents do not have to be analyzed inisolation, but only relative to the query.Given thcse assumptions, we have developed Empire, afast, trainable, precision-based partial parser.
As a partialparser, Empire performs only shallow syntactic analysisof input texts.
Like many partial parsers and NLP systemslk~r information extraction (e.g., Hobbs et al \[9\]), Empirerelies primarily on finite-state technology \[16\] to recog-nize all syntactic and semantic entities as well as their re-lationships to one another.
Parsing proceeds in stages - -the initial stages identit~?
relatively simple constituents:PruningCorpusImprovedRule SetFinal Rule SetFigure 1 : Error-Driven Pruning of Treebank Grammarssimple noun phrases, some prepositional phrases, verbgroups, and clauses.
All linguistic relationships that re-quire higher-level attachment decisions are identified insubsequent stages and rely on output from earlier stages.Our use of finite-state transducers for partial parsing ismost similar to the work of Abney \[1\], who employs aseries of cascaded finite-state machines to build up anincreasingly complex linguistic analysis of an incomingsentence.Unlike most work in this area, however, we do not usehand-crafted patterns to drive the linguistic analysis.
In-stead, we rely on corpus-based learning algorithms to ac-quire the grammars necessary for driving each level of lin-guistic relationship identification.
In particular, we havedeveloped a very simple, yet effective technique for au-tomating the acquisition of grammars through error-driv-en pruning oftreebank grammars \[6\].
As shown in Fig-ure 1, the method first extracts an initial grammar froma "treebank" corpus, i.e., a corpus that has been anno-tated with respect o the linguistic relationship of interest.Consider the base noun phrase relationship - -  the identi-fication of simple, non-recursive noun phrases.
Accurateidentification of base noun phrases is a critical componentof any partial parser; in addition, Smart relies on base NPsas its primary source of linguistic phrase information.
Toextract a grammar for base noun phrase identification, wetag the training text with a part-of-speech tagger (we useMitre's version of Brill's tagger \[3\]) and then extract as anNP rule every unique part-of-speech sequence that coversa base NP annotation.Next, the grammar is improved by discarding rulesthat obtain a low precision-based "benefit" score when ap-plied to a held out portion of the training corpus, the prun-ing corpus.
The resulting "grammar" can then be used toidentify base NPs in a novel text as follows:1091.
Run all lower-level annotators.
For base NPs, forexample, run the part-of-speech annotator.2.
Proceed through the tagged text from left to right,at each point matching the rules against the remain-ing input.
For base NP recognition, match the NPrules against he remaining part-of-speech tags inthe text.3.
If there are multiple rules that match beginning attag or token ti, use the longest matching rule R.Begin the matching process anew at the token thatfollows the last NP.2.2.1 Empire EvaluationUsing this simple grammar extraction and pruning algo-rithm with the naive longest-match heuristic for applyingrules to incoming text, the learned grammars are shown toperform very well for base noun phrase identification.
Adetailed escription of the base noun phrase finder and itsevaluation can be found in Cardie and Pierce \[6\].
In sum-mary, however, we have evaluated the approach on twobase NP corpora derived from the Penn Treebank \[11\].The algorithm achieves 91% precision and recall on baseNPs that correspond directly to non-recursive noun phras-es in the treebank; it achieves 94% precision and recall onslightly less complicated noun phrases.
2We are currently investigating the use of error-drivengrammar pruning to infer the grammars for all phases ofpartial parsing and the associated linguistic relationshipidentification.
Initial results on verb-object recognitionshow 72% precision when tested on a corpus derived fromthe Penn Treebank.
Analysis of the results indicates thatour context-free approach, which worked very well fornoun phrase recognition, does not yield sufficient accu-racy for verb-object recognition.
As a result, we haveused standard machine learning algorithms (i.e., k-nearestneighbor and memory-based learning using the value-dif-ference metric) to classify each proposed verb-object bra-cketing as either correct or incorrect given a 2-word win-dow surrounding the bracketing.
In preliminary experi-ments, the machine learning algorithm obtains 84% gen-eralization accuracy.
If we discard all bracketings it clas-sifies as incorrect, overall precision for verb-object recog-nition increases from 72% to over 80%.
The next sec-tion outlines our general approach for using learning al-gorithms in conjunction with the Empire system.2This corpus further simplifies ome of the the Treebank base NPsby removing ambiguities that we expect other components of our NLPsystem to handle, including: conjunctions, NPs with leading and trailingadverbs and verbs, and NPs that contain prepositions.2.2.2 The Role of Machine Learning AlgorithmsAs noted above, Empire's finite-state partial parsing meth-ods may not be adequate for identifying some linguis-tic relationships.
At a minimum, many linguistic rela-tionships are better identified by taking additional con-text into account.
In these circumstances, we propose theuse of corpus-based machine learning techniques - - bothas a systematic means for correcting errors (as done forverb-object recognition above) and for learning to identifylinguistic relationships that are more complex than thosecovered by the finite-state methods above.In particular, we have employed the Kenmorc knowl-edge acquisition framework for NLP systems \[4, 5\].
Ken-more relies on three major components.
First, it requiresan annotated training corpus, i.e., a collection of on-line documents, that has been annotated with the neces-sary bracketing information.
Second, it requires a robustsentence analyzer, or parser.
For this, we use the Empirepartial parser.
Finally, the framework requires an induc-tive learning algorithm.
Although any inductive learningalgorithm can be used, we have successfully used case-based learning (CBL) algorithms for a number of naturallanguage learning problems.There are two phases to the framework: (1) a partiallyautomated training phase, or acquisition phase, in whicha particular linguistic relationship is learned, and (2) anapplication phase, in which the heuristics learned dur-ing training can be used to identify the linguistic relation-ship in novel texts.
More specifically, the goal of Ken-more's training phase (see Figure 2) is to create a casebase, or memory, of linguistic relationship decisions.
Todo this, the system randomly selects a set of training sen-tences from the annotated corpus.
Next, the sentence an-alyzer processes the selected training sentences, creatingone case for every instance of the linguistic relationshipthat occurs.
As shown in Figure 2, each case has twoparts.
The context portion of the case encodes the con-text in which the linguistic relationship was encountered- -  this is essentially a representation f some or all of theconstituents in the neighborhood of the linguistic relation-ship as denoted in the flat syntactic analysis produced bythe parser.
The solution portion of the case describes howthe linguistic relationship was resolved in the current ex-ample.
In the training phase, this solution information isextracted irectly from the annotated corpus.
As the casesare created, they are stored in the case base.After training, the NLP system uses the case base with-out the annotated corpus to identify new occurrences ofthe linguistic relationship in novel sentences.
Given a sen-tence as input, the sentence analyzer processes the sen-tence and creates a problem case, automatically filling inits context portion based on the constituents appearing the110Annotate~ linguistic relationship solutionI tlinguistic ~ relationships elected sentences I'to identify Sentence \[ ~ \[Training Case Iconte oflinguistic i ,ela ions p l conte= Isolation Iepisode in linguisticrelationshipidentificationti Case-Based Reasoning Component IFigure 2: Kenmore Training/Acquisition Phase.sentence.
To determine whether the linguistic relationshipholds, Kenmore next compares the problem case to eachcase in the case base, retrieves the most similar trainingcase, and returns the decision as indicated in the solutionpart of the case.
The solution information lets Empire de-cide whether the desired relationship exists in the currentsentence.In previous work, we have used Kenmore for part-of-speech tagging, semantic feature tagging, informationextraction concept acquisition, and relative pronoun res-olution \[5\].
We expect hat this approach will be neces-sary for coreference r solution, for some types of subject-object identification, and for handling ap constructs (i.e.,tbr determining that "boy" is the subject of "ate" as wellas the object of "saw" in "Billy saw the boy that ate thecandy").
It is also the approach used to learn the verb-object correction "heuristics" described in the last section.2.2.3 Coreference ResolutionThe final class of linguistic relationship is noun phraseeoreference - -  for every entity in a text, the NLP systemmust locate all of the expressions or phrases that refer to it.As an example, consider the following: "Bill Clinton, cur-rent president of the United States, left Washington Mon-day morning for China.
He will return in two weeks."
Inthis excerpt, the phrases "Bill Clinton," "current president(of the United States)," and "he" refer to the same entity.Smart can use this coreference information to treat the as-sociated terms as equivalents.
For example, it can assumethat all items in the class are present whenever one ap-pears.
In conjunction with coreference r solution, we arealso investigating the usefulness of providing the IR sys-tem with canonicalized noun phrase forms that make useof term invariants identified uring coreference.To date, we have implemented two simple algorithmsfor coreference r solution to use purely as baselines.
Bothoperate only on base noun phrases as identified by Em-pire's base NP finder.
The first heuristic assumes thattwo noun phrases are coreferent if they share any termsin common.
The second assumes that two noun phrasesare coreferent if they have the same head.
Both obtainedhigher scores than expected when tested on the MUC6coreference data set.
The head noun heuristic achieved42% recall and 51% precision; the overlapping terms heur-istic achieved 41% recall and precision.2.2.4 Empire AnnotatorsAll relationships identified by Empire are made availableto Smart in the form of TIPSTER annotations.
We cur-rently have the following annotators in operation:?
tokenizer: identifies tokens, punctuation, etc.?
sentence finder: based on Penn's maximum entropyalgorithm \[ 15\].?
baseNPs: identifies non-recursive noun phrases.?
verb-object: identifies verb-object pairs, either bybracketing the verb group and entire direct objectphrase or by noting just the heads of each.?
head noun coreference heuristic: identifies corefer-ent NPs.?
overlapping terms coreference heuristic: identifiescoreferent NPs.The tokenizer is written in C. The sentence finder is writ-ten in Java.
All other annotators are implemented in Lu-cid/Liquid Common Lisp.1113 TRUESmaTo support our research in user-efficient information re-trieval, we have developed TRUESmart, a Toolbox forResearch in User Efficiency.
As noted above, TRUESmartallows the integration, evaluation, and analysis of IR andNLP algorithms for high-precision searches, context-de-pendent summarization, and duplicate detection.
TRUE-Smart provides three classes of resources that are neces-sary for effective research in the above areas:l. Testbed Collections, including test queries and cor-rect answers2.
Automatic Evaluation Tools, to measure overallhow an approach does on a collection.3.
Failure Analysis Tools, to help the researcher in-vestigate in depth what has happened.These tools are, to a large extent, independent of the actualresearch being done.
However, they are just as vital forgood research as the research algorithms themselves.3.1 TRUESmart  CollectionsThe testbed collections organized for TRUESmart are allbased on TREC \[19\] and SUMMAC \[10\], the large eval-uation workshops run by NIST and DARPA respectively.TREC provides a number of document collections rang-ing up to 500,000 documents in size, along with queriesand relevance judgements that tell whether a document isrelevant to a particular query.Evaluation of our high-precision research can be donedirectly using the TREC collections.
The TREC docu-ments, queries, and relevance judgements are sufficient oevaluate whether particular high-precision algorithms dobetter than others.For summarization research, however, a different est-bed is needed.
The SUMMAC workshop evaluated sum-maries of documents.
The major evaluation measuredwhether human judges were able to judge relevance of en-tire documents just from the summaries.
While very valu-able in giving a one-time absolute measure of how wellsummarization algorithms are doing, human-dependent -valuations are infeasible for a research group to performon ongoing research since different human assessors arerequired whenever a given document or summary is judged.Our summarization testbed is based on the SUMMACQandA evaluation.
Given a set of questions about a docu-ment, and a key describing the locations in the documentwhere those questions are answered, the goal is to evaluatehow well an extraction-based summary of that documentanswers the questions.
So the TRUESmart summarizationtestbed consists of?
A small number of queries?
A small number of relevant documents per query?
A set of questions for each query?
Locations in the relevant documents where eachquestion is answered.Objective valuation of near-duplicate information de-tection is difficult.
As part of our efforts in this area, wehave constructed a small set (50 pairs) of near-duplicatedocuments of newswire articles.
These pairs were deliber-ately chosen to encompass a range of duplication amounts;we include 5 pairs at cosine similarity .95, 5 pairs at .90,and 10 pairs at each of .85, .80, .75, and .70.
In addition,they have been categorized as to exactly what the rela-tionship between the pairs is.
For example, some pairsare slight rewrites by the same author, some are followuparticles, and some are two articles on the same subjectby different authors.
We also have queries that will re-trieve both of these pairs among the top documents.
Thesearticles are tagged: corresponding sections of text fromeach document pair are marked as identical, semanticallyequivalent, or different.Preparing a testbed for multi-document summariza-tion is even more difficult.
We have not done this as yet,but our initial approach will take as a seed the QandAevaluation test collections described above.
This gives usa query and a set of relevant documents with known an-swers to a set of common questions.
Evaluation can bedone by performing a multi-document summarization ona subgroup of this set of relevant documents.
The finalsummary can be evaluated based upon how many ques-tions are answered (a question is answered by a text ex-cerpt in the summary if the excerpt in the correspondingoriginal document was marked as answering the ques-tion), and how many questions are answered more thanonce.
If too many questions are answered more than once,then the duplicate detection algorithms may not be work-ing optimally.
If too few questions are answered at all,then the summarization algorithms may be at fault.
Theevaluation umbers produced by the final summary can becompared against he average valuation umbers for thedocuments in the group.3.2 TRUESmart  EvaluationAutomatic evaluation of research algorithms is critical forrapid progress in all of these areas.
Manual evaluation is112valuable, but impractical when trying to distinguish be-tween small variations of a research group's algorithms.3.2.1 Trec_evalAutomatic evaluation of straight information retrievaltasks is not new.
In particular, we have provided the"trec_eval" program to the TREC community to evalu-ate retrieval in the TREC environment.
It will also be anevaluation component in the TRUESmart ToolBox.
Thetrec_eval measures are described in the TREC-4 workshopproceedings \[8\].3.2.2 Summ_evalThe QandA evaluation of SUMMAC is very close to be-ing automatic once questions and keys are created.
ForSUMMAC, the human assessors till judge whether ornot a given summary answers the questions.
Indeed, fornon-extraction-based ummaries, this is required.
But forevaluation of extraction-based summarization (where thesummaries contain clauses, sentences, or paragraphs ofthe original document), an automatic approximation ftheassessor task is possible.
This enables a research groupto fairly evaluate and compare multiple summaries of thesame document, with no additional manual effort afterthe initial key is determined.
Thus we have written the"summ_eval" evaluator.
This algorithm for the automaticevaluation of summaries:1.
Automatically finds the spans of the text of the orig-inal document that were given as answers in thekeys.2.
Automatically finds the spans of the text of the orig-inal document that appeared in a summarization fthe document.3.
Computes various measures of overlap between thesummarization spans and the answer spans.The effectiveness of two summarization algorithms canbe automatically compared by comparing these overlapmeasures.We ran summ_eval on the summaries produced bythe systems of the SUMMAC workshop.
The compar-ative ranking of systems using summ_eval is very closeto the (presumably) optimal rankings using human asses-sors.
This strongly suggests that automatic scoring ofsumm_eval can be useful for evaluation in circumstanceswhere human scoring is not available3.2.3 Dup_eval"Dup_eval" uses the same algorithms as summ_eval to mea-sure how well an algorithm can detect whether one doc-ument contains information that is duplicated in another.The key (correct answer) for one document out of a pairwill give the spans of text in that document that are dupli-cated in the other, at three different levels of duplication:exact, semantically equivalent, and contained in.
The du-plicate detection algorithm being evaluated will come upwith similar spans.
Dup_eval measures the overlap be-tween the these sets of spans.3.3 TRUESmart  GUIAutomatic evaluation is only the beginning of the researchprocess.
Once evaluation pinpoints the failures and suc-cesses of a particular algorithm, analysis of these failuresmust be done in order to improve the algorithm.
This anal-ysis is often time-consuming and painful.
This motivatesthe implementation f the TRUESmart GUI.
This GUI isnot aimed at being a prototype of a user efficiency GUI.Instead, it offers a basic end-user interface while givingthe researcher the ability to explore the underlying causesof particular algorithm behavior.Figure 3 shows the basic TRUESmart GUI as usedto support high-precision retrieval and context-dependentsummarization.
The user begins by typing a query intothe text input box in the middle, left frame.
The sam-ple query is TREC query number 151: "The documentwill provide information on jail and prison overcrowdingand how inmates are forced to cope with those conditions;or it will reveal plans to relieve the overcrowded condi-tion."
Clicking the SubmitQ button initiates the search.Clicking the NewQ button allows the submission of anew query.
3 Once the query is submitted, Smart initi-ates a global search in order to quickly obtain an initialset of documents for the user.
The document number,similarity ranking, similarity score, source, date, and ti-tle of the top 20 retrieved ocuments are displayed in theupper left frame of the GUI.
Clicking on any documentwill cause its query-dependent summary to be displayedin the large frame on the right.
In Figure 3, the sum-mary of the seventh document is displayed.
In this run,we have set Smart's target summary length to 25% andasked for sentence- (rather than paragraph-) based sum-maries.
Matching query terms are highlighted through-out the summary although they are not visible in thescreen dump.
The left, bottom-most frame of the inter-face lists the most important query terms (e.g., prison,jail,3The "ModQ" and "Mod vec" buttons allow the user to modify thequery and modify the query vector, espectively.
Neither will be dis-cussed further here.113inmat(e), overcrowd) and their associated weights (e.g.,4.69,5.18,7.17, 12.54).Alter the initial display of the top-ranked ocuments,Smart begins a local search in the background: each in-dividual document is reparsed and matched once againagainst he query to see if it satisfies the particular high-precision restriction criteria being investigated.
If itdoesn't he document is removed from the retrieved set;otherwise, the document remains in the final retrieved setwith a score that combines the global and local score.In addition, the user can supply relevance judgements onany document by clicking Rel (relevant), NRel (not rel-evant), or PRel (probably relevant).
Smart uses thesejudgements as feedback, updating the ranking after ev-ery 5 judgements by adding new documents and removingthose already judged from the list of retrieved texts.
Fig-ure 4 shows the state of the session after a number of rel-evance judgements have been made and new documentshave been added to the top 20.The interface, while basic, is valuable in its own right.It was successfully used for the Cornell/SablR experi-ments in the TREC 7 High-Precision track.
In this task,users were asked to find 15 relevant documents within 5minutes for each of 50 queries.
This was a true test of userefficiency; and Cornell/SablR did very well.The most important use of the GUI, though, is to ex-plore what is happening underneath the surface, in orderto aid the researcher.
Operating on either a single docu-ment or a cluster of documents, the researcher can requestseveral different views.
The two main paradigms are:(1) the document map view, which visually indicates therelationships between parts of the selected ocument(s);and (2) the document annotation view, which displays anysubset of the available annotations for the selected ocu-ment(s).
Neither view is shown in Figures 3 and 4.The document annotation view, in particular, is ex-tremely flexible.
The interface allows the user to run anyof the available annotators on a document (or documentset).
Each annotator returns the text(s) and the set of an-notations computed for the text(s).
The GUI, in turn, dis-plays the text with the spans of each annotation type high-lighted in a different color.
Optionally, the values of eachannotation can be displayed in a separate window.
Thus,for instance, a document may be returned with one anno-tation type giving the spans of a document summary, andother annotation types giving the spans of an ideal sum-mary.
The researcher can then immediately see what theproblems are with the document summary.There is no limit to the number of possible annota-tors that can be displayed.
Annotators implemented orplanned include:?
Query term matches (with values in separate win-dow).?
Statistical and/or linguistic phrase matches.?
Summary vs. model summary.?
Summary vs. QandA answers.?
Two documents concatenated with duplicate infor-mation of the second annotated in the first.?
Coreferent noun phrases.?
Subject, verb, or object term matches.?
Verb-object, subject-verb, and subject-object termmatches.?
Subjects or objects of gap constructions annotatedwith the inferred filler if it matches an importantterm.Analyzing the role of linguistic relationships in the IRtasks amounts to requesting the display of some or allof the NLP annotators.
For example, the user can re-quest to see linguistic phrase matches as well as statis-tical phrase matches.
In the example from Figure 3, theresulting annotated summary would show "27 inmates"and "Latino inmates" as matches of the query term "in-mates" because all instances of "inmates" appear as headnouns.
Similarly, it would show a linguistic phrase matchbetween "jail overcrowding" (paragraph 5 of the sum-mary) and "jail and prison overcrowding" (in the query)for the same reason.
When the output of the linguisticphrase annotator is requested, the lower left frame thatlists query terms and weights is updated to include thelinguistic phrases from the query and their correspondingweights.Alternatively, one might want to analyze the role ofthe "subject" annotator.
In the running example, this wouldmodify the summary window to show matches that in-volve terms appearing as the subject of a sentence or clause.For example, all of the following occurrences of "inmates"would be marked as subject matches with the "inmates"query term, which also appears in the subject position("inmates are forced"): "inmates were injured" (paragraphl ), "inmates broke out" (paragraph 2), "inmates refused"(paragraph 2), "inmates are confined" (paragraph 3), etc.Smart can give extra weight o these "subject" term match-es since entities that appear in this syntactic position areoften central topic terms.
The interface helps the devel-oper to quickly locate and determine the correctness ofsubject matches.
As an aside, if the "subject gap con-struction" annotator were requested, "inmates" would befilled in as the implicit subject of "return" in paragraph 2and would be marked as a query term match.114.
.
.
.
.
7.
= .~,-.432478 1 56.80517694 2 56.48434848 3 54.8043~5~5 4 54.31405413 5 53.8551.1274 6 52.11399075 7 51.04<P> REPORT ~$SAILS CONDITIONS AT ~RTE@) ImitATES TO GET $150 ~ UP IF DEIIlE<?> JUDGES ORDE\]~E~ TO OPEN COURT IN N.%<P> "DEATH LOTTB~Y" III~TIZES O~OI~IH~<P> 12 \]NJIJRE~ IN RIOTING AT<P> OI~NGE COUNTY NEMSI~TCH <JP> <JHE~<P> ~ VISTA JAIL ~ I~TER RIOT FC186773 8 48.30I(S061 9 48.02407707 10 47.71180349 11 47.01455171 12 46.784348~5 13 48.28117312 14 45.51444642 15 45,O0523364 16 44.74117307 17 44.57186014 18 43.77515147 19 43.2'6462422 2O 43.21FT 28 OCT 53 / Tumim ~ of Jail overFT 23 SEP 92 / Pria~n violence tops Fr~<P> JAIL OOUm.E-~UHK ~ H~ HITCH: NEFT 25 NOV 93 1 Prison oxtonslon plan ar<P> POSTSCRIPT: JAIL PROG~It GIGS INF#<P> ~ OFFENOERS HAY BE JAILED IN TElLFT 30 OCT ~ / Prison condltions attack<P> STIFF TERHS I~E LITTLE IHPACT ON \[<P> L.A, COJHTY'S (~TR~L JAIL: </P> <FFT 30 OCT 9~ / Prison coalitions are atFT I0 ~ 94 / Leadin9 ~'tlcle: Jail<P> CRONOING COHPOIJNI~ \]TIE PRESSURE4~> JAIL ~DING ~q~ IN COURT AGAIN;Rel ~1 PRel\]The d(x~.ment u l l l  provide in f~t im on Jai l  and ~'l~m ~ \ ]Ne~Q S~i tO HodQ Hod vec351 0 2?~ 4.69668 l~'i~351 0 ~72101 5.18446 Jail351 0 3940/9 7.17984 Irmat351 0 435502 12.54202 owN- ~- end351 0 442675 2.12707 provld351 0 467934 4.73140 cond351 0 46S0~9 5.59571 cope351 0 515220 2.19704 fore~51 0 644141 1.80153 plan351 0 6~9474 4 .2~2 rel ief351 0 680680 2.26442 inform351 0 765274 2.13352 docu3~51 0 7~84 4.15008 reveal351 1 57596 6.39,327 inform l~ovid351 1 79370 10.46741 plan r~mal351 1 1085,.2 11.85068 o~mrcrm~d Ison<P>Calm ~ r~t~'ed Surda9 at the Sen Diego County Jail in 01ula Vista after 27irmat.~ uane injured in a Saturda9 ni9ht r iot,  authorities aid.</P><P>~llblock 3-R u~ locked doun ~t~r  a .aJor dist.ur'bar~ betueen black andLatino irmato~ l~'oka out in ?
co~ an~ and the Inmatos r~u~d to return totheir cells, said Sen Diego Count4j S}m'iff's Capt.<P>lock-an,  in ~'dch lnmat~ are confined to their cells, ~ l i ftedafter an undi~lo~d nuM~" of inmates ~ trar~Ferred to oU'mr count9 Jails.11'm Jail, d~i~d to confine 192inmates, held 782 on Sate'de.</P><P>A national surv~ released l~t ~ reported that San Diego's Jails arenation's ~t  ~ detention fan i l i t i~ .
The stud, dnith examined27 Jall :~Jst~ms with 1,000 or ~ inpat~, fotmd that coont9 Jails operated at21,Tf of cap,  it9 dtrin9 19~.
Over its 10"9~" l i fe,  P r~ l t ian  A uas ~'~Jectodto rai~ $1.6 billion torelieve ov~'crowdln9.</P><P>Count9 ~inlatrators have ~aid their anl9 realistic hope of alleviatin9 Jellovercrowdin9 l i~ in overt urnin9 the court rulin9 that struck down PropceitianA.R court cet t l~t  naachad before the denim ofPrOlX~ition fl called for in,to pqx~latian at the South ~a9 Jail to be reducedto 373 b9 Jul9 I.<P>Photo, Panaaedic~ tend to Irm~to injured in fracas at the Chula Vista Ja i l .Elapsed tire: 17,8OuitFigure 3: TRUESmart GUI After Initial Query.
Note that (other than the text input box) no frameborders, scrolling options, and or button borders are visible in this screen dump.115: t C e,:_ p.~,::' e r405413 JY 1 53.85 <P> 12 IHJLIRE~ IN RIOTING AT399075 JY 2 51.04 <P> ~ VISTA ~AIL ~ AFTER RIOT F\[517694 3 60.31 <P> IHI~TES TO GET $150 ~ tiP IF D811E434848 4 59.01 <P> JLrDGES 0RDE;~ TO 0PE~I COURT IN N.'r43~8~ 5 58.53 <P> "\])ERTH LOTTERY" DRAHATIZES (~OM~l~51.1.274 JN 6 58.51 <P> ORANGE COUNTY NENSZ~TCH </P> </H~ql407707 7 57.94 <P> JAIL DQLrB.E-~UNK ~ WAS HITCH: 1(43247'8 8 57.51 <P> REPORT I~I lLS CON\]HTIONS RT STATE515147 9 54.94 <P> CRONDING COHPOUNI~S THE P IE  TH c434985 10 54.70 <P> DRUG OFF'EHI\]Ei~; HAY ~ JRILEI\] IN TEL186773 11 54.61 ~ 28 OCT 83 1 TLmlm ~ of Jail o~106061 12 52.49 FT 23 SEP ~2 / Prison violence tops Fre523364 13 52.22 <P> L.A. COUNTY'S CENT I~L JAIL,* </P>405314 14 50.72 <P> I~tCIRL ~ AT El.
CHJON JAIL LEAVE196014 15 50.48 FT 10 llqR 84 / Lasdln9 Article: Jail455171 16 50.44 ~> POSTSCRIPT: JAIL ~ GIES Iblh~513003 17 50.31 <P> O.C.
JAIL PRCXS "Eli IN, BJT EACH117312 18 49.29 FT 30 OCT ~ I Prison conditions attad~420075 19 49.28 <P> "YOUNG ftW TE/~ER" -- JAIU.10USE4G2422 20 49.00 ~> JAIL CROMDING CR~ 18 COURT AGAIN:Rel FtRel PRel\[The doo.m~t will provide infcr~etion on Jail and Prison ovorcf~ INeuO ,~ i tO  I'kxl Q Hod ve~352 0 ~ 4.6S668 prison~2 0 372101 5.18446 Jail352 0 394019 7.17984 irmat~52 0 435502 12.542~2 o~o~d"352 0 442675 2.12707 IX'ovid~2 0 4~4 4.73140 cond352 0 469O69 5.59571 con0 515220 2.19704 forc352 0 644141 1.80!53 plan352 0 6~9474 4.2'8532 rel iaf352 0 680680 2.26442 inform352 0 7652.74 2.13352 ckx~u352 0 7~4 4.150~ reveal352 I 57596 6.393227 inform Ix'ovid352 1 793770 10.46741 plan r~wel352 1 10~522 11.95068 ovorcro~,d prison~P>JAIL I}Ot~.E-~JNK ~ HAS HITCH: NO<iF>~4mt I~)<P>Kow that Oron9e Cotmty finell9 h~ a~roval f ro  the stats to bunk two irletasir~tsad of one in each cell of a ~ Jail facillt9 in Santa Aria, officialstJ~9 don't have the mona# for additional 9uards needed to satc~ the extraPri~ne?~.</F><P>situation ~ cr~tsd a paradox in ~Ic~ sore than 200 Jall beds -- r~dyto be filled ~ will r~main ~mpt9 ~ t.~xx~n officials ~9 o~rcrowdin9continue~ to force the re l~ of ~"e than a hundred suspectsd or convictedcriminals a da9 ~ ~Id  otJ~'~iso be incarcerated.But last week the bond voted to helprelieve Oran9e C~nt"a seric~ Jail o~-~-oddln9 ~'~bl~ b9 not ~nelizin9 thecant9 for so-called "double lxmkin" in 216 Of the 384 cells at t~Intake-Rel~ C~mtsr in ,:~nta ~.<~><P>there is an odd ~ t  to the state's r~.Jlations: The Boord of Correctionsonl9 has authcrit9 to penalize a cant9 ~nile the Jail in question is underccr~truction.
~ hasprevio~19 sued the count9 over Jail conditions.</P><P>"lOuble bunkin9 "on the cheap" will nac~i l9  lead to Prisonor-to-pris~rviolence uhich sinIs cells wore intended to eliminate," Herman ~rots in aletter to the stats board.
~JP><P>said the stats's requirement for ainle cells is intended to Protectwlrmeable i rNt~ ~ such as 9oun9 or sontsll9 disturbed people -- frxm thel~'Ison population and to isolate irmat,~ kno.m to be exc~ivel9 violent.</P>~P>0ran9e County bee had ?
~ious l9 owr~-~ded Jail syst.~m for ~ than 109care.
The Board of Ccrrections has rated its c~asit9 at about ~,000 irmet.~,but it nou ho,__~__ rare than 4,000 on are 9iwn da.<P></P><P>"Lesat nongorou~" Re l~<~P><F>In addition, Krona said that Isot yeor 43,675 suspectsd or convicted criminals~ere either turned ewe~ from the Jai l  or 9ivan an eorl9 release to ease theoveroro~flng.
O'P><P>In addition to the 14 requsotsd elxJtias, sheri f f 's  of Ficiala told the Board ofC~,-r~tions that tJ~9 ~Jld need 16 more clerical staff ~rkcrs to ~ theadditional inmates.Rel: 2 PReI: 0 NRel: 1 Elapsed t im:  29,2Figure 4: TRUESmart GUI After Relevance Judgements.116Finally, the role of coreference resolution might alsobe analyzed by requesting to see the output of the coref-erence annotator.
In response to this request, the docu-ment text window would then be updated to highlight inthe same color all of the entities considered in the samecoreference equivalence class.
As noted above (see Sec-tion 2.2), we currently have two simple coreference an-notators: one that uses the head noun heuristic and onethat uses the overlapping terms heuristic.
In our exam-ple, the head noun annotator would assume, among otherthings, that any noun phrase with "inmates" as its headrefers to the same entity: "27 inmates", "black and Latinoinmates", "the inmates", etc.
(Note that many of theseproposed coreferences are incorrect - -  the heuristics areonly meant o be used as baselines with which to compareother, better, coreference algorithms.)
A quick scan ofthe text with all of these occurrences highlighted lets theuser quickly determine how well the annotator is workingfor the current example.
After limited pronoun resolu-tion is added to the coreference annotator, "their" in "theircells" (paragraph 2) would also be highlighted as part ofthe same equivalence class.4 HIGH-PRECISION INFORMA-TION RETRIEVALIn order to maintain general-purpose retrieval capabilities,for example, current IR systems attempt o balance theirsystems with respect o precision and recall measures.
Anumber of information retrieval tasks, however, requireretrieval mechanisms that emphasize precision: users wantto see a small number of documents, most of which aredeemed useful, rather than being given as many usefuldocuments as possible where the useful documents aremixed in with numerous non-useful documents.
As a re-sult, our research in high-precision IR concentrates on im-proving user time efficiency by showing the user only doc-uments that there is very good reason to believe are useful.Precision is increased by restricting an already re-trieved set of documents to those that meet some addi-tional criteria for relevance.
An initial set of documents iretrieved (a global search), and each individual documentis reparsed and matched against he query again to see ifit satisfies the particular estriction criteria being investi-gated (local matching).
If it does, the document is put intothe final retrieved set with a score of some combination ofthe global and local score.
We have investigated a num-ber of re-ranking algorithms.
Three are briefly describedbelow: Boolean filters, clusters, and phrases.4.1 Automatic Boolean FiltersSmart expands user queries by adding terms occurring inthe top documents.
Maintaining the focus of the query isdifficult while expanding; the query tends to drift away to-wards some one aspect of the query while ignoring otheraspects.
Therefore, it is useful to have a re-ranking algo-rithm that emphasizes those top documents which coverall aspects of the query.In recent work \[14\], we construct (soft) Boolean filterscontaining all query aspects and use these for re-ranking.A manually prepared filter can improve average precisionby up to 22%.
In practice, a user is not going to go to thedifficulty of preparing such a filter, however, so an auto-matic approximation is needed.
Aspects are automaticallyidentified by looking at the term-term correlations amongthe query terms.
Highly correlated terms are assumed tobelong to the same aspect, and less correlated terms areassumed to be independent aspects.
The automatic filterincludes all of the independent aspects, and improves av-erage precision by 6 to 13%.4.2 ClustersClustering the top documents can yield improvementsfrom two sources, as we examine in \[12\].
First, outlierdocuments (those documents not strongly related to otherdocuments) can be removed.
This works reasonably formany queries.
Unfortunately, it fails catastrophically forsome hard queries where the outlier may be the only toprelevant document!
Absolute failures need to be avoided,so this approach is not currently recommended.
The sec-ond improvement source is to ensure that query expansionterms come from all clusters.
This is another method tomaintain query focus and balance.
A very modest im-provement of 2 to 3% is obtained; it appears the Booleanfilter approach above is to be preferred, unless clusteringis being done for other purposes in any case.4.3 PhrasesTraditionally, phrases have been viewed as a precision en-hancing device.
In \[13\] and \[12\], we examine the ben-efits of using high quality phrases from the Empire sys-tem.
We discover that the linguistic phrases, when usedby themselves without single terms, are better than tradi-tional Smart statistical phrases.
However, neither group ofphrases ubstantially improves overall performance overjust using single terms, especially at the high precisionend.
Indeed, phrases tend to help at lower precisions wherethere are few clues to whether a document is relevant.
Atthe high precision end, query balance is more important.117There are generally several clues to relevance for the high-est ranked documents, and maintaining balance betweenthem is essential.
A good phrase match often hurts thisbalance by over-emphasizing the aspect covered by thephrase.4.4 TREC 7 High PrecisionCornell/SablR recently participated in the TREC 7 HighPrecision (HP) track.
In this track, the goal of the useris to find 15 relevant documents o a query within 5 min-utes.
This is obviously a nice evaluation testbed for userefficient retrieval.
We used the TRUESmart GUI and in-corporated the automatic Boolean filters described aboveinto some of our Smart retrievals.Only preliminary results are available now and onceagain Cornell/SablR did very well.
All 3 of our usersdid substantially better than the median.
One interestingpoint is that all 3 users are within 1% of each other: Thesame 3 users participated in the TREC 6 HP track last yearwith much more varied results.
Last year, the hardwarespeed and choice of query length were different betweenthe users.
We attempted toequalize these factors this year.The basically identical results suggest (but the sample ismuch too small to prove) that our general approach is rea-sonably user-training independent.
The major activity ofthe user is judging documents, a task for which all usersare presumably qualified.
The results are bounded by useragreement with the official relevance judgements, and thecloseness of the results may indicate we are approachingthat upper-bound.5 CONTEXT-DEPENDENTSUMMARIZATIONAnother application area considered to improve nd-user efficiency is reduction of the text of the documentsthemselves.
Longer documents contain a lot of text thatmay not be of interest o the end-user; techniques thatreduce the amount of this text will improve the speed atwhich the end-user can lind the useful material.
This typeof summarization differs from our previous work in thatthe document summaries are produced within the contextof a query.
This is done byI.
expanding the vocabulary of the query by relatedwords using both a standard Smart cooccurrencebased expansion process, and the output of the stan-dard Smart adhoc relevance f edback expansion pro-cess;2. weighting the expanded vocabulary by importanceto the query; and3.
performing the Smart summarization using only theweighted expanded vocabulary.We participated in both the TIPSTER dry run and theSUMMAC evaluations of summarization.
Once again wedid very well, finishing within the top 2 groups for theSUMMAC adhoc, categorization, and QandA tasks.
In-terestingly, the top 3 groups for the QandA task all usedSmart for their extraction-based summaries.Using the summ_eval evaluation tool on the SUM-MAC QandA task, we are continuing our investigationsinto length versus effectiveness, particularly when com-paring summaries based on extracting sentences as op-posed to paragraphs.
As expected, the longer the sum-mary in comparison with the original document, he moreeffective the summary.
For most evaluation measures, therelationship appears to be linear except at the extremes.For short summaries, sentences are more effectivethan paragraphs.
This is expected; the granularity of para-graphs makes it tough to fit in entire good paragraphs.However, the reverse seems to be true for longer sum-maries, at least for us at our current level of summariza-tion expertise.
The paragraphs tend to include relatedsentences that individually do not seem to use the par-ticular vocabulary our matching algorithms desire.
Thissuggests that work on coreference becomes particularlycrucial when working with sentence based summaries.Multi-Document Summarization.
Our current work in-cludes extending context-dependent summarization tech-niques for use in multi-document, rather than single-doc-ument, summarization.
Our work on duplicate informa-tion detection will also be critical for creating these morecomplicated summaries.
We have no results to report formulti-document summarization at this time.6 DUPLICATE INFORMATIONDETECTIONUsers easily become frustrated when information is du-plicated among the set of retrieved ocuments.
This isespecially a problem when users search text collectionsthat have been created from several distinct sources: anewswire source may have several reports of the sameincident, each of which may vary insignificantly.
If wecan ensure that a user does not see large quantities of du-plicate information then the user time efficiency will beimproved.118(0208-173306)3608,3641Links helow 0.61~ ignored3~.92"~6/q, 03610--361)8- Compare (:Mm 0.611 ( 360~?
3610 ) ( 361)8 3610 )Figure 5: Document-Document Text Relationship Map for Articles 3608 and 3610.
A line connects two paragraphs iftheir similarity is above a predefined threshold.Exact duplicate documents are very easy to detect byany number of techniques.
Documents for which the basiccontent is exactly the same, but differ in document meta-data like Message ID or Time of Message, are also easyto detect by several techniques.
We propose to computea cosine similarity function between all retrieved docu-ments.
Pairs of documents with a similarity of 1.0 will beidentical as far as indexable content erms.The interesting research question is how to examinedocument pairs that are obviously highly related, but donot contain exactly the same terms or vocabulary as eachother.
For this, document-document maps are constructedbetween all retrieved ocuments which are of sufficientsimilarity to each other.
These maps (see Figure 5) show alink between paragraphs of one document and paragraphsof the other if the similarity between the paragraphs i  suf-ficiently strong.
If all of the paragraphs of a documentare strongly linked to paragraphs of a second document,then the content of the first document may be subsumedby the content of the second document.
If there are un-linked paragraphs of a document, then those paragraphscontain new material that should be emphasized when thedocument is shown to the user.The structure of the document maps is an additionalimportant feature to be used to indicate the type of rela-tionship between the documents: is one document an ex-pansion of another, or are they equivalent paraphrases ofeach other, or is one a summary document that includesthe common topic as well as other topics.
All of this infor-mation can be used to decide which document to initiallyshow the user.Document-document maps can be created presentlywithin the Smart system, though they have not been usedin the past for detection of duplicate content \[2, 17, 18\].Figure 5 gives such a document-document map betweentwo newswire reports, one a fuller version of the other.7 SUMMARYIn summary, we have developed supporting technologyfor improving end-user efficiency of information retrieval(IR) systems.
We have made progress in three related ap-plication areas: high precision information retrieval, near-duplicate document detection, and context-dependent doc-ument summarization.
Our research aims to increase nd-user efficiency in each of the above tasks by reducing theamount of text that the user must p.eruse in order to get the119desired useful information.As the underlying technology for the above applica-tions, we use a novel combination of statistical and lin-guistic techniques.
The proposed statistical approachesextend existing methods in IR by pertbrming statisticalcomputations within the context of another query or doc-ument.
The proposed linguistic approaches build on ex-isting work in information extraction and rely on a newtechnique for trainable partial parsing.
The goal of theintegrated approach is to identify selected relationshipsamong important terms in a query or text and use the ex-tracted relationships: (1) to discard or reorder etrievedtexts, (2) to locate redundant information, and (3) togenerate coherent query-dependent summaries.
We be-lieve that the integrated approach offers an innovative andpromising solution to problems in end-user efficiency fora number of reasons:?
Unlike previous attempts to combine natural an-guage understanding and information retrieval, ourapproach always performs linguistic analysis rela-tive to another document or query.?
End-user effectiveness will not be significantly com-promised in the face of errors by the Smart/Empiresystem.?
The partial parser is a trainable system that can betuned to recognize those linguistic relationships thatare most important for the larger IR task.In addition, we have developed TRUESmart, a Tool-box for Research in User Efficiency.
TRUESmart is aset of tools and data supporting researchers in the de-velopment of methods for improving user efficiency forstate-of-the-art information retrieval systems.
In addition,TRUESmart includes a simple graphical user interface thataids system evaluation and analysis by highlighting im-portant term relationships identified by the underlying sta-tistical and linguistic language processing algorithms.
Todate, we have used TRUESmart o integrate and evaluatesystem components in high-precision retrieval and context-dependent summarization.In conclusion, we believe that our statistical-linguisticapproach to automated text retrieval has shown promisingresults and has simultaneously addressed four importantgoals for the TIPSTER program - -  the need for increasedaccuracy in detection systems, increased portability andapplicability of extraction systems, better summarizationof free text, and increased communication across detec-tion and extraction systems.References\[1\] Steven.
Abney.
Partial Parsing via Finite-State Cas-cades.
In Workshop on Robust Parsing, pages 8-15,1996.\[2\] James Allan.
Automatic Hypertext Construction.Cornell University, Ph.D. Thesis, Ithaca, New York,1995.\[3\] Eric Brill.
Transformation-Based Error-DrivenLearning and Natural Language Processing: A CaseStudy in Part-of-Speech Tagging.
ComputationalLinguistics, 21 (4):543-565, 1995.\[4\] C. Cardie.
Domain-Specific Knowledge Acquisi-tion for Conceptual Sentence Analysis.
PhD thesis,University of Massachusetts, Amherst, MA, 1994.Available as University of Massachusetts, CMPSCITechnical Report 94-74.\[5\] C. Cardie.
Embedded machine learning systemsfor natural anguage processing: A general frame-work.
In Stefan Wermter, Ellen Riloff, and GabrieleScheler, editors, Symbolic, connectionist, and sta-tistical approaches to learning for natural anguageprocessing, Lecture Notes in Artificial IntelligenceSeries, pages 315-328.
Springer, 1996.\[6\] C. Cardie and D. Pierce.
Error-Driven Pruning ofTreebank Grammars for Base Noun Phrase Identifi-cation.
In Proceedings of the 36th Annual Meetingof the ACL and COLING-98, pages 218-224.
Asso-ciation for Computational Linguistics, 1998.\[7\] R. Grishman.
TIPSTER Architecture Design Docu-ment Version 2.2.
Technical report, DARPA, 1996.Available at ht tp  : / /www.
t ips ter ,  org/  .\[8\] D. K. Harman.
Appendix a, evaluation techniquesand measures.
In D. K. Harman, editor, Proceedingsof the Fourth Text REtrieval Conference (TREC-4),pages A6-AI4.
NIST Special Publication 500-236,1996.\[91\[lO\]J. Hobbs, D. Appelt, J.
Bear, D. Israel,M.
Kameyama, M. Stickel, and M. Tyson.
FASTUS:A Cascaded Finite-State Transducer for ExtractingInformation from Natural-Language Text.
In E.Roche and Y. Schabes, editor, Finite-State LanguageProcessing, pages 383--406.
MIT Press, Cambridge,MA, 1997.I.
Mani, D. House, G. Klein, L. Hirschman, L. Obrst,T.
Firmin, M. Chrzanowski, and B. Sundheim.
Thetipster summac text summarization evaluation: Finalreport.
Technical report, DARPA, 1998.120[I 1] M. Marcus, M. Marcinkiewicz, and B. Santorini.Building a Large Annotated Corpus of English:The Penn Treebank.
Computational Linguistics,19(2):313-330, 1993.
[12] Mandar Mitra.
High-Precision Information Re-trieval.
PhD thesis, Department of Computer Sci-ence, Cornell University, 1998.
[13] Mandar Mitra, Chris Buckley, Amit Singhal, andClaire Cardie.
An analysis of statistical and syn-tactic phrases.
In L. Devroye and C. Chrisment,editors, Conference Proceedings of RIAO-97, pages200-214, June 1997.
[14] Mandar Mitra, Amit Singhal, and Chris Buckley.Improving automatic query expansion.
In W. BruceCroft, Alistair Moffat, C.J.
van Rijsbergen, RossWilkinson, and Justin Zobel, editors, Proceedings ofthe 21st Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 206-214.
Association for Comput-ing Machinery, 1998.
[ 15] J. Reynar and A. Ratnaparkhi.
A Maximum EntropyApproach to Identifying Sentence Boundaries.
InProceedings of the Fifth Conference on Applied Nat-ural Language Processing, pages 1 6-19, San Fran-cisco, CA, 1997.
Morgan Kaufmann.
[16] Roche, E. and Schabes, Y., editor.
Finite State De-vices for Natural Language Processing.
MIT Press,Cambridge, MA, 1997.
[17] Gerard Salton, James Allan, Chris Buckley, andMandar Mitra.
Automatic analysis, theme genera-tion and summarization f machine-readable texts.Science, 264:1421-1426, June 1994.
[18] Gerard Salton, Amit Singhal, Chris Buckley, andMandar Mitra.
Automatic text decomposition us-ing text segments and text themes.
Technical ReportTR95-1555, Cornell University, 1995.
[191 E. M. Voorhees and D. K. Harman.
Overview ofthe sixth Text REtrieval Conference (TREC-6).
InE.
M. Voorhees and D. K. Harman, editors, The SixthText REtrieval Conference (TREC-6).
NIST SpecialPublication 500-240, 1998.121
