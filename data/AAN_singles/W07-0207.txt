TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 45?52,Rochester, April 2007 c?2007 Association for Computational LinguisticsLatent Semantic Grammar Induction:Context, Projectivity, and Prior DistributionsAndrew M. OlneyInstitute for Intelligent SystemsUniversity of MemphisMemphis, TN 38152aolney@memphis.eduAbstractThis paper presents latent semantic gram-mars for the unsupervised induction ofEnglish grammar.
Latent semantic gram-mars were induced by applying singu-lar value decomposition to n-gram bycontext-feature matrices.
Parsing wasused to evaluate performance.
Exper-iments with context, projectivity, andprior distributions show the relative per-formance effects of these kinds of priorknowledge.
Results show that prior dis-tributions, projectivity, and part of speechinformation are not necessary to beat theright branching baseline.1 IntroductionUnsupervised grammar induction (UGI) generates agrammar from raw text.
It is an interesting problemboth theoretically and practically.
Theoretically, itconnects to the linguistics debate on innate knowl-edge (Chomsky, 1957).
Practically, it has the po-tential to supersede techniques requiring structuredtext, like treebanks.
Finding structure in text withlittle or no prior knowledge is therefore a fundamen-tal issue in the study of language.However, UGI is still a largely unsolved problem.Recent work (Klein and Manning, 2002; Klein andManning, 2004) has renewed interest by using a UGImodel to parse sentences from the Wall Street Jour-nal section of the Penn Treebank (WSJ).
These pars-ing results are exciting because they demonstratereal-world applicability to English UGI.
While othercontemporary research in this area is promising, thecase for real-world English UGI has not been asconvincingly made (van Zaanen, 2000; Solan et al,2005).This paper weaves together two threads of in-quiry.
The first thread is latent semantics, whichhave not been previously used in UGI.
The secondthread is dependency-based UGI, used by Klein andManning (2004), which nicely dovetails with our se-mantic approach.
The combination of these threadsallows some exploration of what characteristics aresufficient for UGI and what characteristics are nec-essary.2 Latent semanticsPrevious work has focused on syntax to the exclu-sion of semantics (Brill and Marcus, 1992; van Zaa-nen, 2000; Klein and Manning, 2002; Paskin, 2001;Klein and Manning, 2004; Solan et al, 2005).
How-ever, results from the speech recognition commu-nity show that the inclusion of latent semantic infor-mation can enhance the performance of their mod-els (Coccaro and Jurafsky, 1998; Bellegarda, 2000;Deng and Khudanpur, 2003).
Using latent semanticinformation to improve UGI is therefore both noveland relevant.The latent semantic information used by thespeech recognition community above is producedby latent semantic analysis (LSA), also known aslatent semantic indexing (Deerwester et al, 1990;Landauer et al, 1998).
LSA creates a semantic rep-resentation of both words and collections of wordsin a vector space, using a two part process.
First,45a term by document matrix is created in which thefrequency of word wi in document dj is the valueof cell cij .
Filters may be applied during this pro-cess which eliminate undesired terms, e.g.
commonwords.
Weighting may also be applied to decreasethe contributions of frequent words (Dumais, 1991).Secondly, singular value decomposition (SVD) isapplied to the term by document matrix.
The re-sulting matrix decomposition has the property thatthe removal of higher-order dimensions creates anoptimal reduced representation of the original ma-trix in the least squares sense (Berry et al, 1995).Therefore, SVD performs a kind of dimensionalityreduction such that words appearing in different doc-uments can acquire similar row vector representa-tions (Landauer and Dumais, 1997).
Words can becompared by taking the cosine of their correspond-ing row vectors.
Collections of words can likewisebe compared by first adding the corresponding rowvectors in each collection, then taking the cosine be-tween the two collection vectors.A stumbling block to incorporating LSA into UGIis that grammars are inherently ordered but LSA isnot.
LSA is unordered because the sum of vectors isthe same regardless of the order in which they wereadded.
The incorporation of word order into LSAhas never been successfully carried out before, al-though there have been attempts to apply word or-der post-hoc to LSA (Wiemer-Hastings and Zipitria,2001).
A straightforward notion of incorporatingword order into LSA is to use n-grams instead of in-dividual words.
In this way a unigram, bigram, andtrigram would each have an atomic vector represen-tation and be directly comparable.It may seem counterintuitive that such an n-gramscheme has never been used in conjunction withLSA.
Simple as this scheme may be, it quickly fallsprey to memory limitations of modern day comput-ers for computing the SVD.
The standard for com-puting the SVD in the NLP sphere is Berry (1992)?sSVDPACK, whose single vector Lanczos recursionmethod with re-orthogonalization was incorporatedinto the BellCore LSI tools.
Subsequently, eitherSVDPACK or the LSI tools were used by the ma-jority of researchers in this area (Schu?tze, 1995;Landauer and Dumais, 1997; Landauer et al, 1998;Coccaro and Jurafsky, 1998; Foltz et al, 1998; Bel-legarda, 2000; Deng and Khudanpur, 2003).
UsingJohn likes string cheese.Figure 1: A Dependency Graphthe equation reported in Larsen (1998), a standardorthogonal SVD of a unigram/bigram by sentencematrix of the LSA Touchstone Applied Science As-sociates Corpus (Landauer et al, 1998) requires over60 gigabytes of random access memory.
This esti-mate is prohibitive for all but current supercomput-ers.However, it is possible to use a non-orthogonalSVD approach with significant memory savings(Cullum and Willoughby, 2002).
A non-orthogonalapproach creates the same matrix decomposition astraditional approaches, but the resulting memorysavings allow dramatically larger matrix decompo-sitions.
Thus a non-orthongonal SVD approach iskey to the inclusion of ordered latent semantics intoour UGI model.3 Dependency grammarsDependency structures are an ideal grammar repre-sentation for evaluating UGI.
Because dependencystructures have no higher order nodes, e.g.
NP, theirevaluation is simple: one may compare with a ref-erence parse and count the proportion of correct de-pendencies.
For example, Figure 1 has three depen-dencies {( John, likes ), ( cheese, likes ), ( string,cheese ) }, so the trial parse {( John, likes ), ( string,likes ), ( cheese, string )} has 1/3 directed dependen-cies correct and 2/3 undirected dependencies cor-rect.
This metric avoids the biases created by brack-eting, where over-generation or undergeneration ofbrackets may cloud actual performance (Carroll etal., 2003).
Dependencies are equivalent with lexical-ized trees (see Figures 1 and 2) so long as the depen-dencies are projective.
Dependencies are projectivewhen all heads and their dependents are a contigu-ous sequence.Dependencies have been used for UGI before withmixed success (Paskin, 2001; Klein and Manning,2004).
Paskin (2001) created a projective model us-ing words, and he evaluated on WSJ.
Although hereported beating the random baseline for that task,both Klein and Manning (2004) and we have repli-46SlikesNPJohn VPlikesJohn likes NPcheesestring cheeseFigure 2: A Lexicalized Treecated the random baseline above Paskin?s results.Klein and Manning (2004), on the other hand, havehandily beaten a random baseline using a projectivemodel over part of speech tags and evaluating on asubset of WSJ, WSJ10.4 Unanswered questionsThere are several unanswered questions independency-based English UGI.
Some of thesemay be motivated from the Klein and Manning(2004) model, while others may be motivatedfrom research efforts outside the UGI community.Altogether, these questions address what kindsof prior knowledge are, or are not necessary forsuccessful UGI.4.1 Parts of speechKlein and Manning (2004) used part of speech tagsas basic elements instead of words.
Although thismove can be motivated on data sparsity grounds, itis somewhat at odds with the lexicalized nature ofdependency grammars.
Since Paskin (2001)?s previ-ous attempt using words as basic elements was un-successful, it is not clear whether parts of speech arenecessary prior knowledge in this context.4.2 ProjectivityProjectivity is an additional constraint that may notbe necessary for successful UGI.
English is a projec-tive language, but other languages, such as Bulgar-ian, are not (Pericliev and Ilarionov, 1986).
Nonpro-jective UGI has not previously been studied, and itis not clear how important projectivity assumptionsare to English UGI.
Figure 3 gives an example of anonprojective construction: not all heads and theirdependents are a contiguous sequence.John string likes cheese.Figure 3: A Nonprojective Dependency Graph1 2 3 4 5 6 7 8 9 1000.511.522.53x 104Words DistantNumber of DependenciesFigure 4: Distance Between Dependents in WSJ104.3 ContextThe core of several UGI approaches is distributionalanalysis (Brill and Marcus, 1992; van Zaanen, 2000;Klein and Manning, 2002; Paskin, 2001; Klein andManning, 2004; Solan et al, 2005).
The key idea insuch distributional analysis is that the function of aword may be known if it can be substituted for an-other word (Harris, 1954).
If so, both words have thesame function.
Substitutability must be defined overa context.
In UGI, this context has typically been thepreceding and following words of the target word.However, this notion of context has an implicit as-sumption of word order.
This assumption is true forEnglish, but is not true for other languages such asLatin.
Therefore, it is not clear how dependent En-glish UGI is on local linear context, e.g.
precedingand following words, or whether an unordered no-tion of context would also be effective.4.4 Prior distributionsKlein and Manning (2004) point their model in theright direction by initializing the probability of de-pendencies inversely proportional to the distance be-tween the head and the dependent.
This is a verygood initialization: Figure 4 shows the actual dis-tances for the dataset used, WSJ10.47Klein (2005) states that, ?It should be emphasizedthat this initialization was important in getting rea-sonable patterns out of this model.?
(p. 89).
How-ever, it is not clear that this is necessarily true for allUGI models.4.5 SemanticsSemantics have not been included in previous UGImodels, despite successful application in the speechrecognition community (see Section 2).
However,there have been some related efforts in unsupervisedpart of speech induction (Schu?tze, 1995).
These ef-forts have used SVD as a dimensionality reductionstep between distributional analysis and clustering.Although not labelled as ?semantic?
this work hasproduced the best unsupervised part of speech in-duction results.
Thus our last question is whetherSVD can be applied to a UGI model to improve re-sults.5 Method5.1 MaterialsThe WSJ10 dataset was used for evaluation to becomparable to previous results (Klein and Manning,2004).
WSJ10 is a subset of the Wall Street Jour-nal section of the Penn Treebank, containing onlythose sentences of 10 words or less after punctuationhas been removed.
WSJ10 contains 7422 sentences.To counteract the data sparsity encountered by usingngrams instead of parts of speech, we used the en-tire WSJ and year 1994 of the North American NewsText Corpus.
These corpora were formatted accord-ing to the same rules as the WSJ10, split into sen-tences (as documents) and concatenated.
The com-bined corpus contained roughly 10 million wordsand 460,000 sentences.Dependencies, rather than the original bracketing,were used as the gold standard for parsing perfor-mance.
Since the Penn Treebank does not label de-pendencies, it was necessary to apply rules to extractdependencies from WSJ10 (Collins, 1999).5.2 ProcedureThe first step is unsupervised latent semantic gram-mar induction.
This was accomplished by first cre-ating n-gram by context feature matrices, where thefeature varies as per Section 4.3.
The Contextglobalapproach uses a bigram by document matrix suchthat word order is eliminated.
Therefore the valueof cellij is the number of times ngrami occurredin documentj .
The matrix had approximate dimen-sions 2.2 million by 460,000.The Contextlocal approach uses a bigram by localwindow matrix.
If there are n distinct unigrams inthe corpus, the first n columns contain the countsof the words preceding a target word, and the last ncolumns contain the counts of the words followinga target word.
For example, the value of at cellijis the number of times unigramj occurred beforethe target ngrami.
The value of celli(j+n) is thenumber of times unigramj occurred after the targetngrami.
The matrix had approximate dimensions2.2 million by 280,000.After the matrices were constructed, eachwas transformed using SVD.
Because the non-orthogonal SVD procedure requires a number ofLanczos steps approximately proportional to thesquare of the number of dimensions desired, thenumber of dimensions was limited to 100.
This keptrunning time and storage requirements within rea-sonable limits, approximately 4 days and 120 giga-bytes of disk storage to create each.Next, a parsing table was constructed.
For eachbigram, the closest unigram neighbor, in terms ofcosine, was found, cf.
Brill and Marcus (1992).
Theneighbor, cosine to that neighbor, and cosines of thebigram?s constituents to that neighbor were stored.The constituent with the highest cosine to the neigh-bor was considered the likely head, based on clas-sic head test arguments (Hudson, 1987).
This datawas stored in a lookup table so that for each bigramthe associated information may be found in constanttime.Next, the WSJ10 was parsed using the parsingtable described above and a minimum spanningtree algorithm for dependency parsing (McDonaldet al, 2005).
Each input sentence was tokenizedon whitespace and lowercased.
Moving from leftto right, each word was paired with all remainingwords on its right.
If a pair existed in the pars-ing table, the associated information was retrieved.This information was used to populate the fully con-nected graph that served as input to the minimumspanning tree algorithm.
Specifically, when a pairwas retrieved from the parsing table, the arc from48the stored head to the dependent was given a weightequal to the cosine between the head and the near-est unigram neighbor for that bigram pair.
Likewisethe arc from the dependent to the head was given aweight equal to the cosine between the dependentand the nearest unigram neighbor for that bigrampair.
Thus the weight on each arc was based on thedegree of substitutability between that word and thenearest unigram neighbor for the bigram pair.If a bigram was not in the parsing table, it wasgiven maximum weight, making that dependencymaximally unlikely.
After all the words in the sen-tence had been processed, the average of all currentweights was found, and this average was used as theweight from a dummy root node to all other nodes(the dummy ROOT is further motivated in Section5.3).
Therefore all words were given equal likeli-hood of being the root of the sentence.
The endresult of this graph construction process is an n byn + 1 matrix, where n is the number of words andthere is one dummy root node.
Then this graph wasinput to the minimum spanning tree algorithm.
Theoutput of this algorithm is a non-projective depen-dency tree, which was directly compared to the goldstandard dependency tree, as well as the respectivebaselines discussed in Section 5.3.To gauge the differential effects of projectivityand prior knowledge, the above procedure was mod-ified in additional evaluation trials.
Projectivity wasincorporated by using a bottom-up algorithm (Cov-ington, 2001).
The algorithm was applied in twostages.
First, it was applied using the nonprojectiveparse as input.
By comparing the output parse to theoriginal nonprojective parse, it is possible to identifyindependent words that could not be incorporatedinto the projective parse.
In the second stage, theprojective algorithm was run again on the nonpro-jective input, except this time the independent wordswere allowed to link to any other words defined bythe parsing table.
In other words, the first stage iden-tifies unattached words, and the second stage ?re-pairs?
the words by finding a projective attachmentfor them.
This method of enforcing projectivity waschosen because it makes use of the same informa-tion as the nonprojective method, but it goes a stepfurther to enforce projectivity.Prior distributions of dependencies, as depicted inFigure 4, were incorporated by inversely weightingROOT John likes string cheeseFigure 5: Right Branching BaselineJohn likes string cheese ROOTFigure 6: Left Branching Baselinegraph edges by the distance between words.
Thismodification transparently applies to both the non-projective case and the projective case.5.3 ScoringTwo performance baselines for dependency parsingwere used in this experiment, the so-called right andleft branching baselines.
A right branching baselinepredicts that the head of each word is the word to theleft, forming a chain from left to right.
An exampleis given in Figure 5.
Conversely, a left branchingbaseline predicts that the head of each word is theword to the right, forming a chain from right to left.An example is given in Figure 6.
Although perhapsnot intuitively very powerful baselines, the right andleft branching baselines can be very effective for theWSJ10.
For WSJ10, most heads are close to theirdependents, as shown in Figure 4.
For example, thepercentage of dependencies with a head either im-mediately to the right or left is 53%.
Of these neigh-boring heads, 17% are right branching, and 36% areleft branching.By using the sign test, the statistical significanceof parsing results can be determined.
The sign test isperhaps the most basic non-parametric tests and so isuseful for this task because it makes no assumptionsregarding the underlying distribution of data.Consider each sentence.
Every word must haveexactly one head.
That means that for n words, thereis a 1/n chance of selecting the correct head (exclud-ing self-heads and including a dummy root head).
Ifall dependencies in a sentence are independent, thena sentence?s dependencies follow a binomial distri-bution, with n equal to the number of words, p equalto 1/n, and k equal to the number of correct depen-dencies.
From this it follows that the expected num-ber of correct dependencies per sentence is np, or 1.Thus the random baseline for nonprojective depen-49dency parsing performance is one dependency persentence.Using the gold standard of the WSJ10, the numberof correct dependencies found by the latent seman-tic model can be established.
The null hypothesisis that one randomly generated dependency shouldbe correct per sentence.
Suppose that r+ sentenceshave more correct dependencies and r?
sentenceshave fewer correct dependencies (i.e.
0).
Under thenull hypothesis, half of the values should be above1 and half below, so p = 1/2.
Since signed dif-ference is being considered, sentences with depen-dencies equal to 1 are excluded.
The correspond-ing binomial distribution of the signs to calculatewhether the model is better than chance is b(n, p) =b(r+ +r?, 1/2).
The corresponding p-value may becalculated using Equation 1.1 ?r+?1?k=0n!k!
(n ?
k)!1/2(1/2)n?k (1)This same method can be used for determiningstatistically significant improvement over right andleft branching baselines.
For each sentence, the dif-ference between the number of correct dependen-cies in the candidate parse and the number of cor-rect dependencies in the baseline may be calculated.The number of positive and negative signed differ-ences are counted as r+ and r?, respectively, andthe procedure for calculating statistically significantimprovement is the same.6 ResultsEach model in Table 6 has significantly better per-formance than item above using statistical proce-dure described in Section 5.2.
A number of ob-servations can be drawn from this table.
First, allthe models outperform random and right branchingbaselines.
This is the first time we are aware ofthat this has been shown with lexical items in de-pendency UGI.
Secondly, local context outperformsglobal context.
This is to be expected given the rel-atively fixed word order in English, but it is some-what surprising that the differences between localand global are not greater.
Thirdly, it is clear that theaddition of prior knowledge, whether projectivity orprior distributions, improves performance.
Fourthly,MethodContext/Projectivity/Prior Dependencies CorrectRandom/no/no 14.2%Right branching 17.6%Global/no/no 17.9%Global/no/yes 21.0%Global/yes/no 21.4%Global/yes/yes 21.7%Local/no/no 22.5%Local/no/yes 25.7%Local/yes/yes 26.3%Local/yes/no 26.7%Left branching 35.8%Table 1: Parsing results on WSJ10projectivity and prior distributions have little addi-tive effect.
Thus it appears that they bring to bearsimilar kinds of constraints.7 DiscussionThe results in Section 6 address the unansweredquestions identified in Section 4, i.e.
parts of speech,semantics, context, projectivity, and prior distribu-tions.The most salient result in Section 6 is successfulUGI without part of speech tags.
As far as we know,this is the first time dependency UGI has been suc-cessful without the hidden syntactic structure pro-vided by part of speech tags.
It is interesting to notethat latent semantic grammars improve upon Paskin(2001), even though that model is projective.
It ap-pears that lexical semantics are the reason.
Thusthese results address two of the unanswered ques-tions from Section 6 regarding parts of speech andsemantics.
Semantics improve dependency UGI.
Infact, they improve dependency UGI so much so thatparts of speech are not necessary to beat a rightbranching baseline.Context has traditionally been defined locally, e.g.the preceding and following word(s).
The resultsabove indicate that a global definition of context isalso effective, though not quite as highly perform-ing as a local definition on the WSJ10.
This sug-gests that English UGI is not dependent on local lin-ear context, and it motivates future exploration ofword-order free languages using global context.
It is50also interesting to note that the differences betweenglobal and local contexts begin to disappear as pro-jectivity and prior distributions are added.
This sug-gests that there is a certain level of equivalence be-tween a global context model that favors local at-tachments and a local context model that has no at-tachment bias.Projectivity has been assumed in previous casesof English UGI (Klein and Manning, 2004; Paskin,2001).
As far as we know, this is the first time anonprojective model has outperformed a random orright branching baseline.
It is interesting that a non-projective model can do so well when it assumes solittle about the structure of a language.
Even moreinteresting is that the addition of projectivity to themodels above increases performance only slightly.It is tempting to speculate that projectivity may besomething of a red herring for English dependencyparsing, cf.
McDonald et al (2005).Prior distributions have been previously assumedas well (Klein and Manning, 2004).
The differentialeffect of prior distributions in previous work has notbeen clear.
Our results indicate that a prior distribu-tion will increase performance.
However, as withprojectivity, it is interesting how well the modelsperform without this prior knowledge and how slightan increase this prior knowledge gives.
Overall, theprior distribution used in the evaluation is not neces-sary to beat the right branching baseline.Projectivity and prior distributions have signifi-cant overlap when the prior distribution favors closerattachments.
Projectivity, by forcing a head to gov-ern a contiguous subsequence, also favors closer at-tachments.
The results reported in Section 6 suggestthat there is a great deal of overlap in the benefit pro-vided by projectivity and the prior distribution usedin the evaluation.
Either one or the other producessignificant benefits, but the combination is much lessimpressive.It is worthwhile to reiterate the sparseness of priorknowledge contained in the basic model used inthese evaluations.
There are essentially four compo-nents of prior knowledge.
First, the ability to createan ngram by context feature matrix.
Secondly, theapplication of SVD to that matrix.
Thirdly, the cre-ation of a fully connected dependency graph fromthe post-SVD matrix.
And finally, the extractionof a minimum spanning tree from this graph.
Al-though we have not presented evaluation on word-order free languages, the basic model just describedhas no obvious bias against them.
We expect thatlatent semantic grammars capture some of the uni-versals of grammar induction.
A fuller explorationand demonstration is the subject of future research.8 ConclusionThis paper presented latent semantic grammars forthe unsupervised induction of English grammar.
Thecreation of latent semantic grammars and their appli-cation to parsing were described.
Experiments withcontext, projectivity, and prior distributions showedthe relative performance effects of these kinds ofprior knowledge.
Results show that assumptions ofprior distributions, projectivity, and part of speechinformation are not necessary for this task.ReferencesJerome R. Bellegarda.
2000.
Large vocabulary speechrecognition with multispan statistical language mod-els.
IEEE Transactions on Speech and Audio Process-ing, 8(1):76?84.Michael W. Berry, Susan T. Dumais, and Gavin W.O?Brien.
1995.
Using linear algebra for intelligent in-formation retrieval.
Society for Industrial and AppliedMathematics Review, 37(4):573?595.Michael W. Berry.
1992.
Large scale singular value com-putations.
International Journal of SupercomputerApplications, 6(1):13?49.Eric Brill and Mitchell Marcus.
1992.
Automaticallyacquiring phrase structure using distributional analy-sis.
In Speech and Natural Language: Proceedingsof a Workshop Held at Harriman, New York, pages155?160, Philadelphia, February 23-26.
Associationfor Computational Linguistics.John Carroll, Guido Minnen, and Ted Briscoe.
2003.Parser evaluation using a grammatical relation anno-tation scheme.
In A. Abeill, editor, Treebanks: Build-ing and Using Syntactically Annotated Corpora, chap-ter 17, pages 299?316.
Kluwer, Dordrecht.Noam Chomsky.
1957.
Syntactic Structures.
Mouton,The Hague.Noah Coccaro and Daniel Jurafsky.
1998.
Towards bet-ter integration of semantic predictors in statistical lan-guage modeling.
In Proceedings of the InternationalConference on Spoken Language Processing, pages2403?2406, Piscataway, NJ, 30th November-4th De-cember.
IEEE.51Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Michael A. Covington.
2001.
A fundamental algorithmfor dependency parsing.
In John A. Miller and Jef-fery W. Smith, editors, Proceedings of the 39th AnnualAssociation for Computing Machinery Southeast Con-ference, pages 95?102, Athens, Georgia.Jane K. Cullum and Ralph A. Willoughby.
2002.
Lanc-zos Algorithms for Large Symmetric Eigenvalue Com-putations, Volume 1: Theory.
Society for Industrialand Applied Mathematics, Philadelphia.Scott C. Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Yonggang Deng and Sanjeev Khudanpur.
2003.
La-tent semantic information in maximum entropy lan-guage models for conversational speech recognition.In Proceedings of Human Language Technology Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 56?63,Philadelphia, May 27-June 1.
Association for Compu-tational Linguistics.Susan Dumais.
1991.
Improving the retrieval of informa-tion from external sources.
Behavior Research Meth-ods, Instruments and Computers, 23(2):229?236.Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-dauer.
1998.
The measurement of textual coherencewith latent semantic analysis.
Discourse Processes,25(2&3):285?308.Zellig Harris.
1954.
Distributional structure.
Word,10:140?162.Richard A. Hudson.
1987.
Zwicky on heads.
Journal ofLinguistics, 23:109?132.Dan Klein and Christopher D. Manning.
2002.
A genera-tive constituent-context model for improved grammarinduction.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 128?135, Philadelphia, July 7-12.
Associationfor Computational Linguistics.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the42nd Annual Meeting of the Association for Computa-tional Linguistics, pages 478?485, Philadelphia, July21-26.
Association for Computational Linguistics.Dan Klein.
2005.
The Unsupervised Learning of NaturalLanguage Structure.
Ph.D. thesis, Stanford Univer-sity.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to plato?s problem: The latent semantic analysistheory of the acquisition, induction, and representationof knowledge.
Psychological Review, 104:211?240.Thomas.
K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
Introduction to latent semantic analysis.Discourse Processes, 25(2&3):259?284.Rasmus M. Larsen.
1998.
Lanczos bidiagonalizationwith partial reorthogonalization.
Technical ReportDAIMI PB-357, Department of Computer Science,Aarhus University.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof Human Language Technology Conference and Con-ference on Empirical Methods in Natural LanguageProcessing, pages 523?530, Philadelphia, October 6-8.
Association for Computational Linguistics.Mark A. Paskin.
2001.
Grammatical bigrams.
In T. G.Dietterich, S. Becker, and Z. Ghahramani, editors, Ad-vances in Neural Information Processing Systems 14,pages 91?97.
MIT Press, Cambridge, MA.Vladimir Pericliev and Ilarion Ilarionov.
1986.
Testingthe projectivity hypothesis.
In Proceedings of the 11thInternational Conference on Computational Linguis-tics, pages 56?58, Morristown, NJ, USA.
Associationfor Computational Linguistics.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of the 7th European As-sociation for Computational Linguistics Conference(EACL-95), pages 141?149, Philadelphia, March 27-31.
Association for Computational Linguistics.Zach Solan, David Horn, Eytan Ruppin, and ShimonEdelman.
2005.
Unsupervised learning of natural lan-guages.
Proceedings of the National Academy of Sci-ences, 102:11629?11634.Menno M. van Zaanen.
2000.
ABL: Alignment-basedlearning.
In Proceedings of the 18th InternationalConference on Computational Linguistics, pages 961?967, Philadelphia, July 31-August 4.
Association forComputational Linguistics.Peter Wiemer-Hastings and Iraide Zipitria.
2001.
Rulesfor syntax, vectors for semantics.
In Proceedings ofthe 23rd Annual Conference of the Cognitive ScienceSociety, pages 1112?1117, Mahwah, NJ, August 1-4.Erlbaum.52
