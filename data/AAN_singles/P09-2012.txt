Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45?48,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPBayesian Learning of a Tree Substitution GrammarMatt Post and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractTree substitution grammars (TSGs) of-fer many advantages over context-freegrammars (CFGs), but are hard to learn.Past approaches have resorted to heuris-tics.
In this paper, we learn a TSG us-ing Gibbs sampling with a nonparamet-ric prior to control subtree size.
Thelearned grammars perform significantlybetter than heuristically extracted ones onparsing accuracy.1 IntroductionTree substition grammars (TSGs) have potentialadvantages over regular context-free grammars(CFGs), but there is no obvious way to learn thesegrammars.
In particular, learning procedures arenot able to take direct advantage of manually an-notated corpora like the Penn Treebank, which arenot marked for derivations and thus assume a stan-dard CFG.
Since different TSG derivations canproduce the same parse tree, learning proceduresmust guess the derivations, the number of which isexponential in the tree size.
This compels heuristicmethods of subtree extraction, or maximum like-lihood estimators which tend to extract large sub-trees that overfit the training data.These problems are common in natural lan-guage processing tasks that search for a hid-den segmentation.
Recently, many groups havehad success using Gibbs sampling to address thecomplexity issue and nonparametric priors to ad-dress the overfitting problem (DeNero et al, 2008;Goldwater et al, 2009).
In this paper we applythese techniques to learn a tree substitution gram-mar, evaluate it on the Wall Street Journal parsingtask, and compare it to previous work.2 Model2.1 Tree substitution grammarsTSGs extend CFGs (and their probabilistic coun-terparts, which concern us here) by allowing non-terminals to be rewritten as subtrees of arbitrarysize.
Although nonterminal rewrites are stillcontext-free, in practice TSGs can loosen the in-dependence assumptions of CFGs because largerrules capture more context.
This is simpler thanthe complex independence and backoff decisionsof Markovized grammars.
Furthermore, subtreeswith terminal symbols can be viewed as learn-ing dependencies among the words in the subtree,obviating the need for the manual specification(Magerman, 1995) or automatic inference (Chiangand Bikel, 2002) of lexical dependencies.Following standard notation for PCFGs, theprobability of a derivation d in the grammar isgiven asPr(d) =?r?dPr(r)where each r is a rule used in the derivation.
Un-der a regular CFG, each parse tree uniquely idenfi-fies a derivation.
In contrast, multiple derivationsin a TSG can produce the same parse; obtainingthe parse probability requires a summation overall derivations that could have produced it.
Thisdisconnect between parses and derivations com-plicates both inference and learning.
The infer-ence (parsing) task for TSGs is NP-hard (Sima?an,1996), and in practice the most probable parse isapproximated (1) by sampling from the derivationforest or (2) from the top k derivations.Grammar learning is more difficult as well.CFGs are usually trained on treebanks, especiallythe Wall Street Journal (WSJ) portion of the PennTreebank.
Once the model is defined, relevant450501001502002503003504000  2  4  6  8  10  12  14subtree heightFigure 1: Subtree count (thousands) across heightsfor the ?all subtrees?
grammar () and the supe-rior ?minimal subset?
() from Bod (2001).events can simply be counted in the training data.In contrast, there are no treebanks annotated withTSG derivations, and a treebank parse tree of nnodes is ambiguous among 2n possible deriva-tions.
One solution would be to manually annotatea treebank with TSG derivations, but in additionto being expensive, this task requires one to knowwhat the grammar actually is.
Part of the thinkingmotivating TSGs is to let the data determine thebest set of subtrees.One approach to grammar-learning is Data-Oriented Parsing (DOP), whose strategy is to sim-ply take all subtrees in the training data as thegrammar (Bod, 1993).
Bod (2001) did this, ap-proximating ?all subtrees?
by extracting from theTreebank 400K random subtrees for each subtreeheight ranging from two to fourteen, and com-pared the performance of that grammar to thatof a heuristically pruned ?minimal subset?
of it.The latter?s performance was quite good, achiev-ing 90.8% F1score1 on section 23 of the WSJ.This approach is unsatisfying in some ways,however.
Instead of heuristic extraction we wouldprefer a model that explained the subtrees foundin the grammar.
Furthermore, it seems unlikelythat subtrees with ten or so lexical items will beuseful on average at test time (Bod did not reporthow often larger trees are used, but did report thatincluding subtrees with up to twelve lexical itemsimproved parser performance).
We expect there tobe fewer large subtrees than small ones.
Repeat-ing Bod?s grammar extraction experiment, this isindeed what we find when comparing these twogrammars (Figure 1).In summary, we would like a principled (model-based) means of determining from the data which1The harmonic mean of precision and recall: F1=2PRP+R.set of subtrees should be added to our grammar,and we would like to do so in a manner that preferssmaller subtrees but permits larger ones if the datawarrants it.
This type of requirement is common inNLP tasks that require searching for a hidden seg-mentation, and in the following sections we applyit to learning a TSG from the Penn Treebank.2.2 Collapsed Gibbs sampling with a DPprior2For an excellent introduction to collapsed Gibbssampling with a DP prior, we refer the reader toAppendix A of Goldwater et al (2009), which wefollow closely here.
Our training data is a set ofparse trees T that we assume was produced by anunknown TSG g with probability Pr(T |g).
UsingBayes?
rule, we can compute the probability of aparticular hypothesized grammar asPr(g | T ) =Pr(T | g) Pr(g)Pr(T )Pr(g) is a distribution over grammars that ex-presses our a priori preference for g. We use a setof Dirichlet Process (DP) priors (Ferguson, 1973),one for each nonterminal X ?
N , the set of non-terminals in the grammar.
A sample from a DPis a distribution over events in an infinite samplespace (in our case, potential subtrees in a TSG)which takes two parameters, a base measure and aconcentration parameter:gX?
DP (GX, ?
)GX(t) = Pr$(|t|; p$)?r?tPrMLE(r)The base measure GXdefines the probability of asubtree t as the product of the PCFG rules r ?
tthat constitute it and a geometric distribution Pr$over the number of those rules, thus encoding apreference for smaller subtrees.3 The parameter ?contributes to the probability that previously un-seen subtrees will be sampled.
All DPs share pa-rameters p$and ?.
An entire grammar is thengiven as g = {gX: X ?
N}.
We emphasize thatno head information is used by the sampler.Rather than explicitly consider each segmen-tation of the parse trees (which would define aTSG and its associated parameters), we use a col-lapsed Gibbs sampler to integrate over all possi-2Cohn et al (2009) and O?Donnell et al (2009) indepen-dently developed similar models.3GX(t) = 0 unless root(t) = X .46S1NPNNADVPRB VBZ S2NPPRPyouVPVBquitSomeone always makesVPFigure 2: Depiction of sub(S2) and sub(S2).Highlighted subtrees correspond with our spinalextraction heuristic (?3).
Circles denote nodeswhose flag=1.ble grammars and sample directly from the poste-rior.
This is based on the Chinese Restaurant Pro-cess (CRP) representation of the DP.
The Gibbssampler is an iterative procedure.
At initialization,each parse tree in the corpus is annotated with aspecific derivation by marking each node in thetree with a binary flag.
This flag indicates whetherthe subtree rooted at that node (a height one CFGrule, at minimum) is part of the subtree contain-ing its parent.
The Gibbs sampler considers ev-ery non-terminal, non-root node c of each parsetree in turn, freezing the rest of the training dataand randomly choosing whether to join the sub-trees above c and rooted at c (outcome h1) or tosplit them (outcome h2) according to the probabil-ity ratio ?(h1)/(?
(h1) + ?
(h2)), where ?
assignsa probability to each of the outcomes (Figure 2).Let sub(n) denote the subtree above and includ-ing node n and sub(n) the subtree rooted at n; ?
isa binary operator that forms a single subtree fromtwo adjacent ones.
The outcome probabilities are:?
(h1) = ?(t)?
(h2) = ?
(sub(c)) ?
?
(sub(c))where t = sub(c) ?
sub(c).
Under the CRP, thesubtree probability ?
(t) is a function of the currentstate of the rest of the training corpus, the appro-priate base measure Groot(t), and the concentra-tion parameter ?:?
(t) =countzt(t) + ?Groot(t)(t)|zt| + ?where ztis the multiset of subtrees in the frozenportion of the training corpus sharing the sameroot as t, and countzt(t) is the count of subtreet among them.3 Experiments3.1 SetupWe used the standard split for the Wall Street Jour-nal portion of the Treebank, training on sections 2to 21, and reporting results on sentences with nomore than forty words from section 23.We compare with three other grammars.?
A standard Treebank PCFG.?
A ?spinal?
TSG, produced by extracting nlexicalized subtrees from each length n sen-tence in the training data.
Each subtree is de-fined as the sequence of CFG rules from leafupward all sharing a head, according to theMagerman head-selection rules.
We detachthe top-level unary rule, and add in countsfrom the Treebank CFG rules.?
An in-house version of the heuristic ?mini-mal subset?
grammar of Bod (2001).4We note two differences in our work that ex-plain the large difference in scores for the minimalgrammar from those reported by Bod: (1) we didnot implement the smoothed ?mismatch parsing?,which permits lexical leaves of subtrees to act aswildcards, and (2) we approximate the most prob-able parse with the top single derivation instead ofthe top 1,000.Rule probabilities for all grammars were setwith relative frequency.
The Gibbs sampler wasinitialized with the spinal grammar derivations.We construct sampled grammars in two ways: bysumming all subtree counts from the derivationstates of the first i sampling iterations togetherwith counts from the Treebank CFG rules (de-noted (?, p$,?i)), and by taking the counts onlyfrom iteration i (denoted (?, p$, i)).Our standard CKY parser and Gibbs samplerwere both written in Perl.
TSG subtrees were flat-tened to CFG rules and reconstructed afterward,with identical mappings favoring the most proba-ble rule.
For pruning, we binned nonterminals ac-cording to input span and degree of binarization,keeping the ten highest scoring items in each bin.3.2 ResultsTable 1 contains parser scores.
The spinal TSGoutperforms a standard unlexicalized PCFG and4All rules of height one, plus 400K subtrees sampled ateach height h, 2 ?
h ?
14, minus unlexicalized subtrees ofh > 6 and lexicalized subtrees with more than twelve words.47grammar size LP LR F1PCFG 46K 75.37 70.05 72.61spinal 190K 80.30 78.10 79.18minimal subset 2.56M 76.40 78.29 77.33(10, 0.7, 100) 62K 81.48 81.03 81.25(10, 0.8, 100) 61K 81.23 80.79 81.00(10, 0.9, 100) 61K 82.07 81.17 81.61(100, 0.7, 100) 64K 81.23 80.98 81.10(100, 0.8, 100) 63K 82.13 81.36 81.74(100, 0.9, 100) 62K 82.11 81.20 81.65(100, 0.7,?100) 798K 82.38 82.27 82.32(100, 0.8,?100) 506K 82.27 81.95 82.10(100, 0.9,?100) 290K 82.64 82.09 82.36(100, 0.7, 500) 61K 81.95 81.76 81.85(100, 0.8, 500) 60K 82.73 82.21 82.46(100, 0.9, 500) 59K 82.57 81.53 82.04(100, 0.7,?500) 2.05M 82.81 82.01 82.40(100, 0.8,?500) 1.13M 83.06 82.10 82.57(100, 0.9,?500) 528K 83.17 81.91 82.53Table 1: Labeled precision, recall, and F1onWSJ?23.the significantly larger ?minimal subset?
grammar.The sampled grammars outperform all of them.Nearly all of the rules of the best single iterationsampled grammar (100, 0.8, 500) are lexicalized(50,820 of 60,633), and almost half of them havea height greater than one (27,328).
Constructingsampled grammars by summing across iterationsimproved over this in all cases, but at the expenseof a much larger grammar.Figure 3 shows a histogram of subtree size takenfrom the counts of the subtrees (by token, not type)actually used in parsing WSJ?23.
Parsing withthe ?minimal subset?
grammar uses highly lexi-calized subtrees, but they do not improve accuracy.We examined sentence-level F1scores and foundthat the use of larger subtrees did correlate withaccuracy; however, the low overall accuracy (andthe fact that there are so many of these large sub-trees available in the grammar) suggests that suchrules are overfit.
In contrast, the histogram of sub-tree sizes used in parsing with the sampled gram-mar matches the shape of the histogram from thegrammar itself.
Gibbs sampling with a DP priorchooses smaller but more general rules.4 SummaryCollapsed Gibbs sampling with a DP prior fitsnicely with the task of learning a TSG.
The sam-pled grammars are model-based, are simple tospecify and extract, and take the expected shape1001011021031041051060  2  4  6  8  10  12number of words in subtree?s frontier(100,0.8,500), actual grammar(100,0.8,500), used parsing WSJ23minimal, actual grammarminimal, used parsing WSJ23Figure 3: Histogram of subtrees sizes used in pars-ing WSJ?23 (filled points), as well as from thegrammars themselves (outlined points).over subtree size.
They substantially outperformheuristically extracted grammars from previouswork as well as our novel spinal grammar, and cando so with many fewer rules.Acknowledgments This work was supported byNSF grants IIS-0546554 and ITR-0428020.ReferencesRens Bod.
1993.
Using an annotated corpus as astochastic grammar.
In Proc.
ACL.Rens Bod.
2001.
What is the minimal set of fragmentsthat achieves maximal parse accuracy.
In Proc.
ACL.David Chiang and Daniel M. Bikel.
2002.
Recoveringlatent information in treebanks.
In COLING.Trevor Cohn, Sharon Goldwater, and Phil Blun-som.
2009.
Inducing compact but accurate tree-substitution grammars.
In Proc.
NAACL.John DeNero, Alexandre Bouchard-Co?te?, and DanKlein.
2008.
Sampling alignment structure undera Bayesian translation model.
In EMNLP.Thomas S. Ferguson.
1973.
A Bayesian analysis ofsome nonparametric problems.
Annals of Mathe-matical Statistics, 1(2):209?230.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.Cognition.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proc.
ACL.T.J.
O?Donnell, N.D. Goodman, J. Snedeker, and J.B.Tenenbaum.
2009.
Computation and reuse in lan-guage.
In Proc.
Cognitive Science Society.Khalil Sima?an.
1996.
Computational complexity ofprobabilistic disambiguation by means of tree gram-mars.
In COLING.48
