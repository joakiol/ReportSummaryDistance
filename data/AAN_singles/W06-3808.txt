Workshop on TextGraphs, at HLT-NAACL 2006, pages 45?52,New York City, June 2006. c?2006 Association for Computational LinguisticsSeeing stars when there aren?t many stars:Graph-based semi-supervised learning for sentiment categorizationAndrew B. GoldbergComputer Sciences DepartmentUniversity of Wisconsin-MadisonMadison, W.I.
53706goldberg@cs.wisc.eduXiaojin ZhuComputer Sciences DepartmentUniversity of Wisconsin-MadisonMadison, W.I.
53706jerryzhu@cs.wisc.eduAbstractWe present a graph-based semi-supervisedlearning algorithm to address the senti-ment analysis task of rating inference.Given a set of documents (e.g., moviereviews) and accompanying ratings (e.g.,?4 stars?
), the task calls for inferring nu-merical ratings for unlabeled documentsbased on the perceived sentiment ex-pressed by their text.
In particular, weare interested in the situation where la-beled data is scarce.
We place this taskin the semi-supervised setting and demon-strate that considering unlabeled reviewsin the learning process can improve rating-inference performance.
We do so by creat-ing a graph on both labeled and unlabeleddata to encode certain assumptions for thistask.
We then solve an optimization prob-lem to obtain a smooth rating functionover the whole graph.
When only lim-ited labeled data is available, this methodachieves significantly better predictive ac-curacy over other methods that ignore theunlabeled examples during training.1 IntroductionSentiment analysis of text documents has receivedconsiderable attention recently (Shanahan et al,2005; Turney, 2002; Dave et al, 2003; Hu andLiu, 2004; Chaovalit and Zhou, 2005).
Unlike tra-ditional text categorization based on topics, senti-ment analysis attempts to identify the subjective sen-timent expressed (or implied) in documents, such asconsumer product or movie reviews.
In particularPang and Lee proposed the rating-inference problem(2005).
Rating inference is harder than binary posi-tive / negative opinion classification.
The goal is toinfer a numerical rating from reviews, for examplethe number of ?stars?
that a critic gave to a movie.Pang and Lee showed that supervised machine learn-ing techniques (classification and regression) workwell for rating inference with large amounts of train-ing data.However, review documents often do not comewith numerical ratings.
We call such documents un-labeled data.
Standard supervised machine learningalgorithms cannot learn from unlabeled data.
As-signing labels can be a slow and expensive processbecause manual inspection and domain expertise areneeded.
Often only a small portion of the documentscan be labeled within resource constraints, so mostdocuments remain unlabeled.
Supervised learningalgorithms trained on small labeled sets suffer inperformance.
Can one use the unlabeled reviews toimprove rating-inference?
Pang and Lee (2005) sug-gested that doing so should be useful.We demonstrate that the answer is ?Yes.?
Ourapproach is graph-based semi-supervised learning.Semi-supervised learning is an active research areain machine learning.
It builds better classifiers orregressors using both labeled and unlabeled data,under appropriate assumptions (Zhu, 2005; Seeger,2001).
This paper contains three contributions:?
We present a novel adaptation of graph-basedsemi-supervised learning (Zhu et al, 2003)45to the sentiment analysis domain, extendingpast supervised learning work by Pang andLee (2005);?
We design a special graph which encodesour assumptions for rating-inference problems(section 2), and present the associated opti-mization problem in section 3;?
We show the benefit of semi-supervised learn-ing for rating inference with extensive experi-mental results in section 4.2 A Graph for Sentiment CategorizationThe semi-supervised rating-inference problem isformalized as follows.
There are n review docu-ments x1 .
.
.
xn, each represented by some standardfeature representation (e.g., word-presence vectors).Without loss of generality, let the first l ?
n doc-uments be labeled with ratings y1 .
.
.
yl ?
C. Theremaining documents are unlabeled.
In our exper-iments, the unlabeled documents are also the testdocuments, a setting known as transduction.
Theset of numerical ratings are C = {c1, .
.
.
, cC}, withc1 < .
.
.
< cC ?
R. For example, a one-star tofour-star movie rating system has C = {0, 1, 2, 3}.We seek a function f : x 7?
R that gives a contin-uous rating f(x) to a document x.
Classification isdone by mapping f(x) to the nearest discrete ratingin C. Note this is ordinal classification, which dif-fers from standard multi-class classification in thatC is endowed with an order.
In the following we use?review?
and ?document,?
?rating?
and ?label?
inter-changeably.We make two assumptions:1.
We are given a similarity measure wij ?
0between documents xi and xj .
wij shouldbe computable from features, so that we canmeasure similarities between any documents,including unlabeled ones.
A large wij im-plies that the two documents tend to expressthe same sentiment (i.e., rating).
We experi-ment with positive-sentence percentage (PSP)based similarity which is proposed in (Pang andLee, 2005), and mutual-information modulatedword-vector cosine similarity.
Details can befound in section 4.2.
Optionally, we are given numerical rating pre-dictions y?l+1, .
.
.
, y?n on the unlabeled doc-uments from a separate learner, for in-stance ?-insensitive support vector regression(Joachims, 1999; Smola and Scho?lkopf, 2004)used by (Pang and Lee, 2005).
This actsas an extra knowledge source for our semi-supervised learning framework to improveupon.
We note our framework is general andworks without the separate learner, too.
(Forthis to work in practice, a reliable similaritymeasure is required.
)We now describe our graph for the semi-supervised rating-inference problem.
We do thispiece by piece with reference to Figure 1.
Our undi-rected graph G = (V,E) has 2n nodes V , andweighted edges E among some of the nodes.?
Each document is a node in the graph (open cir-cles, e.g., xi and xj).
The true ratings of thesenodes f(x) are unobserved.
This is true evenfor the labeled documents because we allow fornoisy labels.
Our goal is to infer f(x) for theunlabeled documents.?
Each labeled document (e.g., xj) is connectedto an observed node (dark circle) whose valueis the given rating yj .
The observed node isa ?dongle?
(Zhu et al, 2003) since it connectsonly to xj .
As we point out later, this servesto pull f(xj) towards yj .
The edge weight be-tween a labeled document and its dongle is alarge number M .
M represents the influenceof yj : if M ?
?
then f(xj) = yj becomes ahard constraint.?
Similarly each unlabeled document (e.g., xi) isalso connected to an observed dongle node y?i,whose value is the prediction of the separatelearner.
Therefore we also require that f(xi)is close to y?i.
This is a way to incorporate mul-tiple learners in general.
We set the weight be-tween an unlabeled node and its dongle arbi-trarily to 1 (the weights are scale-invariant oth-erwise).
As noted earlier, the separate learneris optional: we can remove it and still carry outgraph-based semi-supervised learning.46yi^ xixjyjlabeledreviewsunlabeledreviews1a wijb wijneighborsk?MneighborskFigure 1: The graph for semi-supervised rating in-ference.?
Each unlabeled document xi is connected tokNNL(i), its k nearest labeled documents.Distance is measured by the given similaritymeasure w. We want f(xi) to be consistentwith its similar labeled documents.
The weightbetween xi and xj ?
kNNL(i) is a ?
wij .?
Each unlabeled document is also connected tok?NNU (i), its k?
nearest unlabeled documents(excluding itself).
The weight between xi andxj ?
k?NNU (i) is b ?
wij .
We also wantf(xi) to be consistent with its similar unla-beled neighbors.
We allow potentially differentnumbers of neighbors (k and k?
), and differentweight coefficients (a and b).
These parametersare set by cross validation in experiments.The last two kinds of edges are the key to semi-supervised learning: They connect unobservednodes and force ratings to be smooth throughout thegraph, as we discuss in the next section.3 Graph-Based Semi-Supervised LearningWith the graph defined, there are several algorithmsone can use to carry out semi-supervised learning(Zhu et al, 2003; Delalleau et al, 2005; Joachims,2003; Blum and Chawla, 2001; Belkin et al, 2005).The basic idea is the same and is what we use in thispaper.
That is, our rating function f(x) should besmooth with respect to the graph.
f(x) is not smoothif there is an edge with large weight w betweennodes xi and xj , and the difference between f(xi)and f(xj) is large.
The (un)smoothness over the par-ticular edge can be defined as w(f(xi) ?
f(xj))2.Summing over all edges in the graph, we obtain the(un)smoothness L(f) over the whole graph.
We callL(f) the energy or loss, which should be minimized.Let L = 1 .
.
.
l and U = l + 1 .
.
.
n be labeledand unlabeled review indices, respectively.
With thegraph in Figure 1, the loss L(f) can be written as?i?LM(f(xi)?
yi)2 +?i?U(f(xi)?
y?i)2+?i?U?j?kNNL(i)awij(f(xi)?
f(xj))2+?i?U?j?k?NNU (i)bwij(f(xi)?
f(xj))2.
(1)A small loss implies that the rating of an unlabeledreview is close to its labeled peers as well as its un-labeled peers.
This is how unlabeled data can par-ticipate in learning.
The optimization problem isminf L(f).
To understand the role of the parame-ters, we define ?
= ak + bk?
and ?
= ba , so thatL(f) can be written as?i?LM(f(xi)?
yi)2 +?i?U[(f(xi)?
y?i)2+?k + ?k?
( ?j?kNNL(i)wij(f(xi)?
f(xj))2+?j?k?NNU (i)?wij(f(xi)?
f(xj))2)].
(2)Thus ?
controls the relative weight between labeledneighbors and unlabeled neighbors; ?
is roughlythe relative weight given to semi-supervised (non-dongle) edges.We can find the closed-form solution to the opti-mization problem.
Defining an n?
n matrix W?
,W?ij =??
?0, i ?
Lwij , j ?
kNNL(i)?wij , j ?
k?NNU (i).
(3)Let W = max(W?
, W?>) be a symmetrized versionof this matrix.
Let D be a diagonal degree matrixwithDii =n?j=1Wij .
(4)Note that we define a node?s degree to be the sum ofits edge weights.
Let ?
= D ?W be the combina-torial Laplacian matrix.
Let C be a diagonal dongle47weight matrix withCii ={M, i ?
L1, i ?
U.
(5)Let f = (f(x1), .
.
.
, f(xn))> and y =(y1, .
.
.
, yl, y?l+1, .
.
.
, y?n)>.
We can rewrite L(f) as(f ?
y)>C(f ?
y) +?k + ?k?f>?f .
(6)This is a quadratic function in f .
Setting the gradientto zero, ?L(f)/?f = 0 , we find the minimum lossfunctionf =(C +?k + ?k??)?1Cy.
(7)Because C has strictly positive eigenvalues, the in-verse is well defined.
All our semi-supervised learn-ing experiments use (7) in what follows.Before moving on to experiments, we note aninteresting connection to the supervised learningmethod in (Pang and Lee, 2005), which formulatesrating inference as a metric labeling problem (Klein-berg and Tardos, 2002).
Consider a special case ofour loss function (1) when b = 0 and M ?
?.
Itis easy to show for labeled nodes j ?
L, the opti-mal value is the given label: f(xj) = yj .
Then theoptimization problem decouples into a set of one-dimensional problems, one for each unlabeled nodei ?
U : Lb=0,M??
(f(xi)) =(f(xi)?
y?i)2 +?j?kNNL(i)awij(f(xi)?
yj)2.
(8)The above problem is easy to solve.
It correspondsexactly to the supervised, non-transductive versionof metric labeling, except we use squared differ-ence while (Pang and Lee, 2005) used absolute dif-ference.
Indeed in experiments comparing the two(not reported here), their differences are not statis-tically significant.
From this perspective, our semi-supervised learning method is an extension with in-teracting terms among unlabeled data.4 ExperimentsWe performed experiments using the movie re-view documents and accompanying 4-class (C ={0, 1, 2, 3}) labels found in the ?scale dataset v1.0?available at http://www.cs.cornell.edu/people/pabo/movie-review-data/ and first used in (Pang and Lee,2005).
We chose 4-class instead of 3-class labelingbecause it is harder.
The dataset is divided into fourauthor-specific corpora, containing 1770, 902, 1307,and 1027 documents.
We ran experiments individu-ally for each author.
Each document is representedas a {0, 1} word-presence vector, normalized to sumto 1.We systematically vary labeled set size |L| ?
{0.9n, 800, 400, 200, 100, 50, 25, 12, 6} to observethe effect of semi-supervised learning.
|L| = 0.9nis included to match 10-fold cross validation usedby (Pang and Lee, 2005).
For each |L| we run 20trials where we randomly split the corpus into la-beled and test (unlabeled) sets.
We ensure that allfour classes are represented in each labeled set.
Thesame random splits are used for all methods, allow-ing paired t-tests for statistical significance.
All re-ported results are average test set accuracy.We compare our graph-based semi-supervisedmethod with two previously studied methods: re-gression and metric labeling as in (Pang and Lee,2005).4.1 RegressionWe ran linear ?-insensitive support vector regressionusing Joachims?
SVMlight package (1999) with alldefault parameters.
The continuous prediction on atest document is discretized for classification.
Re-gression results are reported under the heading ?reg.
?Note this method does not use unlabeled data fortraining.4.2 Metric labelingWe ran Pang and Lee?s method based on metric la-beling, using SVM regression as the initial labelpreference function.
The method requires an item-similarity function, which is equivalent to our simi-larity measure wij .
Among others, we experimentedwith PSP-based similarity.
For consistency with(Pang and Lee, 2005), supervised metric labeling re-sults with this measure are reported under ?reg+PSP.
?Note this method does not use unlabeled data fortraining either.PSPi is defined in (Pang and Lee, 2005) as thepercentage of positive sentences in review xi.
Thesimilarity between reviews xi, xj is the cosine angle480 0.2 0.4 0.6 0.8 100.10.20.30.40.50.60.70.8fine?grain ratingmeanandstandarddeviation ofPSPPositive?sentence percentage (PSP) statisticsAuthor (a)Author (b)Author (c)Author (d)Figure 2: PSP for reviews expressing each fine-grainrating.
We identified positive sentences using SVMinstead of Na?
?ve Bayes, but the trend is qualitativelythe same as in (Pang and Lee, 2005).between the vectors (PSPi, 1?PSPi) and (PSPj , 1?PSPj).
Positive sentences are identified using a bi-nary classifier trained on a separate ?snippet dataset?
located at the same URL as above.
The snippetdata set contains 10662 short quotations taken frommovie reviews appearing on the rottentomatoes.comWeb site.
Each snippet is labeled positive or neg-ative based on the rating of the originating review.Pang and Lee (2005) trained a Na?
?ve Bayes classi-fier.
They showed that PSP is a (noisy) measure forcomparing reviews?reviews with low ratings tendto receive low PSP scores, and those with higherratings tend to get high PSP scores.
Thus, two re-views with a high PSP-based similarity are expectedto have similar ratings.
For our experiments we de-rived PSP measurements in a similar manner, but us-ing a linear SVM classifier.
We observed the samerelationship between PSP and ratings (Figure 2).The metric labeling method has parameters(the equivalent of k, ?
in our model).
Pang andLee tuned them on a per-author basis using crossvalidation but did not report the optimal parameters.We were interested in learning a single set ofparameters for use with all authors.
In addition,since we varied labeled set size, it is convenientto tune c = k/|L|, the fraction of labeled reviewsused as neighbors, instead of k. We then usedthe same c, ?
for all authors at all labeled setsizes in experiments involving PSP.
Because c isfixed, k varies directly with |L| (i.e., when lesslabeled data is available, our algorithm considersfewer nearby labeled examples).
In an attempt toreproduce the findings in (Pang and Lee, 2005),we tuned c, ?
with cross validation.
Tuning rangesare c ?
{0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and ?
?
{0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}.The optimal parameters we found are c = 0.2 and?
= 1.5.
(In section 4.4, we discuss an alternativesimilarity measure, for which we re-tuned theseparameters.
)Note that we learned a single set of shared param-eters for all authors, whereas (Pang and Lee, 2005)tuned k and ?
on a per-author basis.
To demonstratethat our implementation of metric labeling producescomparable results, we also determined the optimalauthor-specific parameters.
Table 1 shows the ac-curacy obtained over 20 trials with |L| = 0.9n foreach author, using SVM regression, reg+PSP usingshared c, ?
parameters, and reg+PSP using author-specific c, ?
parameters (listed in parentheses).
Thebest result in each row of the table is highlighted inbold.
We also show in bold any results that cannotbe distinguished from the best result using a pairedt-test at the 0.05 level.
(Pang and Lee, 2005) found that their metric la-beling method, when applied to the 4-class data weare using, was not statistically better than regres-sion, though they observed some improvement forauthors (c) and (d).
Using author-specific parame-ters, we obtained the same qualitative result, but theimprovement for (c) and (d) appears even less sig-nificant in our results.
Possible explanations for thisdifference are the fact that we derived our PSP mea-surements using an SVM classifier instead of an NBclassifier, and that we did not use the same range ofparameters for tuning.
The optimal shared parame-ters produced almost the same results as the optimalauthor-specific parameters, and were used in subse-quent experiments.4.3 Semi-Supervised LearningWe used the same PSP-based similarity measureand the same shared parameters c = 0.2, ?
=1.5 from our metric labeling experiments to per-form graph-based semi-supervised learning.
Theresults are reported as ?SSL+PSP.?
SSL has three49reg+PSP reg+PSPAuthor reg (shared) (specific)(a) 0.592 0.592 0.592 (0.05, 0.01)(b) 0.501 0.498 0.496 (0.05, 3.50)(c) 0.592 0.589 0.593 (0.15, 1.50)(d) 0.496 0.498 0.500 (0.05, 3.00)Table 1: Accuracy using shared (c = 0.2, ?
= 1.5)vs. author-specific parameters, with |L| = 0.9n.additional parameters k?, ?, and M .
Againwe tuned k?, ?
with cross validation.
Tuningranges are k?
?
{2, 3, 5, 10, 20} and ?
?
{0.001, 0.01, 0.1, 1.0, 10.0}.
The optimal parame-ters are k?
= 5 and ?
= 1.0.
These were used for allauthors and for all labeled set sizes.
Note that unlikek = c|L|, which decreases as the labeled set size de-creases, we let k?
remain fixed for all |L|.
We set Marbitrarily to a large number 108 to ensure that theratings of labeled reviews are respected.4.4 Alternate Similarity MeasuresIn addition to using PSP as a similarity measure be-tween reviews, we investigated several alternativesimilarity measures based on the cosine of wordvectors.
Among these options were the cosine be-tween the word vectors used to train the SVM re-gressor, and the cosine between word vectors con-taining only words with high (top 1000 or top 5000)mutual information values.
The mutual informationis computed with respect to the positive and negativeclasses in the 10662-document ?snippet data set.
?Finally, we experimented with using as a similaritymeasure the cosine between word vectors containingall words, each weighted by its mutual information.We found this measure to be the best among the op-tions tested in pilot trial runs using the metric label-ing algorithm.
Specifically, we scaled the mutual in-formation values such that the maximum value wasone.
Then, we used these values as weights for thecorresponding words in the word vectors.
For wordsin the movie review data set that did not appear inthe snippet data set, we used a default weight of zero(i.e., we excluded them.
We experimented with set-ting the default weight to one, but found this led toinferior performance.
)We repeated the experiments described in sec-tions 4.2 and 4.3 with the only difference beingthat we used the mutual-information weighted wordvector similarity instead of PSP whenever a simi-larity measure was required.
We repeated the tun-ing procedures described in the previous sections.Using this new similarity measure led to the opti-mal parameters c = 0.1, ?
= 1.5, k?
= 5, and?
= 10.0.
The results are reported under ?reg+WV?and ?SSL+WV,?
respectively.4.5 ResultsWe tested the five algorithms for all four authors us-ing each of the nine labeled set sizes.
The resultsare presented in table 2.
Each entry in the table rep-resents the average accuracy across 20 trials for anauthor, a labeled set size, and an algorithm.
The bestresult in each row is highlighted in bold.
Any resultson the same row that cannot be distinguished fromthe best result using a paired t-test at the 0.05 levelare also bold.The results indicate that the graph-based semi-supervised learning algorithm based on PSP simi-larity (SSL+PSP) achieved better performance thanall other methods in all four author corpora whenonly 200, 100, 50, 25, or 12 labeled documentswere available.
In 19 out of these 20 learning sce-narios, the unlabeled set accuracy by the SSL+PSPalgorithm was significantly higher than all othermethods.
While accuracy generally degraded as wetrained on less labeled data, the decrease for the SSLapproach was less severe through the mid-range la-beled set sizes.
SSL+PSP remains among the bestmethods with only 6 labeled examples.Note that the SSL algorithm appears to be quitesensitive to the similarity measure used to form thegraph on which it is based.
In the experiments wherewe used mutual-information weighted word vectorsimilarity (reg+WV and SSL+WV), we notice thatreg+WV remained on par with reg+PSP at high la-beled set sizes, whereas SSL+WV appears signif-icantly worse in most of these cases.
It is clearthat PSP is the more reliable similarity measure.SSL uses the similarity measure in more ways thanthe metric labeling approaches (i.e., SSL?s graph isdenser), so it is not surprising that SSL?s accuracywould suffer more with an inferior similarity mea-sure.Unfortunately, our SSL approach did not do aswell with large labeled set sizes.
We believe this50PSP word vector|L| regression reg+PSP SSL+PSP reg+WV SSL+WVAuthor(a)1593 0.592 0.592 0.546 0.592 0.544800 0.553 0.554 0.534 0.553 0.517400 0.522 0.525 0.526 0.522 0.497200 0.494 0.498 0.521 0.494 0.472100 0.463 0.477 0.511 0.462 0.45050 0.439 0.458 0.499 0.438 0.42925 0.408 0.421 0.465 0.400 0.40412 0.401 0.378 0.451 0.335 0.3986 0.390 0.359 0.422 0.314 0.389Author(b)811 0.501 0.498 0.481 0.503 0.473800 0.501 0.497 0.478 0.503 0.474400 0.471 0.471 0.465 0.471 0.450200 0.447 0.449 0.452 0.447 0.429100 0.415 0.423 0.443 0.415 0.39750 0.388 0.396 0.434 0.387 0.37625 0.373 0.380 0.418 0.364 0.36712 0.354 0.360 0.399 0.313 0.3536 0.348 0.352 0.380 0.302 0.347Author(c)1176 0.592 0.589 0.566 0.594 0.514800 0.579 0.585 0.559 0.579 0.509400 0.550 0.556 0.544 0.551 0.491200 0.513 0.519 0.532 0.513 0.479100 0.484 0.495 0.521 0.484 0.46650 0.462 0.476 0.504 0.461 0.45625 0.459 0.472 0.484 0.439 0.45412 0.420 0.405 0.477 0.356 0.4146 0.320 0.382 0.366 0.334 0.322Author(d)924 0.496 0.498 0.495 0.499 0.490800 0.500 0.501 0.495 0.504 0.483400 0.474 0.478 0.486 0.477 0.463200 0.459 0.459 0.468 0.459 0.445100 0.444 0.445 0.460 0.444 0.43750 0.429 0.431 0.445 0.429 0.42825 0.411 0.411 0.425 0.400 0.40912 0.393 0.362 0.405 0.335 0.3916 0.393 0.357 0.403 0.312 0.393Table 2: 20-trial average unlabeled set accuracy for each author across different labeled set sizes and meth-ods.
In each row, we list in bold the best result and any results that cannot be distinguished from it with apaired t-test at the 0.05 level.51is due to two factors: a) the baseline SVM regres-sor trained on a large labeled set can achieve fairlyhigh accuracy for this difficult task without consid-ering pairwise relationships between examples; b)PSP similarity is not accurate enough.
Gain in vari-ance reduction achieved by the SSL graph is offsetby its bias when labeled data is abundant.5 DiscussionWe have demonstrated the benefit of using unla-beled data for rating inference.
There are severaldirections to improve the work: 1.
We will inves-tigate better document representations and similar-ity measures based on parsing and other linguis-tic knowledge, as well as reviews?
sentiment pat-terns.
For example, several positive sentences fol-lowed by a few concluding negative sentences couldindicate an overall negative review, as observed inprior work (Pang and Lee, 2005).
2.
Our methodis transductive: new reviews must be added to thegraph before they can be classified.
We will extendit to the inductive learning setting based on (Sind-hwani et al, 2005).
3.
We plan to experiment withcross-reviewer and cross-domain analysis, such asusing a model learned on movie reviews to help clas-sify product reviews.AcknowledgmentWe thank Bo Pang, Lillian Lee and anonymous re-viewers for helpful comments.ReferencesMikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2005.
On manifold regularization.
In Proceedings ofthe Tenth International Workshop on Artificial Intelli-gence and Statistics (AISTAT 2005).A.
Blum and S. Chawla.
2001.
Learning from labeledand unlabeled data using graph mincuts.
In Proc.
18thInternational Conf.
on Machine Learning.Pimwadee Chaovalit and Lina Zhou.
2005.
Movie re-view mining: a comparison between supervised andunsupervised classification approaches.
In HICSS.IEEE Computer Society.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: opinion extractionand semantic classification of product reviews.
InWWW ?03: Proceedings of the 12th international con-ference on World Wide Web, pages 519?528.Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux.2005.
Efficient non-parametric function induction insemi-supervised learning.
In Proceedings of the TenthInternational Workshop on Artificial Intelligence andStatistics (AISTAT 2005).Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of KDD ?04,the ACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.
ACMPress.T.
Joachims.
1999.
Making large-scale svm learningpractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support VectorLearning.
MIT Press.T.
Joachims.
2003.
Transductive learning via spectralgraph partitioning.
In Proceedings of ICML-03, 20thInternational Conference on Machine Learning.Jon M. Kleinberg and ?Eva Tardos.
2002.
Approxima-tion algorithms for classification problems with pair-wise relationships: metric labeling and markov ran-dom fields.
J. ACM, 49(5):616?639.Bo Pang and Lillian Lee.
2005.
Seeing stars: exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the ACL.Matthias Seeger.
2001.
Learning with labeled and unla-beled data.
Technical report, University of Edinburgh.James Shanahan, Yan Qu, and Janyce Wiebe, editors.2005.
Computing attitude and affect in text.
Springer,Dordrecht, The Netherlands.Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.2005.
Beyond the point cloud: from transductive tosemi-supervised learning.
In ICML05, 22nd Interna-tional Conference on Machine Learning, Bonn, Ger-many.A.
J. Smola and B. Scho?lkopf.
2004.
A tutorial onsupport vector regression.
Statistics and Computing,14:199?222.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of ACL-02, 40th An-nual Meeting of the Association for ComputationalLinguistics, pages 417?424.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using Gaussian fieldsand harmonic functions.
In ICML-03, 20th Interna-tional Conference on Machine Learning.Xiaojin Zhu.
2005.
Semi-supervised learning lit-erature survey.
Technical Report 1530, Com-puter Sciences, University of Wisconsin-Madison.http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.52
