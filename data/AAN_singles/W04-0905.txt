Evaluating the Performance of the OntoSem Semantic AnalyzerSergei NIRENBURG, Stephen BEALE and Marjorie MCSHANEInstitute for Language and Information Technologies (ILIT)University of Maryland Baltimore County1000 Hilltop CircleBaltimore, MD  21250  USAsergei@umbc.edu, sbeale@umbc.edu, marge@umbc.eduAbstractThis paper describes an innovativeevaluation regimen developed for the textmeaning representations (TMRs) producedby the Ontological Semantic (OntoSem)general purpose syntactic-semanticanalyzer.
The goal of evaluation is not onlyto determine the quality of TMRs for giventexts, but also to assign blame for variousclasses of errors, thus suggesting directionsfor continued work on both knowledgeresources and processors.
The paperincludes descriptions of the OntoSemprocessing environment, the evaluationregime itself and results from ur firstevaluation effort.FeO1 IntroductionIn this paper we describe theevaluation regimen for ageneral-purpose syntactic-semantic analyzer, OntoSem,under continuous developmentat the Institute for Languageand Information Technologies(ILIT) of the University ofMaryland Baltimore County.Its top-level architecture isillustrated in Figure 1.
Theknowledge in the factrepository and the ontologyserves not only OntoSem itselfbut also provides a knowledgesubstrate to be used in avariety of reasoningapplications.
At present, theacquisition of the ontology andthe semantic lexicon is carriedout by human acquirers usinginteractive tools.
Theacquisition of the factrepository is mixed, with someof it carried out manually andThe approach to semantic analysis in OntoSem isdescribed in some detail in, e.g., Nirenburg andRaskin 2004, Niresome of it resulting from the opefact extractor on the results of semannburg et al 2003, Beale et alof pre-semantic text processing modules.he preprocessor module deals with mark-up in2003.
Our description here will be necessarilybrief.Text analysis in OntoSem relies on the results of abatteryTthe input text, finds boundaries of sentences andwords, and recognizes dates, numbers, namedentities and acronyms.
Morphological analysisaccepts a string of word forms as input and foreach word form outputs a record containing itscitation form in the lexicon and a set ofmorphological features and their values that corre-spond to the word form from the text.
Once the  oigure 1.
The overall architecture of the OntoSem semantic analyzer.
Thevaluation regimen described in this paper evaluates the production of basic TMRs.ther processing will be evaluated in follow-up work.ration of thetic analysis.morphological analyzer has generated the citationforms for word forms in a text, the system canactivate the relevant lexical entries in its lexicons,including the onomasticon (a lexicon of propernames).
The task of syntactic analysis inontological semantics is, essentially, to determineclause-level dependency structures for an inputtext and assign grammatical categories to clauseconstituents (that is, establish subjects, directobjects, obliques and adjuncts).Semantic analysis proper uses the information(mutual constraints) in the active lexicon entries,the ontology and the results of earlier processing toiAanguage as well as for the specification oflso supports morphological andsy tactic analysis.
Semantically, it specifies whatcarry out, at the first stage, word sensedisambiguation and establish basic semanticdependencies in the text.
The results are recordedvsedasFap(ataThe OntoSem ontology provides a metalanguagefor describing the meaning of the lexical units inlameaning encoded in TMRs.
The ontology containsspecifications of concepts corresponding to classesof things and events in the world.
It is a collectionof frames, or named collections of property-valuepairs, organized into a hierarchy with multipleinheritance.
The expressive power of the ontologyand the TMR is enhanced by multivalued fillers forproperties, implemented using the value ?facets?DEFAULT, SEM, VALUE, and RELAXABLE-TO,among others.
At the time of this writing, theontology contains about 5,500 concepts (events,objects and properties), with, on average, 16properties each.The OntoSem lexicon contains not only semanticinformation, it anconcept, concepts, property or properties ofconcepts defined in the ontology must beinstantiated in the TMR to account for the meaningof a given lexical unit of input.
At the time ofwriting, the latest version of the English semanticlexicon includes over 12,000 handcrafted entries.These entries cover some of the most complexlexical material in the language ?
?closed-class?grammatical lexemes such as conjunctions,prepositions, pronouns, auxiliary and modal verbs,gFigure 2.
Creation of basic TMRs.
The basicsemantic analyzer relies largely on matchinselectional restrictions.
This can lead to incongruitywhen the listed constraints are too strong, or toresidual ambiguity if they are too weak to filter out allbut one candidate.
To resolve these problems,OntoSem uses both static knowledge (multivaluedselectional restrictions and lateral constraints amongco-arguments of a predicate) and context-generatedheuristics, including information in the nascent TMRand measuring distances among any two concepts inthe ontological search space.basic text meaning representations  (TMRs).
nt the next stage, the analyzer determines thee,eech acts, speaker attitudes etc., to produceto thedanger"1 at n4 at n opt +r3 cat ne strNENT)In the lexicon, variables (e.g., $var2) supportsy ?alues of the various modalities, aspect, timpxtended TMRs.
At both stages, the analyzer has toeal with ambiguity, incongruity between the inputnd expectations recorded in the static knowledgeources, unknown words, and non-literal language.igure 2 summarizes the types of heuristics that thenalyzer uses at the first stage.
While all ofrocedures using them have been implementedsee Nirenburg et al 2003), the version of thenalyzer we evaluated involved only a subset ofhem.
We plan to evaluate the analyzer with all thevailable recovery procedures in the near future.etc., as well as about 3,000 of the most frequentverbs.
We illustrate the structure of the lexiconentry on the example of the first verbal sense ofalert (presented in a simplified format):alert-v1example "He alerted usmorph    regularsyn-strucroot    root $var0 cat vsubject root $var croot $var c   objectpp-adjunctroot $var2cat prep root to opt +ect      objroot $vac  s m- ucept   WARN        ;an ontological conagen     t value ^$var1 sem HUMAvalue ^$var3     themebeneficiary value ^$var4instrument value ^$var1sem (orARTIFACT EV^$var2  null-sem +ntax-semantics dependency linking; the caret ?^is read ?the meaning of.?
In this ex ple, am if ^$var1or a descendant of , it occupiestological concepts byrize thear from a recently processed text about ColinACCEPT-70NEFICIARY     ORGANIZATION-71askHOR-TIME))ST-ACTION-69-ON-71ATIONSf.
resolution done-ACTIONA -72 (Colin Powell),whose BENEFICIARY is ORGANIZATION-71 (Unitedbered instances of ontological concepts.
Asdoes not play a significant role in the evaluationean-aided version of theall threenalysis ?put by hand in a textfile.
Preprocessor output is relatively simple toit3.4.
eveloped visual5.6.
analysis.We plan to integrate this capability with ourtoproduce a full-function text processing system.
Theis HUMAN HUMANthe semantic role of AGENT (he alerted us...),whereas if it is ARTIFACT or EVENT (or adescendant of any of those concepts) it isINSTRUMENT (the bell alerted us..., his behavioralerted us...).
For lack of space, we will not be ableto discuss all the representational and descriptivedevices used in the lexicon or the variety of theways in which semantic information in the lexiconand the ontology can interact.
See Nirenburg andRaskin 2004 for discussion.The English Onomasticon (lexicon of propernames) currently contains over 350,000 entries thatare semantically linked to onway of the fact repository.
Onomasticon entries areindexed by name (e.g., New York), while theentries in the fact repository are identified byappending a unique number to the name of theontological concept of which they are instances(e.g., Detroit might be listed as CITY-213).The TMR (automatically generated but shownhere in a simplified presentation format) for thehort sentence He asked the UN to authoswPowell is presented below.
The numbers associatedwith the ontological concepts indicate instances ofthose concepts: e.g., REQUEST-ACTION-69 meansthe 69th time that the concept REQUEST-ACTION hasbeen instantiated in the world model used for, andextended during, the processing of this text orcorpus.REQUEST-ACTION-69AGENT      HUMAN-72THEMEBESOURCE-ROOT-WORDTIME       (< (FIND-ANCACCEPT-70THEME      WAR-73THEME-OF     REQUESOURCE ROOT-WORD   authorizeORGANIZATIHAS-NAME     UNITED-NBENEFICIARY-OF   REQUEST-ACTION-69SOURCE-ROOT-WORD  UNHUMAN-72HAS-NAME    COLIN POWELLAGENT-OF     REQUEST-ACTION-69SOURCE-ROOT-WORD  he ; reWAR-73THEME-OF                ACCEPT-70SOURCE-ROOT-WORD  warThe above says that there is a REQUESTevent whose AGENT is HUM NNations) and whose THEME is ACCEPT.
TheACCEPT event, in turn, has a THEME of WAR-73.Note that the concept ACCEPT is not the same asthe English word accept: its human-orienteddefinition in the ontology is ?To agree to carry outan action, fulfill a request, etc?, which fits wellhere.The Fact Repository contains a list ofrememitregim n reported in this paper, we will provide nofurther description here.2 Generating Gold Standard TMRsWe have developed a humOntoSem analyzer in which the results ofmajor stages of ontological semantic apreprocessor output, syntax output and semanticoutput ?
can be inspected and corrected by ahuman.
For purposes of evaluation, we have used itto produce gold standard (GS) outputs for each ofthe three stages.
The production of gold standardoutputs proceeds as follows:1.
Run the OntoSem analyzer on an input text.2.
Correct preprocessor outread in text format, and we have foundquickest to simply correct it by hand.
It takeson average 1 minute to correct an average-length (> 25 words) sentence.Input the corrected preprocessor results intothe analyzer and produce a syntactic analysis.If necessary, use a specially dediting interface to add or delete edges on thechart that presents the results of syntacticanalysis, to remove spurious parses, to correctphrase and clause boundaries, and to add anymissing phrase or clause parses.Feed the correct syntax back into the analyzerand obtain a semantic analysis.If necessary, correct the semanticknowledge acquisition interfaces in orderside effects of this process will include the creationof a bank of gold standard TMRs as well as,possibly, less importantly, gold standard results ofpreprocessing and syntactic analysis.
Suchresources are clearly valuable as training data forstatistical NLP, and a number of projects aredevoted to entirely or in a large measure to theircreation.
The process of producing gold standardTMRs, unlike most of the resource acquisitionapproaches, is, to a significant degree, automated ?which reduces the incidence of interannotatordisagreement and generally makes the processfaster and cheaper.In the OntoSem research paradigm, knowledgeacquisition (enhancement of the ontology, thexicon and other basic static knowledge sources)ms that do not involve knowledge acquisitionf the kind OntoSem uses.
However, the set ofthe only one we considerractical.
It is not possible for a human to produceluate the results of fullyutomatic analysis (see below); as training data forauto put  we evaluate several?
baseline 1: same as above, except we force thefirst senses in our lexicon entries are?d?analyzer;put to theForprep syntax results; semanticsresults and evaluation results The evaluation isdard outputs and include a) theord/phrase count; b) the number of input wordsleis an ongoing process.
The process of creating goldstandard TMRs provides an empirical impetus forknowledge acquisition.
This process will not at allinterfere with our evaluation regimen because ourapproach does not rely on having a standard testcorpus.
We will simply run the entire evaluationprocedure (starting with the production of the goldstandard TMRs) on a new corpus, analyze theresults and move on to yet another corpus, and soon.This approach cannot be directly exported to thosesysteogold standard TMRs produced through ourevaluation process will be made freely availableand can serve as the test corpus for any othersemantic analyzer (word sense disambiguatorand/or semantic dependency extractor).
This willbe our direct contribution to the resource set in thefield.
Of course, using this resource will involveresolving the differences in the notation andsemantics between the TMR structures and anyother metalanguage.This methodology for producing gold standardsemantic outputs ispgold standard semantic outputs by hand because ofthe complexity of the knowledge, as well as thehigh probability of annotator disagreement due tovalid semantic paraphrasing (e.g., one annotatormight describe the meaning of weapons of massdestruction as the union of BIOLOGICAL-WEAPONand CHEMICAL-WEAPON, whereas another mightdescribe it as WEAPON that has the potential to killmore than 10,000 people).In sum, gold standard  outputs are used for anumber of purposes: to evaamachine learning, with the goal of improving thesystem?s static knowledge sources; to triggermanual acquisition of knowledge for lacunae; or toderive high-confidence TMRs for use in mininginformation for a fact repository.
Last but not least,the gold standard TMRs produced according to ourmethodology can also be directly used in a varietyof applications ?
from human-assisted knowledge-based MT to knowledge acquisition for general-purpose reasoning systems.3 Automated Evaluation of OntologicalSemantic AnalysesOnce the gold standard TMRs are produced, theevaluation of OntoSem proceeds fullymatically.
For each in?runs?
as follows:?
as is: we simply input the text and evaluate theoutputs;analyzer to use the first lexical sense of eachword; thetypically the most central and frequent ones;baseline 2: same as baseline 1, except we usethe first sense that has the correct part ofspeech (as specified in the gold standarpreprocessor results);correct preprocessor output: we use the goldstandard preprocessor output as input to thesyntactic and semantic?
corrected syntax output: we use the goldstandard syntax (and gold standardpreprocessor output) as the insemantic analyzer.each run, we produce four output files:rocessor results;performed by automatically comparing the actualpreprocessor, syntax or semantic results to thecorresponding gold standard outputs.
Theevaluation produces statistics and/or measurementsas follows.General text-level statistics are collected from thegolden stanwthat are not in the OntoSem lexicon; c) thesyntactic ambiguity count, which is the number ofphrases and clauses in the syntactic output; d) thesemantic ambiguity count, which is the product ofthe number of senses of each word, which providesan estimate of the overall theoretical complexity ofsemantic analysis; and e) the word sense ambiguitycount, which is the number of semanticcombinations the analyzer actually needed toexamine to produce the result; this numberprovides an estimate for the actual complexity ofsemantic analysis: syntactic clues often help prunemany spurious analyses and the efficient semanticanalysis algorithm (Beale, et.
al.
1995) reduces thetotal number of combinations that have to beexamined while maintaining accuracy.For this evaluation, the lexicon provided almostcomplete lexical coverage of the input texts (in factonly one word was missing).
We will use theareollected for each evaluation run.ches between anctual run and the gold standard, n is the number ofofhrases, and phrase attachment.turned for eachphrase, with 1.0 reflecting a perfect match.1gstart is the gold standard word numberber at the start of the phrase beingb)ach phrase in thegold standard syntax, it is determined if thereexists a phrase with the same part of speechc)ure looks for a phrase thatoverlaps with it that has the same part ofA sasAna ll Score is then the average score of, b and c.(SD) determination.
For WSD, threemeasures are computed.on is marked with theword number from the input text from which itB)en 0.0 and 1.0 is returned.A mismatch of a word with more senses isC)isontologically ?close?
to the correct sense isresults of this first evaluation as a baseline forfuture evaluation of the degradation of the resultsdue to incompleteness of the static knowledge.Results from the operation of the preprocessor,syntactic analysis and semantic analysiscThe preprocessor statistics are recorded asfollows (m is the number of matamismatches): a) abbreviations, time, date andnumber recognition (m/n); b) named entityrecognition (m/n); c) part of speech tagging (m/n).The overall score of the preprocessor is calculatedas the average of m/m+n for all three measures.Syntactic analysis statistics measure the qualityof the determination of phrase boundaries, headspa) For phrase boundaries, an overall scorebetween 0.0 and 1.0 is reEach phrase in the gold standard syntax outputis compared to its closest match in the outputunder consideration.The output phrase that hasthe same label (NP, CL, etc.
), the same headword, and the closest matching starting andending points is used for the comparison.
Eachphrase is given the score:- (|gstart - start| + |gend - end|)/(gend - gstart)whereat the start of the phrase and start is the wordnumevaluated.
Thus, if the gold standard phrasebegan at word 10 and ended at word 16, and theclosest matching phrase in the output beingevaluated began at word 9 and ended at word17, then the score for this phrase would be 1 -(|10 - 9| + |16 - 17|) / (16 - 10) = (1 - (2 / 6)) =2/3.
If no matching phrase could be found (i.e.no overlapping phrase could be found with thesame phrase label and head word), then a scoreof 0.0 is assigned.
The score for the wholesentence under evaluation is the average of thescores for each of the phrases.For phrase head determination, the standard(m/n) measure is used.
For eand head word that overlaps with the goldstandard phrase.Attachment is also measured as (m/n).
Foreach phrase in the gold standard syntax, theevaluation procedspeech, the same head word and the sameconstituents.
For example, if the gold standardoutput has a PP attached to a NP, it will beshown to be a constituent of that NP.
If theoutput being evaluated attaches the PP at adifferent constituent, then a mismatch will beidentified.core between 0.0 and 1.0 is assigned for b and cfollows: Score = m/(m+n).
The Syntacticlysis OveraaSemantic analysis statistics measure the qualityof word sense disambiguation (WSD) and semanticdependencyA) First, the standard match/mismatch (m/n) isused.
Each TMR element in the gold standardsemantic representatiarose.
The TMR element in the semanticrepresentation being evaluated thatcorresponds to that same word number is thencompared with it.Second, the evaluation system produces aweighted score for WSD complexity.
Anoverall score betwepenalized less than a mismatch of a word withfewer senses.
The score for each mismatch is 1- (2 / number-of-senses), if the word has morethan 2 senses, and 0.0 if it has less than orequal to 2 senses.
An exact match is given ascore of 1.0.
The overall score for the sentenceis the average score for each TMR element.The system also computes a weighted score forWSD ?distance.?
An overall score between 0.0and 1.0 is returned.
A mismatch thatpenalized less than a mismatch that isontologically ?far?
from the correct semantics.The ontological distance is computed using theOntosearch algorithm (Onyshkevych 1997)that returns a score between 0.0 and 1.0reflecting how close the two concepts are inthe ontology, with a score of 1.0 indicating aExample Semantic Evaluation  perfect match.
The overall score for thesentence is the average score of each TMRelement.The quality of semantic dependencydetermination is computed using the standard(m/n) measurWe will now exemplify the evaluation of thesemantic analysis of the sample sentence in 1:D)e. Each TMR element in the goldstandard is compared to the corresponding1.
Hall is scheduled to embark on the 12 houroverland trip to the Iraqi capital, Baghdad.Figure 4.
Gold Standard Syntactic Analysis for a sample sentence.TMR element in the semantics beingevaluated.
Each property modifying the goldstandard TMR element that is also in theevaluation TMR element increments the mcount, each property in the gold standard TMRelement that is not in the evaluation TMRelement increments the n count.
The fillers ofmatching properties are also compared.
If thefiller of the gold standard property is anotherTMR element (as opposed to being a literal),then the filler is also matched against thecorresponding filler in the semanticrepresentation being evaluated, incrementingthe m and n counters as appropriate.
Therelations between TMR elements is one of thecentral aspects of Ontological Semantics whichgoes beyond simple word sensedisambiguation.
This score reflects how wellthe dependency determination was performed.The analyzer produces the syntactic analysisshown in Figure 3.
This analysis contains manyspurious parses (along with the correct ones).
Thegold standard parse of this sentence is shown inFigure 4.
The illustrations are difficult to read butthe number of edges can be visually compared.In order to make an interesting evaluation example,we forced the semantic analyzer to misinterpretcapital.
The analyzer actually chose the correctsense, CAPITAL-CITY, but here we will force it toselect the monetary sense, CAPITAL.We will now demonstrate the calculation andsignificance of the semantic evaluation parameters.A) Match/mismatch of TMR elements.
In thisexample, there will be six matches and onemismatch ?
the CAPITAL concept that shouldbe CAPITAL-CITY.
A score of 6/7 = 0.86 is alsocalculated for use in the overall semanticscore.B) Weighted score for WSD complexity.
Theword capital has three senses in our Englishlexicon, corresponding to the CAPITAL-CITY,CAPITAL (i.e.
monetary) and CAPITAL-EQUIPMENT meanings.
It will receive a scoreof 1 - 2/number-of-senses = 1 - 2/3 = 0.33.
Ifthere were two or less senses, it would havereceived a score of 0.0.
If there were manysenses of capital, its score would have beenhigher, reflecting the fact that there was a morecomplex disambiguation problem.
The othersix TMR elements receive a score of 1.0.
Thetotal score for the sentence is therefore 6.33/7=0.90.Figure 3: Syntactic Analysis of Sample TextA normalized score between 0.0 and 1.0 iscalculated for a and d as follows: Score =m/(m+n).C) Weighted score for WSD distance.
Wedetermine the distance between the choseneaning,-CITY, by submitting the concept pair(ontosearch capital capital-city) ?
0.525CTEEDDEED   IS-A   DOCUMENTd toonnect the two concepts reflects this.
So the scorelysis of capital.
In other cases, mismatcheddependencies can arise by incorrect linkingA scoreOur first evaluation run returned the resultssummarized in Tables 1 and 2.
The motivation forwas given infuture evaluations, forstance, by using the corresponding componentsof the Stanford Lexicalized Parser (accessible fromhttp://nl u/).meaning, CAPITAL, and the correct mCAPITALto Ontosearch:PATH:CAPITAL   IS-A   FINANCIAL-OBJEFINANCIAL-OBJECT   SUBCLASSES   DDOCUMENT   PRODUCED-BY   NATIONNATION   LOCATION-OF   CITYCITY   SUBCLASSES   CAPITAL-CITYOntosearch returns a score between 0.0 and 1.0reflecting the closeness of the two concepts.
Anexact match would return a score of 1.0.Ontosearch also returns the path traversed to linkthe two concepts.
In this case, the score returned isrelatively low, and the ?strange?
path needecfor this TMR element is 0.52.
The other TMRelements in the sentence all receive a score of 1.0,so the score for the sentences is 6.52/7 = 0.93.D.
Semantic dependency determination.
In theexample input, there are six links between TMRelements.
Thus, the instance of SCHEDULE-EVENThas as its THEME the instance of TRAVEL-EVENT,which has an instance of CAPITAL as itsDESTINATION, an instance of HUMAN as its AGENTand an instance of HOUR as its DURATION.
CAPITALis linked to NATION and CITY.
Each link is checkedagainst the gold standard.
In this case, all six linksmatch.
This increments the dependecy matchcounter by  six.
The fillers of the link, i.e.
the TMRelement that it points to, are also checked.
For thisexample, the DESTINATION of the TRAVEL- EVENTshould be CAPITAL-CITY, but it is CAPITAL.
Thisincrements the mismatch counter by one.
The otherfive fillers match with the gold standard, thus thematch counter is incremented by 5.
For the wholesentence, the dependency matches will be 11 andthe mismatches will be 1.
In this case, themismatched dependency was caused by themisanabetween syntactic and semantic structures.of 11/12 = 0.92 is calculated for use in the overallscore.4 Results of the First Evaluation Runthe different statistics and runsSection 3.5 Discussion and Future WorkThe kind of evaluation that we have undertaken sofar reflects our desire to understand the causes ofless-than-maximum results, that is, to assign blameto the various components of the analyzer.
Theresults clearly show that the preprocessor we haveso far been using in the OntoSem system does notperform sufficiently well, and we will change thepreprocessor for theinp.stanford.edword count 204sense count 604syntactic  ambiguity 192semantic  ambiguity   1.9 x 1017word sense ambiguity   4 .8 8  x 10rmine their relativetility and contributions to the quality of semanticrates on selectionalTable 1.
The general statistics for thefirst evaluation run of OntoSemOur WSD evaluation environment differs frommany WSD approaches in that it allows the ?noneof the above?
outcome for the cases when thelexicon entries do not fit the expectations in thetext even after a measure of constraint relaxation.The count of incorrectly determined word sensesincludes the above eventuality but also the casewhen the current system has to select an answerfrom a set of candidates none of which can bepreferred on the basis of available heuristics.
Forfuture evaluations, we plan to use the version ofthe analyzer with additional available means ofambiguity resolution incorporated (see Figure 2 fora brief listing).
In fact, we will use differentcombinations of the procedures for residualambiguity resolution and recovery from?unexpected?
input to deteuanalysis (not only WSD but also semanticdependency determination).The evaluation of semantic dependencydetermination is different from that suggested byGildea and Jurafsky (2002) who designed a systemto automatically learn the semantic roles ofunknown predicates.
First, that system does notactually do WSD; second, it makes assumptionsthat our work does not: it does not use anylanguage-independent metalanguage to recordmeaning and concentrestrictions, a far more limited inventory than theset of all possible relations between conceptsprovided in our ontology.The evaluation environment we have developedreduces the amount of time necessary to produce asense that it is a very important enabling elementfor larger-scale evaluation work that from thispoint on will become standard procgold standard output for each of the three stages ofour analysis process quite dramatically.
It is in thisedure in ourwork on building semantic analyzersBaselineABaselineB As IsCorrectPreprocessorCorrectSyntaxAbbreviations, numbers, etc.
3/2 3/2 3/2 5/0 5/0Named entities 14/10 14/10 14/10 24/0 24/0Parts of Speech 121/83 121/83 121/83 204/0 204/0Preprocessor Total 0.59 0.59 0.59 1.0 1.0Phrase boundary score 0.81 0.8 0.91 0.97 1.0Phrase heads 129/48 127/50 159/25 180/12 182/0Attachments 86/38 87/37 100/53 166/15 `81/0Syntax Total 0.74 0.77 0.81 0.94 1.0WSD 57/54 59/52 63/48 86/25 98/15WSD complexity 0.61 0.62 0.64 0.85 0.96WSD distance 0.79 0.80 0.83 0.92 0.96Semantic dependencies 104/182 113/173 136/150 198/88 229/43Table 2.
Results of the initial evaluation of the OntoSem semantic analyzer.Referencesephen Beale, Sergei Nirenburg and MarjorieMcShane.
2003.
Just-in-time grammar.Proceedings of the 2003 InternationStalGi y.
2002.
AutomatedSeings of HLT-NAACL-03Se ictor Raskin.
2004Onr knowledge-basedtext processing.
Unpublished PhD Dissertation.Carnegie Mellon University.Multiconference in Computer Science andComputer Engineering, Las Vegas, Nevada.ldea, Dan and Dan Jurafsklabeling of semantic roles.
ComputationalLinguistics 28(3): 245-288rgei Nirenburg, Marjorie McShane and StephenBeale.
2003.
Operative strategies in OntologicalSemantics.
ProceedWorkshop on Text Meaning, Edmonton, Alberta,Canada, June 2003.rgei Nirenburg and V(forthcoming).
Ontological Semantics, the MITPress, Cambridge, Mass.yshkevych, Boyan 1997.
Ontosearch: Using anontology as a search space fo
