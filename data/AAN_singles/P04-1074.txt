Applying Machine Learning to Chinese Temporal Relation ResolutionWenjie LiDepartment of ComputingThe Hong Kong Polytechnic University, Hong Kongcswjli@comp.polyu.edu.hkKam-Fai WongDepartment of Systems Engineering and EngineeringManagementThe Chinese University of Hong Kong, Hong Kongkfwong@se.cuhk.edu.hkGuihong CaoDepartment of ComputingThe Hong Kong Polytechnic University, Hong Kongcsghcao@comp.polyu.edu.hkChunfa YuanDepartment of Computer Science and TechnologyTsinghua University, Beijing, China.cfyuan@tsinghua.edu.cnAbstractTemporal relation resolution involves extractionof temporal information explicitly or implicitlyembedded in a language.
This information is of-ten inferred from a variety of interactive gram-matical and lexical cues, especially in Chinese.For this purpose, inter-clause relations (tempo-ral or otherwise) in a multiple-clause sentenceplay an important role.
In this paper, a computa-tional model based on machine learning andheterogeneous collaborative bootstrapping isproposed for analyzing temporal relations in aChinese multiple-clause sentence.
The modelmakes use of the fact that events are representedin different temporal structures.
It takes into ac-count the effects of linguistic features such astense/aspect, temporal connectives, and dis-course structures.
A set of experiments has beenconducted to investigate how linguistic featurescould affect temporal relation resolution.1 IntroductionIn language studies, temporal information de-scribes changes and time of changes expressed in alanguage.
Such information is critical in many typi-cal natural language processing (NLP) applications,e.g.
language generation and machine translation, etc.Modeling temporal aspects of an event in a writtentext is more complex than capturing time in a physi-cal time-stamped system.
Event time may be speci-fied explicitly in a sentence, e.g.
????
1997???????????
(They solved the traffic prob-lem of the city in 1997)?
; or it may be left implicit, tobe recovered by readers from context.
For example,one may know that ?????????????????????
(after the street bridge had been built,they solved the traffic problem of the city)?, yetwithout knowing the exact time when the streetbridge was built.
As reported by Partee (Partee,1984), the expression of relative temporal relationsin which precise times are not stated is common innatural language.
The objective of relative temporalrelation resolution is to determine the type of rela-tive relation embedded in a sentence.In English, temporal expressions have beenwidely studied.
Lascarides and Asher (Lascarides,Asher and Oberlander, 1992) suggested that tempo-ral relations between two events followed from dis-course structures.
They investigated variouscontextual effects on five discourse relations(namely narration, elaboration, explanation, back-ground and result) and then corresponded each ofthem to a kind of temporal relations.
Hitzeman et al(Hitzeman, Moens and Grover, 1995) described amethod for analyzing temporal structure of a dis-course by taking into account the effects of tense,aspect, temporal adverbials and rhetorical relations(e.g.
causation and elaboration) on temporal order-ing.
They argued that rhetorical relations could befurther constrained by event temporal classification.Later, Dorr and Gaasterland (Dorr and Gaasterland,2002) developed a constraint-based approach togenerate sentences, which reflect temporal relations,by making appropriate selections of tense, aspectand connecting words (e.g.
before, after and when).Their works, however, are theoretical in nature andhave not investigated computational aspects.The pioneer work on Chinese temporal relationextraction was first reported by Li and Wong (Li andWong, 2002).
To discover temporal relations em-bedded in a sentence, they devised a set of simplerules to map the combined effects of temporal indi-cators, which are gathered from different grammati-cal categories, to their corresponding relations.However, their work did not focus on relative tem-poral relations.
Given a sentence describing twotemporally related events, Li and Wong only tookthe temporal position words (including before, afterand when, which serve as temporal connectives) andthe tense/aspect markers of the second event intoconsideration.
The proposed rule-based approachwas simple; but it suffered from low coverage andwas particularly ineffective when the interaction be-tween the linguistic elements was unclear.This paper studies how linguistic features in Chi-nese interact to influence relative relation resolution.For this purpose, statistics-based machine learningapproaches are applied.
The remainder of the paperis structured as follows: Section 2 summarizes thelinguistic features, which must be taken into accountin temporal relation resolution, and introduces howthese features are expressed in Chinese.
In Section 3,the proposed machine learning algorithms to identifytemporal relations are outlined; furthermore, a het-erogeneous collaborative bootstrapping techniquefor smoothing is presented.
Experiments designedfor studying the impact of different approaches andlinguistic features are described in Section 4.
Finally,Section 5 concludes the paper.2 Modeling Temporal Relations2.1 Temporal Relation RepresentationsAs the importance of temporal information proc-essing has become apparent, a variety of temporalsystems have been introduced, attempting to ac-commodate the characteristics of relative temporalinformation.
Among those who worked on temporalrelation representations, many took the work of Rei-chenbach (Reichenbach, 1947) as a starting point,while some others based their works on Allen?s (Al-len, 1981).Reichenbach proposed a point-based temporaltheory.
This was later enhanced by Bruce who de-fined seven relative temporal relations (Bruce.
1972).Given two durative events, the interval relations be-tween them were modeled by the order between thegreatest lower bounding points and least upperbounding points of the two events.
In the other camp,instead of adopting time points, Allen took intervalsas temporal primitives and introduced thirteen basicbinary relations.
In this interval-based theory, pointsare relegated to a subsidiary status as ?meetingplaces?
of intervals.
An extension to Allen?s theory,which treated both points and intervals as primitiveson an equal footing, was later investigated by Maand Knight (Ma and Knight, 1994).In natural language, events can either be punctual(e.g.
??
(explore)) or durative (e.g.
??
(built ahouse)) in nature.
Thus Ma and Knight?s model isadopted in our work (see Figure 1).
Taking the sen-tence ?????????????????????
(after the street bridge had been built, they solvedthe traffic problem of the city)?
as an example, therelation held between building the bridge (i.e.
aninterval) and solving the problem (i.e.
a point) isBEFORE.Figure 1.
Thirteen temporal relations between points andintervals2.2 Linguistic Features for Determining RelativeRelationsRelative relations are generally determined bytense/aspect, connecting words (temporal or other-wise) and event classes.Tense/Aspect in English is manifested by verb in-flections.
But such morphological variations are in-applicable to Chinese verbs; instead, they areconveyed lexically (Li and Wong, 2002).
In otherwords, tense and aspect in Chinese are expressedusing a combination of time words, auxiliaries, tem-poral position words, adverbs and prepositions, andparticular verbs.Temporal Connectives in English primarily in-volve conjunctions, e.g.
after, before and when (Dorrand Gaasterland, 2002).
They are key components indiscourse structures.
In Chinese, however, conjunc-tions, conjunctive adverbs, prepositions and positionwords are required to represent connectives.
A fewverbs which express cause and effect also imply aforward movement of event time.
The words, whichcontribute to the tense/aspect and temporal connec-tive expressions, are explicit in a sentence and gen-erally known as Temporal Indicators.Event Class is implicit in a sentence.
Events canbe classified according to their inherent temporalcharacteristics, such as the degree of telicity and/oratomicity (Li and Wong, 2002).
The four widespreadaccepted temporal classes1 are state, process, punc-tual event and developing event.
Based on theirclasses, events interact with the tense/aspect of verbsto define the temporal relations between two events.Temporal indicators and event classes are togetherreferred to as Linguistic Features (see Table 1).
Forexample, linguistic features are underlined in thesentence ?(??)?????(??)?????????????
after/because the street bridge had beenbuilt (i.e.
a developing event), they solved the trafficproblem of the city (i.e.
a punctual event)?.1 Temporal classification refers to aspectual classification.A punctual event (i.e.
represented in time point)A durative event (i.e.
represented in time interval)BEFORE/AFTERMEETS/MET-BYOVERLAPS/OVERLAPPED-BYSTARTS/STARTED-BYDURING/CONTAINSFINISHES/FINISHED-BYSAME-ASTable 1 shows the mapping between a temporalindicator and its effects.
Notice that the mapping isnot one-to-one.
For example, adverbs affecttense/aspect as well as discourse structure.
For an-other example, tense/aspect can be affected by auxil-iary words, trend verbs, etc.
This shows thatclassification of temporal indicators based on part-of-speech (POS) information alone cannot determinerelative temporal relations.3 Machine Learning Approaches for RelativeRelation ResolutionPrevious efforts in corpus-based natural languageprocessing have incorporated machine learningmethods to coordinate multiple linguistic featuresfor example in accent restoration (Yarowsky, 1994)and event classification (Siegel and McKeown,1998), etc.Relative relation resolution can be modeled as arelation classification task.
We model the thirteenrelative temporal relations (see Figure 1) as theclasses to be decided by a classifier.
The resolutionprocess is to assign an event pair (i.e.
the two eventsunder concern)2 to one class according to their lin-guistic features.
For this purpose, we train two clas-sifiers, a Probabilistic Decision Tree Classifier(PDT) and a Na?ve Bayesian Classifier (NBC).
Wethen combine the results by the Collaborative Boot-strapping (CB) technique which is used to mediatethe sparse data problem arose due to the limitednumber of training cases.2 It is an object in machine learning algorithms.3.1 Probabilistic Decision Tree (PDT)Due to two domain-specific characteristics, weencounter some difficulties in classification.
(a) Un-known values are common, for many events aremodified by less than three linguistic features.
(b)Both training and testing data are noisy.
For this rea-son, it is impossible to obtain a tree which can com-pletely classify all training examples.
To overcomethis predicament, we aim to obtain more adjustedprobability distributions of event pairs over theirpossible classes.
Therefore, a probabilistic decisiontree approach is preferred over conventional deci-sion tree approaches (e.g.
C4.5, ID3).
We adopt anon-incremental supervised learning algorithm inTDIDT (Top Down Induction of Decision Trees)family.
It constructs a tree top-down and the processis guided by distributional information learned fromexamples (Quinlan, 1993).3.1.1 Parameter EstimationBased on probabilities, each object in the PDT ap-proach can belong to a number of classes.
Theseprobabilities could be estimated from training caseswith Maximum Likelihood Estimation (MLE).
Let lbe the decision sequence, z the object and c the class.The probability of z belonging to c is:??
?=llzlplcpzclpzcp )|()|()|,()|(  (1)let nBBBl ...21= , by MLE we have:)(),()|()|(nnn BfBcfBcplcp =?
(2)),( nBcf  is the count of the items whose leaf nodesare Bn and belonging to class c. AndLinguistic Feature Symbol POS Tag Effect ExampleWith/WithoutpunctuationsPT Not Applica-bleNot Applicable Not ApplicableSpeech verbs VS TI_vs Tense ?
?, ?
?, ?Trend verbs TR TI_tr Aspect ?
?, ?
?Preposition words P TI_p Discourse Structure/Aspect ?, ?, ?Position words PS TI_f Discourse Structure ?, ?, ?
?Verbs with verbobjectsVV TI_vv Tense/Aspect ?
?, ?
?, ?Verbs expressingwish/hopeVA TI_va Tense ?
?, ?, ?Verbs related tocausalityVC TI_vc Discourse Structure ?
?, ?
?, ?
?Conjunctive words C TI_c Discourse Structure ?, ?
?, ?
?Auxiliary words U TI_u Aspect ?, ?, ?Time words T TI_t Tense ?
?, ?
?, ?
?Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ?
?, ?Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event,Developing Event,ProcessTable 1.
Linguistic features: eleven temporal indicators and one event class),...|(...),,|(),|()|()|(11213121zBBBpzBBBpzBBpzBpzlpnn ?=(3)where)|...()|...(),...|(121121121 zBBBpzBBBBpzBBBBpmmmmmmmm??????
=)|...()|...(121121zBBBfzBBBBfmmmmm???
?= , ( nm ,...,3,2= ).An object might traverse more than one decisionpath if it has unknown attribute values.)|...
( 121 zBBBBf mmm ??
is the count of the item z,which owns the decision paths from B1 to Bm.3.1.2 Classification AttributesObjects are classified into classes based on theirattributes.
In the context of temporal relation resolu-tion, how to categorize linguistic features into classi-fication attributes is a major design issue.
We extractall temporal indicators surrounding an event.
As-sume m and n are the anterior and posterior windowsize.
They represent the numbers of the indicatorsBEFORE and AFTER respectively.
Consider themost extreme case where an event consists of atmost 4 temporal indicators before and 2 after.
Weset m and n to 4 and 2 initially.
Experiments showthat learning performance drops when m>4 and n>2and there is only very little difference otherwise (i.e.when m?4 and n?2).In addition to temporal indicators alone, the posi-tion of the punctuation mark separating the twoclauses describing the events and the classes of theevents are also useful classification attributes.
Wewill outline why this is so in Section 4.1.
Altogether,the following 15 attributes are used to train the PDTand NBC classifiers:,,),(,,,, 211111213141 1rerelelelele TITIeclassTITITITI221212223242,),(,,,,, / 2rerelelelele TITIeclassTITITITIpuncwowili (i=1,2,3,4) and rj (j=1,2) are the ith indictor beforeand the jth indicator after the event ek (k=1,2).
Givena sentence, for example, ?/TI_d ?/E0 ?/TI_u ?
?/n ?/w ?/TI_d ?/E2 ?/TI_u ?
?/n ?/w, the at-tribute vector could be represented as: [0, 0, 0, ?,E0, ?, 0, 1, 0, 0, 0, ?, E2, ?, 0].3.1.3 Attribute Selection FunctionMany similar attribute selection functions wereused to construct a decision tree (Marquez, 2000).These included information gain and informationgain ratio (Quinlan, 1993), 2?
Test and SymmetricalTau (Zhou and Dillon, 1991).
We adopt the one pro-posed by Lopez de Mantaraz (Mantaras, 1991) for itshows more stable performance than Quinlan?sinformation gain ratio in our experiments.
Comparedwith Quinlan?s information gain ratio, Lopez?s dis-tance-based measurement is unbiased towards theattributes with a large number of values and is capa-ble of generating smaller trees with no loss of accu-racy (Marquez, Padro and Rodriguez, 2000).
Thischaracteristic makes it an ideal choice for our work,where most attributes have more than 200 values.3.2 Na?ve Bayesian Classifier (NBC)NBC assumes independence among features.Given the class label c, NBC learns from trainingdata the conditional probability of each attribute Ai(see Section 3.1.2).
Classification is then performedby applying Bayes rule to compute the probability ofc given the particular instance of A1,?,An, and thenpredicting the class with the highest posterior prob-ability ratio.
),...,,,|(maxarg 321*ncAAAAcscorec =  (4)),...,,,|(),...,,,|(),...,,,|(321321321nnn AAAAcpAAAAcpAAAAcscore =  (5)Apply Bayesian rule to (5), we have:),...,,,|(),...,,,|(),...,,,|(321321321nnn AAAAcpAAAAcpAAAAcscore =)()|,...,,,()()|,...,,,(321321cpcAAAApcpcAAAApnn=)()|()()|(11cpcApcpcApniinii??==?
(6))|( cAp i and )|( cAp i  are estimated by MLE fromtraining data with Dirichlet Smoothing method:?=?++= njjiinucAcucAccAp1),(),()|(   (7)?=?++= njjiinucAcucAccAp1),(),()|(   (8)3.3 Collaborative Bootstrapping (CB)PDT and NB are both supervised learning ap-proach.
Thus, the training processes require manylabeled cases.
Recent results (Blum and Mitchell,1998; Collins, 1999) have suggested that unlabeleddata could also be used effectively to reduce theamount of labeled data by taking advantage of col-laborative bootstrapping (CB) techniques.
In previ-ous works, CB trained two homogeneous classifiersbased on different independent feature spaces.
How-ever, this approach is not applicable to our worksince only a few temporal indicators occur in eachcase.
Therefore, we develop an alternative CB algo-rithm, i.e.
to train two different classifiers based onthe same feature spaces.
PDT (a non-linear classifier)and NBC (a linear classifier) are under consideration.This is inspired by Blum and Mitchell?s theory thattwo collaborative classifiers should be conditionallyindependent so that each classifier can make its owncontribution (Blum and Mitchell, 1998).
The learn-ing steps are outlined in Figure 2.Inputs: A collection of the labeled cases and unla-beled cases is prepared.
The labeled casesare separated into three parts, trainingcases, test cases and held-out cases.Loop: While the breaking criteria is not satisfied1 Build the PDT and NBC classifiers us-ing training cases2 Use PDT and NBC to classify the unla-beled cases, and exchange with the se-lected cases which have higherClassification Confidence (i.e.
the un-certainty is less than a threshold).3 Evaluate the PDT and NBC classifierswith the held-out cases.
If the error rateincreases or its reduction is below athreshold break the loop; else go to step1.Output: Use the optimal classifier to label the testcasesFigure 2.
Collaborative bootstrapping algorithm3.4 Classification Confidence MeasurementClassification confidence is the metric used tomeasure the correctness of each labeled case auto-matically (see Step 2 in Figure 2).
The desirablemetric should satisfy two principles:?
It should be able to measure the uncertainty/ cer-tainty of the output of the classifiers; and?
It should be easy to calculate.We adopt entropy, i.e.
an information theorybased criterion, for this purpose.
Let x be the classi-fied object, and },...,,,{ 321 nccccC = the set of output.x is classified as ci with the probability)|( xcp i ni ,..,3,2,1= .
The entropy of the output isthen calculated as:?=?= niii xcpxcpxCe1)|(log)|()|(  (9)Once )|( xcp i is known, the entropy can be deter-mined.
These parameters can be easily determined inPDT, as each incoming case is classified into eachclass with a probability.
However, the incomingcases in NBC are grouped into one class which isassigned the highest score.
We then have to estimate)|( xcp i  from those scores.
Without loss of general-ity, the probability is estimated as:?== njjiixcscorexcscorexcp1)|()|()|(   (10)where )|( xcscore i  is the ranking score of x be-longing to ci.4 Experiment Setup and EvaluationSeveral experiments have been designed to evalu-ate the proposed learning approaches and to revealthe impact of linguistic features on learning per-formance.
700 sentences are extracted from Ta KongPao (a local Hong Kong Chinese newspaper) finan-cial version.
600 cases are labeled manually and 100left unlabeled.
Among those labeled, 400 are used astraining data, 100 as test data and the rest as held-outdata.4.1 Use of Linguistic Features As ClassificationAttributesThe impact of a temporal indicator is determinedby its position in a sentence.
In PDT and NBC, weconsider an indicator located in four positions: (1)BEFORE the first event; (2) AFTER the first eventand BEFORE the second and it modifies the firstevent; (3) the same as (2) but it modifies the secondevent; and (4) AFTER the second event.
Cases (2)and (3) are ambiguous.
The positions of the temporalindicators are the same.
But it is uncertain whetherthese indicators modify the first or the second eventif there is no punctuation separating their roles.
Weintroduce two methods, namely NA and SAP tocheck if the ambiguity affects the two learning ap-proaches.N(atural) O(rder): the temporal indicators betweenthe two events are extracted and compared accord-ing to their occurrence in the sentences regardlesswhich event they modify.S(eparate) A(uxiliary) and P(osition) words: wetry to resolve the above ambiguity with the gram-matical features of the indicators.
In this method,we assume that an indicator modifies the firstevent if it is an auxiliary word (e.g.
?
), a trendverb (e.g.
??)
or a position word (e.g.
?
); oth-erwise it modifies the second event.Temporal indicators are either tense/aspect or con-nectives (see Section 2.2).
Intuitively, it seems thatclassification could be better achieved if connectivefeatures are isolated from tense/ aspect features,allowing like to be compared with like.
MethodsSC1 and SC2 are designed based on this assumption.Table 2 shows the effect the different classificationmethods.SC1 (Separate Connecting words 1): it separatesconjunctions and verbs relating to causality fromothers.
They are assumed to contribute to dis-course structure (intra- or inter-sentence structure),and the others contribute to the tense/aspect ex-pressions for each individual event.
They are builtinto 2 separate attributes, one for each event.SC2 (Separate Connecting words 2): it is the sameas SC1 except that it combines the connectingword pairs (i.e.
as a single pattern) into one attrib-ute.EC (Event Class): it takes event classes into con-sideration.Accuracy Method PDT NBCNO 82.00% 81.00%SAP 82.20% 81.50%SAP +SC1 80.20% 78.00%SAP +SC2 81.70% 79.20%SAP +EC 85.70% 82.25%Table 2.
Effect of encoding linguistic features in the dif-ferent ways4.2 Impact of Individual FeaturesFrom linguistic perspectives, 13 features (see Ta-ble 1) are useful for relative relation resolution.
Toexamine the impact of each individual feature, wefeed a single linguistic feature to the PDT learningalgorithm one at a time and study the accuracy of theresultant classifier.
The experimental results aregiven in Table 3.
It shows that event classes havegreatest accuracy, followed by conjunctions in thesecond place, and adverbs in the third.Feature Accuracy Feature AccuracyPT 50.5% VA 56.5%VS 54% C 62%VC 54% U 51.5%TR 50.5% T 57.2%P 52.2 % D 61.7%PS 58.7% EC 68.2%VS 51.2% None 50.5%Table 3.
Impact of individual linguistic features4.3 DiscussionsAnalysis of the results in Tables 2 and 3 revealssome linguistic insights:1.
In a situation where temporal indicators appearbetween two events and there is no punctuationmark separating them, POS information help re-duce the ambiguity.
Compared with NO, SAPshows a slight improvement from 82% to 82.2%.But the improvement seems trivial and is not asgood as our prediction.
This might due to thesmall percent of such cases in the corpus.2.
Separating conjunctions and verbs relating tocausality from others is ineffective.
This revealsthe complexity of Chinese in connecting expres-sions.
It is because other words (such as adverbs,proposition and position words) also serve sucha function.
Meanwhile, experiments based onSC1 and SC2 suggest that the connecting ex-pressions generally involve more than one wordor phrase.
Although the words in a connectingexpression are separated in a sentence, the actionis indeed interactive.
It would be more useful toregard them as one attribute.3.
The effect of event classification is striking.Taking this feature into account, the accuraciesof both PDT and NB improved significantly.
Asa matter of fact, different event classes may in-troduce different relations even if they are con-strained by the same temporal indicators.4.4 Collaborative BootstrappingTable 4 presents the evaluation results of the fourdifferent classification approaches.
DM is the defaultmodel, which classifies all incoming cases as themost likely class.
It is used as evaluation baseline.Compare with DM, PDT and NBC show improve-ment in accuracy (i.e.
above 60% improvement).And CB in turn outperforms PDT and NBC.
Thisproves that using unlabeled data to boost the per-formance of the two classifiers is effective.Accuracy Approach Close test Open testDM 50.50% 55.00%NBC 82.25% 72.00%PDT 85.70% 74.00%CB 88.70% 78.00%Table 4.
Evaluation of NBC, PDT and CB approaches5 ConclusionsRelative temporal relation resolution receivedgrowing attentions in recent years.
It is important formany natural language processing applications, suchas information extraction and machine translation.This topic, however, has not been well studied, es-pecially in Chinese.
In this paper, we propose amodel for relative temporal relation resolution inChinese.
Our model combines linguistic knowledgeand machine learning approaches.
Two learning ap-proaches, namely probabilistic decision tree (PDT)and naive Bayesian classifier (NBC) and 13 linguis-tic features are employed.
Due to the limited labeledcases, we also propose a collaborative bootstrappingtechnique to improve learning performance.
Theexperimental results show that our approaches areencouraging.
To our knowledge, this is the first at-tempt of collaborative bootstrapping, which involvestwo heterogeneous classifiers, in NLP application.This lays down the main contribution of our research.In this pilot work, temporal indicators are selectedbased on linguistic knowledge.
It is time-consumingand could be error-prone.
This suggests two direc-tions for future studies.
We will try to automate or atleast semi-automate feature selection process.
An-other future work worth investigating is temporalindicator clustering.
There are two methods wecould investigate, i.e.
clustering the recognized indi-cators which occur in training corpus according toco-occurrence information or grouping them intotwo semantic roles, one related to tense/aspect ex-pressions and the other to connecting expressionsbetween two events.AcknowledgementsThe work presented in this paper is partially sup-ported by Research Grants Council of Hong Kong(RGC reference number PolyU5085/02E) andCUHK Strategic Grant (account number 4410001).ReferencesAllen J., 1981.
An Interval-based Represent Actionof Temporal Knowledge.
In Proceedings of 7th In-ternational Joint Conference on Artificial Intelli-gence, pages 221-226.
Los Altos, CA.Blum, A. and Mitchell T., 1998.
Combining Labeledand Unlabeled Data with Co-Training.
In Proceed-ings of the Eleventh Annual Conference on Com-putational Learning Theory, Madison, Wisconsin,pages 92-100Bruce B., 1972.
A Model for Temporal Referencesand its Application in Question-Answering Pro-gram.
Artificial Intelligence, 3(1):1-25.Collins M. and Singer Y, 1999.
Unsupervised Mod-els for Named Entity Classification.
In Proceed-ings of the Joint SIGDAT Conference onEmpirical Methods in Natural Language Process-ing and Very Large Corpora, pages 189-196.
Uni-versity of Maryland.Dorr B. and Gaasterland T., 2002.
Constraints on theGeneration of Tense, Aspect, and ConnectingWords from Temporal Expressions.
(submitted toJAIR)Hitzeman J., Moens M. and Grover C., 1995.
Algo-rithms for Analyzing the Temporal Structure ofDiscourse.
In Proceedings of the 7th EuropeanMeeting of the Association for ComputationalLinguistics, pages 253-260.
Dublin, Ireland.Lascarides A., Asher N. and Oberlander J., 1992.Inferring Discourse Relations in Context.
InProceedings of the 30th Meeting of theAssociation for Computational Linguistics, pages1-8, Newark, Del.Li W.J.
and Wong K.F., 2002.
A Word-based Ap-proach for Modeling and Discovering TemporalRelations Embedded in Chinese Sentences, ACMTransaction on Asian Language Processing,1(3):173-206.Ma J. and Knight B., 1994.
A General TemporalTheory.
The Computer Journal, 37(2):114- 123.M?ntaras L., 1991.
A Distance-based Attribute Se-lection Measure for Decision Tree Induction.
Ma-chine Learning, 6(1): 81?92.M?rquez L., Padr?
L. and Rodr?guez H., 2000.
AMachine Learning Approach to POS Tagging.Machine Learning, 39(1):59-91.
Kluwer Aca-demic Publishers.Partee, B., 1984.
Nominal and Temporal Anaphora.Linguistics and Philosophy, 7(3):287-324.Quinlan J., 1993.
C4.5 Programs for MachineLearning.
Morgan Kauman Press.Reichenbach H., 1947.
Elements of Symbolic Logic.Berkeley CA, University of California Press.Siegel E. and McKeown K., 2000.
Learning Meth-ods to Combine Linguistic Indicators: ImprovingAspectual Classification and Revealing LinguisticInsights.
Computational Linguistics, 26(4): 595-627.Wiebe, J.M., O'Hara, T.P., Ohrstrom-Sandgren, T.and McKeever, K.J, 1998.
An Empirical Approachto Temporal Reference Resolution.
Journal of Ar-tificial Intelligence Research, 9:247-293.Wong F., Li W., Yuan C., etc., 2002.
Temporal Rep-resentation and Classification in Chinese.
Interna-tional Journal of Computer Processing of OrientalLanguages, 15(2):211-230.Yarowsky D., 1994.
Decision Lists for Lexical Am-biguity Resolution: Application to the Accent Res-toration in Spanish and French.
In Proceeding ofthe 32rd Annual Meeting of ACL, San Francisco,CA.Zhou X., Dillon T., 1991.
A Statistical-heuristic Fea-ture Selection Criterion for Decision Tree Induc-tion.
IEEE Transaction on Pattern Analysis andMachine Intelligence, 13(8): 834-841.
