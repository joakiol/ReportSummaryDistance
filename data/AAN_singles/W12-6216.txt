Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 90?98,Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational LinguisticsA finite-state approach to phrase-based statistical machine translationJorge Gonza?lezDepartamento de Sistemas Informa?ticos y Computacio?nUniversitat Polite`cnica de Vale`nciaVale`ncia, Spainjgonzalez@dsic.upv.esAbstractThis paper presents a finite-state approachto phrase-based statistical machine translationwhere a log-linear modelling framework is im-plemented by means of an on-the-fly com-position of weighted finite-state transducers.Moses, a well-known state-of-the-art system,is used as a machine translation reference inorder to validate our results by comparison.Experiments on the TED corpus achieve asimilar performance to that yielded by Moses.1 IntroductionStatistical machine translation (SMT) is a patternrecognition approach to machine translation whichwas defined by Brown et al (1993) as follows: givena sentence s from a certain source language, a cor-responding sentence t?
in a given target languagethat maximises the posterior probability Pr(t|s) isto be found.
State-of-the-art SMT systems modelthe translation distribution Pr(t|s) via the log-linearapproach (Och and Ney, 2002):t?
= argmaxtPr(t|s) (1)?
argmaxtM?m=1?mhm(s, t) (2)where hm(s, t) is a logarithmic function represen-ting an important feature for the translation of s intot, M is the number of features (or models), and ?mis the weight of hm in the log-linear combination.This feature set typically includes several trans-lation models so that different relations betweena source and a target sentence can be considered.Nowadays, these models are strongly based onphrases, i.e.
variable-length n-grams, which meansthat they are built from some other lower-contextmodels that, in this case, are defined at phrase level.Phrase-based (PB) models (Tomas and Casacuberta,2001; Och and Ney, 2002; Marcu and Wong, 2002;Zens et al, 2002) constitute the core of the currentstate-of-the-art in SMT.
The basic idea of PB-SMTsystems is:1. to segment the source sentence into phrases,then2.
to translate each source phrase into a targetphrase, and finally3.
to reorder them in order to compose the finaltranslation in the target language.In a monotone translation framework however,the third step is omitted as the final translation isjust generated by concatenation of the target phrases.Apart from translation functions, the log-linearapproach is also usually composed by means of atarget language model and some other additionalelements such as word penalties or phrase penalties.The word and phrase penalties allow an SMT sys-tem to limit the number of words or target phrases,respectively, that constitute a translation hypothesis.In this paper, a finite-state approach to a PB-SMTstate-of-the-art system, Moses (Koehn et al, 2007),is presented.
Experimental results validate our workbecause they are similar to those yielded by Moses.A related study can be found in Kumar et al (2006)for the alignment template model (Och et al, 1999).902 Log-linear features for monotone SMTAs a first approach to Moses using finite-statemodels, a monotone PB-SMT framework is adopted.Under this constraint, Moses?
log-linear model isusually taking into account the following 7 features:Translation features1.
Direct PB translation probability2.
Inverse PB translation probability3.
Direct PB lexical weighting4.
Inverse PB lexical weightingPenalty features5.
PB penalty6.
Word penaltyLanguage features7.
Target language model2.1 Translation featuresAll 4 features related to translation are PB models,that is, their associated feature functions hm(s, t),which are in any case defined for full sentences,are modelled from other PB distributions ?m(s?, t?
),which are based on phrases.Direct PB translation probabilityThe first feature h1(s, t) = logP (t|s) is based onmodelling the posterior probability by using the seg-mentation between s and t as a hidden variable ?1.In this manner, Pr(t|s) =?
?1Pr(t|s, ?1) is thenapproximated by P (t|s) by using maximizationinstead of summation: P (t|s) = max?1P (t|s, ?1).Given a monotone segmentation between s and t,P (t|s, ?1) is generatively computed as the productof the translation probabilities for each segment pairaccording to some PB probability distributions:P (t|s, ?1) =|?1|?k=1P (t?k |s?k)where |?1| is the number of phrases that s and t aresegmented into, i.e.
every s?k and t?k , respectively,whose dependence on ?1 is omitted for the sake ofan easier reading.Feature 1 is finally formulated as follows:h1(s, t) = logmax?1|?1|?k=1P (t?k |s?k) (3)where ?1(s?, t?)
= P (t?|s?)
is a set of PB probabilitydistributions estimated from bilingual training data,once statistically word-aligned (Brown et al, 1993)by means of GIZA++ (Och and Ney, 2003), whichMoses relies on as far as training is concerned.This information is organized as a translation tablewhere a pool of phrase pairs is previously collected.Inverse PB translation probabilitySimilar to what happens with Feature 1, Feature 2is formulated as follows:h2(s, t) = logmax?2|?2|?k=1P (s?k |t?k) (4)where ?2(s?, t?)
= P (s?|t?)
is another set of PBprobability distributions, which are simultaneouslytrained together with the ones for Feature 1, P (t?|s?
),over the same pool of phrase pairs already extracted.Direct PB lexical weightingGiven the word-alignments obtained by GIZA++,it is quite straight-forward to estimate a maximumlikelihood stochastic dictionary P (ti|sj), which isused to score a weight D(s?, t?)
to each phrase pair inthe pool.
Details about the computation of D(s?, t?
)are given in Koehn et al (2007).
However, as far asthis work is concerned, these details are not relevant.Feature 3 is then similarly formulated as follows:h3(s, t) = logmax?3|?3|?k=1D(s?k , t?k) (5)where ?3(s?, t?)
= D(s?, t?)
is yet another score to usewith the pool of phrase pairs aligned during training.Inverse PB lexical weightingSimilar to what happens with Feature 3, Feature 4is formulated as follows:h4(s, t) = logmax?4|?4|?k=1I(s?k , t?k) (6)91where ?4(s?, t?)
= I(s?, t?)
is another weight vector,which is computed by using a dictionary P (sj |ti),with which the translation table is expanded again,thus scoring a new weight per phrase pair in the pool.2.2 Penalty featuresThe penalties are not modelled in the same way.The PB penalty is similar to a translation feature, i.e.it is based on a monotone sentence segmentation.The word penalty however is formulated as a whole,being taken into account by Moses at decoding time.PB penaltyThe PB penalty scores e = 2.718 per phrase pair,thus modelling somehow the segmentation length.Therefore, Feature 5 is defined as follows:h5(s, t) = logmax?5|?5|?k=1e (7)where ?5(s?, t?)
= e extends the PB table once again.Word penaltyWord penalties are not modelled as PB penalties.In fact, this feature is not defined from PB scores,but it is formulated at sentence level just as follows:h6(s, t) = log e|t| (8)where the exponent of e is the number of words in t.2.3 Language featuresLanguage models approach the a priori probabilitythat a given sentence belongs to a certain language.In SMT, they are usually employed to guarantee thattranslation hypotheses are built according to the pe-culiarities of the target language.Target language modelAn n-gram is used as target language model P (t),where a word-based approach is usually considered.Then, h7(s, t) = logP (t) is based on a modelwhere sentences are generatively built word by wordunder the influence of the last n?
1 previous words,with the cutoff derived from the start of the sentence:h7(s, t) = log|t|?i=1P (ti|ti?n+1 .
.
.
ti?1) (9)where P (ti|ti?n+1 .
.
.
ti?1) are word-based proba-bility distributions learnt from monolingual corpora.3 Data structuresThis section shows how the features from Section 2are actually organized into different data structuresin order to be efficiently used by the Moses decoder,which implements the search defined by Equation 2to find out the most likely translation hypothesis t?for a given source sentence s.3.1 PB modelsThe PB distributions associated to Features 1 to 5are organized in table form as a translation table forthe collection of phrase pairs previously extracted.That builds a PB database similar to that in Table 1Source Target ?1 ?2 ?3 ?4 ?5barato low cost 1 0.3 1 0.6 2.718me gusta I like 0.6 1 0.9 1 2.718es decir that is 0.8 0.5 0.7 0.9 2.718por favor please 0.4 0.2 0.1 0.4 2.718. .
.
.
.
.
2.718Table 1: A Spanish-into-English PB translation table.Each source-target phrase pair is scored by all ?
models.where each phrase pair is scored by all five models.3.2 Word-based modelsWhereas PB models are an interesting approachto deal with translation relations between languages,language modelling itself is usually based on words.Feature 6 is a length model of the target sentence,and Feature 7 is a target language model.Word penaltyPenalties are not models that need to be trained.However, while PB penalties are provided to Mosesto take them into account during the search process(see for example the last column of Table 1, ?5),word penalties are internally implemented in Mosesas part of the log-linear maximization in Equation 2,and are automatically computed on-the-fly at search.Target n-gram modelLanguage models, and n-grams in particular, suf-fer from a sparseness problem (Rosenfeld, 1996).The n-gram probability distributions are smoothedto be able to deal with the unseen events out of train-ing data, thus aiming for a larger language coverage.92This smoothing is based on the backoff method,which introduces some penalties for level down-grading within hierarchical language models.For example, let M be a trigram language model,which, as regards smoothing, needs both a bigramand a unigram model trained on the same data.Any trigram probability, P (c|ab), is then computedas follows:if abc ?
M: PM(c|ab)elseif bc ?
M: BOM(ab)PM(c|b)elseif c ?
M: BOM(ab)BOM(b)PM(c)else : BOM(ab)BOM(b)PM(unk)(10)where PM is the probability estimated by M for thecorresponding n-gram, BOM is the backoff weightto deal with the unseen events out of training data,and finally, PM(unk) is the probability mass re-served for unknown words.The P (ti|ti?n+1 .
.
.
ti?1) term from Equation 9is then computed according to that algorithm above,given the model data organized again in table formas a collection of probabilities and backoff weightsfor the n-grams appearing in the training corpus.This model displays similarly to that in Table 2.n-gram P BOplease 0.02 0.2low cost 0.05 0.3I like 0.1 0.7that is 0.08 0.5. .
.
.
.
.Table 2: An English word-based backoff n-gram model.The likelihood and the backoff model score for each n-gram.4 Weighted finite-state transducersWeighted finite-state transducers (Mohri et al,2002) (WFSTs) are defined by means of a tuple(?,?, Q, q0, f, P ), where ?
is the alphabet of in-put symbols, ?
is the alphabet of output symbols,Q is a finite set of states, q0 ?
Q is the initial state,f : Q ?
R is a state-based weight distribution toquantify that states may be final states, and finally,the partial function P : Q ?
??
?
??
?
Q ?
Rdefines a set of edges between pairs of states in sucha way that every edge is labelled with an input stringin ?
?, with an output string in ?
?, and is assigned atransition weight.When weights are probabilities, i.e.
the range offunctions f and P is constrained between 0 and 1,and under certain conditions, a weighted finite-state transducer may define probability distributions.Then, it is called a stochastic finite-state transducer.4.1 WFSTs for SMT modelsHere, we show how the SMT models described inSection 3 (that is, the five ?
scores in the PB trans-lation table, the word penalty, and the n-gram lan-guage model) are represented by means of WFSTs.First of all, the word penalty feature in Equation 8is equivalently reformulated as another PB score,as in Equations 3 to 7:h6(s, t) = log e|t| = logmax?6|?6|?k=1e|t?k| (11)where the length of t is split up by summationusing the length of each phrase in a segmentation ?6.Actually, this feature is independent of ?6, that is,any segmentation produces the expected value e|t|,and therefore the maximization by ?6 is not needed.However, the main goal is to introduce this feature asanother PB score similar to those in Features 1 to 5,and so it is redefined following the same framework.The PB table can be now extended by means of?6(s?, t?)
= e|t?|, just as Table 3 shows.Source Target ?1 ?2 ?3 ?4 ?5 ?6barato low cost .
.
.
e e2me gusta I like .
.
.
e e2es decir that is .
.
.
e e2por favor please .
.
.
e e. .
.
.
.
.Table 3: A word-penalty-extended PB translation table.The exponent of e in ?6 is the number of words in Target.Now, the translation table including 6 PB scoresand the target-language backoff n-gram model canbe expressed by means of (some stochastic) WFSTs.93Translation tableEach PB model included in the translation table,i.e.
any PB distribution in {?1(s?, t?
), .
.
.
, ?6(s?, t?
)},can be represented as a particular case of a WFST.Figure 1 shows a PB score encoded as a WFST,using a different looping transition per table rowwithin a WFST of only one state.Source Target ?mbarato low cost x1me gusta I like x2.
.
.
.
.
....q0x1barato / low costme gusta / I likex2Figure 1: Equivalent WFST representation of PB scores.Table rows are embedded within as many loopingtransitions of a WFST which has no topology at all;?-scores are correspondingly stored as transition weights.It is straight-forward to see that the application ofthe Viterbi method (Viterbi, 1967) on these WFSTsprovides the corresponding feature value hm(s, t)for all Features 1 to 6 as defined in Equations 3 to 8.Language modelIt is well known that n-gram models are a subclassof stochastic finite-state automata where backoffcan also be adequately incorporated (Llorens, 2000).Then, they can be equivalently turned into trans-ducers by means of the concept of identity, that is,transducers which map every input label to itself.Figure 2 shows a WFST for a backoff bigram model.It is also quite straight-forward to see that h7(s, t)(as defined in Equation 9 for a target n-gram modelwhere backoff is adopted according to Equation 10)is also computed by means of a parsing algorithm,which is actually a process that is simple to carryout given that these backoff n-gram WFSTs are de-terministic.q0 q1 q2 q3BO(q0)BO(q1) BO(q2)BO(q3)q?BigramBigramUnigramUnigramlayerlayeredgesedgeslow / lowcost / costPM(low)PM(cost | low)Figure 2: A WFST example for a backoff bigram model.Backoff (BO) is dealt with failure transitions from the bi-gram layer to the unigram layer.
Unigrams go in the otherdirection and bigrams link states within the bigram layer.To sum up, our log-linear combination scenarioconsiders 7 (some stochastic) WFSTs, 1 per feature:6 of them are PB models related to a translation tablewhile the 7th one is a target-language n-gram model.Next in Section 4.2, we show how these WFSTsare used in conjunction in a homogeneous frame-work.4.2 SearchEquation 2 is a general framework for log-linear ap-proaches to SMT.
This framework is adopted here inorder to combine several features based on WFSTs,which are modelled as their respective Viterbi score.As already mentioned, the computation ofhm(s, t) for each PB-WFST, let us say Tm (with1 ?
m ?
6), provides the most likely segmenta-tion ?m for s and t according to Tm.
However, aconstraint is used here so that all Tm models definethe same segmentation ?
:|?| > 0s = s?1 .
.
.
s?|?|t = t?1 .
.
.
t?|?|where the PB scores corresponding to Features 1 to 6are directly applied on that particular segmentationfor each phrase pair (s?k , t?k) monotonically aligned.Equations 3 to 7 and 11 can be simplified as follows:?m = 1, .
.
.
, 6hm(s, t) = logmax?|?|?k=1?m(s?k , t?k) (12)94Then, Equation 2 can be instanced as follows:t?
= argmaxt7?m=1?m hm(s, t) (13)= argmaxt?
?6?m=1?m max?|?|?k=1log ?m(s?k , t?k)?
?+?7|t|?i=1logP (ti|ti?n+1 .
.
.
ti?1)= argmaxt?
?max?|?|?k=16?m=1?m log ?m(s?k , t?k)?
?+|t|?i=1?7 logP (ti|ti?n+1 .
.
.
ti?1)as logarithm rules are applied to Equations 9 and 12.The square-bracketed expression of Equation 13is a Viterbi-like score which can be incrementallybuilt through the contribution of all the PB-WFSTs(along with their respective ?m-weights) over somephrase pair (s?k , t?k) that extends a partial hypothesis.As these models share their topology, we implementthem jointly including as many scores per transi-tion as needed (Gonza?lez and Casacuberta, 2008).These models can also be merged by means of uniononce their ?m-weights are transferred into them.That allows us to model the whole translation table(see Table 3) by means of just 1 WFST structure T .Therefore, the search framework for single modelscan also be used for their log-linear combination.As regards the remaining term from Equation 13,i.e.
the target n-gram language model for Feature 7,it is seen as a rescoring function (Och et al, 2004)which is applied once the PB-WFST T is explored.The translation model returns the best hypothesesthat are later input to the n-gram language model L,where they are reranked, to finally choose the best t?.However, these two steps can be processed at onceif both the WFST T and the WFST L are mergedby means of their composition T ?L (Mohri, 2004).The product of such an operation is another WFSTas WFSTs are closed under a composition operation.In practice though, the size of T ?L can be very largeso composition is done on-the-fly (Caseiro, 2003),which actually does not build the WFST for T ?
Lbut explores both T and L as if they were composed,using the n-gram scores in L on the target hypo-theses from T as soon as they are partially produced.Equation 13 represents a Viterbi-based compo-sition framework where all the (weighted) modelscontribute to the overall score to be maximized,provided that the set of ?m-weights is instantiated.Using a development corpus, the set of ?m-weightscan be empirically determined by means of runningseveral iterations of this framework, where differentvalues for the ?m-weights are tried in each iteration.5 ExperimentsExperiments were carried out on the TED corpus,which is described in depth throughout Section 5.1.Automatic evaluation for SMT is often consideredand we use the measures enumerated in Section 5.2.Results are shown and also discussed, in Section 5.3.5.1 Corpora dataThe TED corpus is composed of a collection ofEnglish-French sentences from audiovisual contentwhose main statistics are displayed in Table 4.Subset English FrenchTrainSentences 47.5KRunning words 747.2K 792.9KVocabulary 24.6K 31.7KDevelop Sentences 571Running words 9.2K 10.3KVocabulary 1.9K 2.2KTestSentences 641Running words 12.6K 12.8KVocabulary 2.4K 2.7KTable 4: Main statistics from the TED corpus and its split.As shown in Table 4, develop and test partitionsare statistically comparable.
The former is usedto train the ?m-weights in the log-linear approach,in the hope that they can also work well for the latter.5.2 Evaluation measuresSince its appearance as a translation quality mea-sure, the BLEU metric (Papineni et al, 2002), whichstands for bilingual evaluation understudy, has be-come consolidated in the area of automatic evalua-tion as the most widely used SMT measure.
Never-theless, it was later found that its correlation factor95with subjective evaluations (the original reason forits success) is actually not so high as first thought(Callison-Burch et al, 2006).
Anyway, it is still themost popular SMT measure in the literature.However, the word error rate (WER) is a verycommon measure in the area of speech recognitionwhich is also quite usually applied in SMT (Och etal., 1999).
Although it is not so widely employed asBLEU, there exists some work that shows a bettercorrelation of WER with human assessments (Paulet al, 2007).
Of course, the WER measure has somebad reviews as well (Chen and Goodman, 1996;Wang et al, 2003) and one of the main criticismsthat it receives in SMT areas is about the fact thatthere is only one translation reference to comparewith.
The MWER measure (Nie?en et al, 2000) isan attempt to relax this dependence by means of anaverage error rate with respect to a set of multiplereferences of equivalent meaning, provided that theyare available.Another measure also based on the edit distanceconcept has recently arisen as an evolution of WERtowards SMT.
It is the translation edit rate (TER),and it has become popular because it takes into ac-count the basic post-process operations that profes-sional translators usually do during their daily work.Statistically, it is considered as a measure highly cor-related with the result of one or more subjective eval-uations (Snover et al, 2006).The definition of these evaluation measures is asfollows:BLEU: It computes the precision of the unigrams,bigrams, trigrams, and fourgrams that appear inthe hypotheses with respect to the n-grams ofthe same order that occur in the translation ref-erence, with a penalty for too short sentences.Unlike the WER measure, BLEU is not an errorrate but an accuracy measure.WER: This measure computes the minimum num-ber of editions (replacements, insertions ordeletions) that are needed to turn the systemhypothesis into the corresponding reference.TER: It is computed similarly to WER, using an ad-ditional edit operation.
TER allows the move-ment of phrases, besides replacements, inser-tions, and deletions.5.3 ResultsThe goal of this section is to assess experimentallythe finite-state approach to PB-SMT presented here.First, an English-to-French translation is considered,then a French-to-English direction is later evaluated.On the one hand, our log-linear framework istuned on the basis of BLEU as the only evaluationmeasure in order to select the best set of ?m-weights.That is accomplished by means of development data,however, once the ?m-weights are estimated, theyare extrapolated to test data for the final evaluation.Table 5 shows: a) the BLEU translation results forthe development data; and b) the BLEU, WER andTER results for the test data.
In both a) and b), the?m-weights are trained on the development parti-tion.
These results are according to different featurecombinations in our log-linear approach to PB-SMT.As shown in Table 5, the first experimental sce-nario is not a log-linear framework since only onefeature, (a direct PB translation probability model)is considered.
The corresponding results are poorand, judging by the remaining results in Table 5,they reflect the need for a log-linear approach.The following experiments in Table 5 representa log-linear framework for Features 1 to 6,i.e.
the PB translation table encoded as a WFST T ,where different PB models are the focus of attention.Only the log-linear combination of Features 1 and 2Log-linear Develop Testfeatures BLEU BLEU WER TER1 (baseline) 8.5 7.1 102.9 101.51+2 4.0 3.0 116.6 115.61+2+3 22.7 18.4 66.6 64.41+2+3+4 22.8 18.5 66.3 64.21+2+3+4+5 22.7 18.8 65.2 63.21+2+3+4+5+6 23.1 19.1 65.9 63.81+7 24.6 20.5 65.1 62.91+2+7 25.5 21.3 63.7 61.61+2+3+7 25.9 22.2 62.5 60.41+2+3+4+7 26.3 22.0 63.4 61.31+2+3+4+5+7 26.4 22.1 63.1 61.01+2+3+4+5+6+7 27.0 21.8 64.4 62.2Moses (1+.
.
.+7) 27.1 22.0 64.0 61.8Table 5: English-to-French results for development andtest data according to different log-linear scenarios.The set of ?m-weights is learnt from development datafor every feature combination log-linear scenario defined.96is worse than the baseline, which feeds us backon the fact that the ?m-weights can be better trained,that is, the log-linear model for Features 1 and 2can be upgraded until baseline?s results with ?2 = 0.This battery of experiments on Features 1 to 6allows us to see the benefits of a log-linear approach.The baseline results are clearly outperformed now,and we can say that the more features are included,the better are the results.The next block of experiments in Table 5 alwaysinclude Feature 7, i.e.
the target language model L.Features 1 to 6 are progressively introduced into T .These results confirm that the target language modelis still an important feature to take into account,even though PB models are already providing a sur-rounding context for their translation hypotheses be-cause translation itself is modelled at phrase level.These results are significantly better than the oneswhere the target language model is not considered.Again, the more translation features are included,the better are the results on the development data.However, an overtraining is presumedly occurringwith regard to the optimization of the ?m-weights,as results on the test partition do not reach their topthe same way the ones for the development data do,i.e.
when using all 7 features, but when combiningFeatures 1, 2, 3, and 7, instead.
These differencesare not statistically significant though.Finally, our finite-state approach to PB-SMTis validated by comparison, as it allows us to achievesimilar results to those yielded by Moses itself.On the other hand, a translation direction whereFrench is translated into English gets now the focus.Their corresponding results are presented in Table 6.A similar behaviour can be observed in Table 6for the series of French-to-English empirical results.6 Conclusions and future workIn this paper, a finite-state approach to Moses, whichis a PB-SMT state-of-the-art system, is presented.A monotone framework is adopted, where 7 mo-dels in log-linear combination are considered: a di-rect and an inverse PB translation probability model,a direct and an inverse PB lexical weighting model,PB and word penalties, and a target language model.Five out of these models are based on PB scoreswhich are organized under a PB translation table.Log-linear Develop Testfeatures BLEU BLEU WER TER1 (baseline) 7.1 7.4 101.6 100.01+2 4.1 3.5 117.5 116.01+2+3 24.2 21.1 58.9 56.51+2+3+4 24.4 20.8 58.0 55.71+2+3+4+5 24.9 21.2 56.9 54.81+2+3+4+5+6 25.2 21.2 57.1 55.01+7 24.7 22.5 60.0 57.71+2+7 26.0 23.2 58.8 56.51+2+3+7 28.5 23.0 56.1 54.01+2+3+4+7 28.4 23.1 56.0 53.81+2+3+4+5+7 28.8 23.4 56.0 53.91+2+3+4+5+6+7 28.7 23.8 55.8 53.7Moses (1+.
.
.+7) 28.9 23.5 55.8 53.6Table 6: French-to-English results for developmentand test data according to different log-linear scenarios.These models can also be implemented by meansof WFSTs on the basis of the Viterbi algorithm.The word penalty can also be equivalently redefinedas another PB model, similar to the five others,which allows us to constitute a translation model Tcomposed of six parallel WFSTs that are constrainedto share the same monotonic bilingual segmentation.A backoff n-gram model for the target language Lcan be represented as an identity WFST where P (t)is modelled on the basis of the Viterbi algorithm.The whole log-linear approach to Moses is attainedby means of the on-the-fly WFST composition T ?L.Our finite-state log-linear approach to PB-SMTis validated by comparison, as it has allowed usto achieve similar results to those yielded by Moses.Monotonicity is an evident limitation of this work,as Moses can also feature some limited reordering.However, future work on that line is straight-forwardsince the framework described in this paper can beeasily extended to include a PB reordering model R,by means of the on-the-fly composition T ?
R ?
L.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union 7th FrameworkProgramme (FP7/2007-2013) under grant agree-ment no.
287576.
Work also supported by the EC(FEDER, FSE), the Spanish government (MICINN,MITyC, ?Plan E?, grants MIPRCV ?Consolider In-genio 2010?
and iTrans2 TIN2009-14511), and theGeneralitat Valenciana (grant Prometeo/2009/014).97ReferencesP.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra, and R.L.Mercer.
1993.
The mathematics of machine transla-tion.
In Computational Linguistics, volume 19, pages263?311, June.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.Re-evaluating the Role of Bleu in Machine Transla-tion Research.
In Proceedings of the 11th conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 249?256.D.
Caseiro.
2003.
Finite-State Methods in AutomaticSpeech Recognition.
PhD Thesis, Instituto SuperiorTe?cnico, Universidade Te?cnica de Lisboa.S.
F. Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Pro-ceedings of the 34th annual meeting of the Associationfor Computational Linguistics, pages 310?318.J.
Gonza?lez and F. Casacuberta.
2008.
A finite-stateframework for log-linear models in machine transla-tion.
In Proc.
of European Association for MachineTranslation, pages 41?46.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: open source toolkit forstatistical machine translation.
In Proc.
of Associationfor Computational Linguistics, pages 177?180.S.
Kumar, Y. Deng, and W. Byrne.
2006.
A weightedfinite state transducer translation template model forstatistical machine translation.
Natural Language En-gineering, 12(1):35?75.David Llorens.
2000.
Suavizado de auto?matas y traduc-tores finitos estoca?sticos.
PhD Thesis, Departamentode Sistemas Informa?ticos y Computacio?n, UniversidadPolite?cnica de Valencia.D.
Marcu and W. Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.In Proc.
of Empirical methods in natural languageprocessing, pages 133?139.Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer Speech and Language,16(1):69?88.Mehryar Mohri.
2004.
Weighted finite-state transduceralgorithms: An overview.
Formal Languages and Ap-plications, 148:551?564.S.
Nie?en, F. J. Och, G. Leusch, and H. Ney.
2000.
Anevaluation tool for machine translation: Fast evalua-tion for MT research.
In Proceedings of the 2nd in-ternational Conference on Language Resources andEvaluation, pages 39?45.F.J.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of Association for ComputationalLinguistics, pages 295?302.F.J.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29:19?51, March.F.
J. Och, C. Tillmann, and H. Ney.
1999.
Improvedalignment models for statistical machine translation.In Proceedings of the joint conference on EmpiricalMethods in Natural Language Processing and the 37thannual meeting of the Association for ComputationalLinguistics, pages 20?28.F.J.
Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A smorgas-bord of features for statistical machine translation.
InD.
Marcu S. Dumais and S. Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 161?168,Boston, Massachusetts, USA, May 2 - May 7.
Asso-ciation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of the 40thannual meeting of the Association for ComputationalLinguistics, pages 311?318.M.
Paul, A. Finch, and E. Sumita.
2007.
Reducinghuman assessment of machine translation quality tobinary classifiers.
In Proceedings of the conferenceon Theoretical and Methodological Issues in machinetranslation, pages 154?162.R.
Rosenfeld.
1996.
A maximum entropy approach toadaptative statistical language modeling.
ComputerSpeech and Language, 10:187?228.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th biennial conference of theAssociation for Machine Translation in the Americas,pages 223?231.J.
Tomas and F. Casacuberta.
2001.
Monotone statisticaltranslation using word groups.
In Proc.
of the MachineTranslation Summit, pages 357?361.A.
Viterbi.
1967.
Error bounds for convolutional codesand an asymptotically optimum decoding algorithm.IEEE Transactions on Information Theory, 13(2):260?269.Y.
Wang, A. Acero, and C. Chelba.
2003.
Is word errorrate a good indicator for spoken language understand-ing accuracy.
In Proceedings of the IEEE workshopon Automatic Speech Recognition and Understanding,pages 577?582.Richard Zens, Franz Josef Och, and Hermann Ney.
2002.Phrase-based statistical machine translation.
In Proc.of Advances in Artificial Intelligence, pages 18?32.98
