Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 845?853,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsDiscriminative Learning for Joint Template FillingEinat MinkovInformation SystemsUniversity of HaifaHaifa 31905, Israeleinatm@is.haifa.ac.ilLuke ZettlemoyerComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195, USAlsz@cs.washington.eduAbstractThis paper presents a joint model for tem-plate filling, where the goal is to automati-cally specify the fields of target relations suchas seminar announcements or corporate acqui-sition events.
The approach models mentiondetection, unification and field extraction ina flexible, feature-rich model that allows forjoint modeling of interdependencies at all lev-els and across fields.
Such an approach can,for example, learn likely event durations andthe fact that start times should come beforeend times.
While the joint inference space islarge, we demonstrate effective learning witha Perceptron-style approach that uses simple,greedy beam decoding.
Empirical results intwo benchmark domains demonstrate consis-tently strong performance on both mention de-tection and template filling tasks.1 IntroductionInformation extraction (IE) systems recover struc-tured information from text.
Template filling is an IEtask where the goal is to populate the fields of a tar-get relation, for example to extract the attributes of ajob posting (Califf and Mooney, 2003) or to recoverthe details of a corporate acquisition event from anews story (Freitag and McCallum, 2000).This task is challenging due to the wide rangeof cues from the input documents, as well as non-textual background knowledge, that must be consid-ered to find the best joint assignment for the fieldsof the extracted relation.
For example, Figure 1shows an extraction from CMU seminar announce-ment corpus (Freitag and McCallum, 2000).
Here,the goal is to perform mention detection and extrac-tion, by finding all of the text spans, or mentions,Date 5/5/1995Start Time 3:30PMLocation Wean Hall 5409Speaker Raj ReddyTitle Some Necessary Conditions for a Good User InterfaceEnd Time ?Figure 1: An example email and its template.
Field men-tions are highlighted in the text, grouped by color.that describe field values, unify these mentions bygrouping them according to target field, and normal-izing the results within each group to provide thefinal extractions.
Each of these steps requires sig-nificant knowledge about the target relation.
For ex-ample, in Figure 1, the mention ?3:30?
appears threetimes and provides the only reference to a time.
Wemust infer that this is the starting time, that the endtime is never explicitly mentioned, and also that theevent is in the afternoon.
Such inferences may nothold in more general settings, such as extraction formedical emergencies or related events.In this paper, we present a joint modeling andlearning approach for the combined tasks of men-tion detection, unification, and template filling, asdescribed above.
As we will see in Section 2, pre-vious work has mostly focused on learning tagging845models for mention detection, which can be diffi-cult to aggregate into a full template extraction, ordirectly learning template field value extractors, of-ten in isolation and with no reasoning across differ-ent fields in the same relation.
We present a simple,feature-rich, discriminative model that readily incor-porates a broad range of possible constraints on thementions and joint field assignments.Such an approach allows us to learn, for each tar-get relation, an integrated model to weight the dif-ferent extraction options, including for example thelikely lengths for events, or the fact that start timesshould come before end times.
However, there aresignificant computation challenges that come withthis style of joint learning.
We demonstrate empiri-cally that these challenges can be solved with a com-bination of greedy beam decoding, performed di-rectly in the joint space of possible mention clustersand field assignments, and structured Perceptron-style learning algorithm (Collins, 2002).We report experimental evaluations on two bench-mark datasets in different genres, the CMU semi-nar announcements and corporate acquisitions (Fre-itag and McCallum, 2000).
In each case, we evalu-ated both template extraction and mention detectionperformance.
Our joint learning approach providesconsistently strong results across every setting, in-cluding new state-of-the-art results.
We also demon-strate, through ablation studies on the feature set, theneed for joint modeling and the relative importanceof the different types of joint constraints.2 Related WorkResearch on the task of template filling has focusedon the extraction of field value mentions from theunderlying text.
Typically, these values are extractedbased on local evidence, where the most likely entityis assigned to each slot (Roth and Yih, 2001; Siefkes,2008).
There has been little effort towards a compre-hensive approach that includes mention unification,as well as considers the structure of the target rela-tional schema to create semantically valid outputs.Recently, Haghighi and Klein (2010) presenteda generative semi-supervised approach for templatefilling.
In their model, slot-filling entities are firstgenerated, and entity mentions are then realized intext.
Thus, their approach performs coreference atslot level.
In addition to proper nouns (named en-tity mentions) that are considered in this work, theyalso account for nominal and pronominal noun men-tions.
This work presents a discriminative approachto this problem.
An advantage of a discriminativeframework is that it allows the incorporation of richand possibly overlapping features.
In addition, weenforce label consistency and semantic coherence atrecord level.Other related works perform structured relationdiscovery for different settings of information ex-traction.
In open IE, entities and relations may be in-ferred jointly (Roth and Yih, 2002; Yao et al, 2011).In this IE task, the target relation must agree with theentity types assigned to it; e.g., born-in relation re-quires a place as its argument.
In addition, extractedrelations may be required to be consistent with anexisting ontology (Carlson et al, 2010).
Comparedwith the extraction of tuples of entity mention pairs,template filling is associated with a more complextarget relational schema.Interestingly, several researchers have attemptedto model label consistency and high-level relationalconstraints using state-of-the-art sequential modelsof named entity recognition (NER).
Mainly, pre-determined word-level dependencies were repre-sented as links in the underlying graphical model(Sutton and McCallum, 2004; Finkel et al, 2005).Finkel et al (2005) further modelled high-level se-mantic constraints; for example, using the CMUseminar announcements dataset, spans labeled asstart time or end time were required to be seman-tically consistent.
In the proposed framework wetake a bottom-up approach to identifying entity men-tions in text, where given a noisy set of candidatenamed entities, described using rich semantic andsurface features, discriminative learning is appliedto label these mentions.
We will show that this ap-proach yields better performance on the CMU semi-nar announcement dataset when evaluated in termsof NER.
Our approach is complimentary to NERmethods, as it can consolidate noisy overlappingpredictions from multiple systems into coherent sets.3 Problem SettingIn the template filling task, a target relation r is pro-vided, comprised of attributes (also referred to as846Figure 2: The relational schema for the seminars domain.Figure 3: A record partially populated from text.fields, or slots) A(r).
Given a document d, whichis known to describe a tuple of the underlying re-lation, the goal is to populate the fields with valuesbased on the text.The relational schema.
In this work, we describedomain knowledge through an extended relationaldatabase schema R. In this schema, every field ofthe target relation maps to a tuple of another rela-tion, giving rise to a hierarchical view of templatefilling.
Figure 2 describes a relational schema forthe seminar announcement domain.
As shown, eachfield of the seminar relation maps to another rela-tion; e.g., speaker?s values correspond to person tu-ples.
According to the outlined schema, most re-lations (e.g., person) consist of a single attribute,whereas the date and time relations are characterisedwith multiple attributes; for example, the time rela-tion includes the fields of hour, minutes and ampm.We will make use of limited domain knowledge,expressed as relation-level constraints that are typi-cally realized in a database.
Namely, the followingtests are supported for each relation.Tuple validity.
This test reflects data integrity.
Theattributes of a relation may be defined as mandatoryor optional.
Mandatory attributes are denoted with asolid boundary in Figure 2 (e.g., seminar.date), andoptional attributes are denoted with a dashed bound-ary (e.g., seminar.title).
Similar constraints can beposed on a set of attributes; e.g., either day-of-monthor day-of-week must be populated in the date rela-tion.
Finally, a combination of field values may berequired to be valid, e.g., the values of day, month,year and day-of-week must be consistent.Tuple contradiction.
This function checkswhether two valid tuples v1 and v2 are inconsis-tent, implying a negation of possible unification ofthese tuples.
In this work, we consider date and timetuples as contradictory if they contain semanticallydifferent values for some field; tuples of location,person and title are required to have minimal over-lap in their string values to avoid contradiction.Template filling.
Given document d, the hierar-chical schema R is populated in a bottom-up fash-ion.
Generally, parent-free relations in the hierar-chy correspond to generic entities, realized as en-tity mentions in the text.
In Figure 2, these relationsare denoted by double-line boundary, including lo-cation, person, title, date and time; every tuple ofthese relations maps to a named entity mention.1Figure 3 demonstrates the correct mapping ofnamed entity mentions to tuples, as well as tuple uni-fication, for the example shown in Figure 1.
For ex-ample, the mentions ?Wean 5409?
and ?Wean Hall5409?
correspond to tuples of the location relation,where the two tuples are resolved into a unified set.To complete template filling, the remaining relationsof the schema are populated bottom-up, where eachfield links to a unified set of populated tuples.
Forexample, in Figure 3, the seminar.location field islinked to {?Wean Hall 5409?,?Wean 5409?
}.Value normalization of the unified tuples is an-other component of template filling.
We partially ad-dress normalization: tuples of semantically detailed(multi-attribute) relations, e.g., date and time, are re-solved into their semantic union, while textual tuples(e.g., location) are normalized to the longest stringin the set.
In this work, we assume that each tem-plate slot contains at most one value.
This restrictioncan be removed, at the cost of increasing the size ofthe decoding search space.1In the multi-attribute relations of date and time, each at-tribute maps to a text span, where the set of spans at tuple-levelis required to be sequential (up to a small distance d).8474 Structured LearningNext, we describe how valid candidate extrac-tions are instantiated (Sec.
4.1) and how learningis applied to assess the quality of the candidates(Sec.
4.2), where beam search is used to find the topscoring candidates efficiently (Sec.
4.3).4.1 Candidate GenerationNamed entity recognition.
A set of candidate men-tions Sd(a) is extracted from document d per eachattribute a of a relation r ?
L, where L is the setof parent-free relations in T .
We aim at high-recallextractions; i.e., Sd(a) is expected to contain the cor-rect mentions with high probability.
Various IE tech-niques, as well as an ensemble of methods, can beemployed for this purpose.
For each relation r ?
L,valid candidate tuples Ed(r) are constructed fromthe candidate mentions that map to its attributes.Unification.
For every relation r ?
L, we con-struct candidate sets of unified tuples, {Cd(r) ?Ed(r)}.
Naively, the number of candidate sets isexponential in the size of Ed(t).
Importantly, how-ever, the tuples within a candidate unification set arerequired to be non-contradictory.
In addition, thetext spans that comprise the mentions within eachset must not overlap.
Finally, we do not split tupleswith identical string values between different sets.Candidate tuples.
To construct the space of candi-date tuples of the target relation, the remaining rela-tions r ?
{T?L} are visited bottom-up, where eachfield a ?
A(r) is mapped in turn to a (possibly uni-fied) populated tuple of its type.
The valid (and non-overlapping) combinations of field mappings consti-tute a set of candidate tuples of r.The candidate tuples generated using this proce-dure are structured entities, constructed using typednamed entity recognition, unification, and hierarchi-cal assignment of field values (Figure 3).
We willderive features that describe local and global prop-erties of the candidate tuples, encoding both surfaceand semantic information.4.2 LearningWe employ a discriminative learning algorithm, fol-lowing Collins (2002).
Our goal is to find the candi-Algorithm 1: The beam search procedure1.
Populate every low-level relation r ?
L from text d:?
Construct a set of candidate valid tuples Ed(r) givenhigh-recall typed candidate text spans Sd(a), a ?
A(r).?
Group Ed(r) into possibly overlapping unified sets,{Cd(r) ?
Ed(r)}.2.
Iterate bottom-up through relations r ?
{T ?
L}:?
Initialize the set of candidate tuples Ed(r) to an emptyset.?
Iterate through attributes a ?
A(r):?
Retrieve the set of candidate tuples (or unified tuplesets) Ed(r?
), where r?
is the relation that attribute alinks to in T .
Add an empty tuple to the set.?
For every pair of candidate tuples e ?
Ed(r) ande?
?
Ed(r?
), modify e by linking attribute a(e) totuple e?.?
Add the modified tuples, if valid, to Ed(r).?
Apply Equation 1 to rank the partially filled candi-date tuples e ?
Ed(r).
Keep the k top scoring can-didates in Ed(r), and discard the rest.3.
Apply Equation 1 to output a ranked list of extracted recordsEd(r?
), where r?
is the target relation.date that maximizes:F (y, ??)
=m?j=1?jfj(y, d, T ) (1)where fj(d, y, T ), j = 1, ..,m, are pre-defined fea-ture functions describing a candidate record y of thetarget relation given document d and the extendedschema T .
The parameter weights ?j are to belearned from labeled instances.
The training pro-cedure involves initializing the weights ??
to zero.Given ?
?, an inference procedure is applied to findthe candidate that maximizes Equation 1.
If the top-scoring candidate is different from the correct map-ping known, then: (i) ??
is incremented with the fea-ture vector of the correct candidate, and (ii) the fea-ture vector of the top-scoring candidate is subtractedfrom ??.
This procedure is repeated for a fixed num-ber of epochs.
Following Collins, we employ the av-eraged Perceptron online algorithm (Collins, 2002;Freund and Schapire, 1999) for weight learning.4.3 Beam SearchUnfortunately, optimal local decoding algorithms(such as the Viterbi algorithm in tagging problems(Collins, 2002)) can not be applied to our prob-lem.
We therefore propose using beam search to ef-ficiently find the top scoring candidate.
This means848that rather than instantiate the full space of valid can-didate records (Section 4.1), we are interested in in-stantiating only those candidates that are likely to beassigned a high score by F .
Algorithm 1 outlinesthe proposed beam search procedure.
As detailed,only a set of top scoring tuples of size k (beam size)is maintained per relation r ?
T during candidategeneration.
A given relation is populated incremen-tally, having each of its attributes a ?
A(r) map inturn to populated tuples of its type, and using Equa-tion 1 to find the k highest scoring partially popu-lated tuples; this limits the number of candidate tu-ples evaluated to k2 per attribute, and to nk2 for arelation with n attributes.
While beam search is effi-cient, performance may be compromised comparedwith an unconstrained search.
The beam size k al-lows controlling the trade-off between performanceand cost.
An advantage of the proposed approach isthat rather than output a single prediction, a list ofcoherent candidate tuples may be generated, rankedaccording to Equation 1.5 Seminar Extraction TaskDataset The CMU seminar announcement dataset(Freitag and McCallum, 2000) includes 485 emailscontaining seminar announcements.
The dataset hasbeen originally annotated with text spans referring tofour slots: speaker, location, stime, and etime.
Wehave annotated this dataset with two additional at-tributes: date and title.2 We consider this corpus asan example of semi-structured text, where some ofthe field values appear in the email header, in a tabu-lar structure, or using special formatting (Califf andMooney, 1999; Minkov et al, 2005).3We used a set of rules to extract candidate namedentities per the types specified in Figure 2.4 Therules encode information typically used in NER, in-cluding content and contextual patterns, as well aslookups in available dictionaries (Finkel et al, 2005;Minkov et al, 2005).
The extracted candidates arehigh-recall and overlapping.
In order to increaserecall further, additional candidates were extractedbased on document structure (Siefkes, 2008).
The2A modified dataset is available on the author?s homepage.3Such structure varies across messages.
Otherwise, theproblem would reduce to wrapper learning (Zhu et al, 2006).4The rule language used is based on cascaded finite statemachines (Minorthird, 2008).recall for the named entities of type date and time isnear perfect, and is estimated at 96%, 91% and 90%for location, speaker and title, respectively.Features The categories of the features used aredescribed below.
All features are binary and typed.5Lexical.
These features indicate the value andpattern of words within the text spans correspond-ing to each field.
For example, lexical features perFigure 1 include location.content.word.wean, loca-tion.pattern.capitalized.
Similar features are derivedfor a window of three words to the right and to theleft of the included spans.
In addition, we observewhether the words that comprise the text spans ap-pear in relevant dictionaries: e.g., whether the spansassigned to the location field include words typi-cal of location, such as ?room?
or ?hall?.
Lex-ical features of this form are commonly used inNER (Finkel et al, 2005; Minkov et al, 2005).Structural.
It has been previously shown thatthe structure available in semi-structured documentssuch as email messages is useful for information ex-traction (Minkov et al, 2005; Siefkes, 2008).
Asshown in Figure 1, an email message includes aheader, specifying textual fields such as topic, datesand time.
In addition, space lines and line breaks areused to emphasize blocks of important information.We propose a set of features that model correspon-dence between the text spans assigned to each fieldand document structure.
Specifically, these featuresmodel whether at least one of the spans mapped toeach field appears in the email header; captures afull line in the document; is indent; appears withinspace lines; or in a tabular format.
In Figure 1, struc-tural active features include location.inHeader, lo-cation.fullLine, title.withinSpaceLines, etc.Semantic.
These features refer to the semanticinterpretation of field values.
According to the re-lational schema (Figure 2), date and time includedetailed attributes, whereas other relations are rep-resented as strings.
The semantic features encodedtherefore refer to date and time only.
Specifically,these features indicate whether a unified set of tu-ples defines a value for all attributes; for example,in Figure 1, the union of entities that map to thedate field specify all of the attribute values of thisrelation, including day-of-month, month, year, and5Real-value features were discretized into segments.849Date Stime Etime Location Speaker TitleFull model 96.1 99.3 98.7 96.4 87.5 69.5No structural features 94.9 99.1 98.0 96.1 83.8 65.1No semantic features 96.1 98.7 95.4 96.4 87.5 69.5No unification 87.2 97.0 95.1 94.5 76.0 62.7Individual fields 96.5 97.2 - 96.4 86.8 64.5Table 1: Seminar extraction results (5-fold CV): Field-level F1Date Stime Etime Location Speaker TitleSNOW (Roth and Yih, 2001) - 99.6 96.3 75.2 73.8 -BIEN (Peshkin and Pfeffer, 2003) - 96.0 98.8 87.1 76.9 -Elie (Finn, 2006) - 98.5 96.4 86.5 88.5 -TIE (Siefkes, 2008) - 99.3 97.1 81.7 85.4 -Full model 96.3 99.1 98.0 96.9 85.8 67.7Table 2: Seminar extraction results (5-fold CV, trained on 50% of corpus): Field-level F1day-of-week.
Another feature encodes the size of themost semantically detailed named entity that mapsto a field; for example, the most detailed entity men-tion of type stime in Figure 1 is ?3:30?, compris-ing of two attribute values, namely hour and min-utes.
Similarly, the total number of semantic unitsincluded in a unified set is represented as a feature.These features were designed to favor semanticallydetailed mentions and unified sets.
Finally, domain-specific semantic knowledge is encoded as features,including the duration of the seminar, and whether atime value is round (minutes divide by 5).In addition to the features described, one maybe interested in modeling cross-field information.We have experimented with features that encodethe shortest distance between named entity mentionsmapping to different fields (measured in terms ofseparating lines or sentences), based on the hypoth-esis that field values typically co-appear in the samesegments of the document.
These features were notincluded in the final model since their contributionwas marginal.
We leave further exploration of cross-field features in this domain to future work.Experiments We conducted 5-fold cross vali-dation experiments using the seminar extractiondataset.
As discussed earlier, we assume that a sin-gle record is described in each document, and thateach field corresponds to a single value.
Theseassumptions are violated in a minority of cases.In evaluating the template filling task, only exactmatches are accepted as true positives, where partialmatches are counted as errors (Siefkes, 2008).
No-tably, the annotated labels as well as corpus itself arenot error-free; for example, in some announcementsthe date and day-of-week specified are inconsistent.Our evaluation is strict, where non-empty predictedvalues are counted as errors in such cases.Table 1 shows the results of our full model us-ing beam size k = 10, as well as model variants.In order to evaluate the contribution of the proposedfeatures, we eliminated every feature group in turn.As shown in the table, removing the structural fea-tures hurt performance consistently across fields.
Inparticular, structure is informative for the title field,which is otherwise characterised with low contentand contextual regularity.
Removal of the semanticfeatures affected performance on the stime and etimefields, modeled by these features.
In particular, theoptional etime field, which has fewer occurrences inthe dataset, benefits from modeling semantics.An important question to be addressed in evalu-ation is to what extent the joint modeling approachcontributes to performance.
In another experimentwe therefore mimic the typical scenario of templatefilling, in which the value of the highest scoringnamed entity is assigned to each field.
In our frame-work, this corresponds to a setting in which a unifiedset includes no more than a single entity.
The resultsare shown in Table 1 (?no unification?).
Due to re-duced evidence given a single entity versus a a coref-erent set of entities, this results in significantly de-graded performance.
Finally, we experimented withpopulating every field of the target schema indepen-dently of the other fields.
While results are overallcomparable on most fields, this had negative impacton the title field.
This is largely due to erroneous as-signments of named entities of other types (mainly,person) as titles; such errors are avoided in the fulljoint model, where tuple validity is enforced.Table 2 provides a comparison of the full model850Date Stime Etime Location Speaker Title(Sutton and McCallum, 2004) - 96.7 97.2 88.1 80.4 -(Finkel et al, 2005) - 97.1 97.9 90.0 84.2 -Full model 95.4 97.1 97.9 97.0 86.5 75.5Table 3: Seminar extraction results: Token-level F1against previous state-of-the-art results.
These re-sults were all obtained using half of the corpus fortraining, and its remaining half for evaluation; thereported figures were averaged over five randomsplits.
For comparison, we used 5-fold cross vali-dation, where only a subset of each train fold thatcorresponds to 50% of the corpus was used for train-ing.
Due to the reduced training data, the results areslightly lower than in Table 1.
(Note that we used thesame test examples in both cases.)
The best resultsper field are marked in boldface.
The proposed ap-proach yields the best or second-best performanceon all target fields, and gives the best performanceoverall.
While a variety of methods have been ap-plied in previous works, none has modeled templatefilling in a joint fashion.
As argued before, jointmodeling is especially important for irregular fields,such as title; we provide first results on this field.Previously, Sutton and McCallum (2004) andlater Finkel et-al.
(2005), applied sequential modelsto perform NER on this dataset, identifying namedentities that pertain to the template slots.
Both ofthese works incorporated coreference and high-levelsemantic information to a limited extent.
We com-pare our approach to their work, having obtained andused the same 5-fold cross validation splits as bothworks.
Table 3 shows results in terms of token F1.Our results evaluated on the named mention recogni-tion task are superior overall, giving comparable orbest performance on all fields.
We believe that theseresults demonstrate the benefit of performing men-tion recognition as part of a joint model that takesinto account detailed semantics of the underlying re-lational schema, when available.Finally, we evaluate the global quality of the ex-tracted records.
Rather than assess performance atfield-level, this stricter evaluation mode considers awhole tuple, requiring the values assigned to all ofits fields to be correct.
Overall, our full model (Table1) extracts globally correct records for 52.6% of theexamples.
To our knowledge, this is the first workthat provides this type of evaluation on this dataset.Importantly, an advantage of the proposed approachFigure 4: The relational schema for acquisitions.is that it readily outputs a ranked list of coherent pre-dictions.
While the performance at the top of theoutput lists was roughly comparable, increasing kgives higher oracle recall: the correct record wasincluded in the output k-top list 69.7%, 76.1% and80.4% of the time, for k = 5, 10, 20 respectively.6 Corporate AcquisitionsDataset The corporate acquisitions corpus con-tains 600 newswire articles, describing factual or po-tential corporate acquisition events.
The corpus hasbeen annotated with the official names of the partiesto an acquisition: acquired, purchaser and seller, aswell as their corresponding abbreviated names andcompany codes.6 We describe the target schema us-ing the relational structure depicted in Figure 4.
Theschema includes two relations: the corp relation de-scribes a corporate entity, including its full name,abbreviated name and code as attributes; the targetacquisition relation includes three role-designatingattributes, each linked to a corp tuple.Candidate name mentions in this strictly gram-matical genre correspond to noun phrases.
Docu-ments were pre-processed to extract noun phrases,similarly to Haghighi and Klein (2010).Features We model syntactic features, followingHaghighi and Klein (2010).
In order to compen-sate for parsing errors, shallow syntactic featureswere added, representing the values of neighboringverbs and prepositions (Cohen et al, 2005).
Whilenewswire documents are mostly unstructured, struc-tural features are used to indicate whether any of thepurchaser, acquired and seller text spans appears in6In this work, we ignore other fields annotated, as they areinconsistently defined, have low number of occurrences in thecorpus, and are loosely inter-related semantically.851purname purabr purcode acqname acqabr acqcode sellname sellabr sellcodeTIE (batch) 55.7 58.1 - 53.5 55.0 - 31.8 25.8 -TIE (inc) 51.6 55.3 - 49.2 51.7 - 26.0 24.0 -Full model 48.9 55.0 70.2 50.7 55.2 67.2 33.2 36.8 55.4Model variants:No inter-type and struct.
ftrs 45.1 50.5 66.8 49.8 53.9 66.4 34.9 42.2 56.0No semantic features 42.6 38.4 58.1 40.5 36.5 44.8 32.2 26.6 46.6Individual roles 43.9 48.7 62.5 45.0 47.2 52.7 34.1 40.3 47.8Table 4: Corp. acquisition extraction results: Field-level F1purname purabr purcode acqname acqabr acqcode sellname sellabr sellcodeTIE (batch) 52.6 40.5 - 49.2 43.7 28.7 16.4 -TIE (inc) 48.4 38.6 - 44.7 42.7 - 23.6 14.5 -Full model 45.0 48.3 69.8 46.4 59.5 66.9 31.6 33.0 55.0Table 5: Corp. acquisition extraction results: Entity-level F1the article?s header.
Semantic features are appliedto corp tuples: we model whether the abbreviatedname is a subset of the full name; whether the cor-porate code forms exact initials of the full or abbre-viated names; or whether it has high string similarityto any of these values.
Finally, cross-type featuresencode the shortest string between spans mappingto different roles in the acquisition relation.Experiments We applied beam search, wherecorp tuples are extracted first, and acquisition tuplesare constructed using the top scoring corp entities.We used a default beam size k = 10.
The dataset issplit into a 300/300 train/test subsets.Table 4 shows results of our full model in terms offield-level F1, compared against TIE, a state-of-the-art discriminative system (Siefkes, 2008).
Unfortu-nately, we can not directly compare against a gener-ative joint model evaluated on this dataset (Haghighiand Klein, 2010).7 The best results per attribute areshown in boldface.
Our full model performs bet-ter overall than TIE trained incrementally (similarlyto our system), and is competitive with TIE usingbatch learning.
Interestingly, the performance of ourmodel on the code fields is high; these fields donot involve boundary prediction, and thus reflect thequality of role assignment.Table 4 also shows the results of model vari-ants.
Removing the inter type and structural fea-tures mildly hurt performance, on average.
In con-trast, the semantic features, which account for thesemantic cohesiveness of the populated corp tuples,are shown to be necessary.
In particular, remov-7They report average performance on a different set offields; in addition, their results include modeling of pronounsand nominal mentions, which are not considered here.ing them degrades the extraction of the abbreviatednames; these features allow prediction of abbrevi-ated names jointly with the full corporate names,which are more regular (e.g., include a distinctivesuffix).
Finally, we show results of predicting eachrole filler individually.
Inferring the roles jointly(?full model?)
significantly improves performance.Table 5 further shows results on NER, the task ofrecovering the sets of named entity mentions per-taining to each target field.
As shown, the proposedjoint approach performs overall significantly betterthan previous results reported.
These results are con-sistent with the case study of seminar extraction.7 Summary and Future WorkWe presented a joint approach for template fillingthat models mention detection, unification, and fieldextraction in a flexible, feature-rich model.
This ap-proach allows for joint modeling of interdependen-cies at all levels and across fields.
Despite the com-putational challenges of this joint inference space,we obtained effective learning with a Perceptron-style approach and simple beam decoding.An interesting direction of future research isto apply reranking to the output list of candidaterecords using additional evidence, such as support-ing evidence on the Web (Banko et al, 2008).
Also,modeling additional features or feature combina-tions in this framework as well as effective featureselection or improved parameter estimation (Cram-mer et al, 2009) may boost performance.
Finally,it is worth exploring scaling the approach to unre-stricted event extraction, and jointly model extract-ing more than one relation per document.852ReferencesMichele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2008.
Open in-formation extraction from the web.
In Proceedings ofIJCAI.Mary Elaine Califf and Raymond J. Mooney.
1999.
Re-lational learning of pattern-match rules for informationextraction.
In AAAI/IAAI.Mary Elaine Califf and Raymond J. Mooney.
2003.Bottom-up relational learning of pattern matchingrules for information extraction.
Journal of MachineLearning Research, 4.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In Proceedings of WSDM.William W. Cohen, Einat Minkov, and Anthony Toma-sic.
2005.
Learning to understand web site update re-quests.
In Proceedings of the international joint con-ference on Artificial intelligence (IJCAI).Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Conference onEmpirical Methods in Natural Language Processing(EMNLP).Koby Crammer, Alex Kulesza, and Mark Dredze.
2009.Adaptive regularization of weight vectors.
In Ad-vances in Neural Information Processing Systems(NIPS).Jenny Rose Finkel, Trond Grenager, , and Christopher D.Manning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of ACL.Aidan Finn.
2006.
A multi-level boundary classificationapproach to information extraction.
In PhD thesis.Dayne Freitag and Andrew McCallum.
2000.
In-formation extraction with hmm structures learned bystochastic optimization.
In AAAI/IAAI.Yoav Freund and Rob Schapire.
1999.
Large marginclassification using the perceptron algorithm.
MachineLearning, 37(3).Aria Haghighi and Dan Klein.
2010.
An entity-level ap-proach to information extraction.
In Proceedings ofACL.Einat Minkov, Richard C. Wang, and William W. Cohen.2005.
Extracting personal names from emails: Ap-plying named entity recognition to informal text.
InHLT/EMNLP.Minorthird.
2008.
Methods for identifying names andontological relations in text using heuristics for in-ducing regularities from data.
http://http://minorthird.sourceforge.net.Leonid Peshkin and Avi Pfeffer.
2003.
Bayesian infor-mation extraction network.
In Proceedings of the in-ternational joint conference on Artificial intelligence(IJCAI).Dan Roth and Wen-tau Yih.
2001.
Relational learningvia propositional algorithms: An information extrac-tion case study.
In Proceedings of the internationaljoint conference on Artificial intelligence (IJCAI).Dan Roth and Wen-tau Yih.
2002.
Probabilistic reason-ing for entity and relation recognition.
In COLING.Christian Siefkes.
2008.
In An Incrementally TrainableStatistical Approach to Information Extraction.
VDMVerlag.Charles Sutton and Andrew McCallum.
2004.
Collec-tive segmentation and labeling of distant entities in in-formation extraction.
In Technical Report no.
04-49,University of Massachusetts.Limin Yao, Aria Haghighi, Sebastian Riedel, and AndrewMcCallum.
2011.
Structured relation discovery usinggenerative models.
In Proceedings of EMNLP.Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-Ying Ma.
2006.
Simultaneous record detection andattribute labeling in web data extraction.
In Proc.
ofthe ACM SIGKDD Intl.
Conf.
on Knowledge Discoveryand Data Mining (KDD).853
