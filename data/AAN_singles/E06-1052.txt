Investigating a Generic Paraphrase-based Approachfor Relation ExtractionLorenza RomanoITC-irstvia Sommarive, 1838050 Povo (TN), Italyromano@itc.itMilen KouylekovITC-irstvia Sommarive, 1838050 Povo (TN), Italykouylekov@itc.itIdan SzpektorDepartment of Computer ScienceBar Ilan UniversityRamat Gan, 52900, Israelszpekti@cs.biu.ac.ilIdo DaganDepartment of Computer ScienceBar Ilan UniversityRamat Gan, 52900, Israeldagan@cs.biu.ac.ilAlberto LavelliITC-irstvia Sommarive, 1838050 Povo (TN), Italylavelli@itc.itAbstractUnsupervised paraphrase acquisition hasbeen an active research field in recentyears, but its effective coverage and per-formance have rarely been evaluated.
Wepropose a generic paraphrase-based ap-proach for Relation Extraction (RE), aim-ing at a dual goal: obtaining an applicativeevaluation scheme for paraphrase acquisi-tion and obtaining a generic and largelyunsupervised configuration for RE.We an-alyze the potential of our approach andevaluate an implemented prototype of itusing an RE dataset.
Our findings reveal ahigh potential for unsupervised paraphraseacquisition.
We also identify the need fornovel robust models for matching para-phrases in texts, which should address syn-tactic complexity and variability.1 IntroductionA crucial challenge for semantic NLP applica-tions is recognizing the many different ways forexpressing the same information.
This seman-tic variability phenomenon was addressed withinspecific applications, such as question answering,information extraction and information retrieval.Recently, the problem was investigated withingeneric application-independent paradigms, suchas paraphrasing and textual entailment.Eventually, it would be most appealing to applygeneric models for semantic variability to concreteapplications.
This paper investigates the applica-bility of a generic ?paraphrase-based?
approach tothe Relation Extraction (RE) task, using an avail-able RE dataset of protein interactions.
RE ishighly suitable for such investigation since its goalis to exactly identify all the different variations inwhich a target semantic relation can be expressed.Taking this route sets up a dual goal: (a) fromthe generic paraphrasing perspective - an objectiveevaluation of paraphrase acquisition performanceon a concrete application dataset, as well as identi-fying the additional mechanisms needed to matchparaphrases in texts; (b) from the RE perspective -investigating the feasibility and performance of ageneric paraphrase-based approach for RE.Our configuration assumes a set of entailingtemplates (non-symmetric ?paraphrases?)
for thetarget relation.
For example, for the target rela-tion ?X interact with Y?
we would assume a set ofentailing templates as in Tables 3 and 7.
In addi-tion, we require a syntactic matching module thatidentifies template instances in text.First, we manually analyzed the protein-interaction dataset and identified all cases in whichprotein interaction is expressed by an entailingtemplate.
This set a very high idealized upperbound for the recall of the paraphrase-based ap-proach for this dataset.
Yet, obtaining high cover-age in practice would require effective paraphraseacquisition and lexical-syntactic template match-ing.
Next, we implemented a prototype that uti-lizes a state-of-the-art method for learning en-tailment relations from the web (Szpektor et al,2004), the Minipar dependency parser (Lin, 1998)and a syntactic matching module.
As expected,the performance of the implemented system wasmuch lower than the ideal upper bound, yet ob-taining quite reasonable practical results given itsunsupervised nature.The contributions of our investigation follow409the dual goal set above.
To the best of our knowl-edge, this is the first comprehensive evaluationthat measures directly the performance of unsuper-vised paraphrase acquisition relative to a standardapplication dataset.
It is also the first evaluation ofa generic paraphrase-based approach for the stan-dard RE setting.
Our findings are encouraging forboth goals, particularly relative to their early ma-turity level, and reveal constructive evidence forthe remaining room for improvement.2 Background2.1 Unsupervised Information ExtractionInformation Extraction (IE) and its subfield Rela-tion Extraction (RE) are traditionally performedin a supervised manner, identifying the differentways to express a specific information or relation.Given that annotated data is expensive to produce,unsupervised or weakly supervised methods havebeen proposed for IE and RE.Yangarber et al (2000) and Stevenson andGreenwood (2005) define methods for automaticacquisition of predicate-argument structures thatare similar to a set of seed relations, which rep-resent a specific scenario.
Yangarber et al (2000)approach was evaluated in two ways: (1) manuallymapping the discovered patterns into an IE systemand running a full MUC-style evaluation; (2) usingthe learned patterns to perform document filteringat the scenario level.
Stevenson and Greenwood(2005) evaluated their method through documentand sentence filtering at the scenario level.Sudo et al (2003) extract dependency subtreeswithin relevant documents as IE patterns.
The goalof the algorithm is event extraction, though perfor-mance is measured by counting argument entitiesrather than counting events directly.Hasegawa et al (2004) performs unsupervisedhierarchical clustering over a simple set of fea-tures.
The algorithm does not extract entity pairsfor a given relation from a set of documents butrather classifies all relations in a large corpus.
Thisapproach is more similar to text mining tasks thanto classic IE problems.To conclude, several unsupervised approacheslearn relevant IE templates for a complete sce-nario, but without identifying their relevance toeach specific relation within the scenario.
Accord-ingly, the evaluations of these works either did notaddress the direct applicability for RE or evaluatedit only after further manual postprocessing.2.2 Paraphrases and Entailment RulesA generic model for language variability is us-ing paraphrases, text expressions that roughly con-vey the same meaning.
Various methods for auto-matic paraphrase acquisition have been suggestedrecently, ranging from finding equivalent lexicalelements to learning rather complex paraphrasesat the sentence level1.More relevant for RE are ?atomic?
paraphrasesbetween templates, text fragments containing vari-ables, e.g.
?X buy Y ?
X purchase Y?.
Under asyntactic representation, a template is a parsed textfragment, e.g.
?X subj?
interact mod?
with pcomp?n?
Y?
(based on the syntactic dependency relations ofthe Minipar parser).
The parses include part-of-speech tags, which we omit for clarity.Dagan and Glickman (2004) suggested that asomewhat more general notion than paraphrasingis that of entailment relations.
These are direc-tional relations between two templates, where themeaning of one can be entailed from the meaningof the other, e.g.
?X bind to Y?
X interact with Y?.For RE, when searching for a target relation, it issufficient to identify an entailing template since itimplies that the target relation holds as well.
Un-der this notion, paraphrases are bidirectional en-tailment relations.Several methods extract atomic paraphrases byexhaustively processing local corpora (Lin andPantel, 2001; Shinyama et al, 2002).
Learn-ing from a local corpus is bounded by the cor-pus scope, which is usually domain specific (bothworks above processed news domain corpora).
Tocover a broader range of domains several worksutilized the Web, while requiring several manu-ally provided examples for each input relation,e.g.
(Ravichandran and Hovy, 2002).
Taking astep further, the TEASE algorithm (Szpektor et al,2004) provides a completely unsupervised methodfor acquiring entailment relations from the Webfor a given input relation (see Section 5.1).Most of these works did not evaluate their re-sults in terms of application coverage.
Lin andPantel (2001) compared their results to human-generated paraphrases.
Shinyama et al (2002)measured the coverage of their learning algorithmrelative to the paraphrases present in a given cor-pus.
Szpektor et al (2004) measured ?yield?, thenumber of correct rules learned for an input re-1See the 3rd IWP workshop for a sample of recent workson paraphrasing (http://nlp.nagaokaut.ac.jp/IWP2005/).410lation.
Ravichandran and Hovy (2002) evaluatedthe performance of a QA system that is basedsolely on paraphrases, an approach resemblingours.
However, they measured performance usingMean Reciprocal Rank, which does not reveal theactual coverage of the learned paraphrases.3 Assumed Configuration for REPhenomenon ExamplePassive form ?Y is activated by X?Apposition ?X activates its companion, Y?Conjunction ?X activates prot3 and Y?Set ?X activates two proteins, Y and Z?Relative clause ?X, which activates Y?Coordination ?X binds and activates Y?Transparent head ?X activates a fragment of Y?Co-reference ?X is a kinase, though it activates Y?Table 1: Syntactic variability phenomena, demon-strated for the normalized template ?X activate Y?.The general configuration assumed in this pa-per for RE is based on two main elements: a listof lexical-syntactic templates which entail the re-lation of interest and a syntactic matcher whichidentifies the template occurrences in sentences.The set of entailing templates may be collected ei-ther manually or automatically.
We propose thisconfiguration both as an algorithm for RE and asan evaluation scheme for paraphrase acquisition.The role of the syntactic matcher is to iden-tify the different syntactic variations in which tem-plates occur in sentences.
Table 1 presents a listof generic syntactic phenomena that are known inthe literature to relate to linguistic variability.
Aphenomenon which deserves a few words of ex-planation is the ?transparent head noun?
(Grish-man et al, 1986; Fillmore et al, 2002).
A trans-parent noun N1 typically occurs in constructs ofthe form ?N1 preposition N2?
for which the syn-tactic relation involving N1, which is the head ofthe NP, applies to N2, the modifier.
In the examplein Table 1, ?fragment?
is the transparent head nounwhile the relation ?activate?
applies to Y as object.4 Manual Data Analysis4.1 Protein Interaction DatasetBunescu et al (2005) proposed a set of tasks re-garding protein name and protein interaction ex-traction, for which they manually tagged about200 Medline abstracts previously known to con-tain human protein interactions (a binary symmet-ric relation).
Here we consider their RE task ofextracting interacting protein pairs, given that thecorrect protein names have already been identi-fied.
All protein names are annotated in the givengold standard dataset, which includes 1147 anno-tated interacting protein pairs.
Protein names arerather complex, and according to the annotationadopted by Bunescu et al (2005) can be substringsof other protein names (e.g., <prot> <prot>GITR </prot> ligand </prot>).
In suchcases, we considered only the longest names andprotein pairs involving them.
We also ignored allreflexive pairs, in which one protein is markedas interacting with itself.
Altogether, 1052 inter-actions remained.
All protein names were trans-formed into symbols of the type ProtN , where Nis a number, which facilitates parsing.For development purposes, we randomly splitthe abstracts into a 60% development set (575 in-teractions) and a 40% test set (477 interactions).4.2 Dataset analysisIn order to analyze the potential of our approach,two of the authors manually annotated the 575 in-teracting protein pairs in the development set.
Foreach pair the annotators annotated whether it canbe identified using only template-based matching,assuming an ideal implementation of the configu-ration of Section 3.
If it can, the normalized formof the template connecting the two proteins wasannotated as well.
The normalized template formis based on the active form of the verb, strippedof the syntactic phenomena listed in Table 1.
Ad-ditionally, the relevant syntactic phenomena fromTable 1 were annotated for each template instance.Table 2 provides several example annotations.A Kappa value of 0.85 (nearly perfect agree-ment) was measured for the agreement betweenthe two annotators, regarding whether a proteinpair can be identified using the template-basedmethod.
Additionally, the annotators agreed on96% of the normalized templates that should beused for the matching.
Finally, the annotatorsagreed on at least 96% of the cases for each syn-tactic phenomenon except transparent heads, forwhich they agreed on 91% of the cases.
This highlevel of agreement indicates both that template-based matching is a well defined task and that nor-malized template form and its syntactic variationsare well defined notions.Several interesting statistics arise from the an-411Sentence AnnotationWe have crystallized a complex between human FGF1 anda two-domain extracellular fragment of human FGFR2.?
template: ?complex between X and Y??
transparent head: ?fragment of X?CD30 and its counter-receptor CD30 ligand (CD30L) aremembers of the TNF-receptor / TNFalpha superfamily andfunction to regulate lymphocyte survival and differentiation.?
template: ?X?s counter-receptor Y??
apposition?
co-referenceiCdi1, a human G1 and S phase protein phosphatase thatassociates with Cdk2.?
template: ?X associate with Y??
relative clauseTable 2: Examples of annotations of interacting protein pairs.
The annotation describes the normalizedtemplate and the different syntactic phenomena identified.Template f Template f Template fX interact with Y 28 interaction of X with Y 12 X Y interaction 5X bind to Y 22 X associate with Y 11 X interaction with Y 4X Y complex 17 X activate Y 6 association of X with Y 4interaction between X and Y 16 binding of X to Y 5 X?s association with Y 3X bind Y 14 X form complex with Y 5 X be agonist for Y 3Table 3: The 15 most frequent templates and their instance count (f ) in the development set.notation.
First, 93% of the interacting protein pairs(537/575) can be potentially identified using thetemplate-based approach, if the relevant templatesare provided.
This is a very promising finding,suggesting that the template-based approach mayprovide most of the requested information.
Weterm these 537 pairs as template-based pairs.
Theremaining pairs are usually expressed by complexinference or at a discourse level.Phenomenon % Phenomenon %transparent head 34 relative clause 8apposition 24 co-reference 7conjunction 24 coordination 7set 13 passive form 2Table 4: Occurrence percentage of each syntacticphenomenon within template-based pairs (537).Second, for 66% of the template-based pairsat least one syntactic phenomenon was annotated.Table 4 contains the occurrence percentage of eachphenomenon in the development set.
These resultsshow the need for a powerful syntactic matcher ontop of high performance template acquisition, inorder to correctly match a template in a sentence.Third, 175 different normalized templates wereidentified.
For each template we counted its tem-plate instances, the number of times the tem-plate occurred, counting only occurrences that ex-press an interaction of a protein pair.
In total,we counted 341 template instances for all 175templates.
Interestingly, 50% of the template in-stances (184/341) are instances of the 21 most fre-quent templates.
This shows that, though proteininteraction can be expressed in many ways, writ-ers tend to choose from among just a few commonexpressions.
Table 3 presents the most frequenttemplates.
Table 5 presents the minimal numberof templates required to obtain the range of differ-ent recall levels.Furthermore, we grouped template variantsthat are based on morphological derivations (e.g.
?X interact with Y?
and ?X Y interaction?
)and found that 4 groups, ?X interact with Y?,?X bind to Y?, ?X associate with Y?
and ?X com-plex with Y?, together with their morphologicalderivations, cover 45% of the template instances.This shows the need to handle generic lexical-syntactic phenomena, and particularly morpholog-ical based variations, separately from the acquisi-tion of normalized lexical syntactic templates.To conclude, this analysis indicates that thetemplate-based approach provides very high cov-erage for this RE dataset, and a small number ofnormalized templates already provides significantrecall.
However, it is important to (a) developa model for morphological-based template vari-ations (e.g.
as encoded in Nomlex (Macleod etal., )), and (b) apply accurate parsing and developsyntactic matching models to recognize the rather412complex variations of template instantiations intext.
Finally, we note that our particular figuresare specific to this dataset and the biological ab-stracts domain.
However, the annotation and anal-ysis methodologies are general and are suggestedas highly effective tools for further research.R(%) # templates R(%) # templates10 2 60 3920 4 70 7330 6 80 10740 11 90 14150 21 100 175Table 5: The number of most frequent templatesnecessary to reach different recall levels within the341 template instances.5 Implemented PrototypeThis section describes our initial implementationof the approach in Section 3.5.1 TEASEThe TEASE algorithm (Szpektor et al, 2004) isan unsupervised method for acquiring entailmentrelations from the Web for a given input template.In this paper we use TEASE for entailment rela-tion acquisition since it processes an input tem-plate in a completely unsupervised manner anddue to its broad domain coverage obtained fromthe Web.
The reported percentage of correct out-put templates for TEASE is 44%.The TEASE algorithm consists of 3 steps,demonstrated in Table 6.
TEASE first retrievesfrom the Web sentences containing the input tem-plate.
From these sentences it extracts variable in-stantiations, termed anchor-sets, which are identi-fied as being characteristic for the input templatebased on statistical criteria (first column in Ta-ble 6).
Characteristic anchor-sets are assumed touniquely identify a specific event or fact.
Thus,any template that appears with such an anchor-setis assumed to have an entailment relationship withthe input template.
Next, TEASE retrieves fromthe Web a corpus S of sentences that contain thecharacteristic anchor-sets (second column), hop-ing to find occurrences of these anchor-sets withintemplates other than the original input template.Finally, TEASE parses S and extracts templatesthat are assumed to entail or be entailed by theinput template.
Such templates are identified asmaximal most general sub-graphs that contain theanchor sets?
positions (third column in Table 6).Each learned template is ranked by number of oc-currences in S.5.2 Transformation-based Graph MatcherIn order to identify instances of entailing templatesin sentences we developed a syntactic matcher thatis based on transformations rules.
The matcherprocesses a sentence in 3 steps: 1) parsing the sen-tence with the Minipar parser, obtaining a depen-dency graph2; 2) matching each template againstthe sentence dependency graph; 3) extracting can-didate term pairs that match the template variables.A template is considered directly matched in asentence if it appears as a sub-graph in the sen-tence dependency graph, with its variables instan-tiated.
To further address the syntactic phenomenalisted in Table 1 we created a set of hand-craftedparser-dependent transformation rules, which ac-count for the different ways in which syntacticrelationships may be realized in a sentence.
Atransformation rule maps the left hand side of therule, which strictly matches a sub-graph of thegiven template, to the right hand side of the rule,which strictly matches a sub-graph of the sentencegraph.
If a rule matches, the template sub-graph ismapped accordingly into the sentence graph.For example, to match the syntactic tem-plate ?X(N) subj?
activate(V) obj?
Y(N)?
(POStags are in parentheses) in the sentence ?Prot1detected and activated Prot2?
(see Figure 1) weshould handle the coordination phenomenon.The matcher uses the transformation rule?Var1(V) ?
and(U)mod?
Word(V) conj?
Var1(V)?to overcome the syntactic differences.
In thisexample Var1 matches the verb ?activate?, Wordmatches the verb ?detect?
and the syntactic rela-tions for Word are mapped to the ones for Var1.Thus, we can infer that the subject and objectrelations of ?detect?
are also related to ?activate?.6 Experiments6.1 Experimental SettingsTo acquire a set of entailing templates we first ex-ecuted TEASE on the input template ?X subj?
in-teract mod?
with pcomp?n?
Y?, which corresponds tothe ?default?
expression of the protein interaction2We chose a dependency parser as it captures directly therelations between words; we use Minipar due to its speed.413Extracted Anchor-set Sentence containing Anchor-set Learned TemplateX=?chemokines?,Y=?specific receptors?Chemokines bind to specific receptors on the targetcellsX subj?
bind mod?topcomp?n?
YX=?Smad3?, Y=?Smad4?
Smad3 / Smad4 complexes translocate to the nucleus X Y nn?
complexTable 6: TEASE output at different steps of the algorithm for ?X subj?
interact mod?
with pcomp?n?
Y?.1.
X bind to Y 7.
X Y complex 13.
X interaction with Y2.
X activate Y 8.
X recognize Y 14.
X trap Y3.
X stimulate Y 9.
X block Y 15.
X recruit Y4.
X couple to Y 10.
X binding to Y 16.
X associate with Y5.
interaction between X and Y 11.
X Y interaction 17.
X be linked to Y6.
X become trapped in Y 12.
X attach to Y 18.
X target YTable 7: The top 18 correct templates learned by TEASE for ?X interact with Y?.detect(V )subjwwpppppppppppconjmod''NNNNNNNNNNNobj // Prot2(N)Prot1(N) activate(V ) and(U)Figure 1: The dependency parse graph of the sen-tence ?Prot1 detected and activated Prot2?.relation.
TEASE learned 118 templates for thisrelation.
Table 7 lists the top 18 learned templatesthat we considered as correct (out of the top 30templates in TEASE output).
We then extractedinteracting protein pair candidates by applying thesyntactic matcher to the 119 templates (the 118learned plus the input template).
Candidate pairsthat do not consist of two proteins, as tagged in theinput dataset, were filtered out (see Section 4.1;recall that our experiments were applied to thedataset of protein interactions, which isolates theRE task from the protein name recognition task).In a subsequent experiment we iteratively ex-ecuted TEASE on the 5 top-ranked learned tem-plates to acquire additional relevant templates.
Intotal, we obtained 1233 templates that were likelyto imply the original input relation.
The syntacticmatcher was then reapplied to extract candidate in-teracting protein pairs using all 1233 templates.We used the development set to tune a smallset of 10 generic hand-crafted transformation rulesthat handle different syntactic variations.
To han-dle transparent head nouns, which is the only phe-nomenon that demonstrates domain dependence,we extracted a set of the 5 most frequent trans-parent head patterns in the development set, e.g.
?fragment of X?.In order to compare (roughly) our performancewith supervised methods applied to this dataset, assummarized in (Bunescu et al, 2005), we adoptedtheir recall and precision measurement.
Theirscheme counts over distinct protein pairs per ab-stract, which yields 283 interacting pairs in our testset and 418 in the development set.6.2 Manual Analysis of TEASE Recallexperiment pairs instancesinput 39% 37%input + iterative 49% 48%input + iterative + morph 63% 62%Table 8: The potential recall of TEASE in terms ofdistinct pairs (out of 418) and coverage of templateinstances (out of 341) in the development set.Before evaluating the system as a whole wewanted to manually assess in isolation the cover-age of TEASE output relative to all template in-stances that were manually annotated in the devel-opment set.
We considered a template as coveredif there is a TEASE output template that is equalto the manually annotated template or differs fromit only by the syntactic phenomena described inSection 3 or due to some parsing errors.
Count-ing these matches, we calculated the number oftemplate instances and distinct interacting proteinpairs that are covered by TEASE output.Table 8 presents the results of our analysis.
The4141st line shows the coverage of the 119 templateslearned by TEASE for the input template ?X inter-act with Y?.
It is interesting to note that, though weaim to learn relevant templates for the specific do-main, TEASE learned relevant templates also byfinding anchor-sets of different domains that usethe same jargon, such as particle physics.We next analyzed the contribution of the itera-tive learning for the additional 5 templates to recall(2nd line in Table 8).
With the additional learnedtemplates, recall increased by about 25%, showingthe importance of using the iterative steps.Finally, when allowing matching between aTEASE template and a manually annotated tem-plate, even if one is based on a morphologi-cal derivation of the other (3rd line in Table 8),TEASE recall increased further by about 30%.We conclude that the potential recall of the cur-rent version of TEASE on the protein interactiondataset is about 60%.
This indicates that signif-icant coverage can be obtained using completelyunsupervised learning from the web, as performedby TEASE.
However, the upper bound for our cur-rent implemented system is only about 50% be-cause our syntactic matching does not handle mor-phological derivations.6.3 System Resultsexperiment recall precision F1input 0.18 0.62 0.28input + iterative 0.29 0.42 0.34Table 9: System results on the test set.Table 9 presents our system results for the testset, corresponding to the first two experiments inTable 8.
The recall achieved by our current imple-mentation is notably worse than the upper boundof the manual analysis because of two general set-backs of the current syntactic matcher: 1) parsingerrors; 2) limited transformation rule coverage.First, the texts from the biology domain pre-sented quite a challenge for the Minipar parser.For example, in the sentences containing thephrase ?X bind specifically to Y?
the parser consis-tently attaches the PP ?to?
to ?specifically?
insteadof to ?bind?.
Thus, the template ?X bind to Y?
can-not be directly matched.Second, we manually created a small number oftransformation rules that handle various syntacticphenomena, since we aimed at generic domain in-dependent rules.
The most difficult phenomenonto model with transformation rules is transparentheads.
For example, in ?the dimerization of Prot1interacts with Prot2?, the transparent head ?dimer-ization of X?
is domain dependent.
Transforma-tion rules that handle such examples are difficultto acquire, unless a domain specific learning ap-proach (either supervised or unsupervised) is used.Finally, we did not handle co-reference resolutionin the current implementation.Bunescu et al (2005) and Bunescu and Mooney(2005) approached the protein interaction RE taskusing both handcrafted rules and several super-vised Machine Learning techniques, which uti-lize about 180 manually annotated abstracts fortraining.
Our results are not directly comparablewith theirs because they adopted 10-fold cross-validation, while we had to divide the dataset intoa development and a test set, but a rough compari-son is possible.
For the same 30% recall, the rule-based method achieved precision of 62% and thebest supervised learning algorithm achieved preci-sion of 73%.
Comparing to these supervised anddomain-specific rule-based approaches our systemis noticeably weaker, yet provides useful resultsgiven that we supply very little domain specific in-formation and acquire the paraphrasing templatesin a fully unsupervised manner.
Still, the match-ing models need considerable additional researchin order to achieve the potential performance sug-gested by TEASE.7 Conclusions and Future WorkWe have presented a paraphrase-based approachfor relation extraction (RE), and an implementedsystem, that rely solely on unsupervised para-phrase acquisition and generic syntactic templatematching.
Two targets were investigated: (a) amostly unsupervised, domain independent, con-figuration for RE, and (b) an evaluation schemefor paraphrase acquisition, providing a first evalu-ation of its realistic coverage.
Our approach differsfrom previous unsupervised IE methods in that weidentify instances of a specific relation while priormethods identified template relevance only at thegeneral scenario level.We manually analyzed the potential of our ap-proach on a dataset annotated with protein in-teractions.
The analysis shows that 93% of theinteracting protein pairs can be potentially iden-tified with the template-based approach.
Addi-415tionally, we manually assessed the coverage ofthe TEASE acquisition algorithm and found that63% of the distinct pairs can be potentially rec-ognized with the learned templates, assuming anideal matcher, indicating a significant potential re-call for completely unsupervised paraphrase ac-quisition.
Finally, we evaluated our current systemperformance and found it weaker than supervisedRE methods, being far from fulfilling the poten-tial indicated in our manual analyses due to insuf-ficient syntactic matching.
But, even our currentperformance may be considered useful given thevery small amount of domain-specific informationused by the system.Most importantly, we believe that our analysisand evaluation methodologies for an RE datasetprovide an excellent benchmark for unsupervisedlearning of paraphrases and entailment rules.
Inthe long run, we plan to develop and improve ouracquisition and matching algorithms, in order torealize the observed potential of the paraphrase-based approach.
Notably, our findings point to theneed to learn generic morphological and syntacticvariations in template matching, an area which hasrarely been addressed till now.AcknowledgementsThis work was developed under the collaborationITC-irst/University of Haifa.
Lorenza Romanohas been supported by the ONTOTEXT project,funded by the Autonomous Province of Trento un-der the FUP-2004 research program.ReferencesRazvan Bunescu and Raymond J. Mooney.
2005.
Sub-sequence kernels for relation extraction.
In Proceed-ings of the 19th Conference on Neural InformationProcessing Systems, Vancouver, British Columbia.Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-ward M. Marcotte, Raymond J. Mooney, Arun K.Ramani, and Yuk Wah Wong.
2005.
Comparativeexperiments on learning information extractors forproteins and their interactions.
Artificial Intelligencein Medicine, 33(2):139?155.
Special Issue on Sum-marization and Information Extraction from Medi-cal Documents.Ido Dagan and Oren Glickman.
2004.
Probabilis-tic textual entailment: Generic applied modeling oflanguage variability.
In Proceedings of the PAS-CAL Workshop on Learning Methods for Text Un-derstanding and Mining, Grenoble, France.Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.2002.
Seeing arguments through transparent struc-tures.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC 2002), pages 787?791, Las Palmas, Spain.Ralph Grishman, Lynette Hirschman, and Ngo ThanhNhan.
1986.
Discovery procedures for sublanguageselectional patterns: Initial experiments.
Computa-tional Linguistics, 12(3).Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-man.
2004.
Discoverying relations among namedentities from large corpora.
In Proceedings of the42nd Annual Meeting of the Association for Compu-tational Linguistics (ACL 2004), Barcelona, Spain.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
Natural Lan-guage Engineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluation onMINIPAR.
In Proceedings of LREC-98 Workshopon Evaluation of Parsing Systems, Granada, Spain.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
Nomlex: A lexi-con of nominalizations.
In Proceedings of the 8thInternational Congress of the European Associationfor Lexicography, Liege, Belgium.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing surface text patterns for a Question Answeringsystem.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL 2002), Philadelphia, PA.Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, andRalph Grishman.
2002.
Automatic paraphrase ac-quisition from news articles.
In Proceedings ofthe Human Language Technology Conference (HLT2002), San Diego, CA.Mark Stevenson and Mark A. Greenwood.
2005.
Asemantic approach to IE pattern induction.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2005), AnnArbor, Michigan.K.
Sudo, S. Sekine, and R. Grishman.
2003.
An im-proved extraction pattern representation model forautomatic IE pattern acquisition.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL 2003), Sapporo, Japan.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisi-tion of entailment relations.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2004), Barcelona,Spain.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Automatic acquisitionof domain knowledge for information extraction.
InProceedings of the 18th International Conference onComputational Linguistics, Saarbruecken, Germany.416
