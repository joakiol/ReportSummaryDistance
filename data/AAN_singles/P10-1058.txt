Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565?574,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAutomatic Generation of Story HighlightsKristian Woodsend and Mirella LapataSchool of Informatics, University of EdinburghEdinburgh EH8 9AB, United Kingdomk.woodsend@ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we present a joint con-tent selection and compression modelfor single-document summarization.
Themodel operates over a phrase-based rep-resentation of the source document whichwe obtain by merging information fromPCFG parse trees and dependency graphs.Using an integer linear programming for-mulation, the model learns to select andcombine phrases subject to length, cover-age and grammar constraints.
We evalu-ate the approach on the task of generat-ing ?story highlights?
?a small number ofbrief, self-contained sentences that allowreaders to quickly gather information onnews stories.
Experimental results showthat the model?s output is comparable tohuman-written highlights in terms of bothgrammaticality and content.1 IntroductionSummarization is the process of condensing asource text into a shorter version while preservingits information content.
Humans summarize ona daily basis and effortlessly, but producing highquality summaries automatically remains a chal-lenge.
The difficulty lies primarily in the natureof the task which is complex, must satisfy manyconstraints (e.g., summary length, informative-ness, coherence, grammaticality) and ultimatelyrequires wide-coverage text understanding.
Sincethe latter is beyond the capabilities of current NLPtechnology, most work today focuses on extractivesummarization, where a summary is created sim-ply by identifying and subsequently concatenatingthe most important sentences in a document.Without a great deal of linguistic analysis, itis possible to create summaries for a wide rangeof documents.
Unfortunately, extracts are of-ten documents of low readability and text qualityand contain much redundant information.
This isin marked contrast with hand-written summarieswhich often combine several pieces of informa-tion from the original document (Jing, 2002) andexhibit many rewrite operations such as substitu-tions, insertions, deletions, or reorderings.Sentence compression is often regarded as apromising first step towards ameliorating some ofthe problems associated with extractive summa-rization.
The task is commonly expressed as aword deletion problem.
It involves creating a shortgrammatical summary of a single sentence, by re-moving elements that are considered extraneous,while retaining the most important information(Knight and Marcu, 2002).
Interfacing extractivesummarization with a sentence compression mod-ule could improve the conciseness of the gener-ated summaries and render them more informative(Jing, 2000; Lin, 2003; Zajic et al, 2007).Despite the bulk of work on sentence compres-sion and summarization (see Clarke and Lapata2008 and Mani 2001 for overviews) only a handfulof approaches attempt to do both in a joint model(Daume?
III and Marcu, 2002; Daume?
III, 2006;Lin, 2003; Martins and Smith, 2009).
One rea-son for this might be the performance of sentencecompression systems which falls short of attaininggrammaticality levels of human output.
For ex-ample, Clarke and Lapata (2008) evaluate a rangeof state-of-the-art compression systems across dif-ferent domains and show that machine generatedcompressions are consistently perceived as worsethan the human gold standard.
Another reason isthe summarization objective itself.
If our goal isto summarize news articles, then we may be bet-ter off selecting the first n sentences of the docu-ment.
This ?lead?
baseline may err on the side ofverbosity but at least will be grammatical, and ithas indeed proved extremely hard to outperformby more sophisticated methods (Nenkova, 2005).In this paper we propose a model for sum-565marization that incorporates compression into thetask.
A key insight in our approach is to formulatesummarization as a phrase rather than sentenceextraction problem.
Compression falls naturallyout of this formulation as only phrases deemedimportant should appear in the summary.
Ob-viously, our output summaries must meet addi-tional requirements such as sentence length, over-all length, topic coverage and, importantly, gram-maticality.
We combine phrase and dependencyinformation into a single data structure, which al-lows us to express grammaticality as constraintsacross phrase dependencies.
We encode these con-straints through the use of integer linear program-ming (ILP), a well-studied optimization frame-work that is able to search the entire solution spaceefficiently.We apply our model to the task of generat-ing highlights for a single document.
Examplesof CNN news articles with human-authored high-lights are shown in Table 1.
Highlights give abrief overview of the article to allow readers toquickly gather information on stories, and usuallyappear as bullet points.
Importantly, they repre-sent the gist of the entire document and thus of-ten differ substantially from the first n sentencesin the article (Svore et al, 2007).
They are alsohighly compressed, written in a telegraphic styleand thus provide an excellent testbed for modelsthat generate compressed summaries.
Experimen-tal results show that our model?s output is compa-rable to hand-written highlights both in terms ofgrammaticality and informativeness.2 Related workMuch effort in automatic summarization has beendevoted to sentence extraction which is often for-malized as a classification task (Kupiec et al,1995).
Given appropriately annotated trainingdata, a binary classifier learns to predict foreach document sentence if it is worth extracting.Surface-level features are typically used to sin-gle out important sentences.
These include thepresence of certain key phrases, the position ofa sentence in the original document, the sentencelength, the words in the title, the presence ofproper nouns, etc.
(Mani, 2001; Sparck Jones,1999).Relatively little work has focused on extractionmethods for units smaller than sentences.
Jing andMcKeown (2000) first extract sentences, then re-move redundant phrases, and use (manual) recom-bination rules to produce coherent output.
Wanand Paris (2008) segment sentences heuristicallyinto clauses before extraction takes place, andshow that this improves summarization quality.In the context of multiple-document summariza-tion, heuristics have also been used to remove par-enthetical information (Conroy et al, 2004; Sid-dharthan et al, 2004).
Witten et al (1999) (amongothers) extract keyphrases to capture the gist of thedocument, without however attempting to recon-struct sentences or generate summaries.A few previous approaches have attempted tointerface sentence compression with summariza-tion.
A straightforward way to achieve this is byadopting a two-stage architecture (e.g., Lin 2003)where the sentences are first extracted and thencompressed or the other way round.
Other workimplements a joint model where words and sen-tences are deleted simultaneously from a docu-ment.
Using a noisy-channel model, Daume?
IIIand Marcu (2002) exploit the discourse structureof a document and the syntactic structure of itssentences in order to decide which constituents todrop but also which discourse units are unimpor-tant.
Martins and Smith (2009) formulate a jointsentence extraction and summarization model asan ILP.
The latter optimizes an objective func-tion consisting of two parts: an extraction com-ponent, essentially a non-greedy variant of max-imal marginal relevance (McDonald, 2007), anda sentence compression component, a more com-pact reformulation of Clarke and Lapata (2008)based on the output of a dependency parser.
Com-pression and extraction models are trained sepa-rately in a max-margin framework and then inter-polated.
In the context of multi-document summa-rization, Daume?
III?s (2006) vine-growth modelcreates summaries incrementally, either by start-ing a new sentence or by growing already existingones.Our own work is closest to Martins and Smith(2009).
We also develop an ILP-based compres-sion and summarization model, however, severalkey differences set our approach apart.
Firstly,content selection is performed at the phrase ratherthan sentence level.
Secondly, the combination ofphrase and dependency information into a singledata structure is new, and important in allowingus to express grammaticality as constraints acrossphrase dependencies, rather than resorting to a lan-566Most blacks say MLK?s vision fulfilled, poll findsWASHINGTON (CNN) ?
More than two-thirds of African-Americans believe Martin Luther King Jr.?s vision for racerelations has been fulfilled, a CNN poll found ?
a figure upsharply from a survey in early 2008.The CNN-Opinion Research Corp. survey was releasedMonday, a federal holiday honoring the slain civil rightsleader and a day before Barack Obama is to be sworn in asthe first black U.S. president.The poll found 69 percent of blacks said King?s vision hasbeen fulfilled in the more than 45 years since his 1963 ?I havea dream?
speech ?
roughly double the 34 percent who agreedwith that assessment in a similar poll taken last March.But whites remain less optimistic, the survey found.?
69 percent of blacks polled say Martin Luther King Jr?svision realized.?
Slim majority of whites say King?s vision not fulfilled.?
King gave his ?I have a dream?
speech in 1963.9/11 billboard draws flak from Florida Democrats, GOP(CNN) ?
A Florida man is using billboards with an image ofthe burning World Trade Center to encourage votes for a Re-publican presidential candidate, drawing criticism for politi-cizing the 9/11 attacks.
?Please Don?t Vote for a Democrat?
reads the type over thepicture of the twin towers after hijacked airliners hit them onSeptember, 11, 2001.Mike Meehan, a St.
Cloud, Florida, businessman who paid topost the billboards in the Orlando area, said former PresidentClinton should have put a stop to Osama bin Laden and alQaeda before 9/11.
He said a Republican president wouldhave done so.?
Billboards use image from 9/11 to encourage GOP votes.?
9/11 image wrong for ad, say Florida political parties.?
Floridian praises President Bush, says ex-President Clin-ton failed to stop al Qaeda.Table 1: Two example CNN news articles, showing the title and the first few paragraphs, and below, theoriginal highlights that accompanied each story.guage model.
Lastly, our model is more com-pact, has fewer parameters, and does not requiretwo training procedures.
Our approach bears someresemblance to headline generation (Dorr et al,2003; Banko et al, 2000), although we output sev-eral sentences rather than a single one.
Head-line generation models typically extract individualwords from a document to produce a very shortsummary, whereas we extract phrases and ensurethat they are combined into grammatical sentencesthrough our ILP constraints.Svore et al (2007) were the first to foregroundthe highlight generation task which we adopt as anevaluation testbed for our model.
Their approachis however a purely extractive one.
Using an al-gorithm based on neural networks and third-partyresources (e.g., news query logs and Wikipedia en-tries) they rank sentences and select the three high-est scoring ones as story highlights.
In contrast,we aim to generate rather than extract highlights.As a first step we focus on deleting extraneous ma-terial, but other more sophisticated rewrite opera-tions (e.g., Cohn and Lapata 2009) could be incor-porated into our framework.3 The TaskGiven a document, we aim to produce three or fourshort sentences covering its main topics, much likethe ?Story Highlights?
accompanying the (online)CNN news articles.
CNN highlights are written byhumans; we aim to do this automatically.Documents HighlightsSentences 37.2 ?
39.6 3.5 ?
0.5Tokens 795.0 ?
744.8 47.0 ?
9.6Tokens/sentence 22.4 ?
4.2 13.3 ?
1.7Table 2: Overview statistics on the corpus of doc-uments and highlights (mean and standard devia-tion).
A minority of documents are transcripts ofinterviews and speeches, and can be very long; thisaccounts for the very large standard deviation.Two examples of a news story and its associ-ated highlights, are shown in Table 1.
As can beseen, the highlights are written in a compressed,almost telegraphic manner.
Articles, auxiliariesand forms of the verb be are often deleted.
Com-pression is also achieved through paraphrasing,e.g., substitutions and reorderings.
For example,the document sentence ?The poll found 69 percentof blacks said King?s vision has been fulfilled.?
isrephrased in the highlight as ?69 percent of blackspolled say Martin Luther King Jr?s vision real-ized.?.
In general, there is a fair amount of lexi-cal overlap between document sentences and high-lights (42.44%) but the correspondence betweendocument sentences and highlights is not alwaysone-to-one.
In the first example in Table 1, the sec-ond paragraph gives rise to two highlights.
Alsonote that the highlights need not form a coherentsummary, each of them is relatively stand-alone,and there is little co-referencing between them.567(a)SSCCButNPNNSwhitesVPVBPremainADJPRBRlessJJoptimistic,,NPDTtheNNsurveyVPVBDfound..(b)TOPfoundoptimisticwhitesnsubjremaincoplessadvmodccompsurveythedetnsubjFigure 1: An example phrase structure (a) and dependency (b) tree for the sentence ?But whites remainless optimistic, the survey found.
?.In order to train and evaluate the model pre-sented in the following sections we created a cor-pus of document-highlight pairs (approximately9,000) which we downloaded from the CNN.comwebsite.1 The articles were randomly sampledfrom the years 2007?2009 and covered a widerange of topics such as business, crime, health,politics, showbiz, etc.
The majority were newsarticles, but the set alo contained a mixture ofeditorials, commentary, interviews and reviews.Some overview statistics of the corpus are shownin Table 2.
Overall, we observe a high degree ofcompression both at the document and sentencelevel.
The highlights summary tends to be tentimes shorter than the corresponding article.
Fur-thermore, individual highlights have almost halfthe length of document sentences.4 ModelingThe objective of our model is to create the most in-formative story highlights possible, subject to con-straints relating to sentence length, overall sum-mary length, topic coverage, and grammaticality.These constraints are global in their scope, andcannot be adequately satisfied by optimizing eachone of them individually.
Our approach thereforeuses an ILP formulation which will provide a glob-ally optimal solution, and which can be efficientlysolved using standard optimization tools.
Specif-ically, the model selects phrases from which toform the highlights, and each highlight is createdfrom a single sentence through phrase deletion.The model operates on parse trees augmented with1The corpus is available from http://homepages.inf.ed.ac.uk/mlap/resources/index.html.dependency labels.
We first describe how we ob-tain this representation and then move on to dis-cuss the model in more detail.Sentence Representation We obtain syntacticinformation by parsing every sentence twice, oncewith a phrase structure parser and once with adependency parser.
The phrase structure anddependency-based representations for the sen-tence ?But whites remain less optimistic, the sur-vey found.?
(from Table 1) are shown in Fig-ures 1(a) and 1(b), respectively.We then combine the output from the twoparsers, by mapping the dependencies to the edgesof the phrase structure tree in a greedy fashion,shown in Figure 2(a).
Starting at the top node ofthe dependency graph, we choose a node i and adependency arc to node j.
We locate the corre-sponding words i and j on the phrase structuretree, and locate their nearest shared ancestor p. Weassign the label of the dependency i?
j to the firstunlabeled edge from p to j in the phrase structuretree.
Edges assigned with dependency labels areshown as dashed lines.
These edges are importantto our formulation, as they will be represented bybinary decision variables in the ILP.
Further edgesfrom p to j, and all the edges from p to i, aremarked as fixed and shown as solid lines.
In thisway we keep the correct ordering of leaf nodes.Finally, leaf nodes are merged into parent phrases,until each phrase node contains a minimum of twotokens, shown in Figure 2(b).
Because of this min-imum length rule, it is possible for a merged nodeto be a clause rather than a phrase, but in the sub-sequent description we will use the term phraserather loosely to describe any merged leaf node.568(a)SSCCButNPNNSwhitesnsubjVPVBPremaincopADJPRBRlessadvmodJJoptimisticccomp,,NPDTthedetNNsurveynsubjVPVBDfound..(b)SSBut whites remainless optimisticccomp,,NPthe surveynsubjVBDfound .Figure 2: Dependencies are mapped onto phrase structure tree (a) and leaf nodes are merged with parentphrases (b).ILP model The merged phrase structure tree,such as shown in Figure 2(b), is the actual input toour model.
Each phrase in the document is givena salience score.
We obtain these scores from theoutput of a supervised machine learning algorithmthat predicts for each phrase whether it should beincluded in the highlights or not (see Section 5 fordetails).
Let S be the set of sentences in a docu-ment, P be the set of phrases, and Ps ?
P be theset of phrases in each sentence s ?
S .
T is the setof words with the highest tf.idf scores, and Pt ?
Pis the set of phrases containing the token t ?
T .Let fi denote the salience score for phrase i, deter-mined by the machine learning algorithm, and li isits length in tokens.We use a vector of binary variables x ?
{0,1}|P |to indicate if each phrase is to be within a high-light.
These are either top-level nodes in ourmerged tree representation, or nodes whose edgeto the parent has a dependency label (the dashedlines).
Referring to our example in Figure 2(b), bi-nary variables would be allocated to the top-level Snode, the child S node and the NP node.
The vec-tor of auxiliary binary variables y ?
{0,1}|S | in-dicates from which sentences the chosen phrasescome (see Equations (1i) and (1j)).
Let the setsDi ?
P , ?i ?
P capture the phrase dependency in-formation for each phrase i, where each set Dicontains the phrases that depend on the presenceof i.
Our objective function function is given inEquation (1a): it is the sum of the salience scoresof all the phrases chosen to form the highlightsof a given document, subject to the constraintsin Equations (1b)?(1j).
The latter provide a nat-ural way of describing the requirements the outputmust meet.maxx ?i?Pfixi (1a)s.t.
?i?Plixi ?
LT (1b)?i?Pslixi ?
LMys ?s ?
S (1c)?i?Pslixi ?
Lmys ?s ?
S (1d)?i?Ptxi ?
1 ?t ?
T (1e)x j?
xi ?i ?
P , j ?Di (1f)xi?
ys ?s ?
S , i ?
Ps (1g)?s?Sys ?
NS (1h)xi ?
{0,1} ?i ?
P (1i)ys ?
{0,1} ?s ?
S .
(1j)Constraint (1b) ensures that the generated high-lights do not exceed a total budget of LT tokens.This constraint may vary depending on the appli-cation or task at hand.
Highlights on a small screendevice would presumably be shorter than high-lights for news articles on the web.
It is also possi-ble to set the length of each highlight to be withinthe range [Lm,LM].
Constraints (1c) and (1d) en-force this requirement.
In particular, these con-straints stop highlights formed from sentences atthe beginning of the document (which tend to have569high salience scores) from being too long.
Equa-tion (1e) is a set-covering constraint, requiring thateach of the words in T appears at least once inthe highlights.
We assume that words with hightf.idf scores reveal to a certain extent what the doc-ument is about.
Constraint (1e) ensures that someof these words will be present in the highlights.We enforce grammatical correctness throughconstraint (1f) which ensures that the phrase de-pendencies are respected.
Phrases that depend onphrase i are contained in the set Di.
Variable xi istrue, and therefore phrase i will be included, if anyof its dependents x j ?Di are true.
The phrase de-pendency constraints, contained in the set Di andenforced by (1f), are the result of two rules basedon the typed dependency information:1.
Any child node j of the current node i,whose connecting edge i ?
j is of typensubj (nominal subject), nsubjpass (passivenominal subject), dobj (direct object), pobj(preposition object), infmod (infinitival mod-ifier), ccomp (clausal complement), xcomp(open clausal complement), measure (mea-sure phrase modifier) and num (numericmodifier) must be included if node i is in-cluded.2.
The parent node p of the current node i mustalways be included if i is, unless the edgep?
i is of type ccomp (clausal complement)or advcl (adverbial clause), in which case itis possible to include i without including p.Consider again the example in Figure 2(b).There are only two possible outputs from this sen-tence.
If the phrase ?the survey?
is chosen, thenthe parent node ?found?
will be included, and fromour first rule the ccomp phrase must also be in-cluded, which results in the output: ?But whitesremain less optimistic, the survey found.?
If, onthe other hand, the clause ?But whites remain lessoptimistic?
is chosen, then due to our second rulethere is no constraint that forces the parent phrase?found?
to be included in the highlights.
Withoutother factors influencing the decision, this wouldgive the output: ?But whites remain less opti-mistic.?
We can see from this example that encod-ing the possible outputs as decisions on branchesof the phrase structure tree provides a more com-pact representation of many options than would bepossible with an explicit enumeration of all possi-ble compressions.
Which output is chosen (if any)depends on the scores of the phrases involved, andthe influence of the other constraints.Constraint (1g) tells the ILP to create a highlightif one of its constituent phrases is chosen.
Finally,note that a maximum number of highlights NS canbe set beforehand, and (1h) limits the highlights tothis maximum.5 Experimental Set-upTraining We obtained phrase-based saliencescores using a supervised machine learning algo-rithm.
210 document-highlight pairs were chosenrandomly from our corpus (see Section 3).
Twoannotators manually aligned the highlights anddocument sentences.
Specifically, each sentencein the document was assigned one of three align-ment labels: must be in the summary (1), could bein the summary (2), and is not in the summary (3).The annotators were asked to label document sen-tences whose content was identical to the high-lights as ?must be in the summary?, sentenceswith partially overlapping content as ?could be inthe summary?
and the remainder as ?should notbe in the summary?.
Inter-annotator agreementwas .82 (p < 0.01, using Spearman?s ?
rank corre-lation).
The mapping of sentence labels to phraselabels was unsupervised: if the phrase came froma sentence labeled (1), and there was a unigramoverlap (excluding stop words) between the phraseand any of the original highlights, we marked thisphrase with a positive label.
All other phraseswere marked negative.Our feature set comprised surface features suchas sentence and paragraph position information,POS tags, unigram and bigram overlap with thetitle, and whether high-scoring tf.idf words werepresent in the phrase (66 features in total).
The210 documents produced a training set of 42,684phrases (3,334 positive and 39,350 negative).
Welearned the feature weights with a linear SVM,using the software SVM-OOPS (Woodsend andGondzio, 2009).
This tool gave us directly the fea-ture weights as well as support vector values, andit allowed different penalties to be applied to pos-itive and negative misclassifications, enabling usto compensate for the unbalanced data set.
Thepenalty hyper-parameters chosen were the onesthat gave the best F-scores, using 10-fold valida-tion.Highlight generation We generated highlightsfor a test set of 600 documents.
We created and570solved an ILP for each document.
Sentences werefirst tokenized to separate words and punctuation,then parsed to obtain phrases and dependencies asdescribed in Section 4 using the Stanford parser(Klein and Manning, 2003).
For each phrase, fea-tures were extracted and salience scores calcu-lated from the feature weights determined throughSVM training.
The distance from the SVM hyper-plane represents the salience score.
The ILP model(see Equation (1)) was parametrized as follows:the maximum number of highlights NS was 4,the overall limit on length LT was 75 tokens, thelength of each highlight was in the range of [8,28]tokens, and the topic coverage set T contained thetop 5 tf.idf words.
These parameters were chosento capture the properties seen in the majority ofthe training set; they were also relaxed enough toallow a feasible solution of the ILP model (withhard constraints) for all the documents in the testset.
To solve the ILP model we used the ZIB Opti-mization Suite software (Achterberg, 2007; Koch,2004; Wunderling, 1996).
The solution was con-verted into highlights by concatenating the chosenleaf nodes in order.
The ILP problems we createdhad on average 290 binary variables and 380 con-straints.
The mean solve time was 0.03 seconds.Summarization In order to examine the gen-erality of our model and compare with previouswork, we also evaluated our system on a vanillasummarization task.
Specifically, we used thesame model (trained on the CNN corpus) to gen-erate summaries for the DUC-2002 corpus2.
Wereport results on the entire dataset and on a subsetcontaining 140 documents.
This is the same parti-tion used by Martins and Smith (2009) to evaluatetheir ILP model.3Baselines We compared the output of our modelto two baselines.
The first one simply selectsthe ?leading?
three sentences from each document(without any compression).
The second baselineis the output of a sentence-based ILP model, sim-ilar to our own, but simpler.
The model is givenin (2).
The binary decision variables x ?
{0,1}|S |now represent sentences, and fi the salience scorefor each sentence.
The objective again is to max-imize the total score, but now subject only totf.idf coverage (2b) and a limit on the number of2http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html3We are grateful to Andre?
Martins for providing us withdetails of their testing partition.highlights (2c) which we set to 3.
There are nosentence length or grammaticality constraints, asthere is no sentence compression.maxx ?i?Sfixi (2a)s.t.
?i?Stxi ?
1 ?t ?
T (2b)?i?Sxi ?
NS (2c)xi ?
{0,1} ?i ?
S .
(2d)The SVM was trained with the same features usedto obtain phrase-based salience scores, but withsentence-level labels (labels (1) and (2) positive,(3) negative).Evaluation We evaluated summarization qual-ity using ROUGE (Lin and Hovy, 2003).
For thehighlight generation task, the original CNN high-lights were used as the reference.
We report un-igram overlap (ROUGE-1) as a means of assess-ing informativeness and the longest common sub-sequence (ROUGE-L) as a means of assessing flu-ency.In addition, we evaluated the generated high-lights by eliciting human judgments.
Participantswere presented with a news article and its corre-sponding highlights and were asked to rate the lat-ter along three dimensions: informativeness (dothe highlights represent the article?s main topics?
),grammaticality (are they fluent?
), and verbosity(are they overly wordy and repetitive?).
The sub-jects used a seven point rating scale.
An idealsystem would receive high numbers for grammat-icality and informativeness and a low number forverbosity.
We randomly selected nine documentsfrom the test set and generated highlights with ourmodel and the sentence-based ILP baseline.
Wealso included the original highlights as a gold stan-dard.
We thus obtained ratings for 27 (9 ?
3)document-highlights pairs.4 The study was con-ducted over the Internet using WebExp (Kelleret al, 2009) and was completed by 34 volunteers,all self reported native English speakers.With regard to the summarization task, follow-ing Martins and Smith (2009), we used ROUGE-1and ROUGE-2 to evaluate our system?s output.We also report results with ROUGE-L. Each doc-ument in the DUC-2002 dataset is paired with4A Latin square design ensured that subjects did not seetwo different highlights of the same document.5710.10.150.20.250.30.350.40.450.5Recall PrecisionRouge-1F-score Recall PrecisionRouge-LF-scoreScoreLeading-3ILP sentenceILP phraseFigure 3: ROUGE-1 and ROUGE-L results forphrase-based ILP model and two baselines, witherror bars showing 95% confidence levels.a human-authored summary (approximately 100words) which we used as reference.6 ResultsWe report results on the highlight generation taskin Figure 3 with ROUGE-1 and ROUGE-L (errorbars indicate the 95% confidence interval).
Inboth measures, the ILP sentence baseline has thebest recall, while the ILP phrase model has thebest precision (the differences are statistically sig-nificant).
F-score is higher for the phrase-basedsystem but not significantly.
This can be at-tributed to the fact that the longer output of thesentence-based model makes the recall task easier.Average highlight lengths are shown in Table 3,and the compression rates they represent.
Ourphrase model achieves the highest compressionrates, whereas the sentence-based model tends toselect long sentences even in comparison to thelead baseline.
The sentence ILP model outper-forms the lead baseline with respect to recall butnot precision or F-score.
The phrase ILP achievesa significantly better F-score over the lead baselinewith both ROUGE-1 and ROUGE-L.The results of our human evaluation study aresummarized in Table 4.
There was no sta-tistically significant difference in the grammat-icality between the highlights generated by thephrase ILP system and the original CNN high-lights (means differences were compared using aPost-hoc Tukey test).
The grammaticality of thesentence ILP was significantly higher overall asno compression took place (?
< 0.05).
All threes toks/s C.R.Articles 36.5 22.2 ?
4.0 100%CNN highlights 3.5 13.3 ?
1.7 5.8%ILP phrase 3.8 18.0 ?
2.9 8.4%Leading-3 3.0 25.1 ?
7.4 9.3%ILP sentence 3.0 31.3 ?
7.9 11.6%Table 3: Comparison of output lengths: numberof sentences, tokens per sentence, and compres-sion rate, for CNN articles, their highlights, theILP phrase model, and two baselines.Model Grammar Importance VerbosityCNN highlights 4.85 4.88 3.14ILP sentence 6.41 5.47 3.97ILP phrase 5.53 5.05 3.38Table 4: Average human ratings for original CNNhighlights, and two ILP models.systems performed on a similar level with respectto importance (differences in the means were notsignificant).
The highlights created by the sen-tence ILP were considered significantly more ver-bose (?
< 0.05) than those created by the phrase-based system and the CNN abstractors.
Overall,the highlights generated by the phrase ILP modelwere not significantly different from those writtenby humans.
They capture the same content as thefull sentences, albeit in a more succinct manner.Table 5 shows the output of the phrase-based sys-tem for the documents in Table 1.Our results on the complete DUC-2002 cor-pus are shown in Table 6.
Despite the fact thatour model has not been optimized for the originaltask of generating 100-word summaries?insteadit is trained on the CNN corpus, and generateshighlights?the results are comparable with thebest of the original participants5 in each of theROUGE measures.
Our model is also significantlybetter than the lead sentences baseline.Table 7 presents our results on the sameDUC-2002 partition (140 documents) used byMartins and Smith (2009).
The phrase ILP modelachieves a significantly better F-score (for bothROUGE-1 and ROUGE-2) over the lead baseline,the sentence ILP model, and Martins and Smith.We should point out that the latter model is not astraw man.
It significantly outperforms a pipeline5The list of participants is on page 12 of the slidesavailable from http://duc.nist.gov/pubs/2002slides/overview.02.pdf.572?
More than two-thirds of African-Americans believeMartin Luther King Jr.?s vision for race relations hasbeen fulfilled.?
69 percent of blacks said King?s vision has been ful-filled in the more than 45 years since his 1963 ?I have adream?
speech.?
But whites remain less optimistic, the survey found.?
A Florida man is using billboards with an image of theburning World Trade Center to encourage votes for aRepublican presidential candidate, drawing criticism.?
?Please Don?t Vote for a Democrat?
reads the type overthe picture of the twin towers.?
Mike Meehan said former President Clinton shouldhave put a stop to Osama bin Laden and al Qaeda be-fore 9/11.Table 5: Generated highlights for the stories in Ta-ble 1 using the phrase ILP model.Participant ROUGE-1 ROUGE-2 ROUGE-L28 0.464 0.222 0.43219 0.459 0.221 0.43121 0.458 0.216 0.42629 0.449 0.208 0.41927 0.445 0.209 0.417Leading-3 0.416 0.200 0.390ILP phrase 0.454 0.213 0.428Table 6: ROUGE results on the completeDUC-2002 corpus, including the top 5 originalparticipants.
For all results, the 95% confidenceinterval is ?0.008.approach that first creates extracts and then com-presses them.
Furthermore, as a standalone sen-tence compression system it yields state of the artperformance, comparable to McDonald?s (2006)discriminative model and superior to Hedge Trim-mer (Zajic et al, 2007), a less sophisticated deter-ministic system.7 ConclusionsIn this paper we proposed a joint content selectionand compression model for single-document sum-marization.
A key aspect of our approach is therepresentation of content by phrases rather thanentire sentences.
Salient phrases are selected toform the summary.
Grammaticality, length andcoverage requirements are encoded as constraintsin an integer linear program.
Applying the modelto the generation of ?story highlights?
(and sin-gle document summaries) shows that it is a vi-able alternative to extraction-based systems.
BothROUGE scores and the results of our human studyROUGE-1 ROUGE-2 ROUGE-LLeading-3 .400 ?
.018 .184 ?
.015 .374 ?
.017M&S (2009) .403 ?
.076 .180 ?
.076 ?ILP sentence .430 ?
.014 .191 ?
.015 .401 ?
.014ILP phrase .445 ?
.014 .200 ?
.014 .419 ?
.014Table 7: ROUGE results on DUC-2002 cor-pus (140 documents).
?
: only ROUGE-1 andROUGE-2 results are given in Martins and Smith(2009).confirm that our system manages to create sum-maries at a high compression rate and yet maintainthe informativeness and grammaticality of a com-petitive extractive system.
The model itself is rel-atively simple and knowledge-lean, and achievesgood performance without reference to any re-sources outside the corpus collection.Future extensions are many and varied.
An ob-vious next step is to examine how the model gen-eralizes to other domains and text genres.
Al-though coherence is not so much of an issue forhighlights, it certainly plays a role when generat-ing standard summaries.
The ILP model can bestraightforwardly augmented with discourse con-straints similar to those proposed in Clarke andLapata (2007).
We would also like to generalizethe model to arbitrary rewrite operations, as ourresults indicate that compression rates are likelyto improve with more sophisticated paraphrasing.AcknowledgmentsWe would like to thank Andreas Grothey andmembers of ICCS at the School of Informatics forthe valuable discussions and comments through-out this work.
We acknowledge the support of EP-SRC through project grants EP/F055765/1 andGR/T04540/01.ReferencesAchterberg, Tobias.
2007.
Constraint Integer Programming.Ph.D.
thesis, Technische Universita?t Berlin.Banko, Michele, Vibhu O. Mittal, and Michael J. Witbrock.2000.
Headline generation based on statistical translation.In Proceedings of the 38th ACL.
Hong Kong, pages 318?325.Clarke, James and Mirella Lapata.
2007.
Modelling com-pression with discourse constraints.
In Proceedings ofEMNLP-CoNLL.
Prague, Czech Republic, pages 1?11.Clarke, James and Mirella Lapata.
2008.
Global inferencefor sentence compression: An integer linear program-ming approach.
Journal of Artificial Intelligence Research31:399?429.Cohn, Trevor and Mirella Lapata.
2009.
Sentence compres-sion as tree transduction.
Journal of Artificial IntelligenceResearch 34:637?674.573Conroy, J. M., J. D. Schlesinger, J. Goldstein, and D. P.O?Leary.
2004.
Left-brain/right-brain multi-documentsummarization.
In DUC 2004 Conference Proceedings.Daume?
III, Hal.
2006.
Practical Structured Learning Tech-niques for Natural Language Processing.
Ph.D. thesis,University of Southern California.Daume?
III, Hal and Daniel Marcu.
2002.
A noisy-channelmodel for document compression.
In Proceedings of the40th ACL.
Philadelphia, PA, pages 449?456.Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003.Hedge trimmer: A parse-and-trim approach to headlinegeneration.
In Proceedings of the HLT-NAACL 2003Workshop on Text Summarization.
pages 1?8.Jing, Hongyan.
2000.
Sentence reduction for automatic textsummarization.
In Proceedings of the 6th ANLP.
Seattle,WA, pages 310?315.Jing, Hongyan.
2002.
Using hidden Markov modeling to de-compose human-written summaries.
Computational Lin-guistics 28(4):527?544.Jing, Hongyan and Kathleen McKeown.
2000.
Cut and pastesummarization.
In Proceedings of the 1st NAACL.
Seattle,WA, pages 178?185.Keller, Frank, Subahshini Gunasekharan, Neil Mayo, andMartin Corley.
2009.
Timing accuracy of web experi-ments: A case study using the WebExp software package.Behavior Research Methods 41(1):1?12.Klein, Dan and Christopher D. Manning.
2003.
Accurate un-lexicalized parsing.
In Proceedings of the 41st ACL.
Sap-poro, Japan, pages 423?430.Knight, Kevin and Daniel Marcu.
2002.
Summarization be-yond sentence extraction: a probabilistic approach to sen-tence compression.
Artificial Intelligence 139(1):91?107.Koch, Thorsten.
2004.
Rapid Mathematical Prototyping.Ph.D.
thesis, Technische Universita?t Berlin.Kupiec, Julian, Jan O. Pedersen, and Francine Chen.
1995.
Atrainable document summarizer.
In Proceedings of SIGIR-95.
Seattle, WA, pages 68?73.Lin, Chin-Yew.
2003.
Improving summarization performanceby sentence compression ?
a pilot study.
In Proceed-ings of the 6th International Workshop on Information Re-trieval with Asian Languages.
Sapporo, Japan, pages 1?8.Lin, Chin-Yew and Eduard H. Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statistics.In Proceedings of HLT NAACL.
Edmonton, Canada, pages71?78.Mani, Inderjeet.
2001.
Automatic Summarization.
John Ben-jamins Pub Co.Martins, Andre?
and Noah A. Smith.
2009.
Summarizationwith a joint model for sentence extraction and compres-sion.
In Proceedings of the Workshop on Integer LinearProgramming for Natural Language Processing.
Boulder,Colorado, pages 1?9.McDonald, Ryan.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proceedings of the11th EACL.
Trento, Italy.McDonald, Ryan.
2007.
A study of global inference algo-rithms in multi-document summarization.
In Proceedingsof the 29th ECIR.
Rome, Italy.Nenkova, Ani.
2005.
Automatic text summarization ofnewswire: Lessons learned from the Document Under-standing Conference.
In Proceedings of the 20th AAAI.Pittsburgh, PA, pages 1436?1441.Siddharthan, Advaith, Ani Nenkova, and Kathleen McKe-own.
2004.
Syntactic simplification for improving con-tent selection in multi-document summarization.
In Pro-ceedings of the 20th International Conference on Compu-tational Linguistics (COLING 2004).
pages 896?902.Sparck Jones, Karen.
1999.
Automatic summarizing: Factorsand directions.
In Inderjeet Mani and Mark T. Maybury,editors, Advances in Automatic Text Summarization, MITPress, Cambridge, pages 1?33.Svore, Krysta, Lucy Vanderwende, and Christopher Burges.2007.
Enhancing single-document summarization bycombining RankNet and third-party sources.
In Proceed-ings of EMNLP-CoNLL.
Prague, Czech Republic, pages448?457.Wan, Stephen and Ce?cile Paris.
2008.
Experimenting withclause segmentation for text summarization.
In Proceed-ings of the 1st TAC.
Gaithersburg, MD.Witten, Ian H., Gordon Paynter, Eibe Frank, Carl Gutwin, andCraig G. Nevill-Manning.
1999.
KEA: Practical automatickeyphrase extraction.
In Proceedings of the 4th ACMInternational Conference on Digital Libraries.
Berkeley,CA, pages 254?255.Woodsend, Kristian and Jacek Gondzio.
2009.
Exploitingseparability in large-scale linear support vector machinetraining.
Computational Optimization and Applications .Wunderling, Roland.
1996.
Paralleler und objektorientierterSimplex-Algorithmus.
Ph.D. thesis, Technische Univer-sita?t Berlin.Zajic, David, Bonnie J.
Door, Jimmy Lin, and RichardSchwartz.
2007.
Multi-candidate reduction: Sentencecompression as a tool for document summarization tasks.Information Processing Management Special Issue onSummarization 43(6):1549?1570.574
