Active Learning with Multiple Annotations for Comparable DataClassification TaskVamshi Ambati, Sanjika Hewavitharana, Stephan Vogel and Jaime Carbonell{vamshi,sanjika,vogel,jgc}@cs.cmu.eduLanguage Technologies Institute, Carnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213, USAAbstractSupervised learning algorithms for identify-ing comparable sentence pairs from a domi-nantly non-parallel corpora require resourcesfor computing feature functions as well astraining the classifier.
In this paper we pro-pose active learning techniques for addressingthe problem of building comparable data forlow-resource languages.
In particular we pro-pose strategies to elicit two kinds of annota-tions from comparable sentence pairs: classlabel assignment and parallel segment extrac-tion.
We also propose an active learning strat-egy for these two annotations that performssignificantly better than when sampling for ei-ther of the annotations independently.1 IntroductionThe state-of-the-art Machine Translation (MT) sys-tems are statistical, requiring large amounts of paral-lel corpora.
Such corpora needs to be carefully cre-ated by language experts or speakers, which makesbuilding MT systems feasible only for those lan-guage pairs with sufficient public interest or finan-cial support.
With the increasing rate of social mediacreation and the quick growth of web media in lan-guages other than English makes it relevant for lan-guage research community to explore the feasibilityof Internet as a source for parallel data.
(Resnik andSmith, 2003) show that parallel corpora for a varietyof languages can be harvested on the Internet.
It is tobe observed that a major portion of the multilingualweb documents are created independent of one an-other and so are only mildly parallel at the documentlevel.There are multiple challenges in building compa-rable corpora for consumption by the MT systems.The first challenge is to identify the parallelism be-tween documents of different languages which hasbeen reliably done using cross lingual informationretrieval techniques.
Once we have identified a sub-set of documents that are potentially parallel, thesecond challenge is to identify comparable sentencepairs.
This is an interesting challenge as the avail-ability of completely parallel sentences on the inter-net is quite low in most language-pairs, but one canobserve very few comparable sentences among com-parable documents for a given language-pair.
Ourwork tries to address this problem by posing theidentification of comparable sentences from com-parable data as a supervised classification problem.Unlike earlier research (Munteanu and Marcu, 2005)where the authors try to identify parallel sentencesamong a pool of comparable documents, we try tofirst identify comparable sentences in a pool withdominantly non-parallel sentences.
We then builda supervised classifier that learns from user annota-tions for comparable corpora identification.
Train-ing such a classifier requires reliably annotated datathat may be unavailable for low-resource languagepairs.
Involving a human expert to perform suchannotations is expensive for low-resource languagesand so we propose active learning as a suitable tech-nique to reduce the labeling effort.There is yet one other issue that needs to be solvedin order for our classification based approach towork for truly low-resource language pairs.
As wewill describe later in the paper, our comparable sen-tence classifier relies on the availability of an ini-69Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 69?77,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguisticstial seed lexicon that can either be provided by a hu-man or can be statistically trained from parallel cor-pora (Och and Ney, 2003).
Experiments show that abroad coverage lexicon provides us with better cov-erage for effective identification of comparable cor-pora.
However, availability of such a resource cannot be expected in very low-resource language pairs,or even if present may not be of good quality.
Thisopens an interesting research question - Can we alsoelicit such information effectively at low costs?
Wepropose active learning strategies for identifying themost informative comparable sentence pairs which ahuman can then extract parallel segments from.While the first form of supervision provides uswith class labels that can be used for tuning the fea-ture weights of our classifier, the second form of su-pervision enables us to better estimate the featurefunctions.
For the comparable sentence classifier toperform well, we show that both forms of supervi-sion are needed and we introduce an active learningprotocol to combine the two forms of supervisionunder a single joint active learning strategy.The rest of the paper is organized as follows.
InSection 2 we survey earlier research as relevant tothe scope of the paper.
In Section 3 we discuss thesupervised training setup for our classifier.
In Sec-tion 4 we discuss the application of active learning tothe classification task.
Section 5 discusses the caseof active learning with two different annotations andproposes an approach for combining them.
Section 6presents experimental results and the effectivenessof the active learning strategies.
We conclude withfurther discussion and future work.2 Related WorkThere has been a lot of interest in using compara-ble corpora for MT, primarily on extracting paral-lel sentence pairs from comparable sources (Zhaoand Vogel, 2002; Fung and Yee, 1998).
Some workhas gone beyond this focussing on extracting sub-sentential fragments from noisier comparable data(Munteanu and Marcu, 2006; Quirk et al, 2007).The research conducted in this paper has two pri-mary contributions and so we will discuss the relatedwork as relevant to each of them.Our first contribution in this paper is the appli-cation of active learning for acquiring comparabledata in the low-resource scenario, especially rele-vant when working with low-resource languages.There is some earlier work highlighting the needfor techniques to deal with low-resource scenar-ios.
(Munteanu and Marcu, 2005) propose bootstrap-ping using an existing classifier for collecting newdata.
However, this approach works when there isa classifier of reasonable performance.
In the ab-sence of parallel corpora to train lexicons humanconstructed dictionaries were used as an alternativewhich may, however, not be available for a largenumber of languages.
Our proposal of active learn-ing in this paper is suitable for highly impoverishedscenarios that require support from a human.The second contribution of the paper is to ex-tend the traditional active learning setup that is suit-able for eliciting a single annotation.
We highlightthe needs of the comparable corpora scenario wherewe have two kinds of annotations - class label as-signment and parallel segment extraction and pro-pose strategies in active learning that involve multi-ple annotations.
A relevant setup is multitask learn-ing (Caruana, 1997) which is increasingly becom-ing popular in natural language processing for learn-ing from multiple learning tasks.
There has beenvery less work in the area of multitask active learn-ing.
(Reichart et al, 2008) proposes an extension ofthe single-sided active elicitation task to a multi-taskscenario, where data elicitation is performed for twoor more independent tasks at the same time.
(Settleset al, 2008) propose elicitation of annotations forimage segmentation under a multi-instance learningframework.Active learning with multiple annotations also hassimilarities to the recent body of work in learn-ing from instance feedback and feature feedback(Melville et al, 2005).
(Druck et al, 2009) pro-pose active learning extensions to the gradient ap-proach of learning from feature and instance feed-back.
However, in the comparable corpora problemalthough the second annotation is geared towardslearning better features by enhancing the coverageof the lexicon, the annotation itself is not on the fea-tures but for extracting training data that is then usedto train the lexicon.703 Supervised Comparable SentenceClassificationIn this section we discuss our supervised trainingsetup and the classification algorithm.
Our classifiertries to identify comparable sentences from among alarge pool of noisy comparable sentences.
In this pa-per we define comparable sentences as being trans-lations that have around fifty percent or more trans-lation equivalence.
In future we will evaluate the ro-bustness of the classifier by varying levels of noiseat the sentence level.3.1 Training the ClassifierFollowing (Munteanu and Marcu, 2005), we use aMaximum Entropy classifier to identify comparablesentences.
The classifier probability can be definedas:Pr(ci|S, T ) = 1Z(S, T )exp?
?n?j=1?jfij(ci, S, T )?
?where (S, T ) is a sentence pair, ci is the class, fijare feature functions and Z(S) is a normalizing fac-tor.
The parameters ?i are the weights for the fea-ture functions and are estimated by optimizing on atraining data set.
For the task of classifying a sen-tence pair, there are two classes, c0 = comparableand c1 = non parallel.
A value closer to one forPr(c1|S, T ) indicates that (S, T ) are comparable.To train the classifier we need comparable sen-tence pairs and non-parallel sentence pairs.
Whileit is easy to find negative examples online, ac-quiring comparable sentences is non-trivial and re-quires human intervention.
(Munteanu and Marcu,2005) construct negative examples automaticallyfrom positive examples by pairing all source sen-tences with all target sentences.
We, however, as-sume the availability of both positive and negativeexamples to train the classifier.
We use the GISlearning algorithm for tuning the model parameters.3.2 Feature ComputationThe features are defined primarily based on trans-lation lexicon probabilities.
Rather than computingword alignment between the two sentences, we uselexical probabilities to determine alignment pointsas follows: a source word s is aligned to a targetword t if p(s|t) > 0.5.
Target word alignment iscomputed similarly.
Long contiguous sections ofaligned words indicate parallelism.
We use the fol-lowing features:?
Source and target sentence length ratio?
Source and target sentence length difference?
Lexical probability score, similar to IBMmodel 1?
Number of aligned words?
Longest aligned word sequence?
Number of un-aligned wordsLexical probability score, and alignment featuresgenerate two sets of features based on translationlexica obtained by training in both directions.
Fea-tures are normalized with respect to the sentencelength.Figure 1: Seed parallel corpora size vs. Classifier perfor-mance in Urdu-English language pairIn our experiments we observe that the most in-formative features are the ones involving the prob-abilistic lexicon.
However, the comparable corporaobtained for training the classifier cannot be used forautomatically training a lexicon.
We, therefore, re-quire the availability of an initial seed parallel cor-pus that can be used for computing the lexicon andthe associated feature functions.
We notice that thesize of the seed corpus has a large influence on theaccuracy of the classifier.
Figure 1 shows a plot with71the initial size of the corpus used to construct theprobabilistic lexicon on x-axis and its effect on theaccuracy of the classifier on y-axis.
The sentenceswere drawn randomly from a large pool of Urdu-English parallel corpus and it is clear that a largerpool of parallel sentences leads to a better lexiconand an improved classifier.4 Active Learning with MultipleAnnotations4.1 Cost MotivationLack of existing annotated data requires reliablehuman annotation that is expensive and effort-intensive.
We propose active learning for the prob-lem of effectively acquiring multiple annotationsstarting with unlabeled data.
In active learning, thelearner has access to a large pool of unlabeled dataand sometimes a small portion of seed labeled data.The objective of the active learner is then to se-lect the most informative instances from the unla-beled data and seek annotations from a human ex-pert, which it then uses to retrain the underlying su-pervised model for improving performance.A meaningful setup to study multi annotation ac-tive learning is to take into account the cost involvedfor each of the annotations.
In the case of compara-ble corpora we have two annotation tasks, each withcost modelsCost1 andCost2 respectively.
The goalof multi annotation active learning is to select theoptimal set of instances for each annotation so as tomaximize the benefit to the classifier.
Unlike the tra-ditional active learning, where we optimize the num-ber of instances we label, here we optimize the se-lection under a provided budget Bk per iteration ofthe active learning algorithm.4.2 Active Learning SetupWe now discuss our active learning framework forbuilding comparable corpora as shown in Algo-rithm 1.
We start with an unlabeled dataset U0 ={xj =< sj , tj >} and a seed labeled dataset L0 ={(< sj , tj >, ci)}, where c ?
0, 1 are class la-bels with 0 being the non-parallel class and 1 beingthe comparable data class.
We also have T0 = {<sk, tk >} which corresponds to parallel segmentsor sentences identified from L0 that will be used intraining the probabilistic lexicon.
Both T0 and L0can be very small in size at the start of the activelearning loop.
In our experiments, we tried with asfew as 50 to 100 sentences for each of the datasets.We perform an iterative budget motivated activelearning loop for acquiring labeled data over k it-erations.
We start the active learning loop by firsttraining a lexicon with the available Tk and then us-ing that we train the classifier over Lk.
We, thenscore all the sentences in the Uk using the model ?and apply our selection strategy to retrieve the bestscoring instance or a small batch of instances.
In thesimplest case we annotate this instance and add itback to the tuning set Ck for re-training the classi-fier.
If the instance was a comparable sentence pair,then we could also perform the second annotationconditioned upon the availability of the budget.
Theidentified sub-segments (ssi , tti) are added back tothe training data Tk used for training the lexicon inthe subsequent iterations.Algorithm 1 ACTIVE LEARNING SETUP1: Given Unlabeled Comparable Corpus: U02: Given Seed Parallel Corpus: T03: Given Tuning Corpus: L04: for k = 0 to K do5: Train Lexicon using Tk6: ?
= Tune Classifier using Ck7: while Cost < Bk do8: i = Query(Uk,Lk,Tk,?
)9: ci = Human Annotation-1 (si, ti)10: (ssi ,tti) = Human Annotation-2 xi11: Lk = Ck ?
(si, ti, ci)12: Tk = Tk ?
(ssi, tti)13: Uk = Uk - xi14: Cost = Cost1 + Cost215: end while16: end for5 Sampling Strategies for Active Learning5.1 Acquiring Training Data for ClassifierOur selection strategies for obtaining class labels fortraining the classifier uses the model in its currentstate to decide on the informative instances for thenext round of iterative training.
We propose the fol-lowing two sampling strategies for this task.725.1.1 Certainty SamplingThis strategy selects instances where the currentmodel is highly confident.
While this may seemredundant at the outset, we argue that this crite-ria can be a good sampling strategy when the clas-sifier is weak or trained in an impoverished datascenario.
Certainty sampling strategy is a lot sim-ilar to the idea of unsupervised approaches likeboosting or self-training.
However, we make it asemi-supervised approach by having a human in theloop to provide affirmation for the selected instance.Consider the following scenario.
If we select aninstance that our current model prefers and obtaina contradicting label from the human, then this in-stance has a maximal impact on the decision bound-ary of the classifier.
On the other hand, if the labelis reaffirmed by a human, the overall variance re-duces and in the process, it also helps in assigninghigher preference for the configuration of the deci-sion boundary.
(Melville et al, 2005) introduce acertainty sampling strategy for the task of featurelabeling in a text categorization task.
Inspired bythe same we borrow the name and also apply thisas an instance sampling approach.
Given an in-stance x and the classifier posterior distribution forthe classes as P (.
), we select the most informativeinstance as follows:x?
= argmaxxP (c = 1|x)5.1.2 Margin-based SamplingThe certainty sampling strategy only considers theinstance that has the best score for the comparablesentence class.
However we could benefit from in-formation about the second best class assigned tothe same instance.
In the typical multi-class clas-sification problems, earlier work shows success us-ing such a ?margin based?
approach (Scheffer et al,2001), where the difference between the probabil-ities assigned by the underlying model to the firstbest and second best classes is used as the samplingcriteria.Given a classifier with posterior distributionover classes for an instance P (c = 1|x),the margin based strategy is framed as x?
=argminxP (c1|x)?
P (c2|x), where c1 is the bestprediction for the class and c2 is the second bestprediction under the model.
It should be noted thatfor binary classification tasks with two classes, themargin sampling approach reduces to an uncertaintysampling approach (Lewis and Catlett, 1994).5.2 Acquiring Parallel Segments for LexiconTrainingWe now propose two sampling strategies for the sec-ond annotation.
Our goal is to select instances thatcould potentially provide parallel segments for im-proved lexical coverage and feature computation.5.2.1 Diversity SamplingWe are interested in acquiring clean parallel seg-ments for training a lexicon that can be used in fea-ture computation.
It is not clear how one could use acomparable sentence pair to decide the potential forextracting a parallel segment.
However, it is highlylikely that if such a sentence pair has new cover-age on the source side, then it increases the chancesof obtaining new coverage.
We, therefore, proposea diversity based sampling for extracting instancesthat provide new vocabulary coverage .
The scor-ing function tc score(s) is defined below, whereV oc(s) is defined as the vocabulary of source sen-tence s for an instance xi =< si, ti >, T is the setof parallel sentences or segments extracted so far.tc score(s) =|T |?s=1sim(s, s?)
?
1|T | (1)sim(s, s?)
= |(V oc(s) ?
V oc(s?
)| (2)5.2.2 Alignment RatioWe also propose a strategy that provides direct in-sight into the coverage of the underlying lexicon andprefers a sentence pair that is more likely to be com-parable.
We call this alignment ratio and it can beeasily computed from the available set of featuresdiscussed in Section 3 as below:a score(s) = #unalignedwords#alignedwords (3)s?
= argmaxsa score(s) (4)This strategy is quite similar to the diversity basedapproach as both prefer selecting sentences that have73a potential to offer new vocabulary from the com-parable sentence pair.
However while the diver-sity approach looks only at the source side coverageand does not depend upon the underlying lexicon,the alignment ratio utilizes the model for computingcoverage.
It should also be noted that while we havecoverage for a word in the sentence pair, it may notmake it to the probabilistically trained and extractedlexicon.5.3 Combining Multiple AnnotationsFinally, given two annotations and correspondingsampling strategies, we try to jointly select the sen-tence that is best suitable for obtaining both the an-notations and is maximally beneficial to the classi-fier.
We select a single instance by combining thescores from the different selection strategies as ageometric mean.
For instance, we consider a mar-gin based sampling (margin) for the first annota-tion and a diversity sampling (tc score) for the sec-ond annotation, we can jointly select a sentence thatmaximizes the combined score as shown below:total score(s) = margin(s) ?
tc score(s) (5)s?
= argmaxstotal score(s) (6)6 Experiments and Results6.1 DataThis research primarily focuses on identifying com-parable sentences from a pool of dominantly non-parallel sentences.
To our knowledge, there is adearth of publicly available comparable corpora ofthis nature.
We, therefore, simulate a low-resourcescenario by using realistic assumptions of noiseand parallelism at both the corpus-level and thesentence-level.
In this section we discuss the pro-cess and assumptions involved in the creation of ourdatasets and try to mimic the properties of real-worldcomparable corpora harvested from the web.We first start with a sentence-aligned parallel cor-pus available for the language pair.
We then dividethe corpus into three parts.
The first part is calledthe ?sampling pool?
and is set aside to use for draw-ing sentences at random.
The second part is usedto act as a non-parallel corpus.
We achieve non-parallelism by randomizing the mapping of the tar-get sentences with the source sentences.
This is aslight variation of the strategy used in (Munteanuand Marcu, 2005) for generating negative examplesfor their classifier.
The third part is used to synthe-size a comparable corpus at the sentence-level.
Weperform this by first selecting a parallel sentence-pair and then padding either sides by a source andtarget segment drawn independently from the sam-pling pool.
We control the length of the non-parallelportion that is appended to be lesser than or equalto the original length of the sentence.
Therefore, theresulting synthesized comparable sentence pairs areguaranteed to contain at least 50% parallelism.We use this dataset as the unlabeled pool fromwhich the active learner selects instances for label-ing.
Since the gold-standard labels for this corpusare already available, which gives us better controlover automating the active learning process, whichtypically requires a human in the loop.
However,our active learning strategies are in no way limitedby the simulated data setup and can generalize to thereal world scenario with an expert providing the la-bels for each instance.We perform our experiments with data from twolanguage pairs: Urdu-English and Spanish-English.For Urdu-English, we use the parallel corpus NIST2008 dataset released for the translation shared task.We start with 50,000 parallel sentence corpus fromthe released training data to create a corpus of25,000 sentence pairs with 12,500 each of compa-rable and non-parallel sentence pairs.
Similarly, weuse 50,000 parallel sentences from the training datareleased by the WMT 2008 datasets for Spanish-English to create a corpus of 25,000 sentence pairs.We also use two held-out data sets for training andtuning the classifier, consisting of 1000 sentencepairs (500 non-parallel and 500 comparable).6.2 ResultsWe perform two kinds of evaluations: the first, toshow that our active learning strategies perform wellacross language pairs and the second, to show thatmulti annotation active learning leads to a good im-provement in performance of the classifier.6.2.1 How does the Active Learning perform?In section 5, we proposed multiple active learn-ing strategies for both eliciting both kinds of annota-tions.
A good active learning strategy should select74instances that contribute to the maximal improve-ment of the classifier.
The effectiveness of activelearning is typically tested by the number of queriesthe learner asks and the resultant improvement inthe performance of the classifier.
The classifier per-formance in the comparable sentence classificationtask can be computed as the F-score on the held outdataset.
For this work, we assume that both the an-notations require the same effort level and so assignuniform cost for eliciting each of them.
Thereforethe number of queries is equivalent to the total costof supervision.Figure 2: Active learning performance for the compara-ble corpora classification in Urdu-English language-pairFigure 3: Active learning performance for the compara-ble corpora classification in Spanish-English language-pairFigure 2 shows our results for the Urdu-Englishlanguage pair, and Figure 3 plots the Spanish-English results with the x-axis showing the totalnumber of queries posed to obtain annotations andthe y-axis shows the resultant improvement in accu-racy of the classifier.
In these experiments we donot actively select for the second annotation but ac-quire the parallel segment from the same sentence.We compare this over a random baseline where thesentence pair is selected at random and used for elic-iting both annotations at the same time.Firstly, we notice that both our active learn-ing strategies: certainty sampling and margin-basedsampling perform better than the random baseline.For the Urdu-English language pair we can see thatfor the same effort expended (i.e 2000 queries) theclassifier has an increase in accuracy of 8 absolutepoints.
For Spanish-English language pair the ac-curacy improvement is 6 points over random base-line.
Another observation from Figure 3 is that forthe classifier to reach an fixed accuracy of 68 points,the random sampling method requires 2000 querieswhile the from the active selection strategies requiresignificantly less effort of about 500 queries.6.2.2 Performance of Joint Selection withMultiple AnnotationsWe now evaluate our joint selection strategy thattries to select the best possible instance for boththe annotations.
Figure 4 shows our results for theUrdu-English language pair, and Figure 5 plots theSpanish-English results for active learning with mul-tiple annotations.
As before, the x-axis shows thetotal number of queries posed, equivalent to the cu-mulative effort for obtaining the annotations and they-axis shows the resultant improvement in accuracyof the classifier.We evaluate the multi annotation active learningagainst two single-sided baselines where the sam-pling focus is on selecting instances according tostrategies suitable for one annotation at a time.
Thebest performing active learning strategy for the classlabel annotations is the certainty sampling (annot1)and so for one single-sided baseline, we use thisbaseline.
We also obtain the second annotation forthe same instance.
By doing so, we might be se-lecting an instance that is sub-optimal for the sec-ond annotation and therefore the resultant lexiconmay not maximally benefit from the instance.
Wealso observe, from our experiments, that the diver-sity based sampling works well for the second anno-75tation and alignment ratio does not perform as well.So, for the second single-sided baseline we use thediversity based sampling strategy (annot2) and getthe first annotation for the same instance.
Finallywe compare this with the joint selection approachproposed earlier that combines both the annotationstrategies (annot1+annot2).
In both the languagepairs we notice that joint selection for both anno-tations performs better than the baselines.Figure 4: Active learning with multiple annotations andclassification performance in Urdu-EnglishFigure 5: Active learning with multiple annotations andclassification performance in Spanish-English7 Conclusion and Future WorkIn this paper, we proposed active learning with mul-tiple annotations for the challenge of building com-parable corpora in low-resource scenarios.
In par-ticular, we identified two kinds of annotations: classlabels (for identifying comparable vs. non-paralleldata) and clean parallel segments within the com-parable sentences.
We implemented multiple inde-pendent strategies for obtaining each of the abve ina cost-effective manner.
Our active learning experi-ments in a simulated low-resource comparable cor-pora scenario across two language pairs show signif-icant results over strong baselines.
Finally we alsoproposed a joint selection strategy that selects a sin-gle instance which is beneficial to both the annota-tions.
The results indicate an improvement over sin-gle strategy baselines.There are several interesting questions for futurework.
Throughout the paper we assumed uniformcosts for both the annotations, which will need tobe verified with human subjects.
We also hypoth-esize that obtaining both annotations for the samesentence may be cheaper than getting them from twodifferent sentences due to the overhead of contextswitching.
Another assumption is that of the exis-tence of a single contiguous parallel segment in acomparable sentence pair, which needs to be veri-fied for corpora on the web.Finally, active learning assumes availability of anexpert to answer the queries.
Availability of an ex-pert for low-resource languages and feasibility ofrunning large scale experiments is difficult.
We,therefore, have started working on crowdsourcingthese annotation tasks on Amazon Mechanical Turk(MTurk) where it is easy to find people and quicklyrun experiments with real people.AcknowledgementThis material is based upon work supported in partby the U. S. Army Research Laboratory and the U.S. Army Research Office under grant W911NF-10-1-0533, and in part by NSF under grant IIS 0916866.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Rich Caruana.
1997.
Multitask learning.
In MachineLearning, pages 41?75.Gregory Druck, Burr Settles, and Andrew McCallum.2009.
Active learning by labeling features.
In Pro-ceedings of Conference on Empirical Methods in Nat-76ural Language Processing (EMNLP 2009), pages 81?90.Jenny Rose Finkel and Christopher D. Manning.
2010.Hierarchical joint learning: Improving joint parsingand named entity recognition with non-jointly labeleddata.
In Proceedings of ACL 2010.Pascale Fung and Lo Yen Yee.
1998.
An IR approach fortranslating new words from nonparallel, comparabletexts.
In Proceedings of the 36th Annual Meeting ofthe Association for Computational Linguistics, pages414?420, Montreal, Canada.David D. Lewis and Jason Catlett.
1994.
Heterogeneousuncertainty sampling for supervised learning.
In InProceedings of the Eleventh International Conferenceon Machine Learning, pages 148?156.
Morgan Kauf-mann.Prem Melville, Foster Provost, Maytal Saar-Tsechansky,and Raymond Mooney.
2005.
Economical activefeature-value acquisition through expected utility esti-mation.
In UBDM ?05: Proceedings of the 1st interna-tional workshop on Utility-based data mining, pages10?16, New York, NY, USA.
ACM.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th Annual Meeting of the Association for Com-putational Linguistics, pages 81?88, Sydney, Aus-tralia.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
IEEE Transactions on Knowledgeand Data Engineering, 22(10):1345?1359, October.Chris Quirk, Raghavendra U. Udupa, and Arul Menezes.2007.
Generative models of noisy translations withapplications to parallel fragment extraction.
In Pro-ceedings of the Machine Translation Summit XI, pages377?384, Copenhagen, Denmark.Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-poport.
2008.
Multi-task active learning for linguis-tic annotations.
In Proceedings of ACL-08: HLT,pages 861?869, Columbus, Ohio, June.
Associationfor Computational Linguistics.Philip Resnik and Noah A. Smith.
2003.
The web as aparallel corpus.
Comput.
Linguist., 29(3):349?380.Tobias Scheffer, Christian Decomain, and Stefan Wro-bel.
2001.
Active hidden markov models for informa-tion extraction.
In IDA ?01: Proceedings of the 4thInternational Conference on Advances in IntelligentData Analysis, pages 309?318, London, UK.
Springer-Verlag.Burr Settles, Mark Craven, and Soumya Ray.
2008.Multiple-instance active learning.
In In Advances inNeural Information Processing Systems (NIPS, pages1289?1296.
MIT Press.Bing Zhao and Stephan Vogel.
2002.
Full-text storyalignment models for chinese-english bilingual newscorpora.
In Proceedings of the ICSLP ?02, September.77
