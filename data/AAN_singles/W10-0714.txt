Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 89?92,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsMTurk Crowdsourcing:A Viable Method for Rapid Discovery of Arabic Nicknames?Chiara Higgins Elizabeth McGrath Lailla MorettoGeorge Mason University MITRE MITREFairfax, VA. 22030, USA McLean, VA. 20112, USA McLean, VA. 20112, USAchiara.higgins@gmail.com emcgrath@mitre.org lmoretto@mitre.orgAbstractThis paper presents findings on usingcrowdsourcing via Amazon MechanicalTurk (MTurk) to obtain Arabic nicknamesas a contribution to exiting Named Entity(NE) lexicons.
It demonstrates a strategyfor increasing MTurk participation fromArab countries.
The researchers validatethe nicknames using experts, MTurkworkers, and Google search and thencompare them against the Database ofArabic Names (DAN).
Additionally, theexperiment looks at the effect of pay rateon speed of nickname collection and doc-uments an advertising effect whereMTurk workers respond to existing workbatches, called Human Intelligence Tasks(HITs), more quickly once similar higherpaying HITs are posted.1 IntroductionThe question this experiment investigates is: canMTurk crowdsourcing add undocumented nick-names to existing Named Entity (NE) lexicons?This experiment seeks to produce nicknamesto add to DAN Version 1.1, which contains147,739 lines of names.
While DAN does not listnicknames as a metadata type, it does include somecommonly known nicknames.1.1 Traditional collection methods are costlyAccording to DAN?s website, administrators col-lect nicknames using a team of software engineersand native speakers.
They also draw on a ?largevariety of sources including websites, corpora,books, phone directories, dictionaries, encyclope-dias, and university rosters?
(Halpern, 2009).
Col-lecting names by searching various media sourcesor employing linguists and native speakers is amassive effort requiring significant expenditure oftime and money.1.2 Crowdsourcing might work betterThe experiment uses crowdsourcing via MTurksince it offers a web-based problem-solving modeland quickly engages a large number of internation-al workers at low cost.
Furthermore, previous re-search shows the effectiveness of crowdsourcing asa method of accomplishing labor intensive naturallanguage processing tasks  (Callison-Burch, 2009)and the effectiveness of using MTurk for a varietyof  natural language automation tasks (Snow,Jurafsy, & O'Connor, 2008).The experiment answers the following ques-tions:?
Can we discover valid nicknames not cur-rently in DAN??
What do we need to pay workers to gathernicknames rapidly??
How do we convey the task to guide non-experts and increase participation fromArab countries?892 Experiment DesignThe experiment contains three main phases.
First,nicknames are gathered from MTurk workers.Second, the collected names are validated viaMTurk, internet searches, and expert opinion.
Fi-nally, the verified names are compared against theavailable list of names in the DAN.2.1 Collecting nicknames on MTurkIn this phase, we open HITs on MTurk requestingworkers to enter an Arabic nickname they haveheard.
In addition to writing a nickname, theworkers input where they heard the name and theircountry of residence.HIT instructions are kept simple and writ-ten in short sentences to guide non-experts andinclude a basic definition of a nickname.
To en-courage participation of native Arabic speakers,the instructions and search words are in Arabic aswell as English.
Workers are asked to input namesin the Arabic alphabet, thus eliminating any workerwho does not use Arabic often enough to warranthaving an Arabic keyboard.
Further clarifying thetask, words highlighted in red, ?Arabic alphabet?,emphasize what the worker needs to do.While seeking to encourage participationfrom Arab countries, we choose not to block par-ticipation from other countries since there areArabic speakers and immigrants in many countrieswhere Arabic is not the main language.To evaluate the effect of pay rate on nick-name collection rate, HITs have a variety of payrates.
HITs paying $0.03 per HIT are kept upthroughout the experiment, while HITs paying$0.05 and finally $0.25 are added later.2.2 Nickname validation phaseVetting the nicknames, involves a Google checkand asking 3 experts and 5 MTurk workers to rateeach name that is submitted in a valid format.Each expert and MTurk worker has theopportunity to rate the likelihood the nicknamewould occur in the Arab world on a Likert scale(Strongly Agree, Agree, Neither Agree nor Disag-ree, Disagree, Strongly Disagree).The entire validation process is completedtwice, once paying the workers $.01 per validationand once paying $.05 per validation to allow us tofurther research the effect of pay on HIT collectionrate.The Google check vets the names to see ifthey occur on the web thus eliminating, any nick-names that are nowhere in print and therefore notcurrently necessary additions to NE lexicons.2.3 Compare data to ground truth in DANThe third phase is a search for exact matches forthe validated nicknames in DAN to determine ifthey represent new additions to the lexicon.3 ResultsMTurk workers generated 332 nicknames duringthe course of this experiment.
Because the initialcollection rate was slower than expected, we vali-dated and compared only the first 108 names toreport results related to the usefulness of MTurk innickname collection.
Results involving pay andcollection rate draw on the full data.Based on self-reported data, approximately35% of the respondents came from the Arabicspeaking countries of Morocco, Egypt, Lebanon,Jordan, UAE, and Dubai.
46% were submittedfrom India, 13% from the U.S. and 5% elsewhere.Figure 1.
Nicknames by nation38 5014 6Arabic?Speaking?CountriesIndia USA OtherNicknames?by?Nation903.1 Validation resultsEach of the nicknames was verified by MTurkworkers and three experts.
On a five-point Likertscale with 1 representing strong disagreement and5 showing strong agreement, we accepted 51 of thenames as valid because the majority (3 of 5 MTurkworkers and 2 of 3 experts) scored the name as 3or higher.One of the 51 names accepted by othermeans could not be found in a Google search leav-ing us with 50 valid nicknames.Comparing the 50 remaining names toDAN we found that 11 of the valid names werealready in the lexicon.3.2 Effect of increased pay on responsesHolding everything else constant, we increased theworker?s pay during nickname collection.
On aver-age, $0.03 delivered 9.8 names a day, for $0.05 wecollected 25 names a day and for $0.25 we col-lected 100 names in a day.We also posted one of our MTurk verifica-tion files two times, once at $0.01 per HIT andonce at $0.05 per HIT, holding everything constantexcept the pay.
Figure 2 shows the speed withwhich the two batches of HITs were completed.The results show not only an increased collectionspeed for the higher paying HITs, but also an in-creased collection speed for the existing lower pay-ing HIT once the higher paying HITs were posted.Figure 2.
HITS by payment amount over time4 ConclusionsAs our most significant goal, we sought to investi-gate whether MTurk crowdsourcing could success-fully collect undiscovered nicknames to add to anexisting NE lexicon.The results indicate that MTurk is a viablemethod for collecting nicknames; in the course ofthe experiment, we successfully produced 39 veri-fied nicknames that we recommend adding to theDAN.Another goal was to explore the effect ofworker pay on HIT completion rate.
Our initialcollection rate, at $0.03 per HIT, was only 9.8names per day.
By increasing pay, we were able tospeed up the process.
At $0.05 per name, we in-creased the daily collection rate from 9.8 to 25, andby making the pay rate $0.25 we collected 100names in a day.
So increasing pay significantlyimproved collection speed.While working with pricing for the verifi-cation HITs, we were able to quantify an ?advertis-ing effect?
we had noticed previously where theposting of a higher paying HIT causes existingsimilar lower paying HITs to be completed morequickly as well.
Further research could be con-ducted to determine a mix of pay rates that max-imizes collection rate while minimizing cost.Furthermore, the experiment shows that byusing bilingual directions and requiring typing inArabic, we were able to increase the participationfrom Arabic speaking countries.
Based on our pre-vious experience where we posted Arabic languagerelated HITs in English only, Arab country partici-pation on MTurk is minimal.
Other researchershave also found little MTurk participation fromArabic speaking countries (Ross, Zaldivar, Irani, &Tomlinson, 2009).
In this experiment, however,we received more than 35% participation fromworkers in Arabic speaking countries.AcknowledgmentsThanks to the MITRE Corporation for providingthe ground truth DAN data under their research0100200300400500Wed?Feb??Thur?Feb??Thur?Feb??Fri?Feb?26??Fri?Feb?26??Sat?Feb?27?
?Cumulative?HITs?per?Day$.01?per?HIT$.05?per?HIT91license agreement with the CJK Dictionary Insti-tute.
Also thanks to Trent Rockwood of MITREfor providing expert assistance in the Arabic lan-guage and on some technical issues.ReferencesCallison-Burch, C. (2009).
Fast, Cheap, and Creative:Evaluating Translation Quality Using Amazon?s Me-chanical Turk.
Proceedings of the EMNLP.
Singa-pore.Halpern, J.
(2009).
Lexicon-Driven Approach to theRecognition of Arabic Named Entities.
Second In-ternational Conference on Arabic Language Re-sources and Tools.
Cairo.Ross, J., Zaldivar, A., Irani, L., & Tomlinson, B.
(2009).Who are the Turkers?
Worker Demographics inAmazon.
Department of Informatics, University ofCalifornia, Irvine.Snow, R., Jurafsy, D., & O?Connor, B.
(2008).
Cheapand fast ?
but is it good?
: Evaluating Non-ExpertAnnotations for Natural Language Tasks.
Proceed-ings of the 2008 Conference on Empirical Methods inNatural Language Processing, (pp.
254-263).
Hono-lulu.92
