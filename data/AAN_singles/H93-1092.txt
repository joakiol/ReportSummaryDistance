Automatic Extraction of Grammars From Annotated TextSalim Roukos, Principal hlvestigawrroukos@watson .
ibm.comIBM T.J. Watson Research CenterP.O.
Box 704Yorktown Heights,  NY  10598PROJECT GOALS"lhe primary objective of this project is to develop a ro-bust, high-performance parser for English by automaticallyextracting a grammar from an annotated corpus of bracketedsentences, called the Treebank.
The project is a collabora-tion between the IBM Continuous Speech Recognition Groupand the University of Pennsylvania Department of ComputerSciences 1.
Our initial focus is the domain of computer man-uals with a vocabulary of 3000 words.
We use a Treebankthat was developed jointly by IBM and the University of Lan-caster, England, during the past three years.RECENT RESULTSWe have an initial implementation f our parsing model wherewe used a simple set of features to guide us in our develop-ment of the approach.
We used for training a Treebank ofabout 28,000 sentences.
The parser's accuracy on a sampleof 25 new sentences of length 7 to 17 words as judged, whencompared to the Treebank, by three members of the group, is52%.
This is encouraging in light of the fact that we are inthe process of increasing the features that the parser can lookat.
We give below a brief sketch of our approach.Traditionally, parsing relies on a grammar to determine a setof parse trees for a sentence and typically uses a scoringmechanism based on either ule preference or a probabilisticmodel to determine a preferred parse (or some higher levelprocessing is expected to do further disambiguation).
In thisconventional pproach, a linguist must specify the basic con-stituents, the rules for combining basic constituents into largerones, and the detailed conditions under which these rules maybe used.Instead of using a grammar, we rely on a probabilistic model,p(TIW), for the probability that a parse tree, T, is a parse forsentence W. We use data from the Treebank, with appropri-ate statistical modeling techniques, to capture implicitly theplethora of linguistic details necessary to correctly parse mostsentences.
Once we have built our model, we parse a sentenceby simply determining the most probable parse, T*, for thegiven sentence W from the set of all trees that span the givensentence.1 Co-Principal Investigators: Mark Liberman and Mitchell MarcusIn our model of parsing, we associate with any parse treea set of bottom-up derivations; each derivation describing aparticular order in which the parse tree is constructed.
Ourparsing model assigns a probability to a derivation, denotedby p(dlW).
The probability of a parse tree is the sum of theprobability of all derivations leading to the parse tree.The probability of a derivation is a product of probabilities,one for each step of the derivation.
These steps are of threetypes:a tagging step: where we want the probability of tagginga word with a tag in the context of the derivation up tothat point.a labeling step: where we want the probability of assign-ing a non terminal label to a node in the derivation.an extension step: where we want to determine the prob-ability that a labeled node is extended, for example, tothe left or right (i.e.
to combine with the preceding orfollowing constituents).The probability of a step is determined by a decision treeappropriate to the type of the step.
The three decision treesexamine the derivation up to that point to determine the prob-ability of any particular step.PLANS FOR THE COMING YEARWe plan to continue working with our new parser by complet-ing the following tasks:?
implement a set of detailed questions to capture infor-mation about conjunction, prepositional ttachment, etc.?
build automatically a new set of classes for the words inour vocabulary.?
tune the search strategy for theparser.398
