Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 176?180, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsDLS@CU-CORE: A Simple Machine Learning Model of SemanticTextual SimilarityMd.
Arafat Sultan, Steven Bethard, Tamara SumnerInstitute of Cognitive Science and Department of Computer ScienceUniversity of Colorado, Boulder, CO 80309{arafat.sultan, steven.bethard, sumner}@colorado.eduAbstractWe present a system submitted in the SemanticTextual Similarity (STS) task at the SecondJoint Conference on Lexical and Computa-tional Semantics (*SEM 2013).
Given twoshort text fragments, the goal of the system isto determine their semantic similarity.
Our sys-tem makes use of three different measures oftext similarity: word n-gram overlap, charactern-gram overlap and semantic overlap.
Usingthese measures as features, it trains a supportvector regression model on SemEval STS 2012data.
This model is then applied on the STS2013 data to compute textual similarities.
Twodifferent selections of training data result invery different performance levels: while a cor-relation of 0.4135 with gold standards was ob-served in the official evaluation (ranked 63rdamong all systems) for one selection, the otherresulted in a correlation of 0.5352 (that wouldrank 21st).1 IntroductionAutomatically identifying the semantic similaritybetween two short text fragments (e.g.
sentences) isan important research problem having many im-portant applications in natural language processing,information retrieval, and digital education.
Exam-ples include automatic text summarization, questionanswering, essay grading, among others.However, despite having important applications,semantic similarity identification at the level ofshort text fragments is a relatively recent area of in-vestigation.
The problem was formally brought toattention and the first solutions were proposed in2006 with the works reported in (Mihalcea et al2006) and (Li et al 2006).
Work prior to these fo-cused primarily on large documents (or individualwords) (Mihalcea et al 2006).
But the sentence-level granularity of the problem is characterized byfactors like high specificity and low topicality of theexpressed information, and potentially small lexicaloverlap even between very similar texts, asking foran approach different from those that were designedfor larger texts.Since its inception, the problem has seen a largenumber of solutions in a relatively small amount oftime.
The central idea behind most solutions is theidentification and alignment of semantically similaror related words across the two sentences, and theaggregation of these similarities to generate an over-all similarity score (Mihalcea et al 2006; Islam andInkpen, 2008; ?ari?
et al 2012).The Semantic Textual Similarity task (STS) or-ganized as part of the Semantic Evaluation Exer-cises (see (Agirre et al 2012) for a description ofSTS 2012) provides a common platform for evalua-tion of such systems via comparison with human-annotated similarity scores over a large dataset.In this paper, we present a system which wassubmitted in STS 2013.
Our system is based on verysimple measures of lexical and character-level over-lap, semantic overlap between the two sentencesbased on word relatedness measures, and surfacefeatures like the sentences?
lengths.
These measuresare used as features for a support vector regressionmodel that we train with annotated data fromSemEval STS 2012.
Finally, the trained model is ap-plied on the STS 2013 test pairs.Our approach is inspired by the success of simi-lar systems in STS 2012: systems that combine mul-tiple measures of similarity using a machine learn-ing model to generate an overall score (B?r et al2012; ?ari?
et al 2012).
We wanted to investigatehow a minimal system of this kind, making use ofvery few external resources, performs on a large da-taset.
Our experiments reveal that the performanceof such a system depends highly on the trainingdata.
While training on one dataset yielded a best176correlation (among our three runs, described later inthis document) of only 0.4135 with the gold scores,training on another dataset showed a considerablyhigher correlation of 0.5352.2 Computation of Text Similarity: SystemOverviewIn this section, we present a high-level descriptionof our system.
More details on extraction of some ofthe measures of similarity are provided in Section 3.Given two input sentences ?1 and ?2, our algo-rithm can be described as follows:1.
Compute semantic overlap (8 features):a. Lemmatize ?1 and ?2 using a memory-based lemmatizer1 and remove all stopwords.b.
Compute the degree to which the conceptsin ?1 are covered by semantically similarconcepts in ?2 and vice versa (see Section 3for details).
The result of this step is two dif-ferent ?degree of containment?
values (?1 in?2 and vice versa).c.
Compute the minimum, maximum, arith-metic mean and harmonic mean of the twovalues to use as features in the machinelearning model.d.
Repeat steps 1a through 1c for a weightedversion of semantic overlap where eachword in the first sentence is assigned aweight which is proportional to its specific-ity in a selected corpus (see Section 3).2.
Compute word ?-gram overlap (16 features):a.
Extract ?-grams (for ?
= 1, 2, 3, 4) of allwords in ?1 and ?2 for four different setupscharacterized by the four different valuecombinations of the two following varia-bles: lemmatization (on and off), stop-WordsRemoved (on and off).b.
Compute the four measures (min, max,arithmetic and harmonic mean) for eachvalue of n.3.
Compute character ?-gram overlap (16 fea-tures):a. Repeat   all steps in 2 above for character ?-grams (?
= 2, 3, 4, 5).1 http://www.clips.ua.ac.be/pages/MBSP#lemmatizer2 http://conceptnet5.media.mit.edu/data/5.1/as-soc/c/en/cat?
filter=/c/en/dog&limit=14.
Compute sentence length features (2 features):a. Compute the lengths of ?1 and ?2; and theminimum and maximum of the two values.b.
Include the ratio of the maximum to the min-imum and the difference between the maxi-mum and minimum in the feature set.5.
Train a support vector regression model on thefeatures extracted in steps 1 through 4 above us-ing data from SemEval 2012 STS (see Section4 for specifics on the dataset).
We used theLibSVM implementation of SVR in WEKA.6.
Apply the model on STS 2013 test data.3 Semantic Overlap MeasuresIn this section, we describe the computation of thetwo sets of semantic overlap measures mentioned instep 1 of the algorithm in Section 2.We compute semantic overlap between two sen-tences by first computing the semantic relatednessamong their constituent words.
Automatically com-puting the semantic relatedness between words is awell-studied problem and many solutions to theproblem have been proposed.
We compute word re-latedness in two forms: semantic relatedness andstring similarity.
For semantic relatedness, we uti-lize two web services.
The first one concerns a re-source named ConceptNet (Liu and Singh, 2004),which holds a large amount of common senseknowledge concerning relationships between real-world entities.
It provides a web service2 that gener-ates word relatedness scores based on these relation-ships.
We will use the term ?????
(?1, ?2) to de-note the relatedness of the two words ?1 and ?2 asgenerated by ConceptNet.We also used the web service3 provided by an-other resource named Wikipedia Miner (Milne andWitten, 2013).
While ConceptNet successfully cap-tures common sense knowledge about words andconcepts, Wikipedia Miner specializes in identify-ing relationships between scientific concepts pow-ered by Wikipedia's vast repository of scientific in-formation (for example, Einstein and relativity).
Wewill use the term ?????
(?1, ?2) to denote the re-latedness of the two words ?1 and ?2 as generatedby Wikipedia Miner.
Using two systems enabled us3 http://wikipedia-miner.cms.waikato.ac.nz/ser-vices/compare?
term1=cat&term2=dog177to increase the coverage of our word similarity com-putation algorithm.Each of these web services return a score in therange [0, 1] where 0 represents no relatedness and 1represents complete similarity.
A manual inspectionof both services indicates that in almost all caseswhere the services?
word similarity scores deviatefrom what would be the human-perceived similar-ity, they generate lower scores (i.e.
lower than thehuman-perceived score).
This is why we take themaximum of the two services?
similarity scores forany given word pair as their semantic relatedness:??????
(?1, ?2)= max?{?????
(?1, ?2),?????
(?1, ?2)}We also compute the string similarity betweenthe two words by taking a weighted combination ofthe normalized lengths of their longest commonsubstring, subsequence and prefix (normalization isdone for each of the three by dividing its length withthe length of the smaller word).
We will refer to thestring similarity between words ?1 and ?2 as?????????(?
?1, ?2).
This idea is taken from (Islamand Inkpen, 2008); the rationale is to be able to findthe similarity between (1) words that have the samelemma but the lemmatizer failed to lemmatize atleast one of the two surface forms successfully, and(2) words at least one of which has been misspelled.We take the maximum of the string similarity andthe semantic relatedness between two words as thefinal measure of their similarity:???
(?1, ?2)= max?{??????
(?1, ?2), ?????????
(?1, ?2)}At the sentence level, our first set of semanticoverlap measures (step 1b) is an unweighted meas-ure that treats all content words equally.
More spe-cifically, after the preprocessing in step 1a of the al-gorithm, we compute the degree of semantic cover-age of concepts expressed by individual contentwords in ?1 by ?2 using the following equation:?????
(?1, ?2) =?
[max???2{???
(?, ?)}]??
?1|?1|4 http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.htmlwhere ???
(?, ?)
is the similarity between the twolemmas ?
and ?.We also compute a weighted version of semanticcoverage (step 1d in the algorithm) by incorporatingthe specificity of each word (measured by its infor-mation content) as shown in the equation below:????
(?1, ?2) =?
[max???2{??(?).
???
(?, ?)}]??
?1|?1|where ??(?)
stands for the information content ofthe word ?.
Less common words (across a selectedcorpus) have high information content:??(?)
= ln?
?(??)?????(?
)where C is the set of all words in the chosen corpusand f(w) is the frequency of the word w in the cor-pus.
We have used the Google Unigram Corpus4 toassign the required frequencies to these words.4 EvaluationThe STS 2013 test data consists of four datasets:two datasets consisting of gloss pairs (OnWN: 561pairs and FNWN: 189 pairs), a dataset of machinetranslation evaluation pairs (SMT: 750 pairs) and adataset consisting of news headlines (headlines: 750pairs).
For each dataset, the output of a system isevaluated via comparison with human-annotatedsimilarity scores and measured using the PearsonCorrelation Coefficient.
Then a weighted sum of thecorrelations for all datasets are taken to be the finalscore, where each dataset?s weight is the proportionof sentence pairs in that dataset.We computed the similarity scores using threedifferent feature sets (for our three runs) for the sup-port vector regression model:1.
All features mentioned in Section 2.
This set offeatures were used in our run 1.2.
All features except word ?-gram overlap (ex-periments on STS 2012 test data revealed thatusing word n-grams actually lowers the perfor-mance of our model, hence this decision).
Theseare the features that were used in our run 2.3.
Only character ?-gram and length features (justto test the performance of the model without178any semantic features).
Our run 3 was based onthese features.We trained the support vector regression modelon two different training datasets, both drawn fromSTS 2012 data:1.
In the first setup, we chose the training datasetsfrom STS 2012 that we considered the mostsimilar to the test dataset.
The only exceptionwas the FNWN dataset, for which we selectedthe all the datasets from 2012 because no singledataset from STS 2012 seemed to have similar-ity with this dataset.
For the OnWN test dataset,we selected the OnWN dataset from STS 2012.For both headlines and SMT, we selected SMT-news and SMTeuroparl from STS 2012.
The ra-tionale behind this selection was to train the ma-chine learning model on a distribution similar tothe test data.2.
In the second setup, we aggregated all datasets(train and test) from STS 2012 and used thiscombined dataset to train the three models thatwere later applied on each STS 2013 test data.Here the rationale is to train on as much data aspossible.Table 1 shows the results for the first setup.
Thisis the performance of the set of scores which we ac-tually submitted in STS 2013.
The first four col-umns show the correlations of our system with thegold standard for all runs.
The rightmost columnshows the overall weighted correlations.
As we cansee, run 1 with all the features demonstrated the bestperformance among the three runs.
There was a con-siderable drop in performance in run 3 which did notutilize any semantic similarity measure.Table 1.
Results for manually selected training dataRun headlines OnWN FNWN SMT Total1 .4921 .3769 .4647 .3492 .41352 .4669 .4165 .3859 .3411 .40563 .3867 .2386 .3726 .3337 .3309As evident from the table, evaluation results didnot indicate a particularly promising system.
Ourbest system ranked 63rd among the 90 systems eval-uated in STS 2013.
We further investigated to findout the reason: is the set of our features insufficientto capture text semantic similarity, or were the train-ing data inappropriate for their corresponding testdata?
This is why we experimented with the secondsetup discussed above.
Following are the results:Table 2.
Results for combined training dataRun headlines OnWN FNWN SMT Total1 .6854 .5981 .4647 .3518 .53392 .7141 .5953 .3859 .349 .53523 .6998 .4826 .3726 .3365 .4971As we can see in Table 2, the correlations for allfeature sets improved by more than 10% for eachrun.
In this case, the best system with correlation0.5352 would rank 21st among all systems in STS2013.
These results indicate that the primary reasonbehind the system?s previous bad performance (Ta-ble 1) was the selection of an inappropriate dataset.Although it was not clear in the beginning which ofthe two options would be the better, this second ex-periment reveals that selecting the largest possibledataset to train is the better choice for this dataset.5 ConclusionsIn this paper, we have shown how simple measuresof text similarity using minimal external resourcescan be used in a machine learning setup to computesemantic similarity between short text fragments.One important finding is that more training data,even when drawn from annotations on differentsources of text and thus potentially having differentfeature value distributions, improve the accuracy ofthe model in the task.
Possible future expansion in-cludes use of more robust concept alignment strate-gies using semantic role labeling, inclusion of struc-tural similarities of the sentences (e.g.
word order,syntax) in the feature set, incorporating word sensedisambiguation and more robust strategies of con-cept weighting into the process, among others.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-lez-Agirre.
2012.
SemEval-2012 Task 6: a pilot on se-mantic textual similarity.
In Proceedings of the FirstJoint Conference on Lexical and Computational Se-mantics.
ACL, Stroudsburg, PA, USA, 385-393.Daniel B?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
UKP: computing semantic textual simi-larity by combining multiple content similaritymeasures.
In Proceedings of the First Joint Confer-ence on Lexical and Computational Semantics.
ACL,Stroudsburg, PA, USA, 435-440.Aminul Islam and Diana Inkpen.
2008.
Semantic textsimilarity using corpus-based word similarity andstring similarity.
ACM Trans.
Knowl.
Discov.
Data 2,2, Article 10 (July 2008), 25 pages.179Yuhua Li, David Mclean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2006.
Sentence similar-ity based on semantic nets and corpus statistics.
IEEETransactions on Knowledge and Data Engineering,vol.18, no.8, 1138-1150.Hugo Liu and Push Singh.
2004.
ConceptNet ?
a prac-tical commonsense reasoning tool-kit.
BT TechnologyJournal 22, 4 (October 2004), 211-226.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of the 21stnational conference on Artificial intelligence - Volume1 (AAAI'06), Anthony Cohn (Ed.
), Vol.
1.
AAAIPress 775-780.David Milne and Ian H. Witten.
2013.
An open-sourcetoolkit for mining Wikipedia.
Artif.
Intell.
194 (Janu-ary 2013), 222-239.Frane ?ari?, Goran Glava?, Mladen Karan, Jan ?najder,and Bojana Dalbelo Ba?i?.?ari?.
2012.
TakeLab: sys-tems for measuring semantic text similarity.
In Pro-ceedings of the First Joint Conference on Lexical andComputational Semantics.
ACL, Stroudsburg, PA,USA, 441-448.180
