Computer Generation of Multiparagraph English Text 1Wi l l iam C. MannJames  A .
MooreIn fo rmat ion  Sc iences  Ins t i tu teUn ivers i ty  of Southern  Ca l i forn iaMar ina  del Rey, Ca l i forn ia  90291This paper reports recent research into methods for creating natural language text.
Anew processing paradigm called Fragment-and-Compose has been created and an experi-mental system implemented in it.
The knowledge to be expressed in text is first dividedinto small propositional units, which are then composed into appropriate combinations andconverted into text.KDS (Knowledge Delivery System), which embodies this paradigm, has distinct partsdevoted to creation of the propositional units, to organization of the text, to prevention ofexcess redundancy, to creation of combinations of units, to evaluation of these combina-tions as potential sentences, to selection of the best among competing combinations, and tocreation of the final text.
The Fragment-and-Compose paradigm and the computationalmethods of KDS are described.IntroductionComputer  users have di f f icult ies in unders tand ingwhat  knowledge is s tored in their  computers ;  the sys-tems have cor responding dif f icult ies in del iver ing theirknowledge.
The knowledge in the machine may berepresented  in an incomprehens ib le  notat ion ,  or  wemay want  to share the knowledge with a large groupof peop le  who lack the t ra in ing to unders tand  thecomputer 's  formal  notat ion.
For  example,  there arelarge s imulat ion programs that get into very compl icat -ed states we would l ike to be able to unders tand  easi-ly.
There are data base systems with complex know-ledge bur ied in them, but  real prob lems in extract ingit.
There are s tatus-keeping systems from which wewould like to get snapshots.
There are systems thattry to prove things, f rom which we would like to haveprogress reports  and just i f icat ions for var ious actions.Many  other  kinds of systems have knowledge-de l iverydiff icult ies.1 This research was supported in part by National ScienceFoundation grant No.
MCS76-07332 and in part by the Air ForceOffice of Scientific Research contract No.
F49620-79-c-0181.
Theparticipation of Neil Goldman and James Levin is gratefully ac-knowledged.
The views and conclusions contained in this documentare those of the authors and should not be interpreted as necessari-ly representing the official policies or endorsements, either ex-pressed or implied, of the Air Force Office of Scientific Research ofthe U.S. Government.The c i rcumstances that make it part icu lar ly  at t rac-tive to del iver this knowledge in natura l  language are:a) complex i ty  of the source knowledge,  so that  itsnotat ion  is not  easi ly  learned,  b) unpred ic tab i l i ty  ofthe demands  for knowledge,  so that  the actual  de-mands  cannot  be met with speci f ic  p reprogrammedoutput ,  and c) the need to serve a large pool  of un-t ra ined or l ightly t ra ined users of these systems.For  a number  of the kinds of systems ment ionedabove,  gett ing the in format ion  out is one of the pr inci -pal  l imitat ions on the systems'  uses.
If the in format ioncould be accessed more easi ly, then far more peoplecould use the systems.
So we are talk ing in part  aboutfac i l i tat ing exist ing systems,  but  much more  aboutcreat ing new opportun i t ies  for systems to serve people.If  computer  systems could express  themselves  inf luent natura l  language,  many of these di f f icul t ieswould d isappear .
However ,  the necessary processesfor such express ion do not  exist, and there are formi-dable obstac les  even to designing such processes.
Thetheory  of wr i t ing is sketchy and vague, and there arefew interest ing computer  systems to serve as preced-ents.
Any  research ef fort  to create such systems - -systems that know how to write - -  can be s ignif icantboth  in its pract ical  impl icat ions and for the knowl -edge of wr i t ing that it produces.Copyright 1981 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial dvantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/81/010017-13501.00American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 17Wil l iam C. Mann and James A. Moore  Computer Generation of Multiparagraph English TextWriting is an intellectually interesting task, thoughpoorly understood.
If we want to have a better theo-ry, a better characterization of this task, then we canuse computer program design and test as a discoveryprocedure for exploring the subject.
In the presentstate of the art, the same research can create boththeoretical knowledge and practical computationalmethods.Of course, in a limited sense, programs alreadydeliver knowledge in natural language by using"canned text."
A person writes some text, possiblywith the use of blank segments, and the text is storedfor use in association with a particular demand.
Themachine fills in the blanks as needed in a way antici-pated as sufficient for the demand.
This is a veryuseful technique, but it does not tell us much aboutthe task of writing, and it does not generalize to situa-tions in which the need for text has not been wellanticipated in the programming stage.
The basic or-ganization and writing of canned text are done by aperson, so that sort of "writing" falls outside the scopeof this paper.The idea that natural language would be a usefuloutput medium for a computer system is quite old, butthe research to make such output possible is still in avery primitive state.
Most of it has concentrated onsentence generation, in which problems such as wordselection or translation to natural language from logi-cal formulas have been chosen for study.
These areimportant, but the needs for knowledge delivery sim-ply cannot be satisfied by single sentences.
It is nec-essary to have multiple sentences to communicatecomplex information, and multiple paragraphs wouldbe even more valuable.The remainder of this paper thus focuses on com-puter generation of multisentence text.
We presentsome examples of multisentence output from past sys-tems and then discuss our own research.Mult isentent ia l  Text Generation in Past ResearchSeveral systems have generated multisentence text\[2,5,8,10,11\].2 3 Sample texts from three of these areshown in Figure 1.
One example is the SCHOLARsystem, which describes aspects of South Americangeography.
SCHOLAR's  description of a SouthAmerican country was several sentences long \[2\].
TheSAM system, a story generator operating from a con-ceptual dependency script, also created multisententialoutput \[10\].
More recently, Meehan developedTALE-SPIN, a fable generator that worked in a differ-2 Since this paper was written in 1979, additional work in-volving multisentential generation has been done or published\[3,4,7\].
Only the references have been updated.3 All of the cited research has worked with English, but otherlanguages would have served as well.SCHOLAR:*TELL ME ABOUT THE LOCATION OF URUGUAY.
*THE LOCATION IN URUGUAY IS SOUTH AMERICA.THE LATITUDE RANGES FROM -30 TO -35 DEGREES.THE LONGITUDE RANGES FROM -53 TO -58 DEGREES.THE NORTHERN AND EASTERN BORDERING COUNTRY IS BRAZIL.THE WESTERN BORDERING COUNTRY IS ARGENTINA.THE BOUNDARY IS THE URUGUAY RIVER.THE SOUTHEASTERN COAST IS THE ATLANTIC OCEAN.THE SOUTHERN COAST IS THE RIO DE LA PLATA.SAM:John decided he wasgoing to a restaurant.
He went to one.He asked the hostess for a table.
She told him he could goto i t .
He went to i t .
She went to i t .
He sat down in achair.
He got a menu from her.
He read i t .
A waiter sawJohn was at the table.
The waiter went to i t .
Johnordered a lobster.
A chef cooked i t .
The waiter got i tfrom the chef.
The waiter served i t  to John quickly.
Heate i t .
He asked the waiter for a check.
John got i t  fromthe waiter.
John remembered the waiter had served thelobster to him quickly.
John le f t  the waiter a large tip.John paid the check.
He le f t  the restaurant.TALESPIN:ONCE UPON A TIME GEORGE ANT LIVED NEAR A PATCH OF GROUND.THERE WAS A NEST IN AN ASH TREE.
WILMA BIRD LIVED IN THENEST.
THERE WAS SOME WATER IN A RIVER.
WILMA KNEW THATTHE WATER WAS IN THE RIVER.
GEORGE KNEW THAT THE WATER WASIN THE RIVER.
ONE DAY WILMA WAS VERY THIRSTY.
WILMAWANTED TO GET NEAR SOME WATER.
WILMA FLEW FROM HER NESTACROSS A MEADOW THROUGH A VALLEY TO THE RIVER.
WILMA DRANKTHE WATER.
WILMA WASN'T THIRSTY ANY MORE.GEORGE WAS VERY THIRSTY.
GEORGE WANTED TO GET NEAR SOMEWATER.
GEORGE WALKED FROM HIS PATCH OF GROUND ACROSS THEMEADOW THROUGH THE VALLEY TO A RIVER BANK.
GEORGE FELLINTO THE WATER.
GEORGE WANTED TO GET NEAR THE VALLEY.GEORGE COULDN'T GET NEAR THE VALLEY.
GEORGE WANTED TO GETNEAR THE MEADOW.
GEORGE COULDN'T GET NEAR THE MEADOW.WILMA WANTED TO GET NEAR GEORGE.
WILMA GRABBED GEORGE WITHHER CLAW.
WILMA TOOK GEORGE FROM THE RIVER THROUGH THEVALLEY TO THE MEADOW.
GEORGE WAS DEVOTED TO WILMA.
GEORGEOWED EVERYTHING TO WILMA.
WILMA LET GO OF GEORGE.
GEORGEFELL TO THE MEADOW.
THE END.Figure 1.
Some published multisentence t xt samples.ent way, also based on a conceptual dependency rep-resentation \[8\].These systems share several features.
First, thedata structures that are the basis of the generationwere designed for text processing; many of the specialdemands of text processing were anticipated and ac-commodated in the design of the knowledge structuresthemselves.
Second, the sentence boundaries in thesesystems were direct correlates of internal features of18 Amer ican Journal of Computational Linguistics, Volume 7, Number  1, January-March 1981Wi l l iam C. Mann and James A. Moore Computer Generation of Multiparagraph English Textthe data structures themselves.
Often the sentenceorder arose in the same way.
4 Third, these systemshad fixed generation goals, implicit in the code.
Thus,the reader's needs were taken to be fixed and pre-known by the system.
Fourth, although goal-pursuitcould sometimes be described in the material beinggenerated, the systems themselves did not operate on agoal-pursuit algorithm.
Finally, none of these systemschose the particular sentences to use in their output onthe bases of quality assessment or comparisons amongalternatives.In all five of these points, the KDS research con-trasts with these previous efforts.
We have workedwith data structures not designed for text generation;the sentence boundaries we develop are not directcorrelates of internal features of the data structures;there are explicit goals for the generation process tosatisfy; the system itself pursues goals; and the finaltext is chosen through quality comparisons amongalternative ways of saying things.The Task for the Knowledge Delivery SystemIn the light of these considerations, the problemcan be restated more specifically as follows:Given1.
An explicit goal of knowledge xpression,2.
A computer-internal knowledge base ade-quate for some non-text purpose, and3.
Identification of the parts of the knowledgebase that are relevant o the goal,the task is to produce clean, multiparagraph text, inEnglish, which satisfies the goal.The Partitioning ParadigmWhen we have stated this task to AI workers famil-iar with natural language processing, with no furtherspecification, they have expected a particular kind ofsolution.
They say, "Well, there are some sentencegenerators around, but the given information struc-tures are too large to be expressed in single sentences.Therefore what we need is a method for dividing up theinput structure into sentence-size pieces.
Then we cangive the pieces to a suitable sentence generator andget the desired text."
This is the expected solution,and people will simply presume that it is the line ofdevelopment being taken.4 This is not to say that sentence boundaries are always onefor one with data structures, nor that the data structures alwayscontain all the information used in making a sentence.
But theforms of data structures in these systems have been shaped almostexclusively by natural language processing tasks, which tends tomake sentence boundary determination easy.
The content of thosestructures has often been filled in manually, leaving indeterminablethe relative contributions ofprogram and programmer.That approach, which we call the Partitioning para-digm for text generation, was used in all the systemsdescribed above.
For the Partitioning paradigm towork, the generation task must be simplified by fea-tures of the knowledge base:1.
The knowledge base data structures havefeatures that indicate appropriate sen-tence boundaries, and2.
The pieces of information appropriate tobe expressed in an individual sentence areadjacent.
That is, a process can access allof the information appropriate to be ex-pressed in a single sentence by followingthe data structure, without being requiredto traverse information to be expressed inother sentences.These conditions prevail (by design) in all of thesystems described above, but they are not generallytypical of information storage in computers.
As wewill see, KDS takes an entirely different approach tothe problem.Several inherent difficulties become apparent whenwe attempt o use partitioning:1.
Missing adjacencies - -  Since (by ourproblem definition) the knowledge comesfrom a structure not prestructured for thegeneration task, what is and what is notadjacent in the knowledge base may be quitearbitrary.
We may wish to include severalwidely scattered items in a sentence, so thatit is not possible to carve out a piece withthose items in it at all.
The adjacencies thatwe need in order to partition the structureinto sentence-size parts may simply beabsent.2.
Intractable residues - -  Even though wemay be able to find some way to start cuttingout sentence-size objects from the datastructure, there is no assurance at all that wewill be able to run that method to completionand carve the entire structure into sentence-size pieces.
Think of the comparable prob-lem of carving statues from a block of mar-ble.
We may be able to get one statue orseveral, but if every part of the original blockmust end up looking like a statue, ordinarycarving methods are insufficient.
The resi-dues left after carving out the first few stat-ues may be intractable.
A comparable sortof thing can happen in attempting to parti-tion data structures.3.
Lack of boundary correlates - -  In someways the worst difficulty is that an arbitrarygiven data structure does not contain struc-tural correlates of good sentence boundaries.American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 19Wil l iam C. Mann and James A. Moore Computer Generation of Multiparagraph English TextOne cannot inspect the data structure andtell in any way where the sentence bounda-ries ought to be.
Along with the other diffi-culties, this has led us to reject the expectedsolution, the Partitioning paradigm, and tocreate another.The Fragment-and-Compose ParadigmOur solution comes in two steps:1.
Find methods for fragmenting the givendata structure into little pieces, small prop-ositional parts.2.
Find methods for composing ood sentencesand good paragraphs out of those littleparts.We call this the Fragment-and-Compose paradigm.It is interesting to note that other systems employ aFragment-and-Compose approach - -  e.g., buildingconstruction, papermaking, and digestion.
In each,one begins by producing small, easily manipulatedobjects much smaller than the desired end-productstructures, and then assembles these into the desiredend products in a planned, multistage way.
For theblock of marble, the comparable processes are crush-ing and casting.We may not be very encouraged in our text genera-tion task by such precedents.
However,  there areprecedents much closer to our actual task.
The task ofnatural language translation resembles in many waysthe task of translating from a computational knowl-edge source (although it has a comprehension subtaskwhich we lack).
Consider the (annotated) quotationbelow from Toward a Science of Translating \[9\].The process by which one determinesequivalence (faithfully translates) betweensource and receptor languages is obviously ahighly complex one.
However, it may bereduced to two quite simple procedures:(1) "decomposition" of the message into thesimplest semantic structure, with the mostexplicit statement of relationships; and(2) "recomposition" of the message into thereceptor language.The quotation is from Nida's chapter on translationprocedures.
Notice particularly the two steps:decomposition and recomposition, and the emphasis onsimple, explicit semantic structures in the results of thedecomposition.It turns out that this is the central procedural state-ment of Nida's book, and the remainder of the bookcan be seen as giving constraints and considerations onhow this decomposition and recomposition ought totake place.
We have very good reasons here forexpecting that Fragment-and-Compose is anappropriate paradigm for natural language knowledgedelivery.To give a sense of what can be done usingFragment-and-Compose, h re is a piece of a machine-generated text (created by KDS) about what happenswhen fire breaks out in the computer oom.Whenever there is a f i re,  the alarm system isstarted, which sounds a bell and starts a timer.Ninety seconds after the timer starts, unless thealarm system is cancelled, the system calls WellsFargo.
When Wells Fargo is called, they, inturn, call the Fire Department.Description of KDSFigure 2 is a block diagram of KDS, which simplysays that KDS takes in an Expressive Goal (tellingwhat the text should accomplish relative to its reader)and also a pre-identified body of Relevant Knowledgein the notation of its source.
The output is multipara-graph text that is expected to satisfy the goal.Expressive ~goal - ~ \ [  KDSRelevantknowledge / ,MultiparagraphtextFigure 2.
Input and output of KDS.We will be carrying a single example through thisdescription of KDS.
It is the most complex examplehandled by KDS, and it incorporates many ideas fromprevious studies on description of computer messagesystems.A small contingency-plans data base containsknowledge about what happens in various circum-stances, and about people's actions, responsibilities,authorities, and resources.
The particular knowledgeto be delivered concerns a computer room in whichthere may be some indication of fire and in whichthere is a computer operator who should know what todo if that happens.
This operator is the nominal read-er of the text.The general Expressive Goal is that the computeroperator will know what to do in all of the predictablecontingencies that can arise starting with an indicationof fire.
The contingencies are represented in the "FireAlarm Scene," part of the knowledge base.
A sche-matic sketch of the Fire Alarm Scene is given in Fig-ure 3.
(The figure is expository and contains far lessinformation than the actual Scene.
The Scene is a"semantic net," a collection of LISP expressions thatrefer to the same objects.
)20 American Journal of Computational Linguistics, Volume 7, Number  1, January-March 1981William C. Mann and James A. Moore Computer Generation of Multiparagraph English TextI INIT I(bell sounds orfire detected;timer starts)i T,Mo0T I I ATTENO I( ~  (evaluate situation)(Wells Fargo called) (don't cancel) (cancel alarm)iR~SPONSEI I FL'OHT I(Fire Dept.
responds) (evacuate)I~,~,~, !
I~o~o~1(Fire Dept.
fights fire) (Fire Dept.goes home) I ,(end of scene)I ~ I cae~ / I ~A~OWO~I,r sumewo k,Figure 3.
Events in the Fire-Alarm scene.American Journal of Computational Linguistics, Volume 7, Number 1 ,~uary -March  1981 21Will iam C. Mann and James A. Moore Computer Generation of Multiparagraph English TextKDS MODULES MODULE RESPONSIBILITIESFRAGMENTERPROBLEM SOLVERKNOWLEDGE FILTERHILL CLIMBERSURFACE SENTENCE MAKER* Extraction of knowledge from external notatione Division into expressible clauses* Style selectione Gross organization of text?
Cognitive redundancy removal?
Composition of concepts?
Sentence quality seeking?
Final text creationFigure 4.
KDS module responsibilities.The knowledge identified as relevant includes notonly the events of this scene but also enough informa-tion to support another computational task.
In thisexample the knowledge is sufficient to support analternate task, which we call the Motivation Exhibittask, i.e., to exhibit, for each action in the scene, theactor's reasons for performing the action.
So, forexample, the relevant knowledge includes the knowl-edge that fires destroy property, that destroying prop-erty is bad, that the amount of property destroyedincreases with the duration of the fire, and that theFire Department  is able to employ methods forstopping fires.
This is sufficient to be able to explainwhy the Fire Department attempts to stop fires.
KDSdoes not perform the Motivation Exhibit task, but itsknowledge is sufficient for it.
We generate from aknowledge base sufficient for multiple tasks in orderto explore the problems created when the knowledgerepresentation is not designed for text processing.The content of the scene is as follows:In the beginning state, INIT,  the firealarm sounds a bell.
As we follow down theleft side of the figure, we see that the firealarm starts an interval timer, and at the endof the interval, the timer automatical lyphones Wells Fargo Company, the alarm sys-tem manager.
Wells Fargo phones the FireDepartment,  and the Fire Department comes.The Fire Department fights the fire if thereis one, and otherwise goes home.Meanwhile, the computer operator  mustpay attention to the alarm and decide whatto do.
He can block the alarm system's ac-tion, cancelling the alarm, or he can let thealarm system take its course.
In the lattercase, his next duty is to call the Fire Depart-ment himself, which has the same effect asWells Fargo calling it.
After that, his nextduty is to flee.
If he blocks the alarm thenhe is to go back to his previous task.Major Modules of KDSKDS consists of five major modules, as indicated inFigure 4.
A Fragmenter is responsible for extractingthe relevant knowledge from the notation given to itand dividing that knowledge into small expressibleunits, which we call fragments or protosentences.
AProblem Solver, a goal-pursuit engine in the AI tradi-tion, is responsible for selecting the presentationalstyle of the text and also for imposing the gross organ-ization onto the text according to that style.
AKnowledge Filter removes protosentences that neednot be expressed because they would be redundant othe reader.The largest and most interesting module is the HillClimber, which has three responsibilities: to compose22 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981William C. Mann and James A. Moore Computer Generation of Multiparagraph English Textcomplex protosentences from simple ones, to judgerelative quality among the units resulting from com-position, and to repeatedly improve the set of proto-sentences on the basis of those judgments o that it isof the highest overall quality.
Finally, a very simpleSurface Sentence Maker creates the sentences of thefinal text out of protosentences.The data flow of these modules can be thought ofas a simple pipeline, each module processing the rele-vant knowledge in turn.
We will describe each ofthese modules individually.Fragmenter  Modu leThe Fragmenter (Figure 5) takes in the relevantknowledge as it exists externally and produces a set ofindependent protosentences, called the Sayset.
Theseprimitive fragments, the protosentences, have no in-tended order.
(In our final tests, they are presented ina list that is immediately randomized.)
Each primitiveprotosentence can, if necessary, be expressed by anEnglish sentence.Relevant _~tFRAGMENTE R .__D..{SAYSET} KnowledgeFigure 5.
Fragmenter module input and output.To help the reader understand the level of thesefragments, were they to be expressed in English, theywould look like:"Fire destroys objects.
""Fire causes death.
""Death is bad.
""Destroying objects is bad."
etc.So the problem for the remainder of the system isto express well what can surely be expressed badly.
Itis important o note that this is an improvement prob-lem rather than a problem of making expression inEnglish feasible.The protosentences the Fragmenter produces arepropositional and typically carry much less informationthan a sentence of smooth English text.
In our exam-ple, the fragmenter produces the list structures hownin part below for two of its fragments.
((CONSTIT (WHEN (CALLS NIL WELLS-FARGO)(CALLS WELLS-FARGO FIRE-DEPT)))...)((CONSTIT (WHENEVER (STARTS NIL ALARM-SYSTEM)(PROB (SOUNDS ALARM-SYSTEM BELL)...)These fragments encode: "When {unspecified} callsWells Fargo, Wells Fargo calls the Fire Department.
"and "Whenever {unspecified} starts the alarm system,the alarm system probably sounds the bell.
"Problem Solver  Modu leThe second major module is the Problem Solver(Figure 6).
The primary responsibilities of the Prob-lem Solver are to select a text presentation style and toorganize the text content according to the selected style.For this purpose, it has a built-in taxonomy of stylesfrom which it selects.
Although the taxonomy andselection processes are very rudimentary in this partic-ular system, they are significant as representatives ofthe kinds of structures needed for style selection andstyle imposition.Express iveGoal{SAYSET IPROBLEMSOLVER (SAYLIST with ADVICE)Figure 6.
Problem Solver input and output.We believe that text style should be selected on thebasis of the expected effects.
In simple cases this is soobvious as to go unrecognized; in more complex cases,which correspond to complex texts, there are manystylistic choices.
In order to select a style, one needs:1.
A description of the effect the text shouldhave on the reader,2.
Knowledge of how to apply stylisticchoices, and3.
A description of the effects to be expectedfrom each stylistic choice.Note that these are required whether stylisticchoices are distributed or holistic, i.e., whether theyare made in terms of attributes of the final text or interms of particular methods for creating or organizingthe text.The first requirement above, a description of de-sired effects, is (more or less by definition) a goal.The second item is the set of applicable methods, andthe third is the knowledge of their effects.
The Prob-lem Solk, er is a goal-pursuit process that performsmeans-ends analysis in a manner long familiar in AI.The information organization is significant partly be-cause of the demand it puts on the knowledge of style:Knowledge of style must be organized according to ex-pected effect.
Otherwise, the program has no adequatebasis for selecting style.American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 23Will iam C. Mann and James A. Moore Computer Generation of Multiparagraph English TextThe Problem Solver takes in the Sayset producedby the Fragmenter and the Expressive Goal given tothe system and produces a Saylist, which is an orderedlist of the protosentences, ome of which have beenmarked with Advice.
The Problem Solver pursuesgiven goals.
It has several submodules that specializein particular kinds of goals, including modules Tell andInstructional-narrate, which are active on this example.The Problem Solver can operate on the current Saylistwith three kinds of actions in any of its modules:1.
It can Factor the Saylist into two sublistsseparated by a paragraph break.
It ex-tracts all protosentences with a particularcharacter or attribute and places themabove the paragraph break, which isabove all those that lack that attribute.Order within each sublist is retained.2.
It can impose an order on some or all ofthe elements of the Saylist.3.
It can mark protosentences with Advice.Sometimes the Problem Solver knowssome attribute of the final text that oughtto be achieved, perhaps because of a de-mand of the chosen style, but it has noway to effect this directly.
In this case itmarks all the affected protosentences withAdvice, which will be acted on after theProblem Solver has finished.following fragment:(PARAGRAPH-BREAK (REASON: (BOUNDARY NON-H-ACTOR)))((CONSTIT (WHEN (IF (POSSIBLE)(CALL YOU FIRE-DEPT))(EVOKE YOU EVAC-SCENE)))...(ADVlSORS FRAG INST-NARRATE)(ADVICE ...(GOOD YOU)))These represent: "(Put a paragraph break here be-cause the actions of agents other than the hearer endhere)" and "If possible, call the Fire Department;then, in either case, evacuate.
(Advised by FRAG andINST-NARRATE Modules) (Advised that YOU isGOOD)" .Knowledge Filter ModuleThe Knowledge Filter is a necessary part of KDSbecause as soon as we attempt to create text from aknowledge base suitable to support some other compu-tational purpose, we find a great deal of informationthere that ought not to be expressed, because thereader already knows it.This is a general phenomenon that will be encoun-tered whenever we generate from an ordinary compu-tational knowledge base.
As an illustration, considerBadler's work on getting a program to describe amovie in English.Figure 7 describes the rules used in the ProblemSolver that carry out these three kinds of actions.
Inthis example, the Tell module acts beforeInstructional-narrate.
The Factoring rules are appliedsequentially, so that the last one prevails over previousones.The first Tell rule corresponds to the heuristic thatthe existence of something ought to be mentionedbefore its involvement with other things is described.The third rule corresponds to the heuristic that thewriter (KDS) ought to reveal its own goals of writingbefore pursuing those goals.Instructional-narrate uses a presentational tech-nique that makes the reader a participant in the text.So, for example, the final text says, "When you hearthe alarm bell ...," rather than "When the operatorhears the alarm bell...," Instructional-narrate knowsthat the role of "you" should be emphasized in thefinal text, but it has no direct way to achieve this.
Toevery protosentence that refers to "you," it attachesadvice saying that explicit reference to the reader,which is done by mentioning "you" in the final text,has positive value.
This advice is taken inside theHill-climber.In our example the Problem Solver creates theFactoring Rules:TELL1.
Place all (EXISTS ...) propositions in an uppersection.2.
Place all propositions involving anyone's goalsin an upper section.3.
Place all propositions involving the author'sgoals in an upper section.INSTRUCTIONAL-NARRATE1.
Place all propositions with non-reader actor inan upper section.2.
Place all time dependent propositions in a low-er section.Ordering Rules:INSTRUCTIONAL-NARRATE1.
Order time-dependent propositions accordingto the (NEXT ...) propositions.Advice-giving Rules:INSTRUCTIONAL-NARRATE1.
YOU is a good thing to make explicit in thetext.Figure 7.
Rules used in the Problem Solver.24 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981Wi l l iam C. Mann and James A. Moore Computer Generation of Multiparagraph English TextI/36 117 128 13145 10 15Figure 8.
Badler's "Moving Car Scenario".Figure 8 is reproduced from \[1\].
It shows fifteensuccessive scenes from a short computer-generatedmovie.
The graphics system that generates the movieprovides a stock of propositional knowledge about it.The objects in the scene are known to the machineunambiguously and in sufficient detail to generate themovie.
The research task is to create a computer pro-gram that will describe in English the physical activi-ties in this and similar movies.
The detail is volumi-nous, and so Badler is faced with a serious informationsuppression problem.
After several stages of applyingvarious filtering heuristics, such as "Don' t  describedirectly anything that doesn't move," he can representthe movie by the five statements below.1.
There is a car.2.
The car starts moving toward the observerand eastward, then onto the road.3.
The car, whil e going forward, starts turn-ing, moves toward the observer and-east-ward, then northward-and-eastward, thenfrom the driveway and out-of the drive-way, then off-of the driveway.4.
The car, while going forward, movesnorthward-and-eastward, then northward,then around the house and away-from thedriveway, then away-from the house andstops turning.5.
The car, while going forward, movesnorthward, then away.These are still too cumbersome, so additional stagesof reduction are applied, yielding the single statement:The car approaches, then moves onto theroad, then leaves the driveway, then turnsaround the house, then drives away from thehouse, then stops turning, then drives away.Even the longer text above contains only a fractionof the available information about the car and theother objects.
Information on their types, their sub-parts, visibility, mobility, location, orientation and sizeare available from Badler's source.
He also develops asequence of events to describe the movie, based oncertain indicators of continuity and discontinuity.
Thevolume of  information available, the predictability of  itsparts, and the insignificance of  some of  its details aresuch that all o f  it could not have been expressed in asmooth text.American Journal of Computational Linguistics, Vo lume 7, Number  1, January -March  1981 25Wil l iam C. Mann and James A. Moore Computer  Generation of Multiparagraph English TextOne of the principal activities of Badler's system isselection of information to be removed from the set ofideas to be expressed.
Some things need not be ex-pressed because they follow from the reader's generalknowledge about motion of objects; others are re-moved because they represent noise, rather than sig-nificant events, generated by the processes that dis-cern motion.The point for us is simply that the demands ofsmooth text production are incompatible with expressionof all of the available information.
Text productionrequires condensation and selectivity, the process wecall knowledge filtering, on any reasonably completebody of knowledge.
Knowledge filtering is a signifi-cant intellectual task.
It requires coordinated use of adiversity of knowledge about the reader, the knowl-edge to be delivered, and the World in which all reside.We now recognize the necessity of sophisticatedknowledge filtering as part of the process of producingquality text.KDS's Knowledge Filter (Figure 9) inputs the Say-list, including Advice, from the Problem Solver, andoutputs the Saylist with additional Advice, called"Don' t  Express" advice, on some of the protosenten-ces.
So some of the items have been marked for omis-sion from the final text.
(They are marked rather thandeleted so that they are available for use if needed astransitional material or to otherwise make the resultingtext coherent.)
The knowledge filter decides whichprotosentences to mark by consulting its internal mod-el of the reader to see whether the propositional con-tent is known or obvious.
The model of the reader, inthis implementation, is very simple: a collection ofpropositions believed to be known by him.
AlthoughKDS's reader model does not contain any inferencecapabilities about what is obvious, a more robust mod-el certainly would.
We recognize that the work of theKnowledge Filter is a serious intellectual task, and weexpect that such a filter will be an identifiable part offuture text creation programs.In our example the Knowledge Filter produces theDON'T -EXPRESS advice in the following element ofthe Saylist:((CONSTIT (WHENEVER (SOUNDS NIL ALARM-BELL)(HEARS YOU ALARM-BELL)(PROB)))...(ADVISORS INST-NARRATE NONEXP)(ADVICE (GOOD YOU)DON'T-EXPRESS))In this case, the involvement of the reader in(HEARS YOU ALARM-BELL)  arises from theAdvice-giving rule for Instructional-Narrate.
It indi-cates that it is good to express this.
The DON'T -EXPRESS arises from the Knowledge Filter, indicating!
I (SAYLIST(SAYLIST..~IKNOWLEDGEL~.
with addedwith - J FILTER I DON'T-EXPRESSADVICE) l / advice)iReaderModelFigure 9.
Knowledge Filter module input and output.that it is unnecessary to express this.
DON'T -EXPRESS prevails.Hill C l imber ModuleThe Hill Cl imber module (Figure 10) consists ofthree parts:1.
A somewhat unconventional hill-climbingalgorithm that repeatedly selects whichone of an available set of changes tomake on the Saylist.2.
A set of Aggregation rules (with an inter-preter) telling how the protosentencesmay legally be combined.
These corre-spond roughly to the clause-combiningrules of English, and the collection repre-sents something similar to the writer'scompetence at clause coordination.
EachAggregation rule consumes one or moreprotosentences and produces one proto-sentence.
Advice propagates onto theprotosentences produced.3.
A set of Preference rules (with an inter-preter) able to assign a numerical qualityscore to any protosentence.
The scorecomputation is sensitive to Advice.The algorithm is equivalent to the following:Scores are assigned to all of the primitive protosen-tences; then the Aggregation rules are applied to theSaylist in all possible ways to generate potential nextsteps up the hill.
The resultant protosentences arealso evaluated, and the Hill Cl imber algorithm thencompares the scores of units consumed and producedand calculates a net  gain or loss for each potentialapplication of an Aggregation rule.
The best one isexecuted, which means that the consumed units areremoved from the Saylist, and the new unit is added(in one of the positions vacated, which one beingspecified in the Aggregation rule).This process is applied repeatedly until improve-ment ceases.
The output of the Hill Cl imber is a Say-26 American Journal of Computat ional  Linguistics, Volume 7, Number 1, January-March 1981William C. Mann and James A. Moore Computer Generation of Multiparagraph English Text(SAYLIST)PrimitiveprotosentencesHILL CLIMBING I ALGORITHM/Aggregation Rule ApplierAGGREGATION RULES(The allowable clause-combining methods of English)-~ (SAYLIST)Primitive and compositeprotosentencesPreference Rule AppllerPREFERENCE RULES(A numerical score foreach protosentence)(ADVICE taken here)Figure 10.
Hill Climber module.list for which there are no remaining beneficial poten- 1.tial applications of Aggregation rules.The selection algorithm of the Hill Cl imber issomewhat unconventional in that it does not select theAggregation rule application with the largest increase 2.in collective score, which would be the usual practice.The hill of collective scores has many local maxima,which can be traced to the fact that one application ofan aggregation rule will preclude several others.
Be- 3.cause protosentences are consumed, the various appli-cations are in competition, and so a rule that producesa large gain may preclude even more gain.The Hill Climber selects the rule application to usebased on an equation that includes competitive terms.It computes the amount of gain surely precluded byeach application and makes its selection on the basisof maximum net gain, with the precluded gain sub-tracted.The use of hill climbing avoids the combinatorialexplosion involved in searching for the best of all pos- 6.sible ways to express the content.
In general only atiny fraction of the possibilities are actually examined.This Saylist improvement activity is the technicalheart of the text production process; it develops thefinal sentence boundaries and establishes the smooth-ness of the text.Figure 11 shows a few of the Aggregation rules.
(Each of them has been rewritten into an informalnotation suggesting its content.)
Aggregation rules areintended to be meaning-preserving in the reader'sCOMMON CAUSE.Whenever C then X.Whenever C then Y.CONJOIN MID-STATEWhenever Xthen  Y.Whenever Y then Z.DELETE MID-STATEWhenever Xthen  Y.Whenever Y then Z.4.
DELETE EXISTENTIALThere is a Y.<mention of Y>(Y is known unique)5.
IF-THEN-ELSEIf P then Q.ttttIf not P then R.TEST AND BRANCHWhen P then determine if X.
/If X then Q.If not X then R.Whenever C then Xand Y.Whenever Xthen  Yand then Z.Whenever Xthen  Z.<mention of Y>If P then Q otherwise R.When P then determine Xand decide Q or R.Figure 11.
Sample Aggregation rules.comprehension, but are not intended to preserveexplicitness.These are only a few of the Aggregation rules thathave been used in KDS; others have been developed inAmerican Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 27Will iam C. Mann and James A. Moore Computer Generation of Multiparagraph English Textthe course of working on this and other examples.Coverage of English is still very sparse.
In otherexamples, an aggregation rule has been used to pro-duce a multiple-sentence structure with intersententialdependencies.Figure 12 shows the Preference rules.
They werederived empirically, to correspond to those used by theauthor of some comparable human-produced text.be elaborate - -  that being able to advise that a term isgood or a term is bad is adequate.Rule 6 is somewhat of a puzzle.
Empirically, asentence produced by reapplication of an Aggregationrule was always definitely unacceptable, primarily be-cause it was awkward or confusing.
We do not under-stand technically why this should be the case, andsome say it should not be.
We do know that this rulecontributes ignificantly to overall quality.1.
Every protosentence g ts an initial value of-1000.2.
Every primitive protosentence embedded in acomposite protosentence d creases value by10.3.
If there is advice that a term is good, each oc-currence of that term increases value by 100.4.
Each time-sequentially inked protosentenceafter the first increases value by 100.5.
Certain constructions get bonuses of 200: theif-then-else construct and the when-X-determine-Y.6.
Any protosentence produced by multiple appli-cations of the same aggregation rule gets alarge negative value.Figure 12.
Preference rules.One of the surprising discoveries of this work, seenin all of the cases investigated, is that the task of textgeneration is dominated by the need for brevity: Howto avoid saying things is at least as important as howto say things.
Preference Rule 1 introduces a tenden-cy toward brevity, because most of the Aggregationrules consume two or three protosentences but pro-duce only one, yielding a large gain in score.
Sen-tences produced from aggregated protosentences aregenerally briefer than the corresponding sentences forthe protosentences consumed.
For example, applyingRule 1 to the pair:"When you permit 5 the alarm system, call theFire Department if possible.
When you per-mit the alarm system then evacuate.
"yields,"When you permit the alarm system, call theFire Department if possible, then evacuate.
"Rule 3 introduces the sensitivity to advice.
Weexpect that this sort of advice taking does not need to5 This way of using "permit" is unfamiliar to many people,but it is exactly the usage that we found in a manual of instructionfor computer operators on what they should do in case of fire.
Inthe course of attempting to produce comparable text we acceptedthe usage.Sentence Generator  ModuleThe Sentence Generator  (Figure 13) takes the finalordered set of protosentences produced by the HillClimber and produces the final text, one sentence at atime.
Each sentence is produced independently, usinga simple context- free grammar and semantic testingrules.
Because sentence generation has not been thefocus of our work, this module does not representmuch innovation, but merely establishes that the textformation work has been completed and does notdepend on further complex processing.
(Protosentencelist)SENTENCEGENERATORReferring-PhraseGeneratorFinal textFigure 13.
Sentence Generator module input and output.The single significant innovation in the SentenceGenerator is the Referring Phrase Generator,  the onlypart in which prior sentences affect the current sen-tence.
The Referring Phrase Generator keeps track ofwhat objects have been referred to, and how.
It pre-sumes that objects previously referred to are in thereader's attention and that after they have been identi-fied by the first reference, subsequent references needonly distinguish the object f rom others in attention.This process is equivalent o the one described by \[6\]developed for this research.
It knows how to intro-duce terms, refer to objects by incomplete descrip-tions, and introduce pronouns.
However,  none of ourexamples has exercised all of the features of Levin andGoldman's algorithm.Output  TextApplying all of this machinery in our example, weget the result shown in Figure 14.
Note the paragraphbreak, a product of a factoring rule (the first rule inInstructional-narrate) in the Problem Solver module.28 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981Will iam C. Mann and James A. Moore Computer Generation of Multiparagraph English TextWhenever there is a f i re,  the alarm system isstarted, which sounds a bell and starts a timer.Ninety seconds after the timer starts, unless thealarm system is cancelled, the system calls WellsFargo.
When Wells Fargo is called, they, inturn, call the Fire Department.When you hear the alarm bell or smell smoke,stop whatever you are doing, determine whether ornot there is a f ire,  and decide whether to permitthe alarm system or to cancel i t .
When youdetermine whether there is a f i re,  i f  there is,permit the alarm system, otherwise cancel i t .When you permit the alarm system, call the FireDepartment i f  possible, then evacuate.
When youcancel the alarm system, i f  i t  is more than 90seconds since the timer started, the system willhave called Wells Fargo already, otherwisecontinue what you were doing.Figure 14.
Final fire-alarm text from KDS.Conclus ions  and ProspectsThe development of KDS highlights several aspectsof the task of writing that strongly influence text qual-ity.
The overwhelming importance of brevity, seen inboth the Knowledge Filter and the Preference rules, isstriking.
Writing is seen here as a constructive activityrather than simply as interpretive.
That is, it is not somuch a mapping between knowledge representationsas it is the creation of new symbolic objects, notequivalent o older ones, but suitable for achievingparticular effects.
The image of writing as a kind ofgoal pursuit activity helps us to factor the task intoparts.
The task (and the program) is occupied withfinding a good way to say things, not with establishingfeasibility of saying them.The KDS development has also identified importantfeatures of the problem of designing a knowledge-delivery pi~ogram.
The defects of the Partitioning par-adigm are newly appreciated; the Fragment-and-Compose paradigm is much more manageable.
It iseasy to understand, and the creation of Aggregationrules is not difficult.
The separation of Aggregationand Preference actions seems essential to the task, orat least to making the task manageable.
As a kind ofcompetence/performance separation it is also of theo-retical interest.
Knowledge filtering, as one kind ofresponsiveness of the writer to the reader, is essentialto producing ood text.The importance of fragmenting is clear, and thekinds of demands placed on the Fragmenter have beenclarified, but effective methods of fragmenting arbi-trary knowledge sources are still not well understood.In the future, we expect to see the Fragment-and-Compose paradigm reapplied extensively.
We expectto see goal-pursuing processes applied to text organi-zation and style selection.
We expect distinct process-es for aggregating fragments and selecting combina-tions on a preference basis.
We also expect a welldeveloped model of the reader, including inferencecapabilities and methods for keeping the model up todate as the text progresses.
Finally, we expect a greatdeal of elaboration of the kinds of aggregation per-formed and of the kinds of considerations to whichpreference selection responds.References\[1\] Badler, N.I., "The Conceptual Description of Physical Activi-ties," In Proceedings of the 13th Annual Meeting of the Associa-tion for Computational Linguistics, AJCL Microfiche, 35, 1975.\[2\] Carbonell J.R., and A.M. Collins, "Natural Semantics in Artifi-cial Intelligence," In Proceedings of the Third International JointConference on Artificial Intelligence, 1973, 344-351.\[3\] Davey, Anthony, Discourse Production, Edinburgh UniversityPress, Edinburgh, 1979.\[4\] Swartout, William R., "Producing Explanations and Justifica-tions of Expert Consulting Programs," Technical Report TR-251,MIT Laboratory for Computer Science, January 1981.\[5\] Heidorn, George E., "Natural Language Inputs to a SimulationProgramming System," Technical Report NPS-55HD72101A,Naval Postgraduate School, 1972.\[6\] Levin, J.A., and N.M. Goldman, "Process Models of Referencein Context," Research Report 78-72, USC/Information SciencesInstitute, 1978.\[7\] McDonald, D.D., "Natural Language Production as a Process ofDecision-Making Under Constraints," PhD Thesis, MIT, Dept.of Electrical Engineering and Computer Science, 1980.\[8\] Meehan, James R., "TALE-SPIN, An Interactive Program thatWrites Stories."
In Proceedings of the Fifth International JointConference on Artificial Intelligence, 1977.\[9\] Nida, Eugene, Toward a Science of Translating, E.J.
Brill, Leid-en, 1964.\[10\] Schank, Roger C., and the Yale A.I.
Project, "SAM - -  A StoryUnderstander," Research Report 43, Yale University, Dept.
ofComputer Science, 1975.\[11\] Simmons, R., and J. Slocum, "Generating English Discoursefrom Semantic Networks," Comm.
ACM 15, 10 (October1972), 891-905.William C. Mann is a member of the research staffof Information Sciences Institute at the University ofSouthern California.
He received the Ph.D. degree incomputer science from Carnegie-Mellon University in1973.James A. Moore is a member of the research staff ofInformation Sciences Institute at the University ofSouthern California.
He received the Ph.D. degree incomputer science from Carnegie-Mellon University in1974.American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 29
