Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599?607,Beijing, August 2010Dependency-driven Anaphoricity Determination for CoreferenceResolutionFang Kong  Guodong Zhou  Longhua Qian  Qiaoming Zhu*JiangSu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and Technology Soochow University{kongfang, gdzhou, qianlonghua, qmzhu}@suda.edu.cn* Corresponding authorAbstractThis paper proposes a dependency-drivenscheme to dynamically determine the syn-tactic parse tree structure for tree ker-nel-based anaphoricity determination incoreference resolution.
Given a full syntacticparse tree, it keeps the nodes and the pathsrelated with current mention based on con-stituent dependencies from both syntacticand semantic perspectives, while removingthe noisy information, eventually leading toa dependency-driven dynamic syntacticparse tree (D-DSPT).
Evaluation on the ACE2003 corpus shows that the D-DSPT out-performs all previous parse tree structures onanaphoricity determination, and that apply-ing our anaphoricity determination modulein coreference resolution achieves the so farbest performance.1 IntroductionCoreference resolution aims to identify whichnoun phrases (NPs, or mentions) refer to thesame real-world entity in a text.
According toWebber (1979), coreference resolution can bedecomposed into two complementary sub-tasks:(1) anaphoricity determination, determiningwhether a given NP is anaphoric or not; and (2)anaphor resolution, linking together multiplementions of a given entity in the world.
Al-though machine learning approaches have per-formed reasonably well in coreference resolu-tion without explicit anaphoricity determina-tion (e.g.
Soon et al 2001; Ng and Cardie2002b; Yang et al 2003, 2008; Kong et al2009), knowledge of NP anaphoricity is ex-pected to much improve the performance of acoreference resolution system, since anon-anaphoric NP does not have an antecedentand therefore does not need to be resolved.Recently, anaphoricity determination hasbeen drawing more and more attention.
Onecommon approach involves the design of someheuristic rules to identify specific types ofnon-anaphoric NPs, such as pleonastic it (e.g.Paice and Husk 1987; Lappin and Leass 1994,Kennedy and Boguraev 1996; Denber 1998)and definite descriptions (e.g.
Vieira and Poe-sio 2000).
Alternatively, some studies focus onusing statistics to tackle this problem (e.g.,Bean and Riloff 1999; Bergsma et al 2008)and others apply machine learning approaches(e.g.
Evans 2001;Ng and Cardie 2002a,2004,2009; Yang et al 2005; Denis and Bal-bridge 2007; Luo 2007; Finkel and Manning2008; Zhou and Kong 2009).As a representative, Zhou and Kong (2009)directly employ a tree kernel-based method toautomatically mine the non-anaphoric informa-tion embedded in the syntactic parse tree.
Onemain advantage of the kernel-based methods isthat they are very effective at reducing theburden of feature engineering for structuredobjects.
Indeed, the kernel-based methods havebeen successfully applied to mine structuredinformation in various NLP applications likesyntactic parsing (Collins and Duffy, 2001;Moschitti, 2004), semantic relation extraction(Zelenko et al, 2003; Zhao and Grishman,2005; Zhou et al 2007; Qian et al, 2008), se-mantic role labeling (Moschitti, 2004); corefer-ence resolution (Yang et al, 2006; Zhou et al,2008).
One of the key problems for the ker-nel-based methods is how to effectively capturethe structured information according to the na-ture of the structured object in the specific task.This paper advances the state-of-the-art per-formance in anaphoricity determination by ef-599fectively capturing the structured syntactic in-formation via a tree kernel-based method.
Inparticular, a dependency-driven scheme isproposed to dynamically determine the syntac-tic parse tree structure for tree kernel-basedanaphoricity determination by exploiting con-stituent dependencies from both the syntacticand semantic perspectives to keep the neces-sary information in the parse tree as well asremove the noisy information.
Our motivationis to employ critical dependency information inconstructing a concise and effective syntacticparse tree structure, specifically targeted fortree kernel-based anaphoricity determination.The rest of this paper is organized as follows.Section 2 briefly describes the related work onboth anaphoricity determination and exploringsyntactic parse tree structures in related tasks.Section 3 presents our dependency-drivenscheme to determine the syntactic parse treestructure.
Section 4 reports the experimentalresults.
Finally, we conclude our work in Sec-tion 5.2 Related WorkThis section briefly overviews the related workon both anaphoricity determination and ex-ploring syntactic parse tree structures.2.1 Anaphoricity DeterminationPrevious work on anaphoricity determinationcan be broadly divided into three categories:heuristic rule-based (e.g.
Paice and Husk1987;Lappin and Leass 1994; Kennedy andBoguraev 1996; Denber 1998; Vieira and Poe-sio 2000; Cherry and Bergsma 2005), statis-tics-based (e.g.
Bean and Riloff 1999; Cherryand Bergsma 2005; Bergsma et al 2008) andlearning-based methods (e.g.
Evans 2001; Ngand Cardie 2002a; Ng 2004; Yang et al 2005;Denis and Balbridge 2007; Luo 2007; Finkeland Manning 2008; Zhou and Kong 2009; Ng2009).The heuristic rule-based methods focus ondesigning some heuristic rules to identify spe-cific types of non-anaphoric NPs.
Representa-tive work includes: Paice and Husk (1987),Lappin and Leass (1994) and Kennedy andBoguraev (1996).
For example, Kennedy andBoguraev (1996) looked for modal adjectives(e.g.
?necessary?)
or cognitive verbs (e.g.
?It isthought that??
in a set of patterned construc-tions) in identifying pleonastic it.Among the statistics-based methods, Beanand Riloff (1999) automatically identified ex-istential definite NPs which are non-anaphoric.The intuition behind is that many definite NPsare not anaphoric since their meanings can beunderstood from general world knowledge, e.g.
?the FBI?.
They found that existential NPs ac-count for 63% of all definite NPs and 76% ofthem could be identified by syntactic or lexicalmeans.
Cherry and Bergsma (2005) extendedthe work of Lappin and Leass (1994) forlarge-scale anaphoricity determination by addi-tionally detecting pleonastic it.
Bergsma et al(2008) proposed a distributional method in de-tecting non-anaphoric pronouns.
They first ex-tracted the surrounding context of the pronounand gathered the distribution of words that oc-curred within the context from a large corpus,and then identified the pronoun either ana-phoric or non-anaphoric based on the word dis-tribution.Among the learning-based methods, Evans(2001) automatically identified thenon-anaphoricity of pronoun it using variouskinds of lexical and syntactic features.
Ng andCardie (2002a) employed various do-main-independent features in identifying ana-phoric NPs.
They trained an anaphoricity clas-sifier to determine whether a NP was anaphoricor not, and employed an independently-trainedcoreference resolution system to only resolvethose mentions which were classified as ana-phoric.
Experiments showed that their methodimproved the performance of coreferenceresolution by 2.0 and 2.6 to 65.8 and 64.2 inF1-measure on the MUC-6 and MUC-7 cor-pora, respectively.
Ng (2004) examined therepresentation and optimization issues in com-puting and using anaphoricity information toimprove learning-based coreference resolution.On the basis, he presented a corpus-based ap-proach (Ng, 2009) for achieving global opti-mization by representing anaphoricity as a fea-ture in coreference resolution.
Experiments onthe ACE 2003 corpus showed that their methodimproved the overall performance by 2.8, 2.2and 4.5 to 54.5, 64.0 and 60.8 in F1-measureon the NWIRE, NPAPER and BNEWS do-mains, respectively.
However, he did not lookinto the contribution of anaphoricity determi-600nation on coreference resolution of differentNP types.
Yang et al (2005) made use ofnon-anaphors to create a special class of train-ing instances in the twin-candidate model(Yang et al 2003) and improved the perform-ance by 2.9 and 1.6 to 67.3 and 67.2 inF1-measure on the MUC-6 and MUC-7 cor-pora, respectively.
However, their experimentsshow that eliminating non-anaphors using ananaphoricity determination module in advanceharms the performance.
Denis and Balbridge(2007) employed an integer linear program-ming (ILP) formulation for coreference resolu-tion which modeled anaphoricity and corefer-ence as a joint task, such that each local modelinformed the other for the final assignments.Experiments on the ACE 2003 corpus showedthat this joint anaphoricity-coreference ILPformulation improved the F1-measure by3.7-5.3 on various domains.
However, theirexperiments assume true ACE mentions (i.e.
allthe ACE mentions are already known from theannotated corpus).
Therefore, the actual effectof this joint anaphoricity-coreference ILP for-mulation on fully automatic coreference reso-lution is still unclear.
Luo (2007) proposed atwin-model for coreference resolution: a linkcomponent, which models the coreferentialrelationship between an anaphor and a candi-date antecedent, and a creation component,which models the possibility that a NP was notcoreferential with any candidate antecedent.This method combined the probabilities re-turned by the creation component (an ana-phoricity model) with the link component (acoreference model) to score a coreference par-tition, such that a partition was penalizedwhenever an anaphoric mention was resolved.Finkel and Manning (2008) showed that transi-tivity constraints could be incorporated into anILP-based coreference resolution system andmuch improved the performance.
Zhou andKong (2009) employed a global learningmethod in determining the anaphoricity of NPsvia a label propagation algorithm to improvelearning-based coreference resolution.
Experi-ments on the ACE 2003 corpus demonstratedthat this method was very effective.
It couldimprove the F1-measure by 2.4, 3.1 and 4.1 onthe NWIRE, NPAPER and BNEWS domains,respectively.
Ng (2009) presented a novel ap-proach to the task of anaphoricity determina-tion based on graph minimum cuts and demon-strated the effectiveness in improving a learn-ing-based coreference resolution system.In summary, although anaphoricity determi-nation plays an important role in coreferenceresolution and achieves certain success in im-proving the overall performance of coreferenceresolution, its contribution is still far from ex-pectation.2.2 Syntactic Parse Tree StructuresFor a tree kernel-based method, one key prob-lem is how to represent and capture the struc-tured syntactic information.
During recentyears, various tree kernels, such as the convo-lution tree kernel (Collins and Duffy, 2001),the shallow parse tree kernel (Zelenko et al2003) and the dependency tree kernel (Culotaand Sorensen, 2004), have been proposed in theliterature.
Among these tree kernels, the con-volution tree kernel represents the state-of-theart and has been successfully applied byCollins and Duffy (2002) on syntactic parsing,Zhang et al (2006) on semantic relation extrac-tion and Yang et al (2006) on pronoun resolu-tion.Given a tree kernel, the key issue is how togenerate a syntactic parse tree structure for ef-fectively capturing the structured syntactic in-formation.
In the literature, various parse treestructures have been proposed and successfullyapplied in some NLP applications.
As a repre-sentative, Zhang et al (2006) investigated fiveparse tree structures for semantic relation ex-traction and found that the ShortestPath-enclosed Tree (SPT) achieves the bestperformance on the 7 relation types of the ACERDC 2004 corpus.
Yang et al (2006) con-structed a document-level syntactic parse treefor an entire text by attaching the parse trees ofall its sentences to a new-added upper node andexamined three possible parse tree structures(Min-Expansion, Simple-Expansion andFull-Expansion) that contain different sub-structures of the parse tree for pronoun resolu-tion.
Experiments showed that their methodachieved certain success on the ACE 2003corpus and the simple-expansion scheme per-forms best.
However, among the three exploredschemes, there exists no obvious overwhelmingone, which can well cover structured syntacticinformation.
One problem of Zhang et al (2006)601and Yang et al (2006) is that their parse treestructures are context-free and do not considerthe information outside the sub-trees.
Hence,their ability of exploring structured syntacticinformation is much limited.
Motivated byZhang et al (2006) and Yang et al (2006),Zhou et al (2007) extended the SPT to becomecontext-sensitive (CS-SPT) by dynamicallyincluding necessary predicate-linked path in-formation.
Zhou et al (2008) further proposeda dynamic-expansion scheme to automaticallydetermine a proper parse tree structure forpronoun resolution by taking predicate- andantecedent competitor-related information inconsideration.
Evaluation on the ACE 2003corpus showed that the dynamic-expansionscheme can well cover necessary structuredinformation in the parse tree for pronoun reso-lution.
One problem with the above parse treestructures is that they may still contain unnec-essary information and also miss some usefulcontext-sensitive information.
Qian et al (2008)dynamically determined the parse tree structurefor semantic relation extraction by exploitingconstituent dependencies to keep the necessaryinformation in the parse tree as well as removethe noisy information.
Evaluation on the ACERDC 2004 corpus showed that their dynamicsyntactic parse tree structure outperforms allprevious parse tree structures.
However, theirsolution has the limitation in that the depend-encies were found according to some manu-ally-written ad-hoc rules and thus may not beeasily applicable to new domains and applica-tions.This paper proposes a new scheme to dy-namically determine the syntactic parse treestructure for anaphoricity determination andsystematically studies the application of an ex-plicit anaphoricity determination module inimproving coreference resolution.3 Dependency-driven Dynamic Syn-tactic Parse TreeGiven a full syntactic parse tree and a NP inconsideration, one key issue is how to choose aproper syntactic parse tree structure to wellcover structured syntactic information in thetree kernel computation.
Generally, the more asyntactic parse tree structure includes, the morestructured syntactic information would beavailable, at the expense of more noisy (or un-necessary) information.It is well known that dependency informa-tion plays a key role in many NLP problems,such as syntactic parsing, semantic role label-ing as well as semantic relation extraction.
Mo-tivated by Qian et al (2008) and Zhou et al(2008), we propose a new scheme to dynami-cally determine the syntactic parse tree struc-ture for anaphoricity determination by exploit-ing constituent dependencies from both thesyntactic and semantic perspectives to distin-guish the necessary evidence from the unnec-essary information in the syntactic parse tree.That is, constituent dependencies are exploredfrom two aspects: syntactic dependencies andsemantic dependencies.1) Syntactic Dependencies: The Stanford de-pendency parser1 is employed as our syn-tactic dependency parser to automaticallyextract various syntactic (i.e.
grammatical)dependencies between individual words.
Inthis paper, only immediate syntactic de-pendencies with current mention are con-sidered.
The intuition behind is that the im-mediate syntactic dependencies carry themajor contextual information of currentmention.2) Semantic Dependencies: A state-of-the-artsemantic role labeling (SRL) toolkit (Li etal.
2009) is employed for extracting varioussemantic dependencies related with currentmention.
In this paper, semantic dependen-cies include all the predicates heading anynode in the root path from current mentionto the root node and their compatible argu-ments (except those overlapping with cur-rent mention).We name our parse tree structure as a depend-ency-driven dynamic syntactic parse tree(D-DSPT).
The intuition behind is that the de-pendency information related with currentmention in the same sentence plays a criticalrole in anaphoricity determination.
Given thesentence enclosing the mention under consid-eration, we can get the D-DSPT as follows:(Figure 1 illustrates an example of the D-DSPTgeneration given the sentence ?Mary said thewoman in the room bit her?
with ?woman?
ascurrent mention.
)1 http://nlp.stanford.edu/software/lex-parser.shtml602Figure 1:  An example of generating the dependency-driven dynamic syntactic parse tree1) Generating the full syntactic parse tree ofthe given sentence using a full syntactic parser.In this paper, the Charniak parser (Charniak2001) is employed and Figure 1 (a) shows theresulting full parse tree.2) Keeping only the root path from currentmention to the root node of the full parse tree.Figure 1(b) shows the root path correspondingto the current mention ?woman?.
In the fol-lowing steps, we attach the above two types ofdependency information to the root path.3) Extracting all the syntactic dependenciesin the sentence using a syntactic dependencyparser, and attaching all the nodes, which haveimmediate dependency relationship with cur-rent mention, and their corresponding paths tothe root path.
Figure 1(c) illustrates the syntac-tic dependences extracted from the sentence,where the ones in italic mean immediate de-pendencies with current mention.
Figure 1(d)shows the parse tree structure after consideringsyntactic dependencies.4) Attaching all the predicates heading anynode in the root path from current mention tothe root node and their corresponding paths tothe root path.
For the example sentence, thereare two predicates ?said?
and ?bit?, which headthe ?VP?
and ?S?
nodes in the root path re-spectively.
Therefore, these two predicates andtheir corresponding paths should be attached tothe root path as shown in Figure 1(e).
Note thatthe predicate ?bit?
and its corresponding pathhas already been attached in Stop (3).
As a re-sult, the predicate-related information can beattached.
According to Zhou and Kong (2009),such information is important to definite NPresolution.5) Extracting the semantic dependencies re-lated with those attached predicates using a(shallow) semantic parser, and attaching all thecompatible arguments (except those overlap-ping with current mention) and their corre-sponding paths to the root path.
For example,as shown in Figure 1(e), since the arguments?Mary?
and ?her?
are compatible with currentmention ?woman?, these two nodes and theircorresponding paths are attached while the ar-gument ?room?
is not since its gender does notagree with current mention.In this paper, the similarity between twoparse trees is measured using a convolution treekernel, which counts the number of commonsub-tree as the syntactic structure similaritybetween two parse trees.
For details, pleaserefer to Collins and Duffy (2001).6034 Experimentation and DiscussionThis section evaluates the performance of de-pendency-driven anaphoricity determinationand its application in coreference resolution onthe ACE 2003 corpus.4.1 Experimental SettingThe ACE 2003 corpus contains three domains:newswire (NWIRE), newspaper (NPAPER),and broadcast news (BNEWS).
For each do-main, there exist two data sets, training anddevtest, which are used for training and testing.For preparation, all the documents in thecorpus are preprocessed automatically using apipeline of NLP components, including to-kenization and sentence segmentation, namedentity recognition, part-of-speech tagging andnoun phrase chunking.
Among them, namedentity recognition, part-of-speech tagging andnoun phrase chunking apply the samestate-of-the-art HMM-based engine with er-ror-driven learning capability (Zhou and Su,2000 & 2002).
Our statistics finds that 62.0%,58.5% and 61.4% of entity mentions are pre-served after preprocessing on the NWIRE,NPAPER and BNEWS domains of the ACE2003 training data respectively while only89.5%, 89.2% and 94% of entity mentions arepreserved after preprocessing on  the NWIRE,NPAPER and BNEWS domains of the ACE2003 devtest data.
This indicates the difficultyof coreference resolution.
In addition, the cor-pus is parsed using the Charniak parser forsyntactic parsing and the Stanford dependencyparser for syntactic dependencies while corre-sponding semantic dependencies are extractedusing a state-of-the-art semantic role labelingtoolkit (Li et al 2009).
Finally, we use theSVM-light2 toolkit with the tree kernel func-tion as the classifier.
For comparison purpose,the training parameters C (SVM) and ?
(treekernel) are set to 2.4 and 0.4 respectively, asdone in Zhou and Kong (2009).For anaphoricity determination, we reportthe performance in Acc+ and Acc-, whichmeasure the accuracies of identifying anaphoricNPs and non-anaphoric NPs, respectively.
Ob-viously, higher Acc+ means that more ana-phoric NPs would be identified correctly, while2 http://svmlight.joachims.org/higher Acc- means that more non-anaphoricNPs would be filtered out.
For coreferenceresolution, we report the performance in termsof recall, precision, and F1-measure using thecommonly-used model theoretic MUC scoringprogram (Vilain et al 1995).
To see whether animprovement is significant, we also conductsignificance testing using paired t-test.
In thispaper, ?
***?, ?**?
and ?*?
denote p-values of animprovement smaller than 0.01, in-between(0.01, 0,05] and bigger than 0.05, which meansignificantly better, moderately better andslightly better, respectively.4.2 Experimental ResultsPerformance of anaphoricity determinationTable 1 presents the performance of anaphoric-ity determination using the convolution treekernel on D-DSPT.
It shows that our methodachieves the accuracies of 83.27/77.13,86.77/80.25 and 90.02/64.24 on identifyinganaphoric/non-anaphoric NPs in the NWIRE,NPAPER and BNEWS domains, respectively.This suggests that our approach can effectivelyfilter out about 75% of non-anaphoric NPs andkeep about 85% of anaphoric NPs.
In com-parison, in the three domains Zhou and Kong(2009) achieve the accuracies of 76.5/82.3,78.9/81.6 and 74.3/83.2, respectively, using thetree kernel on a dynamically-extended tree(DET).
This suggests that their method can fil-ter out about 82% of non-anaphoric NPs andonly keep about 76% of anaphoric NPs.
Incomparison, their method outperforms ourmethod on filtering out more non-anaphoricNPs while our method outperforms theirmethod on keeping more anaphoric NPs incoreference resolution.
While a coreferenceresolution system can detect somenon-anaphoric NPs (when failing to find theantecedent candidate), filtering out anaphoricNPs in anaphoricity determination would defi-nitely cause errors and it is almost impossibleto recover.
Therefore, it is normally more im-portant to keeping more anaphoric NPs thanfiltering out more non-anaphoric NPs.
Table 1further presents the performance of anaphoric-ity determination on different NP types.
Itshows that our method performs best at keep-ing pronominal NPs and filtering out properNPs.604NWIRE NPAPER BNEWS NP TypeAcc+ Acc- Acc+ Acc- Acc+ Acc-Pronoun 95.07 50.36 96.40 56.44 98.26 54.03Proper NP 84.61 83.17 83.78 79.62 87.61 71.77Definite NP 87.17 46.74 82.24 49.18 86.87 53.65Indefinite NP 86.01 47.52 80.63 48.45 89.71 47.32Over all 83.27 77.13 86.77 80.25 90.02 64.24Table 1: Performance of anaphoricity determination using the D-DSPTNWIRE NPAPER BNEWS Performance ChangeAcc+ Acc- Acc+ Acc- Acc+ Acc-D-DSPT 83.27 77.13 86.77 80.25 90.02 64.24-Syntactic Dependencies 78.67 72.56 80.14 73.74 87.05 60.20-Semantic Dependencies 81.67 76.74 83.47 77.93 89.58 60.67Table 2: Contribution of including syntactic and semantic dependenciesin D-DSPT on anaphoricity determinationNWIRE NPAPER BNEWS SystemR% P% F R% P% F R% P% FPronoun 70.8 57.9 63.7 76.5 63.5 69.4 70.0 60.3 64.8Proper NP 80.3 80.1 80.2 81.8 83.6 82.7 76.3 76.8 76.6Definite NP 35.9 43.4 39.2 43.1 48.5 45.6 47.9 51.9 49.8Indefinite NP 40.3 26.3 31.8 39.7 22.9 29.0 23.6 10.7 14.7Without ana-phoricity de-termination(Baseline)Over all 55.0 63.8 59.1 62.1 65.0 63.5 53.2 60.5 56.6Pronoun 65.9 70.2 68.0 72.6 78.7 75.5 67.7 75.8 71.5Proper NP 80.3 81.0 80.6 81.2 85.1 83.1 76.3 84.4 80.1Definite NP 32.3  63.1 42.7 38.4 61.7 47.3 42.5 66.4 51.8Indefinite NP 36.4 55.3 43.9 34.7 50.7 41.2 20.3 45.4 28.1With D-DSPT-based ana-phoricity de-terminationOver all 52.4 79.6 63.2 58.1 80.3 67.4 50.1 79.8 61.6Pronoun 68.6 71.5 70.1 75.2 80.4 77.7 69.1 77.8 73.5Proper NP 81.7 89.3 85.3 82.6 90.1 86.2 78.6 88.7 83.3Definite NP 41.8 85.9 56.2 44.9 85.2 58.8 45.2 87.9 59.7Indefinite NP 40.3 67.6 50.5 41.2 65.1 50.5 40.9 50.1 45.1With goldenanaphoricitydeterminationOver all 54.6 81.7 65.5 60.4 82.1 69.6 51.9 82.1 63.6Table 3: Performance of anaphoricity determination on coreference resolutionNWIRE NPAPER BNEWS SystemR% P% F R% P% F R% P% FWithout anaphoricity determina-tion (Baseline) 53.1 67.4 59.4 57.7 67.0 62.1 48.0 65.9 55.5Zhou andKong (2009) With Dynamically ExtendedTree-based anaphoricity determi-nation51.6 77.2 61.8 55.2 78.6 65.2 47.5 80.3 59.6Without anaphoricity determina-tion (Baseline)59.1 58.
58.6 60.8 62.6 61.7 57.7 52.6 55.0Ng (2009)With Graph Minimum Cut-basedanaphoricity determination54.1 69.0 60.6 57.9 71.2 63.9 53.1 67.5 59.4Table 4: Performance comparison with other systemsTable 2 further presents the contribution ofincluding syntactic and semantic dependenciesin the D-DSPT on anaphoricity determinationby excluding one or both of them.
It shows thatboth syntactic dependencies and semantic de-pendencies contribute significantly (***).Performance of coreference resolutionWe have evaluated the effect of ourD-DSPT-based anaphoricity determinationmodule on coreference resolution by includingit as a preprocessing step to a baseline corefer-ence resolution system without explicit ana-phoricity determination, by filtering our thosenon-anaphoric NPs according to the anaphoric-ity determination module.
Here, the baselinesystem employs the same set of features, asadopted in the single-candidate model of Yanget al (2003) and uses a SVM-based classifierwith the feature-based RBF kernel.
Table 3presents the detailed performance of thecoreference resolution system without ana-605phoricity determination, with D-DSPT-basedanaphoricity determination and.
with goldenanaphoricity determination.
Table 3 shows that:1) There is a performance gap of 6.4, 6.1 and7.0 in F1-measure on the NWIRE, NPAPERand BNEWS domain, respectively, between thecoreference resolution system with goldenanaphoricity determination and the baselinesystem without anaphoricity determination.This suggests the usefulness of proper ana-phoricity determination in coreference resolu-tion.
This also agrees with Stoyanov et al(2009) which measured the impact of goldenanaphoricity determination on coreferenceresolution using only the annotated anaphors inboth training and testing.2) Compared to the baseline system withoutanaphoricity determination, the D-DSPT-basedanaphoricity determination module improvesthe performance by 4.1(***), 3.9(***) and5.0(***) to 63.2, 67.4 and 61.6 in F1-measureon the NWIRE, NPAPER and BNEWS do-mains, respectively, due to a large gain in pre-cision and a much smaller drop in recall.
Inaddition, D-DSPT-based anaphoricity determi-nation can not only much improve the per-formance of coreference resolution on pro-nominal NPs (***) but also on definiteNPs(***) and indefinite NPs(***) while theimprovement on proper NPs can be ignoreddue to the fact that proper NPs can be well ad-dressed by the simple abbreviation feature inthe baseline system.3) D-DSPT-based anaphoricity determinationstill lags (2.3, 2.2 and 2.0 on the NWIRE,NPAPER and BNEWS domains, respectively)behind golden anaphoricity determination inimproving the overall performance of corefer-ence resolution.
This suggests that there existssome room in the performance improvementfor anaphoricity determination.Performance comparison with other systemsTable 4 compares the performance of our sys-tem with other systems.
Here, Zhou and Kong(2009) use the same set of features with ours inthe baseline system and a dynami-cally-extended tree structure in anaphoricitydetermination.
Ng (2009) uses 33 features asdescribed in Ng (2007) and a graph minimumcut algorithm in anaphoricity determination.
Itshows that the overall performance of ourbaseline system is almost as good as that ofZhou and Kong (2009) and a bit better thanNg?s (2009).For overall performance, our coreferenceresolution system with D-DSPT-based ana-phoricity determination much outperformsZhou and Kong (2009) in F1-measure by 1.4,2.2 and 2.0 on the NWIRE, NPAPER andBNEWS domains, respectively, due to the bet-ter inclusion of dependency information.
De-tailed evaluation shows that such improvementcomes from coreference resolution on bothpronominal and definite NPs (Please refer toTable 6 in Zhou and Kong, 2009).
Comparedwith Zhou and Kong (2009) and Ng (2009), ourapproach achieves the best F1-measure so farfor each dataset.5 Conclusion and Further WorkThis paper systematically studies a depend-ency-driven dynamic syntactic parse tree(DDST) for anaphoricity determination and theapplication of an explicit anaphoricity deter-mination module in improving learning-basedcoreference resolution.
Evaluation on the ACE2003 corpus indicates that D-DSPT-basedanaphoricity determination much improves theperformance of coreference resolution.To our best knowledge, this paper is the firstresearch which directly explores constituentdependencies in tree kernel-based anaphorictydetermination from both syntactic and semanticperspectives.For further work, we will explore morestructured syntactic information in coreferenceresolution.
In addition, we will study the inter-action between anaphoricity determination andcoreference resolution and better integrateanaphoricity determination with coreferenceresolution.AcknowledgmentsThis research was supported by Projects60873150, 60970056, and 90920004 under theNational Natural Science Foundation of China,Project 200802850006 and 20093201110006under the Specialized Research Fund for theDoctoral Program of Higher Education ofChina.606ReferencesD.
Bean and E. Riloff 1999.
Corpus-based Identifi-cation of Non-Anaphoric Noun Phrases.
ACL?1999S.
Bergsma, D. Lin and R. Goebel 2008.
Distribu-tional Identification of Non-referential Pronouns.ACL?2008C.
Cherry and S. Bergsma.
2005.
An expectationmaximization approach to pronoun resolution.CoNLL?2005M.
Collins and N. Duffy.
2001.
Covolution kernelsfor natural language.
NIPS?2001M.
Denber 1998.
Automatic Resolution of Anapho-ria in English.
Technical Report, Eastman Ko-dakCo.P.
Denis and J. Baldridge.
2007.
Global, joint de-termination of anaphoricity and coreferenceresolution using integer programming.NAACL/HLT?2007R.
Evans 2001.
Applying machine learning towardan automatic classification of it.
Literary andLinguistic Computing, 16(1):45-57F.
Kong, G.D. Zhou and Q.M.
Zhu.
2009 Employ-ing the Centering Theory in Pronoun Resolutionfrom the Semantic Perspective.
EMNLP?2009F.
Kong, Y.C.
Li, G.D. Zhou and Q.M.
Zhu.
2009.Exploring Syntactic Features for Pronoun Reso-lution Using Context-Sensitive Convolution TreeKernel.
IALP?2009S.
Lappin and J. L. Herbert.
1994.
An algorithm forpronominal anaphora resolution.
ComputationalLinguistics, 20(4)J.H.
Li.
G.D. Zhou, H. Zhao, Q.M.
Zhu and P.D.Qian.
Improving nominal SRL in Chinese lan-guage with verbal SRL information and auto-matic predicate recognition.
EMNLP '2009X.
Luo.
2007.
Coreference or not: A twin model forcoreference resolution.
NAACL-HLT?2007V.
Ng and C. Cardie 2002.
Identify Anaphoric andNon-Anaphoric Noun Phrases to ImproveCoreference Resolution.
COLING?2002V.
Ng and C. Cardie 2002.
Improving machinelearning approaches to coreference resolution.ACL?2002V.
Ng 2004.
Learning Noun Phrase Anaphoricity toImprove Coreference Resolution: Issues in Rep-resentation and Optimization.
ACL?
2004V.
Ng 2009.
Graph-cut based anaphoricity determi-nation for coreference resolution.
NAACL?2009L.H.
Qian, G.D. Zhou, F. Kong, Q.M.
Zhu and P.D.Qian.
2008.
Exploiting constituent dependenciesfor  tree kernel-based semantic relation extrac-tion.
COLING?2008W.
M. Soon, H. T. Ng and D. Lim  2001.
A ma-chine learning approach to coreference resolutionof noun phrase.
Computational Linguistics,27(4):521-544.V.
Stoyanov, N. Gilbert, C. Cardie and E. Riloff.2009.
Conundrums in Noun Phrase CoreferenceResolution: Making Sense of the State-of-the Art.ACL?2009B.
L. Webber.
1979.
A Formal Approach to Dis-course Anaphora.
Garland Publishing, Inc.X.F.
Yang, G.D. Zhou, J. Su and C.L.
Chew.
2003.Coreference Resolution Using CompetitionLearning Approach.
ACL?2003X.F.
Yang, J. Su and C.L.
Chew.
2005.
A TwinCandidate Model of Coreference Resolution withNon-Anaphor Identification Capability.IJCNLP?2005X.F.
Yang, J. Su and C.L.
Chew.
2006.
Ker-nel-based pronoun resolution with structuredsyntactic knowledge.
COLING-ACL?2006X.F.
Yang, J. Su and C.L.
Tan 2008.
ATwin-Candidate Model for Learning-BasedAnaphora Resolution.
Computational Linguistics34(3):327-356M.
Zhang, J. Zhang, J. Su and G.D. Zhou.
2006.
Acomposite kernel to extract relations between en-tities with both flat and structured features.COLING/ACL?2006S.
Zhao and R. Grishman.
2005.
Extracting relationswith integered information using kernel methods.ACL?2005D.
Zelenko, A. Chinatsu and R. Anthony.
2003.Kernel methods for relation extraction.
MachineLearning Researching 3(2003):1083-1106G.D.
Zhou, F. Kong and Q.M.
Zhu.
2008.
Con-text-sensitive convolution tree kernel for pronounresolution.
IJCNLP?2008G.D.
Zhou and F. Kong.
2009.
Global Learning ofNoun Phrase Anaphoricity in Coreference Reso-lution via Label Propagetion.
EMNLP?2009G.D.
Zhou and J. Su.
2002.
Named Entity recogni-tion using a HMM-based chunk tagger.ACL?2002G.D.
Zhou, M. Zhang, D.H. Ji and Q.M.
Zhu.
2007.Tree kernel-based relation extraction with con-text-sensitive structured parse tree information.EMNLP/CoNLL?2007607
