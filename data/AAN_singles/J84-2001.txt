A Phrase-Structured Grammatical Frameworkfor Transportable Natural Language Processing IBruce W.  BallardAT & T Bell Laboratories600 Mountain AvenueMurray Hill, N.J. 07974Nancy L. T inkhamDepartment of Computer ScienceDuke UniversityDurham, N.C. 27706We present methods of dealing with the syntactic problems that arise in the construction of naturallanguage processors that seek to allow users, as opposed to computational linguists, to customize aninterface to operate with a new domain of data.
In particular, we describe a grammatical formalism,based on augmented phrase-structure rules, which allows a parser to perform many importantdomain-specific disambiguations by reference to a pre-defined grammar and a collection of auxiliaryfiles produced during an initial knowledge acquisition session with the user.
We illustrate the workingsof this formalism with examples from the grammar developed for our Layered Domain Class (LDC)system, though similarly motivated systems ought also to benefit from our formalisms.
In addition toshowing the theoretical advantage of providing many of the fine-tuning capabilities of so-called seman-tic grammars within the context of a domain-independent grammar, we demonstrate several practicalbenefits to our approach.
The results of three experiments with our grammar and parser are alsogiven.1.
IntroductionAs a result of advances in natural language processing,programs that provide practical English-language capabil-ities have begun to rival more conventional means ofcomputer interactions for certain purposes, including data-base retrieval, online help facilities, and limited forms ofoffice assistance.
Although several prototype systems haveprovided customization facilities that allow users to specifysynonyms, syntactic paraphrases, and the like, traditionalapproaches have resulted in systems wedded to a singledomain of data.
That is, users are unable to access noveltypes of data without acquiring ~ new or modified process-or specifically tailored to the new domain by the systemdesigner(s).
Not surprisingly, an important trend innatural anguage system design is in allowing users them-selves to adapt an existing processor for a new domain.Accordingly, prototype systems that permit user customi-zations or rapid customizations by a designer haveincluded REL, POL and ASK (Thompson and Thompson1975, 1981, 1983), CONSUL (Mark 1981; Wilczynski1981), IRUS (Bates and Bobrow 1983), KLAUS (Haas andHendrix 1980), TEAM (Hendrix and Lewis 1981; Grosz1983), a system developed at Bell Labs (Ginsparg 1983),and our own LDC system (Ballard 1982, 1984; Ballard and1 This research was supported in part by the National Science Founda-tion under Grant Numbers MCS-81-16607 and IST-83-01994 and inpart by the Air Force Office of Scientific Research under Grant Number81-0221.Copyright 1984 by t~'z Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted provided that thecopies are not madc for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
To copy otherwise, orto republish, requires a fee and/or spccific permission.0362-613X/84/020081-16503.00Computational Linguistics Volume 10, Number 2, April-June 1984 81Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPLusth 1983, 1984; Ballard, Lusth and Tinkham 1984a,1984b).
Since the successful construction of a transport-able system requires ound methods of representing what isto be learned, the design of formalisms to be used in trans-portable natural language processors relates to the scien-tific, as well as the engineering, aspects of computationallinguistics.In this paper we present methods of dealing with thesyntactic problems that have arisen in the construction ofour LDC system.
In particular, we shall describe a gram-matical formalism, based on augmented phrase-structurerules, which allows a parser to make domain-specific deci-sions by referring to a dictionary and other auxiliary filesproduced during an initial learning session with the user.We illustrate the workings of our grammatical formalismwith examples from the existing LDC grammar, but wenote that similarly motivated systems ought also to benefitfrom our formalisms.
We will also include the results ofsome experiments with our existing grammar as applied toseveral domains.In addition to showing the theoretical advantage ofbeing able to provide many of the fine-tuning capabilitiesof so-called semantic grammars within the context of adomain-independent grammar, we demonstrate severalpractical benefits to our approach.
For example, theconciseness of our formalism allows shorter grammarsthan many previous formalisms would allow, at least forthe intended class of retrieval applications.
This offers notonly added perspicuity but other benefits as well.
Forinstance, we have been able to write simple (almost rivial)LISP routines that pre-process a grammar to construct hefiles used by the parser to increase efficiency and toperform valuable disambiguations.2.
Overv iew of  Syntact i c  Process ingThe primary purpose of this paper is to introduce a gram-matical formalism that we have developed for use in speci-fying grammars for a transportable natural languageprocessor.
As suggested by the term "framework" in thetitle of the paper, however, we will touch upon certainrelated concepts in order to give a complete account of thelanguage constructs that can be dealt with by our formal-ism.
In total, then, we shall discuss1.
a phrase-structured grammatical formalism;2. a required dictionary format and associated compatibil-ity file; and3.
an implied format for parse structures.We begin with a brief indication of the ways in whichthese topics tie together in our existing and in otherconceivable systems.2.1 The  phrase-s t ructure  grammarThe format we have developed to represent our grammarsis intended to capture the spirit of phrase-structure specifi-cations such as the following, which specifies imple nounphrases.the (Ord / Super) (Num) (Super) Adj* Noun=Head PP*Parentheses denote optionality, * denotes the Kleene-star,and / denotes alternation.
To distinguish between termi-nal symbols, parts-of-speech, and multiple-word grammat-ical categories, we have used the convention of no caps,initial cap, and all caps, respectively.In deciding on a precise, internal representation forgrammars, we decided to adopt a LISP-like prefix nota-tion.
Thus, the actual specification of the expressionshown above, assuming it intends to capture the structuresof descriptive noun phrases (say, "NPdesrip"), is asfollows.
Note that we have included a context-sensitiveaugmentation, of a form to be discussed later, for thesecond Super.
(setq NPdescrip '(Seq(Quote the)(Opt Alt (Get Ord)(Get Super))(Opt Get Num)(Opt Get Super = (need not part Super))(* Get Adj)(Get Noun Head)(* Call PP)))Each of the seven command types illustrated here isdescribed in detail in Section 3, together with examples oftheir idiomatic usage.
The reader may notice that ourgrammatical formalism resembles both transition networkand phrase-structure grammars (Woods 1970, 1980;Heidorn 1975), and our later speaking of grammar ulesas "commands" further reveals an affinity with ATNs.For the reasons given in Section 5.2, however, we prefer toview the grammars as collections of augmented phrase-structure rules, while certain aspects of the current top-down parser act very much like an ATN parser.
Thereader may also notice that the current utilization of theformalism has resulted in parse structures having theflavor of case grammars, especially the ways in whichcomplex relative clauses and handled.
On the other hand,the provision for associating a feature list with each phrasesomewhat resembles the systemic structures of Winograd1972.2.2 The  d ic t ionary  and compat ib i l i ty  f i lesOur grammar ules assume that an input to be parsed willbe presented as a sequence of sets of token candidates,where each token candidate corresponds roughly to a wordor inflection of a word found in the system dictionary.Each dictionary listing for a word is made up of one ormore meanings, where each meaning comprises(a) the word itself;(b) its part of speech;(c) the associated root word; and82 Computational Linguistics Volume 10, Number 2, April-June 1984Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLP(d)more possible values.As an example, the entry(offices Subtype office1 2 3says that1.zero or more associated features, each with one or(nt room) (sp plur))4 5the word "offices" has been found or is being proposedfor the current oken candidate;2. the word refers to some of the domain objects of someobject type;3. the root word is "office";4. the objects being referred to are of type "room"; and5.
the word is a plural noun.In addition to the "features" found in the dictionary,which provide for simple context dependencies, a compat-ibility file is assumed to be available which contains infor-mation on acceptable attachments for such units asprepositional phrases and relative clauses.
An example ofhow this information can be used, together with a simpleexample of a possible set of prepositional triples, is foundin Section 3.3.3.2.3 Parse structuresOur grammar assumes that, during parsing, each non-a-tomic syntactic category will have associated with it a"parse structure" consisting of(a) the name of the structure;(b) a list of features giving possible values of variousparameters (as illustrated shortly) associated withthe phrase; and(c) a list of labeled items, namely words and pointers tonested phrases.For example, the parse structure(NP (feats (nt person) (sp plur))(Adj.
lousy)(Head .
advisor))might correspond to the noun phrase "lousy advisors",where the features indicate that the phrase refers todomain objects of nountype (nt) person and is plural(plur), and the items indicate that the head noun is"advisor" and the adjective "lousy" is present.
For thesake of completeness, and to help prepare the reader forthe discussion that follows, Figure 1 gives both completeand "abbreviated" parse structures (i.e.
a skeleton struc-ture without nountype and compatibility information) forthe sentence"How many graduate students were failed by theinstructor that John took AI from?
"As shown in Section 6.2.2, our feature lists are reminiscentof the systemic structures given in Winograd (1972).
Ineffect, they constitute a repository of information aboutwords contained within a phrase (possibly nested within it)and allow information to be passed both up and down aparse tree, thus enabling valuable context-sensitivities,both syntactic and semantic.3.
The Grammatical FormalismOur grammatical formalism is built around seven types ofsyntax rules which we will often refer to as "command"types.
The first three of these ("basic" commands) areused to specify words, parts of speech, and syntactic ate-gories, while the remaining four ("control" commands)provide facilities lor optionahty, possible repetition, alter-nation, and sequence.
In addition to the primary tunctionof each of the commands, through which the grammarwriter (i.e.
system designer) can specify any context-freegrammar, each of the basic commands may be augmentedwith information that enables the grammar writer to spec-ify certain context-sensitive constraints that (a) enablesimilar grammatical constructs to be collapsed into whatwould otherwise be an over-generating unit, and (b) allowthe parser to perform useful disambiguations.
The latterfacility can be especially valuable in a transportable envi-ronment, where the system designer is unable to predictmany of the syntactic, word sense, and other forms ofambiguity that will arise.We now discuss each command type, after which wedescribe ach of the augmentations allowed.3.1 Basic commandsThe three basic grammar commands areQuote to specify a given word or set of words;Get to specify a part of speech; andCall to specify a syntactic ategory.All grammar "commands" are processed by the parser (aLISP program) and should not be confused with operationsin LISP or other programming languages.
We will nowbriefly describe these commands.
As described shortly,these commands may contain augmentations to assureproper agreement among syntactic omponents.The Quote command instructs the parser to find one ofa list of words in the next token slot of the input (i.e.
as thenext token).
For example,(Quote (a an))says to pick up the next word if it is "a" or "an", otherwisefail.
If the list of words contains just one word, the super-fluous parentheses may be dropped, for instance(Quote the) = (Quote (the))The default action of Quote is not to add to the parsestructure being built up, but an optional abel is providedfor.
Thus, if the article "an" is seen by the command(Quote (a an the) Art)then the feature (Art .
an) will be added to the parsestructure.
Our current grammar uses Quote sparingly,partly since few words are known in advance in our trans-portable nvironment.
However, having a Quote facility isComputational Linguistics Volume 10, Number 2, April-June 1984 83Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLP(NP (feats (nt student) (head noun) (sp plural) (func count))(RelO (feats (hi (Subj Verb Obj Part Prep Arg)(instructor fail student nil nil nil))(sp plural))(Subj (feats (nt instructor) (sp sing) (head noun) (Art def))(RelA (feats (nl (Subj Verb Obj Part Prep Arg)(student ake course nil from instructor))(sp sing))(Prep.
from)(Obj (feats (nt course) (sp sing) (head nounval))(Nounval .
AI))(Verb.
take)(Subj (feats (nt student) (sp sing) (head nounval))(Nounval.
John)))(Head.
instructor))(Verb.
fail))(Head .
student)(Nounmod .
graduate))(a) Complete Parse Structure(NP (func count)(RelO (Subj (RelA (Prep .
from)(Obj (Nounval .
AI))(Verb.
take)(Subj (Nounval.
John)))(Head .
instructor))(Verb.
fail))(Head .
student)(Nounmod .
graduate))(b) Abbreviated Parse StructureFigure I.
Complete and Abbreviated Parse Structures for the Sentence"How many graduate students were failed by the instructor that John took A l  from.~'useful in allowing the grammar writer to capture various"noise words" without having to define artificial grammat-ical category names or resort to a proliferation of features.Thus we feel quite comfortable in writing, at the appropri-ate place(s) in a grammar, the command(Quote (whether if))The Get command instructs the parser to find a wordhaving one of a list of parts of speech.
As with Quote, anabbreviation is permitted if only one part of speech is to beallowed (which is most often the case).
Some examplesare:(Get (Ord Super))(Get Noun)When a Get command is processed, the word it picks up isincorporated into the current parse structure and labeledappropriately.
The default label is the part-of-speechcategory but an optional second argument to Get may beused to specify any other label name.
For instance,(Get Noun Head)says to pick up a noun and label it Head.
This would beuseful if several nouns occur within a given phrase andneed to be distinguished.
If it is desired to recognize agiven part of speech without adding to the parse structure,the label "nil" may be given, thus(Get Art nil)would recognize an article without affecting the parsebeing built up.
Finally, if one of several parts of speech isto be allowed, the dummy label "=" may be used to assurethe default action.
However, this is necessary only in situ-ations where augmentations (as discussed in Section 3.3)are present.
For example,(Get (Vpres Vpast) = (vtype intrans))would cause the word being recognized to be labeledaccording to its dictionary specification.The Call command instructs the parser to process anembedded constituent, such as a noun phrase, preposi-tional phrase, relative clause, and so forth, and so our Callis analogous to the "push" operation of conventional ATNs(Woods 1970).
As with the Get command, whatever is84 Computational Linguistics Volume 10, Number 2, April-June 1984Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPfound will be labeled as specified or by default.
Thus, thecommands(Call NP Arg)(Call Relc)call for a noun phrase (NP) to be labeled Arg and a rela-tive clause (Relc) to be labeled Relc.Normally, each Call-ed routine, and also the top-levelconstituent S, will have a separate parse structure associ-ated with it.
Thus, when a constituent phrase has beenparsed using a Call command, it will be pointed to ratherthan having its components physically included in theparent phrase.
In the LISP implementation, the associatedstructure is simply buried one additional level andassigned a feature list (to be discussed shortly) of its own.For example, if in recognizing the phrase "big houses" theadjective "big" is parsed directly by the grammar outinefor noun phrases, it would be placed into the parse struc-ture as indicated by(NP ((sp plur) (nt building))(Head.
house)(Adj .
big))where the first set of nested parentheses give the featurelist of the noun phrase as discussed in Sections 3.3. and6.2.2.
If on the other hand our grammar identified somepotentially larger unit (say, adjective phrase) as a separatesyntactic onstruct, and the word "big" was recognized asa constituent of it, we would have(NP (feats (sp plur) (nt ...))(Head .
house)(AdjPh (feats (nt ...))(Adj .
big)))where the precise "nt" value would be determined by theaugmentations present in the Call command.In some instances, however, either for the sake ofperspicuity or to avoid redundancy, it is useful to isolateand name a sequence of commands as a "macro" forwhich recognized items will be incorporated irectly intothe parent structure, i.e.
the parse structure current whenthe Call command was encountered.
This is handled bygiving a label of nil when the macro interpretation isdesired.
For example, our noun phrase grammar includesthe command(Call Ordnum nil)where Ordnum looks for ordinals, superlatives, andnumbers.3.2 Control commandsIn addition to the three primitive commands justdescribed, we provide commands to specify optionality,possible repetition, alternatives, and sequence.The Opt command instructs the parser to attempt oprocess an arbitrary command, but Opt will succeed evenif this attempt fails.
Some example Opt commands are(Opt Quote of)(Opt Get Noun Head)(Opt Call PP)In addition to applying Opt to a basic command, as shownhere, Opt may be applied to any of the remaining controlcommands discussed below.
Since the scope of Opt, andalso of * as discussed next, consists of a single command,we have avoided introducing a superfluous set of parenthe-ses surrounding its argument.The Star command, or simply *, denotes the Kleene-star and says to perform the embedded command 0 ormore times.
Some examples are(* Get Adj)(* Call Relc)During parsing, a * command is treated as an Opt thatre-invokes itself upon each success.The Alt command instructs the parser to perform exact-ly one of a set of commands, which it tries in the orderthey are given.
Alt will fail if none of its argumentssucceeds.
Some examples are(Alt (Get Noun)(Get Pron))(Alt (Call PP)(Call Relc))An interesting and frequent instance arises when one ofseveral constructs i to be optionally recognized.
We havefound it pragmatically preferable in these instances, bothvisually and for the sake of efficiency, to code for an Alt ofalternatives, and surround this Alt with an Opt, ratherthan apply Opt to each alternative with an Alt outside.That is, we would normally write(Opt Alt (Call PP)(Call Relc))rather than(Alt (Opt Call PP)(Opt Call Relc))As a second pragmatic remark related to Alt, we note thatthe command(Alt (Get Noun)(Get Pron))may function differently from(Get (Noun Pron))when the next word to be parsed has several parts ofspeech associated with it.
In the former case, the nounreading will be taken if at all possible, while in the lattereither a noun or pronoun reading is equally acceptable tothe grammar, so the first grammatical category appearingin the scanner output will determine what is selected.Computational Linguistics Volume 10, Number 2, April-June 1984 85Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPSince the problem of mis-recognitions is especially trouble-some in our projected voice-input environment (Biermannet al 1983), this distinction can be important.Finally, the Seq command instructs the parser toperform a list of commands in order.
For example,(Seq (Get Prep)(Call NP))says to recognize a preposition, then call for a noun phrase.If any member of the list supplied to Seq fails, then theentire Seq command will fail.
Before doing so, however,attempts will be made via the backtracking mechanisms ofthe parser to re-interpret what has already been parsed byprevious commands of the Seq.
Strictly speaking, Seq isredundant since its effect can be obtained by using amacro with a dummy name.
However, we find it conven-ient to be able to code nameless equences where they areused, somewhat analogous to the use of LAMBDA in LISP,or of BEGIN-END blocks in Algol.3.3 AugmentationsThe seven syntax commands described above provide thegrammar writer with convenient means of specifyingcontext-free rules for fragments of natural language (infact, only four of the seven are required in order to dothis).
However, the inadequacies of a pure context-freeformulation of natural language syntax are well recog-nized, and various treatments have been used to overcomethem (Bobrow and Webber 1980; Heidorn 1975; Colmer-auer 1978; Kimball 1972; Marcus 1980; Pereira 1981;Pratt 1975; Rieger and Small 1979; Robinson 1982; SagerandGrishman 1975; and Woods 1970, 1980).
Within ourgrammatical formalism, we have provided means of speci-fying several forms of useful "compatibilities" among theelements of a phrase or clause.
As the reader mayobserve, most of our provisions for context-sensitive spec-ifications could in theory be done in a strictly context-freefashion, though not conveniently (e.g.
a large and poten-tially exponential increase in the number of parts of speechmight be required).
A related use of augmentations is to"annotate" the parse structure with information that willbe useful in its subsequent semantic processing.Compatibility checking is done according to augmenta-tions which occur as optional parameters of Quote, Getand Call commands.
With a few exceptions, augmenta-tions may be used in any combination.
Thus, the generalform of the three basic commands, wfiich we described insimplified form in Section 3.1, is(Quote <literal word(s)> { <label> { <pl> ... <pN> } } )(Get <part(s) of speech> {<label> {<pl> ... <pN>} })(Call <routine name> {<label> {<pl> ... <pN>} })where braces denote optionality and where each parameter(denoted by pi) has one of the forms we now describe.3.3.1 Feature-value pairs and a not-local markerThe simplest ype of augmentation, which applies to any ofthe three basic commands (i.e., Quote, Get, Call), consistsof a feature-value pair that supplies information on, andthus restricts the allowable values for, some "feature" ofthe current phrase.
Since dictionary listings also containfeature-value pairs, and words being incorporated into aphrase must have features compatible with those alreadyin the phrase, a feature-value specification i  the grammarmay serve to restrict he set of legal words to be processed.That is, the information about a word about to be incorpo-rated creates an inconsistency, thus causing the commandbeing considered to fail.
As an example, the command(Get Noun Head (sp sing))contains the feature-value augmentation "(sp sing)" whichsays that the "sp" feature of the current phrase must havethe value "sing".
In essence, the command calls for asingular noun.
The command(Get Ved Verb (type past part))gives an example of a feature-value pair that somewhatmore liberally requires that the "type" feature of thecurrent phrase be one of two possible values.Another use of feature-value pairs is to incorporateinformation into the phrase being processed that will beused to determine the acceptability of subsequentcommands, as described in Section 3.3.4.
For example,the command(Opt Quote by nil (byfront))might be used at the top of a grammar outine for passiverelative clauses to indicate that the phrase actually beganwith the word "by" as opposed to the word being foundelsewhere.
Note that, in situations uch as this, an isolatedfeature name without associated values is sufficient andthus allowed by the formalism.Still another use of feature labels is to annotate theparse structure being built up, as in(Get (Aux = (func yesno))Naturally, different uses of feature labels may occur in asingle command.
For instance, the word "many" in thedetermining phrase "how many" might be coded as(Quote many nil (func count) (sp plural))where the "func" feature is an annotation and the "sp"feature assures that the subsequent head noun will beplural.In some situations, it may be desirable to ignorefeature-value pairs in the dictionary listing for a word and,in this event, a notlocal augmentation may be used.
Forexample, the command(Get Prep Part notlocal)might be used to indicate that the features associated witha preposition are to be ignored when the word is being86 Computational Linguistics Volume 10, Number 2, April-June 1984Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPused as a particle.
By allowing an attachment of"notlocal", we avoid some of the need to associate multiplesenses with all the words that some command is interestedin recoghizing.3.3.2 Feature  labelsThe second type of augmentation applies to Callcommands and consists of a feature label specificationsimilar to the feature-value pairs discussed above.
Theeffect is to require some feature of the current phrase toagree with some possibly different feature of the childphrase about to be created.
That is, the values of the twofeatures are to be shared.
If the parent and child phraseare to agree on the same feature label, this label is merelyincluded at the end of the Call command.
As an example,we might account for the first part of postnominal compar-ative phrases of the form "(which is) (not) <compar>than", where parentheses denote optionality and"<compar>" denotes a comparative such as "better" or"bigger", by(Call CompPh Comp nt sp)which assures that the top-level verb will agree in plurality(sp), and the relative pronoun in nountype (nt), with thehead noun to be modified.
In the event that every featureof the parent and child phrases hould agree, the fictitiouslabel all may be used.
For example, our Adj routifiehandles adjective and present participle modifiers, andalso negated forms of these (e.g.
"non failing"), and thelatter are processed by a low-level NegAdj routine invokedby(Call NegAdj neg all)This allows the scope of the negation to be retained, sincethe modifier is nested inside a "neg" label to the nounphrase being built up, yet assures that the modifier iscompatible with all features in the noun phrase just asthough it were to be processed irectly.When parent and child phrases are to agree on differentlabels, we include an "agree" triple of the form(agree <parent feature> <child feature>)As an example, we might wish to associate two separatenountype features with argument-taking nouns like"classmate", one for the type of world object the worditself refers to and one for the type of world object associ-ated with its argument.
In this case, the word"classmates" might receive the dictionary listing(classmates Argnoun classmate(nt student) (ntarg student) (sp plur))and we might recognize the phrase "classmates of John"by(Seq (Get Argnoun Head (head argnoun))(Quote of)(Call NP NounArg (agree ntarg nt)))since at the time the Call command is encountered theparse structure will be(NP (feats (head argnoun) (ntarg student)(nt student) (sp plur))(Head .
classmate))3.3.3 Non loca l  f rame parametersAnother form of augmentation associated with Callcommands i the nonlocal frame parameter, which is simi-lar to, but more general than, what is available using afeature-value pair.
It allows the grammar writer toenforce agreements between or among phrases, and hasthe form((agreement type) { <labels> <feature> } )*where {... }* indicates repetition of "..." 0 or more times.The required compatibility among the indicated constitu-ents is that an appropriate tuple be found in the set oftuples associated with the specified agreement type.
Eachlabel-feature pair tells what elements of the tuple havealready been found, and which of their features containsthe desired compatibility information.
As indicated inSection 6.1, compatibility tuples are created uring know-ledge acquisition and made available to our parser inparallel with the dictionary listings from the scanner.
Forexample, prepositional compatibilities might be indicatedby a set of triples such as((Head Prep Arg)(book in table)(book on table)(book on chair)(chair on table))so that the prepositional phrase "in the old table" would beallowed to attach to a phrase whose head noun has thenountype book but not to one having the nountype chair.As an example, we might use the command(Call PP PP ((Prepinfo) Head nt))to require a 3-way agreement among the indicated compo-nent (Head) of the current phrase and remaining compo-nents (Prep and Arg) of the routine (PP) about to beinvoked.
The "nt" feature tells where in the currentphrase and the remaining phrases the values for agreementare to be found.
In our current grammar, the preposi-tional phrase (PP) routine locates a single word to fill thePrep slot and finds an NP phrase to fill the Arg slot.
Thus,the 3-way compatibility in effect is to be found among1.
the nt feature of the current phrase,2.
the single word filler for the Prep slot, and3.
the nt feature for the phrase that fills the Arg slot.As a second example of compatibility information, wecurrently maintain 6-tuples for verb phrases, where eachposition corresponds to a possible clause element as shownin Section 4.1, so that the tuplesComputational Linguistics Volume 10, Number 2, April-June 1984 87Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLP((Subj Verb Obj Prep Arg Part)(student fail course nil nil nil)(student ake course from instructor nil)(student make grade in course nil)(instructor fail student nil nil nil)(instructor cross student nil nil up))might be used to say that a student may take a course butnot an instructor, an instructor may fail a student but notvice versa, an instructor may be said to have crossed up astudent, and so forth.In the implementation of our parser, compatibilityinformation is maintained in only one place and all relatedstructures point to this location.
This means that compat-ibility information will be passed both up and down theparse structure under construction to aid in disambigua-tion and subsequent interpretation.In the event that one or more of the clause elementsassociated with a nonlocal frame parameter is to beconsidered optional, a nonlocal frame parameter may beaccompanied by an opt parameter to this effect, so that(Call Passive Relc (Relinfo) Obj nt) (opt Subj))calls for a passive relative clause in which the object (Obj)has been relativized and in which the subject (Subj) maybe omitted, as in "the book given to Paul".
As an exampleof when more than one element of a compatibility tuplewill already have been found when tuple agreement isrequested, suppose we wish to handle "deep" relativiza-tions, as in a "a book Smith wrote the last chapter of".
(Although a linguist friend has recently told one of us thatthis is bad English, it occurs quite often in computationalsettings and is therefore sensible to inject into a realisticgrammar.)
Here, the triple chapter-of-book is to bechecked, so the preposition "of" is preceded by its argu-ment as well as by the noun it modifies.
Thus, assuming"nt!"
gives the nountype of "Smith" that has been passeddown by the methods of Section 3.3.2, we might writeCall Prep PP-hole ((Prepinfo) Head nt Arg nt!
))to find a lone preposition with appropriate agreement.3.3.4 The  "need feature"  parameterAnother form of augmentation, which enables the gram-mar writer to deal with situations where the presence ofcertain information enables rather than blocks anotherphrase, involves a need parameter that may occur with anyof the three basic commands or, with a related effect, withan Opt command.
The effect of this type of augmentationis to allow the associated grammar command to be usedonly if one of a specified list of features or items (parts ofspeech) is already present in the current phrase or, ifdesired, to enable a command only in the absence of previ-ous features or items.
Ordinarily "need" will check for thepresence of a feature, but optional "not", "part", and"tok" flags will cause its behavior to be modified appropri-ately.
As an example usage of "need", assume that wewant to account for "ordinal modifiers", i.e.
phrases uchas "from the right", that may occur as postnominal modi-fiers when a prenominal ordinal has occurred.
That is, wewish to allow "second column from the right" yet disallow"columns from the right".
This might be accomplished byusing the command(Call Ordmod = (need part Ord))As a second example, we might use the command(Get Noun Head (sp plur) (need part Art Num))to indicate that a plural noun is to be accepted only ifeither an article (Art) or number (Num) has alreadyappeared in the current phrase as a "part" (of speech).To check for a token feature, we use "tok".
For exam-ple, to treat "more interesting" but not "more good" as acomparative (thus ruling out the anaphoric comparativereading of a sentence such as "we'll have more good thingsto talk about tomorrow"), we put a "parap" feature in thedictionary listing for paraphrastic adjectives (i.e., thosethat take "more" as opposed to "-er") and then write(Get Adj Compar (need tok parap))As an example of the complementary usage of "need",suppose we wish to allow a post-nominal comparative tooccur with its associated "than" phrase only if a pre-nomi-hal comparative has not been found.
That is, we want toallow "better student than Jack" and "students betterthan Jack" but not "better students better than Jack".
Inthis situation we might check for a possible post-nominalcomparative with the command(Get Compar = (need not part Compar))In the event that a command is to be treated as optional insome cases but is required in others, a "need" augmenta-tion may be used along with an Opt command and, in suchsituations, the command to be considered optional must begiven inside parentheses (otherwise the "need" would beassociated with the command itself and not with its option-ality).
As an example, we might account for ellipticalnoun phrases such as "the seven" or "the last" via thecommand(Opt (Get Noun Head) (need part Ord Num))which says that a Noun must appear unless the phrasebeing processed already has an ordinal (Ord) or a number(Num).3.3.5 The  "next  words"  parameterFinally, to allow a fixed grammar to be applicable todomains in which certain unpredicted idiomaticconstructions arise, we provide a next parameter that indi-cates that the dictionary word being processed must beliterally followed by one or more words.
The "next"parameter is available for both Quote and Get commandsand, more important, may also be contained within thedictionary listing of a word.
As an example, we might88 Computational Linguistics Volume 10, Number 2, April-June 1984Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPaccount for the idiom "to pick up on" by including thefollowing as one of the senses for the word "picked"(picked Ved pick-1 (next up))and then indicate in the compatibility file that "on" acts asa particle for the verb whose root is "pick-l".
To be moreextreme, we might have(kicking Ving kick-2 (next the bucket))together with an indication that "kick-2" is an intransitiveverb, to indicate that the phrase "to kick the bucket" is tobe treated idiomatically.
The "next" feature might also beused in the grammar as a simplification, allowing us toreplace(Seq (Quote at nil (quant leastmany))(Quote least)(Quote as)(Quote many))with the simpler(Quote at nil (next least as many) (quant leastmany))4.
Current Utilization of the Grammatical Formal-ismWe have indicated that the phrase-structure grammaticalformalism discussed in this paper is being used in thecontext of LDC, a transportable natmal language process-or.
We now describe briefly the nature of the parser andcurrent grammar associated with this system, then givethe results of some experiments with the grammar andparser.4.1 The current LDC grammarMost of the present LDC grammar comprises(a) a fairly elaborate noun phrase grammar, and(b) a case-like specification of sentence-level and fairlycomplex relative clauses.For example, we presently provide for relative clauses ofmany varieties (e.g.
"by whom a book was given to Bill" aswell as "who gave a book to Bill") having case frames ofthe formSubj Verb {Object} {{Prep} Arg} {Particle}where braces denote optionality.
We also provide formany kinds of pre-nominal modifiers, including ordinals,superlatives, adjectives, and noun modifiers.
Many formsof comparative phrases are also provided, including ellip-tical ones such as "a longer document han xletter" and"students making a better grade than Bill in CPS152".Our initial grammar was approximately six pages inlength and yet, due to the consolidations made possible byour phrase-structure rules, contained almost all syntacticstructures available in the original 20-page ATN grammarused by NLC from which it was adopted, and many otherstructures (notably, more elaborate comparative and rela-tive clause forms).
However, this grammar provided foronly noun phrases, albeit complex ones.
Our currentgrammar is eight pages in length and, due to consol-idations made possible by enhancements to the formalism,allows both wh- and yes-no questions, imperatives,passives, deep raisings, several forms of fronting, a fewdiscontinuous constituents, and some exotic modifier typesmentioned in Ballard (1984).
The syntactic processing ofa representative input for LDC was indicated in Figure 1,and information of the scope of the current syntactic andsemantic overage of LDC can be found in Ballard (1984).As indicated below, some of the first domains that weused to test the modules of LDC were(a) a final grades domain,(b) a document preparation domain, and(c) the original matrix domain of NLC.In the interest of assuring domain independence, onlythose constructs that arise in two or more of these domains(or others we have tested Prep with) appeared in the initialLDC grammar.
This has caused us to temporarilydiscount certain features, especially (a) low-level syntaxfor domain-specific noun phrases, such as the optionalindefinite article in "a B+" for the final grades domainand floating-point numbers for the matrix domain, and (b)certain unusual or idiomatic verb phrase forms.4.2 The initial to-down parserWhen designing a syntax processor for a class of formal ornatural language grammars, one often begins with a top-down implementation, then uses this system to test andrefine the formalism and/or grammar(s) of interest, andonly later contemplates a more efficient bottom-up, possi-bly table-driven, implementation.
We have followed thispractice in designing an initial top-down parser for LDC.Backtracking is used when a command fails because itherthe wrong type of input word has been encountered or aninconsistency has been created by compatibility informa-tion.
Since semantic information is being used for disam-biguation during parsing, our parser returns the first (andonly) fully acceptable structure if finds.Our kernel parser, which deals with the seven commandtypes but ignores augmentations, was designed, coded andtested by one person in fewer than 10 days and occupiesless than four pages of lightly commented LISP code.
Thefull repertoire of augmentations, on the other hand,required roughly six man-months of effort to design andimplement and occupies about eight additional pages ofcode.
This is partly due to the additions and refinementsbeing made to the formalism as the parser developed.Detailed information on the construction of the initial LDCparser is given in Ballard and Tinkham (1983).4.3.
Experiments with the LDC Grammar andParserHaving completed the parser and constructed a suitablybroad grammar for use in several layered domains, wehave begun to experiment with our parser in the manner ofSlocum (1981) to see how significantly certain of ourComputational Linguistics Volume 10, Number 2, April-June 1984 89Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPpruning methods can reduce the time complexity of pars-ing.
Some of the pruning techniques we have studied areI.
the use of a Start file that tells what grammatical cate-gories may begin each syntax routine,2.
the use of local compatibility checking Section 3.3.1),and3.
the use of non-local compatibility checking (Sections3.3.2 and 3.3.3).In addition to efficiency interests, the latter two tech-niques influence what parse structure is found for spuriousor ambiguous inputs, which is also a concern worthy ofstudy.
However, we have postponed an investigation ofparsing "accuracy" until we have begun to elicit actualinputs from realistic users.
We now describe severalexperiments with the LDC grammar and parser.4.3.1 Usefulness of the "Star t "  fileAs an initial experiment with the LDC grammar andparser, we constructed representative inputs of varyinglengths from each of the three domains mentioned aboveand ran the parser both with and without the pruningcapability provided by the Start file.
This file gives infor-mation analogous to that provided by the First relation ofconventional LL(1) compiler theory, and is created auto-matically by an off-line process that traverses a grammar(see Section 6.1).
The actual inputs that were used follow.Document domain:"the first two sentences""the longest sentence in the first paragraph""the shortest sentence in the first paragraph containing amisspelled word"Final Grades domain:"the best student""the best undergraduate Ballard taught in AI""the student with the highest grade in the course Marymade a B+ in"Matrix domain:"the first two rows""the entries added up by command 7""the second entry the last four commands added five tothat is positive in matrix 2"Statistics were gathered for both the number of grammarcommands executed, and the actual running time.
Inparticular, we had the parser keep track of the number oftimes an attempt was made to execute a Call command,and also how many times the test of whether to invoke thebody of the Call succeeded.
At the system level, we hadUN IX 2 report the amount of time (in seconds) spent by theparser for each of the 18 runs (9 sentences each run withand without Start information).
The results of this studyare given in Table 1.
As can be seen, the use of the Startfile led to a significant reduction both in the number ofCall statements entered or executed and in the actualparsing time.
In fact, for 8 of the 9 inputs, parse time wasreduced by more than half.
An explanation for the fairlylong parse times is given in Section 5.3.4.3.2 Expense of macro-sty le SEQ commandsAfter studying the results given above, we wondered howgreatly the observed improvements were due to the pres-ence of unnecessary Call commands.
In the grammarbeing tested we had, for the sake of clarity, included manyCall commands that were invoked at only one place in thegrammar, and we wondered what the results would be ifthese "superfluous" Call commands were replaced by theirassociated bodies.
Since only Call commands have"Start" lists associated with them, we conjectured that itwould actually be an advantage to have many Callcommands, especially for a lengthy Alt (alternative)command that would do lots of superfluous work.
To testthis hypothesis we ran an experiment o compare theoriginal grammar against a modification of it in which thebodies of several of the superfluous Call commands wereinstantiated at the point of call.
For reasons not relevantto this discussion, it was first necessary to re-order some ofthe grammar rules in order to make a fair comparison.Thus, there were three grammars to be tested.
Eachgrammar was run both with and without the Start infor-mation for several of the sample inputs shown above.
Forthe most complicated input, "the second entry the last fourcommands added five to that is positive in matrix 2", theresults are given in Table 2, where the statistics forcomparison are found in the second and third lines.Although the full significance of these preliminary resultsis not clear, it is apparent that the instantiations had(a) a small positive effect when Start information wasnot being used, and(b) a small negative effect when Start information wasused.These results lend credence to the hypothesis that intro-ducing convenient Call commands will not lead to anincrease in parsing efficiency in the presence of the Startfile.4.3.3 Expense of nonlocal compat ib i l i ty  checkingUpon noting the long parse times in the preceding exper-iments, we wondered how much of this time was beingspent in nonlocal compatibility checking and whether thischecking increased parse time, due to the extra workinvolved, or decreased it, by early pruning of erroneouspotential parses.
Accordingly, we considered two gram-mars, one with augmentations calling for nonlocal check-ing to be done, the other without such parameters, butidentical otherwise, and compared the parsing times foreach grammar for each of the four noun phrases"the first class""the best undergraduate student Ballard taught in AI""the name of the class the student he professor taughtliked"2UN IX is a trademark ofAT&T Bell Laboratories.90 Computational Linguistics Volume 10, Number 2, April-June 1984Bruce W. Ballard and Nancy L. Tinkbam A Phrase-Structured Grammatical Framework for Transportable NLPTable 1.
Reduction in Parsing Times by the Use of the "Start" FileSentence Without Start With Start PercentDomain Length Considered Time Considered Entered Time Savingsshort 21 8.5 6 4 2.9 66Document medium 49 18.9 24 12 8.5 55long 80 32.3 29 16 12.2 62short 21 8.2 6 4 2.7 67Grades medium 36 14.3 16 10 6.5 55long 94 37.i 42 22 13.8 64short 21 7.4 6 4 2.4 68Matrix medium 29 12.0 17 8 5.5 54short 8.0 2.7 66Average medium 15.1 6.8 55long 36.7 16.7 55* "the four tallest classes"where "*" denotes an unacceptable s ntence.
In each caseboth local compatibility checking and checking of Startinformation were done.
The results appear in Table 3.
Aswith the experiments discussed above, these measurementshave limited statistical significance due to the smallsample sizes of grammars and of inputs.
However, thepreliminary indication is that checking for nonlocalcompatibility adds more time to parsing than it saves.This suggests that the benefits of such nonlocal checkingwill be primarily in improved accuracy of disambiguations,rather than speed of parsing.5.
D iscuss ionWe now summarize what we believe to be the most signif-icant aspects of our formalism, briefly comment on therelation of our grammars to conventional ATN grammars,and finally mention some of the drawbacks to ourapproach.5.1 Some advantages  o f  our  fo rmal i smOur goals in developing the grammatical formalismdiscussed in this paper were, first, to prepare for transpor-tability and, second, to be able to specify grammars in asimple, understandable, and succinct fashion.
Concerningthe first goal, we have seen how nountype, plurality, andvarious forms of compatibility information can be conven-iently passed up and down a parse structure underconstruction.
This information can prove useful in disam-biguations and in subsequent semantic processing.
Inparticular, our formalism allows us to state manyrestrictions of semantic grammars within a general gram-mar, independent of the domain(s) at hand.
This is possi-ble because the files created during knowledge acquisition,namely the dictionary and associated compatibility file,contain many types of domain-specific nformation that isuseful in making syntactic and semantic decisions duringparsing.
For instance, we can write a simple and relativelyshort set of syntax routines for relative clauses that over-generates (i.e.
allows many spurious structures) sinceadequate restriction information is available in the pre-processed files.
This is similar in spirit to the isolation ofrestriction information described in Sager and Grishman(1975).Some of the features of our formalism which we consid-er desirable but not related directly to the goal of transpor-tability are the following.1.
Our Quote and Get commands represent a consol-idation over lower-level command types of the ATNform adopted for our previous NLC system.
Forinstance, we use a single Get command to take theplace of what would require three separatecommands in the NLC grammar.2.
We have provided for default labels in the parsestructures being built, which grammar writers canoverride if they choose.3.
We often allow for lists of items where only one itemwould be expected.
For instance, we may ask for oneof several words or parts or speech, as in(Quote (that which who)) or (Get (Num Super)),or specify several compatibility-checking augmenta-tions in a single command.4.
We provide for arbitrary embeddings of commandswithin any of the composite commands, imilar to thenestings provided in LISP and a number of modernprogramming languages.Computational Linguistics Volume 10, Number 2, April-June 1984 91Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPTable 2.
Effect of In-line Instantiation of Superfluous Call CommandsWithout Start With StartGrammar Considered Time Considered Entered TimeOriginal Grammar 84 40.2 54 28 25.1Re-ordered Grammar 94 46.7 59 30 27.0Re-ordered Grammarwith Instantiated Calls 69 45.5 52 27 31.15.
Our grammars do not require dummy node names,such as those occurring in typical ATN grammars.6.
We handle most restriction specifications by a smallnumber of augmentations which cling to the sevencommand types.
This implies the presence within ourgrammars of an easily visible context-free skeletongrammar, which one can detect without having totrace through and ignore various testing commands.7.
Due to the crispness of our grammar commands,presence of consolidations with appropriate defaults,and manner of embedding augmentations withincommands, we are able to work with relativelycompact grammars, which aids in their comprehen-sion and manipulation.8.
The crispness of our seven commands has alsoallowed for useful preprocessing of the grammar tocreate the Start and Adjacency files.
This form ofinformation has been found useful by severalresearchers, yet is often supplied directly from thehuman author of the grammar, and must be updatedwhen the grammar changes.
Our files are createdautomatically.5.2 Compar i sons  w i th  Augmented  Trans i t ionNetwork  Formal i smsAs remarked earlier, our grammatical formalism, which isbased on what we regard as phrase-structure ules, bearssome resemblance to ATN grammars (Woods 1970, 1980),but there are important differences.
First, the notion of anetwork node is almost entirely absent from our formal-ism.
For example, in the following typical ATN node(Q2 (PUSH NP / T(SETR Subj *)(TO Q3)))we note the presence of both (a) the label Q2, and (b) thereference to the successor node Q3, neither of which has acounterpart in our grammars.
By eliminating such nodenames, we reduce the space needed to specify grammarrules, which helps make our grammars more readable.We also reduce the redundancy of the grammar epresen-tation, which makes updates easier and less error-prone.Actually, node names for ATN grammars need not begiven explicitly, but their elimination requires thatnetworks be stored as linked structures that resist conven-ient manipulations with standard text editors.
This wasthe alternative representation chosen for the ATN gram-mars of NLC.
As a final point concerning readability ofgrammars, we note the common practice of conveyingATN grammars by giving an actual network diagram,which by its non-linear nature cannot be so conveyed tothe computer system.A second departure of our formalism from conventionalATNs is that we have systematically avoided the tempta-tion to introduce opportunities for making arbitrary tests (aprovision shared, evidently, with even more glee, in variouslogic grammars, e.g., Pereira 1981).
Such opportunitiesare provided for in ATN grammars by the TST node type,and also by allowing arbitrary LISP predicates at variouspoints.
As indicated in Section 4.3, we have taken pains torestrict the repertoire of conditions that may be checked,hoping that such restrictions will better direct the gram-mar writer to the salient features that need to be consid-ered.
Our interest is more to provide a formalism that canbe used easily and profitably than to provide somethingwith formal Turing ability.
Thus, we have sought to iden-tify a small but adequate number of easily understandableconditions to be checked when doing a Get, Call, and soforth.
Another departure is that many of our augmenta-tions, most conspicuously that described in Section 3.3.3,amounts to setting up demons to monitor the process of aroutine about to be called so that appropriate informationpassing will occur.
This allows a routine to be written justonce, yet lead to different sorts of compatibility checkingbased upon the context in which it is invoked.
This philos-ophy appears to differ from that upon which the essential-ly bottom-up SEND mechanism of ATNs is based.5.3 L imi tat ions  of our approachWe now mention some of the difficulties we have encount-ered with our approach to transportable parsing.
Beforedoing so, we note that the excessive parse times suggestedin Tables 1-3 are no longer a problem, since our parsernow runs on a VAX in compiled LISP (and the tables were92 Computational Linguistics Volume 10, Number 2, April-June 1984Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPTable 3.
Time Spent in Nonlocal Compatibility CheckingSentence Without Checking With Checking Percent OverheadShort 4.6 5.2 13.0Medium 10.2 13.7 34.3Long 18.8 25.7 36.7Incorrect 8.7 9.0 3.4created on a 16-bit PDP-11/70 running fully interpretedLISP without hash tables).
At present, parse times arehovering at around one second.First, we note that at present he noun positions in caseframes and prepositional triples consist of object types ofthe domain at hand.
Although this level of abstraction issomewhat more general than specific surface words, weare considering means whereby the user can make use ofthe hierarchical relationships among domain entities toreduce the redundancy of some case frame specifications.For example, "person" might be used to mean "eitherstudent or instructor", and in fact entire taxonomies mightbe introduced.Second, we have observed a few places in our grammar,especially the treatment of quantifiers, determiners, andtheir allowable co-occurrences, where the old ATN formal-ism we used for the NLC grammar would result in moreconcise and not necessarily less perspicuous grammars.For instance, "all/each of the" is okay but not "every ofthe"; "every/each one of" is okay but not "all one of the";and "all the" is okay but not "each/every the".
In fact,we at times sketch out a new class of constructs to beincorporated into our grammar in a pidgin transitionnetwork form, then seek ways to linearize.Finally, we note that several existing grammaticalformalisms, such as string grammar and ATN grammar,have enjoyed more than a decade of refinement and haveled to quite efficient parsing mechanisms.
We expect heprinciples we have developed to undergo similar improve-ments during the course of our research.
Though much ofthe current LDC grammar was derived from correspondingparts of the ATN grammar of NLC, there is no reason todoubt that we will be able to adopt relevant portions ofother large grammars (e.g.
Robinson 1982; Sager 1981),thus taking advantage of previous research directed moretoward the linguistic versus domain-modeling aspects ofnatural anguage syntax.6.
A G l impse  at the  Overal l  LDC Env i ronmentThe design of LDC began in 1981 with the goal of allowinga system designer to quickly create interfaces to newdomains by supplying vocabulary and domain structureinformation to a customizing module.
However, LDC soondeveloped into a system where all information about a newdomain is acquired from prospective users, as had beendone for REL (Thompson and Thompson 1975) andKLAUS (Haas and Hendrix 1980).
The system is looselybased upon strategies developed for our English-languageprogramming system NLC (Ballard and Biermann 1979;Biermann and Ballard 1980; Sigmon 1981; Fink 1982;Geist, Kraines and Fink 1982; and Biermann, Ballard andSigmon 1983), and has been designed to provide a naturallanguage query capability for office domains whose dataare stored on the computer as informally structured text-edited data files (Ballard and Lusth 1984).
The systemcomprises(a) a knowledge acquisition module called "Prep",(b) a highly-parameterized English-Language processor,and(c) a knowledge-based Retrieval module.We now summarize the operation of the knowledge acqui-sition and English processing components, referring thereader to Ballard and Lusth (1984) for details concerningretrieval processing.6.1 Knowledge  acqu is i t ionThe initial interaction between a user and LDC, whichinvolves telling the system about a new domain, consists ofa knowledge-acquisition session with the preprocessor,which we call "Prep".
In particular Prep asks for(1) the names of each type of "entity" (object) of thedomain;(2) the nature of the relationships among entities;(3) the English words that will be used as nouns, verbs,and modifiers; and(4) morphological and semantic properties of these newwords.For example, in describing a building organizationdomain, a user might supply to Prep the structural andlanguage-related information that "room" is a primitiveentity type; "large", "small", and "vacant" are adjectivemodifiers; "conference" is a noun modifier; "office" is anoun referring to some objects of type "room"; and thatrooms have "wing" as a higher-level domain entity.
AtComputational Linguistics Volume 10, Number 2, April-June 1984 93Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPthis point semantic specifications for the modifiersmentioned above are given, and morphological variants aresupplied, e'g.
"rooms" is the plural of "room", "larger" isthe comparative form of "large", "vacant" has no associ-ated comparative, and so forth.Having completed a session with the user, Prep digestsits newly acquired information to produce various files tobe used during subsequent processing of English inputs bythe English-Language processor.
The first file created byPrep is the Dictonary, which is used as input by the scan-ner, and whose format was suggested in Section 2.2.
Somedictionary listings are provided for common, domain-inde-pendent terms such as articles, ordinals, and certain verbs.The second file created by Prep gives Compatibilityinformation on (a) verb case frames and (b) expectedprepositional attachments.
Verb case frames are passedalong almost directly from the user's specifications, where-as prepositional ttachments are determined by heuristicsrelated to layered domains.
Case frames are specified bysubject, verb, optional particle, optional object, andoptional preposition-argument pair, while legitimateprepositional ttachments are specified by entity-preposi-tion-entity triples.The third file created by Prep tells the parser whatdictionary words can Start each grammatical unit (syntaxroutine) of the grammar.
This information is roughlyequivalent to that provided by the LL(1) tables of compilertheory.
However, Prep takes the attached features intoaccount, and must also account for multiple word mean-ings.
Several existing natural language processors havefound such Start information useful, though to our know-ledge most implementations have had the informationsupplied by hand from the system designer, rather thanautomatically constructed from a novel dictionary file andfor an evolving grammar, as we provide for.
The entirecode needed to create the Start file is about 30 LISP lines.Its brevity and conceptual simplicity are due to the crisp-ness of our phrase-structure formalism.The fourth file created by Prep is a Adjacency file thattells the scanner which dictionary words a given word maybe followed by in a legal input.
This form of information,which is being used by the current "voice scanner" of ourrelated NLC system (Biermann 1981), will likely proveuseful when we introduce voice input to LDC.
To ourknowledge the Adjacency file is without counterpart inconventional compiler design.
Although the recursiveroutines responsible for creating the Adjacency file resem-ble those related to the Start file, they involve combinator-ic considerations and are much more complicated.Finally, two additional files created by Prep, which arenot involved in parsing, supply domain structure informa-tion for semantic processing and adjective and verb seman-tics for retrieval.6.2 English-language processingThe organization of the natural language processingportion of LDC resembles that of interpreters for conven-tional programming languages by exhibiting a linearsequence of modules without complex interaction amongthem.
A pictorial overview of the English-languageprocessor and retrieval module is given in Figure 2, whichalso gives an idea of how the domain-specific filesproduced during the user's interactive session with Prepare used.
As suggested there, the scanner and parsersupply the semantics module with an internal rendering ofthe user's input, whereupon semantics appeals to aretrieval component, and then sends the top-level responseto the output module to be printed in user-readable form.We now comment briefly upon each module involved inEnglish-language processing.6.2.1 ScanningThe role of the scanner is to identify each word of thetyped or spoken input and retrieve information about itfrom the Dictionary file, which will have been created byPrep as described in Section 2.1.
For words having morethan one dictionary listing, all possible meanings (read-ings) are sent to the parser, where context will be used toselect one of them.
For the sake of run-time efficiency,morphological variants (inflections) of domain-specificterms will already have been stored in the dictionary, sorun-time stemming is not needed.The existing LDC scanner assumes typed input but, asdescribed in Biermann et al (1983), we have used aNippon DP-200 voice recognition unit, and more recently acontinuous-speech Verbex device, with our previous NLCsystem, and its introduction into LDC is being contem-plated.
When this occurs, some of the word meanings willhave been taken from a "synophone" list6.2.2 ParsingAs an example of how parse structures are built up,consider the noun phrase"the largest white house in Ohio"for which the scanner will have supplied information suchas(the Art the)(largest Super large (nt building parcel))(white Adj white (nt building))(house Subtype house (sp sing) (nt building))(in Prep in)(Ohio Nounval Ohio (sp sing) (nt state))For simplicity we have given just one reading for eachword, but in general each word may have several.When the NP routine is entered, an initial structurewith a label of NP but null feature and item lists iscreated.
(NP (feats))Since our present grammar checks for the word "the"using Quote, rather than Get, no parser output occurswhen "the" is seen, although optional output is allowed by94 Computational Linguistics Volume 10, Number 2, April-June 1984Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLPDICTIONARYiDJACENC Y?
Scanner HSTARTCOMPATParser H Translatort GrammarDOMAIN MODIFIERSTRUCTURE SEMANTICSL LH Retrieval HtRaw-DataFigure 2.
Pictorial Overview of the English-Language Processor and Retrieval Modulethe Quote command.
Next, since the dictionary listing for"largest" indicates that it can modify entities of typebuilding and parcel, the parse structure upon parsing theword "largest" will become(NP (feats (nt building parcel))(Super.
large))Now since "white" is marked as an adjective that mayonly modify entities of type building, its incorporation willlead to an updated parse structure of(NP (feats (nt building))(Ad j .
white)(Super.
large))Here the fact that more recent phrase elements appear tothe left of previous ones is an artifact of the LISP imple-mentation, and the order is basically ignored during post-parser processing.
Next, the word "house" will beprocessed by the grammar command(Get Subtype Head (head subtype))giving rise to(NP ((head subtype) (sp sing) (nt building))(Head .
house)(Ad j .
white)(Super.
large))Next, the post-modifier "in Ohio" will be processed and,upon returning from a recursive call to-the noun phrasegrammar, the new parse structure will be(NP (feats (head subtype) (sp sing) (nt building))(PrepPh (feats (nl (Head Prep Arg)(building in state)))(NP (feats (head nounval)(nt state) (sp sing))(Head .
Ohio))(Prep.
in))(Head .
house)(Ad j .
white)(Super.
large))The interested reader may wish to reconsider Figure 1 foran example of a complete parse structure for a morecomplicated input.
Further details on the mechanisms ofparsing are given in Ballard and Tinkham (1983).6.2.3 The  t rans la torTraditional approaches to natural language databaseinterface seek to provide access to a user's data byconstructing a formal query to an existing retrieval systemthat arose in the database community without the inten-tion of an eventual English-language interface.
Ourapproach differs in that we have built our own retrievalprocessor and endowed it with the ability ?o act as a know-ledge-base by directly processing complex semantics ofEnglish modifiers (Ballard 1984).
In this manner, weavoid many of the awkward requirements of the typical"translation" process from formatted English (e.g.
parsestructures) to a formal query.
Thus, our translation proc-ess involves traversing the nodes of a parse structure in aprescribed order (e.g.
relative clauses are processed beforeadjectives, which are processed before ordinals andnumbers).
Translation is carried out recursively, startingwith a top-level noun phrase, and proceeding to embeddedphrases.
A complete discussion of the query languageproduced can be found in Ballard and Lusth 1984.Computational Linguistics Volume 10, Number 2, April-June 1984 95Bruce W. Ballard and Nancy L. Tinkham A Phrase-Structured Grammatical Framework for Transportable NLP6.3 Current s tatus  of the LDC sys temThe initial version of the knowledge acquisition module ofLDC was completed in the fall of 1982, and the separatelytested modules of the English-language processor and theretrieval module were integrated in May 1983 to form acomplete system.
Since that time, the system has been runby the system authors and gives a real-time response toeach input in under a minute while time-sharing on aheavily-loaded 16-bit PDP-11/70 minicomputer unningUNIX.
All coding of LDC has been done in a local dialectof LISP, except for the retrieval module which was firstwritten in Pascal and later re-written in C. Some of thedomains which we have been using to test the modules ofLDC, along with sample noun phrase inputs for thesedomains, were mentioned in Section 4.3.1.At present, most work with LDC will is being carriedout by the first author at AT&T Bell Laboratories, wherea conversion from Duke LISP to Franz LISP has enabledthe system to run on a VAX computer.
Both Duke andBell Labs have acquired Symbolics 3670 LISP machines,and a Lisp Machine version of the system, possibly result-ing from redesign as well as recoding, is likely.
In anyevent, the system is expected to undergo substantialenhancements in both syntactic and semantic coverageduring the coming months.ReferencesBallard, B.
1982 A "'domain class" approach to transportable naturallanguage processing.
Cognition and Brain Theory, 5 (3): 269-287.Ballard, B.
1984 The syntax and semantics of user-defined modifiers ina transportable natural anguage processor.
Proceedings of Coling84.Stanford University (July): 52-56.Ballard, B. and Biermann, A.
1979 Programming in naturallanguage: NLC as a prototype.
ACM National Conference, Detroit,Mich.
: 228-237.Ballard, B. and Lusth, J.
1983 An English-language processing systemthat "learns" about new domains.
National Computer Conference:1-46.Ballard, B. and Lusth, J.
1984 The design of DOMINO: a knowledge-based retrieval module for transportable natural anguage access topersonal databases.
Proceedings of the Workshop on Expert DatabaseSystems.
Kiawah Island, South Carolina.Ballard, B.; Lusth, J.; and Tinkham, N. 1984 LDC-I: a transportable,knowledge-based natural anguage processor for office environments.ACM Transactions on Office Information Systems, 2( I ): 1-25.Ballard, B.; Lusth, J.; and Tinkham, N. 1984 Transportable Englishlanguage processing for office environments.
National ComputerConference.Ballard, B. and Tinkham, N. 1983 A phrase-structured grammaticalformalism for transportable natural anguage processing.
TechnicalReport CS- 1983-4, Department ofComputer Science, Duke Universi-ty (April).Bates, M. and Bobrow, R. 1983 A transportable natural anguage inter-face for information retrieval.
Sixth Annual International ACMSIGIR Conference, Washington, D.C.Biermann, A.
1981 Natural Language Programming.
Nato AdvancedStudy Institute on Automatic Program Construction, Bonas, France(September 28 to October 10).Biermann, A. and Baltard, B.
1980 Toward natural anguage computa-tion.
American Journal of Computational Linguistics, 6(2): 71-86.Biermann, A.; Ballard, B.; and Sigmon, A.
1983 An experimental studyof natural anguage programming.
International Journal of Man-Ma-chine Studies 18 (1): 71-87.Biermann, A.; Rodman, R.; Ballard, B.; Betancourt, T.; Bilbro, G.; Deas,H.
; Fineman, L.; Fink, P.; Gilbert, K.; and Heidlage, F. 1983 Inter-active natural anguage processing: a pragmatic approach.
Confer-ence on Applied Natural Language Processing, Santa Monica, Ca.
:180-191.Bobrow, R. and Webber, B.
1980 Knowledge representation forsyntactic/semantic processing.
First Annual Conference on ArtificialIntelligence, Stanford University.Colmerauer, A.
1978 Metamorphosis grammars.
In Bolc, Ed., NaturalLanguage Communication with Computers.
Springer-Verlag.Fink, P. 1982 Conditionals in a natural anguage system.
Master'sthesis, Department ofComputer Science, Duke University.,Geist, R.; Kraines, D.; and Fink, P. 1982 Natural anguage con~putationin a linear algebra course.
National Educational Computer Confer-ence: 203-208.Ginsparg, J.
1983 A robust portable natural anguage data base inter-face.
Conference on Applied Natural Language Processing, SantaMonica, Ca.
: 25-30.Grosz, B.
1983 TEAM: A transportable natural anguage interfacesystem.
Conference on Applied Natural Language Processing, SantaMonica, Ca.
: 39-45.Haas, N. and Hendrix, G. 1980 An approach to acquiring and applyingknowledge.
First National Conference on Artificial Intelligence, Stan-ford University: 235-239.Heidorn, G. i975 Augmented phrase-structure g ammars.
In Webber,B.
and Schank, R., Eds., Theoretical Issues in Natural Language Proc-essing: 1-5.Hendrix, G. and Lewis, W. 1981 Transportable natural-language inter-faces to databases.
Proceedings of the 19th Annual Meeting of theA CL, Stanford University: 159-165.Kimball, J.
1972 Seven principles of surface structure parsing in naturallanguage.
Cognition 2 (1): 15-47.Marcus, M. 1980 A Theory of Syntactic Recognition for NaturalLanguage.
MIT Press, Cambridge, MA.Mark, W. 1981 Representation a d inference in the Consul system.International Joint Conference on Artificial Intelligence.Pereira, F. 1981 Extraposition grammars.
Americal Journal of Compu-tational Linguistics 7(4): 243-256.Pratt, V. 1975 LINGOL - A progress report.
International JointConference on Artificial Intelligence: 422-428.Rieger, C. and Small, S. 1979 Word expert parsing.
International JointConference on Artificial Intelligence: 723-728.Robinson, J.
1982 DIAGRAM: A grammar for dialogues.
Communi-cations of the ACM 25( I ): 27-47.Sager, N. 1981 Natural Language Information Processing: A ComputerGrammar of English and Its Applications.
Addison-Wesley.Sager, N. and Grishman, R. 1975 The restriction language for comput-er grammars of natural anguage.
Communications of the ACM 18:390-400.Sigmon, A.
1981 The semantics of looping structures in naturallanguage computation.
Master's thesis, Department of ComputerScience, Duke University.Slocum, J.
1981 A practical comparison of parsing strategies.
AnnualMeeting of the Assoc.
for Computational Linguistics, 1-6.Thompson, B. and Thompson, F. 1981 Shifting to a higher gear in anatural language system.
National Computer Conference: 657-662.Thompson, B. and Thompson, F. 1983 Introducing ASK, a simpleknowledgeable system.
Conference on Applied Natural LanguageProcessing, Santa Monica, CA: 17-24.Thompson, F. and Thompson, B.
1975 Practical natural anguage proc-essing: the REL system as prototype.
In Rubinoff, M. and Yovits,M., Eds., Advances m Computers, Vo!.
3, Academic Press.Wilczynski, D. 1981 Knowledge acquisition in the Consul system.International Joint Conference on Artificial Intelligence.Winograd, T. 1972 Understanding Natural Language.
Academic Press.Woods, W. 1970 Transition network grammars for natural languageanalysis.
Communications of the A CM, 13 ( I 0): 591-606.Woods, W. 1980 Cascaded ATN grammars.
American Journal ofComputational Linguistics 6( 1 ): 1 - 12.96 Computational Linguistics Volume 10, Number 2, April-June 1984
