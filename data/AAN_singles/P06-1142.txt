Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1129?1136,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning Transliteration Lexicons from the WebJin-Shea Kuo1, 21Chung-Hwa Telecom.Laboratories, Taiwanjskuo@cht.com.twHaizhou LiInstitute for InfocommResearch, Singaporehzli@ieee.orgYing-Kuei Yang22National Taiwan University ofScience and Technology, Taiwanykyang@mouse.ee.ntust.edu.twAbstractThis paper presents an adaptive learningframework for Phonetic SimilarityModeling (PSM) that supports theautomatic construction of transliterationlexicons.
The learning algorithm startswith minimum prior knowledge aboutmachine transliteration, and acquiresknowledge iteratively from the Web.
Westudy the active learning and theunsupervised learning strategies thatminimize human supervision in terms ofdata labeling.
The learning processrefines the PSM and constructs atransliteration lexicon at the same time.We evaluate the proposed PSM and itslearning algorithm through a series ofsystematic experiments, which show thatthe proposed framework is reliablyeffective on two independent databases.1 IntroductionIn applications such as cross-lingual informationretrieval (CLIR) and machine translation (MT),there is an increasing need to translate out-of-vocabulary (OOV) words, for example from analphabetical language to Chinese.
Foreign propernames constitute a good portion of OOV words,which are translated into Chinese throughtransliteration.
Transliteration is a process oftranslating a foreign word into a native languageby preserving its pronunciation in the originallanguage, otherwise known as translation-by-sound.MT and CLIR systems rely heavily onbilingual lexicons, which are typically compiledmanually.
However, in view of the currentinformation explosion, it is labor intensive, if notimpossible, to compile a complete proper nounslexicon.
The Web is growing at a fast pace and isproviding a live information source that is rich intransliterations.
This paper presents a novelsolution for automatically constructing anEnglish-Chinese transliteration lexicon from theWeb.Research on automatic transliteration hasreported promising results for regulartransliteration (Wan and Verspoor, 1998; Li et al2004), where transliterations follow rigidguidelines.
However, in Web publishing,translators in different countries and regions maynot observe common guidelines.
They oftenskew the transliterations in different ways tocreate special meanings to the sound equivalents,resulting in casual transliterations.
In this case,the common generative models (Li et al 2004)fail to predict the transliteration most of the time.For example, ?Coca Cola?
is transliterated into?
?
?
?
?
/Ke-Kou-Ke-Le/?
as a soundequivalent in Chinese, which literately means?happiness in the mouth?.
In this paper, we areinterested in constructing lexicons that coverboth regular and casual transliterations.When a new English word is first introduced,many transliterations are invented.
Most of themare casual transliterations because a regulartransliteration typically does not have manyvariations.
After a while, the transliterationsconverge into one or two popular ones.
Forexample, ?Taxi?
becomes ???
/Di-Shi/?
inChina and ?
?
?
/De-Shi/?
in Singapore.Therefore, the adequacy of a transliteration entrycould be judged by its popularity and itsconformity with the translation-by-soundprinciple.
In any case, the phonetic similarityshould serve as the primary basis of judgment.This paper is organized as follows.
In Section2, we briefly introduce prior works pertaining tomachine transliteration.
In Section 3, we proposea phonetic similarity model (PSM) forconfidence scoring of transliteration.
In Section 4,we propose an adaptive learning process forPSM modeling and lexicon construction.
InSection 5, we conduct experiments to evaluatedifferent adaptive learning strategies.
Finally, weconclude in Section 6.11292 Related WorkIn general, studies of transliteration fall into twocategories: transliteration modeling (TM) andextraction of transliteration pairs (EX) fromcorpora.The TM approach models phoneme-based orgrapheme-based mapping rules using agenerative model that is trained from a largebilingual lexicon, with the objective oftranslating unknown words on the fly.
Theefforts are centered on establishing the phoneticrelationship between transliteration pairs.
Mostof these works are devoted to phoneme1-basedtransliteration modeling (Wan and Verspoor1998, Knight and Graehl, 1998).
Suppose thatEW is an English word and CW is its prospectiveChinese transliteration.
The phoneme-basedapproach first converts EW into an intermediatephonemic representation P, and then converts thephonemic representation into its Chinesecounterpart CW.
In this way, EW and CW forman E-C transliteration pair.In this approach, we model the transliterationusing two conditional probabilities, P(CW|P) andP(P|EW), in a generative model P(CW|EW) =P(CW|P)P(P|EW).
Meng (2001) proposed a rule-based mapping approach.
Virga and Khudanpur(2003) and Kuo et al(2005) adopted the noisy-channel modeling framework.
Li et al(2004)took a different approach by introducing a jointsource-channel model for direct orthographymapping (DOM), which treats transliteration as astatistical machine translation problem undermonotonic constraints.
The DOM approach,which is a grapheme-based approach,significantly outperforms the phoneme-basedapproaches in regular transliterations.
It is notedthat the state-of-the-art accuracy reported by Liet al(2004) for regular transliterations of theXinhua database is about 70.1%, which leavesmuch room for improvement if one expects touse a generative model to construct a lexicon forcasual transliterations.EX research is motivated by informationretrieval techniques, where people attempt toextract transliteration pairs from corpora.
TheEX approach aims to construct a large and up-to-date transliteration lexicon from live corpora.Towards this objective, some have proposedextracting translation pairs from parallel orcomparable bitext using co-occurrence analysis1 Both phoneme and syllable based approaches are referredto as phoneme-based here.or a context-vector approach (Fung and Yee,1998; Nie et al 1999).
These methods comparethe semantic similarities between words withouttaking their phonetic similarities into accounts.Lee and Chang (2003) proposed using aprobabilistic model to identify E-C pairs fromaligned sentences using phonetic clues.
Lam et al(2004) proposed using semantic and phoneticclues to extract E-C pairs from comparablecorpora.
However, these approaches are subjectto the availability of parallel or comparablebitext.
A method that explores non-aligned textwas proposed by harvesting katakana-Englishpairs from query logs (Brill et al 2001).
It wasdiscovered that the unsupervised learning of sucha transliteration model could be overwhelmed bynoisy data, resulting in a decrease in modelaccuracy.Many efforts have been made in using Web-based resources for harvesting transliteration/translation pairs.
These include exploring querylogs (Brill et al 2001), unrelated corpus (Rapp,1999), and parallel or comparable corpus (Fungand Yee, 1998; Nie et al 1999; Huang et al2005).
To establish correspondence, thesealgorithms usually rely on one or more statisticalclues, such as the correlation between wordfrequencies, cognates of similar spelling orpronunciations.
They include two aspects.
First,a robust mechanism that establishes statisticalrelationships between bilingual words, such as aphonetic similarity model which is motivated bythe TM research; and second, an effectivelearning framework that is able to adaptivelydiscover new events from the Web.
In the priorwork, most of the phonetic similarity modelswere trained on a static lexicon.
In this paper, weaddress the EX problem by exploiting a novelWeb-based resource.
We also propose a phoneticsimilarity model that generates confidence scoresfor the validation of E-C pairs.In Chinese webpages, translated ortransliterated terms are frequently accompaniedby their original Latin words.
The latter serve asthe appositives of the former.
A sample searchresult for the query submission ?Kuro?
is thebilingual snippet2 ?...??
Kuro??
P2P??????????
?3 ???
P2P ???????????
C2C (Content to Community)...?.
Theco-occurrence statistics in such a snippet wasshown to be useful in constructing a transitivetranslation model (Lu et al 2002).
In the2 A bilingual snippet refers to a Chinese predominant textwith embedded English appositives.1130example above, ?Content to Community?
is not atransliteration of C2C, but rather an acronymexpansion, while ???
/Ku-Luo/?, as underlined,presents a transliteration for ?Kuro?.
What isimportant is that the E-C pairs are always closelycollocated.
Inspired by this observation, wepropose an algorithm that searches over the closecontext of an English word in a bilingual snippetfor the word?s transliteration candidates.The contributions of this paper include: (i) anapproach to harvesting real life E-Ctransliteration pairs from the Web; (ii) a phoneticsimilarity model that evaluates the confidence ofso extracted E-C pair candidates; (iii) acomparative study of several machine learningstrategies.3 Phonetic Similarity ModelEnglish and Chinese have different syllablestructures.
Chinese is a syllabic language whereeach Chinese character is a syllable in eitherconsonant-vowel (CV) or consonant-vowel-nasal(CVN) structure.
A Chinese word consists of asequence of characters, phonetically a sequenceof syllables.
Thus, in first E-C transliteration, itis a natural choice to syllabify an English wordby converting its phoneme sequence into asequence of Chinese-like syllables, and thenconvert it into a sequence of Chinese characters.There have been several effective algorithmsfor the syllabification of English words fortransliteration.
Typical syllabification algorithmsfirst convert English graphemes to phonemes,referred to as the letter-to-sound transformation,then syllabify the phoneme sequence into asyllable sequence.
For this method, a letter-to-sound conversion is needed (Pagel, 1998;Jurafsky, 2000).
The phoneme-basedsyllabification algorithm is referred to as PSA.Another syllabification technique attempts tomap the grapheme of an English word tosyllables directly (Kuo and Yang, 2004).
Thegrapheme-based syllabification algorithm isreferred to as GSA.
In general, the size of aphoneme inventory is smaller than that of agrapheme inventory.
The PSA therefore requiresless training data for statistical modeling (Knight,1998); on the other hand, the grapheme-basedmethod gets rid of the letter-to-sound conversion,which is one of the main causes of transliterationerrors (Li et al 2004).Assuming that Chinese transliterations alwaysco-occur in proximity to their original Englishwords, we propose a phonetic similaritymodeling (PSM) that measures the phoneticsimilarity between candidate transliteration pairs.In a bilingual snippet, when an English word EWis spotted, the method searches for the word?spossible Chinese transliteration CW in itsneighborhood.
EW can be a single word or aphrase of multiple English words.
Next, weformulate the PSM and the estimation of itsparameters.3.1 Generative ModelLet 1{ ,... ,... }m MES es es es= be a sequence ofEnglish syllables derived from EW, using thePSA or GSA approach, and 1{ ,... ,... }n NCS cs cs cs=be the sequence of Chinese syllables derivedfrom CW, represented by a Chinese characterstring 1,... ,...,n NCW c c c?
.
EW and CW is atransliteration pair.
The E-C transliteration canbe considered a generative process formulated bythe noisy channel model, with EW as the inputand CW as the output.
( / )P EW CW  is estimatedto characterize the noisy channel, known as thetransliteration probability.
( )P CW  is a languagemodel to characterize the source language.Applying Bayes?
rule, we have( / ) ( / ) ( ) / ( )P CW EW P EW CW P CW P EW=   (1)Following the translation-by-sound principle, thetransliteration probability ( / )P EW CW can beapproximated by the phonetic confusionprobability ( / )P ES CS , which is given as( / ) max ( , / ),P ES CS P ES CSD?F= D   (2)where F  is the set of all possible alignmentpaths between ES and CS.
It is not trivial to findthe best alignment path D .
One can resort to adynamic programming algorithm.
Assumingconditional independence of syllables in ES andCS, we have 1( / ) ( / )Mm mmP ES CS p es cs== ?
in aspecial case where M N= .
Note that, typically,we have N M?
due to syllable elision.
Weintroduce a null syllable j  and a dynamicwarping strategy to evaluate ( / )P ES CS  whenM N?
(Kuo et al 2005).
With the phoneticapproximation, Eq.
(1) can be rewritten as( / ) ( / ) ( ) / ( )P CW EW P ES CS P CW P EW?
(3)The language model in Eq.
(3) can berepresented by Chinese characters n-gramstatistics.1 2 11( ) ( / , ,..., )Nn n nnP CW p c c c c- -== ?
(4)1131In adopting bigram, Eq.
(4) is rewritten as1 12( ) ( ) ( / )Nn nnP CW p c p c c -=?
?
.
Note that thecontext of EW usually has a number ofcompeting Chinese transliteration candidates in aset, denoted as W .
We rank the candidates byEq.
(1) to find the most likely CW for a given EW.In this process, ( )P EW  can be ignored because itis the same for all CW candidates.
The CWcandidate that gives the highest posteriorprobability is considered the most probablecandidate CW ?
.arg max ( / )arg max ( / ) ( )CWCWCW P CW EWP ES CS P CW?W?W?
=?
(5)However, the most probable CW ?
isn?tnecessarily the desired transliteration.
The nextstep is to examine if CW ?
and EW indeed form agenuine E-C pair.
We define the confidence ofthe E-C pair as the posterior odds similar to thatin a hypothesis test under the Bayesianinterpretation.
We have 0H , which hypothesizesthat CW ?
and EW  form an E-C pair, and 1H ,which hypothesizes otherwise.
The posteriorodds is given as follows,01'( / ) ( / ') ( ')( / ) ( / ) ( )CWCW CWP H EW P ES CS P CWP H EW P ES CS P CWs?W?= ?
?
(6)where 'CS is the syllable sequence of CW ?
,1( / )p H EW  is approximated by the probabilitymass of the competing candidates of CW ?
,or'( / ) ( )CWCW CWP ES CS P CW?W??
.
The higher the sis, the more probable that hypothesis0H overtakes 1H .
The PSM formulation can beseen as an extension to prior work (Brill et al2001) in transliteration modeling.
We introducethe posterior odds s as the confidence score sothat E-C pairs that are extracted from differentcontexts can be directly compared.
In practice,we set a threshold for s  to decide a cutoff pointfor E-C pairs short-listing.3.2 PSM EstimationThe PSM parameters are estimated from thestatistics of a given transliteration lexicon, whichis a collection of manually selected E-C pairs insupervised learning, or a collection of highconfidence E-C pairs in unsupervised learning.An initial PSM is bootstrapped using priorknowledge such as rule-based syllable mapping.Then we align the E-C pairs with the PSM andderive syllable mapping statistics for PSA andGSA syllabifications.
A final PSM is a linearcombination of the PSA-based PSM (PSA-PSM)and the GSA-based PSM (GSA-PSM).
The PSMparameter ( / )m np es cs can be estimated by anExpectation-Maximization (EM) process(Dempster, 1977).
In the Expectation step, wecompute the counts of events such as# ,m nes cs< >  and # ncs< >  by force-aligning theE-C pairs in the training lexicon Y .
In theMaximization step, we estimate the PSMparameters ( / )m np es cs by( / ) # , /#m n m n np es cs es cs cs= < > < > .
(7)As the EM process guarantees non-decreasinglikelihood probability ( / )P ES CS"Y?
, we letthe EM process iterate until ( / )P ES CS"Y?converges.
The EM process can be thought of asa refining process to obtain the best alignmentbetween the E-C syllables and at the same time are-estimating process for PSM parameters.
It issummarized as follows.Start: Bootstrap PSM parameters( / )m np es cs using prior phonetic mappingknowledgeE-Step: Force-align corpus Y  using existing( / )m np es cs  and compute the counts of# ,m nes cs< >  and # ncs< > ;M-Step: Re-estimate ( / )m np es cs  using thecounts from E-Step.Iterate: Repeat E-Step and M-Step until( / )P ES CS"Y?
converges.4 Adaptive Learning FrameworkWe propose an adaptive learning frameworkunder which we learn PSM and harvest E-C pairsfrom the Web at the same time.
Conceptually,the adaptive learning is carried out as follows.We obtain bilingual snippets from the Web byiteratively submitting queries to the Web searchengines (Brin and Page, 1998).
For each batch ofquerying, the query results are all normalized toplain text, from which we further extractqualified sentences.
A qualified sentence has atleast one English word.
Under this criterion, acollection of qualified sentences can be extractedautomatically.
To label the E-C pairs, eachqualified sentence is manually checked based onthe following transliteration criteria: (i) if an EWis partly translated phonetically and partlytranslated semantically, only the phonetictransliteration constituent is extracted to form a1132transliteration pair; (ii) elision of English soundis accepted; (iii) multiple E-C pairs can appear inone sentence; (iv) an EW can have multiple validChinese transliterations and vice versa.
Thevalidation process results in a collection ofqualified E-C pairs, also referred to as DistinctQualified Transliteration Pairs (DQTPs).As formulated in Section 3, the PSM is trainedusing a training lexicon in a data driven manner.It is therefore very important to ensure that in thelearning process we have prepared a qualitytraining lexicon.
We establish a baseline systemusing supervised learning.
In this approach, weuse human labeled data to train a model.
Theadvantage is that it is able to establish a modelquickly as long as labeled data are available.However, this method also suffers from somepractical issues.
First, the derived model can onlybe as good as the data that it sees.
An adaptivemechanism is therefore needed for the model toacquire new knowledge from the dynamicallygrowing Web.
Second, a massive annotation ofdatabase is labor intensive, if not entirelyimpossible.To reduce the annotation needed, we discussthree adaptive strategies cast in the machinelearning framework, namely active learning,unsupervised learning and active-unsupervisedlearning.
The learning strategies can be depictedin Figure 1 with their difference being discussednext.
We also train a baseline system usingsupervised learning approach as a reference pointfor benchmarking purpose.4.1 Active LearningActive learning is based on the assumption that asmall number of labeled samples, which areDQTPs here, and a large number of unlabeledFigure 1.
An adaptive learning framework forautomatic construction of transliteration lexicon.samples are available.
This assumption is valid inmost NLP tasks.
In contrast to supervisedlearning, where the entire corpus is labeledmanually, active learning selects the most usefulsamples for labeling and adds the labeledexamples to the training set to retrain the model.This procedure is repeated until the modelachieves a certain level of performance.Practically, a batch of samples is selected eachtime.
This is called batch-based sample selection(Lewis and Catlett, 1994), as shown in the searchand ranking block in Figure 1.For an active learning to be effective, wepropose using three measures to select candidatesfor human labeling.
First, we would like to selectthe most uncertain samples that are potentiallyhighly informative for the PSM model.
Theinformativeness of a sample can be quantified byits confidence score s  as in the PSMformulation.
Ranking the E-C pairs by s  isreferred to as C-rank.
The samples of low C-rankare the interesting samples to be labeled.
Second,we would like to select candidates that are of lowfrequency.
Ranking by frequency is called F-rank.
During Web crawling, most of the searchengines use various strategies to preventspamming and one of fundamental tasks is toremove the duplicated Web pages.
Therefore, weassume that the bilingual snippets are all unique.Intuitively, E-C pairs of low frequency indicateuncommon events which are of higher interest tothe model.
Third, we would like to selectsamples upon which the PSA-PSM and GSA-PSM disagree the most.
The disagreed uponsamples represent new knowledge to the PSM.
Inshort, we select low C-rank, low F-rank andPSM-disagreed samples for labeling because thehigh C-rank, high F-rank and PSM-agreedsamples are already well known to the model.4.2 Unsupervised LearningUnsupervised learning skips the human labelingstep.
It minimizes human supervision byautomatically labeling the data.
This can beeffective if prior knowledge about a task isavailable, for example, if an initial PSM can bebuilt based on human crafted phonetic mappingrules.
This is entirely possible.
Kuo et al(2005)proposed using a cross-lingual phoneticconfusion matrix resulting from automaticspeech recognition to bootstrap an initial PSMmodel.
The task of labeling samples is basicallyto distinguish the qualified transliteration pairsfrom the rest.
Unlike the sample selectionmethod in active learning, here we would like toIterate StartFinalPSMInitialPSMSearch &RankingPSMLearningLexiconStopThe WebSelect &LabelingTrainingSamplesLabeledSamplesPSMEvaluation & StopCriterion1133select the samples that are of high C-rank andhigh F-rank because they are more likely to bethe desired transliteration pairs.The difference between the active learning andthe unsupervised learning strategies lies in thatthe former selects samples for human labeling,such as in the select & labeling block in Figure 1before passing on for PSM learning, while thelatter selects the samples automatically andassumes they are all correct DQTPs.
Thedisadvantage of unsupervised learning is that ittends to reinforce its existing knowledge ratherthan to discover new events.4.3 Active-Unsupervised LearningThe active learning and the unsupervisedlearning strategies can be complementary.
Activelearning minimizes the labeling effort byintelligently short-listing informative andrepresentative samples for labeling.
It makes surethat the PSM learns new and informativeknowledge over iterations.
Unsupervisedlearning effectively exploits the unlabelled data.It reinforces the knowledge that PSM hasacquired and allows PSM to adapt to changes atno cost.
However, we do not expectunsupervised learning to acquire new knowledgelike active learning does.
Intuitively, a bettersolution is to integrate the two strategies into one,referred to as the active-unsupervised learningstrategy.
In this strategy, we use active learningto select a small amount of informative andrepresentative samples for labeling.
At the sametime, we select samples of high confidence scorefrom the rest and consider them correct E-C pairs.We then merge the labeled set with the high-confidence set in the PSM re-training.5 ExperimentsWe first construct a development corpus bycrawling of webpages.
This corpus consists ofabout 500 MB of webpages, called SET1 (Kuo etal, 2005).
Out of 80,094 qualified sentences,8,898 DQTPs are manually extracted from SET1,which serve as the gold standard in testing.
Toestablish a baseline system, we first train a PSMusing all 8,898 DQTPs in supervised manner andconduct a closed test on SET1 as in Table 1.
Wefurther implement three PSM learning strategiesand conduct a systematic series of experiments.Precision Recall F-measureclosed-test 0.79 0.69 0.74Table 1.
Supervised learning test on SET15.1 Unsupervised LearningWe follow the formulation described inSection 4.2.
First, we derive an initial PSM usingrandomly selected 100 seed DQTPs and simulatethe Web-based learning process with the SET1:(i) select high F-rank and high C-rank E-C pairsusing PSM, (ii) add the selected E-C pairs to theDQTP pool as if they are true DQTPs, and (iii)reestimate PSM by using the updated DQTP pool.In Figure 2, we report the F-measure overiterations.
The U_HF curve reflects the learningprogress of using E-C pairs that occur more thanonce in the SET1 corpus (high F-rank).
TheU_HF_HR curve reflects the learning progressusing a subset of E-C pairs from U_HF whichhas high posterior odds as defined in Eq.
(6).Both selection strategies aim to select E-C pairs,which are as genuine as possible.00.10.20.30.40.50.60.70.81 2 3 4 5 6# IterationF-measure SupervisedU_HFU_HF_HRFigure 2.
F-measure over iterations forunsupervised learning on SET1.We found that both U_HF and U_HF_HR givesimilar results in terms of F-measure.
Withoutsurprise, more iterations don?t always lead tobetter performance because unsupervisedlearning doesn?t aim to acquiring new knowledgeover iterations.
Nevertheless, unsupervisedlearning improves the initial PSM in the firstiteration substantially.
It can serve as an effectivePSM adaptation method.5.2 Active LearningThe objective of active learning is to minimizehuman supervision by automatically selecting themost informative samples to be labeled.
Theeffect of active learning is that it maximizesperformance improvement with minimumannotation effort.
Like in unsupervised learning,we start with the same 100 seed DQTPs and aninitial PSM model and carry out experiments onSET1: (i) select low F-rank, low C-rank andGSA-PSM and PSA-PSM disagreed E-C pairs;(ii) label the selected pairs by removing the non-E-C pairs and add the labeled E-C pairs to theDQTP pool, and (iii) reestimate the PSM byusing the updated DQTP pool.1134To select the samples, we employ 3 differentstrategies: A_LF_LR, where we only select lowF-rank and low C-rank candidates for labeling.A_DIFF, where we only select those that GSA-PSM and PSA-PSM disagreed upon; andA_DIFF_LF_LR, the union of A_LF_LR andA_DIFF selections.
As shown in Figure 3, the F-measure of A_DIFF (0.729) andA_DIFF_LF_LR (0.731) approximate to that ofsupervised learning 0.735) after four iterations.00.10.20.30.40.50.60.70.81 2 3 4 5 6# IterationF-measure SupervisedA_LF_LRA_DIFFA_DIFF_LF_LRFigure 3.
F-measure over iterations for activelearning on SET1.With almost identical performance assupervised learning, the active learning approachhas greatly reduced the number of samples formanual labeling as reported in Table 2.
It isfound that for active learning to reach theperformance of supervised learning, A_DIFF isthe most effective strategy.
It reduces thelabeling effort by 89.0%, from 80,094 samples to8,750.Sample selection #samples labeledA_LF_LR 1,671A_DIFF 8,750 Active learning A_DIFF_LF_LR 9,683Supervised learning 80,094Table 2.
Number of total samples for manuallabeling in 6 iterations of Figure 3.5.3 Active Unsupervised LearningIt would be interesting to study the performanceof combining unsupervised learning and activelearning.
The experiment is similar to that ofactive learning except that, in step (iii) of activelearning, we take the unlabeled high confidencecandidates (high F-rank and high C-rank as inU_HF_HR of Section 5.1) as the true labeledsamples and add into the DQTP pool.
The resultis shown in Figure 4.
Although activeunsupervised learning was reported havingpromising results (Riccardi and Hakkani-Tur,2003) in some NLP tasks, it has not been aseffective as active learning alone in thisexperiment probably due to the fact theunlabeled high confidence candidates are still toonoisy to be informative.00.10.20.30.40.50.60.70.81 2 3 4 5 6# IterationF-measure SupervisedAU_LF_LRAU_DIFFAU_DIFF_LF_LRFigure 4.
F-measure over iterations for activeunsupervised learning on SET1.5.4 Learning Transliteration LexiconsThe ultimate objective of building a PSM is toextract a transliteration lexicon from the Web byiteratively submitting queries and harvesting newtransliteration pairs from the return results untilno more new pairs.
For example, by submitting?Robert?
to search engines, we may get ?Robert-???
?, ?Richard-???
and ?Charles-???
?in return.
In this way, new queries can begenerated iteratively, thus new pairs arediscovered.
We pick the best performing SET1-derived PSM trained using A_DIFF_LF_LRactive learning strategy and test it on a newdatabase SET2 which is obtained in the sameway as SET1.Before  adaptationAfteradaptation#distinct E-C pairs 137,711 130,456Precision 0.777 0.846#expected DQTPs 107,001 110,365Table 3.
SET1-derived PSM adapted towardsSET2.SET2 contains 67,944 Web pages amountingto 3.17 GB.
We extracted 2,122,026 qualifiedsentences from SET2.
Using the PSM, we extract137,711 distinct E-C pairs.
As the gold standardfor SET2 is unavailable, we randomly select1,000 pairs for manual checking.
A precision of0.777 is reported.
In this way, 107,001 DQTPscan be expected.
We further carry out oneiteration of unsupervised learning usingU_HF_HR to adapt the SET1-derived PSMtowards SET2.
The results before and afteradaptation are reported in Table 3.
Like theexperiment in Section 5.1, the unsupervisedlearning improves the PSM in terms of precisionsignificantly.11356 ConclusionsWe have proposed a framework for harvesting E-C transliteration lexicons from the Web usingbilingual snippets.
In this framework, weformulate the PSM learning and E-C pairevaluation methods.
We have studied threestrategies for PSM learning aiming at reducingthe human supervision.The experiments show that unsupervisedlearning is an effective way for rapid PSMadaptation while active learning is the mosteffective in achieving high performance.
We findthat the Web is a resourceful live corpus for reallife E-C transliteration lexicon learning,especially for casual transliterations.
In thispaper, we use two Web databases SET1 andSET2 for simplicity.
The proposed frameworkcan be easily extended to an incremental learningframework for live databases.
This paper hasfocused solely on use of phonetic clues forlexicon and PSM learning.
We have good reasonto expect the combining semantic and phoneticclues to improve the performance further.ReferencesE.
Brill, G. Kacmarcik, C. Brockett.
2001.Automatically Harvesting Katakana-English TermPairs from Search Engine Query Logs, In Proc.
ofNLPPRS, pp.
393-399.S.
Brin and L. Page.
1998.
The Anatomy of a Large-scale Hypertextual Web Search Engine, In Proc.
of7th WWW, pp.
107-117.A.
P. Dempster, N. M. Laird and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data viathe EM Algorithm, Journal of the Royal StatisticalSociety, Ser.
B. Vol.
39, pp.
1-38.P.
Fung and L.-Y.
Yee.
1998.
An IR Approach forTranslating New Words from Nonparallel,Comparable Texts.
In Proc.
of 17th COLING and36th ACL, pp.
414-420.F.
Huang, Y. Zhang and Stephan Vogel.
2005.
MiningKey Phrase Translations from Web Corpora.
InProc.
of HLT-EMNLP, pp.
483-490.D.
Jurafsky and J. H. Martin.
2000.
Speech andLanguage Processing, pp.
102-120, Prentice-Hall,New Jersey.K.
Knight and J. Graehl.
1998.
MachineTransliteration, Computational Linguistics, Vol.
24,No.
4, pp.
599-612.J.-S. Kuo and Y.-K. Yang.
2004.
ConstructingTransliterations Lexicons from Web Corpora, Inthe Companion Volume, 42nd ACL, pp.
102-105.J.-S. Kuo and Y.-K. Yang.
2005.
IncorporatingPronunciation Variation into Extraction ofTransliterated-term Pairs from Web Corpora, InProc.
of ICCC, pp.
131-138.C.-J.
Lee and J.-S. Chang.
2003.
Acquisition ofEnglish-Chinese Transliterated Word Pairs fromParallel-Aligned Texts Using a Statistical MachineTransliteration Model, In Proc.
of HLT-NAACLWorkshop Data Driven MT and Beyond, pp.
96-103.D.
D. Lewis and J. Catlett.
1994.
HeterogeneousUncertainty Sampling for Supervised Learning, InProc.
of ICML 1994, pp.
148-156.H.
Li, M. Zhang and J. Su.
2004.
A Joint SourceChannel Model for Machine Transliteration, InProc.
of 42nd ACL, pp.
159-166.W.
Lam, R.-Z.
Huang and P.-S. Cheung.
2004.Learning Phonetic Similarity for Matching NamedEntity Translations and Mining New Translations,In Proc.
of 27th ACM SIGIR, pp.
289-296.W.-H. Lu, L.-F. Chien and H.-J Lee.
2002.Translation of Web Queries Using Anchor TextMining, TALIP, Vol.
1, Issue 2, pp.
159- 172.H.
M. Meng, W.-K.
Lo, B. Chen and T. Tang.
2001.Generate Phonetic Cognates to Handle NameEntities in English-Chinese Cross-LanguageSpoken Document Retrieval, In Proc.
of ASRU, pp.311-314.J.-Y.
Nie, P. Isabelle, M. Simard, and R. Durand.1999.
Cross-language Information Retrieval basedon Parallel Texts and Automatic Mining of ParallelText from the Web?, In Proc.
of 22nd ACM SIGIR,pp 74-81.V.
Pagel, K. Lenzo and A.
Black.
1998.
Letter toSound Rules for Accented Lexicon Compression,In Proc.
of ICSLP, pp.
2015-2020.R.
Rapp.
1999.
Automatic Identification of WordTranslations from Unrelated English and GermanCorpora, In Proc.
of 37th ACL, pp.
519-526.G.
Riccardi and D. Hakkani-T?r.
2003.
Active andUnsupervised Learning for Automatic SpeechRecognition.
In Proc.
of 8th Eurospeech.P.
Virga and S. Khudanpur.
2003.
Transliteration ofProper Names in Cross-Lingual InformationRetrieval, In Proc.
of 41st ACL Workshop onMultilingual and Mixed Language Named EntityRecognition, pp.
57-64.S.
Wan and C. M. Verspoor.
1998.
AutomaticEnglish-Chinese Name Transliteration forDevelopment of Multilingual Resources, In Proc.
of17th COLING and 36th ACL, pp.1352-1356.1136
