Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 634?643,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsVector space semantics with frequency-driven motifsShashank SrivastavaCarnegie Mellon UniversityPittsburgh, PA 15217ssrivastava@cmu.eduEduard HovyCarnegie Mellon UniversityPittsburgh, PA 15217hovy@cmu.eduAbstractTraditional models of distributional se-mantics suffer from computational issuessuch as data sparsity for individual lex-emes and complexities of modeling se-mantic composition when dealing withstructures larger than single lexical items.In this work, we present a frequency-driven paradigm for robust distributionalsemantics in terms of semantically cohe-sive lineal constituents, or motifs.
Theframework subsumes issues such as dif-ferential compositional as well as non-compositional behavior of phrasal con-situents, and circumvents some problemsof data sparsity by design.
We designa segmentation model to optimally par-tition a sentence into lineal constituents,which can be used to define distributionalcontexts that are less noisy, semanticallymore interpretable, and linguistically dis-ambiguated.
Hellinger PCA embeddingslearnt using the framework show competi-tive results on empirical tasks.1 IntroductionMeaning in language is a confluence of experien-tially acquired semantics of words or multi-wordphrases, and their semantic composition to createnew meanings.
For instance, successfully inter-preting a sentence such asThe old senator kicked the bucket.requires the knowledge that the semantic conno-tations of ?kicking the bucket?
as a unit are thesame as those for ?dying?.
Short of explicit su-pervision, such semantic mappings must be in-ferred by a new language speaker through induc-tive mechanisms operating on observed linguis-tic usage.
This perspective of acquired meaningaligns with the ?meaning is usage?
adage, conso-nant with Wittgenstein?s view of semantics.
Atthe same time, the ability to adaptively commu-nicate elaborate meanings can only be conciledthrough Frege?s principle of compositionality, i.e.,meanings of larger linguistic constructs can bederived from the meanings of individual compo-nents, modulated by their syntactic interrelations.Indeed, most linguistic usage appears composi-tional.
This is supported by the fact even withvery limited vocabulary, children and non-nativespeakers can often communicate surprisingly ef-fectively.It can be argued that to be sustainable, induc-tive aspects of meaning must be recurrent enoughto be learnable by new users.
That is, a non-compositional phrase such as ?kick the bucket?
islikely to persist in common parlance only if it isfrequently used with its associated semantic map-ping.
If a usage-driven meaning of a motif is notrecurrent enough, learning this mapping is inef-ficient in two ways.
First, the sparseness of ob-servations would severely limit accurate inductiveacquisition by new observers.
Second, the valueof learning a very infrequent semantic mappingis likely marginal.
This motivates the need fora frequency-driven view of lexical semantics.
Inparticular, such a perspective can be especiallyadvantageous for distributional semantics for rea-sons we outline below.Distributional semantic models (DSMs) thatrepresent words as distributions over neighbouringcontexts have been particularly effective in captur-ing fine-grained lexical semantics (Turney et al,2010).
Such models have engendered improve-ments in diverse applications such as selectionalpreference modeling (Erk, 2007), word-sense dis-crimination (McCarthy and Carroll, 2003), auto-matic dictionary building (Curran, 2003), and in-formation retrieval (Manning et al, 2008).
How-ever, while conventional DSMs consider colloca-634With the bad press in wake of the financial crisis, businesses are leaving our shores .crisis: <bad, businesses, financial, leaving, press, shores, wake>financial crisis: <bad press, businesses, in wake of, leaving our shores>Table 1: Meaning representation by conventional DSMs vs notional idealtion strengths (through counts and PMI scores) ofword neighbourhoods, they disregard much of theregularity in human language.
Most significantly,word tokens that act as latent dimensions are of-ten derived from arbitrary tokenization.
The ex-ample given in Table 1 succinctly describes this.The first row in the table shows a representationof the meaning of the token ?crisis?
that a conven-tional DSM might extract from the given sentenceafter stopword removal.
While helpful, the repre-sentation seems unsatisfying since words such as?press?, ?wake?
and ?shores?
seem to have little todo with a crisis.
From a semantic perspective, arepresentation similar to the second is more valu-able: not only does it represent a semantic map-ping for a more specific meaning, but the latent di-mensions of the representation have are less noisy(e.g., while ?wake?
is semantically ambiguous, itssurrounding context in ?in wake of?
disambiguatesit) and more intuitive in regards of semantic in-terepretability.
This is the overarching theme ofthis work: we present a frequency driven paradigmfor extending distributional semantics to phrasaland sentential levels in terms of such semanticallycohesive, recurrent lexical units or motifs.We propose to identify such semanticallycohesive motifs in terms of features inspiredfrom frequency-characteristics, linguistic idiosyn-crasies, and shallow syntactic analysis; and ex-plore both supervised and semi-supervised mod-els to optimally segment a sentence into such mo-tifs.
Through exploiting regularities in languageusage, the framework can efficiently account forboth compositional and non-compositional wordusage, while avoiding the issue of data-sparsity bydesign.
Our principal contributions in this paperare:?
We present a framework for extending dis-tributional semantics to learn semantic repre-sentations of both words and phrases in termsof recurrent motifs, rather than arbitrary wordtokens?
We present a simple model to segment a sen-tence into such motifs using a feature-setdrawing from frequency statistics, informa-tion theory, linguistic theories and shallowsyntactic analysis?
Word and phrasal representations learntthrough the approach outperform conven-tional DSM representations on empiricaltasksThis paper is organized as follows: In Sec-tion 2, we briefly review related work in the do-main of compositional distributional semantics,and motivate our formulation.
Section 3 describesour methodology, which consists of a frequency-driven segmentation model to partition text intosemantically meaningful recurring lineal-subunits,a representation learning framework for learningnew semantic embeddings based on this segmen-tation, and an approach to use such embeddings indownstream applications.
We present experimentsand empirical evaluations for our method in Sec-tion 4.
Finally, we conclude in Section 5 with asummary of our principal findings, and a discus-sion of possible directions for future work.2 Related WorkWhile DSMs have been valuable in representingsemantics of single words, approaches to extendthem to represent the semantics of phrases andsentences has met with only marginal success.While there is considerable variety in approachesand formulations, existing approaches for phrasallevel and sentential semantics can broadly be par-titioned into two categories.2.1 Compositional approachesThese have aimed at using semantic representa-tions for individual words to learn semantic rep-resentations for larger linguistic structures.
Thesemethods implicitly make an assumption of com-positionality, and often include explicit computa-tional models of compositionality.
Notable amongsuch models are the additive and multiplicativemodels of composition by Mitchell and Lapata(2008), Grefenstette et al (2010), Baroni and635Zamparelli?s (2010) model that differentially mod-els content and function words for semantic com-position, and Goyal et al?s SDSM model (2013)that incorporates syntactic roles to model seman-tic composition.
Notable among the most effec-tive distributional representations are the recentdeep-learning approaches by Socher et al (2012),that model vector composition through non-lineartransformations.
While word embeddings and lan-guage models from such methods have been use-ful for tasks such as relation classification, polaritydetection, event coreference and parsing; much ofexisting literature on composition is based on ab-stract linguistic theory and conjecture, and thereis little evidence to support that learnt represen-tations for larger linguistic units correspond totheir semantic meanings.
While works such asthe SDSM model suffer from the problem of spar-sity in composing structures beyond bigrams andtrigrams, methods such as Mitchell and Lapata(2008)and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by signifi-cant model biases in representing semantic com-position by generic algebraic operations.
Finally,the assumption that semantic meanings for sen-tences could have representations similar to thosefor smaller individual tokens is in some sense un-intuitive, and not supported by linguistic or seman-tic theories.2.2 Tree kernelsTree Kernel methods have gained popularity inthe last decade for capturing syntactic informationin the structure of parse trees (Collins and Duffy,2002; Moschitti, 2006).
Instead of procuring ex-plicit representations, the kernel paradigm directlyfocuses on the larger goal of quantifying semanticsimilarity of larger linguistic units.
Structural ker-nels for NLP are based on matching substructureswithin two parse trees , consisting of word-nodeswith similar labels.
These methods have been use-ful for eclectic tasks such as parsing, NER, se-mantic role labeling, and sentiment analysis.
Re-cent approaches such as by Croce et al (2011)and Srivastava et al (2013) have attempted to pro-vide formulations to incorporate semantics intotree kernels through the use of distributional wordvectors at the individual word-nodes.
While thisframework is attractive in the lack of assumptionson representation that it makes, the use of distri-butional embeddings for individual tokens meansthat it suffers from the same shortcomings as de-scribed for the example in Table 1, and hence thesemethods model semantic relations between word-nodes very weakly.
Figure 1 shows an example ofthe shortcomings of this general approach.Figure 1: Tokenwise syntactic and semantic simi-larities don?t imply sentential semantic similarityWhile the two sentences in consideration havenear-identical syntax and could be argued to havesemantically aligned words in similar positions,the semantics of the complete sentences are widelydivergent.
Specifically, the ?bag of words?
as-sumption in tree kernels doesn?t suffice for theselexemes, and a stronger semantic model is neededto capture phrasal semantics as well as diverginginter-word relations such as in ?coffee table?
and?water table?.
Our hypothesis is that a model thatcan even weakly identify recurrent motifs such as?water table?
or ?breaking a fall?
would be help-ful in building more effective semantic represen-tations.
A significant advantage of a frequencydriven view is that it makes the concern of com-positionality of recurrent phrases immaterial.
If amotif occurs frequently enough in common par-lance, its semantics could be captured with distri-butional models irrespective of whether its associ-ated semantics are compositional or acquired.2.3 Identifying multi-word expressionsSeveral approaches have focused on supervisedidentification of multi-word expressions (MWEs)through statistical (Pecina, 2008; Villavicencio etal., 2007) and linguistically motivated (Piao et al,2005) techniques.
More recently, hybrid methodsbased on both statistical as well as linguistic fea-tures have been popular (Tsvetkov and Wintner,2011).
Ramisch et al (2008) demonstrate thatadding part-of-speech tags to frequency countssubstantially improves performance.
Other meth-ods have attempted to exploit morphological, syn-tactic and semantic characteristics of MWEs.
In636particular, approaches such as Bannard (2007) usesyntactic rigidity to characterize MWEs.
Whileexisting work has focused on the classificationtask of categorizing a phrasal constituent as aMWE or a non-MWE, the general ideas of mostof these works are in line with our current frame-work, and the feature-set for our motif segmen-tation model is designed to subsume most ofthese ideas.
It is worthwhile to point out thatthe task of motif segmentation is slightly differ-ent from MWE identification.
Specifically, theonus on recurrent occurrences means that non-decomposibility is not an essential considerationfor a word to be considered a motif.
In line withthe proposed paradigm, typical MWEs such as?shoot the breeze?, ?sour note?
and ?hot dog?
wouldbe considered valid lineal motifs.1In addition,even decomposable recurrent lineal phrases suchas ?love story?, ?federal government?, and ?mil-lions of people?
are marked as meaningful recur-rent motifs.
Finally, and least interestingly, weinclude common named entities such as ?UnitedStates?
and ?Java Virtual Machine?
within the am-bit of motifs.3 MethodIn this section, we define our frequency-drivenframework for distributional semantics in detail.As just described above, our definition for motifsis less specific than MWEs.
With such a workingdefinition, contiguous motifs are likely to makedistributional representations less noisy and alsoassist in disambiguating context.
Also, the lack ofspecificity ensures that such motifs are commonenough to meaningfully influence distributionalrepresentation beyond single tokens.
A methodtowards frequency-driven distributional semanticscould involve the following principal components:3.1 Linear segmentation modelThe segmentation model forms the core of theframework.
Ideally, it fragments a given sen-tence into non-overlapping, semantically mean-ingful, empirically frequent contiguous sub-unitsor motifs.
The model accounts for possible seg-mentations of a sentence into potential motifs, andprefers recurrent and cohesive motifs through fea-tures that capture frequency-based and statistical1We note that since we take motifs as lineal units,the current method doesn?t subsume several common non-contiguous MWEs such as ?let off?
in ?let him off?.features, as well as linguistic idiosyncracies.
Thisis accomplished using a very simple linear chainmodel and a rich feature set consisting of a combi-nation of frequency-driven, information theoreticand linguistically motivated features.Let an observed sentence be denoted by x, withthe individual tokens xidenoting the i?th token inthe sentence.
The segmentation model is a chainLVM (latent variable model) that aims to maxi-mize a linear objective defined by:J =?iwifi(yk, yk?1,x)where fiare arbitrary Markov features that candepend on segments (potential motifs) of the ob-served sentence x, and contiguous latent states.The features are chosen so as to best representfrequency-based, statistical as well as linguisticconsiderations for treating a segment as an ag-glutinative unit, or a motif.
In specific, thesefeatures could encode characteristics such as fre-quency statistics, collocation strengths and syn-tactic distinctness, or inflectional rigidity of theconsidered segments; described in detail in Sec-tion 3.2.
The model is an instantiation of a sim-ple featurized HMM, and the weighted sum of fea-tures corresponding to a segment is cognate withan affinity score for the ?stickiness?
of the segment,i.e., the affinity for the segment to be treated asholistic unit or a single motif.We also associate a penalizing cost for each nonunary-motif to avoid aggressive agglutination oftokens.
In particular, for an ngram occurrence tobe considered a motif, the marginal contributiondue to the affinity of the prospective motif shouldat minimum exceed this penalty.
The weights forthe affinity functions as well as these penalties arelearnt from data using full as well as partial anno-tations.
The latent state-variables ykdenotes themembership of the token xkto a unary or a largermotif; and the state-sequence collectively givesthe segmentation of the sentence.
An individualstate-variable ykencodes a pairing of the size ofthe encompassing ngram motif, and the positionof the word xkwithin it.
For instance, yk= T3denotes that the token xkis the final position in atrigram motif.3.1.1 Inference of optimal segmentationIf the optimal weights wiare known, inferencefor the best motif segmentation can be performed637in linear time (in the number of tokens) follow-ing the generalized Viterbi algorithm.
A slightlymodified version of Viterbi could also be used tofind segmentations that are constrained to agreewith some given motif boundaries, but can seg-ment other parts of the sentence optimally underthese constraints.
This is necessary for the sce-nario of semi-supervised learning of weights withpartially annotated sentences, as described later.3.2 Learning motif affinities and penaltiesWe briefly discuss data-driven learning of weightsfor features that define the motif affinity scoresand penalties.
We describe learning of the modelparameters with fully annotated training data, aswell as an approach for learning motif segmenta-tion that requires only partial supervision.Supervised learning: In the supervised case, op-timal state sequences y(k)are fully observed forthe training set.
For this purpose, we created adataset of 1000 sentences from the Simple En-glish Wikipedia and the Gigaword Corpus, andmanually annotated it with motif boundaries us-ing BRAT (Stenetorp et al, 2012).
In this case,learning can follow the online structured percep-tron learning procedure by Collins (2002), whereweights updates for the k?th training example(x(k),y(k)) are given as:wi?
wi+ ?(fi(x(k),y(k))?
fi(x(k),y?
))Here y?= Decode(x(k),w) is the optimalViterbi decoding using the current estimates ofthe weights.
Updates are run for a large numberof iterations until the change in objective dropsbelow a threshold, and the learning rate ?
isadaptively modified as described in Collins et alImplicitly, the weight learning algorithm can beseen as a gradient descent procedure minimizingthe difference between the scores of highestscoring (Viterbi) state sequences, and the labelstate sequences.Semi-supervised learning: In the semi-supervised case, the labels y(k)iare knownonly for some of the tokens in x(k).
This is acommonplace scenario, where a part of a sentencehas clear motif-boundaries, whereas the rest of thesentence is not annotated.
For accumulating suchdata, we looked for occurrences of 2500 expres-sions from the WikiMWE dataset in sentencesfrom the combined Simple English Wikipediaand Gigaword corpora.
The query expressions inthe retrieved sentences were marked with motifboundaries, while the remaining tokens in thesentences were left unannotated.While the Viterbi algorithm can be used for tag-ging optimal state-sequences given the weights,the structured perceptron can learn optimal modelweights given gold-standard sequence labels.Hence, in this case, we use a variation of the hardEM algorithm for learning.
The algorithm pro-ceeds as follows: in the E-step, we use the currentvalues of weights to compute hard-expectations,i.e., the best scoring Viterbi sequences amongthose consistent with the observed state labels.
Inthe M-step, we take the decoded state-sequencesin the E-step as observed, and run perceptronlearning to update feature weightswi.
Pseudocodeof the learning algorithm for the partially labeledcase is given in Algorithm 1.Algorithm 11: Input: Partially labeled data D = {(x, y)i}2: Output: Weights w3: Initialization: Set wirandomly, ?i4: for i : 1 to maxIter do5: Decode D with current w to find optimalViterbi paths that agree with (partial) groundtruths.6: Run Structured Perceptron algorithm with de-coded tag-sequences to update weights w7: end for8: return wThe semi-supervised approach enables incor-poration of significantly more training data.
Inparticular, this method could be used in conjunc-tion with a supervised approach.
This would in-volve initializing the weights prior to the semi-supervised procedure with the weights from thesupervised learning model, so as to seed the semi-supervised approach with reasonable model, anduse the partially annotated data to fine-tune the su-pervised model.
The sequential approach, akin toannealing weights, can efficiently utilize both fulland partial annotations.3.2.1 Feature engineeringIn this section, we describe the principal featuresused in the segmentation modelTransitional features and penalties:?
Transitional features ftrans(yi?1, yi) =638Iyi?1,yi2describing the transitional affinitiesof state pairs.
Since our state definitions pre-clude certain transitions (such as from stateT2to T1), these weights are initialized to?
?to expedite training.?
N-gram penalties: fngramWe define apenalty for tagging each non-unary motif asdescribed before.
For a motif to be tagged,the improvement in objective score should atleast exceed the corresponding penalty.
e.g.,fqgram(yi) = Iyi=Q4denotes the penalty fortagging a tetragram.3Frequency-based, information theoretic, and POSfeatures:?
Absolute and log-normalized motif frequen-cies fngram(xi?n+1, ...xi?1, xi, yi).
Thisfeature is associated with a particular token-sequence and ngram-tag, and takes thevalue of the motif-frequency if the motiftoken-sequence matches the feature token-sequence, and is marked as with a match-ing tag.
e.g., fbgram(xi?1= love, xi=story, yi= B2).?
Absolute and log-normalized motif frequen-cies for a particular POS-sequence.
Thisfeature is associated with a particular POS-tag sequence and ngram-tag, and takes thevalue of the motif-frequency if the motiftoken-sequence gets a matching tag, and ismarked as with a matching ngram tag.
e.g.,fbgram(pi?1= V B, pi= NN, yi= B2).?
Medians and maxima of pairwise collocationstatistics for tokens for a particular size ofngram motifs: we use the following statis-tics: pointwise mutual information, Chi-square statistic, and conditional probability.We also used POS sensitive versions of these,which performed much better than plain ver-sions in our evaluations.?
Histogram counts of inflectional forms of to-ken sequence for the corresponding ngrammotif and POS sequence: this features takesthe value of the count of inflectional formsof an ngram that account for 90% of occur-rences of all inflectional forms.2Here, I denotes the indicator function3It is straightforward to preclude partial n-gram annota-tions near sentence boundaries with prohibitive penalties.?
Entropies of histogram distributions of inflec-tional variants (described above).?
Features encoding syntactic rigidity: ratiosand log-ratios of frequencies of an ngram mo-tif and variations by replacing a token usingnear synonyms from its synset.Additionally, a few feature for the segmenta-tions model contained minor orthographic featuresbased on word shape (length and capitalizationpatterns).
Also, all numbers, URLs, and cur-rency symbols were normalized to the special NU-MERIC, URL, and CURRENCY tokens respec-tively.
Finally, a gazetteer feature checked for oc-currences of motifs in a gazetteer of named enti-ties.3.3 Representation learningWith the segmentation model described in the pre-vious section, we process text from the English Gi-gaword corpus and the Simple English Wikipediato partition sentences into motifs.
Since the seg-mentation model accounts for the contexts of theentire sentence in determining motifs, different in-stances of the same token could evoke differentmeaning representations.
Consider the followingsentences tagged by the segmentation model, thatwould correspond to different representations ofthe token ?remains?
: once as a standalone motif,and once as part of an encompassing bigram motif(?remains classified?
).Hog prices have declined sharply , while thecost of corn remains relatively high.Even with the release of such documents, ques-tions are not answered, since only the agencyknows what remains classifiedGiven constituent motifs of each sentence in thedata, we can now define neighbourhood distribu-tions for unary or phrasal motifs in terms of othermotifs (as envisioned in Table 1).
In our experi-ments, we use a window-length of 5 adjoining mo-tifs on either side to define the neighbourhood ofa constituent.
Naturally, in the presence of multi-word motifs, the neighbourhood boundary couldbe more extended than in a conventional DSM.With such neighbourhood contexts, the distri-butional paradigm posits that semantic similaritybetween a pair of motifs can be given by a senseof ?distance?
between the two distributions.
Mostpopularly, traditional measures of vector distance639such as the cosine similarity, Euclidean distanceand City-block distance have been used in sev-eral distributional approaches.
Additionally, sev-eral distance measures between discrete distribu-tions exist in statistical literature, most famouslythe Kullback Leibler divergence, Bhattacharyyadistance and the Hellinger distance.
Recent work(Lebret and Lebret, 2013) has shown that theHellinger distance is an especially effective mea-sure in learning distributional embeddings, withHellinger PCA being much more computationallyinexpensive than neural language modeling ap-proaches, while performing much better than stan-dard PCA, and competitive with the state-of-the-art in downstream evaluations.
Hence, we use theHellinger measure between neighbourhood motifdistributions in learning representations.The Hellinger distance between two categoricaldistributions P = (p1...pk) and Q = (q1...qk) isdefined as:H(P,Q) =1?2????k?i=1(?pi??qi)2=1?2???
?P ??Q??
?2The Hellinger measure has intuitively desir-able properties: specifically, it can be seenas the Euclidean distance between the square-roots transformed distributions, where both vec-tors?P and?Q are length-normalized under thesame(Euclidean) norm.
Finally, we perform SVDon the motif similarity matrix (with size of the or-der of the total vocabulary in the corpus), and re-tain the first k principal eigenvectors to obtain low-dimensional vector representations that are moreconvenient to work with.
In our preliminary ex-periments, we found that k = 300 gave quanti-tatively good results, with marginal change withadded dimensionality.
We use this setting for allour experiments.4 ExperimentsIn this section, we describe some experimentalevaluations and findings for our approach.
We firstquantitatively and qualitatively analyze the perfor-mance of the segmentation model, and then evalu-ate the distributional motif representations learntby the model through two downstream applica-tions.4.1 Motif segmentationIn an evaluation of the motif segmentations modelwithin the perspective of our framework, we be-lieve that exact correspondence to human judg-ment is unrealistic, since guiding principles fordefining motifs, such as semantic cohesion, arehard to define and only serve as working princi-ples.
However, for purposes of relative compar-ison, we quantitatively evaluate the performanceof the motif segmentation models on the fully an-notated dataset.
For this experiment, the gold-annotated corpus was split into a training and testsets in a 9:1 proportion.
A small fraction of thetraining split was set apart for development andvalidation.
For this evaluation, we considered amotif boundary as correct only for an exact match,i.e., when both its boundaries (left and right) werecorrectly predicted.
Also, since a majority of mo-tifs are unary tokens, including them into consider-ation artificially boosts the accuracy, whereas weare more interested in the prediction of larger n-gram tokens.
Hence we report results on the per-formance on only non-unary motifs.P R FRule-based baseline 0.85 0.10 0.18Supervised 0.62 0.28 0.39Semi-supervised 0.30 0.17 0.22Supervised + annealing 0.69 0.38 0.49Table 2: Results for motif segmentationsTable 2 shows the performance of the segmen-tation model with the three proposed learning ap-proaches described earlier.
For a baseline, weconsider a rule-based model that simply learns allngram segmentations seen in the training data, andmarks any occurrence of a matching token se-quence as a motif; without taking neighbouringcontext into account.
We observe that this modelhas a very high precision (since many token se-quences marked as motifs would recur in simi-lar contexts, and would thus have the same mo-tif boundaries).
However, the rule-based methodhas a very row recall due to lack of generaliza-tion capabilities.
We see that while all three learn-ing algorithms perform better than the baseline,the performance of the purely unsupervised sys-tem is inferior to supervised approaches.
This isnot unexpected: the supervision provided to themodel is very weak due to a lack of negative ex-amples (which leads to spurious motif taggings,640While men often (openly or privately) sympathized with Prince Charles when the princess went publicabout her rotten marriage , women cheered her on.The healthcare initiative has become a White elephant for the federal government.Chirac and Juppe have made a bad situation worse by seeking to meet Maastricht criteria not bycutting spending, but by raising taxes still further.Now , say Vatican observers , Pope John Paul II wants to show the world that many church membersdid resist the Third Reich and paid the price.Table 3: Examples of output from sentence segmentation modelleading to a low precision), as well as no examplesof transitions between adjacent motifs (to learntransitional weights and penalties).
The super-vised model expectedly outperforms both the rule-based and the semi-supervised systems.
However,the supervised learning model with subsequent an-nealing outperforms the supervised model in termsof both precision and recall; showing the utility ofthe semi-supervised method when seeded with agood initial model, and the additive value of par-tially labeled data.Qualitative analysis of motif-segmented sen-tences shows that our designed feature-set is effec-tive and helpful in identifying semantically cohe-sive ngrams.
Table 3 provides four examples.
Thefirst example correctly identifies ?went public?,while missing out on the potential motif ?cheeredher on?.
In general, these examples illustrate thatthe model can identify idiomatic and idiosyncraticthemes as well as commonly recurrent ngrams (inthe second example, the model picks out ?has be-come?
which is highly recurrent, but doesn?t havethe semantic cohesiveness of some of the othermotifs).
In particular, consider the second exam-ple, where the model picks ?white elephant?
as amotif.
In such cases, the disambiguating influenceof context incorporated by the motif is apparent.Elephant White elephanttusks expensivetrunk spendafrican biggestwhite the projectindian very highbaby multibillion dollarThe above table shows some of the top resultsfor the unary token ?elephant?
by frequency, andfrequent unary and non-unary motifs for the mo-tif ?white elephant?
retrieved by the segmentationmodel.4.2 Distributional representationsFor evaluating distributional representations formotifs (in terms of other motifs) learnt by theframework, we test these representations in twodownstream tasks: sentence polarity classificationand metaphor detection.
For sentence polarity, weconsider the Cornell Sentence Polarity corpus byPang and Lee (2005), where the task is to classifythe polarity of a sentence as positive or negative.The data consists of 10662 sentences from moviereviews that have been annotated as either posi-tive or negative.
For composing the motifs repre-sentations to get judgments on semantic similarityof sentences, we use our recent Vector Tree Ker-nel approach The VTK approach defines a convo-lutional kernel over graphs defined by the depen-dency parses of sentences, using a vector repre-sentation at each graph node that representing asingle lexical token.
For our purposes, we mod-ify the approach to merge the nodes of all tokensthat constitute a motif occurrence, and use the mo-tif representation as the vector associated with thenode.
Table 4 shows results for the sentence polar-ity task.P R F1DSM 0.56 0.50 0.53AVM 0.55 0.53 0.54MVM 0.55 0.49 0.52VTK 0.65 0.58 0.62VTK + MotifDSM 0.66 0.60 0.63Table 4: Results for Sentence Polarity detectionFor this task, the motif based distributional em-beddings vastly outperform a conventional distri-butional model (DSM) based on token distribu-tions, as well as additive (AVM) and multiplica-tive (MVM) models of vector compositionality, as641proposed by Lapata et al The model is compet-itive with the state-of-the-art VTK (Srivastava etal., 2013) that uses the SENNA neural embeddingsby Collobert et al (2011).P R F1CRF 0.74 0.50 0.59SVM+DSM 0.63 0.80 0.71VTK+ SENNA 0.67 0.87 0.76VTK+ MotifDSM 0.70 0.87 0.78Table 5: Results for Metaphor identificationOn the metaphor detection task, we use theMetaphor dataset (Hovy et al, 2013).
The dataconsists of sentences with defined phrases, andthe task consists of identifying the linguistic usein these phrases as metaphorical or literal.
Forthis task, the motif based model is expected toperform well as common metaphorical usage isgenerally through idiosyncratic MWEs, which themotif based models is specially geared to capturethrough the features of the segmentation model.For this task, we again use the VTK formalismfor combining vector representations of the indi-vidual motifs.
Table 5 shows that the motif-basedDSM does better than discriminative models suchas CRFs and SVMs, and also slightly improves onthe VTK kernel with distributional embeddings.5 ConclusionWe have presented a new frequency-driven frame-work for distributional semantics of not only lex-ical items but also longer cohesive motifs.
Thetheme of this work is a general paradigm of seek-ing motifs that are recurrent in common parlance,are semantically coherent, and are possibly non-compositional.
Such a framework for distribu-tional models avoids the issue of data sparsityin learning of representations for larger linguis-tic structures.
The approach depends on drawingfeatures from frequency statistics, statistical cor-relations, and linguistic theories; and this workprovides a computational framework to jointlymodel recurrence and semantic cohesiveness ofmotifs through compositional penalties and affin-ity scores in a data driven way.While being deliberately vague in our work-ing definition of motifs, we have presented simpleefficient formulations to extract such motifs thatuses both annotated as well as partially unanno-tated data.
The qualitative and quantitative analyisof results from our preliminary motif segmenta-tion model indicate that such motifs can help todisambiguate contexts of single tokens, and pro-vide cleaner, more interpretable representations.Finally, we obtain motif representations in formof low-dimensional vector-space embeddings, andour experimental findings indicate value of thelearnt representations in downstream applications.We believe that the approach has considerable the-oretical as well as practical merits, and provides asimple and clean formulation for modeling phrasaland sentential semantics.In particular, we believe that ours is the firstmethod that can invoke different meaning repre-sentations for a token depending on textual contextof the sentence.
The flexibility of having separaterepresentations to model different semantic senseshas considerable valuable, as compared with ex-tant approaches that assign a single representationto each token, and are hence constrained to con-flate several semantic senses into a common repre-sentation.
The approach also elegantly deals withthe problematic issue of differential compositionaland non-compositional usage of words.
Futurework can focus on a more thorough quantitativeevaluation of the paradigm, as well as extension tomodel non-contiguous motifs.ReferencesColin Bannard.
2007.
A measure of syntactic flexibil-ity for automatically identifying multiword expres-sions in corpora.
In Proceedings of the Workshopon a Broader Perspective on Multiword Expressions,pages 1?8.
Association for Computational Linguis-tics.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193.
Association for Computational Linguis-tics.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of the 40th annual meeting on associationfor computational linguistics, pages 263?270.
Asso-ciation for Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Association for Computational Linguistics.642Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Danilo Croce, Alessandro Moschitti, and RobertoBasili.
2011.
Structured lexical similarity via con-volution kernels on dependency trees.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, pages 1034?1046.
Asso-ciation for Computational Linguistics.James Richard Curran.
2003.
From Distributional toSemantic Similarity.
Ph.D. thesis, Institute for Com-municating and Collaborative Systems School of In-formatics University of Edinburgh.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In ACL.Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-maya Sachan, Shashank Srivastava, and EduardHovy.
2013.
A structured distributional semanticmodel: Integrating structure with semantics.
ACL2013, page 20.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, pages 1394?1404.
Asso-ciation for Computational Linguistics.Edward Grefenstette, Mehrnoosh Sadrzadeh, StephenClark, Bob Coecke, and Stephen Pulman.
2010.Concrete sentence spaces for compositional dis-tributional models of meaning.
arXiv preprintarXiv:1101.0309.Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-ney Sanders, and Eduard Hovy.
2013.
Identifyingmetaphorical word use with tree kernels.
Meta4NLP2013, page 52.R?emi Lebret and Ronan Lebret.
2013.
Wordemdeddings through hellinger pca.
arXiv preprintarXiv:1312.5542.Christopher D Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to informationretrieval, volume 1.
Cambridge University PressCambridge.Diana McCarthy and John Carroll.
2003.
Disam-biguating nouns, verbs, and adjectives using auto-matically acquired selectional preferences.
Compu-tational Linguistics, 29(4):639?654.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In ACL, pages236?244.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Machine Learning: ECML 2006, pages 318?329.Springer.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In ACL.Pavel Pecina.
2008.
A machine learning approach tomultiword expression extraction.
In Proceedings ofthe LREC MWE 2008 Workshop, pages 54?57.
Cite-seer.Scott Songlin Piao, Paul Rayson, Dawn Archer, andTony McEnery.
2005.
Comparing and combininga semantic tagger and a statistical tool for mwe ex-traction.
Computer Speech & Language, 19(4):378?397.Carlos Ramisch, Paulo Schreiner, Marco Idiart, andAline Villavicencio.
2008.
An evaluation of meth-ods for the extraction of multiword expressions.In Proceedings of the LREC Workshop-Towardsa Shared Task for Multiword Expressions (MWE2008), pages 50?53.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.
Association for Computational Linguis-tics.Shashank Srivastava, Dirk Hovy, and Eduard H. Hovy.2013.
A walk-based semantically enriched treekernel over distributed word representations.
InEMNLP, pages 1411?1416.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012.
Brat: a web-based tool for nlp-assistedtext annotation.
In Proceedings of the Demonstra-tions at the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 102?107.
Association for Computational Lin-guistics.Yulia Tsvetkov and Shuly Wintner.
2011.
Identifica-tion of multi-word expressions by combining mul-tiple linguistic information sources.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 836?845.
Associationfor Computational Linguistics.Peter D Turney, Patrick Pantel, et al 2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37(1):141?188.Aline Villavicencio, Valia Kordoni, Yi Zhang, MarcoIdiart, and Carlos Ramisch.
2007.
Validation andevaluation of automatically acquired multiword ex-pressions for grammar engineering.
In EMNLP-CoNLL, pages 1034?1043.643
