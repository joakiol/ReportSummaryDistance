Proceedings of the Linguistic Annotation Workshop, pages 9?16,Prague, June 2007. c?2007 Association for Computational LinguisticsEfficient Annotation with the Jena ANnotation Environment (JANE)Katrin Tomanek Joachim Wermter Udo HahnJena University Language & Information Engineering (JULIE) LabFu?rstengraben 30D-07743 Jena, Germany{tomanek|wermter|hahn}@coling-uni-jena.deAbstractWith ever-increasing demands on the diver-sity of annotations of language data, theneed arises to reduce the amount of effortsinvolved in generating such value-added lan-guage resources.
We introduce here the JenaANnotation Environment (JANE), a platformthat supports the complete annotation life-cycle and allows for ?focused?
annotationbased on active learning.
The focus we pro-vide yields significant savings in annotationefforts by presenting only informative itemsto the annotator.
We report on our experi-ence with this approach through simulatedand real-world annotations in the domain ofimmunogenetics for NE annotations.1 IntroductionThe remarkable success of machine-learning meth-ods for NLP has created, for supervised approachesat least, a profound need for annotated language cor-pora.
Annotation of language resources, however,has become a bottleneck since it is performed, withsome automatic support (pre-annotation) though, byhumans.
Hence, annotation is a time-costly anderror-prone process.The demands for annotated language data is in-creasing at different levels.
After the success in syn-tactic (Penn TreeBank (Marcus et al, 1993)) andpropositional encodings (Penn PropBank (Palmer etal., 2005)), more sophisticated semantic data (suchas temporal (Pustejovsky et al, 2003) or opinion an-notations (Wiebe et al, 2005)) and discourse data(e.g., for anaphora resolution (van Deemter and Kib-ble, 2000) and rhetorical parsing (Carlson et al,2003)) are being generated.
Once the ubiquitousarea of newswire articles is left behind, different do-mains (e.g., the life sciences (Ohta et al, 2002)) areyet another major concern.
Furthermore, any newHLT application (e.g., information extraction, doc-ument summarization) makes it necessary to pro-vide appropriate human annotation products.
Be-sides these considerations, the whole field of non-English languages is desperately seeking to enterinto enormous annotation efforts, at virtually all en-coding levels, to keep track of methodological re-quirements imposed by such resource-intensive re-search activities.Given this enormous need for high-quality anno-tations at virtually all levels the question turns uphow to minimize efforts within an acceptable qual-ity window.
Currently, for most tasks several hun-dreds of thousands of text tokens (ranging between200,000 to 500,000 text tokens) have to be scruti-nized unless valid tagging judgments can be learned.While significant time savings have already been re-ported on the basis of automatic pre-tagging (e.g.,for POS and parse tree taggings in the Penn Tree-Bank (Marcus et al, 1993), or named entity taggingsfor the Genia corpus (Ohta et al, 2002)), this kind ofpre-processing does not reduce the number of texttokens actually to be considered.We have developed the Jena ANnotation Environ-ment (JANE) that allows to reduce annotation ef-forts by means of the active learning (AL) approach.Unlike random or sequential sampling of linguisticitems to be annotated, AL is an intelligent selective9sampling strategy that helps reduce the amount ofdata to be annotated substantially at almost no lossin annotation effectiveness.
This is achieved by fo-cusing on those items particularly relevant for thelearning process.In Section 2, we review approaches to annota-tion cost reduction.
We turn in Section 3 to the de-scription of JANE, our AL-based annotation system,while in Section 4 we report on the experience wemade using the AL component in NE annotations.2 Related WorkReduction of efforts for training (semi-) supervisedlearners on annotated language data has always beenan issue of concern.
Semi-supervised learning pro-vides methods to bootstrap annotated corpora from asmall number of manually labeled examples.
How-ever, it has been shown (Pierce and Cardie, 2001)that semi-supervised learning is brittle for NLP taskswhere typically large amounts of high quality anno-tations are needed to train appropriate classifiers.Another approach to reducing the human labelingeffort is active learning (AL) where the learner hasdirect influence on the examples to be manually la-beled.
In such a setting, those examples are takenfor annotation which are assumed to be maximallyuseful for (classifier) training.
AL approaches havealready been tried for different NLP tasks (Engelsonand Dagan, 1996; Hwa, 2000; Ngai and Yarowsky,2000), though such studies usually report on simula-tions rather than on concrete experience with AL forreal annotation efforts.
In their study on AL for basenoun phrase chunking, Ngai and Yarowsky (2000)compare the costs of rule-writing with (AL-driven)annotation to compile a base noun phrase chunker.They conclude that one should rather invest humanlabor in annotation than in rule writing.Closer to our concerns is the study by Hachey etal.
(2005) who apply AL to named entity (NE) an-notation.
There are some differences in the actualAL approach they chose, while their main idea, viz.to apply committee-based AL to speed up real anno-tations, is comparable to our work.
They report onnegative side effects of AL on the annotations andstate that AL annotations are cognitively more diffi-cult for the annotators to deal with (because the sen-tences selected for annotation are more complex).As a consequence, diminished annotation qualityand higher per-sentence annotation times arise intheir experiments.
By and large, however, they con-clude that AL selection should still be favored overrandom selection because the negative implicationsof AL are easily over-compensated by the signifi-cant reduction of sentences to be annotated to yieldcomparable classifier performance as under randomsampling conditions.Whereas Hatchey et al focus only on one groupof entity mentions (viz.
four entity subclasses of theastrophysics domain), we report on broader experi-ence when applying AL to annotate several groupsof entity mentions in biomedical subdomains.
Wealso address practical aspects as to how create theseed set for the first AL round and how one mightestimate the efficiency of AL.
The immense sav-ings in annotation effort we achieve here (up to75%) may mainly depend on the sparseness of manyentity types in biomedical corpora.
Furthermore,we here present a general annotation environmentwhich supports AL-driven annotations for most seg-mentation problems, not just for NE recognition.In contrast, annotation editors, such as e.g.
Word-Freak1, typically offer facilities for supervised cor-rection of automatically annotated text.
This, how-ever, is very different from the AL approach.3 JANE ?
Jena ANnotation EnvironmentJANE, the Jena ANnotation Environment, supportsthe whole annotation life-cycle including the com-pilation of annotation projects, annotation itself (viaan external editor), monitoring, and the deploy-ment of annotated material.
In JANE, an annota-tion project consists of a collection of documentsto be annotated, an associated annotation schema?
a specification of what has to be annotated inwhich way, according to the accompanying annota-tion guidelines ?
a set of configuration parameters,and an annotator assigned to it.We distinguish two kinds of annotation projects:A default project, on the one hand, contains a prede-fined and fixed collection of naturally occurring doc-uments which the annotator handles independentlyof each other.
In an active learning project, on theother hand, the annotator has access to exactly one1http://wordfreak.sourceforge.net10(AL-computed pseudo) document at a time.
Aftersuch a document has completely been annotated, anew one is dynamically constructed which containsthose sentences for annotation which are the mostinformative ones for training a classifier.
Besidesannotators who actually do the annotation, thereare administrators who are in charge of (annota-tion) project management, monitoring the annota-tion progress, and deployment, i.e., exporting thedata to other formats.JANE consists of one central component, the an-notation repository, where all annotation and projectdata is stored centrally, two user interfaces, namelyone for the annotators and one for the administra-tor, and the active learning component which inter-actively generates documents to speed up the anno-tation process.
All components communicate withthe annotation repository through a network socket?
allowing JANE to be run in a distributed envi-ronment.
JANE is largely platform-independent be-cause all components are implemented in Java.
Atest version of JANE may be obtained from http://www.julielab.de.3.1 Active Learning ComponentOne of the most established approaches to activelearning is based on the idea to build an ensembleof classifiers from the already annotated examples.Each classifier then makes its prediction on all unla-beled exampels.
Examples on which the classifiersin the ensemble disagree most in their predictionsare considered informative and are thus requestedfor labeling.
Obviously, we can expect that addingthese examples to the training corpus will increasethe accuracy of a classifier trained on this data (Se-ung et al, 1992).
A common metric to estimatethe disagreement within an ensemble is the so-calledvote entropy, the entropy of the distribution of labelsli assigned to an example e by the ensemble of kclassifiers (Engelson and Dagan, 1996):D(e) = ?
1log k?liV (li, e)k logV (li, e)kOur AL component employs such an ensemble-based approach (Tomanek et al, 2007).
The ensem-ble consists of k = 3 classifiers2.
AL is run on the2Currently, we incorporate as classifiers Naive Bayes, Max-imum Entropy, and Conditional Random Fields.sentence level because this is a natural unit for manysegmentation tasks.
In each round, b sentences withthe highest disagreement are selected.3 The pool of(available) unlabeled examples can be very large formany NLP tasks; for NE annotations in the biomedi-cal domain we typically download several hundredsof thousands of abstracts from PUBMED.4 In or-der to avoid high selection times, we consider onlya (random) subsample of the pool of unlabeled ex-amples in each AL round.
Both the selection size b(which we normally set to b = 30), the compositionof the ensemble, and the subsampling ratio can beconfigured with the administration component.AL selects single, non-contiguous sentences fromdifferent documents.
Since the context of these sen-tences is still crucial for many (semantic) annota-tion decisions, for each selected sentence its origi-nal context is added (but blocked from annotation).When AL selection is finished, a new document iscompiled from these sentences (including their con-texts) and uploaded to the annotation repository.
Theannotator can then proceed with annotation.Although optimized for NE annotations, the ALcomponent may ?
after minor modifications of thefeature sets being used by the classifiers ?
also be ap-plied to other segmentation problems, such as POSor chunk annotations.3.2 Administration ComponentAdministering large-scale annotation projects is achallenging management task for which we supplya GUI (Figure 1) to support the following tasks:User Management Create accounts for adminis-trators and annotators.Creation of Projects The creation of an annota-tion project requires a considerable number of doc-uments and other files (such as annotation schemadefinitions) to be uploaded to the annotation reposi-tory.
Furthermore, several parameters, especially forAL projects have to be set appropriately.Editing a Project The administrator can reset aproject (especially when guidelines change, one3Here, the vote entropy is calculated separately for each to-ken.
The sentence-level vote entropy is then the average overthe respective token sequence.4http://www.ncbi.nlm.nih.gov/11Figure 1: Administration GUI: frame in foreground shows actions that can be performed on an AL project.might want to start the annotation process anew,i.e., delete all previous annotations but keep the restof the project unchanged), delete a project, copy aproject (which is helpful when several annotators la-bel the same documents to check the applicability ofthe guidelines by inter-annotator agreement calcula-tion), and change several AL-specific settings.Monitoring the Annotation Process The admin-istrator can check which documents of an annotationproject have already been annotated, how long anno-tation took on the average, when an annotator loggedin last time, etc.
Furthermore, the progress of ALprojects can be visualized by learning and disagree-ment curves and an enumeration of the number of(unique) entities found so far.Inter-Annotator Agreement For related projects(projects sharing the same annotation schema anddocuments to be annotated) the degree to whichseveral annotators mutually agree in their annota-tions can be calculated.
Such an inter-annotatoragreement (IAA) is common to estimate the qualityand applicability of particular annotation guidelines(Kim and Tsujii, 2006).
Currently, several IAA met-rics of different strictness for NE annotations (andother segmentation tasks) are incorporated.Deployment The annotation repository stores theannotations in a specific XML format (see Sec-tion 3.3).
For deployment, the annotations may beneeded in a different format.
Currently, the admin-istration GUI basically supports export into the IOBformat.
Only documents marked by the annotatorsas ?completely annotated?
are considered.3.3 Annotation ComponentAs the annotators are rather domain experts (in ourcase graduate students of biology or related life sci-ences) than computer specialists, we wanted to makelife for them as easy as possible.
Hence, we pro-vide a separate GUI for the annotators.
After log-inthe annotator is given an overview of his/her annota-tion projects along with a short description.
Doubleclicking on a project, the annotators get a list withall documents in this project.
Documents have dif-ferent flags (raw, in progress, done) to indicate thecurrent annotation state as set by each annotator.Annotation itself is done with MMAX, an externalannotation editor (Mu?ller and Strube, 2003), whichcan be customized with respect to the particular an-notation schema.
The document to be annotated, theannotations, and the configuration parameters arestored in separate XML files.
Our annotation repos-itory reflects this MMAX-specific data structure.Double clicking on a specific document directlyopens MMAX for annotation.
During annotation,the annotation GUI is locked to ensure data in-12tegrity.
When working on an AL project, the anno-tator can start the AL selection process (which thenruns on a separate high-performance machine) afterhaving finished the annotation of the current docu-ment.
During the AL selection process (it usuallytakes up to several minutes) the current project isblocked.
However, meanwhile the annotator can goon annotating other projects.3.4 Annotation RepositoryThe annotation repository is the heart of our annota-tion environment.
All project, user, and annotationrelevant data is stored here centrally.
This is a cru-cial design criterion because it lets the administratoraccess (e.g., for backup or deployment) all annota-tions from one central site.
Furthermore, the anno-tators do not have to care about how to shift the an-notated documents to the managerial staff.
All stateinformation related to the entire annotation cycle isrecorded and kept centrally in this repository.The repository is realized as a relational database5reflecting largely the data structure of MMAX.
Both,the GUIs and the AL component, communicate withthe repository via the JDBC network driver.
Thus,each component can be run on a different machineas long as it has a network connection to the annota-tion repository.
This has two main advantages: First,annotators can work remotely (e.g., from home orfrom a physically dislocated lab).
Second, resource-intensive tasks, e.g., AL selection, can be run on sep-arate machines to which the annotators normally donot have access.
The components communicate witheach other only through the annotation repository.
Inparticular, there is no direct communication betweenthe annotation GUI and the AL component.4 Experience with Real-World AnnotationsWe are currently conducting NE annotations fortwo large-scale information extraction and seman-tic retrieval projects.
Both tasks cover two non-overlapping biomedical subdomains, viz.
one in thefield of hematopoietic stem cell transplantation (im-munogenetics), the other in the area of gene regu-lation.
Entity types of interest are, e.g., cytokinesand their receptors, antigens, antibodies, immune5We chose MYSQL, a fast and reliable open source databasewith native Java driver supportcells, variation events, chemicals, blood diseases,etc.
In this section, we report on our actual ex-perience and findings in annotating entity mentions(drawing mainly on our work in the immunogeneticssubdomain) with JANE, with a focus on methodolog-ical issues related to active learning.In the biomedical domain, there is a vast amountof unlabeled material available for almost any topicof interest.
The most prominent source is probablyPUBMED, a literature database which currently in-cludes over 16 million citations, mostly abstracts,from MEDLINE and other life science sources.
Weused MESH terms6 and publication date ranges7 toselect relevant documents from the immunogenet-ics subdomain.
Thus, we retrieved about 200,000abstracts (?
2,000,000 sentences) as our documentpool of unlabeled examples for immunogenetics.Through random subsampling, only about 40,000sentences are considered for AL selection.For several of our entity annotations, we did bothan active learning (AL) annotation and a gold stan-dard (GS) annotation.
The latter is performed inthe default project mode on 250 abstracts randomlychosen from the entire document pool.
We askeddifferent annotators to annotate the same (subset ofthe) GS to calculate inter-annotator agreement in or-der to make sure that our annotation guidelines werenon-ambiguous.
Furthermore, as the annotation pro-ceeds, we regularly train a classifier on the AL an-notations and evaluate it against the GS annotations.From this learning curve, we can estimate the poten-tial gain of further AL annotation rounds and decidewhen to stop AL annotation.4.1 Reduction of Annotation Effort through ALIn real-world AL annotation projects, the amount ofcost reduction is hard to estimate properly.
We havethus extensively simulated and tested the gain in thereduction of annotation costs of our AL componenton available entity annotations of the biomedical do-main (GENIA8 and PENNBIOIE9) and the general-6MESH (http://www.nlm.nih.gov/mesh/) is theU.S.
National Library of Medicine?s controlled vocabulary usedfor indexing PUBMED articles.7Typically, articles published before 1990 are not consideredto contain relevant information for molecular biology.8http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/9http://bioie.ldc.upenn.edu/130.30.40.50.60.70.80.90  1000  2000  3000  4000  5000  6000  7000  8000F-scoresentencesreduction of annotation costs (75%)AL selectionrandom selectionFigure 2: Learning curves for AL and random selec-tion on variation event entity mentions.language newspaper domain (English data set of theCoNLL-2003 shared task (Tjong Kim Sang and DeMeulder, 2003)).
As a metric for annotation costswe here consider the number of sentences to be an-notated such that a certain F-score is reached withour NE tagger.10 We therefore compare the learningcurves of AL and random selection.
On almost ev-ery scenario, we found that AL yields cost savingsof about 50%, sometimes even up to 75%.As an example, we report on our AL simula-tion on the PENNBIOIE corpus for variation events.These entity mentions include the following six sub-classes: type, event, original state, altered state,generic state, and location.
The learning curvesfor AL and random selection are shown in Figure2.
Using random sampling, an F-score of 80% isreached by random selection after ?
8,000 sentences(200,000 tokens).
In contrast, AL selection yieldsthe same F-score after ?
2,000 sentences (46,000tokens).
This amounts to a reduction of annotationcosts on the order of 75%.Our real-world annotations revealed that AL isespecially beneficial when entity mentions are verysparsely distributed in the texts.
After an initializa-tion phase needed by AL to take off (which can con-siderably be accelerated when one carefully selectsthe sentences of the first AL round, see Section 4.2),AL selects, by and large, only sentences which con-tain at least one entity mention of the type of inter-10The named enatity tagger used throughout in this sectionis based on Conditional Random Fields and similar to the onepresented by (Settles, 2004).050010001500200025003000200  400  600  800  1000  1200  1400  1600  1800  2000entitymentionssentencesAL annotationGS annotationFigure 3: Cumulated entity density on AL and GSannotations of cytokine receptors.est.
In contrast, random selection (or in real anno-tation projects: sequential annotations of abstractsas in our default project mode), may lead to lots ofnegative training examples with no entity mentionsof interest.
When there is no simulation data at hand,the entity density of AL annotations (compared withthe respective GS annotation) is a good estimate ofthe effectiveness of AL.Figure 3 depicts such a cumulated entity densityplot on AL and GS annotations of subtypes of cy-tokine receptors, really very sparse entity types withone entity mention per PUBMED abstract on the av-erage.
The 250 abstracts of the GS annotation onlycontain 193 cytokine receptor entity mentions.
ALannotation of the same number of sentences resultedin 2,800 annotated entity mentions of this type.
Theentity density in our AL corpus is thus almost 15times higher than in our GS corpus.
Such a densecorpus is certainly much more appropriate for clas-sifier training due to the tremendous increase of pos-itive training instances.
We observed comparable ef-fects with other entity types as well, and thus con-clude that the sparser entity mentions of a specifictype are in texts, the more benefical AL-based anno-tation actually is.4.2 Mind the Seed SetFor AL, the sentences to be annotated in the first ALround, the seed set, have to be manually selected.
Asstated above, the proper choice of this set is crucialfor efficient AL based annotation.
One should def-initely refrain from a randomly generated seed set14?
especially, when sparse entity mentions are anno-tated ?
because it might take quite a while for AL totake off.
If, in the worst case, the seed set containsno entity mentions of interest, AL based annotationresembles (for several rounds in the beginning untilincidentally some entity mentions are found) a ran-dom selection ?
which is, as shown in Section 4.1,suboptimal.
Figure 4 shows the simulated effect ofthree different seed sets on variation event annota-tion (PENNBIOIE).
In the tuned seed set, each sen-tence contains at least one variation entity mention.On this seed, AL performs significantly better thanthe randomly assembled seed or the seed with no en-tity mentions at all.
Of course, in the long run, thethree curves converge.
Given this evidence, we stip-ulate that the sparser an entity type is11 or the largerthe document pool to be selected from is, the laterthe point of convergence and, thus, the more rele-vant an effective seed set is.We developed a useful three-step heuristic tocompile effective seed sets without excessive man-ual work.
In the first step, a list is compiledcomprised of as many entity mentions (of inter-est to the current annotation project) as possible.In knowledge- and expert-intensive domains suchas molecular biology, this can either be done byconsulting a domain expert or by harvesting entitymentions from online resources (such as biologicaldatabases).12 In a second step, the compiled listis matched against each sentence of the documentpool.
Third, a ranking procedure orders the sen-tences (in descending order) according to the num-ber of diverse matches of entity mentions.
This en-sures that textual mentions of all items from the listare included in the seed set.
Depending on the vari-ety and density of the specific entity types, our seedsets typically consist of 200 to 500 sentences.4.3 Portability of CorporaWhile we are working in the field of immunogenet-ics, the PENNBIOIE corpus focuses on the subdo-main of oncogenetics and provides a sound annota-11Variation events are not as sparse in PENNBIOIE as, e.g.,cytokine receptors in our subdomain.
Actually, there is a varia-tion entity in almost every second sentence.12In an additional step, some spelling variations of such en-tity mentions could automatically be generated.0.30.40.50.60.70.80  100  200  300  400  500F-scoresentencesrandom seed settuned seed setseed set with no entitiesFigure 4: Effect of different seed sets for AL on vari-ation event annotation.tion of these entity mentions (PBVAR).13 We did aGS annotation on 250 randomly chosen abstracts (?2,000 sentences/65,000 tokens) from our documentpool applying PENNBIOIE?s annotation guidelinesfor variation events to the subdomain of immuno-genetics (IMVAR-Gold).
We then evaluated howwell our entity tagger trained on PBVAR would doon this data.
Surprisingly, the performance was dra-matically low, viz.
31.2% F-score.14Thus, we did further variation event annotationsfor the immunogenetics domain with AL: We anno-tated ?
58,000 tokens (IMVAR-AL).
We trained ourentity tagger on this data and evaluated the tagger onboth IMVAR-Gold and PBVAR.
Table 1 summarizesthe results.
We conclude that porting training cor-pora, even from one related subdomain into another,is only possible to a very limited extent.
This may bebecause current NE taggers (ours, as well) make ex-tensive use of lexical features.
However, the resultsalso reveal that annotations made by AL may bemore robust when ported to another domain: a tag-ger trained on IMVAR-AL still yields about 62.5%F-score on PBVAR, whereas training the tagger onthe respective GS annotation (IMVAR-Gold), onlyabout half the performance is yielded (35.8%).13Although oncogenetics and immunogenetics are differentsubdomains, they share topical overlaps ?
in particular, withrespect to the types of relevant variation entity mentions (suchas ?single nucleotide polymorphism?, ?translocation?, ?in-framedeletion?, ?substitution?, etc.).
Hence, at least at this level thetwo subdomains are related.14Note that in a 10-fold cross-validation on PBVAR our entitytagger yielded about 80% F-score.15evaluation datatraining data PBVAR IMVAR-GoldPBVAR(?
200.000 tokens) ?
80% 31.2%IMVAR-AL(58.251 tokens) 62.5% 70.2%IMVAR-Gold(63.591 tokens) 35.8% ?Table 1: Corpus portability: PENNBIOIE?s variationentity annotations (PBVAR) vs. ours for immuno-genetics (IMVAR-AL and -Gold).5 Conclusion and Future WorkWe introduced JANE, an annotation environmentwhich supports the whole annotation life-cycle fromannotation project compilation to annotation deploy-ment.
As one of its major contributions, JANE al-lows for focused annotation based on active learn-ing, i.e., it automatically presents sentences for an-notation which are of most use for classifier training.We have shown that porting annotated trainingcorpora, even from one subdomain to another andthus related to a good extent, may severely degradeclassifier performance.
Thus, generating new an-notation data will increasingly become important,especially under the prospect that there are moreand more real-world information extraction projectsfor different (sub)domains and languages.
We haveshown that focused, i.e., AL-driven, annotation is areasonable choice to significantly reduce the effortneeded to create such annotations ?
up to 75% in arealistic setting.
Furthermore, we have highlightedthe positive effects of a high-quality seed set for ALand outlined a general heuristic for its compilation.At the moment, the AL component may be usedfor most kinds of segmentation problems (e.g.
POStagging, text chunking, entity recognition).
Futurework will focus on the extension of the AL compo-nent for relation encoding as required for corefer-ences or role and propositional information.AcknowledgementsWe thank Alexander Klaue for implementing theGUIs.
This research was funded by the EC withinthe BOOTStrep project (FP6-028099), and by theGerman Ministry of Education and Research withinthe StemNet project (01DS001A to 1C).ReferencesLynn Carlson, Daniel Marcu, and Mary E. Okurowski.
2003.Building a discourse-tagged corpus in the framework ofRhetorical Structure Theory.
In J. van Kuppevelt and R.Smith, editors, Current Directions in Discourse and Dia-logue, pp.
85?112.
Kluwer.Sean Engelson and Ido Dagan.
1996.
Minimizing manual an-notation cost in supervised training from corpora.
In Proc.of ACL 1996, pp.
319?326.B.
Hachey, B. Alex, and M. Becker.
2005.
Investigating theeffects of selective sampling on the annotation task.
In Proc.of CoNLL-2005, pp.
144?151.Rebecca Hwa.
2000.
Sample selection for statistical grammarinduction.
In Proc.
of EMNLP/VLC-2000, pp.
45?52.Jin-Dong Kim and Jun?ichi Tsujii.
2006.
Corpora and theirannotation.
In S. Ananiadou and J. McNaught, editors, TextMining for Biology and Biomedicine, pp.
179?211.
Artech.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PENNTREEBANK.
Computational Linguistics, 19(2):313?330.C.
Mu?ller and M. Strube.
2003.
Multi-level annotation inMMAX.
In Proc.
of the 4th SIGdial Workshop on Discourseand Dialogue, pp.
198?207.Grace Ngai and David Yarowsky.
2000.
Rule writing or an-notation: Cost-efficient resource usage for base noun phrasechunking.
In Proc.
of ACL 2000, pp.
117?125.Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim.
2002.
The GE-NIA corpus: An annotated research abstract corpus in molec-ular biology domain.
In Proc.
of HLT 2002, pp.
82?86.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semantic roles.Computational Linguistics, 31(1):71?106.David Pierce and Claire Cardie.
2001.
Limitations of co-training for natural language learning from large datasets.
InProc.
of EMNLP 2001, pp.
1?9.James Pustejovsky, Patrick Hanks, Roser Saur?
?, Andrew See,Robert Gaizauskas, Andrea Setzer, Dragomir Radev, BethSundheim, David Day, Lisa Ferro, and Marcia Lazo.
2003.The TIMEBANK corpus.
In Proc.
of the Corpus Linguistics2003 Conference, pp.
647?656.Burr Settles.
2004.
Biomedical named entity recognition usingconditional random fields and rich feature sets.
In Proc.
ofJNLPBA 2004, pp.
107?110.H.
Sebastian Seung, Manfred Opper, and Haim Sompolinsky.1992.
Query by committee.
In Proc.
of COLT 1992, pp.287?294.Erik Tjong Kim Sang and Fien De Meulder.
2003.
Introductionto the CONLL-2003 shared task: Language-independentnamed entity recognition.
In Proc.
of CoNLL 2003, pp.
142?147.Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007.
Anapproach to downsizing annotation costs and maintainingcorpus reusability.
In Proc of EMNLP-CoNLL 2007.Kees van Deemter and Rodger Kibble.
2000.
On coreferring:Coreference in MUC and related annotation schemes.
Com-putational Linguistics, 26(4):629?637.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.
An-notating expressions of opinions and emotions in language.Language Resources and Evaluation, 39(2/3):165?210.16
