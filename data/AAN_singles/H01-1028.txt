Finding Errors Automatically inSemantically Tagged DialoguesJohn Aberdeen, Christine Doran, Laurie Damianos,Samuel Bayer and Lynette HirschmanThe MITRE Corporation202 Burlington RoadBedford, MA 01730 USA+1.781.271.2000{aberdeen,cdoran,laurie,sam,lynette}@mitre.orgABSTRACTWe describe a novel method for detecting errors in task-basedhuman-computer (HC) dialogues by automatically derivingthem from semantic tags.
We examined 27 HC dialogues fromthe DARPA Communicator air travel domain, comparing userinputs to system responses to look for slot valuediscrepancies, both automatically and manually.
For theautomatic method, we labeled the dialogues with semantic tagscorresponding to "slots" that would be filled in "frames" inthe course of the travel task.
We then applied an automaticalgorithm to detect errors in the dialogues.
The same dialogueswere also manually tagged (by a different annotator) to labelerrors directly.
An analysis of the results of the two taggingmethods indicates that it may be possible to detect errorsautomatically in this way, but our method needs further workto reduce the number of false errors detected.
Finally, wepresent a discussion of the differing results from the twotagging methods.KeywordsDialogue, Error detection, DARPA Communicator.1.
INTRODUCTIONIn studying the contrasts between human-computer (HC) andhuman-human (HH) dialogues [1] it is clear that many HCdialogues are plagued by disruptive errors that are rarely seenin HH dialogues.
A comparison of HC and HH dialogues mayhelp us understand such errors.
Conversely, the ability todetect errors in dialogues is critical to understanding thedifferences between HC and HH communication.Understanding HC errors is also crucial to improving HCinteraction, making it more robust, trustworthy and efficient.The goal of the work described in this paper is to provide anannotation scheme that allows automatic calculation ofmisunderstandings and repairs, based on semantic informationpresented at each turn.
If we represent a dialogue as a sequenceof pairs of partially-filled semantic frames (one for the user?sutterances, and one for the user?s view of the system state), wecan annotate the accumulation and revision of information inthe paired frames.
We hypothesized that, with such arepresentation, it would be straightforward to detect when thetwo views of the dialogue differ (a misunderstanding), wherethe difference originated (source of error), and when the twoviews reconverge (correction).
This would be beneficialbecause semantic annotation often is used for independent rea-sons, such as measurements of concepts per turn [8],information bit rate [9], and currently active concepts [10].Given this, if our hypothesis is correct, then by viewingsemantic annotation as a representation of filling slots in userand system frames, it should be possible to detect errorsautomatically with little or no additional annotation.2.
SEMANTIC TAGGINGWe tagged 27 dialogues from 4 different systems thatparticipated in a data collection conducted by the DARPACommunicator  program in the summer of 2000.
These aredialogues between paid subjects and spoken languagedialogue systems operating in the air travel domain.
Eachdialogue was labeled with semantic tags by one annotator.
Wefocused on just the surface information available in thedialogues, to minimize inferences made by the annotator.The semantic tags may be described along two basicdimensions: slot and type.
The slot dimension describes theitems in a semantic frame that are filled over the course of adialogue, such as DEPART_CITY and AIRLINE (see Table 1 forthe complete list).The type dimension describes whether the tag is a PROMPT, aFILL, or an OFFER.
This type dimension is critical to semanticanalysis since it allows one to describe the effect a tag has onslots in the frame.
PROMPTs are attempts to gather values tofill slots, e.g., "what city do you want to fly to".
FILLs areactual slot fills, e.g., "I?d like to fly to San Francisco".
OFFERsrepresent actual flight information based on previous slotFILLs, e.g., "there is a 9:45 flight to San Francisco on Delta".However, OFFERs often do not exactly match slot FILLs (e.g.,the user requests a flight at 9:30, but the closest match flightis at 9:45), and thus must be distinguished from FILLs.In addition to the two basic dimensions of slot and type, eachtag takes a leg attribute to indicate which leg of a trip is beingdiscussed.
There is also an initial USER_ID slot which has twotypes (PROMPT_USER_ID and FILL_USER_ID), but no legattribute.Our semantic tag set alo includes two special tags, YES andNO, for annotating responses to offers and yes/no questions.Finally, we have two tags, PROMPT_ERASE_ FRAMES andFILL_ERASE_FRAMES, for annotating situations where theframes are erased and the dialogue is restarted (e.g., the usersays "start over").
Figure 1 shows part of a sample dialoguewith semantic tags.
Our semantic tagset is summarized in Table1.Table 1.
Semantic TagsetPROMPT FILL OFFERDEPART_CITY X X XARRIVE_CITY X X XDEPART_AIRPORT X X XARRIVE_AIRPORT X X XDATE X X XDEPART_TIME X X XARRIVE_TIME X X XAIRLINE X X XUSER_ID X XERASE_FRAMES X XYES (single bare tag)NO (single bare tag)3.
ERROR DETECTIONTo provide a baseline for comparison to an algorithm thatdetects errors automatically, we had an annotator (not the sameperson who did the semantic tagging described above)manually tag the problem areas.
This annotator marked fouritems:(1) occurrence: where the problem first occurs in thedialogue (e.g.
where the user says the item which thesystem later incorporates incorrectly)(2) detection: where the user could first be aware thatthere is a problem (e.g.
where the system reveals itsmistake)(3) correction attempt: where the user attempts to repairthe error(4) correction detection: where the user is first able todetect that the repair has succeededWe next developed an algorithm for automatically findingerrors in our semantically tagged dialogues.
In this phase ofthe research, we concentrated on deriving an automatic methodfor assigning the first two of the four error categories,occurrence and detection (in a later phase we plan to developautomatic methods for correction attempt and correctiondetection).
First, the algorithm derives the turn-by-turn framestates for both the user's utterances and the system's utterances(i.e., what the user heard the system say), paying specialattention to confirmation tags such as YES or deletion tagslike FILL_ERASE_FRAMES.
Then, the algorithm comparespatterns of user and system events to hypothesize errors.Occurences and detections are hypothesized for three types oferrors: hallucinations (system slot fill without user slot fill),mismatches (system slot fill does not match user slot fill), andprompts after fills (system prompt after user slot fill).Figure 2 shows a sample dialogue that illustrates several errortypes.
Utterance S12 shows a prompt after fill error ?
the userhas already supplied (in utterance U11) the information thesystem is requesting.
In utterance U13 the user suppliescontradictory information, and the system catches this andtries to resolve it in utterances S14 and S16.
Next a mismatcherror is illustrated ?
the user specifies ARRIVE_CITY inutterance U17, and the system shows that it has misrecognizedFigure 1.
Portion of a Sample Dialogue Labeled with Semantic TagsS0: System said: Welcome to our travel planning system.<prompt_user_id>Please say your PIN number</prompt_user_id>.U1: User said: My PIN is <fill_user_id>four four four four</fill_user_id>.S2: Sys tem said: What are your travel plans?U3: User said: I want to go to <fill_arrive_city leg=?1?>San Francisco</fill_arrive_city>.S4: System said: Traveling to <fill_arrive_city leg=?1?>San Francisco</fill_arrive_city>.<prompt_depart_city leg=?1?>Where are you departing from?</prompt_depart_city>U5: User said: <fill_depart_city leg=?1?>Boston</fill_depart_city>.S6: System said: <prompt_date leg=?1?>When would you like to leave</prompt_date><fill_depart_city leg=?1?>Boston</fill_depart_city>?U7: User said: <fill_date leg=?1?>July twenty-fifth</fill_date> at<fill_depart_time leg=?1?>12 noon</fill_depart_time>.S8: System said: I have a <offer_airline leg=?1?>Delta</offer_airline> flight that leaves<offer_depart_city leg=?1?>Boston</offer_depart_city> at<offer_depart_time leg=?1?>11:10AM</offer_depart_time> on<offer_date leg=?1?>July twenty-fifth</offer_date> and arrives in<offer_arrive_city leg=?1?>San Francisco</offer_arrive_city> at<offer_arrive_time>5:19PM</offer_arrive_time>.
Is that OK?it in utterance S18.
The user attempts to correct thismisrecognition in utterance U21, and as can be seen fromutterance S22, the system again has misrecognized the user?sutterance.Below we describe the results from running the automaticalgorithm on our 27 semantically tagged dialogues.4.
RESULTSIn the 27 dialogues considered, a total of 131 items wereflagged by one or both of the methods as error items (60 occur,71 detect).
A breakdown of these errors and which methodfound them is in Table 2.Table 2.
Unique Errors Identified# errors found by: Occur Detect TotalBoth Methods 14 23 37Automatic Only 28 38 66Manual Only 18 10 28Totals 60 71 131As can be seen in Table 2 the automatic method flagged manymore items as errors than the manual method.Table 3.
Error JudgementsOccur DetectE NE Q E NE QAuto 48% 40% 12% 52% 38% 10%Man 84% 13% 3% 82% 15% 3%We carefully examined each of the items flagged as errors bythe two methods.
Three judges (the semantic taggingannotator, the manual error tagging annotator, and a thirdperson who did not participate in the annotation) determinedwhich of the errors found by each of the two methods were realerrors (E), not real errors (NE), or questionable (Q).
Forcalculations in the present analysis, we used E as the baselineof real errors, rather than E+Q.
Table 3 shows the judgementsmade for both the automatic and manual method, which arediscussed in the next section.
It is important to note thathuman annotators do not perform this task perfectly, with errorrates of 13% and 15%.
This is also shown in the precision andrecall numbers for the two methods in Table 4.Table 4.
Precision & RecallOccur DetectPrecision& Recall P R P RAutomatic 0.48 0.57 0.52 0.84Manual 0.84 0.77 0.82 0.715.
ANALYSISThe automatic method flagged 40 items as errors that thejudges determined were not errors (17 occur, 23 detect).
These40 false errors can be classified as follows:A.
10 were due to bugs in the algorithm or source dataB.
19 were false errors that can be eliminated with non-trivial changes to the semantic tagset and/or algorithmC.
3 were false errors that could not be eliminatedwithout the ability to make inferences about worldknowledgeD.
8 were due to mistakes made by the semanticannotatorOne example of the 19 false errors above in B is when the firstuser utterance in a dialogue is a bare location, it is unclearwhether the user intends it to be a departure or arrival location.Our semantic tagset currently has no tags for ambiguoussituations such as these.
Adding underspecified tags to ourtagset (and updating the automatic algorithm appropriately)would solve this problem.
Another example is a situationwhere a system was legitimately asking for clarification abouta slot fill, but the algorithm flagged it as prompting for keysthat had already been filled.
This could be fixed by adding aCLARIFY element to the type dimension (currently PROMPT,FILL, and OFFER).
We believe that making these changeswould not compromise the generality of our semantic tagset.However, as the point of our approach is to derive errorswithout much additional annotation, additions to the semantictagset should only be made when there is substantialjustification.There were also 21 errors (15 occur, 6 detect) that were notdetected by the automatic method, but were judged as realerrors.
These 21 errors may be categorized as follows:A.
2 were due to bugs in the algorithmB.
8 were situations where the algorithm correctlyflagged the detect point of an error, but missed theassociated occur pointC.
6 were situations that could be fixed bymodifications to the semantic tagsetD.
1 was an error that could be fixed either by arevision to the semantic tagset or a revision to thealgorithmE.
2 were situations where the system ignored a userfill, and the automatic algorithm interpreted it as noconfirmation (not an error).
Human judgement isrequired to detect these errorsF.
2 were due to mistakes made by the semanticannotator6.
PREVIOUS WORKIn Hirschman & Pao [5], annotation was done by manualinspection of the exchanges in the dialogue.
Each exchangewas evaluated based on the portion of information "visible tothe other party".
Errors and problems were identified manuallyand traced back to their point of origin.
This is quite similar toour baseline manual annotation described in section 3.There have been other approaches to detecting andcharacterizing errors in HC dialogues.
Danieli [2] usedexpectations to model future user ut terances, and Levow [6][7]used utterance and pause duration, as well as pitch variabilityto characterize errors and corrections.
Dybkj?r, Bernsen &Dybkj?r [4] developed a set of principles of cooperative HCdialogue, as well as a taxonomy of errors typed according towhich of the principles are violated.
Finally, Walker et.
al.
[11][12] have trained an automatic classifier that identifiesand predicts problems in HC dialogues.7.
DISCUSSIONIt is clear that our algorithm and semantic tagset, as they standnow, need improvements to reduce the number of false errorsdetected.
However, even now the automatic method offers someadvantages over tagging errors manually, the most importantof which is that many researchers already annotate theirdialogues with semantic tags for other purposes and thusmany errors can be detected with no additional annotation.Also, the automatic method associates errors with particularslots, enabling researchers to pinpoint aspects of theirdialogue management strategy that need the most work.Finally, Day et.
al.
[3] have shown that correcting existingannotations is more time efficient than annotating fromscratch.
In this way, the automatic method may be used to"seed" an annotation effort, with later hand correction.8.
ACKNOWLEDGMENTSThis work was funded by the DARPA Communicator programunder contract number DAAB07-99-C201.
?
2001 The MITRECorporation.
All rights reserved.9.
REFERENCES[1] Aberdeen, J. and Doran, C. Human-computer and human-human dialogues.
DARPA Communicator PrincipleInvestigators Meeting (Philadelphia, PA USA 2000).http://www.dsic-web.net/ito/meetings/communicator_sep2000/[2] Danieli, M. On the use of expectations for detecting andrepairing human-machine miscommunication.Proceedings of AAAI Workshop on Detecting, Repairingand Preventing Human-Machine Miscommunication(Portland OR, USA 1996).
[3] Day, D., Aberdeen, J., Hirschman, L., Kozierok, R.,Robinson, P. and Vilain, M. Mixed-initiativedevelopment of language processing systems.
InProceedings of the Fifth Conference on Applied NaturalLanguage Processing (Washington DC, USA 1997).
[4] Dybkj?r, L., Bernsen, N.O.
and Dybkj?r, H. Reducingmiscommunication in spoken human-machine dialogue.Proceedings of AAAI Workshop on Detecting, Repairingand Preventing Human-Machine Miscommunication(Portland OR, USA 1996).
[5] Hirschman, L. and Pao, C. The cost of errors in a spokenlanguage system.
Proceedings of the Third EuropeanFigure 2.
Portion of a Sample Dialogue Illustrating Several ErrorsU11: User said: I want to fly from <fill_depart_city leg=?1?>Seattle</fill_depart_city> to<fill_arrive_city leg=?1?>Tokyo</fill_arrive_city>S12: System said: <prompt_arrive_city leg="1">to which city would you like to fly</prompt_arrive_city>from <fill_depart_city leg="1">seattle</fill_depart_city>U13: User said: <fill_arrive_city leg="1">seattle washington</fill_arrive_city>S14: System said: excuse me, i didn't hear you well.
I think you want to fly from<fill_depart_city leg="1">seattle</fill_depart_city> If this is okay, let's continue.
Otherwise,please specify the locations again.U15: User said: <yes>that's okay</yes>S16: System said: <prompt_arrive_city leg="1">what city would you like to fly to?</prompt_arrive_city>U17: User said: <fill_arrive_city leg="1">tokyo japan</fill_arrive_city>S18: System said: flying to <fill_arrive_city leg="1">togiak</fill_arrive_city><prompt_date leg="1">on what date?</prompt_date>U19: User said: <fill_date leg="1">monday September twenty fifth</fill_date>S20: System said: there don't seem to be any flights from <fill_depart_city leg="1">seattle</fill_depart_city>to <fill_arrive_city leg="1">togiak</fill_arrive_city> which depart<fill_depart_time leg="1">about ten PM</fill_depart_time> on<fill_date leg="1">september twenty fifth</fill_date>.U21: User said:  destination <fill_arrive_city leg="1">tokyo japan</fill_arrive_city>S22: System said: did you say you want to fly to<prompt_arrive_city leg="1">san diego</prompt_arrive_city>?Conference on Speech Communication and Technology(Berlin, Germany 1993).
[6] Levow, G.A.
Characterizing and recognizing spokencorrections in human-computer dialogue.
Proceedings ofCOLING-ACL (Montreal, Canada 1998).
[7] Levow, G.A.
Understanding recognition failures in spokencorrections in human-computer dialogue.
Proceedings ofECSA Workshop on Dialogue and Prosody (Eindhoven,The Netherlands 1999).
[8] Luo, X. and Papineni, K. IBM DARPA Communicator v1.0.DARPA Communicator Principle Investigators Meeting(Philadelphia, PA USA 2000).
http://www.dsic-web.net/ito/meetings/communicator_sep2000/[9] Polifroni, J. and Seneff, S. Galaxy-II as an architecture forspoken dialogue evaluation.
Proceedings of the SecondInternational Conference on Language Resources andEvaluation (Athens, Greece 2000).
[10] Rudnicky, A. CMU Communicator.
DARPA CommunicatorPrinciple Investigators Meeting (Philadelphia, PA USA2000).
http://www.dsic-web.net/ito/meetings/communicator_sep2000/[11] Walker, M., Langkilde, I., Wright, J., Gorin, A. and Litman,D.
Learning to predict problematic situations in a spokendialogue system: experiments with how may I help you?Proceedings of the Seventeenth International Conferenceon Machine Learning (Stanford, CA USA 2000).
[12] Walker, M., Wright, J. and Langkilde, I.
Using naturallanguage processing and discourse features to identifyunderstanding errors in a spoken dialogue system.Proceedings of the North American Meeting of theAssociation of Computational Linguistics (Seattle, WAUSA 2000).
