Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 937?946,Honolulu, October 2008. c?2008 Association for Computational LinguisticsCoCQA: Co-Training Over Questions and Answerswith an Application to Predicting Question Subjectivity OrientationBaoli LiEmory Universitycsblli@gmail.comYandong LiuEmory Universityyliu49@emory.eduEugene AgichteinEmory Universityeugene@mathcs.emory.eduAbstractAn increasingly popular method forfinding information online is via theCommunity Question Answering(CQA) portals such as Yahoo!
An-swers, Naver, and Baidu Knows.Searching the CQA archives, and rank-ing, filtering, and evaluating the sub-mitted answers requires intelligentprocessing of the questions and an-swers posed by the users.
One impor-tant task is automatically detecting thequestion?s subjectivity orientation:namely, whether a user is searching forsubjective or objective information.Unfortunately, real user questions areoften vague, ill-posed, poorly stated.Furthermore, there has been little la-beled training data available for realuser questions.
To address these prob-lems, we present CoCQA, a co-trainingsystem that exploits the association be-tween the questions and contributedanswers for question analysis tasks.The co-training approach allowsCoCQA to use the effectively unlim-ited amounts of unlabeled data readilyavailable in CQA archives.
In this pa-per we study the effectiveness ofCoCQA for the question subjectivityclassification task by experimentingover thousands of real users?
questions.1 IntroductionAutomatic question answering (QA) has beenone of the long-standing goals of natural lan-guage processing, information retrieval, andartificial intelligence research.
For a naturallanguage question we would like to respondwith a specific, accurate, and complete an-swer that addresses the question.
Althoughmuch progress has been made, answeringcomplex, opinion, and even many factualquestions automatically is still beyond thecurrent state-of-the-art.
At the same time, therise of popularity in social media and collabo-rative content creation services provides apromising alternative to web search or com-pletely automated QA.
The explicit supportfor social interactions between participants,such as posting comments, rating content, andresponding to questions and comments makesthis medium particularly amenable to Ques-tion Answering.
Some very successful exam-ples of Community Question Answering(CQA) sites are Yahoo!
Answers 1  andNaver2, and Baidu Knows3.
Yahoo!
Answersalone has already amassed hundreds of mil-lions of answers posted by millions of par-ticipants on thousands of topics.The questions posted to such CQA portalsare typically complex, subjective, and rely onhuman interpretation to understand the corre-sponding information need.
At the same time,the questions are also usually ill-phrased,vague, and often subjective in nature.
Hence,analysis of the questions (and of the corre-sponding user intent) in this setting is a par-ticularly difficult task.
At the same time,CQA content incorporates the relationshipsbetween questions and the corresponding an-swers.
Because of the various incentives pro-vided by the CQA sites, answers posted byusers tend to be, at least to some degree, re-sponsive to the question.
This observationsuggests investigating whether the relation-1 http://answers.yahoo.com2 http://www.naver.com3 http://www.baidu.com937ship between questions and answers can beexploited to improve automated analysis of theCQA content and the user intent behind thequestions posted.Figure 1: Example question (Yahoo!
Answers)To this end, we exploit the ideas of co-training, a general semi-supervised learningapproach naturally applicable to cases of com-plementary views on a domain, for example,web page links and content (Blum andMitchell, 1998).
In our setting, we focus on thecomplimentary views for a question, namelythe text of the question and the text of the as-sociated answers.As a concrete case-study of our approachwe focus on one particularly important aspectof intent detection: the subjectivity orientation.We attempt to predict whether a questionposted in a CQA site is subjective or objective.Objective questions are expected to be an-swered with reliable or authoritative informa-tion, typically published online and possiblyreferenced as part of the answer, whereas sub-jective questions seek answers containing pri-vate states, e.g.
personal opinions, judgment,experiences.
If we could automatically predictthe orientation of a question, we would be ableto better rank or filter the answers, improvesearch over the archives, and more accuratelyidentify similar questions.
For example, if aquestion is objective, we could try to find afew highly relevant articles as references,whereas if a question is subjective, useful an-swers are not expected to be found in authori-tative sources and tend to rank low with cur-rent question answering and CQA search tech-niques.
Finally, learning how to identify ques-tion orientation is a crucial component of in-ferring user intent, a long-standing problem inweb information access settings.In particular, we focus on the following re-search questions:?
Can we utilize the inherent structure of theCQA interactions and use the unlimitedamounts of unlabeled data to improve classi-fication performance, and/or reduce theamount of manual labeling required??
Can we automatically predict question sub-jectivity in Community Question Answering?
and which features are useful for this taskin the real CQA setting?The rest of the paper is structured as fol-lows.
We first overview the community ques-tion answering setting, and state the questionorientation classification problem, which weuse as the motivating application for our sys-tem, more precisely.
We then introduce ourCoCQA system for semi-supervised classifi-cation of questions and answers in CQA com-munities (Section 3).
We report the results ofour experiments over thousands of real userquestions in Section 4, showing the effective-ness of our approach.
Finally, we review re-lated work in Section 5, and discuss our con-clusions and future work in Section 6.2 Question Orientation in CQAWe first briefly describe the essential featuresof question answering communities such asYahoo!
Answers or Naver.
Then, we formallystate the problem addressed in this paper, andthe features used for this setting.9382.1 Community Question AnsweringOnline social media content and associatedservices comprise one of the fastest growingsegments on the Web.
The explicit support forsocial interactions between participants, suchas posting comments, rating content, and re-sponding to questions and comments makesthe social media unique.
Question answeringhas been particularly amenable to social mediaby directly connecting information seekerswith the community members willing to sharethe information.
Yahoo!
Answers, with mil-lions of users and hundreds of millions of an-swers for millions of questions is a very suc-cessful implementation of CQA.For example, consider two example user-contributed questions, objective and subjectiverespectively:Q1: What?s the difference betweenchemotherapy and radiation treat-ments?Q2: Has anyone got one of thosehome blood pressure monitors?
andif so what make is it and do youthink they are worth getting?Figure 1 shows an example of communityinteractions in Yahoo!
Answers around thequestion Q2 above.
A user posted the questionin the Health category of the site, and was ableto obtain 10 responses from other users.
Even-tually, the asker chooses the best answer.
Fail-ing that, as shown in the example, the best an-swer can also be chosen according to the votesfrom other users.
Many of the interactions de-pend on the perceived goals of the asker: if theparticipants interpret the question as subjec-tive, they will tend to share their experiencesand opinions, and if they interpret the questionas objective, they may still share their experi-ences but may also provide more factual in-formation.2.2 Problem DefinitionWe now state our problem of question orienta-tion more precisely.
We consider question ori-entation from the perspective of user goals:authors of objective questions request authori-tative, objective information (e.g., publishedliterature or expert opinion), whereas authorsof subjective questions seek opinions or judg-ments of other users in the community.
Westate our problem as follows.Question Subjectivity Problem: Given aquestion Q in a question answering com-munity, predict whether Q has objectiveor subjective orientation, based on ques-tion and answer text as well as the userand community feedback.3 CoCQA: A Co-Training Frame-work over Questions and AnswersIn the CQA setting we could easily obtainthousands or millions of unlabeled examplesfrom the online CQA archives.
On the otherhand, it is difficult to create a labeled datasetwith a reasonable size, which could be usedto train a perfect classifier to analyze ques-tions from different domains and sub-domains.
Therefore, semi-supervised learning(Chapelle et al, 2006) is a natural approachfor this setting.Intuitively, we can consider the text of thequestion itself or answers to it.
In otherwords, we have multiple (at least two) naturalviews of the data, which satisfies the condi-tions of the co-training approach (Blum andMitchell, 1998).
In co-training, two separateclassifiers are trained on two sets of features,respectively.
By automatically labeling theunlabeled examples, these two classifiers it-eratively ?teach?
each other by giving theirpartners a newly labeled data that they canpredict with high confidence.
Based on theoriginal co-training algorithm in (Blum andMitchell, 1998) and other implementations,we develop our algorithm CoCQA shown inFigure 2.At Steps 1 and 2, the K examples are com-ing from different feature spaces, and eachcategory (for example, Subjective and Objec-tive) has top Kj most confident examples cho-sen, where Kj corresponds to the distributionof class in the current set of labeled examplesL.
CoCQA will terminate when the incre-ments of both classifiers are less than a speci-fied threshold X or the maximum number ofiterations are exceeded.
Following the co-training approach, we include the most confi-dently predicted examples as additional ?la-beled?
data.
The SVM output margin valuewas used to estimate confidence; alternative939methods (including reliability of this confi-dence prediction) could further improve per-formance, and we will explore these issues infuture work.
Finally, the next question is howto estimate classification performance withtraining data.
For each pass, we randomly splitthe original training data into N folds (N=10 inour experiments), and keep one part for valida-tion and the rest, augmented with the newlyadded examples, as the expanded training set.After CoCQA terminates, we obtain twoclassifiers.
When a new example arrives, wewill classify it with these two classifiers basedon both of the feature sets, and combine thepredictions of these two classifiers.
We ex-plored two strategies to make the final deci-sion based on the confidence values given bytwo classifiers:?
Choose the class with higher confidence?
Multiply the confidence values, andchoose the class that has the highestproduct.We found the second heuristic to be moreeffective than the first in our experiments.
Asthe base classifier we use SVM in the currentimplementation, but other classifiers could beincorporated as well.4 Experimental EvaluationWe experiment with supervised and semi-supervised methods on a relatively large dataset from Yahoo!
Answers.4.1 DatasetsTo our knowledge, there is no standard data-set of real questions and answers posted byonline users, labeled for subjectivity orienta-tion.
Hence, we had to create a dataset our-selves.
To create our dataset, we downloadedmore than 30,000 resolved questions fromeach of the following top-level categories ofYahoo!
Answers: Arts, Education, Health,Science, and Sports.
We randomly chose 200questions from each category to create a rawdataset with 1,000 questions total.
Then, welabeled the dataset with annotators from theAmazon?s Mechanical Turk service4.For annotation, each question was judgedby 5 Mechanical Turk workers who passed aqualification test of 10 questions (labeled byourselves) with at least 9 of them correctlymarked.
The qualification test was required toensure that the raters were sufficiently com-petent to make reasonable judgments.
Wegrouped the tasks into 25 question batches,where the whole batch was submitted as theMechanical Turk?s Human Intelligence Task(HIT).
The batching of questions was done toeasily detect the ?random?
ratings producedby irresponsible workers.
That is, eachworker rated a batch of 25 questions.While precise definition of subjectivity iselusive, we decided to take the practical per-spective, namely the "majority" interpreta-tion.
The annotators were instructed to guessorientation according to how the questionwould be answered by most people.
We didnot deal with multi-part questions: if any partof question was subjective, the whole ques-tion was labeled as subjective.
The gold stan-dard was thus derived with the majority strat-egy, followed by manual inspection as a ?san-ity check?.
At this stage we removed 22 ques-tions with undeterminable meaning, includinggems such as ?Upward Soccer4 http://www.mturk.comFigure 2: Algorithm CoCQA: A co-training algo-rithm for exploiting redundant feature sets incommunity question answering.Input:?
FQ and FA are Question and Answer feature views?
CQ and CA are classifiers trained on FQ and FA  respec-tively?
L is a set of labeled training examples?
U is a set of unlabeled examples?
K: Number of unlabeled examples to choose oneach iteration?
X:  the threshold for  increment?
R:  the maximal number of iterationsAlgorithm CoCQA1.
Train CQ ,0 on L: FQ , and record resulting   ACCQ,02.
Train CA ,0 on L: FA , and record resulting  ACCA ,03. for j=1 to R do:Use CQ,j-1 to predict labels for U and choosetop K items with highest confidence ?
EQ, , j-1Use CA,j-1 to predict labels for U and  choosetop K items with highest confidence ?
EA, , j-1Move examples EQ, , j-1 U EA, , j-1 ?
LTrain CQ,j on L: FQ and record training  ACCQ,jTrain CA,j on L: FA and record training  ACCA,jif Max(?ACCQ,j, ?
ACCA,j) < X break4.
return final classifiers CQ,j ?
CQ and CA,j ?
CA940Shorts?
?5 and ?1+1=?fdgdgdfg??6.
Fi-nally, we create a labeled dataset consisting of978 resolved questions, available online7.Num.
ofSUB.
QNum.
ofOBJ.
QTotalNum.AnnotatoragreementArts 137 (70%) 58 (30%) 195 0.841Education 127 (64%) 70 (36%) 197 0.716Health 125 (64%) 69 (36%) 194 0.833Science 103 (52%) 94 (48%) 197 0.618Sports 154 (79%) 41 (21%) 195 0.877Total 646 (66%) 332 (34%) 978 0.777Table 1: Labeled dataset statistics.Table 1 reports the statistics of the annotateddataset.
The overall inter-annotator percentageagreement between Mechanical Turk workersand final annotation is 0.777, indicating thatthe task is difficult, but feasible for humans toannotate manually.The statistics of our labeled sample showthat the vast majority of the questions in allcategories except for Science are subjective innature.
The relatively high ratio of subjectivequestions in the Science category is surprising.However, we find that users often post polem-ics and statements instead of questions, usingCQA as a forum to share their opinions oncontroversial topics.
Overall, we were struckby the expressed need in Subjective informa-tion, even for categories such as Health andEducation, where objective information wouldintuitively seem more desirable.4.2 Features Used in ExperimentsFor the subjectivied experiments to follow,we attempt to capture the linguisticcharacteristics identified in previous work(Section 5) in a lightweight and robust manner,due to the informal and noisy nature of CQA.In particular, we use the following featureclasses, computed separately over question andanswer content:?
Character 3-grams?
Words?
Word with Character 3-grams?
Word n-grams (n<=3, i.e.
Wi, WiWi+1,WiWi+1Wi+2)5http://answers.yahoo.com/question/?qid=20060829074901AADBRJ46 http://answers.yahoo.com/question/?qid=10060120036517 Available at http://ir.mathcs.emory.edu/datasets/.?
Word and POS n-gram (n<=3, i.e.
Wi,WiWi+1, Wi POSi+1, POSiWi+1 ,POSiPOSi+1, etc.
).We use the character 3-grams features toovercome spelling errors and problems of ill-formatted or ungrammatical questions, andthe POS information to capture common pat-terns across domains, as words, especially thecontent words, are quite diverse in differenttopical domains.
For word and character 3-gram features, we consider two different ver-sions: case-sensitive and case-insensitive.Case-insensitive features are assumed to behelpful for mitigating negative effects of ill-formatted text.Moreover, we experimented with threeterm weighting schemes: Binary, TF, andTF*IDF.
Term frequency (TF) exhibited bet-ter performance in our development experi-ments, so we use this weighting scheme forall the experiments in Section 4.
Regardingfeatures: both words and structure of the text(e.g., word order) can be used to infer subjec-tivity.
Therefore, the features we employ,such as words and word n-grams, are ex-pected to be useful as a (coarse) proxy togrammatical and phrase features.
Unlike tra-ditional work on news-like text, the text ofCQA and has poor spelling, grammar, andheavily uses non-standard abbreviations,hence our decision to use character n-grams.4.3 Experimental SettingMetrics: Since the prediction  on both sub-jective questions and objective questions isequally important, we use the macro-averaged F1 measure as the evaluation met-ric.
This is computed as the macro average ofF1 measures computed for the Subjective andObjective classes individually.
The F1 meas-ure for either class is computedasRecallPrecisionRecall Precision 2+??
.Methods compared: We compare our ap-proach with both the base supervised learning,as well as GE, a state-of-the-art semi-supervised method:?
Supervised: we use the LibSVM im-plementation (Chang and Lin, 2001)with linear kernel.941?
GE: This is a state-of-the-art semi-supervised learning algorithm, General-ized Expectation (GE), introduced in(McCallum et al, 2007) that incorporatesmodel expectations into the objectivefunctions for parameter estimation.?
CoCQA: Our method (Section 3).For semi-supervised learning experiments,we selected a random subset of 2,000 unla-beled questions for each of the topical catego-ries, for the total of 10,000 unlabeled questions.4.4 Experimental ResultsFirst we report the performance of our Super-vised baseline system with a variety of fea-tures, reporting the average results of 5-foldcross validation.
Then we investigate the per-formance to our new CoCQA framework undera variety of settings.4.4.1 Supervised LearningTable 2 reports the classification perform-ance for varying units of representation (e.g.,question text vs. answer text) and the varyingfeature sets.
We used case-insensitive featuresand TF (term frequency within the text unit) asfeature weights, as these two settings achievedthe best results in our development experi-ments.
The rows show performance consider-ing only the question text (question), the bestanswer (best_ans), text of all answers to aquestion (all_ans), the text of the question andthe best answer (q_bestans), and the text ofthe question with all answers (q_allans), re-spectively.
In particular, using the words inthe question alone achieves F1 of 0.717, com-pared to using words in the question and thebest answers text (F1 of 0.695).
For compari-son, a na?ve baseline that always guesses themajority class (Subjective) obtains F1 of 0.398.With character 3-gram, our system achievesperformance comparable with words as fea-tures, but combining them together does notimprove performance.
We observe a slightgain with more complicated features, e.g.
wordn-gram, or word and POS n-grams, but thegain is not significant, and hence not worth theincreased complexity of the feature generation.Finally, combining question text with answertext does not improve performance.Interestingly, the best answer itself is not aseffective as the question for subjectivityanalysis, nor is using all of the answers sub-mitted.
One possible reason is that approxi-mately 40% of the best answers were chosenby the community and not the asker herself,are hence not necessarily representative of theasker intent.FeaturesetUnitChar3-gramWordWord+Char3-gramWordn-gram(n<=3)WordPOSn-gram(n<=3)question 0.700 0.717 0.694 0.716 0.720best_ans 0.587 0.597 0.578 0.580 0.565all_ans 0.603 0.628 0.607 0.648 0.630q_bestans 0.681 0.695 0.662 0.687 0.712q_allans 0.679 0.677 0.676 0.708 0.689Na?ve (majority class) baseline:  0.398Table 2.
Performance of predicting questionorientation on the mixed dataset with varyingfeature sets (Supervised).Table 3 reports the supervised subjectivityclassification performance for each questioncategory with word features.
The overall clas-sification results are significantly lower com-pared to training and testing on the mixture ofthe questions drawn from all categories,likely caused by the small amount of labeledtraining data for each category.
Another pos-sibility is that the subjectivity clues are nottopical and hence are not category dependent,with the possible exception of the questionsin the Health domain.Category Arts Edu.
Health Science SportsF1 0.448 0.572 0.711 0.647 0.441Table 3.
Experiment results on sub-categorieswith supervised SVM (q_bestans features).As words are simple and effective featuresin this experiment, we will use them in thesubsequent experiments.
Furthermore, thefeature set using the words in the questionwith best answer together (q_bestans) exhibithigher performance than question with allanswers (q_allans).
Thus, we will only con-sider questions and best answers in the fol-lowing experiments with GE and CoCQA.4.4.2 Semi-Supervised LearningWe now focus on investigating the effec-tiveness of CoCQA, our co-training-basedframework for community question answer-ing analysis.
Table 4 summarizes the main942results of this section.
The values for CoCQAare derived with the parameter settings: K=100,X=0.001.
These optimal settings are chosenafter comprehensive experiments with differ-ent combinations, described later in this sec-tion.
GE does not exhibit a significant im-provement over Supervised.
In contrast,CoCQA performs significantly better than thepurely supervised method, with F1 of 0.745compared to the F1 of 0.717 for Supervised.While it may seem surprising that a semi-supervised method outperforms a supervisedone, note that we use all of the available la-beled data as provided to the Supervisedmethod, as well as a large amount of unlabeleddata, that is ultimately responsible for the per-formance improvement.FeaturesMethodQuestion Question+ Best AnswerSupervised 0.717 0.695GE 0.712 (-0.7%) 0.717 (+3.2%)CoCQA 0.731 (+1.9%) 0.745 (+7.2%)Table 4.
Performance of CoCQA, GE, and Su-pervised with the same feature and data settings.As an added advantage, CoCQA approach isalso practical.
In a realistic application, wehave two different situations: offline andonline.
With online processing, we may nothave best answers available to predict ques-tion?s orientation, whereas we can employ in-formation from best answers in offline setting.Co-training is a solution that is applicable toboth situations.
With CoCQA, we have twoclassifiers using the question text and the bestanswer text, respectively.
We can use both ofthem to obtain better results in the offline set-ting, while in online setting, we can use thetext of the question alone.
In contrast, GE maynot have this flexibility.We now analyze the performance ofCoCQA under a variety of settings to deriveoptimal parameters and to better understandthe performance.
Figure 3 reports the perform-ance of CoCQA with varying the K parameterfrom 20 to 200.
In this experiment, we fix X tobe 0.001.
The combination of question andbest answer is superior to that of question andall answers.
When K is 100, the system obtainsthe best result, 0.745.Figure 4 reports the number of co-trainingiterations needed to converge to optimal per-formance.
After 13 iterations (and 2500 unla-beled examples added), CoCQA achieves op-timal performance, and eventually terminatesafter an additional 3 iterations.
While a vali-dation set should have been used for CoCQAparameter tuning, Figures 3 and 4 indicatethat CoCQA is not sensitive to the specificparameter settings.
In particular, we observethat any K is greater than 100, and for anynumber of iterations R is greater than 10,CoCQA exhibits in effectively equivalent per-formance.0.640.650.660.670.680.690.70.710.720.730.740.750.7620 40 60 80 100 120 140 160 180 200K: # labeled examples added on eachco-training iterationF1CoCQA(Question and Best Answer)Supervised Q_bestansCoCQA(Question and All Answers)Supervised Q_allansFigure 3: Performance of CoCQA for varyingthe K (number of examples added on each it-eration of co-training).Figure 5 reports the performance ofCoCQA for varying the number of labeledexamples from 50 to 400 (that is, up to 50%of the available labeled training data).
Notethat for this comparison we use the same fea-ture sets  (words in question and best answertext), but using only the (varying) fractions ofthe manually labeled data.
Surprisingly,CoCQA exhibits comparable performance ofF1=0.685 with only 200 labeled examples areused, compared to the F1=0.695 for Super-vised with all 800 labeled training exampleson this feature set.
In other words, CoCQA isable to achieve comparable performance tosupervised SVM with only one quarter of thelabeled training data.9430.710.720.730.740.75161377776666# co-training iterationsF10500100015002000250030003500Total#UnlabeledAddedCoCQA (Question + Best Answer)SupervisedTotal # UnlabeledFigure 4: Performance and the total number ofunlabeled examples added for varying numberof co-training iterations (K=100, using q_bestansfeatures)0.520.540.560.580.60.620.640.660.680.70.7250 100 150 200 250 300 350 400# of labeled data usedF1CoCQA (Question + Best Answer)Supervised Q_Best AnsFigure 5: Performance of CoCQA with varyingnumber of labeled examples used, compared toSupervised method, on same features.5 Related WorkQuestion analysis, especially question classifi-cation, has been long studied in the questionanswering research community.
However,most of the previous research primarily con-sidered factual questions, with the notable ex-ception of the most recent TREC opinion QAtrack.
Furthermore, the questions were specifi-cally designed for benchmark evaluation.
Arelated thread of research considered deepanalysis of the questions (and correspondingsentences) by manually classifying questionsalong several orientation dimensions, notably(Stoyanov et al, 2005).
In contrast, our workfocuses on analyzing real user questionsposted in a question answering community.These questions are often complex or subjec-tive, and are typically difficult to answerautomatically as the question author probablywas not able to find satisfactory answers withquick web search.Automatic complex question answering hasbeen an active area of research, ranging fromsimple modification to factoid QA techniques(e.g., Soricut and Brill, 2003) to knowledgeintensive approaches for specific domains(e.g., Harabagiu et al 2001, Fushman and Lin2007).
However, the technology does not yetexist to automatically answer open-domaincomplex and subjective questions.
Whilethere has been some recent research (e.g.,Agichtein et al 2008, Bian et al 2008) onretrieving high quality answers from CQAarchives, the subjectivity orientation of thequestions has not been considered as a featurefor ranking.A related corresponding problem is com-plex QA evaluation.
Recent efforts at auto-matic evaluation show that even for well-defined, objective, complex questions,evaluation is extremely labor-intensive andintroduces many challenges (Lin andFushman 2006, Lin and Zhang 2007).
As partof our contribution we showed that it is feasi-ble to use the Amazon Mechanical Turk ser-vice for evaluation by combining large degreeof annotator redundancy (5 annotators perquestion) with more sparse but higher-qualityexpert annotation.The problem of automatic subjective ques-tion answering has recently started to be ad-dressed in the question answering commu-nity, most recently as the first opinion QAtrack in (Dang et al, 2007).
Unlike the con-trolled TREC opinion track (introduced in2007), many of the questions in Yahoo!
An-swers community are inherently subjective,complex, ill-formed, or all of the above.
Toour knowledge, this paper is the first large-scale study of subjective/objective orientationof information needs, and certainly the first inthe CQA environment.A closely related research thread is subjec-tivity analysis at document and sentencelevel.
For example, reference (Yu, H., andHatzivassiloglou, V. 2003; Somasundaran et944al.
2007) attempted to classify sentences intothose reporting facts or opinions.
Also relatedis research on sentiment analysis (e.g., Pang etal., 2004) where the goal is to classify a sen-tence or text fragment as being overall positiveor negative.
More generally, (Wiebe et al2004) and subsequent work focused on theanalysis of subjective language in narrativetext, primarily news.
Our problem is quite dif-ferent in the sense that we are trying to iden-tify the orientation of a question.
Nevertheless,our baseline method is similar to the methodsand features used for sentiment analysis, andone of our contributions is evaluating the use-fulness of the established features and tech-niques to the new CQA setting.In order to predict question orientation, webuild on co-training, one of the known semi-supervised learning techniques.
Many modelsand techniques have been proposed for classi-fication, including support vector machines,decision tree based techniques, boosting-basedtechniques, and many others.
We use LIBSVM(Chang and Lin, 2001) as a robust implemen-tation of SVM algorithms.In summary, while we draw on many tech-niques in question answering, natural languageprocessing, and text classification, our workdiffers from previous research in that a) de-velop a novel co-training based algorithm forquestion and answer classification; b) we ad-dress a relatively new problem of automaticquestion subjectivity prediction; c) demon-strate the effectiveness of our techniques in thenew CQA setting and d) explore the character-istics unique to CQA ?
while showing goodresults for a quite difficult task.6 ConclusionsWe presented CoCQA, a co-training frame-work for modeling the textual interactions inquestion answer communities.
Unlike previouswork, we have focused on real user questions(often noisy, ungrammatical, and vague) sub-mitted in Yahoo!
Answers, a popular commu-nity question answering portal.
We demon-strated CoCQA for one particularly importanttask of automatically identifying question sub-jectivity orientation, showing that CoCQA isable to exploit the structure of questions andcorresponding answers.
Despite the inherentdifficulties of subjectivity analysis for real userquestions, we have shown that by applyingCoCQA to this task we can significantly im-prove prediction performance, and substan-tially reduce the size of the required trainingdata, while outperforming a general state-of-the-art semi-supervised algorithm that doesnot take advantage of the CQA characteris-tics.In the future we plan to explore more so-phisticated features such semantic conceptsand relationships (e.g., derived from WordNetor Wikipedia), and richer syntactic and lin-guistic information.
We also plan to explorerelated variants of semi-supervised learningsuch as co-boosting methods to further im-prove classification performance.
We willalso investigate other applications of our co-training framework to tasks such as sentimentanalysis in community question answeringand similar social media content.AcknowledgmentsThis research was partially supported by theEmory University Research Committee(URC) grant, and by the Emory College Seedgrant.
We thank the Yahoo!
Answers team forproviding access to the Answers API, andanonymous reviewers for their excellent sug-gestions.ReferencesAgichtein, E., Castillo, C., Donato, D., Gionis, A., andMishne, G. 2008.
Finding High-Quality Content inSocial Media with an Application to Community-Based Question Answering.
WSDM2008Bian, J., Liu, Y., Agichtein, E., and H. Zha.
2008, toappear.
Finding the Right Facts in the Crowd: Fac-toid Question Answering over Social Media, Pro-ceedings of the Inter-national World Wide Web Con-ference (WWW), 2008Blum, A., and Mitchell, T. 1998.
Combining Labeledand Unlabeled Data with Co-Training.
Proc.
of theAnnual Conference on Computational LearningTheory.Chang, C. C. and Lin, C. J.
2001.
LIBSVM : a libraryfor support vector machines.
Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvm.Chapelle, O., Scholkopf, B., and Zien, A.
2006.
Semi-supervised Learning.
The MIT Press, Cambridge,Mas-sachusetts.Dang, H. T., Kelly, D., and Lin, J.
2007.
Overview ofthe TREC 2007 Question Answering track.
In Pro-ceedings of TREC-2007.945Demner-Fushman, D. and Lin, J.
2007.
Answering clini-cal questions with knowledge-based and statisticaltechniques.
Computational Linguistics, 33(1):63?103.Harabagiu, S., Moldovan, D., Pasca, M., Surdeanu, M. ,Mihalcea, R., Girju, R., Rusa, V., Lacatusu, F.,Morarescu, P., and Bunescu, R. 2001.
AnsweringComplex, List and Context Questions with LCC'sQuestion-Answering Server.
In Proc.
of TREC 2001.Lin, J. and Demner-Fushman, D. 2006.
Methods forautomatically evaluating answers to complex ques-tions.
In-formation Retrieval, 9(5):565?587Lin, J. and Zhang, P. 2007.
Deconstructing nuggets: thestability and reliability of complex question answeringevaluation.
In Proceedings of the 30th annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 327?334.Mann, G., and McCallum, A.
2007.
Simple, Robust,Scalable Semi-supervised Learning via ExpectationRegularization.
Proceedings of ICML 2007.Pang, B., and Lee, L. 2004.
A Sentimental Education:Sen-timent Analysis Using Subjective SummarizationBased on Minimum Cuts.
In Proc.
of ACL.Prager, J.
2006.
Open-Domain Question-Answering.Foundations and Trends in Information Retrieval.Sindhwani, V., Keerthi, S. 2006.
Large Scale Semi-supervised Linear SVMs.
Proceedings of SIGIR 2006.Somasundaran, S., Wilson, T., Wiebe, J. and Stoyanov,V.
2007.
QA with Attitude: Exploiting Opinion TypeAnalysis for Improving Question Answering in On-line Discussions and the News.
In proceedings of In-ternational Conference on Weblogs and Social Media(ICWSM-2007).Soricut, R. and Brill, E. 2004.
Automatic question an-swering: Beyond the factoid.
Proceedings of HLT-NAACL.Stoyanov, V., Cardie, C., and Wiebe, J.
2005.
Multi-Perspective question answering using the OpQA cor-pus.
In Proceedings of EMNLP.Tri, N. T., Le, N. M., and Shimazu, A.
2006.
UsingSemi-supervised Learning for Question Classification.In Proceedings of ICCPOL-2006.Wiebe, J., Wilson, T., Bruce R., Bell M., and Martin M.2004.
Learning subjective language.
ComputationalLinguistics, 30 (3).Yu, H., and Hatzivassiloglou, V. 2003.
Towards Answer-ing Opinion Questions: Separating Facts from Opin-ions and Identifying the Polarity of Opinion Sentences.In Proceedings of EMNLP-2003.Zhang, D., and Lee, W.S.
2003.
Question ClassificationUsing Support Vector Machines.
Proceedings of the26th Annual International ACM SIGIR Conference onRe-search and Development in Information Retrieval.Zhu, X.
2005.
Semi-supervised Learning LiteratureSurvey.
Technical Report 1530, Computer Sciences,University of Wisconsin-Madison.946
