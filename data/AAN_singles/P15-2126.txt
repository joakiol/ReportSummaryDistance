Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769?774,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsModel Adaptation for Personalized Opinion AnalysisMohammad Al Boni1, Keira Qi Zhou1, Hongning Wang2, and Matthew S. Gerber11Department of Systems and Information Engineering2Department of Computer Science1,2University of Virginia, USA1,2{ma2sm,qz4aq,hw5x,msg8u}@virginia.eduAbstractHumans are idiosyncratic and variable: to-wards the same topic, they might hold dif-ferent opinions or express the same opin-ion in various ways.
It is hence impor-tant to model opinions at the level of in-dividual users; however it is impracticalto estimate independent sentiment classi-fication models for each user with limiteddata.
In this paper, we adopt a model-based transfer learning solution ?
usinglinear transformations over the parame-ters of a generic model ?
for personalizedopinion analysis.
Extensive experimentalresults on a large collection of Amazonreviews confirm our method significantlyoutperformed a user-independent genericopinion model as well as several state-of-the-art transfer learning algorithms.1 IntroductionThe proliferation of user-generated opinionatedtext data has fueled great interest in opinion analy-sis (Pang and Lee, 2008; Liu, 2012).
Understand-ing opinions expressed by a population of usershas value in a wide spectrum of areas, includingsocial network analysis (Bodendorf and Kaiser,2009), business intelligence (Gamon et al., 2005),marketing analysis (Jansen et al., 2009), person-alized recommendation (Yang et al., 2013) andmany more.Most of the existing opinion analysis researchfocuses on population-level analyses, i.e., predict-ing opinions based on models estimated from acollection of users.
The underlying assumption isthat users are homogeneous in the way they ex-press opinions.
Nevertheless, different users mayuse the same words to express distinct opinions.For example, the word ?expensive?
tends to beassociated with negative sentiment in general, al-though some users may use it to describe their sat-isfaction with a product?s quality.
Failure to rec-ognize this difference across users will inevitablylead to inaccurate understanding of opinions.However, due to the limited availability of user-specific opinionated data, it is impractical to es-timate independent models for each user.
In thiswork, we propose a transfer learning based solu-tion, named LinAdapt, to address this challenge.Instead of estimating independent classifiers foreach user, we start from a generic model and adaptit toward individual users based on their own opin-ionated text data.
In particular, our key assump-tion is that the adaptation can be achieved via a setof linear transformations over the generic model?sparameters.
When we have sufficient observationsfor a particular user, the transformations will pushthe adapted model towards the user?s personalizedmodel; otherwise, it will back off to the genericmodel.
Empirical evaluations on a large collectionof Amazon reviews verify the effectiveness of theproposed solution: it significantly outperformed auser-independent generic model as well as severalstate-of-the-art transfer learning algorithms.Our contribution is two-fold: 1) we enable ef-ficient personalization of opinion analysis via atransfer learning approach, and 2) the proposed so-lution is general and applicable to any linear modelfor user opinion analysis.2 Related WorkSentiment Analysis refers to the process of iden-tifying subjective information in source materials(Pang and Lee, 2008; Liu, 2012).
Typical tasks in-clude: 1) classifying textual documents into posi-tive and negative polarity categories, (Dave et al.,2003; Kim and Hovy, 2004); 2) identifying textualtopics and their associated opinions (Wang et al.,2010; Jo and Oh, 2011); and 3) opinion summa-rization (Hu and Liu, 2004; Ku et al., 2006).
Ap-proaches for these tasks focus on population-levelopinion analyses, in which one model is sharedacross all users.
Little effort has been devotedto personalized opinion analyses, where each userhas a particular model, due to the absence of user-769specific opinion data for model estimation.Transfer Learning aims to help improve pre-dictive models by using knowledge from differentbut related problems (Pan and Yang, 2010).
Inthe opinion mining community, transfer learningis used primarily for domain adaptation.
Blitzeret al.
(2006) proposed structural correspondencelearning to identify the correspondences amongfeatures between different domains via the conceptof pivot features.
Pan et al.
(2010) propose a spec-tral feature alignment algorithm to align domain-specific sentiment words from different domainsfor sentiment categorization.
By assuming thatusers tend to express consistent opinions towardsthe same topic over time, Guerra et al.
(2011) ap-plied instance-based transfer learning for real timesentiment analysis.Our method is inspired by a personalized rank-ing model adaptation method developed by Wanget al.
(2013).
To the best of our knowledge, ourwork is the first to estimate user-level classifiersfor opinion analysis.
By adapting a generic opin-ion classification model for each user, heterogene-ity among their expressions of opinions can becaptured and it help us understand users?
opinionsat a finer granularity.3 Linear Transformation Based ModelAdaptationGiven a generic sentiment classification model y=fs(x), we aim at finding an optimal adapted modely = fu(x) for user u, such that fu(x) best cap-tures u?s opinion in his/her generated textual doc-uments Du={xd, yd}|D|d=1, where xdis the featurevector for document d, ydis the sentiment classlabel (e.g., positive v.s., negative).
To achieve so,we assume that such adaptation can be performedvia a series of linear transformations on fs(x)?smodel parameter ws.
This assumption is generaland can be applied to a wide variety of sentimentclassifiers, e.g., logistic regression and linear sup-port vector machines, as long as they have a linearcore function.
Therefore, we name our proposedmethod as LinAdapt.
In this paper, we focus onlogistic regression (Pang et al., 2002); but the pro-posed procedures can be easily adopted for manyother classifiers (Wang et al., 2013).Our global model y=fs(x) can be written as,Ps(yd= 1|xd) =11 + e?wsTxd(1)where wsare the linear coefficients for the corre-sponding document features.Standard linear transformations, i.e., scaling,shifting and rotation, can be encoded via a V ?
(V + 1) matrix Aufor each user u as:???????
?aug(1)cug(1),12cug(1),130 0 bug(1)cug(2),21aug(2)cug(2),23. .
.
0 bug(2)cug(3),31cug(3),32aug(3)...... bug(3)0 .
.
.
.
.
.
.
.
.......0 0 .
.
.
.
.
.
aug(V )bug(V )???????
?where V is the total number of features.However, the above transformation introducesO(V2) free parameters, which are even more thanthe number of free parameters required to estimatea new logistic regression model.
Following the so-lution proposed by Wang et al.
(2013), we furtherassume the transformations can be performed in agroup-wise manner to reduce the size of param-eters in adaptation.
The intuition behind this as-sumption is that features that share similar contri-butions to the classification model are more likelyto be adapted in the same way.
Another advantageof feature grouping is that the feedback informa-tion will be propagated through the features in thesame group while adaptation; hence the featuresthat are not observed in the adaptation data canalso be updated properly.We denote g(?)
as the feature grouping function,which maps V original features to K groups, andauk, bukand cukas the scaling, shifting and rotationoperations over wsin group k for user u.
In addi-tion, rotation is only performed for the features inthe same group, and it is assumed to be symmetric,i.e., cuk,ij= cuk,ji, where g(i) = k and g(j) = k.As a result, the personalized classification modelfu(x) after adaptation can be written as,Pu(yd= 1|xd) =11 + e?
(Auw?s)Txd(2)where w?s= (ws, 1) to accommodate the shiftingoperation.The optimal transformation matrix Aufor useru can be estimated by maximum likelihood esti-mation based on user u?s own opinionated docu-ment collection Du.
To avoid overfitting, we pe-nalize the transformation which increases the dis-crepancy between the adapted model and globalmodel by the following regularization term,R(Au) = ??2K?k=1(auk?
1)2?
?2K?k=1buk2?2K?k=1?i,g(i)=k?j 6=i,g(j)=kcuk,ij2, (3)where ?, ?
and  are trade-off parameters control-ling the balance among shifting, scaling and rota-tion operations in adaptation.770Combining the newly introduced regularizationterm for Auand log-likelihood function for logis-tic regression, we get the following optimizationproblem to estimate the adaptation parameters,maxAuL(Au) = LLR(Du;Pu) +R(Au) (4)whereLLR(Du;Pu) is the log-likelihood of logis-tic regression on collection Du, and Puis definedin Eq (2).Gradient-based method is used to optimizeEq (4), in which the gradient for auk, bukand cukcan be calculated as,?L(Au)?ak=Du?d=1{yd[1?
p(yd|xd)]?i,g(i)=kwsixdi}??(ak?
1)?L(Au)?bk=Du?d=1{yd[1?
p(yd|xd)]?i,g(i)=kxdi}??bk?L(Au)?ck,ij=Du?d=1{yd[1?
p(yd|xd)]wsjxdi}?ck,ij4 Experiments and DiscussionWe performed empirical evaluations of the pro-posed LinAdapt algorithm on a large collection ofproduct review documents.
We compared our ap-proach with several state-of-the-art transfer learn-ing algorithms.
In the following, we will first in-troduce the evaluation corpus and baselines, andthen discuss our experimental findings.4.1 Data Collection and BaselinesWe used a corpus of Amazon reviews providedon Stanford SNAP website by McAuley andLeskovec.
(2013).
We performed simple data pre-processing: 1) annotated the reviews with ratingsgreater than 3 stars (out of total 5 stars) as positive,and others as negative; 2) removed duplicate re-views; 3) removed reviewers who have more than1,000 reviews or more than 90% positive or neg-ative reviews; 4) chronologically ordered the re-views in each user.
We extracted unigrams and bi-grams to construct bag-of-words feature represen-tations for the review documents.
Standard stop-word removal (Lewis et al., 2004) and Porter stem-ming (Willett, 2006) were applied.
Chi-squareand information gain (Yang and Pedersen, 1997)were used for feature selection and the union ofthe resulting selected features are used in the fi-nal controlled vocabulary.
The resulting evalua-tion data set contains 32,930 users, 281,813 posi-tive reviews, and 81,522 negative reviews, whereeach review is represented with 5,000 text featureswith TF-IDF as the feature value.Our first baseline is an instance-based adapta-tion method (Brighton and Mellish, 2002).
The k-nearest neighbors of each testing review documentare found from the shared training set for person-alized model training.
As a result, for each test-ing case, we are estimating an independent clas-sification model.
We denote this method as ?Re-Train.?
The second baseline builds on the model-based adaptation method developed by Geng etal.
(2012).
For each user, it enforces the adaptedmodel to be close to the global model via an ad-ditional L2 regularization when training the per-sonalized model.
But the full set of parameters inlogistic regression need to estimated during adap-tation.
We denote this method as ?Reg-LR.
?In our experiments, all model adaptation is per-formed in an online fashion: we first applied theup-to-date classification model on the given test-ing document; evaluated the model?s performancewith ground-truth; and used the feedback to up-date the model.
Because the class distribution ofour evaluation data set is highly skewed (77.5%positive), it is important to evaluate the adaptedmodels?
performance on both classes.
In the fol-lowing comparisons, we report the average F-1measure of both positive and negative classes.4.2 Comparison of Adaptation PerformanceFirst we need to estimate a global model for adap-tation.
A typical approach is to collect a portionof historical reviews from each user to construct ashared training corpus (Wang et al., 2013).
How-ever, this setting is problematic: it already exploitsinformation from every user and does not reflectthe reality that some (new) users might not existwhen training the global model.
In our experi-ment, we isolated a group of random users forglobal model training.
In addition, since there aremultiple categories in this review collection, suchas book, movies, electronics, etc, and each usermight discuss various categories, it is infeasibleto balance the coverage of different categories inglobal model training by only selecting the users.As a result, we vary the number of reviews in eachdomain from the selected training users to estimatethe global model.
We started with 1000 reviewsfrom the top 5 categories (Movies & TV, Books,Music, Home & Kitchen, and Video Games), thenevaluated the global model on 10,000 testing userswhich consist of three groups: light users with 2 to10 reviews, medium users with 11 to 50 reviews,and heavy users with 51 to 200 reviews.
After eachevaluation run, we added an extra 1000 reviewsand repeated the training and evaluation.771Table 1: Global model training with varying sizeof training corpus.Model Metric 1000 2000 3000 4000 5000GlobalPos F1 0.741 0.737 0.738 0.734 0.729Neg F1 0.106 0.126 0.125 0.132 0.159LinAdaptPos F1 0.694 0.693 0.692 0.694 0.696Neg F1 0.299 0.299 0.296 0.299 0.304Table 2: Effect of feature grouping in LinAdapt.Method Metric 100 200 400 800 1000RandPos F1 0.691 0.692 0.696 0.686 0.681Neg F1 0.295 0.298 0.300 0.322 0.322SVDPos F1 0.691 0.698 0.704 0.697 0.696Neg F1 0.298 0.302 0.300 0.322 0.334CrossPos F1 0.701 0.702 0.705 0.700 0.696Neg F1 0.298 0.299 0.303 0.328 0.331To understand the effect of global model train-ing in model adaptation, we also included the per-formance of LinAdapt, which only used shiftingand scaling operations and Cross feature group-ing method with k = 400 (detailed feature group-ing method will be discussed in the next exper-iment).
Table 1 shows the performance of theglobal model and LinAdapt with respect to differ-ent training corpus size.
We found that the globalmodel converged very quickly with around 5,000reviews, and this gives the best compromise forboth positive and negative classes in both globaland adaptaed model.
Therefore, we will use thisglobal model for later adaptation experiments.We then investigated the effect of feature group-ing in LinAdapt.
We employed the feature group-ing methods of SV D and Cross developed byWang et al.
(2013).
A random feature groupingmethod is included to validate the necessity ofproper feature grouping.
We varied the numberof feature groups from 100 to 1000, and evaluatedthe adapted models using the same 10,000 testingusers from the previous experiment.
As shown inTable 2, Cross provided the best adaptation per-formance and random is the worse; a moderategroup size balances performance between positiveand negative classes.
For the remaining experi-ments, we use the Cross grouping with k = 400in LinAdapt.
In this group setting, we found thatthe average number of features per group is 12.47while the median is 12, which means that featuresare normally distributed across different groups.Next, we investigated the effect of differ-ent linear operations in LinAdapt, and com-pared LinAdapt against the baselines.
We startedLinAdapt with only the shifting operation, andthen included scaling and rotation.
To validatethe necessity of personalizing sentiment classifica-tion models, we also included the global model?sperformance in Figure 1.
In particular, to under-stand the longitudinal effect of personalized modeladaptation, we only used the heavy users (4,021users) in this experiment.
The results indicatethat the adapted models outperformed the globalmodel in identifying the negative class; while theglobal model performs the best in recognizing pos-itive reviews.
This is due to the heavily biasedclass distribution in our collection: global modelputs great emphasis on the positive reviews; whilethe adaptation methods give equal weights to bothpositive and negative reviews.
In particular, inLinAdapt, scaling and shifting operations lead tosatisfactory adaptation performance for the nega-tive class with only 15 reviews; while rotation isessential for recognizing the positive class.To better understand the improvement of modeladaptation against the global model in differenttypes of users, we decomposed the performancegain of different adaptation methods.
For this ex-periment, we used all the 10,000 testing users:we used the first 50% of the reviews from eachuser for adaptation and the rest for testing.
Ta-ble 3 shows the performance gain of different al-gorithms under light, medium and heavy users.For the heavy and medium users, which onlyconsist 0.1% and 35% of the total population inour data set, our adaptation model achieved thebest improvement against the global model com-pared with Reg-LR and ReTrain.
For the lightusers, who cover 64.9% of the total population,LinAdapt was able to improve the performanceagainst the global model for the negative class, butReg-LR and ReTrain had attained higher perfor-mance.
For the positive class, none of those adap-tation methods can improve over the global modelalthough they provide a very close performance (inLinAdapt, the differences are not significant).
Thesignificant improvement in negative class predic-tion from model adaptation is encouraging con-sidering the biased distribution of classes, whichresults in poor performance in the global model.The above improved classification performanceindicates the adapted model captures the hetero-geneity in expressing opinions across users.
Toverify this, we investigated textual features whosesentiment polarities are most/least frequently up-dated across users.
We computed the variance ofthe absolute difference between the learned featureweights in LinAdapt and global model.
High vari-ance indicates the word?s sentiment polarity fre-quently changes across different users.
But thereare two reasons for a low variance: first, a rare77210 20 30 40 50 60 70 80 90 10000.10.20.30.40.50.60.70.80.9F?Measure# adaption documentsshiftingshifting+scalingshifting+scaling+rotationGlobalReTrainReg?LR2 4 6 8 1000.10.20.30.40.50.60.70.8Zoomed Part10 20 30 40 50 60 70 80 90 10000.050.10.150.20.250.30.350.4F?Measure# adaption documentsshiftingshifting+scalingshifting+scaling+rotationGlobalReTrainReg?LR(a) Positive F-1 measure (b) Negative F-1 measureFigure 1: Online adaptation performance comparisons.Table 3: User-level performance gain over globalmodel from ReTrain, Reg-LR and LinAdapt.Method User Class Pos F1 Neg F1ReTrainHeavy -0.092 0.155?Medium -0.095 0.235?Light -0.157?0.255?Reg-LRHeavy -0.010 0.109?Medium -0.005 0.206?Light -0.060 0.232?LinAdaptHeavy -0.046 0.248?Medium -0.049 0.235?Light -0.091 0.117?
?p-value< 0.05 with paired t-test.Table 4: Top 10 words with the highest and lowestvariance of learned polarity in LinAdapt.Variance FeaturesHighestwaste good attemptmoney return savepoor worst annoyLowestlover correct purecare the product oddsex evil less thanword that is not used by many users; second, aword is being used frequently, yet, with the samepolarity.
We are only interested in the second case.Therefore, for each word, we compute its user fre-quency (UF), i.e., how many unique users usedthis word in their reviews.
Then, we selected 1000most popular features by UF, and ranked them ac-cording to the variance of learned sentiment polar-ities.
Table 4 shows the top ten features with thehighest and lowest polarity variance.We inspected the learned weights in the adaptedmodels in each user from LinAdapt, and foundthe words like waste, poor, and good share thesame sentiment polarity as in the global modelbut different magnitudes; while words like money,instead, and return are almost neutral in globalmodel, but vary across the personalized models.On the other hand, words such as care, sex, evil,pure, and correct constantly carry the same sen-Table 5: Learned sentiment polarity range of threetypical words in LinAdapt.Feature Range Global Used as Used asWeight Positive NegativeExperience [-0.231,0.232] 0.002 3348 1503Good [-0.170,0.816] 0.032 8438 1088Money [-0.439,0.074] -0.013 646 6238timent across users.
Table 5 shows the detailedrange of learned polarity for three typical opin-ion words in 10,000 users.
This result indicatesLinAdapt well captures the fact that users expressopinions differently even with the same words.5 Conclusion and Future WorkIn this paper, we developed a transfer learningbased solution for personalized opinion mining.Linear transformations of scaling, shifting and ro-tation are exploited to adapt a global sentimentclassification model for each user.
Empiricalevaluations based on a large collection of opin-ionated review documents confirm that the pro-posed method effectively models personal opin-ions.
By analyzing the variance of the learnedfeature weights, we are able to discover wordsthat hold different polarities across users, whichindicates our model captures the fact that usersexpress opinions differently even with the samewords.
In the future, we plan to further explorethis linear transformation based adaptation fromdifferent perspectives, e.g., sharing adaptation op-erations across users or review categories.6 AcknowledgementsThis research was funded in part by grantW911NF-10-2-0051 from the United States ArmyResearch Laboratory.
Also, Hongning Wang ispartially supported by the Yahoo Academic CareerEnhancement Award.773ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 confer-ence on empirical methods in natural language pro-cessing, pages 120?128.
Association for Computa-tional Linguistics.Freimut Bodendorf and Carolin Kaiser.
2009.
Detect-ing opinion leaders and trends in online social net-works.
In Proceedings of the 2nd ACM workshop onSocial web search and mining, pages 65?68.
ACM.Henry Brighton and Chris Mellish.
2002.
Advancesin instance selection for instance-based learning al-gorithms.
Data mining and knowledge discovery,6(2):153?172.Pedro Henrique Calais Guerra, Adriano Veloso, Wag-ner Meira Jr, and Virg?
?lio Almeida.
2011.
Frombias to opinion: a transfer-learning approach to real-time sentiment analysis.
In Proceedings of the 17thACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 150?158.ACM.Kushal Dave, Steve Lawrence, and David M Pennock.2003.
Mining the peanut gallery: Opinion extractionand semantic classification of product reviews.
InProceedings of the 12th international conference onWorld Wide Web, pages 519?528.
ACM.Michael Gamon, Anthony Aue, Simon Corston-Oliver,and Eric Ringger.
2005.
Pulse: Mining customeropinions from free text.
In Advances in IntelligentData Analysis VI, pages 121?132.
Springer.Bo Geng, Yichen Yang, Chao Xu, and Xian-ShengHua.
2012.
Ranking model adaptation for domain-specific search.
Knowledge and Data Engineering,IEEE Transactions on, 24(4):745?758.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.ACM.Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-dur Chowdury.
2009.
Twitter power: Tweets aselectronic word of mouth.
Journal of the Ameri-can society for information science and technology,60(11):2169?2188.Yohan Jo and Alice H Oh.
2011.
Aspect and senti-ment unification model for online review analysis.In Proceedings of the fourth ACM international con-ference on Web search and data mining, pages 815?824.
ACM.Soo-Min Kim and Eduard Hovy.
2004.
Determin-ing the sentiment of opinions.
In Proceedings ofthe 20th international conference on ComputationalLinguistics, page 1367.
Association for Computa-tional Linguistics.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.2006.
Opinion extraction, summarization and track-ing in news and blog corpora.
In AAAI springsymposium: Computational approaches to analyz-ing weblogs, volume 100107.David D Lewis, Yiming Yang, Tony G Rose, and FanLi.
2004.
Smart stopword list.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Julian McAuley and Jure Leskovec.
2013.
Hidden fac-tors and hidden topics: understanding rating dimen-sions with review text.
In Proceedings of the 7thACM conference on Recommender systems, pages165?172.
ACM.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
Knowledge and Data Engineer-ing, IEEE Transactions on, 22(10):1345?1359.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain sen-timent classification via spectral feature alignment.In Proceedings of the 19th international conferenceon World wide web, pages 751?760.
ACM.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, pages 79?86.
As-sociation for Computational Linguistics.Hongning Wang, Yue Lu, and Chengxiang Zhai.
2010.Latent aspect rating analysis on review text data:a rating regression approach.
In Proceedings ofthe 16th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 783?792.
ACM.Hongning Wang, Xiaodong He, Ming-Wei Chang,Yang Song, Ryen W White, and Wei Chu.
2013.Personalized ranking model adaptation for websearch.
In Proceedings of the 36th internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 323?332.
ACM.Peter Willett.
2006.
The porter stemming algorithm:then and now.
Program, 40(3):219?223.Yiming Yang and Jan O Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In ICML, volume 97, pages 412?420.Dingqi Yang, Daqing Zhang, Zhiyong Yu, and ZhuWang.
2013.
A sentiment-enhanced personalizedlocation recommendation system.
In Proceedings ofthe 24th ACM Conference on Hypertext and SocialMedia, pages 119?128.
ACM.774
