Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1670?1681, Dublin, Ireland, August 23-29 2014.Generating Supplementary Travel Guides from Social MediaLiu Yang1,2, Jing Jiang2,?, Lifu Huang1,2, Minghui Qiu2, Lizi Liao2,31Peking University / Beijing, China, 1008712Singapore Management University / Singapore, Singapore, 1789023Beijing Institute of Technology / Beijing, China, 100081yang.liu@pku.edu.cn, jingjiang@smu.edu.sg{warrior.fu, minghuiqiu, liaolizi.llz}@gmail.comAbstractIn this paper we study how to summarize travel-related information in forum threads to gener-ate supplementary travel guides.
Such summaries presumably can provide additional and moreup-to-date information to tourists.
Existing multi-document summarization methods have limita-tions for this task because (1) they do not generate structured summaries but travel guides usuallyfollow a certain template, and (2) they do not put emphasis on named entities but travel guidesoften recommend points of interest to travelers.
To overcome these limitations, we propose touse a latent variable model to align forum threads with the section structure of well-written travelguides.
The model also assigns section labels to named entities in forum threads.
We thenpropose to modify an ILP-based summarization method to generate section-specific summaries.Evaluation on threads from Yahoo!
Answers shows that our proposed method is able to generatebetter summaries compared with a number of baselines based on ROUGE scores and coverageof named entities.1 IntroductionOnline forums and community question answering (CQA) sites contain much useful information fromordinary users, such as their personal experience, opinions, suggestions and recommendations.
Extract-ing and summarizing information from these rich information sources has a wide range of applications.In this work, we study how to tap into user-generated content in forums such as Yahoo!
Answers togenerate supplementary city travel guides.
Travel guides published by well-known publishers such asLonely Planet are written by a small number of authors based on their travel experience.
Presumablyif we could summarize the large amount of information given by ordinary users about a city, such asummary could supplement the official travel guide and cover more up-to-date information.However, social media content is diverse and noisy because it is contributed by many different au-thors.
Directly applying existing multi-document summarization methods to forum and CQA threadsmay not produce good travel guides for the following reasons: (1) Summaries produced by standardsummarization methods are not structured, but travel guides usually follow a template structure.
(2)Travel guides put much emphasis on points of interest, which are usually location entities, but standardtext summarization methods are not entity-oriented.To illustrate our points, in Table 1 we show (i) the overall structure of a travel guide for Sydney fromLonely Planet, (ii) an excerpt from a summary generated by a state-of-the-art ILP-based summarizationmethod (Gillick and Favre, 2009) from a set of threads related to Sydney, and (iii) excerpts of a structuredsummary generated by our proposed method.
The comparison shows that the summary generated bythe standard ILP method mixes information on different topics together and does not mention many* Corresponding author.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1670Travel Guide from Lonely Planet (http://www.lonelyplanet.com/australia/sydney/)Restaurants:Sepia:There?s nothing washed out or brown-tinged about Sepia?s food: Martin Benn?s picture-perfect creations are presented in .
.
.Icebergs Dining Room: Poised above the famous Icebergs swimming pool, Icebergs views sweep across the Bondi Beach arc to .
.
.Shopping:Strand Arcade: Constructed in 1891, the Strand rivals the QVB in the ornateness stakes.
Three floors of designer fashions .
.
.Westfield Sydney: The city?s newest shopping mall is a bafflingly large complex gobbling up Sydney Tower and a fair chunk of .
.
.Transport:Sydney Airport: Sydneys Kingsford Smith Airport , 10km south of the city centre, is Australias busiest airport, handling flights .
.
.Water Taxis Combined:Fares based on up to four passengers; add $10 per person for additional passengers.
Sample fares .
.
.Yahoo!
Answers Summary Generated by Standard ILP Method Yahoo!
Answers Summary Generated by Our MethodIt ?s not too far from Sydney .
Sydney is the most expensiveplace in Australia .
They are a little lame ... Then you can go toDarling Harbour, a beautiful habour which is a 10-minute walkfrom town hall station .
Make sure , if you are up to it to do thebridge climb , this is a real treat .
There are lots of interestingthings to see and do in and around Sydney .
The suburbs-muchcheaper than the CBD.
It was in the basement of a big shoppingmall .
The only way to do that is to drive .
Got to walk on top ofthe Sydney harbour bridge and go up centre point tower !
Walkaround the street and see the beach .
I would like to stay at a nicehotel .
My friend and I are wanting to take a trip to Sydney forthe summer .
But you ?ll need to get there by taxi .
Sydney is sopretty, so you should be able to find stuff to do .
And they havemany facilities .
Good luck and have fun .
Public transport is notvery good .
Depending on what you ?re in Sydney to do it ?s hardto say .
.
.Restaurants:Go to the two major restaurant areas close to the city Dar-linghurst , along Oxford Street , and Newtown , along KingStreet .
Chinatown which is off George St. in the city look upDixon st. is a great place to get a cheap Chinese meal .
.
.Shopping:Queen Victoria Building and Pitt St Mall , World Square andthe Strand are good ideas to check out .
Hair driers you can getin many places , but the main places would be the departmentstores such as Target , Big W , K-Mart , Myer, David Jones .
.
.Transport:The CBD is about 15 minutes by train from the airport and thereis a station at Circular Quay , right on the Harbour with accessto the bridge and the Opera House .
You can catch an intercitytrain with Cityrail from just about anywhere in Sydney .
.
.Table 1: Comparison of different travel guides about Sydney.
Top: excerpts from Lonely Planet.
Bottom left: excerpt froma summary generated by standard ILP.
Bottom right: excerpts from summary generated by our method.
Named entities arehighlighted in bold font.interesting places to visit.
The summary by our proposed method, in contrast, organizes the informationinto sections and has a high coverage of places a tourist can visit.To generate the kind of summaries as shown in the bottom right of Table 1, we propose to first leveragethe section structure of well-written travel guides and use a latent variable model to align forum threadswith the different sections from these travel guides.
Moreover, observing that points of interest are orga-nized by sections in these travel guides, we also identify location names from user-generated content andtry to uncover their underlying section labels.
We then treat the remaining problem as a multi-documentsummarization task.
We modify an Integer Linear Programming (ILP)-based extractive summarizationframework (Gillick and Favre, 2009) to select sentences from forum threads to generate section-specificsummaries, where we specifically emphasize the inclusion of potential points of interest for each sec-tion.
Experiments using threads from Yahoo!
Answers show that our proposed method generates bettersummaries than a number of baselines in terms of ROUGE scores and coverage of named entities.Our work makes the following contributions.
First, we study a new problem of summarizing multipleforum threads to generate city travel guides based on known template structure from well-written travelguides.
Second, we propose a principled approach based on latent variable models and Integer LinearProgramming.
Third, we evaluate our method using real forum threads and human generated modelsummaries, and the results are positive.2 Overview of Our MethodOur task is to summarize travel-related information from forum threads for potential tourists.
In orderto inject some structure into the generated summaries, we assume that we have a set of I well-writtentravel guides that correspond to I different cities and have the same structure.
We refer to these travelguides as official travel guides.
Each official travel guide consists of a fixed set of S sections such asrestaurants and shopping, and this section structure will be used to organize our generated summaries.We further assume that each section of an official travel guide consists of a list of points of interest, eachwith a name and a short description, as illustrated in Figure 1.
We believe that this is a fairly commonstructure followed by many if not all travel guides.Given a target city, we assume that we can collect a set of threads about this city from travel-relatedforums.
In this paper we use threads from Yahoo!
Answers, but our solution does not use any CQAproperties of the threads, so threads from other general forums can also be used.
Our goal is to generatea text summary with S sections from these threads, where each section has a length limit.1671As we have mentioned, we treat the problem as a multi-document summarization task.
However,different from standard text summarization, our generated summaries should contain S sections.
Toachieve this goal, we first select a set of relevant threads for each section and then perform section-specific summarization from the selected threads.Thread selection: To select relevant threads given a section, a naive solution is to rank the threads basedon their relevance to the section, where relevance can be measured by, for example, cosine similaritybetween a thread and all the text in the given travel guides belonging to the section.
But we observe thatthe language used in forum threads could be very different from that in the official travel guides, makingit hard to measure relevance purely based on lexical overlap.
For example, in the entertainment section,forum threads may contain words such as ?djs,?
?Xmas,?
?b?day?
and ?anni.
?, but these words do notoccur in the official travel guides.
To overcome this difficulty, we propose to use a latent variable modelthat jointly models official travel guides and forum threads.
We treat the S sections as S latent factorsthat govern the generation of the forum threads.
With the latent factors observed in the official travelguides, we receive some supervision; and yet by jointly modeling both the official travel guides and theforum threads, we allow the latent factors to adapt to the lexical variations in user-generated content.
Inthe end, the learned latent factors can help us align forum threads with the sections and subsequentlyselect the most relevant ones for each section.Section-specific summarization: Given the selected relevant threads for a section, we adopt an ILP-based extractive summarization framework that has been shown to be effective (Gillick and Favre, 2009).We modify the objective function in this framework to consider two factors: (1) Since not every sentencein the selected threads is highly relevant to the section, we want to give preference to those more relevantsentences in the objective function, where relevance can be measured using word distributions learnedby the latent variable model.
(2) Since travel guides are expected to recommend points of interest toreaders, we try to maximize the coverage of section-specific location entities in the objective function.3 Joint City Section Model3.1 ModelIn this section we present our Joint City Section Model (JCSM), which links official travel guides andforum threads.
The model is a typical extension of LDA, where a number of latent topics (i.e.
latentfactors) are assumed to have generated the observed text.
First of all, for each pre-defined section thereis a latent topic.
These explain words such as ?food?
and ?menu?
for restaurants and ?store?
and ?mall?for shopping.
In addition, in both travel guides and forum threads, some words are more related to thecity being discussed than any specific section.
For example, when New York City is being discussed,words such as ?NYC?
and ?Manhattan?
may frequently show up in any section.
We therefore furtherassume that for each city there is a city-specific topic.
A switch variable is used to determine whether aword comes from a city-specific or section-specific topic.A special design of our model that differs from many existing LDA extensions is the treatment ofnamed entities.
We first use a named entity recognizer to identify potential names of locations fromforum threads.
We assume that each of these entities belongs to a section, which is indicated by a latentvariable.
We then assume that the section labels of the non-entity words in forum threads are dependenton the section labels of these entities.
By doing so, we emphasize the importance of associating potentialpoints of interest with sections, which will be useful when we generate summaries.We now formally present JCSM.
To simplify the model description, we assume that we work withI cities, each of which has a given, well-written travel guide and a set of forum threads.
Note that inpractice this model can be easily extended such that a target city with forum threads does not need to havea given travel guide to begin with.
Let ?idenote the word distribution for the city-specific latent topicassociated with city i.
Let ?sdenote the word distribution for the section-specific latent topic for sections.
Let di,s,ndenote the n-th word in the s-th section of the i-th city?s travel guide.
Here 1 ?
di,s,n?
V isan index into the vocabulary with size V .
Let xi,s,nbe a switch variable associated with di,s,nto indicatewhether this word is city-specific or section-specific.
For the j-th forum thread related to the i-th city, weassume there is a distribution over sections, denoted as ?i,j.
For the l-th location entity in the k-th post1672of this thread, we assume a latent variable ci,j,k,l(1 ?
ci,j,k,l?
S) that indicates the section label of thisentity.
Then for the m-th word in this post, we first use a switch variable yi,j,k,mto determine whetherthe word is city-specific or section-specific.
If it is section-specific, we then choose one of the entities inthe same post, denoted as zi,j,k,m, and its corresponding section label as the section for this word.All the binary switch variables follow a global Bernoulli distribution parameterized by pi.
There arehyperparameters ?, ?, ?
?and ?
that define the prior distributions.
The complete model is depicted inFigure 1.
The generative process of JCSM is also described as follows.SN M LKJ Id   w c ?x y z?Figure 1: The plate notation of the Joint City Section Model (JCSM).
Dashed variables will be integrated out in Gibbs sampling.For clarity, the Dirichlet and Beta priors are omitted.
The arrow pointing to z indicates that z is drawn from a uniformdistribution over the integers from 1 to L.?
For each city i, (i = 1, 2, ?
?
?
, I), draw a city-specific word distribution ?i?
Dir(??)?
For each section s, (s = 1, 2, ?
?
?
, S), draw a section-specific word distribution ?s?
Dir(?)?
Draw a switch distribution pi ?
Beta(?)?
For each city i (i = 1, 2, ?
?
?
, I)?
For each section s (s = 1, 2, ?
?
?
, S)?
For the n-th word in the given travel guide- Draw xi,s,n?
Bernoulli(pi)- If xi,s,n= 1, draw di,s,n?
Multi(?s); otherwise, draw di,s,n?
Multi(?i).?
For the j-th thread?
Draw a thread specific section distribution ?j?
Dir(?)?
For the k-th post- For the l-th entity, draw ci,j,k,l?
Multi(?j)- For the m-th word, draw yi,j,k,m?
Bernoulli(pi).
If yi,j,k,m= 1, draw zi,j,k,m?
Uniform(1, ?
?
?
, Li,j,k)and then draw wi,j,k,m?
Multi(?ci,j,k,zi,j,k,m); otherwise, draw wi,j,k,m?
Multi(?i).3.2 InferenceWe use collapsed Gibbs sampling to estimate the parameters in the model.
The problem is to computethe Gibbs update rules for sampling xi,s,n, ci,j,k,l, zi,j,k,m, yi,j,k,m.Sample entity topic ci,j,k,lLet b denote {i, j, k, l} and u denote {i, j, k}.
We can derive the Gibbs update rule for sampling entitytopic ci,j,k,las follows:p(cb= s|C?b,W,D,X,Y,Z) =nsi,j,?b+ ?
?Ss?=1ns?i,j,?b+ S??
?Vw=1?nwu,y=1,z=li?=1(nwy=1,z=l,?u+ ?
+ i??
1)?nwy=1,z=l,uj?=1(?Vw=1nwy=1,z=l,?u+ V ?
+ j??
1),where nsi,j,?bdenotes the number of entities whose topic assignments are s in thread {i, j} withoutconsideration of entity {i, j, k, l}.
nwu,y=1,z=ldenotes the number of times term w occurs in the post{i, j, k} with the constraint that y = 1 and z = l. nwy=1,z=l,?uis the number of times term w occurs inall posts except the post {i, j, k} with the constraint that y = 1 and z = l.Sample switch label xi,s,nWe can derive the Gibbs update rule for sampling xi,s,nin a similar way.
Note that the sampling ofxi,s,nis in travel guide word level.
Let g denote{i, s, n}, the Gibbs update rule for sampling xi,s,nis asfollows:1673p(xg= 0|C,W,D?g,X?g,Y,Z) =nx=0?g+ ?
?1x=0nx?g+ 2?
?nwgx=0,i,?g+ ??
?Vw=1nwx=0,i,?g+ V ?
?p(xg= 1|C,W,D?g,X?g,Y,Z) =nx=1?g+ ?
?1x=0nx?g+ 2?
?nwgx=1,s,?g+ ?
?Vw=1nwx=1,s,?g+ V ?Sample post word topic zi,j,k,mand switch label yi,j,k,mFor words in the thread posts, We can derive the Gibbs update rule for sampling post word topic zi,j,k,mand switch label yi,j,k,m.
Note that the sampling of zi,j,k,mand yi,j,k,mis in post word level.
Let fdenote{i, j, k,m}.
The Gibbs update rule for sampling zi,j,k,mand yi,j,k,mis as follows:p(zf= s|C,W?f,D,X,Y?f,Z?f) =nwfy=1,s?,?f+ ?
?Vw=1nwy=1,s?,?f+ V ?
?1Li,j,kp(yf= 0|C,W?f,D,X,Y?f,Z?f) =ny=0?f+ ?
?1y=0ny?f+ 2?
?nwfy=0,i,?f+ ??
?Vw=1nwy=0,i,?f+ V ?
?p(yf= 1|C,W?f,D,X,Y?f,Z?f) =ny=1?f+ ?
?1y=0ny?f+ 2?
?nwfy=1,s?,?f+ ?
?Vw=1nwy=1,s?,?f+ V ?where s?= ci,j,k,lwhich is the topic index of the associated entity of this word.Parameter estimationAfter Gibbs Sampling, we can make the following parameter estimation:?i,j,s=nsi,j+ ?
?Ss?=1ns?i,j+ S?.
thread-section distribution.
?s,w=nws,y=1+ ?
?Vw?=1nw?s,y=1+ V ?.
section-word distribution.
?i,w=nwi,y=0+ ??
?Vw?=1nw?i,y=0+ V ??.
city-word distribution.piy=ny(.
)+ ??1y?=0ny?(.
)+ 2?.
switch distribution.4 Generating Section-specific SummariesWith the JCSM model presented in the last section, we can learn a word distribution for each section,which can help us find more relevant content for the section.
For each section, we rank the forumthreads by how likely the words inside a thread is generated from the corresponding section-specific worddistribution.
We select the top-K threads for each section to perform section-specific summarization.Extractive summarization has been well studied and many algorithms have been proposed.
We chooseto build our solution on top of an ILP-based framework proposed by Gillick and Favre (2009), partlybecause our experiments comparing this ILP framework and other existing methods show its advantageon our data sets (see Section 5).
Below we first briefly review this ILP-based summarization frameworkand then present our proposed improvements.The idea behind the ILP framework by Gillick and Favre (2009) is to maximize the coverage of so-called ?concepts?
from the original corpus in the generated summary.
In practice, bigrams are used asconcepts.
Specifically, let us use i to index all the concepts from the original corpus.
Let widenotethe weight of the i-th concept computed based on its frequency and bi?
{0, 1} denote the absence or1674presence of the concept.
The framework aims to maximize?iwibi, i.e.
the total weighted coverage ofthe concepts, subject to the following constraints:?jljsj?
L, (ljis the length of the j-th sentence in terms of words, and L is the length limit of the summary.
)?i, j : sjoi,j?
bi, (sj?
{0, 1} denotes the absence or presence of the j-th sentence.
)?i :?jsjoi,j?
bi.
(oi,j?
{0, 1} denotes whether concept i occurs in sentence j.
)Although this framework works well for standard summarization, our task is different.
We propose thefollowing changes to this framework:Favoring relevant sentences: Recall that although we select presumably the most relevant threads foreach section, we cannot guarantee that each sentence in these threads is related to the section.
Forexample, we observe that the things-to-do section is often mixed with content from restaurants, sights,transport and entertainment sections.
Also, some sentences are less relevant to the target city thanothers.
In order to select the more relevant sentences in the summary, we propose to add the second termin Eqn.
1 below.
Here j is used to index all the candidate sentences and ujis a weight for sentence jbased on its relevance.We measure relevance with respect to both the city and the section.
Let LL(j, ?)
denote the log like-lihood of generating sentence j from the section-specific topic ?
and LL(j, ?)
denote the log likelihoodof generating sentence j from the city-specific topic ?.
We define ujas follows:uj?
exp (?LL(j, ?)
+ (1?
?
)LL(j, ?))
.ujare then normalized to be between 0 and 1.
Note that here ?
is a manually defined parameter used tocontrol the tradeoff between city-specific relevance and section-specific relevance.
As we will show inSection 5, both relevance factors turn out to be useful.Covering section-specific points of interest: We hypothesize that a good summary travel guide shouldmention potential points of interest to the reader.
To this end, the last term in Eqn.
1 is added.
Specifically,k is an index for unique location names we find that have been labeled as belonging to section s accordingto the JCSM model.
ek?
{0, 1} denotes whether the k-th entity is present in the selected sentences, andvkdenotes the weight for this entity based on its frequency.Eventually, the summarization task is formulated as the following optimization problem:Maximize: ?1?iwibi+ ?2?jujsj+ (1?
?1?
?2)?kvkek(1)Subject to:?jljsj?
L,?i :?jsjoi,j?
bi, ?i, j : sjoi,j?
bi,?j :?ksjpj,k?
ek, ?j, k : sjpj,k?
ek.Here oi,jdenotes whether concept i occurs in sentence j, and pj,kdenotes whether entity k occurs insentence j.
For the weights wiand vk, we normalize them using the total occurrences of bigrams/entitiesto ensure their values are between 0 and 1.
We solve the above optimization problem using the IBMILOG CPLEX Optimizer1.5 Experiments5.1 Data and Experimental SetupWe use real data from Yahoo!
Answers and Lonely Planet for evaluation.
We first crawl the travel guidesfor 10 cities from Lonely Planet, where each travel guide has 8 sections.
We then crawl the top 60000Q&A threads ranked by number of posts related to these 10 cities (6000 for each city) from Yahoo!Answers under the ?travel?
category where all questions have been grouped by cities.
We filter outtrivial factoid questions using features used by Tomasoni and Huang (2010).
We then use the Stanford1http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/1675Method Singapore Sydney New York City Los Angeles Overall AverageR-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4Random 0.4091 0.1046 0.1576 0.4496 0.1100 0.1925 0.4442 0.1192 0.1858 0.4154 0.1130 0.1693 0.4309 0.1115 0.1771Centroid 0.4029 0.0993 0.1484 0.4228 0.1100 0.1764 0.4235 0.1192 0.1722 0.3763 0.0787 0.1386 0.4133 0.1077 0.1640LexRank 0.4396 0.1451 0.1891 0.4406 0.1296 0.1955 0.4304 0.1397 0.1859 0.4032 0.0992 0.1661 0.4350 0.1331 0.1894DivRank 0.4534 0.1504 0.1888 0.4473 0.1161 0.1925 0.4391 0.1167 0.1804 0.4275 0.1180 0.1733 0.4487 0.1317 0.1888GMDS 0.3918 0.0890 0.1415 0.4339 0.1066 0.1784 0.4064 0.0845 0.1576 0.3846 0.0809 0.1413 0.4045 0.0916 0.1553ILP-BL 0.4635 0.1650 0.2000 0.4948 0.1731 0.2333 0.4691 0.1613 0.2073 0.4545 0.1445 0.1981 0.4755 0.1654 0.2136Our Method 0.4723 0.1655 0.2035 0.5078 0.1787 0.2397 0.4716 0.1713 0.2086 0.4543 0.1565 0.1945 0.4804?0.1715?0.2144?Table 2: Comparison of the summarization results.
?means the result is better than others except ILP-BL in the same columnat 5% significance level measured by Wilcoxon signed rank test.
Note that only the average scores are tested for statisticalsignificance based on the 32 summarization tasks in total.NER tool to recognize named entities in these threads.
Since we notice that sometimes entities tagged asPER are also possible points of interest, we include all entities of LOC, ORG and PER types.
In orderto use higher quality threads for evaluation, for each city we pick the top 600 threads that have the mostoverlapping points of interest with the Lonely Planet travel guides.
On average, each thread contains 5.0posts and 618.1 words.
These 600?
10 threads are used to train the JCSM model.We need human generated model summaries for evaluation.
Since it is too time consuming to askhuman annotators to look through 600 threads and generate structured summaries, we instead opt tofirst retrieve the top 30 relevant threads per section per city based on the JCSM results and then askhuman annotators to summarize these 30 threads to generate a section-specific summary.
Our summa-rization method as well as the baselines are also applied to these 30 threads per section per city for faircomparison.
We randomly select 4 cities for human annotation, giving us 8 ?
4 = 32 section-specificsummarization tasks.
For each task, we ask four annotators to read all 30 threads and write a summaryas model summaries in our experiments2.We use the following baseline algorithms for comparison: (1) Random, which randomly picks sum-mary sentences.
(2) Centroid (Radev et al., 2004), which selects sentences according to several featureslike tfidf, cluster centroid and position.
(3) LexRank (Erkan and Radev, 2004b)., which applies a graph-based algorithm .
(4) DivRank (Mei et al., 2010), which employs a time-variant random walk to enhancediversity.
(5) GMDS (Wan, 2008), which incorporates the document-level information and the sentence-to-document relationship into the ranking process.
(6) ILP-BL, which is the method proposed by Gillickand Favre (2009).We empirically set Dirichlet hyperparameters ?
= 0.5, ?
= 0.01, ?
= 0.01, ?
?= 0.1.
We run JCSMwith 400 iterations of Gibbs sampling.
For the weight parameters in the ILP model, we empirically set?1= 0.7, ?2= 0.1, ?
= 0.7 after we conduct multiple experiments to determine the best values of themfrom 0.1 to 0.9.5.2 Summarization ResultsTo compare the summaries generated by our method with those generated by the baselines, we firstcompute their ROUGE scores against the human generated model summaries.
ROUGE scores havebeen widely used for evaluation of summarization systems (Lin and Hovy, 2003).
We use the ROUGEtoolkit3, which provides multiple kinds of ROUGE metrics including ROUGE-N, ROUGE-L, ROUGE-W and ROUGE-SU4.
In the experiment results we report three ROUGE F-measure scores, namely,ROUGE-1, ROUGE-2 and ROUGE-SU4.
The higher the ROUGE scores, the better a summary is.In Table 2 we show the summarization results of our method (with the optimal parameter setting) andthe baseline methods.
For each city, the scores we show are averaged over the 8 sections.
The overallaverage scores on the right hand side are averaged over the 4 cities.
We have the following findings fromthe table: (1) Compared with the other baselines, the ILP-based baseline clearly shows its advantage,justifying our our design choice of adopting an ILP-based framework as the basis of our method.
(2)Our method performs slightly better than ILP-BL based on the overall scores, but the difference is notstatistically significant.2The summary dataset can be found at https://sites.google.com/site/liuyang198908/code-data.3http://www.isi.edu/licensed-sw/see/rouge/1676section Singapore Sydney New York City Los AngelesILP-BL Our Method ILP-BL Our Method ILP-BL Our Method ILP-BL Our Methodrestaurants 0.3750 0.5417 0.5714 0.7143 0.2500 0.3750 0.1053 0.2105hotels 0.4091 0.4091 0.0000 0.5000 0.3636 0.5000 0.4500 0.5500shopping 0.1429 0.5357 0.3750 0.3750 0.1905 0.1905 0.0455 0.1818sights 0.5000 0.5789 0.3846 0.4615 0.3636 0.6364 0.1143 0.2571entertainment 0.1304 0.2174 0.2500 0.7500 0.0909 0.2273 0.2500 0.4167activities 0.4167 0.5833 0.2500 0.2500 0.1250 0.5000 0.2069 0.2759transport 0.3889 0.5556 0.7500 0.7500 0.6000 0.8000 0.3158 0.7368things-to-do 0.2105 0.2632 0.2500 0.5500 0.4583 0.5833 0.0000 0.2000average 0.3217 0.4606 0.3539 0.5439 0.3052 0.4766 0.1860 0.3536Table 3: Comparison of the recall of named entities of ILP-BL and our method.Method Our Complete Model ?EC ?SR ?SecRel ?CityRelR-1 0.4804 0.4520 0.4657 0.4672 0.4796R-2 0.1715 0.1430 0.1669 0.1652 0.1685RSU4 0.2144 0.1987 0.2028 0.2039 0.2120Table 4: Summarization results of the degenerate versions of our method.
???
means removing this component from ourcomplete method.
The table shows the average results over data sets of all cites.Considering that an importance difference between our method and ILP-BL is our focus on pointsof interest, we further compared ILP-BL and our method using a different metric.
The objective is totest the coverage of points of interest in our generated summaries versus the summaries generated byILP-BL.
To this end, we first identify all the named entities in the model summaries using the StanfordNER tool.
We then check the percentage of these named entities covered in the generated summaries andreport these recall scores in Table 3.
We can see that for majority of the 32 section-specific summaries,our method clearly has a higher recall score than ILP-BL, showing that our method generates summarieswith more potential points of interest.To further understand whether all the components of our improved ILP method have contributed tothe performance improvement, we compare our overall method with a few degenerate versions of ourmethod.
In each degenerate version, we remove a single component of the objective function.
The resultsare shown in Table 4, where?EC removes the consideration of entity coverage (i.e.
setting ?1+?2= 1),?SR removes the consideration of sentence relevance (i.e.
setting ?2= 0), ?SecRel removes only thesection-specific relevance of the sentences (i.e.
setting ?
= 0), and ?CityRel removes only the city-specific relevance of the sentences (i.e.
setting ?
= 1).
We can see that each degenerate version of ourmethod performs worse than the complete method, which shows that all components of the objectivefunction are useful.
In particular, entity coverage and section-specific relevance seem to be the moreimportant components.00.20.40.60.81 00.20.40.60.810.40.50.6lambda2lambda10.445 0.450.455 0.460.465 0.470.475 0.480.485 0.490.495(a) R-1(?1, ?2)00.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1rhoR-1R-2RSU4(b) ?Figure 2: Summarization performance of our method by varying the value of the parameters ?1, ?2and ?.16775.3 Analysis of Topic WordsWe show some further analysis of our results.
To begin with, we analyze the learning results of JCSM.The top words in city-specific word distributions and section-specific word distributions learnt by JCSMare presented in Table 5 and Table 6.
Generally we observe clean top words for each city and eachsection.
For each city, city-specific words are those associated with the corresponding city.
For example,for Singapore, we see words such as ?s$?
(Singapore dollars), ?sentosa?
(an island resort in Singapore),?orchard?
(a boulevard that is the retail and entertainment hub of Singapore) and ?bugis?
(a popularshopping place).
For New York City, we see ?square?, ?times?
and ?manhattan?.
For each section,section-specific words are those words which frequently appear when people discuss about this section,such as ?menu?, ?dishes?
and ?
seafood?
for the restaurant section and ?train?, ?bus?
and ?station?
forthe transport section.Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9 Topic 10Singapore SFO Chicago Boston LA NYC Seattle Pairs London Sydneysingapore sf chicago boston beach york downtown paris london sydneys$ san downtown end hollywood nyc seattle de tube harbourcentre francisco park north los park needle metro underground beachfood gate city downtown angeles central space eiffel central manlyshopping golden neighborhood fenway la square market french centre beachessentosa bay north bay downtown times rain la british houseroad bart lake harvard drive manhattan place du palace operaorchard union mile place california broadway pike tower thames quaychinese wharf loop city miles city center des end australianmrt muni ave college hills street waterfront rue kensington rocksbugis square field subway long east area le station bridgeTable 5: Top city specific words discovered by JCSM.Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8restaurants hotels shopping sights entertainment activities transport thingsToDofood hotel shop museum bar visit train barrestaurant rooms store city music park bus placemenu free stores park club tour station tourdishes wi-fi shopping art place fun airport cityplace walk shops building night city time foodbar located find built dance walk line artchicken offers clothes world beer day car dayfish station wear place clubs time walk includingfresh features mall house crowd shopping minutes musicseafood tv place area bars museum hours restaurantTable 6: Top section specific words discovered by JCSM.5.4 Parameter Sensitivity AnalysisWe further give parameter sensitivity analysis for our proposed method.
We show how sensitive ourresults are with respect to the parameters ?1, ?2and ?.
We choose the Sydney data set to performparameter sensitivity analysis.
In Figure 2(a), we show how ROUGE-1 varies with respect to ?1and ?2.We can see that the performance fluctuates within a limited range as we vary ?1and ?2.
We find thetrend for ROUGE-2 and ROUGE-SU4 is similar so we leave out the figures for them.
In Figure 2(b) wesee that the performance is pretty stable as we vary ?.5.5 Sample Output and Case StudyFinally, we show a sample travel guide our method generates for Sydney in Table 7.
We can see that firstof all the sentences selected by our method have high relevance to the corresponding sections.
Second,through observation we find that humans tend to select sentences containing more points of interest assummary.
Our summary sentences contain many points of interest as highlighted, showing the advantageof our method.1678Sample Summary Sentences Generated from Yahoo!
Answers by Our Method for SydneyHotelSorry can not recommend you a hotels as I have no idea of pricing , but if you want a nice area , check hotels in Bondi and Manly Beaches .As for the Acer Arena , that is in the Homebush Olympic Park and you can choose to live in either Parramatta or the city .You need to live in one of the surrounding residential suburbs , close to a train line .
Try Alexandria , Newtown , Surry hills for inner suburbs .
.
.
.SightsYou can walk around the harbor area to the Opera House and you can see the beautiful Harbor Bridge .All this is apart from the Opera House and the Botanical Gardens .
Visit the Custom House Circular Quay and see a model of Sydney.
Youmust also do a day trip to the Blue Mountains .
Harbour Wedding is one of the major attraction in Sydney .
.
.
.EntertainmentGeorge Street has a number of bars .
All the bars around the harbour are really good day and night .
If you want to stay in a hotel where there isentertainment at night , you could look at Woolloomooloo , Darlinghurst , Surry Hills , Kings Cross or Potts Point .
Newtown is good for bars .Get them to see a theatre show or something at the Opera House .
.
.
.Things-to-doIf you are going out for the day , starting with a walk to the city will be most enjoyable .
Take a public ferry from Circular Quay to DarlingHarbour , about 15 minutes across the harbour and under the bridge , when you get to Darling Harbour go and see the Chinese Gardens .
Thereare lots of interesting things to see and do in and around Sydney .
.
.
.ActivitiesThey have good markets at the weekend and great views of the Opera House .
The Opera House is free to have a look at , if you like art then walkthrough the Botanical Gardens and go and see the art gallery .
If you ?re feeling brave , you can do a Harbour Bridge walk , though I think it maybe a little pricey .
.
.
.Table 7: Excerpts from the summary generated from Yahoo!
Answers by our method for Sydney.
We show summaries for the5 sections other than the 3 sections shown in Table1.
Named entities are highlighted in bold font.6 Related WorkMulti-document summarization is a process to generate a text summary by reducing documents in sizewhile retaining the main points of the original documents.
It has been extensively studied in the NLPcommunity, with most efforts on extractive summarization.
Our work is also based on extractive sum-marization.
Extractive summarization essentially selects a set of sentences from the original documentsto form a summary.To select sentences, different features and ranking strategies have been studied.
Early work focuseson finding good features to select summary sentences.
Radev et al.
(2004) proposed a centroid-basedsummarizer which combines several pre-defined features like tfidf, cluster centroid and position to scoresentences.
Lin and Hovy (2002) built the NeATS multi-document summarization system using term fre-quency, sentence position, stigma words and simplified Maximal Marginal Relecvance (MMR).
Nenkovaet al.
(2006) proved that high-frequency words were significant in reflecting the focus of documents.Ouyang et al.
(2010) studied the influence of different word positions in summarization.
Later, graph-based ranking algorithms have been successfully applied to summarization.
LexPageRank (Erkan andRadev, 2004a) is a representative one based on the PageRank algorithm (Page et al., 1999).
Later exten-sions include ToPageRank (Pei et al., 2012), which incorporates topic information into the propagationmechanism, the manifold-ranking based method for topic-focused summarization (Wan et al., 2007) andDivRank (Mei et al., 2010), which introduces a time-variant matrix into a reinforced random walk tobalance prestige and diversity.More recently, Integer Linear Programming (ILP) based framework was introduced as a global infer-ence algorithm for multi-document summarization by McDonald (2007), which considers informationand redundancy at the sentence level.
Gillick and Favre (2009) studied information and redundancy at asub-sentence, ?concept?
level, modeling the value of a summary as a function of the concepts it covers.In our work we also model concept level coverage of the summaries.
Li et al.
(2013) proposed a re-gression model to estimate the frequency of bigrams in the reference summary and analyzed the impactof bigram selection, weight estimation and ILP setup.
Haghighi and Vanderwende (2009) constructeda sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGEgains along the way.
Sauper and Barzilay (2009) investigated an approach for creating a comprehensivetextual overview of subject composed of information drawn from the Internet and applied ILP to opti-mize both local fit of information into each topic and global coherence across the entire overview.
Liet al.
(2011) developed an entity-aspect LDA model to cluster sentences into aspects and then extendLexRank algorithm to rank sentences.
Hu and Wan (2013) proposed to use SVR model and ILP methodto generate presentation slides for academic papers.Our work is different from standard ILP-based multi-document summarization.
We designed a latentvariable model to first separate the threads to be summarized into sections based on model gravel guides.1679We also emphasized the inclusion of potential points of interest in formulating the ILP optimizationproblem.Our work is also closely related to previous work on answer summarization in community-basedQA sites.
Previous work on summarizing answers is mainly based on query focused multi-documentsummarization techniques to summarize multiple answer documents given a single question.
Liu et al.
(2008) proposed a CQA question taxonomy to classify questions in CQA and question-type orientedanswer summarization for better reuse of answers.
Tomasoni and Huang (2010) proposed two concept-scoring functions to combine quality, coverage, relevance and novelty measures for answer summaryin response to a question and showed that their summarized answers constitute a solid complement tobest answers voted by CQA users.
Chan et al.
(2012) presented an answer summarization method forcomplex multi-sentence questions.
For our work, we study a new problem of summarizing multiplethreads to automatically generate city travel guides based on known template structure from well-writtentravel guides, which is different from the setting of single Q&A thread summarization in the previousrelated studies.7 Conclusion and Future WorkIn this paper we proposed a summarization framework to generate well structured supplementary travelguides from social media based on a latent variable model and integer linear programming.
The la-tent variable model could align forum threads with the section structure of well-written travel guides.Compared to standard concept based ILP methods, our method additionally tries to cover more namedentities as points of interest and maximizes sentence relevance scores measured by section-specific andcity-specific word distributions learnt by the latent variable model.
Extensive experiments with real datafrom Yahoo!
Answers show that our proposed method is able to generate better summaries comparedwith a number of multi-document summarization baselines measured by ROUGE scores.Currently our generated summaries may have overlap with the well-written model travel guides.
In thefuture, we plan to improve our method to emphasize the selection of additional information from socialmedia compared with the model travel guides.
We will also look into the problem of how to summarizeinformation that does not fit into the template structure derived from model travel guides.AcknowledgmentsThis work was done during Liu Yang?s visit to Singapore Management University.
The authors wouldlike to thank the reviewers for their valuable comments on this work.ReferencesWen Chan, Xiangdong Zhou, Wei Wang, and Tat-Seng Chua.
2012.
Community answer summarization for multi-sentence question with group l1 regularization.
In Proceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers - Volume 1, ACL ?12, pages 582?591, Stroudsburg, PA, USA.Association for Computational Linguistics.G?unes Erkan and Dragomir R. Radev.
2004a.
Lexpagerank: Prestige in multi-document text summarization.
InProceedings of the Conference on Empirical Methods in Natural Language Processing, volume 4 of EMNLP?04.G?unes Erkan and Dragomir R. Radev.
2004b.
Lexrank: Graph-based lexical centrality as salience in text summa-rization.
J. Artif.
Int.
Res., 22(1):457?479, December.Dan Gillick and Benoit Favre.
2009.
A scalable global model for summarization.
In Proceedings of the Workshopon Integer Linear Programming for Natural Langauge Processing, ILP ?09, pages 10?18, Stroudsburg, PA,USA.
Association for Computational Linguistics.Aria Haghighi and Lucy Vanderwende.
2009.
Exploring content models for multi-document summarization.
InProceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapterof the Association for Computational Linguistics, NAACL ?09, pages 362?370, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.1680Yue Hu and Xiaojun Wan.
2013.
Ppsgen: Learning to generate presentation slides for academic papers.
InProceedings of the 21st international jont conference on Artifical intelligence, IJCAI ?13, pages 2099?2105.Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang.
2011.
Generating aspect-oriented multi-document summariza-tion with event-aspect model.
In Proceedings of the Conference on Empirical Methods in Natural LanguageProcessing, EMNLP ?11, pages 1137?1146, Stroudsburg, PA, USA.
Association for Computational Linguistics.Chen Li, Xian Qian, and Yang Liu.
2013.
Using supervised bigram-based ilp for extractive summarization.
InProceedings of the 51th Annual Meeting of the Association for Computational Linguistics, ACL ?13, pages1004?1013, Stroudsburg, PA, USA.
Association for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2002.
From single to multi-document summarization: A prototype system andits evaluation.
In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL?02, pages 457?464, Stroudsburg, PA, USA.
Association for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evaluation of summaries using n-gram co-occurrence statistics.In Proceedings of the 2003 Conference of the North American Chapter of the Association for ComputationalLinguistics on Human Language Technology - Volume 1, NAACL ?03, pages 71?78, Stroudsburg, PA, USA.Association for Computational Linguistics.Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu.
2008.
Understanding and sum-marizing answers in community-based question answering services.
In Proceedings of the 22Nd InternationalConference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.Association for Computational Linguistics.Ryan McDonald.
2007.
A study of global inference algorithms in multi-document summarization.
In Proceedingsof the 29th European Conference on IR Research, ECIR?07, pages 557?564, Berlin, Heidelberg.
Springer-Verlag.Qiaozhu Mei, Jian Guo, and Dragomir Radev.
2010.
Divrank: The interplay of prestige and diversity in informa-tion networks.
In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discoveryand Data Mining, KDD ?10, pages 1009?1018, New York, NY, USA.
ACM.Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown.
2006.
A compositional context sensitive multi-document summarizer: Exploring the factors that influence summarization.
In Proceedings of the 29th AnnualInternational ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ?06, pages573?580, New York, NY, USA.
ACM.You Ouyang, Wenjie Li, Qin Lu, and Renxian Zhang.
2010.
A study on position information in documentsummarization.
In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,COLING ?10, pages 919?927, Stroudsburg, PA, USA.
Association for Computational Linguistics.Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.
1999.
The pagerank citation ranking: Bring-ing order to the web.
Technical Report 1999-66, Stanford InfoLab, November.
Previous number = SIDL-WP-1999-0120.Yulong Pei, Wenpeng Yin, and Lian?en Huang.
2012.
Generic multi-document summarization using topic-orientedinformation.
In Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelli-gence, PRICAI?12, pages 435?446, Berlin, Heidelberg.
Springer-Verlag.Dragomir R. Radev, Hongyan Jing, Malgorzata Sty?s, and Daniel Tam.
2004.
Centroid-based summarization ofmultiple documents.
Inf.
Process.
Manage., 40(6):919?938, November.Christina Sauper and Regina Barzilay.
2009.
Automatically generating wikipedia articles: A structure-awareapproach.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages208?216, Stroudsburg, PA, USA.
Association for Computational Linguistics.Mattia Tomasoni and Minlie Huang.
2010.
Metadata-aware measures for answer summarization in communityquestion answering.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA.
Association for Computational Linguistics.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007.
Manifold-ranking based topic-focused multi-documentsummarization.
In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI?07,pages 2903?2908, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.Xiaojun Wan.
2008.
An exploration of document impact on graph-based multi-document summarization.
InProceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages755?762, Stroudsburg, PA, USA.
Association for Computational Linguistics.1681
