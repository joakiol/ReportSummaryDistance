First Joint Workshop on Statistical Parsing of Morphologically Rich Languagesand Syntactic Analysis of Non-Canonical Languages, pages 54?65 Dublin, Ireland, August 23-29 2014.Exploring Options for Fast Domain Adaptation of Dependency ParsersViktor Pekar, Juntao Yu, Mohab El-karef, Bernd BohnetSchool of Computer ScienceUniversity of BirminghamBirmingham, UK{v.pekar,jxy362,mxe346,b.bohnet}@cs.bham.ac.ukAbstractThe paper explores different domain-independent techniques to adapt a dependency parsertrained on a general-language corpus to parse web texts (online reviews, newsgroup posts, we-blogs): co-training, word clusters, and a crowd-sourced dictionary.
We examine the relativeutility of these techniques as well as different ways to put them together to achieve maximumparsing accuracy.
While we find that co-training and word clusters produce the most promisingresults, there is little additive improvement when combining the two techniques, which suggeststhat in the absence of large grammatical discrepancies between the training and test domains,they address largely the same problem, that of unknown vocabulary, with word clusters beinga somewhat more effective solution for it.
Our highest results were achieved by a combinationof word clusters and co-training, significantly improving on the baseline, by up to 1.67%.
Eval-uation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012)showed that they outperform all the shared task submissions that used a single parser to parsetest data, averaging the results across all the test sets.1 IntroductionDomain adaptation of a statistical dependency parser is a problem that is of much importance for manypractical NLP applications.
Previous research has shown that the accuracy of parsing significantly dropswhen a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomed-ical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al.,2013).
In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-of-the-art parsers ?
Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet etal., 2013) ?
trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotesv5.0 corpus as well as the in-domain CoNLL09 test set.
The results (see Table 1) indicate that dependingon the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%.Domain MST MALT MateNewswire 84.8 81.7 87.1Pivot Texts 84.9 83.0 86.6Broadcast News 79.4 78.1 81.2Magazines 77.1 74.7 79.3Broadcast Conversation 73.4 70.5 74.4CoNLL09 test 86.9 84.7 90.1Table 1: Labelled accuracy scores achieved by the MST, Malt, and Mate parsers trained on CoNLL09data and tested on different specialist domains.In a typical domain adaptation scenario, there are in-domain texts that are manually annotated andthat are used to train a general-language parser, and out-of-domain or target domain texts that areThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/54parsed during parser testing.
In addition, a certain amount of unlabelled target domain texts may beavailable that can be leveraged in this or that way to facilitate domain adaptation.
To address the problemof domain adaption, previous work focused on weakly supervised methods to re-train parsers on auto-matically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedmanet al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov etal., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the targetdomain (Plank and Van Noord, 2011; S?gaard and Plank, 2012; Khan et al., 2013b).
Another line ofresearch aims specifically to overcome the lexical gap between the training data and the target domaintexts.
These approaches include techniques such as text pre-processing and normalization (Foster, 2010),the use of external lexica and morphological clues to predict PoS tags of unknown target domain words(Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelledtarget domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled fromword co-occurrences obtained from unannotated texts (Zhou et al., 2011).The goal of this paper is to investigate a combination of such techniques to adapt a general-languageparser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manualannotation.
In our study we include several techniques that have been shown to be reasonably effectivefor domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon,as well as automatically annotated texts produced with the help of co-training.
All these techniquesare domain-independent and can be applied to new target domains given unlabelled texts form thesedomains.
We explore the relative utility of these methods and ways to combine them for maximumparser accuracy.2 Related work2.1 Text normalizationUser-generated content on the web is notoriously low-quality, containing slang, abbreviations, inconsis-tent grammar and spelling.
Foster (2010) investigated lexical phenomena that appear on online discus-sion forums that present common problems for parsing and compiled a list of such phenomena alongwith their transformations.
Applying the transformations to test sentences helped to bring the F-score upby 2.7%.
A similar approach was taken by Khan et al.
(2013a) who found that it performed better thanspelling correction based on the Levenshtein distance.
Gadde et al.
(2011) use a word clustering methodand language modelling in order to align misspelled words with their regular spelling.
Their method ofcleaning noisy text helped to increase the accuracy of PoS tagging of SMS data by 3.5%.2.2 External lexicaTo adapt the Link parser to the medical domain, Szolovitz (2003) extended its lexicon with terms fromthe UMLS Specialist Lexicon.
Pyysalo et al.
(2006) take the same approach and together with predictingthe PoS tags for out-of-vocabulary words based on their morphology this allowed them to achieve a 10%reduction in the error rate of parsing.
External lexica have also been used to improve out-of-domain PoStagging (Li et al., 2012).2.3 Word clustersIn order to reduce the amount of annotated data to train a dependency parser, Koo et al.
(2008) usedword clusters computed from unlabelled data as features for training a parser.
The same approach hasproved to be effective for out-of-domain parsing, where there are many words in the test data unseenduring training, and word clusters computed from in-domain data similarly help to deal with the vocab-ulary discrepancies between the training and test datasets.
Discrete word clusters produced by Brown etal.
(1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts(Candito et al., 2011) and web texts (?vrelid and Skj?rholt, 2012).
Word clusters created with Brownclustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013).Bansal et al.
(2014) introduced continuous word representations and showed them to increase parsingaccuracy both on the Penn Treebank and on web data.552.4 Co-trainingCo-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classificationproblem from a limited amount of labelled data and a large amount of unlabelled data, whereby two ormore views on the data, i.e.
feature subsets, or two or more different learning algorithms are employedthat complement each other to bootstrap additional training data from the unlabelled dataset.
Co-trainingalgorithms have been successfully used in NLP tasks, and specifically for parsing.
Sarkar (2001) showedthe both precision and recall of a phrase structure parser can be increased using a co-training procedurethat iteratively adds the most confidently parsed sentences from two different views to the training set.Steedman et al.
(2003) used two different parsers that supplied training data to each other in a bootstrap-ping manner.A number of studies specifically aimed to use co-training for domain adaptation of a dependencyparser.
Sagae (2007) used two different learning algorithms of their graph-based parser to complete aone iteration of co-training, getting an improvement of 2-3%, which was the best result on the out-of-domain track of the CoNLL07 shared task (Nilsson et al., 2007).
An interesting finding of their work wasthat the agreement between the two classifiers during testing was a very good predictor of accuracy.
Morerecently, Zhang et al.
(2012) used a tri-training algorithm for parser domain adaptation.
The algorithmuses three learners and each learner was designed to learn from those automatically classified unlabelleddata where the other two learners agreed on the classification label.3 Experimental set-up3.1 ParsersIn the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira,2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martinset al., 2010).
All the parsers were used with their default settings, and PoS tags used in the input of allthe parsers were the same and came from the Mate parser.3.2 BaselineAs the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptationtechniques were used, i.e.
trained on an in-domain training dataset and applied directly to out-of-domaintest data.3.3 DataThe experiments were conducted on annotated data on web-related domains available in the Ontonotesv.5 and SANCL datasets, since a large amount of unlabelled data required for most domain adaptationtechniques is widely available.OntoNotes.
In experiments with weblog texts, we used the CoNLL09 training dataset (Hajic?
et al.,2009) as the general-language training data.
The CoNLL09 test dataset was used to evaluate in-domainparsing.
To create an out-of-domain test set, we selected the last 10% of the weblogs section of theOntoNotes v5.0 corpus1, in order to make the size of the out-of-domain test data comparable to that ofthe in-domain test data, i.e.
of CoNLL09 test.
The OntoNotes corpus was converted to the CoNLL09format using the LTH constituent-to-dependency conversion tool (Johansson and Nugues, 2007).SANCL.
In order to compare our results with the results achieved by participants in the SANCL-2012shared task, we also ran experiments on the Stanford dependences of three SANCL test sets (answers,newsgroups and reviews).
In these experiments we used the training set, test sets, unlabelled data,as well as the evaluation script provided by SANCL-2012 organizers (Petrov and McDonald, 2012).Tables 2 and 3 show the sizes of the OntoNotes and SANCL datasets as well as several measuresof lexical and grammatical characteristics of the data.
The average sentence length (in tokens) and theaverage number of subjects, roughly corresponding to the number of clauses in the sentence, aim tocharacterize the syntactic complexity of the sentences: the higher these values, the more complex the1https://catalog.ldc.upenn.edu/LDC2013T1956structure of the sentences is likely to be.
The ratio of word forms absent from training data describeshow different the train and test data are in terms of vocabulary.We see that in the OntoNotes test set the average sentence length and the number of subjects persentence is very similar to those in the train data.
In SANCL test sets, these measures are more different,but the values indicate a smaller syntactic complexity than in the train data.
The amount of unknownvocabulary in all the four test sets is between 5% and 8%.CoNLL09 train CoNLL09 test OntoNotes testSentences 39,279 2,399 2,150Tokens 958,167 57,676 42,144Sentence length 24.61 24.59 23.4Subjects 1.8 1.83 1.89Unk.
wordforms ratio 0.0 0.011 0.05Table 2: The size of OntoNotes train and test datasets.SANCL train Answers test Newsgroups test Reviews testSentences 30,060 1,744 1,195 1,906Tokens 731,678 28,823 20,651 28,086Sentence length 24.56 18.44 22.79 16.35Subjects 1.69 1.78 1.62 1.5Unk.
wordforms ratio 0.0 0.064 0.084 0.051Table 3: The size of SANCL train and test datasets.Unlabelled Data.
As unlabelled target domain data we used the unlabelled dataset from the SANCL-2012 shared task.
In experiments with word clusters, the entire dataset was used without any pre-processing.
In the co-training experiments, we pre-processed the data by removing sentences that arelonger than 500 tokens, or contained non-English words (this reduced the test set by 2%).
Table 4 de-scribes the size of the subsets of the unlabelled data.Emails Weblogs Answers Newsgroups ReviewsSentences 1,194,173 524,834 27,274 1,000,000 1,965,350Tokens 17,047,731 10,356,284 424,299 18,424,657 29,289,169Table 4: The size of unlabelled datasets.3.4 Evaluation methodAs a measure of parser accuracy, we report labeled attachment scores (LAS), the percentage of depen-dencies which are attached and labeled correctly.
Significance testing was performed using paired t-test.4 Results and Discussion4.1 Text normalizationWe used a manually compiled lexicon containing Internet-specific spellings of certain words alignedwith their traditional spellings, e.g.
u?
you, gr8?
great, don,t?
don?t, as well as a number of regularexpressions to deal with extra symbols usually added for emphasis (This is sooooo good., This *is*great.).
After the original word forms were read by the parser, the lexicon and the regular expressionswere applied to normalize the spelling of the words.
This produced only a very insignificant gain on thebaseline.
A manual examination of the test data in both OntoNotes and SANCL has shown that in factalthough it comes from the web it contains very few examples of ?Internet speak?.574.2 Word clustersWe used Liang?s (2005) implementation of the Brown clustering algorithm to create clusters of wordsfound in unlabelled domain texts.
The output of the algorithm are word types assigned to discrete hi-erarchical clusters, with clusters assigned ids in the form of bit strings of varying length correspondingto clusters of different granularity.
We experimentally set the maximum length of the bit string to 6,collapsing more fine-grained clusters.
Instead of replacing the original word forms and/or PoS tags withcluster ids as was done in some previous studies (Koo et al., 2008; Candito et al., 2011; Ta?ckstro?m etal., 2013), the ids of clusters were used to generate additional features in the representations of the wordforms, as this also produced better results in the preliminary runs.
Below we describe experiments withseveral other parameters of the clustering algorithm.Number of clusters.
As an input parameter, the Brown clustering algorithm requires a desired numberof clusters.
Initially discarding all word types with a count of less than 3, we experimented with differentnumbers of clusters and found that an optimal settings lies around 600 and 800 clusters, which gives animprovement on the baseline of 0.9% for out-of-domain texts; but there does not seem to be noticeabledifferences between specific numbers of clusters (see Table 5, statistically significant differences to thebaseline are indicated by stars2).Number of clusters CoNLL09 OntoNotes50 90.46** 78.10*100 90.28* 78.40**200 90.27 78.39**400 90.37** 78.20**600 90.40** 78.43**800 90.30* 78.14**Baseline 90.07 77.54Table 5: The effect of the number of word clusters on in- and out-of-domain parsing, using the reviewsand weblogs subsets of the SANCL-2012 unlabelled data.Filtering rare words.
Due to the inevitable data sparseness, the algorithm is likely to mis-clusterinfrequent words.
At the same time, it is rare words that are not seen during parser training and arepotentially of greatest value if included into word clusters.
We examined several thresholds on wordfrequency and their impact on parsing accuracy (see Table 6; statistically significant differences to thebaseline are indicated by stars).
We found very slight differences between these three thresholds, al-though the cut-off point of 3 showed the best results.
Hence in further experiments with word clusterswe used this cut-off point.Min.
freq.
CoNLL09 OntoNotes1 90.36** 78.12*3 90.40** 78.43**5 90.22 78.24**Table 6: The effect of filtering out rare words on word clusters, using the reviews and weblogs subsetsof the SANCL-2012 unlabelled data.Amount of unlabelled data.
To examine the effect that the size of unlabelled data from which wordclusters are computed, has on parser accuracy, we compared parser accuracy achieved when using onlythe reviews and weblogs subsets of the SANCL corpus (39.6 mln word tokens), and when using theentire SANCL dataset (75.2 mln tokens).
These results are shown in Table 7, significant improvementson the smaller set are indicated by stars.
As expected, a larger amount of data does improve the parsingaccuracy, and the improvement is greater for out-of-domain parsing (+0.55% vs. +0.32%).2In this and the following tables, one star indicates significance at the p < 0.05 level, two stars at the p < 0.01 level.58CoNLL09 OntoNotesReviews and Weblogs 90.30 78.14Entire SANCL dataset 90.62* 78.69*Table 7: The effect of the size of unlabelled data on word clusters, discarding word types with count lessthan 3.Relevant domain data.
Furthermore, we were interested if simply adding more unlabelled data, notnecessarily from the relevant domain, produced the same increase in accuracy.
We obtained the plain-text claims and description parts of 13,600 patents freely available in the Global IP Database whichis based on the Espacenet3, creating a corpus with 42.5 mln tokens, i.e.
which was similar in size tothe reviews and weblogs sections of the SANCL unlabelled dataset.
Table 8 compares results achievedwhen building clusters from the patents corpus and when using the reviews and weblogs texts from theSANCL unlabelled dataset.
Despite the fact that the size of the two datasets is comparable, we find thatwhile creating clusters from an irrelevant domain does gain on the baseline (+0.25%), the improvementfor clusters built from the relevant domain texts is noticeably higher (+0.6%).
The difference betweenthe accuracy on the legal texts and the accuracy on the reviews and weblogs texts is significant at thep < 0.05 level.CoNLL09 OntoNotesLegal texts 90.19 77.77Reviews and Weblogs 90.30 78.14*Table 8: The effect of the domain of unlabelled data on word clusters, discarding word types with countless than 3.4.3 External lexiconIt is possible to supply to the dependency parser an external lexicon, where word forms are providedwith PoS tags.
Wiktionary, a companion project for Wikipedia that aims to produce a free, large-scalemultilingual dictionary, is a large and constantly growing crowd-sourced resource that appears attrac-tive for NLP research.
Wiktionary encodes word definitions, pronunciation, translations, etymology,word forms and part-of-speech information.
PoS tag dictionaries derived from Wiktionary have beenpreviously used for out-of-domain PoS tagging (Li et al., 2012) and for PoS tagging of resource-poorlanguages (Ta?ckstro?m et al., 2013).To create a lexicon for the parser, we extracted 753,970 English word forms and their PoS tags froma dump of Wiktionary4.
Wiktionary uses a rather informal set of PoS labels; to convert them to theCoNLL09 tag set, we manually aligned all unique PoS tags found in Wiktionary with those of theCoNLL09 tag set.
We compared the accuracy achieved by the parser when the lexicon was supplied,as well as when the lexicon was supplied together with the best configuration word clusters (800 clustersbuilt from the entire SANCL dataset after filtering words with the count less than 3).
Table 9 showsresults achieved with these settings in comparison to the baseline (improvements on the baseline are in-dicated with stars).
When the lexicon is used on its own, we observe only slight gains on the baseline,on both in-domain and out-domain data, and neither are statistically significant.
When combining thelexicon and word clusters, the accuracy actually decreases compared to using word clusters on their own.Thus the best combination of domain adaptation techniques so far included the use of 800 word clustersbuilt from the entire SANCL unlabelled dataset, after filtering out word forms with the count less than 3,with text normalization, but without the Wiktionary lexicon (+1.15% on the baseline).3http://www.epo.org/searching/free/espacenet.html4http://wiki.dbpedia.org/Wiktionary59CoNLL09 OntoNotesWiktionary 90.22 77.73Clusters 90.62** 78.69**Wiktionary+Clusters 90.44 78.49**Baseline 90.07 77.54Table 9: The effect of the Wiktionary lexicon on parsing accuracy.4.4 Co-TrainingFollowing Sagae (2007), the overall approach to parser co-training we adopted was as follows.
First,several parsers were combined to generate additional training data from unlabelled data, i.e.
were usedas source learners for co-training.
Then, the Mate parser was re-trained on the augmented training setand tested on a test set, i.e.
used as the evaluation learner.
The reason Mate was selected the evaluationlearner was that it achieved the best results on the test data in its default settings (see Table 10).CoNLL09 OntoNotesMate 90.07 77.54MST 86.9 75.35Turbo 85.94 74.85Malt 84.72 72.63Table 10: The baselines of parsers used in co-training experiments.Agreement-based co-training.
We first experimented with three pairwise parser combinations: usingMate as one source learner and each of the other three parsers as the other source learner in order to obtainadditional training data.
If two learners agreed on the parse of an unlabelled sentence, i.e.
assigned eachword form the same dependency label and attached it to the same head, this was taken as an indication ofa correct parse, and the sentence was added to the training set.
We experimented with different amountsof the additional training sentences added to the main training set in such a manner: 10k, 20k, and 30ksentences.
The results of these experiments are shown in Table 11 (significant differences to the baselineresults are indicated by stars).
The best result is obtained by Mate Malt pair, which outperforms thebaseline by just above 1%.+10k +20k +30kMate+Malt 78.22** 78.61** 78.61**Mate+MST 78.10** 78.23** 78.31**Mate+Turbo 77.94** 77.84* 77.99**Baseline 77.54Table 11: Agreement-based co-training using two parsers.Removing short sentences from unlabelled data.
We noticed that among those sentences wheretwo parsers agreed, many tended to be very short: the average number of tokens in generated additionaltraining data was 8 per sentence, while both the training and test set contain much longer sentenceson average: the OntoNotes test set had 19.6 tokens/sentence and the CoNLL09 training set had 24.4tokens/sentence.
Such short sentences in the additional training data may be less useful or even harmfulfor learning an accurate model of the target domain, than those that approximate both training and testdata.
We experimented with several thresholds (4, 5, and 6 tokens) on the sentence length below whichsentences were removed from the additional training data.
Table 12 shows that discarding short sentencesdid improve accuracy by up to 0.25%, though none of the improvements were significant.Three learners co-training.
In the previous experiments, the Mate parser was used both as a sourcelearner and as the evaluation learner.
Therefore it was likely that the additional training data did not60Mate+Malt, +30k Avg.
Length>6 tokens 78.88 13.1>5 tokens 78.61 12.67>4 tokens 78.67 11.94All sentences 78.61 8.35Table 12: The effect of removing short sentences from generated training data.contain sufficiently novel examples based on which the evaluation parser could adapt better to the newdomain.
Thus we next tried the tri-training algorithm (Zhou and Li, 2005), where two parsers are usedas source learners and a third as the evalaution learner.
We used Malt and MST as source learners,identifying sentences which they parsed in the same manner, and using these sentences to retrain theMate parser.
We find that the tri-training algorithm performs better than the set-up with two parsers:on 10k and 20k additional sentences, it achieves an accuracy increase on Mate+Malt, significant at thep < 0.05 level (see Table 13).+10k +20k +30kMate+Malt+MST 78.70* 79.12* 78.95Mate+Malt 78.43 78.70 78.88Table 13: Accuracy scores for tri-training (Mate+Malt+MST) and the best two-parser co-training algo-rithm (Mate+Malt).5 Combining co-training with clusters and an external lexicon5.1 OntoNotes test setWe explored several possibilities to combine co-training with word clusters and an external lexicon, eachtime supplying word clusters and/or the lexicon to the Mate parser when it is being retrained on additionaltraining data and applied to the test data.
The following configurations of each of the techniques wereused:?
Word clusters: 800 clusters generated from the entire SANCL unlabelled dataset, after discardingword types with the count less than 3.?
Lexicon: Wiktionary?
Co-training: Retraining the Mate parser on the combination of initial training set and 20k automat-ically parsed sentences (agreed by Malt and MST) which contained more than 6 tokens.The results showed that all three combinations failed to obtain significant improvements over co-training alone.
The best result is achieved by combining co-training and clusters, which obtains anincrease of only 0.09% on co-training; this is however, the greatest overall improvement on the baseline(+1.67%).
The combination of co-training and a Wiktionary lexicon in fact harms accuracy (see Table14).5.2 SANCL test setIn order to compare different technique combinations with the results achieved by participants of theSANCL-2012 shared task, we evaluated them on the SANCL test set5.As the results in Table 15 indicate, similarly to the results on OntoNotes, word clusters usually faremuch better than the Wiktionary-based lexicon, while the latter fails to produce statistically significant5Note that the data was annotated in the Stanford format.61OntoNotesCo-training 79.12**Clusters 78.69**Wiktionary 77.73Co-training+Clusters 79.21**Co-training+Wiktionary 78.89*Co-training+Clusters+Wiktionary 79.19**Baseline 77.54Table 14: Combination of co-training with word clusters and an external lexicon, OntoNotes test set.improvements on the baseline.
The best accuracy overall was achieved by combinations of techniques,in all the three subdomains, improving on the baseline by up to 1.3%.Comparing the results achieved by our best configurations with the results of the shared task, we seethat our labelled accuracy averaged across the subdomains was just above the Stanford-2 system (80.31vs.
80.25), which ranked 5th of all the twelve submissions (Petrov and McDonald, 2012).
Although ourresults are still 3.15% lower than DCU-Paris13, the best system at SANCL-2012, the top four resultswere all generated by combination systems (Le Roux et al., 2012; Zhang et al., 2012; McClosky et al.,2012); our highest results only produced by the Mate parser, hence our best configuration achieved thebest performance of a single parser.Answers Newsgroups Reviews AverageCo-training 77.18 82.72** 78.21 79.37Clusters 78.04** 83.06* 79.03** 80.04Wiktionary 77.61 82.8 78.32 79.57Clusters+Wiktionary 78.19** 83.38* 79.36** 80.31Co-training+Clusters 78.05* 83.29** 78.8** 80.04Co-training+Clusters+Wiktionary 78.33** 83.35** 78.84* 80.17Baseline 77.03 82.4 78.12 79.18SANCL Stanford-2 77.5 83.56 79.7 80.25SANCL Best (DCU-Paris13) 81.15 85.38 83.86 83.46Table 15: Combination of co-training with word clusters and an external lexicon, SANCL test set.The results on both the OntoNotes and SANCL datasets show that on their own, word clusters and co-training often improve significantly on the baseline, but their combination results only in minor furtherimprovements (only up to 0.32%).
Word clusters aim specifically to deal with the unknown vocabularyproblem, and, since there seem to be no major grammatical differences between the train and test do-mains (see Section 3.3), it is likely that the main benefit derived from co-training is the compensationfor unknown domain vocabulary.
Word clusters also seem a better way to approach this problem: theyperform better than co-training on three out of four subdomains.
The explanation that unknown vocab-ulary is the main issue for domain adaptation in this domain pair is further supported by the fact thatcombinations of word clusters with a Wiktionary lexicon sometimes performed better than combinationsinvolving co-training (on newsgroups and reviews).6 ConclusionIn this paper we described experiments with several domain adaptation techniques, in order to quicklyadapt a general-language parser to parse web data.
We find that the best combination of the techniquesimproves significantly on the baseline (up to 1.67%), and achieves very promising results on the SANCL-2012 shared task data, outperforming all submissions that used a single parser, in terms of labelledaccuracy score averaged across three test sets.62Our experiments with word clusters showed that word clusters derived from unlabelled domain textsconsistently contribute to a greater parsing accuracy, and that both the domain relevance of the unlabelleddata and its quantity are major factors for successful exploitation of word clusters.
Experiments with acrowd-sourced PoS lexicon however were not as conclusive: whereas supplying the lexicon to the parseroften resulted in certain accuracy gains, they were not as large as those for word clusters.
This suggestsword clusters created automatically from relevant domain texts are a better tool to deal with unknownvocabulary than a generic hand-crafted and wide-coverage lexicon.
Another interesting finding wasthat co-training was most effective when the evaluation parser was not used for creating extra trainingdata (the so-called tri-training technique), and when removing very short sentences from automaticallylabelled data before re-training the evaluation parser.With respect to combining co-training with word clusters, we could not find clear evidence for additiveimprovement.
This suggests that co-training solves largely the same problem as word clusters, i.e.,unknown target domain vocabulary, and that for the web texts under study unknown vocabulary is a muchmore significant impediment for domain adaptation than grammatical differences between domains.AcknowledgementsThe research was supported by FP7 ICT project ?Workbench for Interactive Contrastive Analysis ofPatent Documentation?
under grant no.
FP7-SME-606163.ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.
Tailoring continuous word representations for dependencyparsing.
In Proceedings of the 52nd annual meeting of the Association for Computational Linguistics.Avrim Blum and Tom Mitchell.
1998.
Combining labeled and unlabeled data with co-training.
In Proceedings ofthe Eleventh Annual Conference on Computational Learning Theory, COLT?
98, pages 92?100, New York, NY,USA.
ACM.Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richa?rd Farkas Filip Ginter, and Jan Hajic.
2013.
Joint morpho-logical and syntactic analysis for richly inflected languages.
Transactions of the Associtation for ComputationalLinguistics, 1.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-basedn-gram models of natural language.
Computational Linguistics, 18:467?479.Alicia Burga, Joan Codina, Gabriella Ferraro, Horacio Saggion, and Leo Wanner.
2013.
The challenge of syntacticdependency parsing adaptation for the patent domain.
In ESSLLI-13 Workshop on Extrinsic Parse Improvement.Marie Candito, Enrique Henestroza Anguiano, and Djam Seddah.
2011.
A word clustering approach to domainadaptation: Effective parsing of biomedical texts.
In IWPT, pages 37?42.
The Association for ComputationalLinguistics.Jennifer Foster.
2010.
?cba to check the spelling?
: Investigating parser performance on discussion forum posts.
InHLT-NAACL, pages 381?384.
The Association for Computational Linguistics.Phani Gadde, L. V. Subramaniam, and Tanveer A. Faruquie.
2011.
Adapting a WSJ trained part-of-speech tag-ger to noisy text: Preliminary results.
In Proceedings of the 2011 Joint Workshop on Multilingual OCR andAnalytics for Noisy Unstructured Text Data, MOCRAND11, pages 51?58, New York, NY, USA.
ACM.Daniel Gildea.
2001.
Corpus variation and parser performance.
In Lillian Lee and Donna Harman, editors,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ?01,pages 167?202, Stroudsburg.
Association for Computational Linguistics.Jan Hajic?, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?s Ma`rquez,Adam Meyers, Joakim Nivre, Sebastian Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu, Nianwen Xue, andYi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5,Boulder, Colorado, USA.Richard Johansson and Pierre Nugues.
2007.
Extended constituent-to-dependency conversion for english.
In 16thNordic Conference of Computational Linguistics, pages 105?112.
University of Tartu.63Mohammad Khan, Markus Dickinson, and Sandra Ku?bler.
2013a.
Does size matter?
text and grammar revisionfor parsing social media data.
In Proceedings of the Workshop on Language Analysis in Social Media, pages1?10, Atlanta, Georgia, June.
Association for Computational Linguistics.Mohammad Khan, Markus Dickinson, and Sandra Ku?bler.
2013b.
Towards domain adaptation for parsing webdata.
In Galia Angelova, Kalina Bontcheva, and Ruslan Mitkov, editors, RANLP, pages 357?364.
RANLP 2011Organising Committee / ACL.Terry Koo, Xavier Carreras, and Michael Collins.
2008.
Simple semi-supervised dependency parsing.
In In Proc.ACL/HLT.Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl.
2012.
Dcu-paris13 systems for the sancl 2012 shared task.Matthew Lease and Eugene Charniak.
2005.
Parsing biomedical literature.
In Robert Dale, Kam-Fai Wong, JianSu, and Oi Yee Kwong, editors, IJCNLP, volume 3651 of Lecture Notes in Computer Science, pages 58?69.Springer.Shen Li, Joo Graa, and Ben Taskar.
2012.
Wiki-ly supervised part-of-speech tagging.
In EMNLP-CoNLL, pages1389?1398.
ACL.Percy Liang.
2005.
Semi-supervised learning for natural language.
In MASTERS THESIS, MIT.Andre?
FT Martins, Noah A Smith, Eric P Xing, Pedro MQ Aguiar, and Ma?rio AT Figueiredo.
2010.
Turbo parsers:Dependency parsing by approximate variational inference.
In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 34?44.
Association for Computational Linguistics.David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In ACL (Short Papers), pages101?104.
The Association for Computer Linguistics.David McClosky, Wanxiang Che, Marta Recasens, Mengqiu Wang, Richard Socher, and Christopher Manning.2012.
Stanfords system for parsing the english web.
In Workshop on the Syntactic Analysis of Non-CanonicalLanguage (SANCL 2012).
Montreal, Canada.Ryan T McDonald and Fernando CN Pereira.
2006.
Online learning of approximate dependency parsing algo-rithms.
In EACL.Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007.
The conll 2007 shared task on dependency parsing.
InProceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915?932.
sn.Joakim Nivre.
2009.
Non-projective dependency parsing in expected linear time.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 1-Volume 1, pages 351?359.
Association for Computational Linguistics.Lilja ?vrelid and Arne Skj?rholt.
2012.
Lexical categories for improved parsing of web data.
In Proceedings ofCOLING 2012: Posters, pages 903?912, Mumbai, India, December.
The COLING 2012 Organizing Committee.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conversational text with word clusters.
In HLT-NAACL, pages 380?390.
The Association for Computational Linguistics.Slav Petrov and Ryan McDonald.
2012.
Overview of the 2012 shared task on parsing the web.
In Notes of theFirst Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi.
2010.
Uptraining for accurate determin-istic question parsing.
In Proceedings of the 2010 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 705?713, Stroudsburg, PA, USA.
Association for Computational Linguistics.Barbara Plank and Gertjan Van Noord.
2011.
Effective measures of domain similarity for parsing.
In Proceedingsof the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1566?1576.
Association for Computational Linguistics.Sampo Pyysalo, Tapio Salakoski, Sophie Aubin, and Adeline Nazarenko.
2006.
Lexical adaptation of linkgrammar to the biomedical sublanguage: a comparative evaluation of three approaches.
BMC Bioinformat-ics, 7(Suppl 3).64Ines Rehbein.
2011.
Data point selection for self-training.
In Proceedings of the Second Workshop on StatisticalParsing of Morphologically Rich Languages, SPMRL ?11, pages 62?67, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Kenji Sagae.
2007.
Dependency parsing and domain adaptation with lr models and parser ensembles.
In InProceedings of the Eleventh Conference on Computational Natural Language Learning.Anoop Sarkar.
2001.
Applying co-training methods to statistical parsing.
In Proceedings of the Second Meetingof the North American Chapter of the Association for Computational Linguistics on Language Technologies,NAACL ?01, pages 1?8, Stroudsburg, PA, USA.
Association for Computational Linguistics.Anders S?gaard and Barbara Plank.
2012.
Parsing the web as covariate shift.
In Workshop on the SyntacticAnalysis of Non-Canonical Language (SANCL2012), Montreal, Canada.Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrapping statistical parsers from small datasets.
In EACL, pages331?338.
The Association for Computer Linguistics.Peter Szolovits.
2003.
Adding a medical lexicon to an English parser.
In AMIA Annual Symposium Proceedings,volume 2003, page 639.
American Medical Informatics Association.Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan T. McDonald, and Joakim Nivre.
2013.
Token and typeconstraints for cross-lingual part-of-speech tagging.
TACL, 1:1?12.Meishan Zhang, Wanxiang Che, Yijia Liu, Zhenghua Li, and Ting Liu.
2012.
Hit dependency parsing: Bootstrapaggregating heterogeneous parsers.
In Notes of the First Workshop on Syntactic Analysis of Non-CanonicalLanguage (SANCL).Zhi-Hua Zhou and Ming Li.
2005.
Tri-training: Exploiting unlabeled data using three classifiers.
Knowledge andData Engineering, IEEE Transactions on, 17(11):1529?1541.Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011.
Exploiting web-derived selectional preference to improvestatistical dependency parsing.
In Proceedings of the 49th Annual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Volume 1, HLT ?11, pages 1556?1565, Stroudsburg, PA, USA.Association for Computational Linguistics.65
