Memory-Based Shallow ParsingWalter  Dae lemans ,  Sab ine  Buchho lz ,  Jo rn  Veenst raILK, T i lburg University, PO-box 90153, NL 5000 LE T i lburg\ [wa l t  e r ,  buchho l z ,  ve enst ra@kub,  n l \ ]Abst ractWe present a memory-based learning (MBL) approachto shallow parsing in which POS tagging, chunking, andidentification of syntactic relations are formulated asmemory-based modules.
The experiments reported inthis paper show competitive results, the F~=l for theWall Street Journal (WSJ) treebank is: 93.8% for NPchunking, 94.7% for VP chunking, 77.1% for subjectdetection and 79.0% for object detection.In t roduct ionRecently, there has been an increased interest in ap-proaches to automatically learning to recognize shallowlinguistic patterns in text \[Ramshaw and Marcus, 1995,Vilain and Day, 1996, Argamon et al, 1998,Buchholz, 1998, Cardie and Pierce, 1998,Veenstra, 1998, Daelemans et aI., 1999a\].
Shallowparsing is an important component of most textanalysis systems in applications uch as informationextraction and summary generation.
It includesdiscovering the main constituents of sentences (NPs,VPs, PPs) and their heads, and determining syntacticrelationships like subject, object, adjunct relationsbetween verbs and heads of other constituents.Memory-Based Learning (MBL) shares with otherstatistical and learning techniques the advantages ofavoiding the need for manual definition of patterns(common practice is to use hand-crafted regular expres-sions), and of being reusable for different corpora andsublanguages.
The unique property of memory-basedapproaches which sets them apart from other learn-ing methods is the fact that they are lazy learners:they keep all training data available for extrapolation.All other statistical and machine learning methods areeager (or greedy) learners: They abstract knowledgestructures or probability distributions from the train-ing data, forget the individual training instances, andextrapolate from the induced structures.
Lazy learn-ing techniques have been shown to achieve higher ac-curacy than eager methods for many language pro-cessing tasks.
A reason for this is tile intricate in-teraction between regularities, ubregularities and ex-ceptions in most language data.
and the related prob-lem for learners of distinguishing noise from excep-tions.
Eager learning techniques abstract from whatthey consider noise (hapaxes, low-frequency events,non-typical events) whereas lazy learning techniqueskeep all data available, including exceptions whichmay sometimes be productive.
For a detailed analy-sis of this issue, see \[Daelemans et al, 1999a\].
More-over, the automatic feature weighting in the similar-ity metric of a memory-based learner makes the ap-proach well-suited for domains with large numbers offeatures from heterogeneous sources, as it embodies asmoothing-by-similarity method when data is sparse\[Zavrel and Daelemans, 1997\].In this paper, we will provide a empirical evalua-tion of tile MBL approach to syntactic analysis on anumber of shallow pattern learning tasks: NP chunk-ing, \ 'P clmnking, and the assignment of subject-verband object-verb relations.
The approach is evalu-ated by cross-validation on the WSJ treebank corpus\[Marcus et al, 1993\].
We compare the approach quali-tatively and as far as possible quantitatively with otherapproaches.Memory-Based  Sha l low Syntact i cAna lys i sMemory-Based Learning (MBL) is a classification-based, supervised learning approach: a nmmory-basedlearning algorithm constructs a classifier for a task bystoring a set of examples.
Each example associates afeature vector (the problem description) with one of afinite number of classes (the solution).
Given a newfeature vector, the classifier extrapolates its class fromthose of the most similar feature vectors in memory.The metric defining similarity can be automaticallyadapted to the task at hand.In our approach to memory-based syntactic pat-tern recognition, we carve up the syntactic anal-53!ysis process into a number of such classificationtasks with input vectors representing 'a focus itemand a dynamically selected surrounding context.
Asin Natural Language Processing problems in general\[Daelemans, 1995\], these classification tasks can be seg-mentation tasks (e.g.
decide whether a focus word ortag is the start or end of an NP) or disambiguationtasks (e.g.
decide whether a chunk is the subject NP,the object NP or neither).
Output of some memory-based modules (e.g.
a tagger or a chunker) is used asinput by other memory-based modules (e.g.
syntacticrelation assignment).Similar cascading ideas have been explored in otherapproaches to text analysis: e.g.
finite state partialparsing \[Abney, 1996, Grefenstette, 1996\], statisticaldecision tree parsing \[Magerman, 1994\], maximum en-tropy parsing \[Ratnaparkhi, 1997\], and memory-basedlearning \[Cardie, 1994, Daelemans et al, 1996\].A lgor i thms and  Implementat ionFor our experiments we have used TiMBL 1,an MBL software package developed in ourgroup \[Daelemans et al, 1999b\].
We used the fol-lowing variants of MBL:?
IBI-IG: The distance between a test item and eachmemory item is defined as the number of features forwhich they have a different value (overlap metrid).Since in most cases not all features are equally rele-vant for solving the task, the algorithm uses informa-tion gain (an information-theoretic notion measuringthe reduction of uncertainty about the class to be pre-dicted when knowing the value of a feature) to weightthe cost of a feature value mismatch during compari-son.
Then the class of the most similar training itemis predicted to be the class of the test item.
Clas-sification speed is linear to the number of traininginstances times the number of features.?
IGTREE: IBI-IG is expensive in basic memory aztdprocessing requirements.
With IGTREE.
an obliviousdecision tree is created with features as tests, and or-dered according to information gain of features, as aheuristic approximation of the computationally moreexpensive pure MBL variants.
Classification speedis linear to the number of features times the averagebranching factor in the tree, which is less than orequal to the average number of values per feature.For more references and information about thesealgorithms we refer to \[Daelemans et al, 1999b,Daelemans et al, 1999a\].
In \[Daelemans et al, 1996\]both algorithms are explained in detail in the contextITiMBL is available from: http:\[/ilk.kub.nl/of MBT, a memory-based POS tagger, which wepresuppose as an available module in this paper.
Inthe remainder of this paper, we discuss results on thedifferent tasks in section Experiments, and compareour approach to alternative l arning methods in sectionDiscussion and Related Research.Exper imentsWe carried out two series of experiments.
In the firstwe evaluated a memory-based NP and VP chunker, inthe second we used this chunker for memory-based sub-ject/object detection.To evaluate the performance of our trained memory-based classifiers, we will use four measures: ac-curacy (the percentage of correctly predicted out-put classes), precision (the percentage of predictedchunks or subject- or object-verb pairs that is cor-rect), recall (the percentage of chunks or subject-or object-verb pairs to be predicted that is found),and F,~ \[C.J.van Rijsbergen.
1979\], which is given by(~2+1) v,.ec rec with ;3 = 1.
See below for an example.
3 ~- p recq- rec  'For the chunking tasks, we evaluated the algorithmsby cross-validation  all 25 partitions of the WSJ tree-bank.
Each partition in turn was selected as a test set,and the algorithms trained on the remaining partitions.Average precision and recall on the 25 partitions willbe reported for both the IBI-IG and IGTREE variants ofMBL.
For the subject/object detection task, we used10-fold cross-validation on treebank partitions 00-09.In section Related Research we will further evaluate ourchunkers and subject/object detectors.Chunk ingFollowing \[Ramshaw and Marcus.
1995\] we definedchunking as a tagging task, each word in a sentenceis assigned a tag which indicates whether this word isinside or outside a chunk.
We used as tagset:I _NP inside a baseNP.0 outside a baseNP or a baseVP.B_NP inside a baseNP, but the preceding word is inanother baseNP.I_VP and B_VP are used in a similar fashion.Since baseNPs and baseVPs are non-overlapping andnon-recursive these five tags suffice to unambiguouslychunk a sentence.
For example, the sentence:\[NP Pierre Vinken NP\] , \[NP 61 years NP\] old , \[vPwill join vP\] \[NP the board NP\] as \[NP a nonexecutivedirector .~.p\] [NP Nov. 29 .~'e\] ?should be tagged as:54MethodsIGTreeIBI-IGbaseline wordsbaseline POSIGTreeIBI-IGbaseline wordsbaseline POScontext accuracy precision I recall I Fa=INPs2-1 97.5 91.82-1 98.0 93.70 92.9 76.20 94.7 79.5VPs2-12-10099.099.295.597.393.1 92.494.0 93.879.7 77.982.4 80.993.0 94.2 93.694.0 95.5 94.767.5 73.4 70.374.7 87.7 81.2Table 1: Overview of the NP/VP  chunking scores of 25-fold cross-validation  theof two words and POS right and one left, and of using IGTREE with the samecomputed with IGTrtEE using only the focus POS tag or the focus wordWSJ using IB I - IG  with a contextcontext.
The baseline scores areature I 11 5 0 7~Veight 39 40 4 3 2 10 12Inst.1 -1 0 0 seen VBN -Inst.2 1 0 0 seen VBN sisters PRP$Inst.3 2 0 0 seen VNB seen VBN8 9 10 11 12 13 \[ Class18 29 18 31 13 24 Isisters PRP$ seen VBN Sseen VBN man NN lately RB Oman NN lately RBTable 2: Some sample instances for the subject/object detection task.
The second row shows the relative weight ofthe features (truncated and multiplied by 100; from one of the 10 cross-validation experiments).
Thus the order ofimportance of the features is: 2, 1, 11, 9, 13, 10, 8, 12, 7, 6, 3, 4, 5.Pierret_Np Vinkent_NP ,o 61t_Np yearsLNp oldo,o willt.vp joinz_vp the~_Ne boardl_NV aso a~_,venonexecutivet_Np directort_Np Nov.a_Np 29t.~, p -oSuppose that our classifier erroneously tagged di-rector as B_NP instead of I_NP, but classified therest correctly.
Accuracy would then be 17 y~ = 0.94.The resulting chunks would be \[NP a nonexecutive NP\]\[NP director NP\] instead of \[NP a nonexecutive direc-tor Nf'\] (the other chunks being the same as above).Then out of the seven predicted chunks, five are correct(precision= ~ = 71.4%) and from the six chunks thatwere to be found, five were indeed found (recall= ~ =83.3%).
F3=~ is 76.9%.The features for the experiments are the word formand the POS tag (as provided by the WSJ treebank) ofthe two words to the left, the focus word, and one wordto the right.
For the results see Table 1.The baseline for these experiments i  computed withIB I - IG,  with as only feature: i) the focus word, and ii)the focus POS tag.The results of the chunking experiments show thataccurate chunking is possible, with Fz=t values around94~c.Subject/Object DetectionFinding a subject .or object (or any other relation of aconstituent to a verb) is defined in our classification-based approach as a mapping from a pair of words (theverb and the head of the constituent) and a represen-tation of its context to a class describing the type ofrelation (e.g.
subject, object, or neither).
A verb canhave a subject or object relation to more than one wordin case of NP coordination, and a word can be the sub-ject of more than one verb in case of VP coordination.Data FormatIn our representation, the tagged and chunked sentence\ [NP My/PRP$ sisters/NNS NP\ ]  \ [VP have/VBPnot/RB seen/VBN VP\]  \ [NP the/DT old/JJman/NN NP\]  lately~liB ./.will result in the instances in Table 2.Classes are S(ubject), O(bject) or "-" (for anythingelse).
Features are:1 the distance from the verb to the head (a chunk justcounts for one word; a negative distance means thatthe head is to the left of the verb),2 the number of other baseVPs between the verb andthe head (in the current setting, this can maximallybe one),OO# relationsMethodRandom baselineHeuristic baselineIGTreeIBI-IGIGTree & IBI-IG unanimous 97.4Together Subjects Objects51629acc-l p.rec" I rec.
I3.965.996.9 79.596.6 74.489.832755 18874.
prec., 1 'reC': I Fa=i' prec.
1. rec.
I Fa=;3.9 3.9 4.5 4.5 4.5 2.7 2.5 2.666.5 66.2 69.3 61.6 65.2 61.6 75.1 67.773.2 76:2 80.9 71.4 75.8 77.2 76.4 76.876.9 75.6 76?2 76.9 76.5 71.5 76.7 74.068.6 77.8 89.7 67.6 77.1 89.8 70.4 79.0Table 3: Results of the 10-fold cross validation experiment on the subject-verb/object-verb relations data?
Wetrained one classifier to detect subjects as well as objects?
Its performance can be found in the column Together.For expository reasons, we also mention how well this classifier performs when computing precision and recall forsubjects and objects separately.3 the number of commas between the verb and thehead,4 the verb, and5 its POS tag,6-9 the two left context words/chunks ofthe head, rep-resented by the word and its POS10-11 the head itself, and12-13 its right context word/chunk.Features one to three are numeric features.
This prop-erty can only be exploited by IBI- IG.
IGTREE treatsthem as symbolic.
We also tried four additional fea-tures that indicate the sort of chunk (NP, VP or none)of the head and the three context elements respectivelyThese features did not improve performance, presum-ably because this information is mostly inferrable fromthe POS tag.To find subjects and objects in a test sentence, thesentence is first POS tagged (with the Memory-BasedTagger MBT) and chunked (see section Experiments:Chunking).
Subsequently all chunks are reduced totheir heads.
2Then an instance is constructed for every pair of abaseVP and another word/chunk head provided theyare not too distant from each other in the sentence.
Acrucial point here is the definition of "not too distant".If our definition is too strict, we might exclude too manyactual subject-verb or object-verb pairs, which will re-sult in low recall.
If the definition is too broad, we willget very large training and test sets.
This slows downlearning and might even have a negative ffect on pre-cision because the learner is confronted with too much"noise".
Note further that defining distance purely2By definition, the head is the rightmost word of abaseNP or baseVP.as the number of intervening words or chunks is notfully satisfactory as this does not take clause structureinto account?
As one clause normally contains one ba-seVP, we developped the idea of counting interveningbaseVPs.
Counts on the treebank showed that less than1% of the subjects and objects are separated from theirverbs by more than one other baseVP.
We thereforeconstru!
!
ct an instance for every pair of a baseVPand another word/chunk head if they have not morethan one other baseVP in between them.
sThese instances are classified by the memory-basedlearner.
For the training material, the POS.tags "andchunks from the treebank are used directly.
Also,subject-verb and object-verb relations are extracted toyield the class values.Resul ts  and  discussion Tile results in Table 3 showthat finding (unrestricted) subjects and objects is ahard task.
The baseline of classifying instances atrandom (using only the probability distribution of theclasses) is about 4%.
Using the simple heuristic of clas-sifying each (pro)noun directly in front of resp.
after theverb as S resp.
0 yields a much higher baseline of about66%.
Obviously, these are the easy cases.
IGTREE,which is the better overall MBL algorithm on this task,scores 10% above this baseline, i.e.
76.2?A.
The differ-ence ill accuracy between IGTrtEE and IBI - IG is only3The following sentence shows a subject-verb pair (inbold) with one intervening baseVP (in italics):\[,vP The plant .~p\], \[A'P which .~,-p\] \[l'P zs o~med l'P\] by\[:vP Hollingsworth & Vose Co. NP\] , \[vP was vP\] under\[^,p contract ,vp\] with \[.~p Lorillard .x-p\] \[vp to nmke vP\]\[,~p the cigarette filters .we\] .The next example illustrates the same for all object-verbpair:Along \[,vp the way/vp\] , \[.~p he .~p\]/re meets vP\] \[,vp asolicitous Christian chauffeur .vP\] \[.vp who .vp\] \[vP of_fers re\] \[Ne the hero ~ve\] [.~,-p God .re\] \[^-,~ "sphone num-ber .re/; and//re the Sheep Man .x'e\], \[.vP a sweet, rough-hewn figure /v/,\] \[~vt, who .vP\] \[l'P wears t'P\] - /.re whatelse ~,'t,\] - \[,vp a sheepskin ~,'t,\] .56I.
Method IA,D&KR&MC&PIBI-IGIBI-IGIBI-IGIBI-IG,POSonlyTable 4: Comparison of MBL aad MBSL oncarried out with a context of five words andTagger I accuracy precision } recall l FZ=,Brill - 9i.6 91.6 91.6Brill 97.4 92.3 91.8 92.0Brill - 90.7 91.1 90.9Brill 97.2 91.5 91.3 91AMBT 97.3 91.6 91.5 91.6WSJ 97.6 92.2 92.5 92.3WSJ 96.9 90.3 90.1 90.2same dataset of several classifiers, the experiments with IBI- IG are allPOS left and three right0.3%.
In terms of F-values, IBI-IG is better for find-ing subjects, whereas IGTREE is better for objects.
Wealso note that IGTRv.E always yields a higher precisionthan recall, whereas IBI-IG does the opposite.IGTrtEv.
is thus more "cautious" than IBI-IG.
Pre-sumably, this is due to the word-valued features.
Manytest instances contain a word not occurring in the train-ing instances (in that feature).
In that case, search inthe IGTREV.
is stopped and the default class for thatnode is used.
As the "-" class is more than ten timesmore frequent han the other two classes, there is ahigh chance that this default is indeed the "-" class,which is always the "cautious" choice.
IBI-IG, on theother hand, will not stop on encountering an unseenword, but will go on comparing the rest of the fea-tures, which might still opt for a non-"-" class.
Thedifferences in precision and recall surely are a topic forfurther esearch.
So far, this observation led us to com-bine both algorithms by classifying an instance as Sresp.
O only if both algorithms agreed and as "-" oth-erwise.
The combination yields higher precision at thecost of recall, but the overall effect is certainly positive(Fj=~ = 77.8%).D iscuss ion  and  Re la ted  ResearchIn \[Argamon et al, 1998\], an alternative approach tomemox3'-based learning of shallow patterns, memory-based sequence l arning (MBSL), is proposed.
In thisapproach, tasks such as base NP chunking and subjectdetection are formulated as separate bracketing tasks,with as input the POS tags of a sentence.
For ev-ery input sentence, all possible bracketings in context(situated contexts) are hypothesised and the highestscoring ones m'e used for generating a bracketed out-put sentence.
The score of a situated hypothesis de-pends on the scores of the tiles which are part of itand the degree to which they cover the hypothesis.
Atile is defined as a substring of the situated hypoth-esis containing a bracket, and the score of a tile de-pends on the number of times it is found in the train-ing material divided by the total number of times thestring of tags occurs (i.e.
including occurrences withanother or no bracket).
The approach is memory-based because all training data is kept available.
Sim-ilar algorithms have been proposed for grapheme~to-phoneme conversion by \[Dedina nd Nusbaum, 1991\],and \[Yvon, 1996\], and the approach could be seen as alinear algorithmic simplification of the DOP memory-based approach for full parsing \[Bod, 1995\].
In the re-mainder of this section, we show that an empirical com-parison of our computationally simpler MBL approachto MBSL on their data for NP chunking, subject, andobject detection reveals comparable accuracies.Chunk ingFor NP chunking, \[Argamon et al, 1998\] used data ex-tracted from section 15-18 of the WS.J as a fixed trainset and section 20 as a fixed test set, the same dataas \[Ramshaw and Marcus, 1995\].
To find the opti-mal setting of learning algorithms and feature con-struction we used 10-fold cross validation on section15; we found IBI-IG with a context of five wordsand POS-tags to the left and three to the right asa good parameter setting for the chunking task; weused this setting as the default setting for our ex-periments.
For an overview of the results see Ta-ble 4.
Since part of the chunking errors could becaused by POS errors, we also compared the samebaseNP chunker on the santo corpus tagged with i) theBrill tagger as used in \[Ramshaw and Marcus, 1995\],ii) the Memory-Based Tagger (MBT) as described in\[Daelemans et al, 1996\].
We also present the results of\[Argamon et al, 1998\], \[Ramshaw and Marcus: 1995\]and \[Cardie and Pierce, 1998\] in Table 4.
The lattertwo use a transformation-based error-driven learningmethod \[Brill, 1992\].
In \[Ramshaw and Marcus, 1995\],the method is used for NP chunking, and in\[Cardie and Pierce, 1998\] the approach is indirectlyused to evaluate corpus-extracted NP chunking rules.As \[Argamon et al, 1998\] used only POS informa-Subjects# subsequences ?MethodA,D&K .....IGTceeIBI-IG"IBI-IG POS onlyIBI-IG without chunksIBI-IG with treebank chunksObjects3044 1626I prec.
I rec.
I Fa=l prec': I 'rec.
\[ FO=x88.6 84.5 86.5 77.1 89.8 83.079.9 71.7 75.6 84.4 85.8 85.184.7 81.6 83.1 87.3 85.8 86.583.5 77 .9  80.6 76.1 83.3 79.629.2 24.4 26.6 85.0 18 .5  30.489.4 88.6 89.0 91.9 91.3 91.6Table 5: Comparison of MBL and MBSL on subject/object detection as formulated by Argamon et altion for their MBSL chunker, we also experimented withthat option (POSonly in the Table).
Results how thatadding words as information provides useful informa-tion for MBL (see Table 4).Sub jec t /ob jec t  detect ionFor subject/object detection, we trained our algorithmon section 01-09 of the WSJ and tested on Argamon etal.
's test data (section 00).
We also used the treebankPOS tags instead of MBT.
For comparability, we per-formed two separate learning experiments.
The verbwindows are defined as reaching only to the left (up toone intervening baseVP) in the subject experiment andonly to the right (with no intervening baseVP) in theobject experiment.
The relational output of MBL isconverted to the sequence format used by MBSL.
Theconversion program first selects one relation in case ofcoordinated ornested relations.
For objects, the actualconversion is trivial: The V-O sequence xtends fromthe verb up to the head (seen the old man for the ex-ample sentence on page 55).
In the case of subjects, theS-V sequence extends from the beginning of the baseNPof the head up to the first non-modal verb in the ba-seVP (My sisters have).
The program also uses filtersto model some restrictions of the patterns that Arga-monet al used for data extraction.
They extracted e.g.only objects that immediately follow the verb.The results in Table 5 show that highly comparableresults can be obtained with MBL on the (impover-ished) definition of the subject-object task.
IBI-IG aswell as IGTREE are better than MBSL on the objectdata.
They are however worse on the subject data.Two factors may have influenced this result.
Firstly,more than 17% of the precision errors of IBI-IG con-cern cases in which the word proposed by the algorithmis indeed the subject according to the treebank, but thecorresponding sequence is not included in Argamon etal.
's test data due to their restricted extraction pat-?
terns.
Secondly.
there are cases for which MBL cor-rectly found the head of the subject, but the conversionresults in an incorrect sequence.
These are sentenceslike "All \[NP the man NP\] \[NP 's friends NP\] came.
"in which all is part of the subject while not being partof an:)" baseNP.Apart from using a different algorithm, the MBL ex-periments also exploit more information ill the train-ing data than MBSL does.
Ignoring lexical informationin chunking and subject/object detection decreased theFa=I value by 2.5% for subjects and 6.9% for objects.The bigger influence for objects may be due to verbsthat take a predicative object instead of a direct one.Knowing the lexical form of the verb helps to makethis distinction.
In addition, time expressions like "(itrained) last week" can be distinguished from direct ob-jects on the basis of the head noun.
Not chunking" thetext before trying to find subjects and objects decreasesF-values by more than 50%.
Using the "perfect" chunksof the treebank, on the other hand.
increases F by 5.9%for subjects and 5.1% for objects.
These figures slmwhow crucial the chunking step is for the succes of ourmethod.Genera lClear advantages of MBL are its efficiency (especiallywhen using IGTREE), the ease with which informationapart from POS tags can be added to the input (e.g.word information, morphological information, wor(lnettags.
chunk information for subject aIld object detec-tion), and the fact that NP and VP chunking and dif-ferent ypes of relation tagging can be achieved in oneclassification pass.
It is uncleax how MBSL could beextended to incorporate other sources of informationapart from POS tags, and what the effect would beon performance.
More limitations of MBSL are that itcannot find nested sequences, which nevertheless occurfrequently in tasks such as subject identification 4, andthat it does not mark heads.%.g.
\[SV John, who \[SV I like SV\].
Is SV\] angry.58Conc lus ionWe have developed and empirically tested a memory-based learning (MBL) approach to shallow parsing inwhich POS tagging, chunking, and identification ofsyn-tactic relations are formulated as memory-based mod-ules.
A learning approach to shallow parsing allowsfor fast development of modules with high coverage,robustness, and adaptability to different sublanguages.The memory-based algorithms we used (IBI-IG andIGTrtEE) are simple and efficient supervised learningalgorithms.
Our approach was evaluated on NP andVP chunking, and subject/object detection (using out-put from the clmnker).
Fa=l scores are 93.8% for NPchunking, 94.7% for VP chunking, 77.1% for subjectdetection and 79.0% for object detection.
The accu-racy and efficiency of the approach are encouraging (nooptimisation or post-processing of any kind was usedyet), and comparable to or better than state-of-the-artalternative l arning methods.We also extensively compared our approach toa recently proposed new memory-based learning al-gorithm, memory-based sequence learning (MBSL,\[Argamon et al, 1998\] and showed that MBL, whichis a computationally simpler algorithm than MBSL,is able to readl similar precision and recall when re-stricted to the MBSL definition of the NP chunking,subject detection and object detection tasks.
More im-portantly, MBL is more flexible in the definition of theshallow parsingtasks: it allows nested relations to bedetected; it allows the addition and integration intothe task of various additional sources of informationapart from POS tags; it can segment a tagged sentenceinto different types of constituent chunks in one pass; itcan scan a chunked sentence for different relation typesin one pass (though separating subject-verb detectionfrom object-verb detection issurely an option that mustbe investigated).In current research we are extending the approachto other types of constituent chunks and other typesof syntactic relations.
Combined with previous resultson PP-attachment \[Zavrel et al, 1997\], the results pre-sented here will be integrated into a complete shallowparser.AcknowledgementsThis research was carried out in the context of the "In-duction of Linguistic Knowledge" (ILK) research pro-gramme, supported partially by the Foundation of Lan-guage, Speech and Knowledge (TSL), which is fundedby the Netherlands Organisation for Scientific Research(NWO).
The authors would like to thank the othermembers of the ILK group for the fruitful discussionsand comments.Re ferences\[Abney, 1996\] S. Abney.
Statistical methods and lin-guistics.
In Judith L. Klavans and Philip Resnik, ed-itors, The Balancing Act: Combining Symbolic andStatistical Approaches to Language, pages 1-26.
MITPress, Cambridge, MA, 1996.\[Argamon et al, 1998\] S. Argamon, I. Dagan, andY.
Krymolowski.
A menmry-based approach to learn-ing shallow natural anguage patterns.
In Proc.
o/36th annual meeting o/the A CL, pages 67-73, Mon-treal, 1998.\[Bod~ 1995\] R. Bod.
Enriching linguistics with statis-tics: Performance models of natural anguage.
Dis-sertation, ILLC, Universiteit van Amsterdam, 1995.\[Brill, 1992\] E. Brill.
A simple rule-based part-of-speech tagger.
In Proceedings of the Third ACL Ap-plied NLP, pages 152-155, Trento, Italy, 1992.\[Buchholz, 1998\] Sabine Buchholz.
Distinguishingcomplements from adjuncts using memory-basedlearning.
In Proceedings o\] the ESSLLI-98 Work-shop on Automated Acquisition o/Syntax and Pars-:ng, 1998.\[Cardie and Pierce, 1998\] C. Cardie and D. Pierce.Error-driven pruning of treebank grammars for basenoun phrase identification.
In Proc.
of 36th annualmeeting o/the ACL, pages 218-224, Montreal, 1998.\[Cardie, 1994\] C. Cardie.
Domain Specific KnowledgeAcquisition for Conceptual Sentence Analysis.
PhDthesis.
University of Massachusets.
Amherst.
MA,1994.\[C.J.van Rijsbergen, 1979\] C.J.van Rijsbergen.
Infor-mation Retrieval.
Buttersworth, London, 1979.\[Daelemans et al, 1996\] W. Daelemans, J. Zavrel.P.
Berck, and S. Gillis.
MBT: A memory-based part ofspeech tagger generator.
In E. Ejerhed and I. Dagan.editors, Proc.
o\] Fourth Workshop on Very LaTyeCo~l~ora, pages 14-27.
ACL SIGDAT.
1996.\[Daelemans et al, 1999a\] W. Daelemans.A.
Vall den Bosch.
and J. Zavrel.
Forgettingexceptions is harmful in language learning.
Ma-chine Learning, Specml issue on Natural LanguageLearning, 34:11-41, 1999.\[Daelemans et al, 1999b\] W. Daelemans, J. Zavrel,K.
Van der Sloot, and A.
Van den Bosch.
TiMBL:Tilburg Memory Based Learner, version 2.0, ref-erence manual.
Technical Report ILK-9901, ILK,Tilburg University, 1999.59\[Daelemans, 1995\] W. Daelemans.
Memory-based lex-ical acquisition and processing.
In P,:Steffens, ed-itor, Machine Translation and the Lexicon, Lec-ture Notes in Artificial Intelligence, pages 85-98.Springer-Verlag, Berlin, 1995.\[Dedina nd Nusbaum, 1991\] M. J. Dedina and H. C.Nusbaum.
PRONOUNCE: a program for pronunciationby analogy.
Computer Speech and Language, 5:55-64:1991.\[Grefenstette, 1996\] Gregory Grefenstette.
Light pars-ing as finite-state filtering.
In Wolfgaag Wahlster,editor, Workshop on Extended Finite State Models ofLanguage, ECAI'96, Budapest, Hungary.
John Wiley& Sons, Ltd., 1996.\[Magerman, 1994\] D. M. Magerman.
Natural anguageparsing as statistical pattern recognition.
Disserta-tion, Stanford University, 1994.\[Marcus et al, 1993\] M. Marcus, B~ Santorini, andM.A.
Marcinkiewicz.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313-330, 1993.\[Ramshaw and Marcus, 1995\] L.A. Ramshaw and M.P.Marcus.
Text chunking using transformation-basedlearning.
In Proc.
of third workshop on very largecorpora, pages 82-94, June 1995.\[Ratnaparkhi, 1997\] A. Ratnaparkhi.
A linear observedtime statistical parser based on maximum entropymodels.
Technical Report cmp-lg/9706014, Compu-tation and Language, http://xxx.lanl.gov/list/cmp-lg/, June 1997.\[Veenstra, 1998\] J.
B. Veenstra.
Fast np chunking usingmemory-based learning techniques.
In Proceedingsof BENELEARN'98, pages 71-78, Wageningen, TheNetherlands, 1998.\[Vilain and Day, 1996\] M.B.
Vilain and D.S.
DayFinite-state phrase parsing by rule sequences.
InProc.
of COLING, Copenhagen, 1996.\[Yvon, 1996\] F. Yvon.
Prononcerpar analogie: motiva-tion, formalisation et dvaluation.
PhD thesis, EcoleNationale Supdrieure des Tdldcommunication, Paris,1996.\[Zavrel and Daelemans, 1997\] J. Zavrel and W. Daele-roans.
Memory-based learning: Using similarity forsmoothing.
In Proc.
of 35th annual meeting of theACL.
Madrid, 1997.\[Zavrel et al, 1997\] J. Zavrel, W. Daelemans, andJ.
Veenstra.
Resolving pp attachment ambiguitieswith memory-based learning.
In M. Ellison, editor,Froc.
of the Workshop on Computational LanguageLearning (CoNLL'97), ACL, Madrid, 1997.60
