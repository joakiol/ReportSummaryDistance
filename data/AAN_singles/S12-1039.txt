First Joint Conference on Lexical and Computational Semantics (*SEM), pages 294?300,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUConcordia: CLaC Negation Focus Detection at *Sem 2012Sabine Rosenberg and Sabine BerglerCLaC Lab, Concordia University1455 de Maisonneuve Blvd West, Montre?al, QC, Canada, H3W 2B3sabin ro@cse.concordia.ca, bergler@cse.concordia.caAbstractSimply detecting negation cues is not suffi-cient to determine the semantics of negation,scope and focus must be taken into account.While scope detection has recently seen re-peated attention, the linguistic notion of focusis only now being introduced into computa-tional work.
The *Sem2012 Shared Task ispioneering this effort by introducing a suitabledataset and annotation guidelines.
CLaC?sNegFocus system is a solid baseline approachto the task.1 IntroductionNegation has attracted the attention of the NLP com-munity and we have seen an increased advance insophistication of processing tools.
In order to assessfactual information as asserted or not, it is importantto distinguish the difference between(1) (a) Newt Gingrich Not Conceding RaceAfter Losing Florida Primary(b) Newt Gingrich Conceding Race Af-ter Losing Florida PrimaryThis distinction is important and cannot be properlyinferred from the surrounding context, not conced-ing a race after losing is in fact contrary to expecta-tion in the original headline (1a), and the constructed(1b) is more likely in isolation.Negation has been addressed as a task in itself,rather than as a component of other tasks in recentshared tasks and workshops.
Detection of negationcues and negation scope at CoNLL (Farkas et al,2010), BioNLP (Kim et al, 2011) and the Negationand Speculation in NLP Workshop (Morante andSporleder, 2010) laid the foundation for the *Sem2012 Shared Task.
While the scope detection hasbeen extended to fictional text in this task, an impor-tant progression from the newspaper and biomedi-cal genres, the newly defined Focus Detection forNegation introduces the important question: what isthe intended opposition in (1a)?
The negation trig-ger is not, the scope of the negation is the entireverb phrase, but which aspect of the verb phrase isunderscored as being at variance with reality, thatis, which of the following possible (for the sake oflinguistic argument only) continuations is the morelikely one:(2) i .
.
.
, Santorum does.
(?Newt Gingrich)ii .
.
.
, Doubling Efforts (?concede)iii .
.
.
, Demanding Recount (?race)iv .
.
.
, Texas redistricting at fault(?Florida)This notion of focus of negation is thus a prag-matic one, chosen by the author and encoded withvarious means.
Usually, context is necessary to de-termine focus.
Often, different possible interpreta-tions of focus do not change the factual meaning ofthe overall text, but rather its coherence.
In (1 a) theimagined possible contexts (2 ii) and (2 iii) closelycorrespond to a simple negation of (1 b), (2 i) and(2 iv) do not feel properly represented by simplynegating (1 b).
This level of interpretation is con-tentious among people and it is the hallmark of well-written, well-edited text to avoid unnecessary guess-work while at the same time avoiding unnecessary294clarifying repetition.
The potential for ambiguity isdemonstrated by Example (3) from (Partee, 1993),where it is questionable whether the speaker in facthas possession of the book in question.
(3) I didn?t get that book from MaryHere, if the focus is from Mary, it would be likelythat the speaker has possion of the book, but receivedit some other way.
If the focus is that book, thespeaker does not have possession of it.It is important to note hat this notion of focus isnot syntactically determined as shown in (3) (eventhough we use syntactic heuristics here to approxi-mate it) but pragmatically and it correlates with pro-nunciation stress, as discussed in linguistics by (Hanand Romero, 2001).
More recently, focus negationhas been identified as a special use (Poletto, 2008).The difference of scope and focus of negation areelaborated by (Partee, 1993), and have been used forcomputational use by (Blanco and Moldovan, 2011).The *Sem 2012 Task 2 on Focus Detection buildson recent negation scope detection capabilities andintroduces a gold standard to identify the focus item.Focus of negation is annotated over 3,993 sentencesin the WSJ section of the Penn TreeBank markedwith MNEG in PropBank.
It accounts for verbal,analytical and clausal relation to a negation trigger;the role most likely to correspond to the focus wasselected as focus.
All sentences of the training datacontain a negation.
A sample annotation from thegold standard is given in (4), where PropBank se-mantic roles are labelled A1, M-NEG, and M-TMPand focus is underlined (until June).
(4) ?AdecisionA1?
is?n?tM?NEG?
expected?
until June M?TMP ?2 Previous WorkA recent study in combining regular pattern ex-traction with parse information for enhanced in-dexing of radiology reports showed effective de-tection of negated noun phrases for that corpus(Huang and Lowe, 2007).
NegFinder (Mutalik etal., 2001) detects negated concepts in dictated med-ical documents with a simple set of corpus spe-cific context-free rules, and they observe that intheir corpus ?One of the words no, denies/denied,not, or without was present in 92.5 percent ofall negations.?
Interestingly, several of their rulesconcern coordination (and, or) or prepositionalphrase attachment patterns (of, for).
NegEx (Chap-man et al, 2001) is publicly available and main-tained and updated with community-enhanced trig-ger lists (http://code.google.com/p/negex/wiki/NegExTerms).
NegEx ?locates trigger termsindicating a clinical condition is negated or possi-ble and determines which text falls within the scopeof the trigger terms.?
NegEx uses a simple regularexpression algorithm with a small number of nega-tion phrases and focuses on a wide variety of trig-gers but limits them to domain relevant ones.
Con-sequently, the trigger terms and conditions are heav-ily stacked with biomedical domain specific terms.Outside the biomedical text community, sentimentand opinion analysis research features negation de-tection (Wilson, 2008).
Current gold standard anno-tations for explicit negation as well as related phe-nomena include TIMEBANK (Pustejovsky et al,2003), MPQA (Wiebe et al, 2005), and Bio-Scope(Vincze et al, 2008).
(Wiegand et al, 2010) presents a flat feature com-bination approach of features of different granularityand analytic sophistication, since in opinion miningthe boundary between negation and negative expres-sions is fluid.3 CLaC?s NegFocusCLaC Labs?
general, lightweight negation mod-ule is intended to be embedded in any process-ing pipeline.
The heuristics-based system is com-posed of three modules for the GATE (Cunninghamet al, 2011) environment: the first component de-tects and annotates explicit negation cues present inthe corpus, the second component detects and an-notates the syntactic scope of the detected instancesof verbal negation, and the third component im-plements focus heuristics for negation.
The firsttwo steps were developed independently, drawing ondata from MPQA (Wiebe et al, 2005) and TIME-BANK (Pustejovsky et al, 2003) with validation onBio-Scope (Vincze et al, 2008).
The third step hasbeen added based on data for the *Sem 2012 chal-lenge and is intended to validate both, the first two?preprocessing?
steps and the simple heuristic ap-proximation of focus.2953.1 Data PreprocessingParser-based, our focus detection pipeline requiresas input entire sentences.
Therefore, the first steprequires the extraction of each sentence utilizing thesupplied token numbers and save them in the correctformat.
The system then performs standard prepro-cessing: sentence splitting, tokenization, parsing us-ing the Stanford Parser (Klein and Manning, 2003;de Marneffe and Manning, 2006) and morphologi-cal preprocessing.
Note that NegFocus does not useany PropBank annotations nor other provided train-ing annotations, resulting in an independent, parser-based stand-alone module.3.2 Detection of Negation TriggersThe Focus Detection task only considers the explicitnegation cues not, nor, never.
The first step in Neg-Focus is thus to identify these triggers in the sen-tences using an explicit negation trigger word list.3.3 Syntactic Scope DetectionThe Focus Detection task only considers negation ofverbs.
Thus, NegFocus extracts the syntactic com-plement of the verb to form the negated verb phrasefrom the dependency graphs (de Marneffe and Man-ning, 2006).
We annotate this as the syntactic scopeof the negation.
Note that while we use dependencygraphs, our syntactic scope is based on the parse treeand differs from the notion of scope encoded in Bio-Scope (Vincze et al, 2008) and the related formatused for the *Sem 2012 Negation Scope Annotationtask, which represent in our opinion the pragmaticnotion of scope for the logical negation operation.Syntactic scope detection is thus considered to bea basic stepping stone towards the pragmatic scopeand since the Focus Detection task does not providescope annotations, we use syntactic scope here tovalidate this principle.Our heuristics are inspired by (Kilicoglu andBergler, 2011).
In the majority of cases the depen-dency relation which identifies the syntactic scopeis the neg relation.
Traditionally, parse trees iden-tify scope as lower or to the right of the trigger term,and our scope module assumes these grammaticalconstraints, yet includes the verb itself for the pur-poses of the shared task.
Example 5, from the train-ing dataset ?The Hound of the Baskervilles?
by Co-nan Doyle for the *Sem 2012 Negation Scope Anno-tation task, demonstrates our syntactic scope of thenegation (underlined), in contrast with the gold stan-dard scope annotation (in brackets).
The gold anno-tation guidelines follow the proposal of Morante etal.
(Morante et al, 2011)1.
(5) [We did] not [drive up to the door] butgot down near the gate of the avenue.3.4 Focus HeuristicsThe third and final step for NegFocus is to annotatefocus in sentences containing verbal negations.
Us-ing the verbal negation scope annotations of the pre-vious step, four focus heuristics are invoked:3.4.1 BaselineThe Baseline heuristic for this component is de-fined according to notions discussed in (Huddle-ston and Pullum, 2002), where the last constituentin the verb phrase of a clause is commonly the de-fault location to place the heaviest stress, which wehere equate with the focus.
Example (6) depicts aninstance where both NegFocus results (underlined)and the gold focus annotation (in brackets) matchexactly.
The baseline heuristic achieves 47.4% re-call and 49.4% precision on the training set and 47%recall and 49.7% precision on the test set.
(6) NBC broadcast throughout the entirenight and did not go off the air[until noon yesterday] .As pointed out in Section 3.3, focus is not alwaysdetermined by scope (Partee, 1993).
The trainingdata gave rise to three additional heuristics.3.4.2 AdverbWhen an adverb is directly preceding and con-nected through an advmod dependency relation tothe negated verb, the adverb constituent is deter-mined as the focus of the negation.
(7) Although it may not be [legally] obli-gated to sell the company if the buy-out group can?t revive its bid, it mayhave to explore alternatives if the buyerscome back with a bid much lower thanthe group ?s original $ 300-a-share pro-posal.1http://www.clips.ua.ac.be/sites/default/files/ctrs-n3.pdf2963.4.3 Noun Subject PassivePassives are frequent in newspaper articles andpassive constructions front what would otherwisebe the verb complement.
Thus the fronted mate-rial should be eligible for focus assignment.
Pas-sives are flagged through the nsubjpass dependency,and for cases where the negated verb participates inan nsubjpass relation and has no other complement,the nsubjpass is determined as the focus.
(8) [Billings] were n?t disclosed.3.4.4 Negation CueThe challenge data has cases where the negationcue itself is its own focus.
These cases seem to bepragmatically determined.
Error cases were reducedwhen determining the negation cue to be its own fo-cus in two cases.
The first case occurs when thenegated verb has an empty complement (and is not apassive construction), as in Example 9.
(9) Both said the new plan would [n?t] work.The second case occurs when the negated verbembeds a verb that we identify as an implicit nega-tion.
We have a list of implicit negation triggerslargely compiled from MPQA (Wiebe et al, 2005).Implicit negations are verbs that lexically encode apredicate and a negation, such as reject or fail.
(10) Black activist Walter Sisulu said theAfrican National Congress would [n?t]reject violence as a way to pressurethe South African government into con-cessions that might lead to negotiationsover apartheid .
.
.4 ResultsOrdering the heuristics impacts on recall.
We placethe most specific heuristics before the more generalones to avoid starvation effects.
For example, theadverb heuristic followed by the noun subject pas-sive heuristic achieved better results at the begin-ning, since they are more specific then the negationcue heuristic.Table 1 shows the performance of the heuristicsof NegFocus on the test set and on the developmentset.
We observe that the heuristics are stable acrossthe two sets with a 60% accuracy on the test set.
Theworst performer is the baseline, which is very coarsefor such a semantically sophisticated task: assumingthat the last element of the negated verb phrase is thefocus is truly a baseline.heuristic corr.
incorr.
acc.Test Setbaseline 336 238 .59adverb 26 4 .87nsubjpass 10 8 .56neg.
cue 33 20 .62Development Setbaseline 257 174 .6adverb 15 6 .71nsubjpass 10 6 .63neg.
cue 21 19 .53Figure 1: Performance of NegFocus heuristicsThe overall performance of the system is almostbalanced between precision and recall with an f-measure of .58.Test SetPrecision 60.00 [405/675]Recall 56.88 [405/712]F-score 58.40Development SetPrecision 59.65 [303/508]Recall 57.06 [303/531]F-score 58.33Figure 2: System ResultsOur heuristics, albeit simplistic, are based on lin-guistically sound observations.
The heuristic natureallows additional heuristics that are more tailored toa corpus or a task to be added without incurring un-manageable complexity, in fact each heuristic canbe tested on the development set and can report onthe test set to monitor its performance.
The heuris-tics will also provide excellent features for statisticalsystems.5 Error AnalysisWe distinguish 11 classes of errors on the test set.The classes of errors depicted in Table (3) indi-cates that the classes of errors and their frequenciesare consistent across the different data sets.
Thethird error class in Table (3) is of particular inter-297Error Type Test Set Dev Set1 Precision Errors: Verbal Negation Scope not found by NegFocus 37 232 Focus Mismatch: gold focus annotation is the neg.
cue 138 1123 Focus Mismatch: gold focus annotation is a constituent triggeredby the nsubj dependency to the negated verb44 164 Focus Mismatch: gold focus annotation is the constituent trig-gered by the nsubjpass dependency7 125 Focus Mismatch: gold focus annotation is an adverb triggered bythe advmod dependency with the verb, but is not adjacent to theverb14 46 Partial Match: the spans of the gold focus annotation and NegFo-cus annotation overlap6 87 Focus Mismatch: gold focus annotation is not contained withinthe NegFocus Syntactic Scope4 58 NegFocus Syntactic Scope annotation error 10 99 Focus Mismatch: Miscellaneous errors 27 2510 Focus Mismatch: gold focus annotation matches CLaC baselineheuristic, however another CLaC focus heuristic was chosen3 311 Focus Mismatch: gold focus annotation contains two discontinu-ous focus annotation spans17 11TOTAL 307 228Figure 3: System Errorsest to us, as it highlights the different interpretationsof verbal negation scope.
NegFocus will not includethe noun subject in the syntactic negation scope, andtherefore the noun subject constituent is never a fo-cus candidate as required in Example (11).
(11) In New York, [a spokesman for AmericanBrands] would n?t comment.Similarly, the seventh error class in Table (3) con-tains focus annotations that are not contained inNegFocus negation scopes.
Example (12) shows anerror where the sentence begins with a prepositionalphrase that is annotated as the gold focus.
(12) [On some days], the Nucor plant does n?tproduce anything.We disagree with the gold annotations on this andsimilar cases: the prepositional phrase on some daysis not negated, it provides a temporal specificationfor the negated statement the Nucor plant producessomething and in our opinion, the negation negatessomething, contrasting it with(13) [On some days], the Nucor plant does n?tproduce a lot.which allows for some production, which indi-cates to us that without context information, low fo-cus is warranted here.NegFocus incorporates a focus heuristic for deter-mining the passive noun subject constituent as thefocus of the negation, however only in cases wherethe negated verb has an empty complement.
Thefourth error class contains errors in focus determina-tion where this heuristic fails and where the passivesubject is the gold focus despite the complement ofthe negated verb not being empty, requiring furtheranalysis:(14) To simplify the calculations , [com-missions on the option and underlyingstock] are n?t included in the table.NegFocus determines an adverb directly preced-ing the verb trigger as the focus of the negation, but,as described in the fifth error class, the gold focusannotations in a few cases determine adverbs to bethe focus of the negation even when they don?t di-rectly precede the verb, but are linked by the adv-mod relation, as in Example (15).
When we exper-imented with relaxing the adjacency constraint, re-298Error Type Test Set Dev Set1 NegFocus annotation is adverb 2 32 NegFocus annotation is passive noun subject 7 43 NegFocus Scope Error 7 144 NegFocus baseline heuristic at variance with gold annotation 122 91TOTAL 138 112Figure 4: Negation cue annotation missessults suffered.
This, too, is an area where we wishto investigate whether any general patterns are pos-sible and what additional resources they require tobe reliable.
(15) ?
The intervention has been friendly,meaning that they [really] did n?thave to do it, ?
said Maria FioriniRamirez, money-market economist atDrexel Burnham Lambert Inc .The majority of NegFocus errors occur in the sec-ond error class.
Table (4) further analyzes the seconderror class, where the gold annotation puts the nega-tion trigger in the focus but NegFocus finds anotherfocus (usually in the verb complement).The gold standard annotations place the focus ofthe negation of verb v on the negation trigger if itcannot be inferred that an action v occurred (Blancoand Moldovan, 2011).
NegFocus will only make thisassumption when the verb complement constituentis empty, otherwise the baseline focus heuristic willbe triggered, as depicted in Example (16).
(16) AMR declined to comment , andMr.
Trump did [n?t] respondto requests for interviews.Furthermore, the CLaC system will choose to triggerthe subject passive focus heuristic in the case wherethe verb complement constituent is empty, and thepassive noun subject is present.
In contrast, the goldstandard annotations do not necessarily follow thisheuristic as seen in Example (17).
(17) That is n?t 51 %, and the claim is [n?t]documented .Lastly, the gold focus annotations include focusspans which are discontinuous.
NegFocus will onlydetect one continuous focus span within one in-stance of a verbal negation.
The eleventh error classincludes those cases where NegFocus matches oneof the gold focus spans but not the other as seen inExample (18).
(18) [The payments] aren?t expected[to have an impact on coming operatingresults], Linear added .These error cases show that more analysis of thedata, but also of the very notion of focus, is neces-sary.6 ConclusionWe conclude that this experiment confirmed the hy-pothesis that negation trigger detection, syntacticscope determination, and focus determination areusefully modelled as a pipeline of three simple mod-ules that apply after standard text preprocessing anddependency parsing.
Approximating focus from aprincipled, linguistic point of view proved to be aquick and robust exercise.
Performance on develop-ment and test sets is nearly identical and in a rangearound 58% f-measure.
While the annotation stan-dards as well as our heuristics warrant revisiting, webelieve that the value of the focus annotation willprove its value beyond negation.
The challenge dataprovide a valuable resource in themselves, but webelieve that their true value will be shown by usingthe derived notion of focus in downstream applica-tions.
For initial experiments, the simple NegFocuspipeline is a stable prototype.ReferencesE.
Blanco and D. Moldovan.
2011.
Semantic represen-tation of negation using focus detection.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies (ACL-HLT 2011), Portland, OR.299W.
Chapman, W. Bridewell, P. Hanbury, G.F. Cooper, andB.
Buchanan.
2001.
A simple algorithm for identi-fying negated findings and diseases in discharge sum-maries.
Journal of Biomedical Informatics, 34(5):301-310.H.
Cunningham, D. Maynard, K. Bontcheva, V. Tablan,N.
Aswani, I. Roberts, G. Gorrell, A. Funk,A.
Roberts, D. Damljanovic, T. Heitz, M.A.
Green-wood, H. Saggion, J. Petrak, Y. Li, and Wim P. 2011.Text Processing with GATE (Version 6).
GATE (April15, 2011).M.
de Marneffe and C.D.
Manning.
2006.
Generatingtyped dependency parses from phrase structure parses.In LREC.R.
Farkas, V. Vincze, G.Mo?ra, J. Csirik, and G.Szarvas.2010.
The conll-2010 shared task: Learning to detecthedges and their scope in natural language text.
InProceedings of the Fourteenth Conference on Compu-tational Natural Language Learning.C-H. Han and M. Romero.
2001.
Negation, focus andalternative questions.
In K. Megerdoomian and L.A.Bar-el, editors, Proceedings of the West Coast Confer-ence in Formal Linguistics XX, Somerville, MA.
Cas-cadilla Press.Y.
Huang and H.J.
Lowe.
2007.
A novel hybrid approachto automated negation detection in clinical radiologyreports.
Journal of the American Medical InformaticsAssociation : JAMIA, 14(3):304-311.R.D.
Huddleston and G.K. Pullum.
2002.
The Cam-bridge grammar of the English language.
CambridgeUniversity Press, Cambridge, UK; New York.H.
Kilicoglu and S. Bergler.
2011.
Effective bio-eventextraction using trigger words and syntactic dependen-cies.
Computational Intelligence, 27(4):583?609.J.-D. Kim, Y. Wang, T. Takagi, and A. Yonezawa.
2011.Overview of genia event task in bionlp shared task2011.
In Proceedings of BioNLP Shared Task 2011Workshop at ACL-HLT.D.
Klein and C.D.
Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics.R.
Morante and C. Sporleder, editors.
2010.
NeSp-NLP?10: Proceedings of the Workshop on Negation andSpeculation in Natural Language Processing, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.R.
Morante, S. Schrauwen, and W. Daelemans.
2011.Annotation of negation cues and their scope.
guide-lines v1.0.
Technical report, CLiPS, University ofAntwerp.P.
G. Mutalik, A. Deshpande, and P. M. Nadkarni.
2001.Use of general-purpose negation detection to augmentconcept indexing of medical documents: a quantitativestudy using the umls.
Journal of the American Medi-cal Informatics Association : JAMIA, 8(6):598-609.B.
Partee.
1993.
On the ?scope of negation?
and po-larity sensitivity.
In E. Hajicova, editor, FunctionalApproaches to Language Description.C.
Poletto.
2008.
The syntax of focus negation.
Univer-sity of Venice Working Papers in Linguistics, 18.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 39(2-3).M.
Wiegand, B. Roth, D. Klakow, A. Balahur, andA.
Montoyo.
2010.
A survey on the role of negation insentiment analysis.
In Proceedings of the Workshop onNegation and Speculation in Natural Language Pro-cessing (NeSp-NLP 2010).Th.
Wilson.
2008.
Fine-Grained Subjectivity Analysis.Ph.D.
thesis, University of Pittsburgh.
Intelligent Sys-tems Program.300
