R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
933 ?
944, 2005.?
Springer-Verlag Berlin Heidelberg 2005Towards Robust High PerformanceWord Sense Disambiguation of English VerbsUsing Rich Linguistic FeaturesJinying Chen and Martha PalmerDepartment of Computer and Information Science,University of Pennsylvania, Philadelphia, PA, 19104, USA{jinying, mpalmer}@cis.upenn.eduAbstract.
This paper shows that our WSD system using rich linguistic featuresachieved high accuracy in the classification of English SENSEVAL2 verbs forboth fine-grained (64.6%) and coarse-grained (73.7%) senses.
We describethree specific enhancements to our treatment of rich linguistic features and pre-sent their separate and combined contributions to our system?s performance.Further experiments showed that our system had robust performance on testdata without high quality rich features.1   IntroductionWord sense disambiguation (WSD) has been regarded as essential or necessary inmany high-level NLP applications that require a certain degree of semantic interpreta-tion, such as machine translation, information retrieval (IR) and question answering,etc.
However, previous investigations into the role of WSD in IR have shown that lowaccuracy in WSD negated any possible performance increase from ambiguity resolu-tion [1,2].
This suggests that improving the performance of WSD systems is crucialfor applications to attain benefits from WSD.Much effort has been aimed at the creation of sense tagged corpora that can beused to develop supervised WSD systems with high accuracy.
1  However, highlypolysemous words with subtle sense distinctions still pose major challenges for auto-matic systems, as evidenced in SENSEVAL2 [3].
This problem seems more seriousfor verbs, as indicated by the relatively poorer performance achieved by the best sys-tem in the SENSEVAL2 English lexical sample task for verbs: 56.6% accuracy, incontrast with the 64.2% accuracy for all parts-of-speech [4,5].
On the other hand,disambiguating verb senses accurately is very important for lexical selection in MT.
Itis also helpful for information retrieval, especially for fact retrieval systems that takefull-sentence queries as their input.
Therefore, this paper will focus on improving theaccuracy of our supervised WSD system for verbs.We are using a linguistically rich approach for verb sense disambiguation.
Linguis-tically rich approaches [5-9] utilize syntactic and/or semantic features, e.g., syntacticrelations, selectional preferences, and semantic information of NP arguments of verbs,1http://www.senseval.org/934 J. Chen and M. Palmeretc.
In verb sense disambiguation, Dang and Palmer's work [5] demonstrated that theirsystem, which achieved 59.6% accuracy (62.5% in a recent report [10]) in disambigu-ating the SENSEVAL2 English verbs, benefited substantially from using rich linguis-tic features that capture information about a verb's lexical semantics.On the other hand, the performance of a system using rich linguistic features reliesheavily on the quality of preprocessing, such as part-of-speech tagging, parsing, fea-ture extraction and generation, etc.
How accurate and how robust can such a systembe?
In particular, we are interested in the following three questions: How much ad-vantage can we gain from the rich-feature approach by careful extraction and treat-ment of the rich features?
How much will a relatively poor quality of preprocessingnegatively affect the system's performance?
Which strategies can we adopt to allevi-ate these negative effects?To address the first question, we enhance the feature extraction and generation ofour original system, which was inspired by Dang?s system[10], in three ways.
First, toincrease the recall of the extraction of a verb's subject, we carefully handle relativeclauses, nonfinite clauses, and verbs within prepositional phrases by using linguisticknowledge and heuristics.
Second, to treat semantic features of NP arguments ofverbs and prepositions in a more uniform way, we incorporate a rule-based pronounresolver and also unify the semantic features generated by WordNet [11] and by anamed entity tagger.
Third, we treat sentential complements of verbs in a verb-specific way.
Our evaluation on the SENSEVAL2 English verbs shows that our newsystem achieves 64.6% accuracy, which is significantly better than the best system onEnglish verbs in SENSEVAL2 (57.6%) and also outperforms Dang's system (62.5%).Further experiments indicate that the three enhancements are all beneficial.
They eachboost the system's performance by 1.0~1.2 percent and the combined gain is 2.6 per-cent.
A similar performance improvement is achieved for coarse-grained senses:73.7% vs. Dang's 71.7%.
The data analysis of the results suggests that further im-provements may come from disambiguating WordNet synsets and from using statisti-cal methods for subject extraction and pronoun resolution.We address the last two robustness questions in two more experiments.
To investi-gate how the parser's performance affects our system, we divide the test data into aneasy set that is similar to the parser's training material and a hard set that is not.
Theevaluation shows that although our system's accuracy is lower on the hard set, it isstill high (62.2%).
In the second experiment, our system is trained with rich featuresand tested on data with linguistically impoverished features.
The results show littlepenalty from missing rich features at the test phase.
The observations from this ex-periment also suggest the following strategy for using WSD systems that utilize richlinguistic features.
When good parsers are not available at the time of application, theuse of topical features and any available, accurate rich features (e.g., features associ-ated with the verb's direct object) will alleviate penalties.The rest of the paper is organized as follows.
We introduce our system and thethree major enhancements we made in Section 2.
In Section 3, we show the evalua-tion results on SENSEVAL2 English verbs and show how much the three enhance-ments improve our system's performance.
We then discuss the potential improve-ments of our system in the future.
In Section 4, we investigate the robustness of oursystem and propose our strategy for alleviating the negative effects of poor preproc-essing.
We conclude our discussion in Section 5.Towards Robust High Performance Word Sense Disambiguation of English Verbs 9352   System DescriptionOur original WSD system was inspired by the successful MaxEnt WSD system ofDang [5,10].
We used the same machine learning model, Mallet, that implements asmoothing maximum entropy (ME) model with a Gaussian prior [12].
An attractiveproperty of ME models is that there is no assumption of feature independence [13].Empirical studies have shown that a ME model with a Gaussian prior generally out-performs ME models with other smoothing methods [14].
In addition to topical andcollocation features, we also used similar rich syntactic and semantic features, al-though we implemented them in different ways.
Furthermore, we enhanced the treat-ment of certain rich linguistic features, which we believed would boost the system'sperformance.
Before discussing these enhancements, we first briefly describe thebasic syntactic and semantic features used by our system:Syntactic features:1.
Is the sentence passive, semi-passive2 or active?2.
Does the target verb have a subject or object?
If so, what is the head of itssubject or/and object?3.
Does the target verb have a sentential complement?4.
Does the target verb have a PP adjunct?
If so, what is the preposition and what isthe head of the NP argument of the preposition?Semantic features:1.
The Named Entity tags of proper nouns and certain types of common nouns2.
The WordNet synsets and hypernyms of head nouns of the NP arguments ofverbs and prepositionsTo better explore the advantage of using rich syntactic and semantic features, weenhanced our original system in three primary aspects: increasing the recall of theextraction of a verb's subject; unifying the treatment of semantic features of pronouns,common nouns and proper nouns; and providing a verb-specific treatment of senten-tial complements.
These are each described in more detail below.2.1   Increasing Subject Extraction RecallTo extract a subject, our original system simply checks the left NP siblings of thehighest VP that contains the target verb and is within the innermost clause (see Figure1).
This method has high precision but low recall.
Typical examples fromSENSEVAL2 data that are not handled by this approach are shown in (1a-c).3(1) a.
Relative clauses: For Republicanssbj [SBAR who beganverb this campaign withsuch high hopes],  ...b. Nonfinite clauses: Isbj didn't ever want [S to seeverb that woman again].c.Verbs within PP's: Karipo and her womensbj had succeeded [PP in drivingverba hundred invaders from the isle ...]2Verbs that are past participles and are not preceded by be or have verbs are semi-passive.3The target verb and its subject or subject candidates are underlined and the innermost clauseor the PP containing the verb is bracketed.936 J. Chen and M. PalmerFig.
1. position for verb?s subjectTo increase the recall, we refined the procedure of subject extraction by addingrules based on linguistic knowledge and bracketing labels that can handle relativeclauses, nonfinite clauses, and verbs within prepositional phrases (PP's).
For example,for cases like (1a), if a clause containing the target verb has a bracketing label SBARand an NP parent, and is headed by a relative pronoun such as that, which or who,then check its left NP siblings for the verb's subject.
For cases like (1b) and (1c), if theparent node of a nonfinite clause S or a PP is a VP, then continue searching positionsoutside the S or PP.
For the last case, we also use a heuristic, i.e., a check as towhether the subject candidate is a person or an organization, to filter out non-person-and-organization candidate NPs whose parent nodes are not labeled as S or SBAR.Many cases like (2a-b) can be handled correctly using this heuristic.
(2) a.
A number of accounts of the events accused the ministrysbj [PP of pullingverbthe plug on the UAL deal ...].b.
Mr. Wolfsbj faces a monumental task [PP in pullingverb the company backtogether again].The above rule-based approach does not handle difficult cases like (3a-b) very well.
(3) a. Freddy's instinct was [S to keepverb growing by stock mergers and smallexpenditure of cash ...]b.The arrangement I had with him was [S to workverb four hours a day].With this enhancement, our new system extracts about 35% more subjects than be-fore.SNP         VP?VPtarget verbposition for subjectRepublicans   SBARwho        Sbegan this campaign ???
NP(1a)  ?ADVP       VPever    want         Sto see that woman again??
VP(1b)  ?succeeded       PPin driving a hundred ???
VP(1c)  ?Towards Robust High Performance Word Sense Disambiguation of English Verbs 9372.2   Unifying Semantic FeaturesIn this section we describe the changes to the use of semantic features.
In order toprovide a more uniform treatment for the semantic features of the NP arguments ofverbs and prepositions, we first merge the semantic features associated with propernouns and common nouns.
We then extend our treatment to include pronouns byadding a pronoun resolution module.2.2.1   Merging Semantic FeaturesOur system used an automatic named entity tagger, IdentiFinderTM [15], to tag propernouns with Person, Organization and Location and common nouns with Date,Time, Percent and Money.
Additional semantic features are all WordNet synsets andhypernyms4 of the head nouns of NP arguments, i.e., the system does not disambigu-ate different WordNet senses of a head noun.To utilize semantic features more efficiently, we refine their treatment.
Previouslythere was no overlap between semantic features generated by the named entity taggerand by WordNet.
For example, a personal proper noun only has a Person tag that hasno similarity to the WordNet synsets and hypernyms associated with similar commonnouns such as specialist and doctor, etc.
This is likely to be a problem for many WSDtasks that usually have small amounts of training data, such as SENSEVAL2.
Toovercome this problem, our new system associates a common noun (or a noun phrase)with each Named Entity tag (see 4) and adds the WordNet semantic features of thesenouns (or noun phrases) to the original semantic feature set.
(4)  Person ?
someone,   Organization ?
organization,   Location ?
locationTime ?
time unit,   Date ?
time period,   Percent ?
percent,   Money ?
money2.2.2   Adding Pronoun ResolutionOur original system has no special treatment for pronouns, although a rough countshows that about half of the training instances contain pronominal arguments.
Lackinga high performance automatic pronoun resolution module, we adopt a hybrid approach.For personal pronouns, we simply treat them as personal proper nouns.
For the rest ofthe pronouns including they, them, it, themselves and itself, which occur in about 13%of the training instances, we programmed a rather simple rule-based pronoun resolver.In brief, the resolver searches the parse tree for antecedent candidates similarly toHobb's algorithm as exemplified in [16] and uses several syntactic and semantic con-straints to filter out impossible candidates.
The constraints include syntactic constraintsfor anaphora antecedents [16], number agreement, and whether the candidate is a per-son.
The first candidate that survives the filtering is regarded as the antecedent of thepronoun and its semantic features are added to the original feature set.2.3   Verb-Specific Sentential ComplementsThe different types of sentential complements can be very useful for distinguishingcertain verb senses.
(5a-b) shows two sentences containing the verb call in theSENSEVAL2 training data.
Call has WordNet Sense 1 (name) in (5a) and Sense 34A unique number defined in WordNet represents each synset or hypernym.938 J. Chen and M. Palmer(ascribe) in (5b).
In both cases, call takes a small clause as its sentential complement,i.e., it has the subcategorization frame X call Y Z.
The difference is that Z is a NamedEntity when call is in Sense 1, and Z is usually a common NP or an adjective phrase(ADJP) when call is in Sense 3.
(5)  a.
The slender, handsome fellow was calledverb [S Dandy Brandon].b.The White House is purposely not callingverb [S the meeting a summit] ?Another example is shown in (6).
The verb keep has WordNet Sense 1 (maintain)in (6a) and Sense 2 (continue) in (6b).
In Sense 1, keep often takes a small clause andhas the subcategorization frame X keep Y ADJP.
In contrast, keep takes a sententialcomplement the head verb of which is in the present tense when it is in Sense 2.
(6) a.
He shook his head, keptverb [S his face expressionless].b.
We keepverb [S wondering what Mr. Gates wanted to say].Our original system uses a single feature hasSent to represent whether the targetverb has a sentential complement or not, which cannot capture the rich informationthat is crucial to distinguishing certain verb senses but is deeply embedded in thesentential complements, as described above.
Therefore, we treat sentential comple-ments in a more fine-grained, verb-specific way.
We resort to WordNet and PropBank[17] for the information about verb subcategorization frames.
Another advantage ofthis verb-specific treatment is that it can filter out illegal sentential complements gen-erated by the parser.3   System EvaluationSince the more recent SENSEVAL3 data were collected over the internet and had arelatively low quality of annotation, we decided to evaluate our new system on theSENSEVAL2 English verbs.
Ratnaparkhi's MaxEnt sentence boundary detector andPOS tagger [18], Bikel's parsing engine [19], and a named entity tagger, Identi-FinderTM [15], were used to preprocess the training and test data automatically.3.1   Experimental ResultsTable 1 shows the performance of our system (MX-RF) on the 29 verbs with fine-grained WordNet senses.
Columns 2 and 3 show the number of senses and normalizedsense perplexity5 for each verb in the test data respectively.
It also gives the perform-ance of the best system on English verbs in SENSEVAL2, KUNLP [5], and Dang'ssystem [10].
As we see, our system achieves an average accuracy of 64.6%, which issignificantly better than KUNLP  (57.6%) that only uses linguistically impoverishedfeatures (topical and collocation features).
Our system also outperforms Dang's sys-tem (62.5%).
Recall that the types of rich linguistic features used by our system wereoriginally inspired by Dang?s system, although we implemented them in differentways.
Therefore, we attribute the more success of our new system mainly to the three5It is calculated as the entropy of the sense distribution of a verb in the test data divided by thelargest possible entropy, i.e., log2 (the number of senses of the verb in the test data).Towards Robust High Performance Word Sense Disambiguation of English Verbs 939specific enhancements we made.
To our best knowledge, the accuracy our systemachieved is the best result for this task at present.To investigate exactly how much we gain by enhancing the system in the threeways discussed in Section 2, we tested our system by removing our refinements (sub-ject extraction, pronoun coreferences, and verb-specific sentential complements)separately and all together.
The results (columns 8-11) show that each refinementboosts the system's performance by 1.0~1.2 percent and that together they achieve animprovement of 2.6 percent.
This confirms the utility of these enhancements.In addition to fine-grained verb senses, we also evaluated our system on coarse-grained senses (see Table 2).
Previous work [20] suggested that not all NLP applica-tions need fine-grained sense distinctions; in some cases coarser granularities willsuffice.
Furthermore, it has been demonstrated that annotation with coarser senses ismuch faster and more accurate [21].
The SENSEVAL2 verb senses have beengrouped by using both syntactic and semantic criteria, with a resulting inter-annotatoragreement (ITA) of 82% (column 4).
As we expected, the accuracy of our systemincreases by about 9 percent on the coarse-grained senses to 73.7%, which again con-sistently outperforms Dang's system (71.7%).3.2   DiscussionCompared verb-by-verb, the performance of our system is better than or comparableto Dang's on most verbs, except that it has notably lower accuracy on develop, dressand serve.
It is not obvious why, since although our features are similar to Dang's, theimplementations are different.
Nevertheless, an investigation of the specific featuresour system generated for these three verbs gives us a few clues.
The semantic catego-ries  of  the  direct  objects  of  the three verbs are very diverse, so there are notenough instances of similar categories for the model to generalize.
Therefore, thesystem performance benefits little from our enhancements.
In fact, our system maybe more susceptible to noisy data introduced by the pronoun resolver for these threeverbs.
Erroneous antecedents found by the resolver are indistinguishable from theactual direct objects that occur rarely in the training data, and therefore they get thesame treatment from the machine learning algorithm.The experimental results and the above data analysis suggest that our system canbe improved further by increasing the accuracy of subject extraction and pronounresolution.
We expect a state-of-the-art pronoun resolution module and a statisticalsubject finder to do better jobs in the future.
Our current system does not distinguishsenses of nouns when using WordNet synsets and hypernyms as semantic features,which introduces many irrelevant features (associated with the irrelevant senses).The machine learning algorithm sometimes cannot generalize well using these fea-tures.
A potential solution for this problem is to distinguish the senses of the targetverb and its NP arguments simultaneously.
Furthermore, we need to have a bettergeneralization, or clustering, of WordNet synsets and hypernyms, especially whenthe subject or object of a verb has semantic versatility.
More performance improve-ments will bring us closer to our goal of an overall level of accuracy of 80%, espe-cially with respect to coarse-grained senses, that should finally be more beneficial toNLP applications.940 J. Chen and M. PalmerTable 1.
Evaluation of MX-RF on the SENSEVAL2 English verbs, with fine-grained sensesVerb #of SenSenPer-plex.ITA KUNLP Dang 2004MX-RFMX-RFw/o sbjextract.MX-RFw/opron.MX-RFw/o verbspec sent-compMX-RF w/oallthreebegin 7 0.63 81.2 81.4 89.3 91.2 90.0 90.4 89.3 88.6call 17 0.86 69.3 48.5 54.5 56.8 56.8 55.3 53.8 52.3carry 19 0.87 60.7 45.5 39.4 44.7 45.5 40.2 43.2 42.4collab-orate 2 0.47 75.0 90.0 90.0 90.0 90.0 90.0 90.0 90.0develop 14 0.82 67.8 42.0 58.0 49.3 49.3 50.7 49.3 49.3draw 21 0.95 76.7 34.1 31.7 41.5 39.0 34.1 41.5 36.6dress 12 0.79 86.5 71.2 72.9 64.4 64.4 69.5 67.8 64.4drift 9 0.89 50.0 53.1 40.6 67.2 51.6 60.9 64.1 48.4drive 13 0.84 58.8 54.8 59.5 60.7 60.7 58.3 60.7 58.3face 6 0.38 78.6 82.8 83.9 81.2 82.3 83.3 81.2 83.3ferret 0 0.00 1.00 100.0 100.0 100.0 100.0 100.0 100.0 100.0find 17 0.94 44.3 27.9 36.8 41.2 36.8 36.8 36.8 33.8keep 20 0.79 79.1 44.8 61.2 64.2 61.9 65.7 61.2 57.5leave 10 0.86 67.2 50.0 60.6 57.6 57.6 54.5 53.0 50.0live 9 0.70 79.7 59.7 70.1 69.4 69.4 67.9 69.4 67.9match 7 0.79 56.5 52.4 50.0 59.5 61.9 57.1 59.5 57.1play 20 0.85 N/A 37.9 53.0 62.1 59.1 62.1 62.1 62.1pull 25 0.89 68.1 45.0 50.0 58.3 56.6 58.3 53.3 56.7replace 4 0.85 65.9 55.6 60.0 61.1 60.0 55.5 61.1 57.8see 13 0.84 70.9 39.1 39.1 44.2 39.9 42.8 41.3 35.5serve 11 0.85 90.8 68.6 74.5 68.6 66.7 64.7 68.6 66.7strike 20 0.89 76.2 40.7 38.9 51.9 50.0 53.7 51.9 55.6train 8 0.87 28.8 58.7 63.5 60.3 60.3 63.5 60.3 63.5treat 5 0.88 96.9 56.8 50.0 50.0 50.0 52.3 50.0 56.8turn 26 0.93 74.2 37.3 49.3 48.5 44.0 47.8 47.0 46.3use 6 0.65 74.3 65.8 71.1 69.7 72.4 68.4 69.7 68.4wander 5 0.47 65.0 82.0 80.0 82.0 82.0 82.0 82.0 82.0wash 7 0.94 87.5 83.3 66.7 75.0 75.0 75.0 75.0 75.0work 18 0.84 N/A 45.0 45.0 53.3 51.7 50.0 53.3 43.3average 12 0.77 71.3 57.6 62.5 64.6 63.4 63.6 63.4 62.0Table 2.
Evaluation of MX-RF on coarse-grained senses of the SENSEVAL2 English verbs# of grp ITA grp Acc.
of Dang 2004 Acc.
of MX-RFAve.
on 29 verbs 5.9 82.0 71.7 73.7Towards Robust High Performance Word Sense Disambiguation of English Verbs 9414   System RobustnessA frequent criticism of systems using rich linguistic features is that they do not portwell to domains for which accurate preprocessors are not available.
In this section wediscuss two experiments designed to address the following two questions: How muchwill a relatively poor quality of preprocessing negatively affect the system's perform-ance?
Which strategies can we adopt to alleviate these negative effects?4.1   Experiment ISince the parser is the most critical component of our preprocessing and is morelikely to have lower performance when it is used in an unfamiliar data set, we investi-gate how the performance of the parser on different test data sets affects our system.We divided the SENSEVAL2 test data into two sets: an easy set and a hard set.
Thetest data from the Wall Street Journal (wsj) sections of Penn Treebank (PTB) [22] areput into the easy set because they are similar to the parser's training data: 02-21 wsjsections.
The hard set contains test data from the Brown sections of PTB and BNCdata.
It is expected that the parser and therefore the system will perform better on theeasy set.
We trained our system on the whole SENSEVAL2 training data set andevaluated its performance on the easy and hard test sets separately.
The results areshown in Table 3.Table 3.
Performance on different test data setsTest data set Hard Easy Whole SetNum.
of test inst.
895 911 1806Average Acc.
62.2 66.9 64.6As we expected, the system's performance on the hard test set is 4.7 percent lowerthan on the easy set.
On the other hand, even on the hard set, its accuracy (62.2%) isstill high and is comparable to Dang's system.
It is worth noting that the experiment ispreliminary because the easy set and the hard set are most likely to be different notonly on whether they are familiar to the parser but also on the subtlety and distribu-tions of their senses.
Nevertheless, it is evidence of our system's robustness.4.2   Experiment I IThere will be situations where systems trained with rich linguistic features extractedfrom high quality parses will be run on applications where such rich features will notbe available.
It is most likely that systems in such situations will go back to a positionsimilar to where rich features are not available in both the training and test phases.However, could things get even worse?
A machine learning model often tends tofavor informative features (e.g., rich linguistic features in our case) and fit the distri-bution of these features well in its training phase.
Therefore, it is expected that themodel will be penalized more heavily when these informative features are used in itstraining phase but are not accessible in its test phase.
In this subsection, we discuss a942 J. Chen and M. Palmersecond experiment to test the robustness of our system in such situations and explorepossible strategies for alleviating penalties.We trained our system with rich features of the SENSEVAL2 training data andtested its performance on the SENSEVAL2 test data with three different feature sets:a rich set containing topical, collocation, syntactic and semantic features(top+col+syn+sem), a poor set containing topical and collocation features (top+col)and a medium set containing topical and collocation features plus features for directobjects (top+col+obj).
The reason we include the medium set is that a parser canusually find the direct object of verbs.
Furthermore, we trained and tested our systemon SENSEVAL2 data with linguistically impoverished features (top+col) and usedthis result as a control.
As shown in Table 4, the system's accuracy drops to the samelevel as the control (58.0% vs. 58.1%) when it is trained with rich features but testedwith poor features.
When the features associated with the verb's direct object areadded, the system's performance improves (59.1%).The experimental results here suggest that our system has not been penalized verymuch when rich linguistic features are only available in its training phase.
Intuitively,the topical features6 our system uses alleviate the penalty.
As expected, when thetopical features of the test data were excluded, the performance of our system droppedto 54.8%.
But this will be a common problem for all systems using topical features,not only for systems using rich linguistic features.7 These results suggest a strategyfor using our system and other similar systems in a more robust way.
When a state-of-art parser is not available for the application data, topical features can be used to alle-viate the penalty.
Rich features that can be obtained more easily and reliably, e.g.,features associated with the direct object of verbs, can also be used whenever they areavailable.Table 4.
Performance of our system trained and tested on data sets with different featurestop+col+syn+sem top+coltop+col+syn+sem 64.6top+col 58.0 58.1top+col+obj 59.15   ConclusionWe have shown that our system using rich linguistic features was more successful,compared with the previous best systems, in classifying the fine-grained and coarse-grained SENSEVAL2 verb senses.
The three enhancements to the system's treatment6Our system uses all the contextual nouns, verbs, adjectives and adverbs that are not in a stopword list as topical features.7In fact, the performance of our system trained with (top+col) features and tested with onlycollocation features also dropped to 55.8%, in contrast to the control accuracy 58.1%.Training setTest setTowards Robust High Performance Word Sense Disambiguation of English Verbs 943of rich linguistic features were beneficial.
Further improvements may come fromdisambiguating WordNet synsets and improving the accuracy of subject extractionand pronoun resolution.
Furthermore, our system was robust when it was applied totest data that had a relatively poor quality of rich features.
Based on the experimentalresults, we proposed a strategy for using systems with rich features in a more robustway.
Our goal is to continue to improve the performance of our current WSD system,with respect to both fine-grained and coarse-grained senses, so that it becomes in-creasingly beneficial to NLP applications.References1.
Mark Sanderson: Word sense disambiguation and information retrieval.
In Proceedings ofthe 17th Int.
ACM SIGIR, Dublin, IE (1994).2.
Christopher Stokoe, Michael P. Oakes, John Tait: Word sense disambiguation and infor-mation retrieval revisited.
In Proceedings of the 26th annual int.
ACM SIGIR conferenceon research and development in information retrieval.
Toronto, Canada (2003).3.
Philip Edmonds and Scott Cotton: SENSEVAL-2: Overview.
In Proceedings ofSENSEVAL-2: 2nd Int.
Workshop on Evaluating WSD Systems.
ACL-SIGLEX, Tou-louse, France (2001).4.
David Yarowsky, Silviu Cucerzan, Radu Florian, Charles Schafer and Richard Wicen-towski: The Johns hopkins SENSEVAL2 system description.
In Proceedings ofSENSEVAL-2: 2nd Int.Workshop on Evaluating WSD Systems.
Toulouse France (2001).5.
Hoa T. Dang and Martha Palmer: Combining contextual features for word sense disam-biguation.
In Proceedings of the SIGLEX/SENSEVAL Workshop on WSD: Recent Suc-cesses and Future Directions, in conjunction with ACL-02, Philadelphia (2002).6.
Mart?nez David, Agirre Enek.
and M?rquez Liuis: Syntactic Features for High PrecisionWord Sense Disambiguation.
In Proceedings of the 19th International COLING.
Taipei(2002).7.
Dekang Lin: Using Syntactic Dependency as Local Context to Resolve Word Sense Am-biguity In Proceedings of ACL-97, Madrid, Spain (1997).8.
Yoong Keok Lee and Hwee Tou Ng: An empirical evaluation of knowledge sources andlearning algorithms for word sense disambiguation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing (EMNLP) (2002) pages 41?48.9.
Rada Mihalcea and Ehsanul Faruque: Sense Learner: Minimally Supervised Word SenseDisambiguation for All Words in Open Text.
In Proceedings of SENSEVAL-3: Third In-ternational Workshop on the Evaluation of Systems for the Semantic Analysis of Text,Barcelona, Spain (2004).10.
Hoa T. Dang: Investigations into the role of lexical semantics in word sense disambigua-tion.
PhD Thesis.
University of Pennsylvania (2004).11.
Christiane Fellbaum: WordNet - an Electronic Lexical Database.
The MIT Press, Cam-bridge, Massachusetts, London, UK (1998).12.
Andrew K. McCallum: MALLET: A Machine Learning for Language Toolkit.http://www.cs.
umass.edu/~mccallum/mallet (2002).13.
Adam L. Berger, Stephen A. Della Piertra, and Vincent J. Della Pietra: A maximum en-tropy approach to natural language processing.
Compuational Linguistics, (1996) 22(1):39-71.14.
Stanley.
F. Chen and Ronald Rosenfeld: A Gaussian prior for smoothing maximum en-tropy models.
Technical Report CMU-CS-99-108, CMU (1999).944 J. Chen and M. Palmer15.
Daniel M. Bikel, Richard Schwartz and Ralph M. Weischedel: An algorithm that learnswhat's in a name.
Machine Learning, (1999) 34(1-3).
Special Issue on Natural LanguageLearning.16.
Shalom Lappin and Herbert Leass: An algorithm for pronominal anaphora resolution.Computational Linguistics, (1994) 20(4): 535-561.17.
Paul Kingsbury, Martha Palmer, and Mitch Marcus: Adding semantic annotation to thePenn Tree-Bank.
In Proceedings of HLT 2002, San Diego, CA (2002).18.
Adwait Ratnaparkhi: Maximum entropy models for natural language ambiguity resolution.Ph.D.
thesis, University of Pennsylvania (1998).19.
19 Daniel M. Bikel: Design of a multi-lingual, parallel-processing statistical parsing en-gine.In Proceedings of HLT 2002.
San Diego, CA (2002).20.
Paul Buitelaar: Reducing lexical semantic complexity with systematic polysemous classesand underspecification.
In Poceedings of the ANLP Workshop on Syntactic and SemanticComplexity in NLP Systems.
Seattle, WA (2000).21.
Martha Palmer, Olga B. Malaya and Hoa T. Dang: Different sense granularities for differ-ent appli-cations.
In Proceedings of HLT/NAACL-04.
Boston (2004).22.
Mitchell Marcus, Grace Kim, Mary A. Marcinkiewicz, Robert MacIntyre, Mark Ferguson,Karen Katz and Britta Schasberger: The Penn Treebank: annotating predicate argumentstructure.
In Proceedings of the ARPA'94 HLT Workshop (1994).
