NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 49?57,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsDo NLP and machine learning improve traditional readability formulas?Thomas Franc?oisUniversity of PennsylvaniaCENTAL, UCLouvain3401 Walnut Street Suite 400APhiladelphia, PA 19104, USfrthomas@sas.upenn.eduEleni MiltsakakiUniversity of Pennsylvania & Choosito!3401 Walnut Street Suite 400APhiladelphia, PA 19104, USelenimi@seas.upenn.eduAbstractReadability formulas are methods used tomatch texts with the readers?
reading level.Several methodological paradigms have pre-viously been investigated in the field.
Themost popular paradigm dates several decadesback and gave rise to well known readabilityformulas such as the Flesch formula (amongseveral others).
This paper compares this ap-proach (henceforth ?classic?)
with an emerg-ing paradigm which uses sophisticated NLP-enabled features and machine learning tech-niques.
Our experiments, carried on a corpusof texts for French as a foreign language, yieldfour main results: (1) the new readability for-mula performed better than the ?classic?
for-mula; (2) ?non-classic?
features were slightlymore informative than ?classic?
features; (3)modern machine learning algorithms did notimprove the explanatory power of our read-ability model, but allowed to better classifynew observations; and (4) combining ?classic?and ?non-classic?
features resulted in a signif-icant gain in performance.1 IntroductionReadability studies date back to the 1920?s and havealready spawned probably more than a hundred pa-pers with research on the development of efficientmethods to match readers and texts relative to theirreading difficulty.
During this period of time, sev-eral methodological trends have appeared in suc-cession (reviewed in Klare (1963; 1984), DuBay(2004)).
We can group these trends in three ma-jor approaches: the ?classic studies?, the ?structuro-cognitivist paradigm?
and the ?AI readability?, aterm suggested by Franc?ois (2011a).The classic period started right after the seminalwork of Vogel and Washburne (1928) and Gray andLeary (1935) and is characterized by an ideal of sim-plicity.
The models (readability formulas) proposedto predict text difficulty for a given population arekept simple, using multiple linear regression withtwo, or sometimes, three predictors.
The predictorsare simple surface features, such as the average num-ber of syllables per word and the average number ofwords per sentence.
The Flesch (1948) and Dale andChall (1948) formulas are probably the best-knownexamples of this period.With the rise of cognitivism in psychologicalsciences in the 70?s and 80?s, new dimensions oftexts are highlighted such as coherence, cohesion,and other discourse aspects.
This led some schol-ars (Kintsch and Vipond, 1979; Redish and Selzer,1985) to adopt a critical attitude to classic readabil-ity formulas which could only take into account su-perficial features, ignoring other important aspectscontributing to text difficulty.
Kintsch and Vipond(1979) and Kemper (1983), among others, suggestednew features for readability, based on those newlydiscovered text dimensions.
However, despite thefact that the proposed models made use of more so-phisticated features, they failed to outperform theclassic formulas.
It is probably not coincidental thatafter these attempts readability research efforts de-clined in the 90s.More recently, however, the development of ef-ficient natural language processing (NLP) systemsand the success of machine learning methods led to49a resurgence of interest in readability as it becameclear that these developments could impact the de-sign and performance of readability measures.
Sev-eral studies (Si and Callan, 2001; Collins-Thompsonand Callan, 2005; Schwarm and Ostendorf, 2005;Feng et al, 2010) have used NLP-enabled featureextraction and state-of-the-art machine learning al-gorithms and have reported significant gains in per-formance, suggesting that the AI approach might besuperior to previous attempts.Going beyond reports of performance which areoften hard to compare due to a lack of a commongold standard, we are interested in investigating AIapproaches more closely with the aim of understand-ing the reasons behind the reported superiority overclassic formulas.
AI readability systems use NLPfor richer feature extraction and a machine learningalgorithm.
Given that the classic formulas are alsostatistical, is performance boosted because of the ad-dition of NLP-enabled feature extraction or by bettermachine learning algorithms?
In this paper, we re-port initial findings of three experiments designed toexplore this question.The paper is organized as follows.
Section 2 re-views previous findings in the field and the challengeof providing a uniform explanation for these find-ings.
Section 3 gives a brief overview of prior workon French readability, which is the context of ourexperiments (evaluating the readability of Frenchtexts).
Because there is no prior work comparingclassic formulas with AI readablity measures forFrench, we first report the results of this compari-son in Section 3.
Then, we proceed with the resultsof three experiments (2-4), comparing the contribu-tions of the AI enabled features with features usedin classic formulas, different machine learning al-gorithms and the interactions of features with algo-rithms.
There results are reported in Sections 4, 5,and 6, respectively.
We conclude in Section 7 with asummary of the main findings and future work.2 Previous findingsSeveral readability studies in the past decade havereported a performance gain when using NLP-enabled features, language models, and machinelearning algorithms to evaluate the reading difficultyof a variety of texts (Si and Callan, 2001; Collins-Thompson and Callan, 2005; Schwarm and Osten-dorf, 2005; Heilman et al, 2008; Feng et al, 2010).A first explanation for this superiority would berelated to the new predictors used in recent mod-els.
Classic formulas relied mostly on surface lexicaland syntactic variables such as the average numberof words per sentence, the average number of lettersper word, the proportion of given POS tags in thetext or the proportion of out-of-simple-vocabularywords.
In the AI paradigm, several new featureshave been added, including language models, parsetree-based predictors, probability of discourse rela-tions, estimates of text coherence, etc.
It is rea-sonable to assume that these new features capture awider range of readability factors thus bringing intothe models more and, possibly, better information.However, the evidence from comparative studiesis not consistent on this question.
In several cases,AI models include features central to classic formu-las which, when isolated, appear to be the strongerpredictors in the models.
An exception to this trendis the work of Pitler and Nenkova (2008) who re-ported non-significant correlation for the mean num-ber of words per sentence (r = 0.1637, p = 0.3874)and the mean number of characters per word (r =?0.0859, p = 0.6519).
In their study, though, theyused text quality rather than text difficulty as the de-pendent variable.
The data consisted solely of textfrom the Wall Street Journal which is ?intended foran educated adult audience?
text labelled for de-grees of reading fluency.
Feng et al (2010) com-pared a set of similar variables and observed thatlanguage models performed better than classic for-mula features but classic formula features outper-formed those based on parsing information.
Collins-Thompson and Callan (2005) found that the classictype-token ratio or number of words not in the 3000-words Dale list appeared to perform better than theirlanguage model on a corpus from readers, but werepoorer predictors on web-extracted texts.In languages other than English, Franc?ois (2011b)surveyed a wide range of features for French andreports that the feature that uses a limited vocabu-lary list (just like in some classic formulas) has astronger correlation with reading difficulty that a un-igram model and the best performing syntactic fea-ture was the average number of words per sentences.Aluisio et al (2010), also, found that the best corre-50late with difficulty was the average number of wordsper sentence.
All in all, while there is sufficient ev-idence that the AI paradigm outperforms the classisformulas, classic features have often been shown tomake the single strongest predictors.An alternative explanation could be that, by com-parison to the simpler statistical analyses that deter-mined the coefficients of the classic formulas, ma-chine learning algorithms, such as support machinevector (SVM) or logistic regression are more sophis-ticated and better able to learn the regularities intraining data, thus building more accurate models.Work in this direction has been of smaller scale butalready reporting inconsistent results.
Heilman et al(2008) considered the performance of linear regres-sion, ordinal and multinomial logistic regression,and found the latter to be more efficient.
However,Kate et al (2010) obtained contradictory findings,showing that regression-based algorithms performbetter, especially when regression trees are used forbagging.
For French, Franc?ois (2011b) found thatSVMs were more efficient than linear regression, or-dinal and multinomial logistic regression, boosting,and bagging.Finally, it is quite possible that there are interac-tions between types of features and types of statis-tical algorithms and these interactions are primarilyresponsible for the better performance.In what follows, we present the results of threestudies (experiments 2-4), comparing the contribu-tions of the AI enabled features with features usedin classic formulas, different machine learning al-gorithms and the interactions of features with algo-rithms.
As mentioned earlier, all the studies havebeen done on French data, consisting of text ex-tracted from levelled FFL textbooks (French as For-eign Language).
Because there is no prior workcomparing classic formulas with AI readability mea-sures for FFL, we first report the results of this com-parison in the next section (experiment 1).3 Experiment 1: Model comparison forFFLTo compute a classic readability formula for FFL,we used the formula proposed for French by Kandeland Moles (1958).
We compared the results of thisformula with the AI model trained on the FFL dataused by Franc?ois (2011b).The Kandel and Moles (1958) formula is an adap-tation of the Flesch formula for French, based on astudy of a bilingual corpus:Y = 207?
1.015lp?
0.736lm (1)where Y is a readability score ranging from 100(easiest) to 0 (harder); lp is the average number ofwords per sentence and lm is the average number ofsyllables per 100 words.
Although this formula isnot specifically designed for FFL, we chose to im-plement it over formulas proposed for FFL (Tharp,1939; Uitdenbogerd, 2005).
FFL-specific formu-las are optimized for English-speaking learners ofFrench while our dataset is agnostic to the nativelanguage of the learners.The computation of the Kandel and Moles (1958)formula requires a syllabification system for French.Due to unavailability of such a system for French,we adopted a hybrid syllabification method.
Forwords included in Lexique (New et al, 2004), weused the gold syllabification included in the dictio-nary.
For all other words, we generated API pho-netic representations with espeak 1, and then appliedthe syllabification tool used for Lexique3 (Pallier,1999).
The accuracy of this process exceeded 98%.For the comparison with an AI model, we ex-tracted the same 46 features (see Table 2 for thecomplete list) used in Franc?ois?
model 2 and traineda SVM model.For all the study, the gold-standard consisted ofdata taken from textbooks and labeled according tothe classification made by the publishers.
The cor-pus includes a wide range of texts, including ex-tracts from novels, newspapers articles, songs, mail,dialogue, etc.
The difficulty levels are defined bythe Common European Framework of Reference forLanguages (CEFR) (Council of Europe, 2001) asfollows: A1 (Breakthrough); A2 (Waystage); B1(Threshold); B2 (Vantage); C1 (Effective Opera-tional Proficiency) and C2 (Mastery).
The test cor-pus includes 68 texts per level, for a total of 408 doc-uments (see Table 1).We applied both readability models to this testcorpus.
Assessing and comparing the performance1Available at: http://espeak.sourceforge.net/.2Details on how to implement these features can be found inFranc?ois (2011b).51A1 A2 B1 B2 C1 C2 Total68(10, 827) 68(12, 045) 68(17, 781) 68(25, 546) 68(92, 327) 68(39, 044) 408(127, 681)Table 1: Distribution of the number of texts and tokens per level in our test corpus.of the two models with accuracy scores (acc), as iscommon in classification tasks, has proved challeng-ing and, in the end, uninformative.
This is becausethe Kandel and Moles formula?s output scores arenot an ordinal variable, but intervals.
To computeaccuracy we would have to define a set of ratherarbitrary cut off points in the intervals and corre-spond them with level boundaries.
We tried threeapproaches to achieve this task.
First, we usedcorrespondences between Flesch scores and sevendifficulty levels proposed for French by de Land-sheere (1963): ?very easy?
(70 to 80) to ?very dif-ficult?
(-20 to 10).
Collapsing the ?difficult?
and?very difficult?
categories into one, we were ableto roughly match this scale with the A1-C2 scale.The second method was similar, except that thoselevels were mapped on the values from the originalFlesch scale instead of the one adapted for French.The third approach was to estimate normal distribu-tion parameters ?j and ?j for each level j for theKandel and Moles?
formula output scores obtainedon our corpus.
The class membership of a given ob-servation i was then computed as follows:arg6maxj=1P (i ?
j | N(?j , ?j)) (2)Since the parameters were trained on the same cor-pus used for the evaluation, this computation shouldyield optimal class membership thresholds for ourdata.Given the limitations of all three approaches, it isnot surprising that accuracy scores were very low:9% for the first and 12% for the second, which isworse than random (16.6%).
The third approachgave a much improved accuracy score, 33%, but stillquite low.
The problem is that, in a continuous for-mula, predictions that are very close to the actualwill be classified as errors if they fall on the wrongside of the cut off threshold.
These results are, inany case, clearly inferior to the AI formula based onSVM, which classified correctly 49% of the texts.A more suitable evaluation measure for a contin-uous formula would be to compute the multiple cor-relation (R).
The multiple correlation indicates theextent to which predictions are close to the actualclasses, and, when R2 is used, it describes the per-centage of the dependent variable variation whichis explained by the model.
Kandel and Moles?
for-mula got a slightly better performance (R = 0.551),which is still substantially lower that the score (R =0.728) obtained for the SVM model.
To check ifthe difference between the two correlation scoreswas significant, we applied the Hotelling?s T-test fordependent correlation (Hotelling, 1940) (requiredgiven that the two models were evaluated on thesame data).
The result of the test is highly signif-icant (t = ?19.5; p = 1.83e?60), confirming thatthe SVM model performed better that the classic for-mula.Finally, we computed a partial Spearman corre-lation for both models.
We considered the outputof each model as a single variable and we could,therefore, evaluate the relative predictive power ofeach variable when the other variable is controlled.The partial correlation for the Kandel and Moles for-mula is very low (?
= ?0.11; p = 0.04) whilethe SVM model retains a good partial correlation(?
= ?0.53; p < 0.001).4 Experiment 2: Comparison of featuresIn this section, we compared the contribution of thefeatures used in classic formulas with the more so-phisticated NLP-enabled features used in the ma-chine learning models of readability.
Given that thefeatures used in classic formulas are very easy tocompute and require minimal processing by com-parison to the NLP features that require heavy pre-processing (e.g., parsing), we are, also, interested infinding out how much gain we obtain from the NLPfeatures.
A consideration that becomes importantfor tasks requiring real time evaluation of readingdifficulty.To evaluate the relative contribution of each setof features, we experiment with two sets of fea-tures (see Table 2.
We labeled as ?classic?, not only52Family Tag Description of the variable ?
LinearClassicPA-AlteregoProportion of absent words from a list0.652 Noof easy words from AlterEgo1X90FFFC 90th percentile of inflected forms for content words only ?0.641 NoX75FFFC 75th percentile of inflected forms for content words only ?0.63 NoPA-Goug2000Proportion of absent words from 2000 first0.597 Noof Gougenheim et al (1964)?s listMedianFFFC Median of the frequencies of inflected content words ?0.56 YesPM8 Pourcentage of words longer than 8 characters 0.525 NoNL90PLength of the word corresponding to0.521 Nothe 90th percentile of word lengthsNLM Mean number of letters per word 0.483 YesIQFFFC Interquartile range of the frequencies of inflected content words 0.405 NoMeanFFFC Mean of the frequencies of inflected content words ?0.319 NoTTR Type-token ratio based on lemma 0.284 NoNMP Mean number of words per sentence 0.618 NoNWS90 Length (in words) of the 90th percentile sentence 0.61 NoPL30 Percentage of sentences longer than 30 words 0.56 YesPRE/PRO Ratio of prepositions and pronouns 0.345 YesGRAM/PRO Ratio of grammatical words and pronouns 0.34 YesART/PRO Ratio of articles and pronouns 0.326 YesPRE/ALL Proportions of prepositions in the text 0.326 YesPRE/LEX Ratio of prepositions and lexical words 0.322 YesART/LEX Ratio of articles and lexical words 0.31 YesPRE/GRAM Ratio of prepositions and grammatical words 0.304 YesNOM-NAM/ART Ratio of nouns (common and proper) and gramm.
words ?0.29 YesPP1P2 Percentage of P1 and P2 personal pronouns ?0.333 NoPP2 Percentage of P2 personal pronouns ?0.325 YesPPD Percentage of personal pronouns of dialogue 0.318 NoBINGUI Presence of commas 0, 462 NoNon-classicUnigram Probability of the text sequence based on unigrams 0.546 NoMeanNGProb-G Average probability of the text bigrams based on Google 0.407 YesFCNeigh75 75th percentile of the cumulated frequency of neighbors per word ?0.306 YesMedNeigh+Freq Median number of more frequent neighbor for words ?0.229 YesNeigh+Freq90 90th percentile of more frequent neighbor for words ?0.192 YesPPres Presence of at least one present participle in the text 0.44 NoPPres-C Proportion of present participle among verbs 0.41 YesPPasse Presence of at least one past participle 0.388 NoInfi Presence of at least one infinive 0.341 NoImpf Presence of at least one imperfect 0.272 NoSubp Presence of at least one subjunctive present 0.266 YesFutur Presence of at least one future 0.252 NoCond Presence of at least one conditional 0.227 NoPasseSim Presence of at least one simple past 0.146 NoImperatif Presence of at least one imperative 0.019 YesSubi Presence of at least one subjunctive imperfect 0.049 YesavLocalLsa-Lem Average intersentential cohesion measured via LSA 0, 63 NoConcDensEstimate of the conceptual density0.253 Yeswith Denside?es (Lee et al, 2010)NAColl Proportion of MWE having the structure NOUN ADJ 0.286 YesNCPW Average number of MWEs per word 0.135 YesTable 2: List of the 46 features used by Franc?ois (2011b) in his model.
The Spearman correlation reported here alsocomes from this study.53the features that are commonly used in traditionalformulas like Flesch (length of words and numberof words per sentence) but also other easy to com-pute features that were identified in readability work.Specifically, in the ?classic?
set we include num-ber of personal pronouns (given as a list) (Gray andLeary, 1935), the Type Token Ratio (TTR) (Livelyand Pressey, 1923), or even simple ratios of POS(Bormuth, 1966).The ?non-classic?
set includes more complexNLP-enabled features (coherence measured throughLSA, MWE, n-grams, etc.)
and features suggestedby the structuro-cognitivist research (e.g., informa-tion about tense and variables based on orthograph-ical neighbors).For evaluation, we first computed and comparedthe average bivariate correlations of both sets.
Thistest yielded a better correlation for the classic fea-tures (r?
= 0.48 over the non-classic features r?
=0.29)As a second test, we trained a SVM model on eachset and evaluated performances in a ten-fold cross-validation.
For this test, we reduced the number ofclassic features by six to equal the number of pre-dictors of the non-classic set.
Our hypothesis wasthe SVM model using non-classic features wouldoutperform the classic set because the non-classicfeatures bring richer information.
This assumptionwas not strictly confirmed as the non-classic set per-formed only slightly better than the classic set.
Thedifference in the correlation scores was small (0.01)and non-significant (t(9) = 0.49; p = 0.32), but thedifference in accuracy was larger (3.8%) and close tosignificance (t(9) = 1.50; p = 0.08).
Then, in an ef-fort to pin down the source of the SVM gain that didnot come out in the comparison above, we defined aSVM baseline model (b) that included only two typ-ical features of the classic set: the average numberof letter per word (NLM) and the average number ofword per sentence (NMP).
Then, for each of the iremaining variables (44), we trained a model mi in-cluding three predictors: NLM, NMP, and i. Thedifference between the correlation of the baselinemodel and that of the model mi was interpreted asthe information gain carried by the feature i. There-fore, for both sets, of cardinality Ns, we computed:?Nsi=1R(mi)?R(b)Ns(3)where R(mi) is the multiple correlation of modelmi.Our assumption was that, if the non-classic setbrings in more varied information, every predictorshould, on average, improve more theR of the base-line model, while the classic variables, more redun-dant with NLM and NP, would be less efficient.
Inthis test, the mean gain for R was 0.017 for the clas-sic set and 0.022 for the non-classic set.
Althoughthe difference was once more small, this test yieldeda similar trend than the previous test.As a final test, we compared the performance ofthe SVM model trained only on the ?classic?
setwith the SVM trained on both sets.
In this case,the improvement was significant (t(9) = 3.82; p =0.002) with accuracy rising from 37.5% to 49%.
Al-though this test does not help us decide on the natureof the gain as it could be coming just from the in-creased number of features, it shows that combining?classic?
and ?non-classic?
variables is valuable.5 Experiment 3: Comparison of statisticalmodelsIn this section, we explore the hypothesis that AImodels outperform classic formulas because theyuse better statistical algorithms.
We compare theperformance of a?classic?
algorithm, multiple linearregression, with the performance of a machine learn-ing algorithm, in this case SVM.
Note that an SVMshave an advantage over linear regression for featuresnon-linearly related with difficulty.
Bormuth (1966,98-102) showed that several classic features, espe-cially those focusing on the word level, were indeednon-linear.
To control for linearity, we split the 46features into a linear and a non-linear subset, usingthe Guilford?s F test for linearity (Guilford, 1965)and an ?
= 0.05.
This classification yielded twoequal sets of 23 variables (see Table 2).
In Table3, we report the performance of the four models interms of R, accuracy, and adjacent accuracy.
Fol-lowing, Heilman et al (2008), we define ?adjacentaccuracy?
as the proportion of predictions that werewithin one level of the assigned label in the corpus.54Model R Acc.
Adj.
acc.LinearLR 0.58 27% 72%SVM 0.64 38% 73%Non-LinearLR 0.75 36% 81%SVM 0.70 44% 76%Table 3: Multiple correlation coefficient (R), accuracyand adjacent accuracy for linear regression and SVMmodels, using the set of features either linearly or nonlinearly related to difficulty.Adjacent accuracy is closer toR as it is less sensitiveto minor classification errors.Our results showed a contradictory pattern, yield-ing a different result depending on type of evalu-tion: accuracy or R and adjacent accuracy.
Withrespect to accuracy scores, the SVM performed bet-ter in the classification task, with a significant per-formance gain for both linear (gain = 9%; t(9) =2.42; p = 0.02) and non-linear features (gain = 8%;t(9) = 3.01; p = 0.007).
On the other hand, the dif-ference in R was non-significant for linear (gain =0.06; t(9) = 0.80; p = 0.22) and even negative andclose to significance for non-linear (gain = ?0.05;t(9) = 1.61; p = 0.07).
In the light of these re-sults, linear regression (LR) appears to be as effi-cient as SVM accounting for variation in the depen-dant variable (their R2 are pretty similar), but pro-duces poorer predictions.This is an interesting finding, which suggests thatthe contradictory results in prior literature with re-gard to performance of different readability mod-els (see Section 2) might be related to the evalua-tion measure used.
Heilman et al (2008, 7), whocompared linear and logistic regressions, found thatthe R of the linear model was significantly higherthan the R of the logistic model (p < 0.01).
In con-trast, the logistic model behaved significantly better(p < 0.01) in terms of adjacent accuracy.
Similarly,Kate and al.
(2010, 548), which used R as evalua-tion measure, reported that their preliminary results?verified that regression performed better than clas-sification?.
Once they compared linear regressionand SVM regression, they noticed similar correla-tions for both techniques (respectively 0.7984 and0.7915).To conclude this section, our findings suggest that(1) linear regression and SVM are comparable in ac-counting for the variance of text difficulty and (2)SVM has significantly better accuracy scores thanlinear regression.6 Experiment 4: Combined evaluationIn Experiment 2, we saw that ?non-classic?
featuresare slightly, but non-significantly, better than the?classic?
features.
In Experiment 3, we saw thatSVM performs better than linear regression whenthe evaluation is done by accuracy but both demon-strate similar explanatory power in accounting forthe variation.
In this section, we report evaluationresults for four models, derived by combining twosets of features, classic and non-classic, with two al-gorithms, linear regression and SVM.
The results areshown in Table (4).The results are consistent with the findings inthe previous sections.
When evaluated with accu-racy scores SVM performs better with both classic(t(9) = 3.15; p = 0.006) and non-classic features(t(9) = 3.32; p = 0.004).
The larger effect obtainedfor the non-classic features might be due to an in-teraction, i.e., an SVM trained with non-classic fea-tures might be better at discriminating reading lev-els.
However, with respect to R, both algorithms aresimilar, with linear regression outperforming SVMin adjacent accuracy (non-significant).
Linear re-gression and SVM, then, appear to have equal ex-planatory power.As regards the type of features, the explanatorypower of both models seems to increase with non-classic features as shown in the increased R, al-though significance is not reached (t(9) = 0.49; p =0.32 for the regression and t(9) = 1.5; p = 0.08 forthe SVM).7 General discussion and conclusionsRecent readability studies have provided prelimi-nary evidence that the evaluation of readability us-ing NLP-enabled features and sophisticated machinelearning algorithms outperform the classic readabil-ity formulas, such as Flesch, which rely on surfacetextual features.
In this paper, we reported a numberof experiments the purpose of which was to identifythe source of this performance gain.Specifically, we compared the performance ofclassic and non-classic features and the performance55Model R Acc.
Adj.
acc.ClassicLR 0.66 30.6% 78%SVM 0.67 37.5% 76%Non-classicLR 0.68 32% 76%SVM 0.68 41.8% 73%Table 4: Multiple correlation coefficient (R), accuracy and adjacent accuracy for linear regression and SVM modelswith either the classic or the non-classic set of predictors.of two statistical algorithms: linear regression (usedin classic formulas) and SVM (in the context of FFLreadability).
Our results indicate that classic featuresare strong single predictors of readability.
Whilewe were not able to show that the non-classic fea-tures are better predictors by themselves, our find-ings show that leaving out non-classic features has asignificant negative impact on the performance.
Thebest performance was obtained when both classicand non-classic features were used.Our experiments on the comparison of the twostatistical algorithms showed that the SVM outper-forms linear regression by a measure of accuracy,but the two algorithms are comparable in explana-tory power accounting for the same amount of vari-ability.
This observation accounts for contradictoryconclusions reported in previous work.
Our studyshows that different evaluation measures can lead toquite different conclusions.Finally, our comparison of four models derivedby combining linear regression and SVM with ?clas-sic?
and ?non-classic?
features confirms the signif-icant contribution of ?non-classic?
features and theSVM algorithm to classification accuracy.
However,by a measure of adjacent accuracy and explanatorypower, the two algorithms are comparable.From a practical application point of view, itwould be interesting to try these algorithms in webapplications that process large amounts of text inreal time (e.g., READ-X (Miltsakaki, 2009)) to eval-uate the trade-offs between accuracy and efficiency.AcknowledgmentsWe would like to acknowledge the invaluable help ofBernadette Dehottay for the collection of the corpus,as well as the Belgian American Educational Foun-dation that supported Dr. Thomas Franc?ois with aFellowship during this work.ReferencesS.
Aluisio, L. Specia, C. Gasperin, and C. Scarton.
2010.Readability assessment for text simplification.
In FifthWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, pages 1?9, Los Angeles.J.R.
Bormuth.
1966.
Readability: A new approach.Reading research quarterly, 1(3):79?132.K.
Collins-Thompson and J. Callan.
2005.
Predict-ing reading difficulty with statistical language models.Journal of the American Society for Information Sci-ence and Technology, 56(13):1448?1462.Council of Europe.
2001.
Common European Frame-work of Reference for Languages: Learning, Teach-ing, Assessment.
Press Syndicate of the University ofCambridge.E.
Dale and J.S.
Chall.
1948.
A formula for predictingreadability.
Educational research bulletin, 27(1):11?28.G.
de Landsheere.
1963.
Pour une application des testsde lisibilite?
de Flesch a` la langue franc?aise.
Le TravailHumain, 26:141?154.W.H.
DuBay.
2004.
The principles of read-ability.
Impact Information.
Disponible surhttp://www.nald.ca/library/research/readab/readab.pdf.L.
Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.2010.
A Comparison of Features for Automatic Read-ability Assessment.
In COLING 2010: Poster Volume,pages 276?284.R.
Flesch.
1948.
A new readability yardstick.
Journal ofApplied Psychology, 32(3):221?233.T.
Franc?ois.
2011a.
La lisibilite?
computationnelle: un renouveau pour la lisibilite?
du franc?ais languepremie`re et seconde ?
International Journal of Ap-plied Linguistics (ITL), 160:75?99.T.
Franc?ois.
2011b.
Les apports du traitement au-tomatique du langage a` la lisibilite?
du franais languee?trange`re.
Ph.D. thesis, Universite?
Catholique de Lou-vain.
Thesis Supervisors : Ce?drick Fairon and AnneCatherine Simon.G.
Gougenheim, R. Miche?a, P. Rivenc, and A. Sauvageot.1964.
Le?laboration du franc?ais fondamental (1erdegre?).
Didier, Paris.56W.S.
Gray and B.E.
Leary.
1935.
What makes a bookreadable.
University of Chicago Press, Chicago: Illi-nois.J.P.
Guilford.
1965.
Fundamental statistics in psychol-ogy and education.
McGraw-Hill, New-York.M.
Heilman, K. Collins-Thompson, and M. Eskenazi.2008.
An analysis of statistical models and featuresfor reading difficulty prediction.
In Proceedings of theThird Workshop on Innovative Use of NLP for Build-ing Educational Applications, pages 1?8.H.
Hotelling.
1940.
The selection of variates for use inprediction with some comments on the general prob-lem of nuisance parameters.
The Annals of Mathemat-ical Statistics, 11(3):271?283.L.
Kandel and A. Moles.
1958.
Application de l?indicede Flesch a` la langue franc?aise.
Cahiers E?tudes deRadio-Te?le?vision, 19:253?274.R.
Kate, X. Luo, S. Patwardhan, M. Franz, R. Florian,R.
Mooney, S. Roukos, and C. Welty.
2010.
Learn-ing to predict readability using diverse linguistic fea-tures.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (Coling 2010),pages 546?554.S.
Kemper.
1983.
Measuring the inference load of a text.Journal of Educational Psychology, 75(3):391?401.W.
Kintsch and D. Vipond.
1979.
Reading comprehen-sion and readability in educational practice and psy-chological theory.
In L.G.
Nilsson, editor, Perspec-tives on Memory Research, pages 329?365.
LawrenceErlbaum, Hillsdale, NJ.G.R.. Klare.
1963.
The Measurement of Readability.Iowa State University Press, Ames, IA.G.R.
Klare.
1984.
Readability.
In P.D.
Pearson, R. Barr,M.
L. Kamil, P. Mosenthal, and R. Dykstra, edi-tors, Handbook of Reading Research, pages 681?744.Longman, New York.H.
Lee, P. Gambette, E.
Maille?, and C. Thuillier.
2010.Denside?es: calcul automatique de la densite?
des ide?esdans un corpus oral.
In Actes de la douxime Rencon-tre des tudiants Chercheurs en Informatique pour leTraitement Automatique des langues (RECITAL).B.A.
Lively and S.L.
Pressey.
1923.
A method for mea-suring the vocabulary burden of textbooks.
Educa-tional Administration and Supervision, 9:389?398.E.
Miltsakaki.
2009.
Matching readers?
preferences andreading skills with appropriate web texts.
In Proceed-ings of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics:Demonstrations Session, pages 49?52.B.
New, C. Pallier, M. Brysbaert, and L. Ferrand.
2004.Lexique 2: A new French lexical database.
Behav-ior Research Methods, Instruments, & Computers,36(3):516.C.
Pallier.
1999.
Syllabation des repre?sentationsphone?tiques de brulex et de lexique.
Technical report,Technical Report, update 2004.
Lien: http://www.
pal-lier.
org/ressources/syllabif/syllabation.
pdf.E.
Pitler and A. Nenkova.
2008.
Revisiting readabil-ity: A unified framework for predicting text quality.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 186?195.J.C.
Redish and J. Selzer.
1985.
The place of readabilityformulas in technical communication.
Technical com-munication, 32(4):46?52.S.E.
Schwarm and M. Ostendorf.
2005.
Reading levelassessment using support vector machines and statis-tical language models.
Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 523?530.L.
Si and J. Callan.
2001.
A statistical model for sci-entific readability.
In Proceedings of the Tenth Inter-national Conference on Information and KnowledgeManagement, pages 574?576.
ACM New York, NY,USA.J.B.
Tharp.
1939.
The Measurement of Vocabulary Dif-ficulty.
Modern Language Journal, pages 169?178.S.
Uitdenbogerd.
2005.
Readability of French as a for-eign language and its uses.
In Proceedings of the Aus-tralian Document Computing Symposium, pages 19?25.M.
Vogel and C. Washburne.
1928.
An objective methodof determining grade placement of children?s readingmaterial.
The Elementary School Journal, 28(5):373?381.57
