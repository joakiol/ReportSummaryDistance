Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 37?45,Beijing, August 2010Pruning Non-Informative Text Through Non-Expert Annotations toImprove Aspect-Level Sentiment ClassificationJi FangPalo Alto Research CenterJi.Fang@parc.comBob PricePalo Alto Research CenterBob.Price@parc.comLotti PricePalo Alto Research CenterLotti.Price@parc.comAbstractSentiment analysis attempts to extract theauthor?s sentiments or opinions from un-structured text.
Unlike approaches basedon rules, a machine learning approachholds the promise of learning robust, high-coverage sentiment classifiers from la-beled examples.
However, people tendto use different ways to express the samesentiment due to the richness of naturallanguage.
Therefore, each sentiment ex-pression normally does not have many ex-amples in the training corpus.
Further-more, sentences extracted from unstruc-tured text (e.g., I filmed my daughter?sballet recital and could not believe howthe auto focus kept blurring then focus-ing) often contain both informative (e.g.,the auto focus kept blurring then focus-ing) and extraneous non-informative textregarding the author?s sentiment towards acertain topic.
When there are few exam-ples of any given sentiment expression, ex-traneous non-sentiment information can-not be identified as noise by the learn-ing algorithm and can easily become cor-related with the sentiment label, therebyconfusing sentiment classifiers.
In this pa-per, we present a highly effective proce-dure for using crowd-sourcing techniquesto label informative and non-informativeinformation regarding the sentiment ex-pressed in a sentence.
We also showthat pruning non-informative informationusing non-expert annotations during thetraining phase can result in classifiers withbetter performance even when the test dataincludes non-informative information.1 IntroductionNoise in training data can be derived either fromnoisy labeling or from noisy features.
It has beenshown that labeling quality is one of the importantfactors that impacts the performance of a learnedmodel, and that this quality can be improved byapproaches such as using multiple labelers (Shenget al, 2008).
However, noisy features can be aninherent characteristic for some text mining tasks,and it is unclear how they should be handled.For example, sentiment analysis/opinion min-ing from unstructured user generated content suchas online reviews and blogs often relies on learn-ing sentiments from word-based features extractedfrom the training sentences and documents (Panget al, 2002; Dave et al, 2003; Kim and Hovy,2005).
However, not all words in the training datacarry information about sentiment.
For example,in sentence (1),(1)I filmed my daughter?s ballet recital andcould not believe how the auto focus kept blurringthen focusing.although words such as auto focus, blurring andfocusing are informative for learning sentiment re-garding the auto focus capability of the camera,words such as film, daughter and ballet recital arenot informative for that type of sentiment, and theyform noise if included as training data.If the training data contain a lot of examplessuch as (2) in which words such as film, daughterand ballet recital also appear, but the sentence isnot labelled as invoking sentiment regarding autofocus, a machine learning algorithm might learn37that such words are not informative for sentimentclassification.
(2)I filmed my daughter?s ballet recital andcould not believe how good the picture qualitywas.However, due to the richness of natural lan-guage, people tend to use different ways to de-scribe a similar event or to express a similar opin-ion.
Consequently, repeated use of the same ex-pression is not common in the training data forsentiment classification.
Note that this difficultycannot be simply overcome by increasing the sizeof the training data.
For example, a search onthe completely natural phrase ?I filmed my daugh-ter?s ballet recital?
in Google and Bing returns thesame exact sentence as shown in (1).
In otherwords, there appears to be only one sentence con-taining that exact phrase, which implies that evenif we use the entire web as our training data setwe would not find an example such as (2) to helpthe learning algorithm to determine which featurewords in (1) are informative and which are not.Therefore, data sparsity is an inherent problem fora task such as sentiment analysis, and if we adoptthe bag-of-words approach for sentiment classifi-cation (Pang et al, 2002), which uses the wordsthat appear in sentences as training features, ourtraining data will unavoidably include many noisynon-informative features.This paper presents a crowd-sourcing techniqueto identify and prune the non-informative features.We explore the effect of using non-expert annota-tions to gain low-noise training data for sentimentclassification.
We show that the cleaner trainingdata obtained from non-expert annotations signif-icantly improve the performance of the sentimentclassifier.
We also present evidence that this im-provement is due to reduction in confusion be-tween classes due to noise words.The remainder of this paper is organized as fol-lows.
Section 2 discusses the related work.
Sec-tion 3 describes our approach for pruning non-informative features.
Section 4 presents an empir-ical study on the effect of training on informativefeatures in the domain of sentiment analysis.
Con-clusions are summarized in Section 5.2 Related WorkFeature selection in the domain of sentiment anal-ysis has focused on the following issues.a) Should word-based features be selectedbased on frequency or presence?It has been shown that compared to word fre-quency, word presence is a better sentiment indi-cator (Pang et al, 2002; Wiebe et al, 2004; Yanget al, 2006).
In other words, unlike in other do-mains such as topic classification where the fre-quency of words provides useful information re-garding the topic class, sentiment information isnot normally indicated by the frequency of certainwords, because people are unlikely to repeatedlyuse the same word or phrase to express an opin-ion in one document.
Instead, Researchers (Panget al, 2002) found that selecting features based onword presence rather than word frequency leadsto better performance in the domain of sentimentanalysis.b) Which are more useful features: uni-grams, higher-order n-grams or syntactically re-lated terms?This issue seems to be debatable.
While someresearchers (Pang et al, 2002) reported that un-igrams outperform both bigrams as well as thecombination of unigrams and bigrams in classi-fying movie reviews based on sentiment polarity,some others (Dave et al, 2003) reported the oppo-site in some settings.Similarly, some (Dave et al, 2003) found syn-tactically related terms are not helpful for senti-ment classification, whereas others (Gamon, 2004;Matsumoto et al, 2005; Ng et al, 2006) found theopposite to be true.c) In terms of part-of-speech, which types ofwords are more useful features?Adjectives and adverbs are commonly used asfeatures for sentiment learning (Mullen and Col-lier, 2004; Turney, 2002; Whitelaw et al, 2005).However, more recent studies show that all con-tent words including nouns, verbs, adjectives andadverbs are useful features for sentiment analysis(Dillard, 2007).Regardless of which types of features areused, these traditional approaches are still in-herently noisy in the sense that non-informative38words/features within each sentence are includedas described in Section 1.
As far as we are aware,this is an issue that has not been addressed.The closest works are Riloff et al (Riloff andWiebe, 2003) and Pang et al (Pang et al, 2002)?swork.
Riloff et al explored removing the featuresthat are subsumed in other features when a com-bination of different types of features such as un-igrams, bigrams and syntactically related terms isused.
Pang et al speculated that words that appearat certain positions in a movie review are more in-formative for the overall opinion reflected in thatreview.
However, according to Pang et al, for thetask of predicting the overall polarity of a moviereview, training on word features assumed to bemore informative resulted in worse performancethan training on all word features appearing in thereviews.Our approach is different in that we try to iden-tify and prune non-informative word features atthe sentence level.
We focus on identifying whichportion of the sentence is informative for senti-ment classification.
We then completely removethe non-informative portion of the sentence andprevent any terms occurring in that portion frombeing selected as feature vectors representing thatsentence.
Note that the classification of words asnon-informative is not related to their positions ina sentence nor to their frequency count in the train-ing corpus.
Instead, whether a word is informativedepends purely on the semantics and the contextof the sentence.
For example, the word big wouldbe non-informative in (3), but informative in (4).
(3)That was a big trip, and I took a lot of pic-tures using this camera.
(4)This camera has a big LCD screen.Unlike the traditional approach of using ex-pert annotation to identify the non-informative textin a sentence, we instead use non-expert annota-tions without external gold standard comparisons.There have been an increasing number of exper-iments using non-expert annotations for variousNatural Language Processing (NLP) tasks.
For ex-ample, Su et al (Su et al, 2007) use non-expertannotations for hotel name entity resolution.
In(Nakov, 2008), non-expert annotators generatedparaphrases for 250 noun-noun compounds, whichwere then used as the gold standard data for eval-uating an automatic paraphrasing system.
Kaisserand Lowe (Kaisser and Lowe, 2008) also use non-experts to annotate answers contained in sentencesand use the annotation results to help build a ques-tion answering corpus.
Snow et al (Snow etal., 2008) reported experiments using non-expertannotation for the following five NLP tasks: af-fect recognition, word similarity, recognizing tex-tual entailment, event temporal ordering, and wordsense disambiguation.This paper presents a study of using non-expertannotations to prune non-informative word fea-tures and training a sentiment classifier based onsuch non-expert annotations.
The following sec-tion describes our approach in detail.3 Non-Informative Feature PruningThrough Non-Expert AnnotationsTo prune the non-informative features, a tradi-tional approach would be to hire and train anno-tators to label which portion of each training sen-tence is informative or non-informative.
However,this approach is both expensive and time consum-ing.
We overcome these issues by using crowd-sourcing techniques to obtain annotations fromuntrained non-expert workers such as the ones onthe Amazon Mechanical Turk (AMT) platform1.To illustrate our approach, we use an example forsentiment analysis below.The key to our approach relies on careful de-sign of simple tasks or HITs that can elicit thenecessary information for both labeling the senti-ment information and pruning the non-informativetext of a sentence.
These tasks can be performedquickly and inexpensively by untrained non-expertworkers on the AMT platform.
We achieved thisgoal by designing the following two experiments.Experiment 1 asks the workers to judge whethera sentence indicates an opinion towards a certainaspect of the camera, and if so, whether the opin-ion is positive, negative or neutral.
For example,the proper annotations for sentence (5) would beas shown in Figure 1.1This is an online market place that offers a small amountof money to people who perform some ?Human IntelligenceTasks?
(HITs).
https://www.mturk.com/mturk/welcome39(5) On my trip to California, the camera fell andbroke into two pieces.Figure 1: Experiment 1We randomly selected 6100 sentences in totalfor this experiment from the Multi-Domain Senti-ment Dataset created by Blitzer et al (Blitzer etal., 2007).
Each sentence was independently an-notated by two AMT workers.
Each annotationconsisted of a sentence labeled with a camera as-pect and a sentiment toward that aspect.One unique characteristic of Experiment1 isthat it makes the detection of unreliable responsesvery easy.
Because one sentence is unlikely to in-voke many different aspects of cameras, an anno-tation is thus suspicious if many aspects of cam-era are annotated as being invoked.
Figure 2 andFigure 3 illustrate the contrast between a normalreliable response and a suspicious unreliable re-sponse.Due to this favorable characteristic of Experi-ment 1, we did not have to design a qualificationtest.
We approved all of the assignments; how-ever we later filtered out the detected suspiciousresponses, which accounted for 8% of the work.Even though we restricted our AMT workers tothose who have an approval rate of 95% or above,we still found 20% of them unreliable in the sensethat they provided suspicious responses.Given our ability to detecting suspicious re-sponses, we believe it is very unlikely for two reli-able AMT workers to annotate any given sentenceexactly the same way merely by chance.
There-fore, we consider an annotation to be gold whenboth annotators marked the same sentiment towardthe same aspect.
We obtained 2718 gold-standardannotations from the reliable responses.
We definethe agreement rate of annotations as follows.AgreementRate = NumberofGoldAnnotations?2TotalNumberofAnnotations .
(1)Based on this measure, the agreement rate of theAMT workers in this study is 48.4%.We held randomly selected 587 gold annotatedsentences as our test set, and used the remain-ing 2131 sentences as our training sentences.
Toprune the non-informative text from the trainingsentences, we put the 2131 sentences through Ex-periment 2 as described below.Experiment 2 asks the workers to point outthe exact portion of the sentence that indicatesan opinion.
The opinion and its associated fea-ture name are displayed along with the sentence inwhich they appear.
Such information is automati-cally generated from the results derived from Ex-periment 1.
An example of Experiment 2 is givenin Figure 4.Figure 4: Experiment 2The expected answer for this example is the bat-tery door keeps falling off.Using this method, we can remove the non-informative part of the sentences: One thing I haveto mention is that and prevent any of the words inthat part from being selected as our training fea-tures.Experiment 2 requires the workers to enter orcopy and paste text in the box, and 100% of theworkers did it.
In our sentiment classification ex-periment described below, we used all of the re-sults without further filtering.We paid $0.01 for each assignment in both ex-periments, and we acquired all of the annotationsin one week?s time with a total cost of $215, in-cluding fees paid to Amazon.
Our pay rate is about$0.36/hour.
For Experiment 1 alone, if we adopteda traditional approach and hired two annotators,they could likely complete the annotations in five8-hour days.
Using this approach, the cost for Ex-periment 1 alone would be $1200, with a rate of$15/hour.
Therefore, our approach is both cheaperand faster than the traditional approach.40Figure 2: Reliable ResponseFigure 3: Unreliable ResponseHaving described our crowd-souring based ap-proach for pruning the non-informative features,we next present an empirical study on the effect oftraining on informative features.4 Pruning Non-Informative Features forSentiment ClassificationWe conducted an experiment on sentiment classifi-cation in the domain of camera reviews to test theeffect of pruning non-informative features basedon AMT workers?
annotations.In our experiment, we select the Nouns, Verbs,Adjectives and Adverbs as our unigram featuresfor training.
We define non-informative fea-tures as the four types of words occurring in thenon-informative portion of the training sentence;namely, the portion that does not mention any as-pect of the camera or associated sentiment.
Forexample, for a training sentence such as (1) (re-peated below as (6)), training on all features wouldselect the following words: [film, daughter, ballet,recital, not-believe2, auto, focus, kept, blurring,focusing].
(6) I filmed my daughter?s ballet recital andcould not believe how the auto focus kept blurringthen focusing.By contrast, pruning non-informative featureswould yield a shorter list of selected words: [auto,focus, kept, blurring, focusing].In our experiment, we compare the performance2See below for the description regarding how we handlenegation.of the classifier learned from all of the Nouns,Verbs, Adjectives and Adverbs in the sentenceswith the one learned from these word types oc-curring only in the informative part of the sen-tence.
When the training set contains all of the fea-ture words, we refer to it as the All-Features-Set.When the non-informative features are pruned,the training set contains only the informative fea-ture words, which we refer to as the Informative-Features-Set.All of the feature words are stemmed using thePorter Stemmer (Porter, 1980).
Negators are at-tached to the next selected feature word.
We alsouse a small set of stop words3 to exclude copulasand words such as take.
The reason that we choosethese words as stop words is because they are bothfrequent and ambiguous and thus tend to have anegative impact on the classifier.All of our training and test sentences are an-notated through crowd-sourcing techniques as de-scribed in the last section.
In our experimentwe use 2131 sentences in total for training and587 sentences for hold-out testing.
The non-informative part of the test sentences are not re-moved.
The experiment results and implicationsare discussed in detail in the following subsec-tions.3The stop words we use include copulas and the followingwords: take, takes, make, makes, just, still, even, too, much,enough, back, again, far, same414.1 Aspect:Polarity Classification Using SVMIn this experiment, the task is to perform a 45 waysentiment classification.
These 45 classes are de-rived from 22 aspects related to camera purchasessuch as picture quality, LCD screen, battery lifeand customer support and their associated polar-ity values positive and negative, as well as a classof no opinion about any of the 22 aspects.
An ex-ample of such a class is picture quality: positive.The classifier maps each input sentence into oneof the 45 classes.One of the approaches we tested is to train theclassifier based on the All-Features-Set derivedfrom the original raw sentences.
We refer to this as?All Features?.
The other approach is to learn fromthe Informative-Features-Set derived from the sen-tences with the non-informative portion removedby the AMT workers.
We refer to this as ?Informa-tive Features?.
The experiment is conducted us-ing SVM algorithm implemented by Chang et al(Chang and Lin, 2001).
We use linear kernel typeand use the default setting for all other parameters.The classification accuracy is defined as fol-lows.Accuracy = NumberofSentencesCorrectlyClassifiedTotalNumberofSentences .
(2)The experiment results in terms of classificationaccuracy are shown in Table 1.Table 1: Classification AccuracyAll Features Informative Features41.7% 45.8%In this experiment, pruning the non-informativefeatures improves the accuracy by more than 4%.This improvement is statistically significant by aone-tailed sign test at p = 0.15.
Training on the in-formative features also consistently improves theclassification accuracy when we vary the size ofthe training data as illustrated by the Figure 54.4To demonstrate the learning curve, we experimentedwith the use of different percentages of the training sen-tences while always testing on the same 587 test sentences.When the percentage of the training sentences used is lessthan 100%, we randomly pick that percentage of training sen-tences until the test accuracy converges.Figure 5: Learning CurveA salient characteristic of this experiment is thatthe training data tend to be very sparse for two rea-sons.
First, the number of classes is large, whichmeans that the number of training examples foreach class will be fewer.
As shown in Table 2,24 out of the 45 classes have fewer than 30 train-ing examples, which is an indication of how sparsethe training data is.
Second, as shown in Section1, people tend to use different ways to express thetype of sentiments that we aim to learn in this ex-periment.
Therefore, it is difficult to collect re-peated training examples and this difficulty cannotbe simply overcome by increasing the size of thetraining data.
This data sparsity means that it isdifficult for the SVM to learn which feature wordsare non-informative noise.Table 2: Class Distribution in Experiment 1Number of Classes Number of Training Sentences6 fewer than 1014 fewer than 2024 fewer than 3033 fewer than 5041 fewer than 1004 more than 1004.2 Automatic Feature Selection vs. Pruningby AMT WorkersAs shown in the previous subsection, pruning non-informative word features using non-expert anno-tations can significantly improve the performanceof the sentiment classifier.
Can we achieve thesame improvement by using automatic feature se-lection algorithms?We tried three widely used feature se-lection techniques LR(Likelihood Ratio),WLLR(Weighted Log-Likelihood Ratio) (Nigamet al, 2000; Ng et al, 2006) and MI(MutualInformation) and applied them to the original rawtraining data.
We found that in general, the fewer42the feature words selected by these algorithms,the worse the classifier performs.
The classifierperformed the best when using all of the availablefeature words.
In other words, automatic featureselection offered no benefit.
Table 3 shows theresults of using these three automatic featureselection techniques as well as the results ofnot performing automatic feature selection.
Thethreshold for the LR algorithm was set to be 5; thethreshold for the WLLR algorithm was set to be0.005; and the threshold for the MI algorithm wasset to be 2000 (using the top 2000 ranked featuresout of a total of 3279 features).Table 3: Automatic Feature Selection ResultsNo Feature Selection LR WLLR MI41.7% 35.4% 40.2% 41.1%This result is not surprising given the data spar-sity issue in our experiment.
Traditional featureselection methods either try to remove correlatedfeatures which can cause havoc for some meth-ods or to prune out features uncorrelated with la-bels to make learning more efficient.
However, wehave sparse data so correlations calcuated are veryunstable - if a feature appears once with a labelwhat can we conclude?
So the same propertiesthat cause difficulties for the learner cause prob-lems for feature selection techniques as well.To summarize, pruning non-informative wordfeatures using non-expert annotations can signif-icantly improve the performance of the sentimentclassifier even when the test data still contain non-informative features.
We believe this is becausepruning non-informative feature words based onhuman knowledge leads to better training data thatcannot be achieved by using automatic feature se-lection techniques.
The subsection below com-pares the two sets of training sentences we usedin this experiment: one comprises the original rawsentences and the other comprises sentences withthe non-informative text removed.
We show thatour approach of pruning non-informative text in-deed leads to a better set of training data.4.3 Comparison of Training Data Before andAfter the Feature PruningOur assumption is that training data is better if databelonging to closer classes are more similar anddata belonging to further classes are more differ-ent.
In our sentiment classification experiment, anexample of two very close classes are battery life:positive and battery life: negative.
An example oftwo very different classes are battery life: positiveand auto focus: negative.
The more similar thetraining data belonging to closer classes and themore dissimilar the training data belonging to dif-ferent classes, the more accurate the classifier canpredict the involved camera aspect, which in turnshould lead to improvements on the overall classi-fication accuracy.To test whether the pruned text produced bet-ter training data than the original text, an adjustedcosine similarity measure was used.
Note thatour measurement can only reflect partial effectsof AMT workers?
pruning, because our measureis essentially term frequency based, which can re-flect similarity in terms of topic (camera aspectsin our case) but not similarity in terms of polarity(Pang et al, 2002).
Nevertheless, this measure-ment highlights some of the impact resulting fromthe pruning.To compare training data belonging to any twoclasses, we produce a tf-idf score for each wordin those two classes and represent each class as avector containing the tf-idf score for each word inthat class.
Comparing the similarity of two classesinvolves calculating the adjusted cosine similarityin the following formula.similarity = A?B||A||||B|| .
(3)A and B in the above formula are vectors of tf-idf scores, whereas in the standard cosine similar-ity measure A and B would be vectors containingtf scores.
The motivation for using tf-idf scoresinstead of the tf scores is to reduce the importanceof highly common words such as the and a in thecomparison.
The similarity score produced by thisformula is a number between 0 and 1; 0 being nooverlap and 1 indicating that the classes are iden-tical.
Word stemming was not used in this experi-ment.43We compared similarity changes in two situa-tions.
First, when two classes share the same as-pect; this involves comparison between 22 classpairs such as battery life: positive vs. battery life:negative.
Second, when two classes share differentaspects; for example, battery life: positive vs. autofocus: negative and battery life: positive vs. autofocus: positive.
In this situation, we compared thesimilarity changes in 903 class pairs.
If pruningthe non-informative text does indeed provide bet-ter training data, we expect similarity to increasein the first situation and to decrease in the secondsituation after the pruning.
This is precisely whatwe found; our finding is summarized in Table 4.Table 4: Average Similarity Changes in the Pruned TrainingDataSame aspect Different aspect+0.01 -0.02In conclusion, AMT workers, by highlightingthe most pertinent information for classificationand allowing us to discard the rest, provided moreuseful data than the raw text.5 ConclusionsTo summarize, we found that removing the non-informative text from the training sentences pro-duces better training data and significantly im-proves the performance of the sentiment clas-sifier even when the test data still containnon-informative feature words.
We also showthat annotations for both sentiment classes andsentiment-informative texts can be acquired effi-ciently through crowd-sourcing techniques as de-scribed in this paper.6 AcknowledgmentsWe thank Prateek Sarkar, Jessica Staddon and BiChen for insightful discussions and comments.We thank the anonymous reviewers?
helpful sug-gestions and feedback.
We would also like tothank Jason Kessler for implementing part of thesentiment analysis algorithm and the Amazon Me-chanical Turk experiments.ReferencesSheng, Victor S., Provost, Foster, and Ipeirotis, Panagi-otis G.. 2008.
Get Another Label?
Improving DataQuality and Data Mining Using Multiple, Noisy La-belers.
KDD 2008 Proceedings 614-622.Pang, Bo, Lee, Lillian, and Vaithyanathan, Shivakumar.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP) 79-86.Dave, Kushal, Lawrence, Steve, and Pennock, DavidM.. 2003.
Mining the peanut gallery: Opinion ex-traction and semantic classification of product re-views.
Proceedings of WWW 519-528.Kim, Soo?Min and Hovy, Eduard.
2005.
Identifyingopinion holders for question answering in opiniontexts.
Proceedings of the AAAI Workshop on Ques-tion Answering in Restricted Domains.Wiebe, Janyce M. , Wilson, Theresa , Bruce, Rebecca, Bell, Matthew and Martin, Melanie.
2004.
Learn-ing subjective language.
Computational Linguistics,30(3):277-308.Yang, Kiduk , Yu, Ning , Valerio, Alejandro and Zhang,Hui.
2006.
WIDIT in TREC-2006 Blog track.
Pro-ceedings of TREC.Gamon, Michael.
2004.
Sentiment classification oncustomer feedback data: noisy data, large featurevectors, and the role of linguistic analysis.
Proceed-ings of the International Conference on Computa-tional Linguistics (COLING).Matsumoto, Shotaro, Takamura, Hiroya and Okumura,Manabu.
2005.
Sentiment classification using wordsub-sequences and dependency sub-trees.
Proceed-ings of PAKDD05, the 9th Pacific-Asia Conferenceon Advances in Knowledge Discovery and DataMining.Ng, Vincent, Dasgupta, Sajib and Arifin, S. M. Niaz.2006.
Examining the role of linguistic knowledgesources in the automatic identification and classifi-cation of reviews.
Proceedings of the COLING/ACLMain Conference Poster Sessions 611-618.Mullen, Tony and Collier, Nigel.
2004.
Sentimentanalysis using support vector machines with diversein-formation sources.
Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP) 412-418.Turney, Peter.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifi-cation of reviews.
Proceedings of the Associationfor Computational Linguistics (ACL) 417-424.44Whitelaw, Casey, Garg, Navendu and Argamon,Shlomo.
2005.
Using appraisal groups for sen-timent analysis.
Proceedings of the ACM SIGIRConference on Information and Knowledge Man-agement (CIKM) 625-631.Dillard, Logan.
2007.
I Can?t Recommend This PaperHighly Enough: Valence-Shifted Sentences in Senti-ment Classification.
Master Thesis.Riloff, Ellen and Wiebe, Janyce.
2003.
Learning ex-traction patterns for subjective expressions.
Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).Su, Qi, Pavlov, Dmitry, Chow, Jyh-Herng and Baker,Wendell C.. 2007.
Internet-Scale Collection ofHuman- Reviewed Data.
Proceedings of WWW-2007.Nakov, Preslav.
2008.
Paraphrasing Verbs forNoun Compound Interpretation.
Proceedings of theWorkshop on Multiword Expressions, LREC-2008.Kaisser, Michael and Lowe, John B.. 2008.
ARe-search Collection of QuestionAnswer SentencePairs.
Proceedings of LREC-2008.Snow, Rion, O?Connor, Brendan, Jurafsky, Daniel andNg, Andrew Y.
2008.
Cheap and Fast - But isit Good?
Evaluating Non-Expert Annotations forNatural Language Tasks.
Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP).Blitzer, John, Dredze, Mark, Biographies, FernandoPereira., Bollywood, Boom-boxes and Blenders.2007.
Domain Adaptation for Sentiment Classifica-tion.
Proceedings of the Association for Computa-tional Linguistics (ACL).Porter, M.F.. 1980.
An algorithm for suffix stripping.Program.Chang, Chih-Chung and Lin, Chih-Jen.
2001.LIBSVM: a library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Nigam, K., McCallum, A.K., Thrun, S., and Mitchell,T.. 2000.
Text Classification from labeled and unla-beled documents using em.
Machine Learning 39(2-3) 103-134.45
