Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 491?498, Vancouver, October 2005. c?2005 Association for Computational LinguisticsRobust Named Entity extraction from large spoken archivesBeno?
?t FavreThales, MMP LaboratoryColombes, Francebenoit.favre@fr.thalesgroup.comFre?de?ric Be?chetLIA, University of AvignonAvignon, Francefrederic.bechet@univ-avignon.frPascal Noce?raLIA, University of AvignonAvignon, Francepascal.nocera@univ-avignon.frAbstractTraditional approaches to Information Ex-traction (IE) from speech input simplyconsist in applying text based methods tothe output of an Automatic Speech Recog-nition (ASR) system.
If it gives satis-faction with low Word Error Rate (WER)transcripts, we believe that a tighter inte-gration of the IE and ASR modules canincrease the IE performance in more dif-ficult conditions.
More specifically thispaper focuses on the robust extraction ofNamed Entities from speech input wherea temporal mismatch between training andtest corpora occurs.
We describe a NamedEntity Recognition (NER) system, de-veloped within the French Rich Broad-cast News Transcription program ESTER,which is specifically optimized to pro-cess ASR transcripts and can be integratedinto the search process of the ASR mod-ules.
Finally we show how some meta-data information can be collected in or-der to adapt NER and ASR models to newconditions and how they can be used in atask of Named Entity indexation of spokenarchives.1 IntroductionNamed Entity Recognition (NER) is a crucial stepin many Information Extraction (IE) tasks.
Ithas been a specific task in several evaluation pro-grams such as the Message Understanding Confer-ences (MUC), the Conferences on Natural LanguageLearning (CoNLL), the DARPA HUB-5 program ormore recently the French ESTER Rich Transcriptionprogram on Broadcast News data.
Most of theseconferences have studied the impact of using tran-scripts generated by an Automatic Speech Recogni-tion (ASR) system rather than written texts.
It ap-pears from these studies that unlike other IE tasks,NER performance is greatly affected by the WordError Rate (WER) of the transcripts processed.
Totackle this problem, different ideas have been pro-posed: modeling explicitly the ASR errors (Palmerand Ostendorf, 2001) or using the ASR systemalternate hypotheses found in word lattices (Sar-aclar and Sproat, 2004).
However performance inNER decreases dramatically when processing highWER transcripts like the ones that are obtainedwith unmatched conditions between the ASR train-ing model and the data to process.
This paper in-vestigates this phenomenon in the framework of theNER task of the French Rich Transcription programof Broadcast News ESTER (Gravier et al, 2004).Several issues are addressed:?
how to jointly optimize the ASR and the NERmodels ??
what is the impact in term of ASR and NERperformance of a temporal mismatch betweenthe corpora used to train and test the modelsand how can it be recovered by means of meta-data information ??
Can metadata information be used for indexinglarge spoken archives ?491After a quick overview of related works in IEfrom speech input, we present the ESTER evaluationprogram ; then we introduce a NER system tightlyintegrated to the ASR process and show how itcan successfully index high WER spoken databasesthanks to metadata.2 Information extraction from largespoken archivesThe NIST Topic Detection and Tracking (Fiscus andDoddington, 2002) and TREC document retrievalevaluation programs has studied the impact of recog-nition errors in the overall performance of Informa-tion Extraction systems for tasks like story segmen-tation or topic detection and retrieval.
This impacthas been shown to be very limited compared to cleantext corpora.
The main explanation for this phe-nomenon is the redundancy effect: themes, topicsare very likely to be represented in texts by manyoccurrences of salient words characterizing them.Therefore, even if some of these words are missing,numerical Information Extraction methods can usethe remaining salient words and discard the noisegenerated by ASR errors.However, this phenomenon is not true for tasksrelated to the extraction of fine grained entities, likeNamed Entities.
Indeed, several studies have shownthat F-measure and WER are strongly correlated :0.7 points of F-measure lost for each additional 1%of WER according to (Miller et al, 2000) on the ex-periments of 1998 NIST Hub-4 evaluations (Przy-bocki et al, 1999).Despite the continuous improvement of ASRtechniques, high WER transcriptions are inevitablein difficult conditions like those found in large spo-ken archives like in the MALACH project (Ramab-hadran et al, 2003).
Moreover, Named Entities ex-traction performance is greatly affected by a mis-match between training and testing data.
This isdue mainly because proper names, which representmost of the Named Entity items, are a very dynamiccategory of words, strongly related to the period oftime representing the documents to process.
There-fore this mismatch is inevitable when dealing witharchives spreading over a long period of time andcontaining multiple domain information.One way of tackling this problem is to gathermetadata information related to the documents toprocess.
This information can be newspaper corporarelated to the same period of time, abstract describ-ing the document content, or simply lists of terms orentities likely to occur.
Although such collected datacan be used to update the ASR and NER models,the potential gain is rather small unless the metadatacorpus gathered fits perfectly the document to pro-cess and is of a reasonable size.
But another wayof exploiting this metadata information is to use itas set of index terms that are going to be explicitlylooked for in the processed documents.
We presentin section 7 an implementation of this idea that usesword lattices as input.3 The ESTER Named Entity evaluationprogramThis work has been done within the framework ofthe French Rich Transcription program of BroadcastNews ESTER.
ESTER is organized by l?AssociationFrancophone de la Communication Parle?e (AFCP),la De?le?gation Ge?ne?rale pour l?Armement (DGA)and the Evaluation language Resources DistributionAgency (ELDA).
The ESTER corpus is made of 100hours of Broadcast News data (from 6 French speak-ing radio channels), manually transcribed, and la-beled with a tagset of about 30 Named Entity cate-gories folded in 8 main types:?
persons (pers): human beings, fiction charac-ters, animals;?
locations (loc): geographical, traffic lines, elec-tronic and real addresses, dial numbers;?
organizations (org): political, business, nonprofit;?
geo-socio-political groups (gsp): clans, fami-lies, nations, administrative regions;?
amounts (amount): durations, money, lengths,temperature, age, weight and speed;?
time (time): relative and absolute time expres-sions, hours;?
products (prod): art, printings, awards and ve-hicles;492?
facilities (fac): buildings, monuments.This data is divided in 3 sets: a training set (84%),a development set(8%) and a test set (8%).
Thereis a 6 month gap difference between the trainingcorpus and the test corpus while the developmentcorpus matches the training data from a temporalpoint of view: the training corpus contains Broad-cast News spreading from 2002 to December 2003;the development corpus contains news from 2003;the test corpus has been recorded in October 2004.There are also 2 new radio channels in the test cor-pus which were not in the training data.For these reasons the development data is calledthe matched corpus as the recording conditionsmatch those of the training corpus and the test datais called the unmatched corpus.
As a consequence,we can study the effect of unmatched conditions onASR as well as IE performance and propose solu-tions for dealing with this problem.One of the main characteristics of the ESTER cor-pus is the size of the NE tagset and the high ambi-guity rate among the NE categories (eg.
administra-tive regions and geographical locations): 83% of thematched corpus entities occur in the training corpusand 40% of them are ambiguous whereas only 61%of the unmatched corpus entities occur in the train-ing corpus and 32% of them are ambiguous.The most commonly used measures for evaluat-ing NE extraction performance are Slot Error Rate(SER) and F-measure.
SER is very similar to WERbecause it takes into account fine grained errors likeinsertions, deletions and substitutions (entity typeand extent).
The scoring process is based on thesame alignment between reference and hypothesisdata than the one obtained for measuring WER andSER is known for being more accurate and penal-izing than F-measure.
Both measures weights canbe adjusted to favor recall or precision and thereforeadapted to a specific task.SER = 100 ?
?e?E ?e|e||Ref slots| F?
=(1+?2)RPR+?2PR = |Correct slots||Ref slots| P =|Correct slots||Hyp slots|with e ?
E being an error type (insertion, dele-tion, type, extent, type+extent, multiple) and ?e itsweight (resp.
1, 1, .5, .5, .8, 1.5) ; P is the precisionand R the recall; F1 is used in this paper.4 Extracting NE from written text vs. ASRoutputAs previously mentioned in section 2, WER andSER performance are strongly correlated.
Besidesthe intrinsic difficulties of ASR (robustness to noise,speaker variation, lack of coverage of the LanguageModels used, .
.
.
), there is a source of errors whichis particularly important in IE from speech input:the Out-Of-Vocabulary (OOV) word phenomenon.Indeed, ASR models are built on huge textual cor-pus and only contain the most frequent words tolimit computation and memory usage.
If this is theright approach to WER reduction, it is certainly notvaluable to information extraction where unlikelyevents are considered as important.
For instance,many document retrieval models use inverse docu-ment frequency (rareness) as a word weighting pa-rameter.
So, unlikely proper names are not in reachof the ASR transcription system and hence cannotbe spotted by a Named Entity extraction module.In addition to Out-of-Vocabulary words, two otherphenomenons have also a strong impact on NERperformance: the insertion of erroneous propernames that automatically trigger the insertion of anentity and spontaneous speech phenomenons.
Thesespeech dysfluencies (hesitations, filled pauses, falsestarts...) reduce the quality of the transcript becausethey are usually not covered by language models(built from textual data) or artificially introduced.One should remove these from the transcript to im-prove the quality of the labeling.In order to deal with ASR errors two approacheshave been proposed:?
modeling explicitly the ASR errors, thanks toa development corpus and a set of confidencemeasures, in order to detect the possible er-rors of the 1-best word string hypothesis (withthe type of errors) before extracting the NEs(Palmer and Ostendorf, 2001);?
exploiting a search space bigger than the 1-besthypothesis alone, either by taking into accountan n-best list (Zhai et al, 2004) or the wholeword lattice (Saraclar and Sproat, 2004).493The method proposed in this paper is close to thissecond approach where the whole word lattice out-put by the ASR system is used in order to increaseNER performance from noisy input.We will present also in the next section a newstrategy for adapting NER models to ASR tran-scripts, based on one of the main characteristics ofsuch transcripts: a closed vocabulary is used bythe ASR system.
To our knowledge this has neverbeen fully exploited by NER systems.
Indeed whilethe key point of NER systems on written text istheir generalization capabilities when processing un-known words, this feature is not relevant for ASRtranscripts as the system cannot generate words outof the lexicon (there are no unknown words).
There-fore we propose here to fully exploit this constraint(close vocabulary): since the OOV words cannot ap-pear in the ASR transcripts, the NER models canby over-trained on the words belonging to the ASRlexicon.
This is going to be developed in the nextsection.5 Robust Named Entity extractionWe have developed in this study two NER systems:one is based on the freely available NLP tool Ling-pipe1, adapted and trained on the French ESTERcorpus and dedicated to process text input.
This sys-tem is going to be called NERtext in the experimentsection.
The second NER system has been devel-oped for this study and is specifically built for beingtightly integrated with the ASR processes.
The twomain features of this system, called NERasr in thefollowing, are its ability to process word lattices andthe fact that the NER models are trained for a spe-cific ASR lexicon.
These two systems are going tobe presented in the next sections.5.1 Text-based NER system: NERtextAmong all the different methods that have been pro-posed for NER, one can find rule based models(Cunningham et al, 2002), Maximum Entropy mod-els (Brothwick et al, 1998), Conditionnal RandomFields or probabilistic HMM-based models (Bikel etal., 1999).Lingpipe implements an HMM-based model.
Itmaximizes the probability of a tag sequence Ti over1Lingpipe: http://alias-i.com/lingpipe/a word sequence Wi.
A context of two precedingwords and one preceding tag is used to approximatethis probability.
Generalization is done through asimple process: words occurring with low frequencyare replaced by feature based categories (capitalized,contains digits, .
.
.
).
In this approach, there must beone tag per word.
Words starting and ending entitiesare labeled with special tags.
Because some featuresare lacking in ASR transcripts (e.g.
capitalization,digits, sentence boundaries, .
.
. )
some word lists foreach kind of features are added as presented in (Ap-pelt and Martin, 1999).5.2 ASR-based NER system: NERasrErrors occurring in ASR output lead NER systemsto overgenerate NE detections.
This is due to botherroneous words insertions in the ASR transcriptsas well as some abusive generalization made by theNER systems.
If these generalization capabilitiesare very important for processing unknown wordsin written texts, they can be an handicap in a closed-vocabulary situation like the one observed when pro-cessing ASR output.
In order to reduce and con-trol the insertion rate of our NER system, we im-plemented a two level approach: the first level ismade of NE grammars coded as Finite State Ma-chine (FSM) transducers and the second level is astatistical HMM-based tagger.5.2.1 NE transducersTo each NE category is attached a set of regulargrammars, extracted from the ESTER training cor-pus and generalized thanks to the annotation guide-lines and web-gathered word lists.
Theses grammarsare represented by Finite State Machines (FSMs)(thanks to the AT&T GRM/FSM toolkit (Allauzen etal., 2003)).
These FSMs are transducers that acceptword sequences on the input symbols and output NElabels on the output symbols.
They are all groupedtogether in a single transducer, called Tgram, witha filler model that accepts any string of words.
Be-cause these FSMs are lexicalized with the words ofthe ASR lexicon, one can control the generalizationcapabilities of the grammars thanks to the occur-rence contexts of these words in the training corpus.During the NER process, the first step is to composethe FSM representing the NE transducer and the out-put of the ASR module (either a 1-best word string494or a word lattice, both encoded as an FSM called G).5.2.2 NE taggerThe result of the composition of the NE trans-ducer with the ASR output is an FSM (G ?
Tgram)containing all the possible parsing made by the NEgrammars.
In order to find the best analysis a sta-tistical model is used to decide between entity typesand entities boundaries.
This model is a 2nd ordern-gram model (trigram) represented by a weightedFSM (called Ttagger) with the same framework asthe grammars.
The most likely NE label sequenceis obtained by finding the best path in the FSM:G?Tgram ?Ttagger.
This corresponds to maximizethe following probability:PW =n?i=1P (Wi, Ti|Wi?1, Ti?1,Wi?2, Ti?2)This model is similar to the one implemented inLingpipe but it uses different smoothing methods.Similarly, first and last words of entities are repre-sented by special tags (this helps getting more accu-rate boundaries) and low frequency words (appear-ing less than a fixed number of times in the trainingcorpus) are generalized using their Part-Of-Speechtags.
The key points of this approach are that it has abetter control of the generalization capabilities thana feature based NER system, thanks to the NE gram-mars; it integrates the closed vocabulary constraintof the ASR systems; and it is not limited to the 1-best word hypothesis but can use the full ASR searchspace (through word lattices) in order to detect en-tities.
Processing word lattices allows us to output,at the end of the extraction process, an n-best list ofNE hypotheses.
To each hypothesis are attached twoscores:?
the likelihood score given by the ASR model tothe best word string supporting this NE hypoth-esis in the word lattice;?
the probability P (Wn, Tn, .
.
.
,W0, T0) givenby the NE tagger to the sequence of NE la-bels T0, .
.
.
, Tn and the sequence of wordsW0, .
.
.
,Wn.From this n-best list we can estimate the Oracleperformance of the NER system.
This measure isthe recall measure upper bound than can be obtainedby extracting all the possible entities from a wordlattice, thanks to the NE transducers, and simulatinga perfect strategy that always take the right decisionin choosing among all the possible entities.Decision strategies on such an n-best of NE hy-pothesis can also involve other levels of informa-tion on the document to process like the date orthe theme, for example.
In the evaluation presentedin the next section we compare this Oracle perfor-mance measure to the results of the simplest deci-sion strategy which consists in choosing the NE hy-pothesis with the highest likelihood.5.3 EvaluationThe evaluation presented in Tables 1 and 2 is per-formed using the Slot Error Rate and the F-measureon the matched and unmatched corpora presented insection 3.corpus matched unmatchedtagger SER F-m SER F-mNERtext 21 84 27 79NERasr 23 84 37 74WER 0 0Figure 1: F-measure and Slot Error Rate measureson the ESTER reference corpora (matched and un-matched) for both NER systemscorpus matched unmatchedtagger SER F-m SER F-m OracleNERtext 42 72 55 63 61.9NERasr 41 73 54 63 76.9WER 21.2 26.4Figure 2: F-measure, Slot Error Rate and Oracle re-call measures on the ASR output of the matched andunmatched corpora for both NER systemsFigure 1 presents SER and F-measure on the twotest sets (matched and unmatched) for the text ori-ented (NERtext) and the speech oriented (NERasr)NER systems, on clean text (manual transcripts).Figure 2 shows the results obtained on the ASR tran-scripts.As expected on manually transcribed data,NERtext obtains better results than NERasr (which495has poorer generalization capabilities).
On the ASRoutputs the results obtained by both systems arecomparable however NERasr has the advantage ofprocessing word lattices, leading to an interestingOracle performance.
We are studying now moreelaborate decision strategies in order to take fullyadvantage of this feature.The decrease in F-measure observed between thereference and the ASR transcripts is similar to theone obtained in other studies (Miller et al, 2000).One observation that can be made on these results isthe impact of the time mismatch between the train-ing and the test corpora.
A 6 month difference in theunmatched corpus leads to a very big drop in bothSER and F-measure.
This can be explained by thefact that NEs are very time-dependent.
We are goingnow to present some methods developed to tacklethis problem.6 Updating Language and NE models withmetadata informationThe only mismatch between the training and the un-matched corpus of our experiments is a 6 monthstemporal mismatch, therefore we collected a cor-pus of newsletters made on a daily basis by theFrench newspaper Le Monde corresponding to these6 months.
These newsletters contain an abstract ofthe news of each day.
We make the following twohypotheses:?
firstly these newsletters are related to the sametime period as the unmatched corpus, there-fore integrating them into the ASR models (lex-icon+Language Model) should help reducingthe OOV word effect;?
secondly because they represent an abstract ofthe news of each day, the Named Entities oc-curring in a particular newsletter should containall the major events of the corresponding dayand therefore constitutes a useful list of termsthat can be used for indexing a Broadcast Newsdocument related to the same period of time.6.1 NE distribution analysisThis newsletter corpus contains 1M words and afterbeing tagged by the NERtext system, 140k entitieswere extracted.
To check the relevance of this cor-pus for adapting the models to the unmatched testcorpus, we studied the distribution of the words andthe entities for each day, from January to December2004.
The unmatched test corpus is made of Broad-cast News ranging from October 10th to October16th 2004.
The following observations were made:72% of the NEs and 60% of the words contained inthem occur only one day in this corpus; the inter-section of the NEs occurring in both the newsletterof a particular day and the entities belonging to theunmatched test corpus shows a peak, illustrated byfigure 3, for the days of the test corpus; at this peak,25% of the NEs are used the same day in the twocorpora.0510152025-20 -15 -10 -5  0  5  10  15  20%entitiesmatchingtime window in daysFigure 3: Percentage of entities of the unmatchedcorpus occuring at least n days earlier or later in thenewsletter corpus (at a window of 0 days, entitiesappear on the same day in both corpus).The first observation matches those presented in(Whittaker, 2001) and validates our approach whichconsists in carefully adapting the ASR and NERmodels with data corresponding to the exact periodof time as the one of the documents to process: bytaking into account a larger period of time for theadaptation corpus, the necessity of restraining themodels to the most frequent entities would lead todiscard low frequency terms that can be crucial forcharacterizing the news of a given day.If the second observation clearly highlights thecorrelation between the NE distribution in both cor-pora, it also points out that only 25% of the enti-ties of the unmatched corpus occur in the newslet-ters corresponding to the same days.
Therefore thepotential improvement in the overall NER perfor-mance is clearly limited.
This will be confirmed inthe next section, however one can think that if these496entities are shared, for a given day, by both corpora,it is because they represent the key topics of this dayand therefore they can be considered as very rele-vant indexing terms for applications like documentretrieval.
This last point is developed in section 7.6.2 Model adaptationSeveral studies (Whittaker, 2001; Federico andBertoldi, 2001; Chen et al, 2004) propose adap-tation methods of a general language model to thepossibly small corpora extracted from these kinds ofmetadata information (an overview of these meth-ods can be found in (Bellegarda, 2004)).
Dependingon the adaptation method and the kind of metadatainformation used, some gains in performance havebeen reported.
But it appears that the choice of themetadata and the size of the adaptation corpus col-lected are critical in this adaptation process: if theadaptation corpus is not exactly related to the topicsof the document to process, no real gains are ob-tained (e.g.
(Chen et al, 2004) reports that the bestgains were obtained with a story-based adaptationmethod).From all these previous works, our system imple-ments the following adaptation process:?
the text corpus corresponding to the newslettersis added to the ASR language model by meansof a linear interpolation;?
proper names occuring twice or more in thenewsletter corpus are added to the ASR lexi-con;?
for the same days as those of the unmatchedcorpus, this cutoff is suppressed and all theproper names are added;?
the Named Entity wordlists and grammars arealso enriched with these proper names and en-tities extracted from the collected corpus.1K new proper names were added to the 65Kword ASR lexicon.
The general OOV reduction ob-tained was 0.14% leading to an absolute WER re-duction of 0.3%.
Similarly the SER decreased ofabout 0.3% thanks to this adaptation and the Ora-cle recall measure in the word lattices was improvedby an absolute 3%.
These improvements are notsignificant enough to justify the use of this kind ofmetadata information for improving the general per-formance of both ASR and NER processes.
How-ever, if we focus now on the entities occurring in thenewsletters corresponding to the exact days of theunmatched corpus, the improvement is much moresignificant, as presented in the next section.7 Named Entity IndexationAs previously mentioned, 25% of the unmatchedcorpus entities occur in the newsletters correspond-ing to the same day as those of the unmatched test.In order to measure the improvement obtained withour adaptation technique on these particular entities,we did the following experiment:?
a set of 352 entities was selected from thenewsletters related to same period of time as thetest, these entities represent the indexing termsthat are going to be looked for in the word lat-tices of the unmatched corpus;?
the NERasr system was then applied to theseword lattices with two conditions: the wordlattices and the NER models before adaptationand those obtained after adaptation with thenewsletter corpus;?
precision, recall, F-measure and Oracle errorrate were estimated for both conditions.Condition Prec.
Recall F-m Oracleno adaptation 87.0 75.7 80.9 83.6with adaptation 87.5 83.9 85.7 92Figure 4: Extraction results on the selected NEs onthe unmatched corpus with and without adaptationof the ASR and NER models on the newsletter cor-pusAs we can see in table 4, the adaptation processincreases very significantly the recall measure of theNE extraction.
This is particularly relevant in someIE tasks like the document retrieval task.8 ConclusionWe have presented in this paper a robust Named En-tity Recognition system dedicated to process ASRtranscripts.
The FSM-based approach allows us to497control the generalization capabilities of the systemwhile the statistical tagger provides good labelingdecisions.
The main feature of this system is itsability to extract n-best lists of NE hypothesis fromword lattices leaving the decision strategy choosingto either emphasize the recall or the precision of theextraction, according to the task targeted.
A compar-ison between this approach and a standard approachbased on the NLP tools Lingpipe validates our hy-potheses.
This integration of the ASR and the NERprocesses is particularly important in difficult con-ditions like those that can be found in large spokenarchives where the training corpus does not matchall the documents to process.
A study of the use ofmetadata information in order to adapt the ASR andNER models to a specific situation showed that if theoverall improvement is small, some salient informa-tion related to the metadata added can be better ex-tracted by means of this adaptation.ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In ACL?03, Sapporo, Japan.D.
Appelt and D. Martin.
1999.
Named entity extractionfrom speech: Approach and results using the TextProsystem.
In Proceedings Darpa Broadcast News Work-shop.Jerome R. Bellegarda.
2004.
Statistical language modeladaptation: review and perspectives.
Speech Commu-nication, 42 Issue 1:93?108.Daniel M. Bikel, Richard L. Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?sin a name.
volume 24, pages 211?231.Andrew Brothwick, John Sterling, Eugene Agichtein,and Ralph Grishman.
1998.
Exploiting diverse knowl-edge sources via maximum entropy in named entityrecognition.Langzhou Chen, Jean-Luc Gauvain, Lori Lamel, andGilles Adda.
2004.
Dynamic language modeling forbroadcast news.
In In International Conference onSpeech and Language Processing, pages 1281?1284.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A framework and graphi-cal development environment for robust NLP tools andapplications.
In Proceedings of the 40th AnniversaryMeeting of the Association for Computational Linguis-tics.M.
Federico and N. Bertoldi.
2001.
Broadcast news LMadaptation using contemporary texts.
In Proceedingsof European Conference on Speech Communicationand Technology (Eurospeech), pages 239?242, Aal-borg, Denmark.Jonathan G. Fiscus and George R. Doddington.
2002.Topic detection and tracking evaluation overview.Topic detection and tracking: event-based informationorganization, pages 17?31.G.
Gravier, J.F.
Bonastre, E. Geoffrois, S. Galliano,K.
McTait, and K. Choukri.
2004.
ESTER, une cam-pagne d?e?valuation des syste`mes d?indexation automa-tique d?e?missions radiophoniques en franc?ais.
In Proc.Journe?es d?Etude sur la Parole (JEP).David Miller, Sean Boisen, Richard Schwartz, RebeccaStone, and Ralph Weischedel.
2000.
Named entityextraction from noisy input: Speech and OCR.
In Pro-ceedings of ANLP-NAACL 2000, pages 316?324.D.
D. Palmer and M. Ostendorf.
2001.
Improving in-formation extraction by modeling errors in speech rec-ognizer output.
In Proceedings of the First Interna-tional Conference on Human Language TechnologyResearch.M.
A. Przybocki, J. G. Fiscus, J. S. Garofolo, and D. S.Pallett.
1999.
1998 Hub-4 Information ExtractionEvaluation.
In Proceedings Of The DARPA Broad-cast News Workshop, pages 13?18.
Morgan KaufmannPublishers.Bhuvana Ramabhadran, Jing Huang, and MichaelPicheny.
2003.
Towards automatic transcription oflarge spoken archives - english ASR for the MALACHproject.
In Proc.
IEEE International Conference onAcoustics, Speech and Signal Processing, ICASSP,pages 216?219.Murat Saraclar and Richard Sproat.
2004.
Lattice-basedsearch for spoken utterance retrieval.
In HLT-NAACL2004: Main Proceedings, pages 129?136, Boston,Massachusetts, USA.
Association for ComputationalLinguistics.E.
W. D. Whittaker.
2001.
Temporal adaptation of lan-guage models.
In Adaptation Methods for SpeechRecognition, ISCA Tutorial and Research Workshop(ITRW), August.
LM Adaptation for information re-trieval of spoken news/radio programs (i.e.
Speech-Bot).Lufeng Zhai, Pascale Fung, Richard Schwartz, MarineCarpuat, and Dekai Wu.
2004.
Using n-best listsfor named entity recognition from chinese speech.In HLT-NAACL 2004: Short Papers, pages 37?40,Boston, Massachusetts, USA.
Association for Compu-tational Linguistics.498
