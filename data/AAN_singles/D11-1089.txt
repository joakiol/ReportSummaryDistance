Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 959?969,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsSplitting Noun Compounds via Monolingual and Bilingual Paraphrasing:A Study on Japanese Katakana WordsNobuhiro KajiInstitute of Industrial ScienceUniversity of Tokyo, Tokyo, Japankaji@tkl.iis.u-tokyo.ac.jpMasaru KitsuregawaInstitute of Industrial ScienceUniversity of Tokyo, Tokyo, Japankitsure@tkl.iis.u-tokyo.ac.jpAbstractWord boundaries within noun compounds arenot marked by white spaces in a number oflanguages, unlike in English, and it is benefi-cial for various NLP applications to split suchnoun compounds.
In the case of Japanese,noun compounds made up of katakana words(i.e., transliterated foreign words) are par-ticularly difficult to split, because katakanawords are highly productive and are often out-of-vocabulary.
To overcome this difficulty,we propose using monolingual and bilingualparaphrases of katakana noun compounds foridentifying word boundaries.
Experimentsdemonstrated that splitting accuracy is sub-stantially improved by extracting such para-phrases from unlabeled textual data, the Webin our case, and then using that information forconstructing splitting models.1 Introduction1.1 Japanese katakana words and nouncompound splittingBorrowing is a major type of word formationin Japanese, and numerous foreign words (propernames or neologisms etc.)
are continuously beingimported from other languages (Tsujimura, 2006).Most borrowed words in modern Japanese aretransliterations1 from English and they are referredto as katakana words because transliterated foreignwords are primarily spelled by using katakana char-acters in the Japanese writing system.2 Compound-1Some researchers use the term transcription rather thantransliteration (Breen, 2009).
Our terminology is based on stud-ies on machine transliteration (Knight and Graehl, 1998).2The Japanese writing system has four character types: hi-ragana, katakana, kanji, and Latin alphabet.ing is another type of word formation that is com-mon in Japanese (Tsujimura, 2006).
In particu-lar, noun compounds are frequently produced bymerging two or more nouns together.
These twotypes of word formation yield a significant amountof katakana noun compounds, making Japanese ahighly productive language.In Japanese as well as some European and Asianlanguages (e.g., German, Dutch and Korean), con-stituent words of compounds are not separated bywhite spaces, unlike in English.
In those languages,it is beneficial for various NLP applications to splitsuch compounds.
For example, compound splittingenables SMT systems to translate a compound on aword-by-word basis, even if the compound itself isnot found in the translation table (Koehn and Knight,2003; Dyer, 2009).
In the context of IR, decom-pounding has an analogous effect to stemming, andit significantly improves retrieval results (Braschlerand Ripplinger, 2004).
In abbreviation recognition,the definition of an abbreviation is often in the formof a noun compound, and most abbreviation recogni-tion algorithms assume that the definition is properlysegmented; see e.g., (Schwartz and Hearst, 2003;Okazaki et al, 2008).This has led NLP researchers to explore meth-ods for splitting compounds, especially noun com-pounds, in various languages (Koehn and Knight,2003; Nakazawa et al, 2005; Alfonseca et al,2008a).
While many methods have been presented,they basically require expensive linguistic resourcesto achieve high enough accuracy.
For example, Al-fonseca et al (2008b) employed a word dictionary,which is obviously useful for this task.
Other stud-ies have suggested using bilingual resources such asparallel corpora (Brown, 2002; Koehn and Knight,9592003; Nakazawa et al, 2005).
The idea behind thosemethods is that compounds are basically split intoconstituent words when they are translated into En-glish, where the compounded words are separatedby white spaces, and hence splitting rules can belearned by discovering word alignments in bilingualresources.The largest obstacle that makes compound split-ting difficult is the existence of out-of-vocabularywords, which are not found in the abovemen-tioned linguistic resources.
In the Japanese case,it is known that katakana words constitute a largesource of out-of-vocabulary words (Brill et al, 2001;Nakazawa et al, 2005; Breen, 2009).
As we havediscussed, katakana words are very productive, andthus we can no longer expect existent linguistic re-sources to have sufficient coverage.
According to(Breen, 2009), as many as 20% of katakana wordsin news articles, which we think include less out-of-vocabulary words than Web and other noisy textualdata, are out-of-vocabulary.
Those katakana wordsoften form noun compounds, and pose a substantialdifficulty for Japanese text processing (Nakazawa etal., 2005).1.2 Paraphrases as implicit word boundariesTo alleviate the errors caused by out-of-vocabularywords, we explored the use of unlabeled textualdata for splitting katakana noun compounds.
Sincethe amount of unlabeled text available is generallymuch larger than word dictionaries and other expen-sive linguistic resources, it is crucial to establish amethodology for taking full advantage of such eas-ily available textual data.
While several approacheshave already been proposed, their accuracies are stillunsatisfactory (section 2.1).From a broad perspective, our approach can beseen as using paraphrases of noun compounds.
Aswe will see in section 4 and 5, katakana noun com-pounds can be paraphrased into various forms thatstrongly indicate word boundaries within the origi-nal noun compound.
This paper empirically demon-strates that splitting accuracy can be significantlyimproved by extracting such paraphrases from un-labeled text, the Web in our case, and then using thatinformation for constructing splitting models.Specifically, two types of paraphrases are inves-tigated in this paper.
Section 4 explores monolin-gual paraphrases that can be generated by insertingcertain linguistic markers between constituent wordsof katakana noun compounds.
Section 5, in turn,explores bilingual paraphrases (specifically, back-transliteration).
Since katakana words are basicallytransliterations from English, back-transliteratingkatakana noun compounds is also useful for split-ting.
To avoid terminological confusion, mono-lingual paraphrases are simply referred to as para-phrases and bilingual paraphrases are referred to asback-transliterations hereafter.We did experiments to empirically evaluate ourmethod.
The results demonstrated that both para-phrase and back-transliteration substantially im-proved the performance in terms of F1-score, andthe best performance was achieved when theywere combined.
We also confirmed that ourmethod outperforms the previously proposed split-ting methods by a wide margin.
All these resultsstrongly suggest the effectiveness of paraphrasingand back-transliteration for identifying word bound-aries within katakana noun compounds.2 Related Work2.1 Compound splittingA common approach to splitting compounds with-out expensive linguistic resources is an unsuper-vised method based on word or string frequen-cies estimated from unlabeled text (Koehn andKnight, 2003; Ando and Lee, 2003; Schiller, 2005;Nakazawa et al, 2005; Holz and Biemann, 2008).Amongst others, Nakazawa et al (2005) also in-vestigated ways of splitting katakana noun com-pounds.
Although the frequency-based method gen-erally achieves high recall, its precision is not satis-factory (Koehn and Knight, 2003; Nakazawa et al,2005).
Our experiments empirically compared ourmethod with the frequency-based methods, and theresults demonstrate the advantage of our method.Our approach can be seen as augmenting discrim-inative models of compound splitting with large ex-ternal linguistic resources, i.e., textual data on theWeb.
In a similar spirit, Alfonseca et al (2008b) pro-posed the use of query logs for compound splitting.3Their experimental results, however, did not clearly3Although they also proposed using anchor text, this slightlydegraded the performance.960demonstrate their method?s effectiveness.
Withoutthe query logs, the accuracy is reported to droponly slightly from 90.55% to 90.45%.
In contrast,our experimental results showed statistically signifi-cant improvements as a result of using additional re-sources.
Moreover, we used only textual data, whichis easily available, unlike query logs.Holz and Biemann (2008) proposed a methodfor splitting and paraphrasing German compounds.While their work is related to ours, their algorithmis a pipeline model and paraphrasing result is notemployed during splitting.2.2 Other research topicsOur study is closely related to word segmentation,which is an important research topic in Asian lan-guages including Japanese.
Although we can useexisting word segmentation systems for splittingkatakana noun compounds, it is difficult to reach thedesired accuracy, as we will empirically demonstratein section 6.
One reason for this is that katakananoun compounds often include out-of-vocabularywords, which are difficult for the existing segmen-tation systems to deal with.
See (Nakazawa et al,2005) for a discussion of this point.
From a wordsegmentation perspective, our task can be seen asa case study focusing on a certain linguistic phe-nomenon of particular difficulty.
More importantly,we are unaware of any attempts to use paraphrasesor transliterations for word segmentation in the sameway as we do.Recent studies have explored using paraphrasestatistics for parsing (Nakov and Hearst, 2005a;Nakov and Hearst, 2005b; Bansal and Klein, 2011).Although these studies successfully demonstratedthe usefulness of paraphrases for improving parsers,the connection between paraphrases and word seg-mentation (or noun compound splitting) was not atall discussed.Our method of using back-transliterations forsplitting katakana noun compounds (section 5) isclosely related to methods for mining transliterationfrom the Web text (Brill et al, 2001; Cao et al,2007; Oh and Isahara, 2008; Wu et al, 2009).
Whatmost differentiates these studies from our work isthat their primary goal is to build a machine translit-eration system or to build a bilingual dictionary it-self; none of them explored splitting compounds.Table 1: Basic features.ID Feature Description1 yi constituent word 1-gram2 yi?1yi constituent word 2-gram3 LEN(yi) #characters of yi (1, 2, 3, 4, or ?5)4 DICT(yi) true if yi is in the dictionary3 Supervised ApproachThe task we examine in this paper is splittinga katakana noun compound x into its constituentwords, y = (y1, y2 .
.
.
y|y|).
Note that the outputcan be a single word, i.e., |y| = 1.
Since it is pos-sible that the input is an out-of-vocabulary word, itis not at all trivial to identify a single word as such.A naive method would erroneously split an out-of-vocabulary word into multiple constituent words.We formalize our task as a structure predictionproblem that, given a katakana noun compound x,predicts the most probable splitting y?.y?
= argmaxy?Y(x)w ?
?
(y),where Y(x) represents the set of all splitting optionsof x, ?
(y) is a feature vector representation of y,and w is a weight vector to be estimated from la-beled data.Table 1 summarizes our basic feature set.
Fea-tures 1 and 2 are word 1-gram and 2-gram features,respectively.
Feature 3 represents the length of theconstituent word.
LEN(y) returns the number ofcharacters of y (1, 2, 3, 4, or ?5).
Feature 4 indi-cates whether the constituent word is registered inan external dictionary (see section 6.1).
DICT(y) re-turns true if the word y is in the dictionary.In addition to those basic features, we also employparaphrases and back-transliterations of katakananoun compounds as features.
The features are de-tailed in sections 4 and 5, respectively.We can optimize the weight vector w using an ar-bitrary training algorithm.
Here we adopt the aver-aged perceptron algorithm for the sake of time effi-ciency (Freund and Schapire, 1999).
The perceptronoffers efficient online training, and it performs com-paratively well with batch algorithms such as SVMs.Since we use only factored features (see table 1, sec-tion 4 and section 5), dynamic programming can beused to locate y?.961Table 2: Paraphrase rules and examples.
The first column represents the type of linguistic marker to be inserted, thesecond column shows the paraphrase rules, and the last column gives examples.Type Rule ExampleCentered dot X1X2 ?
X1 ?
X2 ????????
(anchovy pasta)??????????
(anchovy ?
pasta)Possessive marker X1X2 ?
X1 ?
X2 ????????
(anchovy pasta)?
??????
(with anchovy)???
(pasta)Verbal suffix X1X2 ?
X1 ??
X2X1X2 ?
X1??
X2??????????
(download file)?????????(downloaded)????
(file)Adjectival suffix X1X2 ?
X1 ?
X2X1X2 ?
X1 ?
X2X1X2 ?
X1 ??
X2????????
(surprise gift)???????(surprising)???
(gift)4 ParaphrasingIn this section, we argue that paraphrases ofkatakana noun compounds provides useful informa-tion on word boundaries.
Consequently, we proposeusing paraphrase frequencies as features for trainingthe discriminative model.4.1 Paraphrasing noun compoundsA katakana noun compound can be paraphrased intovarious forms, some of which provide informationon the word boundaries within the original com-pound.
(1) a.
????????
(anchovy pasta)b.
?????????
(anchovy ?
pasta)c.
??????
(with anchovy)???
(pasta)These examples are paraphrases of each other.
(1a)is in the form of a noun compound, within whichthe word boundary is ambiguous.
In (1b), on theother hand, a centered dot ?
is inserted betweenthe constituent words.
In the Japanese writing sys-tem, the centered dot is sometimes, but not always,used to separate long katakana compounds for thesake of readability.
(1c) is the noun phrase gener-ated from (1a) by inserting the possessive marker??
?, which can be translated as with in this context,between the constituent words.
If we observe para-phrases of (1a) such as (1b) and (1c), we can guessthat a word boundary exists between ??????(anchovy)?
and ????
(pasta)?.4.2 Paraphrase rulesThe above discussion led us to use paraphrasefrequencies estimated from Web text for splittingkatakana noun compounds.
For this purpose, weestablished the seven paraphrase rules illustrated inTable 2.
The rules are in the form of X1X2 ?X1MX2, where X1 and X2 represent nouns, andM is a certain linguistic marker (e.g., the posses-sive marker ???).
The left-hand term correspondsto a compound to be paraphrased and the right-handterm represents its paraphrase.
For instance, X1 =??????
(anchovy)?, X2 = ????
(pasta)?, andM = ???.
The paraphrase rules we use are based onthe rules proposed by Kageura et al (2004) for ex-panding complex terms, primarily noun compounds,into their variants.4.3 Web-based frequency as featuresWe introduce a new feature using the paraphraserules and Web text.
As preprocessing, we use reg-ular expressions to count the frequencies of all po-tential paraphrases of katakana noun compounds onthe Web in advance.(katakana)+?
(katakana)+(katakana)+?
(katakana)+(katakana)+??
(katakana)+.
.
.where (katakana) corresponds to one katakana char-acter.
Given a candidate segmentation y at test time,we generate paraphrases of the noun compound bysetting X1 = yi?1 and X2 = yi, and applying theparaphrase rules.
We then use log(F + 1), where Fis the sum of the Web-based frequencies of the gen-962erated paraphrases, as the feature of the boundarybetween yi?1 and yi.As the feature value, we use the logarithmic fre-quency, rather than the raw frequency, for scaling.Since the other features have binary value, we found,in initial experiments, that the importance of thisfeature is overemphasized if we use the raw fre-quency.
Note that we use log(F + 1) rather thanlog F so as to avoid the feature value being zerowhen F = 1.5 Back-transliterationMost katakana words are transliterations from En-glish, where words are separated by white spaces.It is, therefore, reasonable to think that back-transliterating katakana noun compounds into En-glish would provide information on word bound-aries, in a similar way to paraphrasing.This section presents a method for extractingback-transliterations of katakana words from mono-lingual Web text, and establishing word alignmentsbetween those katakana and English words (Table3).
In what follows, the pair of katakana wordsand its English back-transliteration is referred to as atransliteration pair.
If the transliteration pair is an-notated with word alignment information as in Table3, it is referred to as a word-aligned transliterationpair.Using word-aligned transliteration pairs extractedfrom the Web text, we derive a binary feature in-dicating whether katakana word yi corresponds toa single English word.
Additionally, we derive an-other feature indicating whether a katakana word 2-gram yi?1yi corresponds to an English word 2-gram.5.1 Parenthetical expressionsIn Japanese and other Asian languages, transliter-ated words are sometimes followed by their Englishback-transliterations inside parentheses:(2) a.
??????
?????(junk)???
(food)(junk food)?...b.
???????
?
???(spam)(spam)????
?...where the underline indicates the Japanese textthat is followed by English back-transliteration.We extract word-aligned transliteration pairs fromTable 3: Word-aligned transliteration pairs.
The numberindicates the word alignment.Japanese English????1???
2 junk1 food2??
?3 spam3such parenthetical expressions by establishing thecorrespondences between pre-parenthesis and in-parenthesis words.To accomplish this, we have to resolve three prob-lems: (a) English words inside parenthesis do notalways provide a back-transliteration of the pre-parenthesis text, (b) the left boundary of the pre-parenthesis text, denoted as ???
in the example, hasto be identified, and (c) pre-parenthesis text, whichis a katakana noun compound in our case, has to besegmented into words.Although several studies have explored miningtransliterations from such parenthetical expressions(Cao et al, 2007; Wu et al, 2009), the last problemhas not been given much attention.
In the past stud-ies, the pre-parenthesis text is assumed to be cor-rectly segmented by, typically, using existent wordsegmentation systems.
This is, however, not appro-priate for our purpose, because pre-parenthesis textis a katakana noun compound, which is hard for ex-isting systems to handle, and hence the alignmentquality is inevitably affected by segmentation errors.To handle these three problems, we use the pho-netic properties of the transliterations.
For the pur-pose of explanation, we shall first focus on problem(c).
Since transliterated katakana words preserve thepronunciation of the original English words to someextent (Knight and Graehl, 1998), we can discoverthe correspondences between substrings of the twolanguages based on phonetic similarity:(3) a.
[???]1[?]2[??]3[?
]4b.
[jun]1[k]2 [foo]3[d]4Note that these are the pre-parenthesis and in-parenthesis text in (2a).
The substrings surroundedby square brackets with the same number corre-spond to each other.
Given such a correspondence,we can segment the pre-parenthesis text (3a) accord-ing to its English counterpart (3b), in which words963Table 4: Example of the substring alignment A betweenf =????????
?
and e =?junkfood?
(|A| = 4).
(fi, ei) log p(fi, ei)(??
?, jun) ?10.767(?, k) ?5.319(?
?, foo) ?11.755(?
, d) ?5.178are separated by white space.
We can recognize thatthe katakana string ?????
?, which is the con-catenation of the first two substrings in (3a), formsa single word because it corresponds to the Englishword junk, and so on.
Consequently, (3a) can be seg-mented into two words, ?????
(junk)?
and ????
(food)?.
The word alignment is trivially estab-lished.For problems (a) and (b), we can also use thephonetic similarity between pre-parenthesis and in-parenthesis text.
If the parenthetical expression doesnot provide the transliteration, or if the left boundaryis erroneously identified, we can expect the phoneticsimilarity to become small.
Such situations thus canbe identified.The remainder of this section details this ap-proach.
Section 5.2 presents a probabilistic modelfor discovering substring alignment such as (3).
Sec-tion 5.3 shows how to extract word-aligned translit-eration pairs by using the probabilistic model.5.2 Phonetic similarity modelTo establish the substring alignment betweenkatakana and Latin alphabet strings, we use theprobabilistic model proposed by (Jiampojamarn etal., 2007).
Let f and e be katakana and alphabetstrings, and A be the substring alignment betweenthem.
More precisely, A is a set of correspondingsubstring pairs (fi, ei) such that f = f1f2 .
.
.
f|A|and e = e1e2 .
.
.
e|A|.
The probability of such align-ment is defined aslog p(f, e, A) =?
(fi,ei)?Alog p(fi, ei).Since A is usually unobservable, it is treated as ahidden variable.
Table 4 illustrates an example ofthe substring alignment between f =????????
?
and e =?junkfood?, and the likelihood of eachsubstring pair estimated in our experiment.The model parameters are estimated from a set oftransliteration pairs (f, e) using the EM algorithm.In the E-step, we estimate p(A|f, e) based on thecurrent parameters.
In the parameter estimation, werestrict both fi and ei to be at most three characterslong.
Doing this not only makes the E-step compu-tationally efficient but avoids over-fitting by forbid-ding too-long substrings to be aligned.
In the M-step, the parameter is re-estimated using the resultof the E-step.
We can accomplish this by using anextension of the forward-backward algorithm.
See(Jiampojamarn et al, 2007) for details.Given a new transliteration pair (f, e), we can de-termine the substring alignment asA?
= argmaxAlog p(f, e, A).In finding the substring alignment, a white space onthe English side is used as a constraint, so that theEnglish substring ei does not span a white space.5.3 Extracting word-aligned transliterationpairsThe word-aligned transliteration pairs are extractedusing the phonetic similarity model, as follows.First, candidate transliteration pairs (f, e) are ex-tracted from the parenthetical expressions.
This isdone by extracting English words inside parenthe-ses and pre-parenthesis text written in katakana.
En-glish words are normalized by lower-casing capitalletters.Second, we determine the left boundary by usingthe confidence score: 1N log p(f, e, A?
), where N isthe number of English words.
The term 1N preventsthe score from being unreasonably small when thereare many words.
We truncate f by removing theleftmost characters one by one, until the confidencescore exceeds a predefined threshold ?.
If f becomesempty, the pair is regarded as a non-transliterationand discarded.Finally, for the remaining pairs, the Japanese sideis segmented and the word alignment is establishedaccording to A?.
This results in a list of word-aligned transliteration pairs (Table 3).6 Experiments and DiscussionWe conducted experiments to investigate how theuse of the paraphrasing and the back-transliteration964improves the performance of the discriminativemodel.6.1 Experimental settingTo train the phonetic similarity model, we useda set of transliteration pairs extracted from theWikipedia.4 Since person names are almost alwaystransliterated when they are imported from Englishinto Japanese, we made use of the Wikipedia arti-cles that belong to the Living people category.
Fromthe titles of those articles, we automatically ex-tracted person names written in katakana, togetherwith their English counterparts obtainable via themultilingual links provided by the Wikipedia.
Thisyielded 17,509 transliteration pairs for training.
Inperforming the EM algorithm, we tried ten differ-ent initial parameters and selected the model thatachieved the highest likelihood.The data for training and testing the percep-tron was built using a Japanese-English dictionaryEDICT.5 We randomly extracted 5286 entries writ-ten in katakana from EDICT and manually anno-tated word boundaries by establishing word corre-spondences to their English transliterations.
SinceEnglish transliterations are already provided byEDICT, the annotation can be trivially done by na-tive speakers of Japanese.
Using this data set, weperformed 2-fold cross-validation for testing the per-ceptron.
The number of iterations was set to 20 in allthe experiments.To compute the dictionary-based feature DICT(y)in our basic feature set, we used NAIST-jdic.6 It isthe largest dictionary used for Japanese word seg-mentation, and it includes 19,885 katakana words.As Web corpora, we used 1.7 G sentences ofblog articles.
From the corpora, we extracted14,966,205 (potential) paraphrases of katakana nouncompounds together with their frequencies.
Wealso extracted 151,195 word-aligned transliterationpairs.
In doing this, we ranged the threshold ?
in{?10,?20, ?
?
?
?
150} and chose the value that per-formed the best (?
= ?80).The results were evaluated using precision, recall,F1-score, and accuracy.
Precision is the number ofcorrectly identified words divided by the number of4http://ja.wikipedia.org/5http://www.csse.monash.edu.au/?jwb/edict doc.html6http://sourceforge.jp/projects/naist-jdicall identified words, recall is the number of correctlyidentified words divided by the number of all ora-cle words, the F1-score is their harmonic mean, andaccuracy is the number of correctly split katakananoun compounds divided by the number of all thekatakana noun compounds.6.2 Baseline systemsWe compared our system with three frequency-based baseline system, two supervised baselines,and two state-of-the-art word segmentation base-lines.
The first frequency-based baseline, UNI-GRAM, performs compound splitting based on aword 1-gram language model (Schiller, 2005; Al-fonseca et al, 2008b):y?
= argmaxy?Y(x)?ip(yi),where p(yi) represents the probability of yi.
Thesecond frequency-based baseline, GMF, outputs thesplitting option with the highest geometric mean fre-quency of the constituent words (Koehn and Knight,2003):y?
= argmaxy?Y(x)GMF(y) = argmaxy?Y(x){?if(yi)}1/|y|,where f(yi) represents the frequency of yi.
Thethird frequency-based baseline, GMF2, is a mod-ification of GMF proposed by Nakazawa et al(2005).
It is based on the following score insteadof GMF(y):GMF2(y) =????
?GMF(y) (|y| = 1)GMF(y)CNl +?
(|y| ?
2),where C , N , and ?
are hyperparameters and l is theaverage length of the constituent words.
Following(Nakazawa et al, 2005), the hyperparameters wereset as C = 2500, N = 4, and ?
= 0.7.
We estimatedp(y) and f(y) from the Web corpora.The first supervised baseline, AP, is the aver-aged perceptron model trained using only the ba-sic feature set.
The second supervised baseline,AP+GMF2 is a combination of AP and GMF2,which performed the best amongst the frequency-based baselines.
Following (Alfonseca et al,965Table 5: Comparison with baseline systems.Type System P R F1 AccFrequency UNIGRAM 64.2 49.7 56.0 63.0GMF 42.9 62.0 50.7 47.5GMF2 67.4 76.0 71.5 72.5Supervised AP 81.9 82.5 82.2 83.4AP+GMF2 83.0 83.9 83.4 84.2PROPOSED 86.4 87.4 87.1 87.6Word seg.
JUMAN 71.4 60.1 65.3 69.8MECAB 72.4 73.7 67.8 71.62008b), GMF2 is integrated into AP as two bi-nary features indicating whether GMF2(y) is largerthan any other candidates, and whether GMF2(y) islarger than the non-split candidate.
Although Alfon-seca et al (2008b) also proposed using (the log of)the geometric mean frequency as a feature, doing sodegraded performance in our experiment.Regarding the two state-of-the-art word segmen-tation systems, one is JUMAN,7 a rule-based wordsegmentation system (Kurohashi and Nagao, 1994),and the other is MECAB,8 a supervised word seg-mentation system based on CRFs (Kudo et al,2004).
These two baselines were chosen in order toshow how well existing word segmentation systemsperform this task.
Although the literature states thatit is hard for existing systems to deal with katakananoun compounds (Nakazawa et al, 2005), no empir-ical data on this issue has been presented until now.6.3 Splitting resultTable 5 compares the performance of our system(PROPOSED) with the baseline systems.
First of all,we can see that PROPOSED clearly improved the per-formance of AP, demonstrating the effectiveness ofusing paraphrases and back-transliterations.Our system also outperformed all the frequency-based baselines (UNIGRAM, GMF, and GMF2).
Thisis not surprising, since the simple supervised base-line, AP, already outperformed the unsupervisedfrequency-based ones.
Indeed similar experimentalresults were also reported by Alfonseca (2008a).
Aninteresting observation here is the comparison be-tween PROPOSED and AP+GMF2.
It reveals thatour approach improved the performance of AP morethan the frequency-based method did.
These results7http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html8http://sourceforge.net/projects/mecabindicate that paraphrasing and back-transliterationare more informative clues than the simple fre-quency of constituent words.
We would like tonote that the higher accuracy of PROPOSED in com-parison with the baselines is statistically significant(p < 0.01, McNemar?s test).The performance of the two word segmenta-tion baselines (JUMAN and MECAB) is significantlyworse in our task than in the standard word segmen-tation task, where nearly 99% precision and recallare reported (Kudo et al, 2004).
This demonstratesthat splitting a katakana noun compound is not atall a trivial task to resolve, even for the state-of-the-art word segmentation systems.
On the other hand,PROPOSED outperformed both JUMAN and MECABin this task, meaning that our technique can suc-cessfully complement the weaknesses of the existingword segmentation systems.By analyzing the errors, we interestingly foundthat some of the erroneous splitting results are stillacceptable to humans.
For example, while ???????
(upload)?
was annotated as a single word inthe test data, our system split it into ????
(up)?and ????
(load)?.
Although the latter splittingmay be useful in some applications, it is judged aswrong in our evaluation framework.
This impliesthe importance of evaluating the splitting results insome extrinsic tasks.
We leave it to a future work.6.4 Investigation on out-of-vocabulary wordsIn our test data, 2681 out of the 5286 katakana nouncompounds contained at least one out-of-vocabularyword that are not registered in NAIST-jdic.
Table 6illustrates the results of the supervised systems forthose 2681 and the remaining 2605 katakana nouncompounds (referred to as w/ OOV and w/o OOVdata, respectively).
While the accuracy exceeds 90%for w/o OOV data, it is substantially degraded for w/OOV data.
This is consistent with our claim that out-of-vocabulary words are a major source of errors insplitting noun compounds.The three supervised systems performed almostequally for w/o OOV data.
This is because AP triv-ially performs very well on this subset, and it is dif-ficult to get any further improvement.
On the otherhand, we can see that there are substantial perfor-mance gaps between the systems for w/ OOV data.This result reflects the effect of the additional fea-966Table 6: Splitting results of the supervised systems for w/ OOV and w/o OOV data.w/ OOV data w/o OOV dataSystem P R F1 Acc P R F1 AccAP 66.9 69.9 68.3 72.8 95.4 93.2 94.3 94.2AP+GMF2 69.7 73.7 71.6 75.2 95.2 92.4 93.7 93.6PROPOSED 76.8 79.3 78.0 80.9 95.3 94.2 94.8 94.5tures more directly than is shown in table 5.6.5 Effect of the two new featuresTo see the effect of the new features in more detail,we looked at the performances of our system usingdifferent feature sets (Table 7).
The first columnrepresents the feature set we used: BASIC, PARA,TRANS, and ALL represent the basic features, theparaphrase feature, the back-transliteration feature,and all the features.
The results demonstrate thatadding either of the new features improved the per-formance, and the best result was when they wereused together.
In all cases, the improvement overBASIC was statistically significant (p < 0.01, Mc-Nemar?s test).Next, we investigated the coverage of the features.Our test data comprised 7709 constituent words,4937 (64.0%) of which were covered by NAIST-jdic.
The coverage was significantly improved whenusing the back-transliteration feature.
We observedthat 6216 words (80.6%) are in NAIST-jdic or word-aligned transliteration pairs extracted from the Webtext.
This shows that the back-transliteration fea-ture successfully reduced the number of out-of-vocabulary words.
On the other hand, we observedthat the paraphrase and back-transliteration featureswere activated for 79.5% (1926/2423) and 15.5%(376/2423) of the word boundaries in our test data.Overall, we see that the coverage of these fea-tures is reasonably good, although there is still roomfor further improvement.
It would be beneficial touse larger Web corpora or more paraphrase rules,for example, by having a system that automaticallylearns rules from the corpora (Barzilay and McKe-own, 2001; Bannard and Callison-Burch, 2005).6.6 Sensitivity on the threshold ?Finally we investigated the influence of the thresh-old ?
(Figure 1 and 2).
Figure 1 illustrates the systemperformance in terms of F1-score for different valuesTable 7: Effectiveness of paraphrase (PARA) and back-transliteration feature (TRANS).Feature set P R F1 AccBASIC 81.9 82.5 82.2 83.4BASIC+PARA 85.1 85.3 85.2 85.9BASIC+TRANS 85.1 86.3 85.7 86.5ALL 86.4 87.4 87.1 87.6of ?.
While the F1-score drops when the value of ?is too large (e.g., ?20), the F1-score is otherwise al-most constant.
This demonstrates it is generally easyto set ?
near the optimal value.
More importantly,the F1-score is consistently higher than BASIC irre-spective of the value of ?.
Figure 2 represents thenumber of distinct word-aligned transliteration pairsthat were extracted from the Web corpora.
We seethat most of the extracted transliteration pairs havehigh confidence score.7 ConclusionIn this paper, we explored the idea of using monolin-gual and bilingual paraphrases for splitting katakananoun compounds in Japanese.
The experimentsdemonstrated that our method significantly im-proves the splitting accuracy by a large marginin comparison with the previously proposed meth-ods.
This means that paraphrasing provides a sim-ple and effective way of using unlabeled textualdata for identifying implicit word boundaries withinkatakana noun compounds.Although our investigation was restricted tokatakana noun compounds, one might expect that asimilar approach would be useful for splitting othertypes of noun compounds (e.g., German noun com-pounds), or for identifying general word boundaries,not limited to those between nouns, in Asian lan-guages.
We think these are research directions worthexploring in the future.96787888586core8384F 1-sc8182ThresholdFigure 1: Influence of the threshold ?
(x-axis) on the F1-score (y-axis).
The triangles and squares represent sys-tems using the ALL and BASIC feature sets, respectively.200000pairs100000150000erationp50000f translite0#ofThresholdFigure 2: The number of distinct word-aligned transliter-ations pairs that were extracted from the Web corpora fordifferent values of ?.AcknowledgementThis work was supported by the Multimedia WebAnalysis Framework towards Development of SocialAnalysis Software program of the Ministry of Ed-ucation, Culture, Sports, Science and Technology,Japan.ReferencesEnrique Alfonseca, Slaven Bilac, and Stefan Pharies.2008a.
Decompoundig query keywords from com-pounding languages.
In Proceedings of ACL, ShortPapers, pages 253?256.Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.2008b.
German decompounding in a difficult corpus.In Proceedings of CICLing, pages 128?139.Rie Kubota Ando and Lillian Lee.
2003.
Mostly-unsupervised statistical segmentation of JapaneseKanji sequences.
Natural Language Engineering,9(2):127?149.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL, pages 597?604.Mohit Bansal and Dan Klein.
2011.
Web-scale featuresfor full-scale parsing.
In Proceedings of ACL, pages693?702.Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceedingsof ACL, pages 50?57.Martin Braschler and Ba?rbel Ripplinger.
2004.
How ef-fective is stemming and decompounding for Germantext retrieval?
Information Retrieval, 7:291?316.Jamese Breen.
2009.
Identification of neologisms inJapanese by corpus analysis.
In Proceedings of eLexi-cography in the 21st centry conference, pages 13?22.Eric Brill, Gray Kacmarcik, and Chris Brockett.
2001.Automatically harvesting katakana-English term pairsfrom search engine query logs.
In Proceedings of NL-PRS, pages 393?399.Ralf D. Brown.
2002.
Corpus-driven splitting of com-pound words.
In Proceedings of TMI.Guihong Cao, Jianfeng Gao, and Jian-Yun Nie.
2007.
Asystem to mine large-scale bilingual dictionaries frommonolingual Web pages.
In Proceedings of MT Sum-mit, pages 57?64.Chris Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for MT.
In Proceedings ofNAACL, pages 406?414.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine Learning, 37(3):277?296.Florian Holz and Chris Biemann.
2008.
Unsupervisedand knowledge-free learning of compound splits andperiphrases.
In CICLing, pages 117?127.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignment andhidden Markov models to letter-to-phoneme conver-sion.
In HLT-NAACL, pages 372?379.Kyo Kageura, Fuyuki Yoshikane, and Takayuki Nozawa.2004.
Parallel bilingual paraphrase rules for nouncompounds: Concepts and rules for exploring Weblanguage resources.
In Proceedings of Workshop onAsian Language Resources, pages 54?61.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4):599?612.Philip Koehn and Kevin Knight.
2003.
Empirical meth-ods for compound splitting.
In Proceedings of EACL,pages 187?193.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields to Japanesemorphological analysis.
In Proceedings of EMNLP,pages 230?237.Sadao Kurohashi and Makoto Nagao.
1994.
Im-provements of Japanese morphological analyzer JU-MAN.
In Proceedings of the International Workshopon Sharable Natural Language Resources, pages 22?38.968Toshiaki Nakazawa, Daisuke Kawahara, and SadaoKurohashi.
2005.
Automatic acquisition of basicKatakana lexicon from a given corpus.
In Proceedingsof IJCNLP, pages 682?693.Preslav Nakov and Marti Hearst.
2005a.
Search en-gine statistics beyond the n-gram: Application to nouncompound bracketing.
In Proceedings of CoNLL,pages 17?24.Preslav Nakov and Marti Hearst.
2005b.
Using the Webas an implicit training set: Application to structuralambiguity resolution.
In Proceedings of HLT/EMNLP,pages 835?342.Jong-Hoon Oh and Hitoshi Isahara.
2008.
Hypothesisselection in machine transliteration: A Web miningapproach.
In Proceedings of IJCNLP, pages 233?240.Naoaki Okazaki, Sophia Ananiadou, and Jin?ichi Tsujii.2008.
A discriminative alignment model for abbrevi-ation recognition.
In Proceedings of COLING, pages657?664.Anne Schiller.
2005.
German compound analysis withwfsc.
In Proceedings of Finite State Methods and Nat-ural Language Processing, pages 239?246.Ariel S. Schwartz and Marti A. Hearst.
2003.
A sim-ple algorithm for identifying abbreviation definitionsin biomedical text.
In Proceedings of PSB, pages 451?462.Natsuko Tsujimura.
2006.
An Introduction to JapaneseLinguistics.
Wiley-Blackwell.Xianchao Wu, Naoaki Okazaki, and Jun?ichi Tsujii.2009.
Semi-supervised lexicon mining from paren-thetical expressions in monolingual Web pages.
InProceedings of NAACL, pages 424?432.969
