A Rule-Based Approach to Prepositional Phrase AttachmentDisambiguationEric Brill Philip Resnik*Spoken Language Systems Group Sun Microsystems l,aboratories, In(:.Laboratory for Colnputer Science, M.I.T.
Chelmsford, MA 01824 ~1195 U.S.A(\]ambridge, Ma.
02139 U.S.A philip.resnil;(~east.sun.combrill~goldilocks.lcs.mit.eduAbstractI:n this paper, we describe a new corpus-based ap-proach to prepositional phrase attachment disam-biguation, and present results colnparing peffo>mange of this algorithm with other corpus-basedapproaches to this problem.IntroductionPrel)ositioual phrase attachment disambiguationis a difficult problem.
Take, for example, the sen-rouge:( l )  Buy a ear \[p,o with a steering wheel\].We would guess that the correct interpretation isthat one should buy cars that come with steer-ing wheels, and not that one should use a steeringwheel as barter for purchasing a car.
\]n this case,we are helped by our world knowledge about auto-mobiles and automobile parts, and about typicalmethods of barter, which we can draw upon to cor-rectly disambignate he sentence.
Beyond possiblyneeding such rich semantic or conceptual int'ornla-tion, A l tmann and Steedman (AS88) show thatthere a,re certain cases where a discourse modelis needed to correctly disambiguate prepositionalphrase atta.chment.However, while there are certainly cases of an>biguity that seem to need some deep knowledge,either linguistic or conceptual, one might ask whagsort of performance could 1oe achieved by a sys-tem thai uses somewhat superficial knowledge au-*Parts of this work done a.t the Computer and hPlbrmation Science Department, University of Penn-sylvania were supported by by DARPA and AFOSRjointly under grant No.
AFOSR-90-0066, and by AROgrant No.
DAAL 03-89-C0031 PR\[ (first author) andby an IBM gradmtte fellowship (second author).
Thiswork was also supported at MIT by ARPA under Con-tract N000t4-89-J-la32= monitored through the Officeof Naval resear<:h (lirst a.uthor).tomatically ~xtracted from a large corpus.
Recentwork has shown thai; this approach olds promise(H\]~,91, HR93).hi this paper we describe a new rule-based ap-proach to prepositional phrase attachment, disam-biguation.
A set of silnple rules is learned au-tomatically to try to prediet proper attachmentbased on any of a number of possible contextualgiles.Baselinel l indle and Rooth (IIR91, 1\[17{93) describecorpus-based approach to disambiguating betweenprepositional phrase attachlnent to the main verband to the object nonn phrase (such as in the ex-ample sentence above).
They first point out thatsimple attachment s rategies snch as right associa-tion (Kim73) and miuimal a.tbtchment (Fra78) donot work well i,l practice' (see (WFB90)).
Theythen suggest using lexical preference, estimatedfrom a large corpus of text, as a method of re-solving attachment ambiguity, a technique the}'call "lexical association."
From a large corpus ofpursed text, they first find all nonn phrase heads,and then record the verb (if' any) that precedes thehead, and the preposition (if any) that follows it,as well as some other syntactic inforlnation aboutthe sentence.
An algorithm is then specified 1,o tryto extract attachment information h'om this tableof co-occurrences.
I!
'or instance, a table entry iscousidered a definite instance of the prepositionalphrase attaching to the noun if:'\['he noun phrase occm:s in a context whereno verb could license the prepositional phrase,specifically if the noun phrase is in a subjeetor other pre-verbal position.They specify seven different procedures for decid-ing whether a table entry is au instance of noattachment, sure noun attach, sm:e verb attach,or all ambiguous attach.
Using these procedures,they are able to extract frequency information,1198counting t, he numl)e,r of times a ptu:ticular verbor ncmn a.ppe~u:s with a pal:tieuh~r l~reposition.These frequen(;ies erve a.s training d~t;a forthe statistical model they use to predict correcti~ttachmenL To dismnbigu;~te s ntence (l), theywould compute the likelihood of the prepositionwith giwm the verb buy, {rod eolltrast that withthe likelihood of that preposition given I:he liOttllwhed.
()he, problem wit;h this ,~pproa~ch is tll~tt itis limited in what rel~tionships are examined tomake mi ~d;tachment decision.
Simply extendingt\[indle and l{,ooth's model to allow R)r relalion-ships such as tlml~ I)e.tweell the verb and the' ob-ject o\[' the preposition would i:esult ill too largea.
parameter spa.ce, given ~my realistic quantity oftraiuing data.
Another prol)lem of the method,shared by ma.ny statistical approaches, is thatthe.
model ~(:quired (Inring training is rel)reser~tedin a huge, t~d)le of probabilities, pl:ecludiug anystra.ightf'orward analysis of its workings.'
l~-ansformat ion-Based Er ror -Dr ivenLearn ingTra, nS\]bl'm~d;ion-lmsed errol:-dHven learlting is ~sin@e learning a.lgorithm tlmt has t)eeu appliedto a. number of natural la.ngm,ge prol)ie.ms, includ-Jllg l)a.t't O\[' speech tagging and syuta.cl, ic l)m:sing(1h:i92, \]h:i93a, Bri!
)gb, Bri9d).
Figure :1 illus-trates the learning l)l:OCC'SS, l:irsL, tlll;21nlola, tedtext; is l)assed through the initial-st;ate mmota-tot.
'l'lw~ initial-stat, e area)tater can range in com-plexity from quite trivial (e.g.
assigning rmtdomstrll(:ttll:C) to quit, e sophistica.ted (e.g.
assigningthe output of a. I{nowledge-based ;/l/llot;~l, tol' thatwas created by hand).
Ouce text has beeu passedthrough the iuitia.l-state almOl, at.or, it.
is then (;ore-pared to the h'ugh,, as indicated ill a luamlally an-nota,teA eorl)llS , and transformations are le~u'nedthat can be applied to the oul, put of the iuitialstate remora, tot t;o make it, better resemble the:ruffs.So far, ouly ~ greedy search al)proach as beenused: at eaeh itera.tion o\[' learning, t.he tra nsfo>nl~tion is found whose application results in thegreatest iml)rovenmnt; ha.t transfk)rmation is thenadded to the ordered trmlsforlmLtiou list and thecorpus is upd~d.ed by a.pplying the.
learned transformation.
(See, (I{,Mg,\[) for a detailed discussiouof this algorithm in the context of machiue, le, aru--iug issues.
)Ottce 3,11 ordered list; of transform~tions ilearned, new text, can be mmotated hy first aI>plying the initial state ~mnotator to it and thenapplying each o\[' the traaM'ormations, iu order.UNANNOTATI{D \]"I'I~X'I'1NH'\[AI, lSTATEANNOTATliDTEXT TI~.I j'\['l l, ~ e , N  El( ~-~ RUI ,I-SFigure \[: Transfonm~tion-I~ased Error.-Drivenl,earlfiUg.r lh:ansformation-B ased Prepos i t iona lPhrase  At tachmentWe will now show how transformation-based e.rrol>driwm IGmfing can be used to resolve prep(~si-tiered phrase at, tachment ambiguity.
The l)reposi-tioiml phrase a.tt~Munent |ea.riter learns tra.nsfor--Ill~ttiollS \[?onl a C,)l:l>tls O\[ 4-tuples of the \['orm (vI11 I\] 1|9), where v is ~1 w;rl), nl  is the head of itsobjecl, llolni \]phrase, i ) is the \])l'epositioll, and 11:2is the head of the noun phrase, governed by theprel)c, sition (for e,-:anq~le, sce/v :1~' bo:q/,l o,/pthe h711/~2).
1,'or all sentences that conlbrm to thispattern in the Penn Treeb~mk W{dl St, l:eet 3ourlmlcorpns (MSM93), such a 4-tuplc was formed, attdeach :l-tuple was paired with the at~aehnteut de-cision used in the Treebauk parse) '\['here were12,766 4q;ul)les in all, which were randomly splitinto 12,206 trnining s**mples and 500 test samples.\[n this e?periment (as in (\[II~,9\], I\]l{93)), tim at-tachment choice For l)repositional i)hrases was I)e-I,ween the oh.iecl~ mmn and l,he matrix verb.
\[n theinitial sl,~te mmotator, all prepositional phrasesI \])at.terns were extra.clxxl usJ.ng tgrep, a. tree-basedgrep program written by Rich Pito.
'\]'\]te 4-tuples werecxtract;ed autom~tk:ally, a.ud mista.kes were not.
m~vntta.lly pruned out.1199are attached to the object, noun.
2 This is tile at-tachment predicted by right association (Kim73).The allowable transforlnations are describedby the following templates:?
Change the attachment location from X to Y if:- n l i sW- n2 is W- v isW-- p is W- n l  is W1 and n2 is W2- n l  i sWl  andv isW2Here "from X to Y" can be either "from nl  to v"or "from v to nl ,"  W (W1, W2, etc.)
can be anyword, and the ellipsis indicates that the completeset of transformations permits matching on anycombination of values for v, n l ,  p, and n2, withthe exception of patterns that specify vahms for allfour.
For example, one allowable transformationwould beChange the attachment location from nl  to vif p is "until".Learning proceeds as follows.
First, the train-ing set is processed according to the start stateannotator, in this case attaching all prepositionalphrases low (attached to nl) .
Then, in essence,each possible transtbrmation is scored by apply-ing it to the corpus and cornputing the reduction(or increase) in error rate.
in reality, the searchis data driven, and so the vast majority of al-lowable transformations are not examined.
Thebest-scoring transformation then becomes the firsttransformation i the learned list.
It is applied tothe training corpus, and learning continues on themodified corpus.
This process is iterated until norule can he found that reduces the error rate.In the experiment, a tol, al of 471 transfor-mations were learned - -  Figure 3 shows the firsttwenty.
3 Initial accuracy on the test set is 64.0%when prepositional phrases are always attached tothe object noun.
After applying the transforma-tions, accuracy increases to 80.8%.
Figure 2 showsa plot of test-set accuracy as a function of thenulnber of training instances.
It is interesting tonote that the accuracy curve has not yet, reached a2If it is the case that attaching to the verb wouldbe a better start state in some corpora, this decisioncould be parameterized.ZIn transformation #8, word token amount appearsbecause it was used as the head noun for noun phrasesrepresenting percentage amounts, e.g.
"5%."
The rulecaptures the very regular appearance in the Penn Tree-bank Wall Street Journal corpus of parses like Sales forthe yea," \[v'P rose \[Np5Yo\]\[pP in fiscal 1988\]\].Accuracy81.00 rl80.00 !
!79,00 t77.00 !--R .
.
.
.
.
/ - -F  .
.
.
.
.
%oo!1 / I74:001 .
.
.
.
_ _ t  .... _ _73.00j -72.00 l l  i _ __ / __ .
,?
!>2 -70.0069.0068.0067.0064.000.00 5.00I q!itT!aining Size x 10310.00Figure 2: Accuracy as a function of l;raining corpussize (no word class information).plateau, suggesting that more training data wonldlead to further improvements.Adding Word Class In format ionIn the above experiment, all trans\[brmations are.triggered hy words or groups of words, and it issurprising that good performance is achieved evenin spite of the inevitable sparse data problems.There are a number of ways to address the sparsedata problem.
One of the obvious ways, mappingwords to part of speech, seerns unlikely to help.
h>stead, semanl, ic class information is an attracLivealternative.We incorporated the idea of using semantic inotbrmation in the lbllowing way.
Using the Word~Net noun hierarchy (Milg0), each noun in  theffa{ning and test corpus was associated with a setcontaining the noun itself ph.ts the name of everysemantic lass that noun appears in (if any).
4 Thetransformation template is modified so that in ad-dition to asking if a nmm matches ome word W,4Class names corresponded to unique "synonynlset" identifiers within the WordNet noun database.A noun "appears in" a class if it falls within the hy-ponym (IS-A) tree below that class.
In the experimentsreported here we used WordNet version :l.2.12001245(378910II12:13\]415\[617\[81192()_Change Att{:~ehmentLocationl"r~m~ To ( ;oMit ionN1 V P is atN\ ]  \ /  P is asN1 V I ) is iuloN:I \/ P is ,l}'omN:I V P is withN\] V N2 is yearN 1 V P is byI?
is i~ andN I V NI ix amounlN \[ \/  \]' is lhroughNI V \]) is d'urb~gNI V V ix p,ulN1 V N2 is mou.lkN\[ V 1' is ulldcrNJ V 1 ) is afterV is have andN1 V I' is b~N:\[ V P is wilk.oulV NI P is ofV is buy andN1 \/ P is forN:I V P is beJbl"(V is have andNI V P is o~x/L(Vv ~Vv /VV,/Figure 3: The \[irst 20 transforntat;ions learned tbrpreposil;ional phrase ~ttachme, n|;.it: (~an a/so ask if" it is a~ member  of some class C. sThis al)proaeh I;o data.
sparseness i similar to tllatof (l{,es93b, li, l\[93), where {~ method ix proposedfor using WordNet in conjunction with a corpusto ohtain class-based statisl, ie,q.
()lit' method hereis ltlllC\]l simpler, however, in I;hat we a.re only us-ing Boolean values to indieal;e whel;her ~ word canbe a member of' a class, rather than esl, imat ing ~filll se{, of joint probabil it ies involving (:lasses.Since the tr;ulsformation-based al)l/roach withclasses cCm gener~dize ill a way that the approachwithout classes is ml~l)le to, we woldd expect f'cwerl;ransf'ormal;ions to be necessary, l!
;xperimeaH, ally,this is indeed the case.
In a second experiment;,l;raining a.ml testing were era:tied out on the samesamples as i ,  the previous experiment,  bul; I;hist ime using the ext, ende, d tra ns lbrmat ion t(;ml)la.tesfor word classes.
A total of 266 transformationswere learned.
Applying l.hese transt'ormai.ions tothe test set l'eslllted in a.n accuracy of' 81.8%.\[n figure 4 we show tile lirst 20 tra.nsform{~l, ionslem'ned using ilOllll classes.
Class descriptions arcsurrounded by square bracl{ets.
(; 'Phe first; grans-Ibrmation st~l.cs thai.
if" N2 is a. nomt I, hal; describestime (i.e.
ix a. member  of WordNet class that  in-cludes tim nouns "y(;ar," "month," "week," andothers), thell the preltositiomd phrase should beal;tache(\[ t,() the w;rb, since, tim(; is \]nlMl morel ikely Io modify a yet'It (e.g.
le,vc lh(: re(cling iuan hour) thaJl a, lloun.This exlw, r iment also demonstrates how rely\[~?~l;ul:e-based lexicon or word classiflcat, ion schemecau triviaJly be incorlJorated into the learner, byexLencling l;ransfot'nlal,iolls to allow thent to makel'efel'eAlc(?
|;o it WOl:(\[ g i l t \ [  { l i ly  O\[' its features.\],valuation against OtherAlgorithmsIn (lIl~91, HR93), tra.inittg is done on asuperset el' sentence types ttsed ill train-ing the transforlJ~atiolFbased learner.
Thetransformation-based learner is I, rained on sen-tences containing v, n\[  and p, whereas the algo-r i thm describe.d by l l indle and I~,ooth ca.n zdso usesentences (;ontailfing only v and p, (n' only nl andi1.
\[11 their lmper, they tra.in on ow~r 200,000 sen-Lettces with prel)ositions f'rotn the Associated Press(APt newswire, trod I;hey quote a.n accuracy of 78-80% on AP test &~ta..~' For reasons of ~: u n- t ime c\[lk:icn(:y, transfonmLl, ionstmddng re\['crence 1:o tile classes of both n l  a,nd n2 wereIlOI; p(~l?lXiitl, tR(I.GI;or expository purposes, the u.iqm'.
WordNetid('.ntilicrs luwe been replaced by words Lh~LL describethe cont, cnt of the class.1207(~lml~.ge \]At tachment ,  /Location /# li'rom t 'Fo \[ Condition1 N1 V N2 is \[time\]2 N1 V P is al3 N1 V P is as4 N1 V P is into5 N 1 V P is from6 N1 V 1 ) is wilh7 N1 V P is ofP is in andNI is8 N 1 V \[measure, quanlily, amou~l\]P is by all.el9 N1 V N2 is \[abslraclion\]I 0 NI V P is lhro'ugh1) is in andN I is11 NI V \[group,group.in.g\]12 V N 1 V is be13 NI V V is pul14 NI V P is underP is i~ andN\] is15 N1 V \[written co.mmlu~ication\]16 N1 V l ) is wilhoul17 N1 V P is during18 N 1 V19 NI V20 N1.
Vl ) is on andNt is \[U~.ing\]P is afterV is buy andP is forFigure 4: The first 20 transformations learnedfor prepositional phrase attachment, using nounclasses.ofMethod Accuracy Transformst-Scores 70.4- 75.8'\]?anstbrma~ions 80.8 471Trans\['ormati ons(no N2) 79.2 418Transformations(classes) 8:1.8 26(5Figure 5: Comparing l{esults in PP Attachment.In order to compare the two approaches, wereimplemen:ed the ~flgorithm fi'om (IIR.91) andtested it using the same training and test setused for the above experiments.
Doing so re-sull;ed in an attachment accuracy of 70.4%.
Next,the training set was expanded to include not onlythe cases o\[' ambiguous attachment \]Fonnd in theparsed Wall Street Journal corpus, as before, butalso all the unambiguous prepositional phrase at-tachments tbnnd in the corpus, as well (contiml-ing to exclnde the tesl, set, of course).
Accuracyimproved to 75.8% r using the larger training set,still significantly lower than accuracy obtained us--lag tam tl:ansformal;ion-based approach.
The t.ech-nique described in (Res93b, 1{1t93), which com-bined Hindle and Rooth's lexical association tech-nique with a WordNet-based conceptual associa-tion measure, resulted in an accuracy of 76.0%,also lower than the results obtained using trans-formations.Since llindle and Rooth's approach doesnot make reference to n2, we re-ran thetransformation-learner disalk)wing all transforma-tions that make reference ~o n2.
Doing so resultedin an accuracy of 79.2%.
See figure 5 h)r a sun>mary of results.It is possihle Lo compare; the results describedhere with a somewhat similar approach devel-.oped independently by Ratnaparkhi  and I/,oukos(l{R94), since they also used training and test datt~drawn from the Penn Treebank's Wall Street Jour-nal corpus.
Instead of' using mammlly coustructedlexical classes, they nse word classes arrived at viamutmd information clustering in a training corpus(BDd+92), resulting in a representation i whicheach word is represented by a sequence of bits.As in the experiments here, their statistical modelalso makes use of a 4-tuple context (v, c<l, p, n2),and can use the identit.ies of the words, class inl'or-marion (tbr them, wdues of any of the class bits),rThe difference between these results ~nd tile resultthey quoted is likely due to a much bLrger training setused in their origimd experiments.1202or both Mnds of i ld'ormation as eotll;extual fea-tlll?eS riley {lescril)e a search process use(\[ to{letePn6\]m what, sul)set of the available ill\['or,~Ht-l ion will Im used in the model.
(\]iv{;\]\] a eh{}iceof features, they train ;t prol}abi/islie model ForI)r(Sitclcoutext), and in {.esl.ing choose Site :-: voP Site = n l  a~ccordi\]lg I;o which has {he highereomlitional probal)i\]ity.t~,atnal)~Pkhi and Roukos rel}ort an aecuraeyoi' 81.6% using bot, h word and class iui'orma, tionon Wall SI;re.et 3ourna\] text,, using a t:raining COl:-pus twice as la, rgc as that  used in ouP experiments.They also report that  a (leeision tree mode/ eon-st\];u(:t~d using the same features m,d I,i;aining dataac\[lieve{I I)erformanee of 77.71~, (}n t\[:e same I.estset,A llUll ll)el' o\[' other  reseaPehers have exl)loredeorlms-I)ased approaches I;o l)repositional phraseattaehmet, t  disaml)iguation tM~t n\]~d{c use of wordclasses, l"or example, Weisehed{q cl al.
(WAIH91)and Basili el al.
(BI}V91) bol,\]l deseril)e theuse of lnanual ly coustrueted, donmhv Sl){~eitic wordclasses together with cori}us-tmsed si,t~tisties in o fd{2r to resolve i)rel)ositional 1)hrase a.t, taehlllellt &Ill-.I}iguity.
I{e(;a.llSe these papers deseril)e results ol)-tained on different corpora, however, it is (lifIicull;to II~,:'tl,:.
{; a.
1)(;r\['(}rllla, iic{!
COl\[lD~/l:iSOll,Conc lus ionsThe.
tPansl 'ormation-hased approach to resolvingpreposit ional phl:ase disanlbiguat ion has a mlmt)erof advaiH;ages over (}l,\]ler ;i.l)l)roatehes.
\[11 a (\]irecteoml);u:ison with lexical association, higher ble(;ll-vaey is achieved using words alolm (wen thoughat tachment  inf\}rnlation is captured i*l a relativelysmall  numl)er of simple, rea(lable rules, as opl)osedto a. large lllllll\])eF Of lexical co-oeetlrreltee l)l'o\])a --I)ilities.\]u addit ion, we have shown how thel;raus\['orln~Lion-based l arner can casity be e?.-tended to incorporate word-class i/fformatiou.This  resulted in a slight; increase in 1)erformanee,but, more notal)\]y it resulted in a reduct;ion hyroughly half  in the l;ota\[ mnnl)er of transfor-mat ion rules needed.
And in (:outrast to ap-pro~ches using class--based prol)abil istic models(BPV91, Res93e, WAI~ F91) or classes derived vi;~statist ical  clusl.ering methods (1~.R94), t:his {ech-l l ique pro(hlees a, I:HIO set that  (:al}l;ltr{es eolteepl:~lalgeueralizal;ions couciseIy a.ml ill \]mman-rea{Ial}\]efor I n.F/\]rthel:lllOl:e, iuso\['ar as (:oHq)a, risolls e&ll I)oina(h- all lOl lg separa, Le exl'}el:llllel/l;s l lsi lt~ Wai lStreet Jour\]ml training aml test data (( l lRgl) ,reiml)l('meute(l as reI)oPted above; (l{es93e,1t1193); (IH1.94)), the rule-based approach de..scribed here achieves better perl'orlttaucc, using mlalgol:ithm tlmt is eoncel}tually quite Mml)le am/ iul)l'~l.
(;tiea\] teFlttS extretuely easy to i lnplenlel~t, sA more genera\] pointix tha.t the transl'orm~d,ion-based ;~l)l}roateh is eas-ily a(lapl,ed t;o s i tuat ions in which some learning1"rein a (:orpus is desiral)le, 1}ui, hand-construetc{Il}l:ior knowledge is also available.
Exist ing knowl-e{lge, such as structural  strategies or even a priorih;xieal l}references, (;all 1)e incorl)orated into I;hestart  state annotator ,  so theft the learning ~dgo.I:ithm begins with n,ore refiued input.
And knowuexceptious {:au 1)e handh'(l t ransparent ly  simply hyadding add\]: \[onal rules to tim set thai; is learned,IlSillg tile sall le representat io\] l .A disadwmtage of the al)l)roach is that it re-quires supervised t ra in ing  that  is, a representa-tive set of "true" c~ses t'FOlll which Co learn.
Ilow-ever, this l)eeomes less of a probh'.m as atmotatedeorl}ora beeolne increasingly available, and sug-gests the comhinat ion o1:' supexvised and uusupervised methods as a.u ilfl;eresth G ave\]me \['or \['urtherrese;ire\] \[.References\[AS88\] (~.
All, mann and M. Steedmau.
Inter-action with context during hmnan sen-ten(u', I)ro<'essing.
Co.qnitio~, ;}0:191238, 1988.\[l;DdF92\] I}eter I?.
\]~Powa, Vhleell~ J. l)ella\]}ietr;~, l)eter V. {leSouza, 3enni\['er (LI,ai, and Robert I,.
Mereer.
(21ass.based n-gr+url models of natural \[aw-gua.ge.
Compulational l, ingui.slic.%18(d):467 480, December  1(,)92.\[BPV91\] H. Basili, M. Pazienza, and P. Velardi.Combin ing NLI } ~md statistica.l teelPniques for lexical aequisit ion.
\]:n Pro-ccedings of the AAA 1 Fall 5'ymposhtm.on Probabilistie Approaches to NaturalLanguage, Cambridge,  MassaehusetLs,Octobew 1!
)9 I.\[Ih:i92\] F,.
t r i l l .
A simple rule-bused part ofspeech t~Gge\]:.
In t'voeecding~ of lhPThird UoT@'rence on Applied Natu-ral Lan.guagc Processing, A (..'g, Trent;o,ltaly, 1992.\[Bri93a\] I';.
t r i l l .
Automat ic  g rammar  m-duel;ion and parsing fi:ee text: At r arts \['orlnation-1}ased al~l)roaeh.
I\]1Proceedings o,f tit{; 31sl Mceling of lheAssociation of Compulational 1,inguis-tics, Columbus,  Oh., 1993.8()ur code is being made pul}licly ~waihdAe.
(?on-tact {.Its'.
aJ\]thors \[br inl)-}l:ltlaJ, ioll Oll how to obtain it.1203\[Bri93b\]\[Bri94\]\[l!
'ra78\]\[Hi{9 ~I\]\[n,~,ga\]\[1(im73\]\[Mil90\]\[MSM93\]\[Res93a\]\[Res93b\]\[l{,es93c\]\[ltnga\]E. Brill.
A Corpus-Based Approach loLanguage l, cmming.
Phi) thesis, De-partment of' Computer and lnfbrrna-lion Science, University of Pennsylva-nia, 1993.E.
Brill.
Some advances in rule-basedpart of speech tagging.
In Proceed-ings of lhe 7'welflh National Co,@r-once on Artificial \]nlclligence (AAAI-94) , Seattle, Wa., 1994.I,.
Frazier.
0)~ comprehending sen-tences: synlaelic parsi~Lq sleategies.PhD thesis, University of Connecticut,1978.1).
Hindle and M. f{,ooth.
Structuralambiguity and lexicai relations.
InProccedin~ls of the ~f,J~l~ Annual Mee#in9 of lhe Associa*ion for" Computa-tional l,i~Lquisties, Berkeley, Ca., 1991.I).
Iiindle and M. l{ooth.
Structura.1ambiguity and lexical relations.
Com-putational Li~Lq'uislies, 19(l):103 120,1993.J.
Kimball.
Seven principles of surfacestructure parsing in natm'al language.Co q~ilimh 2, 1973.G.
Miller.
Wordnet: an on-line lexi-cal cla.l~abase, hflernational , our~al ofLezieography, 3(4), 1990.M.
Marcus,B.
Santorini, and M. Marcinkiewicz.Building a large annotat, ed corpus ofl!
;nglish: the Petm 'Freebank.
Compu-talional Linguistics, 19(2), 1993.P.
l{esnik.
Selection a.~d lnforma-lion: A Chtss-lhtsed Approach Io Lezi-eal t{elatio~ships.
PhD thesis, Unive>sity of Pennsylvania, December 1993.
(Institute for H,esearch in CognitiveScience report IRCS-9'3-42).P.
l{,esnik.
Semantic lasses and syn-tactic ambiguity.
In Proceedings of/heA ~PA Workshop on I~urna~ LanguageTechnology.
Morgan Kamfinan, \[993.P.
H,esnik.
Semantic lasses and syn-tactic ambiguity.
AftPA Workshop onIhman Language q?echnology, Mz~rch1993, Princeton.P.
Resnik and M. Hearst.
Syntacticambiguity and conceptual relations.
InK.
Church, editor, Pr'oceedings of lheACL Workshop on Very Large Cor-pora, pages 58 64, .June 1993.\[R, M94\]\[mv~q\[WAB + 9 :l\]\[WFB90\]L. i{amshaw and M. Marcus.
Explo>ing the statistical derivation of trans-formational rule sequences for part-of-speech tagging.
In a. Klavans andP.
l{esnik, editors, The Balanci~g Ac*:Proceedings of lhe AC.L Workshop onCombining Symbolic and 5'tatislicalApproaches to Language, New MexicoState University, July 1994.A.
Ratnaparkhi and S. Roukos.
Amaximum entropy model \[br prepo-sitional phrase attachment.
In Pro-ceedings of lhe ARPA Workshop on\]\[uma~ Language Technology, Plains-I,oro, N J, March 1994.IC Weischedel, D. Ayuso, R.. Bobrow,S.
Boisen, R. lngria, and J. Palmucei.Partial parsing: a report of work inprogress.
In Proceedings of the l,b'urthDA.RPA 5'pecch and Nalural LanguageWorkshop, February .199 l, 1991.G.
Whi~temore, K. Ferrara, andH.
Brunner.
Empirical study ofpredictive powers of simple attach-lnent schemes for post-modifier p epo-sitional phrases.
In .Procecdi~gs of the281h Annual Meeting of the Associ-ation for Comlmtalional Linguistics,1990.1204
