Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 178?186,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsAbstract Meaning Representation for SembankingLaura BanarescuSDLlbanarescu@sdl.comClaire BonialLinguistics Dept.Univ.
Coloradoclaire.bonial@colorado.eduShu CaiISIUSCshucai@isi.eduMadalina GeorgescuSDLmgeorgescu@sdl.comKira GriffittLDCkiragrif@ldc.upenn.eduUlf HermjakobISIUSCulf@isi.eduKevin KnightISIUSCknight@isi.eduPhilipp KoehnSchool of InformaticsUniv.
Edinburghpkoehn@inf.ed.ac.ukMartha PalmerLinguistics Dept.Univ.
Coloradomartha.palmer@colorado.eduNathan SchneiderLTICMUnschneid@cs.cmu.eduAbstractWe describe Abstract Meaning Represen-tation (AMR), a semantic representationlanguage in which we are writing downthe meanings of thousands of English sen-tences.
We hope that a sembank of simple,whole-sentence semantic structures willspur new work in statistical natural lan-guage understanding and generation, likethe Penn Treebank encouraged work onstatistical parsing.
This paper gives anoverview of AMR and tools associatedwith it.1 IntroductionSyntactic treebanks have had tremendous impacton natural language processing.
The Penn Tree-bank is a classic example?a simple, readable fileof natural-language sentences paired with rooted,labeled syntactic trees.
Researchers have ex-ploited manually-built treebanks to build statisti-cal parsers that improve in accuracy every year.This success is due in part to the fact that we havea single, whole-sentence parsing task, rather thanseparate tasks and evaluations for base noun iden-tification, prepositional phrase attachment, tracerecovery, verb-argument dependencies, etc.
Thosesmaller tasks are naturally solved as a by-productof whole-sentence parsing, and in fact, solved bet-ter than when approached in isolation.By contrast, semantic annotation today is balka-nized.
We have separate annotations for named en-tities, co-reference, semantic relations, discourseconnectives, temporal entities, etc.
Each annota-tion has its own associated evaluation, and trainingdata is split across many resources.
We lack a sim-ple readable sembank of English sentences pairedwith their whole-sentence, logical meanings.
Webelieve a sizable sembank will lead to new work instatistical natural language understanding (NLU),resulting in semantic parsers that are as ubiquitousas syntactic ones, and support natural languagegeneration (NLG) by providing a logical seman-tic input.Of course, when it comes to whole-sentence se-mantic representations, linguistic and philosophi-cal work is extensive.
We draw on this work to de-sign an Abstract Meaning Representation (AMR)appropriate for sembanking.
Our basic principlesare:?
AMRs are rooted, labeled graphs that areeasy for people to read, and easy for pro-grams to traverse.?
AMR aims to abstract away from syntac-tic idiosyncrasies.
We attempt to assign thesame AMR to sentences that have the samebasic meaning.
For example, the sentences?he described her as a genius?, ?his descrip-tion of her: genius?, and ?she was a ge-nius, according to his description?
are all as-signed the same AMR.?
AMR makes extensive use of PropBankframesets (Kingsbury and Palmer, 2002;Palmer et al 2005).
For example, we rep-resent a phrase like ?bond investor?
usingthe frame ?invest-01?, even though no verbsappear in the phrase.?
AMR is agnostic about how we might wantto derive meanings from strings, or vice-versa.
In translating sentences to AMR, wedo not dictate a particular sequence of ruleapplications or provide alignments that re-flect such rule sequences.
This makes sem-banking very fast, and it allows researchersto explore their own ideas about how strings178are related to meanings.?
AMR is heavily biased towards English.
Itis not an Interlingua.AMR is described in a 50-page annotation guide-line.1 In this paper, we give a high-level descrip-tion of AMR, with examples, and we also providepointers to software tools for evaluation and sem-banking.2 AMR FormatWe write down AMRs as rooted, directed, edge-labeled, leaf-labeled graphs.
This is a com-pletely traditional format, equivalent to the sim-plest forms of feature structures (Shieber et al1986), conjunctions of logical triples, directedgraphs, and PENMAN inputs (Matthiessen andBateman, 1991).
Figure 1 shows some of theseviews for the sentence ?The boy wants to go?.
Weuse the graph notation for computer processing,and we adapt the PENMAN notation for humanreading and writing.3 AMR ContentIn neo-Davidsonian fashion (Davidson, 1969), weintroduce variables (or graph nodes) for entities,events, properties, and states.
Leaves are labeledwith concepts, so that ?
(b / boy)?
refers to an in-stance (called b) of the concept boy.
Relations linkentities, so that ?
(d / die-01 :location (p / park))?means there was a death (d) in the park (p).
Whenan entity plays multiple roles in a sentence, weemploy re-entrancy in graph notation (nodes withmultiple parents) or variable re-use in PENMANnotation.AMR concepts are either English words(?boy?
), PropBank framesets (?want-01?
), or spe-cial keywords.
Keywords include special entitytypes (?date-entity?, ?world-region?, etc.
), quan-tities (?monetary-quantity?, ?distance-quantity?,etc.
), and logical conjunctions (?and?, etc).AMR uses approximately 100 relations:?
Frame arguments, following PropBankconventions.
:arg0, :arg1, :arg2, :arg3, :arg4,:arg5.?
General semantic relations.
:accompa-nier, :age, :beneficiary, :cause, :compared-to,:concession, :condition, :consist-of, :degree,:destination, :direction, :domain, :duration,1AMR guideline: amr.isi.edu/language.htmlLOGIC format:?
w, b, g:instance(w, want-01) ?
instance(g, go-01) ?instance(b, boy) ?
arg0(w, b) ?arg1(w, g) ?
arg0(g, b)AMR format (based on PENMAN):(w / want-01:arg0 (b / boy):arg1 (g / go-01:arg0 b))GRAPH format:Figure 1: Equivalent formats for representatingthe meaning of ?The boy wants to go?.
:employed-by, :example, :extent, :frequency,:instrument, :li, :location, :manner, :medium,:mod, :mode, :name, :part, :path, :polarity,:poss, :purpose, :source, :subevent, :subset,:time, :topic, :value.?
Relations for quantities.
:quant, :unit,:scale.?
Relations for date-entities.
:day, :month,:year, :weekday, :time, :timezone, :quarter,:dayperiod, :season, :year2, :decade, :cen-tury, :calendar, :era.?
Relations for lists.
:op1, :op2, :op3, :op4,:op5, :op6, :op7, :op8, :op9, :op10.AMR also includes the inverses of all these rela-tions, e.g., :arg0-of, :location-of, and :quant-of.
Inaddition, every relation has an associated reifica-tion, which is what we use when we want to mod-ify the relation itself.
For example, the reificationof :location is the concept ?be-located-at-91?.Our set of concepts and relations is designed toallow us represent all sentences, taking all wordsinto account, in a reasonably consistent manner.
Inthe rest of this section, we give examples of howAMR represents various kinds of words, phrases,and sentences.
For full documentation, the readeris referred to the AMR guidelines.179Frame arguments.
We make heavy use ofPropBank framesets to abstract away from Englishsyntax.
For example, the frameset ?describe-01?has three pre-defined slots (:arg0 is the describer,:arg1 is the thing described, and :arg2 is what it isbeing described as).
(d / describe-01:arg0 (m / man):arg1 (m2 / mission):arg2 (d / disaster))The man described the mission as a disaster.The man?s description of the mission:disaster.As the man described it, the mission was adisaster.Here, we do not annotate words like ?as?
or ?it?,considering them to be syntactic sugar.General semantic relations.
AMR also in-cludes many non-core relations, such as :benefi-ciary, :time, and :destination.
(s / hum-02:arg0 (s2 / soldier):beneficiary (g / girl):time (w / walk-01:arg0 g:destination (t / town)))The soldier hummed to the girl as shewalked to town.Co-reference.
AMR abstracts away from co-reference gadgets like pronouns, zero-pronouns,reflexives, control structures, etc.
Instead we re-use AMR variables, as with ?g?
above.
AMRannotates sentences independent of context, so ifa pronoun has no antecedent in the sentence, itsnominative form is used, e.g., ?
(h / he)?.Inverse relations.
We obtain rooted structuresby using inverse relations like :arg0-of and :quant-of.
(s / sing-01:arg0 (b / boy:source (c / college)))The boy from the college sang.
(b / boy:arg0-of (s / sing-01):source (c / college))the college boy who sang ...(i / increase-01:arg1 (n / number:quant-of (p / panda)))The number of pandas increased.The top-level root of an AMR represents the fo-cus of the sentence or phrase.
Once we have se-lected the root concept for an entire AMR, thereare no more focus considerations?everything elseis driven strictly by semantic relations.Modals and negation.
AMR represents nega-tion logically with :polarity, and it expressesmodals with concepts.
(g / go-01:arg0 (b / boy):polarity -)The boy did not go.
(p / possible:domain (g / go-01:arg0 (b / boy)):polarity -))The boy cannot go.It?s not possible for the boy to go.
(p / possible:domain (g / go-01:arg0 (b / boy):polarity -))It?s possible for the boy not to go.
(p / obligate-01:arg2 (g / go-01:arg0 (b / boy)):polarity -)The boy doesn?t have to go.The boy isn?t obligated to go.The boy need not go.
(p / obligate-01:arg2 (g / go-01:arg0 (b / boy):polarity -))The boy must not go.It?s obligatory that the boy not go.
(t / think-01:arg0 (b / boy):arg1 (w / win-01:arg0 (t / team):polarity -))The boy doesn?t think the team will win.The boy thinks the team won?t win.Questions.
AMR uses the concept ?amr-unknown?, in place, to indicate wh-questions.
(f / find-01:arg0 (g / girl):arg1 (a / amr-unknown))What did the girl find?
(f / find-01:arg0 (g / girl):arg1 (b / boy):location (a / amr-unknown))Where did the girl find the boy?180(f / find-01:arg0 (g / girl):arg1 (t / toy:poss (a / amr-unknown)))Whose toy did the girl find?Yes-no questions, imperatives, and embedded wh-clauses are treated separately with the AMR rela-tion :mode.Verbs.
Nearly every English verb and verb-particle construction we have encountered has acorresponding PropBank frameset.
(l / look-05:arg0 (b / boy):arg1 (a / answer))The boy looked up the answer.The boy looked the answer up.AMR abstracts away from light-verb construc-tions.
(a / adjust-01:arg0 (g / girl):arg1 (m / machine))The girl adjusted the machine.The girl made adjustments to the machine.Nouns.We use PropBank verb framesets to rep-resent many nouns as well.
(d / destroy-01:arg0 (b / boy):arg1 (r / room))the destruction of the room by the boy ...the boy?s destruction of the room ...The boy destroyed the room.We never say ?destruction-01?
in AMR.
Somenominalizations refer to a whole event, while oth-ers refer to a role player in an event.
(s / see-01:arg0 (j / judge):arg1 (e / explode-01))The judge saw the explosion.
(r / read-01:arg0 (j / judge):arg1 (t / thing:arg1-of (p / propose-01))The judge read the proposal.
(t / thing:arg1-of (o / opine-01:arg0 (g / girl)))the girl?s opinionthe opinion of the girlwhat the girl opinedMany ?-er?
nouns invoke PropBank framesets.This enables us to make use of slots defined forthose framesets.
(p / person:arg0-of (i / invest-01))investor(p / person:arg0-of (i / invest-01:arg1 (b / bond)))bond investor(p / person:arg0-of (i / invest-01:manner (s / small)))small investor(w / work-01:arg0 (b / boy):manner (h / hard))the boy is a hard workerthe boy works hardHowever, a treasurer is not someone who trea-sures, and a president is not (just) someone whopresides.Adjectives.
Various adjectives invoke Prop-Bank framesets.
(s / spy:arg0-of (a / attract-01))the attractive spy(s / spy:arg0-of (a / attract-01:arg1 (w / woman)))the spy who is attractive to women?-ed?
adjectives frequently invoke verb framesets.For example, ?acquainted with magic?
maps to?acquaint-01?.
However, we are not restricted toframesets that can be reached through morpholog-ical simplification.
(f / fear-01:arg0 (s / soldier):arg1 (b / battle-01))The soldier was afraid of battle.The soldier feared battle.The soldier had a fear of battle.For other adjectives, we have defined new frame-sets.
(r / responsible-41:arg1 (b / boy):arg2 (w / work))The boy is responsible for the work.The boy has responsibility for the work.While ?the boy responsibles the work?
is not goodEnglish, it is perfectly good Chinese.
Similarly,we handle tough-constructions logically.181(t / tough:domain (p / please-01:arg1 (g / girl)))Girls are tough to please.It is tough to please girls.Pleasing girls is tough.?please-01?
and ?girl?
are adjacent in the AMR,even if they are not adjacent in English.
?-able?adjectives often invoke the AMR concept ?possi-ble?, but not always (e.g., a ?taxable fund?
is actu-ally a ?taxed fund?).
(s / sandwich:arg1-of (e / eat-01:domain-of (p / possible)))an edible sandwich(f / fund:arg1-of (t / tax-01))a taxable fundPertainym adjectives are normalized to root form.
(b / bomb:mod (a / atom))atom bombatomic bombPrepositions.
Most prepositions simply sig-nal semantic frame elements, and are themselvesdropped from AMR.
(d / default-01:arg1 (n / nation):time (d2 / date-entity:month 6))The nation defaulted in June.Time and location prepositions are kept if theycarry additional information.
(d / default-01:arg1 (n / nation):time (a / after:op1 (w / war-01))The nation defaulted after the war.Occasionally, neither PropBank nor AMR has anappropriate relation, in which case we hold ournose and use a :prep-X relation.
(s / sue-01:arg1 (m / man):prep-in (c / case))The man was sued in the case.Named entities.
Any concept in AMR can bemodified with a :name relation.
However, AMRincludes standardized forms for approximately 80named-entity types, including person, country,sports-facility, etc.
(p / person:name (n / name:op1 "Mollie":op2 "Brown"))Mollie Brown(p / person:name (n / name:op1 "Mollie":op2 "Brown"):arg0-of (s / slay-01:arg1 (o / orc)))the orc-slaying Mollie BrownMollie Brown, who slew orcsAMR does not normalize multiple ways of re-ferring to the same concept (e.g., ?US?
versus?United States?).
It also avoids analyzing seman-tic relations inside a named entity?e.g., an orga-nization named ?Stop Malaria Now?
does not in-voke the ?stop-01?
frameset.
AMR gives a clean,uniform treatment to titles, appositives, and otherconstructions.
(c / city:name (n / name:op1 "Zintan"))Zintanthe city of Zintan(p / president:name (n / name:op1 "Obama"))President ObamaObama, the president ...(g / group:name (n / name:op1 "Elsevier":op2 "N.V."):mod (c / country:name (n2 / name:op1 "Netherlands")):arg0-of (p / publish-01))Elsevier N.V., the Dutch publishing group...Dutch publishing group Elsevier N.V. ...Copula.
Copulas use the :domain relation.
(w / white:domain (m / marble))The marble is white.
(l / lawyer:domain (w / woman))The woman is a lawyer.
(a / appropriate:domain (c / comment):polarity -))The comment is not appropriate.182The comment is inappropriate.Reification.
Sometimes we want to use anAMR relation as a first-class concept?to be ableto modify it, for example.
Every AMR relation hasa corresponding reification for this purpose.
(m / marble:location (j / jar))the marble in the jar ...(b / be-located-at-91:arg1 (m / marble):arg2 (j / jar):polarity -):time (y / yesterday))The marble was not in the jar yesterday.If we do not use the reification, we run into trou-ble.
(m / marble:location (j / jar:polarity -):time (y / yesterday))yesterday?s marble in the non-jar ...Some reifications are standard PropBank frame-sets (e.g., ?cause-01?
for :cause, or ?age-01?
for:age).This ends the summary of AMR content.
Forlack of space, we omit descriptions of compara-tives, superlatives, conjunction, possession, deter-miners, date entities, numbers, approximate num-bers, discourse connectives, and other phenomenacovered in the full AMR guidelines.4 Limitations of AMRAMR does not represent inflectional morphologyfor tense and number, and it omits articles.
Thisspeeds up the annotation process, and we do nothave a nice semantic target representation for thesephenomena.
A lightweight syntactic-style repre-sentation could be layered in, via an automaticpost-process.AMR has no universal quantifier.
Words like?all?
modify their head concepts.
AMR does notdistinguish between real events and hypothetical,future, or imagined ones.
For example, in ?the boywants to go?, the instances of ?want-01?
and ?go-01?
have the same status, even though the ?go-01?may or may not happen.We represent ?history teacher?
nicely as ?
(p /person :arg0-of (t / teach-01 :arg1 (h / history)))?.However, ?history professor?
becomes ?
(p / pro-fessor :mod (h / history))?, because ?profess-01?is not an appropriate frame.
It would be reason-able in such cases to use a NomBank (Meyers etal., 2004) noun frame with appropriate slots.5 Creating AMRsWe have developed a power editor for AMR, ac-cessible by web interface.2 The AMR Editor al-lows rapid, incremental AMR construction via textcommands and graphical buttons.
It includes on-line documentation of relations, quantities, reifi-cations, etc., with full examples.
Users log in,and the editor records AMR activity.
The ed-itor also provides significant guidance aimed atincreasing annotator consistency.
For example,users are warned about incorrect relations, discon-nected AMRs, words that have PropBank frames,etc.
Users can also search existing sembanks forphrases to see how they were handled in the past.The editor also allows side-by-side comparison ofAMRs from different users, for training purposes.In order to assess inter-annotator agreement(IAA), as well as automatic AMR parsing accu-racy, we developed the smatch metric (Cai andKnight, 2013) and associated script.3 Smatch re-ports the semantic overlap between two AMRs byviewing each AMR as a conjunction of logicaltriples (see Figure 1).
Smatch computes precision,recall, and F-score of one AMR?s triples againstthe other?s.
To match up variables from two in-put AMRs, smatch needs to execute a brief search,looking for the variable mapping that yields thehighest F-score.Smatch makes no reference to English stringsor word indices, as we do not enforce any par-ticular string-to-meaning derivation.
Instead, wecompare semantic representations directly, in thesame way that the MT metric Bleu (Papineni etal., 2002) compares target strings without makingreference to the source.For an initial IAA study, and prior to adjust-ing the AMR Editor to encourage consistency, 4expert AMR annotators annotated 100 newswiresentences and 80 web text sentences.
They thencreated consensus AMRs through discussion.
Theaverage annotator vs. consensus IAA (smatch) was0.83 for newswire and 0.79 for web text.
Whennewly trained annotators doubly annotated 382web text sentences, their annotator vs. annotatorIAA was 0.71.2AMR Editor: amr.isi.edu/editor.html3Smatch: amr.isi.edu/evaluation.html1836 Current AMR BankWe currently have a manually-constructed AMRbank of several thousand sentences, a subset ofwhich can be freely downloaded,4 the rest beingdistributed via the LDC catalog.In initially developing AMR, the authors builtconsensus AMRs for:?
225 short sentences for tutorial purposes?
142 sentences of newswire (*)?
100 sentences of web data (*)Trained annotators at LDC then produced AMRsfor:?
1546 sentences from the novel ?The LittlePrince??
1328 sentences of web data?
1110 sentences of web data (*)?
926 sentences from Xinhua news (*)?
214 sentences from CCTV broadcast con-versation (*)Collections marked with a star (*) are also inthe OntoNotes corpus (Pradhan et al 2007;Weischedel et al 2011).Using the AMR Editor, annotators are able totranslate a full sentence into AMR in 7-10 minutesand postedit an AMR in 1-3 minutes.7 Related WorkResearchers working on whole-sentence semanticparsing today typically use small, domain-specificsembanks like GeoQuery (Wong and Mooney,2006).
The need for larger, broad-coverage sem-banks has sparked several projects, including theGroningen Meaning Bank (GMB) (Basile et al2012a), UCCA (Abend and Rappoport, 2013),the Semantic Treebank (ST) (Butler and Yoshi-moto, 2012), the Prague Dependency Treebank(Bo?hmova?
et al 2003), and UNL (Uchida et al1999; Uchida et al 1996; Martins, 2012).Concepts.
Most systems use English wordsas concepts.
AMR uses PropBank frames (e.g.,?describe-01?
), and UNL uses English WordNetsynsets (e.g., ?200752493?).Relations.
GMB uses VerbNet roles (Schuler,2005), and AMR uses frame-specific PropBankrelations.
UNL has a dedicated set of over 30 fre-quently used relations.Formalism.
GMB meanings are written inDRT (Kamp et al 2011), exploiting full first-4amr.isi.edu/download.htmlorder logic.
GMB and ST both include universalquantification.Granularity.
GMB and UCCA annotate shorttexts, so that the same entity can participate inevents described in different sentences; other sys-tems annotate individual sentences.Entities.
AMR uses 80 entity types, whileGMB uses 7.Manual versus automatic.
AMR, UNL, andUCCA annotation is fully manual.
GMB and STproduce meaning representations automatically,and these can be corrected by experts or crowds(Venhuizen et al 2013).Derivations.
AMR and UNL remain agnosticabout the relation between strings and their mean-ings, considering this a topic of open research.ST and GMB annotate words and phrases directly,recording derivations as (for example) Montague-style compositional semantic rules operating onCCG parses.Top-down verus bottom-up.
AMR annota-tors find it fast to construct meanings from thetop down, starting with the main idea of the sen-tence (though the AMR Editor allows bottom-upconstruction).
GMB and UCCA annotators workbottom-up.Editors, guidelines, genres.
These projectshave graphical sembanking tools (e.g., Basile et al(2012b)), annotation guidelines,5 and sembanksthat cover a wide range of genres, from news tofiction.
UNL and AMR have both annotated manyof the same sentences, providing the potential fordirect comparison.8 Future WorkSembanking.
Our main goal is to continuesembanking.
We would like to employ a largesembank to create shared tasks for natural lan-guage understanding and generation.
Thesetasks may additionally drive interest in theoreti-cal frameworks for probabilistically mapping be-tween graphs and strings (Quernheim and Knight,2012b; Quernheim and Knight, 2012a; Chiang etal., 2013).Applications.
Just as syntactic parsing hasfound many unanticipated applications, we expectsembanks and statistical semantic processors to beused for many purposes.
To get started, we areexploring the use of statistical NLU and NLG in5UNL guidelines: www.undl.org/unlsys/unl/unl2005184a semantics-based machine translation (MT) sys-tem.
In this system, we annotate bilingual Chi-nese/English data with AMR, then train compo-nents to map Chinese to AMR, and AMR to En-glish.
A prototype is described by Jones et al(2012).Disjunctive AMR.
AMR aims to canonicalizemultiple ways of saying the same thing.
We planto test how well we are doing by building AMRson top of large, manually-constructed paraphrasenetworks from the HyTER project (Dreyer andMarcu, 2012).
Rather than build individual AMRsfor different paths through a network, we will con-struct highly-packed disjunctive AMRs.
With thisapplication in mind, we have developed a guide-line6 for disjunctive AMR.
Here is an example:(o / *OR*:op1 (t / talk-01):op2 (m / meet-03):OR (o2 / *OR*:mod (o3 / official):arg1-of (s / sanction-01:arg0 (s2 / state))))official talksstate-sanctioned talksmeetings sanctioned by the stateAMR extensions.
Finally, we would liketo deepen the AMR language to include morerelations (to replace :mod and :prep-X, forexample), entity normalization (perhaps wik-ification), quantification, and temporal rela-tions.
Ultimately, we would like to also in-clude a comprehensive set of more abstractframes like ?Earthquake-01?
(:magnitude, :epi-center, :casualties), ?CriminalLawsuit-01?
(:de-fendant, :crime, :jurisdiction), and ?Pregnancy-01?
(:father, :mother, :due-date).
Projects likeFrameNet (Baker et al 1998) and CYC (Lenat,1995) have long pursued such a set.ReferencesO.
Abend and A. Rappoport.
2013.
UCCA: Asemantics-based grammatical annotation scheme.
InProc.
IWCS.C.
Baker, C. Fillmore, and J. Lowe.
1998.
The Berke-ley FrameNet project.
In Proc.
COLING.V.
Basile, J. Bos, K. Evang, and N. Venhuizen.
2012a.Developing a large semantically annotated corpus.In Proc.
LREC.6Disjunctive AMR guideline: amr.isi.edu/damr.1.0.pdfV.
Basile, J. Bos, K. Evang, and N. Venhuizen.
2012b.A platform for collaborative semantic annotation.
InProc.
EACL demonstrations.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.2003.
The Prague dependency treebank.
In Tree-banks.
Springer.A.
Butler and K. Yoshimoto.
2012.
Banking meaningrepresentations from treebanks.
Linguistic Issues inLanguage Technology, 7.S.
Cai and K. Knight.
2013.
Smatch: An accu-racy metric for abstract meaning representations.
InProc.
ACL.D.
Chiang, J. Andreas, D. Bauer, K. M. Hermann,B.
Jones, and K. Knight.
2013.
Parsing graphs withhyperedge replacement grammars.
In Proc.
ACL.D.
Davidson.
1969.
The individuation of events.In N. Rescher, editor, Essays in Honor of Carl G.Hempel.
D. Reidel, Dordrecht.M.
Dreyer and D. Marcu.
2012.
Hyter: Meaning-equivalent semantics for translation evaluation.
InProc.
NAACL.B.
Jones, J. Andreas, D. Bauer, K. M. Hermann, andK.
Knight.
2012.
Semantics-based machine trans-lation with hyperedge replacement grammars.
InProc.
COLING.H.
Kamp, J.
Van Genabith, and U. Reyle.
2011.
Dis-course representation theory.
In Handbook of philo-sophical logic, pages 125?394.
Springer.P.
Kingsbury and M. Palmer.
2002.
From TreeBank toPropBank.
In Proc.
LREC.D.
B. Lenat.
1995.
Cyc: A large-scale investment inknowledge infrastructure.
Communications of theACM, 38(11).R.
Martins.
2012.
Le Petit Prince in UNL.
In Proc.LREC.C.
M. I. M. Matthiessen and J.
A. Bateman.
1991.Text Generation and Systemic-Functional Linguis-tics.
Pinter, London.A.
Meyers, R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004.The NomBank project: An interim report.
In HLT-NAACL 2004 workshop: Frontiers in corpus anno-tation.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1).K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In ACL, Philadelphia, PA.185S.
Pradhan, E. Hovy, M. Marcus, M. Palmer,L.
Ramshaw, and R. Weischedel.
2007.
Ontonotes:A unified relational semantic representation.
In-ternational Journal of Semantic Computing (IJSC),1(4).D.
Quernheim and K. Knight.
2012a.
DAGGER: Atoolkit for automata on directed acyclic graphs.
InProc.
FSMNLP.D.
Quernheim and K. Knight.
2012b.
Towards prob-abilistic acceptors and transducers for feature struc-tures.
In Proc.
SSST Workshop.K.
Schuler.
2005.
VerbNet: A broad-coverage, com-prehensive verb lexicon.
Ph.D. thesis, University ofPennsylvania.S.
Shieber, F. C. N. Pereira, L. Karttunen, and M. Kay.1986.
Compilation of papers on unification-basedgrammar formalisms.
Technical Report CSLI-86-48, Center for the Study of Language and Informa-tion, Stanford, California.H.
Uchida, M. Zhu, and T. Della Senta.
1996.
UNL:Universal Networking Language?an electronic lan-guage for communication, understanding and col-laboration.
Technical report, IAS/UNU Tokyo.H.
Uchida, M. Zhu, and T. Della Senta.
1999.
Agift for a millennium.
Technical report, IAS/UNUTokyo.N.
Venhuizen, V. Basile, K. Evang, and J. Bos.
2013.Gamification for word sense labeling.
In Proc.IWCS.R.
Weischedel, E. Hovy, M. Marcus, M. Palmer,R.
Belvin, S. Pradhan, L. Ramshaw, and N. Xue.2011.
OntoNotes: A large training corpus for en-hanced processing.
In J. Olive, C. Christianson, andJ.
McCary, editors, Handbook of Natural LanguageProcessing and Machine Translation.
Springer.Y.
W. Wong and R. J. Mooney.
2006.
Learning for se-mantic parsing with statistical machine translation.In Proc.
HLT-NAACL.186
