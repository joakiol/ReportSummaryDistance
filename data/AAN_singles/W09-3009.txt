Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 56?59,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPExplorations in Automatic Image Annotation using Textual FeaturesChee Wee LeongComputer Science & EngineeringUniversity of North Texascheeweeleong@my.unt.eduRada MihalceaComputer Science & EngineeringUniversity of North Texasrada@cs.unt.eduAbstractIn this paper, we report our work onautomatic image annotation by combiningseveral textual features drawn from thetext surrounding the image.
Evaluation ofour system is performed on a dataset ofimages and texts collected from the web.We report our findings through compar-ative evaluation with two gold standardcollections of manual annotations on thesame dataset.1 IntroductionDespite the usefulness of images in expressingideas, machine understanding of the meaning ofan image remains a daunting task for comput-ers, as the interplay between the different visualcomponents of an image does not conform to anyfixed pattern that allows for formal reasoning ofits semantics.
Often, the machine interpretation ofthe concepts present in an image, known as auto-matic image annotation, can only be inferred byits accompanying text or co-occurrence informa-tion drawn from a large corpus of texts and im-ages (Li and Wang, 2008; Barnard and Forsyth,2001).
Not surprisingly, humans have the innateability to perform this task reliably, but given alarge database of images, manual annotation isboth labor-intensive and time-consuming.Our work centers around the question : Pro-vided an image with its associated text, can weuse the text to reliably extract keywords that rel-evantly describe the image ?
Note that we are notconcerned with the generation of keywords for animage, but rather their extraction from the relatedtext.
Our goal eventually is to automate this taskby leveraging on texts which are naturally occur-ring with images.
In all our experiments, we onlyconsider the use of nouns as annotation keywords.2 Related WorkAlthough automatic image annotation is a popu-lar task in computer vision and image processing,there are only a few efforts that leverage on themultitude of resources available for natural lan-guage processing to derive robust linguistic basedimage annotation models.
Most of the work hasposed the annotation task as a classification prob-lem, such as (Li and Wang, 2008), where imagesare annotated using semantic labels associated toa semantic class.The most recent work on image annotation us-ing linguistic features (Feng and Lapata, 2008)involves implementing an extended version ofthe continuous relevance model that is proposedin (Jeon et al, 2003).
The basic idea underlyingtheir work is to perform annotation of a test im-age by using keywords shared by similar trainingimages.
Evaluation of their system performanceis based on a dataset collected from the news do-main (BBC).
Unlike them, in this paper, we at-tempt to perform image annotation on datasetsfrom unrestricted domains.
We are also interestedin extending the work pursued in (Deschacht andMoens, 2007), where visualness and salience areproposed as important textual features for discov-ering named entities present in an image, by ex-tracting other textual features that can further im-prove existing image annotation models.3 Data SetsWe use 180 images collected from the Web, frompages that have a single image within a specifiedsize range (width and height of 275 to 1000 pix-els).
110 images are used for development, whilethe remaining 70 are used for test.
We create twodifferent gold standards.
The first, termed as Intu-itive annotation standard (GSintuition), presents auser with the image in the absence of its associatedtext, and asks the user for the 5 most relevant anno-tations.
The second, called Contextual annotationstandard (GScontext), provides the user with a listof candidates1 for annotation, with the user free tochoose any of the candidates deemed relevant todescribe the image.
The user, however, is not con-1Union of candidates proposed by all systems participat-ing in the evaluation, including the baseline system56strained to choose any candidate word, nor is sheobligated to choose a specified number of candi-dates.
For each image I in the evaluation set, weinvited five users to perform the annotation taskper gold standard.
The agreement is 7.78% forGSintuition and 22.27% for GScontext, where weconsider an annotation that is proposed by three ormore users as one that is being agreed upon.
Theunion of their inputs forms the set GSintuition(I)and GScontext(I) respectively.
We do not considerimage captions for use as a gold standard here dueto their absence in many of the images ?
a ran-dom sampling of 15 images reveals that 7 of themlack captions.
Contrary to their use as a proxy forannotation keywords in (Feng and Lapata, 2008;Deschacht and Moens, 2007), where evaluation isperformed on datasets gleaned from authoritativenews websites, most captions in our dataset arenot guaranteed to be noise free.
However, they areused as part of the text for generating annotationswhere they exist.4 Automatic Image AnnotationWe approach the task of automatic image anno-tation using four methods.
Due to the orthogo-nal nature in their search for keywords, the out-put for each method is generated separately andlater combined in an unsupervised setting.
How-ever, all four methods perform their discriminationof words by drawing information exclusively fromthe text associated to the image, using no imagevisual features in the process.4.1 Semantic Cloud (Sem)Every text describes at least one topic that can besemantically represented by a collection of words.Intuitively, there exists several ?clouds?
of seman-tically similar words that form several, possiblyoverlapping, sets of topics.
Our task is to se-lect the dominant topic put forward in the text,with the assumption that such a topic is beingrepresented by the largest set of words.
We usean adapted version of the K-means clustering ap-proach, which attempts to find natural ?clusters?of words in the text by grouping words with a com-mon centroid.
Each centroid is the semantic cen-ter of the group of words and the distance betweeneach centroid and the words are approximated byESA (Gabrilovich and Markovitch, 2007).
Fur-ther, we perform our experiments with the follow-ing assumptions : (1) To maximize recall, we as-sume that there are only two topics in every text.
(2) Every word or collocation in the text must beclassified under one of these two topics, but notboth.
In cases, where there is a tie, the classi-fication is chosen randomly.
For each dominantcluster extracted, we rank the words in decreasingorder of their ESA distance to the centroid.
To-gether, they represent the gist of the topic and areused as a set of candidates for labeling the image.4.2 Lexical Distance (Lex)Words that are lexically close to the picture in thedocument are generally well-suited for annotat-ing the image.
The assumption is drawn from theobservation that the caption of an image is usu-ally located close to the image itself.
For imageswithout captions, we consider words surroundingthe image as possible candidates for annotation.Whenever a word appears multiple times withinthe text, its occurrence closest to the image is usedto calculate the lexical distance.
To discriminateagainst general words, we weigh the Lexical Dis-tance Score (LDS) for each word by its tf * idfscore as in the equation shown below :LDS(Wi) = tf * idf(Wi)/LS(Wi) (1)where LS(Wi) is the minimum lexical distance ofWi to the image, and idf is calculated using countsfrom the British National Corpus.4.3 Saliency (Sal)To our knowledge, all word similarity metrics pro-vide a symmetric score between a pair of wordsw1 and w2 to indicate their semantic similarity.Intuitively, this is not always the case.
In psy-cholinguistics terms, uttering w1 may bring intomind w2, while the appearance of w2 without anycontextual clues may not associate with w1 at all.Thus, the degree of similarity of w1 with respectto w2 should be separated from that of w2 withrespect to w1.
We use a directional measure ofsimilarity:DSim(wi, wj) =CijCi?
Sim(wi, wj) (2)where Cij is the count of articles in Wikipediacontaining words wi and wj , Ci is the count of ar-ticles containing words wi, and Sim(wi, wj) is thecosine similarity of the ESA vectors representingthe two words.
The directional weight (Cij /Ci)amounts to the degree of association of wi with re-spect to wj .
Using the directional inferential sim-ilarity scores as directed edges and distinct wordsas vertices, we obtain a graph for each text.
Thedirected edges denotes the idea of ?recommenda-tion?
where we sayw1 recommendsw2 if and onlyif there is a directed edge from w1 to w2, withthe weight of the recommendation being the di-rectional similarity score.
By employing the graphiteration algorithm proposed in (Mihalcea and Ta-rau, 2004), we can compute the rank of a vertex in57the entire graph.
The output generated is a sortedlist of words in decreasing order of their ranks,which serves as a list of candidates for annotatingthe image.
Note that the top-ranked word must in-fer some or all of the words in the text.Table 1: An image annotation exampleSem symptoms, treatment, medical treat-ment, medical care, sore throat, fluids,cough, tonsils, strep throat, swabLex strep throat, cotton swab, lymph nodes,rheumatic fever, swab, strep, fever, sorethroat, lab, scarlet feverSal strep, swab, nemours, teens, ginger ale,grapefruit juice, sore, antibiotics, kids,feverPic throat, runny nose, strep throat, sorethroat, hand washing, orange juice, 24hours, medical care, beverages, lymphnodesCombined treatment, cough, tonsils, swab, fluids,strep throatDoc Title strep throattf * idf strep, throat, antibiotics, symptoms,child, swab, fever, treatment, teens,nemoursGScontext medical care, medical treatment, doc-tor, cotton swab, treatment, tonsils, sorethroat, swab, throat, sore, sample, symp-toms, throat, cough, medication, bacte-ria, lab, scarlet fever, strep throat, teens,culture, kids, child, streptococcus, doctor,strepGSintuition tongue, depressor, exam, eyes, cartoon,doctor, health, child, tonsils, fingers, hair,mouth, dentist, sample, cloth, curly, tip,examine4.4 Picturable Cues (Pic)Some words are more picturable than others.
Forinstance, it is easy to find a picture that describesthe word banana than another word paradigm.Clearly, picturable words in the associated text ofan image are natural candidates for labeling it.
Un-like the work in (Deschacht and Moens, 2007),we employ a corpus-based approach to computeword to word similarity.
We collect a list of 200manually-annotated words2 that are deemed to bepicturable by humans.
We use this list of wordsas our set of seed words, Sseed.
We then iterate abootstrapping process where each word in the textis compared to every word in the set of seed words,and any word having a maximum ESA score of2http://simple.wikipedia.org/wiki/Wikipedia:Basic English picture wordlistgreater than 0.95 is added to Sseed.
Similarly, themaximum ESA score of each word over all Sseedwords is recorded.
This is the picturability scoreof the word.5 Experiments and EvaluationsWe investigate the performance of each of the fourannotation methods individually, followed by acombined approach using all of them.
In the in-dividual setting, we simply obtain the set of candi-dates proposed by each method as possible anno-tation keywords for the image.
In the unsupervisedcombined setting, only the labels proposed by allindividual methods are selected, and listed in re-verse order of their combined rankings.We allow each system to produce a re-rankedlist of top k words to be the final annotations for agiven image.
A system can discretionary generateless (but not more) than k words that is appropri-ate to its confidence level.
Similar to (Feng andLapata, 2008), we evaluate our systems using pre-cision, recall and F-measure for k=10, k=15 andk=20 words.For comparison, we also implemented twobaselines systems: tf * idf and Doc Title, whichsimply takes all the words in the title of theweb page and uses them as annotation labels forthe image.
In the absence of a document title,we use the first sentence in the document.
Theresults for GSintuition and GScontext are tabu-lated in Tables 2 and 3 respectively.
We fur-ther illustrate our results with an annotation ex-ample (an image taken from a webpage discussingstrep throat among teens) in Table 1.
Words inbold matches GScontext while those underlinedmatches GSintuition.6 DiscussionAs observed, the system implementing the Se-mantic Cloud method significantly outperformsthe rest of the systems in terms of recall and F-measure using the gold standard GSintuition.
Theunsupervised combined system yields the high-est precision at 16.26% (at k=10,15,20) but ata low recall of 1.52%.
Surprisingly, the base-line system using tf * idf performs relatively wellacross all the experiments using the gold stan-dard GSintuition, outperforming two of our pro-posed methods Salience (Sal) and PicturabilityCues (Pic) consistently for all k values.
The otherbaseline, Doc Title, records the highest precisionat 16.33% at k=10 with a low recall of 3.81%.
Fork=15 and k=20, the F-measure scored 6.31 and6.29 respectively, both lower than that scored bytf * idf.58Table 2: Results for Automatic Image Annotation for GSintuition.
In both Tables 2 and 3, statisticallysignificant results are marked with ?
(measured against Doc Title, p<0.05, paired t-test), ?
(measuredagainst tf*idf, p<0.1, paired t-test), ?
(measured against tf*idf, p<0.05, paired t-test).GSintuitionk=10 k=15 k=20P R F P R F P R FSem 11.71 6.25?
8.15 11.31 8.91??
9.97??
10.36 9.45??
9.88?
?Lex 9.00 4.80 6.26 7.33 5.86 6.51 7.14 7.62 7.37Sal 4.57 2.43 3.17 6.28 5.03 5.59 6.38 6.78 6.57Pic 7.14 3.81 4.97 6.09 4.87 5.41 5.64 6.02 5.82Combined 16.26 1.52 2.78 16.26?
1.52 2.78 16.26?
1.52 2.78Doc Title 16.33 3.81 6.18 15.56 3.96 6.31 15.33 3.96 6.29tf * idf 9.71 5.18 6.76 8.28 6.63 7.36 7.14 7.62 7.37Table 3: Results for Automatic Image Annotation for GScontextGScontextk=10 k=15 k=20P R F P R F P R FSem 71.57 26.20??
38.36??
68.00 37.34??
48.21??
64.56 47.17??
54.51?
?Lex 61.00 22.23 32.59 58.95 32.37 41.79 56.92 41.68 48.12Sal 46.42 16.99 24.88 51.14 28.08 36.25 54.59 39.80 46.04Pic 51.71 21.12 29.99 56.85 31.22 40.31 56.35 41.26 47.64Combined 75.60??
4.86 9.13 75.60??
4.86 9.13 75.60??
4.86 9.13Doc Title 32.67 5.23 9.02 32.33 5.64 9.60 32.15 5.70 9.68tf * idf 55.85 20.44 29.93 54.19 29.75 38.41 49.07 35.93 41.48When performing evaluations using the goldstandard GScontext, significantly higher precision,recall and F-measure values are scored by all thesystems, including both baselines.
This is perhapsdue to the availability of candidates that suggestsa form of cued recall, rather than free recall, asis the case with GSintuitive.
The user is able toannotate an image with higher accuracy e.g.
la-belling a Chihuahua as a Chihuahua instead of adog.
Again, the Semantic Cloud method contin-ues to outperform all the other systems in terms ofrecall and F-measure consistently for k=10, k=15and k=20 words.
A similar trend as observed us-ing the gold standard of GSintuition is seen here,where again our combined system favors precisionover recall at all values of k.A possible explanation for the poor perfor-mance of the Saliency method is perhaps due toover-specific words that infer all other words in thetext, yet unknown to the knowledge of most hu-man annotators.
For instance, the word Mussolini,referring to the dictator Benito Mussolini, was notselected as an annotation for an image showingscenes of World War II depicting the Axis troops,though it suggests the concepts of war, World WarII and so on.
The Pic method is also not perform-ing as well as expected under the two gold anno-tation standards, mainly due to the fact that it fo-cuses on selecting picturable nouns but not nec-essarily those that are semantically linked to theimage itself.7 Future WorkThe use of the semantic cloud method to generateautomatic annotations is promising.
Future workwill consider using additional semantic resourcessuch as ontological information and ency-clopaedic knowledge to enhance existing models.We are also interested to pursue human knowledgemodeling to account for the differences in annota-tors in order create a more objective gold standard.ReferencesKobus Barnard and David Forsyth.
2001.
Learning the se-mantics of words and pictures.
In Proceedings of Interna-tional Conference on Computer Vision.Koen Deschacht and Marie-Francine Moens.
2007.
Textanalysis for automatic image annotation.
In Proceedingsof the Association for Computational Linguisticd.Yansong Feng and Mirella Lapata.
2008.
Automatic imageannotation using auxiliary text information.
In Proceed-ings of the Association for Computational Linguistics.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Comput-ing semantic relatedness using wikipedia-based explicitsemantic analysis.
In International Joint Conferences onArtificial Intelligence.J Jeon, V Lavrenko, and R Manmatha.
2003.
Automatic im-age annotation and retrieval using cross-media relevancemodels.
In Proceedings of the ACM SIGIR Conference onResearch and Development in Information Retrieval.Jia Li and James Wang.
2008.
Real-time computerized an-notation of pictures.
In Proceedings of International Con-ference on Computer Vision.Rada Mihalcea and Paul Tarau.
2004.
Textrank: Bringingorder into texts.
In in Proceedings of Empirical Methodsin Natural Language Processing.59
