Merging Stories with Shallow SemanticsFiona McNeillSchool of InformaticsUniv.
of Edinburghf.j.mcneill@ed.ac.ukHarry HalpinSchool of InformaticsUniv.
of Edinburghh.halpin@ed.ac.ukEwan KleinSchool of InformaticsUniv.
of Edinburghewan@inf.ed.ac.ukAlan BundySchool of InformaticsUniv.
of Edinburghbundy@inf.ed.ac.ukAbstractWe demonstrate a proof-of-concept sys-tem that uses a shallow chunking-basedtechnique for knowledge extraction fromnatural language text, in particular lookingat the task of story understanding.
Thistechnique is extended with a reasoningengine that borrows techniques from dy-namic ontology refinement to discover thesemantic similarity of stories and to mergethem together.1 IntroductionMany NLP applications would benefit from theavailability of broad-coverage knowledge extrac-tion from natural language text.
Despite some re-cent advances in this direction (Bos et al, 2004), itis still the case that it is hard to obtain deep seman-tic analyses which are accurate enough to supportlogical inference (Lev et al, 2004).Our problem can be stated in abstract terms.Given a formalism F for the semantic representa-tion of natural language, such as first-order clausesin predicate calculus, and a set of sentences S, givea translation function T (S) ?
F .
The goal ofsuch a translation would be to solve a problemP (such as paraphrasing or question-answering)where F allows P to be solved by some reason-ing process, or else the domain exhibits a type ofstructure easily represented in the formalism F .If we accept that current parsing technologycannot reliably combine accurate semantic anal-ysis with robustness, then the question ariseswhether ?noisy?
semantics can be ameliorated us-ing some other techniques.
In this paper, we adoptthe hypothesis that methods drawn from dynamicontology refinement (McNeill et al, 2004) can in-deed help with this task.
In the limit, we wouldlike to be able to show that semantic content drawnfrom a wide variety of sources can be comparedand merged to reveal the shared common ground.However, in this paper we have limited ourselvesto a much more modest goal, namely merging andcomparing semantic content from a set of variantsof a single story.1We obtained the variants by asking adults toretell the story, based on hearing the original, orreading it themselves.
We have developed a sys-tem that can take in any number of such storiesand produce a merged version of the stories.
Ourre-tellers were instructed not to elaborate upon orintentionally change the original and consequentlythe stories are fairly similar, not just in meaningbut to an extent also in wording.In the next two sections, we will first describehow semantic clauses are extracted from text, andsecond, how clauses obtained from different textsare merged.2 Extracting Clauses from TextThe method we have adopted for extracting first-order clauses from text can be called ?semanticchunking.?
This seems an appropriate term fortwo reasons.
First, we use a syntactic chunkerto identify noun groups and verb groups (i.e.
non-recursive clusters of related words with a noun orverb head respectively).
Second, we use a cas-cade of finite state rules to map from this shallowsyntactic structure into first-order clauses; this cas-cade is conceptually very similar to the chunkingmethod pioneered by Abney?s Cass chunker (Ab-ney, 1996).The text processing framework we have useddraws heavily on a suite of XML tools developed1We used a simplified version of Oscar Wilde?s fairy storyThe Selfish Giant.36 KRAQ06for generic XML manipulation (LTXML (Thomp-son et al, 1997)) as well as NLP-specific XMLtools (LT-TTT (Grover et al, 2000), LT-CHUNK(Finch and Mikheev, 1997)).
More recently, sig-nificantly improved upgrades of these tools havebeen developed, most notably the program lx-transduce, which performs rule-based transduc-tions of XML structures.
We have used lxtransduceboth for the syntactic chunking (based on rule de-veloped by Grover) and for construction of seman-tic clauses.The main steps in the processing pipeline are asfollows:1.
Words and sentences are tokenized.2.
The words are tagged for their part of speechusing the CandC tagger (Clark and Curran,2004) and the Penn Treebank tagset.3.
Pronoun resolution is carried out usingthe Glencova Pronoun Resolution algorithm(Halpin et al, 2004), based on a series ofrules similar to the CogNIAC engine (Bald-win, 1997), but without gender information-based rules since this is not provided by thePenn Treebank tagset.4.
The words are then reduced to their morpho-logical stem (lemma) using Morpha (Minnenet al, 2001).5.
The lxtransduce program is used to chunk thesentence into verb groups and noun groups.6.
In an optional step, words are tagged asNamed Entities, using the CandC taggertrained on MUC data.7.
The partially chunked sentences are selec-tively mapped into semantic clauses in a se-ries of steps, described in more detail below.8.
The XML representation of the clauses is con-verted using an XSLT stylesheet into a moreconventional syntactic format for use by Pro-log or other logic-based systems.The output of the syntactic processing is anXML file containing word elements which areheavily annotated with attributes.
FollowingCoNLL BIO notation (Tjong et al, 2000), chunkinformation is recorded at the word level.
Headsof noun groups and verb groups are assigned se-mantic tags such as arg and rel respectively.
Inaddition, other semantically relevant forms suchas conjunction, negation, and prepositions are alsotagged.
Most other input and syntactic infor-mation is discarded at this stage.
However, wemaintain a record through shared indices of whichterms belong to the same chunks.
This is used, forinstance, to build coordinated arguments.Regular expressions over the semanticallytagged elements are used to compose clauses, us-ing the heuristic that an arg immediately preced-ing a pred is the subject of the clause, while argsfollowing the pred are complements.
Since theheads of verb groups are annotated for voice, wecan treat passive clauses appropriately, yielding arepresentation that is equivalent to the active con-gener.
We also implement simple heuristics thatallow us to capture simple cases of control andverb phrase ellipsis in many cases.3 Knowledge Refinement and MergingOnce the clauses have been extracted from thetext, each story becomes a list of predicates, rep-resenting verbs, each with a number of arguments(possibly zero), representing nouns.
Two storiescan thus be compared by considering how theclauses from one story relate to the clauses fromanother story.
This is done both by consideringhow the predicates (verbs) from one story relate tothose from another story and also by consideringthe arguments (nouns) that these related predicatesconnect.
This allows us to consider not just thesimilarity of the words used in the story but also,to some extent, the structure of the sentences.The aim of merging is to build up, initially, aworking merged story that includes all aspects ofeach story so far; then, when all stories have beenmerged, to refine the working merged story by re-moving aspects of it that are considered to be pe-ripheral to the main core.
The output is a singlestory in the same format as the inputs, and whichreflects common elements from across the set ofvariants.If text is represented in such clause form, thenthe number of ways in which these clausal rep-resentations of the story can differ is strictly lim-ited.
Clauses have only two attributes: predicatesand arguments.
The predicates may find an exactmatch, an inexact match or no match.
If the predi-cates find some kind of match, their arguments canthen be examined.
Each of these will find an exact,inexact, or no match with the corresponding ar-37 KRAQ06gument in the related predicate; additionally, theirordering and number may be different.
Thus it ispossible to create an exhaustive list of the possibledifferences between clauses.
We currently con-sider only WordNet information concerning syn-onyms, hypernyms and hyponyms when determin-ing matches: we do not perform inference usingantonyms, for example, nor do we consider impli-cation cases.The techniques that are used in the mergingprocess were inspired by work on dynamic on-tology refinement (McNeill et al, 2004), whichdeals with the problem of reasoning failure causedby small inconsistencies between largely similarontologies.
This is achieved by analysing onto-logical objects that were expected to, but do not,match, and diagnosing and patching these mis-matches through consideration of how they dif-fer through reasoning.
Examples of mismatchesthat are common between similar ontologies arechanged names of predicates (often to sub- orsuper-types), changed arity, and changed types ofarguments.
These types of ontological mismatchesare currently handled by our system, since in thedomain of stories people often use different namesor different levels of description for things.
Theapplication of these techniques for determiningdifferences and similarities between the story rep-resentations therefore forms the basis of the merg-ing process.In order to merge a new story with the currentworking merged story (WMS), the facts in the WMSare examined one by one in an attempt to matchthem to a fact in the story to be merged.
Such amatch may be exact (the predicates and all theirarguments are identical), inexact (the predicateshave the same name but their arguments differ),similar (the predicates are synonyms) or related(the predicates are hyponyms or hypernyms).
In-formation about synonyms, hyponyms and hyper-nyms is extracted from WordNet and used as thetype hierarchy for our refinement (see Section 5;for an explanation of our usage of WordNet, seethe WordNet project (Miller, 1995) for general de-tails).
Another potential kind of match is wherethe arguments match but no link can be found be-tween the predicates; however, this is not consid-ered in the current system.
If a match of any kindis found for a predicate from the WMS, the predi-cate from the new story with which it matches isappended to its entry in the WMS.
Each entry in theWMS is annotated with a score to indicate in howmany stories a match of some kind has been foundfor it.
For example, an entry in the WMS, ([1]play(child)) may find an inexact match withcavort(child) in a new story, to create an entryof ([2] play(child), cavort(child)) in thenew WMS.Once all the facts in the WMS have been exam-ined and matched where possible, there will usu-ally be some facts in the story to be merged thathave not been found as a match for anything in theWMS.
These should not be ignored; they have nomatch with anything thus far, but it may be thatstories to be merged in future will find a matchwith them.
Thus, they are added to the mergedstory with a score of 1.
The initial merged storyis found by simply annotating the first story to bemerged so that each entry has a score of 1.
Itis possible, but not necessary, to provide a rangevalue to the merging process, so that matches areonly sought in the given vicinity of the fact in theWMS.
If no range value is given, this is set to be ar-bitrarily large so that all of the story to be mergedis examined.Once all of the stories to be merged have beenexamined, we have a complete WMS, which needsto be processed to produce the merged output.
Athreshold value is used to determine which of theseshould be immediately discarded: anything with ascore less than the threshold.
Those with a scoreof more than or equal to the threshold must be pro-cessed so that each is represented by a single fact,rather than a list of different versions of the fact.
Ifall versions of the fact are identical, this single ver-sion is the output.
Otherwise, both a ?canonical?name and ?canonical?
arguments for a fact mustbe calculated.
In the current system, a simple ap-proach is taken to this.
For the predicate, if thereis more than one version, then the most commonlyoccurring one is chosen as the canonical represen-tative.
If there are two or more that are jointly themost commonly occurring, then the most specificof the names is chosen (i.e., the one that is lowestin the class hierarchy).
When choosing the argu-ments for the merged predicate, any argument thathas a match in at least one other version of thepredicate is included.
If the match is inexact ?i.e., the arguments are not of the same class, butare of related classes ?
then the most specific ofthe classes is chosen.38 KRAQ064 Worked ExampleWe now work through a simplified example to il-lustrate the merging process.
Consider the follow-ing three lists of facts, which are drawn from dif-ferent retellings of our example story:Story 1: come(child), play(garden),visit(friend), forget(friend),come(giant), yell(child)Story 2: go(giant), visit(friend),be(giant), come(child), play(garden)Story 3: be(giant), come(giant),play(garden),bellow(child,anger,giant),happy(giant)The first WMS (working merged story) is pro-duced by marking up the first story:([1] come(child)),([1] play(garden)),([1] visit(friend)),([1] forget(friend)),([1] come(giant)),([1] yell(child))This is then merged with the second story.
Thefirst fact of Story 1, come(child), matches exactlywith the fourth fact of the Story 2; the fifth factmatches inexactly with the fourth fact of the Story2, and so on.
The resulting WMS is:([2] come(child), come(child)),([2] play(garden), play(garden)),([2] visit(friend) visit(friend)),([1] forget(friend)),([1] come(giant)),([1] yell(child)),([1] come(giant)),([1] go(giant)),([1] be(giant))This is then merged with Story 3 to produce:([3] come(child), come(child),come(giant)),([3] play(garden), play(garden),play(garden)),([2] visit(friend) visit(friend)),([1] forget(friend)),([1] come(giant)),([2] yell(child),bellow(child,anger,giant)),([1] come(giant)),([1] go(giant)),([2] be(giant), be(giant)),([1] happy(giant))We then proceed to merge all the automat-ically extracted knowledge representations ofthe three stories.
To create the output mergedstory, those predicates with a score of 1 areignored.
The others are each merged to pro-duce a single predicate.
For example, ([3]come(child), come(child), come(giant))becomes come(child): giant does not matchwith any other argument and is dropped.
([2]yell(child), bellow(child,anger,giant))becomes yell(child) because yell is a subclassof bellow, and thus preferred, and child is theonly argument that has a match in both facts.
Thusthe resulting merged story is:come(child),play(garden),visit(friend),yell(child),be(giant)It is clear from this example that our current ap-proach is, at times, naive.
For example, the deci-sions about which arguments to include in outputfacts, and how to order facts that are unmatchedin working merged stories could be made signifi-cantly more effective.
We view this current systemas a proof of concept that such an approach canbe useful; we certainly do not consider the systemto be complete approach to the problem.
Furtherwork on the system would result in improved per-formance.5 Extracting Ontological Informationfrom WordNetIn order to perform some of the tasks involved inmerging and matching, it is necessary to have in-formation about how words are related.
We extractthis information from WordNet by getting the on-tology refinement engine to call WordNet with aword and retrieve both its synset (i.e., synomymset) and its hyponyms and hypernyms.
We col-lect these for every sense of the word, since ournatural language pipeline currently does not in-clude word-sense disambiguation.
When it is nec-essary during the merging process to obtain in-formation about whether two words are related(i.e., when two words do not match exactly), weextract synonym and hyper/hyponym informationfrom WordNet for these words and examine it todiscover whether an exact match exists or not.
Wetreat synonyms as equivalent, and we treat hyper-nym and hyponym synsets as the super- and sub-type of a word respectively, and then traverse thetype hierarchy for exact matches.
To avoid spu-rious equivalence we use a bound to restrict thesearch, and from our experience a bound of twotype-levels in either direction and a stop-list of?upper ontology?
types yields good results.6 Related WorkThis work breaks some new ground by being thefirst to use dynamic refinement to compare andmerge information from natural language texts.
Ingeneral, recent work in natural language process-ing has currently relied heavily on ?purely statisti-cal?
methods for tasks such as text similarity and39 KRAQ06summarization.
However, there is also a rich log-ical tradition in linguistic semantics, and work inthis vein can bring the two closer together.Current work in story understanding is focus-ing on the use of logical forms, yet these are notextracted from text automatically (Mueller, 2003).The natural language processing and story con-version pipeline are improvements over a pipelinethat was shown to successfully compare stories ina manner similar to a teacher (Halpin et al, 2004).The merging task is a more logic-based ap-proach than similar techniques like informationfusion used in multi-document summarization(Barzilay et al, 1999).
Our approach has somefeatures in common with (Wan and Dale, 2001),however, we have chosen to focus exclusively onmerger at the semantic level, rather than trying toalso incorporate syntactic structure.There has also been a revival in using weightedlogical forms in structured relational learning,such as Markov Logic Networks (Domingos andKok, 2005), and this is related to the scoringof facts used by the current system in mergingtexts.
As mentioned at the beginning of this paper,the conversion of unrestricted text to some logi-cal form has experienced a recent revival recently(Bos et al, 2004).
Although our approach deliber-ately ignores much semantic detail, this may becompensated for by increased robustness due tothe reliance on finite-state methods for semantictranslation and chunking.7 Further WorkThe work we have done thus far suggests many av-enues for further work.
One obvious improvementwould be to enable the system to deal with morecomplex input, so that stories could be representedwith nested predicates and with a more complexnotion of time.
Time can be conceived of ontologi-cally in a number of differing manners with highlydiffering properties, and relating these notions oftime to the extraction of tense (which the lxtrans-duce-based chunker currently does automatically)would be a fruitful task (Hayes, 1996).
Makingdecisions about what to include in a merged storyis currently done in a fairly naive manner.
Fur-ther research into what constituted a good mergedstory and under what circumstances it is advanta-geous to be sceptical or generous as to what shouldbe included in the merged story, would allow thisto become much more effective.
Once the systemhas been suitably improved, it should be tested ona more complex domain, such as news stories.
Fi-nally, the primary benefit of the use of knowledgerepresentation is the possibility of using inference.The current system could easily take advantageof an external knowledge-base of domain-specificfacts and rules to aid refinement and merging.We have not implemented a baseline for the sys-tem using purely word-based statistical features,such as reducing the words to the their morpho-logical stem and then using WordNet synsets tocompare the stories without any predicate struc-ture.
This is because at this stage in developmentthe extraction of the correct clauses is itself thegoal of the task.
If connected to larger system,comparison with a purely statistical model wouldbe useful.
However, we would hazard a guess thatin the domain of computational story understand-ing, it is unlikely that purely statistical methodswould work well, since stories by their nature con-sist of events involving the actions of characters ina particular temporal order, and this type of struc-tural complexity would seem to be best accountedfor by some structure-preserving features that at-tempt to model and extract these events explicitly.8 ConclusionAlthough many questions remain unanswered,the development of the current system demon-strates that this kind of refinement-based approachto matching and merging texts can be producepromising results.
Much could be done to im-prove the effectiveness of both the clause extrac-tion and the merging components of the system,and the breadth of task that these techniques havebeen tested on remains very narrow.
Nevertheless,this work represents a reasonably successful firstinvestigation of the problem, and we intend to useit as the basis for further work.ReferencesSteven Abney.
1996.
Partial parsing via finite-state cascades.
Natural Language Engineering,2(4):337?344.Breck Baldwin.
1997.
CogNIAC: A High PrecisionPronoun Resolution Engine.
In Operational Factorsin Practical, Robust Anaphora Resolution for Unre-stricted Texts (ACL-97 workshop), pages 38?45.R.
Barzilay, K. McKeown, and M. Elhadad.
1999.
In-formation fusion in the context of multi-documentsummarization.
In In Proceedings of Association for40 KRAQ06Computational Linguistics, pages 550?557, Mary-land.Johan Bos, Stephen Clark, Mark Steedman, James Cur-ran, and Julia Hockenmaier.
2004.
Wide-coveragesemantic representations from a CCG parser.
In InProceedings of the 20th International Conferenceon Computational Linguistics (COLING ?04), pages1240?1246, Geneva, Switzerland.Stephen Clark and James Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In Pro-ceedings of the 42nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2004),pages 104?111, Barcelona, Spain.Pedro Domingos and Stanley Kok.
2005.
Learning thestructure of Markov Logic Networks.
In Proceed-ings of the International Conference on MachineLearning, pages 441?448, Bonn.Steve Finch and Andrei Mikheev.
1997.
A workbenchfor finding structure in texts.
In Walter Daelemansand Miles Osborne, editors, Proceedings of the FifthConference on Applied Natural Language Process-ing (ANLP-97).
Washington D.C.Claire Grover, Colin Matheson, Andrei Mikheev, andMarc Moens.
2000.
LT TTT?a flexible tokenisa-tion tool.
In LREC 2000?Proceedings of the 2ndInternational Conference on Language Resourcesand Evaluation, pages 1147?1154.Harry Halpin, Johanna Moore, and Judy Robertson.2004.
Automatic analysis of plot for story rewriting.In Proceedings of Empirical Methods in NaturalLanguage Processing, pages 127?133, Barcelona,Spain.Pat Hayes.
1996.
A catalog of temporal theories.Technical Report UIUC-BI-AI-96-01, University ofIllinois.Iddo Lev, Bill MacCartney, Christopher D. Manning,and Roger Levy.
2004.
Solving logic puzzles: Fromrobust processing to precise semantics.
In 2nd Work-shop on Text Meaning and Interpretation at ACL2004, pages 9?16.Fiona McNeill, Alan Bundy, and Chris Walton.
2004.Facilitating agent communication through detecting,diagnosing and refining ontological mismatch.
InProceedings of the KR2004 Doctoral Consortium.AAAI Technical Report.George Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 11(38):39?41.Guido Minnen, John Carroll, and David Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?203.Erik T. Mueller.
2003.
Story understanding throughmulti-representation model construction.
In GraemeHirst and Sergei Nirenburg, editors, Text Meaning:Proceedings of the HLT-NAACL 2003 Workshop,pages 46?53, East Stroudsburg, PA. Association forComputational Linguistics.Henry Thompson, Richard Tobin, David McKelvie,and Chris Brew.
1997.
LT XML: Software API andtoolkit for XML processing.Erik F. Tjong, Kim Sang, and Sabine Buchholz.2000.
Introduction to the CoNLL-2000 shared task:Chunking.
In Proceedings of the Conference onNatural Language Learning (CoNLL-2000).
Lisbon,Portugal.Stephen Wan and Robert Dale.
2001.
Merging sen-tences using shallow semantic analysis: A first ex-periment.
In Proceedings of the 2001 AustralasianNatural Language Processing Workshop, Sydney,April.
Macquarie University.41 KRAQ06
