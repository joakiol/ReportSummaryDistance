Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188?194,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMulti-Modal Representations for Improved Bilingual Lexicon LearningIvan Vuli?cLanguage Technology Lab, DTALUniversity of Cambridgeiv250@cam.ac.ukDouwe KielaComputer LaboratoryUniversity of Cambridgedk427@cam.ac.ukStephen ClarkComputer LaboratoryUniversity of Cambridgesc609@cam.ac.ukMarie-Francine MoensDepartment of Computer ScienceKU Leuvensien.moens@cs.kuleuven.beAbstractRecent work has revealed the potential ofusing visual representations for bilinguallexicon learning (BLL).
Such image-basedBLL methods, however, still fall shortof linguistic approaches.
In this paper,we propose a simple yet effective multi-modal approach that learns bilingual se-mantic representations that fuse linguis-tic and visual input.
These new bilingualmulti-modal embeddings display signifi-cant performance gains in the BLL task forthree language pairs on two benchmark-ing test sets, outperforming linguistic-onlyBLL models using three different typesof state-of-the-art bilingual word embed-dings, as well as visual-only BLL models.1 IntroductionBilingual lexicon learning (BLL) is the task offinding words that share a common meaningacross different languages.
It plays an impor-tant role in a variety of fundamental tasks in IRand NLP, e.g.
cross-lingual information retrievaland statistical machine translation.
The major-ity of current BLL models aim to learn lexiconsfrom comparable data.
These approaches workby (1) mapping language pairs to a shared cross-lingual vector space (SCLVS) such that words areclose when they have similar meanings; and (2)extracting close lexical items from the inducedSCLVS.
Bilingual word embedding (BWE) in-duced models currently hold the state-of-the-art onBLL (Hermann and Blunsom, 2014; Gouws et al,2015; Vuli?c and Moens, 2016).Although methods for learning SCLVSs are pre-dominantly text-based, this space need not be lin-guistic in nature: Bergsma and van Durme (2011)and Kiela et al (2015) used labeled images fromthe Web to learn bilingual lexicons based on visualfeatures, with features derived from deep convolu-tional neural networks (CNNs) leading to the bestresults (Kiela et al, 2015).
However, vision-basedBLL does not yet perform at the same level asstate-of-the-art linguistic models.
Here, we unifythe strengths of both approaches into one singlemulti-modal vision-language SCLVS.It has been found in multi-modal semanticsthat linguistic and visual representations are oftencomplementary in terms of the information theyencode (Deselaers and Ferrari, 2011; Bruni et al,2014; Silberer and Lapata, 2014).
This is the firstwork to test the effectiveness of the multi-modalapproach in a BLL setting.
Our contributionsare: We introduce bilingual multi-modal seman-tic spaces that merge linguistic and visual com-ponents to obtain semantically-enriched bilingualmulti-modal word representations.
These repre-sentations display significant improvements forthree language pairs on two benchmarking BLLtest sets in comparison to three different bilinguallinguistic representations (Mikolov et al, 2013;Gouws et al, 2015; Vuli?c and Moens, 2016), aswell as over the uni-modal visual representationsfrom Kiela et al (2015).We also propose a weighting technique basedon image dispersion (Kiela et al, 2014) that gov-erns the influence of visual information in fusedrepresentations, and show that this technique leadsto robust multi-modal models which do not requirefine tuning of the fusion parameter.2 Methodology2.1 Linguistic RepresentationsWe use three representative linguistic BWE mod-els.
Given a source and target vocabulary VSand VT, BWE models learn a representation ofeach word w ?
VS?
VTas a real-valued vec-188tor: wling= [fling1, .
.
.
, flingdl], where flingk?R is the value of the k-th cross-lingual featurefor w. Similarity between w, v ?
VS?
VTis computed through a similarity function (SF),simling(w, v) = SF (wling,vling), e.g., cosine.Type 1: M-EMB This type of BWE induc-tion model assumes the following setup for learn-ing the SCLVS (Mikolov et al, 2013; Faruquiand Dyer, 2014; Dinu et al, 2015; Lazaridou etal., 2015a): First, two monolingual spaces, RdSand RdT, are induced separately in each languageusing a standard monolingual embedding model.The bilingual signal is provided in the form ofword translation pairs (xi, yi), where xi?
VS,yi?
VT, and xi?
RdS, yi?
RdT.
Train-ing is cast as a multivariate regression problem:it implies learning a function that maps the sourcelanguage vectors to their corresponding target lan-guage vectors.
A standard approach (Mikolov etal., 2013; Dinu et al, 2015) is to assume a linearmap W ?
RdS?dT, which is learned through anL2-regularized least-squares error objective.
Anypreviously unseen source language word vectorxumay be mapped into the target embeddingspace RdTas Wxu.
After mapping all vectors x,x ?
VS, the target space RdTserves as a SCLVS.Type 2: G-EMB Another collection of BWE in-duction models optimizes two monolingual objec-tives jointly, with the cross-lingual objective act-ing as a cross-lingual regularizer during training(Gouws et al, 2015; Soyer et al, 2015).
In a sim-plified formulation (Luong et al, 2015), the ob-jective is: ?
(MonoS+ MonoT) + ?Bi.
The mono-lingual objectives MonoSand MonoTensure thatsimilar words in each language are assigned sim-ilar embeddings and aim to capture the seman-tic structure of each language, whereas the cross-lingual objective Bi ensures that similar wordsacross languages are assigned similar embeddings,and ties the two monolingual spaces together intoa SCLVS.
Parameters ?
and ?
govern the influenceof the monolingual and bilingual components.1The bilingual signal used as the cross-lingual reg-ularizer during the joint training is obtained fromsentence-aligned parallel data.
We opt for the Bil-1Setting ?
= 0 reduces the model to the bilingual modelstrained solely on parallel data (Hermann and Blunsom, 2014;Chandar et al, 2014).
?
= 1 results in the models fromGouws et al (2015) and Soyer et al (2015).
Although theyuse the same data sources, all G-EMB models differ in thechoice of monolingual and cross-lingual objectives.BOWA model from Gouws et al (2015) as the rep-resentative model to be included in the compar-isons, due to its solid performance and robustnessin the BLL task (Luong et al, 2015), its reducedcomplexity reflected in fast computations on mas-sive datasets and its public availability.2Type 3: V-EMB The third set of models re-quires a different bilingual signal to induce aSCLVS: document alignments.
Vuli?c and Moens(2016) created a collection of pseudo-bilingualdocuments by merging every pair of aligned doc-uments in the data, in a way that preserves im-portant local information ?
which words appearednext to which other words (in the same language),and which words appeared in the same region ofthe document (in different languages).
This col-lection was then used to train word embeddingswith monolingual skip-gram with negative sam-pling using word2vec.
With pseudo-bilingualdocuments, the ?context?
of a word is redefinedas a mixture of neighboring words (in the origi-nal language) and words that appeared in the sameregion of the document (in the foreign language).Bilingual contexts for each word in each pseudo-bilingual document steer the final model towardsconstructing a SCLVS.2.2 Visual RepresentationsOnly a few studies have tried to make use of the in-tuition that words in different languages denotingthe same concepts are similarly grounded in theperceptual system (bicycles resemble each otherirrespective of whether we call them bicyle, v?lo,fiets or Fahrrad, see Fig.
1) (Bergsma and vanDurme, 2011; Kiela et al, 2015).
Although theidea is promising, such visual methods are stilllimited in comparison with linguistic ones, es-pecially for more abstract concepts (Kiela et al,2015).
Recent findings in multi-modal semanticssuggest that visual representations encode piecesof semantic information complementary to lin-guistic information derived from text (Deselaersand Ferrari, 2011; Silberer and Lapata, 2014).We compute visual representations in a similarfashion to Kiela et al (2015): For each word weretrieve n images from Google image search (seeFig.
1), and for each image we extract the pre-softmax layer of an AlexNet (Krizhevsky et al,2012) that has been pre-trained on the ImageNet2https://github.com/gouwsmeister/bilbowa189Figure 1: Example images for several languages.classification task (Deng et al, 2009; Russakovskyet al, 2015) using Caffe (Jia et al, 2014).Each image is thus represented as a 4096-dimensional feature vector extracted from a con-volutional neural network (CNN).
We use twomethods for computing visual similarity: (1)CNN-MAX produces a single visual vector bytaking the pointwise maximum across the n im-age vector representations from the image set.The representation of each word w ?
VS?
VTin a visual SCLVS is now a real-valued vectorwvis= [fvis1, .
.
.
, fvisdv], where fvisk?
R denotesthe score for the k-th visual cross-lingual fea-ture for w within a dv-dimensional visual SCLVS(dv= 4096).
As before, similarity between twowords w, v ?
VS?VTis computed by applying asimilarity function on their representations in thevisual SCLVS: simvis(w, v) = SF (wvis,vvis),e.g.
cosine.
(2) CNN-AVGMAX: An alternativestrategy, introduced by Bergsma and van Durme(2011), is to consider the similarities between in-dividual images from the two sets and take the av-erage of the maximum similarity scores as the finalsimilarity simvis(w, v).2.3 Multi-Modal RepresentationsWe experiment with two ways of fusing infor-mation stemming from the linguistic and visualmodalities.
Following recent work in multi-modalsemantics (Bruni et al, 2014; Kiela and Bottou,2014), we construct representations by concate-nating the centered and L2-normalized linguisticand visual feature vectors:wmm= ?
?wling|| (1?
?
)?wvis(1)where || denotes concatenation and ?
is a pa-rameter governing the contributions of each uni-modal representation.
The final similarity mayagain be computed by applying an SF on the multi-modal representations.
We call this method Early-Fusion.
Note that it is possible only with CNN-MAX.
The alternative is not to build a full multi-modal (MM) representation, but instead to com-bine the individual similarity scores from eachuni-modal SCLVS.
The similarity sim(w, v) be-tween two words w and v is:??
simling(w, v) + (1?
?)?
simvis(w, v) == ??
SF (wling,vling) + (1?
?)?
SF (wvis,vvis)where ?
again controls for the importance of theuni-modal scores in the final combined scores.
Wecall this method Late-Fusion3.3 Experimental SetupTask: Bilingual Lexicon Learning Given asource language word ws, the task is to find a tar-get language word wtclosest to wsin the SCLVS,and the resulting pair (ws, wt) is a bilingual lexi-con entry.
Performance is measured using the BLLstandard Top 1 accuracy (Acc1) metric (Gaussieret al, 2004; Gouws et al, 2015).Test Sets We work with three language pairs:English-Spanish/Dutch/Italian (EN-ES/NL/IT),and two benchmarking BLL test sets:(1) BERGSMA500: consisting of a set of 500ground truth noun pairs for the three languagepairs, it is considered a benchmarking test set inprior work on BLL using vision (Bergsma and vanDurme, 2011)4.
Translation direction in our testsis EN ?
ES/IT/NL.
(2) VULIC1000: constructed to measure the gen-eral performance of linguistic BLL models fromcomparable Wikipedia data (Vuli?c and Moens,2013), this is considered a benchmarking test setfor (linguistic) BLL models from comparable data(Vuli?c and Moens, 2016)5.
It comprises 1, 000nouns in ES, IT, and NL, along with their one-to-one ground-truth word translations in EN com-piled semi-automatically.
Translation direction isES/IT/NL?
EN .Training Data and Setup We used standardtraining data and suggested settings to learnM/G/V-EMB model representations.
M-EMB andG-EMB were trained on the full cleaned and tok-enized Wikipedias from the Polyglot website (Al-Rfou et al, 2013).
V-EMB was trained on thefull tokenized document-aligned Wikipedias from3Under the assumption of having the centered and L2-normalized feature vectors, and cos as SF, Early-Fusion maybe transformed into Late-Fusion with adapted weighting:?2?
cos(wling,vling) + (1?
?)2?
cos(wvis,vvis)4http://www.clsp.jhu.edu/~sbergsma/LexImg/5http://www.cl.cam.ac.uk/~dk427/bli.html190Pair: B: EN?ES|V: ES?EN B: EN?IT|V: IT?EN B: EN?NL|V: NL?ENModels M-EMB G-EMB V-EMB M-EMB G-EMB V-EMB M-EMB G-EMB V-EMBLinguisticd = 300 0.71 0.77 0.60 0.73 0.68 0.82 0.77 0.76 0.63 0.71 0.75 0.79 0.77 0.76 0.59 0.75 0.74 0.79VisualCNN-Max 0.51 0.35 0.51 0.35 0.51 0.35 0.54 0.22 0.54 0.22 0.54 0.22 0.56 0.33 0.56 0.33 0.56 0.33CNN-AvgMax 0.55 0.38 0.54 0.38 0.54 0.38 0.56 0.25 0.56 0.25 0.56 0.25 0.60 0.34 0.60 0.34 0.60 0.34Multi-modal with global ?Max-E-0.5 0.76 0.79 0.66 0.79 0.71 0.83 0.83 0.75 0.72 0.70 0.80 0.80 0.85 0.80 0.69 0.78 0.80 0.81Max-E-0.7 0.75 0.80 0.62 0.76 0.70 0.85 0.81 0.77 0.66 0.73 0.78 0.82 0.84 0.80 0.61 0.79 0.80 0.82Max-L-0.7 0.76 0.80 0.64 0.78 0.71 0.85 0.82 0.77 0.69 0.73 0.80 0.82 0.85 0.82 0.64 0.79 0.81 0.83Avg-L-0.5 0.77 0.78 0.68 0.79 0.73 0.83 0.84 0.77 0.75 0.70 0.81 0.79 0.86 0.80 0.76 0.78 0.83 0.81Avg-L-0.7 0.77 0.81 0.66 0.79 0.72 0.85 0.83 0.78 0.72 0.75 0.80 0.83 0.86 0.83 0.70 0.81 0.81 0.83Multi-modal with image dispersion (ID) weightingMax-E-ID 0.76 0.80 0.66 0.78 0.71 0.84 0.81 0.77 0.69 0.73 0.80 0.81 0.84 0.80 0.64 0.79 0.81 0.82Max-L-ID 0.77 0.80 0.66 0.78 0.72 0.85 0.82 0.77 0.70 0.73 0.80 0.81 0.84 0.82 0.65 0.79 0.81 0.82Avg-L-ID 0.77 0.81 0.67 0.79 0.73 0.84 0.83 0.78 0.74 0.73 0.80 0.83 0.85 0.82 0.72 0.80 0.82 0.82Table 1: Summary of theAcc1scores on BERGSMA500 (regular font) and VULIC1000 (italic) across allBLL runs.
M/G/V-EMB denotes the BWE linguistic model.
Other settings are in the form Y-Z-0.W: (1)Y denotes the visual metric, (2) Z denotes the fusion model: E is for Early-Fusion, L is for Late-Fusion,and (3) 0.W denotes the ?
value.
Highest scores per column are in bold.LinguaTools6.
The 100K most frequent wordswere retained for all models.We followed related work (Mikolov et al, 2013;Lazaridou et al, 2015a) for learning the mappingW in M-EMB: starting from the BNC word fre-quency list (Kilgarriff, 1997), the 6, 318 most fre-quent EN words were translated to the three otherlanguages using Google Translate.
The lists weresubsequently cleaned, removing all pairs that con-tain IT/ES/NL words occurring in the test sets andleast frequent pairs, to build the final 3?5K train-ing pairs.
We trained two monolingual SGNSmodels, using SGD with a global learning rateof 0.025.
For G-EMB, as in the original work(Gouws et al, 2015), the bilingual signal forthe cross-lingual regularization was provided inthe first 500K sentences from Europarl.v7 (Tiede-mann, 2012).
We used SGD with a global learningrate 0.15.
For V-EMB, monolingual SGNS wastrained on pseudo-bilingual documents using SGDwith a global learning rate 0.025.
All BWEs weretrained with d = 300.7Other parameters are: 15epochs, 15 negatives, subsampling rate 1e?4.
Wereport results with two ?
standard values: 0.5 and0.7 (more weight assigned to the linguistic part).4 Results and DiscussionTable 1 summarizes Acc1scores, focusing oninteresting comparisons across different dimen-6http://linguatools.org/tools/corpora/7Similar trends were observed with all models and d =64, 500.
We also vary the window size from 4 to 16 in steps of4, and always report the best scoring linguistic embeddings.sions8.
There is a marked difference in per-formance on BERGSMA500 and VULIC1000:visual-only BLL models on VULIC1000 performtwo times worse than linguistic-only BLL models.This is easily explained by the increased abstract-ness of test words in VULIC1000 in comparisonto BERGSMA5009, which highlights the need fora multi-modal approach.Multi-Modal vs. Uni-Modal The multi-modalmodels outperform both linguistic and visualmodels across all setups and combinations onBERGSMA500.
On VULIC1000 multi-modalmodels again outperform their uni-modal compo-nents in both modalities.
In the latter case, im-provements are dependent on the amount of vi-sual information included in the model, as gov-erned by ?.
Since the dataset alo contains highlyabstract words, the inclusion of visual informa-tion may be detrimental to performance.
Thesemodels outperform the uni-modal models acrossa wide variety of settings: they outperform thethree linguistic-only BLL models that held best re-portedAcc1scores on the evaluation set (Vuli?c andMoens, 2016).
The largest improvements are sta-tistically significant according to McNemar?s test,p < 0.01.
We find improvements on both test setsfor all three BWE types.The relative ranking of the visual metrics intro-8Similar rankings of different models are also visible withmore lenient Acc10scores, not reported for brevity.9The average image dispersion value (Kiela et al, 2014),which indicates abstractness, on VULIC1000 is 0.711 com-pared to 0.642 on BERGSMA500.191duced in Kiela et al (2015) extends to the MMsetting: Late-Fusion with CNN-AVGMAX is themost effective MM BLL model on average, but allother tested MM configurations also yield notableimprovements.Concreteness To measure concreteness, we usean unsupervised data-driven method, shown toclosely mirror how concrete a concept is: imagedispersion (ID) (Kiela et al, 2014).
ID is definedas the average pairwise cosine distance betweenall the image representations/vectors {i1.
.
.
in} inthe set of images for a given word w:id(w) =2n(n?
1)?j<k?n1?ij?
ik|ij||ik|(2)Intuitively, more concrete words display morecoherent visual representations and consequentlylower ID scores (see Footnote 9 again).
The low-est improvements on VULIC1000 are reported forthe IT-EN language pair, which is incidentally themost abstract test set.There is some evidence that abstract conceptsare also perceptually grounded (Lakoff and John-son, 1999), albeit in a more complex way, sinceabstract concepts will relate more varied situations(Barsalou and Wiemer-Hastings, 2005).
Conse-quently, uni-modal visual representations are notpowerful enough to capture all the semantic in-tricacies of such abstract concepts, and the lin-guistic components are more beneficial in suchcases.
This explains an improved performancewith ?
= 0.7, but also calls for a more intelligentdecision mechanism on how much perceptual in-formation to include in the multi-modal models.The decision should be closely related to the de-gree of a concept?s concreteness, e.g., eq.
(2).Image Dispersion Weighting The intuition thatthe inclusion of visual information may lead tonegative effects in MM modeling has been ex-ploited by Kiela et al (2014) in their work onimage-dispersion filtering: Although the filteringmethod displays some clear benefits, its short-coming lies in the fact that it performs a binarydecision which can potentially discard valuableperceptual information for less concrete concepts.Here, we introduce a weighting scheme where theperceptual information is weighted according toits ID value.
Early-Fusion is now computed as:wmm= ?
(id)?wling|| (1?
?
(id))?wvisLate-Fusion model becomes:?(id)?
SF (wling,vling) + (1?
?(id))?
SF (wvis,vvis)?
(id) denotes a weight that is proportional to theID score of the source language word w: we optfor a simple approach and specify ?
(id) = id(w).Instead of having one global parameter ?, the IDweighting adjusts the amount of information lo-cally according to each concept?s concreteness.The results are summarised in Table 1.
Allmulti-modal models with ID-based weighting areoutperforming their uni-modal components.
TheID-weighted BLL models reach (near-)optimalBLL results across a variety of language-visioncombinations without any fine-tuning.5 ConclusionWe have presented a novel approach to bilin-gual lexicon learning (BLL) that combines lin-guistic and visual representations into new bilin-gual multi-modal (MM) models.
Two simple yeteffective ways to fuse the linguistic and visual in-formation for BLL have been described.
SuchMM models outperform their linguistic and vi-sual uni-modal component models on two stan-dard benchmarking BLL test sets for three lan-guage pairs.
Comparisons with three differentstate-of-the-art bilingual word embedding induc-tion models demonstrate that the gains of MMmodeling are generally applicable.As future work, we plan to analyse the ability ofmulti-view representation learning algorithms toyield fused multi-modal representations in bilin-gual settings (Lazaridou et al, 2015b; Rastogi etal., 2015; Wang et al, 2015), as well as to ap-ply multi-modal bilingual spaces in other taskssuch as zero-short learning (Frome et al, 2013) orcross-lingual MM information search and retrievalfollowing paradigms from monolingual settings(Pereira et al, 2014; Vuli?c and Moens, 2015).The inclusion of perceptual data, as this pa-per reveals, seems especially promising in bilin-gual settings (Rajendran et al, 2016; Elliott et al,2016), since the perceptual information demon-strates the ability to transcend linguistic borders.AcknowledgmentsThis work is supported by ERC ConsolidatorGrant LEXICAL (648909) and KU Leuven GrantPDMK/14/117.
SC is supported by ERC StartingGrant DisCoTex (306920).
We thank the anony-mous reviewers for their helpful comments.192ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual NLP.
In CoNLL, pages 183?192.Lawrence W. Barsalou and Katja Wiemer-Hastings.2005.
Situating abstract concepts.
In D. Pecher andR.
Zwaan, editors, Grounding cognition: The roleof perception and action in memory, language, andthought, pages 129?163.Shane Bergsma and Benjamin van Durme.
2011.Learning bilingual lexicons using the visual similar-ity of labeled web images.
In IJCAI, pages 1764?1769.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.2014.
Multimodal distributional semantics.
Journalof Artiifical Intelligence Research, 49:1?47.Sarath A.P.
Chandar, Stanislas Lauly, Hugo Larochelle,Mitesh M. Khapra, Balaraman Ravindran, Vikas C.Raykar, and Amrita Saha.
2014.
An autoencoderapproach to learning bilingual word representations.In NIPS, pages 1853?1861.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, KaiLi, and Fei-Fei Li.
2009.
ImageNet: A large-scalehierarchical image database.
In CVPR, pages 248?255.Thomas Deselaers and Vittorio Ferrari.
2011.
Vi-sual and semantic similarity in ImageNet.
In CVPR,pages 1777?1784.Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-roni.
2015.
Improving zero-shot learning by miti-gating the hubness problem.
In ICLR Workshop Pa-pers.D.
Elliott, S. Frank, K. Sima?an, and L. Specia.
2016.Multi30K: Multilingual English-German Image De-scriptions.
CoRR, abs/1605.00459.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In EACL, pages 462?471.Andrea Frome, Gregory S. Corrado, Jonathon Shlens,Samy Bengio, Jeffrey Dean, Marc?Aurelio Ranzato,and Tomas Mikolov.
2013.
Devise: A deep visual-semantic embedding model.
In NIPS, pages 2121?2129.
?ric Gaussier, Jean-Michel Renders, Irina Matveeva,Cyril Goutte, and Herv?
D?jean.
2004.
A geometricview on bilingual lexicon extraction from compara-ble corpora.
In ACL, pages 526?533.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2015.
BilBOWA: Fast bilingual distributed repre-sentations without word alignments.
In ICML, pages748?756.Karl Moritz Hermann and Phil Blunsom.
2014.
Multi-lingual models for compositional distributed seman-tics.
In ACL, pages 58?68.Yangqing Jia, Evan Shelhamer, Jeff Donahue, SergeyKarayev, Jonathan Long, Ross B. Girshick, SergioGuadarrama, and Trevor Darrell.
2014.
Caffe: Con-volutional architecture for fast feature embedding.In ACM Multimedia, pages 675?678.Douwe Kiela and L?on Bottou.
2014.
Learning imageembeddings using convolutional neural networks forimproved multi-modal semantics.
In EMNLP, pages36?45.Douwe Kiela, Felix Hill, Anna Korhonen, and StephenClark.
2014.
Improving multi-modal representa-tions using image dispersion: Why less is sometimesmore.
In ACL, pages 835?841.Douwe Kiela, Ivan Vuli?c, and Stephen Clark.
2015.Visual bilingual lexicon induction with transferredConvNet features.
In EMNLP, pages 148?158.Adam Kilgarriff.
1997.
Putting frequencies in thedictionary.
International Journal of Lexicography,10(2):135?155.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-ton.
2012.
ImageNet classification with deep con-volutional neural networks.
In NIPS, pages 1106?1114.George Lakoff and Mark Johnson.
1999.
Philosophyin the flesh: The embodied mind and its challenge toWestern thought.Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-roni.
2015a.
Hubness and pollution: Delving intocross-space mapping for zero-shot learning.
In ACL,pages 270?280.Angeliki Lazaridou, Nghia The Pham, and Marco Ba-roni.
2015b.
Combining language and vision witha multimodal skip-gram model.
In NAACL-HLT,pages 153?163.Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015.
Bilingual word representations withmonolingual quality in mind.
In Proceedings of the1st Workshop on Vector Space Modeling for NaturalLanguage Processing, pages 151?159.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013.Exploiting similarities among languages for ma-chine translation.
CoRR, abs/1309.4168.Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle,Nikhil Rasiwasia, Gert R. G. Lanckriet, Roger Levy,and Nuno Vasconcelos.
2014.
On the role of corre-lation and abstraction in cross-modal multimedia re-trieval.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 36(3):521?535.193Janarathanan Rajendran, Mitesh M. Kapra, SarathChandar, and Balaraman Ravindran.
2016.
Bridgecorrelational neural networks for multilingual multi-modal representation learning.
In NAACL.Pushpendre Rastogi, Benjamin Van Durme, and RamanArora.
2015.
Multiview LSA: Representation learn-ing via generalized CCA.
In NAACL, pages 556?566.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-drej Karpathy, Aditya Khosla, Michael S. Bernstein,Alexander C. Berg, and Fei-Fei Li.
2015.
ImageNetlarge scale visual recognition challenge.
Interna-tional Journal of Computer Vision, 115(3):211?252.Carina Silberer and Mirella Lapata.
2014.
Learn-ing grounded meaning representations with autoen-coders.
In ACL, pages 721?732.Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa.2015.
Leveraging monolingual data for crosslingualcompositional word representations.
In ICLR.J?rg Tiedemann.
2012.
Parallel data, tools and inter-faces in OPUS.
In LREC, pages 2214?2218.Ivan Vuli?c and Marie-Francine Moens.
2013.
A studyon bootstrapping bilingual vector spaces from non-parallel data (and nothing else).
In EMNLP, pages1613?1624.Ivan Vuli?c and Marie-Francine Moens.
2015.
Mono-lingual and cross-lingual information retrieval mod-els based on (bilingual) word embeddings.
In SI-GIR, pages 363?372.Ivan Vuli?c and Marie-Francine Moens.
2016.Bilingual distributed word representations fromdocument-aligned comparable data.
Journal of Arti-ficial Intelligence Research, 55:953?994.Weiran Wang, Raman Arora, Karen Livescu, andJeff A. Bilmes.
2015.
On deep multi-view repre-sentation learning.
In ICML, pages 1083?1092.194
