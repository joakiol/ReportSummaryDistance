Proceedings of NAACL-HLT 2013, pages 433?438,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsUsing a Supertagged Dependency Language Model to Selecta Good Translation in System CombinationWei-Yun Ma Kathleen McKeownDepartment of Computer Science Department of Computer ScienceColumbia University Columbia UniversityNew York, NY 10027, USA New York, NY 10027, USAma@cs.columbia.edu kathy@cs.columbia.eduAbstractWe present a novel, structured languagemodel - Supertagged Dependency LanguageModel to model the syntactic dependenciesbetween words.
The goal is to identifyungrammatical hypotheses from a set ofcandidate translations in a MT systemcombination framework and help select thebest translation candidates using a variety ofsentence-level features.
We use a two-stepmechanism based on constituent parsing andelementary tree extraction to obtain supertagsand their dependency relations.
Ourexperiments show that the structured languagemodel provides significant improvement inthe framework of sentence-level systemcombination.1 IntroductionIn recent years, there has been a burgeoninginterest in incorporating syntactic structure intoStatistical machine translation (SMT) models (e..g,Galley et al 2006; DeNeefe and Knight 2009;Quirk et al 2005).
In addition to modelingsyntactic structure in the decoding process, amethodology for candidate translation selectionhas also emerged.
This methodology first generatesmultiple candidate translations followed byrescoring using global sentence-level syntacticfeatures to select the final translation.
Theadvantage of this methodology is that it allows foreasy integration of complex syntactic features thatwould be too expensive to use during the decodingprocess.
The methodology is usually applied in twoscenarios: one is as part of an n-best reranking(Och et al 2004; Hasan et al 2006), where n-bestcandidate translations are generated through adecoding process.
The other is translation selectionor reranking (Hildebrand and Vogel 2008;Callison-Burch et al 2012), where candidatetranslations are generated by different decodingprocesses or different decoders.This paper belongs to the latter; the goal is toidentify ungrammatical hypotheses from givencandidate translations using grammaticalknowledge in the target language that expressessyntactic dependencies between words.
To achievethat, we propose a novel Structured LanguageModel (SLM) - Supertagged DependencyLanguage Model (SDLM) to model the syntacticdependencies between words.
Supertag (Bangaloreand Joshi, 1999) is an elementary syntacticstructure based on Lexicalized Tree AdjoiningGrammar (LTAG).
Traditional supertagged n-gramLM predicts the next supertag based on theimmediate words to the left with supertags, so itcan not explicitly model long-distance dependencyrelations.
In contrast, SDLM predicts the nextsupertag using the words with supertags on whichit syntactically depend, and these words could beanywhere and arbitrarily far apart in a sentence.
Acandidate translation?s grammatical degree or?fluency?
can be measured by simply calculatingthe SDLM likelihood of the supertaggeddependency structure that spans the entire sentence.To obtain the supertagged dependency structure,the most intuitive way is through a LTAG parser(Schabes et al 1988).
However, this could be very433slow as it has time complexity of O(n6).
Insteadwe propose an alternative mechanism in this paper:first we use a constituent parser1 of O(n3) ~ O(n5)to obtain the parse of a sentence, and then weextract elementary trees with dependencies fromthe parse in linear time.
Aside from theconsideration of time complexity, anothermotivation of this two-step mechanism is thatcompared with LTAG parsing, the mechanism ismore flexible for defining syntactic structures ofelementary trees for our needs.
Because thosestructures are defined only within the elementarytree extractor, we can easily adjust the definition ofthose structures within the extractor and avoidredesigning or retraining our constituent parser.We experiment with sentence-level translationcombination of five different translation systems;the goal is for the system to select the besttranslation for each input source sentence amongthe translations provided by the five systems.
Theresults show a significant improvement of 1.45Bleu score over the best single MT system and0.72 Bleu score over a baseline sentence-levelcombination system of using consensus and n-gram LM.2 Related WorkOch et al (2004) investigated various syntacticfeature functions to rerank the n-best candidatetranslations.
Most features are syntacticallymotivated and based on alignment informationbetween the source sentence and the targettranslation.
The results are rather disappointing.Only the non-syntactic IBM model 1 yieldedsignificant improvement.
All other tree-basedfeature functions had only a very small effect onthe performance.In contrast to (Och et al 2004)?s bilingualsyntax features, Hasan et al (2006) focused onmonolingual syntax features in n-best reranking.They also investigated the effect of directly usingthe log-likelihood of the output of a HMM-basedsupertagger, and found it did not improveperformance significantly.
It is worth noticing thatthis log-likelihood is based on supertagged n-gram1  Stanford parser (http://nlp.stanford.edu/software/lex-parser.shtml).
We use its PCFG version of O(n3) for SDLMtraining of part of Gigaword in addition to Treebank and useits factor version of O(n5) to calculate the SDLM likelihood oftranslations.LM, which is one type of class-based n-gram LM,so it does not model explicit syntacticdependencies between words in contrast to thework we describe in this paper.
Hardmeier et al(2012) use tree kernels over constituency anddependency parse trees for either the input oroutput sentences to identify constructions that aredifficult to translate in the source language, anddoubtful syntactic structures in the output language.The tree fragments extracted by their tree kernelsare similar to our elementary trees but they onlyregard them as the individual inputs of supportvector machine regression while binary relations ofour elementary trees are considered in aformulation of a structural language model.Outside the field of candidate translationselection, Hassan et al (2007) proposed a phrase-based SMT model that integrates supertags into thetarget side of the translation model and the targetn-gram LM.
Two kinds of supertags are employed:those from LTAG and Combinatory CategorialGrannar (CCG), and both yield similarimprovements.
They found that using both oreither of the supertag-based translation model andsupertagged LM can achieve significantimprovement.
Again, the supertagged LM is aclass-based n-gram LM and does not modelexplicit syntactic dependencies during decoding.In the field of MT system combination, word-level confusion network decoding is one of themost successful approaches (Matusov et al 2006;Rosti et al 2007; He et al2008; Karakos et al2008; Sim et al2007; Xu et al2011).
It is capableof generating brand new translations but it isdifficult to consider more complex syntax such asdependency LM during decoding since it adds oneword at a time while a dependency based LM mustparse a complete sentence.
Typically, a confusionnetwork approach selects one translation as thebest and uses this as the backbone for theconfusion network.
The work we present herecould provide a more sophisticated mechanism forselecting the backbone.
Alternatively, one canenhance confusion network models bycollaborating with a sentence-level combinationmodel which uses complex syntax to re-rank n-bestoutputs of a confusion network model.
This kind ofcollaboration is one of our future works.4343 LTAG and SupertagLTAG (Joshi et al 1975; Schabes et al 1988) is aformal tree rewriting formalism, which consists ofa set of elementary trees, corresponding to minimallinguistic structures that localize dependencies,including long-distance dependencies, such aspredicate-argument structure.
Each elementary treeis associated with at least one lexical item on itsfrontier.
The lexical item associated with anelementary tree is called the anchor in that tree; anelementary tree thus serves as a description ofsyntactic constraints of the anchor.
The elementarysyntactic structures of elementary trees are calledsupertags (Bangalore and Joshi, 1999), in order todistinguish them from the standard part-of-speechtags.
Some examples are provided in figure 1 (b).Elementary trees are divided into initial andauxiliary trees.
Initial trees are those for which allnon-terminal nodes on the frontier are substitutable.Auxiliary trees are defined as initial trees, exceptthat exactly one frontier, non-terminal node mustbe a foot node, with the same label as the root node.Two operations - substitution and adjunction - areprovided in LTAG to combine elementary treesinto a derived tree.4 SDLMOur goal is to use SDLM to calculate thegrammaticality of translated sentences.
We do thisby calculating the likelihood of the supertaggeddependency structure that spans the entire sentenceusing SDLM.
To obtain the supertaggeddependency linkage, the most intuitive way isthrough a LTAG parser (Schabes et al 1988).However, this could be very slow as it has timecomplexity of O(n6).
Another possibility is tofollow the procedure in (Joshi and Srinivas 1994,Bangalore and Joshi, 1999): use a HMM-basedsupertagger to assign words with supertags,followed by derivation of a shallow parse in lineartime based on only the supertags to obtain thedependencies.
But since this approach uses onlythe local context, in (Joshi and Srinivas 1994), theyalso proposed another greedy algorithm based onsupertagged dependency probabilities to graduallyselect the path with the maximum path probabilityto extend to the remaining directions in thedependency list.In contrast to the LTAG parsing andsupertagging-based approaches, we propose analternative mechanism: first we use a state-of-the-art constituent parser to obtain the parse of asentence, and then we extract elementary trees withdependencies from the parse to assign each wordwith an elementary tree.
The second step is similarto the approach used in extracting elementary treesfrom the TreeBank (Xia, 1999; Chen and Vijay-Shanker, 2000).4.1 Elementary Tree ExtractionWe use an elementary tree extractor, amodification of (Chen and Vijay-Shanker, 2000),to serve our purpose.
Heuristic rules were used todistinguish arguments from adjuncts, and theextraction process can be regarded as a process thatgradually decomposes a constituent parse tomultiple elementary trees and records substitutionsand adjunctions.
From elementary trees, we canobtain supertags by only considering syntacticstructure and ignoring anchor words.
Take thesentence ?
?The hungry boys ate dinner?
as anexample; the constituent parse and extractedsupertags are shown in Figure 1.In Figure 1 (b), dotted lines represent theoperations of substitution and adjunction.
Note thateach word in a translated sentence would beassigned exactly one elementary syntactic structurewhich is associated with a unique supertag id forthe whole corpus.
Different anchor words couldown the same elementary syntactic structure andwould be assigned the same supertag id, such as?
1?
?
for ?boys?
and ?dinner?.
For our corpus,around 1700 different elementary syntacticstructures (1700 supertag ids) are extracted.Figure 1.
(a) Parse of ?The hungry boys ate dinner?435NPtheNP*boysSNP1?
VPateNP2?NPdinnerDTanchorwordNPhungryNP*JJanchorword VBanchorwordNNanchorwordNPNNanchorwordanchorword:elementarysyntacticstructure(supertag):supertag id: 1?
1?2?1?
2?Figure 1.
(b) Extracted elementary trees4.2 ModelBangalore and Joshi (1999) gave a concisedescription for dependencies between supertags:?A supertag is dependent on another supertag if theformer substitutes or adjoins into the latter?.Following this description, for the example inFigure 1 (b), supertags of ?the?
and ?hungry?
aredependent on the supertag of ?boys?, and supertagsof ?boys?
and ?dinner?
are dependent on thesupertag of ?ate?.
These dependencies betweensupertags also provide the dependencies betweenanchor words.Since the syntactic constraints for each word inits context are decided and described through itssupertag, the likelihood of SDLM for a sentencecould also be regarded as the degree of violationsof the syntactic constraints on all words in thesentence.
Consider a sentence S = w1 w2 ?wn withcorresponding supertags T = t1 t2 ?tn.
We use di=jto represent the dependency relations for words orsupertags.
For example, d3 = 5 means that w3depends on w5 or t3 depends on t5.
We propose fivedifferent bigram SDLM as follows and evaluatetheir effects in section 5.SDLM model (2) is the approximation form ofmodel (1); model (3) and (4) are individual termsof model (2); model (5) models word dependenciesbased on elementary tree dependencies.
Theestimation of the probabilities is done usingmaximum likelihood estimations with Laplacesmoothing.
Take Figure 1 (b) as an example; ifusing model (1), the SDLM likelihood of ?Thehungry boys ate dinner?
is)|2,(*)2,|1,(*)2,|1,(*)1,|2,(*)1,|1,(rootatePatedinnerPateboysPboyshungryPboystheP????????
?In our experiment on sentence-level translationcombination, we use a log-linear model to integrateall features including SDLM models.
Thecorresponding weights are trained discriminativelyfor Bleu score using Minimum Error Rate Training(MERT).5 ExperimentOur experiments are conducted and reported on theChinese-English dataset from NIST 2008(LDC2010T01).
It consists of four humanreference translations and corresponding machinetranslations for the NIST Open MT08 test set,which consists of newswire and web data.
The testset contains 105 documents with 1312 sentencesand output from 23 machine translation systems.Each system provides the top one translationhypothesis for every sentence.
We further dividethe NIST Open MT08 test set into the tuning setand test set for our experiment of sentence-leveltranslation combination.
We divided the 1312sentences into tuning data of 524 sentences and thetest set of 788 sentences.
Out of 23 MT systems,we manually select the top five MT systems as ourMT systems for our combination experiment.In terms of SDLM training, since the size ofTreeBank-extracted elementary trees is muchsmaller compared to most practical n-gram LMstrained from the Gigaword corpus, we also extractelementary trees from automatically-generatedparses of part of the Gigaword corpus (around one-year newswire of ?afp_eng?
in Gigaword 4) inaddition to TreeBank-extracted elementary trees.5.1 Feature FunctionsFor the baseline combination system, we use thefollowing feature functions in the log-linear modelto calculate the score of a system translation.z Sentence consensus based on Translation EditRatio (TER)z Gigaword-trained 3-gram LM and wordpenalty|?
i model(5) SDLM                                                    )(model(4) SDLM                                                       )|(model(3) SDLM                                                       )|(model(2) SDLM       )|()|()|(model(1) SDLM                                             )|(?????
?idiiiidiiiidiiddiiiddiiiiiiiiiwwPtwPttPtwPttPtwtwPtwtwP436For testing SDLM, in additional to all featuresthat the baseline combination system uses, we addsingle or multiple SDLM models in the log-linearmodel, and each SDLM model has its own weight.5.2 ResultFrom table 1, we can see that the combination ofSDLM model 3, 4 and 5 yields the bestperformance, which is better than the best MTsystem by Bleu of 1.45, TER of 0.67 andMETEOR of 1.25, and also better than the baselinecombination system by Bleu of 0.72, TER of 0.25and METEOR of 0.44.
Compared with SDLMmodel 5, which represents a type of worddependency LM without labels, the results showthat adding appropriate syntactic ?labels?
(here,they are ?supertags?)
on word dependencies bringsbenefits.Table 1.
Result of Sentence-level Translation Combination6 ConclusionIn this paper we presented SupertaggedDependency Language Model for explicitlymodeling syntactic dependencies of the words oftranslated sentences.
Our goal is to select the mostgrammatical translation from candidate translations.To obtain the supertagged dependency structure ofa translation candidate, a two-step mechanismbased on constituent parsing and elementary treeextraction is also proposed.
SDLM shows itseffectiveness in the scenario of translationselection.There are several avenues for future work: wehave focused on bigram dependencies in ourmodels; extension to more than two dependentelementary trees is straightforward.
It would alsobe worth investigating the performance of usingour sentence-level model to re-rank n-best outputsof a confusion network model.
And in terms ofapplications, SDLM can be directly applied tomany other NLP tasks, such as speech recognitionand natural language generation.AcknowledgmentsWe would like to thank Owen Rambow forproviding the elementary tree extractor and alsothank the anonymous reviewers for their helpfulcomments.
This work is supported by the NationalScience Foundation via Grant No.
0910778entitled ?Richer Representations for MachineTranslation?.
All views expressed in this paper arethose of the authors and do not necessarilyrepresent the view of the National ScienceFoundation.ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.Supertagging: An approach to almost parsing.Computational Linguistics, 25(2):237?265.John Chen and K. Vijay-Shanker.
2000.
Automatedextraction of TAGs from the Penn treebank.
InProceedings of the Sixth International Workshop onParsing TechnologiesChris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut and Lucia Specia.
2012.Findings of the 2012 Workshop on StatisticalMachine Translation.
In Proceedings of WMT12.Steve DeNeefe and Kevin Knight.
2009 SynchronousTree Adjoining Machine Translation.
In Proceedingsof EMNLPMichel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, Ignacio Thayer.2006.
Scalable Inference and Training of Context-Rich Syntactic Translation Models.
In Proceedings ofthe Annual Meeting of the Association forComputational LinguisticsChristian Hardmeier, Joakim Nivre and J?rg Tiedemann.2012.
Tree Kernels for Machine Translation QualityEstimation.
In Proceedings of WMT12S.
Hasan, O. Bender, and H. Ney.
2006.
Rerankingtranslation hypotheses using structural properties.
InProceedings of the EACL'06 Workshop on LearningStructured Information in Natural LanguageApplicationsHany Hassan , Khalil Sima'an and Andy Way.
2007.Supertagged Phrase-Based Statistical MachineTranslation.
In Proceedings of the Annual Meeting ofthe Association for Computational LinguisticsXiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,and Robert Moore.
2008.
Indirect-hmm-basedhypothesis alignment for computing outputs frommachine translation systems.
In Proceedings ofEMNLPBleu TER METEORBest MT system 30.16 55.45 54.43baseline 30.89 55.03 55.24baseline+ model 1 31.29 54.99 55.63baseline+ model 2 31.25 55.23 55.37baseline+ model 3 31.25 55.06 55.40baseline+ model 4 31.44 54.70 55.54baseline+ model 5 31.39 55.15 55.68baseline+ model 3+model 4+ model 5 31.61 54.78 55.68437Almut Silja Hildebrand and Stephan Vogel.
2008.Combination of machine translation systems viahypothesis selection from combined n-best lists.
InProceedings of the Eighth Conference of theAssociation for Machine Translation in the AmericasAravind K. Joshi and B. Srinivas.
1994.
Disambiguationof super parts of speech (or supertags): Almostparsing.
In Proceedings of the 15th InternationalConference on Computational LinguisticsAravind K. Joshi, Leon S. Levy, and Masako Takahashi.1975.
Tree Adjunct Grammars.
Journal of Computerand System Science, 10:136?163.Evgeny Matusov, Nicola Ueffing, and Hermann Ney2006.
Computing consensus translation from multiplemachine translation systems using enhancedhypotheses alignment.
In Proceedings of EACLFranz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004 Asmorgasbord of features for statistical machinetranslation.
In Proceedings of the Meeting of theNorth American chapter of the Association forComputational LinguisticsDamianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translationsystem combination using ITG-based alignments.
InProceedings of ACL-HLTChris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency Treelet Translation: SyntacticallyInformed Phrasal SMT, In Proceedings of theAssociation for Computational LinguisticsAntti-Veikko I. Rosti, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved word-level systemcombination for machine translation.
In Proceedingsof ACLYves Schabes, Anne Abeille and Aravind K. Joshi.1988.
Parsing strategies with 'lexicalized' grammars:Application to Tree Adjoining Grammars.
InProceedings of the 12th International Conference onComputational LinguisticsK.C.
Sim, W.J.
Byrne, M.J.F.
Gales, H. Sahbi and P.C.Woodland .2007.
Consensus Network Decoding forStatistical Machine Translation System Combination.In Proceedings of ICASSPFei Xia.
1999.
Extracting Tree Adjoining Grammarsfrom Bracketed Corpora.
In Proceedings of the 5thNatural Language Processing Pacific RimSymposium (NLPRS-1999)Daguang Xu, Yuan Cao, Damianos Karakos.
2011.Description of the JHU System Combination Schemefor WMT 2011.
In Proceedings of the SixthWorkshop on Statistical Machine Translation438
