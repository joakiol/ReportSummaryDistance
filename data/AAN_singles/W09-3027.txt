Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 150?153,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAnnotating Semantic Relations Combining Facts and OpinionsKoji Murakami?
Shouko Masuda??
Suguru Matsuyoshi?Eric Nichols?
Kentaro Inui?
Yuji Matsumoto?
?Nara Institute of Science and Technology8916-5, Takayama, Ikoma, Nara 630-0192 JAPAN?Osaka Prefecture University1-1, Gakuen, Naka-ku, Sakai, Osaka 599-8531 JAPAN{kmurakami,shouko,matuyosi,eric-n,inui,matsu}@is.naist.jpAbstractAs part of the STATEMENT MAP project,we are constructing a Japanese corpus an-notated with the semantic relations bridg-ing facts and opinions that are necessaryfor online information credibility evalua-tion.
In this paper, we identify the se-mantic relations essential to this task anddiscuss how to efficiently collect valid ex-amples from Web documents by splittingcomplex sentences into fundamental unitsof meaning called ?statements?
and an-notating relations at the statement level.We present a statement annotation schemeand examine its reliability by annotatingaround 1,500 pairs of statements.
We arepreparing the corpus for release this win-ter.1 IntroductionThe goal of the STATEMENT MAP project (Mu-rakami et al, 2009) is to assist internet users withevaluating the credibility of online information bypresenting them with a comprehensive survey ofopinions on a topic and showing how they relateto each other.
However, because real text on theWeb is often complex in nature, we target a sim-pler and more fundamental unit of meaning whichwe call the ?statement.?
To summarize opinionsfor the statement map users, we first convert allsentences into statements and then, organize theminto groups of agreeing and conflicting opinionsthat show the logical support for each group.For example, a user who is concerned about po-tential connections between vaccines and autismwould be presented with a visualization of theopinions for and against such a connection to-gether with the evidence supporting each view asshown in Figure 1.When the concerned user in our example looksat this STATEMENT MAP, he or she will see thatsome opinions support the query ?Do vaccinescause autism??
while other opinions do not, butit will also show what support there is for each ofthese viewpoints.
So, STATEMENT MAP can helpuser come to an informed conclusion.2 Semantic Relations betweenStatements2.1 Recognizing Semantic RelationsTo generate STATEMENT MAPs, we need to an-alyze a lot of online information retrieved on agiven topic, and STATEMENT MAP shows usersa summary with three major semantic relations.AGREEMENT to group similar opinionsCONFLICT to capture differences of opinionsEVIDENCE to show support for opinionsIdentifying logical relations between texts is thefocus of Recognizing Textual Entailment (RTE).A major task of the RTE Challenge (Dagan et al,2005) is the identification of [ENTAILMENT] or[CONTRADICTION] between Text (T) and Hy-pothesis (H).
For this task, several corpora havebeen constructed over the past few years, and an-notated with thousands of (T,H) pairs.While our research objective is to recognize se-mantic relations as well, our target domain is textfrom Web documents.
The definition of contradic-tion in RTE is that T contradicts H if it is very un-likely that both T and H can be true at the sametime.
However, in real documents on the Web,there are many examples which are partially con-tradictory, or where one statement restricts the ap-plicability of another like in the example below.
(1) a. Mercury-based vaccines actually cause autism inchildren.150!Mercury-based vaccine preservatives actually have caused autism inchildren.
!It?s biologically plausible that the MMR vaccine causes autism.VACCINES CAUSE AUTISM!There is no valid scientific evidence that vaccinescause autism.
!The weight of the evidence indicates that vaccinesare not associated with autism.VACCINES DON?T CAUSE AUTISM!My son then had the MMR, and then when he was three he wasdiagnosed with autism.
!He then had the MMR, and then when he was three he wasdiagnosed with autism.MY CHILD WAS DIAGNOSED WITH AUTISMRIGHT AFTER THE VACCINE!Vaccinations are given around the same timechildren can be first diagnosed.
!The plural of anecdote is not data.ANECDOTES ARE NOT EVIDENCE[CONFLICT]![FOCUS]![EVIDENCE]!
[EVIDENCE]!Query : Do vaccines cause autism?!
[CONFLICT]!Figure 1: An example STATEMENT MAP for the query ?Do vaccines cause autism??b.
Vaccines can trigger autism in a vulnerable subset ofchildren.While it is difficult to assign any relation to thispair in an RTE framework, in order to constructstatement maps we need to recognize a contradic-tion between (1a) and (1b).There is another task of recognizing relationsbetween sentences, CST (Cross-Document Struc-ture Theory) which was developed by Radev(2000).
CST is an expanded rhetorical structureanalysis based on RST (Mann and Thompson,1988), and attempts to describe relations betweentwo or more sentences from both single and mul-tiple document sets.
The CSTBank corpus (Radevet al, 2003) was constructed to annotate cross-document relations.
CSTBank is divided into clus-ters in which topically-related articles are gath-ered.
There are 18 kinds of relations in this corpus,including [EQUIVALENCE], [ELABORATION],and [REFINEMENT].2.2 Facts and OpinionsRTE is used to recognize logical and factual re-lations between sentences in a pair, and CST isused for objective expressions because newspa-per articles related to the same topic are used asdata.
However, the task specifications of both RTEand CST do not cover semantic relations betweenopinions and facts as illustrated in the followingexample.
(2) a.
There must not be a connection between vaccinesand autism.b.
I do believe that there is a link between vaccinationsand autism.Subjective statements, such as opinions, are re-cently the focus of many NLP research topics,such as review analysis, opinion extraction, opin-ion QA, or sentiment analysis.
In the corpus con-structed by the MPQA Project (Multi-PerspectiveQuestion Answering) (Wiebe et al, 2005), indi-vidual expressions are marked that correspond toexplicit mentions of private states, speech events,and expressive subjective elements.Our goal is to annotate instances of the threemajor relation classes: [AGREEMENT], [CON-FLICT] and [EVIDENCE], between pairs of state-ments in example texts.
However, each relationhas a wide range, and it is very difficult to definea comprehensive annotation scheme.
For exam-ple, different kinds of information can act as cluesto recognize the [AGREEMENT] relations.
So,we have prepared a wide spectrum of semantic re-lations depending on different types of informa-tion regarded as clues to identify a relation class,such as [AGREEMENT] or [CONFLICT].
Table 1shows the semantic relations needed for carry-ing out the anotation.
Although detecting [EVI-DENCE] relations is also essential to the STATE-MENT MAP project, we do not include them in ourcurrent corpus construction.3 Constructing a Japanese Corpus3.1 Targeting Semantic Relations BetweenStatementsReal data on the Web generally has complex sen-tence structures.
That makes it difficult to rec-ognize semantic relations between full sentences.but it is possible to annotate semantic relation be-tween parts extracted from each sentence in manycases.
For example, the two sentences A and Bin Figure 2 cannot be annotated with any of thesemantic relations in Table 1, because each sen-tence include different types of information.
How-ever, if two parts extracted from these sentences Cand D are compared, the parts can be identified as[EQUIVALENCE] because they are semanticallyclose and each extracted part does not contain adifferent type of information.
So, we attempt tobreak sentences from the Web down into reason-able text segments, which we call ?statements.
?When a real sentence includes several pieces of se-151Table 1: Definition of semantic relations and example in the corpusRelation Class Relation Label ExampleAGREEMENTEquivalence A: The overwhelming evidence is that vaccines are unrelated to autism.B: There is no link between the MMR vaccine and autism.Equivalent OpinionA: We think vaccines cause autism.B: I am the mother of a 6 year old that regressed into autism because of his 18month vaccinations.Specific A: Mercury-based vaccine preservatives actually have caused autism in children.B: Vaccines cause autism.CONFLICTContradiction A: Mercury-based vaccine preservatives actually have caused autism in children.B: Vaccines don?t cause autism.Confinement A: Vaccines can trigger autism in a vulnerable subset of children.B: Mercury-based vaccine actually have caused autism in children.Conflicting Opinion A: I don?t think vaccines cause autism.B: I believe vaccines are the cause of my son?s autism.According to Departmentof Medicine, there is nolink between the MMRvaccine and autism.
!There is no link between theMMR vaccine and autism.
!The weight of theevidence indicates thatvaccines are notassociated with autism.
!Vaccines are notassociated with autism.!
(A) Real sentence (1) (B) Real sentence (2)!
(C) Statement (1)!
(D) Statement (2)!
(E) [EQUIVALENCE]!Figure 2: Extracting statements from sentencesand annotating a semantic relation between themmantic segments, more than one statement can beextracted.
So, a statement can reflect the writer?saffirmation in the original sentence.
If the ex-tracted statements lack semantic information, suchas pronouns or other arguments, human annota-tors manually add the missing information.
Fi-nally we label pairs of statements with either oneof the semantic relations from Table 1 or with ?NORELATION,?
which means that two sentences (1)are not semantically related, or (2) have a relationother than relations defined in Table 1.3.2 Corpus Construction ProcedureWe automatically gather sentences on related top-ics by following the procedure below:1.
Retrieve documents related to a set number oftopics using a search engine2.
Extract real sentences that include major sub-topic words which are detected based on TF orDF in the document set3.
Reduce noise in data by using heuristics toeliminate advertisements and comment spam4.
Reduce the search space for identifying sen-tence pairs and prepare pairs, which look fea-sible to annotate.Dolan and Brockett (2005) proposed a methodto narrow the range of sentence pair candidatesand collect candidates of sentence-level para-phrases which correspond [EQUIVALENCE] in[AGREEMENT] class in our task.
It worked wellfor collecting valid sentence pairs from a largecluster which was constituted by topic-related sen-tences.
The method also seem to work well for[CONFLICT] relations, because lexical similar-ity based on bag-of-words (BOW) can narrow therange of candidates with this relation as well.We calculate the lexical similarity between thetwo sentences based on BOW.
We also used hy-ponym and synonym dictionaries (Sumida et al,2008) and a database of relations between predi-cate argument structures (Matsuyoshi et al, 2008)as resources.
According to our preliminary exper-iments, unigrams of KANJI and KATAKANA ex-pressions, single and compound nouns, verbs andadjectives worked well as features, and we calcu-late the similarity using cosine distance.
We didnot use HIRAGANA expressions because they arealso used in function words.4 Analyzing the CorpusFive annotators annotated semantic relations ac-cording to our specifications in 22 document setsas targets.
We have annotated target statementpairs with either [AGREEMENT], [CONFLICT]or [NO RELATION].
We provided 2,303 realsentence pairs to human annotators, and theyidentified 1,375 pairs as being invalid and 928pairs as being valid.
The number of annotatedstatement pairs are 1,505 ([AGREEMENT]:862,[CONFLICT]:126, [NO RELATION]:517).Next, to evaluate inter annotator agreement, 207randomly selected statement pairs were annotatedby two human annotators.
The annotators agreedin their judgment for 81.6% of the examples,which corresponds to a kappa level of 0.49.
Theannotation results are evaluated by calculating re-call and precision in which one annotation resultis treated as a gold standard and the other?s as theoutput of the system, as shown in Talbe 2.152Table 2: Inter-annotator agreement for 2 annota-torsAnnotator AAGR.
CON.
NONE TOTALAGR.
146 7 9 162Anno- CON.
0 13 1 14tator B NONE 17 4 10 31TOTAL 163 24 20 2075 DiscussionThe number of sentence pairs that annotators iden-tified as invalid examples shows that around 60%of all pairs were invalid, showing that there is stillroom to improve our method of collecting sen-tence pairs for the annotators.
Developing moreeffective methods of eliminating sentences pairsthat are unlikely to contain statements with plau-sible relations is important to improve annotatorefficiency.
We reviewed 50 such invalid sentencepairs, and the results indicate two major consider-ations: (1) negation, or antonyms have not been re-garded as key information, and (2) verbs in KANJIhave to be handled more carefully.
The polaritiesof sentences in all pairs were the same althoughthere are sentences which can be paired up withopposite polarities.
So, we will consider the po-larity of words and sentences as well as similaritywhen considering candidate sentence pairs.In Japanese, the words which consist ofKATAKANA expressions are generally nouns, butthose which contain KANJI can be nouns, verbs,or adjectives.
Sharing KATAKANA words wasthe most common way of increasing the simi-larity between sentences.
We need to assign ahigher weight to verbs and adjectives that containKANJI, to more accurately calculate the similaritybetween sentences.Another approach to reducing the search spacefor statement pairs is taken by Nichols et al(2009), who use category tags and in-article hyper-links to organize scientific blog posts into discus-sions on the same topic, making it easier to iden-tify relevant statements.
We are investigating theapplicability of these methods to the constructionof our Japanese corpus but suffer from the lack ofa richly-interlinked data source comparable to En-glish scientific blogs.6 ConclusionIn this paper, we described the ongoing construc-tion of a Japanese corpus consisting of statementpairs annotated with semantic relations for han-dling web arguments.
We designed an annotationscheme complete with the necessary semantic re-lations to support the development of statementmaps that show [AGREEMENT], [CONFLICT],and [EVIDENCE] between statements for assist-ing users in analyzing credibility of informationin Web.
We discussed the revelations made fromannotating our corpus, and discussed future direc-tions for refining our specifications of the corpus.We are planning to annotate relations for morethan 6,000 sentence pairs in this summer, and thefinished corpus will consist of around 10,000 sen-tence pairs.
The first release of our annotationspecifications and the corpus will be made avail-able on the Web1 this winter.AcknowledgmentsThis work is supported by the National Instituteof Information and Communications TechnologyJapan.ReferencesIdo Dagan, Oren Glickman, and Bernardo Magnini.
2005.The pascal recognising textual entailment challenge.
InProc.
of the PASCAL Challenges Workshop on RTE.Bill Dolan and Chris Brockett.
2005.
Automatical ly con-structing a corpus of sentential paraphrases.
In Proc.
ofthe IWP 2005, pages 9?16.William Mann and Sandra Thompson.
1988.
Rhetoricalstructure theory: towards a functional theory of text or-ganization.
Text, 8(3):243?281.Suguru Matsuyoshi, Koji Murakami, Yuji Matsumoto, , andKentaro Inui.
2008.
A database of relations betweenpredicate argument structures for recognizing textual en-tailment and contradiction.
In Proc.
of the ISUC 2008.Koji Murakami, Eric Nichols, Suguru Matsuyoshi, AsukaSumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-sumoto.
2009.
Statement map: Assisting informationcredibility analysis by visualizing arguments.
In Proc.
ofthe WICOW 2009, pages 43?50.Eric Nichols, Koji Murakami, Kentaro Inui, and Yuji Mat-sumoto.
2009.
Constructing a scientific blog corpus forinformation credibility analysis.
In Proc.
of the AnnualMeeting of ANLP.Dragomir Radev, Jahna Otterbacher, and Zhu Zhang.2003.
CSTBank: Cross-document Structure Theory Bank.http://tangra.si.umich.edu/clair/CSTBank.Dragomir R. Radev.
2000.
Common theory of informa-tion fusion from multiple text sources step one: Cross-document structure.
In Proc.
of the 1st SIGdial workshopon Discourse and dialogue, pages 74?83.Asuka Sumida, Naoki Yoshinaga, and Kentaro Torisawa.2008.
Boosting precision and recall of hyponymy rela-tion acquisition from hierarchical layouts in wikipedia.
InProc.
of the LREC 2008.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions in lan-guage.
Language Resources and Evaluation, 39(2-3):165?210.1http://cl.naist.jp/stmap/corpus/ja153
