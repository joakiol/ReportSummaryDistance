Discriminative Parse Reranking for Chinese with Homogeneous andHeterogeneous AnnotationsWeiwei Sun??
and Rui Wang?
and Yi Zhang??
?Department of Computational Linguistics, Saarland University?German Research Center for Artificial Intelligence (DFKI)D-66123, Saarbru?cken, Germany{wsun,rwang,yzhang}@coli.uni-saarland.deAbstractDiscriminative parse reranking has beenshown to be an effective technique to im-prove the generative parsing models.
Inthis paper, we present a series of exper-iments on parsing the Tsinghua ChineseTreebank with hierarchically split-mergegrammars and reranked with a perceptron-based discriminative model.
In addition tothe homogeneous annotation on TCT, wealso incorporate the PCTB-based parsingresult as heterogeneous annotation intothe reranking feature model.
The rerank-ing model achieved 1.12% absolute im-provement on F1 over the Berkeley parseron a development set.
The head labels inTask 2.1 are annotated with a sequencelabeling model.
The system achieved80.32 (B+C+H F1) in CIPS-SIGHAN-2010 Task 2.1 (Open Track) and 76.11(Overall F1) in Task 2.2 (Open Track)1.1 IntroductionThe data-driven approach to syntactic analysis ofnatural language has undergone revolutionary de-velopment in the last 15 years, ever since thefirst few large scale syntactically annotated cor-pora, i.e.
treebanks, became publicly available inthe mid-90s of the last century.
One and a halfdecades later, treebanks remain to be an expensivetype of language resources and only available for1This result is achieved with a bug-fixed version of thesystem and does not correspond to the numbers in the origi-nal evaluation report.a small number of languages.
The main issue thathinders large treebank development projects isthe difficulties in creating a complete and consis-tent annotation guideline which then constitutesthe very basis for sustainable parallel annotationand quality assurance.
While traditional linguisticstudies typically focus on either isolated languagephenomena or limited interaction among a smallgroups of phenomena, the annotation scheme intreebanking project requires full coverage of lan-guage use in the source media, and proper treat-ment with an uniformed annotation format.
Suchhigh demand from the practical application of lin-guistic theory has given rise to a countless num-ber of attempts and variations in the formaliza-tion frameworks.
While the harsh natural selec-tion set the bar high and many attempts failed toeven reach the actual annotation phase, a hand-ful highly competent grammar frameworks havegiven birth to several large scale treebanks.The co-existence of multiple treebanks withheterogeneous annotation presents a new chal-lenge to the consumers of such resources.
The im-mediately relevant task is the automated syntacticanalysis, or parsing.
While many state-of-the-artstatistical parsing systems are not bound to spe-cific treebank annotation (assuming the formal-ism is predetermined independently), almost allof them assume homogeneous annotation in thetraining corpus.
Therefore, such treebanks can notbe simply put together when training the parser.One approach would be to convert them into anuniformed representation, although such conver-sion is usually difficult and by its nature error-prune.
The differences in annotations constitutedifferent generative stories: i.e., when the pars-ing models are viewed as mechanisms to producestructured sentences, each treebank model will as-sociate its own structure with the surface string in-dependently.
On the other hand, if the discrimina-tive view is adopted, it is possible to use annota-tions in different treebanks as indication of good-ness of the tree in the original annotation.In this paper, we present a series of experi-ments to improve the Chinese parsing accuracyon the Tsinghua Chinese Treebank.
First, we usecoarse-to-fine parsing with hierarchically split-merge generative grammars to obtain a list of can-didate trees in TCT annotation.
A discriminativeparse selection model is then used to rerank thelist of candidates.
The reranking model is trainedwith both homogeneous (TCT) and heterogeneous(PCTB) data.
A sequence labeling system is usedto annotate the heads in Task 2-1.The remaining part of the paper is organized asfollows.
Section 2 reviews the relevant previousstudy on generative split-merge parsing and dis-criminative reranking models.
Section 3 describesthe work flow of our system participated in theCIPS-SIGHAN-2010 bake-off Task 2.
Section 4describes the detailed settings for the evaluationand the empirical results.
Section 5 concludes thepaper.2 BackgroundStatistical constituent-based parsing is popular-ized through the decade-long competition on pars-ing the Wall Street Journal sections of the EnglishPenn Treebank.
While the evaluation setup hasfor long seen its limitation (a frustratingly lowof 2% overall improvement throughout a decadeof research), the value of newly proposed pars-ing methods along the way has clearly much moreprofound merits than the seemly trivial increase inevaluation figures.
In this section we review twoeffective techniques in constituent-based statisti-cal parsing, and their potential benefits in parsingChinese.Comparing with many other languages, statisti-cal parsing for Chinese has reached early success,due to the fact that the language has relativelyfixed word order and extremely poor inflectionalmorphology.
Both facts allow the PCFG-basedstatistical modeling to perform well.
On the otherhand, the much higher ambiguity between basicword categories like nouns and verbs makes Chi-nese parsing interestingly different from the situ-ation of English.The type of treebank annotations also affectsthe performance of the parsing models.
Tak-ing the Penn Chinese Treebank (PCTB; Xueet al (2005)) and Tsinghua Chinese Treebank(TCT; Zhou (2004)) as examples, PCTB is anno-tated with a much more detailed set of phrase cat-egories, while TCT uses a more fine-grained POStagset.
The asymmetry in the annotation informa-tion is partially due to the difference of linguis-tic treatment.
But more importantly, it shows thatboth treebanks have the potential of being refinedwith more detailed classification, on either phrasalor word categories.
One data-driven approach toderive more fine-grained annotation is the hierar-chically split-merge parsing (Petrov et al, 2006;Petrov and Klein, 2007), which induces subcat-egories from coarse-grained annotations throughan expectation maximization procedure.
In com-bination with the coarse-to-fine parsing strategy,efficient inference can be done with a cascadeof grammars of different granularity.
Such pars-ing models have reached (close to) state-of-the-artperformance for many languages including Chi-nese and English.Another effective technique to improve parsingresults is discriminative reranking (Charniak andJohnson, 2005; Collins and Koo, 2005).
Whilethe generative models compose candidate parsetrees, a discriminative reranker reorders the listof candidates in favor of those trees which max-imizes the properties of being a good analysis.Such extra model refines the original scores as-signed by the generative model by focusing its de-cisions on the fine details among already ?good?candidates.
Due to this nature, the set of featuresin the reranker focus on those global (and poten-tially long distance) properties which are difficultto model with the generative model.
Also, sinceit is not necessary for the reranker to generate thecandidate trees, one can easily integrate additionalexternal information to help adjust the ranking ofthe analysis.
In the following section, we will de-BerkeleyParser...ParseRerankerTCTHeadClassifier...HHHA BC DCD BACTask 2.1Task 2.2Opene.g.
??
???
?
?
?PCTBParserFigure 1: Workflow of the Systemscribe the reranking model we developed for theCIPS-SIGHAN-2010 parsing tasks.
We will alsoshow how the heterogeneous parsing results canbe integrated through the reranker to further im-prove the performance of the system.3 System DescriptionIn this section, we will present our approachin detail.
The whole system consists of threemain components, the Berkeley Parser, the ParseReranker, and the Head Classifier.
The workflowis shown in Figure 1.
Firstly, we use the Berke-ley Parser trained on the TCT to parse the in-put sentence and obtain a list of possible parses;then, all the parses2 will be re-ranked by the ParseReranker; and finally, the Head Classifer will an-notate the head information for each constituent2In practice, we only take the top n parses.
We have dif-ferent n values in the experiment settings, and n is up to 50.Algorithm 1: The Perptron learning proce-dure.input : Data {(xt, yt), t = 1, 2, ...,m}Initialize: w?
(0, ..., 0)1for i = 1, 2, ..., I do2for t =SHUFFLE (1, ...,m) do3y?t =4arg maxy?GENbestn (xt) w>?
(xt, y)if y?t 6= yt then5w?
w+(?
(xt, yt)??
(xt, y?t ))6end7end8wi ?
w9end10return aw = 1I?Ii=1 wi11on the best parse tree.
For parse reranking, wecan extract features either from TCT-style parsesor together with the PCTB-style parse of the samesentence.
For example, we can check whetherthe boundary predictions given by the TCT parserare agreed by the PCTB parser.
Since the PCTBparser is trained on a different treebank from TCT,our reranking model can be seen as a method touse a heterogenous resource.
The best parse treegiven by the Parse Reranker will be the result forTask 2.2; and the final output of the system willbe the result for Task 2.1.
Since we have alreadymentioned the Berkeley Parser in the related work,we will focus on the other two modules in the restof this section.3.1 Parse RerankerWe follow Collins and Koo (2005)?s discrimina-tive reranking model to score possible parse treesof each sentence given by the Berkeley Parser.Previous research on English shows that struc-tured perceptron (Collins, 2002) is one of thestrongest machine learning algorithms for parsereranking (Collins and Duffy, 2002; Gao et al,2007).
In our system, we use the averaged per-ceptron algorithm to do parameter estimation.
Al-gorithm 1 illustrates the learning procedure.
Theparameter vector w is initialized to (0, ..., 0).
Thelearner processes all the instances (t is from 1 ton) in each iteration (i).
If current hypothesis (w)fails to predict xt, the learner update w throughcalculating the difference between ?
(xt, y?t ) and?
(xt, yt).
At the end of each iteration, the learnersave the current model as w + i, and finally allthese models will be added up to get aw.3.2 FeaturesWe use an example to show the features we extractin Figure 2.vpv?eatnpv?buyuJDE?n?
?appleFigure 2: An ExampleRules The context-free rule itself:np?
v + uJDE + np.Grandparent Rules Same as the Rules, butalso including the nonterminal above the rule:vp(np?
v + uJDE + np)Bigrams Pairs of nonterminals from the left toright of the the rule.
The example rule would con-tribute the bigrams np(STOP, v), np(v,uJDE),np(uJDE,np) and np(np, STOP).Grandparent Bigrams Same as Bigrams, butalso including the nonterminal above the bigrams.For instance, vp(np(STOP, v))Lexical Bigrams Same as Bigrams, but withthe lexical heads of the two nonterminals also in-cluded.
For instance, np(STOP,?
).Trigrams All trigrams within the rule.
Theexample rule would contribute the trigramsnp(STOP, STOP, v), np(STOP, v,uJDE),np(v,uJDE,np), np(uJDE,np,STOP) andnp(np,STOP,STOP).Combination of Boundary Words andRules The first word and the rule (i.e.?+(np?
v + uJDE + np)), the last wordand the rule one word before and the rule, oneword after and the rule, the first word, the lastword and the rule, and the first word?s POS, lastword?s POS and the rule.Combination of Boundary Words and PhrasalCategory : Same as combination of boundarywords and rules, but substitute the rule with thecategory of current phrases.Two level Rules Same as Rules, but alsoincluding the entire rule above the rule:vp?
v + (np?
v + uJDE + np)Original Rank : The logarithm of the originalrank of n-best candidates.Affixation features In order to better handleunknown words, we also extract morphologi-cal features: character n-gram prefixes and suf-fixes for n up to 3.
For example, for word/tagpair ???
?/n, we add the following fea-tures: (prefix1,?,n), (prefix2,?
?,n), (prefix3,??
?,n), (suffix1,?,n), (suffix2,?
?,n), (suf-fix3,??
?,n).Apart from training the reranking model usingthe same dataset (i.e.
the TCT), we can also useanother treebank (e.g.
the PCTB).
Although theyhave quite different annotations as well as the datasource, it would still be interesting to see whethera heterogenous resource is helpful with the parsereranking.Consist Category If a phrase is also analyzedas one phrase by the PCTB parser, both the TCTand PCTB categories are used as two individualfeatures.
The combination of the two categoriesare also used.Inconsist Category If a phrase is not analyzedas one phrase by the PCTB parser, the TCT cate-gory is used as a feature.Number of Consist and Inconsist phrases Thetwo number are used as two individual featuers.We also use the ratio of the number of consistphrases and inconsist phrase (we add 0.1 to eachnumber for smoothing), the ratio of the numberof consist/inconsist phrases and the length of thecurrent sentence.POS Tags For each word, the combination ofTCT and PCTB POS tags (with or without wordcontent) are used.3.3 Head ClassifierFollowing (Song and Kit, 2009), we apply a se-quence tagging method to find head constituents.We suggest readers to refer to the original paperfor details of the method.
However, since the fea-ture set is different, we give the discription ofthem in this paper.
To predict whether currentphrase is a head phrase of its parent, we use thesame example above (Figure 2) for convenience.If we consider np as our current phrase, the fol-lowing features are extracted,Rules The generative rule, vp?
v + (np).Category of the Current Phrase and its Parentnp, vp, and (np, vp).Bigrams and Trigrams (v, np), (np,STOP),(STOP, v,np), and (np,STOP,STOP).Parent Bigrams and Trigrams vp(v, np),vp(np,STOP), vp(STOP, v, np),vp(np,STOP,STOP).Lexical Unigram The first word ?, the lastword ?
?, and together with the parent, (vp,?
)and (vp,??
)4 Evaluation4.1 DatasetsThe dataset used in the CIPS-ParsEval-2010 eval-uation is converted from the Tsinghua ChineseTreebank (TCT).
There are two subtasks: (1)event description sub-sentence analysis and (2)complete sentence parsing.
On the assumptionthat the boundaries and relations between theseevent description units are determined separately,the first task aims to identify the local fine-grainedsyntactic structures.
The goal of the second taskis to evaluate the performance of the automaticparsers on complete sentences in real texts.
Thetraining dataset is a mixture of several genres, in-cluding newspaper texts, encyclopedic texts andnovel texts.The annotation in the dataset is different tothe other frequently used Chinese treebank (i.e.PCTB) Whereas TCT annotation strongly reflectsearly descriptive linguistics, PCTB draws primar-ily on Government-Binding (GB) theory from1980s.
PCTB annotation differs from TCT anno-tation from many perspectives:?
TCT and PCTB have different segmentationstandards.?
TCT is somehow branching-rich annota-tion, while PCTB annotation is category-rich.
Specifically the topological tree struc-tures is more detailed in TCT, and thereare not many flat structures.
However con-stituents are detailed classified, namely thenumber of phrasal categories is small.
On thecontrary, though flat structures are very com-mon in PCTB, the categorization of phrasesis fine-grained.
In addition, PCTB containsfunctional information.
Function tags ap-pended to constituent labels are used to in-dicate additional syntactic or semantic infor-mation.?
TCT contains head indices, making headidentification of each constituent an impor-tant goal of task 1.?
Following the GB theory, PCTB assumethere are movements, so there are empty cat-egory annotation.
Because of different theo-retical foundations, there are different expla-nations for a series of linguistic phenomenasuch as the usage of function word ??
?.In the reranking experiments, we also use aparser trained on PCTB to provide more syntac-tic clues.4.2 SettingIn order to gain a representative set of trainingdata, we use cross-validation scheme described in(Collins, 2000).
The dataset is a mixture of threegenres.
We equally split every genre data into 10subsets, and collect three subset of different gen-res as one fold of the whole data.
In this way, wecan divide the whole data into 10 balanced sub-sets.
For each fold data, a complement parser istrained using all other data to produce multiple hy-potheses for each sentence.
This cross-validationn 1 2 5 10 20 30 40 50F1 79.97 81.62 83.51 84.63 85.59 86.07 86.38 86.60Table 1: Upper bound of f-score as a function of number n of n-best parses.scheme can prevent the initial model from beingunrealistically ?good?
on the training sentences.We use the first 9 folds as training data and the lastfold as development data for the following exper-iments.
For the final submission of the evaluationtask, we re-train a reranking model using all 10folds data.
All reranking models are trained with30 iterations.For parsing experiments, we use the Berkeleyparser3.
All parsers are trained with 5 iterationsof split, merge, smooth.
To produce PCTB-styleanalysis, we train the Berkeley parse with PCTB5.0 data that contains 18804 sentences and 508764words.
For the evaluation of development experi-ments, we used the EVALB tool4 for evaluation,and used labeled recall (LR), labeled precision(LP) and F1 score (which is the harmonic meanof LR and LP) to measure accuracy.For the head classification, we use SVMhmm5,an implementation of structural SVMs for se-quence tagging.
The main setting of learning pa-rameter is C that trades off margin size and train-ing error.
In our experiments, the head classifica-tion is not sensitive to this parameter and we setit to 1 for all experiments reported.
For the kernelfunction setting, we use the simplest linear kernel.4.3 Results4.3.1 Upper Bound of RerankingThe upper bound of n-best parse reranking isshown in Table 1.
From the 1-best result we seethat the base accuracy of the parser is 79.97.
2-best and 10-best show promising oracle-rate im-provements.
After that things start to slow down,and we achieve an oracle rate of 86.60 at 50-best.4.3.2 Reranking Using Homogeneous DataTable 2 summarizes the performance of the ba-sic reranking model.
It is evaluated on short sen-3http://code.google.com/p/berkeleyparser/4http://nlp.cs.nyu.edu/evalb/5http://www.cs.cornell.edu/People/tj/svm_light/svm_hmm.htmltences (less than 40 words) from the developmentdata of the task 2.
When 40 reranking candidatesare used, the model gives a 0.76% absolute im-provement over the basic Berkeley parser.POS(%) LP(%) LR(%) F1Baseline 93.59 85.60 85.36 85.48n = 2 93.66 85.84 85.54 85.69n = 5 93.62 86.04 85.73 85.88n = 10 93.66 86.22 85.85 86.04n = 20 93.70 86.19 85.87 86.03n = 30 93.70 86.32 86.00 86.16n = 40 93.76 86.40 86.09 86.24n = 50 93.73 86.10 85.81 85.96Table 2: Reranking performance with differentnumber of parse candidates on the sentences thatcontain no more than 40 words in the developmentdata.4.3.3 Reranking Using Heterogeneous DataTable 3 summarizes the reranking performanceusing PCTB data.
It is also evaluated on short sen-tences of the task 2.
When 30 reranking candi-dates are used, the model gives a 1.12% absoluteimprovement over the Berkeley parser.
Compar-ison of Table 2 and 3 shows an improvement byusing heterogeneous data.POS(%) LP(%) LR(%) F1n = 2 93.70 85.98 85.67 85.82n = 5 93.75 86.52 86.19 86.35n = 10 93.77 86.64 86.29 86.47n = 20 93.79 86.71 86.34 86.53n = 30 93.80 86.72 86.48 86.60n = 40 93.80 86.54 86.22 86.38n = 50 93.89 86.73 86.41 86.57Table 3: Reranking performance with differentnumber of parse candidates on the sentences thatcontain no more than 40 words in the developmentdata.Task 1 ?B+C?-P ?B+C?-R ?B+C?-F1 ?B+C+H?-P ?B+C+H?-R ?B+C+H?-F1 POSOld data 82.37 83.05 82.71 79.99 80.65 80.32 81.87Table 4: Final results of task 1.Task 2 dj-P dj-R dj-F1 fj-P fj-R fj-F1 Avg.
POSOld data 79.37 79.27 79.32 71.06 73.22 72.13 75.72 81.23New data 79.60 79.13 79.36 70.01 75.94 72.85 76.11 89.05Table 5: Final results of task 2.4.3.4 Head ClassificationThe head classification performance is evalu-ated using gold-standard syntactic trees.
For eachconstituent in a gold parse tree, a structured clas-sifier is trained to predict whether it is a head con-stituent of its parent.
Table 6 shows the overallperformance of head classification.
We can seethat the head classification can achieve a high per-formance.P(%) R(%) F?=198.59% 98.20% 98.39Table 6: Head classification performance withgold trees on the development data.4.3.5 Final ResultTable 4 and 5 summarize the final results.
Herewe use the reranking model with heterogeneousdata.
The second line of Table 5 shows the offi-cal final results.
In this submission, we trained amodel using an old version of training data.
Notethat, the standard of POS tags of the ?old?
versionis different from the latest version which is alsoused as test data.
For example, the name of sometags are changed.
The third line of Table 46 showsthe results predicted by the newest data7.
This re-sult is comparable to other systems.5 ConclusionIn this paper, we described our participation ofthe CIPS-SIGHAN-2010 parsing task.
The gen-6There are two sentences that are not parsed by the Berke-ley parser.
We use a simple strategy to solve this problem:We first roughly segment the sentence according to punctu-ation; Then the parsed sub-sentences are merged as a singlezj.7We would like to thank the organizer to re-test our newsubmission.erative coarse-to-fine parsing model is integratedwith a discriminative parse reranking model, aswell as a head classifier based on sequence la-beling.
We use the perceptron algorithm to trainthe reranking models and experiment with bothhomogenous and heterogenous data.
The resultsshow improvements over the baseline in bothcases.AcknowledgmentsThe first author is supported by the German Aca-demic Exchange Service (DAAD).
The secondauthor is supported by the PIRE scholarship pro-gram; the third author thanks DFKI and the Clus-ter of Excellence on Multimodal Computing andInteraction for their support of the work.ReferencesCharniak, E. and M Johnson.
2005. oarse-to-fine n-best parsing and maxent discriminative reranking.In Proceedings of ACL, pages 173?180.Collins, Michael and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
InProceedings of 40th Annual Meeting of the Associa-tion for Computational Linguistics, pages 263?270,Philadelphia, Pennsylvania, USA, July.
Associationfor Computational Linguistics.Collins, Michael and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
In Compu-tational Linguistics, volume 31(1), pages 25?69.Collins, Michael.
2000.
Discriminative reranking fornatural language parsing.
In Computational Lin-guistics, pages 175?182.
Morgan Kaufmann.Collins, Michael.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In EMNLP ?02:Proceedings of the ACL-02 conference on Empiri-cal methods in natural language processing, pages1?8, Morristown, NJ, USA.
Association for Com-putational Linguistics.Gao, Jianfeng, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study ofparameter estimation methods for statistical naturallanguage processing.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 824?831, Prague, Czech Repub-lic, June.
Association for Computational Linguis-tics.Petrov, S. and D. Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL-2007, Rochester, NY, USA, April.Petrov, Slav, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440,Sydney, Australia, July.
Association for Computa-tional Linguistics.Song, Yan and Chunyu Kit.
2009.
Pcfg parsing withcrf tagging for head recognition.
In Proceedings ofthe CIPS-ParsEval-2009.Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Zhou, Qiang.
2004.
Annotation scheme for chinesetreebank (in chinese).
Journal of Chinese Informa-tion Processing, 18(4):1?8.
