Proceedings of the 6th Workshop on Statistical Machine Translation, pages 85?91,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsMeteor 1.3: Automatic Metric for Reliable Optimization and Evaluation ofMachine Translation SystemsMichael Denkowski and Alon LavieLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15232, USA{mdenkows,alavie}@cs.cmu.eduAbstractThis paper describes Meteor 1.3, our submis-sion to the 2011 EMNLP Workshop on Sta-tistical Machine Translation automatic evalua-tion metric tasks.
New metric features includeimproved text normalization, higher-precisionparaphrase matching, and discrimination be-tween content and function words.
We includeRanking and Adequacy versions of the metricshown to have high correlation with humanjudgments of translation quality as well as amore balanced Tuning version shown to out-perform BLEU in minimum error rate trainingfor a phrase-based Urdu-English system.1 IntroductionThe Meteor1 metric (Banerjee and Lavie, 2005;Denkowski and Lavie, 2010b) has been shown tohave high correlation with human judgments in eval-uations such as the 2010 ACL Workshop on Statisti-cal Machine Translation and NIST Metrics MATR(Callison-Burch et al, 2010).
However, previousversions of the metric are still limited by lack ofpunctuation handling, noise in paraphrase matching,and lack of discrimination between word types.
Weintroduce new resources for all WMT languages in-cluding text normalizers, filtered paraphrase tables,and function word lists.
We show that the addition ofthese resources to Meteor allows tuning versions ofthe metric that show higher correlation with humantranslation rankings and adequacy scores on unseen1The metric name has previously been stylized as ?ME-TEOR?
or ?METEOR?.
As of version 1.3, the official stylizationis simply ?Meteor?.test data.
The evaluation resources are modular, us-able with any other evaluation metric or MT soft-ware.We also conduct a MT system tuning experimenton Urdu-English data to compare the effectivenessof using multiple versions of Meteor in minimumerror rate training.
While versions tuned to varioustypes of human judgments do not perform as wellas the widely used BLEU metric (Papineni et al,2002), a balanced Tuning version of Meteor consis-tently outperforms BLEU over multiple end-to-endtune-test runs on this data set.The versions of Meteor corresponding to thetranslation evaluation task submissions, (Rankingand Adequacy), are described in Sections 3 through5 while the submission to the tunable metrics task,(Tuning), is described in Section 6.2 New Metric Resources2.1 Meteor NormalizerWhereas previous versions of Meteor simply strippunctuation characters prior to scoring, version 1.3includes a new text normalizer intended specifi-cally for translation evaluation.
The normalizer firstreplicates the behavior of the tokenizer distributedwith the Moses toolkit (Hoang et al, 2007), includ-ing handling of non-breaking prefixes.
After tok-enization, we add several rules for normalization,intended to reduce meaning-equivalent punctuationstyles to common forms.
The following two rulesare particularly helpful:?
Remove dashes between hyphenated words.
(Example: far-off ?
far off)85?
Remove full stops in acronyms/initials.
(Exam-ple: U.N. ?
UN)Consider the behavior of the Moses tokenizerand Meteor normalizers given a reference trans-lation containing the phrase ?U.S.-basedorganization?
:Moses: U.S.-based organizationMeteor ?1.2: U S based organizationMeteor 1.3: US based organizationOf these, only the Meteor 1.3 normalizationallows metrics to match all of the followingstylizations:U.S.-based organizationUS-based organizationU.S.
based organizationUS based organizationWhile intended for Meteor evaluation, use of thisnormalizer is a suitable preprocessing step for othermetrics to improve accuracy when reference sen-tences are stylistically different from hypotheses.2.2 Filtered Paraphrase TablesThe original Meteor paraphrase tables (Denkowskiand Lavie, 2010b) are constructed using the phrasetable ?pivoting?
technique described by Bannardand Callison-Burch (2005).
Many paraphrases suf-fer from word accumulation, the appending of un-aligned words to one or both sides of a phrase ratherthan finding a true rewording from elsewhere in par-allel data.
To improve the precision of the para-phrase tables, we filter out all cases of word accumu-lation by removing paraphrases where one phrase isa substring of the other.
Table 1 lists the number ofphrase pairs found in each paraphrase table beforeand after filtering.
In addition to improving accu-racy, the reduction of phrase table sizes also reducesthe load time and memory usage of the Meteor para-phrase matcher.
The tables are a modular resourcesuitable for other MT or NLP software.2.3 Function Word ListsCommonly used metrics such as BLEU and ear-lier versions of Meteor make no distinction betweencontent and function words.
This can be problem-atic for ranking-based evaluations where two systemLanguage Phrase Pairs After FilteringEnglish 6.24M 5.27MCzech 756K 684KGerman 3.52M 3.00MSpanish 6.35M 5.30MFrench 3.38M 2.84MTable 1: Sizes of paraphrase tables before and after filter-ingLanguage Corpus Size (sents) FW LearnedEnglish 836M 93Czech 230M 68French 374M 85German 309M 92Spanish 168M 66Table 2: Monolingual corpus size (words) and number offunction words learned for each languageoutputs can differ by a single word, such as mistrans-lating either a main verb or a determiner.
To improveMeteor?s discriminative power in such cases, we in-troduce a function word list for each WMT languageand a new ?
parameter to adjust the relative weightgiven to content words (any word not on the list) ver-sus function words (see Section 3).
Function wordlists are estimated according to relative frequency inlarge monolingual corpora.
For each language, wepool freely available WMT 2011 data consisting ofEuroparl (Koehn, 2005), news (sentence-uniqued),and news commentary data.
Any word with relativefrequency of 10?3 or greater is added to the func-tion word list.
Table 2 lists corpus size and numberof function words learned for each language.
In ad-dition to common words, punctuation symbols con-sistently rise to the tops of function word lists.3 Meteor ScoringMeteor evaluates translation hypotheses by align-ing them to reference translations and calculatingsentence-level similarity scores.
This section de-scribes our extended version of the metric.For a hypothesis-reference pair, the search spaceof possible alignments is constructed by identifyingall possible matches between the two sentences ac-cording to the following matchers:Exact: Match words if their surface forms are iden-86tical.Stem: Stem words using a language-appropriateSnowball Stemmer (Porter, 2001) and match if thestems are identical.Synonym: Match words if they share member-ship in any synonym set according to the Word-Net (Miller and Fellbaum, 2007) database.Paraphrase: Match phrases if they are listed asparaphrases in the paraphrase tables described inSection 2.2.All matches are generalized to phrase matcheswith a start position and phrase length in each sen-tence.
Any word occurring less than length posi-tions after a match start is considered covered bythe match.
The exact and paraphrase matchers sup-port all five WMT languages while the stem matcheris limited to English, French, German, and Spanishand the synonym matcher is limited to English.Once matches are identified, the final alignment isresolved as the largest subset of all matches meetingthe following criteria in order of importance:1.
Require each word in each sentence to be cov-ered by zero or one matches.2.
Maximize the number of covered words acrossboth sentences.3.
Minimize the number of chunks, where a chunkis defined as a series of matches that is contigu-ous and identically ordered in both sentences.4.
Minimize the sum of absolute distances be-tween match start positions in the two sen-tences.
(Break ties by preferring to align wordsand phrases that occur at similar positions inboth sentences.
)Given an alignment, the metric score is calculatedas follows.
Content and function words are iden-tified in the hypothesis (hc, hf ) and reference (rc,rf ) according to the function word lists described inSection 2.3.
For each of the matchers (mi), countthe number of content and function words coveredby matches of this type in the hypothesis (mi(hc),mi(hf )) and reference (mi(rc), mi(rf )).
Calculateweighted precision and recall using matcher weights(wi...wn) and content-function word weight (?
):P =?iwi ?
(?
?mi(hc) + (1?
?)
?mi(hf ))?
?
|hc|+ (1?
?)
?
|hf |Target WMT09 WMT10 CombinedEnglish 20,357 24,915 45,272Czech 11,242 9,613 20,855French 2,967 5,904 7,062German 6,563 10,892 17,455Spanish 3,249 3,813 7,062Table 3: Human ranking judgment data from 2009 and2010 WMT evaluationsR =?iwi ?
(?
?mi(rc) + (1?
?)
?mi(rf ))?
?
|rc|+ (1?
?)
?
|rf |The parameterized harmonic mean of P and R (vanRijsbergen, 1979) is then calculated:Fmean =P ?R?
?
P + (1?
?)
?RTo account for gaps and differences in word order,a fragmentation penalty is calculated using the totalnumber of matched words (m, average over hypoth-esis and reference) and number of chunks (ch):Pen = ?
?
(chm)?The Meteor score is then calculated:Score = (1?
Pen) ?
FmeanThe parameters ?, ?, ?, ?, and wi...wn are tunedto maximize correlation with human judgments.4 Parameter Optimization4.1 Development DataThe 2009 and 2010 WMT shared evaluation datasets are made available as development data forWMT 2011.
Data sets include MT system outputs,reference translations, and human rankings of trans-lation quality.
Table 3 lists the number of judgmentsfor each evaluation and combined totals.4.2 Tuning ProcedureTo evaluate a metric?s performance on a data set, wecount the number of pairwise translation rankingspreserved when translations are re-ranked by met-ric score.
We then compute Kendall?s ?
correlationcoefficient as follows:?
=concordant pairs?discordant pairstotal pairs87Tune ?
(WMT09) Test ?
(WMT10)Lang Met1.2 Met1.3 Met1.2 Met1.3English 0.258 0.276 0.320 0.343Czech 0.148 0.162 0.220 0.215French 0.414 0.437 0.370 0.384German 0.152 0.180 0.170 0.155Spanish 0.216 0.240 0.310 0.326Table 5: Meteor 1.2 and 1.3 correlation with rankingjudgments on tune and test dataFor each WMT language, we learn Meteor pa-rameters that maximize ?
over the combined 2009and 2010 data sets using an exhaustive parametricsweep.
The resulting parameters, listed in Table 4,are used in the default Ranking version of Meteor1.3.For each language, the ?
parameter is above 0.5,indicating a preference for content words over func-tion words.
In addition, the fragmentation penaltiesare generally less severe across languages.
The ad-ditional features in Meteor 1.3 allow for more bal-anced parameters that distribute responsibility forpenalizing various types of erroneous translations.5 Evaluation ExperimentsTo compare Meteor 1.3 against previous versions ofthe metric on the task of evaluating MT system out-puts, we tune a version for each language on 2009WMT data and evaluate on 2010 data.
This repli-cates the 2010 WMT shared evaluation task, allow-ing comparison to Meteor 1.2.
Table 5 lists correla-tion of each metric version with ranking judgmentson tune and test data.
Meteor 1.3 shows significantlyhigher correlation on both tune and test data for En-glish, French, and Spanish while Czech and Germandemonstrate overfitting with higher correlation ontune data but lower on test data.
This overfitting ef-fect is likely due to the limited number of systemsproviding translations into these languages and thedifficulty of these target languages leading to sig-nificantly noisier translations skewing the space ofmetric scores.
We believe that tuning to combined2009 and 2010 data will counter these issues for theofficial Ranking version.Meteor-1.2 r Meteor-1.3 rTune / Test MT08 MT09 MT08 MT09MT08 0.620 0.625 0.650 0.636MT09 0.612 0.630 0.642 0.648Tune / Test P2 P3 P2 P3P2 -0.640 -0.596 -0.642 -0.594P3 -0.638 -0.600 -0.625 -0.612Table 6: Meteor 1.2 and 1.3 correlation with adequacyand H-TER scores on tune and test data5.1 Generalization to Other TasksTo evaluate the impact of new features on otherevaluation tasks, we follow Denkowski and Lavie(2010a), tuning versions of Meteor to maximizelength-weighted sentence-level Pearson?s r correla-tion coefficient with adequacy and H-TER (Snoveret al, 2006) scores of translations.
Data sets in-clude 2008 and 2009 NIST Open Machine Trans-lation Evaluation adequacy data (Przybocki, 2009)and GALE P2 and P3 H-TER data (Olive, 2005).For each type of judgment, metric versions are tunedand tested on each year and scores are compared.We compare Meteor 1.3 results with those from ver-sion 1.2 with results shown in Table 6.
For bothadequacy data sets, Meteor 1.3 significantly outper-forms version 1.2 on both tune and test data.
Theversion tuned on MT09 data is selected as the officialAdequacy version of Meteor 1.3.
H-TER versionseither show no improvement or degradation due tooverfitting.
Examination of the optimal H-TER pa-rameter sets reveals a mismatch between evalua-tion metric and human judgment type.
As H-TERevaluation is ultimately limited by the TER aligner,there is no distinction between content and functionwords, and words sharing stems are considered non-matches.
As such, these features do not help Meteorimprove correlation, but rather act as a source of ad-ditional possibility for overfitting.6 MT System Tuning ExperimentsThe 2011 WMT Tunable Metrics task consists ofusing Z-MERT (Zaidan, 2009) to tune a pre-builtUrdu-English Joshua (Li et al, 2009) system to anew evaluation metric on a tuning set with 4 refer-ence translations and decoding a test set using the re-sulting parameter set.
As this task does not provide a88Language ?
?
?
?
wexact wstem wsyn wparEnglish 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60Czech 0.95 0.20 0.60 0.80 1.00 ?
?
0.40French 0.90 1.40 0.60 0.65 1.00 0.20 ?
0.40German 0.95 1.00 0.55 0.55 1.00 0.80 ?
0.20Spanish 0.65 1.30 0.50 0.80 1.00 0.80 ?
0.60Table 4: Optimal Meteor parameters for WMT target languages on 2009 and 2010 data (Meteor 1.3 Ranking)devtest set, we select a version of Meteor by explor-ing the effectiveness of using multiple versions ofthe metric to tune phrase-based translation systemsfor the same language pair.We use the 2009 NIST Open Machine Transla-tion Evaluation Urdu-English parallel data (Przy-bocki, 2009) plus 900M words of monolingual datafrom the English Gigaword corpus (Parker et al,2009) to build a standard Moses system (Hoang etal., 2007) as follows.
Parallel data is word alignedusing the MGIZA++ toolkit (Gao and Vogel, 2008)and alignments are symmetrized using the ?grow-diag-final-and?
heuristic.
Phrases are extracted us-ing standard phrase-based heuristics (Koehn et al,2003) and used to build a translation table and lex-icalized reordering model.
A standard SRI 5-gramlanguage model (Stolke, 2002) is estimated frommonolingual data.
Using Z-MERT, we tune this sys-tem to baseline metrics as well as the versions ofMeteor discussed in previous sections.
We also tuneto a balanced Tuning version of Meteor designed tominimize bias.
This data set provides a single setof reference translations for MERT.
To account forthe variance of MERT, we run end-to-end tuning 3times for each metric and report the average resultson two unseen test sets: newswire and weblog.
Testset translations are evaluated using BLEU, TER, andMeteor 1.2.
The parameters for each Meteor versionare listed in Table 7 while the results are listed inTable 8.The results are fairly consistent across both testsets: the Tuning version of Meteor outperformsBLEU across all metrics while versions of Meteorthat perform well on other tasks perform poorly intuning.
This illustrates the differences between eval-uation and tuning tasks.
In evaluation tasks, metricsare engineered to score 1-best translations from sys-tems most often tuned to BLEU.
As listed in Table 7,NewswireTuning Metric BLEU TER Met1.2BLEU 23.67 72.48 50.45TER 25.35 59.72 48.60TER-BLEU/2 26.25 61.66 49.69Meteor-tune 24.89 69.54 51.29Meteor-rank 19.28 94.64 49.78Meteor-adq 22.86 77.27 51.40Meteor-hter 25.23 66.71 50.90WeblogTuning Metric BLEU TER Met1.2BLEU 17.10 76.28 41.86TER 17.07 64.32 39.75TER-BLEU/2 18.14 65.77 40.68Meteor-tune 18.07 73.83 42.78Meteor-rank 14.34 98.86 42.75Meteor-adq 16.76 81.63 43.43Meteor-hter 18.12 70.47 42.28Table 8: Average metric scores for Urdu-English systemstuned to baseline metrics and versions of Meteorthese parameters are often skewed to emphasize thedifferences between system outputs.
In the tuningscenario, MERT optimizes translation quality withrespect to the tuning metric.
If a metric is biased (forexample, assigning more weight to recall than preci-sion), it will guide the MERT search toward patho-logical translations that receive lower scores acrossother metrics.
Balanced between precision and re-call, content and function words, and word choiceversus fragmentation, the Tuning version of Meteoris significantly less susceptible to gaming.
Chosenas the official submission for WMT 2011, we be-lieve that this Tuning version of Meteor will furthergeneralize to other tuning scenarios.89Task ?
?
?
?
wexact wstem wsyn wparRanking 0.85 0.20 0.60 0.75 1.00 0.60 0.80 0.60Adequacy 0.75 1.40 0.45 0.70 1.00 1.00 0.60 0.80H-TER 0.40 1.50 0.35 0.55 1.00 0.20 0.60 0.80Tuning 0.50 1.00 0.50 0.50 1.00 0.50 0.50 0.50Table 7: Parameters for Meteor 1.3 tasks7 ConclusionsWe have presented Ranking, Adequacy, and Tun-ing versions of Meteor 1.3.
The Ranking and Ad-equacy versions are shown to have high correlationwith human judgments except in cases of overfittingdue to skewed tuning data.
We believe that theseoverfitting issues are lessened when tuning to com-bined 2009 and 2010 data due to increased varietyin translation characteristics.
The Tuning version ofMeteor is shown to outperform BLEU in minimumerror rate training of a phrase-based system on smallUrdu-English data and we believe that it will gener-alize well to other tuning scenarios.
The source codeand all resources for Meteor 1.3 and the version ofZ-MERT with Meteor integration will be availablefor download from the Meteor website.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Proc.of ACL WIEEMMTS 2005.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proc.
ofACL2005.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 Joint Workshop on Sta-tistical Machine Translation and Metrics for MachineTranslation.
In Proc.
of ACL WMT/MetricsMATR2010.Michael Denkowski and Alon Lavie.
2010a.
Choosingthe Right Evaluation for Machine Translation: an Ex-amination of Annotator and Automatic Metric Perfor-mance on Human Judgment Tasks.
In Proc.
of AMTA2010.Michael Denkowski and Alon Lavie.
2010b.
METEOR-NEXT and the METEOR Paraphrase Tables: ImproveEvaluation Support for Five Target Languages.
InProc.
of ACL WMT/MetricsMATR 2010.Qin Gao and Stephan Vogel.
2008.
Parallel Implemen-tations of Word Alignment Tool.
In Proc.
of ACLWSETQANLP 2008.Hieu Hoang, Alexandra Birch, Chris Callison-burch,Richard Zens, Rwth Aachen, Alexandra Constantin,Marcello Federico, Nicola Bertoldi, Chris Dyer,Brooke Cowan, Wade Shen, Christine Moran, and On-dej Bojar.
2007.
Moses: Open Source Toolkit for Sta-tistical Machine Translation.
In Proc.
of ACL 2007.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of NAACL/HLT 2003.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proc.
of MT Sum-mit 2005.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar Zaidan.
2009.Joshua: An Open Source Toolkit for Parsing-basedMachine Translation.
In Proc.
of WMT 2009.George Miller and Christiane Fellbaum.
2007.
WordNet.http://wordnet.princeton.edu/.Joseph Olive.
2005.
Global Autonomous Language Ex-ploitation (GALE).
DARPA/IPTO Proposer Informa-tion Pamphlet.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proc.
of ACL 2002.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword Fourth Edi-tion.
Linguistic Data Consortium, LDC2009T13.Martin Porter.
2001.
Snowball: A language for stem-ming algorithms.
http://snowball.tartarus.org/texts/.Mark Przybocki.
2009.
NIST OpenMachine Translation 2009 Evaluation.http://www.itl.nist.gov/iad/mig/tests/mt/2009/.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proc.
of AMTA 2006.Andreas Stolke.
2002.
SRILM - an Extensible LanguageModeling Toolkit.
In Proc.
of ICSLP 2002.C.
van Rijsbergen, 1979.
Information Retrieval, chap-ter 7.
2nd edition.90Omar F. Zaidan.
2009.
Z-MERT: A Fully ConfigurableOpen Source Tool for Minimum Error Rate Trainingof Machine Translation Systems.
The Prague Bulletinof Mathematical Linguistics.91
