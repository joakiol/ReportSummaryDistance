//i/II/IIlII//Modularity in Inductively-Learned Word Pronunciation Systems *Antal van den Bosch 1, Ton Weijters 2, Walter Daelemans 11 ILK / Computational LinguisticsTilburg UniversityP.O.
Box 90153NL-5000 LE TilburgThe Netherlands{ant alb, walt er}@kub, nl2 Department of Information TechnologyEindhoven University of TechnologyP.O.
Box 513NL-5600 MB EindhovenThe NetherlandsA.
J.M.M.
Weijt ers@tm, tue.nlAbstractIn leading morpho-phonological theories andstate-of-the-art text-to-speech systems it isassumed that word pronunciation cannot belearned or performed without in-between anal-yses at several abstraction levels (e.g., mor-phological, graphemic, phonemic, syllabic, andstress levels).
We challenge this assump-tion for the case of English word pronunci-ation.
Using IGTR~B, an inductive-learningdecision-tree algorithms, we train and testthree word-pronunciation systems in which thenumber of abstraction levels (implemented assequenced modules) is reduced from five, viathree, to one.
The latter system, classifyingletter strings directly as mapping to phonemeswith stress markers, yields signitlcemtly bettergenerali~tion accuracies than the two multi-module systems.
Analyses of empirical resultsindicate that positive utility etfects of sequenc-ing modules are outweighed by cascading er-rors passed on between modules.1 IntroductionLearning word pronunciation can be a hard taskwhen the relation between the spelling of a languageand its corresponding pronunciation is many-to-many.
The English writing system and its pronunci-ation are a notoriously complex example, mused byan apparent conflict between analog~/and inconsis-~enc~/:Analogy.
When two words or word chunks have asimilar spelling, they tend to have a slmil~r pro-nunciation.
This tendency (which generalises toother language tasks as well) is usually referredto as the analogy principle(De Saussure, 1916;Yvon, 1996; Daelemans, 1996).
*This research was partially performed by the firstand second author at the Department ofComputer Sci-ence of the Universiteit Manstricht (The Netherlands),and partially in the context of the "Induction of Lin-guistic Knowledge" research programme, partially sup-ported by the Foundation for Language Speech and Logic(TSL), funded by the Netherlands Organization for Sci-entific Research (NWO).Inconsistency.
Much of the analogy in Englishword pronunciation is disrupted by productiveand complex word morphology, word stress, andgmphematics.Influential pre-Chomskyan \]ingu~tic theories havebeen pointing at the analogy principle as the under-lying principle for language learning (De Sanssure,1916), and at induction as the reasoning methodfor generalising from learned instances of languagetasks to new instances through analogy (Bloomfield,1933).
However, methods and resources (e.g., com-puter technology) were not available then to demon-strate how induction through analogy could be em-ployed to learn and model language tasks.
Partlydue to this lack of demonstrating power, Chomskylater stated"...
I don't see any way of explaining theresulting final state \[of language learning\]in terms of any proposed general devel-opmental mecha, i~_m that has been sug-gested by artificial intelligence, sensorimo-tot mechanisms, or anything else" (Chore-sky, in (Piatelll-Palmadni, 1980), p. 100).Chomsky's argument is based on the assump-tion that generic learning methods uch as induc-tion cannot discover autonomously essential levelsof abstraction in language processing tasks.
Ap-pl;ed to morpho-phonology, the argument s ates thatgeneric learning methods are not able to discovermorphology, graphematies, and stress patterns au-tonomonsly when learning word pronunciation, al-though this knowledge appears essential.
Phonologi-cal and morphological theories, influenced by Chom-skyan theory across the board since the publica-tion of spy.
(Chomsky and Halle, 1968), have gen-erally adopted the idea of abstraction levels in var-ious guises (e.g., levels, tapes, tiers, grids) (Gold-smith, 1976; Liberman and Prince, 1977; Kosken-niemi, 1984; Mohanan, 1986).
Although there is nogeneral consensus on which levels of abstraction canbe discerned in phonology and morphology, there isa rough, global agreement on the fact that wordscan be represented on different abstraction levels asvan den Bosch, Weiflers and Daelemans 185 Modul,~rity in Word Pronunciation systemsAntal van den Bosch, Ton Weijters and Walter Daelemans (1998) Modularity in Inductively-Learned Word PronunciationSystems.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational NaturalLanguage Learning, ACL, pp 185-194.strings ofletters, graphemes, morphemes, phonemes,syllables, and stress patterns.According to these leading morpho-phonologicaltheories, systems that (learn to) convert spelledwords to phonemic words in one pass, i.e., withoutmaking use of abstraction levels, axe assumed to beunable to generalise to new cases: going throughthe relevant abstraction levels is deemed essential toyield correct conversions of previously unseen words.This assumption implies that if one wants to builda system that converts text to speech, one shouldimplement explicitly the relevant levels of abstrac-tion.
Such explicit implementations of abstractionlevels can indeed be witnessed in many state-of-the-art speech synthesisers, implemented as(sequential)modules (Allen, Hunnicutt, and Klatt, 1987; Daele-mans, 1988).In this paper we challenge the assumption thatlevels of abstraction must be made explicit in learn-ing and performing the word-pronunciation task.We do this by applying an inductive-learning al-gorithm from machine learning to word pronunci-ation.
From a wealth of existing algorithms in ma-chine learning (Mitchell, 1997), we choose IGTlt~B(Daelemans, Van den Bosch, and Weijters, 1997), aninductive-learning decision-tree l arning algorithm.IGTR~.E is a fast algorithm which has been demon-strated to be applicable to language tasks (Vanden Bosch and Daelemans, 1993; Van den Bosch,Daclemans, and Weijters, 1996; Daelemans, Van denBosch, and Weijters, 1997).
We construct IGTRE~decision trees for word pronunciation, and performempirical tests to estimate the trees' generalisationaccuracy, i.e., their ability to process new, unseenword-pronunciation instances correctly.Rather than constructing and testing a single sys-tem, our approach is to test dflferent moduiari-sations of the word-pronunciation task systemati-cally, to allow for an empirical comparison of word-pronunciation systems with and without he explicitlearning of abstraction levels.
First, we train (byinductive learning) and test a word-pronunciationmodel reflecting linguistic assumptions on abstrac-tion levels quite closely: the model is composed offive sequentially-coupled modules.
Second, we trainand test a model in which the number of modulesis reduced to three, integrating two pairs of levelsof abstraction.
Third, we train and test a modelperforming word pronunciation i a single pass, i.e.,without modular decomposition.The paper is structured as follows: first, in Sec-tion 2 we provide a description of IGTREE, the dataon which the IGTRI~B is trained and tested, and theapplied experimental methodology.
Second, in Sec-tion 3 we introduce the three word-pronunciationsystems, and for each system we describe the exper-iments performed and discuss the results obtained.In Section 4 we compare the three systems and anal-yse the consequences of modularisation.
Section 5briefly mentions related work on inductive learningof word pronunciation.
Section 6 summarises theresults obtained and lists some points of discussion.2 A lgor i thm,  Data ,  Methodo logy2.1 Algor i thm: IGTREEIGTR~.E (Daelemans, Van den Bosch, and Weij-ters, 1997) is a top-down induction of decision trees(TDIDT) algorithm (Breiman et al, 1984; Quinlan,1993).
TDIDT is a widely-used method in super-vised machine learning (Mitchell, 1997).
IGTREEis designed as an optlmi~ed approximation of theinstance-based learning algorithm IBI-IQ (Daele-mans and Van den Bosch, 1992; Dademans, Vanden Bosch, and Weijters, 1997).
In I6TR~E, infor-mation gain is used as a guiding function to com-press a data base of instances of a certain task intoa decision tree 1.
Instances are stored in the tree aspaths of connected nodes ending in leaves which con-tain classification i formation.
Nodes are connectedvia arcs denoting feature values.
Information gainis used in IGTREE to determine the order in whichfeature values are added as arcs to the tree.
Informa-tion gain is a function from information theory, andis used similarly in ID3 (Qululan, 1986) and c4.5(Qnlnlan, 1993).The idea behind computing the information gainof features is to interpret he training set (i.e., theset of task instances for which all classifications avegiven and which are used for training the learningalgorithm) as an information source capable of gen-erating a number of messages (i.e., classifications)with a certain probability.
The information entropyH of such an information source can be comparedin turn for each of the features characterlsing theinstances (let n equal the number of features), tothe average information entropy of the informationsource when the value of those features axe known.Data-base information entropy H(D) is equal to thenumber of bits of information eeded to know theclassification given an instance.
It is computed byequation 1, where p~ (the probability of classifica-tion i) is estimated by its relative frequency in thetraining set.= - p jog2p,  (I)iTo determine the information gain of each of the nfeatures fx .
.
.
fn, we compute the average informa-tion entropy for each feature and subtract it fromthe information entropy of the data base.
To com-pute the information entropy for a feature fl, givenin equation 2, we take the weighted average informa-tion entropy of the data base restricted to each pos-sible value for the feature.
The expression DLf~=~ \]X IGTB.BE can function with any feature weightingmethod, such as gain ratio (QuinIaa, 1993); for all ex-periments reported here, information gain was used.van den Bosch, Weijters and Daelemans 186 Modularity in Word Pronunciation systemsIIIIIIl|III/|///refers to those patterns in the data base that havevalue vj for feature f~, j is the number of possiblevalues of f~, and V is the set of possible values forfeature /~.
Finally, \]DI is the number of patterns inthe (sub) data base.
'v,/EVInformation gain of feature fi is then obtained byequation 3.G(y,) = IZ(D) - H(Z~t~,\]) (3)In IGTREE, feature-value information is stored in thedecision tree on arcs.
The first feature values, storedas arcs connected to the tree's top node, axe thoserepresenting the values of the feature with the high-est information gain, followed at the second level ofthe tree by the values of the feature with the second-highest information gain, etc., until the classifica-tion information represented by a path is unambigu-ous.
Knowing the value of the most important fea-ture may already uniquely identify a classification, inwhich case the other feature values of that instanceneed not be stored in the tree.
Alternatively, it maybe necessary for disambiguation tostore s long pathin the tree.Apart from storing uniquely identified class labelsat leafs, IGTREE stores at each non-terminal node in-formation on the most probable classification giventhe path so far.
The most probable classification isthe most frequently occurring classification in thesubset of instances being compressed in the pathbeing expanded.
Storing the most probable classat non-terminal nodes is essential when processingnew instances.
Processing a new instance involvestraversing the tree by matching the feature values ofthe test instance with arcs the tree, in the order ofthe feature information gain.
Traversal ends when(i) a leaf is reached or when (fi) matching a featurevalue with an arc fails.
In case (i), the classificationstored at the leaf is taken as output.
In case (ii),we use the most probable classification on the lastnon-terminal node most recently visited instead.2.2 Data Acquisit ion and PreprocessingThe resource of word-pronunciation instances usedin our experiments i  the CELEX lexical data baseof English (Burnage, 1990).
All items in the cgLv.xdata bases contain hyphenated spelling, syllabifiedand stressed phonemic transcriptions, and detailedmorphological nalyses.
We extracted from the En-giish data base of CZLZX all the above information,resulting in a data base containing 77,565 uniqueitems (word forms with syllabified, stressed pronun-ciations and morphdogical segmentations).For use in experiments with learning algorithms,the data is preprocessed to derive fixed-size in-stances.
In the experiments reported in this papervan den Bosch, Weijters and Daelemans 187different morpho-phonological (sub)tasks are inves-tigated; for each (sub)task, an instance base (train-ing set) is constructed containing instances producedby windowing (Sejnowski and Rosenbezg, 1987) andattaching to each instance the classification appro-priate for the (sub)task under investigation.
Table 1displays example instances derived from the sampleword booking.
With this method, for each (sub) taskan instance base of 675,745 instances i built.In the table, six classification fields axe shown, oneof which is a composite field; each field refers to oneof the (sub)tasks investigated here.
M stands formorphological decomposition: determine whether aletter is the initial letter of a morpheme (class '1')or not (class 'O').
x is graphemic parsing2: deter-mine whether a letter is the first or only letter of agrapheme (class '1') or not (class '0'); a grapheme isa cluster of one or more letters mapping to a singlephoneme.
G is grapheme-phoneme conversion: de-termine the phonemic mapping of the middle letter.y is syllabification: determine whether the middlephoneme is syllable-initial, s is stress assignment:determine the stress level of the middle phoneme.Finally, GS is integrated grapheme-phoneme conver-sion and stress assignment.
The example instancesin Table 1 show that each (sub)task is phrased as aclassification task on the basis of windows of lettersor phonemes (the stress assignment task s is inves-tigated with both letters and phonemes as input).Each window represents a snapshot of a part of aword or phonemic transcription, and is labelled bythe classification associated with the middle letter ofthe window.
For example, the first letter-window in-stance __book is linked with label '1' for the morpho-logical segmentation task (M), since the middle letterb is the first letter of the morpheme book;, the otherinstance labelled with morphological-segmentationclass '1 ~ is the instance with i in the middle, sincei is the first letter of the (inflectional) morphemeing.
Classifications may either be binary ('1' or'0') for the segmentation tasks (M, A, and y), orhave more values, such as 62 possible phonemes (~)or tbxee stress markers (primary, secondary, or nostress, s), or a combination of these classes (159 com-bined phonemes and stress markers, Gs).2.3 MethodologyOur empirical study focuses on measuring the abil-ity of the IQTP~Z learning algorithm to use theknowledge accumulated during learning for the clas-sification of new, unseen instances of the same(sub)task, i.e., we measure their generalisation accu-racy.
(Weiss and Kulikowski, 1991) describe n-foldcross valida~iolz (~z-fold cv) as a procedure for mea-2Graphemic parsing is not represented in the CELBXdata.
We used an automatic alignment algorithm(Daelemans and Van den Bosch, 1997) to determinewhich letters axe the first o~ only letters of a grapheme.Modularity in Word Pronunciation systemsinstancenumber12347letter-window instancesleftcontext I focus-'_ _ "b_ _ b o_b  o oboo  koo  k iok  i nk i n gfightcontext~o kok  ik i ni n gn g _gJ classificationsII M A Q s Gs1 1 /b /  1 /b /1o 1 lul o lulOo o i - I  o I-IOo l l k l  OlklO1 1 IU 0 IWO0 1 I~10 I~100 0 I-I, ,,0 I-I0phoneme-window instancescontext focus_ /b/- fi>l InlI(>l lul I'1fi>l Inl I'1 Iktlul I'1 Ikl I11I - I  Ikl I~1 I~1Ikl  Id I~JI I'1fight elassif.context Y s/u/ /-/ /k/ 1 1/-/ /k/ /U 0 0/k/ h/ /~/  0 0/~/ /~/ /-/ 1 0/~/  /-/ .
0 0/-/ _ 0 0- 0 0Table 1: Example of instances generated from the word booking, with dassificstious for all of the subtasksinvestigated, viz.
M, A, Q, Y, s, and Gs.suzing generalisation accaxacy.
For our experimentswith IGTRBE, we set up 10-fold cv experiments con-sisting of five steps.
(i) On the basis of a data set, npaxtitionings axe generated of the data set into onetra~ing set containing ( (n -1 ) /n ) th  of the data set,and one test set contslnlng ( l /n)th of the data set,per partitioning.
For each partitioning, the threefollowing steps axe repeated: (ii) Information-gainvalues for all (seven) features axe computed on thebasis of the trAi~ing set (cf.
Subsection 2.1).
(iii)IQTRE~.
is applied to the trai~i~g set, yielding aninduced decision tree (el.
Subsection 2.1).
(iv) Thetree is tested by letting it classify all instances in thetest set, which results in a percentage ofincorrectlyclassified test instances.
(v) When each of the n foldshas produced an error percentage on test material,a mean generalisation error of the leaxned model iscomputed.
(Weiss and Kulikowski, 1991) argue thatby using n-fold cv, preferably with n _> 10, one canretrieve a good estimate of the true generalisationerror of a leaxning algorithm given an instance base.Mean results can be employed further in significancetests.
In our experiments, n = 10, and one-tailed t-tests axe performed.3 Three  word-pronunc ia t ionarch i tec turesOut experiments axe grouped in three series, eachinvolving the application of IGTR~.B to a paxticu-la~ word-pronunciation system.
The a~chitecturesof these systems axe displayed in Figure 1.
In thefollowing subsections, each system is introduced, anoutline is given of the experiments performed on thesystem, and the results a~e briefly discussed.3.1  M-A-G-Y-SThe axchitectu~e of the M-A-G-Y-S system is inspixedby SGUND1 (Hunnicutt, 1976; Hunnicutt, 1980),the word-pronunciation subsystem of the MIT~kLKtext-to-speech system (Allen, Hunnicutt, and Klatt,1987).
When the MITALK system is faced with an un-known word, sounD1 produces on the basis of thatvan den Bosch, Weijters and Daelemans 188word a phonemic transcription with stress markers(Allen, Hunnieutt, and Klatt, 1987).
This word-pronunciation process is divided into the followingfive processing components:1. morphological segmentalion, which we imple-ment as the module referred to as M;2. graphemic parsing, module A;3. grapheme-phoneme conversion, module G;4. sfllabifica~ion, module y;5. stress assignment, module s.The axchiteeture ofthe M-A-G-Y-S system is visu-alised in the left of Figure 1.
It can be seen that therepresentations i clude direct output from previousmodules, as well as representations from eaxlier mod-ules.
For example, the s module takes as input thesyllable boundaries generated by the Y module, butalso the phoneme string generated by the G module,and the morpheme boundaxles generated by the Mmodule.M-A-G-Y-S is put to the test by applying IGTREEin 10-fold cv experiments o the five subtasks, con-necting the modules after tr~i~i~g, and measuringthe combined score on correctly classified phonemesand stress maxkers, which is the desired output ofthe word-pronunciation system.
An individual mod-ule can be trained on data from C~.L~.X directly asinput, but this method ignores the fact that mod-ules in a working modular system can be expectedto generate some amount of error.
When one modulegenerates an error, the subsequent module receivesthis error as input, assumes it is correct, and maygenerate another error.
In a five-module system, thistype of cascading errors may seriously hamper gen-eralisation accuracy.
To counteract this potentialdisadvantage, modules can also be trained on theoutput of previous modules.
Modules cannot be ex-pected to leaxn to repair completely random, irreg-ular errors, but whenever a previous module makescon.sistent errors on a specific input, this may berecoguised by the subsequent module.
Having de-tected a consistent error, the subsequent module isModularity in Word Pronunciation systemsIIIIIIIIIkkkIIIIIIIIIIIIIIwdtten~phoneme transcnp~onwffn stresswr~en wo~phonemic ~ar, scnpeonwith slre~swritten wordphonerr~c Wanscdp~onw~ s,~ssM- morpholog~e analysisA-  9raphernk: ~G-  gra~erne-pho~emeconversionY-  s ~ nS - s~asm~com~n~ gr~-pho.GS-  convegonan~stress ass~nmer~tFigure 1: Architectures of the three investigated word-pronunciation systems.
Left: M-A-G-Y-S; middle:M-G-S; right: GS.
Rectangular boxes represent modules; the letter in the box corresponds to the subtask aslisted in the legends (far right).
Arrows depict data flows from the raw input or a module, to s module orthe output.J12.0 -10.0 -8.08.04.
{)2.00.07.675-5,14 I 5.251.N),M A d V S10.59Figure 2: Generalisation errors on the M-A-G-Y-$system in terms of the percentage ofincorrectly alas-sifted test instances by IGTREE on the five subtasksM, A, G, Y, and s, and on phonemes and stress mark-ers jointly (PS).then able to repair the error and continue with suc-cessful processing.
Earlier experiments performedon the tasks investigated in this paper have shownthat classification errors on  test instances are indeedconsistently and significantly decreased when mod-ules are trained on the output of previous modulesrather than on data extracted irectly from C~.LP.X(Van den Bosch, 1997).
Therefore, we train the M-A-G-Y-S system, with IGTRE~., by training the modulesof the system on the output of predeceasing modules.We henceforth refer to this type of training as adap-tive tra;-;-g, referring to the adaptation of a moduleto the errors of a predecessing module.Figure 2 displays the results obtained with IGTREEunder the adaptive variant of M-A-G-Y-S.
The fig-ure shows all percentages (displayed above the bars;error bars on top of the main bars indicate standardvan den Bosch, Weijters and Daelemans 189deviations) of incorrectly classified instances for eachof the five subtasks, and a joint error on incorrectlyclassified phonemes with stress markers, which is thedesired output of the system.
The latter classifica-tion error, labelled PS in Figure 2, regards classifi-cation of an instance as incorrect if either or bothof the phoneme and stress marker is incorrect.
Thefigure shows that the joint error on phonemes andstress markers is 10.59% of test instances, on aver-age.
Computed in terms of transcribed words, only35.89% of all test words are converted to stressedphonemic transcriptions flawlessly.
The joint erroris lower than the sum of the errors on the G subtaskand the s subtask, 12.95%, suggesting that about20% of the incorrectly classified test instances in-volve an incorrect classification of both the phonemeand the stress marker.8.2 M-G-SThe subtasks of graphemic parsing (A) andgrapheme-phoneme conversion (G) are clearly re-lated.
While A attempts to parse s letter stringinto grsphemes, G converts gzaphemes tophonemes.Although they axe performed independently in M-A-G-Y-S, they can be integrated easily when theelass-'l'-instances of the A task are mapped to theiIassociated phoneme rather than '1', and the class-'0'-instances axe mapped to a phonemic null, /-/,rather than '0' (of.
Table 1).
This task integrationis also used in the NETTALK model (Sejnowski andRosenberg, 1987).
A similar argument can be madefor integrating the syllabification and stress assign-ment modules into a single stress-assignment mod-ule.
Stress markers, in our definition of the stress-assignment subtask, are placed solely on the posi-tions which are also marked as syllable boundaries(i.e., on syllable-initial phonemes).
Removing theModularity in Word Pronunciation systemsg_0=12.010.08.06.0.4,0-2,0.0.07.86M G S PSFigure 3: Generalisation er rors  on the M-G-S systemin terms of the percentage of incorrectly classifiedtest instances by IGTREE on the three snbtasks M,G, and s, and on phonemes and stress markers jointly(PS).12"0 l10.0 g~ 6.0 \] 7.41=~?0 3.79 3.97=~ 4.0- g2.0-0,0 .G S PSFigure 4: Percentage of generalisation ezrozs madeby IGTRBE on the GS task, in terms of the percent-age incorrectly classified test instances as well as onphonemes and stress assignments computed sepa-rately.syllabification subtask makes finding those syllableboundaries which are rdevant for stress assignmentan integrated paxt of stress assignment.
Syllabifica-tion (Y) and stress assignment (s) can thus be inte-grated in a single stress-ussignment module s.When both pairs of modules are reduced to sin-gle modnles, the three-modnle system M-G-S is ob-tained.
Figure 1 displays the architecture of theM-G-S system in the middle.
Experiments on thissystem axe performed analogous to the experimentswith the M-A-G-Y-S system; Figuxe 3 displays the av-erage percentages ofgeneralisation errors generatedby mTRP.E on the three subtasks and phonemes andstress markers jointly (the error bar labelled PS).Removing raphemic parsing (A) and syllabifica-tion (Y) as explicit in-between modules yields bet-ter accuracies on the grapheme-phoneme conver-sion (G) and stress assignment (s) subtasks thanin the M-A-G-Y-S system.
Both differences are sig-nltlcant; for G, (t(19) = 43.70,p < 0.001), and forS (t(19) = 32.00,p < 0.001).
The joint accaxacyon phonemes and stress markers is also significantlybetter in the M-G-S system than in the M-A-G-Y-Ssystem (g(37.50,p < 0.001).
Ditferent from M-A-G-Y-S, the sum of the errors on phonemes and stressmarkers, 8.09%, is hardly more than the joint er-ror on PSs, 7.86%: there is haxdly an overlap ininstances with incorrectly classified phonemes andstress markers.
The percentage of flawlessly pro-cessed test words is 44.89%, which is maxkedly bet-ter than the 35.89% of M-A-G-Y-S.3.3 GSGS is a single-module system in which only one clas-sification task is performed in one pass.
The GStask integrates grapheme-phoneme conversion andstress assignment: o classify letter windows as cor-responding to a phoneme wi~h a stress marker (PS).In the GS system, a PS can be either (i) a phonemeor a phonemic null with stress marker '0', or (ii)a phoneme with stress marker '1' (i.e., the firstphoneme of a syllable receiving primary stress), or(iii) a phoneme with stress marker '2' (i.e., the firstphoneme of a syllable receiving secondary stress).The simple architecture ofGS, which does not reflectany linguistic expert knowledge about decomposi-tions of the word-pronunciation task, is visualisedas the rightmost architectaxe in Figure 1.
It onlyassumes the presence of letters at the input, andphonemes and stress maxkers at the output.
Ta-ble 1 displays example instance PS classificationsgenerated on the basis of the word booking.
Thephonemes with stress markers (PSs) axe denoted bycomposite labels.
For example, the first instance inTable 1, __book, maps to class label ~b/l, denot-ing a /b /  which is the first phoneme of a syllablereceiving primary stress.The experiments with GS were performed with thesame data set of word pronunciation as used with M-X-G-Y-S and M-G-S.
The number of PS classes (i.e.,all possible combinations of phonemes and stressmarkers) occurring in this data base of tasks is 159.Figure 4 displays the generalisation errors in termsof incorrectly classified test instances.
The figurealso displays the percentage of classification errorsmade on phonemes and stress markers computedseparately.IGTEEE yields significantly better generalisationaccuracy on phonemes and stress markers, bothjointly and independently.
In terms of PSs, the accu-racy on GS is significantly better than that of M-G-Swith (t(19) = 40.48,p < 0.001), and that of M-A-G-Y-S with (~(19) = 6.90,p < 0.001).
Its accuracyon flawlessly transcribed test words, 59.38%, is alsoconsiderably better than that of the modnlax sys-tems.
Compared to accuracies reported in relatedzeseaxch on learning English word pronunciation (Se-jnowski and Rosenbezg, 1987; Wolpert, 1990; Diet-van den Bosch, Weijters and Daelemans 190 Modularity in Word Pronunciation systemsII|llIlIIIII///,4,4A5000004O0OOO--~ 300000E= 2ooooo c:100000SS GGG)M MI~A-G-Y-S M-G-$ C~Figure 5: Average numbers of nodes in the decisiontrees generated by IGTREE for the M-A-G-Y-S, M-G-S, and Gs systems.
Compartments indicate thenumbers of nodes needed for the trees of the subtasksspecified by their labels.terich, Kiid, and Bakifi, 1995; Yvon, 1996) and ongeneral quality demands of text-to-speech applica-tions, an error of 3.79% on phonemes and 30.62%on words can be considered adequate, though stillnot excellent (?von, 1996; Van den Bosch, 1997).4 Comparisons of M-A-G-Y-S,M-G-S, and GSWe have given significance results howing that, un-der our experimental conditions and using IGTREEas the learning algorithm, optimal generalisation ac-curacy on word pronunciation is obtained with GS,the system that does not incorporate any explicitdecomposition of the word-pronunciation task.
Inthis section we perform two additional comparisonsof the three systems.
First, we compare the sizes ofthe trees constructed by IGTREE on the three sys-tems; second, we analyse the positive and negativeeffects of learning the subtasks in their specific sys-tems' context.Tree sizesAn advantage ofusing less or no decompositions iterms ofcomputationul ei~ciency is the total amountof memory needed for storing the trees.
Althoughthe applieation of IGTREE generally results in smalltrees that fit well inside small computer memories(for out modulax (sub)tasks, tree sizes waxy from64,821 nodes for the M-modules to 153,678 nodesfor the G-module in M-A-G-Y-S, occupying 453,747to 1,075,746 bytes of memory), keeping five trees inmemory would not be a desirable feature for a sys-tem optimised on memory use.
Figure 5 displaysthe summed number of nodes for each of the fourIGTReE-tramed systems under the adaptive vaxiant.Each bax is divided into compartments indicatingthe amount of nodes in the trees generated for eachof the modular subtasks.van den Bosch.
Weijters and Daelemans 191Figure 5 shows that the model with the best gen-eralisation accuracy, GS, is also the model taking upthe smallest number of nodes.
The amount of nodesin the single Gs tree, 111,062, is not only smallerthan the sum of the amount of nodes needed forthe G and s modules in the M-G-S system (204,345nodes); it is even smaller than the single tree con-structed for the G subtask in the M-G-S system(125,182 nodes).A minor difference in tree size can be seen betweenthe trees built for the G-module in the M-G-S system,125,182 nodes, and the G-module in the M-A-G-Y-Ssystem, 153,678 nodes.
A similar difference can beseen for the s-modules, taking up 79,163 nodes inthe M-G-S system, and 96,998 nodes in the M-A-G-Y-S system.
The size of the trees built for modulesappears to increase when the module is preceded bymore modules, which suggests that IGTREE is facedwith a more complex task, including potentially er-roneous output from more modules, when buildinga tree for a module further down a sequence of mod-ules.Ut i l i ty effectsThe paxticunax sequence of the five modules as inthe M-A-G-Y-S system reflects a number of assump-tions on the utilit~l of using output from one subtaskas input to another subtask.
Morphological knowl-edge is useful as input to grapheme-phoneme conver-sion (e.g., to avoid pronouncing ph in loophole as/ f / ,or red in barred as/ted/) ;  graphemic parsing is use-ful as input to grapheme-phoneme conversion (e.g.,to avoid the pronunciation of gh in through); etc.Thus, feeding the output of a module A into a subse-quent module B implies that one expects to performbetter on module B with A's input than without.The accuracy results obtained with the modules ofthe M-A-G-Y-S, M-G-S ,  and  GS systems can serve astests for their respective underlying utility assump-tions, when they axe compared to the accuracies ob-tained with their snbtasks learned in isolation.To measure the utility/effects ofincluding the out-puts of modules as inputs to other modules, we per-formed the following experiments:1.
We applied IGTREE in 10-fold cv experiments oeach of the five subtasks M, A, G, Y, and s, onlyusing letters (with the M, A, G, and s snbtasks)or phonemes (with the Y and the s subtasks)as input, and their respective classification asoutput (cf.
Table 1).
The input is directly ex-tracted from CELEX.
These experiments pro-vide the baseline score for each subtask, andaxe referred to as the isolated experiments.2.
We applied IGTIIEE in 10-fold Cv experimentsto all subtasks of the M-A-G-Y-S, M-G-S, aald GSsystems, training end testing on input extracteddirectly from CP.LEX.
The results from these ex-periments reflect what wound be the accuracy ofModularity in Word Pronunciation systems~ a t i o n  errorisolated \[ ideal (utility) I actual (utility)M-A-G-Y-SM 5.14 5.14 (o.oo) 5.14 (0.00)A 1.39 1.66 (--0.27) 1.50 (--0.11)Q 3.72 3.68 (+0.04) 7.67 (-3.95)y 0.45 0.75 (-0.30) 2.63 (-2.16)s 7.96 2.67 (+5.29) 5.28 (+2.68)M-G-SM 5.14 5.14 (0.00) 5.14 (O.O0)G 3.72 3.66 (+0.06) 3.99 (-0.27)s 7.96 3.97 (+3.99) 4.10 (+3.86)GSo 3.721 - - 3.79 (-0.07)s 4.71 I - - 3.97 (+0.74)Table 2: Overview of utility effects of learning sub-tasks (M, A, G, Y, and s) as modules or partial tasksin the M-A-O-Y-S, M-O-S, and GS systems.
For eachmodule, in each system, the utility of tra;~ing themodule with ideal data (middle) and actual, modu-lar data under the adaptive variant (fight), is com-pared against the accuracy obtained with learningthe subtasks in isolation (left).
Accuracies are givenin percentage of incorrectly classified test instances.the modular systems when each module wouldperform perfectly flawless.
We refer to these ex-periments as idealWith the results of these experiments we mea-sure, for each subtask in each of the three systems,the utility effect of including the input of precedingmodules, for the ideal case (with input straight fromCP.LEX) as well as for the actual case (with inputfrom preceding modules).
A utility effect is the dif-ference between IGTItEE'S generalJsation error on thesubtask in modular context (either ideal or actual)and its accuracy on the same subtask in isolation.Table 2 lists all computed utility effects.For the ease of the M-A-G-Y-S system, it canbe seen that the only large utility effect, even inthe ideal case, could be obtained with the stress-assignment subtask.
In the isolated case, the inputconsists of phonemes; in the M-A-G-Y-S system, theinput contains morpheme boundaries, phonemes,and syllable boundaries.
The ideal positive effecton the s module of 5.29% less errors turns outto be a positive effect of 2.68% in the actual sys-tem.
The latter positive effect is outweighed by arather large negative utility effect on the grapheme-phoneme conversion task of-3.95%.
Both the A andy subtasks do not profit from morphological bound-aries as input, even in the ideal case; in the actual M-A-G-Y-S system, the utility effect of including mor-phological boundaries from M and phonemes from Gin the syllabification module Y is markedly negative:-2.16%.In the M-G-S system, the utility effects are gen-erally less negative than in the M-A-G-Y-S system.There is a small utility effect in the ideal casewith including morphological boundaries as inputto grapheme-phoneme conversion; in the actual M-Q-S system, the utility effect is negative (-0.27%).The stress-assignment module benefits from includ-ing morphological boundaries and phonemes in itsinput, both in the ideal case and in the actual M-G-S system.The Gs system does not contain separate mod-ules, but it is possible to compare the errors madeon phonemes and stress assignments separately tothe results obtained on the subtasks learned in isola-tion.
Grapheme-phoneme conversion is learned withalmost he same accuracy when learned in isolationas when learned as partial task of the Gs task.
Learn-ing the grapheme-phoneme task, IGTR~.~.
is neitherhelped nor hampered significantly by learning stressassignment simultaneously.
There is a positive util-ity effect in learning stress assignment, however.When stress assignment is learned in isolation withletters as input, IGTI~B classifies 4.71% of test in-stances incorrectly, on average.
(This is a lower errorthan obtained with learning stress assignment on thebasis of phonemes, indicating that stress assignmentshould take letters as input rather than phonemes.
)When the stress-assignment task is learned alongwith grapheme-phoneme conversion in the Gs sys-tem, a marked improvement is obtained: 0.74% lessclassification errors are made.Snmmaxising, comparing the accuracies on modu-lax subtasks to the accuracies on their isolated coun-terpart asks shows only a few positive utility effectsin the actual system, all obtained with stress as-signment.
The largest utility effect is found on thestress-assigument subtask of M-G-S.
However, thispositive utility eifect does not lead to optimal ac-curacy on the s subtask; in the Gs system, stressassignment is performed with letters as input, yield-ing the best accuracy on stress assignment in ourinvestigations, viz.
3.97% incorrectly classified testinstances.5 Re la ted  workThe classical NETTXLE paper by (Sejnowski andP~osenberg, 1987) can be seen as a primaxy sourceof inspiration for the present study; it has been sofor a considerable amount of related work.
Althoughit has been cfiticised for being vague and presumptu-ons and for presenting eneralisation accuracies thatcan be improved easily with other learning meth-ods (Stanfill and Waltz, 1986; Wolpert, 1990; Weij-ters, 1991; Yvon, 1996), it was the first paper toinvestigate gtapheme-phoneme conversion as an in-teresting application for general-purpose learning al-gofithms.
However, few reports have been made onvan den Bosch, Weijters and Daelemans 192 Modularity in Word Pronunciation systemsss|mmmIIImIEI///IIII/I///the joint accuracies on stress markers and phonemesin work on the NETTALK data.
To our knowledge,only (Shsvlik, Mooney, and Towell, 1991) and (Di-etterich, Hild, and Bnkiri, 1995) provides uch re-ports.
In terms of incorrectly processed test in-stances, (Shavlik, Mooney, and Towcll, 1991) ob-tain better performance with the back-propagationalgorithm trained on distributed output (27.7% er-rors) than with the IV3 (Qnlnlan, 1986) decision-treealgorithm (34.7% errors), both trained and testedon small non-overlapping sets of about 1,000 in-stances.
(Dietterich, Hild, and Baklri, 1995) re-ports similar errors on similarly-sized tradning andtest sets (29.1% for BP and 34.4% for Iv3); with alarger training set of 19,003 words fxom the NETT&LKdata and an input encoding tlfteen letters, previousphoneme and stress classifications, some domain-specific features, and error-correcting output codesIV3 generates 8.6% errors on test instances (Diet-terich, Hild, and Bakiri, 1995), which does not com-pare favourably to the results obtained with theNETTALK-Iike GS task (a valid comparison cannotbe made; the data employed in the current studycontains considerably more instances).An interesting counterargument against the repre-sentation of the word-pronunciation task using fixed-size windows, put forward by Yvon (Yvon, 1996), isthat an induetive-leaxning approach to grapheme-phoneme conversion should be based on associatingvaxiable-length c unks of letters to variable-lengthchunks of phonemes.
The chunk-based approachis shown to be applicable, with adequate accu-racy, to several corpora, including corpora of Frenchword pronunciations and, as mentioned above, theNBTTALK data (Yvon, 1996).
Experiments on other(larger) corpora, comparing both approaches, wouldbe needed to analyse their differences empirically.6 D iscuss ionWe have demonstrated that a decision-tree l arningalgorithm, IGTREP., is able to learn English word pro-nuneiation with modest o adequate generalisationaccuracy: the less the leanting task is decomposed insubtasks, the more adequate the generalization accu-racy obtained by IGTP,.EE is.
The best generalisationaccuracy is obtained with the GS system, which doesnot decompose the task at all.
The general disad-vantage of the investigated modular systems i  thatmodules do not perform their tasks flawlessly, whiletheir expert-based decompositions do assume flaw-less performance.
In practice, modules produce aconsiderable amount of irregular errors which causesubsequent modules to generate subsequent 'cascad-ing' errors.
Only the subtask of stress assignment isshown to be learned more successfully on the basisof modular input.The best-performing system, Gs, is trained to mapwindows of letters to combined class labels repre-seating phonemes and stress maskers.
Comparedto the M-A-G-Y-S and M-G-S systems, the Gs sys-tem (i) lacks an explicit morphological segmenta-tion and (ii) learns stress assignment jointly withgrapheme-phoneme conversion on the basis of let-ter windows rather than phoneme windows.
Thesetwo advantageous properties of the ~s system leadto three suggestions.
First, it appears better to leavemorphological segmentation a implicit snbtask; itcan be left to the learning algorithm to extract henecessary morphological information eeded to dis-ambiguate between alternative pronunciations di-rectly from the letter-window input.
Second, letter-window instances provide the most reliable source ofinput for both grapheme-phoneme conversion sadstress assignment.
Third, stress assignment andgrapheme-phoneme conversion can be integrated inone task, i.e., to map letter instances to 'stressedphonemes'.A warning on the scope of these suggestions needsto be issued.
The results described here are notonly dependent of the resource (tELEX) and the(sub)task definitions (classification of windowed in-stances), but also on the use of IQTI~EE as the learn-ing algorithm.
The CEL~.X data appears robust sadprovides an abundance of English word pronunci-ations, not an inappropriately skewed subset of theEnglish vocabulary.
The windowing method appeassa salient method to rephrase language tasks as clas-sification tasks based on fixed-length inputs.
It isnot cleat, however, to what extent IGTREE can beheld responsible for the low accuracy on M-A-G-Y-S and M-G-S; IGTREE may be negatively sensitivein terms of generalisation accuracy to irregular ex-rots in the input of a modular subtask.
Althoughirregulax errors axe an inherent problem for modu-lax systems, other leaxning algorithms may be ableto handle such errors differently.
Experiments withback-propagation learning applied to the same mod-nlar systems how siginficantly worse performancethan that of IQTRv.E (Van den Bosch, 1997).
Itmight be possible that instance-based l arning algo-ritkms (e.g., IBI-IG (Daelemans and Van den Bosch,1992; Daelemans, Van den Bosch, and Weijters,1997)), which have been demonstrated to outper-form IGTREE on several language tasks (Daelemans,GiIlis, and Durieux, 1994; Van den Bosch, Dacle-roans, and Weijtets, 1996; Van den Bosch, 1997),perform better on the modular systems.
Althoughsuch systems trained with IBI-IG would be compu-rationally rather inefficient (Van den Bosch, 1997),employing IBI-IG in learning modulas ubtasks maylead to other differences in accuracy between modu-lax systems.A conclusion to be drawn from our study is thatit is possible to learn the complex language task ofEnglish word pronunciation with a general-purposeinductive-learning al orithm, with an adequate l velof generalisation accuracy.
The results suggest thatvan den Bosch, Weijters and Daelemans 193 Modularity in Word Pronunciation systemsthe necessity of decomposing word-pronunciationin several subtasks should be reconsidered case-fully when designing an accuracy-oriented word-pronunciation system.
Undesired errors generatedby sequenced modules may outweigh the desired pos-itive utility effects easily.AcknowledgementsWe thank Eric Postma, Maria Wolters, David Aha,Bertjan Busser, Jskub Zavrel, and the other mem-bers of the Tilburg ILK group for fruitful discus-sions.ReferencesAllen, J., S. Hunnicutt, and D. Klatt.
1987.
From testto speech: The MITaik system.
Cambridge, UK: Cam-bidge University Press.Bloomfield, L. 1933.
Language.
New York: Holt, Rine-hard and Winston.Breiman, L., J. Friedman, R. Ohlsen, and C. Stone.1984.
Classification and regression trees.
Belmont,CA: Wadsworth International Group.Burnage, G., 1990.
CBLBX: A guide for users.
Centrefor Lexical Information, Nijmegen.Chomsky, N. and M. Halle.
1968.
The sound pattern ofEnglish.
New York, NY: Harper and Row.Daelemems, W. 1988.
Grafon: A grapheme-to-phonemesystem for Dutch.
In Proceedings T, velflh Inter-national Conference on Computational Linguistics(COLING-88), Budapest, pages 133-138.Daelemans, W. 1996.
Experience-driven language ac-quisition and processing.
In M. Van der Avoird andC.
Corsius, editors, Proceedings of the CLS OpeningAcademic Year 1996-1997.
Tilbexg: CLS, pages 83-95.Daelemans, W., S. Gillis, and G. Durieux.
1994.
Theacquisition of stress: a data-oriented approach.
Com-putational ?inguistics, 20(3):421-451.Daelemans, W. and A.
Van den Bosch.
1992.
Generali-sation performance of backpropagation learning on asyllabification task.
In M. F. J. Drossaers and A. Ni-jholt, editors, TWLT3: Connectionism and NaturalLanguage Processing, pages 27-37, Enschede.
TwenteUniversity.Daelemans, W. and A.
Van den Bosch.
1997.
Language-independent data-oriented grapheme-to-phoneme con-version.
In J. P. H. Van Santen, R. W. Sproat, J. P.Olive, and J. Hirschberg, editors, Progress in SpeechProcessing.
Berlin: Springer-Verlag, pages 77-89.Daclemans, W., A.
Van den Bosch, and A. Weijters.1997.
IGTree: using trees for classification in lazylearning algorithms.
Artificial Intelligence Revietv,11:407-423.De Saussure, F. 1916.
Course de linguistique g~n~rale.Paris: Payot.
edited posthumously by C. Bully andA.
Riedimger.Dietterich, T. G., H. Hi\]d, and G. Baklzi.
1995.
A com-parison of Iv3 and backpropagation for English text-to-speech mapping.
Machine Learning, 19(1):5-28.Goldsmith, J.
1976.
An overview of autosegmentulphonology.
Linguistic Analysis, 2:23-68.Hunnicutt, S. 1976.
Phonological rules for a text-to-speech system.
American Journal of ComputationalLinguistics, Microfiche 57:1-72.Hunnicutt, S. 1980.
Grapheme-phoneme rules: a review.Technical Report STL QPSR 2-3, Speech Transmis-sion Laboratory, KTH, Sweden.Koskenniemi, K. 1984.
A general computational modelfor wordform recognition and production.
In Proceed-ings of the Tenth International Conference on Compu-tational Linguistics / ~nd Annual Conference of theACL, pages 178-181.Liberman, M. and A.
Prince.
1977.
On stress and lin-guistic rhythm.
Linguistic Inquiry, (8):249-336.Mitchell, T. 1997.
Machine learning.
New York, N'Y:McGraw Hill.Mohanan, K. P. 1988.
The theoey of lez~cal phonology.Dordxecht: D. Reidel.Piatelli-Palmarini, M., editor.
1980.
Language l arning:The debate bettveen Jean Piaget and Noam Chon,~k-y.Cambridge, MA: Harvard University Press.Qulnlan, J. R. 1986.
Induction of decision trees.
Ma-chine Learning, 1:81-206.Qulnlan, J. R. 1993. c4.5: Programs for machine learn-ing.
San Marco, CA: Morgan Kanfmann.Sejnowski, T. J. and C. S. Rosenberg.
1987.
Parallel net-works that learn to pronounce English text.
ComplezSyster~, 1:145-168.Shavlik, J. W., R. J. Mooney, and G. G. Towell.
1991.Symbolic and neural earning algorithms: An experi-mental comparison.
Machine Learning, 6:111-143.Stanfdl, C. and D. Waltz.
1986.
Toward memory-basedreasoning.
Communications ofthe ACM, 29(12):1213-1228.Van den Bosch, A.
1997.
Learning to pronounce mritten~vords, a study in inductive language learning.
Ph.D.thesis, Uulversiteit Maastricht.Van den Bosch, A. and W. Daclemans.
1993.
Data-oriented methods for grapheme-to-phoneme conver-sion.
In Proceedings of the 6th Conference of theEA CL, pages 45-53.Van den Bosch, A., W. Daelemans, and A. Weij-ters.
1996.
Morphological analysis as .
?lassi$.ca-tion: an inductive~learning approach.
In K. Oflszerand H. Somers, editors, Proceedings of NeMLaP-~,Ankara, Turkey, pages 79-89.Weijters, A.
1991.
A simple look-up procedure supe-rior to NETtalk?
In Proceedings oJICANN.91, Espoo,Finland.Weiss, S. and C. Kulikowski.
1991.
Computer system8that learn.
San Mateo, CA: Morgan Kaufmann.Wolpert, D. H. 1990.
Constructing a generalizer supe-rior to NETtalk via a mathematical theory of gener-a~ation.
Neural Networks, 3:445-452.Yvon, F. 1996.
Peononcer par analogie: motivation,forrnalisation et ~valuation.
Ph.D. thesis, Ecole Na-tionale Sup~rieure des T~l~communication, Paris.van den Bosch, Weijters and Daelemans 194 Modularity in Word Pronunciation systemsIIlIIIIIIlIII
