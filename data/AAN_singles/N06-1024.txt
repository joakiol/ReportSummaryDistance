Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 184?191,New York, June 2006. c?2006 Association for Computational LinguisticsFully Parsing the Penn TreebankRyan Gabbard Mitchell MarcusDepartment ofComputer and Information ScienceUniversity of Pennsylvania{gabbard,mitch}@cis.upenn.eduSeth KulickInstitute for Research inCognitive ScienceUniversity of Pennsylvaniaskulick@cis.upenn.eduAbstractWe present a two stage parser that recov-ers Penn Treebank style syntactic analy-ses of new sentences including skeletalsyntactic structure, and, for the first time,both function tags and empty categories.The accuracy of the first-stage parser onthe standard Parseval metric matches thatof the (Collins, 2003) parser on which itis based, despite the data fragmentationcaused by the greatly enriched space ofpossible node labels.
This first stage si-multaneously achieves near state-of-the-art performance on recovering functiontags with minimal modifications to the un-derlying parser, modifying less than tenlines of code.
The second stage achievesstate-of-the-art performance on the recov-ery of empty categories by combining alinguistically-informed architecture and arich feature set with the power of modernmachine learning methods.1 IntroductionThe trees in the Penn Treebank (Bies et al, 1995) areannotated with a great deal of information to makevarious aspects of the predicate-argument structureeasy to decode, including both function tags andmarkers of ?empty?
categories that represent dis-placed constituents.
Modern statistical parsers suchas (Collins, 2003) and (Charniak, 2000) however ig-nore much of this information and return only anWe would like to thank Fernando Pereira, Dan Bikel, TonyKroch and Mark Liberman for helpful suggestions.
This workwas supported in part under the GALE program of the DefenseAdvanced Research Projects Agency, Contract No.
HR0011-06-C-0022, and in part by the National Science Foundation un-der grants NSF IIS-0520798 and NSF EIA 02-05448 and underan NSF graduate fellowship.impoverished version of the trees.
While there hasbeen some work in the last few years on enrich-ing the output of state-of-the-art parsers that outputPenn Treebank-style trees with function tags (e.g.
(Blaheta, 2003)) or empty categories (e.g.
(Johnson,2002; Dienes and Dubey, 2003a; Dienes and Dubey,2003b), only one system currently available, the de-pendency graph parser of (Jijkoun and de Rijke,2004), recovers some representation of both theseaspects of the Treebank representation; its output,however, cannot be inverted to recover the originaltree structures.
We present here a parser,1 the firstwe know of, that recovers full Penn Treebank-styletrees.
This parser uses a minimal modification of theCollins parser to recover function tags, and then usesthis enriched output to achieve or better state-of-the-art performance on recovering empty categories.We focus here on Treebank-style output for tworeasons: First, annotators developing additionaltreebanks in new genres of English that conform tothe Treebank II style book (Bies et al, 1995) mustcurrently add these additional annotations by hand, amuch more laborious process than correcting parseroutput (the currently used method for annotating theskeletal structure itself).
Our new parser is now inuse in a new Treebank annotation effort.
Second, theaccurate recovery of semantic structure from parseroutput requires establishing the equivalent of the in-formation encoded within these representations.Our parser consists of two components.
The first-stage is a modification of Bikel?s implementation(Bikel, 2004) of Collins?
Model 2 that recovers func-tion tags while parsing.
Remarkably little modifica-tion to the parser is needed to allow it to producefunction tags as part of its output, yet without de-creasing the regular Parseval metric.
While it is dif-ficult to evaluate function tag assignment in isola-1The parser consists of two boxes; those who prefer to labelit by its structure, as opposed to what it does, might call it aparsing system.184(S(NP-SBJ (DT The) (NN luxury)(NN auto) (NN maker) )(NP-TMP (JJ last) (NN year) )(VP (VBD sold)(NP (CD 1,214) (NNS cars) )(PP-LOC (IN in)(NP (DT the) (NNP U.S.) ))))Figure 1: Example Treetion across the output of different parsers, our re-sults match or exceed all but the very best of earliertagging results, even though this earlier work is farmore complicated than ours.
The second stage usesa cascade of statistical classifiers which recovers themost important empty categories in the Penn Tree-bank style.
These classifiers utilize a wide range offeatures, including crucially the function tags recov-ered in the first stage of parsing.2 MotivationFunction tags are used in the current Penn Treebanksto augment nonterminal labels for various syntacticand semantic roles (Bies et al, 1995).
For example,in Figure 1, -SBJ indicates the subject, -TMP indi-cates that the NP last year is serving as a tem-poral modifier, and -LOC indicates that the PP isspecifying a location.
Note that without these tags,it is very difficult to determine which of the two NPsdirectly dominated by S is in fact the subject.
Thereare twenty function tags in the Penn Treebank, andfollowing (Blaheta, 2003), we collect them into thefive groups shown in Figure 2.
While nonterminalscan be assigned tags from different groups, they donot receive more than one tag from within a group.The Syntactic and Semantic groups are by far themost common tags, together making up over 90% ofthe function tag instances in the Penn Treebank.Certain non?local dependencies must also be in-cluded in a syntactic analysis if it is to be most use-ful for recovering the predicate?argument structureof a complex sentence.
For instance, in the sentence?The dragon I am trying to slay is green,?
it is im-portant to know that I is the semantic subject andthe dragon the semantic object of the slaying.
ThePenn Treebank (Bies et al, 1995) represents suchdependencies by including nodes with no overt con-tent (empty categories) in parse trees.
In this work,we consider the three most frequent2 and semanti-cally important types of empty category annotationsin most Treebank genres:Null complementizers are denoted by the sym-bol 0.
They typically appear in places where, forexample, an optional that or who is missing: ?Theking said 0 he could go.?
or ?The man (0) I saw.
?Traces of wh?movement are denoted by *T*,such as the noun phrase trace in ?What1 do youwant (NP *T*-1)??
Note that wh?traces are co?indexed with their antecedents.
(NP *)s are used for several purposes in thePenn Treebank.
Among the most common are pas-sivization ?
(NP-1 I) was captured (NP *-1),?and control ?
(NP-1 I) tried (NP *-1) to get thebest results.
?Under this representation the above sentencewould look like ?
(NP-1 The dragon) 0 (NP-2 I) amtrying (NP *-2) to slay (NP *T*-1) is green.
?Despite their importance, these annotations havelargely been ignored in statistical parsing work.
Theimportance of returning this information for mostreal applications of parsing has been greatly ob-scured by the Parseval metric (Black et al, 1991),which explicitly ignores both function tags and nullelements.
Because much statistical parsing researchhas been driven until recently by this metric, whichhas never been updated, the crucial role of parsingin recovering semantic structure has been generallyignored.
An early exception to this was (Collins,1997) itself, where Model 2 used function tags dur-ing the training process for heuristics to identify ar-guments (e.g., the TMP tag on the NP in Figure 1disqualifies the NP-TMP from being treated as anargument).
However, after this use, the tags are ig-nored, not included in the models, and absent fromthe parser output.
Collins?
Model 3 attempts to re-cover traces of Wh-movement, with limited success.3 Function Tags: ApproachOur system for restoring function tags is a modifica-tion of Collins?
Model 2.
We use the (Bikel, 2004)2Excepting empty units (e.g.
?$ 1,000,000 *U*?
), which arenot very interesting.
According to Johnson, (NP *)s occur28,146 times in the training portion of the PTB, (NP *T*)soccur 8,620 times, 0s occur 7,969 times, and (ADVP *T*)soccur 2,492 times.
In total, the types we consider cover roughly84% of all the instances of empty categories in the training cor-pus.185Syntactic (55.9%) Semantic (36.4%) Misc (1.0%) CLR (5.8%)DTV Dative NOM Nominal EXT Extent CLF It-cleft CLR Closely-LGS Logical subj ADV Non-specific LOC Location HLN Headline RelatedPRD Predicate Adverbial MNR Manner TTL TitlePUT LOC of ?put?
BNF Benefactive PRP Purpose TopicalizationSBJ Subject DIR Direction TMP Temporal (2.6%)VOC Vocative TPC TopicFigure 2: Function Tags - Also shown is the percentage of each category in the Penn Treebankemulation of the Collins parser.3 Remarkably littlemodification to the parser is needed to allow it toproduce function tags as part its output, without de-creasing the regular Parseval metric.The training process for the unmodified Collinsparser carries out various preprocessing steps, whichmodify the trees in various ways before taking ob-servations from them for the model.
One of thesesteps is to identify and mark arguments with a parserinternal tag (-A), using function tags as part of theheuristics for doing so.
A following preprocessingstep then deletes the original function tags.We modify the Collins parser in a very simpleway: the parser now retains the function tags af-ter using them for argument identification, and soincludes them in all the parameter classes.
Wealso augment the argument identification heuristicto treat any nonterminal with any of the tags in theSyntactic group to be an argument; these are treatedas synonyms for the internal tag that the parser usesto mark arguments.
This therefore extends (Collins,2003)?s use of function tags for excluding potentialargument to also use them for including arguments.4The parser is then trained as before.4 Function Tags: EvaluationWe compare our tagging results in isolation with thetagging systems of (Blaheta, 2003), since that workhas the first highly detailed accounting of functiontag results on the Penn Treebank, and with two re-cent tagging systems.
We use both Blaheta?s metricand his function tag groupings, shown in Figure 2,3Publicly available at http://www.cis.upenn.edu/?dbikel/software.html.4Bikel?s parser, in its latest version, already does somethinglike this for Chinese and Arabic.
However, the interaction withthe subcat frame is different, in that it puts all nonterminals witha function tag into the miscellaneous slot in the subcat frame.although our assignments are made by a fully inte-grated system.
There are two aspects of Blaheta?smetric that require discussion: First, this metric in-cludes only constituents that were otherwise parsedcorrectly (ignoring function tag).
Second, the metricignores cases in which both the gold and test nonter-minals are lacking function tags, since they wouldinflate the results.5 Function Tags: ResultsWe trained the Bikel emulations of Collins?
model2 and our modified versions on sections 2-21 andtested on section 23.
Scores are for all sentences,not just those with less than 40 words.Parseval labelled recall/precision scores for theunmodified and modified parsers, show that there isalmost no difference in the scores:Parser LR/LPModel 2 88.12/88.31Model 2-FuncB 88.23/88.31We find this somewhat surprising, as we had ex-pected that sparse data problems would arise, dueto the shattering of NP into NP-TMP, NP-SBJ, etc.Table 1 shows the overall results and the break-down for the different function tag groups.
For pur-poses of comparison, we have calculated our over-all score both with and without CLR.5 The (Blaheta,2003) numbers in parentheses in Table 1 are fromhis feature trees specialized for the Syntactic and Se-mantic groups, while all his other numbers, includ-ing the overall score, are from using a single featureset for his four function tag groups.65(Jijkoun and de Rijke, 2004) do not state whether they areincluding CLR, but since they are comparing their results to(Blaheta and Charniak, 2000), we are assuming that they do.They do not break their results down by group.6The P/R/F scores in (Blaheta, 2003)[p. 23] are internally186?
Overall ?
?
Breakdown by Function Tag Group ?w/CLR w/o CLR Syn Sem Top Misc CLRTag Group Frequency 55.87% 36.40% 2.60% 1.03% 5.76%Model2-Ftags 88.95 90.78 95.76 84.56 93.89 17.31 65.8688.28 95.16 79.81 93.72 39.44Blaheta, 2003 (95.89) (83.37)Jijkoun and de Rijke, 2004 88.50Musillo and Merlo, 2005 96.5 85.6Table 1: Overall Results (F-measure) and Breakdown by Function Tag GroupsEven though our tagging system results from onlyeliminating a few lines of code from the Collinsparse, it has a higher overall score than (Blaheta,2003), and a large increase over Blaheta?s non-specialized Semantic score (79.81).
It also out-performs even Blaheta?s specialized Semantic score(83.37), and is very close to Blaheta?s specializedscore for the Syntactic group (95.89).
However,since the evaluation is over a different set of non-terminals, arising from the different parsers,7 it isdifficult to draw conclusions as to which system isdefinitively ?better?.
It does seem clear, though,that by integrating the function tags into the lexi-calized parser, the results are roughly comparablewith the post-processing work, and it is much sim-pler, without the need for a separate post-processinglevel or for specialized feature trees for the differenttag groups.8Our results clarify, we believe, the recent resultsof (Musillo and Merlo, 2005), now state-of-the-art,which extends the parser ofreport a significant modification of the Hendersonparser to incorporate strong notions of linguistic lo-cality.
They also manually restructure some of thefunction tags using tree transformations, and thentrain on these relabelled trees.
Our results indicatethat perhaps the simplest possible modification of anexisting parser suffices to perform better than post-inconsistent for the Semantic and Overall scores.
We have keptthe Precision and Recall and recalculated the F-measures, ad-justing the Semantic score upwards from 79.15% to 79.81% andthe Overall score downward from 88.63% to 88.28%.7And the (Charniak, 2000) parser that (Blaheta, 2003) usedhas a reported F-measure of 89.5, higher than the Bikel parserused here.8Our score on the Miscellaneous category is significantlylower, but as can be seen from Figure 2 and repeated in 1, thisis a very rare category.processing approaches.
The linguistic sophisticationof the work of (Musillo and Merlo, 2005) then pro-vides an added boost in performance over simple in-tegration.6 Empty Categories: ApproachMost learning?based, phrase?structure?based(PSLB) work9 on recovering empty categorieshas fallen into two classes: those which integrateempty category recovery into the parser (Dienes andDubey, 2003a; Dienes and Dubey, 2003b) and thosewhich recover empty categories from parser outputin a post?processing step (Johnson, 2002; Levy andManning, 2004).
Levy and Manning note that thusfar no PSLB post?processing approach has comeclose to matching the integrated approach on themost numerous types of empty categories.However, there is a rule?based post?processingapproach consisting of a set of entirely hand?designed rules (Campbell, 2004) which has better9As above, we consider only that work which both inputsand outputs phrase?structure trees.
This notably excludes Ji-jkoun and de Rijke (Jijkoun and de Rijke, 2004), who have asystem which seems to match the performance of Dienes andDubey.
However, they provide only aggregate statistics over allthe types of empty categories, making any sort of detailed com-parison impossible.
Finally, it is not clear that their numbersare in fact comparable to those of Dienes and Dubey on parseddata because the metrics used are not quite equivalent, partic-ularly for (NP *)s: among other differences, unlike Jijkounand de Rijke?s metric (taken from (Johnson, 2002)), Dienes andDubey?s is sensitive to the string extent of the antecedent node,penalizing them if the parser makes attachment errors involvingthe antecedent even if the system recovered the long?distancedependency itself correctly.
Johnson noted that the two metricsdid not seem to differ much for his system, but we found thatevaluating our system with the laxer metric reduced error by20% on the crucial task of restoring and finding the antecedentsof (NP *)s, which make up almost half the empty categoriesin the Treebank.187results than the integrated approach.
Campbell?srules make heavy use of aspects of linguistic repre-sentation unexploited by PSLB post?processing ap-proaches, most importantly function tags and argu-ment annotation.107 Empty Categories: Method7.1 RuntimeThe algorithm applies a series five maximum?entropy and two perceptron?based classifiers:[1] For each PP, VP, and S node, ask the classifierNPTRACE to determine whether to insert an (NP*) as the object of a preposition, an argument of averb, or the subject of a clause, respectively.
[2] For each node , ask NULLCOMP to determinewhether or not to insert a 0 to the right.
[3] For each S node , ask WHXPINSERT to de-termine whether or not to insert a null wh?word tothe left.
If one should be, ask WHXPDISCERN todecide if it should be a (WHNP 0) or a (WHADVP0).
[4] For each S which is a sister of WHNP orWHADVP, consider all possible places beneath it awh?trace could be placed.
Score each of them usingWHTRACE, and insert a trace in the highest scoringposition.
[5] For any S lacking a subject, insert (NP *).
[6] For each (NP *) in subject position, look atall NPs which c?command it.
Score each of these us-ing PROANTECEDENT, and co?index the (NP *)with the NPwith the highest score.
For all (NP *)sin non?subject positions, we follow Campbell in as-signing the local subject as the controller.
[7] For each (NP *), ask ANTECEDENTLESS todetermine whether or not to remove the co?indexingbetween it and its antecedent.The sequencing of classifiers and choice of howto frame the classification decisions closely followsCampbell with the exception of finding antecedentsof (NP *)s and inserting wh?traces, which followLevy and Manning in using a competition?based ap-proach.
We differ from Levy and Manning in usinga perceptron?based approach for these, rather than a10The non?PSLB system of Jijkoun and de Rijke uses func-tion tags, and Levy and Manning mention that the lack of thisinformation was sometimes an obstacle for them.
Also, accessto argument annotation inside the parser may account for a partof the good performance of Dienes and Dubey.maximum?entropy one.
Also, rather than introduc-ing an extra zero node for uncontrolled (NP *)s,we always assign a controller and then remove co?indexing from uncontrolled (NP *)s using a sepa-rate classifier.7.2 TrainingEach of the maximum?entropy classifiers men-tioned above was trained using MALLET (McCal-lum, 2002) over a common feature set.
The mostnotable departure of this feature list from previousones is in the use of function tags and argumentmarkings, which were previously ignored for the un-derstandable reason that though they are present inthe Penn Treebank, parsers generally do not producethem.
Another somewhat unusual feature examinedright and left sisters.The PROANTECEDENT perceptron classifieruses the local features of the controller and the con-trolled (NP *), whether the controller precedes orfollows the controlled (NP *), the sequence of cat-egories on the path between the two (with the ?turn-ing?
category marked), the length of that path, andwhich categories are contained anywhere along thepath.The WHTRACE perceptron classifier uses the fol-lowing features each conjoined with the type of wh?trace being sought: the sequence of categories foundon the path between the trace and its antecedent,the path length, which categories are contained any-where along the path, the number of bounding cat-egories crossed and whether the trace placement vi-olates subjacency, whether or not the trace insertionsite?s parent is the first verb on the path, whether ornot the insertion site?s parent contains another verbbeneath it, and if the insertion site?s parent is a verb,whether or not the verb is saturated.11All maximum?entropy classifiers were trained onsections 2-21 of the Penn Treebank?s Wall StreetJournal section; the perceptron?based classifierswere trained on sections 10-18.
Section 24 was usedfor development testing while choosing the feature11To provide the verb saturation feature, we calculated thenumber of times each verb in the training corpus occurs witheach number of NP arguments (both overt and traces).
Whencalculating the feature value, we compare the number of in-stances seen in the training corpus of the verb with the numberof argument NPs it overtly has with the number of times in thecorpus the verb occurs with one more argument NP.188set and other aspects of the system, and section 23was used for the final evaluation.8 Empty Categories: Results8.1 MetricsFor the sake of easy comparison, we report our re-sults using the most widely?used metric for perfor-mance on this task, that proposed by Johnson.
Thismetric judges an entity correct if it matches the goldstandard in type and string position (and, if there isan antecedent, in its label and string extent).
Be-cause Campell reports results by category using onlyhis own metric, we use this metric to compare ourresults to his.
There is much discussion in the litera-ture of metrics for this task; Levy and Manning andCampbell both note that the Johnson metric fails tocatch when an empty category has a correct stringposition but incorrect parse tree attachment.
Whilewe do not have space to discuss this issue here, themetrics they in turn propose also have significantweaknesses.
In any event, we use the metrics thatallow the most widespread comparison.8.2 Comparison to Other PSLB MethodsCategory Pres LM J DDComb.
0 87.8 87.0 77.1COMP-SBAR 91.9 88.0 85.5COMP-WHNP 61.5 47.0 48.8COMP-WHADVP 69.0NP * 69.1 61.1 55.6 70.3Comb.
wh?trace 78.2 63.3 75.2 75.3NP *T* 80.9 80.0 82.0ADVP *T* 69.8 56 53.6Table 2: F1 scores comparing our system to thetwo PSLB post?processing systems and Dienes andDubey?s integrated system on automatically parsedtrees from section 23 using Johnson?s metric.F1 scores on parsed sentences from section 23are given in table 2.
Note that our system?sparsed scores were obtained using our modifiedversion of Bikel?s implementation of Collins?s the-sis parser which assigns function tags, while theother PSLB post?processing systems use Charniak?sparser (Charniak, 2000) and Dienes and Dubey inte-grate empty category recovery directly into a variantof Collins?s parser.On parsed trees, our system outperforms otherPSLB post?processing systems.
On the most numer-ous category by far, (NP *), our system reducesthe error of the best PSLB post?processing approachby 21%.
Comparing our aggregate wh?trace resultsto the others,12 we reduce error by 41% over Levyand Manning and by 12% over Johnson.System Precision Recall F1D&D 78.50 68.08 72.92Pres 74.70 74.62 74.66Table 3: Comparison of our system with that of Di-enes and Dubey on parsed data from section 23 overthe aggregation of all categories in table 2 except-ing the infrequent (WHADVP 0)s, which they donot report but which we almost certainly outperformthem on.Performance on parsed data compared to the inte-grated system of Dienes and Dubey is split.
We re-duce error by 25% and 44% on plain 0s and (WHNP0)s, respectively and by 12% on wh?traces.
Weincrease error by 4% on (NP *)s. Aggregatingover all the categories under consideration, the morebalanced precision and recall of our system puts itahead of Dienes and Dubey?s, with a 6.4% decreasein error (table 3).8.3 Comparison to CampbellCategory Present CampbellNP * 88.8 86.9NP *T* 96.3 96.0ADVP *T* 82.2 79.90 99.8 98.5Table 4: A comparison of the present system withCampbell?s rule?based system on gold?standardtrees from section 23 using Campbell?s metric12Levy and Manning report Johnson to have an aggregatewh?trace score of 80, but Johnson?s paper gives 80 as his scorefor (NP *T*)s only, with 56 as his score for (ADVP *T*)s.A similar problem seems to have occured with Levy and Man-ning?s numbers for Dienes and Dubey on this and on (NP *)s.This error makes the other two systems appear to outperformLevy andManning onwh?traces by a slightly larger margin thanthey actually do.189Classifier Features with largest weightsNPTRACE daughter categories, function tags, argumentness, heads, and POS tags, subjectlessS.
.
.NULLCOMP is first daughter?, terminalness, aunt?s label and POS tag, mother?s head, daughters?heads, great?grandmother?s label.
.
.WHXPINSERT is first daughter?, left sister?s terminalness, labels of mother, aunt, and left sister,aunt?s head.
.
.WHXPDISCERN words contained by grandmother, grandmother?s head, aunt?s head, grandmother?sfunction tags, aunt?s label, aunt?s function tags.
.
.WHTRACE lack of subject, daughter categories, child argument information, subjacency viola-tion, saturation, whether or not there is a verb below, path information.
.
.PROANTECEDENT controller?s sisters?
function tags, categories path contains, path length, path shape,controller?s function tags, controller?s sisters?
heads, linear precedence informa-tion.
.
.ANTECEDENTLESS mother?s function tags, great?grandmother?s label, aunt?s head (?It is difficultto.
.
.
?
), grandmother?s function tag, mother?s head.
.
.Table 5: A few of the most highly weighted features for various classifiersOn gold-standard trees,13 our system out-performs Campbell?s rule?based system on all fourcategories, reducing error by 87% on 0s,14 by 11%on (ADVP *T*)s, by 7% on (NP *T*)s, and by8% on the extremely numerous (NP *)s.9 Empty Categories: DiscussionWe have shown that a PSLB post?processing ap-proach can outperform the state?of?the?art inte-grated approach of Dienes and Dubey.15 Given thattheir modifications to Collins?s parser caused a de-crease in local phrase structure parsing accuracydue to sparse data difficulties (Dienes and Dubey,2003a), our post?processing approach seems to bean especially attractive choice.
We have furthershown that our PSLB approach, using only sim-ple, unconjoined features, outperforms Campbell?sstate?of?the?art, complex system on gold?standarddata, suggesting that much of the power of his sys-tem lies in his richer linguistic representation andhis structuring of decisions rather than the hand?designed rules.We have also compared our system to that of Levyand Manning which is based on a similar learningtechnique and have shown large increases in perfor-13Only aggregate statistics over a different set of empty cat-egories were available for Campbell on parsed data, making acomparison impossible.14Note that for comparison with Campbell, the 0 numbershere exclude (WHNP 0)s and (WHADVP 0)s.15And therefore also very likely outperforms thedependency?based post?processing approach of Jijkounand de Rijke, even if its performance does in fact equal Dienesand Dubey?s.mance on all of the most common types of emptycategories; this increase seems to have come al-most entirely from an enrichment of the linguisticrepresentation and a slightly different structuring ofthe problem, rather than any use of more powerfulmachine?learning techniquesWe speculate that the primary source of our per-formance increase is the enrichment of the linguis-tic representation with function tags and argumentmarkings from the parser?s first stage, as table 5 at-tests.
We also note that several classifiers make useof the properties of aunt nodes, which have previ-ously been exploited only in a limite form in John-son?s patterns.
For example, ANTECEDENTLESSuses the aunt?s head word to learn an entire class ofuncontrolled PRO constructions like ?It is difficult(NP *) to imagine living on Mars.
?10 ConclusionThis work has presented a two stage parser that re-covers Penn Treebank style syntactic analyses ofnew sentences including skeletal syntactic structure,and, for the first time, both function tags and emptycategories.
The accuracy of the first-stage parseron the standard Parseval metric matches that of the(Collins, 2003) parser on which it is based, despitethe data fragmentation caused by the greatly en-riched space of possible node labels for the Collinsstatistical model.
This first stage simultaneouslyachieves near state-of-the-art performance on recov-ering function tags with minimal modifications tothe underlying parser, modifying less than ten lines190of code.
We speculate that this success is due to thelexicalization of the Collins model, combined withthe sophisticated backoff structure already built intothe Collins model.
The second stage achieves state-of-the-art performance on the recovery of empty cat-egories by combining the linguistically-informed ar-chitecture of (Campbell, 2004) and a rich feature setwith the power of modern machine learning meth-ods.
This work provides an example of how smallenrichments in linguistic representation and changesin the structure of the problem having significanteffects on the performance of a machine?learning?based system.
More concretely, we showed for thefirst time that a PSLB post?processing system canoutperform the state?of?the?art for both rule?basedpost?processing and integrated approaches to theempty category restoration problem.Most importantly from the point of view of theauthors, we have constructed a system that recov-ers sufficiently rich syntactic structure based on thePenn Treebank to provide rich syntactic guidance forthe recovery of predicate-argument structure in thenear future.
We also expect that productivity of syn-tactic annotation of further genres of English will besignificantly enhanced by the use of this new tool,and hope to have practical evidence of this in thenear future.ReferencesAnn Bies, Mark Ferguson, Karen Katz, and Robert Mac-Intyre.
1995.
Bracketing guidelines for Treebank IIstyle Penn Treebank project.
Technical report, Uni-versity of Pennsylvania.Daniel M. Bikel.
2004.
On the Parameter Space of Lex-icalized Statistical Parsing Models.
Ph.D. thesis, De-partment of Computer and Information Sciences, Uni-versity of Pennsylvania.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitatively comparing the syntactic cov-erage of English grammars.
In Proceedings of theFourth DARPA Workshop on Speech and Natural Lan-guage, pages 306?311, CA.Don Blaheta and Eugene Charniak.
2000.
Assigningfunction tags to parsed text.
In Proceedings of the1st Annual Meeting of the North American Chapter ofthe Association for Computational Linguistics, pages234?240, Seattle.Don Blaheta.
2003.
Function Tagging.
Ph.D. thesis,Brown University.Richard Campbell.
2004.
Using linguistic principles torecover empty categories.
In Proceedings of ACL.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st Annual Meeting of theNorth American Chapter of the Association for Com-putational Linguistics.Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Computa-tional Linguistics, Madrid.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29:589?637.Peter Dienes and Amit Dubey.
2003a.
Antecedent recov-ery: Experiments with a trace tagger.
In Proceedingsof EMNLP.Peter Dienes and Amit Dubey.
2003b.
Deep process-ing by combining shallow methods.
In Proceedings ofACL.James Henderson.
2003.
Inducing history representa-tions for broad coverage statistical parsing.
In Pro-ceedings of NLT-NAACL 2003, Edmonton, Alberta,Canada.
Association for Computational Linguistics.Valentin Jijkoun and Maarten de Rijke.
2004.
Enrich-ing the output of a parser using memory-based learn-ing.
In Proceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics, Barcelona,Spain.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40st Annual Meet-ing of the Association for Computational Linguistics,Philadelphia, PA.Roger Levy and Christopher Manning.
2004.
Deep de-pendencies from context?free statistical parsers: cor-recting the surface dependency approximation.
In Pro-ceedings of the ACL.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Gabriele Musillo and Paolo Merlo.
2005.
Lexical andstructural biases for function parsing.
In Proceedingsof the Ninth International Workshop on Parsing Tech-nology, pages 83?92, Vancouver, British Columbia,October.
Association for Computational Linguistics.191
