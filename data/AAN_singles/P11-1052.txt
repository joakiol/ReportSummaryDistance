Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 510?520,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Class of Submodular Functions for Document SummarizationHui LinDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195, USAhlin@ee.washington.eduJeff BilmesDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195, USAbilmes@ee.washington.eduAbstractWe design a class of submodular functionsmeant for document summarization tasks.These functions each combine two terms,one which encourages the summary to berepresentative of the corpus, and the otherwhich positively rewards diversity.
Critically,our functions are monotone nondecreasingand submodular, which means that an efficientscalable greedy optimization scheme hasa constant factor guarantee of optimality.When evaluated on DUC 2004-2007 corpora,we obtain better than existing state-of-artresults in both generic and query-focuseddocument summarization.
Lastly, we showthat several well-established methods fordocument summarization correspond, in fact,to submodular function optimization, addingfurther evidence that submodular functions area natural fit for document summarization.1 IntroductionIn this paper, we address the problem of generic andquery-based extractive summarization from collec-tions of related documents, a task commonly knownas multi-document summarization.
We treat this taskas monotone submodular function maximization (tobe defined in Section 2).
This has a number of criti-cal benefits.
On the one hand, there exists a simplegreedy algorithm for monotone submodular func-tion maximization where the summary solution ob-tained (say S?)
is guaranteed to be almost as goodas the best possible solution (say Sopt) according toan objective F .
More precisely, the greedy algo-rithm is a constant factor approximation to the car-dinality constrained version of the problem, so thatF(S?)
?
(1 ?
1/e)F(Sopt) ?
0.632F(Sopt).
Thisis particularly attractive since the quality of the so-lution does not depend on the size of the problem,so even very large size problems do well.
It is alsoimportant to note that this is a worst case bound, andin most cases the quality of the solution obtained willbe much better than this bound suggests.Of course, none of this is useful if the objectivefunction F is inappropriate for the summarizationtask.
In this paper, we argue that monotone nonde-creasing submodular functionsF are an ideal class offunctions to investigate for document summarization.We show, in fact, that many well-established methodsfor summarization (Carbonell and Goldstein, 1998;Filatova and Hatzivassiloglou, 2004; Takamura andOkumura, 2009; Riedhammer et al, 2010; Shen andLi, 2010) correspond to submodular function opti-mization, a property not explicitly mentioned in thesepublications.
We take this fact, however, as testamentto the value of submodular functions for summariza-tion: if summarization algorithms are repeatedly de-veloped that, by chance, happen to be an instance of asubmodular function optimization, this suggests thatsubmodular functions are a natural fit.
On the otherhand, other authors have started realizing explicitlythe value of submodular functions for summarization(Lin and Bilmes, 2010; Qazvinian et al, 2010).Submodular functions share many properties incommon with convex functions, one of which is thatthey are closed under a number of common combi-nation operations (summation, certain compositions,restrictions, and so on).
These operations give us thetools necessary to design a powerful submodular ob-jective for submodular document summarization thatextends beyond any previous work.
We demonstratethis by carefully crafting a class of submodular func-510tions we feel are ideal for extractive summarizationtasks, both generic and query-focused.
In doing so,we demonstrate better than existing state-of-the-artperformance on a number of standard summarizationevaluation tasks, namely DUC-04 through to DUC-07.
We believe our work, moreover, might act as aspringboard for researchers in summarization to con-sider the problem of ?how to design a submodularfunction?
for the summarization task.In Section 2, we provide a brief background on sub-modular functions and their optimization.
Section 3describes how the task of extractive summarizationcan be viewed as a problem of submodular functionmaximization.
We also in this section show that manystandard methods for summarization are, in fact, al-ready performing submodular function optimization.In Section 4, we present our own submodular func-tions.
Section 5 presents results on both generic andquery-focused summarization tasks, showing as faras we know the best known ROUGE results for DUC-04 through DUC-06, and the best known precisionresults for DUC-07, and the best recall DUC-07 re-sults among those that do not use a web search engine.Section 6 discusses implications for future work.2 Background on SubmodularityWe are given a set of objects V = {v1, .
.
.
, vn} and afunctionF : 2V ?
R that returns a real value for anysubset S ?
V .
We are interested in finding the subsetof bounded size |S| ?
k that maximizes the function,e.g., argmaxS?V F(S).
In general, this operationis hopelessly intractable, an unfortunate fact sincethe optimization coincides with many important ap-plications.
For example, F might correspond to thevalue or coverage of a set of sensor locations in anenvironment, and the goal is to find the best locationsfor a fixed number of sensors (Krause et al, 2008).If the function F is monotone submodular then themaximization is still NP complete, but it was shownin (Nemhauser et al, 1978) that a greedy algorithmfinds an approximate solution guaranteed to be withine?1e ?
0.63 of the optimal solution, as mentionedin Section 1.
A version of this algorithm (Minoux,1978), moreover, scales to very large data sets.
Sub-modular functions are those that satisfy the propertyof diminishing returns: for anyA ?
B ?
V \v, a sub-modular functionF must satisfyF(A+v)?F(A) ?F(B + v)?F(B).
That is, the incremental ?value?of v decreases as the context in which v is consideredgrows from A to B.
An equivalent definition, usefulmathematically, is that for any A,B ?
V , we musthave that F(A) +F(B) ?
F(A ?B) +F(A ?B).If this is satisfied everywhere with equality, thenthe function F is called modular, and in such caseF(A) = c +?a?A~fa for a sized |V | vector ~f ofreal values and constant c. A set function F is mono-tone nondecreasing if ?A ?
B, F(A) ?
F(B).
Asshorthand, in this paper, monotone nondecreasingsubmodular functions will simply be referred to asmonotone submodular.Historically, submodular functions have their rootsin economics, game theory, combinatorial optimiza-tion, and operations research.
More recently, submod-ular functions have started receiving attention in themachine learning and computer vision community(Kempe et al, 2003; Narasimhan and Bilmes, 2005;Krause and Guestrin, 2005; Narasimhan and Bilmes,2007; Krause et al, 2008; Kolmogorov and Zabin,2004) and have recently been introduced to naturallanguage processing for the tasks of document sum-marization (Lin and Bilmes, 2010) and word align-ment (Lin and Bilmes, 2011).Submodular functions share a number of proper-ties in common with convex and concave functions(Lova?sz, 1983), including their wide applicability,their generality, their multiple options for their repre-sentation, and their closure under a number of com-mon operators (including mixtures, truncation, com-plementation, and certain convolutions).
For exam-ple, if a collection of functions {Fi}i is submodular,then so is their weighted sum F =?i ?iFi where?i are nonnegative weights.
It is not hard to showthat submodular functions also have the followingcomposition property with concave functions:Theorem 1.
Given functions F : 2V ?
R andf : R?
R, the composition F ?
= f ?
F : 2V ?
R(i.e., F ?
(S) = f(F(S))) is nondecreasing sub-modular, if f is non-decreasing concave and F isnondecreasing submodular.This property will be quite useful when defining sub-modular functions for document summarization.5113 Submodularity in Summarization3.1 Summarization with knapsack constraintLet the ground set V represents all the sentences(or other linguistic units) in a document (or docu-ment collection, in the multi-document summariza-tion case).
The task of extractive document sum-marization is to select a subset S ?
V to representthe entirety (ground set V ).
There are typically con-straints on S, however.
Obviously, we should have|S| < |V | = N as it is a summary and shouldbe small.
In standard summarization tasks (e.g.,DUC evaluations), the summary is usually requiredto be length-limited.
Therefore, constraints on Scan naturally be modeled as knapsack constraints:?i?S ci ?
b, where ci is the non-negative cost ofselecting unit i (e.g., the number of words in the sen-tence) and b is our budget.
If we use a set functionF : 2V ?
R to measure the quality of the summaryset S, the summarization problem can then be for-malized as the following combinatorial optimizationproblem:Problem 1.
FindS?
?
argmaxS?VF(S) subject to:?i?Sci ?
b.Since this is a generalization of the cardinalityconstraint (where ci = 1,?i), this also constitutesa (well-known) NP-hard problem.
In this case aswell, however, a modified greedy algorithm with par-tial enumeration can solve Problem 1 near-optimallywith (1?1/e)-approximation factor ifF is monotonesubmodular (Sviridenko, 2004).
The partial enumer-ation, however, is too computationally expensive forreal world applications.
In (Lin and Bilmes, 2010),we generalize the work by Khuller et al (1999) onthe budgeted maximum cover problem to the gen-eral submodular framework, and show a practicalgreedy algorithm with a (1?
1/?e)-approximationfactor, where each greedy step adds the unit with thelargest ratio of objective function gain to scaled cost,while not violating the budget constraint (see (Linand Bilmes, 2010) for details).
Note that in all cases,submodularity and monotonicity are two necessaryingredients to guarantee that the greedy algorithmgives near-optimal solutions.In fact, greedy-like algorithms have been widelyused in summarization.
One of the more popularapproaches is maximum marginal relevance (MMR)(Carbonell and Goldstein, 1998), where a greedyalgorithm selects the most relevant sentences, andat the same time avoids redundancy by removingsentences that are too similar to ones already selected.Interestingly, the gain function defined in the originalMMR paper (Carbonell and Goldstein, 1998) satisfiesdiminishing returns, a fact apparently unnoticed untilnow.
In particular, Carbonell and Goldstein (1998)define an objective function gain of adding elementk to set S (k /?
S) as:?Sim1(sk, q)?
(1?
?)
maxi?SSim2(si, sk), (1)where Sim1(sk, q) measures the similarity betweenunit sk to a query q, Sim2(si, sk) measures the simi-larity between unit si and unit sk, and 0 ?
?
?
1 isa trade-off coefficient.
We have:Theorem 2.
Given an expression forFMMR such thatFMMR(S ?
{k})?FMMR(S) is equal to Eq.
1, FMMRis non-monotone submodular.Obviously, diminishing-returns hold sincemaxi?SSim2(si, sk) ?
maxi?RSim2(si, sk)for all S ?
R, and therefore FMMR is submodular.On the other hand,FMMR, would not be monotone, sothe greedy algorithm?s constant-factor approximationguarantee does not apply in this case.When scoring a summary at the sub-sentencelevel, submodularity naturally arises.
Concept-basedsummarization (Filatova and Hatzivassiloglou, 2004;Takamura and Okumura, 2009; Riedhammer et al,2010; Qazvinian et al, 2010) usually maximizes theweighted credit of concepts covered by the summary.Although the authors may not have noticed, their ob-jective functions are also submodular, adding moreevidence suggesting that submodularity is natural forsummarization tasks.
Indeed, let S be a subset ofsentences in the document and denote ?
(S) as theset of concepts contained in S. The total credit of theconcepts covered by S is thenFconcept(S) ,?i??
(S)ci,where ci is the credit of concept i.
This function isknown to be submodular (Narayanan, 1997).512Similar to the MMR approach, in (Lin and Bilmes,2010), a submodular graph based objective functionis proposed where a graph cut function, measuringthe similarity of the summary to the rest of document,is combined with a subtracted redundancy penaltyfunction.
The objective function is submodular butagain, non-monotone.
We theoretically justify thatthe performance guarantee of the greedy algorithmholds for this objective function with high probability(Lin and Bilmes, 2010).
Our justification, however,is shown to be applicable only to certain particularnon-monotone submodular functions, under certainreasonable assumptions about the probability distri-bution over weights of the graph.3.2 Summarization with covering constraintAnother perspective is to treat the summarizationproblem as finding a low-cost subset of the documentunder the constraint that a summary should coverall (or a sufficient amount of) the information in thedocument.
Formally, this can be expressed asProblem 2.
FindS?
?
argminS?V?i?Sci subject to: F(S) ?
?,where ci are the element costs, and set function F(S)measure the information covered by S. When Fis submodular, the constraint F(S) ?
?
is calleda submodular cover constraint.
When F is mono-tone submodular, a greedy algorithm that iterativelyselects k with minimum ck/(F(S ?
{k}) ?
F(S))has approximation guarantees (Wolsey, 1982).
Re-cent work (Shen and Li, 2010) proposes to modeldocument summarization as finding a minimum dom-inating set and a greedy algorithm is used to solvethe problem.
The dominating set constraint is alsoa submodular cover constraint.
Define ?
(S) be theset of elements that is either in S or is adjacent tosome element in S. Then S is a dominating set if|?
(S)| = |V |.
Note thatFdom(S) , |?
(S)|is monotone submodular.
The dominating setconstraint is then also a submodular cover constraint,and therefore the approaches in (Shen and Li, 2010)are special cases of Problem 2.
The solutions foundin this framework, however, do not necessarilysatisfy a summary?s budget constraint.
Consequently,a subset of the solution found by solving Problem 2has to be constructed as the final summary, and thenear-optimality is no longer guaranteed.
Therefore,solving Problem 1 for document summarizationappears to be a better framework regarding globaloptimality.
In the present paper, our framework isthat of Problem 1.3.3 Automatic summarization evaluationAutomatic evaluation of summary quality is impor-tant for the research of document summarization asit avoids the labor-intensive and potentially inconsis-tent human evaluation.
ROUGE (Lin, 2004) is widelyused for summarization evaluation and it has beenshown that ROUGE-N scores are highly correlatedwith human evaluation (Lin, 2004).
Interestingly,ROUGE-N is monotone submodular, adding furtherevidence that monotone submodular functions arenatural for document summarization.Theorem 3.
ROUGE-N is monotone submodular.Proof.
By definition (Lin, 2004), ROUGE-N is then-gram recall between a candidate summary and aset of reference summaries.
Precisely, let S be thecandidate summary (a set of sentences extracted fromthe ground set V ), ce : 2V ?
Z+ be the number oftimes n-gram e occurs in summary S, and Ri be theset of n-grams contained in the reference summary i(suppose we have K reference summaries, i.e., i =1, ?
?
?
,K).
Then ROUGE-N can be written as thefollowing set function:FROUGE-N(S) ,?Ki=1?e?Rimin(ce(S), re,i)?Ki=1?e?Rire,i,where re,i is the number of times n-gram e occursin reference summary i.
Since ce(S) is monotonemodular and min(x, a) is a concave non-decreasingfunction of x, min(ce(S), re,i) is monotone sub-modular by Theorem 1.
Since summation preservessubmodularity, and the denominator is constant, wesee that FROUGE-N is monotone submodular.Since the reference summaries are unknown, it isof course impossible to optimize FROUGE-N directly.Therefore, some approaches (Filatova and Hatzivas-siloglou, 2004; Takamura and Okumura, 2009; Ried-hammer et al, 2010) instead define ?concepts?.
Alter-513natively, we herein propose a class of monotone sub-modular functions that naturally models the quality ofa summary while not depending on an explicit notionof concepts, as we will see in the following section.4 Monotone Submodular ObjectivesTwo properties of a good summary are relevance andnon-redundancy.
Objective functions for extractivesummarization usually measure these two separatelyand then mix them together trading off encouragingrelevance and penalizing redundancy.
The redun-dancy penalty usually violates the monotonicity ofthe objective functions (Carbonell and Goldstein,1998; Lin and Bilmes, 2010).
We therefore proposeto positively reward diversity instead of negativelypenalizing redundancy.
In particular, we model thesummary quality asF(S) = L(S) + ?R(S), (2)where L(S) measures the coverage, or ?fidelity?,of summary set S to the document, R(S) rewardsdiversity in S, and ?
?
0 is a trade-off coefficient.Note that the above is analogous to the objectiveswidely used in machine learning, where a lossfunction that measures the training set error (wemeasure the coverage of summary to a document),is combined with a regularization term encouragingcertain desirable (e.g., sparsity) properties (inour case, we ?regularize?
the solution to be morediverse).
In the following, we discuss how both L(S)andR(S) are naturally monotone submodular.4.1 Coverage functionL(S) can be interpreted either as a set function thatmeasures the similarity of summary set S to the docu-ment to be summarized, or as a function representingsome form of ?coverage?
of V by S. Most naturally,L(S) should be monotone, as coverage improveswith a larger summary.
L(S) should also be submod-ular: consider adding a new sentence into two sum-mary sets, one a subset of the other.
Intuitively, theincrement when adding a new sentence to the smallsummary set should be larger than the incrementwhen adding it to the larger set, as the informationcarried by the new sentence might have already beencovered by those sentences that are in the larger sum-mary but not in the smaller summary.
This is exactlythe property of diminishing returns.
Indeed, Shan-non entropy, as the measurement of information, isanother well-known monotone submodular function.There are several ways to define L(S) in ourcontext.
For instance, we could use L(S) =?i?V,j?S wi,j where wi,j represents the similaritybetween i and j. L(S) could also be facilitylocation objective, i.e., L(S) =?i?V maxj?S wi,j ,as used in (Lin et al, 2009).
We could also useL(S) =?i??
(S) ci as used in concept-basedsummarization, where the definition of ?concept?and the mechanism to extract these concepts becomeimportant.
All of these are monotone submodular.Alternatively, in this paper we propose the follow-ing objective that does not reply on concepts.
LetL(S) =?i?Vmin {Ci(S), ?
Ci(V )} , (3)where Ci : 2V ?
R is a monotone submodular func-tion and 0 ?
?
?
1 is a threshold co-efficient.
Firstly,L(S) as defined in Eqn.
3 is a monotone submodularfunction.
The monotonicity is immediate.
To see thatL(S) is submodular, consider the fact that f(x) =min(x, a) where a ?
0 is a concave non-decreasingfunction, and by Theorem 1, each summand in Eqn.
3is a submodular function, and as summation pre-serves submodularity, L(S) is submodular.Next, we explain the intuition behind Eqn.
3.
Basi-cally, Ci(S) measures how similar S is to element i,or how much of i is ?covered?
by S. Then Ci(V ) isjust the largest value that Ci(S) can achieve.
We calli ?saturated?
by S when min{Ci(S), ?Ci(V )} =?Ci(V ).
When i is already saturated in this way,any new sentence j can not further improve thecoverage of i even if it is very similar to i (i.e.,Ci(S ?
{j}) ?
Ci(S) is large).
This will give othersentences that are not yet saturated a higher chanceof being better covered, and therefore the resultingsummary tends to better cover the entire document.One simple way to define Ci(S) is just to useCi(S) =?j?Swi,j (4)where wi,j ?
0 measures the similarity between iand j.
In this case, when ?
= 1, Eqn.
3 reducesto the case where L(S) =?i?V,j?S wi,j .
As wewill see in Section 5, having an ?
that is less than5141 significantly improves the performance comparedto the case when ?
= 1, which coincides with ourintuition that using a truncation threshold improvesthe final summary?s coverage.4.2 Diversity reward functionInstead of penalizing redundancy by subtracting fromthe objective, we propose to reward diversity byadding the following to the objective:R(S) =K?i=1?
?j?Pi?Srj .
(5)where Pi, i = 1, ?
?
?K is a partition of the groundset V (i.e.,?i Pi = V and the Pis are disjoint) intoseparate clusters, and ri ?
0 indicates the singletonreward of i (i.e., the reward of adding i into the emptyset).
The value ri estimates the importance of i tothe summary.
The functionR(S) rewards diversityin that there is usually more benefit to selecting asentence from a cluster not yet having one of itselements already chosen.
As soon as an elementis selected from a cluster, other elements from thesame cluster start having diminishing gain, thanksto the square root function.
For instance, considerthe case where k1, k2 ?
P1, k3 ?
P2, and rk1 = 4,rk2 = 9, and rk3 = 4.
Assume k1 is already in thesummary set S. Greedily selecting the next elementwill choose k3 rather than k2 since?13 < 2 + 2.
Inother words, adding k3 achieves a greater reward as itincreases the diversity of the summary (by choosingfrom a different cluster).
Note,R(S) is distinct fromL(S) in that R(S) might wish to include certainoutlier material that L(S) could ignore.It is easy to show that R(S) is submodular byusing the composition rule from Theorem 1.
Thesquare root is non-decreasing concave function.Inside each square root lies a modular functionwith non-negative weights (and thus is monotone).Applying the square root to such a monotone sub-modular function yields a submodular function, andsumming them all together retains submodularity, asmentioned in Section 2.
The monotonicity ofR(S)is straightforward.
Note, the form of Eqn.
5 is similarto structured group norms (e.g., (Zhao et al, 2009)),recently shown to be related to submodularity (Bach,2010; Jegelka and Bilmes, 2011).Several extensions to Eqn.
5 are discussednext: First, instead of using a ground set partition,intersecting clusters can be used.
Second, thesquare root function in Eqn.
5 can be replaced withany other non-decreasing concave functions (e.g.,f(x) = log(1 + x)) while preserving the desiredproperty ofR(S), and the curvature of the concavefunction then determines the rate that the rewarddiminishes.
Last, multi-resolution clustering (orpartitions) with different sizes (K) can be used, i.e.,we can use a mixture of components, each of whichhas the structure of Eqn.
5.
A mixture can betterrepresent the core structure of the ground set (e.g.,the hierarchical structure in the documents (Celiky-ilmaz and Hakkani-tu?r, 2010)).
All such extensionspreserve both monotonicity and submodularity.5 ExperimentsThe document understanding conference (DUC)(http://duc.nist.org) was the main forumproviding benchmarks for researchers workingon document summarization.
The tasks in DUCevolved from single-document summarization tomulti-document summarization, and from genericsummarization (2001?2004) to query-focused sum-marization (2005?2007).
As ROUGE (Lin, 2004)has been officially adopted for DUC evaluationssince 2004, we also take it as our main evaluationcriterion.
We evaluated our approaches on DUCdata 2003-2007, and demonstrate results on bothgeneric and query-focused summarization.
In allexperiments, the modified greedy algorithm (Lin andBilmes, 2010) was used for summary generation.5.1 Generic summarizationSummarization tasks in DUC-03 and DUC-04 aremulti-document summarization on English newsarticles.
In each task, 50 document clusters aregiven, each of which consists of 10 documents.For each document cluster, the system generatedsummary may not be longer than 665 bytes includingspaces and punctuation.
We used DUC-03 asour development set, and tested on DUC-04 data.We show ROUGE-1 scores1 as it was the mainevaluation criterion for DUC-03, 04 evaluations.1ROUGE version 1.5.5 with options: -a -c 95 -b 665 -m -n 4-w 1.2515Documents were pre-processed by segmenting sen-tences and stemming words using the Porter Stemmer.Each sentence was represented using a bag-of-termsvector, where we used context terms up to bi-grams.Similarity between sentence i and sentence j, i.e.,wi,j , was computed using cosine similarity:wi,j =?w?sitfw,i ?
tfw,j ?
idf2w?
?w?sitf2w,si idf2w?
?w?sjtf2w,j idf2w,where tfw,i and tfw,j are the numbers of times thatw appears in si and sentence sj respectively, andidfw is the inverse document frequency (IDF) ofterm w (up to bigram), which was calculated as thelogarithm of the ratio of the number of articles thatw appears over the total number of all articles in thedocument cluster.Table 1: ROUGE-1 recall (R) and F-measure (F) results(%) on DUC-04.
DUC-03 was used as development set.DUC-04 R FPi?VPj?S wi,j 33.59 32.44L1(S) 39.03 38.65R1(S) 38.23 37.81L1(S) + ?R1(S) 39.35 38.90Takamura and Okumura (2009) 38.50 -Wang et al (2009) 39.07 -Lin and Bilmes (2010) - 38.39Best system in DUC-04 (peer 65) 38.28 37.94We first tested our coverage and diversity re-ward objectives separately.
For coverage, we use amodular Ci(S) =?j?S wi,j for each sentence i, i.e.,L1(S) =?i?Vmin???
?j?Swi,j , ??k?Vwi,k???.
(6)When ?
= 1, L1(S) reduces to?i?V,j?S wi,j ,which measures the overall similarity of summaryset S to ground set V .
As mentioned in Section 4.1,using such similarity measurement could possiblyover-concentrate on a small portion of the documentand result in a poor coverage of the whole document.As shown in Table 1, optimizing this objectivefunction gives a ROUGE-1 F-measure score 32.44%.On the other hand, when using L1(S) with an ?
< 1(the value of ?
was determined on DUC-03 usinga grid search), a ROUGE-1 F-measure score 38.65%36.236.436.636.83737.237.437.60 5 10 15 20ROUGE-1F-measure(%)K=0.05NK=0.1NK=0.2NaFigure 1: ROUGE-1 F-measure scores on DUC-03 when?
and K vary in objective function L1(S) + ?R1(S),where ?
= 6 and ?
= aN .is achieved, which is already better than the bestperforming system in DUC-04.As for the diversity reward objective, we definethe singleton reward as ri = 1N?j wi,j , which isthe average similarity of sentence i to the rest of thedocument.
It basically states that the more similar tothe whole document a sentence is, the more rewardthere will be by adding this sentence to an emptysummary set.
By using this singleton reward, wehave the following diversity reward function:R1(S) =K?k=1?
?j?S?Pk1N?i?Vwi,j .
(7)In order to generate Pk, k = 1, ?
?
?K, we usedCLUTO2 to cluster the sentences, where the IDF-weighted term vector was used as feature vector, anda direct K-mean clustering algorithm was used.
Inthis experiment, we set K = 0.2N .
In other words,there are 5 sentences in each cluster on average.And as we can see in Table 1, optimizing thediversity reward function alone achieves comparableperformance to the DUC-04 best system.Combining L1(S) and R1(S), our system outper-forms the best system in DUC-04 significantly, andit also outperforms several recent systems, includinga concept-based summarization approach (Takamuraand Okumura, 2009), a sentence topic model basedsystem (Wang et al, 2009), and our MMR-styledsubmodular system (Lin and Bilmes, 2010).
Figure 1illustrates how ROUGE-1 scores change when ?
andK vary on the development set (DUC-03).2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview516Table 2: ROUGE-2 recall (R) and F-measure (F) results(%) on DUC-05, where DUC-05 was used as training set.DUC-05 R FL1(S) + ?RQ(S) 8.38 8.31Daume?
III and Marcu (2006) 7.62 -Extr, Daume?
et al (2009) 7.67 -Vine, Daume?
et al (2009) 8.24 -Table 3: ROUGE-2 recall (R) and F-measure (F) resultson DUC-05 (%).
We used DUC-06 as training set.DUC-05 R FL1(S) + ?RQ(S) 7.82 7.72Daume?
III and Marcu (2006) 6.98 -Best system in DUC-05 (peer 15) 7.44 7.435.2 Query-focused summarizationWe evaluated our approach on the task of query-focused summarization using DUC 05-07 data.
InDUC-05 and DUC-06, participants were given 50document clusters, where each cluster contains 25news articles related to the same topic.
Participantswere asked to generate summaries of at most 250words for each cluster.
For each cluster, a title anda narrative describing a user?s information need areprovided.
The narrative is usually composed of aset of questions or a multi-sentence task description.The main task in DUC-07 is the same as in DUC-06.In DUC 05-07, ROUGE-2 was the primarycriterion for evaluation, and thus we also reportROUGE-23 (both recall R, and precision F).
Docu-ments were processed as in Section 5.1.
We used boththe title and the narrative as query, where stop words,including some function words (e.g., ?describe?)
thatappear frequently in the query, were removed.
Allqueries were then stemmed using the Porter Stemmer.Note that there are several ways to incorporatequery-focused information into both the coverageand diversity reward objectives.
For instance, Ci(S)could be query-dependent in how it measures howmuch query-dependent information in i is coveredby S. Also, the coefficient ?
could be query and sen-tence dependent, where it takes larger value when asentence is more relevant to query (i.e., a larger valueof ?
means later truncation, and therefore more pos-sible coverage).
Similarly, sentence clustering andsingleton rewards in the diversity function can also3ROUGE version 1.5.5 was used with option -n 2 -x -m -2 4-u -c 95 -r 1000 -f A -p 0.5 -t 0 -d -l 250Table 4: ROUGE-2 recall (R) and F-measure (F) results(%) on DUC-06, where DUC-05 was used as training set.DUC-06 R FL1(S) + ?RQ(S) 9.75 9.77Celikyilmaz and Hakkani-tu?r (2010) 9.10 -Shen and Li (2010) 9.30 -Best system in DUC-06 (peer 24) 9.51 9.51Table 5: ROUGE-2 recall (R) and F-measure (F) re-sults (%) on DUC-07.
DUC-05 was used as trainingset for objective L1(S) + ?RQ(S).
DUC-05 and DUC-06 were used as training sets for objective L1(S) +??
??RQ,?
(S).DUC-07 R FL1(S) + ?RQ(S) 12.18 12.13L1(S) +P3?=1 ??RQ,?
(S) 12.38 12.33Toutanova et al (2007) 11.89 11.89Haghighi and Vanderwende (2009) 11.80 -Celikyilmaz and Hakkani-tu?r (2010) 11.40 -Best system in DUC-07 (peer 15) 12.45 12.29be query-dependent.
In this experiment, we explorean objective with a query-independent coverage func-tion (R1(S)), indicating prior importance, combinedwith a query-dependent diversity reward function,where the latter is defined as:RQ(S) =K?k=1????
?j?S?Pk(?N?i?Vwi,j + (1?
?
)rj,Q),where 0 ?
?
?
1, and rj,Q represents the rel-evance between sentence j to query Q. Thisquery-dependent reward function is derived byusing a singleton reward that is expressed as aconvex combination of the query-independent score( 1N?i?V wi,j) and the query-dependent score (rj,Q)of a sentence.
We simply used the number ofterms (up to a bi-gram) that sentence j overlaps thequery Q as rj,Q, where the IDF weighting is notused (i.e., every term in the query, after stop wordremoval, was treated as equally important).
Bothquery-independent and query-dependent scores werethen normalized by their largest value respectivelysuch that they had roughly the same dynamic range.To better estimate of the relevance between queryand sentences, we further expanded sentences withsynonyms and hypernyms of its constituent words.
Inparticular, part-of-speech tags were obtained for eachsentence using the maximum entropy part-of-speechtagger (Ratnaparkhi, 1996), and all nouns were then517expanded with their synonyms and hypernyms usingWordNet (Fellbaum, 1998).
Note that these expandeddocuments were only used in the estimation rj,Q, andwe plan to further explore whether there is benefit touse the expanded documents either in sentence sim-ilarity estimation or in sentence clustering in our fu-ture work.
We also tried to expand the query with syn-onyms and observed a performance decrease, presum-ably due to noisy information in a query expression.While it is possible to use an approach that issimilar to (Toutanova et al, 2007) to learn thecoefficients in our objective function, we trained allcoefficients to maximize ROUGE-2 F-measure scoreusing the Nelder-Mead (derivative-free) method.Using L1(S)+?RQ(S) as the objective and with thesame sentence clustering algorithm as in the genericsummarization experiment (K = 0.2N ), our system,when both trained and tested on DUC-05 (results inTable 2), outperforms the Bayesian query-focusedsummarization approach and the search-basedstructured prediction approach, which were alsotrained and tested on DUC-05 (Daume?
et al, 2009).Note that the system in (Daume?
et al, 2009) thatachieves its best performance (8.24% in ROUGE-2recall) is a so called ?vine-growth?
system, whichcan be seen as an abstractive approach, whereas oursystem is purely an extractive system.
Comparingto the extractive system in (Daume?
et al, 2009), oursystem performs much better (8.38% v.s.
7.67%).More importantly, when trained only on DUC-06 andtested on DUC-05 (results in Table 3), our approachoutperforms the best system in DUC-05 significantly.We further tested the system trained on DUC-05on both DUC-06 and DUC-07.
The results onDUC-06 are shown in Table.
4.
Our system outper-forms the best system in DUC-06, as well as tworecent approaches (Shen and Li, 2010; Celikyilmazand Hakkani-tu?r, 2010).
On DUC-07, in terms ofROUGE-2 score, our system outperforms PYTHY(Toutanova et al, 2007), a state-of-the-art supervisedsummarization system, as well as two recent systemsincluding a generative summarization system basedon topic models (Haghighi and Vanderwende,2009), and a hybrid hierarchical summarizationsystem (Celikyilmaz and Hakkani-tu?r, 2010).
Italso achieves comparable performance to the bestDUC-07 system.
Note that in the best DUC-07system (Pingali et al, 2007; Jagarlamudi et al, 2006),an external web search engine (Yahoo!)
was usedto estimate a language model for query relevance.
Inour system, no such web search expansion was used.To further improve the performance of our system,we used both DUC-05 and DUC-06 as a trainingset, and introduced three diversity reward termsinto the objective where three different sentenceclusterings with different resolutions were produced(with sizes 0.3N, 0.15N and 0.05N ).
Denotinga diversity reward corresponding to clustering ?as RQ,?
(S), we model the summary quality asL1(S) +?3?=1 ??RQ,?(S).
As shown in Table 5,using this objective function with multi-resolutiondiversity rewards improves our results further, andoutperforms the best system in DUC-07 in terms ofROUGE-2 F-measure score.6 Conclusion and discussionIn this paper, we show that submodularity naturallyarises in document summarization.
Not only domany existing automatic summarization methods cor-respond to submodular function optimization, butalso the widely used ROUGE evaluation is closelyrelated to submodular functions.
As the correspond-ing submodular optimization problem can be solvedefficiently and effectively, the remaining questionis then how to design a submodular objective thatbest models the task.
To address this problem, weintroduce a powerful class of monotone submodularfunctions that are well suited to document summariza-tion by modeling two important properties of a sum-mary, fidelity and diversity.
While more advancedNLP techniques could be easily incorporated into ourfunctions (e.g., language models could define a betterCi(S), more advanced relevance estimations for thesingleton rewards ri, and better and/or overlappingclustering algorithms for our diversity reward), wealready show top results on standard benchmark eval-uations using fairly basic NLP methods (e.g., termweighting and WordNet expansion), all, we believe,thanks to the power and generality of submodularfunctions.
As information retrieval and web searchare closely related to query-focused summarization,our approach might be beneficial in those areas aswell.518ReferencesF.
Bach.
2010.
Structured sparsity-inducing normsthrough submodular functions.
Advances in NeuralInformation Processing Systems.J.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documents andproducing summaries.
In Proc.
of SIGIR.A.
Celikyilmaz and D. Hakkani-tu?r.
2010.
A hybrid hier-archical model for multi-document summarization.
InProceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics, pages 815?824,Uppsala, Sweden, July.
Association for ComputationalLinguistics.H.
Daume?, J. Langford, and D. Marcu.
2009.
Search-based structured prediction.
Machine learning,75(3):297?325.H.
Daume?
III and D. Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of the 21stInternational Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, page 312.C.
Fellbaum.
1998.
WordNet: An electronic lexicaldatabase.
The MIT press.E.
Filatova and V. Hatzivassiloglou.
2004.
Event-basedextractive summarization.
In Proceedings of ACL Work-shop on Summarization, volume 111.A.
Haghighi and L. Vanderwende.
2009.
Exploring con-tent models for multi-document summarization.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 362?370, Boulder, Colorado, June.
Associationfor Computational Linguistics.J.
Jagarlamudi, P. Pingali, and V. Varma.
2006.
Queryindependent sentence scoring approach to DUC 2006.In DUC 2006.S.
Jegelka and J.
A. Bilmes.
2011.
Submodularity beyondsubmodular energies: coupling edges in graph cuts.In Computer Vision and Pattern Recognition (CVPR),Colorado Springs, CO, June.D.
Kempe, J. Kleinberg, and E. Tardos.
2003.
Maximiz-ing the spread of influence through a social network.In Proceedings of the 9th Conference on SIGKDD In-ternational Conference on Knowledge Discovery andData Mining (KDD).S.
Khuller, A. Moss, and J. Naor.
1999.
The budgetedmaximum coverage problem.
Information ProcessingLetters, 70(1):39?45.V.
Kolmogorov and R. Zabin.
2004.
What energy func-tions can be minimized via graph cuts?
IEEE Trans-actions on Pattern Analysis and Machine Intelligence,26(2):147?159.A.
Krause and C. Guestrin.
2005.
Near-optimal nonmy-opic value of information in graphical models.
In Proc.of Uncertainty in AI.A.
Krause, H.B.
McMahan, C. Guestrin, and A. Gupta.2008.
Robust submodular observation selection.
Jour-nal of Machine Learning Research, 9:2761?2801.H.
Lin and J. Bilmes.
2010.
Multi-document summariza-tion via budgeted maximization of submodular func-tions.
In North American chapter of the Associationfor Computational Linguistics/Human Language Tech-nology Conference (NAACL/HLT-2010), Los Angeles,CA, June.H.
Lin and J. Bilmes.
2011.
Word alignment via submod-ular maximization over matroids.
In The 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies (ACL-HLT), Port-land, OR, June.H.
Lin, J. Bilmes, and S. Xie.
2009.
Graph-based submod-ular selection for extractive summarization.
In Proc.IEEE Automatic Speech Recognition and Understand-ing (ASRU), Merano, Italy, December.C.-Y.
Lin.
2004.
ROUGE: A package for automatic eval-uation of summaries.
In Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop.L.
Lova?sz.
1983.
Submodular functions and convexity.Mathematical programming-The state of the art,(eds.
A.Bachem, M. Grotschel and B. Korte) Springer, pages235?257.M.
Minoux.
1978.
Accelerated greedy algorithms formaximizing submodular set functions.
OptimizationTechniques, pages 234?243.M.
Narasimhan and J. Bilmes.
2005.
A submodular-supermodular procedure with applications to discrimi-native structure learning.
In Proc.
Conf.
Uncertainty inArtifical Intelligence, Edinburgh, Scotland, July.
Mor-gan Kaufmann Publishers.M.
Narasimhan and J. Bilmes.
2007.
Local search forbalanced submodular clusterings.
In Twentieth Inter-national Joint Conference on Artificial Intelligence (IJ-CAI07), Hyderabad, India, January.H.
Narayanan.
1997.
Submodular functions and electricalnetworks.
North-Holland.G.L.
Nemhauser, L.A. Wolsey, and M.L.
Fisher.
1978.
Ananalysis of approximations for maximizing submodularset functions I.
Mathematical Programming, 14(1):265?294.P.
Pingali, K. Rahul, and V. Varma.
2007.
IIIT Hyderabadat DUC 2007.
Proceedings of DUC 2007.V.
Qazvinian, D.R.
Radev, and A. Ozgu?r.
2010.
Cita-tion Summarization Through Keyphrase Extraction.
InProceedings of the 23rd International Conference onComputational Linguistics (Coling 2010), pages 895?903.519A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In EMNLP, volume 1, pages133?142.K.
Riedhammer, B. Favre, and D. Hakkani-Tu?r.
2010.Long story short-Global unsupervised models forkeyphrase based meeting summarization.
Speech Com-munication.C.
Shen and T. Li.
2010.
Multi-document summarizationvia the minimum dominating set.
In Proceedings of the23rd International Conference on Computational Lin-guistics (Coling 2010), pages 984?992, Beijing, China,August.
Coling 2010 Organizing Committee.M.
Sviridenko.
2004.
A note on maximizing a submodu-lar set function subject to a knapsack constraint.
Oper-ations Research Letters, 32(1):41?43.H.
Takamura and M. Okumura.
2009.
Text summariza-tion model based on maximum coverage problem andits variant.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 781?789.
Association forComputational Linguistics.K.
Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,H.
Suzuki, and L. Vanderwende.
2007.
The PYTHYsummarization system: Microsoft research at DUC2007.
In the proceedings of Document UnderstandingConference.D.
Wang, S. Zhu, T. Li, and Y. Gong.
2009.
Multi-document summarization using sentence-based topicmodels.
In Proceedings of the ACL-IJCNLP 2009 Con-ference Short Papers, pages 297?300, Suntec, Singa-pore, August.
Association for Computational Linguis-tics.L.A.
Wolsey.
1982.
An analysis of the greedy algorithmfor the submodular set covering problem.
Combinator-ica, 2(4):385?393.P.
Zhao, G. Rocha, and B. Yu.
2009.
Grouped and hier-archical model selection through composite absolutepenalties.
Annals of Statistics, 37(6A):3468?3497.520
