Unsupervised Multilingual SentenceBoundary DetectionTibor Kiss?Ruhr-Universita?t BochumJan Strunk?
?Ruhr-Universita?t BochumIn this article, we present a language-independent, unsupervised approach to sentence boundarydetection.
It is based on the assumption that a large number of ambiguities in the determinationof sentence boundaries can be eliminated once abbreviations have been identified.
Instead ofrelying on orthographic clues, the proposed system is able to detect abbreviations with highaccuracy using three criteria that only require information about the candidate type itself andare independent of context: Abbreviations can be defined as a very tight collocation consistingof a truncated word and a final period, abbreviations are usually short, and abbreviationssometimes contain internal periods.
We also show the potential of collocational evidence fortwo other important subtasks of sentence boundary disambiguation, namely, the detectionof initials and ordinal numbers.
The proposed system has been tested extensively on elevendifferent languages and on different text genres.
It achieves good results without any furtheramendments or language-specific resources.
We evaluate its performance against three differentbaselines and compare it to other systems for sentence boundary detection proposed in theliterature.1.
IntroductionThe sentence is a fundamental and relatively well understood unit in theoretical andcomputational linguistics.
Many linguistic phenomena?such as collocations, idioms,and variable binding, to name a few?are constrained by the abstract concept ?sentence?in that they are confined by sentence boundaries.
The successful determination of theseboundaries is thus a prerequisite for proper sentence processing.
Sentence boundarydetection is not a trivial task, though.
Graphemes often serve more than one purposein writing systems.
The period, which is employed as sentence boundary marker, is noexception.
It is also used to mark abbreviations, initials, ordinal numbers, and ellipses.Moreover, a period can be used to mark an abbreviation and a sentence boundary at thesame time.
In such cases, the second period is haplologically omitted and only one pe-riod is used as end-of-sentence and abbreviation marker.1 Sentence boundary detection?
Sprachwissenschaftliches Institut, Ruhr-Universita?t Bochum, 44780 Bochum, Germany.E-mail: tibor@linguistics.rub.de.??
Sprachwissenschaftliches Institut, Ruhr-Universita?t Bochum, 44780 Bochum, Germany.E-mail: strunk@linguistics.rub.de.1 See Nunberg (1990) for a linguistic discussion of punctuation and the ambiguity of the period.Submission received: 1 October 2003; revised submission received: 28 January 2006; accepted forpublication: 5 July 2006.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 4thus has to be considered as an instance of ambiguity resolution.
The ambiguity of theperiod is illustrated by example (1).Example 1CELLULAR COMMUNICATIONS INC. sold 1,550,000 common shares at $21.75 eachyesterday, according to lead underwriter L.F. Rothschild & Co.(cited from Wall Street Journal 05/29/1987)Periods that form part of an abbreviation but are taken to be end-of-sentencemarkers or vice versa do not only introduce errors in the determination of sentenceboundaries.
As has been reported in Walker et al (2001) and Kiss and Strunk (2002b),segmentation errors propagate into further components, which rely on accurate sen-tence segmentation, and subsequent analyses are affected negatively.In this article, we present an approach to sentence boundary detection that buildson language-independent methods and determines sentence boundaries with highaccuracy.
It does not make use of additional annotations, part-of-speech tagging, orprecompiled lists to support sentence boundary detection, but extracts all necessarydata from the corpus to be segmented.
Also, it does not use orthographic informationas primary evidence and is thus suited to processing single-case text.
It focuses onrobustness and flexibility in that it can be applied with good results to a variety oflanguages without any further adjustments.
At the same time, the modular structureof the system makes it possible to integrate language-specific methods and clues tofurther improve its accuracy.
The basic algorithm has been determined experimentallyon the basis of an unannotated development corpus of English.
We have applied theresulting system to further corpora of English text as well as to corpora from tenother languages: Brazilian Portuguese, Dutch, Estonian, French, German, Italian, Nor-wegian, Spanish, Swedish, and Turkish.
Without further additions or amendments tothe system produced through experimentation on the development corpus, the meanaccuracy of sentence boundary detection on newspaper corpora in the eleven languagesis 98.74%.We approach sentence boundary detection by first determining possible abbrevi-ations in the text.
Quantitatively, abbreviations are a major source of ambiguities insentence boundary detection since they often constitute up to 30% of the possiblecandidates for sentence boundaries in running text; see Section 6.1.
Abbreviations canbe characterized by a set of robust as well as cross-linguistically valid properties.
Thesame cannot be said of the concept ?sentence boundary?.
The end of a sentence cannoteasily be characterized as either appearing after a particular word, between two par-ticular words, after a particular word class, or in between two particular word classes.But, as we will show, an abbreviation can be cross-linguistically characterized in sucha way.It is our basic assumption that abbreviations are collocations of the truncated wordand the following period, and hence, that methods for the detection of collocations canbe successfully applied to abbreviation detection.
Firth (1957, page 181) characterizesthe collocations of a word as ?statements of the habitual or customary places of thatword.?
In languages that mark abbreviations with a following period, one could saythat the abbreviation is habitually made up of a truncated word (or sequence of words)and a following period.
But this might even be too weak a formulation.
While typicalelements of a collocation can also appear together with other words, the abbreviation isstrongly tied to the following period.
Ideally, in the absence of homography and typing486Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectionerrors, an abbreviation should always end in a final period.2 Hence, we characterize anabbreviation as a very strict collocation and use standard techniques for the detection ofcollocations.
These techniques will be modified appropriately to account for the strictertie between an abbreviated word and the following period.
It should be clear from theoutset that abbreviations cannot simply be handled by listing them because they forma productive and hence open word class; see also Mu?ller, Amerl, and Natalis (1980,pages 52ff.)
and Mikheev (2002, page 291).
We corroborate this fact with an experimentin Section 6.4.4.We offer a formal characterization of abbreviations in terms of three major proper-ties, which only rely on the candidate word type itself and not on the local context inwhich an instance of the candidate type appears.
First, as was already mentioned, anabbreviation looks like a very tight collocation in that the abbreviated word precedingthe period and the period itself form a close bond.
Second, abbreviations have thetendency to be rather short.
This does not mean that we have to assume a fixed upperbound for the length of a possible abbreviation, but that the likelihood of being anabbreviation declines if candidates become longer.
Using the length of a candidate as acounterbalance to the collocational bond between candidate and final period allows ourmethod to identify quite long abbreviations, as long as the collocational bond betweenthe candidate type and the period is very strong.
As a third characteristic property,we have identified the occurrence of word-internal periods contained in many abbre-viations.
While we have determined the aforementioned properties experimentally, webelieve that they indeed represent crucial traits of abbreviations.Using just these three characteristics, our system is able to detect abbreviations witha mean accuracy of 99.38% on newspaper corpora in eleven languages.
The effectivenessof the three properties is further corroborated by an experiment we have carried outwith a log-linear classifier; compare Section 6.4.7.
The reported figure does not includeinitials and ordinal numbers because these subclasses of abbreviations cannot be discov-ered using these characteristics and have to be treated differently.
The complete systemwith special heuristics for initials and ordinal numbers achieves an accuracy of 99.20%for the detection of abbreviations, initials, and ordinal numbers.The determination of abbreviation types already yields a large percentage of allsentence boundaries because all periods occurring after non-abbreviation types canbe classified as end-of-sentence markers.
Such a disambiguation on the type level,however, is insufficient by itself because it still has to be determined for every periodfollowing an abbreviation whether it serves as a sentence boundary marker at the sametime.
The detection of initials and of ordinal numbers, which are represented by digitsfollowed by a period in several languages, also requires the application of token-basedmethods because these subclasses of abbreviations are problematic for type-based meth-ods.
These observations suggest a two-stage treatment of sentence boundary detection,which is both type and token based.
We define a classifier as type based if it uses globalevidence, for example, the distribution of a type in a corpus, to classify a type as a whole.In contrast, a token-based classifier determines a class for each individual token basedon its local context.In the first stage, a resolution is performed on the type level to detect abbreviationtypes and ordinary word types.
After this stage, the corpus receives an intermediate2 There are several types of abbreviations that are usually not marked with a final period: acronymssuch as NATO or unit abbreviations such as kg (kilogram).
These do not present a problem for sentenceboundary detection and are therefore not discussed further in this article.
We will henceforth use theterm abbreviation to refer only to classes of abbreviations that are normally marked with a final period.487Computational Linguistics Volume 32, Number 4Figure 1Architecture of the Punkt System.annotation where all instances of abbreviations detected by the first stage are markedas such with the tag <A> and all ellipses with the tag <E>.
All periods following non-abbreviations are assumed to be sentence boundary markers and receive the annotation<S>.3 The second, token-based stage employs additional heuristics on the basis ofthe intermediate annotation to refine and correct the output of the first classifier foreach individual token.
The token-based classifier is particularly suited to determineabbreviations and ellipses at the end of a sentence giving them the final annotation<A><S> or <E><S>.
But it is also used to correct the intermediate annotation bydetecting initials and ordinal numbers that cannot easily be recognized with type-basedmethods and thus often receive the wrong annotation from the first stage.
The overallarchitecture of the present system, which we have baptized Punkt (German for period),is given in Figure 1.The present article is structured as follows: Likelihood ratios can be consideredthe heart of the present proposal.
Both the type-based and the token-based classifiersemploy likelihood ratios to determine collocational bonds between a possible abbrevi-ation and its final period, between the sentence boundary period and a word following3 These tags are not intended as XML tags and there are thus no corresponding closing tags </A>, </E>,and </S>.
The tags are attached to the right edge of all tokens that end in a final period.488Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectionit, and between words that surround a period.
Section 2 introduces the concept of alikelihood ratio and discusses the specific properties of the likelihood ratios employedby Punkt.
Section 3 describes the type-based classification stage, while Section 4 in-troduces the token-based reclassification methods.
Section 5 gives a short accountof how Punkt was developed and how we determined some necessary parameters.The experiments carried out with the present system are discussed in Section 6.
InSection 7, we compare Punkt to other sentence boundary detection systems proposedin the literature.2.
Likelihood RatiosPunkt employs likelihood ratios to determine collocational ties in the type-based as wellas in the token-based stage.
The usefulness of likelihood ratios for collocation detectionhas been made explicit by Dunning (1993) and has been confirmed by an evaluationof various collocation detection methods carried out by Evert and Krenn (2001).
Kissand Strunk (2002a, 2002b) characterize abbreviations as collocations and use Dunning?slog-likelihood ratio (log ?)
to detect them on the type level.
The present proposal differsfrom Kiss and Strunk?s earlier suggestion in employing a highly modified log-likelihoodratio for abbreviation detection in the type-based stage.
The reasons for this divergencewill be discussed in Section 2.1.
In the token-based stage, we employ Dunning?s originallog ?, but add an additional constraint to make it one-sided.
This version of log ?
willbe described in Section 2.2.2.1 Likelihood Ratios in the Type-based StageThe log-likelihood ratio by Dunning (1993) tests whether the probability of a word isdependent on the occurrence of the preceding word type.
When applied to abbrevi-ations, the following two hypotheses are compared in a Dunning-style log-likelihoodratio.Null hypothesis H0: P(?|w) = p = P(?|?w) (1)Alternative hypothesis HA: P(?|w) = p1 = p2 = P(?|?w) (2)The null hypothesis in (1) states that the probability of occurrence of a period is notdependent on the preceding word.
The alternative hypothesis in (2) assumes a depen-dency between the period and the preceding word.
The log-likelihood ratio is calculatedusing the binomial distribution to estimate the likelihoods of the two hypotheses asin (3).
As the probabilities p, p1, and p2 are determined by maximum-likelihood esti-mation (MLE) and the null hypothesis includes fewer parameters than the alternativehypothesis, the ratio log ?
is asymptotically ?2-distributed and can thus be used as atest statistic.log ?
= ?2 logPbinom(H0)Pbinom(HA)(3)489Computational Linguistics Volume 32, Number 4However, we have decided to employ different hypotheses for abbreviation detec-tion.
While the revised null hypothesis given in (4) is quite close to (1), the revisedalternative hypothesis in (5) differs sharply from the one suggested by Dunning.4Revised null hypothesis H0: P(?|w) = PMLE(?)
=C(?
)N (4)Revised alternative hypothesis HA: P(?|w) = 0.99 (5)The formulation of the alternative hypothesis in (5) reflects that we do not only requirethat a period occurs together with an abbreviated word more often than expected,but instead that a period almost always occurs after the truncated word.
By choosingthe value 0.99 instead of 1, we can provide for a certain probability that an abbre-viation type sometimes erroneously occurs without a final period in a corpus.
Thehypothesis in (5) captures our intuitions about abbreviations better than the originalversion in (2) because it is no longer sufficient that a word type appears more oftenthan average with a following period to yield a high log-likelihood score.
Instead, thelikelihood of a period appearing after a type should be almost 1 in order for it to getassigned a high log-likelihood score and thus a high likelihood of being classified asan abbreviation.Due to the revision of the hypotheses H0 and HA, the log-likelihood ratio forabbreviation detection is no longer asymptotically ?2-distributed.
However, this is not adisadvantage since the resulting log-likelihood value expresses only one of three crucialproperties of abbreviations.
Since this value is counterbalanced by other factors and theresulting log-likelihood score thus scaled in various ways, the ?2 distribution could notbe retained anyway, as will become clear in Section 3.2.2 Likelihood Ratios in the Token-based StageIn the token-based classification stage, Dunning?s log-likelihood ratio is used in twodifferent heuristics.
The collocation heuristic, described in Section 4.1.2, takes a pairof words w1 and w2 surrounding a period and tests whether a collocational tie existsbetween them.
A positive answer to this question is used as evidence against anintervening sentence boundary.
The frequent sentence starter heuristic, described inSection 4.1.3, makes use of the results of the type-based classifier and searches for wordtypes that form a collocation with a preceding sentence boundary, that is, which occurparticularly often after end-of-sentence periods.Dunning?s formulation of the log-likelihood ratio is a two-tailed statistical test.
Fora pair of word types w1 and w2, the null hypothesis H0: P(w2 | w1) = p = P(w2 | ?w1),and the alternative hypothesis HA: P(w2 | w1) = p1 = p2 = P(w2 | ?w1), the log ?
valueis high if p1 and p2 significantly diverge from each other.
But in fact, one should onlyconsider those pairs of words as collocations for which p1 is much higher than p2.
Onlythe latter case means that w2 occurs more often than expected after w1, whereas if p1 isless than p2 this means that w2 occurs less often after w1 than expected.
Manning and4 N represents the number of tokens in the corpus.
C(?)
is the number of times a token-final period occursin the corpus.
C(.
.
. )
always signifies the absolute frequency of some element or elements in the corpus.490Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionSchu?tze (1999, page 172) comment in their description that ?
[w]e assume that p1 >> p2if Hypothesis 2 [i.e., the alternative hypothesis, TK/JS] is true.
The case p1 << p2 is rare,and we will ignore it here.?
To us, this step seems to be premature since it ignores thatDunning?s log ?
is a two-tailed test, where the equation in (6) holds.log ?
= 0 iffC(w2)N =C(w1, w2)C(w1)(6)If the two sides of this equation diverge, log ?
will take a value greater than 0.
Inthe usual case considered by Dunning (1993) and discussed by Manning and Schu?tze(1999), the right-hand side of the equation is larger than the left-hand side.
A highlog ?
value thus properly expresses the fact that w1 and w2 form a collocation becausethe occurrence of w2 after w1 is more likely than expected from the unconditionallikelihood of w2.
If, however, the left side of the equation is greater than the right side,we still get a log ?
value greater than 0, but this time, this indicates that w2 occursless often than expected after w1.
This is obviously at odds with the general idea ofa collocation.For collocations in general, the assumption made by Manning and Schu?tze (1999)can indeed be considered safe, since the types w1 and w2 are likely to occur only rarely.For this reason, we do not expect a large negative deviation, where the right-handside of equation (6) is significantly smaller than the left-hand side.
However, some ofthe types that we test for collocational ties in the token-based stage occur very often.The frequent sentence starter heuristic, for example, tests whether a given word w2forms a collocation with a preceding sentence boundary w1, that is, with the sentenceboundary symbol <S> inserted by the type-based first stage of Punkt.
The abstract type?sentence boundary?
(i.e., <S>) may be very frequent in many corpora, as can be wit-nessed from a sample from a German newspaper corpus, where C(<S>) = 35,775 andN = 847,206.In Table 1, we have tested for four words in the German newspaper corpus whetherthey are frequent sentence starters or not.
The first two words, ist (the third-personsingular form of the verb sein ?to be?)
and zu (infinitive marker or preposition ?to?)
occurvery often, while the latter two words do not occur so often.
Neither ist nor zu shouldbe considered frequent sentence starters, while both dennoch (?nevertheless?)
and erstens(?first?)
are true frequent sentence starters.
Yet, all four words receive very high log ?values, since in all cases p1 diverges significantly from p2.
However, it holds only fordennoch and erstens that they occur more often after <S> than expected, while ist andzu occur much less often than expected after <S>.
In order to exclude cases like istand zu from being detected as collocates of a preceding sentence boundary, we addTable 1Correct and incorrect frequent sentence starters.w1 w2 C(w1) C(w2) C(w1, w2) log ?<S> ist 35,775 9,758 182 169.8390 p1 < p2 incorrect<S> zu 35,775 7,643 71 298.8915 p1 < p2 incorrect<S> dennoch 35,775 231 80 221.4709 p1 > p2 correct<S> erstens 35,775 38 21 82.1377 p1 > p2 correct491Computational Linguistics Volume 32, Number 4the constraint in (7) to log ?
calculations for the frequent sentence starter heuristic andsimilarly apply it to the collocation heuristic.
?w1, w2?
is a collocation if log ?
> threshold and C(w1,w2)C(w1) >C(w2 )N (7)In general, it seems to be a good idea to add the one-sidedness condition given in(7) to the log-likelihood ratio used for the purpose of collocation detection.
The versionof the likelihood ratio that is calculated for the type-based classifier in Section 3 is notaffected by this problem since it does not follow a ?2 distribution and is in fact no longertwo-tailed because of the form of the two hypotheses that are compared.3.
Type-based ClassificationThe type-based classification stage of Punkt employs three characteristic properties ofabbreviations:1.
Strong collocational dependency: Abbreviations always occur with afinal period.52.
Brevity: Abbreviations tend to be short.3.
Internal periods: Many abbreviations contain additional internal periods.As these three characteristics do not change for each individual instance of a type, wecombine them in a type-based approach to abbreviation detection.The implementation of the first property makes use of a likelihood ratio withthe revised hypotheses introduced in (4) and (5).
The list of all types that ever occurwith a following period in the corpus is sorted according to this likelihood ratio.
Theresulting value for a type expresses the assumption that this type is more likely to be anabbreviation than all types having lower values.The first four columns of Table 2 show a section of this sorted list, where non-abbreviations are indicated in italics.
The figures of occurrence in Table 2, which aredrawn from an actual corpus sample comprising 351,529 tokens of Wall Street Journaltext, also illustrate a potential problem, namely, that most of the candidate types arequite rare.
As has been pointed out by Dunning (1993), the calculation of log ?
assumes abinomial distribution.
It is therefore better suited to deal with sparse data than statisticsthat are based on a normal distribution, such as the t test.
This consideration carries overto the modified log-likelihood ratio employed here.Some true abbreviations in the left half of Table 2 are either ranked lower than non-abbreviations or receive the same log-likelihood value as non-abbreviations.
Accordingto the criterion of strong collocational dependency, ounces is a very good candidate foran abbreviation, as it never occurs without a final period in the corpus.
The collocationalcriterion alone is thus not sufficient to detect abbreviations with high precision.Table 2 confirms that abbreviations tend to be rather short: Each non-abbreviation islonger than the longest abbreviation.
We therefore use brevity as a further characteristic5 If all or certain abbreviations do not occur with a final period in a language, the problem of decidingbetween the sentence boundary and the abbreviation marker does not occur in that language or forthese abbreviations.492Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionTable 2Candidate list from an English test corpus.Candidate type C(w, ?)
C(w,??)
Revised log ?
Final sorting Scaled log ?n.h 5 0 28.08 n.h 7.60u.s.a 5 0 28.08 a.g 6.08alex 8 2 26.75 m.j 4.56ounces 4 0 22.46 u.n 4.56a.g 4 0 22.46 u.s.a 4.19ga 4 0 22.46 ga 3.04vt 4 0 22.46 vt 3.04ore 5 1 18.99 ore 0.321990s 5 1 18.99 reps 0.31mo 8 3 17.67 mo 0.30m.j 3 0 16.85 1990s 0.26depositor 3 0 16.85 ounces 0.06reps 3 0 16.85 alex 0.03u.n 3 0 16.85 depositor 0.00property to counterbalance the likelihood ratio.
In contrast to other proposals such asthe one by Mikheev (2002, page 299), we refrain from using a fixed maximum lengthfor abbreviations.
Instead, we multiply the likelihood ratio for each candidate withan inversely exponential scaling factor derived from the length of that candidate.
Thelength of the candidate is defined as the number of characters in front of the final periodminus the number of internal periods, as illustrated in example (8).length(u.s.a.) = 3 (8)We exclude internal periods from the count because they are good evidence that acandidate should be classified as an abbreviation (see below).
We thus prevent a coun-terintuitive, higher penalty for candidates with internal periods.
The exact form of thelength factor is given in (9).Flength(w) = 1elength(w)(9)We have chosen an inversely exponential scaling factor since it reflects well the likeli-hood that a type of a certain length is an abbreviation.
Typically, short types are morelikely to be abbreviations than longer types.
The validity of this assumption can bewitnessed from Figure 2.
It shows that the ratio of the number of abbreviation typesto the number of non-abbreviation types decreases with growing length in a Dutchnewspaper corpus.
While about 96% of all types of length one are abbreviations, thispercentage drops rapidly to 59% for types of length two and to 15% for types of lengththree and so on.In addition to avoiding any higher penalty on the factor Flength caused by internalperiods, we use another scaling factor Fperiods as given in (10).
This factor expresses theintuition that a higher number of internal periods increases the likelihood that a type is493Computational Linguistics Volume 32, Number 4Figure 2Percentage of types of different lengths that are abbreviations in a Dutch corpus.a true abbreviation.
The scaling factor has been designed to leave unchanged the valuesof candidates that do not contain internal periods, while those candidates that containinternal periods receive an extra advantage.Fperiods(w) = number of internal periods in w + 1 (10)Multiplying the log-likelihood ratio with these two scaling factors leads to a signifi-cantly better sorting of the candidate list.The scaled log-likelihood ratio does not exclude a candidate from being classified asan abbreviation just because it has occurred without a final period once or twice in thecorpus if there is otherwise good evidence that it is a true abbreviation.
For most lan-guages, this increased robustness is unproblematic because almost all ordinary wordsoccur without a period a sufficient number of times.
However, for some languages,the scaled log-likelihood ratio is not restrictive enough.
Verb-final languages, such asTurkish, where certain very common verbs appear at the end of a sentence most of thetime, are one example.
In such a case, the scaled log-likelihood ratio described so farruns into difficulties because it mistakes the occurrences of these verbs without a periodas exceptions.
To remedy this problem, the calculated log ?
values are multiplied by athird factor, which penalizes occurrences without a final period exponentially:Fpenalty(w) = 1length(w)C(w,??
)(11)It should be noted that we use the length of the candidate as a basis for the calcula-tion because the likelihood of existence of homographic non-abbreviations for a given494Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectionabbreviation type is dependent on the length of the candidate.
Homographic non-abbreviations occur particularly often with abbreviations of length 1, and accordingly,there will be no penalty at all.
With a length of 2, the penalty factor is still moderate,but increases with length to reflect that longer abbreviations are not very likely tohave homographic non-abbreviations.
Furthermore, the penalty factor is exponentiallyscaled.
Hence, it will mostly affect candidate types with a high number of occurrenceswithout a final period.
A candidate type that occurs six times in all and one time withouta final period is much less affected by (11) than a candidate that occurs 600 times in alland 100 times without a final period.
This reflects our intuition that homographic non-abbreviations and typing errors can always occur, but that a high number of instanceswithout a final period requires a strong penalty.As we cannot rely any longer on the asymptotic ?2 distribution of the log-likelihoodratio (cf.
Section 2.1), we propose a new threshold value.
All candidates above itwill be considered abbreviations; all candidates below it will be classified as ordinarywords.
We use the classification function defined in (12) with a threshold value of 0.3?represented by a dashed line in Table 2.
We have determined the threshold value bymanual experimentation (cf.
Section 5) and have used it throughout the evaluation inSection 6.For each candidate word type w:If log ?
(w)?Flength(w)?Fperiods(w)?Fpenalty(w)?0.3 ?
w is an abbreviation.If log ?
(w)?Flength(w)?Fperiods(w)?Fpenalty(w)<0.3 ?
w is not an abbreviation.
(12)The last two columns of Table 2 show the final sorting of the candidates after applyingthe three scaling factors to the log-likelihood value for each candidate.
Multiplicationwith the three factors has led to a much cleaner separation of the true abbreviationtypes from the non-abbreviations.4.
Token-based ClassificationThe second classification stage of Punkt operates on the token level and improves theintermediate annotation of the corpus provided by the type-based classification stage.For every token with a final period, the system decides on the basis of its immediateright context whether the intermediate annotation has to be modified or corrected.For this classification step, the relevant tokens are separated into different classes(cf.
Figure 1).
Each class triggers a reexamination with a different combination of a fewbasic heuristics.
The most important classes are ordinary abbreviations and ellipses,which may appear at the end of a sentence.
Moreover, there are two special classesof abbreviations, which are problematic for a type-based approach, namely, possibleinitials and possible ordinal numbers.Three basic heuristics are employed in the token-based classification stage: theorthographic heuristic, whose task is to test for orthographic clues for the detectionof sentence boundaries after abbreviations and ellipses; the collocation heuristic, whichdetermines whether two words surrounding a period form a collocation and interprets apositive answer to this question as evidence against an intervening sentence boundary;and finally the frequent sentence starter heuristic, which suggests a preceding sentenceboundary if a word appearing after a period is found on a list of frequent sentencestarters induced on the fly from the text that is to be segmented.495Computational Linguistics Volume 32, Number 44.1 Heuristics in the Token-based Stage4.1.1 The Orthographic Heuristic.
At first sight, it might seem reasonable to rely onorthographic conventions for the detection of sentence boundaries.
For instance, acapitalized word usually indicates a preceding sentence boundary in mixed-case text.However, such a procedure is perilous for various reasons.
To begin with, certain wordclasses are capitalized even if they occur sentence-internally as is the case with themajority of proper nouns in English and all nouns in German.
Even a lowercase firstletter does not guarantee that the word in question is not preceded by a sentenceboundary.
This is particularly evident for mathematical variables or names that areconventionally written without capitalization such as amnesty international; see alsoNunberg (1990, pages 54ff.).
Finally, any method that relies solely on capitalization willnot help at all with single-case text.Still, we think that capitalization information?if used cautiously?can help todetermine whether an abbreviation or ellipsis precedes a sentence boundary or not;see Section 6.4.5 for a discussion of the importance of orthographic information incomparison to other types of evidence.
In order to make the usage of orthographicinformation safer and more robust, Punkt counts how often every word type occurswith an uppercase and lowercase first letter at the beginning of a sentence and sentence-internally in the corpus; see Table 3 for some example statistics.
It bases its calculationson the sentence boundaries determined by the type-based classification stage.
It doesnot count tokens occurring after an abbreviation or an ellipsis because we have notyet determined whether they start a new sentence or not.
The algorithm also ignorestokens that occur after possible initials and numbers.
Again, a reclassification is likelyto happen; see Section 4.3.
In sum, as the counts are based on imperfectly annotateddata, we try to exclude most doubtful cases.Figure 3 gives a pseudocode description of the orthographic heuristic, which de-cides for a token following an abbreviation or an ellipsis on the basis of the orthographicstatistics gathered for all word types whether it represents good evidence for a preced-ing sentence boundary or not.If the word following an abbreviation or ellipsis is capitalized, the heuristic deter-mines whether it occurs with a lowercase first letter in the text and whether it occurswith an uppercase first letter sentence-internally.
Only if it occurs with a lowercase firstletter at least once and never occurs with an uppercase first letter sentence-internally,the heuristic opts for a sentence boundary after the abbreviation or ellipsis.
Otherwise,it returns undecided.Table 3Example data for the orthographic heuristic.Type Uppercase Lowercase Uppercase Lowercase Uppercase Lowercaseall all after sure after sure clearly clearlysentence sentence sentence sentenceboundary boundary internally internallya 2,229 34,483 720 0 654 34,466across 2 129 1 0 0 129actual 3 52 2 0 0 52ask 9 140 1 0 7 140psychologists 2 4 1 0 0 4smith 348 0 6 0 218 0496Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionFigure 3Pseudo code of the orthographic heuristic.If the token following the abbreviation or ellipsis has a lowercase first letter, theheuristic decides against a sentence boundary if that type also occurs with an uppercasefirst letter or if it never occurs with a lowercase first letter after a sentence boundary.
Inall other cases, the heuristic returns undecided.For example, if the input of the orthographic heuristic is the capitalized token A,the heuristic would return undecided given the data in Table 3, because the type a occurscapitalized both at the beginning of and inside a sentence.
If the input is the lowercasetoken a, it would return no sentence boundary.
For the proper name Smith, the resultwould be undecided since the name never occurs with a lowercase first letter at all.
Forthe input tokens Across, Actual, and Psychologists, the heuristic would decide in favor ofa sentence boundary.
If the same tokens were not capitalized, the decision would be nosentence boundary.In the worst-case scenario of an all-uppercase corpus, the orthographic heuristicwould always return undecided.
If all tokens in the corpus begin with a lowercase letter, itwould either return undecided or no sentence boundary, which we think is the safest optionsince in this case, the heuristic cannot adduce any additional token-based evidence andmost sentence boundaries have already been discovered by the type-based stage.
Theoption to refuse a decision on the basis of capitalization information thus makes theorthographic heuristic very robust.4.1.2 The Collocation Heuristic.
The basic intuition behind the collocation heuristic isthat sentence boundaries block collocational ties; compare, for example, Manning andSchu?tze (1999, page 195).
If a period is surrounded by two words that form a collocation,we do not expect it to act as a sentence boundary marker.
A period should thereforebe interpreted as an abbreviation marker and not as a sentence boundary marker ifthe two tokens surrounding it can indeed be considered as a collocation according toDunning?s (1993) original log-likelihood ratio amended with the one-sidedness con-straint introduced in Section 2.2.
Following the asymptotic ?2 distribution of Dunning?soriginal proposal, we require a log ?
value of at least 7.88, which represents a confidenceof 99.5%.
If this condition is met, we assume that the two words surrounding thepotential sentence boundary form a collocation and hence represent evidence againstan intervening sentence boundary.4.1.3 The Frequent Sentence Starter Heuristic.
We also employ Dunning?s log ?
asan unsupervised method for the extraction of frequent sentence starters, that is, word497Computational Linguistics Volume 32, Number 4types that occur particularly often after a sentence boundary.
We take the viewpointof collocation detection and define a frequent sentence starter as a word type that hasa strong collocational tie to a preceding sentence boundary.
The occurrence of such afrequent sentence starter after a period can thus be used to adduce further evidencethat the preceding period marks a sentence boundary.In contrast to Mikheev (2002, page 297), we do not extract the list of frequentsentence starters from an additional corpus but use the test corpus itself.
The basic ideais to build a list of frequent sentence starters on the fly by counting how often everyword type occurs following a sure sentence boundary, as determined by the type-basedfirst stage.
Sure sentence boundaries are all single periods following words that havebeen classified as non-abbreviations and are not possibly initials or numbers, that is,single letters or a sequence of one or more digits.
Once the problem has been formulatedas a problem of collocation detection, we can use the amended version of Dunning?slog ?, as described in Section 2.2, to test the candidate word types for a collocationaltie to a preceding sentence boundary.
Since we are relying on uncertain information,namely, the intermediate annotation, we assume an exceptionally high threshold valueof 30 for the classification.
Only if this value is reached or exceeded do we put thecandidate on the list of frequent sentence starters.
The high cutoff value has beendetermined experimentally during the development of Punkt (cf.
Section 5).Finally, there exists an interaction between the frequent sentence starter heuristicand the collocation heuristic described in the preceding section in that the frequentsentence starter heuristic may help to counterbalance the collocation heuristic.
Some-times, the collocation heuristic will detect a collocation across a sentence boundary,particularly if the word preceding the boundary occurs quite often at the end of asentence and the word following the sentence boundary is a frequent sentence starter.By determining the frequent sentence starters in the corpus and preventing the detectionof collocations with these types as second elements, collocation detection is made safer.4.2 Token-based Reclassification of Abbreviations and EllipsesThe main question for all tokens classified as abbreviations by the type-based first stageand all ellipses is whether they precede a sentence boundary or not.
A sentence bound-ary after these two classes of candidate tokens is assumed by Punkt if the orthographicheuristic applied to the token following the abbreviation or ellipsis decides in favor of asentence boundary, or the token following the abbreviation or ellipsis is a capitalized fre-quent sentence starter.
However, only abbreviations that are longer than one letter andthus not possibly initials are reclassified in this way.
Initials present special problemsand are therefore reclassified differently, as will be discussed in the following section.4.3 Token-based Detection of Initials and Ordinal NumbersInitials are a subclass of abbreviations consisting of a single letter followed by a period.6As there are only about thirty different letters in the average Latin-derived alphabet, the6 Sometimes, two or more initials are not separated from each other by spaces, compare L.F. Rothschild inexample (1).
Although these cases are usually still regarded as initials, our system does not treat them assuch but as ordinary abbreviations because it cannot distinguish abbreviations with internal periods fromsuch run-on combinations of initials.
This is however not harmful because combinations like L.F. arenormally short, contain internal periods, and will probably not occur without a following period so thatthere is a high likelihood that they will be recognized as abbreviations in the type-based stage.498Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectionlikelihood of being a homograph of an ordinary word is very high for initials; consider,for example, the Portuguese definite articles o and a or the Swedish preposition i (?in?
).Moreover, there are also various other uses for single letters: in formulas, enumerations,and so on.
Initials are therefore often not detected by the type-based first stage ofthe Punkt system.
For this reason, all single letters followed by a token-final periodare treated as possible initials during the token-based reclassification?regardless ofwhether they have been classified as abbreviations or not by the type-based stage.Luckily, initials are very often part of a complex name and can often be identifiedusing collocational evidence.
If a possible initial forms a collocation with the followingtoken and the following token is not a frequent sentence starter, the period in betweenis reclassified as an abbreviation marker.
Alternatively, if the orthographic heuristicdecides against a sentence boundary on the basis of the token following the possibleinitial, the period is also reclassified as an abbreviation period.
Last but not least, weemploy a special heuristic for initials: If the orthographic heuristic returns undecided andthe type following the possible initial always occurs with an uppercase first letter, it isassumed to be a proper name and the period between the two tokens is again classifiedas an abbreviation marker.
The system never reclassifies a period following a possibleinitial as a sentence boundary marker because we assume that if a single letter is indeednot used as an abbreviation but, for example, as a mathematical symbol or if it is anordinary word, there will usually be enough occurrences of this type without a finalperiod so that the type-based stage will classify all periods following instances of thetype in question as sentence boundary periods.In many languages, such as German, ordinal numbers written in digits are alsomarked by a token-final period; compare example (2).Example 2Was sind die Konsequenzen der Abstimmung vom 12.
Juni?What are the consequences of the poll of the 12th of June?
(cited from NZZ 06/13/1994)As every numeric type can also be used as a cardinal number, it cannot be decidedby a type-based algorithm whether a period after a number is an abbreviation markeror a sentence boundary marker.
Numbers are therefore treated in the same way asinitials.
If the token following a number with a final period forms a collocation with theabstract type ##number##7 and is not a frequent sentence starter, the period in between isclassified as an abbreviation period.
The same conclusion is reached if the orthographicheuristic decides against a sentence boundary on the basis of the following token.5.
Development of the Punkt SystemThe three scaling factors used to improve on the initial sorting by the collocationalcriterion for abbreviation detection were obtained by manual experiments on a 10 MBdevelopment corpus of American English containing Wall Street Journal (WSJ) articles.This corpus is distinct from the portions of the WSJ we use for evaluation purposes inSection 6.
From this development corpus, a candidate list of possible abbreviation typeswas extracted and sorted according to the log-likelihood ratio described in Section 2.1.7 As specific numbers often occur very infrequently, we fold all numeric types into one abstract type##number## for the purposes of collocation detection.499Computational Linguistics Volume 32, Number 4We experimented with different factors and measured their impact in terms of precisionand recall on the candidates above a given threshold value.
Our goal was to maximizeprecision and recall for the top part of the list, that is, to get a clear separation of trueabbreviations at the top of the list from non-abbreviations at the bottom of the list.The factors Flength (9) and Fperiods (10) were conceived and tested solely on the basisof the WSJ development corpus.
We have added the third factor Fpenalty (11) to cope withthe problem of very common ordinary words that precede a sentence boundary mostof the time.
We encountered this problem in the Turkish test corpus, but it wouldprobably arise for other verb-final languages as well.
After fixing the final form ofthe scaling factors, we also used the candidate list from the development corpus todetermine the ideal threshold value for type-based abbreviation detection by manualinspection.
The best combination of the different methods in the token-based stage andthe threshold value 30 used for finding frequent sentence starters were also determinedby manual experimentation on the development corpus.
The same parameters andcombinations of heuristics have been employed in all tests described in Section 6 unlessotherwise noted.6.
EvaluationWe have tested our system extensively for a number of different languages and underdifferent circumstances.
We report the results that we obtained from our experiments inSection 6.4, after giving a short characterization of the test corpora on which we did ourevaluation in Section 6.1, defining the performance measures we use in Section 6.2 andproposing three baselines as lower bounds and standards of comparison in Section 6.3.We compare our approach to other systems for sentence boundary detection proposedin the literature in Section 7.6.1 Test CorporaWe have evaluated Punkt on corpora from eleven different languages: BrazilianPortuguese, Dutch, English, Estonian, French, German, Italian, Norwegian, Spanish,Swedish, and Turkish.
For all of these languages, we have created test corpora con-taining newspaper text, the genre that is most often used to test sentence boundarydetection systems; compare Section 7.
Table 4 provides a short description for eachone of these newspaper corpora.
Five of them?Dutch, French, Italian, Spanish, andSwedish?are parts of corpora taken from the CD-ROM Multilingual Corpus 1 dis-tributed by the European Corpus Initiative (ECI).
For English, we have used sections03-06 of the WSJ portion of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz1993) distributed by the Linguistic Data Consortium (LDC), which have frequently beenused to evaluate sentence boundary detection systems before; compare Section 7.
Forthe other languages, we have chosen newspaper corpora that were either available onthe Internet (Brazilian Portuguese), distributed by the newspaper itself (German), orkindly provided to us by other research institutions (Estonian, Norwegian, and Turk-ish).
While the Swedish corpus contains a small amount of literary fiction in addition tonewspaper articles from several Swedish newspapers, the other corpora consist solely ofnewswire text.To determine whether Punkt is also suitable for different text genres, we haveadditionally evaluated it on a piece of American English literature (compare Table 5)obtained from Project Gutenberg (www.gutenberg.org).
Last but not least, we have500Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionTable 4Newspaper corpora used in the evaluation.Language Origin ContentBrazilian Portuguese CETENFolha corpus (Linguateca) Folha de S. PauloDutch Multilingual Corpus 1 (ECI) De LimburgerEnglish Penn Treebank (LDC) Wall Street JournalEstonian By courtesy of the University of Tartu Eesti EkspressFrench Multilingual Corpus 1 (ECI) Le MondeGerman Neue Zu?rcher Zeitung AG CD-ROM Neue Zu?rcher ZeitungItalian Multilingual Corpus 1 (ECI) La Stampa, Il MattinoNorwegian By courtesy of the Centre for HumanitiesInformation Technologies, BergenBergens Tidende (Bokma?land Nynorsk)Spanish Multilingual Corpus 1 (ECI) SurSwedish Multilingual Corpus 1 (ECI) Dagens Nyheter (and others)Turkish METU Turkish Corpus (Tu?rkc?e DerlemProjesi), by courtesy of the Universityof AnkaraMilliyetTable 5Other corpora used in the evaluation.Language Origin ContentEnglish Brown corpus Balanced corpus of American English, different text genresEnglish Project Gutenberg The Works of Edgar Allan Poe in Five Volumes, Volumes I?III,literary fictionalso tested it on the Brown corpus of American English (Francis and Kucera 1982),which has often been used to evaluate other sentence boundary detection systems.This corpus contains a mixture of text genres including news, scientific articles, andliterary fiction.For the evaluation, we have created annotated versions of the test corpora, in whichall periods were disambiguated by hand and labeled with the correct tag from Table 6.Table 7 illustrates some statistical properties of the test corpora.
For each corpus, weprovide the number of tokens it contains and the number of all tokens with a finalperiod, that is, all periods that had to be classified by Punkt.
As an indication of thedifficulty of the sentence boundary detection task, we also give information on howmany abbreviations each corpus contains and what percentage of all the tokens witha final period actually are abbreviations.
Finally, the last column shows the number ofdifferent abbreviation types occurring in each test corpus.Table 6Tags used in the evaluation.<S> Sentence boundary<A> Abbreviation<E> Ellipsis<A><S> Abbreviation at the end of sentence<E><S> Ellipsis at the end of sentence501Computational Linguistics Volume 32, Number 4Table 7Statistical properties of the test corpora.Corpus Tokens Tokens with Abbr.
Abbr.
Abbr.final periods tokens tokens (%) typesB.
Portuguese 321,032 15,250 481 3.15 102Dutch 340,238 20,075 1,270 6.33 141English ?
WSJ 469,396 26,980 7,297 27.05 196English ?
Brown 1,105,348 54,722 5,586 10.21 213English ?
Poe 324,247 11,247 600 5.33 59Estonian 358,894 25,825 2,517 9.75 248French 369,506 12,890 375 2.91 91German 847,207 38,062 3,603 9.47 139Italian 312,398 11,561 442 3.82 156Norwegian 479,225 28,368 1,882 6.63 242Spanish 352,773 13,015 570 4.38 84Swedish 338,948 19,724 769 3.90 100Turkish 333,451 21,047 598 2.84 1036.2 Performance MeasuresThe most important measure we use is the error rate given in (13).
It is definedas the ratio of the number of incorrectly classified candidates to the number of allcandidates.error rate =false positives + false negativesnumber of all candidates(13)In addition, we use precision and recall to provide better information on what kindsof errors were made by Punkt.
Precision is the ratio between the number of candidatetokens that have been correctly assigned to a class and the number of all candidates thathave been assigned to this class.precision =true positivestrue positives + false positives(14)Recall is defined as the proportion of all candidates truly belonging to a certain classthat have also been assigned to that class by the evaluated system.recall =true positivestrue positives + false negatives(15)Finally, the so-called F measure is the harmonic mean of precision and recall (vanRijsbergen 1979).F measure =2 ?
precision ?
recallprecision + recall(16)502Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionAll the measures we use are based on counting true and false positives and true andfalse negatives.
There are, however, three possibilities of what classifications could beregarded as a positive or a negative outcome because Punkt actually performs threeclassification tasks at the same time.
The most important one is the decision whethera token-final period marks a sentence boundary or not.
But the system also decidesfor each candidate token ending in a period whether it is an abbreviation or not andwhether it is an ellipsis or not; compare Table 6.
The performance measures for sen-tence boundary detection, abbreviation detection, and ellipsis detection do not directlydepend on each other because a candidate can be an abbreviation (or an ellipsis) andprecede a sentence boundary at the same time.
We thus calculate error rate, precision,recall, and F measure for the sentence boundary detection problem and for the ab-breviation detection problem separately.
For sentence boundary detection, we countthe following tags as positives: <S>, <A><S>, and <E><S>.
The tags <A> and<A><S> are considered positives for the abbreviation detection task.
As the correctclassification of ellipses is straightforward, we do not give figures for the detectionof ellipses.6.3 BaselinesIn Sections 6.4.1 and 6.4.2, we evaluate Punkt against three different baseline algorithms.These baselines serve several purposes.
First, they establish a lower bound for the task ofsentence boundary detection.
Any sentence boundary detection system should performsignificantly better than these baseline algorithms.
Second, although we compare Punktto other systems proposed in the literature in Section 7, most previous work on sentenceboundary detection considered at most three different languages so that no directcomparison is possible for many of the corpora and languages that we have used inour evaluation.
A comparison with the performance of the three baselines can at leastgive an indication of how well our system did on these corpora.
Third, there is still anassumption held in the field that simple algorithms such as the baselines presented hereare sufficiently reliable to be used for sentence boundary detection.
This opinion was,for example, held by a reviewer of Kiss and Strunk (2002a).
As will become clear in thefollowing sections, a baseline algorithm may perform pretty well on one corpus, but thisperformance typically does not carry over to other languages or corpora.
The baselinesthus also serve to illustrate the complexity of the sentence boundary detection problem.The absolute baseline (AbsBL) is the simplest approach to sentence boundary de-tection we can think of.
It simply assumes that all token-final periods in a test corpusrepresent sentence boundaries.
Consequently, all periods are tagged with <S>.The second baseline algorithm (TokBL) relies only on the local context of a period.
Itis a token-based approach that uses only orthographic information.
All token-final pe-riods (including those that form part of an ellipsis) that do not precede a token startingwith a lowercase letter, a digit, or one of the following sentence internal punctuationmarks [; : ,] are classified as sentence boundary markers and annotated with <S>.
Allother token-final periods are either classified as abbreviations (<A>) or ellipses (<E>).The third baseline algorithm (TypeBL) is based on a method described byGrefenstette (1999) and also by Mikheev (2002, page 299).
It is a type-based approachthat decides for each candidate type whether it is an abbreviation or not.
All instancesof candidates that ever occur in an unambiguous position, that is, before a lowercaseletter or a sentence-internal punctuation mark [; : ,] are classified as abbreviations,and a period following them is not considered as an end-of-sentence marker.
All other503Computational Linguistics Volume 32, Number 4Table 8Results of classification?newspaper corpora (mixed case).Corpus Error Prec.
Recall F Error Prec.
Recall F(<S>) (<S>) (<S>) (<S>) (<A>) (<A>) (<A>) (<A>)(%) (%) (%) (%) (%) (%) (%) (%)B.
Port.
1.11 99.14 99.72 99.43 0.99 96.88 70.89 81.87Dutch 0.97 99.25 99.72 99.48 0.66 99.31 90.24 94.55English 1.65 99.13 98.64 98.89 0.71 99.86 97.52 98.68Estonian 2.12 98.58 99.07 98.83 1.75 98.22 83.51 90.27French 1.54 99.31 99.08 99.19 0.72 95.19 79.20 86.46German 0.35 99.69 99.93 99.81 0.26 99.91 97.34 98.61Italian 1.13 99.32 99.49 99.41 0.74 96.60 83.48 89.56Norw.
0.81 99.45 99.68 99.56 0.72 98.16 90.81 94.34Spanish 1.06 99.66 99.23 99.45 0.35 98.70 93.33 95.94Swedish 1.76 98.82 99.36 99.09 1.48 94.10 66.32 77.80Turkish 1.31 99.40 99.24 99.32 0.43 95.35 89.13 92.13Mean 1.26 99.25 99.38 99.31 0.80 97.48 85.62 90.93SD 0.49 0.33 0.38 0.29 0.46 1.99 10.19 6.69candidate types are treated as ordinary words and a period following them is classifiedas a sentence boundary marker.
No sentence boundary is assumed after ellipses.6.4 ExperimentsWe have tested Punkt on the various corpora introduced in Section 6.1.
In all cases,it was only provided with the unannotated test corpus as input and no further datawhatsoever, most importantly, no lexicon and no list of abbreviations.
Its main classifi-cation task was to decide for all token-final periods whether they indicated the end ofa sentence or not.8 In addition, it had to decide for all token-final periods whether theywere used as an abbreviation marker or were part of an ellipsis.The results Punkt achieved on the newspaper corpora are presented in Section 6.4.1.Those obtained for the remaining corpora are given in Section 6.4.2.
In Section 6.4.3, weprovide the results of an experiment in which we evaluated our system on all-uppercaseand all-lowercase corpora.
As many competing systems require a list of abbreviations,we have carried out an experiment to determine the usefulness of abbreviation listsderived from general-purpose dictionaries.
The results are reported in Section 6.4.4.Last but not least, we take a closer look at the architecture of Punkt in Section 6.4.5by examining the contributions of its individual parts, look at remaining errors andproblems in Section 6.4.6, and discuss the hypothesis that the methods and heuristicswe use can be called language independent in Section 6.4.7.6.4.1 Results on the Newspaper Corpora.
Table 8 shows the results that we obtainedfor the tasks of sentence boundary detection and abbreviation detection on the elevennewspaper corpora.
We performed two test runs for each language: one with detection8 Only periods were classified.
We did not include the less ambiguous exclamation and question marksin the evaluation.504Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionTable 9Comparison with the baselines?newspaper corpora (mixed case).Error <S> Error <A>Corpus Punkt AbsBL TokBL TypeBL Punkt AbsBL TokBL TypeBL(%) (%) (%) (%) (%) (%) (%) (%)B.
Port.
1.11 3.17 2.01 1.74 0.99 3.15 2.11 1.51Dutch 0.97 6.50 5.66 1.63 0.66 6.33 5.59 1.36English 1.65 25.60 13.37 7.14 0.71 27.05 14.96 5.40Estonian 2.12 10.03 4.86 7.45 1.75 9.75 4.94 6.53French 1.54 4.20 3.02 2.87 0.72 2.91 2.20 1.19German 0.35 9.50 6.23 8.74 0.26 9.47 6.22 8.60Italian 1.13 4.45 3.40 3.14 0.74 3.82 3.11 2.53Norw.
0.81 6.57 2.98 5.44 0.72 6.63 3.09 5.10Spanish 1.06 4.23 3.17 2.61 0.35 4.38 3.40 1.75Swedish 1.76 4.02 1.68 2.58 1.48 3.90 1.83 1.79Turkish 1.31 3.47 5.25 26.40 0.43 2.84 4.66 25.44Mean 1.26 7.43 4.69 6.23 0.80 7.29 4.74 5.56SD 0.49 6.46 3.24 7.48 0.46 7.01 3.69 7.05of ordinal numbers and one without a special treatment of numbers.
For languagessuch as English, which do not usually mark ordinal numbers with a final period, itis obviously preferable not to try to detect them.
In Table 8, we only report the bestresult from the two test runs for each language.9 Those languages in which the periodis not usually used to mark ordinal numbers and for which the test without specialtreatment of numbers achieved better results are italicized in the following tables.
How-ever, even if the special treatment of numbers was not turned off for such languages,the resulting increase in the error rate was not very high, maximally 0.03%; see alsoSection 6.4.5.For the sentence boundary detection task, the error rates Punkt achieved on theeleven newspaper corpora range from 2.12% on the Estonian corpus to only 0.35% onthe German corpus with an average error rate of 1.26%.
The error rates for abbreviationdetection are slightly lower, lying between 1.75% on the Estonian corpus and 0.26% onthe German corpus with an average of 0.80% for all eleven corpora.Table 9 compares Punkt?s performance to that of the three baseline algorithms.
Theerror rates achieved by Punkt for the sentence boundary task are reduced by about83% on average compared to the absolute baseline, by about 73% compared to thetoken-based baseline, and by almost 80% compared to the type-based baseline.
Theerror rates for the abbreviation detection task have decreased even more considerably,namely, by approximately 89% in comparison to the absolute baseline, by about 83% incomparison to the token-based baseline, and by almost 86% in comparison to the type-based baseline.
Table 9 also shows that whereas the good performance of our systemis quite stable across the eleven corpora with a standard deviation of only 0.49% for9 One exception is the French corpus.
It contains rankings from sports events in which ranks are indicatedusing digits and a following period.
Using the special detection of ordinal numbers on this corpus resultsin a lower error rate of 1.33% for the task of sentence boundary detection and 0.50% for abbreviationdetection.
However, as ordinal numbers in French are not usually indicated with final periods, we havegiven the results of the system without special treatment of ordinal numbers for French in Table 8.505Computational Linguistics Volume 32, Number 4sentence boundary detection and 0.46% for abbreviation detection, the performance ofthe baselines is not reliable at all.
Although one of the baselines sometimes performedwell on one corpus?such as TokBL on the Swedish corpus, the only case where Punktwas not better than all of the baselines, or TypeBL on the Brazilian Portuguese andDutch corpora, the baselines exhibit a very large standard deviation in their error ratesacross the eleven corpora and sometimes seem to fail completely, such as TokBL on theEnglish corpus and TypeBL on the Turkish corpus.6.4.2 Results on the Other Corpora.
In order to show that Punkt is also suited to processtext genres different from newspaper text and that its performance carries over to othertext types, we have tested it on two additional corpora of American English?the entireBrown corpus and The Works of Edgar Allan Poe (volumes I?III).
Table 10 provides theresults Punkt achieved on the two additional corpora.The error rate on the Brown corpus is 1.02% for the sentence boundary detectiontask and 0.82% for the abbreviation detection task.
This represents a reduction of about90% compared to the absolute baseline, a reduction of more than 85% compared tothe token-based baseline, and a reduction of more than 70% compared to the type-based baseline; see Table 11.
The error rate on The Works of Edgar Allan Poe is 0.80% forsentence boundary detection and 0.46% for abbreviation detection, which correspondsto a reduction of about 85% in comparison to AbsBL, a reduction by more than 80%compared to TokBL, and by about 75% compared to TypeBL.
These results achieved onthe literary Poe corpus and the Brown corpus with its balanced content fall within therange of the error rates achieved on the newspaper corpora and thus indicate that Punktis also well suited to deal with literary texts and corpora containing mixed content.6.4.3 Experiments with Single-case Corpora.
We have also tested the applicabilityof Punkt to single-case text.
The newspaper test corpora have been converted toall-uppercase and all-lowercase versions in order to determine how much Punkt isaffected by the loss of capitalization information.
In fact, one should keep in mindTable 10Results of classification?other corpora (mixed case).Corpus Error Prec.
Recall F Error Prec.
Recall F(<S>) (<S>) (<S>) (<S>) (<A>) (<A>) (<A>) (<A>)(%) (%) (%) (%) (%) (%) (%) (%)Brown 1.02 99.14 99.75 99.44 0.82 98.92 92.17 95.43Poe 0.80 99.71 99.45 99.58 0.46 95.36 96.00 95.68Table 11Comparison with the baselines?other corpora (mixed case).Error <S> Error <A>Corpus Punkt AbsBL TokBL TypeBL Punkt AbsBL TokBL TypeBL(%) (%) (%) (%) (%) (%) (%) (%)Brown 1.02 9.83 7.17 3.59 0.82 10.21 7.55 3.27Poe 0.80 5.03 4.12 3.12 0.46 5.33 4.42 2.76506Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectionthat single-case text does not only lack useful capitalization information, but actuallycontains information that is highly misleading for systems that rely primarily on cap-italization.
Table 12 shows the performance of our system on the single-case corpora.The left half of the table contains the error rates and F values for sentence boundarydetection and abbreviation detection on the lowercase corpora.
The right half givesthe corresponding values for the tests on the uppercase corpora.
The last two rowsof the table compare these results with those Punkt achieved on the mixed-case (MC)versions of the newspaper corpora; compare Section 6.4.1.
As can be seen in Table 12,the performance of our system is only minimally affected by the loss of capitalizationinformation, slightly more so on the all-lowercase corpora.
The error rate our systemproduces for the task of sentence boundary detection is 0.41% higher on average onthe lowercase corpora than on the mixed-case corpora.
The increase in the error rateon the uppercase corpora is slightly lower: 0.29%.
For the task of abbreviation detection,the increase in the error rates is even lower: 0.14% on the lowercase corpora and 0.13%on the uppercase corpora.
This is expected because Punkt only uses capitalizationinformation as evidence during the token-based correction and reclassification stageand not as primary evidence for the detection of abbreviations.
The experiments onthe single-case corpora show that Punkt is quite robust and well suited also to processsingle-case text.6.4.4 Experiments with Additional Abbreviation Lists.
Punkt is able to dynamicallydetect abbreviations in the test corpus itself.
It therefore does not depend on precom-piled abbreviation lists like some of its competitors; compare Section 7.
But eventhough an abbreviation list is not necessary for Punkt to perform well, such a list caneasily be integrated into its architecture.
The abbreviations read from such a list aresimply added to those the system has detected in the test corpus after the type-basedTable 12Results of classification?newspaper corpora (single case).All-lowercase corpora All-uppercase corporaCorpus Error F Error F Error F Error F(<S>) (<S>) (<A>) (<A>) (<S>) (<S>) (<A>) (<A>)(%) (%) (%) (%) (%) (%) (%) (%)B.
Port.
1.25 99.36 1.11 82.68 1.19 99.39 1.04 81.28Dutch 1.00 99.47 0.72 94.12 0.85 99.55 0.57 95.34English 2.30 98.44 0.71 98.66 2.04 98.63 0.72 98.65Estonian 2.57 98.57 1.66 91.01 2.80 98.45 2.10 88.13French 2.56 98.65 0.85 84.36 1.99 98.96 0.85 84.38German 0.40 99.78 0.26 98.62 0.47 99.74 0.30 98.38Italian 1.48 99.23 0.83 88.38 1.37 99.29 0.80 88.97Norwegian 1.55 99.17 1.32 89.73 1.41 99.25 1.18 90.54Spanish 1.31 99.31 0.45 94.69 1.12 99.41 0.32 96.24Swedish 2.39 98.76 1.86 72.88 2.28 98.82 1.74 74.25Turkish 1.53 99.20 0.57 89.74 1.54 99.20 0.58 89.30Mean 1.67 99.09 0.94 89.53 1.55 99.15 0.93 89.59SD 0.70 0.42 0.50 7.54 0.67 0.40 0.56 7.56Mean (MC) 1.26 99.31 0.80 90.93 1.26 99.31 0.80 90.93Difference 0.41 ?0.22 0.14 ?1.40 0.29 ?0.16 0.13 ?1.34507Computational Linguistics Volume 32, Number 4stage.
Ideally, one would use a domain-specific abbreviation list if the domain of thetest corpus is known beforehand.
However, we wanted to determine the usefulnessof general-purpose abbreviation lists derived from general-purpose dictionaries.
Wehave therefore built such abbreviation lists by extracting by hand all abbreviationsfrom a German spelling dictionary?the Rechtschreibduden (Dudenredaktion 2004)?and all English abbreviations from a bilingual dictionary?the small Muret-SandersEnglish-German dictionary by Langenscheidt (Willmann and Messinger 1996).
Thisyielded a total number of 769 abbreviations for German and 1,537 for English; compareTable 13.We then made three additional versions of these lists from which we deleted po-tentially harmful entries: one from which we removed all abbreviations that had ob-vious non-abbreviation homographs, one from which we removed all single-characterabbreviations, and one from which we removed both.
Table 13 gives the number ofremaining abbreviations for these different versions.
We produced these additionalversions to test how much care is needed when preparing abbreviation lists for a systemlike Punkt.We then carried out two experiments on the German and English newspapercorpora and the Brown corpus.
In the first experiment, we tested how well Punktperformed when it was provided with the different abbreviation lists in addition tothe abbreviations it was able to detect on the fly; see Table 14 for the results.The results show that Punkt can indeed benefit from additional abbreviationlists, but only if these are prepared with care.
Providing such a carefully preparedTable 13Abbreviation lists.List English GermanAll abbreviations 1537 769No homographs 1115 729No single characters 1513 742No homographs and no single characters 1112 703Table 14Results of classification?using an additional abbreviation list.Error <S>Corpus No list (%) All (%) No homogr.
(%) No single chars.
(%) Neither (%)WSJ 1.65 1.97 1.58 1.96 1.58Brown 1.02 1.75 0.93 1.72 0.92NZZ 0.35 0.37 0.32 0.37 0.32Error <A>Corpus No list (%) All (%) No homogr.
(%) No single chars.
(%) Neither (%)WSJ 0.71 0.89 0.63 0.88 0.63Brown 0.82 1.44 0.69 1.42 0.70NZZ 0.26 0.28 0.23 0.28 0.23508Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectionabbreviation list reduced the error rate of our system on the WSJ corpus from 1.65%to 1.58%, the error rate on the Brown corpus from 1.02% to 0.92%, and the error rate onthe German NZZ corpus from 0.35% to 0.32%.
Additional general-purpose abbreviationlists thus do improve the performance of our system, but the decrease of the error rateis not very great.
Table 14 also shows that abbreviation lists from which abbreviationshomographic to ordinary words and single-character abbreviations have not been re-moved are not helpful at all and instead lead to an increased error rate on all of thethree corpora.In the second experiment, Punkt could only use the abbreviations on the differentlists and was not allowed to add any additional abbreviations on the fly.
This experi-ment thus really tests the coverage of general-purpose abbreviation lists and also theproductivity of abbreviation use in the test corpora.
Table 15 contains the results fromthis experiment.
The column On the fly gives the error rates that Punkt achieved in itsnormal configuration detecting abbreviations on the fly without being provided with anadditional abbreviation list.
The remaining columns show the results it produced whenit could use a fixed list of abbreviations only.
A comparison between the first column andthe other columns makes clear that abbreviation use in the corpora is quite productiveand that fixed general-purpose abbreviation lists are clearly not sufficient for sentenceboundary detection.
A versatile sentence boundary detection system should thereforealways be able to detect unknown abbreviations on the fly.6.4.5 Contributions of the Individual Parts of the System.
In this section, we take a lookat the contributions of the individual parts of the system to its overall performance.First, we tried to determine the effectiveness of reclassifying the different candidateclasses in the token-based stage separately using specific combinations of evidence,namely, ordinary abbreviations, ellipses, initials, and numbers.
We therefore built fourdifferent versions of our system, which are described in Table 16.
The different config-urations become cumulatively more specialized in their treatment of the different can-didate classes from System 1 with no token-based reclassification at all to the completeSystem 4.Table 17 gives the results that the four different configurations achieved on theeleven newspaper corpora.
It shows that the cumulatively more specialized treatmentTable 15Results of classification?using only a fixed abbreviation list.Error <S>Corpus On the fly (%) All (%) No homogr.
(%) No single chars.
(%) Neither (%)WSJ 1.65 5.33 16.62 5.35 16.65Brown 1.02 2.53 5.24 2.60 5.26NZZ 0.35 2.55 2.64 2.59 2.67Error <A>Corpus On the fly (%) All (%) No homogr.
(%) No single chars.
(%) Neither (%)WSJ 0.71 4.57 16.70 4.58 16.73Brown 0.82 2.32 5.32 2.43 5.36NZZ 0.26 2.48 2.56 2.51 2.60509Computational Linguistics Volume 32, Number 4Table 16Configurations for testing the effectiveness of separate reclassification in the token-based stage.System Description1 Only type-based stage2 Type-based stage + Reclassification of abbreviations (including initials) and ellipses3 Type-based stage + Reclassification of abbreviations and ellipses + Separate treatmentof initials4 Type-based stage + Reclassification of abbreviations and ellipses + Separate treatmentof initials + Detection of ordinal numbers (Complete System)of the different candidate classes helps to improve on the error rate of the type-basedstage considerably.
Moreover, a separate reclassification of initials and numbers is quiteeffective for reducing the error rate on the newspaper corpora, often even more sothan the detection of sentence boundaries after abbreviations and ellipses.
The separatetreatment of initials is quite beneficial for the English corpus, for example, reducingthe error rate from 2.06% to 1.65% (System 2 vs. System 3), but also for Dutch, French,Italian, Norwegian and Spanish, while the detection of ordinal numbers is a veryimportant factor for the German newspaper corpus, reducing the error rate from 2.25%to only 0.35% (System 3 vs. System 4), and also for the Estonian, Norwegian, and Turkishcorpora.
For all languages that use the period to mark ordinal numbers, the detectionof ordinal numbers thus turns out to be a very important subtask of sentence boundarydisambiguation.
A comparison between System 3 and System 4 also shows that leavingthe detection of ordinal numbers on for languages that do not mark them with a finalperiod is not really harmful, resulting in a maximal increase in the error rate of 0.03%.In a second experiment, we have tested the usefulness of the different heuristicsused during the token-based stage.
Table 18 provides information on the five differentconfigurations we have evaluated.
We have again added heuristics cumulatively: first,the collocation heuristic; next the frequent sentence starter heuristic; then the ortho-graphic heuristic; and finally, the special orthographic heuristic for initials.Table 17Contributions of separate reclassifications?newspaper corpora (mixed case).Error <S>Corpus System 1 (%) System 2 (%) System 3 (%) System 4 (%)B. Portuguese 1.37 1.27 1.11 1.12Dutch 1.96 2.73 1.27 0.97English 2.71 2.06 1.65 1.68Estonian 7.37 6.88 6.46 2.12French 2.94 2.04 1.54 1.33German 2.38 2.32 2.25 0.35Italian 2.18 1.90 1.13 1.14Norwegian 4.09 3.92 3.38 0.81Spanish 1.81 1.67 1.06 1.08Swedish 2.42 2.16 1.95 1.76Turkish 1.89 1.72 1.70 1.31510Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionTable 18Configurations for testing the effectiveness of the heuristics in the token-based stage.System DescriptionA Only type-based stageB Type-based stage + Collocation heuristicC Type-based stage + Collocation heuristic + Frequent sentence starter heuristicD Type-based stage + Collocation heuristic + Frequent sentence starter heuristic +Orthographic heuristicE Type-based stage + Collocation heuristic + Frequent sentence starter heuristic +Orthographic heuristic + Special orthographic heuristic for initials (Complete System)Table 19 contains the error rates produced by Systems A to E on the eleven news-paper corpora.
It confirms that all heuristics contribute to the performance of thesystem, though to different degrees depending on the specific corpus.
It also showsthat the collocation heuristic is very effective in reducing the error rate on the differentcorpora, more effective in fact than the orthographic heuristic.
This fact supports ourargument that the importance of brittle orthographic evidence can be reduced andsentence boundary detection can be made more robust by relying more on collocationalevidence.
The collocation heuristic reduces the error rate from 7.37% to 2.94% on theEstonian corpus, for example, and is also very effective for German, Norwegian, andSpanish.
The impact of the frequent sentence starter heuristic is somewhat smaller,but it still leads to a substantial decrease in the error rate from 2.61% to 1.96% forFrench and to smaller reductions for all other languages.
Although Punkt does not relyso much on capitalization, the orthographic heuristic still reduces the error rate from2.80% to 2.18% for Estonian, for example, and leads to smaller improvements for theother languages except for English, where it causes a small increase in the error rate.As most combinations of initials and a following proper name are already captured bythe collocation heuristic, the special orthographic heuristic for initials is only applied tocomplex names that occur infrequently and thus does not result in a large reduction ofthe error rate.
Still, it never has a negative effect and is able to reduce the error rate fromTable 19Contributions of the heuristics?newspaper corpora (mixed case).Error <S>Corpus System A (%) System B (%) System C (%) System D (%) System E (%)B. Portuguese 1.37 1.28 1.13 1.12 1.11Dutch 1.96 1.17 1.13 1.05 0.97English 2.71 2.15 1.80 1.84 1.65Estonian 7.37 2.94 2.80 2.18 2.12French 2.94 2.61 1.96 1.78 1.54German 2.38 0.47 0.42 0.36 0.35Italian 2.18 1.60 1.39 1.23 1.13Norwegian 4.09 1.44 1.34 0.87 0.81Spanish 1.81 1.39 1.25 1.08 1.06Swedish 2.42 2.13 1.94 1.76 1.76Turkish 1.89 1.58 1.54 1.31 1.31511Computational Linguistics Volume 32, Number 41.84% to 1.65% on the English corpus and from 1.78% to 1.54% on the French corpus.We conclude that our heuristics are well motivated in that they decrease the errorrates on the newspaper corpora substantially and never have any severe detrimentaleffect.
Moreover, as Section 6.4.3 shows, they work effectively even for single-casecorpora.6.4.6 Remaining Errors and Problematic Cases.
The preceding sections have shownthat Punkt?s type-based abbreviation detection stage by itself is already quite effectiveand that the token-based heuristics are successful at further reducing the error rates onthe test corpora.
However, there remain some problematic cases, which Punkt currentlyis not able to deal with well and which we have to leave for further work.
The problemsthat resulted in errors on the test corpora can be grouped into a few major types: (1)homography, (2) inconsistent use of abbreviations, (3) data sparseness, (4) insufficientor contradicting orthographic evidence, and (5) problems with text structure.The first type of error results from the type-based nature of our approach to ab-breviation detection.
Abbreviations may not be recognized as such and wrong sen-tence boundaries may be introduced if an abbreviation is a homograph of an ordinaryword or an acronym because the type-based abbreviation detection stage is not able todistinguish between homographic types.
For example, the English abbreviation in.
forinch might not be recognized because it coincides with the frequent preposition in.
Alloccurrences of in with and without a final period will be added together and the type inwill most probably be classified as a non-abbreviation.
All instances of in followed bya period will then be marked as ordinary words preceding a sentence boundary.
Thistype of error was common in the English, Italian, and Portuguese test corpora.Inconsistent use of abbreviations within the same corpus represents a related prob-lem.
Sometimes, (certain) abbreviations are not obligatorily marked with a final period.For example, abbreviations for physical units such as m for meter or kg for kilogram aresometimes marked with a final period and sometimes not.
If both usages occur withinthe same test corpus, Punkt may classify such a type as a non-abbreviation, which maylead to incorrectly introduced sentence boundaries if the final period of some instancesof this type serves as abbreviation marker only.
This problem was important for theSwedish test corpus, where many frequent abbreviation types, such as osv.
?and so on?,were sometimes used with a final period and sometimes without one.The type-based abbreviation detection stage is also affected by data sparseness.
If anabbreviation type, especially a long one, occurs only very infrequently, there is simplynot enough collocational evidence to recognize it as an abbreviation.
Similarly, if aninfrequent ordinary word by chance always occurs in front of a period, it might bemistaken for an abbreviation.
This type of error occurred frequently in the Norwegianand Spanish corpora, in which quite a few long and rare abbreviations were used.
Theissue of data sparseness can also have an impact on the collocational and orthographicheuristics in the token-based stage.Sentence boundaries after abbreviations and ellipses are sometimes not found if thetoken following the period belongs to a type that does not provide good orthographicevidence for assuming a preceding sentence boundary.
This is the case if the type isalways capitalized, for example, because it is a proper noun, or if it occurs with anuppercase first letter within a sentence.
If the type is rare it may also occur only at thebeginning of a sentence by chance and will thus only be encountered in capitalizedform.
This lack of orthographic evidence had a detrimental effect on the recognitionof abbreviations at the end of a sentence in the English corpus and on the detection ofsentence boundaries after ellipses, for example, in the French corpus.512Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionLast but not least, not all parts of a text can be easily divided into sentences.
Forexample, headlines are usually not terminated with a period.
If an abbreviation isthe last word in a headline, Punkt often tags it with <A><S> because the followingtoken starts a new sentence and may provide good evidence for a preceding sentenceboundary.
However, a human annotator will probably consider the period following theabbreviation as an abbreviation marker only and not as a sentence boundary period inanalogy to other headlines.
Another problem relating to text structure is the question ofwhether an ordinal number in an enumerated list belongs to the same sentence as the listitem itself or whether there is a sentence boundary between them.
List items often beginwith a capitalized word; compare, for example, the enumeration in Section 3.
Punkttherefore mostly assumes that there is a sentence boundary between the number andthe list item.
Our human annotators, however, have considered the ordinal number tobe part of the following sentence.
These problems suggest that the automatic recognitionof text structure could be quite beneficial for sentence boundary detection.6.4.7 Language Independence and Optional Recalibration.
Punkt is conceived as alanguage and domain-independent corpus preprocessing tool that can be used out ofthe box for all languages that use an alphabetic writing system and employ the samesymbol to mark abbreviations and the end of sentence.
It is our hypothesis that thethreshold values we use should not vary much from language to language and fromcorpus to corpus.
We have therefore determined optimal thresholds once on an Englishdevelopment corpus and retained these values for all experiments described so far;compare Section 5.
We have carried out two experiments to substantiate our hypothesis.In the first experiment, we have tested different threshold values for the type-basedabbreviation detection stage.
Figure 4 shows that the threshold value of 0.3, which wehave used so far, turns out to be the ideal value for three of the corpora and that theminimum error rate for all eleven newspaper corpora lies at or is very close to thisthreshold value.
Table 20 gives the differences between the best error rate on each corpusand the error rate produced by our system with the chosen threshold value of 0.3.
TheFigure 4Error rates for different classification threshold values in the type-based stage.513Computational Linguistics Volume 32, Number 4maximal difference is 0.13% for Italian.
However, the average difference is only 0.03%.Moreover, Table 20 also indicates that the threshold values that produced the best resultsfor the different languages?specified in parentheses in the Lowest column?all lie veryclose to 0.3.
The biggest deviation is the optimal threshold value 0.6 for Estonian.
Theoutcome of this experiment shows that abbreviations in the eleven languages behavevery similarly and that the crucial type-based stage of Punkt can indeed be calledlanguage independent.This is further corroborated by a second experiment in which we trained gener-alized linear models for the detection of abbreviation types on the eleven newspapercorpora using a logit link function and no intercept term.
In this experiment, we used thefollowing three factors, which correspond to the collocational factor, the length factor,and the internal periods factor used in the type-based stage of Punkt:1.
Ratio: The ratio of occurrences of a candidate with a final period to alloccurrences of this candidate.2.
Length: The length of the candidate type (excluding periods).3.
Periods: The number of internal periods.We then examined the resulting parameters of the trained models in order todetermine whether the evidence we use in the type-based stage of Punkt is significantinformation for an effective model for abbreviation detection.
All three factors alwaysmake a highly significant contribution and cannot be dropped from the models.
Thefactor Periods is sometimes a little less important than the other two as there are somecorpora in which most abbreviations do not contain internal periods.
Figure 5 indicatesthe variation of the parameters that we obtained for the three factors (on the logit linkscale).
It is remarkably small.
The coefficient for the factor Periods exhibits the mostvariability, but is still quite stable.
This relatively small variation of the parametersfurther substantiates our claim that the evidence we use for abbreviation detection canbe considered language independent.We know from further experiments that the threshold values 7.88 and 30, whichwe have chosen for the collocation heuristic and the frequent sentence starter heuristic,respectively, work well for our test corpora but are not always the optimal valuesfor each individual corpus.
Although Punkt is conceived as a flexible, unsupervisedsystem, one can optionally recalibrate the threshold values by providing it with a hand-annotated training corpus.
We have tested this possibility by annotating a second FrenchTable 20Difference between lowest error rate and error rate achieved with a threshold of 0.3.Language 0.3 Lowest (at) Diff.
Language 0.3 Lowest (at) Diff.B.
Port.
1.11% 1.10% (0.4) 0.01% Italian 1.13% 1.00% (0.2) 0.13%Dutch 0.97% 0.93% (0.2) 0.04% Norwegian 0.81% 0.81% (0.3) 0.00%English 1.65% 1.59% (0.2) 0.06% Spanish 1.06% 1.06% (0.3) 0.00%Estonian 2.12% 2.10% (0.6) 0.02% Swedish 1.76% 1.76% (0.3) 0.00%French 1.54% 1.51% (0.2) 0.03% Turkish 1.31% 1.25% (0.4) 0.06%German 0.35% 0.34% (0.2) 0.01% Average difference: 0.03%514Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionFigure 5Variation of parameters in a log-linear model for type-based abbreviation detection.corpus by hand.
It again comprises articles from the newspaper Le Monde.
It contains371,526 tokens in all and 13,664 tokens ending in a final period.
Punkt achieved anerror rate for sentence boundary detection of 1.84% on this corpus.
We used this secondFrench corpus as training corpus to recalibrate the threshold values for the collocationheuristic and the frequent sentence starter heuristic.
The optimal values determined onthis training corpus were 17 for the collocation heuristic and 5 for the frequent sentencestarter heuristic.
We then used these values in a second test run on our original Frenchtest corpus.
The resulting error rate for the task of sentence boundary detection was1.44%, while the error rate for abbreviation detection was 0.71%.
These results are alittle better than the ones achieved without recalibration (1.54% and 0.72%); compareSection 6.4.1.
The optimal threshold values determined on the original test corpusitself are 6 for the collocation heuristic and 5 for the frequent sentence starter heuristic.The large difference between the best threshold values for the collocation heuristic onthe two French corpora shows that the ideal value can vary from corpus to corpus,even if two corpora contain text of the same language and the same genre: in this casenewspaper articles from Le Monde.
Nevertheless, the lower error rate obtained in thisexperiment shows that Punkt can optionally be recalibrated on a training corpus tofurther optimize its performance.
It can thus benefit from supervised training data,such as abbreviation lists or a training corpus, but does not require such data in order toperform well.7.
Comparison to Other SystemsThe results we presented in the previous section show that Punkt is able to achieve lowerror rates on corpora from eleven different languages, that it is well-suited to processdifferent text genres, and that it is robust enough to deal with single-case text.
Moreover,it reliably outperforms the three baseline algorithms, and its performance is much morestable than that of the baselines.In this section, we want to compare the performance of our system directly tothat of competing systems and discuss advantages and disadvantages of the differentapproaches to sentence boundary detection.
Unless otherwise indicated, the term errorrate in this section always refers to the error rate for the task of sentence boundarydetection.515Computational Linguistics Volume 32, Number 47.1 Rule-based SystemsRule-based systems make use of hard-coded rules and fixed lists of lexical items such asabbreviations in order to identify which punctuation marks signal sentence boundariesand which do not; that is, they neither learn from an annotated training corpus nor usethe test corpus itself to induce the required knowledge but rather employ precompiledresources usually provided by a human expert.
We will discuss one such system bySilla, Valle, and Kaestner (2003) and compare its performance directly with that of oursystem.
Moreover, we will also refer to results by Grefenstette (1999).7.1.1 The RE system by Silla, Valle, and Kaestner (2003).
The RE system10 scans thetest corpus until it encounters a period.
It then compares the one token preceding andthe one token following the period with a database of regular expressions that describeexceptions such as Web addresses, decimal numbers, and, most importantly, abbrevi-ations, in which the period does not indicate the end of a sentence.
If the precedingtoken and/or the following token match a regular expression in the database, the REsystem concludes that the period does not indicate a sentence boundary and searchesfor the next period.
If no matching regular expression is found, the period is classifiedas a sentence boundary.In Strunk, Silla, and Kaestner (2006), we have compared Punkt?s performance tothat of the RE system on two test collections: on articles from the WSJ taken from theTIPSTER document collection (TREC reference number: WSJ-910130) and on the Brazil-ian Portuguese Lacio-Web Corpus (Aluisio et al 2003).
The RE system was specificallydeveloped for English newspaper texts.
In order to use it on the Portuguese Lacio-Webcorpus, 240 new regular expressions, which match Portuguese abbreviations, had to beadded.
Silla and Kaestner (2004) describe the adaptation process as ?easy, although timeconsuming.
?On the English TIPSTER test corpus, Punkt achieved results that were only slightlyworse than those of the RE system: The RE system reached a precision of 92.39% anda recall of 91.18%, which yielded an F measure of 91.78%, while Punkt achieved aslightly lower precision of 90.70% and a slightly higher recall of 92.34% resulting in an Fmeasure of 91.51%.
When comparing these results, it has to be kept in mind that the REsystem employs a handcrafted list of more than 700 abbreviations and was specificallydeveloped for English newspaper text, while Punkt was not given any informationbesides the test corpus itself.
Punkt?s performance on the English TIPSTER corpus isthus quite impressive.
This is further corroborated by the fact that Punkt was ableto outperform the RE system on the Portuguese Lacio-Web corpus, even though 240Portuguese abbreviations had been collected for the database of the RE system by hand:The RE system scored a precision of 91.80% and a recall of 88.02%, which resulted inan F measure of 89.87%, while Punkt achieved a precision of 97.58% and a recall of96.87%, yielding a much better F measure of 97.22%.
In sum, Punkt almost matched theperformance of the RE system on the English test corpus and clearly outperformed it onthe Portuguese test corpus.7.1.2 Grefenstette (1999).
This book chapter by Grefenstette?which is based on earlierwork by Grefenstette and Tapanainen (1994)?discusses different approaches to sen-10 This is the name given to it in Silla and Kaestner (2004).
RE stands for regular expressions.
It is availablefrom http://www.ppgia.pucpr.br/?silla/softwares/yasd.zip.516Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectiontence boundary disambiguation using different sources of information and evaluatesthem on the Brown corpus, on which Punkt achieved an error rate of 1.02%.
The firstapproach described is a simple regular expressions approach that tries to recognizeabbreviations by matching against the following patterns (Grefenstette 1999, page 127): a single capital followed by a period, such as ?A.
?, ?B.,?
and ?C.?
; a sequence of letter-period-letter-period?s, such as ?U.S.
?, ?i.e.,?and ?m.p.h.?
; a capital letter followed by a sequence of consonants followed bya period, such as ?Mr.
?, ?St.,?
and ?Assn.
?.This approach produces a high error rate of 2.34%.
Moreover, it makes language-specificassumptions in that strings such as Mr might well be valid ordinary words in otherlanguages.
Grefenstette considers a second approach in which he additionally triesto identify abbreviations with a type-based heuristic, which inspired our type-basedbaseline (TypeBL) (Grefenstette 1999, pages 128 and 129):Let us define as a likely abbreviation any string of letters terminated by a period andfollowed by either a comma or semi-colon, a question mark, a lower-case letter, or anumber, or followed by a word beginning with a capital letter and ending in a period.[.
.
. ]
We can apply the corpus itself as a filter by eliminating from the list of likelyabbreviations those strings that appear without a terminal period in the corpus.The token-based and type-based heuristics combined produce an error rate of1.65%, which is still much higher than that of Punkt.
An alternative approach evaluatedby Grefenstette is to use a lexicon containing all ordinary words in the Brown corpus butno abbreviations or proper names in combination with the type-based heuristic for ab-breviation detection.
Even an approach with this massive amount of lexical knowledgestill produces an error rate that is higher than that achieved by Punkt: 1.73% versus1.02%.
Only when he uses the complete lexicon and a list of common abbreviations,is Grefenstette (1999) able to attain a result that is slightly better than ours: His mostresource intensive system achieves an error rate of 0.93%.These results indicate that Punkt can keep up well with rule-based systems evenwhen tested on the specific language and text type that they have been developed for.Whereas rule-based systems either require extensive lexical resources or a large amountof manual labor, Punkt can be applied to new languages and corpora out of the boxwith no manual adaptation.
The fact that it can recognize new abbreviations on the flyis especially a great advantage because the results of the RE system on the Lacio-Webcorpus and our own experiments in Section 6.4.4 show that rule or list-based systemsare often not sufficient to cover the productivity of abbreviation use in new corporaand languages; compare also Mikheev (2002, pages 298, 299, and 311) and Silla andKaestner (2004).7.2 Supervised Machine-Learning SystemsIn this section, we discuss several supervised machine-learning approaches to sen-tence boundary detection described in the literature and compare their results to those517Computational Linguistics Volume 32, Number 4achieved by Punkt.
We regard those sentence boundary systems as supervised thatrequire a set of manually disambiguated instances as training data.7.2.1 Riley (1989).
Riley induces a decision tree for sentence boundary detection usingthe following features (Riley 1989, pages 351 and 352): Prob[word with ?.?
occurs at end of sentence] Prob[word after ?.?
occurs at beginning of sentence] Length of word with ?.
? Length of word after ?.
? Case of word with ?.?
: Upper, Lower, Cap, Numbers Case of word after ?.?
: Upper, Lower, Cap, Numbers Punctuation after ?.?
(if any) Abbreviation class of word with ?.??
e.g., month name, unit-of-measure, title, address name, etc.The resulting decision tree is able to classify the periods in the Brown corpus with avery low error rate of only 0.2%, which is 0.82% better than that achieved by Punkt.However, the impressive performance of Riley?s approach also requires impressiveamounts of training data: He calculated the probabilities that a certain word occursbefore or after a sentence boundary from 25 million words of AP newswire text.
Sucha large training corpus is probably not available for many languages; see also thecomments in Palmer and Hearst (1997, page 245).
Moreover, the last of Riley?s features,namely, abbreviation class, requires quite specific lexical knowledge about abbreviationtypes that can only be taken from additional handcrafted resources.
It is unclear howwell his approach would do with a realistic amount of training data and without thesespecific lexical resources.7.2.2 The Satz System by Palmer and Hearst (1997).
The Satz system11 uses estimatesof the part-of-speech distribution of the words surrounding potential end-of-sentencepunctuation marks as input to a machine-learning algorithm.
The part-of-speech infor-mation is derived from a lexicon that contains part-of-speech frequency data.
In casea word is not in the lexicon, a part-of-speech distribution is estimated by differentguessing heuristics.
In addition, Satz also uses an abbreviation list and capitalizationinformation.
After training the system on a small training and a small cross-validationcorpus, which consist of documents with sentence boundaries annotated by hand, itcan be used on new documents to detect sentence boundaries.
The system can workwith any kind of machine-learning approach in principle.
Palmer and Hearst?s originalresults were obtained using neural networks and decision trees.We have used the same portion of the Wall Street Journal for evaluation in thepreceding sections as Palmer and Hearst (1997) so that a direct comparison betweenthe systems is possible.
Palmer and Hearst report an error rate of 1.5% for the initialversion of their system, which uses neural nets as machine-learning algorithm, a lexicon11 Available from: http://elib.cs.berkeley.edu/src/satz/.518Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectionof 30,000 words including an abbreviation list comprising 206 items, and was trained ona training set of 573 cases and a cross-validation set of 258 cases.
This result is onlyslightly better than the error rate of 1.65% that we obtained with Punkt.
Their bestresult was produced by a version of their system that used decision tree induction, alexicon of 5,000 words including the 206 abbreviations, and was trained on a set of6,373 hand-annotated items.
This configuration achieved an error rate of 1.0% on thesame test corpus.
Palmer and Hearst have also evaluated their system on one Frenchcorpus and two German corpora.
For the French corpus, they report an error rate of0.4%.
On the two German corpora, their system produced error rates of 1.3% and 0.5%,respectively.
Whereas the results of the Satz system for French are better than those forour system, their results on the two German corpora are worse than ours.
However, aswe were not able to use the same test corpora in our evaluation, these results are notdirectly comparable.
Strunk, Silla, and Kaestner (2006) give the results of a comparativeevaluation of the present system against three other approaches, including Satz, onEnglish and Brazilian Portuguese corpora.
For both of these corpora, Satz achieved aslightly lower error rate than our system; compare Table 22.While it is true that Satz performs somewhat better than Punkt in general, this isonly the case if enough training data and additional resources are available.
When itwas not provided with a precompiled list of abbreviations, for example, it producedan error rate as high as 4.9% on the WSJ corpus (Palmer and Hearst 1997, page 255).This result combined with the results from Section 6.4.4, which showed that general-purpose abbreviation lists are not sufficient due to the productivity of abbreviation use,suggests that a reliable performance of the Satz system on new corpora can only beensured if it is provided with an abbreviation list suitable for the domain in questionand it is ideally trained on documents of the same genre as the corpus it will be testedon.
Moreover, Palmer and Hearst report that Satz possesses a similar robustness withregard to single-case corpora as our system.
However, this is again only the case if it hasbeen retrained specifically on single-case corpora (Palmer and Hearst 1997, pages 255,256, 259).7.2.3 MxTerminator.
Reynar and Ratnaparkhi (1997) use maximum-entropy modelingto learn contextual features from a hand-annotated training corpus that can be used toidentify sentence boundaries.
Their system, called MxTerminator,12 employs featuressuch as the token preceding a potential sentence boundary, the token following it,capitalization information about these tokens, whether one or both of them are abbrevi-ations or not, and so on in its most portable version.
It also induces a list of abbreviationsfrom the training corpus by considering as an abbreviation every token in the trainingcorpus that contains a possible end-of-sentence symbol but does not indicate a sentenceboundary.
This portable version does not depend on any lexical resources such asthe part-of-speech information required by Satz.
Reynar and Ratnaparkhi also built aversion specialized for English newspaper text, which makes use of additional hand-crafted resources: a list of honorific abbreviations such as Ms. and Dr. and a list ofcorporate-designating abbreviations such as Corp. and S.p.A.
This specialized systemachieved an error rate of 1.2% on the English WSJ test corpus also used by Palmer andHearst and in our work.
While this result is clearly better than that of our system, whichproduced an error rate of 1.65%, MxTerminator had to be trained on 39,441 sentences ofWSJ text and used hand-crafted lexical resources.
For the more portable version of their12 Available from: ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz.519Computational Linguistics Volume 32, Number 4system without language-specific abbreviation lists, Reynar and Ratnaparkhi report anerror rate of 2.0% on the same text corpus, an error rate that is higher than that achievedby our system without training or lexical resources.We have evaluated the portable version of MxTerminator on our eleven newspapercorpora using ten-fold cross-validation with nine tenths of the corpus as training set andone tenth as test set.
Table 21 gives the average number of training cases and the meanerror rates for MxTerminator on each corpus and compares them to those achieved byPunkt.
Although MxTerminator achieves a slightly lower error rate on two corpora,namely Brazilian Portuguese and English, it produces an average error rate of 1.77% onthe newspaper corpora, which lies well above our system?s mean error rate of 1.26%,even though the number of training instances was sometimes as high as 34,256 (forGerman) and never fell below 10,000.
MxTerminator?s performance was also worse thanthat of our system in a comparative evaluation on English and Brazilian Portuguesecorpora described in Strunk, Silla, and Kaestner (2006).MxTerminator also shows that one cannot in general expect that the performanceof a machine-learning system carries over to new corpora written in the same languagewithout retraining.
When Reynar and Ratnaparkhi evaluated their system on the Browncorpus without retraining, it achieved a relatively high error rate of 2.1% (cf.
Punkt?serror rate of 1.02%).
The authors themselves remark:We present the Brown corpus performance to show the importance of training on thegenre of text on which testing will be performed.This points to the general problem that supervised systems that are not able todynamically incorporate new knowledge, for example, by discovering abbrevia-tion types on the fly, cannot be expected to perform reliably on new corpora ifthe specific domain or genre is not known beforehand; compare also Mikheev (2002,pages 298, 314).7.2.4 Stamatatos, Fakotakis, and Kokkinakis (1999).
These authors apply a specificallyadapted version of transformation-based learning (Brill 1995) to the problem of sentenceboundary detection.
For the initial-state annotation, they assume that every possiblesentence-ending punctuation mark does indeed indicate a sentence boundary.
In thefirst learning stage, their system uses characteristics such as identity, length, capital-Table 21Comparison between MxTerminator (Reynar and Ratnaparkhi 1997) and the Punkt System.Error <S>Corpus Cases MxTerm.
Punkt Corpus Cases MxTerm.
Punkt(%) (%) (%) (%)B.
Port.
13,725 1.10 1.11 Italian 10,405 2.45 1.13Dutch 18,068 1.13 0.97 Norwegian 25,531 1.34 0.81English 24,282 1.53 1.65 Spanish 11,714 1.60 1.06Estonian 23,243 2.79 2.12 Swedish 17,752 2.39 1.76French 11,601 2.66 1.54 Turkish 18,942 1.77 1.31German 34,256 0.63 0.35 Mean Error MxTerm.
: 1.76%, Punkt: 1.26%520Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionTable 22Direct comparison between Punkt and other systems for sentence boundary detection.F Measure Error <S>System Tipster?WSJ (%) Lacio Web (%) Brown (%) WSJ (%)Punkt 91.51 97.22 1.02 1.65RE 91.78 89.87 ?
?Grefenstette & Tapanainen ?
?
0.93 ?Riley ?
?
0.20 ?Satz 91.88 99.16 ?
1.00MxTerminator ?
?
2.10 1.20MxTerminator (portable) 91.22 96.46 2.50 2.00Mikheev ?
?
0.28 0.45Mikheev (tagger only) ?
?
0.98 1.95Mikheev (+ tagger) ?
?
0.20 0.31ization of the token containing a possible end-of-sentence boundary marker and ofthe token immediately following it as possible triggering environments to learn rulesthat remove sentence boundaries.
In the second stage, it learns rules that reinsert sen-tence boundaries using the same space of possible triggering environments.
Stamatatos,Fakotakis, and Kokkinakis (1999) trained their system on a hand-annotated corpusof Greek newspaper articles that contained 9,136 candidate punctuation marks.
Theytested it on a corpus containing articles from the same newspaper with 10,977 candidatepunctuation marks and a lower bound of 79.6%.
Their system learned 312 rules inall and produced an error rate of 0.6% on all punctuation marks, including periods,exclamation and question marks, and ellipses.
When only periods and ellipses wereconsidered, it achieved an error rate of 0.57% with a set of 234 rules.
Punkt achieved arespectable error rate of 1.50% on this corpus without any training at all.
As Stamatatos,Fakotakis, and Kokkinakis (1999) use individual words and brittle features such ascapitalization information as triggering environments, their system is probably not veryrobust in that it requires training on a corpus that is very close to the texts that the systemis intended to be used on in order to guarantee a low error rate.7.3 Unsupervised Machine-Learning SystemsUnsupervised systems are systems that neither require specific hand-written rulesor lexical resources nor have to be trained on hand-annotated training examples.Instead, they extract the required information from the test corpus itself and/oradditional unannotated text.
We also regard our own approach as an unsupervisedsystem.7.3.1 Mikheev (2002).
Mikheev proposes to combine sentence boundary detection,proper name identification, and abbreviation detection in one system.
He tackles thesentence boundary disambiguation task with a set of simple rules that can be appliedafter the tokens immediately to the left and to the right of a potential sentence boundarymarker have been fully disambiguated.
The most important questions are whether thetoken preceding a period is an abbreviation and whether the token following a period521Computational Linguistics Volume 32, Number 4is a proper name.
In order to answer these questions, he uses a combination of type-based abbreviation-guessing heuristics, some of which have already been discussedin Grefenstette (1999), and what he calls the document-centered approach to abbrevi-ation detection and proper name identification.
This approach is based on the idea ofclassifying a candidate type as a whole as a proper name or an abbreviation based oninstances of that type that occur in unambiguous contexts: For example, a type thatalways appears with a final period and occurs before a lowercase word is likely to bean abbreviation, while a type that occurs with an uppercase first letter in the middle ofa sentence is likely to be a proper name.
He enhances these methods with the abilityto distinguish between homographs (between an abbreviation and an ordinary wordand between a proper name and an ordinary word) by collecting common sequences ofmore than one token that contain the candidate.
His system requires some additionalresources, namely, a list of common words (i.e., not proper names), a list of commonwords that are frequent sentence starters, a list of frequent proper names that coincidewith common words, and a domain-specific list of abbreviations, which can all becreated from an unannotated corpus without human intervention.Mikheev evaluated his system on the WSJ corpus and the Brown corpus of Amer-ican English, extracting the required additional resources from a 300,000 word corpusof New York Times articles.
It achieved an error rate of 0.45% on the WSJ corpus and anerror rate of 0.28% on the Brown corpus, while Punkt?s error rates on these corpora were1.65% and 1.02%, respectively.13 Mikheev himself admits that the use of an additionaldomain-specific abbreviation list, which has to be recreated for every new domain, isnot always possible, especially if the system is expected ?to handle documents fromunknown origin?
(Mikheev 2002, pages 305, 306).
When his system was not equippedwith an additional abbreviation list, the error rates rose to 1.41% on the WSJ corpusand 0.65% on the Brown corpus and were more comparable with those achieved byPunkt.
Mikheev also tested his system in conjunction with a morphological analyzeron a corpus of BBC articles in Russian and obtained an error rate of 0.1% for sentenceboundary detection.While Mikheev?s system and Punkt are quite similar in spirit, his system usesadvanced methods for the identification of proper names, while we have mostly con-centrated on abbreviation detection.
In fact, combining Mikheev?s insights with ourmethods of abbreviation detection would most likely lead to another performanceincrease: Mikheev?s abbreviation detection methods achieved an error rate of 6.6% onthe WSJ and an error rate of 8.9% on the Brown corpus for the task of abbreviationdetection when no additional abbreviation list was used.
Even when such a list wasconsulted, his error rates?0.8% on the WSJ and 1.2% on the Brown corpus?remainedabove those achieved by Punkt?0.71% and 0.82%, respectively.Abbreviation detection is also the area where we have spotted some critical issuesin Mikheev?s approach.
Although Mikheev aims at a domain-independent system, hemakes some decisions that could be harmful for domains other than news and literaryfiction.
He places an arbitrary length limit on possible abbreviations by applying hisdocument-centered approach to abbreviation detection only to candidates that have amaximal length of four letters (Mikheev 2002, page 299).
This limit is already too strictfor some abbreviations found in English newspaper corpora, such as Messrs., Calif., andThurs.
The abbreviation Calif.(ornia), for instance, occurs 88 times in the portion of the13 However, Mikheev seems to have tested on the whole WSJ portion of the Penn Treebank, while we haveused only Sections 03-06.522Kiss and Strunk Unsupervised Multilingual Sentence Boundary DetectionWSJ corpus we used in our evaluation.
The abbreviation lists that we have extractedfrom a German dictionary and a bilingual English?German dictionary (cf.
Section 6.4.4)show that such an arbitrary length limit can be quite problematic.
In the German list,28% of all abbreviation types have a length greater than four.
In the English list, 26% arelonger than four characters.Moreover, Mikheev?s document-centered approach is mostly based on capitaliza-tion information and therefore unable to uncover abbreviations that are always followedby a capitalized word, while our approach uses collocational information pertainingto the abbreviation itself and its final period and thus does not incur this problem.Mikheev?s strong reliance on capitalization also renders his approach unsuitable forsingle-case text and leads to some problems for German where all nouns are alwayscapitalized and not only proper names (Mikheev 2002, page 315).
Last but not least,Mikheev?s system does not include a specialized treatment of ordinal numbers, whichwe have shown to be quite important for some languages; compare Section 6.4.5.Mikheev (2000, 2002) describes the combination of a trigram tagger with hisdocument-centered approach to abbreviation detection and proper name identification.After bootstrapping the training process on 20,000 words of tagged text, he trained thetagger in the unsupervised mode on the Brown corpus and evaluated it on the WSJcorpus and vice versa.
When he used the tagger alone for the disambiguation of possibleend-of-sentence marks, it achieved an error rate of 1.95% on the WSJ corpus (vs. 1.65%achieved by Punkt) and an error rate of 0.98% on the Brown corpus (vs. 1.02% achievedby our system).
Enhancing the tagger with the heuristics to identify proper names andabbreviations described above improved the error rates to 0.31% on the WSJ corpus and0.20% on the Brown corpus.
When the heuristics were applied only to the test corporathemselves and no additional abbreviation list was employed, the resulting error rateswere 1.39% on the WSJ corpus and 0.65% on the Brown corpus.
When comparingthese results achieved by using a part-of-speech tagger with those of our system, ithas to be kept in mind that although Mikheev trained his tagger in the unsupervisedmode, this technique normally still requires an extensive lexicon that contains thepossible parts of speech for each lexical item, a resource that is not always available forevery language.8.
ConclusionWe have presented an unsupervised multilingual sentence boundary detection systemthat does not depend on any additional resources besides the corpus it is supposed tosegment into sentences.
It uses collocational information as a new type of evidence forthe detection of abbreviations, initials, and ordinal numbers and is therefore much lessdependent on orthographic information than competing systems.
This fact enables itto accurately detect sentence boundaries even for single-case corpora.
The experimentsthat we have carried out for eleven languages show that it is an accurate method wellsuited for different languages and text genres.
Although its performance is slightlyinferior to the best results published in the literature on sentence boundary detection(cf.
Table 22), it can keep up well with rule-based methods and more straightforwardsupervised systems.
Moreover, we were able to show that abbreviation use is quiteproductive and that systems that rely on resources such as abbreviation lists willhave to be adapted specifically to every new language and every new domain, whileno manual work and no language-specific resources are needed to adapt Punkt tonew corpora.523Computational Linguistics Volume 32, Number 4AcknowledgmentsWe would like to thank Antti Arppe, J?rgAsmussen, Marti Hearst, Knut Hofland,Heiki-Jaan Kaalep, Celso Kaestner, CristinaMota, Umut O?zge, Bilge Say, Carlos Silla Jr.,Efstathios Stamatatos, and particularly KatjaKe?elmeier, Anneli von Ko?nemann, andKays Mutlu for their invaluable assistance inthe construction of our test corpora.
We aremuch obliged to the Gesellschaft derFreunde der Ruhr-Universita?t e.V.
forfinancial support.
We are also grateful tothree anonymous reviewers who gave usvery helpful comments and suggestions.ReferencesAluisio, Sandra M., Gisele M. Pinheiro,Marcelo Finger, Maria das Grac?as V.Nunes, and Stella E. O. Tagnin.
2003.
TheLacio-Web Project: Overview and issuesin Brazilian Portuguese corpora creation.In Proceedings of Corpus Linguistics,pages 14?21, Lancaster, UK.Brill, Eric.
1995.
Transformation-basederror-driven learning and naturallanguage processing: A case study inpart-of-speech tagging.
ComputationalLinguistics, 21(4):543?565.Dudenredaktion, editor.
2004.
Duden.
Diedeutsche Rechtschreibung (23rd ed.
),volume 1 of Der Duden in zwo?lf Ba?nden.Dudenverlag, Mannheim.Dunning, Ted.
1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19(1):61?74.Evert, Stefan and Brigitte Krenn.
2001.Methods for the qualitative evaluationof lexical association measures.
InProceedings of the 39th Annual Meeting of theAssociation for Computational Linguistics,pages 188?195, Toulouse, France.Firth, John R. 1957.
A synopsis of linguistictheory 1930-55.
In Studies in LinguisticAnalysis (special volume of the PhilologicalSociety).
The Philological Society, Oxford,pages 1?32.Francis, Nelson W. and Henry Kucera.
1982.Frequency Analysis of English Usage: Lexiconand Grammar.
Houghton Mifflin,New York.Grefenstette, Gregory.
1999.
Tokenization.In Hans van Halteren, editor, SyntacticWordclass Tagging.
Kluwer AcademicPublishers, Dordrecht, pages 117?133.Grefenstette, Gregory and Pasi Tapanainen.1994.
What is a word, what is a sentence?Problems of tokenization.
In Proceedingsof the 3rd International Conference onComputational Lexicography, pages 79?87,Budapest, Hungary.Kiss, Tibor and Jan Strunk.
2002a.
Scaledlog likelihood ratios for the detectionof abbreviations in text corpora.In Proceedings of COLING 2002,pages 1228?1232, Taipei, Taiwan.Kiss, Tibor and Jan Strunk.
2002b.
Viewingsentence boundary detection as collocationidentification.
In Proceedings of KONVENS2002, pages 75?82, Saarbru?cken, Germany.Manning, Christopher D. and HinrichSchu?tze.
1999.
Foundations of StatisticalNatural Language Processing.
The MITPress, Cambridge, MA.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Mikheev, Andrei.
2000.
Tagging sentenceboundaries.
In Proceedings ofANLP-NAACL 2000, pages 264?271,Seattle, Washington.Mikheev, Andrei.
2002.
Periods, capitalizedwords, etc.
Computational Linguistics,28(3):289?318.Mu?ller, Hans, V. Amerl, and G. Natalis.
1980.Worterkennungsverfahren als Grundlageeiner Universalmethode zurautomatischen Segmentierung von Textenin Sa?tze.
Ein Verfahren zur maschinellenSatzgrenzenbestimmung im Englischen.Sprache und Datenverarbeitung, 4(1):46?64.Nunberg, Geoffrey.
1990.
The Linguistics ofPunctuation, volume 18 of CSLI LectureNotes.
CSLI Publications, Stanford, CA.Palmer, David D. and Marti A. Hearst.
1997.Adaptive multilingual sentence boundarydisambiguation.
Computational Linguistics,23(2):241?267.Reynar, Jeffrey C. and Adwait Ratnaparkhi.1997.
A maximum entropy approach toidentifying sentence boundaries.
InProceedings of the Fifth ACL Conferenceon Applied Natural Language Processing,pages 16?19, Washington, DC.Riley, Michael D. 1989.
Some applications oftree-based modeling to speech andlanguage indexing.
In Proceedings of theDARPA Speech and Natural LanguageWorkshop, pages 339?352, Cape Cod, MA.Silla Jr., Carlos N. and Celso A.
A. Kaestner.2004.
An analysis of sentence boundarydetection systems for English andPortuguese documents.
In Proceedings ofCICLing 2004, pages 135?141, Seoul, Korea.Silla Jr., Carlos N., Jaime Dalla Valle Jr., andCelso A.
A. Kaestner.
2003.
Detecc?a?o524Kiss and Strunk Unsupervised Multilingual Sentence Boundary Detectionautoma?tica de sentenc?as com o uso deexpresso?es regulares.
In Proceedings ofCBComp 2003, pages 548?560, Itaja?
?, Brazil.Stamatatos, Efstathios, Nikos Fakotakis, andGeorge K. Kokkinakis.
1999.
Automaticextraction of rules for sentence boundarydisambiguation.
In Proceedings of theWorkshop on Machine Learning in HumanLanguage Technology, pages 88?92,Chania, Greece.Strunk, Jan, Carlos N. Silla Jr., and CelsoA.
A. Kaestner.
2006.
A comparativeevaluation of a new unsupervisedsentence boundary detection approach ondocuments in English and Portuguese.
InProceedings of CICLing 2006, pages 132?143,Mexico City, Mexico.van Rijsbergen, Cornelis J.
1979.
InformationRetrieval.
Butterworths, London.Walker, Daniel J., David E. Clements, MakiDarwin, and Jan W. Amtrup.
2001.Sentence boundary detection: Acomparison of paradigms for improvingMT quality.
In Proceedings of the MT SummitVIII, Santiago de Compostela, Spain.Willmann, Helmut and Heinz Messinger,editors.
1996.
LangenscheidtsGro?wo?rterbuch Englisch.
Der Kleine Muret-Sanders (7th ed.
), volume I: English?German.
Langenscheidt, Berlin/Munich.525
