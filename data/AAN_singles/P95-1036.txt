Some Novel Applications of Explanation-Based Learning toParsing Lexicalized Tree-Adjoining Grammars"B. Sr in ivas  and Arav ind  K. JoshiDepar tment  of Computer  and In format ion  ScienceUnivers i ty of Pennsy lvaniaPhi ladelphia,  PA 19104, USA{srini, joshi} @linc.cis.upenn.eduAbst rac tIn this paper we present some novel ap-plications of Explanation-Based Learning(EBL) technique to parsing LexicalizedTree-Adjoining rammars.
The novel as-pects are (a) immediate generalization ofparses in the training set, (b) generaliza-tion over recursive structures and (c) rep-resentation of generalized parses as FiniteState Transducers.
A highly impoverishedparser called a "stapler" has also been in-troduced.
We present experimental resultsusing EBL for different corpora and archi-tectures to show the effectiveness of our ap-proach.1 In t roduct ionIn this paper we present some novel applications ofthe so-called Explanation-Based Learning technique(EBL) to parsing Lexicalized Tree-Adjoining ram-mars (LTAG).
EBL techniques were originally intro-duced in the AI literature by (Mitchell et al, 1986;Minton, 1988; van Harmelen and Bundy, 1988).
Themain idea of EBL is to keep track of problems olvedin the past and to replay those solutions to solvenew but somewhat similar problems in the future.Although put in these general terms the approachsounds attractive, it is by no means clear that EBLwill actually improve the performance of the systemusing it, an aspect which is of great interest o ushere.Rayner (1988) was the first to investigate thistechnique in the context of natural anguage pars-ing.
Seen as an EBL problem, the parse of a sin-gle sentence represents an explanation of why thesentence is a part of the language defined by thegrammar.
Parsing new sentences amounts to find-ing analogous explanations from the training sen-tences.
As a special case of EBL, Samuelsson and*This work was partiaJly supported by ARC) grantDAAL03-89-0031, ARPA grant N00014-90-J-1863, NSFSTC grsmt DIR-8920230, and Ben Franklin PartnershipProgram (PA) gremt 93S.3078C-6Rayner (1991) specialize a grammar for the ATISdomain by storing chunks of the parse trees presentin a treebank of parsed examples.
The idea is toreparse the training examples by letting the parsetree drive the rule expansion process and halting theexpansion of a specialized rule if the current nodemeets a 'tree-cutting' criteria.
However, the prob-lem of specifying an optimal 'tree-cutting' criteriawas not addressed in this work.
Samuelsson (1994)used the information-theoretic measure of entropy toderive the appropriate sized tree chunks automati-cally.
Neumann (1994) also attempts to specializea grammar given a training corpus of parsed exam-pies by generalizing the parse for each sentence andstoring the generalized phrasal derivations under asuitable index.Although our work can be considered to be inthis general direction, it is distinct in that it ex-ploits some of the key properties of LTAG to (a)achieve an immediate generalization f parses in thetraining set of sentences, (b) achieve an additionallevel of generalization f the parses in the trainingset, thereby dealing with test sentences which arenot necessarily of the same length as the trainingsentences and (c) represent the set of generalizedparses as a finite state transducer (FST), which isthe first such use of FST in the context of EBL, tothe best of our knowledge.
Later in the paper, wewill make some additional comments on the relation-ship between our approach and some of the earlierapproaches.In addition to these special aspects of our work,we will present experimental results evaluating theeffectiveness of our approach on more than one kindof corpus.
We also introduce a device called a "sta-pler", a considerably impoverished parser, whoseonly job is to do term unification and compute alter-nate attachments for modifiers.
We achieve substan-tial speed-up by the use of "stapler" in combinationwith the output of the FST.The paper is organized as follows.
In Section 2we provide a brief introduction to LTAG with thehelp of an example.
In Section 3 we discuss ourapproach to using EBL and the advantages provided268(a) (b)Figure 1: Substitution and Adjunction in LTAG~ t~~ b U Wby LTAG.
The FST representation used for EBL isillustrated in Section 4.
In Section 5 we present he"stapler" in some detail.
The results of some of theexperiments based on our approach are presentedin Section 6.
In Section 7 we discuss the relevanceof our approach to other lexicalized grammars.
InSection 8 we conclude with some directions for futurework.2 Lexicalized Tree-AdjoiningGrammarLexicalized Tree-Adjoining Grammar (LTAG) (Sch-abes et al, 1988; Schabes, 1990) consists of ELE-MENTARY TREES, with each elementary tree hav-ing a lexical item (anchor) on its frontier.
An el-ementary tree serves as a complex description ofthe anchor and provides a domain of locality overwhich the anchor can specify syntactic and semantic(predicate-argument) constraints.
Elementary treesare of two kinds - (a) INITIAL TREES and (b) AUX-ILIARY TREES.Nodes on the frontier of initial trees are markedas substitution sites by a '~'.
Exactly one node onthe frontier of an auxiliary tree, whose label matchesthe label of the root of the tree, is marked as a footnode by a ' . '
;  the other nodes on the frontier of anauxiliary tree are marked as substitution sites.
El-ementary trees are combined by Subst i tu t ion  andAd junct ion  operations.Each node of an elementary tree is associated withthe top and the bottom feature structures (FS).
Thebottom FS contains information relating to the sub-tree rooted at the node, and the top FS containsinformation relating to the supertree at that node.
1The features may get their values from three differ-ent sources such as the morphology of anchor, thestructure of the tree itself, or by unification duringthe derivation process.
FS are manipulated by sub-stitution and adjunction as shown in Figure 1.The initial trees (as) and auxiliary trees (/3s) forthe sentence show me the flights from Boston toPhiladelphia re shown in Figure 2.
Due to the lim-ited space, we have shown only the features on the a ltree.
The result of combining the elementary trees1Nodes marked for substitution are associated withonly the top FS.shown in Figure 2 is the der ived  t ree,  shown in Fig-ure 2(a).
The process of combining the elementarytrees to yield a parse of the sentence is representedby the der ivat ion  t ree,  shown in Figure 2(b).
Thenodes of the derivation tree are the tree names thatare anchored by the appropriate lexical items.
Thecombining operation is indicated by the nature ofthe arcs-broken line for substitution and bold linefor adjunction-while the address of the operation isindicated as part of the node label.
The derivationtree can also be interpreted as a dependency tree 2with unlabeled arcs between words of the sentenceas shown in Figure 2(c).Elementary trees of LTAG are the domains forspecifying dependencies.
Recursive structures arespecified via the auxiliary trees.
The three aspectsof LTAG - (a) lexicalization, (b)-extended domain oflocality and (c) factoring of recursion, provide a nat-ural means for generalization during the EBL pro-ce88.3 Overview of our approach to usingEBLWe are pursuing the EBL approach in the contextof a wide-coverage grammar development systemcalled XTAG (Doran et al, 1994).
The XTAG sys-tem consists of a morphological nalyzer, a part-of-speech tagger, a wide-coverage LTAG English gram-mar, a predictive left-to-right Early-style parser forLTAG (Schabes, 1990) and an X-windows interfacefor grammar development (Paroubek et al, 1992).Figure 3 shows a flowchart of the XTAG system.The input sentence is subjected to morphologicalanalysis and is parts-of-speech tagged before beingsent to the parser.
The parser retrieves the elemen-tary trees that the words of the sentence anchor andcombines them by adjunction and substitution op-erations to derive a parse of the sentence.Given this context, the training phase of the EBLprocess involves generalizing the derivation treesgenerated by XTAG for a training sentence and stor-ing these generalized parses in the generalized parse2There axe some differences between derivation treesand conventional dependency trees.
However we will notdiscuss these differences in this paper as they are notrelevant to the present work.269I, rlI ?
~ .
.u .
, , , ( \ ]  ,,,,,(-.,-1 ~.~, - \ ]Idm~NIPINI14IDIekeC~3NIPi)elP ~ NII$&eld~~4NP r~t*  Pr  A IP NP~, NI IJ~l ~5AI~ NPI,INI~6f r?
?
~ lq rN le f  I~me llrlr ~ ?
I fle ?
I f  D I?I I I I Ip ~ ~ N - - .
.
.
.uI I(a)al \[daow\]~Z \[reel (2.2) ~ (n~td  (~L.~) .a3\ [~\ ]  O) ?~ ( t ry \ ]  (0) p2 (to\] (e) tb, t r~ to' ' I I !
!a5 (~o1 (2.2) ~ (l~niladdp~Ja\] (2.2) ~ Pbl~ael:(b) (c)Figure 2: (as and/~s) Elementary trees, (a) Derived Tree, (b) Derivation Tree, and (c) Dependency tree forthe sentence: show me the flights from Boston to Philadelphia.270Input Segtcncett iJ L -I P.O.SBb~ 11, ,Tree ,?peb?tionDerivation Structm~Figure 3: Flowchart of the XTAG systemIwa l fag~- - ?
~ - = o. .
.
.
.
.
.
o. .
.
.
.
.
.
~ .
.
.
.
.
.
JFigure 4: Flowchart of the XTAG system withthe EBL componentdatabase under an index computed from the mor-phological features of the sentence.
The applicationphase of EBL is shown in the flowchart in Figure 4.An index using the morphological features of thewords in the input sentence is computed.
Using thisindex, a set of generalized parses is retrieved fromthe generalized parse database created in the train-ing phase.
If the retrieval fails to yield any gener-alized parse then the input sentence is parsed usingthe full parser.
However, if the retrieval succeedsthen the generalized parses are input to the "sta-pler".
Section 5 provides a description of the "sta-pler".3.1 Impl icat ions of LTAG representat ionfor EBLAn LTAG parse of a sentence can be seen as a se-quence of elementary trees associated with the lexi-cal items of the sentence along with substitution andadjunction links among the elementary trees.
Also,the feature values in the feature structures of eachnode of every elementary tree are instantiated by theparsing process.
Given an LTAG parse, the general-ization of the parse is truly immediate in that a gen-eralized parse is obtained by (a) uninstantiating theparticular lexical items that anchor the individual el-ementary trees in the parse and (h) uninstantiatingthe feature values contributed by the morphology ofthe anchor and the derivation process.
This type ofgeneralization is called feature-generalization.In other EBL approaches (Rayner, 1988; Neu-mann, 1994; Samuelsson, 1994) it is necessary towalk up and down the parse tree to determine theappropriate subtrees to generalize on and to sup-press the feature values.
In our approach, the pro-cess of generalization is immediate, once we have theoutput of the parser, since the elementary trees an-chored by the words of the sentence define the sub-trees of the parse for generalization.
Replacing theelementary trees with unistantiated feature values isall that is needed to achieve this generalization.The generalized parse of a sentence is stored in-dexed on the part-of-speech (POS) sequence of thetraining sentence.
In the application phase, the POSsequence of the input sentence is used to retrieve ageneralized parse(s) which is then instantiated withthe features of the sentence.
This method of retriev-ing a generalized parse allows for parsing of sen-tences of the same lengths and the same POS se-quence as those in the training corpus.
However,in our approach there is another generalization thatfalls out of the LTAG representation which allows forflexible matching of the index to allow the system toparse sentences that are not necessarily of the samelength as any sentence in the training corpus.Auxiliary trees in LTAG represent recursive struc-tures.
So if there is an auxiliary tree that is used inan LTAG parse, then that tree with the trees forits arguments can be repeated any number of times,or possibly omitted altogether, to get parses of sen-tences that differ from the sentences of the trainingcorpus only in the number of modifiers.
This type ofgeneralization is called modifier-generalization.
Thistype of generalization is not possible in other EBLapproaches.This implies that the POS sequence covered bythe auxiliary tree and its arguments can be repeatedzero or more times.
As a result, the index of a gener-alized parse of a sentence with modifiers is no longera string but a regular expression pattern on the POSsequence and retrieval of a generalized parse involvesregular expression pattern matching on the indices.If, for example, the training example was(1) Show/V me/N the/D fiights/N from/PBoston/N to/P Philadelphia/N.then, the index of this sentence is(2) VNDN(PN)*since the two prepositions in the parse of this sen-tence would anchor (the same) auxiliary trees.271The most efficient method of performing regularexpression pattern matching is to construct a finitestate machine for each of the stored patterns andthen traverse the machine using the given test pat-tern.
If the machine reaches the final state, then thetest pattern matches one of the stored patterns.Given that the index of a test sentence matchesone of the indices from the training phase, the gen-eralized parse retrieved will be a parse of the testsentence, modulo the modifiers.
For example, if thetest sentence, tagged appropriately, is(3) Show/V me/S the/D flights/N from/PBoston/N to/P Philadelphia/N on/PMonday/N.then, Mthough the index of the test sentencematches the index of the training sentence, the gen-eralized parse retrieved needs to be augmented toaccommodate the additional modifier.To accommodate he additional modifiers thatmay be present in the test sentences, we need to pro-vide a mechanism that assigns the additional modi-fiers and their arguments he following:1.
The elementary trees that they anchor and2.
The substitution and adjunction links to thetrees they substitute or adjoin into.We assume that the additional modifiers alongwith their arguments would be assigned the sameelementary trees and the same substitution and ad-junction links as were assigned to the modifier andits arguments of the training example.
This, ofcourse, means that we may not get al the possi-ble attachments of the modifiers at this time.
(butsee the discussion of the "stapler" Section 5.
)4 FST  Representat ionThe representation in Figure 6 combines the gener-alized parse with the POS sequence (regular expres-sion) that it is indexed by.
The idea is to annotateeach of the finite state arcs of the regular expressionmatcher with the elementary tree associated withthat POS and also indicate which elementary tree itwould be adjoined or substituted into.
This resultsin a Finite State Transducer (FST) representation,illustrated by the example below.
Consider the sen-tence (4) with the derivation tree in Figure 5.
(4) show me the flights from Boston toPhiladelphia.An alternate representation f the derivation treethat is similar to the dependency representation,is to associate with each word a tuple (this_tree,head_word, head_tree, number).
The description ofthe tuple components is given in Table 1.Following this notation, the derivation tree in Fig-ure 5 (without he addresses of operations) is repre-sented as in (5).al \[d~ow\]oo'%%~2 \[me\] (2.~) a~ \[n~,ht~\] (Z3)as l t l~l (1) I~ \[frem\] (0) 1~2 \[to\] (0)Z I !
!a5 \[m~tou\] (2.2) ~ \[\]~t-&lpU~\] (2.2)Figure 5: Derivation Tree for the sentence: show methe flights from Boston  to Philadelphiathis_tree : the elementary tree that the wordanchorshead_word : the word on which the currentword is dependent on; "-" if thecurrent word does notdepend on any other word.head_tree : the tree anchored by the head word;"-" if the current word does notdepend on any other word.number : a signed number that indicates thedirection and the ordinal position ofthe particular head elementary treefrom the position of the currentword OR: an unsigned number that indicatesthe Gorn-address (i.e., the nodeaddress) in the derivation tree towhich the word attaches OR: "-" if the current word does notdepend on any other word.Table 1: Description of the tuple components(5)show/(al, -, -, -)the/(a3, flights, ~4,+1)from/(fll, flights, a4, 2)to/(fi2, flights,a4, 2)me/(a2, show,al,-l)fiights/ (a4,show , ~I , - I )Boston/(as, from, fll -1)Philadelphia/(as, to, f12,-1)Generalization f this derivation tree results in therepresentation in (6).
(6)- ,  - ,  - )D/(a3, N, a4,+l)(P/(fil, N, a4, 2)(P/(fl2, N, a4, 2)N/(a~, V,al,-1)N/(c~4,V, C~l,-1)N/(as, P, fl,-1))*N/(a6, P, fl,-1))*After generalization, the trees /h and f12 are nolonger distinct so we denote them by ft.
The treesa5 and a6 are also no longer distinct, so we denotethem by a.
With this change in notation, the twoKleene star regular expressions in (6) can be mergedinto one, and the resulting representation is (7)272v/(al,-,- ,-) N/(a2,v,a1,-t) I)/(%, l~.a 4 ,+t )  N/(a4,v, at,-1 ) P/( ~.N.a 4,2)~Y( a, P, ~, -t)Figure 6: Finite State Transducer Representation for the sentences: show me the flights from Boston toPhiladelphia, show me the flights from Boston to Philadelphia on Monday, .
.
.
(v)- ,  - ,  - )D/(as,  N, o~4,+1)(P/(3, N, o~4, 2)V,al,-1)N/(~4,V, ~1,-1)N/(a,  P, 3 , -1)  )*which can be seen as a path in an FST as in Figure 6.This FST representation is possible due to the lex-icalized nature of the elementary trees.
This repre-sentation makes a distinction between dependenciesbetween modifiers and complements.
The number inthe tuple associated with each word is a signed num-ber if a complement dependency is being expressedand is an unsigned number if a modifier dependencyis being expressed, s5 S tap lerIn this section, we introduce a device called "sta-pler", a very impoverished parser that takes as in-put the result of the EBL lookup and returns theparse(s) for the sentence.
The output of the EBLlookup is a sequence of elementary trees annotatedwith dependency links - an almost parse.
To con-struct a complete parse, the "stapler" performs thefollowing tasks:?
Identify the nature of link: The dependencylinks in the almost parse are to be distinguishedas either substitution links or adjunction links.This task is extremely straightforward since thetypes (initial or auxiliary) of the elementarytrees a dependency link connects identifies thenature of the link.?
Modifier Attachment: The EBL lookup is notguaranteed to output all possible modifier-head dependencies for a give input, sincethe modifier-generalization assigns the samemodifier-head link, as was in the training ex-ample, to all the additional modifiers.
So it isthe task of the stapler to compute all the alter-nate attachments for modifiers.?
Address of Operation: The substitution and ad-junction links are to be assigned a node ad-dress to indicate the location of the operation.The "staPler" assigns this using the structure of3In a complement auxiliary tree the anchor subcat-egorizes for the foot node, which is not the case for amodifier auxiliaxy tree.the elementary trees that the words anchor andtheir linear order in the sentence.Feature Instantiation: The values of the fea-tures on the nodes of the elementary trees areto be instantiated by a process of unification.Since the features in LTAGs are finite-valuedand only features within an elementary treecan be co-indexed, the "stapler" performs term-unification to instantiate the features.6 Exper iments  and  Resu l tsWe now present experimental results from two dif-ferent sets of experiments performed to show theeffectiveness of our approach.
The first set of ex-periments, (Experiments l(a) through 1(c)), are in-tended to measure the coverage of the FST represen-tation of the parses of sentences from a range of cor-pora (ATIS, IBM-Manual and Alvey).
The resultsof these experiments provide a measure of repeti-tiveness of patterns as described in this paper, atthe sentence level, in each of these corpora.Exper iment  l (a):  The details of the experimentwith the ATIS corpus are as follows.
A total of 465sentences, average length of 10 words per sentence,which had been completely parsed by the XTAG sys-tem were randomly divided into two sets, a train-ing set of 365 sentences and a test set of 100 sen-tences, using a random number generator.
For eachof the training sentences, the parses were ranked us-ing heuristics 4 (Srinivas et al, 1994) and the topthree derivations were generMized and stored as anFST.
The FST was tested for retrieval of a gener-alized parse for each of the test sentences that werepretagged with the correct POS sequence (In Ex-periment 2, we make use of the POS tagger to dothe tagging).
When a match is found, the outputof the EBL component is a generalized parse thatassociates with each word the elementary tree thatit anchors and the elementary tree into which it ad-joins or substitutes into - an almost parse, s4We axe not using stochastic LTAGs.
For work onStochastic LTAGs see (Resnik, 1992; Schabes, 1992).SSee (Joshi and Srinivas, 1994) for the role of almostparse in supertag disaanbiguation.273CorpusATISIBMAlveySize of # of states % Coverage Response TimeTraining set (sees)365 6000 80% 1.00 see/sent1100 21000 40% 4.00 sec/sent80 500 50% 0.20 sec/NPTable 2: Coverage and Retrieval times for various corporaExper iment  l (b )  and 1(c): Similar experimentswere conducted using the IBM-manual corpus and aset of noun definitions from the LDOCE dictionarythat were used as the Alvey test set (Carroll, 1993).Results of these experiments are summarized inTable 2.
The size of the FST obtained for each of thecorpora, the coverage of the FST and the traversaltime per input are shown in this table.
The cover-age of the FST is the number of inputs that were as-signed a correct generalized parse among the parsesretrieved by traversing the FST.Since these experiments measure the performanceof the EBL component on various corpora we willrefer to these results as the 'EBL-Lookup times'.The second set of experiments measure the perfor-mance improvement obtained by using EBL withinthe XTAG system on the ATIS corpus.
The per-formance was measured on the same set of 100 sen-tences that was used as test data in Experiment l(a).The FST constructed from the generalized parses ofthe 365 ATIS sentences used in experiment l(a) hasbeen used in this experiment as well.Exper iment  2(a): The performance of XTAG onthe 100 sentences i  shown in the first row of Table 3.The coverage represents he percentage ofsentencesthat were assigned a parse.Exper iment  2(b): This experiment is similar toExperiment l(a).
It attempts to measure the cov-erage and response times for retrieving a general-ized parse from the FST.
The results are shown inthe second row of Table 3.
The difference in theresponse times between this experiment and Exper-iment l(a) is due to the fact that we have includedhere the times for morphological nalysis and thePOS tagging of the test sentence.
As before, 80%of the sentences were assigned a generalized parse.However, the speedup when compared to the XTAGsystem is a factor of about 60.Exper iment  2(c): The setup for this experiment isshown in Figure 7.
The almost parse from the EBLlookup is input to the full parser of the XTAG sys-tem.
The full parser does not take advantage of thedependency information present in the almost parse,however it benefits from the elementary tree assign-ment to the words in it.
This information helps thefull parser, by reducing the ambiguity of assigninga correct elementary tree sequence for the words ofthe sentence.
The speed up shown in the third rowof Table 3 is entirely due to this ambiguity reduc-tion.
If the EBL lookup fails to retrieve a parse,which happens for 20% of the sentences, then thes .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
il~ .
ivs t tm l lmFigure 7: System Setup for Experiment 2(c).tree assignment ambiguity is not reduced and thefull parser parses with all the trees for the words ofthe sentence.
The drop in coverage is due to the factthat for 10% of the sentences, the generalized parseretrieved could not be instantiated to the features ofthe sentence.System Coverage % Average time(in  es)XTAG 100% 125.18EBL lookup 80% 1.78EBL+XTAG parser 90% 62.93EBL+Stapler 70% 8.00Table 3: Performance comparison of XTAG withand without EBL  componentExperiment 2(d): The setup for this experimentis shown in Figure 4.
In this experiment, the almostparse resulting from the EBL lookup is input to the"stapler" that generates all possible modifier attach-ments and performs term unification thus generatingall the derivation trees.
The "stapler" uses both theelementary tree assignment information and the de-pendency information present in the almost parseand speeds up the performance even further, by afactor of about 15 with further decrease in coverageby 10% due to the same reason as mentioned in Ex-periment 2(c).
However the coverage of this systemis limited by the coverage of the EBL lookup.
Theresults of this experiment are shown in the fourthrow of Table 3.2747 Re levance  to  o ther  lex ica l i zedgrammarsSome aspects of our approach can be extended toother lexicalized grammars,  in particular to catego-rial grammars (e.g.
Combinatory Categorial Gram-mar (CCG) (Steedman, 1987)).
Since in a categorialgrammar the category for a lexical item includes itsarguments, the process of generalization of the parsecan also be immediate in the same sense of our ap-proach.
The generalization over recursive structuresin a categorial grammar, however, will require fur-ther annotations of the proof trees in order to iden-tify the 'anchor' of a recursive structure.
If  a lexi-cal item corresponds to a potential recursive struc-ture then it will be necessary to encode this informa-tion by making the result part of the functor to beX --+ X.
Further annotation of the proof tree willbe required to keep track of dependencies in orderto represent the generalized parse as an FST.8 ConclusionIn this paper, we have presented some novel applica-tions of EBL technique to parsing LTAG.
We havealso introduced a highly impoverished parser calledthe "stapler" that in conjunction with the EBL re-suits in a speed up of a factor of about 15 over asystem without the EBL component.
To show theeffectiveness of our approach we have also discussedthe performance of EBL on different corpora, anddifferent architectures.As part of the future work we will extend our ap-proach to corpora with fewer repetitive sentence pat-terns.
We propose to do this by generalizing at thephrasal evel instead of at the sentence level.Re ferencesJohn Carroll.
1993.
Practical Unification-based Parsingof Natural Language.
University of Cambridge, Com-puter Laboratory, Cambridge, England.Christy Doran, DahLia Egedi, Beth Ann Hockey, B. Srini-vas, and Martin Zaidel.
1994.
XTAG System - A WideCoverage Grammar for English.
In Proceedings of the17 *h International Conference on Computational Lin-guistics (COLING '9~), Kyoto, Japan, August.Aravind K. Joshi and B. Srinivas.
1994.
Disambigu~-tion of Super Parts of Speech (or Supertags): AlmostParsing.
In Proceedings of the 17 th International Con-\]erence on Computational Linguistics (COLING '9~),Kyoto, Japan, August.Steve Minton.
1988.
Qunatitative Results concerningthe utility of Explanation-Based Learning.
In Proceed-ings of 7 ~h AAAI  Conference, pages 564-569, SaintPaul, Minnesota.Tom M. Mitchell, Richard M. Keller, and Smadax T.Kedar-Carbelli.
1986.
Explanation-Based Generaliza-tion: A Unifying View.
Machine Learning 1, 1:47-80.Gfinter Neumann.
1994.
Application of Explanation-based Learning for Efficient Processing of Constraint-based Grammars.
In 10 th IEEE Conference on Artifi-cial Intelligence for Applications, Sazt Antonio, Texas.Patrick Paroubek, Yves Schabes, and Aravind K. Joshi.1992.
Xtag - a graphical workbench for developingtree-adjoining grammars.
In Third Conference on Ap-plied Natural Language Processing, Trento, Italy.Manny Rayner.
1988.
Applying Explanation-BasedGeneralization to Natural Langua4ge Processing.
InProceedings of the International Conference on FifthGeneration Computer Systems, Tokyo.Philip Resnik.
1992.
Probabilistic tree-adjoining gram-max as a framework for statistical natural languageprocessing.
In Proceedings of the Fourteenth In-ternational Conference on Computational Linguistics(COLING '9~), Ntntes, France, July.Christer Samuelsson aJad Manny Rayner.
1991.
Quan-titative Evaluation of Explanation-Based Learning asan Optimization Tool for Large-Scale Natural Laat-guage System.
In Proceedings of the I~  h Interna.tional Joint Conference on Artificial Intelligence, Syd-ney, Australia.Chister Samuelsson.
1994.
Grammar Specializationthrough Entropy Thresholds.
In 32nd Meeting ofthe Association for Computational Linguistics, LasCruces, New Mexico.Yves Schabes, Anne Abeill~, aJad Aravind K. Joshi.1988.
parsing strategies with 'lexicalized' grammars:Application to "l~ee Adjoining Grammars.
In Pro-ceedings of the 12 *4 International Con/erence on Com-putational Linguistics ( COLIN G '88), Budapest, Hun-gary, August.Yves Sch&bes.
1990.
Mathematical nd ComputationalAspects of Lexicalized Grammars.
Ph.D. thesis, Com-puter Science Department, University of Pennsylva-nia.Yves Schabes.
1992.
Stochastic lexicalized tree-adjoining grammars.
In Proceedings o\] the FourteenthInternational Con\]erence on Computational Linguis-tics (COLING '9~), Nantes, Fr&ace, July.B.
Srinivas, Christine Dora,s, Seth Kullck, and AnoopSarkar.
1994.
Evaluating a wide-coverage grammar.Manuscript, October.Mark Steedman.
1987.
Combinatory Graanmaxs andPaxasitic Gaps.
Natural Language and Linguistic The-ory, 5:403-439.Frank van Haxmelen a~d Allan Bundy.
1988.Explemation-Based Generafization -- Paxtial Evalua-tion.
Artificial Intelligence, 36:401-412.275
