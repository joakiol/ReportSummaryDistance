Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 28?37,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsEfficient, correct, unsupervised learning of context-sensitive languagesAlexander ClarkDepartment of Computer ScienceRoyal Holloway, University of Londonalexc@cs.rhul.ac.ukAbstractA central problem for NLP is grammar in-duction: the development of unsupervisedlearning algorithms for syntax.
In this pa-per we present a lattice-theoretic represen-tation for natural language syntax, calledDistributional Lattice Grammars.
Theserepresentations are objective or empiri-cist, based on a generalisation of distribu-tional learning, and are capable of repre-senting all regular languages, some but notall context-free languages and some non-context-free languages.
We present a sim-ple algorithm for learning these grammarstogether with a complete self-containedproof of the correctness and efficiency ofthe algorithm.1 IntroductionGrammar induction, or unsupervised learning ofsyntax, no longer requires extensive justificationand motivation.
Both from engineering and cog-nitive/linguistic angles, it is a central challengefor computational linguistics.
However good al-gorithms for this task are thin on the ground.There are numerous heuristic algorithms, some ofwhich have had significant success in inducingconstituent structure (Klein and Manning, 2004).There are algorithms with theoretical guaranteesas to their correctness ?
such as for exampleBayesian algorithms for inducing PCFGs (John-son, 2008), but such algorithms are inefficient: anexponential search algorithm is hidden in the con-vergence of the MCMC samplers.
The efficientalgorithms that are actually used are heuristic ap-proximations to the true posteriors.
There are al-gorithms like the Inside-Outside algorithm (Lariand Young, 1990) which are guaranteed to con-verge efficiently, but not necessarily to the rightanswer: they converge to a local optimum thatmay be, and in practice nearly always is very farfrom the optimum.
There are naive enumerativealgorithms that are correct, but involve exhaus-tively enumerating all representations below a cer-tain size (Horning, 1969).
There are no correctand efficient algorithms, as there are for parsing,for example.There is a reason for this: from a formal pointof view, the problem is intractably hard for thestandard representations in the Chomsky hierar-chy.
Abe and Warmuth (1992) showed that train-ing stochastic regular grammars is hard; Angluinand Kharitonov (1995) showed that regular gram-mars cannot be learned even using queries; theseresults obviously apply also to PCFGs and CFGsas well as to the more complex representationsbuilt by extending CFGs, such as TAGs and soon.
However, these results do not necessarily ap-ply to other representations.
Regular grammarsare not learnable, but deterministic finite automataare learnable under various paradigms (Angluin,1987).
Thus it is possible to learn by changing torepresentations that have better properties: in par-ticular DFAs are learnable because they are ?ob-jective?
; there is a correspondence between thestructure of the language, (the residual languages)and the representational primitives of the formal-ism (the states) which is expressed by the Myhill-Nerode theorem.In this paper we study the learnability of a classof representations that we call distributional lat-tice grammars (DLGs).
Lattice-based formalismswere introduced by Clark et al (2008) and Clark(2009) as context sensitive formalisms that are po-tentially learnable.
Clark et al (2008) establisheda similar learnability result for a limited class ofcontext free languages.
In Clark (2009), the ap-proach was extended to a significantly larger classbut without an explicit learning algorithm.
Most ofthe building blocks are however in place, thoughwe need to make several modifications and ex-28tensions to get a clean result.
Most importantly,we need to replace the representation used there,which naively could be exponential, with a lazy,exemplar based model.In this paper we present a simple algorithmfor the inference of these representations andprove its correctness under the following learningparadigm: we assume that as normal there is a sup-ply of positive examples, and additionally that thelearner can query whether a string is in the lan-guage or not (an oracle for membership queries).We also prove that the algorithm is efficient inthe sense that it will use a polynomial amount ofcomputation and makes a polynomial number ofqueries at each step.The contributions of this paper are as follows:after some basic discussion of distributional learn-ing in Section 2, we define in Section 3 anexemplar-based grammatical formalism which wecall Distributional Lattice Grammars.
We thengive a learning algorithm under a reasonable learn-ing paradigm, together with a self contained proofin elementary terms (not presupposing any exten-sive knowledge of lattice theory), of the correct-ness of this algorithm.2 Basic definitionsWe now define our notation; we have a finite al-phabet ?
; let ??
be the set of all strings (the freemonoid) over ?, with ?
the empty string.
A (for-mal) language is a subset of ??.
We can concate-nate two languagesA andB to getAB = {uv|u ?A, b ?
B}.A context or environment, as it is called in struc-turalist linguistics, is just an ordered pair of stringsthat we write (l, r) where l and r refer to left andright; l and r can be of any length.
We can com-bine a context (l, r) with a string u with a wrap-ping operation that we write : so (l, r)  u isdefined to be lur.
We will sometimes write f fora context (l, r).
There is a special context (?, ?
):(?, ?)
w = w. We will extend this to sets ofcontexts and sets of strings in the natural way.
Wewill write Sub(w) = {u|?
(l, r) : lur = w} forthe set of substrings of a string, and Con(w) ={(l, r)|?u ?
??
: lur = w}.For a given string w we can define the distribu-tion of that string to be the set of all contexts that itcan appear in: CL(w) = {(l, r)|lwr ?
L}, equiv-alently {f |f  w ?
L}.
Clearly (?, ?)
?
CL(w)iff w ?
L.Distributional learning (Harris, 1954) as a tech-nical term refers to learning techniques whichmodel directly or indirectly properties of the dis-tribution of strings or words in a corpus or a lan-guage.
There are a number of reasons to takedistributional learning seriously: first, historically,CFGs and other PSG formalisms were intendedto be learnable by distributional means.
Chomsky(2006) says (p. 172, footnote 15):The concept of ?phrase structuregrammar?
was explicitly designed to ex-press the richest system that could rea-sonably be expected to result from theapplication of Harris-type procedures toa corpus.Second, empirically we know they work well atleast for lexical induction, (Schu?tze, 1993; Cur-ran, 2003) and are a component of some imple-mented unsupervised learning systems (Klein andManning, 2001).
Linguists use them as one of thekey tests for constituent structure (Carnie, 2008),and finally there is some psycholinguistic evidencethat children are sensitive to distributional struc-ture, at least in artificial grammar learning tasks(Saffran et al, 1996).
These arguments togethersuggest that distributional learning has a some-what privileged status.3 Lattice grammarsClark (2009) presents the theory of lattice basedformalisms starting algebraically from the theoryof residuated lattices.
Here we will largely ig-nore this, and start from a straightforward com-putational treatment.
We start by defining the rep-resentation.Definition 1.
Given a non-empty finite alphabet,?, a distributional lattice grammar (DLG) is a 3-tuple consisting of ?K,D,F ?, where F is a finitesubset of ??
?
?
?, such that (?, ?)
?
F , K is afinite subset of ??
which contains ?
and ?, and Dis a subset of (F KK).K here can be thought of as a finite set of exem-plars, which correspond to substrings or fragmentsof the language.
F is a set of contexts or fea-tures, that we will use to define the distributionalproperties of these exemplars; finally D is a setof grammatical strings, the data; a finite subset ofthe language.
F KK using the notation above is{luvr|u, v ?
K, (l, r) ?
F}.
This is the finite partof the language that we examine.
If the language29we are modeling is L, then D = L ?
(F KK).Since ?
?
K,K ?
KK.We define a concept to be an ordered pair ?S,C?where S ?
K and C ?
F , which satisfies the fol-lowing two conditions: first C  S ?
D; that isto say every string in S can be combined with anycontext in C to give a grammatical string, and sec-ondly they are maximal in that neither K nor Fcan be increased without violating the first condi-tion.We define B(K,D,F ) to be the set of all suchconcepts.
We use theB symbol (Begriff ) to bringout the links to Formal Concept Analysis (Ganterand Wille, 1997; Davey and Priestley, 2002).
Thislattice may contain exponentially many concepts,but it is clearly finite, as the number of concepts isless than min(2|F |, 2|K|).There is an obvious partial order defined by?S1, C1?
?
?S2, C2?
iff S1 ?
S2, Note thatS1 ?
S2 iff C2 ?
C1.Given a set of strings S we can define a set ofcontexts S?
to be the set of contexts that appearwith every element of S.S?
= {(l, r) ?
F : ?w ?
S, lwr ?
D}Dually we can define for a set of contexts C theset of strings C ?
that occur with all of the elementsof C:C ?
= {w ?
K : ?
(l, r) ?
C, lwr ?
D}The concepts ?S,C?
are just the pairs that sat-isfy S?
= C and C ?
= S; the two maps denotedby ?
are called the polar maps.
For any S ?
K,S???
= S?
and for any C ?
F , C ???
= C ?.
Thus wecan form a concept from any set of strings S ?
Kby taking ?S?
?, S??
; this is a concept as S???
= S?.We will write this as C(S), and for any C ?
F ,we will write C(C) = ?C ?, C ??
?.If S ?
T then T ?
?
S?, and S??
?
T ??.
For anyset of strings S ?
K, S ?
S?
?.One crucial concept here is the concept de-fined by (?, ?)
or equivalently by the set K ?
Dwhich corresponds to all of the elements in thelanguage.
We will denote this concept by L =C({(?, ?)})
= C(K ?D).We also define a meet operation by?S1, C1?
?
?S2, C2?
= ?S1 ?
S2, (S1 ?
S2)?
?This is the greatest lower bound of the two con-cepts; this is a concept since if S?
?1 = S1 andS?
?2 = S2 then (S1 ?
S2)??
= (S1 ?
S2).
Note thatthis operation is associative and commutative.
Wecan also define a join operation dually; with theseoperationsB(K,D,D) is a complete lattice.So far we have only used strings in F K; wenow define a concatenation operation as follows.
?S1, C1?
?
?S2, C2?
= ?(S1S2)?
?, (S1S2)???
?Since S1 and S2 are subsets of K, S1S2 is a sub-set of KK, but not necessarily of K. (S1S2)?
isthe set of contexts shared by all elements of S1S2and (S1S2)??
is the subset of K, not KK, thathas all of the contexts of (S1S2)?.
(S1S2)???
mightbe larger than (S1S2)?.
We can also write this asC((S1S2)?
).Both ?
and ?
are monotonic in the sense that ifX ?
Y then X ?
Z ?
Y ?
Z, Z ?
X ?
Z ?
Yand X ?
Z ?
Y ?
Z.
Note that all of these oper-ations can be computed efficiently; using a perfecthash, and a naive algorithm, we can do the polarmaps and ?
operations in timeO(|K||F |), and theconcatenation in time O(|K|2|F |).We now define the notion of derivation in thisrepresentation.
Given a string w we recursivelycompute a concept for every substring of w; thisconcept will approximate the distribution of thestring.
We define ?G as a function from ??
toB(K,D,F ); we define it recursively:?
If |w| ?
1, then ?G(w) = ?{w}?
?, {w}???
If |w| > 1 then?G(w) =?u,v?
?+:uv=w ?G(u) ?
?G(v)The first step is well defined because all of thestrings of length at most 1 are already in K sowe can look them up directly.
To clarify the sec-ond step, if w = abc then ?G(abc) = ?G(a) ?
?G(bc) ?
?G(ab) ?
?G(c); we compute the stringfrom all possible non-trivial splits of the stringinto a prefix and a suffix.
By using a dynamicprogramming table that stores the values of ?
(u)for all u ?
Sub(w) we can compute this in timeO(|K|2|F ||w|3); this is just an elementary variantof the CKY algorithm.
We define the language de-fined by the DLG G to beL(G) = {w|?G(w) ?
C({(?, ?
)})}That is to say, a string is in the language if wepredict that a string has the context (?, ?).
We nowconsider a trivial example: the Dyck language.30Example 1.
Let L be the Dyck language (matchedparenthesis language) over ?
= {a, b}, wherea corresponds to open bracket, and b to closebracket.
Define?
K = {?, a, b, ab}?
F = {(?, ?
), (?, b), (a, ?)}.?
D = {?, ab, abab, aabb}G = ?K,D,F ?
is a DLG.
We will now write downthe 5 elements of the lattice:?
> = ?K, ???
?
= ?
?, F ??
L = ?
{?, ab}, {(?, ?)}??
A = ?
{a}, {(?, b)}??
B = ?
{b}, {(a, ?
)}?To compute the concatenation A ?
B we firstcompute {a}{b} = {ab}; we then compute {ab}?which is {(?, ?
)}, and {(?, ?)}?
= {?, ab}, soA ?
B = L. Similarly to compute L ?
L, we firsttake {?, ab}{?, ab} = {?, ab, abab}.
These allhave the context (?, ?
), so the result is the con-cept L. If we compute A ?A we get {a}{a} whichis {aa} which has no contexts so the result is >.We have ?G(?)
= L, ?G(a) = A,?G(b) = B.Applying the recursive computation we can verifythat ?G(w) = L iff w ?
L and so L(G) = L. Wecan also see that D = L ?
(F KK).4 SearchIn order to learn these grammars we need to find asuitable set of contexts F , a suitable set of stringsK, and then work out which elements of F KKare grammatical.
So given a choice for K and Fit is easy to learn these models under a suitableregime: the details of how we collect informationabout D depend on the learning model.The question is therefore whether it is easyto find suitable sets, K and F .
Because of theway the formalism is designed, it transpires thatthe search problem is entirely tractable.
In or-der to analyse the search space, we define twomaps between the lattices as K and F are in-creased.
We are going to augment our notationslightly; we will write B(K,L, F ) for B(K,L ?
(FKK), F ) and similarly ?K,L, F ?
for ?K,L?
(F KK), F ?.
When we use the two polar maps(such as C ?, S?
), though we are dealing with morethan one lattice, there is no ambiguity as the mapsagree; we will when necessary explicitly restrictthe output (e.g.
C ?
?
J) to avoid confusion.Definition 2.
For any language L and any set ofcontexts F ?
G, and any sets of strings J ?K ?
??.
We define a map g from B(J, L, F ) toB(K,L, F ) (from the smaller lattice to the largerlattice) as g(?S,C?)
= ?C ?, C?.We also define a map f from B(K,L,G)to B(K,L, F ), (from larger to smaller) asf(?S,C?)
= ?
(C ?
F )?, C ?
F ?.These two maps are defined in opposite direc-tions: this is because of the duality of the lattice.By defining them in this way, as we will see, wecan prove that these two maps have very similarproperties.
We can verify that the outputs of thesemaps are in fact concepts.We now need to define two monotonicity lem-mas: these lemmas are crucial to the success ofthe formalism.
We show that as we increase Kthe language defined by the formalism decreasesmonotonically, and that as we increase F the lan-guage increases monotonically.
There is some du-plication in the proofs of the two lemmas; wecould prove them both from more abstract prop-erties of the maps f, g which are what are calledresidual maps, but we will do it directly.Lemma 1.
Given two lattices B(K,L, F ) andB(K,L,G) where F ?
G; For all X,Y ?B(K,L,G) we have that1.
f(X) ?
f(Y ) ?
f(X ?
Y )2. f(X) ?
f(Y ) ?
f(X ?
Y )Proof.
The proof is elementary but difficult toread.
We write X = ?SX , CX?
and similarly forY .
For part 1 of the lemma: Clearly (S?X ?
F ) ?S?X , so (S?X ?
F )?
?
S?
?X = SX and the same forSY .
So (S?X ?F )?
(S?Y ?F )?
?
SXSY (as subsetsofKK).
So ((S?X?F )?
(S?Y ?F )?)?
?
(SXSY )?
?
(SXSY )???.
Now by definition, f(X) ?
f(Y ) isC(Z) where Z = ((S?X ?F )?
(S?Y ?F )?)?
?F andf(X ?Y ) has the set of contexts ((SXSY )???
?F ).Therefore f(X ?
Y ) has a bigger set of contextsthan f(X) ?
f(Y ) and is thus a smaller concept.For part 2: by definition f(X ?
Y ) = ?
((SX ?Sy)?
?
F )?, (SX ?
Sy)?
?
F ?
and f(X) ?
f(Y ) =?
(S?X?F )??
(S?y?F )?, ((S?X?F )??
(S?y?F )?)?
?F ?Now S?X ?
F ?
S?X , so (since S?
?X = SX ) SX ?
(S?X?F )?, and so SX?Sy ?
(S?X?F )??
(S?y?F )?.31So (SX ?
Sy)?
?
((S?X ?
F )?
?
(S?y ?
F )?)
whichgives the result by comparing the context sets ofthe two sides of the inequality.Lemma 2.
For any language L, and two sets ofcontexts F ?
G, and any K, if we have two DLGs?K,L, F ?
with map ?F : ??
?
B(K,L, F ) and?K,L,G?
with map ?G : ??
?
B(K,L,G) thenfor all w, f(?G(w)) ?
?F (w).Proof.
By induction on the length of w; clearlyif |w| ?
1, f(?G(w)) = ?F (w).
We now takethe inductive step; by definition, (suppressing thedefinition of u, v in the meet)f(?G(w)) = f(?u,v?G(u) ?
?G(v))By Lemma 1, part 2:f(?G(w)) ?
?u,vf(?G(u) ?
?G(v))By Lemma 1, part 1:f(?G(w)) ?
?u,vf(?G(u)) ?
f(?G(v))By the inductive hypothesis we have f(?G(u)) ?
?F (u) and similarly for v and so by the mono-tonicity of ?
and ?
:f(?G(w)) ?
?u,v?F (u) ?
?F (v)Since the right hand side is equal to ?F (w), theproof is done.It is then immediate thatLemma 3.
If F ?
G then L(?K,L, F ?)
?L(?K,L,G?),Proof.
If w ?
L(?K,L, F ?
), then ?F (w) ?
L,and so f(?G(w)) ?
L and so ?G(w) has the con-text (?, ?)
and is thus in L(?K,L,G?
).We now prove the corresponding facts about g.Lemma 4.
For any J ?
K and any conceptsX,YinB(J, L, F ), we have that1.
g(X) ?
g(Y ) ?
g(X ?
Y )2. g(X) ?
g(Y ) ?
g(X ?
Y )Proof.
For the first part: Write X = ?SX , CX?
asbefore.
Note that SX = C ?X ?
J .
SX ?
C?X , soSXSY ?
C ?XC?Y , and so (SXSY )??
?
(C ?XC?Y )?
?,and ((SXSY )??
?
J)?
?
(C ?XC?Y )???.
By calcu-lation g(X) ?
g(Y ) = ?
(C ?XC?Y )?
?, (C ?XC?Y )???
?On the other hand, g(X ?
Y ) = ?
((SXSY )??
?J)?
?, ((SXSY )??
?
J)??
and so g(X ?
Y ) is smalleras it has a larger set of contexts.For the second part: g(X) ?
g(Y ) = ?C ?X ?C ?Y , (C?X ?
C?Y )??
and g(X ?
Y ) = ?
(SX ?SY )?
?, (SX ?
SY )??.
Since SX = C ?X ?
J , SX ?C ?X , so (SX ?
SY ) ?
C?X ?
C?Y , and therefore(SX ?
SY )??
?
(C ?X ?
C?Y )??
= C ?X ?
C?Y .We now state and prove the monotonicitylemma for g.Lemma 5.
For all J ?
K ?
????
?, and for allstrings w; we have that g(?J(w)) ?
?K(w).Proof.
By induction on length of w. Both J andK include the basic elements of ?
and ?.
Firstsuppose |w| ?
1, then ?J(w) = ?
(CL(w) ?F )?
?J,CL(w)?F ?, and g(?J(w)) = ?
(CL(w)?F )?, CL(w) ?
F ?
which is equal to ?K(w).Now suppose true for all w of length at most k,and take some w of length k + 1.
By definition of?J :g(?J(w)) = g(?u,v?J(u) ?
?J(v))Next by Lemma 4, Part 2g(?J(w)) ?
?u,vg(?J(u) ?
?J(v))By Lemma 4, Part 1g(?J(w)) ?
?u,vg(?J(u)) ?
g(?J(v))By the inductive hypothesis and monotonicity of ?and ?
:g(?J(w)) ?
?u,v?K(u) ?
?K(v) = ?K(w)Lemma 6.
If J ?
K then L(?J, L, F ?)
?L(?K,L, F ?)Proof.
Suppose w ?
L(?K,L, F ?).
this meansthat ?K(w) ?
LK .
therefore g(?J(w)) ?Lk; which means that (?, ?)
is in the conceptg(?J(w)), which means it is in the concept ?J(w),and therefore w ?
L(?J, L, F ?
).32Given these two lemmas we can make the fol-lowing observations.
First, if we have a fixed Land F , then as we increase K, the language willdecrease until it reaches a limit, which it will at-tain after a finite limit.Lemma 7.
For all L, and finite context sets F ,there is a finite K such that for all K2, K ?
K2,L(?K,L, F ?)
= L(?K2, L, F ?).Proof.
We can define the latticeB(?
?, L, F ).
De-fine the following equivalence relation betweenpairs of strings, where (u1, v1) ?
(u2, v2) iffC(u1) = C(u2) and C(v1) = C(v2) and C(u1v1) =C(u2v2).
The number of equivalence classes isclearly finite.
If K is sufficiently large that there isa pair of strings (u, v) in K for each equivalenceclass, then clearly the lattice defined by thisK willbe isomorphic toB(?
?, L, F ).
Any superset ofKwill not change this lattice.Moreover this language is unique for each L,F .We will call this the limit language ofL,F , and wewill write it as L(??
?, L, F ?
).If F ?
G, then L(??
?, L, F ?)
?L(??
?, L,G?).
Finally, we will show thatthe limit languages never overgeneralise.Lemma 8.
For any L, and for any F ,L(??
?, L, F ?)
?
L.Proof.
Recall that C(w) = ?{w}?
?, {w}??
is thereal concept.
If G is a limit grammar, we canshow that we always have ?G(w) > C(w), whichwill give us the result immediately.
First notethat C(u) ?
C(v) ?
C(uv), which is immedi-ate by the definition of ?.
We proceed, again,by induction on the length of w. For |w| ?
1,?G(w) = C(w).
For the inductive step we have?G(w) =?u,v ?G(u) ?
?G(v); by inductive hy-pothesis we have that this must be more than?u,v C(u) ?
C(v) >?u,v C(uv) = C(w)5 Weak generative powerFirst we make the following observation: if weconsider an infinite variant of this, where we setK = ??
and F = ??
?
??
and D = L, wecan prove easily that, allowing infinite ?represen-tations?, for any L, L(?K,D,F ?)
= L. In thisinfinite data limit, ?
becomes associative, and thestructure ofB(K,D,F ) becomes a residuated lat-tice, called the syntactic concept lattice of the lan-guage L, B(L).
This lattice is finite iff the lan-guage is regular.
The fact that this lattice now hasresiduation operations suggest interesting links tothe theory of categorial grammar.
It is the finitecase that interests us.We will use LDLG to refer to the class of lan-guages that are limit languages in the sense de-fined above.LDLG = {L|?F,L(??
?, L, F ?)
= L}Our focus in this paper is not on the languagetheory: we present the following propositions.First LDLG properly contains the class of regularlanguages.
Secondly LDLG contains some non-context-free languages (Clark, 2009).
Thirdly itdoes not contain all context-free languages.A natural question to ask is how to convert aCFG into a DLG.
This is in our view the wrongquestion, as we are not interested in modelingCFGs but modeling natural languages, but giventhe status of CFGs as a default model for syn-tactic structure, it will help to give a few exam-ples, and a general mechanism.
Consider a non-terminal N in a CFG with start symbol S. Wecan define C(N) = {(l, r)|S??
lNr} and theyield Y (N) = {w|N??
w}.
Clearly C(N)Y (N) ?
L, but these are not necessarily maxi-mal, and thus ?C(N), Y (N)?
is not necessarily aconcept.
Nonetheless in most cases, we can con-struct a grammar where the non-terminals will cor-respond to concepts, in this way.The basic approach is this: for each non-terminal, we identify a finite set of contexts thatwill pick out only the set of strings generatedfrom that non-terminal: we find some set of con-texts FN typically a subset of C(N) such thatY (N) = {w|?
(l, r) ?
FN , lwr ?
L}.
We saythat we can contextually define this non-terminalif there is such a finite set of contexts FN .
If aCFG in Chomsky normal form is such that everynon-terminal can be contextually defined then thelanguage defined by that grammar is in LDLG.
Ifwe can do that, then the rest is trivial.
We takeany set of features F that includes all of these FN ;probably just F =?N FN ; we then pick a set ofstrings K that is sufficiently large to rule out allincorrect generalisations, and then define D to beL ?
(F KK).Consider the language L = {anbncm|n,m ?0} ?
{ambncn|n,m ?
0}.
L is a classic ex-ample of an inherently ambiguous and thus non-deterministic language.The natural CFG in CNF for L hasnon-terminals that generate the following33sets: {anbn|n ?
0}, {an+1bn|n ?
0},{bncn|n ?
0}, {bncn+1|n ?
0}, {a?
}and {c?}.
We note that the six contexts(aa, bbc), (aa, bbbc), (abb, cc)(abbb, cc), (?, a)and (c, ?)
will define exactly these sets, inthe sense that the set of strings that oc-cur in each context will be exactly thecorresponding set.
We can also pick out?, a, b, c with individual contexts.
Let F ={(?, ?
), (aaabb, bccc), (aaabbc, ?
), (?, abbccc),(aaab, bccc), (aa, bbc), (aa, bbbc), (abb, cc),(abbb, cc), (?, a), (c, ?)}.
If we take a sufficientlylarge set K, say ?, a, b, c, ab, aab, bc, bcc, abc, andset D = L ?
F KK, then we will have a DLGfor the language L. In this example, it is sufficientto have one context per non-terminal.
This is notin general the case.Consider L = {anbn|n ?
0} ?
{anb2n|n ?0}.
Here we clearly need to identify sets of stringscorresponding to the two parts of this language,but it is easy to see that no one context will suffice.However, note that the first part is defined by thetwo contexts (?, ?
), (a, b) and the second by thetwo contexts (?, ?
), (a, bb).
Thus it is sufficient tohave a set F that includes these four contexts, aswell as similar pairs for the other non-terminals inthe grammar, and some contexts to define a and b.We can see that we will not always be able to dothis for every CFG.
One fixable problem is if theCFG has two separate non-terminals, M,N suchthat C(M) ?
C(N).
If this is the case, then wemust have that Y (N) ?
Y (M), If we pick a setof contexts to define Y (N), then clearly any stringin Y (M) will also be picked out by the same con-texts.
If this is not the case, then we can clearly tryto rectify it by adding a rule N ?
M which willnot change the language defined.However, we cannot always pick out the non-terminals with a finite set of contexts.
Considerthe language L = {anb|n > 0} ?
{ancm|m >n > 0} defined in Clark et al (2008).
Sup-pose wlog that F contains no context (l, r) suchthat |l| + |r| ?
k. Then it is clear that we willnot be able to pick out b without also picking outck+1, since CL(ck+1) ?
F ?
CL(b) ?
F .
ThusL, which is clearly context-free, is not in LDLG.Luckily, this example is highly artificial and doesnot correspond to any phenomena we are aware ofin linguistics.In terms of representing natural languages, weclearly will in many cases need more than onecontext to pick out syntactically relevant groupsof strings.
Using a very simplified example fromEnglish, if we want to identify say singular nounphrases, a context like (that is, ?)
will not be suf-ficient since as well as noun phrases we will alsohave some adjective phrases.
However if we in-clude multiple contexts such as (?, is over there)and so on, eventually we will be able to pick outexactly the relevant set of strings.
One of thereasons we need to use a context sensitive repre-sentation, is so that we can consider every possi-ble combination of contexts simultaneously: thiswould require an exponentially large context freegrammar.6 Learning ModelIn order to prove correctness of the learning algo-rithm we will use a variant of Gold-style inductiveinference (Gold, 1967).
Our choice of this ratherold-fashioned model requires justification.
Thereare two problems with learning ?
the informationtheoretic problems studied under VC-dimensionetc., and the computational complexity issues ofconstructing a hypothesis from the data.
In ourview, the latter problems are the key ones.
Ac-cordingly, we focus entirely on the efficiency is-sue, and allow ourself a slightly unrealistic model;see (Clark and Lappin, 2009) for arguments thatthis is a plausible model.We assume that we have a sequence of posi-tive examples, and that we can query examples formembership.
Given a language L a presentationfor L is an infinite sequence of strings w1, w2, .
.
.such that {wi|i ?
N} = L. An algorithm receivesa sequence T and an oracle, and must produce ahypothesis H at every step, using only a polyno-mial number of queries to the membership oracle?
polynomial in the total size of the presentation.It identifies in the limit the language L iff for ev-ery presentation T of L there is a N such that forall n > N Hn = HN , and L(HN ) = L. We sayit identifies in the limit a class of languages L iffit identifies in the limit all L in L. We say that itidentifies the class in polynomial update time iffthere is a polynomial p, such that at each step themodel uses an amount of computation (and thusalso a number of queries) that is less than p(n, l),where n is the number of strings and l is the max-imum length of a string in the observed data.
Wenote that this is slightly too weak.
It is possibleto produce vacuous enumerative algorithms that34can learn anything by only processing a logarith-mically small prefix of the string (Pitt, 1989).7 Learning AlgorithmWe now define a simple learning algorithm, thatestablishes learnability under this paradigm.There is one minor technical detail we need todeal with.
We need to be able to tell when addinga string to a lazy DLG will leave the grammar un-changed.
We use a slightly weaker test.
GivenG1 = ?K,D,F ?
we define as before the equiva-lence relation between pairs of strings ofK, where(u1, v1) ?G1 (u2, v2) iff CD(u1) = CD(u2) andCD(v1) = CD(v2) and CD(u1v1) = CD(u2v2).Note that CD(u) = {(l, r)|lur ?
D}.Given two grammars G1 = ?K,D,F ?
andG2 = ?K2, D2, F ?
where K ?
K2 and D ?
D2but F is unchanged, we say that these two areindistinguishable iff the number of equivalenceclasses of K ?K under ?G1 is equal to the num-ber of equivalence classes of K2?K2 under?G2 .This can clearly be computed efficiently using aunion-find algorithm, in time polynomial in |K|and |F |.
If they are indistinguishable then they de-fine the same language.7.1 AlgorithmAlgorithm 1 presents the basic algorithm.
At var-ious points we compute sets of strings like (FKK)?L; these can be computed using the mem-bership oracle.First we prove that the program is efficient inthe sense that it runs in polynomial update time.Lemma 9.
There is a polynomial p, such that Al-gorithm 1, for each wn, runs in time bounded byp(n, l) where l is the maximum length of a stringin w1, .
.
.
wn.Proof.
First we note that K,K2 and F are alwayssubsets of Sub(E)??
andCon(E), and thus both|K| and |F | are bounded by nl(l+1)/2+ |?|+1.Computing D is efficient as |F KK| is boundedby |K|2|F |.
We can compute ?G as mentionedabove in time |K|2|F |l3; distinguishability is asobserved earlier also polynomial.Before we prove the correctness of the algo-rithm we make some informal points.
First, weare learning under a rather pessimistic model ?
thepositive examples may be chosen to confuse us,so we cannot make any assumptions.
Accordinglywe have to very crudely add all substrings and allAlgorithm 1: DLG learning algorithmData: Input strings S = {w1, w2 .
.
.
, },membership oracle OResult: A sequence of DLGs G1, G2, .
.
.K ?
?
?
{?
}, K2 = K ;F ?
{(?, ?
)}, E = {} ;D = (F KK) ?
L ;G = ?K,D,F ?
;for wi doE ?
E ?
{wi} ;K2 ?
K2 ?
Sub(wi) ;if there is some w ?
E that is not inL(G) thenF ?
Con(E) ;K ?
K2 ;D = (F KK) ?
L ;G = ?K,D,F ?
;endelseD2 ?
(F K2K2) ?
L ;if ?K2, D2, F ?
not indistinguishablefrom ?K,D,F ?
thenK ?
K2 ;D = (F KK) ?
L ;G = ?K,D,F ?
;endendOutput G;endcontexts, rather than using sensible heuristics toselect frequent or likely ones.Intuitively the algorithm works as follows: if weobserve a string not in our current hypothesis, thenwe increase the set of contexts which will increasethe language defined.
Since we only see positiveexamples, we will never explicitly find out that ourhypothesis overgenerates, accordingly we alwaysadd strings to a tester set K2 and see if this givesus a more refined model.
If this seems like it mightgive a tighter hypothesis, then we increase K.In what follows we will say that the hypothesisat step n, Gn = ?Kn, Dn, Fn?, and the languagedefined is Ln.
We will assume that the target lan-guage is some L ?
LDLG and w1, .
.
.
is a presen-tation of L.Lemma 10.
Then there is a point n, and a finite setof contexts F such that for all N > n, FN = F .,and L(??
?, L, F ?)
= L.Proof.
Since L ?
LDLG there is some set of con-35texts G ?
Con(L), such that L = L(??
?, L,G?
).Any superset ofG will define the correct limit lan-guage.
Let n be the smallest n such that G is asubset of Con({w1, .
.
.
, wn}).
Consider Fn.
IfFn defines the correct limit language, then we willnever change F as the hypothesis will be a super-set of the target.
Otherwise it must define a subsetof the correct language.
Then either there is someN > n at which it has converged to the limit lan-guage which will cause the first condition in theloop to be satisfied and F will be increased to asuperset ofG, or F will be increased before it con-verges, and thus the result holds.Lemma 11.
After F converges according to theprevious lemma, there is some n, such that for allN > n, KN = Kn and L(?Kn, L, Fn?)
= L.Proof.
let n0 be the convergence point of F ; forall n > n0 the hypothesis will be a superset ofthe target language; therefore the only change thatcan happen is that K will increase.
By definitionof the limit language, it must converge after a finitenumber of examples.Theorem 1.
For every language L ?
LDLG, andevery presentation of L, Algorithm 1 will convergeto a grammar G such that L(G) = L.This result is immediate by the two precedinglemmas.8 ConclusionWe have presented an efficient, correct learning al-gorithm for an interesting class of languages; thisis the first such learning result for a class of lan-guages that is potentially large enough to describenatural language.The results presented here lack a couple of tech-nical details to be completely convincing.
In par-ticular we would like to show that given a repre-sentation of size n, we can learn once we have seena set of examples that is polynomially bounded byn.
This will be challenging, as the size of the Kwe need to converge can be exponentially largein F .
We can construct DFAs where the num-ber of congruence classes of the language is anexponential function of the number of states.
Inorder to learn languages like this, we will needto use a more efficient algorithm that can learneven with ?insufficient?
K: that is to say whenthe lattice B(K,L, F ) has fewer elements thatB(KK,L, F ).This algorithm can be implemented directly andfunctions as expected on synthetic examples, butwould need modification to run efficiently on nat-ural languages.
In particular rather than consider-ing whole contexts of the form (l, r) it would benatural to restrict them just to a narrow windowof one or two words or tags on each side.
Ratherthan using a membership oracle, we could prob-abilistically cluster the data in the table of countsof strings in F  K. In practice we will have alimited amount of data to work with and we cancontrol over-fitting in a principled way by control-ling the relative size of K and F .This formalism represents a process of anal-ogy from stored examples, based on distributionallearning ?
this is very plausible in terms of whatwe know about cognitive processes, and is com-patible with much non-Chomskyan theorizing inlinguistics (Blevins and Blevins, 2009).
The classof languages is a good fit to the class of naturallanguages; it contains, as far as we can tell, allstandard examples of context free grammars, andincludes non-deterministic and inherently ambigu-ous grammars.
It is hard to say whether the classis in fact large enough to represent natural lan-guages; but then we don?t know that about any for-malism, context-free or context-sensitive.
All wecan say is that there are no phenomena that we areaware of that don?t fit.
Only large scale empiricalwork can answer this question.Ideologically these models are empiricist ?
thestructure of the representation is based on thestructure of the data: this has to be a good thingfor computational modeling.
By minimizing theamount of hidden, unobservable structure, we canimprove learnability.
Languages are enormouslycomplex, and it would be simplistic to try to re-duce their acquisition to a few pages of mathe-matics; nonetheless, we feel that the representa-tions and grammar induction algorithms presentedin this paper could be a significant piece of thepuzzle.36ReferencesN.
Abe and M. K. Warmuth.
1992.
On the computa-tional complexity of approximating distributions byprobabilistic automata.
Machine Learning, 9:205?260.D.
Angluin and M. Kharitonov.
1995.
When won?tmembership queries help?
J. Comput.
Syst.
Sci.,50:336?355.D.
Angluin.
1987.
Learning regular sets from queriesand counterexamples.
Information and Computa-tion, 75(2):87?106.James P. Blevins and Juliette Blevins.
2009.
Analogyin grammar: Form and acquisition.
Oxford Univer-sity Press.A.
Carnie.
2008.
Constituent structure.
Oxford Uni-versity Press, USA.Noam Chomsky.
2006.
Language and mind.
Cam-bridge University Press, 3rd edition.Alexander Clark and Shalom Lappin.
2009.
Anotherlook at indirect negative evidence.
In Proceedings ofthe EACL Workshop on Cognitive Aspects of Com-putational Language Acquisition, Athens, March.Alexander Clark, Re?mi Eyraud, and Amaury Habrard.2008.
A polynomial algorithm for the inference ofcontext free languages.
In Proceedings of Interna-tional Colloquium on Grammatical Inference, pages29?42.
Springer, September.Alexander Clark.
2009.
A learnable representationfor syntax using residuated lattices.
In Proceedingsof the 14th Conference on Formal Grammar, Bor-deaux, France.J.R.
Curran.
2003.
From distributional to semanticsimilarity.
Ph.D. thesis, University of Edinburgh.B.
A. Davey and H. A. Priestley.
2002.
Introduction toLattices and Order.
Cambridge University Press.B.
Ganter and R. Wille.
1997.
Formal Concept Analy-sis: Mathematical Foundations.
Springer-Verlag.E.
M. Gold.
1967.
Language identification in the limit.Information and control, 10(5):447 ?
474.Zellig Harris.
1954.
Distributional structure.
Word,10(2-3):146?62.J.
J. Horning.
1969.
A Study of Grammatical Infer-ence.
Ph.D. thesis, Stanford University, ComputerScience Department, California.M.
Johnson.
2008.
Using adaptor grammars to identifysynergies in the unsupervised acquisition of linguis-tic structure.
In 46th Annual Meeting of the ACL,pages 398?406.Dan Klein and Chris Manning.
2001.
Distribu-tional phrase structure induction.
In Proceedings ofCoNLL 2001, pages 113?121.Dan Klein and Chris Manning.
2004.
Corpus-basedinduction of syntactic structure: Models of depen-dency and constituency.
In Proceedings of the 42ndAnnual Meeting of the ACL.K.
Lari and S. J.
Young.
1990.
The estimation ofstochastic context-free grammars using the inside-outside algorithm.
Computer Speech and Language,4:35?56.L.
Pitt.
1989.
Inductive inference, dfa?s, and computa-tional complexity.
In K. P. Jantke, editor, Analogicaland Inductive Inference, number 397 in LNAI, pages18?44.
Springer-Verglag.J.
R. Saffran, R. N. Aslin, and E. L. Newport.
1996.Statistical learning by eight month old infants.
Sci-ence, 274:1926?1928.Hinrich Schu?tze.
1993.
Part of speech induction fromscratch.
In Proceedings of the 31st annual meet-ing of the Association for Computational Linguis-tics, pages 251?258.37
