Annealing Techniques for Unsupervised Statistical Language LearningNoah A. Smith and Jason EisnerDepartment of Computer Science / Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218 USA{nasmith,jason}@cs.jhu.eduAbstractExploiting unannotated natural language data is hardlargely because unsupervised parameter estimation ishard.
We describe deterministic annealing (Rose et al,1990) as an appealing alternative to the Expectation-Maximization algorithm (Dempster et al, 1977).
Seek-ing to avoid search error, DA begins by globally maxi-mizing an easy concave function and maintains a localmaximum as it gradually morphs the function into thedesired non-concave likelihood function.
Applying DAto parsing and tagging models is shown to be straight-forward; significant improvements over EM are shownon a part-of-speech tagging task.
We describe a vari-ant, skewed DA, which can incorporate a good initializerwhen it is available, and show significant improvementsover EM on a grammar induction task.1 IntroductionUnlabeled data remains a tantalizing potential re-source for NLP researchers.
Some tasks can thriveon a nearly pure diet of unlabeled data (Yarowsky,1995; Collins and Singer, 1999; Cucerzan andYarowsky, 2003).
But for other tasks, such as ma-chine translation (Brown et al, 1990), the chiefmerit of unlabeled data is simply that nothing elseis available; unsupervised parameter estimation isnotorious for achieving mediocre results.The standard starting point is the Expectation-Maximization (EM) algorithm (Dempster et al,1977).
EM iteratively adjusts a model?s parame-ters from an initial guess until it converges to a lo-cal maximum.
Unfortunately, likelihood functionsin practice are riddled with suboptimal local max-ima (e.g., Charniak, 1993, ch.
7).
Moreover, max-imizing likelihood is not equivalent to maximizingtask-defined accuracy (e.g., Merialdo, 1994).Here we focus on the search error problem.
As-sume that one has a model for which improvinglikelihood really will improve accuracy (e.g., at pre-dicting hidden part-of-speech (POS) tags or parsetrees).
Hence, we seek methods that tend to locatemountaintops rather than hilltops of the likelihoodfunction.
Alternatively, we might want methods thatfind hilltops with other desirable properties.11Wang et al (2003) suggest that one should seek a high-In ?2 we review deterministic annealing (DA)and show how it generalizes the EM algorithm.
?3shows how DA can be used for parameter estimationfor models of language structure that use dynamicprogramming to compute posteriors over hiddenstructure, such as hidden Markov models (HMMs)and stochastic context-free grammars (SCFGs).
In?4 we apply DA to the problem of learning a tri-gram POS tagger without labeled data.
We then de-scribe how one of the received strengths of DA?its robustness to the initializing model parameters?can be a shortcoming in situations where the ini-tial parameters carry a helpful bias.
We presenta solution to this problem in the form of a newalgorithm, skewed deterministic annealing (SDA;?5).
Finally we apply SDA to a grammar induc-tion model and demonstrate significantly improvedperformance over EM (?6).
?7 highlights future di-rections for this work.2 Deterministic annealingSuppose our data consist of a pairs of random vari-ables X and Y , where the value of X is observedand Y is hidden.
For example, X might rangeover sentences in English and Y over POS tag se-quences.
We use X and Y to denote the sets ofpossible values of X and Y , respectively.
We seekto build a model that assigns probabilities to each(x, y) ?
X?Y.
Let ~x = {x1, x2, ..., xn} be a corpusof unlabeled examples.
Assume the class of modelsis fixed (for example, we might consider only first-order HMMs with s states, corresponding notion-ally to POS tags).
Then the task is to find good pa-rameters ~?
?
RN for the model.
The criterion mostcommonly used in building such models from un-labeled data is maximum likelihood (ML); we seekthe parameters ~??
:argmax~?Pr(~x | ~?)
= argmax~?n?i=1?y?YPr(xi, y | ~?)
(1)entropy hilltop.
They argue that to account for partially-observed (unlabeled) data, one should choose the distributionwith the highest Shannon entropy, subject to certain data-drivenconstraints.
They show that this desirable distribution is one ofthe local maxima of likelihood.
Whether high-entropy localmaxima really predict test data better is an empirical question.Input: ~x, ~?
(0) Output: ~??i?
0do:(E) p?(~y)?Pr(~x,~y|~?(i))?~y?
?Yn Pr(~x,~y?|~?
(i)),?~y(M) ~?
(i+1) ?
argmax~?
Ep?
(~Y )[log Pr(~x, ~Y | ~?)]i?
i+ 1until ~?
(i) ?
~?(i?1)~??
?
~?(i)Fig.
1: The EM algorithm.Each parameter ?j corresponds to the conditionalprobability of a single model event, e.g., a state tran-sition in an HMM or a rewrite in a PCFG.
ManyNLP models make it easy to maximize the likeli-hood of supervised training data: simply count themodel events in the observed (xi, yi) pairs, and setthe conditional probabilities ?i to be proportional tothe counts.
In our unsupervised setting, the yi areunknown, but solving (1) is almost as easy providedthat we can obtain the posterior distribution of Ygiven each xi (that is, Pr(y | xi) for each y ?
Yand each xi).
The only difference is that we mustnow count the model events fractionally, using theexpected number of occurrences of each (xi, y) pair.This intuition leads to the EM algorithm in Fig.
1.It is guaranteed that Pr(~x | ~?
(i+1)) ?
Pr(~x | ~?
(i)).For language-structure models like HMMs andSCFGs, efficient dynamic programming algorithms(forward-backward, inside-outside) are available tocompute the distribution p?
at the E step of Fig.
1and use it at the M step.
These algorithms run inpolynomial time and space by structure-sharing thepossible y (tag sequences or parse trees) for eachxi, of which there may be exponentially many inthe length of xi.
Even so, the majority of time spentby EM for such models is on the E steps.
In this pa-per, we can fairly compare the runtime of EM andother training procedures by counting the number ofE steps they take on a given training set and model.2.1 Generalizing EMFigure 2 shows the deterministic annealing (DA) al-gorithm derived from the framework of Rose et al(1990).
It is quite similar to EM.2 However, DAadds an outer loop that iteratively increases a value?, and computation of the posterior in the E step ismodified to involve this ?.2Other expositions of DA abound; we have couched ours indata-modeling language.
Readers interested in the Lagrangian-based derivations and analogies to statistical physics (includingphase transitions and the role of ?
as the inverse of temperaturein free-energy minimization) are referred to Rose (1998) for athorough discussion.Input: ~x, ~?
(0), ?max>?min>0, ?>1 Output: ~??i?
0; ?
?
?minwhile ?
?
?max:do:(E) p?(~y)?Pr(~x,~y|~?(i))??~y?
?Yn Pr(~x,~y?|~?(i))?
,?~y(M) ~?
(i+1) ?
argmax~?
Ep?
(~Y )[log Pr(~x, ~Y | ~?)]i?
i+ 1until ~?
(i) ?
~?(i?1)?
?
?
?
?end while~??
?
~?(i)Fig.
2: The DA algorithm: a generalization of EM.When ?
= 1, DA?s inner loop will behave exactlylike EM, computing p?
at the E step by the same for-mula that EM uses.
When ?
?
0, p?
will be closeto a uniform distribution over the hidden variable ~y,since each numerator Pr(~x, ~y | ~?)?
?
1.
At such?-values, DA effectively ignores the current param-eters ?
when choosing the posterior p?
and the newparameters.
Finally, as ?
?
+?, p?
tends to placenearly all of the probability mass on the single mostlikely ~y.
This winner-take-all situation is equivalentto the ?Viterbi?
variant of the EM algorithm.2.2 Gradated difficultyIn both the EM and DA algorithms, the E step se-lects a posterior p?
over the hidden variable ~Y andthe M step selects parameters ~?.
Neal and Hinton(1998) show how the EM algorithm can be viewedas optimizing a single objective function over both ~?and p?.
DA can also be seen this way; DA?s objectivefunction at a given ?
isF(~?, p?, ?)=1?H(p?)
+Ep?
(~Y )[log Pr(~x, ~Y | ~?
)](2)The EM version simply sets ?
= 1.
A completederivation is not difficult but is too lengthy to givehere; it is a straightforward extension of that givenby Neal and Hinton for EM.It is clear that the value of ?
allows us to manip-ulate the relative importance of the two terms whenmaximizing F. When ?
is close to 0, only the Hterm matters.
The H term is the Shannon entropyof the posterior distribution p?, which is known to beconcave in p?.
Maximizing it is simple: set al x to beequiprobable (the uniform distribution).
Thereforea sufficiently small ?
drives up the importance ofH relative to the other term, and the entire problembecomes concave with a single global maximum towhich we expect to converge.In gradually increasing ?
from near 0 to 1, westart out by solving an easy concave maximizationproblem and use the result to initialize the next max-imization problem, which is slightly more difficult(i.e., less concave).
This continues, with the solu-tion to each problem in the series being used to ini-tialize the subsequent problem.
When ?
reaches 1,DA behaves just like EM.
Since the objective func-tion is continuous in ?
where ?
> 0, we can vi-sualize DA as gradually morphing the easy concaveobjective function into the one we really care about(likelihood); we hope to ?ride the maximum?
as ?moves toward 1.DA guarantees iterative improvement of the ob-jective function (see Ueda and Nakano (1998) forproofs).
But it does not guarantee convergence toa global maximum, or even to a better local maxi-mum than EM will find, even with extremely slow?-raising.
A new mountain on the surface of theobjective function could arise at any stage that ispreferable to the one that we will ultimately find.To run DA, we must choose a few control param-eters.
In this paper we set ?max = 1 so that DAwill approach EM and finish at a local maximum oflikelihood.
?min and the ?-increase factor ?
can beset high for speed, but at a risk of introducing lo-cal maxima too quickly for DA to work as intended.
(Note that a ?fast?
schedule that tries only a few ?values is not as fast as one might expect, since it willgenerally take longer to converge at each ?
value.
)To conclude the theoretical discussion of DA, wereview its desirable properties.
DA is robust to ini-tial parameters, since when ?
is close to 0 the ob-jective hardly depends on ~?.
DA gradually increasesthe difficulty of search, which may lead to the avoid-ance of some local optima.
By modifying the an-nealing schedule, we can change the runtime of theDA algorithm.
DA is almost exactly like EM in im-plementation, requiring only a slight modification tothe E step (see ?3) and an additional outer loop.2.3 Prior workDA was originally described as an algorithm forclustering data in RN (Rose et al, 1990).
Its pre-decessor, simulated annealing, modifies the objec-tive function during search by applying random per-turbations of gradually decreasing size (Kirkpatricket al, 1983).
Deterministic annealing moves therandomness ?inside?
the objective function by tak-ing expectations.
DA has since been applied tomany problems (Rose, 1998); we describe two keyapplications in language and speech processing.Pereira, Tishby, and Lee (1993) used DA for softhierarchical clustering of English nouns, based onthe verbs that select them as direct objects.
In theircase, when ?
is close to 0, each noun is fuzzilyplaced in each cluster so that Pr(cluster | noun)is nearly uniform.
On the M step, this results inclusters that are almost exactly identical; there isone effective cluster.
As ?
is increased, it becomesincreasingly attractive for the cluster centroids tomove apart, or ?split?
into two groups (two effectiveclusters), and eventually they do so.
Continuing toincrease ?
yields a hierarchical clustering throughrepeated splits.
Pereira et al describe the tradeoffgiven through ?
as a control on the locality of influ-ence of each noun on the cluster centroids, so that as?
is raised, each noun exerts less influence on moredistant centroids and more on the nearest centroids.DA has also been applied in speech recognition.Rao and Rose (2001) used DA for supervised dis-criminative training of HMMs.
Their goal wasto optimize not likelihood but classification errorrate, a difficult objective function that is piecewise-constant (hence not differentiable everywhere) andriddled with shallow local minima.
Rao and Roseapplied DA,3 moving from training a nearly uni-form classifier with a concave cost surface (?
?
0)toward the desired deterministic classifier (?
?+?).
They reported substantial gains in spokenletter recognition accuracy over both a ML-trainedclassifier and a localized error-rate optimizer.Brown et al (1990) gradually increased learn-ing difficulty using a series of increasingly complexmodels for machine translation.
Their training al-gorithm began by running an EM approximation onthe simplest model, then used the result to initializethe next, more complex model (which had greaterpredictive power and many more parameters), andso on.
Whereas DA provides gradated difficultyin parameter search, their learning method involvesgradated difficulty among classes of models.
Thetwo are orthogonal and could be used together.3 DA with dynamic programmingWe turn now to the practical use of determinis-tic annealing in NLP.
Readers familiar with theEM algorithm will note that, for typical stochas-tic models of language structure (e.g., HMMs andSCFGs), the bulk of the computational effort is re-quired by the E step, which is accomplished bya two-pass dynamic programming (DP) algorithm(like the forward-backward algorithm).
The M stepfor these models normalizes the posterior expectedcounts from the E step to get probabilities.43With an M step modified for their objective function: it im-proved expected accuracy under p?, not expected log-likelihood.4That is, assuming the usual generative parameterization ofsuch models; if we generalize to Markov random fields (alsoknown as log-linear or maximum entropy models) the M step,while still concave, might entail an auxiliary optimization rou-tine such as iterative scaling or a gradient-based method.Running DA for such models is quite simple andrequires no modifications to the usual DP algo-rithms.
The only change to make is in the valuesof the parameters passed to the DP algorithm: sim-ply replace each ?j by ?
?j .
For a given x, the forwardpass of the DP computes (in a dense representation)Pr(y | x, ~?)
for all y.
Each Pr(y | x, ~?)
is a productof some of the ?j (each ?j is multiplied in once foreach time its corresponding model event is presentin (x, y)).
Raising the ?j to a power will also raisetheir product to that power, so the forward pass willcompute Pr(y | x, ~?)?
when given ~??
as parametervalues.
The backward pass normalizes to the sum;in this case it is the sum of the Pr(y | x, ~?)?
, andwe have the E step described in Figure 2.
We there-fore expect an EM iteration of DA to take the sameamount of time as a normal EM iteration.54 Part-of-speech taggingWe turn now to the task of inducing a trigram POStagging model (second-order HMM) from an unla-beled corpus.
This experiment is inspired by theexperiments in Merialdo (1994).
As in that work,complete knowledge of the tagging dictionary is as-sumed.
The task is to find the trigram transitionprobabilities Pr(tag i | tag i?1, tag i?2) and emis-sion probabilities Pr(word i | tag i).
Merialdo?s keyresult:6 If some labeled data were used to initializethe parameters (by taking the ML estimate), then itwas not helpful to improve the model?s likelihoodthrough EM iterations, because this almost alwayshurt the accuracy of the model?s Viterbi tagging ona held-out test set.
If only a small amount of labeleddata was used (200 sentences), then some accuracyimprovement was possible using EM, but only fora few iterations.
When no labeled data were used,EM was able to improve the accuracy of the tagger,and this improvement continued in the long term.Our replication of Merialdo?s experiment usedthe Wall Street Journal portion of the Penn Tree-bank corpus, reserving a randomly selected 2,000sentences (48,526 words) for testing.
The remain-ing 47,208 sentences (1,125,240 words) were usedin training, without any tags.
The tagging dictionarywas constructed using the entire corpus (as done byMerialdo).
To initialize, the conditional transitionand emission distributions in the HMM were set touniform with slight perturbation.
Every distributionwas smoothed using add-0.1 smoothing (at every M5With one caveat: less pruning may be appropriate becauseprobability mass is spread more uniformly over different recon-structions of the hidden data.
This paper uses no pruning.6Similar results were found by Elworthy (1994).Fig.
3:  Learning curves forEM and DA.
Steps in DA?s curvecorrespond to   ?changes.
The shape ofthe DA curve is partly a function of the an?nealing schedule, which only gradually (andaway from the uniform distribution.in steps) allows the parameters to move?40455055606570750  200  400  600  800  1000  1200%correct ambiguoustest tagsEM iterationsDAEMstep).
The criterion for convergence is that the rela-tive increase in the objective function between twoiterations fall below 10?9.4.1 ExperimentIn the DA condition, we set ?min = 0.0001, ?max =1, and ?
= 1.2.
Results for the completely unsuper-vised condition (no labeled data) are shown in Fig-ure 3 and Table 1.
Accuracy was nearly monotonic:the final model is approximately the most accurate.DA happily obtained a 10% reduction in tag er-ror rate on training data, and an 11% reduction ontest data.
On the other hand, it did not manage toimprove likelihood over EM.
So was the accuracygain mere luck?
Perhaps not.
DA may be more re-sistant to overfitting, because it may favor modelswhose posteriors p?
have high entropy.
At least inthis experiment, its initial bias toward such modelscarried over to the final learned model.7In other words, the higher-entropy local maxi-mum found by DA, in this case, explained the ob-served data almost as well without overcommit-ting to particular tag sequences.
The maximum en-tropy and latent maximum entropy principles (Wanget al, 2003, discussed in footnote 1) are best justi-fied as ways to avoid overfitting.For a supervised tagger, the maximum entropyprinciple prefers a conditional model Pr(~y | ~x) thatis maximally unsure about what tag sequence ~y toapply to the training word sequence ~x (but expectsthe same feature counts as the true ~y).
Such a modelis hoped to generalize better to unsupervised data.We can make the same argument.
But in our case,the split between supervised/unsupervised data isnot the split between training/test data.
Our super-vised data are, roughly, the fragments of the trainingcorpus that are unambiguously tagged thanks to thetag dictionary.8 The EM model may overfit some7We computed the entropy over possible tags for each wordin the test corpus, given the sentence the word occurs in.
Onaverage, the DA model had 0.082 bits per tag, while EM hadonly 0.057 bits per tag, a statistically significant difference (p <10?6) under a binomial sign test on word tokens.8Without the tag dictionary, our learners would treat the tagfinal training cross- final test cross- % correct training tags % correct test tagsE steps entropy (bits/word) entropy (bits/word) (all) (ambiguous) (all) (ambiguous)EM 279 9.136 9.321 82.04 66.61 82.08 66.63DA 1200 9.138 9.325 83.85 70.02 84.00 70.25Table 1: EM vs. DA on unsupervised trigram POS tagging, using a tag dictionary.
Each of the accuracy results is significant whenaccuracy is compared at either the word-level or sentence-level.
(Significance at p < 10?6 under a binomial sign test in eachcase.
E.g., on the test set, the DA model correctly tagged 1,652 words that EM?s model missed while EM correctly tagged 726words that DA missed.
Similarly, the DA model had higher accuracy on 850 sentences, while EM had higher accuracy on only 287.These differences are extremely unlikely to occur due to chance.)
The differences in cross-entropy, compared by sentence, weresignificant in the training set but not the test set (p < 0.01 under a binomial sign test).
Recall that lower cross entropy means higherlikelihood.parameters to these fragments.
The higher-entropyDA model may be less likely to overfit, allowing itto do better on the unsupervised data?i.e., the restof the training corpus and the entire test corpus.We conclude that DA has settled on a local maxi-mum of the likelihood function that (unsurprisingly)corresponds well with the entropy criterion, and per-haps as a result, does better on accuracy.4.2 SignificanceSeeking to determine how well this result general-ized, we randomly split the corpus into ten equally-sized, nonoverlapping parts.
EM and DA were runon each portion;9 the results were inconclusive.
DAachieved better test accuracy than EM on three often trials, better training likelihood on five trials,and better test likelihood on all ten trials.10 Cer-tainly decreasing the amount of data by an order ofmagnitude results in increased variance of the per-formance of any algorithm?so ten small corporawere not enough to determine whether to expect animprovement from DA more often than not.4.3 Mixing labeled and unlabeled data (I)In the other conditions described by Merialdo, vary-ing amounts of labeled data (ranging from 100 sen-tences to nearly half of the corpus) were used toinitialize the parameters ~?, which were then trainedusing EM on the remaining unlabeled data.
Onlyin the case where 100 labeled examples were used,and only for a few iterations, did EM improve thenames as interchangeable and could not reasonably be evalu-ated on gold-standard accuracy.9The smoothing parameters were scaled down so as to beproportional to the corpus size.10It is also worth noting that runtimes were longer with the10%-sized corpora than the full corpus (EM took 1.5 times asmany E steps; DA, 1.3 times).
Perhaps the algorithms traveledfarther to find a local maximum.
We know of no study of theeffect of unlabeled training set size on the likelihood surface,but suggest two issues for future exploration.
Larger datasetscontain more idiosyncrasies but provide a stronger overall sig-nal.
Hence, we might expect them to yield a bumpier likelihoodsurface whose local maxima are more numerous but also dif-fer more noticeably in height.
Both these tendencies of largerdatasets would in theory increase DA?s advantage over EM.accuracy of this model.
We replicated these experi-ments and compared EM with DA; DA damaged themodels even more than EM.
This is unsurprising; asnoted before, DA effectively ignores the initial pa-rameters ~?(0).
Therefore, even if initializing with amodel trained on small amounts of labeled data hadhelped EM, DA would have missed out on this ben-efit.
In the next section we address this issue.5 Skewed deterministic annealingThe EM algorithm is quite sensitive to the initial pa-rameters ~?(0).
We touted DA?s insensitivity to thoseparameters as an advantage, but in scenarios wherewell-chosen initial parameters can be provided (asin ?4.3), we wish for DA to be able exploit them.In particular, there are at least two cases where?good?
initializers might be known.
One is thecase explored by Merialdo, where some labeled datawere available to build an initial model.
The other isa situation where a good distribution is known overthe labels y; we will see an example of this in ?6.We wish to find a way to incorporate an initializerinto DA and still reap the benefit of gradated diffi-culty.
To see how this will come about, consideragain the E step for DA, which for all y:p?
(y)?Pr(x, y | ~?
)?Z ?
(~?, ?
)=Pr(x, y | ~?)?u(y)1?
?Z(~?, ?
)where u is the uniform distribution over Y andZ ?
(~?, ?)
and Z(~?, ?)
= Z ?
(~?, ?)
?
u(y)1??
are nor-malizing terms.
(Note that Z(~?, ?)
does not dependon y because u(y) is constant with respect to y.)
Ofcourse, when ?
is close to 0, DA chooses the uni-form posterior because it has the highest entropy.Seen this way, DA is interpolating in the log do-main between two posteriors: the one given by yand ~?
and the uniform one u; the interpolation coef-ficient is ?.
To generalize DA, we will replace theuniform u with another posterior, the ?skew?
pos-terior p?, which is an input to the algorithm.
Thisposterior might be specified directly, as it will be in?6, or it might be computed using an M step fromsome good initial ~?
(0).The skewed DA (SDA) E step is given by:p?(y)?1Z(?
)Pr(x, y | ?)?
p?(y)1??
(3)When ?
is close to 0, the E step will choose p?
tobe very close to p?.
With small ?, SDA is a ?cau-tious?
EM variant that is wary of moving too farfrom the initializing posterior p?
(or, equivalently, theinitial parameters ~?(0)).
As ?
approaches 1, the ef-fect of p?
will diminish, and when ?
= 1, the algo-rithm becomes identical to EM.
The overall objec-tive (matching (2) except for the boxed term) is:F?
(~?, p?, ?)=1?H(p?)
+Ep?
(~Y )[log Pr(~x, ~Y | ~?
)]+ 1?
??Ep?
(~Y )[log p?
(~Y)]Mixing labeled and unlabeled data (II) Return-ing to Merialdo?s mixed conditions (?4.3), we foundthat SDA repaired the damage done by DA but didnot offer any benefit over EM.
Its behavior in the100-labeled sentence condition was similar to thatof EM?s, with a slightly but not significantly higherpeak in training set accuracy.
In the other condi-tions, SDA behaved like EM, with steady degrada-tion of accuracy as training proceeded.
It ultimatelydamaged performance only as much as EM did ordid slightly better than EM (but still hurt).This is unsurprising: Merialdo?s result demon-strated that ML and maximizing accuracy are gener-ally not the same; the EM algorithm consistently de-graded the accuracy of his supervised models.
SDAis simply another search algorithm with the samecriterion as EM.
SDA did do what it was expectedto do?it used the initializer, repairing DA damage.6 Grammar inductionWe turn next to the problem of statistical grammarinduction: inducing parse trees over unlabeled text.An excellent recent result is by Klein and Manning(2002).
The constituent-context model (CCM) theypresent is a generative, deficient channel model ofPOS tag strings given binary tree bracketings.
Wefirst review the model and describe a small mod-ification that reduces the deficiency, then compareboth models under EM and DA.6.1 Constituent-context modelLet (x, y) be a (tag sequence, binary tree) pair.
xjidenotes the subsequence of x from the ith to thejth word.
Let yi,j be 1 if the yield from i to j is aconstituent in the tree y and 0 if it is not.
The CCMgives to a pair (x, y) the following probability:Pr(x, y) = Pr(y) ??1?i?j?|x|[?(xji???
yi,j)?
?
(xi?1, xj+1| yi,j)]where ?
is a conditional distribution over possi-ble tag-sequence yields (given whether the yield isa constituent or not) and ?
is a conditional distribu-tion over possible contexts of one tag on either sideof the yield (given whether the yield is a constituentor not).
There are therefore four distributions to beestimated; Pr(y) is taken to be uniform.The model is initialized using expected counts ofthe constituent and context features given that allthe trees are generated according to a random-splitmodel.11The CCM generates each tag not once but O(n2)times, once by every constituent or non-constituentspan that dominates it.
We suggest the followingmodification to alleviate some of the deficiency:Pr(x, y) = Pr(y) ??1?i?j?|x|[?(xji???
yi,j , j ?
i+ 1)??
(xi?1, xj+1| yi,j)]The change is to condition the yield feature ?
onthe length of the yield.
This decreases deficiency bydisallowing, for example, a constituent over a four-tag yield to generate a seven-tag sequence.
It alsodecreases inter-parameter dependence by breakingthe constituent (and non-constituent) distributionsinto a separate bin for each possible constituentlength.
We will refer to Klein and Manning?s CCMand our version as models 1 and 2, respectively.6.2 ExperimentWe ran experiments using both CCM models onthe tag sequences of length ten or less in the WallStreet Journal Penn Treebank corpus, after extract-ing punctuation.
This corpus consists of 7,519 sen-tences (52,837 tag tokens, 38 types).
We reportPARSEVAL scores averaged by constituent (ratherthan by sentence), and do not give the learner creditfor getting full sentences or single tags as con-stituents.12 Because the E step for this model iscomputationally intensive, we set the DA parame-ters at ?min = 0.01, ?
= 1.5 so that fewer E stepswould be necessary.13 The convergence criterionwas relative improvement < 10?9 in the objective.The results are shown in Table 2.
The first pointto notice is that a uniform initializer is a bad idea,as Klein and Manning predicted.
All conditions but11We refer readers to Klein and Manning (2002) or Coverand Thomas (1991, p. 72) for details; computing expectedcounts for a sentence is a closed form operation.
Klein andManning?s argument for this initialization step is that it is lessbiased toward balanced trees than the uniform model used dur-ing learning; we also found that it works far better in practice.12This is why the CCM 1 performance reported here differsfrom Klein and Manning?s; our implementation of the EM con-dition gave virtually identical results under either evaluationscheme (D. Klein, personal communication).13A pilot study got very similar results for ?min = 10?6.E steps cross-entropy (bits/tag) UR UP F CBCCM 1 EM (uniform) 146 103.1654 61.20 45.62 52.27 1.69DA 403 103.1542 55.13 41.10 47.09 1.91EM (split) 124 103.1951 78.14 58.24 66.74 0.98SDA (split) 339 103.1651 62.71 46.75 53.57 1.62CCM 2 EM (uniform) 26 84.8106 57.60 42.94 49.20 1.86DA 331 84.7899 40.81 30.42 34.86 2.66EM (split) 44 84.8049 78.56 58.56 67.10 0.98SDA (split) 290 84.7940 79.64 59.37 68.03 0.93Table 2: The two CCM models, trained with two unsupervised algorithms, each with two initializers.
Note that DA is equivalentto SDA initialized with a uniform distribution.
The third line corresponds to the setup reported by Klein and Manning (2002).UR is unlabeled recall, UP is unlabeled precision, F is their harmonic mean, and CB is the average number of crossing bracketsper sentence.
All evaluation is on the same data used for unsupervised learning (i.e., there is no training/test split).
The highcross-entropy values arise from the deficiency of models 1 and 2, and are not comparable across models.one find better structure when initialized with Kleinand Manning?s random-split model.
(The exceptionis SDA on model 1; possibly the high deficiency ofmodel 1 interacts poorly with SDA?s search in someway.
)Next we note that with the random-split initial-izer, our model 2 is a bit better than model 1 onPARSEVAL measures and converges more quickly.Every instance of DA or SDA achieved higherlog-likelihood than the corresponding EM condi-tion.
This is what we hoped to gain from annealing:better local maxima.
In the case of model 2 withthe random-split initializer, SDA significantly out-performed EM (comparing both matches and cross-ing brackets per sentence under a binomial sign test,p < 10?6); we see a > 5% reduction in averagecrossing brackets per sentence.
Thus, our strategyof using DA but modifying it to accept an initial-izer worked as desired in this case, yielding our bestoverall performance.The systematic results we describe next suggestthat these patterns persist across different trainingsets in this domain.6.3 SignificanceThe difficulty we experienced in finding generaliza-tion to small datasets, discussed in ?4.2, was appar-ent here as well.
For 10-way and 3-way random,nonoverlapping splits of the dataset, we did not haveconsistent results in favor of either EM or SDA.
In-terestingly, we found that training model 2 (usingEM or SDA) on 10% of the corpus resulted on av-erage in models that performed nearly as well ontheir respective training sets as the full corpus con-dition did on its training set; see Table 3.
In ad-dition, SDA sometimes performed as well as EMunder model 1.
For a random two-way split, EMand SDA converged to almost identical solutions onone of the sub-corpora, and SDA outperformed EMsignificantly on the other (on model 2).In order to get multiple points of comparison ofEM and SDA on this task with a larger amount ofdata, we jack-knifed the WSJ-10 corpus by split-ting it randomly into ten equally-sized nonoverlap-ping parts then training models on the corpus witheach of the ten sub-corpora excluded.14 These trialsare not independent of each other; any two of thesub-corpora have 89 of their training data in com-mon.
Aggregate results are shown in Table 3.
Usingmodel 2, SDA always outperformed EM, and in 8 of10 cases the difference was significant when com-paring matching constituents per sentence (7 of 10when comparing crossing constituents).15 The vari-ance of SDA was far less than that of EM; SDA notonly always performed better with model 2, but itsperformance was more consistent over the trials.We conclude this experimental discussion by cau-tioning that both CCM models are highly deficientmodels, and it is unknown how well they generalizeto corpora of longer sentences, other languages, orcorpora of words (rather than POS tags).7 Future workThere are a number of interesting directions for fu-ture work.
Noting the simplicity of the DA algo-rithm, we hope that current devotees of EM willrun comparisons of their models with DA (or SDA).Not only might this improve performance of exist-14Note that this is not a cross-validation experiment; resultsare reported on the unlabeled training set, and the excluded sub-corpus remains unused.15Binomial sign test, with significance defined as p < 0.05,though all significant results had p < 0.001.10% corpus 90% corpus?F ?F ?F ?FCCM 1 EM 65.00 1.091 66.12 0.6643SDA 63.00 4.689 53.53 0.2135CCM 2 EM 66.74 1.402 67.24 0.7077SDA 66.77 1.034 68.07 0.1193Table 3: The mean ?
and standard deviation ?
of F -measureperformance for 10 trials using 10% of the corpus and 10 jack-knifed trials using 90% of the corpus.ing systems, it will contribute to the general under-standing of the likelihood surface for a variety ofproblems (e.g., this paper has raised the question ofhow factors like dataset size and model deficiencyaffect the likelihood surface).DA provides a very natural way to graduallyintroduce complexity to clustering models (Roseet al, 1990; Pereira et al, 1993).
This comes aboutby manipulating the ?
parameter; as it rises, thenumber of effective clusters is allowed to increase.An open question is whether the analogues of ?clus-ters?
in tagging and parsing models?tag symbolsand grammatical categories, respectively?might betreated in a similar manner under DA.
For instance,we might begin with the CCM, the original formula-tion of which posits only one distinction about con-stituency (whether a span is a constituent or not) andgradually allow splits in constituent-label space, re-sulting in multiple grammatical categories that, wehope, arise naturally from the data.In this paper, we used ?max = 1.
It wouldbe interesting to explore the effect on accuracy of?quenching,?
a phase at the end of optimizationthat rapidly raises ?
from 1 to the winner-take-all(Viterbi) variant at ?
= +?.Finally, certain practical speedups may be possi-ble.
For instance, increasing ?min and ?, as notedin ?2.2, will vary the number of E steps required forconvergence.
We suggested that the change mightresult in slower or faster convergence; optimizingthe schedule using an online algorithm (or deter-mining precisely how these parameters affect theschedule in practice) may prove beneficial.
Anotherpossibility is to relax the convergence criterion forearlier ?
values, requiring fewer E steps before in-creasing ?, or even raising ?
slightly after every Estep (collapsing the outer and inner loops).8 ConclusionWe have reviewed the DA algorithm, describingit as a generalization of EM with certain desir-able properties, most notably the gradual increaseof difficulty of learning and the ease of imple-mentation for NLP models.
We have shown howDA can be used to improve the accuracy of a tri-gram POS tagger learned from an unlabeled cor-pus.
We described a potential shortcoming of DAfor NLP applications?its failure to exploit goodinitializers?and then described a novel algorithm,skewed DA, that solves this problem.
Finally, we re-ported significant improvements to a state-of-the-artgrammar induction model using SDA and a slightmodification to the parameterization of that model.These results support the case that annealing tech-niques in some cases offer performance gains overthe standard EM approach to learning from unla-beled corpora, particularly with large corpora.AcknowledgementsThis work was supported by a fellowship to the first au-thor from the Fannie and John Hertz Foundation, andby an NSF ITR grant to the second author.
The viewsexpressed are not necessarily endorsed by the sponsors.The authors thank Shankar Kumar, Charles Schafer,David Smith, and Roy Tromble for helpful commentsand discussions; three ACL reviewers for advice that im-proved the paper; Eric Goldlust for keeping the Dynacompiler (Eisner et al, 2004) up to date with the de-mands made by this work; and Dan Klein for sharingdetails of his CCM implementation.ReferencesP.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Je-linek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990.A statistical approach to machine translation.
ComputationalLinguistics, 16(2):79?85.E.
Charniak.
1993.
Statistical Language Learning.
MIT Press.M.
Collins and Y.
Singer.
1999.
Unsupervised models fornamed-entity classification.
In Proc.
of EMNLP.T.
M. Cover and J.
A. Thomas.
1991.
Elements of InformationTheory.
John Wiley and Sons.S.
Cucerzan and D. Yarowsky.
2003.
Minimally supervisedinduction of grammatical gender.
In Proc.
of HLT/NAACL.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maximum likeli-hood estimation from incomplete data via the EM algorithm.Journal of the Royal Statistical Society B, 39:1?38.J.
Eisner, E. Goldlust, and N. A. Smith.
2004.
Dyna: A declar-ative language for implementing dynamic programs.
In Proc.of ACL (companion volume).D.
Elworthy.
1994.
Does Baum-Welch re-estimation help tag-gers?
In Proc.
of ANLP.S.
Kirkpatrick, C. D. Gelatt, and M. P. Vecchi.
1983.
Optimiza-tion by simulated annealing.
Science, 220:671?680.D.
Klein and C. D. Manning.
2002.
A generative constituent-context model for grammar induction.
In Proc.
of ACL.B.
Merialdo.
1994.
Tagging English text with a probabilisticmodel.
Computational Linguistics, 20(2):155?72.R.
Neal and G. Hinton.
1998.
A view of the EM algorithmthat justifies incremental, sparse, and other variants.
In M. I.Jordan, editor, Learning in Graphical Models.
Kluwer.F.
C. N. Pereira, N. Tishby, and L. Lee.
1993.
Distributionalclustering of English words.
In Proc.
of ACL.A.
Rao and K. Rose.
2001.
Deterministically annealed designof Hidden Markov Model speech recognizers.
IEEE Transac-tions on Speech and Audio Processing, 9(2):111?126.K.
Rose, E. Gurewitz, and G. C. Fox.
1990.
Statistical me-chanics and phase transitions in clustering.
Physical ReviewLetters, 65(8):945?948.K.
Rose.
1998.
Deterministic annealing for clustering, com-pression, classification, regression, and related optimizationproblems.
Proc.
of the IEEE, 86(11):2210?2239.N.
Ueda and R. Nakano.
1998.
Deterministic annealing EMalgorithm.
Neural Networks, 11(2):271?282.S.
Wang, D. Schuurmans, and Y. Zhao.
2003.
The latent maxi-mum entropy principle.
In review.D.
Yarowsky.
1995.
Unsupervised word sense disambiguationrivaling supervised methods.
In Proc.
of ACL.
