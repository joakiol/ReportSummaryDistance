Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1499?1510,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsModeling Biological Processes for Reading ComprehensionJonathan Berant?, Vivek Srikumar?, Pei-Chun Chen, Brad Huang and Christopher D. ManningStanford University, StanfordAbby Vander Linden and Brittany HardingUniversity of Washington, SeattleAbstractMachine reading calls for programs thatread and understand text, but most currentwork only attempts to extract facts fromredundant web-scale corpora.
In this pa-per, we focus on a new reading compre-hension task that requires complex reason-ing over a single document.
The input isa paragraph describing a biological pro-cess, and the goal is to answer questionsthat require an understanding of the re-lations between entities and events in theprocess.
To answer the questions, we firstpredict a rich structure representing theprocess in the paragraph.
Then, we mapthe question to a formal query, which isexecuted against the predicted structure.We demonstrate that answering questionsvia predicted structures substantially im-proves accuracy over baselines that useshallower representations.1 IntroductionThe goal of machine reading is to develop pro-grams that read text to learn about the worldand make decisions based on accumulated knowl-edge.
Work in this field has focused mostly onmacro-reading, i.e., processing large text collec-tions and extracting knowledge bases of facts (Et-zioni et al., 2006; Carlson et al., 2010; Fader et al.,2011).
Such methods rely on redundancy, and arethus suitable for answering common factoid ques-tions which have ample evidence in text (Fader etal., 2013).
However, reading a single document(micro-reading) to answer comprehension ques-tions that require deep reasoning is currently be-yond the scope of state-of-the-art systems.In this paper, we introduce a task where givena paragraph describing a process, the goal is to?Both authors equally contributed to the paper.answer reading comprehension questions that testunderstanding of the underlying structure.
In par-ticular, we consider processes in biology text-books such as this excerpt and the question thatfollows:?.
.
.
Water is split, providing a source of elec-trons and protons (hydrogen ions, H+) and giv-ing off O2as a by-product.
Light absorbed bychlorophyll drives a transfer of the electronsand hydrogen ions from water to an acceptorcalled NADP+.
.
.
?Q What can the splitting of water lead to?a Light absorptionb Transfer of ionsThis excerpt describes a process in which a com-plex set of events and entities are related to oneanother.
A system trying to answer this ques-tion must extract a rich structure spanning multi-ple sentences and reason that water splitting com-bined with light absorption leads to transfer ofions.
Note that shallow methods, which rely onlexical overlap or text proximity, will fail.
Indeed,both answers are covered by the paragraph and thewrong answer is closer in the text to the question.We propose a novel method that tackles thischallenging problem (see Figure 1).
First, we traina supervised structure predictor that learns to ex-tract entities, events and their relations describingthe biological process.
This is a difficult prob-lem because events have complex interactions thatspan multiple sentences.
Then, treating this struc-ture as a small knowledge-base, we map ques-tions to formal queries that are executed againstthe structure to provide the answer.Micro-reading is an important aspect of naturallanguage understanding (Richardson et al., 2013;Kushman et al., 2014).
In this work, we focusspecifically on modeling processes, where eventsand entities relate to one another through com-plex interactions.
While we work in the biology1499?.
.
.
Water is split, providing a source of elec-trons and protons (hydrogen ions, H+) andgiving off O2as a by-product.
Light ab-sorbed by chlorophyll drives a transfer ofthe electrons and hydrogen ions from waterto an acceptor called NADP+ .
.
.
?Q What can the splitting of water lead to?a Light absorptionb Transfer of ionswatersplitTHEMEabsorblightTHEMEtransfer ionsTHEMEENABLECAUSEwatersplitabsorblightTHEME(CAUSE|ENABLE)+THEMEwatersplittransfer ionsTHEME(CAUSE|ENABLE)+THEMEStep 1Step 2Step 3: Answer = bFigure 1: An overview of our reading comprehension system.
First, we predict a structure from the input paragraph (thetop right portion shows a partial structure skipping some arguments for brevity).
Circles denote events, squares denote argu-ments, solid arrows represent event-event relations, and dashed arrows represent event-argument relations.
Second, we mapthe question paired with each answer into a query that will be answered using the structure.
The bottom right shows the queryrepresentation.
Last, the two queries are executed against the structure, and a final answer is returned.domain, processes are abundant in domains suchas chemistry, economics, manufacturing, and eveneveryday events like shopping or cooking, and ourmodel can be applied to these domains as well.The contributions of this paper are:1.
We propose a reading comprehension taskwhich requires deep reasoning over struc-tures that represent complex relations be-tween multiple events and entities.2.
We present PROCESSBANK, a new datasetconsisting of descriptions of biological pro-cesses, fully-annotated with rich processstructures, and accompanied by multiple-choice questions.3.
We present a novel method for answer-ing questions, by predicting process struc-tures and mapping questions to queries.
Wedemonstrate that by predicting structures wecan improve reading comprehension accu-racy over baselines that do not exploit the un-derlying structure.The data and code for this paper are avail-able at http://www-nlp.stanford.edu/software/bioprocess.2 Task Definition and SetupThis section describes the reading comprehensiontask we address and the accompanying dataset.We will use the example in Figure 1 as our run-ning example throughout the paper.Our goal is to tackle a complex reading com-prehension setting that centers on understandingthe underlying meaning of a process description.We target a multiple-choice setting in which eachinput consists of a paragraph of text describing abiological process, a question, and two possibleanswers.
The goal is to identify the correct answerusing the text (Figure 1, left).
We used the 148paragraphs from the textbook Biology (Campbelland Reece, 2005) that were manually identified byScaria et al.
(2013).
We extended this set to 200paragraphs by including additional paragraphs thatdescribe biological processes.
Each paragraph inthe collection represents a single biological pro-cess and describes a set of events, their partici-pants and their interactions.Because we target understanding of paragraphmeaning, we use the following desiderata forbuilding the corpus of questions and answers:1.
The questions should focus on the events andentities participating in the process describedin the paragraph, and answering the questionsshould require reasoning about the relationsbetween those events and entities.2.
Both answers should have similar lexicaloverlap with the paragraph.
Moreover, namesof entities and events in the question and an-swers should appear as in the paragraph andnot using synonyms.
This is to ensure that thetask revolves around reading comprehensionrather than lexical variability.1A biologist created the question-answer part of1Lexical variability is an important problem in NLP, butis not the focus of this task.1500the corpus comprising of 585 questions spreadover the 200 paragraphs.
A second annotator val-idated 326 randomly chosen questions and agreedon the correct answer with the first annotator in98.1% of cases.
We provide the annotation guide-lines in the supplementary material.Figure 1 (left) shows an excerpt of a paragraphdescribing a process and an example of a ques-tion based on it.
In general, questions test an un-derstanding of the interactions between multipleevents (such as causality, inhibition, temporal or-dering), or between events and entities (i.e., rolesof entities in events), and require complex reason-ing about chains of event-event and event-entityrelations.3 The Structure of ProcessesA natural first step for answering reading compre-hension questions is to identify a structured rep-resentation of the text.
In this section, we definethis structure.
We broadly follow the definition ofScaria et al.
(2013), but modify important aspects,highlighted at the end of this section.A paragraph describing a process is a sequenceof tokens that describes events, entities and theirrelations (see Figure 1, top right).
A process isa directed graph (T ,A, Ett, Eta), where the nodesT are labeled event triggers, the nodes A are ar-guments, Ettare labeled edges describing event-event relations, and Etaare labeled edges fromtriggers to arguments denoting semantic roles (seeFigure 1 top right for a partial structure of the run-ning example).
The goal of process extraction isto generate the process graph given the input para-graph.Triggers and arguments A trigger is a tokenspan denoting the occurrence of an event.
In Fig-ure 1, split, absorbed and transfer are event trig-gers.
In rare cases, a trigger denotes the non-occurrence of an event.
For example, in ?sym-patric speciation can occur when gene flow isblocked?, sympatric speciation occurs if gene flowdoes not happen.
Thus, nodes in T are labeled aseither a T-YES or T-NO to distinguish triggers ofevents that occur from triggers of events that donot occur.
Arguments are token spans denotingentities that participate in the process (such as wa-ter, light and ions in Figure 1).Semantic roles The edges Etafrom triggersto arguments are labeled by the semantic rolesAGENT, THEME, SOURCE, DESTINATION, LO-CATION, RESULT, and OTHER for all other roles.Our running example shows three THEME seman-tic roles for the three triggers.
For brevity, the fig-ure does not show the RESULT of the event split,namely, both source of electrons and protons (hy-drogen ions, H+) and O2.Event-event relations The directed edges Ettbetween triggers are labeled by one of eight pos-sible event-event relations.
These relations arecentral to answering reading comprehension ques-tions, which test understanding of the depen-dencies and causal relations between the processevents.
We first define three relations that expressa dependency between two event triggers u and v.1.
CAUSE denotes that u starts before v, and ifu happens then v happens (Figure 1).2.
ENABLE denotes that u creates conditionsnecessary for the occurrence of v. Thismeans that u starts before v and v can onlyhappen if u happens (Figure 1).23.
PREVENT denotes that u starts before v andif u happens, then v does not happen.In processes, events sometimes depend on morethan one other event.
For example, in Figure 1(right top) transfer of ions depends on both watersplitting as well as light absorption.
Conversely,in Figure 2, the shifting event results in either oneof two events but not both.
To express both con-junctions and disjunctions of related events weadd the relations CAUSE-OR, ENABLE-OR andPREVENT-OR, which express disjunctions, whilethe default CAUSE, ENABLE, and PREVENT ex-press conjunction (Compare the CAUSE-OR rela-tions in Figure 2 with the relations in Figure 1).We define the SUPER relation to denote thatevent u is part of event v. (In Figure 2, slip-page is a sub-event of replication.)
Last, we usethe event coreference relation SAME to denote twoevent mentions referring to the same event.Notice that the assignments of relation labels in-teract across different pairs of events.
As an ex-ample, if event u causes event v, then v can notcause u.
Our inference algorithm uses such struc-tural constraints when predicting process structure(Section 4).2In this work, we do not distinguish causation from facil-itation, where u can help v but is not absolutely required.
Weinstructed the annotators to ignore the inherent uncertainty inthese cases and use CAUSE.1501Figure 2: Partial example of a process, as annotated in our dataset.Avg Min Max# of triggers 7.0 2 18# of arguments 11.3 1 36# of relation 7.9 1 37Table 1: Statistics of triggers, arguments and rela-tions over the 200 annotated paragraphs.Three biologists annotated the same 200 para-graphs described in Section 2 using the brat anno-tation tool (Stenetorp et al., 2012).
For each para-graph, one annotator annotated the process, anda second validated its correctness.
Importantly,the questions and answers were authored sepa-rately by a different annotator, thus ensuring thatthe questions and answers are independent fromthe annotated structures.
Table 1 gives statisticsover the dataset.
The annotation guidelines are in-cluded in the supplementary material.Relation to Scaria et al.
(2013) Scaria et al.
(2013) also defined processes as graphs wherenodes are events and edges describe event-eventrelations.
Our definition differs in a few importantaspects.First, the set of event-event relations in thatwork included temporal relations in addition tocausal ones.
In this work, we posit that becauseevents in a process are inter-related, causal depen-dencies are sufficient to capture the relevant tem-poral ordering between them.
Figure 1 illustratesthis phenomenon, where the temporal ordering be-tween the events of water splitting and light ab-sorption is unspecified.
It does not matter whetherone happens before, during, or after the other.
Fur-thermore, the incoming causal links to transfer im-ply that the event should happen after splitting andabsorption.A second difference is that Scaria et al.
(2013)do not include disjunctions and conjunctions ofevents in their formulation.
Last, Scaria et al.
(2013) predict only relations given input triggers,while we predict a full process structure.4 Predicting Process StructuresWe now describe the first step of our algorithm.Given an input paragraph we predict events, theirarguments and event-event relations (Figure 1,top).
We decompose this into three sub-problems:1.
Labeling trigger candidates using a multi-class classifier (Section 4.1).2.
For each trigger, identifying an over-complete set of possible arguments, using aclassifier tuned for high recall (Section 4.2).3.
Jointly assigning argument labels and rela-tion labels for all trigger pairs (Section 4.3).The event-event relations CAUSE, ENABLE,CAUSE-OR and ENABLE-OR, form a semanticcluster: If (u, v) is labeled by one of these, thenthe occurrence of v depends on the occurrence ofu.
Since our dataset is small, we share statistics bycollapsing all four labels to a single ENABLE la-bel.
Similarly, we collapse the PREVENT andPREVENT-OR labels, overall reducing the numberof relations to four.For brevity, in what follows we only providea flavor of the features we extract, and refer thereader to the supplementary material for details.4.1 Predicting Event TriggersThe first step is to identify the events in the pro-cess.
We model the trigger detector as a multi-class classifier that labels all content words inthe paragraph as one of T-YES, T-NO or NOT-TRIGGER (Recall that a word can trigger an eventthat occurred, an event that did not occur, or notbe a trigger at all).
For simplicity, we model trig-gers as single words, but in the gold annotationabout 14% are phrases (such as gene flow).
Thus,we evaluate trigger prediction by taking heads ofgold phrases.
To train the classifier, we extract1502the lemma and POS tag of the word and adja-cent words, dependency path to the root, POStag of children and parent in the dependency tree,and clustering features from WordNet (Fellbaum,1998), Nomlex (Macleod et al., 1998), Levin verbclasses (Levin, 1993), and a list of biological pro-cesses compiled from Wikipedia.4.2 Filtering Argument CandidatesLabeling trigger-argument edges is similar to se-mantic role labeling.
Following the standard ap-proach (Punyakanok et al., 2008), for each triggerwe collect all constituents in the same sentence tobuild an over-complete set of plausible candidatearguments.
This set is pruned with a binary classi-fier that is tuned for high recall (akin to the argu-ment identifier in SRL systems).
On the develop-ment set we filter more than half of the argumentcandidates, while achieving more than 99% recall.This classifier is trained using argument identifica-tion features from Punyakanok et al.
(2008).At the end of this step, each trigger has a set ofcandidate arguments which will be labeled duringjoint inference.
In further discussion, the argumentcandidates for trigger t are denoted by At.4.3 Predicting Arguments and RelationsGiven the output of the trigger classifier, our goalis to jointly predict event-argument and event-event relations.
We model this as an integer linearprogram (ILP) instance described below.
We firstdescribe the inference setup assuming a model thatscores inference decisions and defer description oflearning to Section 4.4.
The ILP has two types ofdecision variables: arguments and relations.Argument variables These variables capturethe decision that a candidate argument a, belong-ing to the set Atof argument candidates, takes alabel A (from Section 3).
We denote the Booleanvariables by yt,a,A, which are assigned a scorebt,a,Aby the model.
We include an additional labelNULL-ARG, indicating that the candidate is not anargument for the trigger.Event-event relation variables These variablescapture the decision that a pair of triggers t1andt2are connected by a directed edge (t1, t2) labeledby the relation R. We denote these variables byzt1,t2,R, which are associated with a score ct1,t2,R.Again, we introduce a label NULL-REL to indicatetriggers that are not connected by an edge.Name DescriptionUnique labels Every argument candidate and trigger pair has ex-actly one label.Argument overlap Two arguments of the same trigger cannot overlap.Relation symmetry The SAME relation is symmetric.
All other rela-tions are anti-symmetric, i.e., for any relation la-bel other than SAME, at most one of (ti, tj) or(tj, ti) can take that label and the other is assignedthe label NULL-REL.Max arguments pertriggerEvery trigger can have no more than two argumentswith the same label.Max triggers per ar-gumentThe same span of text can not be an argument formore than two triggers.Connectivity The triggers must form a connected graph, framedas flow constraints as in Magnanti and Wolsey(1995) and Martins et al.
(2009).Shared arguments If the same span of text is an argument of two trig-gers, then the triggers must be connected by a rela-tion that is not NULL-REL.
This ensures that trig-gers that share arguments are related.Unique parent For any trigger, at most one outgoing edge can belabeled SUPER.Table 2: Constraints for joint inference.Formulation Given the two sets of variables,the objective of inference is to find a global as-signment that maximizes the score.
That is, theobjective can be stated as follows:maxy,z?t,a?At,Abt,a,A?
yt,a,A+?t1,t2,Rct1,t2,R?
zt1,t2,RHere, y and z refer to all the argument and rela-tion variables respectively.Clearly, all possible assignments to the infer-ence variables are not feasible and there are bothstructural as well as prior knowledge constraintsover the output space.
Table 2 states the con-straints we include, which are expressed as linearinequalities over output variables using standardtechniques (e.g., (Roth and Yih, 2004)).4.4 Learning in the Joint ModelWe train both the trigger classifier and the argu-ment identifier using L2-regularized logistic re-gression.
For the joint model, we use a linearmodel for the scoring functions, and train jointlyusing the structured averaged perceptron algo-rithm (Collins, 2002).Since argument labeling is similar to semanticrole labeling (SRL), we extract standard SRL fea-tures given the trigger and argument from the syn-tactic tree for the corresponding sentence.
In ad-dition, we add features extracted from an off-the-shelf SRL system.
We also include all feature con-junctions.
For event relations, we include the fea-tures described in Scaria et al.
(2013), as well ascontext features for both triggers, and the depen-dency path between them, if one exists.15035 Question Answering via StructuresThis section describes our question answering sys-tem that, given a process structure, a question andtwo answers, chooses the correct answer (steps 2and 3 in Figure 1).Our strategy is to treat the process structure asa small knowledge-base.
We map each answeralong with the question into a structured query thatwe compare against the structure.
The query canprove either the correctness or incorrectness of theanswer being considered.
That is, either we get avalid match for an answer (proving that the cor-responding answer is correct), or we get a refu-tation in the form of a contradicted causal chain(thus proving that the other answer is correct).This is similar to theorem proving approaches sug-gested in the past for factoid question answering(Moldovan et al., 2003).The rest of this section is divided into threeparts: Section 5.1 defines the queries we use, Sec-tion 5.2 describes a rule-based algorithm for con-verting a question and an answer into a query andfinally, 5.3 describes the overall algorithm.5.1 Queries over ProcessesWe model a query as a directed graph path withregular expressions over edge labels.
The bot-tom right portion of Figure 1 shows examples ofqueries for our running example.
In general, givena question and one of the answer candidates, oneend of the path is populated by a trigger/argumentfound in the question and the other is populatedwith a trigger/ argument from the answer.We define a query to consist of three parts:1.
A regular expression over relation labels, de-scribing permissible paths,2.
A source trigger/argument node, and3.
A target trigger/argument node.For example, the bottom query in Figure 1 looksfor paths labeled with CAUSE or ENABLE edgesfrom the event split to the event transfer.Note that the representation of questions as di-rected paths is a modeling choice and did not influ-ence the authoring of the questions.
Indeed, whilemost questions do fit this model, there are rarecases that require a more complex query structure.5.2 Query GenerationMapping a question and an answer into a queryinvolves identifying the components of the querylisted above.
We do this in two phases: (1) In thealignment phase, we align triggers and argumentsin the question and answer to the process structureto give us candidate source and target nodes.
(2)In the query construction phase, we identify theregular expression and the direction of the queryusing the question, the answer and the alignment.We identify three broad categories of QA pairs(see Table 3) that can be identified using simplelexical rules: (a) Dependency questions ask whichevent or argument depends on another event or ar-gument, (b) Temporal questions ask about tempo-ral ordering of events, and (c) True-false questionsask whether some fact is true.
Below, we describethe two phases of query generation primarily in thecontext of dependency questions with a brief dis-cussion about temporal and true-false questions atthe end of the section.Alignment Phase We align triggers in the struc-ture to the question and the answer by matchinglemmas or nominalizations.
In case of multiplematches, we use the context to disambiguate andresolve ties using the highest matching candidatein the syntactic dependency tree.We align arguments in the question and the an-swer in a similar manner.
Since arguments aretypically several words long, we prefer maximalspans.
Additionally, if a question (or an answer)contains an aligned trigger, we prefer to alignwords to its arguments.Query Construction Phase We construct aquery using the aligned question and answer trig-gers/arguments.
We will explain query construc-tion using our running example (reproduced as thedependency question in Table 3).First, we identify the source and the target ofthe query.
We select either the source or the tar-get to be a question node and populate the otherend of the query path with an answer node.
Tomake the choice between source or target for thequestion node, we use the main verb in the ques-tion, its voice and relative position of the questionword with respect to the main verb.
In our exam-ple, the main verb lead to is in active voice and thequestion word what is not in subject position.
Thisplaces the trigger from the question as the sourceof the query path (see both queries in the bottomright portion of the running example).
In contrast,had the verb been require, the trigger would be thetarget of the query.
We construct two verb clustersthat indicate query direction using a small seed set1504Type Example # (%)Dependency Q: What can the splitting of water lead to?
407 (69.57%)a: Light absorptionb: Transfer of ionsTemporal Q: What is the correct order of events?
57 (9.74%)a: PDGF binds to tyrosine kinases, then cells divide, then wound healingb: Cells divide, then PDGF binds to tyrosine kinases, then wound healingTrue-False Q: Cdk associates with MPF to become cyclin 121 (20.68%)a: Trueb: FalseTable 3: Examples and statistics for each of the three coarse types of questions.Is main verb trigger?Condition Regular Exp.Wh- word subjective?
AGENTWh- word object?
THEMECondition Regular Exp.default (ENABLE|SUPER)+DIRECT (ENABLE|SUPER)PREVENT (ENABLE|SUPER)?PREVENT(ENABLE|SUPER)?Yes NoFigure 3: Rules for determining the regular expressions for queries concerning two triggers.
In each table, the conditioncolumn decides the regular expression to be chosen.
In the left table, we make the choice based on the path from the root tothe Wh- word in the question.
In the right table, if the word directly modifies the main trigger, the DIRECT regular expressionis chosen.
If the main verb in the question is in the synset of prevent, inhibit, stop or prohibit, we select the PREVENT regularexpression.
Otherwise, the default one is chosen.
We omit the relation label SAME from the expressions, but allow goingthrough any number of edges labeled by SAME when matching expressions to the structure.that we expand using WordNet.The final step in constructing the query is toidentify the regular expression for the path con-necting the source and the target.
Due to paucityof data, we do not map a question and an answerto arbitrary regular expressions.
Instead, we con-struct a small set of regular expressions, and builda rule-based system that selects one.
We used thetraining set to construct the regular expressionsand we found that they answer most questions (seeSection 6.4).
We determine the regular expressionbased on whether the main verb in the sentence isa trigger and whether the source and target of thepath are triggers or arguments.
Figure 3 shows thepossible regular expressions and the procedure forchoosing one when both the source and target aretriggers.
If either of them are argument nodes, weappend the appropriate semantic role to the regu-lar expression, based on whether the argument isthe source or the target of the path (or both).True-false questions are treated similarly, ex-cept that both source and target are chosen fromthe question.
For temporal questions, we seek toidentify the ordering of events in the answers.
Weuse the keywords first, then, or simultaneously toidentify the implied order in the answer.
We usethe regular expression SUPER+for questions ask-ing about simultaneous events and ENABLE+forthose asking about sequential events.5.3 Answering QuestionsWe match the query of an answer to the processstructure to identify the answer.
In case of a match,the corresponding answer is chosen.
The matchingpath can be thought of as a proof for the answer.If neither query matches the graph (or both do),we check if either answer contradicts the struc-ture.
To do so, we find an undirected path fromthe source to the target.
In the event of a match, ifthe matching path traverses any ENABLE edge inthe incorrect direction, we treat this as a refutationfor the corresponding answer and select the otherone.
In our running example, in addition to thevalid path for the second query, for the first querywe see that there is an undirected path from splitto absorb through transfer that matches the firstquery.
This tells us that light absorption cannotbe the answer because it is not along a causal pathfrom split.Finally, if none of the queries results in a match,we look for any unlabeled path between the sourceand the target, before backing off to a dependency-based proximity baseline described in Section 6.When there are multiple aligning nodes in thequestion and answer, we look for any proof orrefutation before backing off to the baselines.15056 Empirical EvaluationIn this section we aim to empirically evaluatewhether we can improve reading comprehensionaccuracy by predicting process structures.
We firstprovide details of the experimental setup.6.1 Experimental setupWe used 150 processes (435 questions) for train-ing and 50 processes (150 questions) as the testset.
For development, we randomly split the train-ing set 10 times (80%/20%), and tuned hyper-parameters by maximizing average accuracy onquestion answering.
We preprocessed the para-graphs with the Stanford CoreNLP pipeline ver-sion 3.4 (Manning et al., 2014) and Illinois SRL(Punyakanok et al., 2008; Clarke et al., 2012).
Weused the Gurobi optimization package3for infer-ence.We compare our system PROREAD to baselinesthat do not have access to the process structure:1.
BOW: For each answer, we compute theproportion of content word lemmas coveredby the paragraph and choose the one withhigher coverage.
For true-false questions, wecompute the coverage of the question state-ment, and answer ?True?
if it is higher than athreshold tuned on the development set.2.
TEXTPROX: For dependency questions, wealign content word lemmas in both the ques-tion and answer against the text and select theanswer whose aligned tokens are closer to thealigned tokens of the question.
For tempo-ral questions, we return the answer for whichthe order of events is identical to their orderin the paragraph.
For true-false questions, wereturn ?True?
if the number of bigrams fromthe question covered in the text is higher thana threshold tuned on the development set.3.
SYNTPROX: For dependency questions, weuse proximity as in TEXTPROX, except thatdistance is measured using dependency treeedges.
To support multiple sentences we con-nect roots of adjacent sentences with bidi-rectional edges.
For temporal questions thisbaseline is identical to TEXTPROX.
For true-false questions, we compute the number ofdependency tree edges in the question state-ment covered by edges in the paragraph (anedge has a source lemma, relation, and targetlemma), and answer ?True?
if the coverage is3http://www.gurobi.com/Method Depen.
Temp.
True-falseAllPROREAD 68.1 80.0 55.6 66.7SYNTPROX 61.9 70.0 48.1 60.0TEXTPROX 58.4 70.0 33.3 54.7BOW 47.8 40.0 44.4 46.7GOLD 77.9 80.0 70.4 76.7Table 4: Reading comprehension test set accuracy.
The Allcolumn shows overall accuracy across all questions.
The firstthree columns show accuracy for each coarse type.higher than a threshold tuned on the trainingset.To separate the contribution of process struc-tures from the performance of our structure pre-dictor, we also run our QA system given manuallyannotated gold standard structures (GOLD).46.2 Reading Comprehension TaskWe evaluate our system using accuracy, i.e., theproportion of questions answered correctly.
Ta-ble 4 presents test set results, where we breakdown questions by their coarse-type.PROREAD improves accuracy compared to thebest baseline by 6.7 absolute points (last column).Most of the gain is due to improvement on de-pendency questions, which are the most commonquestion type.
The performance of BOW indicatesthat lexical coverage alone does not distinguish thecorrect answer from the wrong answer.
In fact,guessing the answer with higher lexical overlapresults in performance that is slightly lower thanrandom.
Text proximity and syntactic proximityprovide a stronger cue, but exploiting predictedprocess structures substantially outperforms thesebaselines.Examining results using gold information high-lights the importance of process structures inde-pendently of the structure predictor.
Results ofGOLD demonstrate that given gold structures wecan obtain a dramatic improvement of almost 17points compared to the baselines, using our sim-ple deterministic QA system.Results on true-false questions are low forPROREAD and all the baselines.
True-false ques-tions are harder for two main reasons.
First, independency and temporal questions, we create aquery for both answers, and can find a proof ora refutation for either one of them.
In true-false4We also ran an experiment where gold triggers aregiven and arguments and relations are predicted.
We foundthat this results in slightly higher performance compared toPROREAD.1506Precision Recall F1Triggers 75.4 73.9 74.6Arguments 43.4 34.4 38.3Relations 27.0 22.5 24.6Table 5: Structured prediction test set results.questions we must determine given a single state-ment whether it holds.
Second, an analysis of true-false questions reveals that they focus less on re-lations between events and entities in the process,and require modeling lexical variability.56.3 Structure Prediction TaskOur evaluation demonstrates that gold structuresimprove accuracy substantially more than pre-dicted structures.
To examine this, we now di-rectly evaluate the structure predictor by com-paring micro-average precision, recall and F1be-tween predicted and gold structures (Table 5).While performance for trigger identification isreasonable, performance on argument and relationprediction is low.
This explains the higher perfor-mance obtained in reading comprehension givengold structures.
Note that errors in trigger predic-tion propagate to argument and relation prediction?
a relation cannot be predicted correctly if eitherone of the related triggers is not previously identi-fied.
One reason for low performance is the smallsize of the dataset.
Thus, training process predic-tors with less supervision is an important directionfor future work.
Furthermore, the task of processprediction is inherently difficult, because often re-lations are expressed only indirectly in text.
Forexample, in Figure 1 the relation between watersplitting and transfer of ions is only recoverableby understanding that water provides the ions thatneed to be transferred.Nevertheless, we find that questions can oftenbe answered correctly even if the structure con-tains some errors.
For example, the gold structurefor the sentence ?Some .
.
.
radioisotopes havelong half-lives, allowing .
.
.
?, contains the triggerlong half-lives, while we predict have as a triggerand long half-lives as an argument.
This is goodenough to answer questions related to this part ofthe structure correctly, and overall, to improve per-formance using predicted structures.5The low performance of TEXTPROX and SYNTPROX ontrue-false questions can also be attributed to the fact that wetuned a threshold parameter on the training set, and this didnot generalize well to the test set.Reason GOLD PROREADAlignment 35% 15%Missing from annotation 25% 10%Entity coreference 20% 10%Missing regular expression 10%Lexical variability 5% 10%Error in predicted structure 55%Other 5%Table 6: Error analysis results.
An explanation of the vari-ous categories are in the body of the paper.6.4 Error AnalysisThis section presents the results of an analysis of20 sampled errors of GOLD (gold structures), and20 errors of PROREAD (predicted structures).
Wehave categorized the primary reason for error inTable 6.As expected, the main problem when using pre-dicted structures, is structure errors which accountfor more than half of the errors.Errors in GOLD are distributed across variouscategories, which we briefly describe.
Alignmenterrors occur due to multiple words aligning to mul-tiple triggers and arguments.
For example, in thequestion ?What is the result of gases being pro-duced in the lysosome?
?, the answer ?engulfedpathogens are poisoned?
is incorrectly aligned tothe trigger engulfed rather than to poisoned.Another reason for errors are cases where ques-tions are asked about parts of the paragraph thatare missing from annotation.
This is possible sincequestions were authored independently of struc-ture annotation.
Two other causes for errors areentity coreference errors, where a referent for anentity is missing from the structure, and lexicalvariability, where the author of questions usesnames for triggers or arguments that are missingfrom the paragraph, and so alignment fails.Last, in 10% of the cases in GOLD we foundthat the answer could not be retrieved using the setof regular expressions that are currently used byour QA system.7 DiscussionThis work touches on several strands of work inNLP including information extraction, semanticrole labeling, semantic parsing and reading com-prehension.Event and relation extraction have been studiedvia the ACE data (Doddington et al., 2004) andrelated work.
The BioNLP shared tasks (Kim etal., 2009; Kim et al., 2011; Riedel and McCal-1507lum, 2011) focused on biomedical data to extractevents and their arguments.
Event-event relationshave been mostly studied from the perspective oftemporal ordering; e.g., (Chambers and Jurafsky,2008; Yoshikawa et al., 2009; Do et al., 2012; Mc-Closky and Manning, 2012).
The process struc-ture predicted in this work differs from these linesof work in two important ways: First, we predictevents, arguments and their interactions from mul-tiple sentences, while most earlier work focusedon one or two of these components.
Second, wemodel processes, and thus target causal relationsbetween events, rather than temporal order only.Our semantic role annotation is similar to ex-isting SRL schemes such as PropBank (Palmer etal., 2005), FrameNet (Ruppenhofer et al., 2006)and BioProp (Chou et al., 2006).
However, in con-trast to PropBank and FrameNet, we do not allowall verbs to trigger events and instead let the an-notators decide on biologically important triggers,which are not restricted to verbs (unlike BioProp,where 30 pre-specified verbs were selected for an-notation).
Like PropBank and BioProp, the argu-ment labels are not trigger specific.Mapping questions to queries is effectively a se-mantic parsing task.
In recent years, several linesof work addressed semantic parsing using vari-ous formalisms and levels of supervision (Zettle-moyer and Collins, 2005; Wong and Mooney,2006; Clarke et al., 2010; Berant et al., 2013).In particular, Krishnamurthy and Kollar (2013)learned to map natural language utterances to ref-erents in an image by constructing a KB from theimage and then mapping the utterance to a queryover the KB.
This is analogous to our process ofconstructing a process structure and performingQA by querying that structure.
In our work, weparse questions into graph-based queries, suitablefor modeling processes, using a rule-based heuris-tic.
Training a statistical semantic parser that willreplace the QA system is an interesting directionfor future research.Multiple choice reading comprehension testsare a natural choice for evaluating machine read-ing.
Hirschman et al.
(1999) presented a bag-of-words approach to retrieving sentences for read-ing comprehension.
Richardson et al.
(2013) re-cently released the MCTest reading comprehen-sion dataset that examines understanding of fic-tional stories.
Their work shares our goal of ad-vancing micro-reading, but they do not focus onprocess understanding.Developing programs that perform deep reason-ing over complex descriptions of processes is animportant step on the road to fulfilling the highergoals of machine reading.
In this paper, we presentan end-to-end system for reading comprehen-sion of paragraphs which describe biological pro-cesses.
This is, to the best of our knowledge, thefirst system to both predict a rich structured rep-resentation that includes entities, events and theirrelations, and utilize this structure for answeringreading comprehension questions.
We also createda new dataset, PROCESSBANK, which contains200 paragraphs that are both fully-annotated withprocess structure, as well as accompanied by ques-tions.
We empirically demonstrated that model-ing biological processes can substantially improvereading comprehension accuracy in this domain.AcknowledgmentsThe authors would like to thank LukeAmuchastegui for authoring the multiple-choicequestions, and also the anonymous reviewers fortheir constructive feedback.
We thank the AllenInstitute for Artificial Intelligence for assistancein funding this work.ReferencesJonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of EMNLP.Neil Campbell and Jane Reece.
2005.
Biology.
Ben-jamin Cummings.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of AAAI.Nathanael Chambers and Daniel Jurafsky.
2008.Jointly combining implicit constraints improvestemporal ordering.
In Proceedings of EMNLP.Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-ShanSu, Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu.2006.
A semi-automatic method for annotating abiomedical proposition bank.
In Proceedings of theWorkshop on Frontiers in Linguistically AnnotatedCorpora 2006, July.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing from theworld?s response.
In Proceedings of CoNLL.James Clarke, Vivek Srikumar, Mark Sammons, andDan Roth.
2012.
An NLP Curator (or: How I1508Learned to Stop Worrying and Love NLP Pipelines).In Proceedings of LREC.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof ACL.Quang Do, Wei Lu, and Dan Roth.
2012.
Joint infer-ence for event timeline construction.
In Proceedingsof EMNLP-CoNLL.George R. Doddington, Alexis Mitchell, Mark A. Przy-bocki, Lance A. Ramshaw, Stephanie Strassel, andRalph M. Weischedel.
2004.
The Automatic Con-tent Extraction (ACE) Program-Tasks, Data, andEvaluation.
In Proceedings of LREC.Oren Etzioni, Michele Banko, and Michael J. Cafarella.2006.
Machine reading.
In Proceedings of AAAI.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of EMNLP.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-Driven Learning for Open Ques-tion Answering.
In Proceedings of ACL.Christiane Fellbaum, editor.
1998.
WordNet: An elec-tronic lexical database.
MIT Press.Lynette Hirschman, Marc Light, Eric Breck, andJohn D. Burger.
1999.
Deep read: A reading com-prehension system.
In Proceedings of ACL.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Junichi Tsujii.
2009.
Overview ofBioNLP 09 shared task on event extraction.
In Pro-ceedings of BioNLP.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, and Junichi Tsujii.
2011.
Overview ofBioNLP shared task 2011.
In Proceedings ofBioNLP.Jayant Krishnamurthy and Thomas Kollar.
2013.Jointly learning to parse and perceive: Connect-ing natural language to the physical world.
TACL,1:193?206.Nate Kushman, Yoav Artzi, Luke Zettlemoyer, andRegina Barzilay.
2014.
Learning to automaticallysolve algebra word problems.
In Proceedings ofACL.Beth Levin.
1993.
English verb classes and alter-nations: A preliminary investigation.
University ofChicago Press.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
1998.
Nomlex: Alexicon of nominalizations.
In Proceedings of EU-RALEX.Thomas L. Magnanti and Laurence A. Wolsey.
1995.Optimal trees.
Handbooks in operations researchand management science, 7:503?615.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of ACL:System Demonstrations.Andr?e L. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formu-lations for dependency parsing.
In Proceedings ofACL/IJCNLP.David McClosky and Christopher D. Manning.
2012.Learning constraints for consistent timeline extrac-tion.
In Proceedings of EMNLP-CoNLL.Dan Moldovan, Christine Clark, Sanda Harabagiu, andSteve Maiorano.
2003.
Cogex: A logic proverfor question answering.
In Proceedings of NAACL-HLT.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2).Matthew Richardson, Christopher JC Burges, and ErinRenshaw.
2013.
MCTest: A challenge dataset forthe open-domain machine comprehension of text.
InProceedings of EMNLP.Sebastian Riedel and Andrew McCallum.
2011.
Fastand robust joint models for biomedical event extrac-tion.
In Proceedings of EMNLP.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Proceedings of CoNLL.Josef Ruppenhofer, Michael Ellsworth, Miriam RLPetruck, Christopher R. Johnson, and Jan Schef-fczyk.
2006.
FrameNet II: Extended theory andpractice.
Berkeley FrameNet Release, 1.Aju Thalappillil Scaria, Jonathan Berant, MengqiuWang, Peter Clark, Justin Lewis, Brittany Harding,and Christopher D. Manning.
2013.
Learning bi-ological processes with global constraints.
In Pro-ceedings of EMNLP.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012.
Brat: a web-based tool for NLP-assistedtext annotation.
In Proceedings of the demonstra-tions at EACL.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proceedings of HLT-NAACL.1509Katsumasa Yoshikawa, Sebastian Riedel, MasayukiAsahara, and Yuji Matsumoto.
2009.
Jointly identi-fying temporal relations with Markov logic.
In Pro-ceedings of ACL/IJCNLP.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proceedings of UAI.1510
