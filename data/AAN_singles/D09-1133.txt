Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280?1288,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPImproving Nominal SRL in Chinese Language with Verbal SRL In-formation and Automatic Predicate RecognitionJunhui Li?
Guodong Zhou??
Hai Zhao??
Qiaoming Zhu?
Peide Qian??
Jiangsu Provincial Key Lab for Computer Information Processing TechnologiesSchool of Computer Science and TechnologySoochow University, Suzhou, China 215006?
Department of Chinese, Translation and LinguisticsCity University of HongKong, ChinaEmail: {lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn?
Corresponding authorAbstractThis paper explores Chinese semantic role la-beling (SRL) for nominal predicates.
Besidesthose widely used features in verbal SRL,various nominal SRL-specific features arefirst included.
Then, we improve the perform-ance of nominal SRL by integrating usefulfeatures derived from a state-of-the-art verbalSRL system.
Finally, we address the issue ofautomatic predicate recognition, which is es-sential for a nominal SRL system.
Evaluationon Chinese NomBank shows that our researchin integrating various features derived fromverbal SRL significantly improves the per-formance.
It also shows that our nominal SRLsystem much outperforms the state-of-the-artones.1.
IntroductionSemantic parsing maps a natural language sen-tence into a formal representation of its meaning.Due to the difficulty in deep semantic parsing,most of previous work focuses on shallow se-mantic parsing, which assigns a simple structure(such as WHO did WHAT to WHOM, WHEN,WHERE, WHY, HOW) to each predicate in asentence.
In particular, the well-defined seman-tic role labeling (SRL) task has been drawingmore and more attention in recent years due toits importance in deep NLP applications, such asquestion answering (Narayanan and Harabagiu,2004), information extraction (Surdeanu et al,2003), and co-reference resolution (Ponzetto andStrube, 2006).
Given a sentence and a predicate(either a verb or a noun) in it, SRL recognizesand maps all the constituents in the sentence intotheir corresponding semantic arguments (roles)of the predicate.
According to the predicatetypes, SRL could be divided into SRL for verbalpredicates (verbal SRL, in short) and SRL fornominal predicates (nominal SRL, in short).During the past few years, verbal SRL hasdominated the research on SRL with the avail-ability of FrameNet (Baker et al, 1998), Prop-Bank (Palmer et al, 2005), and the consecutiveCoNLL shared tasks (Carreras and M?rquez,2004 & 2005) in English language.
As a com-plement to PropBank on verbal predicates,NomBank (Meyers et al, 2004) annotates nomi-nal predicates and their corresponding semanticroles using similar semantic framework asPropBank.
As a representative, Jiang and Ng(2006) pioneered the exploration of variousnominal SRL-specific features besides the tradi-tional verbal SRL-related features on NomBank.They achieved the performance of 72.73 and69.14 in F1-measure on golden and automaticsyntactic parse trees, respectively, given goldennominal predicates.For SRL in Chinese, Sun and Jurafsky (2004)and Pradhan et al (2004) pioneered the researchon Chinese verbal and nominal SRLs, respec-tively, on small private datasets.
Taking the ad-vantage of recent release of Chinese PropBank(Xue and Palmer, 2003) and Chinese NomBank(Xue, 2006a), Xue and his colleagues (Xue andPalmer 2005; Xue 2006b; Xue, 2008) pioneeredthe exploration of Chinese verbal and nominalSRLs, given golden predicates.
Among them,Xue and Palmer (2005) studied Chinese verbalSRL using Chinese PropBank and achieved theperformance of 91.3 and 61.3 in F1-measure ongolden and automatic syntactic parse trees, re-spectively.
Xue (2006b) extended their study onChinese nominal SRL and attempted to improvethe performance of nominal SRL by simply in-1280cluding the Chinese PropBank training instancesinto the training data for nominal SRL on Chi-nese NomBank.
However, such integration wasempirically proven unsuccessful due to the dif-ferent nature of certain features for verbal andnominal SRLs.
Xue (2008) further improved theperformance on both verbal and nominal SRLswith a better syntactic parser and new features.Ding and Chang (2008) focused on argumentclassification for Chinese verbal predicates withhierarchical feature selection strategy.
Theyachieved the classification precision of 94.68%on golden parse trees on Chinese PropBank.This paper focuses on Chinese nominal SRL.This is done by adopting a traditional verbalSRL architecture to handle Chinese nominalpredicates with additional nominal SRL-specificfeatures.
Moreover, we significantly enhance theperformance of nominal SRL by properly inte-grating various features derived from verbalSRL.
Finally, this paper investigates the effect ofautomatic nominal predicate recognition on theperformance of Chinese nominal SRL.
Althoughprevious research (e.g.
CoNLL?2008) in Englishnominal SRL reveals the importance of auto-matic predicate recognition, there has no re-ported research on automatic predicaterecognition in Chinese nominal SRL.The rest of this paper is organized as follows:Section 2 introduces Chinese NomBank whilethe baseline nominal SRL system is described inSection 3 with traditional and nominal SRL-specific features.
Then, the baseline nominalSRL system is improved by integrating usefulfeatures derived from verbal SRL (Section 4)and extended with automatic recognition ofnominal predicates (Section 5).
Section 6 givesexperimental results and discussion.
Finally,Section 7 concludes the paper.2.
Chinese NomBankChinese NomBank (Xue, 2006a) adopts similarsemantic framework as NomBank, and focuseson Chinese nominal predicates with their argu-ments in Chinese TreeBank.
The semantic ar-guments include:1) Core arguments: Arg0 to Arg5.
Generally,Arg0 and Arg1 denotes the agent and thepatient, respectively, while arguments fromArg2 to Arg5 are predicate-specific.2) Adjunct arguments, which are universal toall predicates, e.g.
ArgM-LOC for locative,and ArgM-TMP for temporal.All the arguments are annotated on parse treenodes with their boundaries aligning with thespans of tree nodes.
Figure 1 gives an examplewith two nominal predicates and their respectivearguments, while the nominal predicate ???/investment?
has two core arguments, ?NN(?
?/foreign businessman)?
as Arg0 and ?NN(??/bank)?
as Arg1, and the other nominal predicate???
/loan?
also has two core arguments,?NP(????
/Bank of China)?
as Arg1 andFigure 1: Two nominal predicates and their arguments in the style of NomBank.???
??
??????
?PNN NN NNVVNN NNArg0/Rel1 Rel1 Arg1/Rel1NPPPArg0/Rel2ArgM-MNR/Rel2 Rel2NPCDQPNPVPVP???
??
?NN NNPUNPArg1/Rel2IP??
?
?Sup/Rel2Bank of ChinatoForeign  Investment  Bankprovide4 billionRMB loan.Bank of China provides 4 billion RMB loan to Foreign Investment Bank.1281?PP(???????
/to Foreign InvestmentBank)?
as Arg0,  and 1 adjunct argument,?NN(???/RMB)?
as ArgM-MNR, denotingthe manner of loan.
It is worth noticing thatthere is a (Chinese) NomBank-specific label inFigure 1, Sup (support verb) (Xue, 2006a), inhelping introduce the arguments, which occuroutside the nominal predicate-headed nounphrase.
This is illustrated by the nominal predi-cate ??
?/loan?, whose Arg0 and Arg1 are bothrealized outside the nominal predicate-headednoun phrase, NP(???????
?/4 billionRMB loan).
Normally, a verb is marked as asupport verb only when it shares some argu-ments with the nominal predicate.3.
Baseline: Chinese Nominal SRLPopular SRL systems usually formulate SRL asa classification problem, which annotates eachconstituent in a parse tree with a semantic rolelabel or with the non-argument label NULL.
Be-sides, we divide the system into three consecu-tive phases so as to overcome the imbalancebetween the training instances of the NULLclass and those of any other argument classes.Argument pruning.
Here, several heuristicrules are adopted to filter out constituents, whichare most likely non-arguments.
According to theargument structures of nominal predicates, wecategorize arguments into two types: argumentsinside NP (called inside arguments) and argu-ments introduced via a support verb (called out-side arguments), and handle them separately.For the inside arguments, the following threeheuristic rules are applied to find inside argu-ment candidates:z All the sisters of the predicate are candi-dates.z If a CP or DNP node is a candidate, its chil-dren are candidates too.z For any node X, if its parent is an ancestralnode of the predicate, and the internalnodes along the path between X and thepredicate are all NPs, then X is a candidate.For outside arguments, we look for the sup-port verb of the focus nominal predicate, andthen adopt the rules as proposed in Xue andPalmer (2005) to find the candidates for the sup-port verb, since outside argument candidates areintroduced via this support verb.
That to say, theargument candidates of the support verb are re-garded as outside argument candidates of thenominal predicate.
However, as support verbsare not annotated explicitly in the testing phase,we identify intervening verbs as alternatives tosupport verbs in both training and testing phaseswith the path between the nominal predicate andintervening verb in the form of?VV<VP>[NP>]+NN?, where ?[NP>]+?
denotesone or more NPs.
Our statistics on ChineseNomBank shows that 51.96% of nominal predi-cates have no intervening verb while 48.04% ofnominal predicates have only one interveningverb.Taken the nominal predicate ???/loan?
inFigure 1 as an example, NN(??
?/RMB) andQP(???
/4 billion) are identified as insideargument candidates, while PP(??????
?/to Foreign Investment Bank) and NP(???
?/Bank of China) are identified as outside ar-gument candidates via the support verb VV(?
?/provide).Argument identification.
A binary classifieris applied to determine the candidates as eithervalid arguments or non-arguments.
It is worthpointing out that we only mark those candidatesthat are most likely to be NULL (with probabil-ity > 0.90) as non-arguments.
Our empiricalstudy shows that this little trick much benefitsnominal SRL, since argument identification fornominal predicates is much more difficult thanthat for verbal predicates and thus many argu-ments would have been falsely marked as non-arguments if the threshold is set as 0.5.Argument classification.
A multi-class classi-fier is employed to label identified argumentswith specific argument labels (including theNULL class for non-argument).In the following, we first adapt some tradi-tional features, which have been proven effec-tive in verbal SRL, to nominal SRL, and thenintroduce several nominal SRL-specific features.3.1.
Traditional FeaturesUsing the feature naming convention as adoptedin Jiang and Ng (2006), Table 1 lists the tradi-tional features, where ?I?
and ?C?
indicate thefeatures for argument identification and classifi-cation, respectively.
Among them, the predicateclass (b2) feature was first introduced in Xueand Palmer (2005) to overcome the imbalance ofthe predicate distribution in that some predicatescan be only found in the training data whilesome predicates in the testing data are absentfrom the training data.
In particular, the verbclass is classified along three dimensions: thenumber of arguments, the number of framesetsand selected syntactic alternations.
For example,1282the verb class of ?C1C2a?
means that it has twoframesets, with the first frameset having oneargument and the second having two arguments.The symbol ?a?
in the second frameset repre-sents a type of syntactic alternation.Feature Remarks: b1-b5(C, I), b6-b7(C)b1 Predicate: the nominal predicate itself.
(?
?/loan)b2 Predicate class: the verb class that the predi-cate belongs to.
(C4a)b3 Head word (b3H) and its POS (b3P).
(?
?/bank, NN)b4 Phrase type: the syntactic category of theconstituent.
(NP)b5 Path: the path from the constituent to thenominal predicate.
(NP<IP>VP>VP>NP>NP>NN)b6 Position: the positional relationship of theconstituent with the predicate.
?left?
or?right?.
(left)b7 First word (b7F) and last word (b7L) of thefocus constituent.
(?
?/China, ?
?/bank)Combined features: b11-b14(C, I), b15(C)b11: b1&b4;       b12: b1&b3H;       b13: b2&b4;b14: b2&b3H;    b15: b5&b6Table 1: Traditional features and their instantiationsfor argument identification and classification, withNP(???
?/Bank of China)  as the focus constitu-ent and NN(?
?/loan) as the nominal predicate, re-garding Figure 1.3.2.
Nominal SRL-specific FeaturesTo capture more useful information in the predi-cate-argument structure, we also study addi-tional features which provide extra information.Statistics on Chinese NomBank show that about40% of pruned inside candidates are arguments.Since inside arguments usually locate near to thenominal predicate, its surroundings are expectedto be helpful in SRL.
Table 2 shows the featuresin better capturing the details between insidearguments and nominal predicates.
Specially,features ai6 and ai7 are sister-related features,inspired by the features related with theneighboring arguments in Jiang and Ng (2006).Statistics on NomBank and Chinese Nom-Bank show that about 20% and 22% of argu-ments are introduced via a support verb,respectively.
Since a support verb pivots outsidearguments and the nominal predicate on its twosides, support verbs play an important role inlabeling these arguments.
Here, we also identifyintervening verbs as alternatives to support verbssince support verbs are not explicitly in the test-ing phase.
Table 3 lists the intervening verb-related features (ao1-ao4, ao11-ao14) employedin this paper.Feature Remarksai1 Whether the focus constituent is adjacent tothe predicate.
Yes or No.
(Yes)ai2 The headword (ai2H) and pos (ai2P) of thepredicate?s nearest right sister.
(?
?/bank,NN)ai3 Whether the predicate has right sisters.
Yesor No.
(Yes)ai4 Compressed path of b5: compressing se-quences of identical labels into one.
(NN<NP>NN)ai5 Whether the predicate has sisters.
Yes orNo.
(Yes)ai6 For each sister of the focus constituent,combine b3H&b4&b5&b6.
( ?
?/bank&NN & NN<NP>NN&right)ai7 Coarse version of ai6, b4&b6.
(NN&right)Table 2: Additional features and their instantiationsfor inside argument candidates, with ?NN(?
?/foreign businessman)?
as the focus constituent and?NN(??
/investment)?
as the nominal predicate,regarding Figure1.Feature Remarksao1 Intervening verb itself.
(?
?/provide)ao2 The verb class that the intervening verbbelongs to.
(C3b)ao3 The path from the focus constituent to theintervening verb.
(NP<IP>VP>VP>VV)ao4 The compressed path of ao3: compressingsequences of identical labels into one.
(NP<IP>VP>VV)Combined features: ao11-ao14ao11: ao1&ao3;      ao12: ao1&ao4;ao13: ao2&ao3;      ao14: ao2&ao4.Table 3: Additional features and their instantiationsfor outside argument candidates, with ?NP(???
?/Bank of China)?
as the focus constituent and ???/loan?
as the nominal predicate, regarding Figure1.Feature selection.
Some Features proposedabove may not be effective in tasks of identifica-tion and classification.
We adopt the greedy fea-ture selection algorithm as described in Jiangand Ng (2006) to pick up positive features em-pirically and incrementally according to theircontributions on the development data.
The al-gorithm repeatedly selects one feature each timewhich contributes most, and stops when addingany of the remaining features fails to improvethe performance.
As far as the SRL task con-cerned, the whole feature selection process couldbe done as follows: 1).
Feature selection for ar-gument identification: run the selection algo-1283rithm with the basic set of features (b1-b5, b11-b14) to pick up effective features from (ai1-ai7,ao1-ao4, ao11-ao14); 2).
Feature selection forargument classification: fix the output returnedin step1 as the feature set of argument identifica-tion, and run the selection algorithm with thebasic set of features (b1-b7, b11-b15) to selectpositive features from (ai1-ai7, ao1-ao4, ao11-ao14) for argument classification.4.
Integrating Features derived fromVerbal SRLSince Chinese PropBank and NomBank are an-notated on the same data set with the same lexi-cal guidelines (e.g.
frame files), it may beinteresting to investigate the contribution ofChinese verbal SRL on the performance of Chi-nese nominal SRL.
In the frame files, argumentlabels are defined with regard to their semanticroles to the predicate, either a verbal or nominalpredicate.
For example, in the frame file ofpredicate ??
?/loan?, the borrower is alwayslabeled with Arg0 and the lender labeled withArg1.
This can be demonstrated by the follow-ing two sentences: ???/loan?
is annotated as anominal and a verbal predicate in S1 and S2,respectively.S1 [Arg1 ???
?/Bank of China] [Arg0 ??????
?/to Foreign Investment Bank] ?
?/provide [Rel?
?/loan]S2  [Arg0 ???
?/Bank of China] [Arg1 ??????
?/from Foreign Investment Bank] [Rel?
?/loan]Therefore, it is straightforward to augmentnominal training instances with verbal ones.However, Xue (2006b) found that simply addingthe training instances for verbal SRL to thetraining data for nominal SRL and indiscrimi-nately extracting the same features in both ver-bal and nominal SRLs hurt the performance.This may be due to that certain features (e.g.
thepath feature) are much different for verbal andnominal SRLs.
This can be illustrated in sen-tences S1 and S2: the verbal instances in S2 arenegative for semantic role labeling of the nomi-nal predicate ???/loan?
in S1, since ????
?/Bank of China?
takes opposite roles in S1and S2.
So does ???????
?/(from/to)Foreign Investment Bank?.Although several support verb-related features(ao1-ao4, ao11-ao14) have been proposed, onemay still ask how large the role support verbscan play in nominal SRL.
It is interesting to notethat outside arguments and the highest NPphrase headed by the nominal predicate are alsoannotated as arguments of the support verb inChinese PropBank.
For example, Chinese Prop-Bank marks ????
?/Bank of China?
as Arg0and ????????
?/4 billion RMB loan?as Arg1 for verb ???/provide?
in Figure1.
LetOA be the outside argument, VV be the supportverb, and NP be the highest NP phrase headedby the nominal predicate NN, then there exists apattern ?OA VV NN?
in the sentence, where thesupport verb VV plays a certain role in trans-ferring roles between OA and NN.
For example,if OA is the agent of VV, then OA is also theagent of phrase VP(VV NN).
Like the examplein Figure1, supposing a NP is the agent of sup-port verb ???/provide?
as well as VP phrase(???????????
/provide 4 billionRMB loan?
), we can infer that the NP is thelender of the nominal predicate ???/loan?
in-dependently on any other information, such asthe NP content and the path from the NP to thenominal predicate ??
?/loan?.Let C be the focus constituent, V be the inter-vening verb, and NP be the highest NP headedby the nominal predicate.
Table 4 shows the fea-tures (ao5-ao8, p1-p7) derived from verbal SRL.In this paper, we develop a state-of-the-art Chi-nese verbal SRL system, similar to the one asshown in Xue (2008), to achieve the goal.
Basedon golden parse trees on Chinese PropBank, ourChinese verbal SRL system achieves the per-formance of 92.38 in F1-measure, comparable toXue (2008) which achieved the performance of92.0 in F1-measure.Feature Remarksao5 Whether C is an argument for V. Yes or Noao6 The semantic role of C for V.ao7 Whether NP is an argument for V. Yes or Noao8 The semantic role of NP for V.Combined features: p1-p7p1: ao1&ao5;         p2: ao1&ao6;    p3: ao1&ao5&b1;p4: ao1&ao6&b1;  p5: ao1&apo7;  p6: ao1&ao8;p7: ao5&ao7.Table 4: Features derived from verbal SRL.5.
Automatic Predicate RecognitionUnlike Chinese PropBank where almost all theverbs are annotated as predicates, Chinese Nom-Bank only marks those nouns having argumentsas predicates.
Statistics on Chinese NomBankshow that only 17.5% of nouns are marked aspredicates.
It is possible that a noun is a predi-1284cate in some cases but not in others.
PreviousChinese nominal SRL systems (Xue, 2006b;Xue, 2008) assume that nominal predicates havealready been manually annotated and thus areavailable.
To our best knowledge, there is noreport on addressing automatic recognition ofnominal predicates on Chinese nominal SRL.Automatic recognition of nominal predicatescan be cast as a binary classification (e.g., Predi-cate vs. Non-Predicate) problem.
This paperemploys the convolution tree kernel, as proposedin Collins and Duffy (2001), on automatic rec-ognition of nominal predicates.Given the convolution tree kernel, the keyproblem is how to extract a parse tree structurefrom the parse tree for a nominal predicate can-didate.
In this paper, the parse tree structure isconstructed as follows: 1) starting from thepredicate candidate?s POS node, collect all of itssister nodes (with their headwords); 2).
recur-sively move one level up and collect all of itssister nodes (with their headwords) till reachinga non-NP node.
Specially, in order to explicitlymark the positional relation between a node andthe predicate candidate, all nodes on the left sideof the candidate are augmented with tags 1 and 2for nodes on the right side.
Figure 2 shows anexample of the parse tree structure with regardto the predicate candidate ???/loan?
as shownin Figure 1.In our extra experiments we found global sta-tistic features (e.g.
g1-g5) about the predicatecandidate are helpful in a feature vector-basedmethod for predicate recognition.
Figure 2makes an attempt to utilize those features in ker-nel-based method.
We have explored other waysto include those global features.
However, theway in Figure 2 works best.Let the predicate candidate be w0, and its leftand right neighbor words be w-1 and w1, respec-tively.
The five global features are defined asfollows.g1 Whether w0 is ever tagged as a verb in thetraining data?
Yes or No.g2 Whether w0 is ever annotated as a nominalpredicate in the training data?
Yes or No.g3 The most likely label for w0 when it occurstogether with w-1 and w1.g4 The most likely label for w0 when it occurstogether with w-1.g5 The most likely label for w0 when it occurstogether with w1.6.
Experiment Results and DiscussionWe have evaluated our Chinese nominal SRLsystem on Chinese NomBank with ChinesePropBank 2.0 as its counterpart.6.1.
Experimental SettingsThis version of Chinese NomBank consists ofstandoff annotations on the files (chtb_001 to1151.fid) of Chinese Penn TreeBank 5.1.
Fol-lowing the experimental setting in Xue (2008),648 files (chtb_081 to 899.fid) are selected asthe training data, 72 files (chtb_001 to 040.fidand chtb_900 to 931.fid) are held out as the testdata, and 40 files (chtb_041 to 080.fid) as thedevelopment data, with 8642, 1124, and 731propositions, respectively.As Chinese words are not naturally segmentedin raw sentences, two Chinese automatic parsersare constructed: word-based parser (assuminggolden word segmentation) and character-basedparser (with automatic word segmentation).Here, Berkeley parser (Petrov and Klein, 2007)1is chosen as the Chinese automatic parser.
Withregard to character-based parsing, we employ aChinese word segmenter, similar to Ng and Low(2004), to obtain the best automatic segmenta-tion result for a given sentence, which is thenfed into Berkeley parser for further syntacticparsing.
Both the word segmenter and Berkeleyparser are developed with the same training anddevelopment datasets as our SRL experiments.The word segmenter achieves the performanceof 96.1 in F1-measure while the Berkeley parsergives a performance of 82.5 and 85.5 in F1-measure on golden and automatic word segmen-tation, respectively2.???
1 In addition, SVMLight with the tree kernelfunction (Moschitti, 2004) 3  is selected as ourclassifier.
In order to handle multi-classification1 Berkeley Parser.
http://code.google.com/p/berkeleyparser/2 POSs are not counted in evaluating the performance ofword-based syntactic parser, but they are counted in evalu-ating the performance of character-based parser.
Thereforethe F1-measure for the later is higher than that for the for-mer.3 SVM-LIGHT-TK.
http://dit.unitn.it/~moschitt/Figure 2: Semantic sub-tree for nominal predicateRMB??loan??
1provide???
14 billionVV1NN1 NNNPQP1NPVPg1 ?.
g51285problem in argument classification, we apply theone vs. others strategy, which builds K classifi-ers so as to separate one class from all others.For argument identification and classification,we adopt the linear kernel and the training pa-rameter C is fine-tuned to 0.220.
For automaticrecognition of nominal predicates, the trainingparameter C and the decay factor ?
in the con-volution tree kernel are fine-tuned to 2.0 and 0.2,respectively.6.2.
Results with Golden Parse Trees andGolden Nominal PredicatesEffect of nominal SRL-specific featuresRec.
(%) Pre.
(%) F1traditional features 62.83 73.58 67.78+nominal SRL-specificfeatures69.90 75.11 72.55Table 5: The performance of nominal SRL on thedevelopment data with golden parse trees and goldennominal predicatesAfter performing the greedy feature selectionalgorithm on the development data, features{ao1, ai6, ai2P, ai5, ao2, ao12, ao14}, as pro-posed in Section 3.2, are selected consecutivelyfor argument identification, while features {ai7,ao1, ai1, ao2, ai5, ao4} are selected for argumentclassification.
Table 5 presents the SRL resultson the development data.
It shows that nominalSRL-specific features significantly improve theperformance from 67.78 to 72.55 ( )in F1-measure.05.0;2 <p?Effect of features derived from verbal SRLFeatures Rec.
(%) Pre.
(%) F1baseline 67.86 73.63 70.63+ao5 68.15 73.60 70.77 (+0.14)+ao6 67.66 72.80 70.14 (-0.49)+ao7 68.20 75.41 71.62 (+0.99)+ao8 68.30 75.39 71.67 (+1.04)+p1 67.91 74.40 71.00 (+0.37)+p2 67.76 74.20 70.83 (+0.20)+p3 67.96 74.69 71.16 (+0.53)+p4 68.01 74.18 70.96 (+0.33)+p5 68.01 75.01 71.39 (+0.76)+p6 68.20 75.12 71.49 (+0.86)+p7 68.40 75.70 71.87 (+1.24)Table 6: Effect of features derived from verbal SRLon the performance of nominal SRL on the test datawith golden parse trees and golden nominal predi-cates.
The first row presents the performance usingtraditional and nominal SRL-specific features.Rec.
(%) Pre.
(%) F1baseline  67.86 73.63 70.63+features derivedfrom verbal SRL68.40 77.51 72.67Xue (2008) 66.1 73.4 69.6Table 7: The performance of nominal SRL on the testdata with golden parse trees and golden nominalpredicatesTable 6 shows the effect of features derivedfrom verbal SRL in an incremental way.
Itshows that only the feature ao6 has negative ef-fect due to its strong relevance with interveningverbs and thus not included thereafter.
Table 7shows the performance on the test data with orwithout using the features derived from the ver-bal SRL system.
It shows these features signifi-cantly improve the performance ( )on nominal SRL.
Table 7 also shows our systemoutperforms Xue (2008) by 3.1 in F1-measure.05.0;2 <p?6.3.
Results with Automatic Parse Treesand Golden Nominal PredicatesIn previous section we have assumed the avail-ability of golden parse trees during the testingprocess.
Here we conduct experiments on auto-matic parse trees, using the Berkeley parser.Since arguments come from constituents inparse trees, those arguments, which do not alignwith any syntactic constituents, are simply dis-carded.
Moreover, for any nominal predicatesegmented incorrectly by the word segmenter,all its arguments are unable to be labeled neither.Table 8 presents the SRL performance on thetest data by using automatic parse trees.
It showsthat the performance drops from 72.67 to 60.87in F1-measure when replacing golden parse treeswith word-based automatic ones, partly due tothe absence of 6.9% arguments in automatictrees, and wrong POS tagging of nominal predi-cates.
Table 8 also compares our system withXue (2008).
It shows that our system also out-performs Xue (2008) on Chinese NomBank.Rec.
(%) Pre.
(%) F1This paper 56.95(53.55) 66.74(66.69) 60.87(59.40)Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3)Table 8: The performance of nominal SRL on the testdata with automatic parse trees and golden predicates.Here, the numbers outside the parentheses indicatethe performance using a word-based parser, while thenumbers inside indicate the performance using acharacter-based parser4.4 About 1.6% nominal predicates are mistakenly segmentedby the character-based parser, thus their arguments aremissed directly.12866.4.
Results with Automatic Nominal Predi-catesSo far nominal predicates are assumed to bemanually annotated and available.
Here we turnto a more realistic scenario in which both theparse tree and nominal predicates are automati-cally obtained.
In the following, we first reportthe results of automatic nominal predicate rec-ognition and then the results of nominal SRL onautomatic recognition of nominal predicates.Results of nominal predicate recognitionParses g1-g5 Rec.
(%) Pre.
(%) F1no 91.46 88.93 90.18 goldenyes 92.62 89.36 90.96word-based yes 86.39 81.80 84.03character-based yes 84.79 81.94 83.34Table 9: The performance of automatic nominalpredicate recognition on the test dataTable 9 lists the predicate recognition results,using the parse tree structure, as shown in Sec-tion 5, and the convolution tree kernel, as pro-posed in Collins and Duffy (2001).
The secondcolumn (g1-g5) indicates whether the global fea-tures (g1-g5) are included in the parse tree struc-ture.
We have also defined a simple rule thattreats a noun which is ever a verb or a nominalpredicate in the training data as a nominal predi-cate.
Based on golden parse trees, the rule re-ceives the performance of 81.40 in F1-measure.This suggests that our method significantly out-performs the simple rule-based one.
Table 9 alsoshows that:z As a complement to local structural informa-tion, global features improve the performanceof automatic nominal predicate recognitionby 0.78 in F1-measure.z The word-based syntactic parser decreasesthe F1-measure from 90.96 to 84.03, mostlydue to the POSTagging errors between NNand VV, while the character-based syntacticparser further drops the F1-measure by 0.69,due to automatic word segmentation.Results with automatic predicatesParses Predicates Rec.
(%) Pre.
(%) F1golden 68.40 77.51 72.67 goldenautomatic 65.07 74.65 69.53golden 55.95 66.74 60.87 word-based automatic 52.67 59.56 55.90golden 53.55 66.69 59.40 character-based automatic 50.66 59.60 54.77Table 10: The performance of nominal SRL on thetest data with the choices of golden/automatic parsetrees and golden/automatic predicatesIn order to have a clear performance comparisonamong nominal SRL on golden/automatic parsetrees and golden/automatic predicates, Table 10lists all the results in those scenarios.6.5.
ComparisonChinese nominal SRL vs. Chinese verbal SRLComparison with Xue (2008) shows that the per-formance of Chinese nominal SRL is about 20lower (e.g.
72.67 vs. 92.38 in F1-measure) thanthat of Chinese verbal SRL, partly due to thesmaller amount of annotated data (about 1/5) inChinese NomBank than that in Chinese Prop-Bank.
Moreover, according to Chinese Nom-Bank annotation criteria (Xue 2006a), evenwhen a noun is a true deverbal noun, not all ofits modifiers are legitimate arguments or ad-juncts of this predicate.
Only arguments that canco-occur with both the nominal and verbal formsof the predicate are considered in the NomBankannotation.
This means that the judgment of ar-guments is semantic rather than syntactic.
Thesefacts may also partly explain the lower nominalSRL performance, especially the performance ofargument identification.
This can be illustratedby the statistics on the development data that96% (40%) of verbal (nominal) predicates?
sis-ters are annotated as arguments.
Finally, thepredicate-argument structure of nominal predi-cates is more flexible and complicated than thatof verbal predicates as illustrated in Xue (2006a).Chinese nominal SRL vs. English nominalSRLLiu and Ng (2007) reported the performance of77.04 and 72.83 in F1-measure on English Nom-Bank when golden and automatic parse trees areused, respectively.
Taking into account that Chi-nese verbal SRL achieves comparable perform-ance with English verbal SRL on golden parsetrees, the performance gap between Chinese andEnglish nominal SRL (e.g.
72.67 vs. 77.04 inF1-measure) presents great challenge for Chi-nese nominal SRL.
Moreover, while automaticparse trees only decrease the performance ofEnglish nominal SRL by about 4.2 in F1-measure, automatic parse trees significantly de-crease the performance of Chinese nominal SRLby more than 12 in F1-measure due to the muchlower performance of Chinese syntactic parsing.7.
ConclusionIn this paper we investigate nominal SRL inChinese language.
In particular, some nominalSRL-specific features are included to improve1287the performance.
Moreover, various featuresderived from verbal SRL are properly integratedinto nominal SRL.
Finally, a convolution treekernel is adopted to address the issue of auto-matic nominal predicates recognition, which isessential in a nominal SRL system.To our best knowledge, this is the first re-search on1) Exploring Chinese nominal SRL on auto-matic parse trees with automatic predicaterecognition;2) Successfully integrating features derivedfrom Chinese verbal SRL into Chinese nomi-nal SRL with much performance improve-ment.AcknowledgementThis research was supported by Project60673041 and 60873150 under the NationalNatural Science Foundation of  China, Project2006AA01Z147 under the  ?863?
NationalHigh-Tech Research and Development of China,and Project BK2008160 under the Natural Sci-ence Foundation of the Jiangsu province ofChina.
We also want to thank Dr. Nianwen Xuefor share of the verb class file.
We also want tothank the reviewers for insightful comments.ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The Berkeley FrameNet Project.
InProceedings of COLING-ACL 1998.Xavier Carreras and Lluis M?rquez.
2004.
Introduc-tion to the CoNLL-2004 Shared Task: SemanticRole Labeling.
In Proceedings of CoNLL 2004.Xavier Carreras and Lluis M?rquez.
2005.
Introduc-tion to the CoNLL-2005 Shared Task: SemanticRole Labeling.
In Proceedings of CoNLL 2005.Michael Collins and Nigel Duffy.
2001.
ConvolutionKernels for Natural Language.
In Proceedings ofNIPS 2001.Weiwei Ding and Baobao Chang.
2008.
ImprovingChinese Semantic Role Classification with Hierar-chical Feature Selection Strategy.
In Proceedingsof EMNLP 2008.Zheng Ping Jiang and Hwee Tou Ng.
2006.
Semanticrole Labeling of NomBank: a Maximum EntropyApproach.
In Proceedings of EMNLP 2006.Chang Liu and Hwee Tou Ng.
2007.
Learning Predic-tive Structures for Semantic Role Labeling ofNomBank.
In Proceedings of ACL 2007.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V.Zielinska, B. Yong, and R. Grishman.
2004.
Anno-tating Noun Argument Structure for NomBank.
InProceedings of LREC 2004.Alessandro Moschitti.
2004.
A Study on ConvolutionKernels for Shallow Semantic Parsing.
In Pro-ceedings of ACL 2004.Srini Narayanan and Sanda Harabagiu.
2004.
Ques-tion Answering based on Semantic Structures.
InProceedings of COLING 2004.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese Part-of-speech Tagging: One-at-a-time or All-at-once?Word-based or Character-based?
In Proceedingsof EMNLP 2004.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics.Slav Petrov.
and Dan Klein.
2007.
Improved Infer-ence for Unlexicalized Parsing.
In Proceesings ofNAACL 2007.Simone Paolo Ponzetto and Michael Strube.
2006.Semantic Role Labeling for Coreference Resolu-tion.
In Proceedings of EACL 2006.Sameer Pradhan, Honglin Sun, Wayne Ward, JamesH.
Martin, and Dan Jurafsky.
2004.
Parsing Ar-guments of Nominalizations in English and Chi-nese.
In Proceedings of NAACL-HLT 2004.Honglin Sun and Daniel Jurafsky.
2004.
ShallowSemantic Parsing of Chinese.
In Proceedings ofNAACL 2004.Mihai Surdeanu, Sanda Harabagiu, John Williamsand Paul Aarseth.
2003.
Using Predicate-argumentStructures for Information Extraction.
In Proceed-ings of ACL 2003.Mihai Surdeanu, Richard Johansson, Adam Meyers,Lluis M?rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing ofSyntactic and Semantic Dependencies.
In Pro-ceedings of CoNLL 2008.Nianwen Xue and Martha Palmer.
2003.
Annotatingthe Propositions in the Penn Chinese TreeBank.
InProceedings of 2nd SIGHAN Workshop on ChineseLanguage Processing.Nianwen Xue and Martha Palmer.
2005.
AutomaticSemantic Role Labeling for Chinese verbs.
InProceedings of IJCAI 2005.Nianwen Xue.
2006a.
Annotating the Predicate-Argument Structure of Chinese Nominalizations.In Proceedings of the LREC 2006.Nianwen Xue.
2006b.
Semantic Role Labeling ofNominalized Predicates in Chinese.
In Proceed-ings of HLT-NAACL 2006.Nianwen Xue.
2008.
Labeling Chinese Predicateswith Semantic Roles.
Computational Linguistics,34(2):225-255.1288
