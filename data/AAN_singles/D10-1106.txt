Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1088?1098,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsLearning First-Order Horn Clauses from Web TextStefan Schoenmackers, Oren Etzioni, Daniel S. WeldTuring CenterUniversity of WashingtonComputer Science and EngineeringBox 352350Seattle, WA 98125, USAstef,etzioni,weld@cs.washington.eduJesse DavisKatholieke Universiteit LeuvenDepartment of Computer SciencePOBox 02402 Celestijnenlaan 200aB-3001 Heverlee, Belgiumjesse.davis@cs.kuleuven.beAbstractEven the entire Web corpus does not explic-itly answer all questions, yet inference can un-cover many implicit answers.
But where doinference rules come from?This paper investigates the problem of learn-ing inference rules from Web text in an un-supervised, domain-independent manner.
TheSHERLOCK system, described herein, is afirst-order learner that acquires over 30,000Horn clauses from Web text.
SHERLOCK em-bodies several innovations, including a novelrule scoring function based on Statistical Rel-evance (Salmon et al, 1971) which is effec-tive on ambiguous, noisy and incomplete Webextractions.
Our experiments show that in-ference over the learned rules discovers threetimes as many facts (at precision 0.8) as theTEXTRUNNER system which merely extractsfacts explicitly stated in Web text.1 IntroductionToday?s Web search engines locate pages that matchkeyword queries.
Even sophisticated Web-basedQ/A systems merely locate pages that contain an ex-plicit answer to a question.
These systems are help-less if the answer has to be inferred from multiplesentences, possibly on different pages.
To solve thisproblem, Schoenmackers et al(2008) introduced theHOLMES system, which infers answers from tuplesextracted from text.HOLMES?s distinction is that it is domain inde-pendent and that its inference time is linear in thesize of its input corpus, which enables it to scale tothe Web.
However, HOLMES?s Achilles heel is thatit requires hand-coded, first-order, Horn clauses asinput.
Thus, while HOLMES?s inference run timeis highly scalable, it requires substantial labor andexpertise to hand-craft the appropriate set of Hornclauses for each new domain.Is it possible to learn effective first-order Hornclauses automatically from Web text in a domain-independent and scalable manner?
We refer to theset of ground facts derived from Web text as open-domain theories.
Learning Horn clauses has beenstudied extensively in the Inductive Logic Program-ming (ILP) literature (Quinlan, 1990; Muggleton,1995).
However, learning Horn clauses from open-domain theories is particularly challenging for sev-eral reasons.
First, the theories denote instances ofan unbounded and unknown set of relations.
Sec-ond, the ground facts in the theories are noisy, andincomplete.
Negative examples are mostly absent,and certainly we cannot make the closed-world as-sumption typically made by ILP systems.
Finally,the names used to denote both entities and relationsare rife with both synonyms and polysymes makingtheir referents ambiguous and resulting in a particu-larly noisy and ambiguous set of ground facts.This paper presents a new ILP method, which isoptimized to operate on open-domain theories de-rived from massive and diverse corpora such as theWeb, and experimentally confirms both its effective-ness and superiority over traditional ILP algorithmsin this context.
Table 1 shows some example rulesthat were learned by SHERLOCK.This work makes the following contributions:1.
We describe the design and implementation ofthe SHERLOCK system, which utilizes a novel,unsupervised ILP method to learn first-orderHorn clauses from open-domain Web text.1088IsHeadquarteredIn(Company, State) :-IsBasedIn(Company, City) ?
IsLocatedIn(City, State);Contains(Food, Chemical) :-IsMadeFrom(Food, Ingredient) ?
Contains(Ingredient, Chemical);Reduce(Medication, Factor) :-KnownGenericallyAs(Medication, Drug) ?
Reduce(Drug, Factor);ReturnTo(Writer, Place) :- BornIn(Writer, City) ?
CapitalOf(City, Place);Make(Company1, Device) :- Buy(Company1, Company2) ?
Make(Company2, Device);Table 1: Example rules learned by SHERLOCK from Web extractions.
Note that the italicized rules are unsound.2.
We derive an innovative scoring function that isparticularly well-suited to unsupervised learn-ing from noisy text.
For Web text, the scoringfunction yields more accurate rules than severalfunctions from the ILP literature.3.
We demonstrate the utility of SHERLOCK?sautomatically learned inference rules.
Infer-ence using SHERLOCK?s learned rules identi-fies three times as many high quality facts (e.g.,precision ?
0.8) as were originally extractedfrom the Web text corpus.The remainder of this paper is organized as fol-lows.
We start by describing previous work.
Sec-tion 3 introduces the SHERLOCK rule learning sys-tem, with Section 3.4 describing how it estimatesrule quality.
We empirically evaluate SHERLOCK inSection 4, and conclude.2 Previous WorkSHERLOCK is one of the first systems to learn first-order Horn clauses from open-domain Web extrac-tions.
The learning method in SHERLOCK belongsto the Inductive logic programming (ILP) subfieldof machine learning (Lavrac and Dzeroski, 2001).However, classical ILP systems (e.g., FOIL (Quin-lan, 1990) and Progol (Muggleton, 1995)) makestrong assumptions that are inappropriate for opendomains.
First, ILP systems assume high-quality,hand-labeled training examples for each relation ofinterest.
Second, ILP systems assume that constantsuniquely denote individuals; however, in Web textstrings such as ?dad?
or ?John Smith?
are highlyambiguous.
Third, ILP system typically assumecomplete, largely noise-free data whereas tuples ex-tracted from Web text are both noisy and radicallyincomplete.
Finally, ILP systems typically utilizenegative examples, which are not available whenlearning from open-domain facts.
One system thatdoes not require negative examples is LIME (Mc-Creath and Sharma, 1997); We compare SHERLOCKwith LIME?s methods in Section 4.3.
Most prior ILPand Markov logic structure learning systems (e.g.,(Kok and Domingos, 2005)) are not designed to han-dle the noise and incompleteness of open-domain,extracted facts.NELL (Carlson et al, 2010) performs coupledsemi-supervised learning to extract a large knowl-edge base of instances, relations, and inferencerules, bootstrapping from a few seed examples ofeach class and relation of interest and a few con-straints among them.
In contrast, SHERLOCK fo-cuses mainly on learning inference rules, but does sowithout any manually specified seeds or constraints.Craven et al(1998) also used ILP to help infor-mation extraction on the Web, but required trainingexamples and focused on a single domain.Two other notable systems that learn inferencerules from text are DIRT (Lin and Pantel, 2001)and RESOLVER (Yates and Etzioni, 2007).
How-ever, both DIRT and RESOLVER learn only a lim-ited set of rules capturing synonyms, paraphrases,and simple entailments, not more expressive multi-part Horn clauses.
For example, these systems maylearn the rule X acquired Y =?
X bought Y ,which captures different ways of describing a pur-chase.
Applications of these rules often depend oncontext (e.g., if a person acquires a skill, that doesnot mean they bought the skill).
To add the neces-sary context, ISP (Pantel et al, 2007) learned selec-tional preferences (Resnik, 1997) for DIRT?s rules.The selectional preferences act as type restrictions1089Figure 1: Architecture of SHERLOCK.
SHERLOCK learnsinference rules offline and provides them to the HOLMESinference engine, which uses the rules to answer queries.on the arguments, and attempt to filter out incorrectinferences.
While these approaches are useful, theyare strictly more limited than the rules learned bySHERLOCK.The Recognizing Textual Entailment (RTE)task (Dagan et al, 2005) is to determine whetherone sentence entails another.
Approaches to RTEinclude those of Tatu and Moldovan (2007), whichgenerates inference rules from WordNet lexicalchains and a set of axiom templates, and Pennac-chiotti and Zanzotto (2007), which learns inferencerules based on similarity across entailment pairs.
Incontrast with this work, RTE systems reason overfull sentences, but benefit by being given the sen-tences and training data.
SHERLOCK operates oversimpler Web extractions, but is not given guidanceabout which facts may interact.3 System DescriptionSHERLOCK takes as input a large set of open domainfacts, and returns a set of weighted Horn-clause in-ference rules.
Other systems (e.g., HOLMES) use therules to answer questions, infer additional facts, etc.SHERLOCK?s basic architecture is depicted inFigure 1.
To learn inference rules, SHERLOCK per-forms the following steps:1.
Identify a ?productive?
set of classes and in-stances of those classes2.
Discover relations between classes3.
Learn inference rules using the discovered rela-tions and determine the confidence in each ruleThe first two steps help deal with the synonyms,homonyms, and noise present in open-domain the-ories by identifying a smaller, cleaner, and more co-hesive set of facts to learn rules over.SHERLOCK learns inference rules from a collec-tion of open-domain extractions produced by TEX-TRUNNER (Banko et al, 2007).
The rules learnedby SHERLOCK are input to an inference engine andused to find answers to a user?s query.
In this paper,SHERLOCK utilizes HOLMES as its inference enginewhen answering queries, and uses extracted factsof the form R(arg1, arg2) provided by the authorsof TEXTRUNNER, but the techniques presented aremore broadly applicable.3.1 Finding Classes and InstancesSHERLOCK first searches for a set of well-definedclasses and class instances.
Instances of the sameclass tend to behave similarly, so identifying a goodset of instances will make it easier to discover thegeneral properties of the entire class.Options for identifying interesting classes includemanually created methods (WordNet (Miller et al,1990)), textual patterns (Hearst, 1992), automatedclustering (Lin and Pantel, 2002), and combina-tions (Snow et al, 2006).
We use Hearst patternsbecause they are simple, capture how classes and in-stances are mentioned in Web text, and yield intu-itive, explainable groups.Hearst (1992) identified a set of textual patternswhich indicate hyponymy (e.g., ?Class such as In-stance?).
Using these patterns, we extracted 29 mil-lion (instance, class) pairs from a large Web crawl.We then cleaned them using word stemming, nor-malization, and by dropping modifiers.Unfortunately, the patterns make systematic er-rors (e.g., extracting Canada as the name of a cityfrom the phrase ?Toronto, Canada and other cities.?
)To address this issue, we discard the low frequencyclasses of each instance.
This heuristic reduces thenoise due to systematic error while still capturing theimportant senses of each word.
Additionally, we usethe extraction frequency to estimate the probabilitythat a particular mention of an instance refers to eachof its potential classes (e.g., New York appears as acity 40% of the time, a state 35% of the time, and aplace, area, or center the rest of the time).1090Ambiguity presents a significant obstacle whenlearning inference rules.
For example, the corpuscontains the sentences ?broccoli contains this vita-min?
and ?this vitamin prevents scurvy,?
but it is un-clear if the sentences refer to the same vitamin.
Thetwo main sources of ambiguity we observed are ref-erences to a more general class instead of a specificinstance (e.g., ?vitamin?
), and references to a personby only their first or last name.
We eliminate thefirst by removing terms that frequently appear as theclass name with other instances, and the second byremoving common first and last names.The 250 most frequently mentioned class namesinclude a large number of interesting classes (e.g.,companies, cities, foods, nutrients, locations) aswell as ambiguous concepts (e.g., ideas, things).
Wefocus on the less ambiguous classes by eliminatingany class not appearing as a descendant of physicalentity, social group, physical condition, or event inWordNet.
Beyond this filtering we make no use of atype hierarchy and treat classes independently.In our corpus, we identify 1.1 million distinct,cleaned (instance, class) pairs for 156 classes.3.2 Discovering Relations between ClassesNext, SHERLOCK discovers how classes relate toand interact with each other.
Prior work in relationdiscovery (Shinyama and Sekine, 2006) has investi-gated the problem of finding relationships betweendifferent classes.
However, the goal of this work isto learn rules on top of the discovered typed rela-tions.
We use a few simple heuristics to automati-cally identify interesting relations.For every pair of classes (C1, C2), we find a setof typed, candidate relations from the 100 most fre-quent relations in the corpus where the first argu-ment is an instance of C1 and the second argumentis an instance of C2.
For extraction terms with mul-tiple senses (e.g., New York), we split their weightbased on how frequently they appear with each classin the Hearst patterns.However, many discovered relations are rare andmeaningless, arising from either an extraction erroror word-sense ambiguity.
For example, the extrac-tion ?Apple is based in Cupertino?
gives some evi-dence that a fruit may possibly be based in a city.We attempt to filter out incorrectly-typed relationsusing two heuristics.
We first discard any relationwhose weighted frequency falls below a threshold,since rare relations are more likely to arise due toextraction errors or word-sense ambiguity.
We alsoremove relations whose pointwise mutual informa-tion (PMI) is below a threshold T=exp(2) ?
7.4:PMI(R(C1, C2)) =p(R,C1, C2)p(R, ?, ?)
?
p(?, C1, ?)
?
p(?, ?, C2)where p(R, ?, ?)
is the probability a random extrac-tion has relation R, p(?, C1, ?)
is the probability arandom extraction has an instance of C1 as its firstargument, p(?, ?, C2) is similar for the second argu-ment, and p(R,C1, C2) is the probability that a ran-dom extraction has relation R and instances of C1and C2 as its first and second arguments, respec-tively.
A low PMI indicates the relation occurred byrandom chance, which is typically due to ambiguousterms or extraction errors.Finally, we use two TEXTRUNNER specific clean-ing heuristics: we ignore a small set of stop-relations(?be?, ?have?, and ?be preposition?)
and extractionswhose arguments are more than four tokens apart.This process identifies 10,000 typed relations.3.3 Learning Inference RulesSHERLOCK attempts to learn inference rules foreach typed relation in turn.
SHERLOCK receives atarget relation, R, a set of observed examples of therelation, E+, a maximum clause length k, a mini-mum support, s, and an acceptance threshold, t, asinput.
SHERLOCK generates all first-order, definiteclauses up to length k, where R appears as the headof the clause.
It retains each clause that:1.
Contains no unbound variables2.
Infers at least s examples from E+3.
Scores at least t according to the score functionWe now propose a novel score function, and empir-ically validate our choice in Sections 4.3 and 4.4.3.4 Evaluating Rules by Statistical RelevanceThe problem of evaluating candidate rules has beenstudied by many researchers, but typically in either asupervised or propositional context whereas we arelearning first-order Horn-clauses from a noisy set ofpositive examples.
Moreover, due to the incompletenature of the input corpus and the imperfect yield of1091extraction?many true facts are not stated explicitlyin the set of ground assertions used by the learner toevaluate rules.The absence of negative examples, coupled withnoise, means that standard ILP evaluation functions(e.g., (Quinlan, 1990) and (Dzeroski and Bratko,1992)) are not appropriate.
Furthermore, when eval-uating a particular rule with consequent C and an-tecedent A, it is natural to consider p(C|A) but, dueto missing data, this absolute probability estimate isoften misleading: in many cases C will hold givenA but the fact C is not mentioned in the corpus.Thus to evaluate rules over extractions, we needto consider relative probability estimates.
I.e., isp(C|A)  p(C)?
If so, then A is said to be sta-tistically relevant to C (Salmon et al, 1971).Statistical relevance tries to infer the simplest setof factors which explain an observation.
It can beviewed as searching for the simplest propositionalHorn-clause which increases the likelihood of a goalproposition g. The two key ideas in determining sta-tistical relevance are discovering factors which sub-stantially increase the likelihood of g (even if theprobabilities are small in an absolute sense), and dis-missing irrelevant factors.To illustrate these concepts, consider the follow-ing example.
Suppose our goal is to predict if NewYork City will have a storm (S).
On an arbitraryday, the probability of having a storm is fairly low(p(S)  1).
However, if we know that the atmo-spheric pressure on that day is low, this substantiallyincreases the probability of having a storm (althoughthat absolute probability may still be small).
Ac-cording to the principle of statistical relevance, lowatmospheric pressure (LP ) is a factor which predictsstorms (S :- LP ), since p(S|LP ) p(S) .The principle of statistical relevance also identi-fies and removes irrelevant factors.
For example, letM denote the gender of New York?s mayor.
Sincep(S|LP,M) p(S), it na?
?vely appears that stormsin New York depend on the gender of the mayor inaddition to the air pressure.
The statistical relevanceprinciple sidesteps this trap by removing any fac-tors which are conditionally independent of the goal,given the remaining factors.
For example, we ob-serve p(S|LP )=p(S|LP,M), and so we say that Mis not statistically relevant to S. This test applies Oc-cam?s razor by searching for the simplest rule whichexplains the goal.Statistical relevance appears useful in the open-domain context, since all the necessary probabilitiescan be estimated from only positive examples.
Fur-thermore, approximating relative probabilities in thepresence of missing data is much more reliable thandetermining absolute probabilities.Unfortunately, Salmon defined statistical rele-vance in a propositional context.
One technicalcontribution of our work is to lift statistical rele-vance to first order Horn-clauses as follows.
Forthe Horn-clause Head(v1, ..., vn):-Body(v1, ..., vm)(where Body(v1, ..., vm) is a conjunction of function-free, non-negated, first-order relations, and vi ?
Vis the set of typed variables used in the rule), we saythe body helps explain the head if:1.
Observing an instance of the body substantiallyincreases the probability of observing the head.2.
The body contains no irrelevant (conditionallyindependent) terms.We evaluate conditional independence of termsusing ILP?s technique of ?-subsumption, ensuringthere is no more general clause that is similarlypredictive of the head.
Formally, clause C1 ?-subsumes clause C2 if and only if there exists a sub-stitution ?
such thatC1?
?
C2 where each clause istreated as the set of its literals.
For example, R(x, y)?-subsumes R(x, x), since {R(x, y)}?
?
{R(x, x)}when ?={y/x}.
Intuitively, if C1 ?-subsumes C2,it means that C1 is more general than C2.Definition 1 A first-order Horn-clauseHead(...):-Body(...) is statistically relevant ifp(Head(...)|Body(...))  p(Head(...)) and if thereis no clause body B?(...)?
?
Body(...) such thatp(Head(...)|Body(...)) ?
p(Head(...)|B?(...
))In practice it is difficult to determine the proba-bilities exactly, so when checking for statistical rele-vance we ensure that the probability of the rule is atleast a factor t greater than the probability of anysubsuming rule, that is, p(Head(...)|Body(...)) ?t ?
p(Head(...)|B?(...
))We estimate p(Head(...)|B(...)) from the observedfacts by assuming values of Head(...) are generatedby sampling values of B(...) as follows: for variablesvs shared between Head(...) and B(...), we sample1092values of vs uniformly from all observed ground-ings of B(...).
For variables vi, if any, that appearin Head(...) but not in B(...), we sample their valuesaccording to a distribution p(vi|classi).
We estimatep(vi|classi) based on the relative frequency that viwas extracted using a Hearst pattern with classi.Finally, we ensure the differences are statisticallysignificant using the likelihood ratio statistic:2NrXH(...)?{Head(...),?Head(...)}p(H(...)|Body(...))
?
logp(H(...)|Body(...))p(H(...)|B?(...
))where p(?Head(...)|B(...)) = 1?p(Head(...)|B(...))and Nr is the number of results inferred by therule Head(...):-Body(...).
This test is distributed ap-proximately as ?2 with one degree of freedom.
Itis similar to the statistical significance test used inmFOIL (Dzeroski and Bratko, 1992), but has twomodifications since SHERLOCK doesn?t have train-ing data.
In lieu of positive and negative examples,we use whether or not the inferred head value wasobserved, and compare against the distribution of asubsuming clause B?(...)
rather than a known prior.This method of evaluating rules has two impor-tant differences from ILP under a closed world as-sumption.
First, our probability estimates considerthe fact that examples provide varying amounts ofinformation.
Second, statistical relevance finds ruleswith large increases in relative probability, not nec-essarily a large absolute probability.
This is crucialin an open domain setting where most facts are false,which means the trivial rule that everything is falsewill have high accuracy.
Even for true rules, the ob-served estimates p(Head(...)|Body(...))  1 due tomissing data and noise.3.5 Making InferencesIn order to benefit from learned rules, we needan inference engine; with its linear-time scalabil-ity, HOLMES is a natural choice (Schoenmackerset al, 2008).
As input HOLMES requires a targetatom H(...), an evidence set E and weighted ruleset R as input.
It performs a form of knowledgebased model construction (Wellman et al, 1992),first finding facts using logical inference, then esti-mating the confidence of each using a Markov LogicNetwork (Richardson and Domingos, 2006).Prior to running inference, it is necessary to assigna weight to each rule learned by SHERLOCK.
Sincethe rules and inferences are based on a set of noisyand incomplete extractions, the algorithms used forboth weight learning and inference should capturethe following characteristics of our problem:C1.
Any arbitrary unknown fact is highly unlikelyto be true.C2.
The more frequently a fact is extracted from theWeb, the more likely it is to be true.
However,facts in E should have a confidence boundedby a threshold pmax < 1.
E contains system-atic extraction errors, so we want uncertainty ineven the most frequently extracted facts.C3.
An inference that combines uncertain factsshould be less likely than each fact it uses.Next, we describe the needed modifications to theweight learning and inference algorithm to achievethe desired behavior.3.5.1 Weight LearningWe use the discriminative weight learning proce-dure described by Huynh and Mooney (2008).
Set-ting the weights involves counting the number oftrue groundings for each rule in the data (Richard-son and Domingos, 2006).
However, the noisy na-ture of Web extractions will make this an overesti-mate.
Consequently, we compute ni(E), the numberof true groundings of rule i, as follows:ni(E) =?jmaxk?B(...)?Bodyijkp(B(...)) (1)where E is the evidence, j ranges over heads of therule, Bodyijk is the body of the kth grounding forjth head of rule i, and p(B(...)) is approximated us-ing a logistic function of the number of times B(...)was extracted,1 scaled to be in the range [0,0.75].This models C2 by giving increasing but boundedconfidence for more frequently extracted facts.
Inpractice, this also helps address C3 by giving longerrules smaller values of ni, which reflects that infer-ences arrived at through a combination of multiple,noisy facts should have lower confidence.
Longerrules are also more likely to have multiple ground-ings that infer a particular head, so keeping only themost likely grounding prevents a head from receiv-ing undue weight from any single rule.1We note that this approximation is equivalent to an MLNwhich uses only the two rules defined in 3.5.21093Finally, we place a very strong Gaussian prior(i.e., L2 penalty) on the weights.
Longer rules have ahigher prior to capture the notion that they are morelikely to make incorrect inferences.
Without a highprior, each rule would receive an unduly high weightas we have no negative examples.3.5.2 Probabilistic InferenceAfter learning the weights, we add the followingtwo rules to our rule set:1.
H(...) with negative weight wprior2.
H(...):-ExtractedFrom(H(...),sentencei)with weight 1The first rule models C1, by saying that most factsare false.
The second rule models C2, by stating theprobability of fact depends on the number of times itwas extracted.
The weights of these rules are fixed.We do not include these rules during weight learningas doing so swamps the effects of the other inferencerules (i.e., forces them to zero).HOLMES attempts to infer the truth value of eachground atom H(...) in turn by treating all other ex-tractionsE in our corpus as evidence.
Inference alsorequires computing ni(E) which we do according toEquation 1 as in weight learning.4 ExperimentsOne can attempt to evaluate a rule learner by esti-mating the quality of learned rules, or by measuringtheir impact on a system that uses the learned rules.Since the notion of ?rule quality?
is vague exceptin the context of an application, we evaluate SHER-LOCK in the context of the HOLMES inference-basedquestion answering system.Our evaluation focuses on three main questions:1.
Does inference utilizing learned Horn rules im-prove the precision/recall of question answer-ing and by how much?2.
How do different rule-scoring functions affectthe performance of learning?3.
What role does each of SHERLOCK?s compo-nents have in the resulting performance?4.1 MethodologyOur objective with rule learning was to improve thesystem?s ability to answer questions such as ?Whatfoods prevent disease??
So we focus our evaluationon the task of computing as many instances as pos-sible of an atomic pattern Rel(x, y).
In this exam-ple, Rel would be bound to ?Prevents?, xwould havetype ?Food?
and y would have type ?Disease.
?But which relations should be used in the test?There is a large variance in behavior across relations,so examining any particular relation may give mis-leading results.
Instead, we examine the global per-formance of the system by querying HOLMES forall open-domain relations identified in Section 3.2as follows:1.
Score all candidate rules according to the rulescoring metric M , accept all rules with a scoreat least tM (tuned on a small development set ofrules), and learn weights for all accepted rules.2.
Find all facts inferred by the rules and use therule weights to estimate the fact probabilities.3.
Reduce type information.
For each fact, (e.g.,BasedIn(Diebold, Ohio)) which has been de-duced with multiple type signatures (e.g., Ohiois both a state and a geographic location), keeponly the one with maximum probability (i.e.,conservatively assuming dependence).4.
Place all results into bins based on their proba-bilities, and estimate the precision and the num-ber of correct facts in the bin using a randomsample.In these experiments we consider rules with up tok = 2 relations in the body.
We use a corpus of1 million raw extractions, corresponding to 250,000distinct facts.
SHERLOCK found 5 million candidaterules that infer at least two of the observed facts.
Un-less otherwise noted, we use SHERLOCK?s rule scor-ing function to evaluate the rules (Section 3.4).The results represent a wide variety of domains,covering a total of 10,672 typed relations.
We ob-serve between a dozen and 2,375 distinct, groundfacts for each relation.
SHERLOCK learned a totalof 31,000 inference rules.2 Learning all rules, rule2The learned rules are available at:http://www.cs.washington.edu/research/sherlock-hornclauses/109400.20.40.60.810 350000 700000 1050000 1400000PrecisionofInferredFactsEstimated Number of Correct FactsBenefits of Inference using Learned RulesSherlock With Complex RulesSherlock With Only Simple EntailmentsNo InferenceInferred by SimpleEntailment RulesInferred byMulti-PartHorn RulesExtractedFactsFigure 2: Inference discovers many facts which are notexplicitly extracted, identifying 3x as many high qualityfacts (precision 0.8) and more than 5x as many facts over-all.
Horn-clauses with multiple relations in the body in-fer 30% more correct facts than are identified by simplerentailment rules, inferring many facts not present in thecorpus in any form.weights, and performing the inference took 50 min-utes on a 72 core cluster.
However, we note that forhalf of the relations SHERLOCK accepts no inferencerules, and remind the reader that the performance onany particular relation may be substantially differ-ent, and depends on the facts observed in the corpusand on the rules learned.4.2 Benefits of InferenceWe first evaluate the utility of the learned Horn rulesby contrasting the precision and number of correctand incorrect facts identified with and without infer-ence over learned rules.
We compare against twosimpler variants of SHERLOCK.
The first is a no-inference baseline that uses no rules, returning onlyfacts that are explicitly extracted.
The second base-line only accepts rules of length k = 1, allowing it tomake simple entailments but not more complicatedinferences using multiple facts.Figure 2 compares the precision and estimatednumber of correct facts with and without inference.As is apparent, the learned inference rules substan-tially increase the number of known facts, quadru-pling the number of correct facts beyond what areexplicitly extracted.The Horn rules having a body-length of two iden-tify 30% more facts than the simpler length-onerules.
Furthermore, we find the Horn rules yieldslightly increased precision at comparable levels ofrecall, although the increase is not statistically sig-nificant.
This behavior can be attributed to learn-ing smaller weights for the length-two rules thanthe length-one rules, allowing the length-two rulesprovide a small amount of additional evidence asto which facts are true, but typically not enough toovercome the confidence of a more reliable length-one rule.Analyzing the errors, we found that aboutone third of SHERLOCK?s mistakes are dueto metonymy and word sense ambiguity (e.g.,confusing Vancouver, British Columbia withVancouver, Washington), one third are due toinferences based on incorrectly-extracted facts(e.g., inferences based on the incorrect factIsLocatedIn(New York, Suffolk County),which was extracted from sentences like ?DeerPark, New York is located in Suffolk County?
),and the rest are due to unsound or incorrectinference rules (e.g., BasedIn(Company, City):-BasedIn(Company, Country)?
CapitalOf(City,Country)).
Without negative examples it is difficultto distinguish correct rules from these unsoundrules, since the unsound rules are correct more oftenthan expected by chance.Finally, we note that although simple, length-onerules capture many of the results, in some respectsthey are just rephrasing facts that are extracted inanother form.
However, the more complex, length-two rules synthesize facts extracted from multiplepages, and infer results that are not stated anywherein the corpus.4.3 Effect of Scoring FunctionWe now examine how SHERLOCK?s rule scoringfunction affects its results, by comparing it withthree rule scoring functions used in prior work:LIME.
The LIME ILP system (McCreath andSharma, 1997) proposed a metric that generalizedMuggleton?s (1997) positive-only score functionby modeling noise and limited sample sizes.M-Estimate of rule precision.
This is a commonapproach for handling noise in ILP (Dzeroski andBratko, 1992).
It requires negative examples,which we generated by randomly swapping argu-ments between positive examples.109500.20.40.60.810 500000 1000000 1500000 2000000 2500000PrecisionofInferredFactsEstimated Number of Correct FactsComparison of Rule Scoring FunctionsSherlockLIMEM-EstimateL1 Reg.Figure 3: SHERLOCK identifies rules that lead to moreaccurate inferences over a large set of open-domain ex-tracted facts, deducing 2x as many facts at precision 0.8.L1 Regularization.
As proposed in (Huynh andMooney, 2008), this learns weights for all can-didate rules using L1-regularization (encouragingsparsity) instead of L2-regularization, and retainsonly those with non-zero weight.Figure 3 compares the precision and estimatednumber of correct facts inferred by the rules ofeach scoring function.
SHERLOCK has consistentlyhigher precision, and finds twice as many correctfacts at precision 0.8.M-Estimate accepted eight times as many rules asSHERLOCK, increasing the number of inferred factsat the cost of precision and longer inference times.Most of the errors in M-Estimate and L1 Regulariza-tion come from incorrect or unsound rules, whereasmost of the errors for LIME stem from systematicextraction errors.4.4 Scoring Function Design DecisionsSHERLOCK requires a rule to have statistical rele-vance and statistical significance.
We perform anablation study to understand how each of these con-tribute to SHERLOCK?s results.Figure 4 compares the precision and estimatednumber of correct facts obtained when requiringrules to be only statistically relevant, only statisti-cally significant, or both.
As is expected, there isa precision/recall tradeoff.
SHERLOCK has higherprecision, finding more than twice as many results atprecision 0.8 and reducing the error by 39% at a re-call of 1 million correct facts.
Statistical significancefinds twice as many correct facts as SHERLOCK, butthe extra facts it discovers have precision < 0.4.00.20.40.60.810 500000 1000000 1500000 2000000 2500000 3000000PrecisionofInferredFactsEstimated Number of Correct FactsDesign Decisions of Sherlock?s Scoring FunctionSherlockStatistical RelevanceStatistical SignificanceFigure 4: By requiring rules to have both statistical rel-evance and statistical significance, SHERLOCK rejectsmany error-prone rules that are accepted by the metricsindividually.
The better rule set yields more accurate in-ferences, but identifies fewer correct facts.Comparing the rules accepted in each case, wefound that statistical relevance and statistical signifi-cance each accepted about 180,000 rules, comparedto about 31,000 for SHERLOCK.
The smaller setof rules accepted by SHERLOCK not only leads tohigher precision inferences, but also speeds up in-ference time by a factor of seven.In a qualitative analysis, we found the statisti-cal relevance metric overestimates probabilities forsparse rules, which leads to a number of very highscoring but meaningless rules.
The statistical signif-icance metric handles sparse rules better, but is stilloverconfident in the case of many unsound rules.4.5 Analysis of Weight LearningFinally, we empirically validate the modifications ofthe weight learning algorithm from Section 3.5.1.The learned-rule weights only affect the probabil-ities of the inferred facts, not the inferred facts them-selves, so to measure the influence of the weightlearning algorithm we examine the recall at preci-sion 0.8 and the area under the precision-recall curve(AuC).
We build a test set by holding SHERLOCK?sinference rules constant and randomly sampling 700inferred facts.
We test the effects of:?
Fixed vs.
Variable Penalty - Do we use thesame L2 penalty on the weights for all rules ora stronger L2 penalty for longer rules??
Full vs.
Weighted Grounding Counts - Do wecount all unweighted rule groundings (as in(Huynh and Mooney, 2008)), or only the bestweighted one (as in Equation 1)?1096Recall(p=0.8) AuCVariable Penalty, Weighted 0.35 0.735Counts (used by SHERLOCK)Variable Penalty, Full Counts 0.28 0.726Fixed Penalty, Weighted Counts 0.27 0.675Fixed Penalty, Full Counts 0.17 0.488Table 2: SHERLOCK?s modified weight learning algo-rithm gives better probability estimates over noisy and in-complete Web extractions.
Most of the gains come frompenalizing longer rules more, but using weighted ground-ing counts further improves recall by 0.07, which corre-sponds to almost 100,000 additional facts at precision 0.8.We vary each of these independently, and give theperformance of all 4 combinations in Table 2.The modifications from Section 3.5.1 improveboth the AuC and the recall at precision 0.8.
Mostof the improvement is due to using stronger penal-ties on longer rules, but using the weighted countsin Equation 1 improves recall by a factor of 1.25 atprecision 0.8.
While this may not seem like much,the scale is such that it leads to almost 100,000 ad-ditional correct facts at precision 0.8.5 ConclusionThis paper addressed the problem of learning first-order Horn clauses from the noisy and heteroge-neous corpus of open-domain facts extracted fromWeb text.
We showed that SHERLOCK is ableto learn Horn clauses in a large-scale, domain-independent manner.
Furthermore, the learned rulesare valuable, because they infer a substantial numberof facts which were not extracted from the corpus.While SHERLOCK belongs to the broad categoryof ILP learners, it has a number of novel features thatenable it to succeed in the challenging, open-domaincontext.
First, SHERLOCK automatically identifiesa set of high-quality extracted facts, using severalsimple but effective heuristics to defeat noise andambiguity.
Second, SHERLOCK is unsupervised anddoes not require negative examples; this enables it toscale to an unbounded number of relations.
Third, itutilizes a novel rule-scoring function, which is toler-ant of the noise, ambiguity, and missing data issuesprevalent in facts extracted from Web text.
The ex-periments in Figure 3 show that, for open-domainfacts, SHERLOCK?s method represents a substantialimprovement over traditional ILP scoring functions.Directions for future work include inducinglonger inference rules, investigating better methodsfor combining the rules, allowing deeper inferencesacross multiple rules, evaluating our system on othercorpora and devising better techniques for handlingword sense ambiguity.AcknowledgementsWe thank Sonal Gupta and the anonymous review-ers for their helpful comments.
This research wassupported in part by NSF grant IIS-0803481, ONRgrant N00014-08-1-0431, the WRF / TJ Cable Pro-fessorship and carried out at the University of Wash-ington?s Turing Center.
The University of Washing-ton gratefully acknowledges the support of DefenseAdvanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract nos.
FA8750-09-C-0179 and FA8750-09-C-0181.
Any opinions,findings, and conclusion or recommendations ex-pressed in this material are those of the author(s) anddo not necessarily reflect the view of the DARPA,AFRL, or the US government.ReferencesM.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the Web.
In Procs.
of IJCAI.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of theTwenty-Fourth Conference on Artificial Intelligence(AAAI 2010).M.
Craven, D. DiPasquo, D. Freitag, A.K.
McCallum,T.
Mitchell, K. Nigam, and S. Slattery.
1998.
Learn-ing to Extract Symbolic Knowledge from the WorldWide Web.
In Procs.
of the 15th Conference of theAmerican Association for Artificial Intelligence, pages509?516, Madison, US.
AAAI Press, Menlo Park, US.I.
Dagan, O. Glickman, and B. Magnini.
2005.
ThePASCAL Recognising Textual Entailment Challenge.Proceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment, pages 1?8.S.
Dzeroski and I. Bratko.
1992.
Handling noise in in-ductive logic programming.
In Proceedings of the 2nd1097International Workshop on Inductive Logic Program-ming.M.
Hearst.
1992.
Automatic Acquisition of Hyponymsfrom Large Text Corpora.
In Procs.
of the 14th In-ternational Conference on Computational Linguistics,pages 539?545, Nantes, France.T.N.
Huynh and R.J. Mooney.
2008.
Discriminativestructure and parameter learning for Markov logic net-works.
In Proceedings of the 25th international con-ference on Machine learning, pages 416?423.
ACM.Stanley Kok and Pedro Domingos.
2005.
Learning thestructure of markov logic networks.
In ICML ?05:Proceedings of the 22nd international conference onMachine learning, pages 441?448, New York, NY,USA.
ACM.N.
Lavrac and S. Dzeroski, editors.
2001.
RelationalData Mining.
Springer-Verlag, Berlin, September.D.
Lin and P. Pantel.
2001.
DIRT ?
Discovery of Infer-ence Rules from Text.
In KDD.D.
Lin and P. Pantel.
2002.
Concept discovery from text.In Proceedings of the 19th International Conferenceon Computational linguistics (COLING-02), pages 1?7.E.
McCreath and A. Sharma.
1997.
ILP with noiseand fixed example size: a Bayesian approach.
In Pro-ceedings of the Fifteenth international joint conferenceon Artifical intelligence-Volume 2, pages 1310?1315.Morgan Kaufmann Publishers Inc.G.
Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.
Miller.
1990.
Introduction to WordNet: An on-linelexical database.
International Journal of Lexicogra-phy, 3(4):235?312.S.
Muggleton.
1995.
Inverse entailment and Progol.New Generation Computing, 13:245?286.S.
Muggleton.
1997.
Learning from positive data.
Lec-ture Notes in Computer Science, 1314:358?376.P.
Pantel, R. Bhagat, B. Coppola, T. Chklovski, andE.
Hovy.
2007.
ISP: Learning inferential selectionalpreferences.
In Proceedings of NAACL HLT, vol-ume 7, pages 564?571.M.
Pennacchiotti and F.M.
Zanzotto.
2007.
LearningShallow Semantic Rules for Textual Entailment.
Pro-ceedings of RANLP 2007.J.
R. Quinlan.
1990.
Learning logical definitions fromrelations.
Machine Learning, 5:239?2666.Philip Resnik.
1997.
Selectional preference and sensedisambiguation.
In Proc.
of the ACL SIGLEX Work-shop on Tagging Text with Lexical Semantics: Why,What, and How?M.
Richardson and P. Domingos.
2006.
Markov LogicNetworks.
Machine Learning, 62(1-2):107?136.W.C.
Salmon, R.C.
Jeffrey, and J.G.
Greeno.
1971.
Sta-tistical explanation & statistical relevance.
Univ ofPittsburgh Pr.S.
Schoenmackers, O. Etzioni, and D. Weld.
2008.
Scal-ing Textual Inference to the Web.
In Procs.
of EMNLP.Y.
Shinyama and S. Sekine.
2006.
Preemptive informa-tion extraction using unrestricted relation discovery.In Procs.
of HLT/NAACL.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InCOLING/ACL 2006.M.
Tatu and D. Moldovan.
2007.
COGEX at RTE3.
InProceedings of the ACL-PASCAL Workshop on TextualEntailment and Paraphrasing, pages 22?27.M.P.
Wellman, J.S.
Breese, and R.P.
Goldman.
1992.From knowledge bases to decision models.
TheKnowledge Engineering Review, 7(1):35?53.A.
Yates and O. Etzioni.
2007.
Unsupervised resolutionof objects and relations on the Web.
In Procs.
of HLT.1098
