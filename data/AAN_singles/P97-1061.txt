Retrieving Collocationsby Co-occurrences and Word Order ConstraintsSayor i  Sh imohata ,  Tosh iyuk i  Sug io  and  Jun j i  NagataKansa i  Laboratory ,  Research  &: Deve lopment  GroupOki  E lectr ic  Indust ry  Co.,  Ltd.Crys ta l  Tower  1-2-27, Sh i romi ,Chuo-ku ,  Osaka ,  540, Japan{ sayori, sugio, nagat a} ?kansai.
oki.
co. j pAbst rac tIn this paper, we describe a method forautomatically retrieving collocations fromlarge text corpora.
This method retrievecollocations in the following stages: 1) ex-tracting strings of characters as units ofcollocations 2) extracting recurrent combi-nations of strings in accordance with theirword order in a corpus as collocations.Through the method, various range of col-locations, especially domain specific collo-cations, are retrieved.
The method is prac-tical because it uses plain texts without anyinformation dependent on a language suchas lexical knowledge and parts of speech.1 IntroductionA collocation is a recurrent combination of words,ranging from word level to sentence level.
In this pa-per, we classify collocations into two types accordingto their structures.
One is an uninterrupted colloca-tion which consists of a sequence of words, the otheris an interrupted collocation which consists of wordscontaining one or several gaps filled in by substi-tutable words or phrases which belong to the samecategory.The features of collocations are defined as follows:?
collocations are recurrent?
collocations consist of one or several lexicalunits?
order of units are rigid in a collocation.For language processing such as machine trans-lation, a knowledge of domain specific collocationsis indispensable because what collocations mean aredifferent from their literal meaning and the usageand meaning of a collocation is totally dependenton each domain.
In addition, new collocations areproduced one after another and most of them aretechnical jargons.There has been a growing interest in corpus-basedapproaches which retrieve collocations from largecorpora (Nagao and Mori, 1994), (Ikehara et al,1996) (Kupiec, 1993), (Fung, 1995), (Kitamura andMatsumoto, 1996), (Smadja, 1993), (Smadja et al,1996), (Haruno et al, 1996).
Although these ap-proaches achieved good results for the task consid-ered, most of them aim to extract fixed collocations,mainly noun phrases, and require the informationwhich is dependent on each language such as dictio-naries and parts of speech.
From a practical point ofview, however, a more robust and flexible approachis desirable.We propose a method to retrieve interruptedand uninterrupted collocations by the frequenciesof co-occurrences and word order constraints froma monolingual corpus.
The method comprises twostages: the first stage extracts sequences of words(or characters) t from a corpus as units of colloca-tions and the second stage extracts recurrent com-binations of units and constructs collocations by ar-ranging them in accordance with word order in thecorpus.2 Algorithm2.1 Ext rac t ing  units of collocation(Nagao and Mori, 1994) developed a method tocalculate the frequencies of strings composed of ncharacters(a grams).
Since this method generatesall n-character strings appeared in a text, the outputcontains a lot of fragments and useless expressions.For example, even if "local", "area", and "network"always appear as the substrings of '% local area net-work" in a corpus, this method generates redundantstrings such as "a local", "a local area" and "areanetwork".To filter out the fragments, we measure the dis-tribution of adjacent words preceding and following1A word is recognized as a minimum unit in such alanguage as English where writespace is used to delimitwords, while a character is recognized as that in suchlanguages as Japanese and Chinese which have no worddelimiters.
Although the method escribed in this paperis applicable to either kinds of languages, we have takenEnglish as an example.476the strings using entropy threshold.
This is basedon the idea that adjacent words will be widely dis-tributed if the string is meaningful, and they willbe localized if the string is a substring of a mean-ingful string.
Taking the example mentioned above,the words which follow % local area" are practi-cally identified as "network" because % local area"is a substring of % local area network" in the cor-pus.
On the contrary, the words which follow %local area network" are hardly identified because "alocal area network" is a unit of expression and innu-merable words are possible to follow the string.
Itmeans that the distribution of adjacent words is ef-fective to judge whether the string is an appropriateunit or not.We introduce ntropy value, which is a measure ofdisorder.
Let the string be str, the adjacent wordswl...wn, and the frequency of str freq(str).
Theprobability of each possible adjacent word p(wi) isthen:y~eq(wi)p(wi)-  freq(str) (1)At that time, the entropy of str H(str) is definedas:7 lH(str) = ~ -p(wi)logp(wi) (2)i=1H(str) takes the highest value if n = freq(str) and1 for all and it takes the lowest value 0 p(wi )  = -~ wi,if n = 1 and p(wi) = 1.
Calculating the entropy ofboth sides of the string, we adopt the lower one asthe entropy of the string.
Str is accepted only if thefollowing inequation is satisfied:H(str) > Tentropu (3)Fragmental strings such as "a local" and "areanetwork" are filtered out with these procedures be-cause their entropy values are expected to be small.Most of the strings extracted in this stage are mean-ingful units such as compound words, prepositionalphrases, and idiomatic expressions.
These stringsare uninterrupted collocations of themselves whilethey are used in the next stage to construct colloca-tions.
This method is useful for the languages with-out word delimiters, and for the other languages aswell.2.2 Extracting col locationsBy the use of each string derived in the previousstage, this stage extracts strings which frequentlyco-occur with the string and constructs them as acollocation.
It is based on the idea that there is astring which is used to induce a collocation.
We callthis string % key string", hereafter.
The followingsare the procedures to retrieve a collocation:1.
Take a key string strk from the strings stri(i =1...n), and retrieve sentences containing strkfrom the corpus.2.
Examine how often each possible combinationsof str~ and stri co-occurs, and extract stri ifthe frequency exceeds a given threshold Tire q.3.
Examine every two strings stri and strj andrefine them by the following steps alternately:?
Combine stri and strj when they overlapor adjoin each other and the following in-equation is satisfied:freq(stri, strj )freq(stri) > Tratio (4)?
Filter out stri if strj subsumes tri and thefollowing inequation is satisfied:freq(strj)freq(srti) >Tratio (5)4.
Construct a collocation by arranging the stringsstri in accordance with the word order in thecorpus.The second step and the third step narrow downthe strings to the units of collocation.
Through thesesteps, only the strings which significantly co-occurwith the key string strk are extracted.The second step eliminates the strings that are notfrequent enough.
Consider the example of Figure 1.This is a list of sentences containing the key string"Refer to" retrieved and each underlined string cor-responds to a string stri.
Assuming the frequencythreshold Tlr~q as 2, the strings which co-occur withstr~ more than twice are extracted in the secondstep.
Table 1 shows the result of this step.
Al-though it is very simple technique, almost all theuseless trings are excluded through this step.stri f req( strk , stri )the 4manual 4for specific instructions 3on 2Table 1: Result of the second stepThe third step reorganizes the strings to be opti-mum units in the specific context.
This is based onthe idea that a longer string is more significant as aunit of collocations if it is frequent enough.
Assum-ing that the threshold Tra~io is 0.75, first, a string"manual for specific instructions" is produced as theinequation (4) is satisfied.
Next, "manual" and "forspecific instructions" are deleted as the inequation(5) is satisfied.
This process is repeated until nostring satisfies the inequations.
Table 2 shows a re-sult of this step.The fourth step constructs a collocation by ar-ranging the strings in accordance with the word or-der in the sentences retrieved in the first step.
Tak-ing stri in order of frequency, this step determines477Refer to the appropriate manual for instructions o_nn...Refer t.o.
the manual for specific instructions.Refer to the installation manual for specific instructions fo__?r ...Refer to the manual for specific in '~~-~f fn  ~Figure 1: Sentences containing "Refer to"l s t r i  f req( strk , stri )the 4manual for specific instructions 3on 2Table 2: Result of the third stepwhere stri is placed in a collocation.
In this example,the position of "the" is examined first.
Accordingto the sentences shown in Figure 1, "the" is alwaysplaced next to "Refer to".
Then its position is de-termined to follow "Refer to".
Next, the position of"manual for specific instructions" is examined and itis determined to follow a gap placed after "Refer tothe".
Finally, the following collocation is produced:"Refer  to the ... manua lfor specific instruct ions on ..."The broken lines in the collocation indicates the gapswhere any substitutable words or phrases can befilled in.
In the example, "appropriate" or "installa-tion" is filled in the first gap.Thus, we retrieve an arbitrary length of inter-rupted or uninterrupted collocation induced by thekey string.
This procedure is performed for eachstring obtained in the previous tage.
By changingthe threshold, various levels of collocations are re-trieved.3 Eva luat ionWe performed an experiment for evaluating the al-gorithm.
The corpus used in the experiment isa computer manual written in English comprising1,311,522 words (in 120,240 sentences).In the first stage of this method, 167,387 stringsare produced.
Among them, 650, 1950, 6774 stringsare extracted over the entropy threshold 2, 1.5, 1 re-spectively.
For 650 strings whose entropy is greaterthan 2, 162 strings (24.9%) are complete sentences,297 strings (45.7%) are regarded as grammaticallyappropriate units, and 114 strings (17.5%) are re-garded as meaningful units even though they are notgrammatical.
This told us that the precision of thefirst stage is 88.1%.Table 3 shows top 20 strings in order of entropyvalue.
They are quite representative of the given do-main.
Most of them are technical jargons related tocomputers and typical expressions used in manualdescriptions although they vary in their construc-tions.
It is interesting to note that the strings whichdo not belong to the grammatical units also takehigh entropy value.
Some of them contain punctua-tion, and some of them terminate in articles.
Punc-tuation marks and function words in the strings areuseful to recognize how the strings are used in a cor-pus.Table 4 illustrates how the entropy is changed withthe change of string length.
The third column in thetable shows the kinds of adjacent words which followthe strings.
The table shows that the ungrammaticalstrings such as "For more information on" and "Formore information, refer to" act more cohesively thanthe grammatical string "For more information" inthe corpus.
Actually, the former strings are moreuseful to construct collocations in the second stage.In the second stage, we extracted collocationsfrom 411 key strings retrieved in the first stage (297grammatical units and 114 meaningful units).
Nec-essary thresholds are given by the following set ofequations:r I~q ~ x 0.1 ~- \]req(str~)Tratio = 0.8As a result, 269 combinations of units are retrievedas collocations.
Note that collocations are not gen-erated from all the key strings because some of themare uninterrupted collocations in themselves like No.2 in Table 3.
Evaluation is done by human check and180 collocations are regarded as meaningful.
Theprecision is 43.8% when the number of meaning-ful collocation is divided by the number of the keystrings and 66.9% when it is divided by the numberof the collocations retrieved in the second stage 2.Table 5 shows the collocations extracted with theunderlined key strings.
The table indicates that ar-bitrary length of collocations, which are frequentlyused in computer manuals, are retrieved throughthe method.
As the method focuses on the co-occurrence of strings, most of the collocations arespecific to the given domain.
Common collocationsare tend to be ignored because they are not used re-peatedly in a single text.
It is not a serious problem,2Usually the latter ratio is adopted as precision.478however, becausecommon collocations are limitedin number and we can efficiently obtain them fromdictionaries or by human reflection.No.
7 and 8 in Table 5 are the examples of in-valid collocations.
They contain unnecessary stringssuch as "to a" and ", the" in them.
The majority ofinvalid collocations are of this type.
One possible so-lution is to eliminate unnecessary strings at the sec-ond stage.
Most of the unnecessary strings consist ofonly punctuation marks and function words.
There-fore, by filtering out these strings, invalid colloca-tions produced by the method should be reduced.Figure 2 summarizes the result of the evaluation.In the experiment, 573 strings are retrieved as appro-priate units of collocations and 180 combinations ofunits are retrieved as appropriate collocations.
Pre-cision is 88.1% in the first stage, and 66.9% in thesecond stage.1st stage 2nd stageCS= 162(24.9%)GU=297(45.7%)MU=114(17.5%)F=77(11.9%)MC=180(43.8%)F=89(21.7%)NC= 142(34.5%)CS: complete sentencesGU: grammatical unitsMU: meaningful unitsMC: meaningful collocationsF: fragmentsNC: not capturedFigure 2: Summary of evaluationAlthough evaluation of retrieval systems is usu-ally performed with precision and recall, we cannotexamine recall rate in the experiment.
It is difficultto recognize how many collocations are in a corpusbecause the measure differs largely dependent on thedomain or the application considered.
As an alter-native way to evaluate the algorithm, we are plan-ning to apply the collocations retrieved to a machinetranslation system and evaluate how they contributeto the quality of translation.4 Re la ted  workAlgorithms for retrieving collocations has been de-scribed (Smadja, 1993) (Haruno et al, 1996).
(Smadja, 1993) proposed a method to re-trieve collocations by combining bigrams whose co-occurrences are greater than a given threshold 3.
Intheir approach, the bigrams are valid only whenthere are fewer than five words between them.
Thisis based on the assumption that "most of the lexicalrelations involving a word w can be retrieved by ex-amining the neighborhood of w wherever it occurs,within a span of five (-5 and +5 around w) words.
"While the assumption is reasonable for some lan-guages such as English, it cannot be applied to allthe languages, especially to the languages withoutword delimiters.
(Haruno et al, 1996) constructed collocations bycombining a couple of strings 4 of high mutual in-formation iteratively.
But the mutual informationis estimated inadequately lower when the cohesive-ness between two strings is greatly different.
Take"in spite (of)", for example.
Despite the fact that"spite" is frequently used with "in", mutual informa-tion between "in" and "spite" is small because "in"is used in various ways.
Thus, there is the possibilitythat the method misses ignificant collocations eventhough one of the strings have strong cohesiveness.In contrast o these methods, our method focuseson the distribution of adjacent words (or charac-ters) when retrieving units of collocation and theco-occurrence frequencies and word order between akey string and other strings when retrieving colloca-tions.
Through the method, various kinds of collo-cations induced by key strings are retrieved regard-less of the number of units or the distance betweenunits in a collocation.
Another distinction is thatour method does not require any lexical knowledgeor language dependent information such as part ofspeech.
Owing to this, the method have good appli-cability to many languages.5 Conclus ionIn this paper, we described a robust and practi-cal method for retrieving collocations by the co-occurrence of strings and word order constraints.Through the method, various range of collocationswhich are frequently used in a specific domain areretrieved automatically.
This method is applicableto various languages because it uses a plain tex-tual corpus and requires only the general informa-tion appeared in the corpus.
Although the colloca-tions retrieved by the method are monolingual andthey are not available to the machine application forthe present, the results will be extensible in variousways.
We plan to compile a knowledge of bilingualcollocations by incorporating the method with con-ventional bilingual approaches.3This approach is similar to the process of the stringrefinement described in this paper.4They call the strings word chunks.479No.
str H(str) freq(str)1 the current functional area 3.8 452 Before you install this device : 3.78 443 This could introduce data corruption .
3.37 294 All rights are reserved .
3.37 295 Note that the 2.93 536 , such as 2.91 877 Information on minor numbers is in 2.45 208 , for example , 2.44 239 The default is 2.44 5210 , you can use the 2.26 2511 to see if the 2.2 2412 stands for 2.15 3013 system accounting : 2.14 4814 These are 2.12 3715 allocation policy 2.1 2116 For example , the 2.1 9717 For more information on 2.1 9618 permission bits 2.07 2619 By default ,  the 2.06 3220 The syntax for 2.03 57Table 3: Top 20 strings extracted at the first stagestr H(str) n fveq(str)For more 0.13 7 200For more information 0.33 3 168For more information , 0.21 4 46For more information , see 1.03 8 25For more information , refer to 1.17 6 15For more information on 2.1 56 96For more information about 1.69 21 35Table 4: Strings including "For more"No.
collocation1 For more information on ..., refer to the ... manual.2 You can use the ... to help you.3 The syntax for .... is : ...4 output from the execution of.. .
commands.5 ..., use the ... command with the ... option6 ... have a special meaning in this manual.7 ... to a (such as ..., and ...).8 ... if the system ...or a ...for a...,the..Table 5: Examples of collocations extracted at the second stage480ReferencesPascale Fung.
1995.
Compiling bilingual exicon en-tries from a non-parallel English-Chinese corpus.In Proceedings ofthe 3rd Workshop on Very LargeCorpora, pages 173-183.Masahiko Haruno, Satoru Ikehara, and Take-fumi Yamazaki.
1996.
Learning Bilingual Col-locations by Word-Level Sorting.
In Proceedingso/ the 16th COLING, pages 525-530.Satoru Ikehara, Satoshi Shirai, and Hajime Uchino.1996.
A statistical method for extracting unin-terrupted and interrupted collocations from verylarge corpora.
In Proceedings of the 16th COL-INC. pages 574-579.Mihoko Kitamura and Yuji Matsumoto.
1996.
Au-tomatic extraction of word sequence correspon-dences in parallel corpora.
In Proceedings of the4th Workshop on Very Large Corpora, pages 79-87.Julian Kupiec.
1993.
An algorithm for finding nounphrase correspondences in bilingual corpora.
InProceedings of the 31th Annual Meeting of ACL,pages 17-22.Makoto Nagao and Shinsuke Mori.
1994.
NewMethod of n-gram statistics for large number of nand automatic extranetion of words and phrasesfrom large text data of Japanese.
In Proceedingsof the 15th COLING, pages 611-615.Frank Smadja.
1993.
Retrieving collocations fromtext: Xtraet.
In Computational Linguistics,19(1), pages 143-177.Frank Smadja, Kathleen MaKe-own, and Vasileios Hatzivassiloglou.
1996.
Trans-lating collocations for bilingual lexicons: A sta-tistical approach.
In Computational Linguistics,22(1), pages 1-38.481
