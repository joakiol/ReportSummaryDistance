Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 795?804,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCollective Classification for Fine-grained Information StatusKatja Markert1,2, Yufang Hou2, Michael Strube21 School of Computing, University of Leeds, UK, scskm@leeds.ac.uk2 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany(yufang.hou|michael.strube)@h-its.orgAbstractPrevious work on classifying information sta-tus (Nissim, 2006; Rahman and Ng, 2011)is restricted to coarse-grained classificationand focuses on conversational dialogue.
Wehere introduce the task of classifying fine-grained information status and work on writ-ten text.
We add a fine-grained informationstatus layer to the Wall Street Journal portionof the OntoNotes corpus.
We claim that theinformation status of a mention depends notonly on the mention itself but also on othermentions in the vicinity and solve the task bycollectively classifying the information statusof all mentions.
Our approach strongly outper-forms reimplementations of previous work.1 IntroductionSpeakers present already known and yet to be es-tablished information according to principles re-ferred to as information structure (Prince, 1981;Lambrecht, 1994; Kruijff-Korbayova?
and Steedman,2003, inter alia).
While information structure af-fects all kinds of constituents in a sentence, we hereadopt the more restricted notion of information sta-tus which concerns only discourse entities realizedas noun phrases, i.e.
mentions1.
Information status(IS henceforth) describes the degree to which a dis-course entity is available to the hearer with regard tothe speaker?s assumptions about the hearer?s knowl-edge and beliefs (Nissim et al, 2004).
Old men-tions are known to the hearer and have been referred1Since not all noun phrases are referential, we call nounphrases which carry information status mentions.to previously.
Mediated mentions have not beenmentioned before but are also not autonomous, i.e.,they can only be correctly interpreted by referenceto another mention or to prior world knowledge.
Allother mentions are new.IS can be beneficial for a number of NLP tasks,though the results have been mixed.
Nenkova etal.
(2007) used IS as a feature for generating pitchaccent in conversational speech.
As IS is restrictedto noun phrases, while pitch accent can be assignedto any word in an utterance, the experiments werenot conclusive.
For determining constituent order ofGerman sentences, Cahill and Riester (2009) incor-porate features modeling IS to good effect.
Rahmanand Ng (2011) showed that IS is a useful feature forcoreference resolution.Previous work on learning IS (Nissim, 2006; Rah-man and Ng, 2011) is restricted in several ways.It deals with conversational dialogue, in particularwith the corpus annotated by Nissim et al (2004).However, many applications that can profit from ISconcentrate on written texts, such as summariza-tion.
For example, Siddharthan et al (2011) showthat solving the IS subproblem of whether a per-son proper name is already known to the reader im-proves automatic summarization of news.
There-fore, we here model IS in written text, creating anew dataset which adds an IS layer to the alreadyexisting comprehensive annotation in the OntoNotescorpus (Weischedel et al, 2011).
We also reportthe first results on fine-grained IS classification bymodelling further distinctions within the categoryof mediated mentions, such as comparative andbridging anaphora (see Examples 1 and 2, re-795spectively).2 Fine-grained IS is a prerequisite tofull bridging/comparative anaphora resolution, andtherefore necessary to fill gaps in entity grids (Barzi-lay and Lapata, 2008) based on coreference only.Thus, Examples 1 and 2 do not exhibit any corefer-ential entity coherence but coherence can be estab-lished when the comparative anaphor others is re-solved to others than freeway survivor Buck Helm,and the bridging anaphor the streets is resolved tothe streets of Oranjemund, respectively.
(1) the condition of freeway survivor BuckHelm .
.
.
, improved, hospital officials said.Rescue crews, however, gave up hope thatothers would be found.
(2) Oranjemund, the mine headquarters, is alonely corporate oasis of 9,000 residents.Jackals roam the streets at night .
.
.We approach the challenge of modeling IS viacollective classification, using several novel linguis-tically motivated features.
We reimplement Nissim?s(2006) and Rahman and Ng?s (2011) approaches asbaselines and show that our approach outperformsthese by a large margin for both coarse- and fine-grained IS classification.2 Related WorkIS annotation schemes and corpora.
We en-hance the approach in Nissim et al (2004) in twomajor ways (see also Section 3.1).
First, compar-ative anaphora are not specifically handled in Nis-sim et al (2004) (and follow-on work such as Ritzet al (2008) and Riester et al (2010)), althoughsome of them might be included in their respectivebridging subcategories.
Second, we apply theannotation scheme reliably to a new genre, namelynews.
This is a non-trivial extension: Ritz et al(2008) applied a variation of the Nissim et al (2004)scheme to a small set of 220 NPs in a Germannews/commentary corpus but found that reliabilitythen dropped significantly to the range of ?
= 0.55to 0.60.
They attributed this to the higher syntac-tic complexity and semantic vagueness in the com-mentary corpus.
Riester et al (2010) annotated a2All examples in this paper are from the OntoNotes cor-pus.
The mention in question is typed in boldface; antecedents,where applicable, are displayed in italics.German news corpus marginally reliable (?
= 0.66)for their overall scheme but their confusion ma-trix shows even lower reliability for several subcate-gories, most importantly deixis and bridging.While standard coreference corpora do not con-tain IS annotation, some corpora annotated forbridging are emerging (Poesio, 2004; Korzen andBuch-Kromann, 2011) but they are (i) not annotatedfor comparative anaphora or other IS categories, (ii)often not tested for reliability or reach only low reli-ability, (iii) often very small (Poesio, 2004).To the best of our knowledge, we thereforepresent the first English corpus reliably annotatedfor a wide range of IS categories as well as fullanaphoric information for three main anaphora types(coreference, bridging, comparative).Automatic recognition of IS.
Vieira and Poesio(2000) describe heuristics for processing definite de-scriptions in news text.
As their approach is re-stricted to definites, they only analyse a subset ofthe mentions we consider carrying IS.
Siddharthanet al (2011) also concentrate on a subproblem of ISonly, namely the hearer-old/hearer-new distinctionsfor person proper names.Nissim (2006) and Rahman and Ng (2011) bothpresent algorithms for IS detection on Nissim etal.
?s (2004) Switchboard corpus.
Both papers treatIS classification as a local classification problemwhereas we look at dependencies between the ISstatus of different mentions, leading to collectiveclassification.
In addition, they only distinguish thethree main categories old, mediated and new.Finally, we work on news corpora which poses dif-ferent problems from dialogue.Anaphoricity determination (Ng, 2009; Zhou andKong, 2009) identifies many or most old men-tions.
However, no distinction between mediatedand new mentions is made.
Most approaches tobridging resolution (Meyer and Dale, 2002; Poe-sio et al, 2004) or comparative anaphora (Mod-jeska et al, 2003; Markert and Nissim, 2005)address only the selection of the antecedent forthe bridging/comparative anaphor, not its recogni-tion.
Sasano and Kurohashi (2009) do also tacklebridging recognition, but they depend on language-specific non-transferrable features for Japanese.7963 Corpus Creation3.1 Annotation SchemeOur scheme follows Nissim et al (2004) in dis-tinguishing three major IS categories old, newand mediated.
A mention is old if it is ei-ther coreferential with an already introduced entityor a generic or deictic pronoun.
We follow theOntoNotes (Weischedel et al, 2011) definition ofcoreference to be able to integrate our annotationswith it.
This definition includes coreference withnoun phrase as well as verb phrase antecedents3 .Mediated refers to entities which have not yetbeen introduced in the text but are inferrable viaother mentions or are known via world knowl-edge.
We distinguish the following six subcate-gories: The category mediated/comparativecomprises mentions compared via either a contrastor similarity to another one (see Example 1).
Thiscategory is novel in our scheme.
We also in-clude a category mediated/bridging (see Ex-amples 2, 3 and 4).
Bridging anaphora can beany noun phrase and are not limited to definite NPsas in Poesio et al (2004), Gardent and Manue?lian(2005), Riester et al (2010).
In contrast to Nissimet al (2004), antecedents for both comparative andbridging categories are annotated and can be nounphrases, verb phrases or even clauses.
The categorymediated/knowledge is inspired by the hearer-old distinction introduced by Prince (1992) and cov-ers entities generally known to the hearer.
It includesmany proper names, such as Poland.4 Mentions thatare syntactically linked via a possessive relation or aPP modification to other, old or mediated men-tions fall into the type mediated/synt (see Ex-amples 5 and 6).5 With no change to Nissim et al?sscheme, coordinated mentions where at least one el-ement in the conjunction is old or mediated arecovered by the category mediated/aggregate,and mentions referring to a value of a previouslymentioned function by the type mediated/func.All other mentions are annotated as new, includ-3In contrast to Nissim et al (2004), but in accordance withOntoNotes, we do not consider generics for coreference.4This class corresponds roughly to Nissim et al?s (2004)mediated/general.5This class expands Nissim et al?s (2004) poss categorythat only considers possessives but not PP modification.ing most generics as well as newly introduced, spe-cific mentions such as Example 7.
(3) Initial steps were taken at Poland?s first en-vironmental conference, which I attendedlast month.
.
.
.
it was no accident that par-ticipants urged the free flow of information(4) The Bakersfield supermarket went out ofbusiness last May.
The reason was .
.
.
(5) One Washington couple sold their liquorstore(6) the main artery into San Francisco(7) the owner was murdered by robbers3.2 Agreement StudyWe carried out an agreement study with 3 annota-tors, of which Annotator A was the scheme devel-oper and first author of this paper.
All texts usedwere from the Wall Street Journal (WSJ) portion ofOntoNotes.
There were no restrictions on whichtexts to include apart from (i) exclusion of lettersto the editor as they contain cross-document linksand (ii) a preference for longer texts with potentiallyricher discourse structure.Mentions were automatically preselected for theannotators using the gold-standard syntactic annota-tion.6 The existing coreference annotation was auto-matically carried over to the IS task by marking allmentions in a coreference chain (apart from the firstmention in the chain) as old.
The annotation taskconsisted of marking all mentions for their IS (old,mediated or new) as well as marking mediatedsubcategories (see Section 3.1) and the antecedentsfor comparative and bridging anaphora.The scheme was developed on 9 texts, which werealso used for training the annotators.
Inter-annotatoragreement was measured on 26 new texts, which in-cluded 5905 pre-marked potential mentions.
The an-notations of 1499 of these were carried over fromOntoNotes, leaving 4406 potential mentions for an-notation and agreement measurement.
In addition to6Some non-mentions such as idioms could not be filteredout via the syntactic annotation and had to be excluded duringhuman annotation.797A-B A-C B-COverall Percentage coarse 87.5 86.3 86.5Overall ?
coarse 77.3 75.2 74.7Overall Percentage fine 86.6 85.3 85.7Overall ?
fine 80.1 77.7 77.3Table 1: Agreement ResultsA-B A-C B-C?
Non-mention 81.5 78.9 86.0?
Old 80.5 83.2 79.3?
New 76.6 74.0 74.3?
Mediated/Knowledge 82.1 78.4 74.1?
Mediated/Synt 88.4 87.8 87.6?
Mediated/Aggregate 87.0 85.4 86.0?
Mediated/Func 6.0 83.2 6.9?
Mediated/Comp 81.8 78.3 81.2?
Mediated/Bridging 70.8 60.6 62.3Table 2: Agreement Results for individual categoriespercentage agreement, we measured Cohen?s ?
(Art-stein and Poesio, 2008) between all 3 possible anno-tator pairings.
We also report single-category agree-ment for each category, where all categories but oneare merged and then ?
is computed as usual.
Table 1shows agreement results for the overall scheme atthe coarse-grained (4 categories: non-mention, old,new, mediated) and the fine-grained level (9 cate-gories: non-mention, old, new and the 6 mediatedsubtypes).
The results show that the scheme is over-all reliable, with not too many differences betweenthe different annotator pairings.7Table 2 shows the individual category agreementfor all 9 categories.
We achieve high reliability formost categories.8 Particularly interesting is the factthat hearer-old entities (mediated/knowledge)can be identified reliably although all annotators hadsubstantially different backgrounds.
The reliabil-ity of the category bridging is more annotator-dependent, although still higher, sometimes con-siderably, than other previous attempts at bridg-7Often, annotation is considered highly reliable when ?
ex-ceeds 0.80 and marginally reliable when between 0.67 and 0.80(Carletta, 1996).
However, the interpretation of ?
is still underdiscussion (Artstein and Poesio, 2008).8The low reliability of the rare category func, when involv-ing Annotator B, was explained by Annotator B forgetting aboutthis category after having used it once.
Pair A-C achieved highreliability (?
83.2 for pair A-C).ing annotation (Poesio et al, 2004; Gardent andManue?lian, 2005; Riester et al, 2010).3.3 Gold StandardOur final gold standard corpus consists of 50 textsfrom the WSJ portion of the OntoNotes corpus-The corpus will be made publically available asOntoNotes annotation layer via http://www.h-its.org/nlp/download.Disagreements in the 35 texts used for annota-tor training (9 texts) and testing (26 texts) were re-solved via discussion between the annotators.
Anadditional 15 texts were annotated by Annotator A.Finally, Annotator A carried out consistency checksover all texts.
?
The gold standard includes 10,980true mentions (see Table 3).Texts 50Mentions 10,980old 3237coref 3,143generic deictic pr 94mediated 3,708world knowledge 924syntactic 1,592aggregate 211func 65comparative 253bridging 663new 4,035Table 3: Gold Standard Distribution4 FeaturesIn this Section, we describe both the local as well asthe relational features we use.4.1 Features for Local ClassificationWe use the following local features, including thefeatures in Nissim (2006) and Rahman and Ng(2011) to be able to gauge how their systems fare onour corpus and as a comparison point for our novelcollective classification approach.The features developed by Nissim (2006) areshown in Table 4.
Nissim shows clearly thatthese features are useful for IS classification.Thus, subjects are more likely to be old as as-sumed by, e.g., centering theory (Grosz et al,798Feature Valuefull prev mention {yes, no, NA}9mention time {first, second, more}partial prev mention {yes, no, NA}determiner {bare, def, dem, indef, poss, NA}NP type {pronoun, common, proper, other}NP length numericgrammatical role {subject, subjpass, pp, other}Table 4: Nissim?s (2006) feature set1995).
Also, previously unmentioned proper namesare more likely to be hearer-old and thereforemediated/knowledge, although their exact sta-tus will depend on how well known a particularproper name is.Rahman and Ng (2011) add all unigrams appear-ing in any mention in the training set as features.They also integrated (via a convolution tree-kernelSVM (Collins and Duffy, 2001)) partial parse treesthat capture the generalised syntactic context of amention e and include the mention?s parent and sib-ling nodes without lexical leaves.
However, they useno structure underneath the mention node e itself,assuming that ?any NP-internal information has pre-sumably been captured by the flat features?.To these feature sets, we add a small set of otherlocal features otherlocal.
These track partial previ-ous mentions by also counting partial previous men-tion time as well as the previous mention of con-tent words only.
We also add a mention?s number asone of singular, plural or unknown, and whether themention is modified by an adjective.
Another featureencapsulates whether the mention is modified by acomparative marker, using a small set of 10 markerssuch as another, such, similar .
.
.
and the presenceof adjectives or adverbs in the comparative.
Finally,we include the mention?s semantic class as one of 12coarse-grained classes, including location, organisa-tion, person and several classes for numbers (such asdate, money or percent).4.2 Relations for Collective ClassificationBoth Nissim (2006) and Rahman and Ng (2011)classify each mention individually in a standard su-pervised ML setting, not considering potential de-pendencies between the IS categories of different9We changed the value of ?full prev mention?
from ?nu-meric?
to {yes, no, NA}.mentions.
However, collective or joint classifica-tion has made substantial impact in other NLP tasks,such as opinion mining (Pang and Lee, 2004; Soma-sundaran et al, 2009), text categorization (Yang etal., 2002; Taskar et al, 2002) and the related task ofcoreference resolution (Denis and Baldridge, 2007).We investigate two types of relations between men-tions that might impact on IS classification.Syntactic parent-child relations.
Two media-ted subcategories account for accessibility via syn-tactic links to another old or mediated men-tion: mediated/synt is used when at least onechild of a mention is mediated or old, with childrelations restricted to pre- or postnominal posses-sives as well as PP children in our scheme (see Sec-tion 3.1).
mediated/aggregate is for coordi-nations in which at least one of the children is oldor mediated.
In these two cases, a mention?sIS depends directly on the IS of its children.
Wetherefore link a mention m1 to a mention m2 via ahasChild relation if (i) m2 is a possessive or prepo-sitional modification ofm1, or (ii)m1 is a coordina-tion and m2 is one of its children.Using such a relational feature catches two birdswith one stone: firstly, it integrates the internal struc-ture of a mention into the algorithm, which Rah-man and Ng (2011) ignore; secondly, it captures de-pendencies between parent and child classification,which would not be possible if we integrated the in-ternal structure via flat features or additional treekernels.
We hypothesise that the higher syntacticcomplexity of our news genre (14.5% of all men-tions are mediated/synt) will make this featurehighly effective in distinguishing between new andmediated categories.Syntactic precedence relations.
IS is said to in-fluence word order (Birner and Ward, 1998; Cahilland Riester, 2009) and this fact has been exploitedin work on generation (Prevost, 1996; Filippova andStrube, 2007; Cahill and Riester, 2009).
Therefore,we integrate dependencies between the IS classifica-tion of mentions in precedence relations.m1 precedes m2 if (i) m1 and m2 are in the sameclause, allowing for trace subjects in gerund and in-finitive constructions, (ii) m1 and m2 are dependenton the same verb or noun, allowing for interven-ing nodes via modal, auxiliary, gerund and infinitive799constructions, (iii) m1 is neither a child nor a parentof m2, and (iv) m1 occurs before m2.For Example 8 (slightly simplified) we extract theprecedence relations shown in Table 5.
(8) She was sent by her mother to a whitewoman?s house to do chores in exchange formeals and a place to sleep.
(She)old >p (her mother)med/synt(She)old >p (a white-woman?s house)new(She)old >p (chores)new(She)old >p (exchange .....sleep)new(her mother)med/synt >p (a white woman?s house)new(chores)new >p (exchange .
.
.
sleep)new(meals)new >p (a place to sleep)newTable 5: Precedence Relations for Example 8.
She is atrace subject for do.Proper names behave differently from commonnouns.
For example, they can occur at many differ-ent places in the clause when functioning as spatialor temporal scene-setting elements, such as In NewYork.
We therefore exclude all precedence relationswhere one element of the pair is a proper name.We extract 2855 precedence relations.
Table 6shows the statistics on precedence with the first men-tion in a pair in rows and the second in columns.
Me-diated and new mentions indeed rarely precede oldmentions, so that precedence should improve sepa-rating of old vs other mentions.old mediated newold 136 387 519mediated 88 357 379new 85 291 613Table 6: Precedence relations in our corpus5 Experiments5.1 Experimental SetupWe use our gold standard corpus (see Section 3.3)via 10-fold cross-validation on documents for all ex-periments.
Following Nissim (2006) and Rahmanand Ng (2011), we perform all experiments on goldstandard mentions and use the human WSJ syntac-tic annotation for feature extraction, when neces-sary.
For the extraction of semantic class, we useOntoNotes entity type annotation for proper namesand an automatic assignment of semantic class viaWordNet hypernyms for common nouns.Coarse-grained versions of all algorithms distin-guish only between the three old, mediated,new categories.
Fine-grained versions distinguishbetween the categories old, the six mediatedsubtypes, and new.
We report overall accuracy aswell as precision, recall and F-measure per category.Significance tests are conducted using McNemar?stest on overall algorithm accuracy, at the level of 1%.5.2 Local ClassifiersWe reimplemented the algorithms in Nissim (2006)and Rahman and Ng (2011) as comparison base-lines, using their feature and algorithm choices.
Al-gorithm Nissim is therefore a decision tree J48 withstandard settings in WEKA with the features in Ta-ble 4.
Algorithm RahmanNg is an SVM with a com-posite kernel and one-vs-all training/testing (toolkitSVMLight).
They use the features in Table 4 plusunigram and tree kernel features, described in Sec-tion 4.1.
We add our additional set of otherlocalfeatures to both baseline algorithms (yielding Nis-sim+ol and RahmanNg+ol) as they aim specificallyat improving fine-grained classification.5.3 Collective ClassificationFor incorporating our inter-mention links, we use avariant of Iterative Collective classification (ICA),which has shown good performance over a varietyof tasks (Lu and Getoor, 2003) and has been usedin NLP for example for opinion mining (Somasun-daran et al, 2009).
ICA is normally faster thanGibbs sampling and ?
in initial experiments ?
didnot yield significantly different results from it.ICA initializes each mention with its most likelyIS, according to the local classifier and features.
Itthen iterates a relational classifier, which uses bothlocal and relational features (our hasChild and pre-cedes features) taking IS assignments to neighbour-ing mentions into account.
We use the exist aggre-gator to define the dependence between mentions.We use NetKit (Macskassy and Provost, 2007)with its standard ICA settings for collective infer-ence, as it allows direct comparison between localand collective classification.
The relational classi-fiers are always exactly the same classifiers as the800local collectiveNissim+ol Nissim+olNissim Nissim+ol+hasChild +hasChild+precedesR P F R P F R P F R P FCoarseold 82.2 86.4 84.2 81.2 88.6 84.8 81.7 88.6 85.0 80.9 89.1 84.8mediated 51.9 60.2 55.7 57.8 64.6 61.0 68.4 77.4 72.6 68.8 76.9 72.6new 74.2 63.6 68.5 78.4 67.3 72.4 87.7 75.1 80.9 87.9 75.0 80.9acc 69.0 72.3 79.4 79.4Fineold 84.0 83.3 83.6 85.0 83.9 84.5 84.3 84.7 84.5 84.1 85.2 84.6med/knowledge 61.3 60.0 60.6 61.0 69.5 65.0 62.3 70.0 65.9 60.6 70.0 65.0med/synt 37.2 59.7 45.8 44.7 60.0 51.3 76.8 81.4 79.0 75.7 80.1 77.9med/agg 26.0 42.0 32.2 20.4 38.4 26.6 42.6 55.9 48.4 43.1 55.8 48.7med/func 0.0 NA NA 32.3 65.6 43.3 33.8 53.7 41.5 35.4 53.5 48.7med/comp 0.4 7.70 0.7 79.0 82.6 80.0 80.6 82.9 81.8 81.4 82.0 81.7med/bridging 6.6 26.2 10.6 8.9 30.9 13.8 9.6 34.4 15.1 12.2 41.7 18.9new 82.6 61.0 70.2 82.7 65.1 72.8 88.0 74.0 80.4 87.7 73.3 79.8acc 66.6 70.0 77.0 76.8Table 7: Collective classification compared to Nissim?s local classifier.
Best performing algorithms are bolded.local ones with the relational features added: thus, ifthe local classifier is a tree kernel SVM so is the rela-tional one.
One problem when using the SVM Treekernel as relational classifier is that it allows only forbinary classification so that we need to train severalbinary networks in a one-vs-all paradigm (see also(Rahman and Ng, 2011)), which will not be able touse the multiclass dependencies of the relational fea-tures to optimum effect.5.4 ResultsTable 7 shows the comparison of collective classifi-cation to local classification, using Nissim?s frame-work and features, and Table 8 the equivalent tablefor Rahman and Ng?s approach.The improvements using the additional local fea-tures over the original local classifiers are sta-tistically significant in all cases.
In particu-lar, the inclusion of semantic classes improvesmediated/knowledge and mediated/func,and comparative anaphora are recognised highly re-liably via a small set of comparative markers.The hasChild relation leads to significant im-provement in accuracy over local classification inall cases, showing the value of collective clas-sification.
The improvement here is centeredon the categories of mediated/synt (for bothcases) and mediated/aggregate (for Nis-sim+ol+hasChild) as well as their distinction fromnew.10 It is also interesting that collective clas-sification with a concise feature set and a sim-ple decision tree as used in Nissim+ol+hasChild,performs equally well as RahmanNg+ol+hasChild,which uses thousands of unigram and tree featuresand a more sophisticated local classifier.
It alsoshows more consistent improvements over all fine-grained classes.The precedes relation does not lead to any fur-ther improvement.
We investigated several varia-tions of the precedence link, such as restricting itto certain grammatical relations, taking into accountdefiniteness or NP type but none of them led toany improvement.
We think there are two reasonsfor this lack of success.
First, the precedence ofmediated vs. new mentions does not follow aclear order and is therefore not a very predictive fea-ture (see Table 6).
At first, this seems to contradictstudies such as Cahill and Riester (2009) that finda variety of precedences according to informationstatus.
However, many of the clearest precedencesthey find are more specific variants of the old >pmediated or old >p new precedence or theyare preferences at an even finer level than the one weannotate, including for example the identification ofgenerics.
Second, the clear old >p mediated10For RhamanNg+ol+hasChild, the aggregate class suf-fers from collective classification.
We hypothesise that this isan artefact of the one-vs-all training/testing for rare categories.801local collectiveRahmanNg+ol RahmanNg+olRahmanNg RahmanNg+ol+hasChild +hasChild+precedesR P F R P F R P F R P FCoarseold 81.3 90.1 85.5 82.6 91.4 86.8 83.5 87.8 85.6 82.9 87.2 85.0mediated 61.4 68.6 64.8 61.5 71.9 66.3 66.7 79.5 72.6 64.8 76.7 70.3new 82.1 69.9 75.5 84.9 70.1 76.8 89.0 74.9 81.3 86.9 73.5 79.6acc 74.9 76.3 79.8 78.3Fineold 85.1 87.0 86.0 85.6 87.9 86.7 85.3 87.4 86.3 85.8 87.5 86.4med/knowledge 65.8 67.2 66.5 64.8 72.6 68.5 67.1 69.6 68.3 64.7 73.2 68.7med/synt 55.8 72.1 62.9 55.8 72.6 63.1 79.8 78.1 78.9 79.8 78.1 78.9med/agg 29.9 75.9 42.9 29.9 75.9 42.9 17.1 53.7 25.9 14.2 49.2 22.1med/func 27.7 38.3 32.1 38.5 69.4 49.5 40.0 44.1 42.0 40.0 40.0 40.0med/comp 25.3 86.5 39.1 76.7 82.2 79.3 74.3 62.7 68.0 74.3 62.7 68.0med/bridging 10.6 44.6 17.1 9.0 47.2 15.2 1.0 15.2 2.0 1.0 13.7 1.9new 87.3 66.3 75.4 89.0 67.8 77.0 89.2 74.6 81.2 89.2 74.6 81.2acc 72.6 74.6 77.5 77.4Table 8: Collective classification compared to Rahman and Ng?s local classifier.
Best performing algorithms arebolded.and old >p new preferences are partially alreadycaptured by the local features, especially the gram-matical role, as, for example, subjects are often bothold as well as early on in a sentence.With regard to fine-grained classification, manycategories including comparative anaphora, areidentified quite reliably, especially in the multiclassclassification setting (Nissim+ol+hasChild).
Bridg-ing seems to be the by far most difficult categoryto identify with final best F-measures still very low.Most bridging mentions do not have any clear inter-nal structure or external syntactic contexts that sig-nal their presence.
Instead, they rely more on lexi-cal and world knowledge for recognition.
Unigramscould potentially encapsulate some of this lexicalknowledge but ?
without generalization ?
are toosparse for a relatively rare category such as bridg-ing (6% of all mentions) to perform well.
The diffi-culty of bridging recognition is an important insightof this paper as it casts doubt on the strategy in pre-vious research to concentrate almost exclusively onantecedent selection (see Section 2).6 ConclusionsWe presented a new approach to information sta-tus classification in written text, for which we alsoprovide the first reliably annotated English languagecorpus.
Based on linguistic intuition, we define fea-tures for classifying mentions collectively.
We showthat our collective classification approach outper-forms the state-of-the-art in coarse-grained IS classi-fication by about 10% (Nissim, 2006) and 5% (Rah-man and Ng, 2011) accuracy.
The gain is almostentirely due to improvements in distinguishing be-tween new and mediatedmentions.
For the latter,we also report the ?
to our knowledge ?
first fine-grained IS classification results.Since the work reported in this paper relied ?
fol-lowing Nissim (2006) and Rahman and Ng (2011)?
on gold standard mentions and syntactic anno-tations, we plan to perform experiments with pre-dicted mentions as well.
We also have to im-prove the recognition of bridging, ideally combiningrecognition and antecedent selection for a completeresolution component.
In addition, we plan to inte-grate IS resolution with our coreference resolutionsystem (Cai et al, 2011) to provide us with a morecomprehensive discourse processing system.Acknowledgements.
Katja Markert received a Fel-lowship for Experienced Researchers by the Alexander-von-Humboldt Foundation and Yufang Hou is funded bya PhD scholarship from the Research Training GroupCo-herence in Language Processing at Heidelberg Univer-sity.
We thank the Heidelberg Institute for TheoreticalStudies for hosting Katja Markert and funding the anno-tation study, and the annotators for their diligent work.802ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Betty J. Birner and Gregory Ward.
1998.
InformationStatus and NoncanonicalWord Order in English.
JohnBenjamins, Amsterdam, The Netherlands.Aoife Cahill and Arndt Riester.
2009.
Incorporating in-formation status into generation ranking.
In Proceed-ings of the Joint Conference of the 47th Annual Meet-ing of the Association for Computational Linguisticsand the 4th International Joint Conference on NaturalLanguage Processing, Singapore, 2?7 August 2009,pages 817?825.Jie Cai, ?Eva Mu?jdricza-Maydt, and Michael Strube.2011.
Unrestricted coreference resolution via globalhypergraph partitioning.
In Proceedings of the SharedTask of the 15th Conference on Computational Natu-ral Language Learning, Portland, Oreg., 23?24 June2011, pages 56?60.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249?254.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Advances in NeuralInformation Processing Systems 14, Vancouver, B.C.,Canada, 3?8 December, 2001, pages 625?632, Cam-bridge, Mass.
MIT Press.Pascal Denis and Jason Baldridge.
2007.
Joint determi-nation of anaphoricity and coreference resolution us-ing integer programming.
In Proceedings of HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics, Rochester, N.Y., 22?27 April2007, pages 236?243.Katja Filippova and Michael Strube.
2007.
Generat-ing constituent order in German clauses.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics, Prague, Czech Republic,23?30 June 2007, pages 320?327.Claire Gardent and He?le`ne Manue?lian.
2005.
Cre?ationd?un corpus annote?
pour le traitement des descrip-tions de?finies.
Traitement Automatique des Langues,46(1):115?140.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Iorn Korzen and Matthias Buch-Kromann.
2011.Anaphoric relations in the Copenhagen dependencytreebanks.
In S. Dipper and H. Zinsmeister, edi-tors, Corpus-based Investigations of Pragmatic andDiscourse Phenomena, volume 3 of Bochumer Lin-guistische Arbeitsberichte, pages 83?98.
University ofBochum, Bochum, Germany.Ivana Kruijff-Korbayova?
and Mark Steedman.
2003.Discourse and information structure.
Journal of Logic,Language and Information.
Special Issue on Dis-cource and Information Structure, 12(3):149?259.Knud Lambrecht.
1994.
Information Structure and Sen-tence Form.
Cambridge, U.K.: Cambridge UniversityPress.Qing Lu and Lise Getoor.
2003.
Link-based classifica-tion.
In Proceedings of the 20th International Confer-ence on Machine Learning, Washington, D.C., 21?24August 2003, pages 496?503.Sofus A. Macskassy and Foster Provost.
2007.
Classi-fication in networked data: A toolkit and a univariatecase study.
Journal of Machine Learning Research,8:935?983.Katja Markert and Malvina Nissim.
2005.
Comparingknowledge sources for nominal anaphora resolution.Computational Linguistics, 31(3):367?401.Josef Meyer and Robert Dale.
2002.
Mining a corpus tosupport associative anaphora resolution.
In Proceed-ings of the 4th International Conference on DiscourseAnaphora and Anaphor Resolution, Lisbon, Portugal,18?20 September, 2002.Natalia M. Modjeska, Katja Markert, and Malvina Nis-sim.
2003.
Using the web in machine learning forother-anaphora resolution.
In Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing, Sapporo, Japan, 11?12 July 2003,pages 176?183.Ani Nenkova, Jason Brenier, Anubha Kothari, Sasha Cal-houn, LauraWhitton, David Beaver, and Dan Jurafsky.2007.
To memorize or to predict: Prominence labelingin conversational speech.
In Proceedings of HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics, Rochester, N.Y., 22?27 April2007, pages 9?16.Vincent Ng.
2009.
Graph-cut-based anaphoricity deter-mination for coreference resolution.
In Proceedings ofHuman Language Technologies 2009: The Conferenceof the North American Chapter of the Association forComputational Linguistics, Boulder, Col., 31 May ?
5June 2009, pages 575?583.Malvina Nissim, Shipara Dingare, Jean Carletta, andMark Steedman.
2004.
An annotation scheme for in-formation status in dialogue.
In Proceedings of the 4thInternational Conference on Language Resources andEvaluation, Lisbon, Portugal, 26?28 May 2004, pages1023?1026.803Malvina Nissim.
2006.
Learning information status ofdiscourse entities.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, Sydney, Australia, 22?23 July 2006, pages94?012.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics, Barcelona, Spain, 21?26 July 2004, pages272?279.Massimo Poesio, Rahul Mehta, Axel Maroudas, andJanet Hitzeman.
2004.
Learning to resolve bridgingreferences.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,Barcelona, Spain, 21?26 July 2004, pages 143?150.Massimo Poesio.
2004.
The MATE/GNOME proposalsfor anaphoric annotation, revisited.
In Proceedings ofthe 5th SIGdial Workshop on Discourse and Dialogue,Cambridge, Mass., 30 April ?
1 May 2004, pages 154?162.Scott Prevost.
1996.
An information structural approachto spoken language generation.
In Proceedings of the34th Annual Meeting of the Association for Computa-tional Linguistics, Santa Cruz, Cal., 24?27 June 1996,pages 294?301.Ellen F. Prince.
1981.
Towards a taxonomy of given-newinformation.
In P. Cole, editor, Radical Pragmatics,pages 223?255.
Academic Press, New York, N.Y.Ellen F. Prince.
1992.
The ZPG letter: Subjects,definiteness, and information-status.
In W.C. Mannand S.A. Thompson, editors, Discourse Description.Diverse Linguistic Analyses of a Fund-Raising Text,pages 295?325.
John Benjamins, Amsterdam.Altaf Rahman and Vincent Ng.
2011.
Learning the in-formation status of noun phrases in spoken dialogues.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, Edinburgh,Scotland, U.K., 27?29 July 2011, pages 1069?1080.Arndt Riester, David Lorenz, and Nina Seemann.
2010.A recursive annotation scheme for referential informa-tion status.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation,La Valetta, Malta, 17?23 May 2010, pages 717?722.Julia Ritz, Stefanie Dipper, and Michael Go?tze.
2008.Annotation of information structure: An evaluationacross different types of texts.
In Proceedings of the6th International Conference on Language Resourcesand Evaluation, Marrakech, Morocco, 26 May ?
1June 2008, pages 2137?2142.Ryohei Sasano and Sadao Kurohashi.
2009.
A prob-abilistic model for associative anaphora resolution.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, Singapore,6?7 August 2009, pages 1455?1464.Advaith Siddharthan, Ani Nenkova, and Kathleen McK-eown.
2011.
Information status distinctions and re-ferring expressions: An empirical study of referencesto people in news summaries.
Computational Linguis-tics, 37(4):811?842.Swapna Somasundaran, Galileo Namata, Janyce Wiebe,and Lise Getoor.
2009.
Supervised and unsupervisedmethods in employing discourse relations for improv-ing opinion polarity classification.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, Singapore, 6?7 August 2009.Ben Taskar, Pieter Abbeel, and Daphne Koller.
2002.Discriminative probabilistic models for relational data.In Proceedings of the 18th Conference on Uncertaintyin Artificial Intelligence, Edmonton, Alberta, Canada,1-4 August 2002, pages 485?492.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4):539?593.Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-anwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, Mohammed El-Bachouti, Robert Belvin,and Ann Houston.
2011.
OntoNotes release 4.0.LDC2011T03, Philadelphia, Penn.
: Linguistic DataConsortium.Yiming Yang, Sea?n Slattery, and Rayid Ghani.
2002.
Astudy of approaches to hypertext categorization.
Jour-nal of Intelligent Information Systems, 18(2-3):219?241.Guodong Zhou and Fang Kong.
2009.
Global learning ofnoun phrase anaphoricity in coreference resolution vialabel propagation.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, Singapore, 6?7 August 2009, pages 978?986.804
