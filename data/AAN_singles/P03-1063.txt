Text Chunking by Combining Hand-Crafted Rules and Memory-BasedLearningSeong-Bae Park Byoung-Tak ZhangSchool of Computer Science and EngineeringSeoul National UniversitySeoul 151-744, Korea{sbpark,btzhang}@bi.snu.ac.krAbstractThis paper proposes a hybrid of hand-crafted rules and a machine learningmethod for chunking Korean.
In the par-tially free word-order languages such asKorean and Japanese, a small numberof rules dominate the performance dueto their well-developed postpositions andendings.
Thus, the proposed method isprimarily based on the rules, and then theresidual errors are corrected by adopting amemory-based machine learning method.Since the memory-based learning is anefficient method to handle exceptions innatural language processing, it is good atchecking whether the estimates are excep-tional cases of the rules and revising them.An evaluation of the method yields the im-provement in F-score over the rules or var-ious machine learning methods alone.1 IntroductionText chunking has been one of the most interest-ing problems in natural language learning commu-nity since the first work of (Ramshaw and Marcus,1995) using a machine learning method.
The mainpurpose of the machine learning methods applied tothis task is to capture the hypothesis that best deter-mine the chunk type of a word, and such methodshave shown relatively high performance in English(Kudo and Matsumoto, 2000; Zhang et.
al, 2001).In order to do it, various kinds of information, suchas lexical information, part-of-speech and grammat-ical relation, of the neighboring words is used.
Sincethe position of a word plays an important role as asyntactic constraint in English, the methods are suc-cessful even with local information.However, these methods are not appropriate forchunking Korean and Japanese, because such lan-guages have a characteristic of partially free word-order.
That is, there is a very weak positional con-straint in these languages.
Instead of positional con-straints, they have overt postpositions that restrictthe syntactic relation and composition of phrases.Thus, unless we concentrate on the postpositions,we must enlarge the neighboring window to geta good hypothesis.
However, enlarging the win-dow size will cause the curse of dimensionality(Cherkassky and Mulier, 1998), which results in thedeficiency in the generalization performance.Especially in Korean, the postpositions and theendings provide important information for nounphrase and verb phrase chunking respectively.
Withonly a few simple rules using such information,the performance of chunking Korean is as goodas the rivaling other inference models such as ma-chine learning algorithms and statistics-based meth-ods (Shin, 1999).
Though the rules are approxi-mately correct for most cases drawn from the do-main on which the rules are based, the knowledgein the rules is not necessarily well-represented forany given set of cases.
Since chunking is usuallyprocessed in the earlier step of natural language pro-cessing, the errors made in this step have a fatal in-fluence on the following steps.
Therefore, the ex-ceptions that are ignored by the rules must be com-Training Phasew 1 ... w N(PO S1 ... PO SN)Rule BasedD eterm inationRule BaseFor Each W ord w iCorrectlyD eterm ined?Find Error TypeN oFinishYesE rror C ase LibraryC lassification Phasew 1 ... w N(PO S1 ... PO SN)Rule BasedD eterm inationRule BaseFor Each W ord w iE rror C ase LibraryM em ory BasedD eterm inationC 1 ... C NCom binationFigure 1: The structure of Korean chunking model.
This figure describes a sentence-based learning andclassification.pensated for by some special treatments of them forhigher performance.To solve this problem, we have proposed a com-bining method of the rules and the k-nearest neigh-bor (k-NN) algorithm (Park and Zhang, 2001).
Theproblem in this method is that it has redundant k-NNs because it maintains a separate k-NN for eachkind of errors made by the rules.
In addition, be-cause it applies a k-NN and the rules to each exam-ples, it requires more computations than other infer-ence methods.The goal of this paper is to provide a new methodfor chunking Korean by combining the hand-craftedrules and a machine learning method.
The chunktype of a word in question is determined by the rules,and then verified by the machine learning method.The role of the machine learning method is to de-termine whether the current context is an exceptionof the rules.
Therefore, a memory-based learning(MBL) is used as a machine learning method thatcan handle exceptions efficiently (Daelemans et.
al,1999).The rest of the paper is organized as follows.
Sec-tion 2 explains how the proposed method works.Section 3 describes the rule-based method forchunking Korean and Section 4 explains chunkingby memory-based learning.
Section 5 presents theexperimental results.
Section 6 introduces the issuesfor applying the proposed method to other problems.Finally, Section 7 draws conclusions.2 Chunking KoreanFigure 1 shows the structure of the chunking modelfor Korean.
The main idea of this model is to applyrules to determine the chunk type of a word wiin asentence, and then to refer to a memory based clas-sifier in order to check whether it is an exceptionalcase of the rules.
In the training phase, each sentenceis analyzed by the rules and the predicted chunk typeis compared with the true chunk type.
In case of mis-prediction, the error type is determined according tothe true chunk type and the predicted chunk type.The mispredicted chunks are stored in the error caselibrary with their true chunk types.
Since the errorcase library accumulates only the exceptions of therules, the number of cases in the library is small ifthe rules are general enough to represent the instancespace well.The classification phase in Figure 1 is expressedas a procedure in Figure 2.
It determines the chunktype of a word wigiven with the context Ci.
First ofall, the rules are applied to determine the chunk type.Then, it is checked whether Ciis an exceptional caseof the rules.
If it is, the chunk type determined bythe rules is discarded and is determined again by thememory based reasoning.
The condition to make adecision of exceptional case is whether the similar-ity between Ciand the nearest instance in the errorProcedure CombineInput : a word wi, a context Ci, and the threshold tOutput : a chunk type c[Step 1] c = Determine the chunk type of wiusing rules.
[Step 2] e = Get the nearest instance of Ciin error caselibrary.
[Step 3] If Similarity(Ci, e) ?
t,then c = Determine chunk type of wiby memory-based learning.Figure 2: The procedure for combining the rules andmemory based learning.case library is larger than the threshold t. Since thelibrary contains only the exceptional cases, the moresimilar is Cito the nearest instance, the more prob-able is it an exception of the rules.3 Chunking by RulesThere are four basic phrases in Korean: noun phrase(NP), verb phrase (VP), adverb phrase (ADVP), andindependent phrase (IP).
Thus, chunking by rules isdivided into largely four components.3.1 Noun Phrase ChunkingWhen the part-of-speech of wiis one of determiner,noun, and pronoun, there are only seven rules todetermine the chunk type of widue to the well-developed postpositions of Korean.1.
If POS(wi?1) = determiner and wi?1does not have apostposition Then yi= I-NP.2.
Else If POS(wi?1) = pronoun and wi?1does not havea postposition Then yi= I-NP.3.
Else If POS(wi?1) = noun and wi?1does not have apostposition Then yi= I-NP.4.
Else If POS(wi?1) = noun and wi?1has a possessivepostposition Then yi= I-NP.5.
Else If POS(wi?1) = noun andwi?1has a relative post-fix Then yi= I-NP.6.
Else If POS(wi?1) = adjective and wi?1has a relativeending Then yi= I-NP.7.
Else yi= B-NP.Here, POS(wi?1) is the part-of-speech of wi?1.B-NP represents the first word of a noun phrase,while I-NP is given to other words in the nounphrase.Since determiners, nouns and pronouns play thesimilar syntactic role in Korean, they form a nounphrase when they appear in succession without post-position (Rule 1?3).
The words with postpositionsbecome the end of a noun phrase, but there are onlytwo exceptions.
When the type of a postpositionis possessive, it is still in the mid of noun phrase(Rule 4).
The other exception is a relative postfix? (jeok)?
(Rule 5).
Rule 6 states that a simple rela-tive clause with no sub-constituent also constitutes anoun phrase.
Since the adjectives of Korean have nodefinitive usage, this rule corresponds to the defini-tive usage of the adjectives in English.3.2 Verb Phrase ChunkingThe verb phrase chunking has been studied for along time under the name of compound verb pro-cessing in Korean and shows relatively high accu-racy.
Shin used a finite state automaton for verbphrase chunking (Shin, 1999), while K.-C. Kim usedknowledge-based rules (Kim et.
al, 1995).
For theconsistency with noun phrase chunking, we use therules in this paper.
The rules used are the ones pro-posed by (Kim et.
al, 1995) and the further explana-tion on the rules is skipped.
The number of the rulesused is 29.3.3 Adverb Phrase ChunkingWhen the adverbs appear in succession, they have agreat tendency to form an adverb phrase.
Though anadverb sequence is not always one adverb phrase, itusually forms one phrase.
Table 1 shows this empiri-cally.
The usage of the successive adverbs is investi-gated from STEP 2000 dataset1 where 270 cases areobserved.
The 189 cases among them form a phrasewhereas the remaining 81 cases form two phrases in-dependently.
Thus, it can be said that the possibilitythat an adverb sequence forms a phrase is far higherthan the possibility that it forms two phrases.When the part-of-speech of wiis an adjective, itschunk type is determined by the following rule.1.
If POS(wi?1) = adverb Then yi= I-ADVP.2.
Else yi= B-ADVP.1This dataset will be explained in Section 5.1.No.
of Cases ProbabilityOne Phrase 189 0.70Two Phrases 81 0.30Table 1: The probability that an adverb sequenceforms a chunk.3.4 Independent Phrase ChunkingThere is no special rule for independent phrasechunking.
It can be done only through knowledgebase that stores the cases where independent phrasestake place.
We designed 12 rules for independentphrases.4 Chunking by Memory-Based LearningMemory-based learning is a direct descent of thek-Nearest Neighbor (k-NN) algorithm (Cover andHart, 1967).
Since many natural language process-ing (NLP) problems have constraints of a large num-ber of examples and many attributes with differentrelevance, memory-based learning uses more com-plex data structure and different speedup optimiza-tion from the k-NN.It can be viewed with two components: a learningcomponent and a similarity-based performance com-ponent.
The learning component involves addingtraining examples to memory, where all examplesare assumed to be fixed-length vectors of n at-tributes.
The similarity between an instance x andall examples y in memory is computed using a dis-tance metric, ?(x,y).
The chunk type of x is thendetermined by assigning the most frequent categorywithin the k most similar examples of x.The distance from x and y, ?
(x,y) is defined tobe?
(x,y) ?n?i=1?i?
(xi, yi),where ?iis the weight of i-th attribute and?
(xi, yi) ={0 if xi= yi,1 if xi= yi.When ?iis determined by information gain (Quin-lan, 1993), the k-NN algorithm with this metric iscalled IB1-IG (Daelemans et.
al, 2001).
All the ex-periments performed by memory-based learning inthis paper are done with IB1-IG.Table 2 shows the attributes of IB1-IG for chunk-ing Korean.
To determine the chunk type of a wordwi, the lexicons, POS tags, and chunk types ofsurrounding words are used.
For the surroundingwords, three words of left context and three wordsof right context are used for lexicons and POS tags,while two words of left context are used for chunktypes.
Since chunking is performed sequentially, thechunk types of the words in right context are notknown in determining the chunk type of wi.5 Experiments5.1 DatasetFor the evaluation of the proposed method, all exper-iments are performed on STEP 2000 Korean Chunk-ing dataset (STEP 2000 dataset)2.
This dataset isderived from the parsed corpus, which is a productof STEP 2000 project supported by Korean govern-ment.
The corpus consists of 12,092 sentences with111,658 phrases and 321,328 words, and the vocab-ulary size is 16,808.
Table 3 summarizes the infor-mation on the dataset.The format of the dataset follows that of CoNLL-2000 dataset (CoNLL, 2000).
Figure 3 shows an ex-ample sentence in the dataset3.
Each word in thedataset has two additional tags, which are a part-of-speech tag and a chunk tag.
The part-of-speech tagsare based on KAIST tagset (Yoon and Choi, 1999).Each phrase can have two kinds of chunk types: B-XP and I-XP.
In addition to them, there is O chunktype that is used for words which are not part of anychunk.
Since there are four types of phrases andone additional chunk type O, there exist nine chunktypes.5.2 Performance of Chunking by RulesTable 4 shows the chunking performance when onlythe rules are applied.
Using only the rules gives97.99% of accuracy and 91.87 of F-score.
In spiteof relatively high accuracy, F-score is somewhat low.Because the important unit of the work in the appli-cations of text chunking is a phrase, F-score is farmore important than accuracy.
Thus, we have muchroom to improve in F-score.2The STEP 2000 Korean Chunking dataset is available inhttp://bi.snu.ac.kr/?sbpark/Step2000.3The last column of this figure, the English annotation, doesAttribute Explanation Attribute ExplanationWi?3word of wi?3POSi?3POS of wi?3Wi?2word of wi?2POSi?2POS of wi?2Wi?1word of wi?1POSi?1POS of wi?1Wiword of wiPOSiPOS of wiWi+1word of wi+1POSi+1POS of wi+1Wi+2word of wi+2POSi+2POS of wi+2Wi+3word of wi+3POSi+3POS of wi+3Ci?3chunk of wi?3Ci?2chunk of wi?2Ci?1chunk of wi?1Table 2: The attributes of IB1-IG for chunking Korean.Information ValueVocabulary Size 16,838Number of total words 321,328Number of chunk types 9Number of POS tags 52Number of sentences 12,092Number of phrases 112,658Table 3: The simple statistics on STEP 2000 KoreanChunking dataset. nq B-NP Korea?
jcm I-NP Postposition : POSS nq I-NP Sejong ncn I-NP base jcj I-NP and mmd I-NP the ncn I-NP surrounding ncn I-NP base	 jxt I-NP Postposition: TOPIC ncn B-NP western South Pole ncn B-NP south	 nq I-NP Shetland?
jcm I-NP Postposition : POSSnq I-NP King George Island jca I-NP Postposition : LOCA paa B-VP is located ef I-VP Ending : DECL.
sf OFigure 3: An example of STEP 2000 dataset.Type Precision Recall F-scoreADVP 98.67% 97.23% 97.94IP 100.00% 99.63% 99.81NP 88.96% 88.93% 88.94VP 92.89% 96.35% 94.59All 91.28% 92.47% 91.87Table 4: The experimental results when the rules areonly used.Error Type No.
of Errors Ratio (%)B-ADVP I-ADVP 89 1.38B-ADVP I-NP 9 0.14B-IP B-NP 9 0.14I-IP I-NP 2 0.03B-NP I-NP 2,376 36.76I-NP B-NP 2,376 36.76B-VP I-VP 3 0.05I-VP B-VP 1,599 24.74All 6,463 100.00Table 5: The error distribution according to the mis-labeled chunk type.Table 5 shows the error types by the rules andtheir distribution.
For example, the error type ?B-ADVP I-ADVP?
contains the errors whose true la-bel is B-ADVP and that are mislabeled by I-ADVP.There are eight error types, but most errors are re-lated with noun phrases.
We found two reasons forthis:1.
It is difficult to find the beginning of nounphrases.
All nouns appearing successivelywithout postpositions are not a single nounphrase.
But, they are always predicted to besingle noun phrase by the rules, though theycan be more than one noun phrase.2.
The postposition representing a noun coordi-nation, ? (wa)?
is very ambiguous.
When? (wa)?
is representing the coordination, thechunk types of it and its next word should be?I-NP I-NP?.
But, when it is just an adverbialpostposition that implies ?with?
in English, thechunk types should be ?I-NP B-NP?.Decision Tree SVM MBLAccuracy 97.95?0.24% 98.15?0.20% 97.79?0.29%Precision 92.29?0.94% 93.63?0.81% 91.41?1.24%Recall 90.45?0.80% 91.48?0.70% 91.43?0.87%F-score 91.36?0.85 92.54?0.72 91.38?1.01Table 6: The experimental results of various ma-chine learning algorithms.5.3 Performance of Machine LearningAlgorithmsTable 6 gives the 10-fold cross validation result ofthree machine learning algorithms.
In each fold, thecorpus is divided into three parts: training (80%),held-out (10%), test (10%).
Since held-out set isused only to find the best value for the threshold tin the combined model, it is not used in measuringthe performance of machine learning algorithms.The machine learning algorithms tested are (i)memory-based learning (MBL), (ii) decision tree,and (iii) support vector machines (SVM).
We useC4.5 release 8 (Quinlan, 1993) for decision tree in-duction and SV Mlight (Joachims, 1998) for supportvector machines, while TiMBL (Daelemans et.
al,2001) is adopted for memory-based learning.
De-cision trees and SVMs use the same attributes withmemory-based learning (see Table 2).
Two of the al-gorithms, memory-based learning and decision tree,show worse performance than the rules.
The F-scores of memory-based learning and decision treeare 91.38 and 91.36 respectively, while that of therules is 91.87 (see Table 4).
On the other hand, sup-port vector machines present a slightly better perfor-mance than the rules.
The F-score of support vectormachine is 92.54, so the improvement over the rulesis just 0.67.Table 7 shows the weight of attributes whenonly memory-based learning is used.
Each valuein this table corresponds to ?iin calculating?(x,y).
The more important is an attribute, thelarger is the weight of it.
Thus, the most im-portant attribute among 17 attributes is Ci?1, thechunk type of the previous word.
On the otherhand, the least important attributes are Wi?3andCi?3.
Because the words make less influenceon determining the chunk type of wiin ques-tion as they become more distant from wi.
Thatnot exist in the dataset.
It is given for the explanation.Attribute Weight Attribute WeightWi?30.03 POSi?30.04Wi?20.07 POSi?20.11Wi?10.17 POSi?10.28Wi0.22 POSi0.38Wi+10.14 POSi+10.22Wi+20.06 POSi+20.09Wi+30.04 POSi+30.05Ci?30.03 Ci?20.11Ci?10.43Table 7: The weights of the attributes in IB1-IG.
Thetotal sum of the weights is 2.48.fold Precision (%) Recall (%) F-score t1 94.87 94.12 94.49 1.962 93.52 93.85 93.68 1.983 95.25 94.72 94.98 1.954 95.30 94.32 94.81 1.955 92.91 93.54 93.22 1.876 94.49 94.50 94.50 1.927 95.88 94.35 95.11 1.948 94.25 94.18 94.21 1.949 92.96 91.97 92.46 1.9110 95.24 94.02 94.63 1.97Avg.
94.47?1.04 93.96?0.77 94.21?0.84 1.94Table 8: The final result of the proposed method bycombining the rules and the memory-based learning.The average accuracy is 98.21?0.43.is, the order of important lexical attributes is?Wi,Wi?1,Wi+1,Wi?2,Wi+2,Wi+3,Wi?3?.
Thesame phenomenon is found in part-of-speech(POS) and chunk type (C).
In comparing the part-of-speech information with the lexical information,we find out that the part-of-speech is more impor-tant.
One possible explanation for this is that thelexical information is too sparse.The best performance on English reported is94.13 in F-score (Zhang et.
al, 2001).
The reasonwhy the performance on Korean is lower than thaton English is the curse of dimensionality.
That is,the wider context is required to compensate for thefree order of Korean, but it hurts the performance(Cherkassky and Mulier, 1998).5.4 Performance of the Hybrid MethodTable 8 shows the final result of the proposedmethod.
The F-score is 94.21 on the average whichis improvement of 2.34 over the rules only, 1.67 oversupport vector machines, and 2.83 over memory-based learning.
In addition, this result is as high asthe performance on English (Zhang et.
al, 2001).80828486889092949698100ADVP IP NP VPPhrasesF-scoreRule OnlyHybridFigure 4: The improvement for each kind of phrasesby combining the rules and MBL.The threshold t is set to the value which producesthe best performance on the held-out set.
The totalsum of all weights in Table 7 is 2.48.
This impliesthat when we set t > 2.48, only the rules are ap-plied since there is no exception with this threshold.When t = 0.00, only the memory-based learning isused.
Since the memory-based learning determinesthe chunk type of wibased on the exceptional casesof the rules in this case.
the performance is poor witht = 0.00.
The best performance is obtained when tis near 1.94.Figure 4 shows how much F-score is improved foreach kind of phrases.
The average F-score of nounphrase is 94.54 which is far improved over that of therules only.
This implies that the exceptional cases ofthe rules for noun phrase are well handled by thememory-based learning.
The performance is muchimproved for noun phrase and verb phrase, while itremains same for adverb phrases and independentphrases.
This result can be attributed to the fact thatthere are too small number of exceptions for adverbphrases and independent phrases.
Because the ac-curacy of the rules for these phrases is already highenough, most cases are covered by the rules.
Mem-ory based learning treats only the exceptions of therules, so the improvement by the proposed methodis low for the phrases.6 DiscussionIn order to make the proposed method practical andapplicable to other NLP problems, the following is-sues are to be discussed:1.
Why are the rules applied before thememory-based learning?When the rules are efficient and accurateenough to begin with, it is reasonable to ap-ply the rules first (Golding and Rosenbloom,1996).
But, if they were deficient in someway, we should have applied the memory-basedlearning first.2.
Why don?t we use all data for the machinelearning method?In the proposed method, memory-based learn-ing is used not to find a hypothesis for inter-preting whole data space but to handle the ex-ceptions of the rules.
If we use all data for boththe rules and memory-based learning, we haveto weight the methods to combine them.
But, itis difficult to know the weights of the methods.3.
Why don?t we convert the memory-basedlearning to the rules?Converting between the rules and the cases inthe memory-based learning tends to yield inef-ficient or unreliable representation of rules.The proposed method can be directly applied tothe problems other than chunking Korean if theproper rules are prepared.
The proposed method willshow better performance than the rules or machinelearning methods alone.7 ConclusionIn this paper we have proposed a new methodto learn chunking Korean by combining the hand-crafted rules and a memory-based learning.
Ourmethod is based on the rules, and the estimates onchunks by the rules are verified by a memory-basedlearning.
Since the memory-based learning is anefficient method to handle exceptional cases of therules, it supports the rules by making decisions onlyfor the exceptions of the rules.
That is, the memory-based learning enhances the rules by efficiently han-dling the exceptional cases of the rules.The experiments on STEP 2000 dataset showedthat the proposed method improves the F-score ofthe rules by 2.34 and of the memory-based learn-ing by 2.83.
Even compared with support vectormachines, the best machine learning algorithm intext chunking, it achieved the improvement of 1.67.The improvement was made mainly in noun phrasesamong four kinds of phrases in Korean.
This isbecause the errors of the rules are mostly relatedwith noun phrases.
With relatively many instancesfor noun phrases, the memory-based learning couldcompensate for the errors of the rules.
We also em-pirically found the threshold value t used to deter-mine when to apply the rules and when to applymemory-based learning.We also discussed some issues in combining arule-based method and a memory-based learning.These issues will help to understand how the methodworks and to apply the proposed method to otherproblems in natural language processing.
Since themethod is general enough, it can be applied to otherproblems such as POS tagging and PP attachment.The memory-based learning showed good perfor-mance in these problems, but did not reach the state-of-the-art.
We expect that the performance will beimproved by the proposed method.AcknowledgementThis research was supported by the Korean Ministryof Education under the BK21-IT program and by theKorean Ministry of Science and Technology underNRL and BrainTech programs.ReferencesV.
Cherkassky and F. Mulier.
1998.
Learning from Data:Concepts, Theory, and Methods, John Wiley & Sons,Inc.CoNLL.
2000.
Shared Task for ComputationalNatural Language Learning (CoNLL), http://lcg-www.uia.ac.be/conll2000/chunking.T.
Cover and P. Hart.
1967.
Nearest Neighbor Pat-tern Classification, IEEE Transactions on InformationTheory, Vol.
13, pp.
21?27.W.
Daelemans, A. Bosch and J. Zavrel.
1999.
ForgettingExceptions is Harmful in Language Learning, Ma-chine Learning, Vol.
34, No.
1, pp.
11?41.W.
Daelemans, J. Zavrel, K. Sloot and A. Bosch.
2001.TiMBL: Tilburg Memory Based Learner, version 4.1,Reference Guide, ILK 01-04, Tilburg University.A.
Golding and P. Rosenbloom.
1996.
Improving Accu-racy by Combining Rule-based and Case-based Rea-soning, Artificial Intelligence, Vol.
87, pp.
215?254.T.
Joachims.
1998.
Making Large-Scale SVM LearningPractical, LS8, Universitaet Dortmund.K.-C. Kim, K.-O.
Lee, and Y.-S. Lee.
1995.
KoreanCompound Verbals Processing driven by Morpholog-ical Analysis, Journal of KISS, Vol.
22, No.
9, pp.1384?1393.Taku Kudo and Yuji Matsumoto.
2000.
Use of SupportVector Learning for Chunk Identification, In Proceed-ings of the Fourth Conference on Computational Nat-ural Language Learning, pp.
142?144.S.-B.
Park and B.-T. Zhang.
2001.
Combining a Rule-based Method and a k-NN for Chunking Korean Text,In Proceedings of the 19th International Conferenceon Computer Processing of Oriental Languages, pp.225?230.R.
Quinlan.
1993.
C4.5: Programs for Machine Learn-ing, Morgan Kaufmann Publishers.L.
Ramshaw and M. Marcus.
1995.
Text Chunking Us-ing Transformation-Based Learning, In Proceedingsof the Third ACL Workshop on Very Large Corpora,pp.
82?94.H.-P. Shin.
1999.
Maximally Efficient Syntatic Parsingwith Minimal Resources, In Proceedings of the Con-ference on Hangul and Korean Language InfomrationProcessing, pp.
242?244.J.-T. Yoon and K.-S. Choi.
1999.
Study on KAIST Cor-pus, CS-TR-99-139, KAIST CS.T.
Zhang, F. Damerau and D. Johnson.
2001.
TextChunking Using Regularized Winnow, In Proceed-ings of the 39th Annual Meeting of the Association forComputational Linguistics, pp.
539?546.
