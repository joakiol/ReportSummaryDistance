Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116?122,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsTINE: A Metric to Assess MT AdequacyMiguel Rios, Wilker Aziz and Lucia SpeciaResearch Group in Computational LinguisticsUniversity of WolverhamptonStafford Street, Wolverhampton, WV1 1SB, UK{m.rios, w.aziz, l.specia}@wlv.ac.ukAbstractWe describe TINE, a new automatic evalua-tion metric for Machine Translation that aimsat assessing segment-level adequacy.
Lexicalsimilarity and shallow-semantics are used asindicators of adequacy between machine andreference translations.
The metric is based onthe combination of a lexical matching com-ponent and an adequacy component.
Lexi-cal matching is performed comparing bags-of-words without any linguistic annotation.The adequacy component consists in: i) us-ing ontologies to align predicates (verbs), ii)using semantic roles to align predicate argu-ments (core arguments and modifiers), andiii) matching predicate arguments using dis-tributional semantics.
TINE?s performanceis comparable to that of previous metricsat segment level for several language pairs,with average Kendall?s tau correlation from0.26 to 0.29.
We show that the addition ofthe shallow-semantic component improves theperformance of simple lexical matching strate-gies and metrics such as BLEU.1 IntroductionThe automatic evaluation of Machine Translation(MT) is a long-standing problem.
A number of met-rics have been proposed in the last two decades,mostly measuring some form of matching betweenthe MT output (hypothesis) and one or more human(reference) translations.
However, most of thesemetrics focus on fluency aspects, as opposed to ad-equacy.
Therefore, measuring whether the meaningof the hypothesis and reference translation are thesame or similar is still an understudied problem.The most commonly used metrics, BLEU (Pap-ineni et al, 2002) and alike, perform simple exactmatching of n-grams between hypothesis and refer-ence translations.
Such a simple matching proce-dure has well known limitations, including that thematching of non-content words counts as much asthe matching of content words, that variations ofwords with the same meaning are disregarded, andthat a perfect matching can happen even if the orderof sequences of n-grams in the hypothesis and ref-erence translation are very different, changing com-pletely the meaning of the translation.A number of other metrics have been proposedto address these limitations, for example, by allow-ing for the matching of synonyms or paraphrasesof content words, such as in METEOR (Denkowskiand Lavie, 2010).
Other attempts have been madeto capture whether the reference translation and hy-pothesis translations share the same meaning us-ing shallow semantics, i.e., Semantic Role Labeling(Gime?nez and Ma?rquez, 2007).
However, these arelimited to the exact matching of semantic roles andtheir fillers.We propose TINE, a new metric that comple-ments lexical matching with a shallow semanticcomponent to better address adequacy.
The maincontribution of such a metric is to provide a moreflexible way of measuring the overlap between shal-low semantic representations that considers both thesemantic structure of the sentence and the contentof the semantic elements.
The metric uses SRLssuch as in (Gime?nez and Ma?rquez, 2007).
However,it analyses the content of predicates and argumentsseeking for either exact or ?similar?
matches.
The116inexact matching is based on the use of ontologiessuch as VerbNet (Schuler, 2006) and distributionalsemantics similarity metrics, such as Dekang Lin?sthesaurus (Lin, 1998) .In the remainder of this paper we describe somerelated work (Section 2), present our metric - TINE- (Section 3) and its performance compared to pre-vious work (Section 4) as well as some further im-provements.
We then provide an analysis of theseresults and discuss the limitations of the metric (Sec-tion 5) and present conclusions and future work(Section 6).2 Related WorkA few metrics have been proposed in recent yearsto address the problem of measuring whether a hy-pothesis and a reference translation share the samemeaning.
The most well-know metric is probablyMETEOR (Banerjee and Lavie, 2005; Denkowskiand Lavie, 2010).
METEOR is based on a general-ized concept of unigram matching between the hy-pothesis and the reference translation.
Alignmentsare based on exact, stem, synonym, and paraphrasematches between words and phrases.
However, thestructure of the sentences is not considered.Wong and Kit (2010) measure word choice andword order by the matching of words based onsurface forms, stems, senses and semantic similar-ity.
The informativeness of matched and unmatchedwords is also weighted.Liu et al (2010) propose to match bags of uni-grams, bigrams and trigrams considering both recalland precision and F-measure giving more impor-tance to recall, but also using WordNet synonyms.Tratz and Hovy (2008) use transformations in or-der to match short syntactic units defined as Ba-sic Elements (BE).
The BE are minimal-lengthsyntactically well defined units.
For example,nouns, verbs, adjectives and adverbs can be con-sidered BE-Unigrams, while a BE-Bigram could beformed from a syntactic relation (e.g.
subject+verb,verb+object).
BEs can be lexically different, but se-mantically similar.Pado?
et al (2009) uses Textual Entailment fea-tures extracted from the Standford Entailment Rec-ognizer (MacCartney et al, 2006).
The Textual En-tailment Recognizer computes matching and mis-matching features over dependency parses.
The met-ric then predicts the MT quality with a regressionmodel.
The alignment is improved using ontologies.He et al (2010) measure the similarity betweenhypothesis and reference translation in terms ofthe Lexical Functional Grammar (LFG) represen-tation.
The representation uses dependency graphsto generate unordered sets of dependency triples.Calculating precision, recall, and F-score on thesets of triples corresponding to the hypothesis andreference segments allows measuring similarity atthe lexical and syntactic levels.
The measure alsomatches WordNet synonyms.The closest related metric to the one proposed inthis paper is that by Gime?nez and Ma?rquez (2007)and Gime?nez et al (2010), which also uses shallowsemantic representations.
Such a metric combines anumber of components, including lexical matchingmetrics like BLEU and METEOR, as well as com-ponents that compute the matching of constituentand dependency parses, named entities, discourserepresentations and semantic roles.
However, the se-mantic role matching is based on exact matching ofroles and role fillers.
Moreover, it is not clear whatthe contribution of this specific information is for theoverall performance of the metric.We propose a metric that uses a lexical similar-ity component and a semantic component in orderto deal with both word choice and semantic struc-ture.
The semantic component is based on seman-tic roles, but instead of simply matching the surfaceforms (i.e.
arguments and predicates) it is able tomatch similar words.3 Metric DescriptionThe rationale behind TINE is that an adequacy-oriented metric should go beyond measuring thematching of lexical items to incorporate informationabout the semantic structure of the sentence, as in(Gime?nez et al, 2010).
However, the metric shouldalso be flexible to consider inexact matches of se-mantic components, similar to what is done with lex-ical metrics like METEOR (Denkowski and Lavie,2010).
We experiment with TINE having Englishas target language because of the availability of lin-guistic processing tools for this language.
The met-ric is particularly dependent on semantic role label-117ing systems, which have reached satisfactory perfor-mance for English (Carreras and Ma?rquez, 2005).TINE uses semantic role labels (SRL) and lexical se-mantics to fulfill two requirements by: (i) compareboth the semantic structure and its content acrossmatching arguments in the hypothesis and refer-ence translations; and (ii) propose alternative waysof measuring inexact matches for both predicatesand role fillers.
Additionally, it uses an exact lexi-cal matching component to reward hypotheses thatpresent the same lexical choices as the referencetranslation.
The overall score s is defined using thesimple weighted average model in Equation (1):s(H,R) = max{?L(H,R) + ?A(H,R)?+ ?
}R?R(1)where H represents the hypothesis translation, Rrepresents a reference translation contained in the setof available references R; L defines the (exact) lex-ical match component in Equation (2), A defines theadequacy component in Equation (3); and ?
and ?are tunable weights for these two components.
Ifmultiple references are provided, the score of thesegment is the maximum score achieved by compar-ing the segment to each available reference.L(H,R) =|H?R|?|H| ?
|R|(2)The lexical match component measures the over-lap between the two representations in terms of thecosine similarity metric.
A segment, either a hypoth-esis or a reference, is represented as a bag of tokensextracted from an unstructured representation, thatis, bag of unigrams (words or stems).
Cosine sim-ilarity was chosen, as opposed to simply checkingthe percentage of overlapping words (POW) becausecosine does not penalize differences in the length ofthe hypothesis and reference translation as much asPOW.
Cosine similarity normalizes the cardinalityof the intersection |H?R| using the geometric mean?|H| ?
|R| instead of the union |H?R|.
This is par-ticularly important for the matching of arguments -which is also based on cosine similarity.
If an hy-pothesized argument has the same meaning as itsreference translation, but differs from it in length,cosine will penalize less the matching than POW.That is specially interesting when core argumentsget merged with modifiers due to bad semantic rolelabeling (e.g.
[A0 I] [T bought] [A1 something to eatyesterday] instead of [A0 I] [T bought] [A1 some-thing to eat] [AM-TMP yesterday]).A(H,R) =?v?V verb score(Hv, Rv)|Vr|(3)In the adequacy component, V is the set of verbsaligned between H and R, and |Vr| is the number ofverbs in R. Hereafter the indexes h and r stand forhypothesis and reference translations, respectively.Verbs are aligned using VerbNet (Schuler, 2006) andVerbOcean (Chklovski and Pantel, 2004).
A verb inthe hypothesis vh is aligned to a verb in the refer-ence vr if they are related according to the follow-ing heuristics: (i) the pair of verbs share at least oneclass in VerbNet; or (ii) the pair of verbs holds a re-lation in VerbOcean.For example, in VerbNet the verbs spook and ter-rify share the same class amuse-31.1, and in VerbO-cean the verb dress is related to the verb wear.verb score(Hv, Rv) =?a?Ar?Atarg score(Ha, Ra)|Ar|(4)The similarity between the arguments of a verbpair (vh, vr) in V is measured as defined in Equa-tion (4), where Ah and At are the sets of labeledarguments of the hypothesis and the reference re-spectively and |Ar| is the number of arguments ofthe verb in R. In other words, we only measure thesimilarity of arguments in a pair of sentences that areannotated with the same role.
This ensures that thestructure of the sentence is taken into account (forexample, an argument in the role of agent would notbe compared against an argument in a role of experi-encer).
Additionally, by restricting the comparisonto arguments of a given verb pair, we avoid argumentconfusion in sentences with multiple verbs.The arg score(Ha, Ra) computation is based onthe cosine similarity as in Equation (2).
We treatthe tokens in the argument as a bag-of-words.
How-ever, in this case we change the representation ofthe segments.
If the two sets do not match exactly,we expand both of them by adding similar words.For every mismatch in a segment, we retrieve the11820-most similar words from Dekang Lin?s distribu-tional thesaurus (Lin, 1998), resulting in sets withricher lexical variety.The following example shows how the computa-tion of A(H,R) is performed, considering the fol-lowing hypothesis and reference translations:H: The lack of snow discourages people from orderingski stays in hotels and boarding houses.R: The lack of snow is putting people off booking skiholidays in hotels and guest houses.1.
extract verbs from H: Vh = {discourages, ordering}2. extract verbs from R: Vr = {putting, booking}3. similar verbs aligned with VerbNet (shared classget-13.5.1): V = {(vh = order,vr = book)}4. compare arguments of (vh = order,vr = book):Ah = {A0, A1, AM-LOC}Ar = {A0, A1, AM-LOC}5.
Ah ?Ar = {A0, A1, AM-LOC}6. exact matches:HA0 = {people} and RA0 = {people}argument score = 17. different word forms: expand the representation:HA1 = {ski, stays} and RA1 = {ski, holidays}expand to:HA1 = {{ski},{stays, remain... journey...}}RA1 = {{ski},{holidays, vacations, trips... jour-ney...}}argument score = 0.58. similarly to HAM?LOC and RAM?LOCargument score = 0.729. verb score (order, book) = 1+0.5+0.723 = 0.7410.
A(H,R) = 0.742 = 0.37Different from previous work, we have not usedWordNet to measure lexical similarity for two mainreasons: problems with lexical ambiguity and lim-ited coverage in WordNet (instances of named enti-ties are not in WordNet, e.g.
Barack Obama).
Forexample, in WordNet the aligned verbs (order/book)from the previous hypothesis and reference trans-lations have: 9 senses - order (e.g.
give instruc-tions to or direct somebody to do something withauthority, make a request for something, etc.)
- and4 senses - book (engage for a performance, arrangefor and reserve (something for someone else) in ad-vance, etc.).
Thus, a WordNet-based similarity mea-sure would require disambiguating segments, an ad-ditional step and a possible source of errors.
Second,a thresholds would need to be set to determine whena pair of verbs is aligned.
In contrast, the structure ofVerbNet (i.e.
clusters of verbs) allows a binary deci-sion, although the VerbNet heuristic results in someerrors, as we discuss in Section 5.4 ResultsWe set the weights ?
and ?
by experimental test-ing to ?
= 1 and ?
= 0.25.
The lexical componentweight is prioritized because it has shown a good av-erage Kendall?s tau correlation (0.23) on a develop-ment dataset (Callison-Burch et al, 2010).
Table 1shows the correlation of the lexical component withhuman judgments for a number of language pairs.Table 1: Kendall?s tau segment-level correlation of thelexical component with human judgmentsMetric cz-en fr-en de-en es-en avgLexical 0.27 0.21 0.26 0.19 0.23We use the SENNA1 SRL system to tag thedataset with semantic roles.
SENNA has shown tohave achieved an F-measure of 75.79% for taggingsemantic roles over the CoNLL 2005 2 benchmark.We compare our metric against standard BLEU(Papineni et al, 2002), METEOR (Denkowski andLavie, 2010) and other previous metrics reported in(Callison-Burch et al, 2010) which also claim to usesome form of semantic information (see Section 2for their description).
The comparison is made interms of Kendall?s tau correlation against the humanjudgments at a segment-level.
For our submission tothe shared evaluation task, system-level scores areobtained by averaging the segment-level scores.TINE achieves the same average correlation withBLUE, but outperforms it for some language pairs.Additionally, TINE outperforms some of the previ-ous which use WordNet to deal with synonyms aspart of the lexical matching.The closest metric to TINE (Gime?nez et al,2010), which also uses semantic roles as one of its1http://ml.nec-labs.com/senna/2http://www.lsi.upc.edu/ srlconll/119Table 2: Comparison with previous semantically-oriented metrics using segment-level Kendall?s tau cor-relation with human judgmentsMetric cz-en fr-en de-en es-en avg(Liu et al,2010)0.34 0.34 0.38 0.34 0.35(Gime?nezet al, 2010)0.34 0.33 0.34 0.33 0.33(Wong andKit, 2010)0.33 0.27 0.37 0.32 0.32METEOR 0.33 0.27 0.36 0.33 0.32TINE 0.28 0.25 0.30 0.22 0.26BLEU 0.26 0.22 0.27 0.28 0.26(He et al,2010)0.15 0.14 0.17 0.21 0.17(Tratzand Hovy,2008)0.05 0.0 0.12 0.05 0.05components, achieves better performance.
However,this metric is a rather complex combination of anumber of other metrics to deal with different lin-guistic phenomena.4.1 Further ImprovementsAs an additional experiment, we use BLEU as thelexical component L(H,R) in order to test if theshallow-semantic component can contribute to theperformance of this standard evaluation metric.
Ta-ble 3 shows the results of the combination of BLEUand the shallow-semantic component using the sameparameter configuration as in Section 4.
The addi-tion of the shallow-semantic component increasedthe average correlation of BLEU from 0.26 to 0.28.Table 3: TINE-B: Combination of BLEU and theshallow-semantic componentMetric cz-en fr-en de-en es-en avgTINE-B 0.27 0.25 0.30 0.30 0.28Finally, we improve the tuning of the weights ofthe components (?
and ?
parameters) by using asimple genetic algorithm (Back et al, 1999) to se-lect the weights that maximize the correlation withhuman scores on a development set (we use the de-velopment sets from WMT10 (Callison-Burch et al,2010)).
The configuration of the genetic algorithmis as follows:?
Fitness function: Kendall?s tau correlation?
Chromosome: two real numbers, ?
and ??
Number of individuals: 80?
Number of generations: 100?
Selection method: roulette?
Crossover probability: 0.9?
Mutation probability: 0.01Table 4 shows the parameter values obtainingfrom tuning for each language pair and the corre-lation achieved by the metric with such parameters.With such an optimization step the average correla-tion of the metric increases to 0.29.Table 4: Optimized values of the parameters using a ge-netic algorithm and Kendall?s tau and final correlation ofthe metric on the test setsLanguage pair Correlation ?
?cz-en 0.28 0.62 0.02fr-en 0.25 0.91 0.03de-en 0.30 0.72 0.1es-en 0.31 0.57 0.02avg 0.29 ?
?5 DiscussionIn what follows we discuss with a few examplessome of the common errors made by TINE.
Over-all, we consider the following categories of errors:1.
Lack of coverage of the ontologies.R: This year, women were awarded the Nobel Prize in allfields except physicsH: This year the women received the Nobel prizes in allcategories less physicalThe lack of coverage in VerbNet prevented thedetection of the similarity between receive andaward.2.
Matching of unrelated verbs.R: If snow falls on the slopes this week, Christmas willsell out too, says Schiefert.H: If the roads remain snowfall during the week, the datesof Christmas will dry up, said Schiefert.In VerbOcean remain and say are incorrectly120said to be related.
VerbOcean was cre-ated by a semi-automatic extraction algorithm(Chklovski and Pantel, 2004) with an averageaccuracy of 65.5%.3.
Incorrect tagging of the semantic roles bySENNA.R: Colder weather is forecast for Thursday, so if anythingfalls, it should be snow.H: On Thursday , must fall temperatures and, if there israin, in the mountains should.The position of the predicates affects the SRLtagging.
The predicate fall has the followingroles (A1, V, and S-A1) in the reference, andthe following roles (AM-ADV, A0, AM-MOD,and AM-DIS) in the hypothesis.
As a con-sequence, the metric cannot attempt to matchthe fillers.
Also, SRL systems do not detectphrasal verbs such as in the example of Section3, where the action putting people off is similarto discourages.6 Conclusions and Future WorkWe have presented an MT evaluation metric basedon the alignment of semantic roles and flexiblematching of role fillers between hypothesis and ref-erence translations.
To deal with inexact matches,the metric uses ontologies and distributional seman-tics, as opposed to lexical databases like WordNet,in order to minimize ambiguity and lack of cover-age.
The metric also uses an exact lexical matchingcomponent to reward hypotheses that present lexicalchoices similar to those of the reference translation.Given the simplicity of the metric, it has achievedcompetitive results.
We have shown that the additionof the shallow-semantic component into a lexicalcomponent yields absolute improvements in the cor-relation of 3%-6% on average, depending on the lex-ical component used (cosine similarity or BLEU).In future work, in order to improve the perfor-mance of the metric we plan to add components toaddress a few other linguistic phenomena such asin (Gime?nez and Ma?rquez, 2007; Gime?nez et al,2010).
In order to deal with the coverage problemof an ontology, we plan to use distributional seman-tics (i.e.
word space models) also to align the pred-icates.
We consider using a backoff model for theshallow-semantic component to deal with the veryfrequent cases where there are no comparable pred-icates between the reference and hypothesis transla-tions, which result in a 0 score from the semanticcomponent.
Finally, we plan to improve the lexicalcomponent to better tackle fluency, for example, byadding information about the word order.ReferencesThomas Back, David B. Fogel, and ZbigniewMichalewicz, editors.
1999.
Evolutionary Com-putation 1, Basic Algorithms and Operators.
IOPPublishing Ltd., Bristol, UK, 1st edition.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72, Ann Arbor, Michigan, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden, July.Xavier Carreras and Llu?
?s Ma?rquez.
2005.
Introductionto the conll-2005 shared task: Semantic role labeling.In Proceedings of the 9th Conference on Natural Lan-guage Learning, CoNLL-2005, Ann Arbor, MI USA.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained Semantic VerbRelations.
In Dekang Lin and Dekai Wu, editors, Pro-ceedings of EMNLP 2004, pages 33?40, Barcelona,Spain, July.Michael Denkowski and Alon Lavie.
2010.
Meteor-nextand the meteor paraphrase tables: Improved evaluationsupport for five target languages.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Transla-tion and MetricsMATR, pages 339?342, July.Jesu?s Gime?nez and Llu?
?s Ma?rquez.
2007.
Linguistic fea-tures for automatic evaluation of heterogenous mt sys-tems.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, StatMT ?07, pages 256?264, Stroudsburg, PA, USA.Jesu?s Gime?nez, Llu?
?s Ma?rquez, Elisabet Comelles, IreneCastello?n, and Victoria Arranz.
2010.
Document-level automatic mt evaluation based on discourse rep-resentations.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metrics-MATR, WMT ?10, pages 333?338, Stroudsburg, PA,USA.121Yifan He, Jinhua Du, Andy Way, and Josef van Gen-abith.
2010.
The dcu dependency-based metric inwmt-metricsmatr 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translation andMetricsMATR, WMT ?10, pages 349?353, Strouds-burg, PA, USA.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics - Volume 2, ACL ?98, pages 768?774, Stroudsburg, PA, USA.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010.Tesla: translation evaluation of sentences with linear-programming-based analysis.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Transla-tion and MetricsMATR, WMT ?10, pages 354?359,Stroudsburg, PA, USA.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Manning.2006.
Learning to recognize features of valid textualentailments.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, pages 41?48,New York City, USA, June.Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,and Christopher D. Manning.
2009.
Measuring ma-chine translation quality as semantic equivalence: Ametric based on entailment features.
Machine Trans-lation, 23:181?193, September.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.Karin Kipper Schuler.
2006.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. thesis,University of Pennsylvania.Stephen Tratz and Eduard Hovy.
2008.
Summarisationevaluation using transformed basic elements.
In Pro-ceedings TAC 2008.Billy T.-M. Wong and Chunyu Kit.
2010.
The parameter-optimized atec metric for mt evaluation.
In Proceed-ings of the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, WMT ?10, pages 360?364, Stroudsburg, PA, USA.122
