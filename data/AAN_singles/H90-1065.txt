Training Set Issues in SRI's DECIPHERSpeech Recognition SystemHy Murveit, Mitch Weintraub, Mike CohenSpeech Research ProgramSRI International333 Ravenswood Ave.Menlo Park, CA 94025AbstractSRI has developed the DECIPHER system, ahidden Markov model (HMM) based continuous peechrecognition system typically used in a speaker-indepen-dent manner.
Initially we review the DECIPHER sys-tem, then we show that DECIPHER's speaker-independent performance improved by 20% when thestandard 3990-sentence speaker-independent test set wasaugmented with training data from the 7200-sentence r -source management speaker-dependent training sentences.We show a further improvement of over 20% when aversion of corrective training was implemented.
Finallywe show improvement using parallel male- and female-trained models in DECIPHER.
The word-error ratewhen all three improvements were combined was 3.7%on DARPA's February 1989 speaker-independent test setusing the standard perplexity 60 wordpair grammar.System DescriptionFront End AnalysisDecipher uses a FFT-based Mel-cepstra frontend.
Twenty-five FFT-Mel filters spanning 100 to 6400hz are used to derive 12 Mel-cepstra coefficients every10-rns frame.
Four features are derived every frame fromthis cepstra sequence.
They are:?
Vector-quantized nergy-normalized Mel-cepstra?
Vector-quantized smoothed 40-ms time derivatives ofthe Mel-cepstra?
Energy?
Smoothed 40-ms energy differencesWe use 256-word speaker-independent code-books to vector-quantize the Mel-cepstra and the Mel-cepstral differences.
The resulting four-feature-per-frame vector is used as input to the DECIPHER HMM-based speech recognition system.Pronunciation ModelsDECIPHER uses pronunciation models generat-ed by applying a phonological rule set to word base-forms.
The technique used to generate the rules aredescribed in Murveit89 and Cohen90.
These generate ap-proximately 40 pronunciations per word as measured onthe DARPA resource management vocabulary.
Speaker-independent pronunciation probabilities are then estimat-ed using these bushy word networks and the forward-backward algorithm in DECIPHER.
The networks arethen pruned so that only the likely pronunciations re-main--typically about four pronunciations per word forthe resource management task.This modeling of pronunciation is one of theways that DECIPHER is distinguished from otherHMM-based systems.
We have shown in Cohen90 thatthis modeling improves ystem performance.Acoustic ModelingDECIPHER builds and trains word models byusing context-based phone models arranged according tothe pronunciation etworks for the word being modeled.Models used include unique-phone-in-word, phone-in-word, triphone, biphone, and generalized-phone forms ofbiphones and triphones, as well as context-independentmodels.
Similar contexts are automatically smoothed to-gether, if they do not adequately model the training da-m, according to a deleted-estimation interpolationalgorithm developed at SRI (similar to Jelinek80).
Theacoustic models reflect both inter-word and across-wordcoarticulatory effects.Training proceeds as follows:?
Initially, context-independent boot models areestimated from hand-labeled portions of the trainingpart of the TIMIT database.?
The boot models are used as input for a 2-iterationcontext-independent model training run, wherecontext-independent models are refined andpronunciation probabilities are calculated using thelarge 40-pronunciation word networks.
As statedabove, these large networks are then pruned to aboutfour pronunciations per word.337?
Context-dependent models are then estimated from asecond 2-iteration forward-backward run, which usesthe context-independent models and the prunednetworks as input.System EvaluationDECIPHER has been evaluated on the speaker-independent continuous-speech DARPA resource manage-ment test sets \[Price88\] \[Pallet89\].
DECIPHER wasevaluated on the November 1989 test set (evaluated bySRI in March 1990) and had 6% word error on the per-plexity 60 task.
This performance was equal to the bestpreviously reported error rate for that condition.
We re-cendy evaluated on the June 1990 task, and achieved6.5% word error for a system trained on 3990 sentencesand 4.8% word error using 11,190 training sentences.Since the October 1989 evaluation, DECI-PHER's performance has improved in three ways:?
We noted when using that the standard 3990-sentenceresource management raining set, that many ofDECIPHER's probability distributions were poorlyestimated.
Therefore., we evaluated DECIPHER withseveral different amounts of training data.
The largesttraining set we used, an ll,190-sentence resourcemanagement training set, improved the word error rateby about 20%.?
We implemented a modified version of IBM'scorrective training algorithm, additionally improvingthe word error rate by about 20%.?
We separated the male and female training data,estimated different H/vIM output distributions foreach sex.
This also improved word accuracy by 20%.These improvements are described in more detailbelow.Effects of Training DataIn a recent study, we discovered that DECI-PHER's word error rate on its training set using the per-plexity 60 grammar was very low (0.7% over the 3990resource management sentences).
Since the test-set errorrate for that system was about 7%, we concluded thatthe system would profit from more training data.
Totest this, we evaluated the system with four databaseseasily available to us as is shown in Table 1.
There SI re-fers to the 3990-sentence speaker-independent portion ofthe resource management (RM) database--109 speakers,30 or 40 sentences each, SD refers to the speaker-depen-dent portion of that database--12 speakers, 600 sentenceseach, and TIMIT refers to the training portion of theTIMIT database--420 speakers, 8 sentences each.
Notethat all SI and SD sentences are related to the resourcemanagement task, while TIMIT's sentences are not relat-ed to that task.
All systems were tested using a continu-ous-speech, speaker-independent condition with theperplexity 60 resource management grammar testing onDARPA's 300-sentence February 1989 speaker-indepen-dent test set.Trainin2 data Sentences Word errorSD 7200 7.3SI 3990 6.7SI+TIMIT 7350 5.8SI+SD 11190 5.3Table 1.Word Error as a Function of Training SetTable 1 shows that performance improved as da-m increased, even when adding the out-of-task TIMIT da-m.
The only exception was that training with 3990sentences from 100 talkers was slighdy better than 7200sentences from 12 talkers.
This is to be expected in aspeaker-independent system.
This last result is consis-tent with the findings in Kubala90 that showed thatthere was not a big performance drop when the numberof speakers was drastically reduced (from 109 to 12) inspeaker-independent systems.
It is likely that more train-ing data would continue to improve performance on thistask; however, we believe that a more sensible studywould be to focus on how large training sets could im-prove performance across tasks and vocabularies.
(See,for instance, Hon90.
)Separating Male and Female ModelsWe experimented with maintaining sex consis-tency in DECIPHER's hypotheses by partitioning maleand female training data and using parallel recognitionsystems as in Bush87.
Two subrecognizers are run in par-allel on unknown speech and the hypothesis from eitherrecognizer with the highest probability is used.
The dis-advantage of this approach is that it makes inefficient useof training data.
That is, in the best scenario the malemodels are trained from only half of the training dataand the female models use only half.
This is inefficientbecause ven though there may be a fundamental differ-ence between the two types of speech, they still havemany things in common and could profit from the oth-ers' training data if used properly.It is no wonder, then, that this approach hasbeen successful in digit recognition systems with anabundance of training data for each parameter to be esti-mated, but has not significantly improved performancein large-vocabulary systems with a relatively smallamount of training data \[Paul89\].
To validate the idea ofsex consistency, we trained male-only and female-onlyversions of the DECIPHER speech recognition system us-ing the l ll90-sentence SI+SD training set to make surethe data partitions had enough data.
We produced SI+SD338subsets with 4160 female and 7030 male sentences.
Thesesystems were tested on the DARPA February 1989speaker-independent test set using the DARPA word-pair grammar (perplexity 60) and are compared below toa similar recognition system trained on all 11190 sen-tences.Standard Male/FemaleMale speakers 5.5 4.6Female speakers 4.9 4.0All speakers 5.3 4.3Table 2.
Speaker-Independent %Word Errorfor Male/Female Parallel Recognizers(February 1989 SI Test Set)The results in Table 2 show a 19% reduction inthe error rate when using sex-consistent recognition sys-terns.
This is a significant error rate reduction.
A closerlook at the system's performance showed that it correct-ly assigned the talker's sex in each of the 300 test sen-tences.Discriminative Techniques Currently inDECIPHERWe have implemented a type of corrective train-ing \[Bah188, Lee89\] in the DECIPHER system.
Our im-plementation is similar to that described in Lee89 withthe following exceptions or notes:1.
We use four partitions (rather than two) for our de-leted estimation technique.
In this way, the recogni-tion systems used to generate alignments forcorrective training are as similar as possible to theoverall recognition system.2.
We do not alter the actual HMM counts for states,but rather scale the states' vector output probabili-ties by the ratio (#correct+#deletions-#insertions)divided by #correct.
These counts are generated byframe alignments of the recognizer hypothesis andthe correct sentence.
This improves performancefrom 5.9% word error to 5.1% on the February 1989test set using the standard SI training set--the uncor-rected system has 6.7% word error.
The reason forthis improvement may be that adjusting the countsof a model affects other models (given our deletedinterpolation estimation smoothing algorithms) thatdo not require correction.
Scaling model probabili-ties only adjusts the models that require change.3.
We do not generate r inforcement errors.
We planto do so using an N-best algorithm to generate alter-nate hypotheses.4.
We can not iterate the algorithm until the N-best re-inforcement is implemented, because the second itera-tion error rate on the sentences that had beencorrected by the first iteration was under 0.3%.Our implementation reduced the error rate onthe February 1989 test set by 24% (6.7% to 5.1%) whichis approximately the improvement gained by Lee89 andBah188.Points 3 and 4 above are a concern, because theylimit the efficiency with which this algorithm could useits already limited training data.
To examine this, weperformed the following two experiments.
(1) We add-ed a second pass of corrective training, using the speaker-dependent RM training sentences (SD).
(2) We combinedSD and the SI sentences, thereby using a larger overalltraining set, but continued to use one pass of correctivetraining.
Table 3 shows that, not surprisingly, thoughSystem Training Word Errorno correction SI 6.7 %I pass correction SI 5.1%add 2nd SD pass SI 4.6%no correction SI+SD 5.3%1 pass correction SI+SD 4.1%Table 3.
Corrective Trainingwith Extra Data(Uses February 1989 RM Test Set)there was improvement when extra data were used as asecond pass for the corrective training algorithm, it wasbetter to use these data to simply augment he trainingdata (4.6% versus 4.1% word error).
It is also interest-ing to note that the improvement gained by correctivetraining with the 3990 SI sentences (6.7% to 5.1%, 24%fewer errors) was approximately equal to the improve-ment gained by applying corrective training to the larger11190 SI+SD sentences (5.3% to 4.1%, 23% fewer er-rors).
This leads us to believe that lack of training datais not more of a bottleneck for corrective training thanit is for the system as a whole.Combining Corrective Training andSex ConsistencyWe combined both sex consistency and correc-tive training and arrived at the improvement shown inTable 4.
We didn't achieve the same 20% improvement asin the past, probably due to training data limitations.Attempting the combined system with the stan-dard 3990-sentence training set resulted in poor perfor-mance, primarily because the female models used to train339the corrective training partitions had only 870 sentencesof training data.Svstem Trainin~ Data Word errorStandard SI 6.7Standard SI+SD 5.3+disc SI 5.1+sex SI+SD 5.3+disc SI+SD 4.1+disc+sex SI+SD 3.7Table 4.
Summary of Improvementsfor DECIPHER(Uses February 1989 RM Test Set)SummaryWe have shown significant improvements forthe DECIPHER speech recognition system by (1) increas-ing training data size, (2) implementing corrective train-ing, and (3) separating male and female training data.We have combined all three improvements to achieve ourbest performing system, one that has a word-error ateof 3.7% on DARPA's resource management February1989 speaker-independent t st set.We believe that the use of a large training set allows significant improvements in speech recognition ac-curacy, and therefore we advocate using the largertraining set as a standard in future system evaluations.L.N.
Kanal (editors), Pattern Recognition inPractice, North Holland Publishing Company,Amsterdam, the Netherlands.\[Kubala90\] Kubala, Francis, Richard Schwartz, andChris Barry, "Speaker Adaptation from aSpeaker Independent Training Corpus,"Proceedings ICASSP-90.\[Lee89\] Lee, K.F., and S. Mahajan, "Corrective andReinforcement Learning for Speaker-IndependentContinuous Speech Recognition," TechnicalReport CMU-CS-89-100, Carnegie MellonUniversity, January 1989.\[Murveit89\] Murveit, Hy , M. Cohen, P. Price, G.Baldwin, M. Weintraub, and J. Bernstein,"SRI's DECIPHER System," Proceedings ofthe DARPA Speech and Natural LanguageWorkshop, February, 1989.\[Pallet89\] Pallet, D., Benchmark Tests for DARPAResource Management Database PerformanceEvaluations," Proceedings ICASSP-89.\[Paul89\] Paul, Douglas, "The Lincoln Continuous SpeechRecognition System: Recent Developments andResults," Proceedings of the DARPA Speechand Natural Language Workshop, February,1989.\[Price88\] Price, P., W.M.
Fisher, J. Bernstein, and D.S.Pallet, "The DARPA 1000-Word ResourceManagement Database for Continuous SpeechRecognition," Proceedings ICASSP-88.References\[Bah188\] Bahl, L.R., P.F.
Brown, P.V.
De Souza, R.L.Mercer, "A New Algorithm for the Estimationof Hidden Markov Model Parameters,"Proceedings ICASSP-88.\[Bush87\] Bush, Marcia A., and Gary E. Kopec, "Network-Based Connected Speech Recognition," IEEETrans.
Acoust., Speech, Signal Processing, vol.ASSP-35, October 1987\[Cohen90\] Cohen, Michael, Hy Murveit, Jared Bemstein,Patti Price, and Mitch Weintraub, "TheDECIPHER Speech Recognition System,"Proceedings ICASSP-90.\[Hon90\] Hon, Hsiao-Wuen, and Kai-Fu Lee, "OnVocabulary-Independent Speech Modeling,"Proceedings ICASSP-90.\[Jelinek80\] Jelinek.
F. and R. Mercer, "InterpolatedEstimation of Markov Source Parameters fromSparse Data," pp.
381-397 in E.S.
Gelsima and340
