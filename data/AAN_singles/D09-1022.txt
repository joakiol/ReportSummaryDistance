Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 210?218,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPExtending Statistical Machine Translation withDiscriminative and Trigger-Based Lexicon ModelsArne Mauser and Sa?sa Hasan and Hermann NeyHuman Language Technology and Pattern Recognition GroupChair of Computer Science 6, RWTH Aachen University, Germany<surname>@cs.rwth-aachen.deAbstractIn this work, we propose two extensions ofstandard word lexicons in statistical ma-chine translation: A discriminative wordlexicon that uses sentence-level source in-formation to predict the target words anda trigger-based lexicon model that extendsIBM model 1 with a second trigger, allow-ing for a more fine-grained lexical choiceof target words.
The models capture de-pendencies that go beyond the scope ofconventional SMT models such as phrase-and language models.
We show that themodels improve translation quality by 1%in BLEU over a competitive baseline on alarge-scale task.1 IntroductionLexical dependencies modeled in standard phrase-based SMT are rather local.
Even though the deci-sion about the best translation is made on sentencelevel, phrase models and word lexicons usually donot take context beyond the phrase boundaries intoaccount.
This is especially problematic since theaverage source phrase length used during decod-ing is small.
When translating Chinese to English,e.g., it is typically close to only two words.The target language model is the only modelthat uses lexical context across phrase boundaries.It is a very important feature in the log-linear setupof today?s phrase-based decoders.
However, itscontext is typically limited to three to six wordsand it is not informed about the source sentence.In the presented models, we explicitly take advan-tage of sentence-level dependencies including thesource side and make non-local predictions for thetarget words.
This is an important aspect whentranslating from languages like German and Chi-nese where long-distance dependencies are com-mon.
In Chinese, for example, tenses are often en-coded by indicator words and particles whose po-sition is relatively free in the sentence.
In German,prefixes of verbs can be moved over long distancestowards the end of the sentence.In this work, we propose two models that canbe categorized as extensions of standard word lex-icons: A discriminative word lexicon that usesglobal, i.e.
sentence-level source information topredict the target words using a statistical classi-fier and a trigger-based lexicon model that extendsthe well-known IBM model 1 (Brown et al, 1993)with a second trigger, allowing for a more fine-grained lexical choice of target words.
The log-linear framework of the discriminative word lexi-con offers a high degree of flexibility in the selec-tion of features.
Other sources of information suchas syntax or morphology can be easily integrated.The trigger-based lexicon model, or simplytriplet model since it is based on word triplets,is not trained discriminatively but uses the classi-cal maximum likelihood approach (MLE) instead.We train the triplets iteratively on a training cor-pus using the Expectation-Maximization (EM) al-gorithm.
We will present how both models al-low for a representation of topic-related sentence-level information which puts them close to wordsense disambiguation (WSD) approaches.
As willbe shown later, the experiments indicate that thesemodels help to ensure translation of content wordsthat are often omitted by the baseline system.
Thisis a common problem in Chinese-English transla-tion.
Furthermore, the models are often capable toproduce a better lexical choice of content words.210The structure of the paper is as follows: In Sec-tion 2, we will address related work and brieflypin down how our models differentiate from pre-vious work.
Section 3 will describe the discrimi-native lexical selection model and the triplet modelin more detail, explain the training procedures andshow how the models are integrated into the de-coder.
The experimental setup and results will begiven in Section 4.
A more detailed discussionwill be presented in Section 5.
In the end, we con-clude our findings and give an outlook for furtherresearch in Section 6.2 Related WorkSeveral word lexicon models have emerged in thecontext of multilingual natural language process-ing.
Some of them were used as a machine transla-tion system or as a part of one such system.
Thereare three major types of models: Heuristic modelsas in (Melamed, 2000), generative models as theIBM models (Brown et al, 1993) and discrimina-tive models (Varea et al, 2001; Bangalore et al,2006).Similar to this work, the authors of (Varea etal., 2001) try to incorporate a maximum entropylexicon model into an SMT system.
They usethe words and word classes from the local con-text as features and show improvements with n-best rescoring.The models in this paper are also related toword sense disambiguation (WSD).
For example,(Chan et al, 2007) trained a discriminative modelfor WSD using local but also across-sentence un-igram collocations of words in order to refinephrase pair selection dynamically by incorporat-ing scores from the WSD classifier.
They showedimprovements in translation quality in a hierar-chical phrase-based translation system.
AnotherWSD approach incorporating context-dependentphrasal translation lexicons is given in (Carpuatand Wu, 2007) and has been evaluated on sev-eral translation tasks.
Our model differs from thelatter in three ways.
First, our approach mod-els word selection of the target sentence based onglobal sentence-level features of the source sen-tence.
Second, instead of disambiguating phrasesenses as in (Carpuat and Wu, 2007), we modelword selection independently of the phrases usedin the MT models.
Finally, the training is done in adifferent way as will be presented in Sections 3.1.1and 3.2.1.Recently, full translation models using discrim-inative training criteria emerged as well.
Theyare designed to generate a translation for a givensource sentence and not only score or disam-biguate hypotheses given by a translation system.In (Ittycheriah and Roukos, 2007), the model canpredict 1-to-many translations with gaps and useswords, morphologic and syntactic features fromthe local context.The authors of (Venkatapathy and Bangalore,2007) propose three different models.
The firstone is a global lexical selection model which in-cludes all words of the source sentence as features,regardless of their position.
Using these features,the system predicts the words that should be in-cluded in the target sentence.
Sentence structure isthen reconstructed using permutations of the gen-erated bag of target words.
We will also use thistype of features in our model.One of the simplest models in the context oflexical triggers is the IBM model 1 (Brown etal., 1993) which captures lexical dependencies be-tween source and target words.
It can be seenas a lexicon containing correspondents of transla-tions of source and target words in a very broadsense since the pairs are trained on the full sen-tence level.
The trigger-based lexicon model usedin this work follows the training procedure intro-duced in (Hasan et al, 2008) and is integrated di-rectly in the decoder instead of being applied inn-best list reranking.
The model is very close tothe IBM model 1 and can be seen as an extensionof it by taking another word into the condition-ing part, i.e.
the triggering items.
Thus, insteadof p(f |e), it models p(f |e, e?).
Furthermore, sincethe second trigger can come from any part of thesentence, there is a link to long-range monolin-gual triggers as presented in (Tillmann and Ney,1997) where a trigger language model was trainedusing the EM algorithm and helped to reduce per-plexities and word error rates in a speech recog-nition experiment.
In (Rosenfeld, 1996), anotherapproach was chosen to model monolingual trig-gers using a maximum-entropy based framework.Again, this adapted LM could improve speechrecognition performance significantly.A comparison of a variant of the trigger-basedlexicon model applied in decoding and n-best listreranking can be found in (Hasan and Ney, 2009).In order to reduce the number of overall triplets,the authors use the word alignments for fixing the211first trigger to the aligned target word.
In general,this constraint performs slightly worse than the un-constrained variant used in this work, but allowsfor faster training and decoding.3 Extended Lexicon ModelsIn this section, we present the extended lexiconmodels, how they are trained and integrated intothe phrase-based decoder.3.1 Discriminative Lexicon ModelDiscriminative models have been shown to outper-form generative models on many natural languageprocessing tasks.
For machine translation, how-ever, the adaptation of these methods is difficultdue to the large space of possible translations andthe size of the training data that has to be used toachieve significant improvements.In this section, we propose a discriminativeword lexicon model that follows (Bangalore et al,2007) and integrate it into the standard phrase-based machine translation approach.The core of our model is a classifier that pre-dicts target words, given the words of the sourcesentence.
The structure of source as well as tar-get sentence is neglected in this model.
We donot make any assumptions about the location ofthe words in the sentence.
This is useful in manycases, as words and morphology can depend on in-formation given at other positions in the sentence.An example would be the character?
in Chinesethat indicates a completed or past action and doesnot need to appear close to the verb.We model the probability of the set of targetwords in a sentence e given the set of source wordsf .
For each word in the target vocabulary, we cancalculate a probability for being or not being in-cluded in the set.
The probability of the whole setthen is the product over the entire target vocabu-lary VE:P (e|f) =?e?eP (e+|f) ?
?e?VE\eP (e?|f) (1)For notational simplicity, we use the event e+when the target word e is included in the targetsentence and e?if not.
We model the individualfactors p(e|f) of the probability in Eq.
1 as a log-linear model using the source words from f as bi-nary features?
(f, f) ={1 if f ?
f0 else(2)and feature weights ?f,?
:P (e+|f) =exp(?f?f?f,e+ ?
(f, f))?e?{e+,e?}exp(?f?f?f,e?
(f, f))(3)Subsequently, we will call this model discrimina-tive word lexicon (DWL).Modeling the lexicon on sets and not on se-quences has two reasons.
Phrase-based MT alongwith n-gram language models is strong at predict-ing sequences but only uses information from a lo-cal context.
By using global features and predict-ing words in a non-local fashion, we can augmentthe strong local decisions from the phrase-basedsystems with sentence-level information.For practical reasons, translating from a set toa set simplifies the parallelization of the trainingprocedure.
The classifiers for the target words canbe trained separately as explained in the followingsection.3.1.1 TrainingCommon classification tasks have a relativelysmall number of classes.
In our case, the num-ber of classes is the size of the target vocabulary.For large translation tasks, this is in the range of ahundred thousand classes.
It is far from what con-ventional out-of-the-box classifiers can handle.The discriminative word lexicon model has theconvenient property that we can train a separatemodel for each target word making paralleliza-tion straightforward.
Discussions about possibleclassifiers and the choice of regularization canbe found in (Bangalore et al, 2007).
We usedthe freely available MegaM Toolkit1for training,which implements the L-BFGS method (Byrd etal., 1995).
Regularization is done using Gaussianpriors.
We performed 100 iterations of the train-ing algorithm for each word in the target vocabu-lary.
This results in a large number of classifiers tobe trained.
For the Arabic-English data (cf.
Sec-tion 4), the training took an average of 38 secondsper word.
No feature cutoff was used.3.1.2 DecodingIn search, we compute the model probabilities asan additional model in the log-linear model com-bination of the phrase-based translation approach.To reduce the memory footprint and startup timeof the decoding process, we reduced the number of1http://www.cs.utah.edu/?hal/megam/212parameters by keeping only large values ?f,esincesmaller values tend to have less effect on the over-all probability.
In experiments we determined thatwe could safely reduce the size of the final modelby a factor of ten without losing predictive power.In search, we compute the model probabilities asan additional model in the log-linear combination.When scoring hypotheses from the phrase-basedsystem, we see the translation hypothesis as theset of target words that are predicted.
Words fromthe target vocabulary which are not included inthe hypothesis are not part of the set.
During thesearch process, however, we also have to score in-complete hypotheses where we do not know whichwords will not be included.
This problem is cir-cumvented by rewriting Eq.
1 asP (e|f) =?e?VEP (e?|f) ?
?e?eP (e+|f)P (e?|f).The first product is constant given a source sen-tence and therefore does not affect the search.
Us-ing the model assumption from Eq.
3, we can fur-ther simplify the computation and compute themodel score entirely in log-space which is numer-ically stable even for large vocabularies.
Exper-iments showed that using only the first factor ofEq.
1 is sufficient to obtain good results.In comparison with the translation model from(Bangalore et al, 2007) where a threshold on theprobability is used to determine which words areincluded in the target sentence, our approach relieson the phrase model to generate translation candi-dates.
This has several advantages: The length ofthe translation is determined by the phrase model.Words occurring multiple times in the translationdo not have to be explicitly modeled.
In (Banga-lore et al, 2007), repeated target words are treatedas distinct classes.The main advantage of the integration beingdone in a way as presented here is that the phrasemodel and the discriminative word lexicon modelare complementary in the way they model thetranslation.
While the phrase model is good inpredicting translations in a local context, the dis-criminative word lexicon model is able to predictglobal aspects of the sentence like tense or vocabu-lary changes in questions.
While the phrase modelis closely tied to the structure of word and phrasealignments, the discriminative word lexicon modelcompletely disregards the structure in source andtarget sentences.3.2 Trigger-based Lexicon ModelThe triplets of the trigger-based lexicon model,i.e.
p(e|f, f?
), are composed of two words in thesource language triggering one target languageword.
We chose this inverse direction since itcan be integrated directly into the decoder and,thus, does not rely on a two-pass approach us-ing reranking, as it is the case for (Hasan et al,2008).
The triggers can originate from words ofthe whole source sentence, also crossing phraseboundaries of the conventional bilingual phrasepairs.
The model is symmetric though, mean-ing that the order of the triggers is not relevant,i.e.
(f, f??
e) = (f?, f ?
e).
Nevertheless,the model is able to capture long-distance effectssuch as verb splits or adjustments to lexical choiceof the target word given the topic-triggers of thesource sentence.
In training, we determine theprobability of a target sentence eI1given the sourcesentence fJ1within the model byp(eI1|fJ1) =I?i=1p(ei|fJ1)=I?i=12J(J + 1)J?j=0J?j?=j+1p(ei|fj, fj?
), (4)where f0denotes the empty word and, thus, forfj= ?, allows for modeling the conventional (in-verse) IBM model 1 lexical probabilities as well.Since the second trigger fj?always starts right ofthe current first trigger, the model is symmetricand does not need to look at all trigger pairs.
Eq.
4is used in the iterative EM training on all sentencepairs of the training data which is described inmore detail in the following.3.2.1 TrainingFor training the trigger-based lexicon model, weapply the Expectation-Maximization (EM) algo-rithm (Dempster et al, 1977).
The goal is to max-imize the log-likelihood Ftripof this model fora given bilingual training corpus {(fJn1, eIn1)}N1consisting of N sentence pairs:Ftrip:=N?n=1log p(eIn1|fJn1),where Inand Jnare the lengths of the n-th tar-get and source sentence, respectively.
An aux-iliary function Q(?
; ??)
is defined based on Ftrip213where ??
is the updated estimate within an itera-tion which is to be derived from the current esti-mate ?.
Here, ?
stands for the entire set of modelparameters, i.e.
the set of all {?
(e|f, f?)}
with theconstraint?e?
(e|f, f?)
= 1.
The accumulators?(?)
are therefore iteratively trained on the train-ing data by using the current estimate, i.e.
deriv-ing the expected value (E-step), and maximizingtheir likelihood afterwards to reestimate the distri-bution.
Thus, the perplexity of the training data isreduced in each iteration.3.2.2 DecodingIn search, we can apply this model directly whenscoring bilingual phrase pairs.
Given a trainedmodel for p(e|f, f?
), we compute the feature scorehtrip(?)
of a phrase pair (e?,?f) ashtrip(e?,?f, fJ0) = (5)?
?ilog(2J ?
(J + 1)?j?j?>jp(e?i|fj, fj?
)),where i moves over all target words in the phrasee?, the second sum selects all source sentencewords fJ0including the empty word, and j?> jincorporates the rest of the source sentence right ofthe first trigger.
We take negative log-probabilitiesand normalize to obtain the final score (represent-ing costs) for the given phrase pair.
Note that insearch, we can only use this direction, p(e|f, f?
),since the whole source sentence is available fortriggering effects whereas not all target wordshave been generated so far, as it would be neces-sary for the standard direction, p(f |e, e?
).Due to the enormous number of triplets, wetrained the model on a subset of the overall train-ing data.
The subcorpus, mainly consisting ofnewswire articles, contained 1.4M sentence pairswith 32.3M running words on the English side.We trained two versions of the triplet lexicon, oneusing 4 EM iterations and another one that wastrained for 10 EM iterations.
Due to trimmingof triplets with small probabilities after each it-eration, the version based on 10 iterations wasslightly smaller, having 164 million triplets butalso performed slightly worse.
Thus, for the ex-periments, we used the version based on 4 itera-tions which contained 291 million triplets.Note that decoding with this model can be quiteefficient if caching is applied.
Since the givensource sentence does not change, we have to cal-culate p(e|f, f?)
for each e only once and can re-train (C/E) test08 (NW/WT)Sent.
pairs 9.1M 480 490Run.
words 259M/300M 14.8K 12.3KVocabulary 357K/627K 3.6K 3.2KTable 1: GALE Chinese-English corpus statisticsincluding two test sets: newswire and web text.train C/E ?
A/E nist08 C/ASent.
pairs 7.3M 4.6M 1357Words (M) 185/196 142/139 36K/46KVocab.
(K) 163/265 351/361 6.4K/9.6KTable 2: NIST Chinese-English and Arabic-English corpus statistics including the official2008 test sets.trieve the probabilities from the cache for consec-utive scorings of the same target word e. This sig-nificantly speeds up the decoding process.4 Experimental EvaluationIn this section we evaluate our lexicon models onthe GALE Chinese-English task for newswire andweb text translation and additionally on the of-ficial NIST 2008 task for both Chinese-Englishand Arabic-English.
The baseline system wasbuilt using a state-of-the art phrase-based MT sys-tem (Zens and Ney, 2008).
We use the standardset of models with phrase translation probabilitiesfor source-to-target and target-to-source direction,smoothing with lexical weights, a word and phrasepenalty, distance-based and lexicalized reorderingand a 5-gram (GALE) or 6-gram (NIST) targetlanguage model.We used training data provided by the Linguis-tic Data Consortium (LDC) consisting of 9.1Mparallel Chinese-English sentence pairs of vari-ous domains for GALE (cf.
Table 1) and smalleramounts of data for the NIST systems (cf.
Ta-ble 2).
The DWL and Triplet models were inte-grated into the decoder as presented in Section 3.For the GALE development and test set, we sep-arated the newswire and web text parts and didseparate parameter tuning for each genre usingthe corresponding development set which consistsof 485 sentences for newswire texts and 533 sen-tences of web text.
The test set has 480 sentencesfor newswire and 490 sentences for web text.
ForNIST, we tuned on the official 2006 eval set andused the 2008 evaluation set as a blind test set.214GALE NW WTtest08 BLEU TER BLEU TER[%] [%] [%] [%]Baseline 32.3 59.38 25.3 64.40DWL 33.1 58.90 26.2 63.75Triplet 32.9 58.59 26.2 64.20DWL+Trip.
33.3 58.23 26.3 63.87Table 3: Results on the GALE Chinese-Englishtest set for the newswire and web text setting(case-insensitive evaluation).4.1 Translation ResultsThe translation results on the two GALE testsets are shown in Table 3 for newswire and webtext.
Both the discriminative word lexicon and thetriplet lexicon can individually improve the base-line by approximately +0.6?0.9% BLEU and -0.5?0.8% TER.
For the combination of both lexiconson the newswire setting, we observe only a slightimprovement on BLEU but also an additionalboost in TER reduction, arriving at +1% BLEUand -1.2% TER.
For web text, the findings are sim-ilar: The combination of the discriminative andtrigger-based lexicons yields +1% BLEU and de-creases TER by -0.5%.We compared these results against an inverseIBM model 1 but the results were inconclusivewhich is consistent with the results presented in(Och et al, 2004) where no improvements wereachieved using p(e|f).
In our case, inverse IBM1improves results by 0.2?0.4% BLEU on the devel-opment set but does not show the same trend onthe test sets.
Furthermore, combining IBM1 withDWL or Triplets often even degraded the transla-tion results, e.g.
only 32.8% BLEU was achievedon newswire for a combination of the IBM1, DWLand Triplet model.
In contrast, combinations ofthe DWL and Triplet model did not degrade per-formance and could benefit from each other.In addition to the automatic scoring, we alsodid a randomized subjective evaluation where thehypotheses of the baseline was compared againstthe hypotheses generated using the discrimina-tive word lexicon and triplet models.
We evalu-ated 200 sentences from newswire and web text.In 80% of the evaluated sentences, the improvedmodels were judged equal or better than the base-line.We tested the presented lexicon models also onanother large-scale system, i.e.
NIST, for two lan-NIST Chinese-Eng.
Arabic-Eng.nist08 BLEU TER BLEU TER[%] [%] [%] [%]Baseline 26.8 65.11 42.0 50.55DWL 27.6 63.56 42.4 50.01Triplet 27.7 63.60 42.9 49.76DWL+Trip.
27.9 63.56 43.0 49.15Table 4: Results on the test sets for the NIST 2008Chinese-English and Arabic-English task (case-insensitive evaluation).guage pairs, namely Chinese-English and Arabic-English.
Interestingly, the results obtained forArabic-English are similar to the findings forChinese-English, as can be seen in Table 4.
Theoverall improvements for this language pair are+1% BLEU and -1.4% TER.
In contrast to theGALE Chinese-English task, the triplet lexiconmodel for the Arabic-English language pair per-forms slightly better than the discriminative wordlexicon.These results strengthen the claim that the pre-sented models are capable of improving lexicalchoice of the MT system.
In the next section, wediscuss the observed effects and analyze our re-sults in more detail.5 DiscussionIn terms of automatic evaluation measures, the re-sults indicate that it is helpful to incorporate theextended lexicon models into the search process.In this section, we will analyze some more detailsof the models and take a look at the lexical choicethey make and what differentiates them from thebaseline models.
In Table 5, we picked an ex-ample sentence from the GALE newswire test setand show the different hypotheses produced by oursystem.
As can be seen, the baseline does notproduce the present participle of the verb restorewhich makes the sentence somewhat hard to un-derstand.
Both the discriminative and the trigger-based lexicon approach are capable of generatingthis missing information, i.e.
the correct use ofrestoring.
Figure 1 gives an example how discon-tinuous triggers affect the word choice on the tar-get side.
Two cases are depicted where high proba-bilities of triplets including emergency and restor-ing on the target side influence the overall hypoth-esis selection.
The non-local modeling advantagesof the triplet model can be observed as well: The215??
, ??
??
?
??
??
??
??
??
.sourcetarget [...] the emergency rescue group is [...] restoring  the ventilation system.p(restoring | ?
?,  ? )
= 0.1572p(emergency | ?
?, ??)
= 0.3445Figure 1: Triggering effect for the example sentence using the triplet lexicon model.
The Chinese sourcesentence is shown in its segmented form.
Two triplets are highlighted that have high probability andfavor the target words emergency and restoring.Figure 2: Ranking of words for the example sentence for IBM1, Triplet and DWL model.
Ranks aresorted at IBM1, darker colors indicate higher probabilities within the model.triggering events do not need to be located nextto each other or within a given phrase pair.
Theymove across the whole source sentence, thus al-lowing for capturing of long-range dependencies.Table 6 shows the top ten content words that arepredicted by the two models, discriminative wordlexicon and triplet lexicon model.
IBM model 1ranks are indicated by subscripts in the columnof the triplet model.
Although the triplet modelis similar to IBM1, we observe differences in theword lists.
Comparing this to the visualization ofthe probability distribution for the example sen-tence, cf.
Figure 2, we argue that, although theIBM1 and Triplet distributions look similar, thetriplet model is sharper and favors words such asthe ones in Table 6, resulting in different wordchoice in the translation process.
In contrast, theDWL approach gives more distinct probabilities,selecting content words that are not chosen by theother models.Table 7 shows an example from the web texttest set.
Here, the baseline hypothesis containsan incorrect word, anna, which might have beenmistaken for the name ying.
Interestingly, the hy-potheses of the DWL lexicon and the combina-tion of DWL and Triplet contain the correct con-tent word remarks.
The triplet model makes an er-ror by selecting music, an artifact that might comefrom words that co-occur frequently with the cor-responding Chinese verb to listen, i.e.
?
, in thedata.
Although the TER score of the baseline isbetter than the one for the alternative models forthis particular example, we still think that the ob-served effects show how our models help produc-ing different hypotheses that might lead to subjec-tively better translations.An Arabic-English translation example isshown in Table 8.
Here, the term incidents of mur-der in apartments was chosen over the baseline?skillings inside the flats.
Both translations are un-derstandable and the difference in the wording isonly based on synonyms.
The translation usingthe discriminative and trigger-based lexicons bet-ter matches the reference translation and, thus, re-flects a better lexical choice of the content words.6 ConclusionWe have presented two lexicon models that useglobal source sentence context and are capableof predicting context-specific target words.
Themodels have been directly integrated into the de-coder and have shown to improve the translationquality of a state-of-the-art phrase-based machinetranslation system.
The first model was a dis-criminative word lexicon that uses sentence-levelfeatures to predict if a word from the target vo-cabulary should be included in the translation ornot.
The second model was a trigger-based lexi-216Source ??
, ??
??
?
??
????????
.Baseline at present, the accident and rescueteams are currently emergency re-covery ventilation systems.DWL at present, the emergency rescueteams are currently restoring theventilation system.Triplet at present, the emergency rescuegroup is in the process of restoringthe ventilation system.DWL+Tripletat present, the accident emergencyrescue teams are currently restor-ing the ventilation system.Reference right now, the accident emergencyrescue team is making emergencyrepair on the ventilation system.Table 5: Translation example from the GALEnewswire test set, comparing the baseline and theextended lexicon models given a reference trans-lation.
The Chinese source sentence is presentedin its segmented form.con that uses triplets to model long-range depen-dencies in the data.
The source word triggers canmove across the whole sentence and capture thetopic of the sentence and incorporate more fine-grained lexical choice of the target words withinthe decoder.Overall improvements are up to +1% in BLEUand -1.5% in TER on large-scale systems forChinese-English and Arabic-English.
Comparedto the inverse IBM model 1 which did not yieldconsistent improvements, the presented modelsare valuable additional features in a phrase-basedstatistical machine translation system.
We will testthis setup for other language pairs and expect thatlanguages like German where long-distance ef-fects are common can benefit from these extendedlexicon models.In future work, we plan to extend the discrimi-native word lexicon model in two directions: ex-tending context to the document level and featureengineering.
For the trigger-based model, we planto investigate more model variants.
It might beinteresting to look at cross-lingual trigger mod-els such as p(f |e, f?)
or constrained variants likep(f |e, e?)
with pos(e?)
< pos(e), i.e.
the secondtrigger coming from the left context within a sen-tence which has already been generated.
TheseDWL Tripletemergency 0.894 emergency10.048currently 0.330 system20.032current 0.175 rescue80.027emergencies 0.133 accident30.022present 0.133 ventilation70.021accident 0.119 work330.021recovery 0.053 present50.011group 0.046 currently90.010dealing 0.042 rush600.010ventilation 0.034 restoration310.009Table 6: The top 10 content words predicted byeach model for the GALE newswire example sen-tence.
Original ranks for the related IBM model 1are given as subscripts for the triplet model.Source ??
????
, ??????
.Baseline i have listened to anna, happy andlaugh.DWL i have listened to the remarks,happy and laugh.Triplet i have listened to the music, a roarof laughter.DWL+Tripleti have listened to the remarks,happy and laugh.Reference hearing ying?s remark, i laughedaloud happily.Table 7: Translation example from the GALE webtext test set.
In this case, the baseline has a bet-ter TER but we can observe a corrected contentword (remark) for the extended lexicon models.The Chinese source sentence is shown in its seg-mented form.extensions could be integrated directly in searchas well and would enable the system to combineboth directions (standard and inverse) to some ex-tent which was previously shown to help when ap-plying the standard direction p(f |e, e?)
as an addi-tional reranking step, cf.
(Hasan and Ney, 2009).AcknowledgmentsThis material is partly based upon work supportedby the Defense Advanced Research ProjectsAgency (DARPA) under Contract No.
HR0011-06-C-0023, and was partly realized as part ofthe Quaero Programme, funded by OSEO, FrenchState agency for innovation.The authors would like to thank Christian Buck217Source?j.??
@ ?I?Q?K ???
@HBAm?'@??
@XY?HQ??Y??KX????@?j??@??K.IKA?
?.
A?Q?
?????
@ ?g@X ?J??
@HX@?k??K.??Y?
?
PQ.??
?XBaseline some saudi newspapers have published a number of cases that had been subjected toimprisonment without justification, as well as some killings inside the flats and others.DWL+Tripletsome of the saudi newspapers have published a number of cases which were subjectedto imprisonment without justification, as well as some incidents of murder in apartmentsand others.Reference some saudi newspapers have published a number of cases in which people were unjusti-fiably imprisoned, as well as some incidents of murder in apartments and elsewhere.Table 8: Translation example from the NIST Arabic-English test set.
The DWL and Triplet modelsimprove lexical word choice by favoring incidents of murder in apartments instead of killings inside theflats.
The Arabic source is shown in its segmented form.and Juri Ganitkevitch for their help training the ex-tended lexicon models.ReferencesS.
Bangalore, P. Haffner, and S. Kanthak.
2006.
Se-quence classification for machine translation.
InNinth International Conf.
on Spoken Language Pro-cessing, Interspeech 2006 ?
ICSLP, pages 1722?1725, Pitsburgh, PA, September.S.
Bangalore, P. Haffner, and S. Kanthak.
2007.
Statis-tical machine translation through global lexical se-lection and sentence reconstruction.
In 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 152?159, Prague, Czech Republic,June.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19(2):263?312, June.R.
H. Byrd, P. Lu, J. Nocedal, and C. Zhu.
1995.
Alimited memory algorithm for bound constrained op-timization.
SIAM Journal on Scientific Computing,16(5):1190?1208.M.
Carpuat and D. Wu.
2007.
Improving statisticalmachine translation using word sense disambigua-tion.
In Joint Conf.
on Empirical Methods in Nat-ural Language Processing and Computational Nat-ural Language Learning (EMNLP-CoNLL 2007),Prague, Czech Republic, June.Y.
S. Chan, H. T. Ng, and D. Chiang.
2007.
Word sensedisambiguation improves statistical machine trans-lation.
In 45th Annual Meeting of the Associationof Computational Linguistics, pages 33?40, Prague,Czech Republic, June.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical So-ciety, Series B, 39(1):1?22.S.
Hasan and H. Ney.
2009.
Comparison of extendedlexicon models in search and rescoring for SMT.
InNAACL HLT 2009, Companion Volume: Short Pa-pers, pages 17?20, Boulder, Colorado, June.S.
Hasan, J. Ganitkevitch, H. Ney, and J. Andr?es-Ferrer.2008.
Triplet lexicon models for statistical machinetranslation.
In EMNLP, pages 372?381, Honolulu,Hawaii, October.A.
Ittycheriah and S. Roukos.
2007.
Direct translationmodel 2.
In HLT-NAACL 2007: Main Conference,pages 57?64, Rochester, New York, April.I.
D. Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith,K.
Eng, V. Jain, Z. Jin, and D. Radev.
2004.
A smor-gasbord of features for statistical machine transla-tion.
pages 161?168, Boston, MA, May.R.
Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
ComputerSpeech and Language, 10(3):187?228.C.
Tillmann and H. Ney.
1997.
Word triggersand the EM algorithm.
In Proc.
Special InterestGroup Workshop on Computational Natural Lan-guage Learning (ACL), pages 117?124, Madrid,Spain, July.I.
Garc?
?a Varea, F. J. Och, H. Ney, and F. Casacu-berta.
2001.
Refined lexicon models for statisticalmachine translation using a maximum entropy ap-proach.
In ACL ?01: 39th Annual Meeting on Asso-ciation for Computational Linguistics, pages 204?211, Morristown, NJ, USA.S.
Venkatapathy and S. Bangalore.
2007.
Threemodels for discriminative machine translation us-ing global lexical selection and sentence reconstruc-tion.
In SSST, NAACL-HLT 2007 / AMTA Workshopon Syntax and Structure in Statistical Translation,pages 96?102, Rochester, New York, April.R.
Zens and H. Ney.
2008.
Improvements in dynamicprogramming beam search for phrase-based statis-tical machine translation.
In International Work-shop on Spoken Language Translation, Honolulu,Hawaii, October.218
