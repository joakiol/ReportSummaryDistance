Empirical Risk Minimization forProbabilistic Grammars: SampleComplexity and Hardness of LearningShay B. Cohen?Columbia UniversityNoah A.
Smith?
?Carnegie Mellon UniversityProbabilistic grammars are generative statistical models that are useful for compositional andsequential structures.
They are used ubiquitously in computational linguistics.
We present aframework, reminiscent of structural risk minimization, for empirical risk minimization of prob-abilistic grammars using the log-loss.
We derive sample complexity bounds in this frameworkthat apply both to the supervised setting and the unsupervised setting.
By making assumptionsabout the underlying distribution that are appropriate for natural language scenarios, we are ableto derive distribution-dependent sample complexity bounds for probabilistic grammars.
We alsogive simple algorithms for carrying out empirical risk minimization using this framework in boththe supervised and unsupervised settings.
In the unsupervised case, we show that the problem ofminimizing empirical risk is NP-hard.
We therefore suggest an approximate algorithm, similarto expectation-maximization, to minimize the empirical risk.1.
IntroductionLearning from data is central to contemporary computational linguistics.
It is in com-mon in such learning to estimate a model in a parametric family using the maximumlikelihood principle.
This principle applies in the supervised case (i.e., using anno-tated data) as well as semisupervised and unsupervised settings (i.e., using unan-notated data).
Probabilistic grammars constitute a range of such parametric familieswe can estimate (e.g., hidden Markov models, probabilistic context-free grammars).These parametric families are used in diverse NLP problems ranging from syntacticand morphological processing to applications like information extraction, questionanswering, and machine translation.Estimation of probabilistic grammars, in many cases, indeed starts with the prin-ciple of maximum likelihood estimation (MLE).
In the supervised case, and withtraditional parametrizations based on multinomial distributions, MLE amounts to?
Department of Computer Science, Columbia University, New York, NY 10027, United States.E-mail: scohen@cs.columbia.edu.
This research was completed while the first author was at CarnegieMellon University.??
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States.E-mail: nasmith@cs.cmu.edu.Submission received: 1 November 2010; revised submission received: 21 June 2011; accepted for publication:3 August 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 3normalization of rule frequencies as they are observed in data.
In the unsupervised case,on the other hand, algorithms such as expectation-maximization are available.
MLE isattractive because it offers statistical consistency if some conditions are met (i.e., if thedata are distributed according to a distribution in the family, then we will discover thecorrect parameters if sufficient data is available).
In addition, under some conditions itis also an unbiased estimator.An issue that has been far less explored in the computational linguistics literatureis the sample complexity of MLE.
Here, we are interested in quantifying the number ofsamples required to accurately learn a probabilistic grammar either in a supervisedor in an unsupervised way.
If bounds on the requisite number of samples (known as?sample complexity bounds?)
are sufficiently tight, then they may offer guidance tolearner performance, given various amounts of data and a wide range of parametricfamilies.
Being able to reason analytically about the amount of data to annotate, andthe relative gains in moving to a more restricted parametric family, could offer practicaladvantages to language engineers.We note that grammar learning has been studied in formal settings as a problem ofgrammatical inference?learning the structure of a grammar or an automaton (Angluin1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008,among others).
Our setting in this article is different.
We assume that we have a fixedgrammar, and our goal is to estimate its parameters.
This approach has shown greatempirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005)and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein andManning 2004; Cohen and Smith 2010a) settings.
There has also been some discus-sion of sample complexity bounds for statistical parsing models, in a distribution-freesetting (Collins 2004).
The distribution-free setting, however, is not ideal for analysisof natural language, as it has to account for pathological cases of distributions thatgenerate data.We develop a framework for deriving sample complexity bounds using the max-imum likelihood principle for probabilistic grammars in a distribution-dependentsetting.
Distribution dependency is introduced here by making empirically justifiedassumptions about the distributions that generate the data.
Our framework uses andsignificantly extends ideas that have been introduced for deriving sample complexitybounds for probabilistic graphical models (Dasgupta 1997).
Maximum likelihood esti-mation is put in the empirical risk minimization framework (Vapnik 1998) with the lossfunction being the log-loss.
Following that, we develop a set of learning theoretic toolsto explore rates of estimation convergence for probabilistic grammars.
We also developalgorithms for performing empirical risk minimization.Much research has been devoted to the problem of learning finite state automata(which can be thought of as a class of grammars) in the Probably Approximately Correctsetting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989;Pitt 1989; Terwijn 2002).
Typically, the setting in these cases is different from our setting:Error is measured as the probability mass of strings that are not identified correctly bythe learned finite state automaton, instead of measuring KL divergence between theautomaton and the true distribution.
In addition, in many cases, there is also a focus onthe distribution-free setting.
To the best of our knowledge, it is still an open problemwhether finite state automata are learnable in the distribution-dependent setting whenmeasuring the error as the fraction of misidentified strings.
Other work (Ron 1995; Ron,Singer, and Tishby 1998; Clark and Thollard 2004; Palmer and Goldberg 2007) also givestreatment to probabilistic automata with an error measure which is more suitable forthe probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance.480Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsThese also focus on learning the structure of finite state machines.
As mentioned earlier,in our setting we assume that the grammar is fixed, and that our goal is to estimate itsparameters.We note an important connection to an earlier study about the learnability ofprobabilistic automata and hidden Markov models by Abe and Warmuth (1992).
Inthat study, the authors provided positive results for the sample complexity for learningprobabilistic automata?they showed that a polynomial sample is sufficient for MLE.We demonstrate positive results for the more general class of probabilistic grammarswhich goes beyond probabilistic automata.
Abe and Warmuth also showed that theproblem of finding or even approximating the maximum likelihood solution for a two-state probabilistic automaton with an alphabet of an arbitrary size is hard.
Even thoughthese results extend to probabilistic grammars to some extent, we provide a novel proofthat illustrates the NP-hardness of identifying the maximum likelihood solution forprobabilistic grammars in the specific framework of ?proper approximations?
that wedefine in this article.
Whereas Abe and Warmuth show that the problem of maximumlikelihood maximization for two-state HMMs is not approximable within a certainfactor in time polynomial in the alphabet and the length of the observed sequence, weshow that there is no polynomial algorithm (in the length of the observed strings) thatidentifies the maximum likelihood estimator in our framework.
In our reduction, from3-SAT to the problem of maximum likelihood estimation, the alphabet used is binaryand the grammar size is proportional to the length of the formula.
In Abe andWarmuth,the alphabet size varies, and the number of states is two.This article proceeds as follows.
In Section 2 we review the background necessaryfrom Vapnik?s (1988) empirical risk minimization framework.
This framework is re-duced to maximum likelihood estimation when a specific loss function is used: the log-loss.1 There are some shortcomings in using the empirical risk minimization frameworkin its simplest form.
In its simplest form, the ERM framework is distribution-free, whichmeans that we make no assumptions about the distribution that generated the data.Naively attempting to apply the ERM framework to probabilistic grammars in thedistribution-free setting does not lead to the desired sample complexity bounds.
Thereason for this is that the log-loss diverges whenever small probabilities are allocated inthe learned hypothesis to structures or strings that have a rather large probability in theprobability distribution that generates the data.
With a distribution-free assumption,therefore, we would have to give treatment to distributions that are unlikely to betrue for natural language data (e.g., where some extremely long sentences are veryprobable).To correct for this, we move to an analysis in a distribution-dependent setting, bypresenting a set of assumptions about the distribution that generates the data.
In Sec-tion 3 we discuss probabilistic grammars in a general way and introduce assumptionsabout the true distribution that are reasonable when our data come from natural lan-guage examples.
It is important to note that this distribution need not be a probabilisticgrammar.The next stepwe take, in Section 4, is approximating the set of probabilistic grammarsover which we maximize likelihood.
This is again required in order to overcome thedivergence of the log-loss for probabilities that are very small.
Our approximations are1 It is important to remember that minimizing the log-loss does not equate to minimizing the error of alinguistic analyzer or natural language processing application.
In this article we focus on the log-losscase because we believe that probabilistic models of language phenomena have inherent usefulnessas explanatory tools in computational linguistics, aside from their use in systems.481Computational Linguistics Volume 38, Number 3based on bounded approximations that have been used for deriving sample complexitybounds for graphical models in a distribution-free setting (Dasgupta 1997).Our approximations have two important properties: They are, by themselves, prob-abilistic grammars from the family we are interested in estimating, and they become atighter approximation around the family of probabilistic grammars we are interested inestimating as more samples are available.Moving to the distribution-dependent setting and defining proper approximationsenables us to derive sample complexity bounds.
In Section 5 we present the samplecomplexity results for both the supervised and unsupervised cases.
A question thatlingers at this point is whether it is computationally feasible to maximize likelihoodin our framework even when given enough samples.In Section 6, we describe algorithms we use to estimate probabilistic grammarsin our framework, when given access to the required number of samples.
We showthat in the supervised case, we can indeed maximize likelihood in our approximationframework using a simple algorithm.
For the unsupervised case, however, we show thatmaximizing likelihood is NP-hard.
This fact is related to a notion known in the learningtheory literature as inherent unpredictability (Kearns and Vazirani 1994): Accuratelearning is computationally hard evenwith enough samples.
To overcome this difficulty,we adapt the expectation-maximization algorithm (Dempster, Laird, and Rubin 1977)to approximately maximize likelihood (or minimize log-loss) in the unsupervised casewith proper approximations.In Section 7 we discuss some related ideas.
These include the failure of an alter-native kind of distributional assumption and connections to regularization by maxi-mum a posteriori estimation with Dirichlet priors.
Longer proofs are included in theappendices.
A table of notation that is used throughout is included as Table D.1 inAppendix D.This article builds on two earlier papers.
In Cohen and Smith (2010b) we presentedthe main sample complexity results described here; the present article includes signifi-cant extensions, a deeper analysis of our distributional assumptions, and a discussion ofvariants of these assumptions, as well as related work, such as that about the Tsybakovnoise condition.
In Cohen and Smith (2010c) we proved NP-hardness for unsupervisedparameter estimation of probalistic context-free grammars (PCFGs) (without approxi-mate families).
The present article uses a similar type of proof to achieve results adaptedto empirical risk minimization in our approximation framework.2.
Empirical Risk Minimization and Maximum Likelihood EstimationWe begin by introducing some notation.
We seek to construct a predictive model thatmaps inputs from space X to outputs from space Z.
In this work, X is a set of stringsusing some alphabet ?
(X ?
??
), and Z is a set of derivations allowed by a grammar(e.g., a context-free grammar).
We assume the existence of an unknown joint probabilitydistribution p(x, z) over X?
Z.
(For the most part, we will be discussing discrete inputand output spaces.
This means that p will denote a probability mass function.)
We areinterested in estimating the distribution p from examples, either in a supervised setting,where we are provided with examples of the form (x, z) ?
X?
Z, or in the unsupervisedsetting, where we are provided only with examples of the form x ?
X.
We first considerthe supervised setting and return to the unsupervised setting in Section 5.
We will useq to denote the estimated distribution.482Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsIn order to estimate p as accurately as possible using q(x, z), we are interested inminimizing the log-loss, that is, in finding qopt, from a fixed family of distributions Q(also called ?the concept space?
), such thatqopt = argminq?QEp[?
log q]= argminq?Q?
?x,zp(x, z) log q(x, z) (1)Note that if p ?
Q, then this quantity achieves theminimumwhen qopt = p, in which casethe value of the log-loss is the entropy of p. Indeed, more generally, this optimization isequivalent to finding q such that it minimizes the KL divergence from p to q.Because p is unknown, we cannot hope to minimize the log-loss directly.
Given aset of examples (x1, z1), .
.
.
, (xn, zn), however, there is a natural candidate, the empiricaldistribution p?n, for use in Equation (1) instead of p, defined as:p?n(x, z) = n?1n?i=1I {(x, z) = (xi, zi)}where I {(x, z) = (xi, zi)} is 1 if (x, z) = (xi, zi) and 0 otherwise.2 We then set up theproblem as the problem of empirical risk minimization (ERM), that is, trying to find qsuch thatq?
= argminq?QEp?n[?
log q](2)= argminq?Q?n?1n?i=1log q(xi, zi)= argmaxq?Qn?1n?i=1log q(xi, zi) (3)Equation (3) immediately shows that minimizing empirical risk using the log-loss isequivalent to the maximizing likelihood, which is a common statistical principle usedfor estimating a probabilistic grammar in computational linguistics (Charniak 1993;Manning and Schu?tze 1999).3As mentioned earlier, our goal is to estimate the probability distribution p whilequantifying how accurate our estimate is.
One way to quantify the estimation accuracyis by bounding the excess risk, which is defined asEp(q;Q) = Ep(q)  Ep[?
log q]?minq??QEp[?
log q?
](4)We are interested in bounding the excess risk for q?, Ep(q?).
The excess risk isreduced to KL divergence between p and q if p ?
Q, because in this case the quantityminq?
?Q E[?
log q?
]is minimized with q?
= p, and equals the entropy of p. In a typical2 We note that p?n itself is a random variable, because it depends on the sample drawn from p.3 We note that being able to attain the minimum through an hypothesis q?
is not necessarily possible inthe general case.
In our instantiations of ERM for probabilistic grammars, however, the minimum can beattained.
In fact, in the unsupervised case the minimum can be attained by more than a single hypothesis.In these cases, q?
is arbitrarily chosen to be one of these minimizers.483Computational Linguistics Volume 38, Number 3case, where we do not necessarily have p ?
Q, then the excess risk of q is bounded fromabove by the KL divergence between p and q.We can bound the excess risk by showing the double-sided convergence of theempirical process Rn(Q), defined as follows:Rn(Q)  supq?Q??Ep?n[?
log q]?
Ep[?
log q]???
0 (5)as n?
?.
For any  > 0, if, for large enough n it holds thatsupq?Q??Ep?n[?
log q]?
Ep[?
log q]??
<  (6)(with high probability), then we can ?sandwich?
the following quantities:Ep[?
log qopt]?
Ep[?
log q?](7)?
Ep?n[?
log q?
]+ ?
Ep?n[?
log qopt]+ ?
Ep[?
log qopt]+ 2 (8)where the inequalities come from the fact that qopt minimizes the expected riskEp[?
log q]for q ?
Q, and q?
minimizes the empirical risk Ep?n[?
log q]for q ?
Q. Theconsequence of Equations (7) and (8) is that the expected risk of q?
is at most 2 awayfrom the expected risk of qopt, and as a result, we find the excess risk Ep(q?
), for largeenough n, is smaller than 2.
Intuitively, this means that, under a large sample, q?
doesnot give much worse results than qopt under the criterion of the log-loss.Unfortunately, the regularity conditions which are required for the convergence ofRn(Q) do not hold because the log-loss can be unbounded.
This means that a modifi-cation is required for the empirical process in a way that will actually guarantee somekind of convergence.
We give a treatment to this in the next section.We note that all discussion of convergence in this section has been about conver-gence in probability.
For example, we want Equation (6) to hold with high probability?for most samples of size n. We will make this notion more rigorous in Section 2.2.2.1 Empirical Risk Minimization and Structural Risk Minimization MethodsIt has been noted in the literature (Vapnik 1998; Koltchinskii 2006) that often the class Qis too complex for empirical risk minimization using a fixed number of data points.It is therefore desirable in these cases to create a family of subclasses {Q?
| ?
?
A}that have increasing complexity.
The more data we have, the more complex our Q?can be for empirical risk minimization.
Structural risk minimization (Vapnik 1998) andthe method of sieves (Grenander 1981) are examples of methods that adopt such anapproach.
Structural risk minimization, for example, can be represented in many casesas a penalization of the empirical risk method, using a regularization term.In our case, the level of ?complexity?
is related to allocation of small probabilities toderivations in the grammar by a distribution q ?
Q.
The basic problem is this: Wheneverwe have a derivation with a small probability, the log-loss becomes very large (inabsolute value), and this makes it hard to show the convergence of the empirical process484Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsRn(Q).
Because grammars can define probability distributions over infinitely manydiscrete outcomes, probabilities can be arbitrarily small and log-loss can be arbitrarilylarge.To solve this issuewith the complexity ofQ, we define in Section 4 a series of approx-imations {Qn | n ?
N} for probabilistic grammars such that?n Qn = Q.
Our frameworkfor empirical riskminimization is then set up tominimize the empirical risk with respectto Qn, where n is the number of samples we draw for the learner:q?n = argminq?QnEp?n[?
log q](9)We are then interested in the convergence of the empirical processRn(Qn) = supq?Qn??Ep?n[?
log q]?
Ep[?
log q]??
(10)In Section 4 we show that the minimizer q?n is an asymptotic empirical risk minimizer(in our specific framework), which means that Ep[?
log q?n]?
Ep[?
log q?].
Becausewe have?n Qn = Q, the implication of having asymptotic empirical risk minimizationis that we have Ep(q?n ;Qn) ?
Ep(q?
;Q).2.2 Sample Complexity BoundsKnowing that we are interested in the convergence of Rn(Qn) = supq?Qn |Ep?n[?
log q]?Ep[?
log q]|, a natural question to ask is: ?At what rate does this empirical processconverge?
?Because the quantity Rn(Qn) is a random variable, we need to give a probabilistictreatment to its convergence.
More specifically, we ask the question that is typicallyasked when learnability is considered (Vapnik 1998): ?How many samples n are re-quired so that with probability 1?
?
we have Rn(Qn) < ??
Bounds on this numberof samples are also called ?sample complexity bounds,?
and in a distribution-freesetting they are described as a function N(, ?,Q), independent of the distribution p thatgenerates the data.A complete distribution-free setting is not appropriate for analyzing natural lan-guage.
This setting poses technical difficulties with the convergence ofRn(Qn) and needsto take into account pathological cases that can be ruled out in natural language data.Instead, we will make assumptions about p, parametrize these assumptions in severalways, and then calculate sample complexity bounds of the form N(, ?,Q, p), where thedependence on the distribution is expressed as dependence on the parameters in theassumptions about p.The learning setting, then, can be described as follows.
The user decides on a levelof accuracy () which the learning algorithm has to reach with confidence (1?
?).
Then,N(, ?,Q, p) samples are drawn from p and presented to the learning algorithm.
Thelearning algorithm then returns an hypothesis according to Equation (9).3.
Probabilistic GrammarsWe begin this section by discussing the family of probabilistic grammars.
A probabilisticgrammar defines a probability distribution over a certain kind of structured object (aderivation of the underlying symbolic grammar) explained step-by-step as a stochastic485Computational Linguistics Volume 38, Number 3process.
Hidden Markov models (HMMs), for example, can be understood as a randomwalk through a probabilistic finite-state network, with an output symbol sampled ateach state.
PCFGs generate phrase-structure trees by recursively rewriting nonterminalsymbols as sequences of ?child?
symbols (each itself either a nonterminal symbol or aterminal symbol analogous to the emissions of an HMM).Each step or emission of an HMM and each rewriting operation of a PCFG isconditionally independent of the others given a single structural element (one HMMor PCFG state); this Markov property permits efficient inference over derivations givena string.In general, a probabilistic grammar ?G,??
defines the joint probability of a string xand a grammatical derivation z:q(x, z | ?,G) =K?k=1Nk?i=1?
?k,i(x,z)k,i= expK?k=1Nk?i=1?k,i(x, z) log?k,i (11)where ?k,i is a function that ?counts?
the number of times the kth distribution?sith event occurs in the derivation.
The parameters ?
are a collection of K multi-nomials ?
?1, .
.
.
,?K?, the kth of which includes Nk competing events.
If we let ?k =?
?k,1, .
.
.
,?k,Nk?, each ?k,i is a probability, such that?k,?i, ?k,i ?
0?k,Nk?i=1?k,i = 1We denote by ?G this parameter space for ?.
The grammar G dictates the supportof q in Equation (11).
As is often the case in probabilistic modeling, there are differ-ent ways to carve up the random variables.
We can think of x and z as correlatedstructure variables (often x is known if z is known), or the derivation event counts?
(x, z) = ?
?k,i(x, z)?1?k?K,1?i?Nk as an integer-vector random variable.
In this article,we assume that x is always a deterministic function of z, so we use the distributionp(z) interchangeably with p(x, z).Note that there may be many derivations z for a given string x?perhaps eveninfinitely many in some kinds of grammars.
For HMMs, there are three kinds of multi-nomials: a starting state multinomial, a transitionmultinomial per state and an emissionmultinomial per state.
In that case K = 2s+ 1, where s is the number of states.
The valueof Nk depends on whether the kth multinomial is the starting state multinomial (inwhich case Nk = s), transition multinomial (Nk = s), or emission multinomial (Nk = t,with t being the number of symbols in the HMM).
For PCFGs, each multinomialamong the K multinomials corresponds to a set of Nk context-free rules headed bythe same nonterminal.
The parameter ?k,i is then the probability of the ith rule for thekth nonterminal.We assume that G denotes a fixed grammar, such as a context-free or regular gram-mar.
We let N =?Kk=1Nk denote the total number of derivation event types.
We useD(G) to denote the set of all possible derivations of G. We define Dx(G) = {z ?
D(G) |yield(z) = x}.
We use deg(G) to denote the ?degree?
of G, i.e., deg(G) = maxk Nk.
Welet |x| denote the length of the string x, and |z| =?Kk=1?Nki=1 ?k,i(z) denote the ?length?
(number of event tokens) of the derivation z.486Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsGoing back to the notation in Section 2, Q would be a collection of probabilisticgrammars, parametrized by ?, and q would be a specific probabilistic grammar witha specific ?.
We therefore treat the problem of ERM with probabilistic grammars as theproblem of parameter estimation?identifying ?
from complete data or incomplete data(strings x are visible but the derivations z are not).
We can also view parameter esti-mation as the identification of a hypothesis from the concept space Q = H(G) = {h?
(z) |?
?
?G} (where h?
is a distribution of the form of Equation [11]) or, equivalently, fromnegated log-concept space F(G) = {?
log h?
(z) | ?
?
?G}.
For simplicity of notation, weassume that there is a fixed grammar G and use H to refer to H(G) and F to referto F(G).3.1 Distributional Assumptions about LanguageIn this section, we describe a parametrization of assumptions we make about the dis-tribution p(x, z), the distribution that generates derivations from D(G) (note that p doesnot have to be a probabilistic grammar).
We first describe empirical evidence about thedecay of the frequency of long strings x.Figure 1 shows the frequency of sentence length for treebanks in various lan-guages.4 The trend in the plots clearly shows that in the extended tail of the curve, alllanguages have an exponential decay of probabilities as a function of sentence length.
Totest this, we performed a simple regression of frequencies using an exponential curve.We estimated each curve for each language using a curve of the form f (l; c,?)
= cl?.This estimation was done by minimizing squared error between the frequency ver-sus sentence length curve and the approximate version of this curve.
The data pointsused for the approximation are (li, pi), where li denotes sentence length and pi denotesfrequency, selected from the extended tail of the distribution.
Extended tail here refersto all points with length longer than l1, where l1 is the length with the highest frequencyin the treebank.
The goal of focusing on the tail is to avoid approximating the headof the curve, which is actually a monotonically increasing function.
We plotted theapproximate curve together with a length versus frequency curve for new syntacticdata.
It can be seen (Figure 1) that the approximation is rather accurate in these corpora.As a consequence of this observation, we make a few assumptions about G andp(x, z): Derivation length proportional to sentence length: There is an ?
?
1 suchthat, for all z, |z| ?
?|yield(z)|.
Further, |z| ?
|x|.
(This prohibits unarycycles.
) Exponential decay of derivations: There is a constant r < 1 and a constantL ?
0 such that p(z) ?
Lr|z|.
Note that the assumption here is about thefrequency of length of separate derivations, and not the aggregatedfrequency of all sentences of a certain length (cf.
the discussion abovereferring to Figure 1).4 Treebanks offer samples of cleanly segmented sentences.
It is important to note that the distributionsestimated may not generalize well to samples from other domains in these languages.
Our argumentis that the family of the estimated curve is reasonable, not that we can correctly estimate the curve?sparameters.487Computational Linguistics Volume 38, Number 3Figure 1A plot of the tail of frequency vs. sentence length in treebanks for English, German, Bulgarian,Turkish, Spanish, and Chinese.
Red lines denote data from the treebank, blue lines denote anapproximation which uses an exponential function of the form f (l; c,?)
= cl?
(the blue line usesdata which is different from the data used to estimate the curve parameters, c and ?).
Theparameters (c,?)
are (0.19, 0.92) for English, (0.06, 0.94) for German, (0.26, 0.89) for Bulgarian,(0.26, 0.83) for Turkish, (0.11, 0.93) for Spanish, and (0.03, 0.97) for Chinese.
Squared errors are0.0005, 0.0003, 0.0007, 0.0003, 0.001, and 0.002 for English, German, Bulgarian, Turkish, Spanish,and Chinese, respectively. Exponential decay of strings: Let ?
(k) = |{z ?
D(G) | |z| = k}| be thenumber derivations of length k in G. We assume that ?
(k) is an increasingfunction, and complete it such that it is defined over positive numbers bytaking ?
(t)  ?(t).
Taking r as before, we assume there exists a constantq < 1, such that ?2(k)rk ?
qk (and as a consequence, ?
(k)rk ?
qk).
Thisimplies that the number of derivations of length kmay be exponentiallylarge (e.g., as with many PCFGs), but is bounded by (q/r)k.488Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars Bounded expectations of rules: There is a B < ?
such that Ep[?k,i(z)]?
Bfor all k and i.These assumptions must hold for any pwhose support consists of a finite set.
Theseassumptions also hold in many cases when p itself is a probabilistic grammar.
Also, wenote that the last requirement of bounded expectations is optional, and it can be inferredfrom the rest of the requirements: B = L/(1?
q)2.
We make this requirement explicit forsimplicity of notation later.
We denote the family of distributions that satisfy all of theserequirements by P(?,L, r, q,B,G).There are other cases in the literature of language learning where additional as-sumptions are made on the learned family of models in order to obtain positive learn-ability results.
For example, Clark and Thollard (2004) put a bound on the expectedlength of strings generated from any state of probabilistic finite state automata, whichresembles the exponential decay of strings we have for p in this article.An immediate consequence of these assumptions is that the entropy of p is finiteand bounded by a quantity that depends on L, r and q.5 Bounding entropy of labels(derivations) given inputs (sentences) is a common way to quantify the noise in adistribution.
Here, both the sentential entropy (Hs(p) = ?
?x p(x) log p(x)) is boundedas well as the derivational entropy (Hd(p) = ?
?x,z p(x, z) log p(x, z)).
This is stated in thefollowing result.Proposition 1Let p ?
P(?,L, r, q,B,G) be a distribution.
Then, we haveHs(p) ?
Hd(p) ?
?
log L+L log r(1?
q)2log 1r +(1+ log L)/ log 1r e ?
(?1+ log Llog 1r?
)ProofFirst note that Hs(p) ?
Hd(p) holds by the data processing inequality (Cover andThomas 1991) because the sentential probability distribution p(x) is a coarser versionof the derivational probability distribution p(x, z).
Now, consider p(x, z).
For simplicityof notation, we use p(z) instead of p(x, z).
The yield of z, x, is a function of z, and thereforecan be omitted from the distribution.
It holds thatHd(p) = ?
?zp(z) log p(z)= ?
?z?Z1p(z) log p(z)?
?z?Z2p(z) log p(z)= Hd(p,Z1)+Hd(p,Z2)where Z1 = {z | p(z) > 1/e} and Z2 = {z | p(z) ?
1/e}.
Note that the function ??
log?reaches its maximum for ?
= 1/e.
We therefore haveHd(p,Z1) ?|Z1|e5 For simplicity and consistency with the log-loss, we measure entropy in nats, which means we use thenatural logarithm when computing entropy.489Computational Linguistics Volume 38, Number 3We give a bound on |Z1|, the number of ?high probability?
derivations.
Because we havep(x, z) ?
Lr|z|, we can find the maximum length of a derivation that has a probability ofmore than 1/e (and hence, it may appear in Z1) by solving 1/e ?
Lr|z| for |z|, which leadsto |z| ?
log(1/eL)/ log r. Therefore, there are at most?(1+logL)/ log 1r k=1?
(k) derivations in|Z1| and therefore we have|Z1| ??
(1+ log L)/ log 1r??(?
(1+ logL)/ log 1r?
)Hd(p,Z1) ??
(1+ log L)/ log 1r?e ?(?
(1+ logL)/ log 1r?
)(12)where we use the monotonicity of ?.
Consider Hd(p,Z2) (the ?low probability?
deriva-tions).
We have:Hd(p,Z2) ?
?
?z?Z2Lr|z| log(Lr|z|)?
?
log L?
(L log r)?z?Z2|z|r|z|?
?
log L?
(L log r)??k=1?(k)krk?
?
log L?
(L log r)?
?k=1kqk (13)= ?
log L+L log r(1?
q)2log 1q (14)where Equation (13) holds from the assumptions about p. Putting Equation (12) andEquation (14) together, we obtain the result.
We note that another common way to quantify the noise in a distribution is throughthe notion of Tsybakov noise (Tsybakov 2004; Koltchinskii 2006).
We discuss this furtherin Section 7.1, where we show that Tsybakov noise is too permissive, and probabilisticgrammars do not satisfy its conditions.3.2 Limiting the Degree of the GrammarWhen approximating a family of probabilistic grammars, it is much more convenientwhen the degree of the grammar is limited.
In this article, we limit the degree of thegrammar by making the assumption that all Nk ?
2.
This assumption may seem, at firstglance, somewhat restrictive, but we show next that for PCFGs (and as a consequence,other formalisms), this assumption does not limit the total generative capacity that wecan have across all context-free grammars.We first show that any context-free grammar with arbitrary degree can be mappedto a corresponding grammar with all Nk ?
2 that generates derivations equivalent toderivations in the original grammar.
Such a grammar is also called a ?covering gram-mar?
(Nijholt 1980; Leermakers 1989).
Let G be a CFG.
Let A be the kth nonterminal.Consider the rules A?
?i for i ?
Nk where A appears on the left side.
For each rule490Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsFigure 2Example of a context-free grammar and its equivalent binarized form.A?
?i, i < Nk, we create a new nonterminal in G?
such that Ai has two rewrite rules:Ai ?
?i and Ai ?
Ai+1.
In addition, we create rules A?
A1 and ANk ?
?Nk .
Figure 2demonstrates an example of this transformation on a small context-free grammar.It is easy to verify that the resulting grammar G?
has an equivalent capacity tothe original CFG, G. A simple transformation that converts each derivation in thenew grammar to a derivation in the old grammar would involve collapsing any pathof nonterminals added to G?
(i.e., all Ai for nonterminal A) so that we end up withnonterminals from the original grammar only.
Similarly, any derivation in G can beconverted to a derivation in G?
by adding new nonterminals through unary applicationof rules of the form Ai ?
Ai+1.
Given a derivation z in G, we denote by ?G?G?
(z) thecorresponding derivation in G?
after adding the new non-terminals Ai to z. Throughoutthis article, we will refer to the normalized form of G?
as a ?binary normal form.
?6Note that K?, the number of multinomials in the binary normal form, is a func-tion of both the number of nonterminals in the original grammar and the number ofrules in that grammar.
More specifically, we have that K?
=?Kk=1Nk + K. To make theequivalence complete, we need to show that any probabilistic context-free grammar canbe translated to a PCFG with maxk Nk ?
2 such that the two PCFGs induce the sameequivalent distributions over derivations.Utility Lemma 1Let ai ?
[0, 1], i ?
{1, .
.
.
,N} such that?i ai = 1.
Define b1 = a1, c1 = 1?
a1, bi =(aiai?1)(bi?1ci?1), and ci = 1?
bi for i ?
2.
Then ai =??i?1?j=1cj??
bi.See Appendix A for the proof of Utility Lemma 1.Theorem 1Let ?G,??
be a probabilistic context-free grammar.
Let G?
be the binarizing transforma-tion of G as defined earlier.
Then, there exists ??
for G?
such that for any z ?
D(G) wehave p(z | ?,G) = p(?G?G?
(z) | ??,G?
).6 We note that this notion of binarization is different from previous types of binarization appearing incomputational linguistics for grammars.
Typically in previous work about binarized grammars such asCFGs, the grammars are constrained to have at most two nonterminals in the right side in Chomskynormal form.
Another form of binarization for linear context-free rewriting systems is restriction of thefan-out of the rules to two (Go?mez-Rodr?
?guez and Satta 2009; Gildea 2010).
We, however, limit thenumber of rules for each nonterminal (or more generally, the number of elements in each multinomial).491Computational Linguistics Volume 38, Number 3ProofFor the grammar G, index the set {1, ...,K} with nonterminals ranging from A1 to AK.DefineG?
as before.We need to define ??.
Index themultinomials inG?
by (k, i), each hav-ing two events.
Let ?
(k,i),1 = ?k,i, ?
(k,i),2 = 1?
?k,i for i = 1 and set ?k,i,1 = ?k,i/?
(k,i?1),2,and ?
(k,i?1),2 = 1?
?(k,i?1),2.?G?,??
is a weighted context-free grammar such that the ?
(k,i),1 corresponds to theith event in the k multinomial of the original grammar.
Let z be a derivation in G andz?
= ?G?G?
(z).
Then, from Utility Lemma 1 and the construction of g?, we have that:p(z | ?,G) =K?k=1Nk?i=1??k,i(z)k,i=K?k=1Nk?i=1?k,i(z)?l=1?k,i=K?k=1Nk?i=1?k,i(z)?l=1??i?1?j=1?(k,j),2???(k,i),1=K?k=1Nk?i=1??i?1?j=1??k,i(z)(k,j),2????k,i(z)(k,i),1=K?k=1Nk?j=12?i=1??k,j(z?
)(k,j),i= p(z?
| ?,G?
)From Chi (1999), we know that the weighted grammar ?G?,??
can be converted toa probabilistic context-free grammar ?G?,??
?, through a construction of ??
based on ?,such that p(z?
| ?,G?)
= p(z?
| ??,G?).
The proof for Theorem 1 gives a construction the parameters ??
ofG?
such that ?G,?
?is equivalent to ?G?,???.
The construction of ??
can also be reversed: Given ??
for G?, wecan construct ?
for G so that again we have equivalence between ?G,??
and ?G?,??
?.In this section, we focused on presenting parametrized, empirically justified distri-butional assumptions about language data that will make the analysis in later sectionsmoremanageable.We showed that these assumptions bound the amount of entropy as afunction of the assumption parameters.
We also made an assumption about the structureof the grammar family, and showed that it entails no loss of generality for CFGs.
Manyother formalisms can follow similar arguments to show that the structural assumptionis justified for them as well.4.
Proper ApproximationsIn order to follow the empirical risk minimization described in Section 2.1, we haveto define a series of approximations for F, which we denote by the log-concept spacesF1,F2, .
.
.
.
We also have to replace two-sided uniform convergence (Equation [6]) withconvergence on the sequence of concept spaces we defined (Equation [10]).
The conceptspaces in the sequence vary as a function of the number of samples we have.
We next492Cohen and Smith Empirical Risk Minimization for Probabilistic Grammarsconstruct the sequence of concept spaces, and in Section 5 we return to the learningmodel.
Our approximations are based on the concept of bounded approximations (Abe,Takeuchi, and Warmuth 1991; Dasgupta 1997), which were originally designed forgraphical models.7 A bounded approximation is a subset of a concept space which iscontrolled by a parameter that determines its tightness.
Here we use this idea to definea series of subsets of the original concept space F as approximations, while having twoasymptotic properties that control the series?
tightness.Let Fm (for m ?
{1, 2, .
.
.})
be a sequence of concept spaces.
We consider threeproperties of elements of this sequence, which should hold for m > M for a fixedM.The first is containment in F:Fm ?
FThe second property is boundedness:?Km ?
0,?f ?
Fm, E[| f | ?
I {| f | ?
Km}]?
bound(m)where bound is a non-increasing function such that bound(m) ??m??0.
This states thatthe expected values of functions from Fm on values larger than some Km is small.This is required to obtain uniform convergence results in the revised empirical riskminimization model from Section 2.1.
Note that Km can grow arbitrarily large.The third property is tightness:?Cm ?
F ?
Fm, p??
?f?F{z | Cm( f )(z)?
f (z) ?
tail(m)}??
?
tail(m)where tail is a non-increasing function such that tail(m) ??m?
?0, and Cm denotes anoperator that maps functions in F to Fm.
This ensures that our approximation actuallyconverges to the original concept space F. We will show in Section 4.3 that this isactually a well-motivated characterization of convergence for probabilistic grammarsin the supervised setting.We say that the sequence Fm properly approximates F if there exist tail(m), bound(m),and Cm such that, for all m larger than some M, containment, boundedness, and tight-ness all hold.In a good approximation, Km would increase at a fast rate as a function of m andtail(m) and bound(m) decrease quickly as a function ofm.
As wewill see in Section 5, wecannot have an arbitrarily fast convergence rate (by, for example, taking a subsequenceof Fm), because the size of Km has a great effect on the number of samples required toobtain accurate estimation.7 There are other ways to manage the unboundedness of KL divergence in the language learning literature.Clark and Thollard (2004), for example, decompose the KL divergence between probabilistic finite-stateautomata into several terms according to a decomposition of Carrasco (1997) and then bound each termseparately.493Computational Linguistics Volume 38, Number 3Table 1Example of a PCFG where there is more than a single way to approximate it by truncation with?
= 0.1, because it has more than two rules.
Any value of ?
?
[0,?]
will lead to a differentapproximation.Rule ?
General ?
= 0 ?
= 0.01 ?
= 0.005S?
NP VP 0.09 0.01 0.1 0.1 0.1S?
NP 0.11 0.11?
?
0.11 0.1 0.105S?
VP 0.8 0.8?
?+ ?
0.79 0.8 0.7954.1 Constructing Proper Approximations for Probabilistic GrammarsWenow focus on constructing proper approximations for probabilistic grammarswhosedegree is limited to 2.
Proper approximations could, in principle, be used with lossesother than the log-loss, though their main use is for unbounded losses.
Starting fromthis point in the article, we focus on using such proper approximations with thelog-loss.We construct Fm.
For each f ?
F we define a transformation T( f,?)
that shifts everybinomial parameter ?k = ??k,1,?k,2?
in the probabilistic grammar by at most ?:??k,1,?k,2?
?????
?, 1?
??
if ?k,1 < ??1?
?, ??
if ?k,1 > 1?
??
?k,1, ?k,2?
otherwiseNote that T( f,?)
?
F for any ?
?
1/2.
Fix a constant s > 1.8 We denote by T(?,?)
thesame transformation on ?
(which outputs the new shifted parameters) and we denoteby ?G(?)
= ?(?)
the set {T(?,?)
| ?
?
?G}.
For each m ?
N, define Fm = {T( f,m?s) |f ?
F}.When considering our approach to approximate a probabilistic grammar by in-creasing its parameter probabilities to be over a certain threshold, it becomes clearwhy we are required to limit the grammar to have only two rules and why we arerequired to use the normal from Section 3.2 with grammars of degree 2.
Consider thePCFG rules in Table 1.
There are different ways to move probability mass to the rulewith small probability.
This leads to a problem with identifability of the approximation:How does one decide how to reallocate probability to the small probability rules?
Bybinarizing the grammar in advance, we arrive at a single way to reallocate mass whenrequired (i.e., move mass from the high-probability rule to the low-probability rule).This leads to a simpler proof for sample complexity bounds and a single bound (ratherthan different bounds depending on different smoothing operators).
We note, however,that the choices made in binarizing the grammar imply a particular way of smoothingthe probability across the original rules.We now describe how this construction of approximations satisfies the proper-ties mentioned in Section 4, specifically, the boundedness property and the tightnessproperty.8 By varying swe get a family of approximations.
The larger s is, the tighter the approximation is.
Also,the larger s is, as we see later, the looser our sample complexity bound will be.494Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsProposition 2Let p ?
P(?,L, r, q,B,G) and let Fm be as defined earlier.
There exists a constant ?
=?
(L, q, p,N) > 0 such that Fm has the boundedness property with Km = sN log3m andbound(m) = m??
logm.See Appendix A for the proof of Proposition 2.Next, Fm is tight with respect to F with tail(m) =N log2mms ?
1 .Proposition 3Let p ?
P(?,L, r, q,B,G) and let Fm as defined earlier.
There exists anM such that for anym > Mwe havep??
?f?F{z | Cm( f )(z)?
f (z) ?
tail(m)}??
?
tail(m)for tail(m) =N log2mms ?
1 and Cm( f ) = T( f,m?s).See Appendix A for the proof of Proposition 3.We now have proper approximations for probabilistic grammars.
These approx-imations are defined as a series of probabilistic grammars, related to the family ofprobabilistic grammars we are interested in estimating.
They consist of three prop-erties: containment (they are a subset of the family of probabilistic grammars weare interested in estimating), boundedness (their log-loss does not diverge to infinityquickly), and they are tight (there is a small probability mass at which they are not tightapproximations).4.2 Coupling Bounded Approximations with Number of SamplesAt this point, the number of samples n is decoupled from the bounded approximation(Fm) that we choose for grammar estimation.
To couple between these two, we needto define m as a function of the number of samples, m(n).
As mentioned earlier, thereis a clear trade-off between choosing a fast rate for m(n) (such as m(n) = nk for somek > 1) and a slower rate (such as m(n) = logn).
The faster the rate is, the tighter thefamily of approximations that we use for n samples.
If the rate is too fast, however,then Km grows quickly as well.
In that case, because our sample complexity bounds areincreasing functions of such Km, the bounds will degrade.To balance the trade-off, we choose m(n) = n. As we see later, this gives samplecomplexity bounds which are asymptotically interesting for both the supervised andunsupervised case.495Computational Linguistics Volume 38, Number 34.3 Asymptotic Empirical Risk MinimizationIt would be compelling to determine whether the empirical risk minimizer over Fn isan asymptotic empirical risk minimizer.
This would mean that the risk of the empiricalrisk minimizer over Fn converges to the risk of the maximum likelihood estimate.
As aconclusion to this section about proper approximations, we motivate the three re-quirements that we posed on proper approximations by showing that this is indeedtrue.
We now unify n, the number of samples, and m, the index of the approxima-tion of the concept space F. Let f ?n be the minimizer of the empirical risk over F,( f ?n = argminf?F Ep?n[f]) and let gn be the minimizer of the empirical risk over Fn(gn = argminf?Fn Ep?n[f]).Let D = {z1, ..., zn} be a sample from p(z).
The operator (gn =) argminf?Fn Ep?n [ f ] isan asymptotic empirical risk minimizer if E[Ep?n[gn]?
Ep?n [ f?n ]]?
0 as n?
?
(Shalev-Shwartz et al 2009).
Then, we have the followingLemma 1Denote by Z,n the set?f?F{z | Cn( f )(z)?
f (z) ?
}.
Denote by A,n the event ?one ofzi ?
D is in Z,n.?
If Fn properly approximates F, then:E[Ep?n[gn]?
Ep?n[f ?n]](15)???
?E[Ep?n[Cn( f?n )]| A,n]???p(A,n)+??
?E[Ep?n[f ?n]| A,n]??
?p(A,n)+ tail(n)where the expectations are taken with respect to the data set D.See Appendix A for the proof of Lemma 1.Proposition 4Let D = {z1, ..., zn} be a sample of derivations from G. Then gn = argminf?Fn Ep?n[f]isan asymptotic empirical risk minimizer.ProofLet f0 ?
F be the concept that puts uniform weights over ?, namely, ?k = ?
12 ,12 ?
for all k.Note that|E[Ep?n[f ?n]| A,n]|p(A,n)?
|E[Ep?n[f0]| A,n]|p(A,n) =log 2n?nl=1?k,i E[?k,i(zl) | A,n]p(A,n)496Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsLet Aj,,n for j ?
{1, .
.
.
,n} be the event ?zj ?
Z,n?.
Then A,n =?j Aj,,n.
We havethatE[?k,i(zl) | A,n]p(A,n) ??j?zlp(zl,Aj,,n)|zl|?
?j =l?zlp(zl)p(Aj,,n)|zl|+?zlp(zl,Al,,n)|zl| (16)???
?j =lp(Aj,,n)?
?B+ E[?k,i(z) | z ?
Z,n]p(z ?
Z,n)?
(n?
1)Bp(z ?
Z,n)+ E[?k,i(z) | z ?
Z,n]p(z ?
Z,n)where Equation (16) comes from zl being independent.
Also, B is the constant fromSection 3.1.
Therefore, we have:1nn?l=1?k,iE[?k,i(zl) | A,n]p(A,n)?
?k,i(E[?k,i(z) | z ?
Z,n]p(z ?
Z,n)+ (n?
1)Bp(z ?
Z,n))From the construction of our proper approximations (Proposition 3), we know that onlyderivations of length log2 n or greater can be in Z,n.
ThereforeE[?k,i | Z,n]p(Z,n) ?
?z:|z|>log2 np(z)?k,i(z) ??
?l>log2 nL?
(l)rll ?
?qlog2 n = o(1)where ?
> 0 is a constant.
Similarly, we have p(z ?
Z,n) = o(n?1).
This means that|E[Ep?n[?
log?f?n ] | A,n]|p(A,n) ??n??0.
In addition, it can be shown that |E[Ep?n[Cn( f?n ) |A,n]|p(A,n) ??n?
?0 using the same proof technique we used here, while relying on thefact that Cn( f?n ) ?
Fn, and therefore Cn( f?n )(z) ?
sN|z| log n. 5.
Sample Complexity BoundsEquipped with the framework of proper approximations as described previously, wenow give our main sample complexity results for probabilistic grammars.
These resultshinge on the convergence of supf?Fn |Ep?n[f]?
Ep[f]|.
Indeed, proper approximationsreplace the use of F in these convergence results.
The rate of this convergence can befast, if the covering numbers for Fn do not grow too fast.5.1 Covering Numbers and Bounds on Covering NumbersWe next give a brief overview of covering numbers.
A cover provides a way to reducea class of functions to a much smaller (finite, in fact) representative class such that eachfunction in the original class is represented using a function in the smaller class.
Let G497Computational Linguistics Volume 38, Number 3be a class of functions.
Let d(f, g) be a distance measure between two functions f, g fromG.
An -cover is a subset of G, denoted by G?, such that for every f ?
G there exists anf ?
?
G?
such that d( f, f ?)
< .
The covering number N(,G, d) is the size of the smallest-cover of G for the distance measure d.We are interested in a specific distancemeasure which is dependent on the empiricaldistribution p?n that describes the data z1, ..., zn.
Let f, g ?
G. We will usedp?n ( f, g) = Ep?n[| f ?
g|]=?z?D(G)| f (z)?
g(z)| p?n(z)= 1n?ni=1| f (zi)?
g(zi)|Instead of using N(,G, dp?n ) directly, we bound this quantity with N(,G) = supp?nN(,G, dp?n ), where we consider all possible samples (yielding p?n).
The following is thekey result regarding the connection between covering numbers and the double-sidedconvergence of the empirical process supf?Fn |Ep?n[f]?
Ep[f]| as n?
?.
This resultis a general-purpose result that has been used frequently to prove the convergence ofempirical processes of the type we discuss in this article.Lemma 2Let Fn be a permissible class9 of functions such that for every f ?
Fn we have E[| f | ?I {| f | ?
Kn}] ?
bound(n).
Let Ftruncated,n = {f ?
I {f ?
Kn} | f ?
Fm}, namely, the set offunctions from Fn after being truncated by Kn.
Then for  > 0 we havep(supf?Fn|Ep?n[f]?
Ep[f]| > 2)?
8N(/8,Ftruncated,n) exp(?
1128n2/K2n)+ bound(n)/provided n ?
K2n/42 and bound(n) < .See Pollard (1984; Chapter 2, pages 30?31) for the proof of Lemma 2.
See also Ap-pendix A.Covering numbers are rather complex combinatorial quantities which are hardto compute directly.
Fortunately, they can be bounded using the pseudo-dimension(Anthony and Bartlett 1999), a generalization of the Vapnik-Chervonenkis (VC)dimension for real functions.
In the case of our ?binomialized?
probabilistic grammars,the pseudo-dimension of Fn is bounded by N, because we have Fn ?
F, and thefunctions in F are linear with N parameters.
Hence, Ftruncated,n also has pseudo-dimension that is at most N. We then have the following.9 The ?permissible class?
requirement is a mild regularity condition regarding measurability that holds forproper approximations.
We refer the reader to Pollard (1984) for more details.498Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsLemma 3(From Pollard [1984] and Haussler [1992].)
Let Fn be the proper approximations forprobabilistic grammars, for any 0 <  < Kn we have:N(,Ftruncated,n) < 2(2eKn log2eKn)N5.2 Supervised CaseWe turn to give an analysis for the supervised case.
This analysis is mostly described as apreparation for the unsupervised case.
In general, the families of probabilistic grammarswe give a treatment to are parametric families, and the maximum likelihood estimatorfor these families is a consistent estimator in the supervised case.
In the unsupervisedcase, however, lack of identifiability prevents us from getting these traditional consis-tency results.
Also, the traditional results about the consistency of MLE are based on theassumption that the sample is generated from the parametric family we are trying toestimate.
This is not the case in our analysis, where the distribution that generates thedata does not have to be a probabilistic grammar.Lemmas 2 and 3 can be combined to get the following sample complexity result.Theorem 2LetG be a grammar.
Let p ?
P(?,L, r, q,B,G) (Section 3.1).
Let Fn be a proper approxima-tion for the corresponding family of probabilistic grammars.
Let z1, .
.
.
, zn be a sampleof derivations.
Then there exists a constant ?
(L, q, p,N) and constantM such that for any0 < ?
< 1 and 0 <  < Kn and any n > M and ifn ?
max{128K2n2(2N log(16eKn/)+ log32?
),log 4/?+ log 1/?
(L, q, p,N)}then we haveP(supf?Fn|Ep?n[f]?
Ep[f]| ?
2)?
1?
?where Kn = sN log3 n.Proof Sketch?
(L, q, p,N) is the constant from Proposition 2.
The main idea in the proof is to solve forn in the following two inequalities (based on Equation [17] [see the following]) whilerelying on Lemma 3:8N(/8,Ftruncated,n) exp(?
1128n2/K2n)?
?/2bound(n)/ ?
?/2499Computational Linguistics Volume 38, Number 3Theorem 2 gives little intuition about the number of samples required for accurateestimation of a grammar because it considers the ?additive?
setting: The empirical riskis within  from the expected risk.
More specifically, it is not clear how we should pick for the log-loss, because the log-loss can obtain arbitrary values.We turn now to converting the additive bound in Theorem 2 to a multiplicativebound.
Multiplicative bounds can be more informative than additive bounds when therange of the values that the log-loss can obtain is not known a priori.
It is importantto note that the two views are equivalent (i.e., it is possible to convert a multiplicativebound to an additive bound and vice versa).
Let ?
?
(0, 1) and choose  = ?Kn.
Then,substituting this  in Theorem 2, we get that ifn ?
max{128?2(2N log 16e?
+ log32?
),log 4/?+ log 1/??
(L, q, p,N)}then, with probability 1?
?,supf?Fn?????1?Ep?n[f]Ep[f]????????
2sN log3(n)H(p)(17)where H(p) is the Shannon entropy of p. This stems from the fact that Ep[f]?
H(p) forany f .
This means that if we are interested in computing a sample complexity boundsuch that the ratio between the empirical risk and the expected risk (for log-loss) isclose to 1 with high probability, we need to pick up ?
such that the righthand side ofEquation (17) is smaller than the desired accuracy level (between 0 and 1).
Note thatEquation (17) is an oracle inequality?it requires knowing the entropy of p or someupper bound on it.5.3 Unsupervised CaseIn the unsupervised setting, we have n yields of derivations from the grammar, x1, ..., xn,and our goal again is to identify grammar parameters ?
from these yields.
Our conceptclasses are now the sets of log marginalized distributions from Fn.
For each f?
?
Fn, wedefine f ??
asf ??
(x) = ?
log?z?Dx(G)exp(?f?
(z)) = ?
log?z?Dx(G)exp??K?k=1Nk?i=1?i,k(z)?i,k?
?We denote the set of { f ??}
by F?n.
Analogously, we define F?.
Note that we also need todefine the operator C?n( f?)
as a first step towards defining F?n as proper approximations(for F?)
in the unsupervised setting.
Let f ?
?
F?.
Let f be the concept in F such thatf ?
(x) =?z f (x, z).
Then we define C?n( f?
)(x) =?z Cn( f )(x, z).It does not immediately follow that F?n is a proper approximation for F?.
It is nothard to show that the boundedness property is satisfied with the same Kn and the sameform of bound(n) as in Proposition 2 (we would have ?bound(m) = m???
logm for some??
(L, q, p,N) = ??
> 0).
This relies on the property of bounded derivation length of p (seeAppendix A, Proposition 7).
The following result shows that we have tightness as well.500Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsUtility Lemma 2For ai, bi ?
0, if ?
log?i ai + log?i bi ?
 then there exists an i such that ?
log ai +log bi ?
.Proposition 5There exists anM such that for any n > Mwe havep??
?f ??F?
{x | C?n(f?)(x)?
f ?
(x) ?
tail(n)}??
?
tail(n)for tail(n) =N log2 nns ?
1 and the operator C?n( f ) as defined earlier.Proof SketchFrom Utility Lemma 2 we havep??
?f ??F?
{x | C?n( f?)(x)?
f ?
(x) ?
tail(n)}??
?
p??
?f?F{x | ?zCn( f )(z)?
f (z) ?
tail(n)}?
?Define X(n) to be all x such that there exists a z with yield(z) = x and |z| ?
log2 n.From the proof of Proposition 3 and the requirements on p, we know that there existsan ?
?
1 such thatp(?f?F{x | ?z s.t.Cn( f )(z)?
f (z) ?
tail(n)})??x?X(n)p(x)?
?x:|x|?log2 n/?p(x) ??
?k=log2 n/?L?
(k)rk ?
tail(n)where the last inequality happens for some n larger than a fixedM.
Computing either the covering number or the pseudo-dimension of F?n is a hardtask, because the function in the classes includes the ?log-sum-exp.?
Dasgupta (1997)overcomes this problem for Bayesian networks with fixed structure by giving a boundon the covering number for (his respective) F?
which depends on the covering numberof F.Unfortunately, we cannot fully adopt this approach, because the derivations ofa probabilistic grammar can be arbitrarily large.
Instead, we present the followingproposition, which is based on the ?Hidden Variable Rule?
from Dasgupta (1997).
Thisproposition shows that the covering number of F?
(or more accurately, its boundedapproximations) can be bounded in terms of the covering number of the bounded501Computational Linguistics Volume 38, Number 3approximations of F, and the constants which control the underlying distribution pmentioned in Section 3.Utility Lemma 3For any two positive-valued sequences (a1, .
.
.
, an) and (b1, .
.
.
, bn) we have that?i | log ai/bi| ?
| log (?ai/?bi) |.Proposition 6 (Hidden Variable Rule for Probabilistic Grammars)Let m =log4Kn(1?
q)log 1q.
Then, N(,F?truncated,n) ?
N(2?
(m),Ftruncated,n).ProofLet Z(m) = {z | |z| ?
m} be the subset of derivations of length shorter than m. Considerf, f0 ?
Ftruncated,n.
Let f ?
and f ?0 be the corresponding functions in F?truncated,n.
Then, for anydistribution p,dp( f ?, f ?0) =?x| f ?(x)?
f ?0(x)| p(x) ?
?x?z| f (x, z)?
f0(x, z)| p(x)=?x?z?Z(m)| f (x, z)?
f0(x, z)| p(x)+?x?z/?Z(m)| f (x, z)?
f0(x, z)| p(x)?
?x?z?Z(m)| f (x, z)?
f0(x, z)| p(x)+?x?z/?Z(m)2Knp(x) (18)?
?x?z?Z(m)| f (x, z)?
f0(x, z)| p(x)+ 2Kn?x : |x|?m|Dx(G)|p(x)?
?x?z?Z(m)| f (x, z)?
f0(x, z)| p(x)+ 2Kn??k=m?2(k)rk?
dp?
( f, f0)|Z(m)|+ 2Knqm1?
qwhere p?
(x, z) is a probability distribution that uniformly divides the probability massp(x) across all derivations for the specific x, that is:p?
(x, z) =p(x)|Dx(G)|The inequality in Equation (18) stems from Utility Lemma 3.Set m to be the quantity that appears in the proposition to get the necessary result( f ?
and f are arbitrary functions in F?truncated,n and Ftruncated,n respectively.
Then considerf ?0 and f0 to be functions from the respective covers.).
For the unsupervised case, then, we get the following sample complexity result.502Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsTheorem 3Let G be a grammar.
Let F?n be a proper approximation for the corresponding family ofprobabilistic grammars.
Let p(x, z) be a distribution over derivations which satisfies therequirements in Section 3.1.
Let x1, .
.
.
, xn be a sample of strings from p(x).
Then thereexists a constant ??
(L, q, p,N) and constant M such that for any 0 < ?
< 1, 0 <  < Kn,any n > M, and ifn ?
max{128K2n2(2N log(32eKn?
(m))+ log 32?
),log 4/?+ log 1/??
(L, q, p,N)}(19)where m =log4Kn(1?
q)log 1q, we have thatp(supf?F?n|Ep?n[f]?
Ep[f]| ?
2)?
1?
?where Kn = sN log3 n.Theorem 3 states that the number of samples we require in order to accurately esti-mate a probabilistic grammar from unparsed strings depends on the level of ambiguityin the grammar, represented as ?(m).
We note that this dependence is polynomial, andwe consider this a positive result for unsupervised learning of grammars.
More specif-ically, if ?
is an exponential function (such as the case with PCFGs), when compared tothe supervised learning, there is an extra multiplicative factor in the sample complexityin the unsupervised setting that behaves like O(log logKn ).We note that the following Equation (20) can again be reduced to a multiplicativecase, similarly to the way we described it for the supervised case.
Setting  = ?Kn (?
?
(0, 1)), we get the following requirement on n:n ?
max{128?2(2N log(32e?
t(?)?
)+ log 32?
),log 4/?+ log 1/??
(L, q, p,N)}(20)where t(?)
=log 4?(1?
q)log 1q.6.
Algorithms for Empirical Risk MinimizationWe turn now to describing algorithms and their properties for minimizing empiricalrisk using the framework described in Section 4.6.1 Supervised CaseERM with proper approximations leads to simple algorithms for estimating the proba-bilities of a probabilistic grammar in the supervised setting.
Given an  > 0 and a ?
> 0,we draw n examples according to Theorem 2.
We then set ?
= n?s.
To minimize thelog-loss with respect to these n examples, we use the proper approximation Fn.503Computational Linguistics Volume 38, Number 3Note that the value of the empirical log-loss for a probabilistic grammar param-etrized by ?
isEp?n[?
log h(x, z | ?
)]= ?
?x,zp?n(x, z) log h(x, z | ?
)= ?
?x,zp?n(x, z)K?k=1Nk?i=1?k,i(x, z) log(?k,i)= ?K?k=1Nk?i=1log(?k,i)Ep?n[?k,i]Because we make the assumption that deg(G) ?
2 (Section 3.2), we haveEp?n[?
log h(x, z | ?
)]= ?K?k=1(log(?k,1)Ep?n[?k,1]+ log(1?
?k,1)Ep?n[?k,2])(21)To minimize the log-loss with respect to Fn, we need to minimize Equation (21) underthe constraint that ?
?
?k,i ?
1?
?
and ?k1 + ?k,2 = 1.
It can be shown that the solutionfor this optimization problem is?k,i = min???1?
?,max????,??n?j=1??j,k,i??/??n?j=12?i?=1??j,k,i?????????
(22)where ?
?j,k,i is the number of times that ?k,i fires in Example j.
(We include a fullderivation of this result in Appendix B.)
The interpretation of Equation (22) is simple:We count the number of times a rule appears in the samples and then normalize thisvalue by the total number of times rules associated with the same multinomial appearin the samples.
This frequency count is the maximum likelihood solution with respectto the full hypothesis class H (Corazza and Satta 2006; see Appendix B).
Because weconstrain ourselves to obtain a value away from 0 or 1 by a margin of ?, we need totruncate this solution, as done in Equation (22).This truncation to amargin ?
can be thought of as a smoothing factor that enables usto compute sample complexity bounds.
We explore this connection to smoothing witha Dirichlet prior in a Maximum a posteriori (MAP) Bayesian setting in Section 7.2.6.2 Unsupervised CaseSimilarly to the supervised case, minimizing the empirical log-loss in the unsupervisedsetting requires minimizing (with respect to ?)
the following:Ep?n[?
log h(x | ?
)]= ?
?xp?n(x) log?zh(x, z | ?)
(23)with the constraint that ?
?
?k,i ?
1?
?
(i.e., ?
?
?(?))
where ?
= n?s.
This is doneafter drawing n examples according to Theorem 3.504Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars6.2.1 Hardness of ERM with Proper Approximations.
It turns out that minimizing Equa-tion (23) under the specified constraints is actually an NP-hard problem when G is aPCFG.
This result follows using a similar proof to the one in Cohen and Smith (2010c)for the hardness of Viterbi training and maximizing log-likelihood for PCFGs.
We turnto giving the full derivation of this hardness result for PCFGs and the modificationrequired for adapting the results from Cohen and Smith to the case of having anarbitrary ?margin constraint.In order to show an NP-hardness result, we need to ?convert?
the problem of themaximization of Equation (23) to a decision problem.
We do so by stating the followingdecision problem.Problem 1 (Unsupervised Minimization of the Log-Loss with Margin)Input: A binarized context-free grammar G, a set of sentences x1, .
.
.
, xn, a value ?
?
[0, 12 ), and a value ?
?
[0, 1].Output: 1 if there exists ?
?
?(?)
(and hence, h ?
H(G)) such that?
?xp?n(x) log?zh(x, z | ?)
?
?
log(?)
(24)and 0 otherwise.We will show the hardness result both when ?
is not restricted at all as well as whenwe allow ?
> 0.
The proof of the hardness result is achieved by reducing the problem3-SAT (Sipser 2006), known to be NP-complete, to Problem 1.
The problem 3-SAT isdefined as follows:Problem 2 (3-SAT)Input: A formula ?
=?mi=1 (ai ?
bi ?
ci) in conjunctive normal form, such that eachclause has three literals.Output: 1 if there is a satisfying assignment for ?, and 0 otherwise.Given an instance of the 3-SAT problem, the reduction will, in polynomial time,create a grammar and a single string such that solving Problem 1 for this grammar andstring will yield a solution for the instance of the 3-SAT problem.Let ?
=?mi=1 (ai ?
bi ?
ci) be an instance of the 3-SAT problem, where ai, bi, andci are literals over the set of variables {Y1, .
.
.
,YN} (a literal refers to a variable Yj orits negation, Y?j).
Let Cj be the jth clause in ?, such that Cj = aj ?
bj ?
cj.
We define thefollowing CFG G?
and string to parse s?:1.
The terminals of G?
are the binary digits ?
= {0, 1}.2.
We create N nonterminals VYr , r ?
{1, .
.
.
,N} and rules VYr ?
0 andVYr ?
1.3.
We create N nonterminals VY?r , r ?
{1, .
.
.
,N} and rules VY?r ?
0 andVY?r ?
1.4.
We create UYr,1 ?
VYrVY?r and UYr,0 ?
VY?rVYr .5.
We create the rule S1 ?
A1.
For each j ?
{2, .
.
.
,m}, we create a ruleSj ?
Sj?1Aj where Sj is a new nonterminal indexed by ?j ?ji=1 Ciand Aj is also a new nonterminal indexed by j ?
{1, .
.
.
,m}.505Computational Linguistics Volume 38, Number 36.
Let Cj = aj ?
bj ?
cj be clause j in ?.
Let Y(aj) be the variable that ajmentions.
Let (y1, y2, y3) be a satisfying assignment for Cj where yk ?
{0, 1}and is the value of Y(aj), Y(bj), and Y(cj), respectively, for k ?
{1, 2, 3}.
Foreach such clause-satisfying assignment, we add the ruleAj ?
UY(aj ),y1UY(bj ),y2UY(cj ),y3For each Aj, we would have at most seven rules of this form, because onerule will be logically inconsistent with aj ?
bj ?
cj.7.
The grammar?s start symbol is Sn.8.
The string to parse is s?
= (10)3m, that is, 3m consecutive occurrences ofthe string 10.A parse of the string s?
using G?
will be used to get an assignment by settingYr = 0 if the rule VYr ?
0 or VY?r ?
1 is used in the derivation of the parse tree, and 1otherwise.
Notice that at this point we do not exclude ?contradictions?
that come fromthe parse tree, such as VY3 ?
0 used in the tree together with VY3 ?
1 or VY?3 ?
0.
Tomaintain the restriction on the degree of grammars, we convertG?
to the binary normalform described in Section 3.2.
The following lemma gives a condition under which theassignment is consistent (so that contradictions do not occur in the parse tree).Lemma 4Let ?
be an instance of the 3-SAT problem, and let G?
be a probabilistic CFG based onthe given grammar with weights ??.
If the (multiplicative) weight of the Viterbi parse(i.e., the highest scoring parse according to the PCFG) of s?
is 1, then the assignmentextracted from the parse tree is consistent.ProofBecause the probability of the Viterbi parse is 1, all rules of the form {VYr ,VY?r} ?
{0, 1}which appear in the parse tree have probability 1 as well.
There are two possible typesof inconsistencies.
We show that neither exists in the Viterbi parse:1.
For any r, an appearance of both rules of the form VYr ?
0 and VYr ?
1cannot occur because all rules that appear in the Viterbi parse tree haveprobability 1.2.
For any r, an appearance of rules of the form VYr ?
1 and VY?r ?
1 cannotoccur, because whenever we have an appearance of the rule VYr ?
0, wehave an adjacent appearance of the rule VY?r ?
1 (because we parsesubstrings of the form 10), and then we again use the fact that all rules inthe parse tree have probability 1.
The case of VYr ?
0 and VY?r ?
0 ishandled analogously.Thus, both possible inconsistencies are ruled out, resulting in a consistent assignment.Figure 3 gives an example of an application of the reduction.506Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsFigure 3An example of a Viterbi parse tree which represents a satisfying assignment for?
= (Y1 ?
Y2 ?
Y?4) ?
(Y?1 ?
Y?2 ?
Y3).
In ?
?, all rules appearing in the parse tree haveprobability 1.
The extracted assignment would be Y1 = 0,Y2 = 1,Y3 = 1,Y4 = 0.Note that there is no usage of two different rules for a single nonterminal.Lemma 5Define ?
and G?
as before.
There exists ??
such that the Viterbi parse of s?
is 1 if andonly if ?
is satisfiable.
Moreover, the satisfying assignment is the one extracted from theparse tree with weight 1 of s?
under ??.Proof(=?)
Assume that there is a satisfying assignment.
Each clause Cj = aj ?
bj ?
cj is sat-isfied using a tuple (y1, y2, y3), which assigns values for Y(aj), Y(bj), and Y(cj).
Thisassignment corresponds to the following rule:Aj ?
UY(aj ),y1UY(bj ),y2UY(cj ),y3Set its probability to 1, and set al other rules of Aj to 0.
In addition, for each r, ifYr = y, set the probabilities of the rules VYr ?
y and VY?r ?
1?
y to 1 and VY?r ?
y andVYr ?
1?
y to 0.
The rest of the weights for Sj ?
Sj?1Aj are set to 1.
This assignment ofrule probabilities results in a Viterbi parse of weight 1.
(?=) Assume that the Viterbi parse has probability 1.
From Lemma 4, we know that wecan extract a consistent assignment from the Viterbi parse.
In addition, for each clauseCj we have a ruleAj ?
UY(aj ),y1UY(bj ),y2UY(cj ),y3that is assigned probability 1, for some (y1, y2, y3).
One can verify that (y1, y2, y3) arethe values of the assignment for the corresponding variables in clause Cj, and thatthey satisfy this clause.
This means that each clause is satisfied by the assignment weextracted.
We are now ready to prove the following result.Theorem 4Problem 1 is NP-hard when either requiring ?
> 0 or when fixing ?
= 0.507Computational Linguistics Volume 38, Number 3ProofWe first describe the reduction for the case of ?
= 0.
In Problem 1, set ?
= 0, ?
= 1,G = G?, ?
= 0, and x1 = s?.
If ?
is satisfiable, then the left side of Equation (24) can getvalue 0, by setting the rule probabilities according to Lemma 5, hence we would return1 as the result of running Problem 1.If ?
is unsatisfiable, then we would still get value 0 only if L(G) = {s?}.
If G?
gen-erates a single derivation for (10)3m, then we actually do have a satisfying assignmentfrom Lemma 4.
Otherwise (more than a single derivation), the optimal ?
would haveto give fractional probabilities to rules of the form VYr ?
{0, 1} (or VY?r ?
{0, 1}).
Inthat case, it is no longer true that (10)3m is the only generated sentence, and this is acontradiction to getting value 0 for Problem 1.We next show that Problem 1 is NP-hard even if we require ?
> 0.
Let ?
< 120m .Set ?
= ?, and the rest of the inputs to Problem 1 the same as before.
Assume that ?is satisfiable.
Let ?
be the rule probabilities from Equation (5) after being shifted with amargin of ?.
Then, because there is a derivation that uses only rules that have probability1?
?, we haveh(x1 | T(?,?),G?)
=?zp(x1, z | T(?,?),G?)?
(1?
?
)10m> ?because the size of the parse tree for (10)3m is at most 10m (using the binarized G?
)and assuming ?
= ?
< (1?
?)10m.
This inequality indeed holds whenever ?
< 120m .Therefore, we have ?
log h(x1 | ?)
> ?
log?.
Problem 1 would return 0 in this case.Now, assume that ?
is not satisfiable.
That means that any parse tree for the string(10)3m would have to contain two different rules headed by the same non-terminal.
Thismeans thath(x1 | T(?,?),G?)
=?zp(x1, z | T(?,?),G?)?
?and therefore ?
log h(x1 | T(?,?))
?
?
log?, and Problem 1 would return 1.
6.2.2 An Expectation-Maximization Algorithm.
Instead of solving the optimization prob-lem implied by Equation (21), we propose a rather simple modification to theexpectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to ap-proximate the optimal solution?this algorithm finds a local maximum for the max-imum likelihood problem using proper approximations.
The modified algorithm isgiven in Algorithm 1.The modification from the usual expectation-maximization algorithm is done in theM-step: Instead of using the expected value of the sufficient statistics by counting andnormalizing, we truncate the values by ?.
It can be shown that if ?
(0) ?
?(?
), then thelikelihood is guaranteed to increase (and hence, the log-loss is guaranteed to decrease)after each iteration of the algorithm.508Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsAlgorithm 1: Expectation-Maximization Algorithmwith Proper Approximations.Input: grammar G in binary normal form, initial parameters ?
(0),  > 0, ?
> 0,s > 1Output: learned parameters ?draw x = ?x1, ..., xn?
from p following Theorem 3; t?
1 ;?
?
n?s;repeat// E?
(t?1)[?k,i(z) | xj]denotes the expected counts of event i in multinomial kunder the distribution p?n(x)p(z | x,?
(t?1))Compute for each training example j ?
{1, .
.
.
,n}, for each event i ?
{1, 2} in eachmultinomial k ?
{1, .
.
.
,K}: ?
?j,k,i ?
E?
(t?1)[?k,i(z) | xj];Set ?
(t)i,k= min{1?
?,max{?,(?nj=1 ?
?j,k,i)/(?nj=1?2i?=1 ??j,k,i?)}};t?
t+ 1;until convergence;return ?
(t)The reason for this likelihood increase stems from the fact that the M-step solvesthe optimization problem of minimizing the log-loss (with respect to ?
?
?(?))
whenthe posterior calculate at the E-step as the base distribution is used.
This means that theM-step minimizes (in iteration t): Er[?
log h(x, z | ?
(t) )]where the expectation is takenwith respect to the distribution r(x, z) = p?n(x)p(z | x,?(t?1)).
With this notion in mind,the likelihood increase after each iteration follows from principles similar to thosedescribed in Bishop (2006) for the EM algorithm.7.
DiscussionOur framework can be specialized to improve the two main criteria which have a trade-off: the tightness of the proper approximation and the sample complexity.
For example,we can improve the tightness of our proper approximations by taking a subsequenceof Fn.
This will make the sample complexity bound degrade, however, because Kn willgrow faster.
Table 2 shows the trade-offs between parameters in our model and theeffectiveness of learning.We note that the sample complexity bounds that we give in this article giveinsight about the asymptotic behavior of grammar estimation, but are not necessarilyTable 2Trade-off between quantities in our learning model and effectiveness of different criteria.
Kn isthe constant that satisfies the boundedness property (Theorems 2 and 3) and s is a fixed constantlarger than 1 (Section 4.1).criterion as Kn increases .
.
.
as s increases .
.
.tightness of proper approximation improves improvessample complexity bound degrades degrades509Computational Linguistics Volume 38, Number 3sufficiently tight to be used in practice.
It still remains an open problem to obtainsample complexity bounds which are sufficiently tight in this respect.
For a discussionabout the connection of grammar learning in theory and practice, we refer the readerto Clark and Lappin (2010).It is also important to note that MLE is not the only option for estimating finitestate probabilistic grammars.
There has been some recent advances in learning finitestate models (HMMs and finite state transducers) by using spectral analysis of matriceswhich consist of quantities estimated from observations only (Hsu, Kakade, and Zhang2009; Balle, Quattoni, and Carreras 2011), based on the observable operator models ofJaeger (1999).
These algorithms are not prone to local minima, and converge to thecorrect model as the number of samples increases, but require some assumptions aboutthe underlying model that generates the data.7.1 Tsybakov NoiseIn this article, we chose to introduce assumptions about distributions that generatenatural language data.
The choice of these assumptions was motivated by observationsabout properties shared among treebanks.
The main consequence of making theseassumptions is bounding the amount of noise in the distribution (i.e., the amount ofvariation in probabilities across labels given a fixed input).There are other ways to restrict the noise in a distribution.
One condition for suchnoise restriction, which has received considerable recent attention in the statistical liter-ature, is the Tsybakov noise condition (Tsybakov 2004; Koltchinskii 2006).
Showing thata distribution satisfies the Tsybakov noise condition enables the use of techniques (e.g.,from Koltchinskii 2006) for deriving distribution-dependent sample complexity boundsthat depend on the parameters of the noise.
It is therefore of interest to see whetherTsybakov noise holds under the assumptions presented in Section 3.1.
We show thatthis is not the case, and that Tsybakov noise is too permissive.
In fact, we show that pcan be a probabilistic grammar itself (and hence, satisfy the assumptions in Section 3.1),and still not satisfy the Tsybakov noise conditions.Tsybakov noise was originally introduced for classification problems (Tsybakov2004), and was later extended to more general settings, such as the one we are facing inthis article (Koltchinskii 2006).
We now explain the definition of Tsybakov noise in ourcontext.Let C > 0 and ?
?
1.
We say that a distribution p(x, z) satisfies the (C,?)
Tsybakovnoise condition if for any  > 0 and h, g ?
H such that h, g ?
{h?
| Ep(h?,H) ?
}, wehavedist(g, h) ???
?Ep[(log glog h)2]?
C1/?
(25)This interpretation of Tsybakov noise implies that the diameter of the set of functionsfrom the concept class that has small excess risk should shrink to 0 at the rate inEquation (25).
Distribution-dependent bounds from Koltchinskii (2006) are monotonewith respect to the diameter of this set of functions, and therefore demonstrating that itgoes to 0 enables sharper derivations of sample complexity bounds.510Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsWe turn now to illustrating that the Tsybakov condition does not hold for proba-bilistic grammars in most cases.
Let G be a probabilistic grammar.
Define A = AG(?)
asa matrix such that(AG(?))(k,i),(k?,i? )
E[?k,i ??k?,i?]E[?k,i]E[?k?,i?
]Theorem 5Let G be a grammar with K ?
2 and degree 2.
Assume that p is ?G,???
for some ?
?, suchthat ?
?1,1 = ?
?2,1 = ?
and that c1 ?
c2.
If AG(??)
is positive definite, then p does not satisfythe Tsybakov noise condition for any (C,?
), where C > 0 and ?
?
1.See Appendix C for the proof of Theorem 5.In Appendix C we show that AG(?)
is positive semi-definite for any choice of ?.The main intuition behind the proof is that given a probabilistic grammar p, we canconstruct an hypothesis h such that the KL divergence between p and h is small, butdist(p, h) is lower-bounded and is not close to 0.We conclude that probabilistic grammars, as generative distributions of data, donot generally satisfy the Tsybakov noise condition.
This motivates an alternative choiceof assumptions that could lead to better understanding of rates of convergences andbounds on the excess risk.
Section 3.1 states such assumptions which were also justifiedempirically.7.2 Comparison to Dirichlet Maximum A Posteriori SolutionsThe transformation T(?,?)
from Section 4.1 can be thought of as a smoother for theprobabilities ?
: It ensures that the probability of each rule is at least ?
(and as a result,the probabilities of all rules cannot exceed 1?
?).
Adding pseudo-counts to frequencycounts is also a common way to smooth probabilities in models based on multinomialdistributions, including probabilistic grammars (Manning and Schu?tze 1999).
Thesepseudo-counts can be framed as a maximum a posteriori (MAP) alternative to themaximum likelihood problem, with the choice of Bayesian prior over the parameters inthe form of a Dirichlet distribution.
In comparison to our framework, with (symmetric)Dirichlet smoothing, instead of truncating the probabilities with a margin ?
we wouldset the probability of each rule (in the supervised setting) to?
?k,i =?nj=1 ?
?j,k,i + ??
1?nj=1 ?
?j,k,1 +?nj=1 ?
?j,k,2 + 2(??
1)(26)for i = 1, 2, where ?
?k,i are the counts in the data of event i in multinomial k for Example j.Dirichlet smoothing can be formulated as the result of adding a symmetric Dirichletprior over the parameters ?k,i with hyperparameter ?.
Then Equation (26) is the modeof the posterior after observing ?
?k,i appearances of event i in multinomial k.The effect of Dirichlet smoothing becomes weaker as we have more samples,because the frequency counts ?
?j,k,i become dominant in both the numerator and thedenominator when there are more data.
In this sense, the prior?s effect on learningdiminishes as we use more data.
A similar effect occurs in our framework: ?
= n?swhere n is the number of samples?the more samples we have, the more we trust the511Computational Linguistics Volume 38, Number 3counts in the data to be reliable.
There is a subtle difference, however.
With the DirichletMAP solution, the smoothing is less dominant only if the counts of the features are large,regardless of the number of samples we have.
With our framework, smoothing dependsonly on the number of samples we have.
These two scenarios are related, of course: Themore samples we have, the more likely it is that the counts of the events will grow large.7.3 Other Derivations of Sample Complexity BoundsIn this section, we discuss other possible solutions to the problem of deriving samplecomplexity bounds for probabilistic grammars.7.3.1 Using Talagrand?s Inequality.
Our bounds are based on VC theory together withclassical results for empirical processes (Pollard 1984).
There have been some recentdevelopments to the derivation of rates of convergence in statistical learning theory(Massart 2000; Bartlett, Bousquet, and Mendelson 2005; Koltchinskii 2006), mostprominently through the use of Talagrand?s inequality (Talagrand 1994), which is aconcentration of measure inequality, in the spirit of Lemma 2.The bounds achieved with Talagrand?s inequality are also distribution-dependent,and are based on the diameter of the -minimal set?the set of hypotheses which havean excess risk smaller than .
We saw in Section 7.1 that the diameter of the -minimalset does not follow the Tsybakov noise condition, but it is perhaps possible to findmeaningful bounds for it, in which case we may be able to get tighter bounds usingTalagrand?s inequality.
We note that it may be possible to obtain data-dependent boundsfor the diameter of the -minimal set, following Koltchinskii (2006), by calculating thediameter of the -minimal set using p?n.7.3.2 Simpler Bounds for the Supervised Case.As noted in Section 6.1, minimizing empiricalrisk with the log-loss leads to a simple frequency count for calculating the estimatedparameters of the grammar.
In Corazza and Satta (2006), it has been also noted that tominimize the non-empirical risk, it is necessary to set the parameters of the grammar tothe normalized expected count of the features.This means that we can get bounds on the deviation of a certain parameter fromthe optimal parameter by applying modifications to rather simple inequalities suchas Hoeffding?s inequality, which determines the probability of the average of a set ofi.i.d.
random variables deviating from its mean.
The modification would require usto split the event space into two cases: one in which the count of some features islarger than some fixed value (which will happen with small probability because of thebounded expectation of features), and one in which they are all smaller than that fixedvalue.
Handling these two cases separately is necessary because Hoeffding?s inequalityrequires that the count of the rules is bounded.The bound on the deviation from the mean of the parameters (the true probability)can potentially lead to a bound on the excess risk in the supervised case.
This formula-tion of the problem would not generalize to the unsupervised case, however, where theempirical risk minimization does not amount to simple frequency count.7.4 Open ProblemsWe conclude the discussion with some directions for further exploration and futurework.512Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars7.4.1 Sample Complexity Bounds with Semi-Supervised Learning.
Our bounds focus on thesupervised case and the unsupervised case.
There is a trivial extension to the semi-supervised case.
Consider the objective function to be the sum of the likelihood for thelabeled data together with the marginalized likelihood of the unlabeled data (this sumcould be a weighted sum).
Then, use the sample complexity bounds for each summandto derive a sample complexity bound on this sum.It would be more interesting to extend our results to frameworks such as the onedescribed by Balcan and Blum (2010).
In that case, our discussion of sample complexitywould attempt to identify how unannotated data can reduce the space of candidateprobabilistic grammars to a smaller set, after which we can use the annotated datato estimate the final grammar.
This reduction of the space is accomplished through anotion of compatibility, a type of fitness that the learner believes the estimated grammarshould have given the distribution that generates the data.
The key challenge in thecase of probabilistic grammars would be to properly define this compatibility notionsuch that it fits the log-loss.
If this is achieved, then similar machinery to that describedin this paper (with proper approximations) can be followed to derive semi-supervisedsample complexity bounds for probabilistic grammars.7.4.2 Sharper Bounds for the Pseudo-Dimension of Probabilistic Grammars.
The pseudo-dimension of a probabilistic grammar with the log-loss is bounded by the number ofparameters in the grammar, because the logarithm of a distribution generated by aprobabilistic grammar is a linear function.
Typically the set of counts for the featurevectors of a probabilistic grammar resides in a subspace of a dimension which is smallerthan the full dimension specified by the number of parameters, however.
The reason forthis is that there are usually relationships (which are often linear) between the elementsin the feature counts.
For example, with HMMs, the total feature count for emissionsshould equal the total feature count for transitions.
With PCFGs, the total number oftimes that nonterminal rules fire equals the total number of times that features withthat nonerminal in the right-hand side fired, again reducing the pseudo-dimension.
Anopen problem that remains is characterization of the exact value pseudo-dimension fora given grammar, determined by consideration of various properties of that grammar.We conjecture, however, that a lower bound on the pseudo-dimension would be ratherclose to the full dimension of the grammar (the number of parameters).It is interesting to note that there has been some work to identify the VC dimensionand pseudo-dimension for certain types of grammars.
Bane, Riggle, and Sonderegger(2010), for example, calculated the VC dimension for constraint-based grammars.Ishigami and Tani (1993, 1997) computed the VC dimension for finite state automatawith various properties.7.5 ConclusionWe presented a framework for performing empirical risk minimization for probabilis-tic grammars, in which sample complexity bounds, for the supervised case and theunsupervised case, can be derived.
Our framework is based on the idea of boundedapproximations used in the past to derive sample complexity bounds for graphicalmodels.Our framework required assumptions about the probability distribution that gener-ates sentences or derivations in the language of the given grammar.
These assumptionswere tested using corpora, and found to fit the data well.513Computational Linguistics Volume 38, Number 3We also discussed algorithms that can be used for minimizing empirical risk inour framework, given enough samples.
We showed that directly trying to minimizeempirical risk in the unsupervised case is NP-hard, and suggested an approximationbased on an expectation-maximization algorithm.Appendix A. ProofsWe include in this appendix proofs for several results in the article.Utility Lemma 1Let ai ?
[0, 1], i ?
{1, .
.
.
,N} such that?i ai = 1.
Define b1 = a1, c1 = 1?
a1, bi =(aiai?1)(bi?1ci?1), and ci = 1?
bi for i ?
2.
Then ai =??i?1?j=1cj??
bi.ProofProof by induction on i ?
{1, .
.
.
,N}.
Clearly, the statement holds for i = 1.
Assume itholds for arbitrary i < N. Then:ai+1 =(aiai)ai+1 =????i?1?j=1cj??
bi??ai+1ai=????i?1?j=1cj??
bi??cibi+1bi=??i?j=1cj??
bi+1and this completes the proof.
Lemma 1Denote by Z,n the set?f?F{z | Cn( f )(z)?
f (z) ?
}.
Denote by A,n the event ?one ofzi ?
D is in Z,n.?
If Fn properly approximates F, then:E[Ep?n[gn]?
Ep?n[f ?n]](A.1)???
?E[Ep?n[Cn( f?n )]| A,n]???p(A,n)+??
?E[Ep?n[f ?n]| A,n]??
?p(A,n)+ tail(n)where the expectations are taken with respect to the data set D.ProofConsider the following:E[Ep?n[gn]?
Ep?n[f ?n]]= E[Ep?n[gn]?
Ep?n[Cn( f?n )]+ Ep?n[Cn( f?n )]?
Ep?n[f ?n]]= E[Ep?n[gn]?
Ep?n[Cn( f?n )]]+ E[Ep?n[Cn( f?n )]?
Ep?n[f ?n]]514Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsNote first that E[Ep?n[gn]?
Ep?n[Cn( f?n )]]?
0, by the definition of gn as the mini-mizer of the empirical risk.
We next bound E[Ep?n[Cn( f?n )]?
Ep?n[f ?n]].
We know fromthe requirement of proper approximation that we haveE[Ep?n[Cn( f?n )]?
Ep?n[f ?n]]= E[Ep?n[Cn( f?n )]?
Ep?n[f ?n]| A,n]p(A,n)+ E[Ep?n[Cn( f?n )]?
Ep?n[f ?n]| ?A,n](1?
p(A,n))?
|E[Ep?n[Cn( f?n )]| A,n]|p(A,n)+ |E[Ep?n[f ?n]| A,n]|p(A,n)+ tail(n)and that equals the right side of Equation (Appendix A.1).
Proposition 2Let p ?
P(?,L, r, q,B,G) and let Fm be as defined earlier.
There exists a constant ?
=?
(L, q, p,N) > 0 such that Fm has the boundedness property with Km = sN log3m andbound(m) = m??
logm.ProofLet f ?
Fm.
Let Z(m) = {z | |z| ?
log2m}.
Then, for all z ?
Z(m) we have | f (z)| =??i,k?
(k, i) log?k,i ??i,k?
(k, i)(p logm) ?
sN log3m = Km, where the first inequalityfollows from f ?
Fm (?k,i ?
m?s) and the second from |z| ?
log2m.
In addition, from therequirements on p we haveE[| f | ?
I {| f | ?
Km}]?
(sN log3m)????k>log2mL?(k)rkk??
?(?
log3m)?
(qlog2m)for ?
= sNL(1?
q)2.
Finally, for ?
(L, q, p,N)  log?+ 1+ log 1q = ?
> 0 and if m > 1 then(?
log3m)(qlog2m)?
m??
logm.
Utility Lemma 4(From [Dasgupta 1997].)
Let a ?
[0, 1] and let b = a if a ?
[?, 1?
?
], b = ?
if a ?
?,and b = 1?
?
if a ?
1?
?.
Then for any  ?
1/2 such that ?
?
/(1+ ) we havelog a/b ?
.Proposition 3Let p ?
P(?,L, r, q,B,G) and let Fm as defined earlier.
There exists anM such that for anym > Mwe havep??
?f?F{z | Cm( f )(z)?
f (z) ?
tail(m)}??
?
tail(m)for tail(m) =N log2mms ?
1 and Cm( f ) = T( f,m?s).515Computational Linguistics Volume 38, Number 3ProofLet Z(m) be the set of derivations of size bigger than log2m.
Let f ?
F. Define f ?
=T(f,m?s).
For any z /?
Z(m) we have thatf ?(z)?
f (z) = ?K?k=1(?k,1(z) log?k,1 + ?k,2(z) log?k,2 ?
?k,1(z) log?
?k,1 ?
?k,1(z) log?
?k,2)?K?k=1log2m(max{0, log(?
?k,1/?k,1)}+max{0, log(?
?k,2/?k,2)})(A.2)Without loss of generality, assume tail(n)/N log2m ?
1/2.
Let ?
=tail(m)/N log2m1+ tail(m)/N log2m=1/ms.
From Utility Lemma 4 we have that log(?
?k,i/?k,i) ?
tail(m)/N logm.
Plug thisinto Equation A.2 (N = 2K) to get that for all z /?
Z(m) we have f ?(z)?
f (z) ?
tail(m).It remains to show that the measure p(Z(m)) ?
tail(m).
Note that?z?Z(m) p(z) ??k>log2mL?
(k)rk ?
L?k>log2mqk = Lqlog2m/(1?
q) < tail(m) for m > M where M isfixed.
Proposition 7There exists a ??
(L, p, q,N) > 0 such that F?m has the boundedness property with Km =sN log3m and bound(m) = m???
logm.ProofFrom the requirement of p, we know that for any x we have a z such that yield(z) = xand |z| ?
?|x|.
Therefore, if we let X(m) = {x | |x| ?
log2m/?
}, then we have for anyf ?
F?m and x ?
X(m) that f (x) ?
sN log3m = Km (similarly to the proof of Proposition 2).Denote by f1(x, z) the function in Fm such that f (x) = ?
log?z exp(?f1(x, z)).In addition, from the requirements on p and the definition of Km we haveE[| f | ?
I {| f | ?
Km}]=?xp(x)f (x)I { f ?
Km}=?x:|x|>log2m/?p(x)f (x)?
?x:|x|>log2m/?p(x)f1(x, z(x))516Cohen and Smith Empirical Risk Minimization for Probabilistic Grammarswhere z(x) is some derivation for x.
We have?x:|x|>log2m/?p(x)f1(x, z(x)) ??x:|x|?log2m/?
?z?Dx(G)p(x, z)f1(x, z(x))?
sN logm?x:|x|>log2m/?
?zp(x, z)|z(x)|?
sN logm?k>log2m?(k)rkk?
sN logm?k>log2mqkk ?
?
logmqlog2mfor some constant ?
> 0.
Finally, for some ??
(L, p, q,N) = ??
> 0 and some constant M,if m > M then ?
logm(qlog2m)?
m???
logm.
Utility Lemma 2For ai, bi ?
0, if ?
log?i ai + log?i bi ?
 then there exists an i such that ?
log ai +log bi ?
.ProofAssume ?
log ai + log bi <  for all i.
Then, bi/ai < e, therefore?i bi/?i ai < e, there-fore ?
log?i ai + log?i bi <  which is a contradiction to ?
log?i ai + log?i bi ?
.The next lemma is the main concentation of measure result that we use.
Its proofrequires some simple modification to the proof given for Theorem 24 in Pollard (1984,pages 30?31).Lemma 2Let Fn be a permissible class of functions such that for every f ?
Fn we have E[| f | ?I {| f | ?
Kn}] ?
bound(n).
Let Ftruncated,n = { f ?
I { f ?
Kn} | f ?
Fm}, that is, the set offunctions from Fn after being truncated by Kn.
Then for  > 0 we havep(supf?Fn|Ep?n[f]?
Ep[f]| > 2)?
8N(/8,Ftruncated,n) exp(?
1128n2/K2n)+ bound(n)/provided n ?
K2n/42 and bound(n) < .ProofFirst note thatsupf?Fn|Ep?n[f]?
Ep[f]| ?
supf?Fn|Ep?n[f I {| f | ?
Kn}]?
Ep[f I {| f | ?
Kn}]|+ supf?FnEp?n[| f |I {| f | ?
Kn}]+ supf?FnEp[| f |I {| f | ?
Kn}]517Computational Linguistics Volume 38, Number 3We have supf?Fn Ep[| f |I {| f | ?
Kn}]?
bound(n) < , and also, from Markov in-equality, we haveP(supf?FnEp?n[| f |I {| f | ?
Kn}]> ) ?
bound(n)/At this point, we can follow the proof of Theorem 24 in Pollard (1984), and itsextension on pages 30?31 to get Lemma 2, using the shifted set of functions Ftruncated,n.Appendix B.
Minimizing Log-Loss for Probabilistic GrammarsCentral to our algorithms for minimizing the log-loss (both in the supervised case andthe unsupervised case) is a convex optimization problem of the formmin?K?k=1ck,1 log?k,1 + ck,2 log?k,2such that ?k ?
{1, .
.
.
,K} :?k,1 + ?k,2 = 1?
?
?k,1 ?
1?
??
?
?k,2 ?
1?
?for constants ck,i which depend on p?n or some other intermediate distribution in thecase of the expectation-maximization algorithm and ?
which is a margin determinedby the number of samples.
This minimization problem can be decomposed into severaloptimization problems, one for each k, each having the following form:max?c1?1 + c2?2 (B.1)such that exp(?1)+ exp(?2) = 1 (B.2)?
?
?1 ?
1?
?
(B.3)?
?
?2 ?
1?
?
(B.4)where ci ?
0 and 1/2 > ?
?
0.
Ignore for a moment the constraints ?
?
?i ?
1?
?.
Inthat case, this can be thought of as a regular maximum likelihood estimation problem,so ?i = ci/(c1 + c2).
We give a derivation of this result in this simple case for completion.We use Lagranian multipliers to solve this problem.
Let F(?1,?2) = c1?1 + c2?2.
Definethe Lagrangian:g(?)
= inf?L(?,?
)= inf?c1?1 + c2?2 + ?
(exp(?1)+ exp(?2)?
1)518Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsTaking the derivative of the term we minimize in the Lagrangian, we have?L?
?i= ci + ?
exp(?i)Setting the derivatives to 0 for minimization, we haveg(?)
= c1 log(?c1/?
)+ c2 log(?c2/?
)+ ?(?c1/??
c2/??
1) (B.5)g(?)
is the objective function of the dual problem of Equation (B.1)?Equation (B.2).We would like to minimize Equation (B.5) with respect to ?.
The derivative of g(?)
is?g?
?= ?c1/??
c2/??
1hence when equating the derivative of g(?)
to 0, we get ?
= ?
(c1 + c2), and thereforethe solution is ?
?i = log (ci/(c1 + c2)).
We need to verify that the solution to the dualproblem indeed gets the optimal value for the primal.
Because the primal problem isconvex, it is sufficient to verify that the Karush-Kuhn-Tucker (KKT) conditions hold(Boyd and Vandenberghe 2004).
Indeed, we have?F??i(??
)+ ?
?h??i(??)
= ci ?
(c1 + c2)?cic1 + c2= 0where h(?)
 exp(?
)+ exp(?)?
1 stands for the equality constraint.
The rest of theKKT conditions trivially hold, therefore ??
is the optimal solution for Equations (B.1)?
(B.2).Note that if 1?
?
< ci/(c1 + c2) < ?, then this is the solution even when againadding the constraints in Equation (B.3) and (B.4).
When c1/(c1 + c2) < ?, then thesolution is ?
?1 = ?
and ?
?2 = 1?
?.
Similarly, when c2/(c1 + c2) < ?
then the solution is?
?2 = ?
and ?
?1 = 1?
?.
We describe why this is true for the first case.
The second casefollows very similarly.
Assume c1/(c1 + c2) < ?.
We want to show that for any choice of?
?
[0, 1] such that ?
> ?we havec1 log?+ c2 log(1?
?)
?
c1 log?+ c2 log(1?
?
)Divide both sides of the inequality by c1 + c2 and we get that we need to show thatc1c1 + c2log(?/?
)+c2c1 + c2log(1?
?1?
?)?
0Becausewehave?
> ?, andwealsohave c1/(c1 + c2) < ?, it is sufficient to show that?
log(?/?
)+ (1?
?)
log(1?
?1?
?)?
0 (B.6)Equation (B.6) is precisely the definition of the KL divergence between the distribu-tion of a coinwith probability ?
of heads and the distribution of a coinwith probability?519Computational Linguistics Volume 38, Number 3of heads, and therefore the right side in Equation (B.6) is positive, and we get whatwe need.Appendix C. Counterexample to Tsybakov Noise (Proofs)Lemma 6A = AG(?)
is positive semi-definite for any probabilistic grammar ?G,?
?.ProofLet dk,i be a collection of constants.
Define the random variable:R(z) =?i,kdk,iE[?k,i]?k,i(z)We have thatE[R2]=?i,i??k,k?A(k,i),(k?,i?
)dk,idk?,i?which is always larger or equal to 0.
Therefore, A is positive semi-definite.
Lemma 7Let 0 < ?
< 1/2, c1, c2 ?
0.
Let ?,C > 0.
Also, assume that c1 ?
c2.
For any  > 0, define:a = ?(exp(C1/?
+ /2c1))= ?1?b = ?(exp(?C1/?
+ /2c2))= ?2?t() = c1(1?
?1?
a)+ c2(1?
?1?
b)?
(c1 + c2) exp(/2)Then, for small enough , we have t() ?
0.ProofWe have that t() ?
0 ifac2 + bc1 ?
?
(c1 + c2)(1?
a)(1?
b)1?
?
exp(/2)+ c1 + c2= (c1 + c2)(1?(1?
a)(1?
b)(1?
?)
exp(?/2))(C.1)First, show that(1?
a)(1?
b)(1?
?)
exp(?/2)?
1?
?
(C.2)520Cohen and Smith Empirical Risk Minimization for Probabilistic Grammarswhich happens if (after substituting a = ?1?, b = ?2?)?
?
(?1 + ?2 ?
2)/(1?
?1?2)Note we have ?1?2 > 1 because c1 ?
c2.
In addition, we have ?1 + ?2 ?
2 ?
0 for smallenough  (can be shown by taking the derivative, with respect to  of?1 + ?2 ?
2, whichis always positive for small enough , and in addition, noticing that the value of ?1 +?2 ?
2 is 0 when  = 0.)
Therefore, Equation (C.2) is true.Substituting Equation (C.2) in Equation (C.1), we have that t() ?
0 ifac2 + bc1 ?
(c1 + c2)?which is equivalent toc2?1 + c1?2 ?
c1 + c2 (C.3)Taking again the derivative of the left side of Equation (C.3), we have that it is anincreasing function of  (if c1 ?
c2), and in addition at  = 0 it obtains the value c1 + c2.Therefore, Equation (C.3) holds, and therefore t() ?
0 for small enough .
Theorem 5Let G be a grammar with K ?
2 and degree 2.
Assume that p is ?G,???
for some ?
?, suchthat ?
?1,1 = ?
?2,1 = ?
and that c1 ?
c2.
If AG(??)
is positive definite, then p does not satisfythe Tsybakov noise condition for any (C,?
), where C > 0 and ?
?
1.ProofDefine ?
to be the eigenvalue of AG(?)
with the smallest value (?
is positive).
Also,define v(?)
to be a vector indexed by k, i such thatvk,i(?)
= E[?k,i]log?
?k,i?k,i.Simple algebra shows that for any h ?
H(G) (and the fact that p ?
H(G)), we haveEp(h) = DKL(p?h) =K?k=1(Ep[?k,1]log?
?k,1?k,1+ Ep[?k,1]log(1?
??k,11?
?k,1))For a C > 0 and ?
?
1, define ?
= C1/?.
Let  < ?.
First, we construct an h suchthat DKL(p?h) < + /2 but dist(p, h) > C1/?
as  ?
0.
The construction follows.Parametrize h by ?
such that ?
is identical to ??
except for k = 1, 2, in which case wehave?1,1 = ?
?1,1(exp(?+ /2c1))= ?
(exp(?+ /2c1))(C.4)?2,1 = ??2,1(exp(?
?+ /2c2))= ?(exp(?
?+ /2c2))(C.5)521Computational Linguistics Volume 38, Number 3Note that ?
?
?1,1 ?
1/2 and ?2,1 < ?.
Then, we have thatDKL(p?h) =K?k=1(Ep[?k,1]log?
?k,1?k,1+ Ep[?k,1]log(1?
??k,11?
?k,1))= + c1 log1?
??k,11?
?1,1+ c2 log1?
??k,21?
?2,1= + c1 log1?
?1?
?1,1+ c2 log1?
?1?
?2,1We also havec1 log1?
?1?
?1,1+ c2 log1?
?1?
?2,1?
0 (C.6)ifc1 ?1?
?1?
?1,1+ c2 ?1?
?1?
?2,1?
c1 + c2 (C.7)(This can be shown by dividing Equation [C.6] by c1 + c2 and then using the concavity ofthe logarithm function.)
From Lemma 7, we have that Equation (C.7) holds.
Therefore,DKL(p?h) ?
2Now, consider the following, which can be shown through algebraic manipulation:dist(p, h) = E[(logph)2]=?k,k?
?i,i?E[?k,i ??k?,i?](log??k,i?k,i)(log??k?,i??k?,i?
)Then, additional algebraic simplification shows thatE[(logph)2]= v(?)Av(?
)A fact from linear algebra states thatv(?)Av(?
) ?
?||v(?
)||22where ?
is the smallest eigenvalue in A.
From the construction of ?
and Equation (C.4)?
(C.5), we have that ||v(?
)||22 > ?2.
Therefore,E[(logph)2]?
?
?2which means dist(p, h) ???C1/?.
Therefore, p does not satisfy the Tsybakov noisecondition with parameters (D,?)
for any D > 0.
522Cohen and Smith Empirical Risk Minimization for Probabilistic GrammarsAppendix D. NotationTable D.1 gives a table of notation for symbols used throughout this article.Table 1Table of notation symbols used in this article.Symbol Description 1st MentionERMX Instance space (natural language sentences) Sec.
2Z Output space (grammar derivations) Sec.
2p Distribution generating the data Sec.
2Q Concept space, a family of distributions Sec.
2q An estimated distribution Sec.
2qopt Risk minimizer Eq.
1n Number of available samples Sec.
2p?n Empirical distribution Sec.
2q?
Empirical risk minimizer Eq.
2Ep(q;Q) Excess risk Eq.
4Rn(Q) Empirical process for the log-loss Eq.
5GrammarsG Grammar (for example, CFG rules) Sec.
3?
Probabilistic grammar parameters Sec.
3K Number of multinomials in the probabilistic grammar Eq.
11Nk Size of the kth multinomial of the probabilistic grammar Eq.
11N?Kk=1Nk Sec.
3x Sentence in the language of the grammar Sec.
3z Derivation in the grammar Sec.
3?k,i(x, z) Count of the ith event firing in the kth multinomial in x and z Eq.
11?G Parameter space for a given probabilistic grammar G Eq.
11?
Parameters for a probabilistic grammar Eq.
11deg(G) The degree of G, maxk Nk Sec.
3Dx(G) The set of derivations for string x Sec.
3H,H(G) Concept space, a set of probabilistic grammars Sec.
3F,F(G) Negated log-concept space, {?
log h | h ?
H(G)} Sec.
3L Constant determining distributional assumption Sec.
3.1q Constant determining distributional assumption Sec.
3.1r Constant determining distributional assumption Sec.
3.1ProperApproximationsFn Element n in a proper approximation (contained in F) Sec.
4tail(n) Convergence rate for the boundedness property Sec.
4bound(n) Convergence rate for the tightness property Sec.
4Cn( f ) A map for f ?
F to f ?
?
Fn Sec.
4T(?,?)
Parameters ?with shifted probabilities Sec.
4.1T( f,?)
f ?
F with shifted probabilities Sec.
4.1?G(?)
Set of parameters {T(?,?)
| ?
?
?G} for a given G Sec.
4.1s A constant larger than 1 on which boundedness propertydependsSec.
4.1?
(L, q, p,N) A constant on which sample complexity depends for the su-pervised caseProp.
2F?n Element n in a proper approximation (contained in F) Sec.
4C?n( f ) A map for f ?
F to f?
?
Fn Sec.
4?tail(n) Convergence rate for the soundness property Sec.
4?bound(n) Convergence rate for the tightness property Sec.
4??
(L, q, p,N) A constant on which sample complexity depends for theunsupervised caseSec.
5.3523Computational Linguistics Volume 38, Number 3AcknowledgmentsThe authors thank the anonymous reviewersfor their comments and Avrim Blum, SteveHanneke, Mark Johnson, John Lafferty, DanRoth, and Eric Xing for useful conversations.This research was supported by NationalScience Foundation grant IIS-0915187.ReferencesAbe, N., J. Takeuchi, and M. Warmuth.1991.
Polynomial learnabilityof probabilistic concepts withrespect to the Kullback-Leiberdivergence.
In Proceedings of theConference on Learning Theory,pages 277?289.Abe, N. and M. Warmuth.
1992.
Onthe computational complexity ofapproximating distributions byprobabilistic automata.MachineLearning, 2:205?260.Angluin, D. 1987.
Learning regular sets fromqueries and counterexamples.
Informationand Computation, 75:87?106.Anthony, M. and P. L. Bartlett.
1999.Neural Network Learning: TheoreticalFoundations.
CambridgeUniversity Press.Balcan, M. and A. Blum.
2010.A discriminative model for semi-supervised learning.
Journal of theAssociation for Computing Machinery,57(3):1?46.Balle, B., A. Quattoni, and X. Carreras.2011.
A spectral learning algorithm forfinite state transducers.
In Proceedingsof the European Conference on MachineLearning/the Principles and Practice ofKnowledge Discovery in Databases,pages 156?171.Bane, M., J. Riggle, and M. Sonderegger.2010.
The VC dimension ofconstraint-based grammars.Lingua, 120(5):1194?1208.Bartlett, P., O. Bousquet, and S. Mendelson.2005.
Local Rademacher complexities.Annals of Statistics, 33(4):1497?1537.Bishop, C. M. 2006.
Pattern Recognition andMachine Learning.
Springer, Berlin.Boyd, S. and L. Vandenberghe.
2004.Convex Optimization.
CambridgeUniversity Press.Carrasco, R. 1997.
Accurate computationof the relative entropy betweenstochastic regular grammars.Theoretical Informatics and Applications,31(5):437?444.Carroll, G. and E. Charniak.
1992.
Twoexperiments on learning probabilisticdependency grammars from corpora.Technical report, Brown University,Providence, RI.Charniak, E. 1993.
Statistical LanguageLearning.
MIT Press, Cambridge, MA.Charniak, E. and M. Johnson.
2005.Coarse-to-fine n-best parsing and maxentdiscriminative reranking.
In Proceedings ofthe Association for Computational Linguistics,pages 173?180.Chi, Z.
1999.
Statistical properties ofprobabilistic context-free grammars.Computational Linguistics, 25(1):131?160.Clark, A., R. Eyraud, and A. Habrard.
2008.A polynomial algorithm for the inferenceof context free languages.
In Proceedings ofthe International Colloquium on GrammaticalInference, pages 29?42.Clark, A. and S. Lappin.
2010.
Unsupervisedlearning and grammar induction.In Alexander Clark, Chris Fox, andShalom Lappin, editors, The Handbookof Computational Linguistics and NaturalLanguage Processing.
Wiley-Blackwell,London, pages 197?220.Clark, A. and F. Thollard.
2004.PAC-learnability of probabilisticdeterministic finite state automata.Journal of Machine Learning Research,5:473?497.Cohen, S. B. and N. A. Smith.
2010a.Covariance in unsupervised learning ofprobabilistic grammars.
Journal of MachineLearning Research, 11:3017?3051.Cohen, S. B. and N. A. Smith.
2010b.Empirical risk minimization withapproximations of probabilisticgrammars.
In Proceedings of theAdvances in Neural InformationProcessing Systems, pages 424?432.Cohen, S. B. and N. A. Smith.
2010c.
Viterbitraining for PCFGs: Hardness results andcompetitiveness of uniform initialization.In Proceedings of the Association forComputational Linguistics, pages 1502?1511.Collins, M. 2003.
Head-driven statisticalmodels for natural language processing.Computational Linguistics, 29:589?637.Collins, M. 2004.
Parameter estimation forstatistical parsing models: Theory andpractice of distribution-free methods.In H. Bunt, J. Carroll, and G. Satta, Text,Speech and Language Technology (NewDevelopments in Parsing Technology).Kluwer, Dordrecht, pages 19?55.Corazza, A. and G. Satta.
2006.
Cross-entropyand estimation of probabilistic context-free524Cohen and Smith Empirical Risk Minimization for Probabilistic Grammarsgrammars.
In Proceedings of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 335?342.Cover, T. M. and J.
A. Thomas.
1991.Elements of Information Theory.
Wiley,London.Dasgupta, S. 1997.
The sample complexityof learning fixed-structure bayesiannetworks.Machine Learning,29(2?3):165?180.de la Higuera, C. 2005.
A bibliographicalstudy of grammatical inference.
PatternRecognition, 38:1332?1348.Dempster, A., N. Laird, and D. Rubin.
1977.Maximum likelihood estimation fromincomplete data via the EM algorithm.Journal of the Royal Statistical Society B,39:1?38.Gildea, D. 2010.
Optimal parsing strategiesfor linear context-free rewriting systems.In Proceedings of the North American Chapterof the Association for ComputationalLinguistics, pages 769?776.Go?mez-Rodr?
?guez, C. and G. Satta.2009.
An optimal-time binarizationalgorithm for linear context-freerewriting systems with fan-out two.In Proceedings of the Association forComputational Linguistics-InternationalJoint Conference on Natural LanguageProcessing, pages 985?993.Grenander, U.
1981.
Abstract Inference.
Wiley,New York.Haussler, D. 1992.
Decision-theoreticgeneralizations of the PAC modelfor neural net and other learningapplications.
Information andComputation, 100:78?150.Hsu, D., S. M. Kakade, and T. Zhang.2009.
A spectral algorithm forlearning hidden Markov models.In Proceedings of the Conference onLearning Theory.Ishigami, Y. and S. Tani.
1993.
TheVC-dimensions of finite automatawith n states.
In Proceedings ofAlgorithmic Learning Theory,pages 328?341.Ishigami, Y. and S. Tani.
1997.VC-dimensions of finite automata andcommutative finite automata with k lettersand n states.
Applied Mathematics,74(3):229?240.Jaeger, H. 1999.
Observable operator modelsfor discrete stochastic time series.
NeuralComputation, 12:1371?1398.Kearns, M. and L. Valiant.
1989.Cryptographic limitations on learningBoolean formulae and finite automata.In Proceedings of the 21st Associationfor Computing Machinery Symposiumon the Theory of Computing,pages 433?444.Kearns, M. J. and U. V. Vazirani.
1994.An Introduction to ComputationalLearning Theory.
MIT Press,Cambridge, MA.Klein, D. and C. D. Manning.
2004.Corpus-based induction of syntacticstructure: Models of dependency andconstituency.
In Proceedings of theAssociation for Computational Linguistics,pages 478?487.Koltchinskii, V. 2006.
Local Rademachercomplexities and oracle inequalitiesin risk minimization.
The Annals ofStatistics, 34(6):2593?2656.Leermakers, R. 1989.
How to cover agrammar.
In Proceedings of the Associationfor Computational Linguistics,pages 135?142.Manning, C. D. and H. Schu?tze.
1999.Foundations of Statistical NaturalLanguage Processing.
MIT Press,Cambridge, MA.Massart, P. 2000.
Some applications ofconcentration inequalities to statistics.Annales de la Faculte?
des Sciences deToulouse, IX(2):245?303.Nijholt, A.
1980.
Context-Free Grammars:Covers, Normal Forms, and Parsing(volume 93 of Lecture Notes inComputer Science).
Springer-Verlag,Berlin.Palmer, N. and P. W. Goldberg.
2007.PAC-learnability of probabilisticdeterministic finite state automatain terms of variation distance.In Proceedings of Algorithmic LearningTheory, pages 157?170.Pereira, F. C. N. and Y. Schabes.
1992.Inside-outside reestimation from partiallybracketed corpora.
In Proceedings of theAssociation for Computational Linguistics,pages 128?135.Pitt, L. 1989.
Inductive inference, DFAs, andcomputational complexity.
Analogical andInductive Inference, 397:18?44.Pollard, D. 1984.
Convergence of StochasticProcesses.
Springer-Verlag, New York.Ron, D. 1995.
Automata Learning and ItsApplications.
Ph.D. thesis, HebrewUniversity of Jerusalem.Ron, D., Y.
Singer, and N. Tishby.
1998.On the learnability and usage of acyclicprobabilistic finite automata.
Journalof Computer and System Sciences,56(2):133?152.525Computational Linguistics Volume 38, Number 3Shalev-Shwartz, S., O. Shamir, K. Sridharan,and N. Srebro.
2009.
Learnability andstability in the general learning setting.In Proceedings of the Conference onLearning Theory.Sipser, M. 2006.
Introduction to the Theory ofComputation, Second Edition.
ThomsonCourse Technology, Boston, MA.Talagrand, M. 1994.
Sharper bounds forGaussian and empirical processes.Annals of Probability, 22:28?76.Terwijn, S. A.
2002.
On the learnability ofhidden Markov models.
In P. Adriaans,H.
Fernow, & M. van Zaane.
GrammaticalInference: Algorithms and Applications(Lecture Notes in Computer Science).Springer, Berlin, pages 344?348.Tsybakov, A.
2004.
Optimal aggregation ofclassifiers in statistical learning.
The Annalsof Statistics, 32(1):135?166.Vapnik, V. N. 1998.
Statistical Learning Theory.Wiley-Interscience, New York.526
