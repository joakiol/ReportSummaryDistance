Proceedings of the 43rd Annual Meeting of the ACL, pages 419?426,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsExtracting Relations with Integrated Information Using Kernel MethodsShubin Zhao               Ralph GrishmanDepartment of Computer ScienceNew York University715 Broadway, 7th Floor, New York, NY 10003shubinz@cs.nyu.edu     grishman@cs.nyu.eduAbstractEntity relation detection is a form of in-formation extraction that finds predefinedrelations between pairs of entities in text.This paper describes a relation detectionapproach that combines clues from differ-ent levels of syntactic processing usingkernel methods.
Information from threedifferent levels of processing is consid-ered: tokenization, sentence parsing anddeep dependency analysis.
Each source ofinformation is represented by kernel func-tions.
Then composite kernels are devel-oped to integrate and extend individualkernels so that processing errors occurringat one level can be overcome by informa-tion from other levels.
We present anevaluation of these methods on the 2004ACE relation detection task, using Sup-port Vector Machines, and show that eachlevel of syntactic processing contributesuseful information for this task.
Whenevaluated on the official test data, our ap-proach produced very competitive ACEvalue scores.
We also compare the SVMwith KNN on different kernels.1 IntroductionInformation extraction subsumes a broad range oftasks, including the extraction of entities, relationsand events from various text sources, such asnewswire documents and broadcast transcripts.One such task, relation detection, finds instancesof predefined relations between pairs of entities,such as a Located-In relation between the entitiesCentre College and Danville, KY in the phraseCentre College in Danville, KY.
The ?entities?
arethe individuals of selected semantic types (such aspeople, organizations, countries, ?)
which are re-ferred to in the text.Prior approaches to this task (Miller et al, 2000;Zelenko et al, 2003) have relied on partial or fullsyntactic analysis.
Syntactic analysis can find rela-tions not readily identified based on sequences oftokens alone.
Even ?deeper?
representations, suchas logical syntactic relations or predicate-argumentstructure, can in principle capture additional gener-alizations and thus lead to the identification of ad-ditional instances of relations.
However, a generalproblem in Natural Language Processing is that asthe processing gets deeper, it becomes less accu-rate.
For instance, the current accuracy of tokeniza-tion, chunking and sentence parsing for English isabout 99%, 92%, and 90% respectively.
Algo-rithms based solely on deeper representations in-evitably suffer from the errors in computing theserepresentations.
On the other hand, low level proc-essing such as tokenization will be more accurate,and may also contain useful information missed bydeep processing of text.
Systems based on a singlelevel of representation are forced to choose be-tween shallower representations, which will havefewer errors, and deeper representations, whichmay be more general.Based on these observations, Zhao et al (2004)proposed a discriminative model to combine in-formation from different syntactic sources using akernel SVM (Support Vector Machine).
Weshowed that adding sentence level word trigramsas global information to local dependency contextboosted the performance of finding slot fillers for419management succession events.
This paper de-scribes an extension of this approach to the identi-fication of entity relations, in which syntacticinformation from sentence tokenization, parsingand deep dependency analysis is combined usingkernel methods.
At each level, kernel functions (orkernels) are developed to represent the syntacticinformation.
Five kernels have been developed forthis task, including two at the surface level, one atthe parsing level and two at the deep dependencylevel.
Our experiments show that each level ofprocessing may contribute useful clues for thistask, including surface information like word bi-grams.
Adding kernels one by one continuouslyimproves performance.
The experiments were car-ried out on the ACE RDR (Relation Detection andRecognition) task with annotated entities.
UsingSVM as a classifier along with the full compositekernel produced the best performance on this task.This paper will also show a comparison of SVMand KNN (k-Nearest-Neighbors) under differentkernel setups.2 Kernel MethodsMany machine learning algorithms involve onlythe dot product of vectors in a feature space, inwhich each vector represents an object in the ob-ject domain.
Kernel methods (Muller et al, 2001)can be seen as a generalization of feature-basedalgorithms, in which the dot product is replaced bya kernel function (or kernel) ?
(X,Y) between twovectors, or even between two objects.
Mathemati-cally, as long as ?
(X,Y) is symmetric and the ker-nel matrix formed by ?
is positive semi-definite, itforms a valid dot product in an implicit Hilbertspace.
In this implicit space, a kernel can be bro-ken down into features, although the dimension ofthe feature space could be infinite.Normal feature-based learning can be imple-mented in kernel functions, but we can do morethan that with kernels.
First, there are many well-known kernels, such as polynomial and radial basiskernels, which extend normal features into a highorder space with very little computational cost.This could make a linearly non-separable problemseparable in the high order feature space.
Second,kernel functions have many nice combinationproperties: for example, the sum or product of ex-isting kernels is a valid kernel.
This forms the basisfor the approach described in this paper.
Withthese combination properties, we can combine in-dividual kernels representing information fromdifferent sources in a principled way.Many classifiers can be used with kernels.
Themost popular ones are SVM, KNN, and voted per-ceptrons.
Support Vector Machines (Vapnik, 1998;Cristianini and Shawe-Taylor, 2000) are linearclassifiers that produce a separating hyperplanewith largest margin.
This property gives it goodgeneralization ability in high-dimensional spaces,making it a good classifier for our approach whereusing all the levels of linguistic clues could resultin a huge number of features.
Given all the levelsof features incorporated in kernels and trainingdata with target examples labeled, an SVM canpick up the features that best separate the targetsfrom other examples, no matter which level thesefeatures are from.
In cases where an error occurs inone processing result (especially deep processing)and the features related to it become noisy, SVMmay pick up clues from other sources which arenot so noisy.
This forms the basic idea of our ap-proach.
Therefore under this scheme we can over-come errors introduced by one processing level;more particularly, we expect accurate low levelinformation to help with less accurate deep levelinformation.3 Related WorkCollins et al (1997) and Miller et al (2000) usedstatistical parsing models to extract relational factsfrom text, which avoided pipeline processing ofdata.
However, their results are essentially basedon the output of sentence parsing, which is a deepprocessing of text.
So their approaches are vulner-able to errors in parsing.
Collins et al (1997) ad-dressed a simplified task within a confined contextin a target sentence.Zelenko et al (2003) described a recursive ker-nel based on shallow parse trees to detect person-affiliation and organization-location relations, inwhich a relation example is the least common sub-tree containing two entity nodes.
The kernelmatches nodes starting from the roots of two sub-trees and going recursively to the leaves.
For eachpair of nodes, a subsequence kernel on their childnodes is invoked, which matches either contiguousor non-contiguous subsequences of node.
Com-pared with full parsing, shallow parsing is morereliable.
But this model is based solely on the out-420put of shallow parsing so it is still vulnerable toirrecoverable parsing errors.
In their experiments,incorrectly parsed sentences were eliminated.Culotta and Sorensen (2004) described a slightlygeneralized version of this kernel based on de-pendency trees.
Since their kernel is a recursivematch from the root of a dependency tree down tothe leaves where the entity nodes reside, a success-ful match of two relation examples requires theirentity nodes to be at the same depth of the tree.This is a strong constraint on the matching of syn-tax so it is not surprising that the model has goodprecision but very low recall.
In their solution abag-of-words kernel was used to compensate forthis problem.
In our approach, more flexible ker-nels are used to capture regularization in syntax,and more levels of syntactic information are con-sidered.Kambhatla (2004) described a Maximum En-tropy model using features from various syntacticsources, but the number of features they used islimited and the selection of features has to be amanual process.1 In our model, we use kernels toincorporate more syntactic information and let aSupport Vector Machine decide which clue is cru-cial.
Some of the kernels are extended to generatehigh order features.
We think a discriminative clas-sifier trained with all the available syntactic fea-tures should do better on the sparse data.4 Kernel Relation Detection4.1 ACE Relation Detection TaskACE (Automatic Content Extraction)2 is a researchand development program in information extrac-tion sponsored by the U.S. Government.
The 2004evaluation defined seven major types of relationsbetween seven types of entities.
The entity typesare PER (Person), ORG (Organization), FAC (Fa-cility), GPE (Geo-Political Entity: countries, cities,etc.
), LOC (Location), WEA (Weapon) and VEH(Vehicle).
Each mention of an entity has a mentiontype: NAM (proper name), NOM (nominal) or1 Kambhatla also evaluated his system on the ACE relationdetection task, but the results are reported for the 2003 task,which used different relations and different training and testdata, and did not use hand-annotated entities, so they cannotbe readily compared to our results.2Task description: http://www.itl.nist.gov/iad/894.01/tests/ace/ACE guidelines: http://www.ldc.upenn.edu/Projects/ACE/PRO (pronoun); for example George W. Bush, thepresident and he respectively.
The seven relationtypes are EMP-ORG (Employ-ment/Membership/Subsidiary), PHYS (Physical),PER-SOC (Personal/Social), GPE-AFF (GPE-Affiliation), Other-AFF (Person/ORG Affiliation),ART (Agent-Artifact) and DISC (Discourse).There are also 27 relation subtypes defined byACE, but this paper only focuses on detection ofrelation types.
Table 1 lists examples of each rela-tion type.Type ExampleEMP-ORG the CEO of MicrosoftPHYS a military base in GermanyGPE-AFF U.S.  businessmanPER-SOC a spokesman for the senatorDISC many of these peopleART the makers of the KurskOther-AFF Cuban-American  peopleTable 1.
ACE relation types and examples.
Theheads of the two entity arguments in a relation aremarked.
Types are listed in decreasing order offrequency of occurrence in the ACE corpus.Figure 1 shows a sample newswire sentence, inwhich three relations are marked.
In this sentence,we expect to find a PHYS relation between Hez-bollah forces and areas, a PHYS relation betweenSyrian troops and areas and an EMP-ORG relationbetween Syrian troops and Syrian.
In our ap-proach, input text is preprocessed by the Charniaksentence parser (including tokenization and POStagging) and the GLARF (Meyers et al, 2001) de-pendency analyzer produced by NYU.
Based ontreebank parsing, GLARF produces labeled deepdependencies between words (syntactic relationssuch as logical subject and logical object).
It han-dles linguistic phenomena like passives, relatives,reduced relatives, conjunctions, etc.Figure 1.
Example sentence from newswire text4.2 DefinitionsIn our model, kernels incorporate information fromPHYS PHYS EMP-ORGThat's because Israel was expected to retaliate againstHezbollah forces in areas controlled by Syrian troops.421tokenization, parsing and deep dependency analy-sis.
A relation candidate R is defined asR = (arg1, arg2, seq, link, path),where arg1 and arg2 are the two entity argumentswhich may be related; seq=(t1, t2, ?, tn) is a tokenvector that covers the arguments and interveningwords; link=(t1, t2, ?, tm) is also a token vector,generated from seq and the parse tree; path is adependency path connecting arg1 and arg2 in thedependency graph produced by GLARF.
path canbe empty if no such dependency path exists.
Thedifference between link and seq is that link onlyretains the ?important?
words in seq in terms ofsyntax.
For example, all noun phrases occurring inseq are replaced by their heads.
Words and con-stituent types in a stop list, such as time expres-sions, are also removed.A token T is defined as a string triple,T = (word, pos, base),where word, pos and base are strings representingthe word, part-of-speech and morphological baseform of T. Entity is a token augmented with otherattributes,E = (tk, type, subtype, mtype),where tk is the token associated with E; type, sub-type and mtype are strings representing the entitytype, subtype and mention type of E. The subtypecontains more specific information about an entity.For example, for a GPE entity, the subtype tellswhether it is a country name, city name and so on.Mention type includes NAM, NOM and PRO.It is worth pointing out that we always treat anentity as a single token: for a nominal, it refers toits head, such as boys in the two boys; for a propername, all the words are connected into one token,such as Bashar_Assad.
So in a relation example Rwhose seq is (t1, t2, ?, tn), it is always true thatarg1=t1 and arg2=tn.
For names, the base form ofan entity is its ACE type (person, organization,etc.).
To introduce dependencies, we define a de-pendency token to be a token augmented with avector of dependency arcs,DT=(word, pos, base, dseq),where dseq = (arc1, ... , arcn ).
A dependency arc isARC = (w, dw, label, e),where w is the current token; dw is a token con-nected by a dependency to w; and label and e arethe role label and direction of this dependency arcrespectively.
From now on we upgrade the type oftk in arg1 and arg2 to be dependency tokens.
Fi-nally, path is a vector of dependency arcs,path = (arc1 , ... , arcl ),where l is the length of the path and arci (1?i?l)satisfies arc1.w=arg1.tk, arci+1.w=arci.dw andarcl.dw=arg2.tk.
So path is a chain of dependenciesconnecting the two arguments in R. The arcs in itdo not have to be in the same direction.Figure 2.
Illustration of a relation example R. Thelink sequence is generated from seq by removingsome unimportant words based on syntax.
The de-pendency links are generated by GLARF.Figure 2 shows a relation example generated fromthe text ??
in areas controlled by Syrian troops?.In this relation example R, arg1 is ((?areas?,?NNS?, ?area?, dseq), ?LOC?, ?Region?,?NOM?
), and arg1.dseq is ((OBJ, areas, in, 1),(OBJ, areas, controlled, 1)).
arg2 is ((?troops?,?NNS?, ?troop?, dseq), ?ORG?, ?Government?,?NOM?)
and arg2.dseq = ((A-POS, troops, Syrian,0), (SBJ, troops, controlled, 1)).
path is ((OBJ, ar-eas, controlled, 1), (SBJ, controlled, troops, 0)).The value 0 in a dependency arc indicates forwarddirection from w to dw, and 1 indicates backwarddirection.
The seq and link sequences of R areshown in Figure 2.Some relations occur only between very restrictedtypes of entities, but this is not true for every typeof relation.
For example, PER-SOC is a relationmainly between two person entities, while PHYScan happen between any type of entity and a GPEor LOC entity.4.3 Syntactic KernelsIn this section we will describe the kernels de-signed for different syntactic sources and explainthe intuition behind them.We define two kernels to match relation examplesat surface level.
Using the notation just defined, wecan write the two surface kernels as follows:1) Argument kerneltroopsareas controlled byA-POS OBJarg1 arg2 SBJOBJpathinseqlinkareas controlled by Syrian troopsCOMP422where KE is a kernel that matches two entities,KT is a kernel that matches two tokens.
I(x, y) is abinary string match operator that gives 1 if x=yand 0 otherwise.
Kernel ?1 matches attributes oftwo entity arguments respectively, such as type,subtype and lexical head of an entity.
This is basedon the observation that there are type constraintson the two arguments.
For instance PER-SOC is arelation mostly between two person entities.
So theattributes of the entities are crucial clues.
Lexicalinformation is also important to distinguish relationtypes.
For instance, in the phrase U.S. presidentthere is an EMP-ORG relation between presidentand U.S., while in a U.S. businessman there is aGPE-AFF relation between businessman and U.S.2)  Bigram kernelwhereOperator <t1, t2> concatenates all the string ele-ments in tokens t1 and t2 to produce a new token.So ?2 is a kernel that simply matches unigrams andbigrams between the seq sequences of two relationexamples.
The information this kernel provides isfaithful to the text.3) Link sequence kernelwhere min_len is the length of the shorter link se-quence in R1 and R2.
?3 is a kernel that matchestoken by token between the link sequences of tworelation examples.
Since relations often occur in ashort context, we expect many of them have simi-lar link sequences.4) Dependency path kernelwhere).',.()).',.
( earcearcIdwarcdwarcK jijiT ?Intuitively the dependency path connecting twoarguments could provide a high level of syntacticregularization.
However, a complete match of twodependency paths is rare.
So this kernel matchesthe component arcs in two dependency paths in apairwise fashion.
Two arcs can match only whenthey are in the same direction.
In cases where twopaths do not match exactly, this kernel can still tellus how similar they are.
In our experiments weplaced an upper bound on the length of depend-ency paths for which we computed a non-zero ker-nel.5) Local dependencywhere).',.()).',.
( earcearcIdwarcdwarcK jijiT ?This kernel matches the local dependency contextaround the relation arguments.
This can be helpfulespecially when the dependency path between ar-guments does not exist.
We also hope the depend-encies on each argument may provide some usefulclues about the entity or connection of the entity tothe context outside of the relation example.4.4 Composite KernelsHaving defined all the kernels representing shallowand deep processing results, we can define com-posite kernels to combine and extend the individ-ual kernels.1) Polynomial extensionThis kernel combines the argument kernel ?1 andlink kernel ?3 and applies a second-degree poly-nomial kernel to extend them.
The combination of?1 and ?3 covers the most important clues for thistask: information about the two arguments and theword link between them.
The polynomial exten-sion is equivalent to adding pairs of features as),arg.,arg.
(),( 212,1211 iiiE RRKRR ?==?++= ).,.().,.
(),( 212121 typeEtypeEItkEtkEKEEK TE).,.().,.
( 2121 mtypeEmtypeEIsubtypeEsubtypeEI ++= ).,.
(),( 2121 wordTwordTITTKT).,.().,.
( 2121 baseTbaseTIposTposTI +),.,.
(),( 21212 seqRseqRKRR seq=??
?<?
<?+=lenseqi lenseqjjiTseq tktkKseqseqK.0 .
'0)',(('),())',',,( 11 ><>< ++ jjiiT tktktktkK).,.
(),( 21213 linkRlinkRKRR link=?,)..,..
( 21min_0iileniT ktlinkRktlinkRK?<?=),.,.
(),( 21214 pathRpathRKRR path=?
)',( pathpathK path?
?<?
<?+=lenpathi lenpathjji labelarclabelarcI.0 .'0).',.(((,).arg.,.arg.
(),(2,121215 ?==iiiD dseqRdseqRKRR?
)',( dseqdseqK D?
?<?
<?+=lendseqi lendseqjji labelarclabelarcI.0 .'0).',.
((4/)()(),( 23131211 ????
+++=?
RR423new features.
Intuitively this introduces new fea-tures like: the subtype of the first argument is acountry name and the word of the second argumentis president, which could be a good clue for anEMP-ORG relation.
The polynomial kernel isdown weighted by a normalization factor becausewe do not want the high order features to over-whelm the original ones.
In our experiment, usingpolynomial kernels with degree higher than 2 doesnot produce better results.2) Full kernelThis is the final kernel we used for this task, whichis a combination of all the previous kernels.
In ourexperiments, we set al the scalar factors to 1.
Dif-ferent values were tried, but keeping the originalweight for each kernel yielded the best results forthis task.All the individual kernels we designed are ex-plicit.
Each kernel can be seen as a matching offeatures and these features are enumerable on thegiven data.
So it is clear that they are all valid ker-nels.
Since the kernel function set is closed underlinear combination and polynomial extension, thecomposite kernels are also valid.
The reason wepropose to use a feature-based kernel is that we canhave a clear idea of what syntactic clues it repre-sents and what kind of information it misses.
Thisis important when developing or refining kernels,so that we can make them generate complementaryinformation from different syntactic processingresults.5 ExperimentsExperiments were carried out on the ACE RDR(Relation Detection and Recognition) task usinghand-annotated entities, provided as part of theACE evaluation.
The ACE corpora contain docu-ments from two sources: newswire (nwire) docu-ments and broadcast news transcripts (bnews).
Inthis section we will compare performance of dif-ferent kernel setups trained with SVM, as well asdifferent classifiers, KNN and SVM, with the samekernel setup.
The SVM package we used isSVMlight.
The training parameters were chosen us-ing cross-validation.
One-against-all classificationwas applied to each pair of entities in a sentence.When SVM predictions conflict on a relation ex-ample, the one with larger margin will be selectedas the final answer.5.1 CorpusThe ACE RDR training data contains 348 docu-ments, 125K words and 4400 relations.
It consistsof both nwire and bnews documents.
Evaluation ofkernels was done on the training data using 5-foldcross-validation.
We also evaluated the full kernelsetup with SVM on the official test data, which isabout half the size of the training data.
All the datais preprocessed by the Charniak parser andGLARF dependency analyzer.
Then relation ex-amples are generated based these results.5.2 ResultsTable 2 shows the performance of the SVM ondifferent kernel setups.
The kernel setups in thisexperiment are incremental.
From this table we cansee that adding kernels continuously improves theperformance, which indicates they provideadditional clues to the previous setup.
The argu-ment kernel treats the two arguments asindependent entities.
The link sequence kernelintroduces the syntactic connection betweenarguments, so adding it to the argument kernelboosted the performance.
Setup F shows theperformance of adding only dependency kernels tothe argument kernel.
The performance is not asgood as setup B, indicating that dependencyinformation alone is not as crucial as the linksequence.Kernel           Performance   prec       recall    F-scoreA Argument (?1) 52.96%    58.47%   55.58%B A + link (?1+?3) 58.77%    71.25%   64.41%*C B-poly (?1) 66.98%    70.33%   68.61%*D C + dep (?1+?4+?5) 69.10%    71.41%   70.23%*E D + bigram (?2) 69.23%    70.50%   70.35%F A + dep (?1+?4+?5) 57.86%    68.50%   62.73%Table 2.
SVM performance on incremental kernelsetups.
Each setup adds one level of kernels to theprevious one except setup F. Evaluated on theACE training data with 5-fold cross-validation.
F-scores marked by * are significantly better than theprevious setup (at 95% confidence level).2541212 ),( ??????
+++?=?
RR424Another observation is that adding the bigramkernel in the presence of all other level of kernelsimproved both precision and recall, indicating thatit helped in both correcting errors in otherprocessing results and providing supplementaryinformation missed by other levels of analysis.
Inanother experiment evaluated on the nwire dataonly (about half of the training data), adding thebigram kernel improved F-score 0.5% and thisimprovement is statistically significant.Type KNN (?1+?3) KNN (?2) SVM (?2)EMP-ORG 75.43% 72.66% 77.76%PHYS 62.19 % 61.97% 66.37%GPE-AFF 58.67% 56.22% 62.13%PER-SOC 65.11% 65.61% 73.46%DISC 68.20% 62.91% 66.24%ART 69.59% 68.65% 67.68%Other-AFF 51.05% 55.20% 46.55%Total 67.44% 65.69% 70.35%Table 3.
Performance of SVM and KNN (k=3) ondifferent kernel setups.
Types are ordered in de-creasing order of frequency of occurrence in theACE corpus.
In SVM training, the sameparameters were used for all 7 types.Table 3 shows the performance of SVM andKNN (k Nearest Neighbors) on different kernelsetups.
For KNN, k was set to 3.
In the first setupof KNN, the two kernels which seem to containmost of the important information are used.
Itperforms quite well when compared with the SVMresult.
The other two tests are based on the fullkernel setup.
For the two KNN experiments,adding more kernels (features) does not help.
Thereason might be that all kernels (features) wereweighted equally in the composite kernel ?2 andthis may not be optimal for KNN.
Another reasonis that the polynomial extension of kernels does nothave any benefit in KNN because it is a monotonictransformation of similarity values.
So the resultsof KNN on kernel (?1+?3) and ?1 would be ex-actly the same.
We also tried different k for KNNand k=3 seems to be the best choice in either case.For the four major types of relations SVM doesbetter than KNN, probably due to SVM?sgeneralization ability in the presence of largenumbers of features.
For the last three types withmany fewer examples, performance of SVM is notas good as KNN.
The reason we think is thattraining of SVM on these types is not sufficient.We tried different training parameters for the typeswith fewer examples, but no dramaticimprovement obtained.We also evaluated our approach on the officialACE RDR test data and obtained very competitivescores.3 The primary scoring metric4 for the ACEevaluation is a 'value' score, which is computed bydeducting from 100 a penalty for each missing andspurious relation; the penalty depends on the typesof the arguments to the relation.
The value scoresproduced by the ACE scorer for nwire and bnewstest data are 71.7 and 68.0 repectively.
The valuescore on all data is 70.1.5 The scorer also reports anF-score based on full or partial match of relationsto the keys.
The unweighted F-score for this testproduced by the ACE scorer on all data is 76.0%.For this evaluation we used nearest neighbor todetermine argument ordering and relationsubtypes.The classification scheme in our experiments isone-against-all.
It turned out there is not so muchconfusion between relation types.
The confusionmatrix of predictions is fairly clean.
We also triedpairwise classification, and it did not help much.6 DiscussionIn this paper, we have shown that using kernels tocombine information from different syntacticsources performed well on the entity relationdetection task.
Our experiments show that eachlevel of syntactic processing contains usefulinformation for the task.
Combining them mayprovide complementary information to overcomeerrors arising from linguistic analysis.
Especially,low level information obtained with high reliabilityhelped with the other deep processing results.
Thisdesign feature of our approach should be bestemployed when the preprocessing errors at eachlevel are independent, namely when there is nodependency between the preprocessing modules.The model was tested on text with annotatedentities, but its design is generic.
It can work with3 As ACE participants, we are bound by the participationagreement not to disclose other sites?
scores, so no directcomparison can be provided.4 http://www.nist.gov/speech/tests/ace/ace04/software.htm5 No comparable inter-annotator agreement scores are avail-able for this task, with pre-defined entities.
However, theagreement scores across multiple sites for similar relationtagging tasks done in early 2005, using the value metric,ranged from about 0.70 to 0.80.425noisy entity detection input from an automatictagger.
With all the existing information from otherprocessing levels, this model can be also expectedto recover from errors in entity tagging.7 Further WorkKernel functions have many nice properties.
Thereare also many well known kernels, such as radialbasis kernels, which have proven successful inother areas.
In the work described here, only linearcombinations and polynomial extensions of kernelshave been evaluated.
We can explore other kernelproperties to integrate the existing syntactickernels.
In another direction, training data is oftensparse for IE tasks.
String matching is notsufficient to capture semantic similarity of words.One solution is to use general purpose corpora tocreate clusters of similar words; another option isto use available resources like WordNet.
Theseword similarities can be readily incorporated intothe kernel framework.
To deal with sparse data,we can also use deeper text analysis to capturemore regularities from the data.
Such analysis maybe based on newly-annotated corpora likePropBank (Kingsbury and Palmer, 2002) at theUniversity of Pennsylvania and NomBank (Meyerset al, 2004) at New York University.
Analyzersbased on these resources can generate regularizedsemantic representations for lexically orsyntactically related sentence structures.
Althoughdeeper analysis may even be less accurate, ourframework is designed to handle this and stillobtain some improvement in performance.8 AcknowledgementThis research was supported in part by the DefenseAdvanced Research Projects Agency under GrantN66001-04-1-8920 from SPAWAR San Diego,and by the National Science Foundation underGrant ITS-0325657.
This paper does not necessar-ily reflect the position of the U.S. Government.
Wewish to thank Adam Meyers of the NYU NLPgroup for his help in producing deep dependencyanalyses.ReferencesM.
Collins and S. Miller.
1997.
Semantic tagging usinga probabilistic context free grammar.
In Proceedingsof the 6th Workshop on Very Large Corpora.N.
Cristianini and J. Shawe-Taylor.
2000.
An introduc-tion to support vector machines.
Cambridge Univer-sity Press.A.
Culotta and J. Sorensen.
2004.
Dependency TreeKernels for Relation Extraction.
In Proceedings ofthe 42nd Annual Meeting of the Association forComputational Linguistics.D.
Gildea and M. Palmer.
2002.
The Necessity of Pars-ing for Predicate Argument Recognition.
In Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics.N.
Kambhatla.
2004.
Combining Lexical, Syntactic, andSemantic Features with Maximum Entropy Modelsfor Extracting Relations.
In Proceedings of the 42ndAnnual Meeting of the Association for Computa-tional Linguistics.P.
Kingsbury and M. Palmer.
2002.
From treebank topropbank.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002).C.
D. Manning and H. Schutze 2002.
Foundations ofStatistical Natural Language Processing.
The MITPress, page 454-455.A.
Meyers, R. Grishman, M. Kosaka and S. Zhao.
2001.Covering Treebanks with GLARF.
In Proceedings ofthe 39th Annual Meeting of the Association forComputational Linguistics.A.
Meyers, R. Reeves, Catherine Macleod, RachelSzekeley, Veronkia Zielinska, Brian Young, and R.Grishman.
2004.
The Cross-Breeding of Dictionar-ies.
In Proceedings of the 5th International Confer-ence on Language Resources and Evaluation (LREC-2004).S.
Miller, H. Fox, L. Ramshaw, and R. Weischedel.2000.
A novel use of statistical parsing to extract in-formation from text.
In 6th Applied Natural Lan-guage Processing Conference.K.-R. M?ller, S. Mika, G. Ratsch, K. Tsuda and B.Scholkopf.
2001.
An introduction to kernel-basedlearning algorithms, IEEE Trans.
Neural Networks,12, 2, pages 181-201.V.
N. Vapnik.
1998.
Statistical Learning Theory.
Wiley-Interscience Publication.D.
Zelenko, C. Aone and A. Richardella.
2003.
Kernelmethods for relation extraction.
Journal of MachineLearning Research.Shubin Zhao, Adam Meyers, Ralph Grishman.
2004.Discriminative Slot Detection Using Kernel Methods.In the Proceedings of the 20th International Confer-ence on Computational Linguistics.426
