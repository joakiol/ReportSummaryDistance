Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ?13), pages 20?28,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsLearning Hierarchical Linguistic Descriptions of Visual DatasetsRoni Mittelman?, Min Sun?, Benjamin Kuipers?, Silvio Savarese??
Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor?
Department of Computer Science and Engineering, University of Washington, Seatlermittelm,kuipers,silvio@umich.edu, sunmin@cs.washington.eduAbstractWe propose a method to learn succinct hi-erarchical linguistic descriptions of visualdatasets, which allow for improved navigationefficiency in image collections.
Classic ex-ploratory data analysis methods, such as ag-glomerative hierarchical clustering, only pro-vide a means of obtaining a tree-structuredpartitioning of the data.
This requires the userto go through the images first, in order to re-veal the semantic relationship between the dif-ferent nodes.
On the other hand, in this workwe propose to learn a hierarchy of linguisticdescriptions, referred to as attributes, whichallows for a textual description of the seman-tic content that is captured by the hierarchy.Our approach is based on a generative model,which relates the attribute descriptions asso-ciated with each node, and the node assign-ments of the data instances, in a probabilisticfashion.
We furthermore use a nonparametricBayesian prior, known as the tree-structuredstick breaking process, which allows for thestructure of the tree to be learned in an unsu-pervised fashion.
We also propose appropriateperformance measures, and demonstrate supe-rior performance compared to other hierarchi-cal clustering algorithms.1 IntroductionWith the abundance of images available both for per-sonal use and in large internet based datasets, suchas Flickr and Google Image Search, hierarchies ofimages are an important tool that allows for conve-nient browsing and efficient search and retrieval.
In-tuitively, desirable hierarchies should capture simi-larity in a semantic space, i.e.
nearby nodes shouldinclude categories which are semantically more sim-ilar, as compared to nodes which are more distant.Recent works that are concerned with learning im-age hierarchies (Bart et al 2011; Sivic et al 2008),have relied on a bag of visual-words feature space,and therefore have been shown to provide unsatis-factory results with respect to the latter requirement(Li et al 2010).A recent trend in visual recognition systems, hasbeen to shift from using a low-level feature basedrepresentation to an attribute based feature space,which can capture higher level semantics (Farhadiet al 2009; Lampert et al 2009; Parikh & Grau-man, 2011; Berg et al 2011; Ferrari & Zisserman,2007).
Attributes are detectors that are trained usingannotation data, to identify particular properties inan instance image.
By evaluating these detectors ona query image, one can obtain a linguistic descrip-tion of the image.
Therefore, learning a visual hier-archy based on an attribute representation can allowfor an improved semantic grouping, as compared tothe previous use of low-level image features.In this work we wish to utilize an attribute basedrepresentation to learn a hierarchical linguistic de-scription of a visual dataset, in which few attributesare associated with each node of the tree.
As is il-lustrated in Figure 1, such an attribute hierarchy istightly related to a category hierarchy, in which theinstances associated with every node are describedusing all the attributes associated with all the nodesalong the path leading up to the root node (the in-stances in Figure 1 are described by the correspond-ing photographs).
This ?duality?
between the at-20!"#$%&'()*+#$!
'##$,'-.#//$01-2+34$%5(6#$ 7#"(.5#/$8(29-:;$$<#'-9=2<>(.$/+'*.+*'#$ ,#9<5;$:"##5$?#+<5$8--9$8"##5$??
1"<55#2@#/A$B..5*/(-2;$C-(/=$<&'()*+#$/.-'#/3$Figure 1: Attribute and category hierarchies.tribute and category hierarchies, offers an impor-tant advantage when characterizing the dataset to theend-user, since it eliminates the need to visually in-spect the images assigned to each node, in order toreveal the semantic relationship between the cate-gories that are associated with the different nodes.Exploratory data analysis methods for learning hier-archies, such as agglomerative hierarchical cluster-ing (AHC) (Jain & Dubes, 1988 p. 59), only assigninstances to different nodes in the tree, whereas ourapproach learns an attribute hierarchy which is usedto assign the instances to the nodes of the tree.
Theattribute hierarchy provides a linguistic descriptionof a category hierarchy.We develop a generative model, which we referto as the attribute tree process (ATP), and which tiestogether the attribute and category hierarchies in aprobabilistic fashion.
The tree structure is learnedby incorporating a nonparametric Bayesian prior,known as the tree-structured stick breaking process(TSSBP) (Adams et al 2010), in the probabilis-tic formulation.
An important observation whichwe make about the attribute hierarchies which arelearned using the ATP, is that attributes which arerelated to more image instances tend to be associ-ated with nodes which are closer to the root, and viceversa, attributes which are associated with fewer in-stances tend to be associated with leaf nodes.
A hi-erarchical clustering algorithm that is based on theTSSBP for binary feature vectors was developed in(Adams et al 2010), and is known as the factoredBernoulli likelihood model (FBLM).
However, sim-ilarly to AHC, it does not produce the attribute hier-archy in which we are interested.In order to evaluate the ATP quantitatively, wecompare its performance to other hierarchical clus-tering algorithms.
If the ground truth of the categoryhierarchy is available, we propose to use the seman-tic distance between the categories, that is given bythe ground truth hierarchy, to evaluate the degree towhich the semantic distance between the categoriesis preserved by the hierarchical clustering algorithm.If the ground truth is not available, we use two cri-teria, which as we argue, capture the properties thatshould be demonstrated by desirable semantic hier-archies.
The first is the ?purity criterion?
(Manninget al 2009 p. 357) which measures the degree towhich each node is occupied by instances from asingle class, and the second is the ?locality criterion?which we propose, and which measures the degreeto which instances from the same class are assignedto nearby nodes in the hierarchy.
Our experimen-tal results show that when compared to AHC andFBLM, our approach captures the ground truth se-mantic distance between the categories more accu-rately, and without significant dependence on hyper-parameters.The remaining of this paper is organized as fol-lows.
In Sec.
2 we provide background on agglom-erative hierarchical clustering, and on the TSSBP,and in Sec.
3 we develop the generative model forthe ATP.
In Sec.
4 we propose evaluation metrics forthe attribute hierarchy, and in Sec.
5 we present theexperimental results.
Sec.
6 concludes this paper.2 Background2.1 Agglomerative hierarchical clusteringAHC uses a bottom up approach to clustering.
Inthe first iteration, each cluster includes a single in-stance of the dataset, and at each following iteration,the two clusters which are closest to each other arejoined into a single cluster.
This requires a distancemetric, which measures the distance between clus-ters, to be defined.
The algorithm concludes whenthe distance between the farthest clusters is smallerthan some threshold.2.2 Tree structured stick breaking processThe TSSBP is an infinite mixture model, where eachmixture component has one-to-one correspondencewith one of the nodes of an infinitely branching andinfinitely deep tree.
The weights of the infinite mix-ture model are generated by interleaving two stick-breaking processes (Teh et al 2006), which allowsthe number of mixture components to be inferred21zix i N!!
0!1 !
2 !
3!1!1 !1!2 !2!1 !
3 !1 !
3 !2(a) TSSBPziyi N!
!0!1 !
2 !
3!1 !1 !1!2 !2!1 !
3 !1 !
3 !2 !x i(b) ATPFigure 2: The graphical model representations of the probability distribution functions for the (a) TSSBP, and (b) ATP(ours).
The parameter ? is associated with node  in the tree, and T denotes the parameters {pi}?T from the TSSBPconstruction, where T is the set of all the nodes in the tree.from the data in a Bayesian fashion.
Since each mix-ture component is associated with a unique node inthe infinite tree, this is equivalent to inferring thestructure of the tree from the data.
Let T denotethe infinite set of node indices, and let pi denote thecorresponding weight of the mixture component as-sociated with node  ?
T , then one can sample anode z in the tree usingz ?
??Tpi?(z), (1)where ?() denotes a Dirac delta function at .Since the cardinality of the set T is unbounded,sampling from (1) is not trivial, however, an effi-cient sampling scheme was presented in (Adams etal., 2010).
Similarly, an efficient scheme for sam-pling from the posterior of {pi}?T given the nodeassignments of all the data instances, was also de-veloped in (Adams et al 2010).2.2.1 Factored Bernoulli likelihood modelThe FBLM was used in (Adams et al 2010)to perform hierarchical clustering of color imagesusing binary feature vectors.
Let xi ?
{0, 1}D,i = 1, .
.
.
, N , denote a set of binary training vec-tors that are available for learning the hierarchy.The graphical model representation of the proba-bility distribution function is shown in Figure 2a,where for the sake of clarity of the exposition, thetree that is shown here is finite.
The parameters? = [?
(1) , .
.
.
, ?
(D) ]T satisfy?
(d) = ?
(d)Pa() + n(d) , d = 1, .
.
.
, D, (2)where n(d) ?
N (0, ?2), and Pa() denotes the par-ent of node .
The indicator variable z is sampledusing (1), and the likelihood of a binary observationvector x = [x(1), .
.
.
, x(D)]T follows a Bernoullidistribution whose parameter is a logistic functionf(x|?z) =D?d=1(1+ exp {??
(d)z })?x(d)?
(1 + exp {?
(d)z })?(1?x(d)).
(3)3 The attribute tree processIn this section, we develop a new generative modelthat is based on the TSSBP, however unlike theFBLM it also reveals a hierarchy of attributes, whichallows for a linguistic description of the imagedataset.
This is achieved by relating the attribute hi-erarchy, and the assignment of image instances tonodes, in a probabilistic manner.3.1 The attribute hierarchyIn order to allow for a probabilistic description of theattribute hierarchy, we associate a parameter vector?
= [?
(1) , .
.
.
, ?
(D) ]T with each node  ?
T , whereD denotes the number of attributes.
The attributesy(d)i , d = 1, .
.
.
, D that are associated with a datainstance i that is assigned to node , are generatedusing the following scheme:1.
For each ?
?
A(), draw ?
(d)?,i ?Bernoulli(?(d)?
), d = 1, .
.
.
, D,2.
Set y(d)i =??
?A() ?
(d)?,i, d = 1, .
.
.
, D,where ?
(d)?,i is an auxiliary random variable,?denotes the logical or operation, and A() de-notes the set composed of all the ancestors ofnode .
By marginalizing with respect to ?
(d)?,i,we obtain a simplified representation: first set22h(d) = 1????A()(1?
?(d)?
), and then sampley(d)i ?
Bernoulli(h(d) ) for every d = 1, .
.
.
, D.We use the parameters h(d) to define the attributehierarchy, since they represent the probability of anattribute being associated with an instance assignedto node .
Furthermore, they satisfy the propertythat the likelihood of any attribute can only increasewhen moving deeper into the tree.
We can obtainan attribute hierarchy, similar to that in Figure 1, bythresholding h(d) , and only displaying attributes thathave not been detected at any ancestor node.In order to complete our probabilistic formulationfor the attribute hierarchy, we need to specify theprior for the node parameters ?.
We use a finite ap-proximation to a hierarchical Beta process (Thibaux& Jordan, 2007; Paisley & Carin, 2009).
This choicepromotes sparsity, and therefore only few attributeswill be associated with each node.
Specifically, theparameters at the root node follow?
(d)0 ?
Beta(a/D, b(D ?
1)/D), d = 1, .
.
.
, D,(4)and the parameters in the other nodes follow?
(d) ?
Beta(c(d)?
(d)Pa(), c(d)(1??
(d)Pa())), d = 1, .
.
.
, D,(5)where Pa() denotes the parent of node , and wherea, b, and c(d), d = 1, .
.
.
, D are positive scalar pa-rameters.In this work we used a uniform prior for the pre-cision hyper-parameter c(d) ?
U [l, u] with ` = 20,and u = 100.
We also used the hyper-parameter val-ues a = 10, and b = 5 (unless otherwise stated).
InSection 5.1.1 we demonstrate that the performanceof our algorithm depends only weakly on the choiceof these parameters.3.2 Assigning images to nodesIn order to assign every image instance to one of thenodes, we combine the attribute hierarchy with theTSSBP.
The resulting graphical model representa-tion of the probability distribution function is shownin Figure 2b.
For every data instance i, a node ziin the tree is sampled from the TSSBP.
The ob-served attribute vector xi is obtained by samplingy(d)i ?
Bernoulli(h(d)zi ), and flipping y(d)i with prob-ability ?, which models the effect of the noisy at-tribute detectors.
By marginalizing over y(d)i , wehave thatp(x(d)i = 1|?)
= 1?
((1?
h(d)zi )(1?
?
(d)).+ h(d)zi ?(d)).
(6)The prior for ?
is ?
?
Beta(?0, ?1), where in thiswork we used ?0 = 5, and ?1 = 20, which pro-motes smaller values for ?.
Our algorithm is highlyinsensitive to the choice of ?0, ?1, as long as they arechosen to promote small values of ?.3.3 InferenceInference in the ATP is based on Gibbs samplingscheme.
In order to sample from the posterior ofthe node parameter ?
(d) , we note thatp(?
(d) |?)
?????D()(1?
((1?
h(d)?
)(1?
?
(d)) + h(d)?
?(d)))n(1,d)??
((1?
h(d)?
)(1?
?
(d)) + h(d)?
?(d))n(0,d)??????Ch()Beta(?(d)??
; c(d)?
(d) , c(d)(1?
?
(d) ))?
Beta(?
(d) ; a(d) , b(d) ), (7)where n(j,d) =?i|zi=?j(x(d)i ) for j = 0, 1, D()denotes the set composed of all the descendants ofnode , Ch() denotes the child nodes of node , anda(d) = a/D, b(d) = b(D ?
1)/D, for  = 0 (theroot node), and for any other node: a(d) = c(d)?
(d) ,b(d) = c(d)(1 ?
?
(d) ).
The expression in (7) is ahighly complicated function of ?
(d) , and thereforewe use slice-sampling (Neal, 2000) in order to sam-ple from the posterior.
The slice-sampler is verymuch a ?black-box?
algorithm, which only requiresthe log likelihood of (7) and very few parameters,and returns a sample from the posterior.
We sam-ple the node parameters using a two-pass approach,starting from the leaf nodes and moving up to theroot, and subsequently moving down the tree fromthe root to the leaves.In order to sample from ?, we first sample thebinary random variables y(d)i usingp(y(d)i = j|?)
?
p(y(d)i = j|?
)(?j(x(d)i )(1?
?
(d))+ ?1?j(x(d)i )?
(d)), j = 0, 1, (8)23and then sample ?
using?(d)|?
?Beta(?0 +N?i=1?1(y(d)i xor x(d)i ),?1 +N?i=1?0(y(d)i xor x(d)i )).
(9)Sampling from the posterior of the hyper-parameterc(d) was also performed using slice sampling.
Wenote that slice sampling each of the parameters ?
(d)for d = 1, .
.
.
, D, and each of c(d) for d = 1, .
.
.
, D,can be implemented in a parallel fashion.
There-fore, the computational bottleneck in the ATP is thenumber of nodes in the tree, rather than the num-ber of attributes.
Sampling from the posterior ofthe TSSBP parameters is performed using the algo-rithms developed in (Adams et al 2010).
The pa-rameters of the stick-breaking processes involved inthe TSSBP construction are also learned from thedata using slice-sampling, by assuming a uniformprior on some interval (as was also performed in(Adams et al 2010)).4 Evaluating the attribute hierarchyIn order to quantify the performance of the attributehierarchies, we evaluate the performance of the ATPas a hierarchical clustering algorithm.
We considertwo cases, in the first, the ground truth category hi-erarchy is available and can be used to compare dif-ferent hierarchies quantitatively.
In the second case,the ground truth is unavailable.4.1 Using the ground truth category hierarchyThe category hierarchy should capture the distancebetween the categories in a semantic space.
Forinstance, since car and bus are both vehicles, theyshould be assigned to nodes which are closer, com-pared to the categories car and sheep, which are se-mantically less similar.
Given the ground truth cate-gory hierarchy, we can ?measure?
the semantic dis-tance between different categories by counting thenumber of edges that separate any two categories inthe graph.In order to compare the hierarchies learned usingdifferent hierarchical clustering algorithms, we pro-pose a criterion which measures the degree to whichthe semantic distance which a hierarchy assigns todifferent image instances, diverges from the seman-tic distance that is given by the ground truth cate-gory hierarchy.
Let dGT (c1, c2) denote the num-ber of edges separating categories c1 and c2 in theground truth category hierarchy, and let dH(i, j) de-note the number of edges separating instances i andj in a hierarchy that is learned using a hierarchicalclustering algorithm.
Our proposed criterion, whichwe refer to as the average edge error (AEE), takesthe form2N(N ?
1)N?1?i=1N?j=i+1|dH(i, j)?
dGT (c(i), c(j))|,(10)where c(i) denotes the category of instance i, andNdenotes the number of image instances.4.2 Without ground truth hierarchyWhen the ground truth of the category is unavail-able, we propose to use the following two criteria inorder to evaluate the hierarchies.
The first is knownas the purity criterion (Manning et al 2009 p. 357),and the second is the locality criterion which we pro-pose.
The purity criterion measures the degree towhich each node is occupied by instances from asingle class, and takes the formPurity =1N??TN?i=1?c? (c(i)), (11)whereN denotes the number of instances in node ,and c? is the class which is most frequent in node .The locality criterion measures the degree towhich each class is concentrated in few adjacentnodes.
Quantitatively we define the category local-ity for class c asCLc = ?2(|C| ?
1)|C|?i, j ?
C,i 6= jdist(i, j),(12)where | ?
| denotes the cardinality of a set, C ={i|ci = c} where ci is the class associated with in-stance i, and dist(i, j) denotes the number of edgesalong the path separating nodes i and j .
The cate-gory locality is negative or equal to zero.
Values thatare closer to 0 indicate that the instances of category24c are concentrated in a few adjacent nodes, and neg-ative values indicate that the category instances aremore dispersed in the tree.
We define the locality asthe weighted average of the category locality, wherethe weights are the category instance frequencies.We note that each of these objectives can gener-ally be improved on the account of the other: localitycan usually be improved by joining nodes (which ingeneral makes purity worse), and purity can usuallybe improved by splitting nodes (which in generalmakes locality worse).
Therefore, we argue that adesirable hierarchy should offer an acceptable com-promise between these two performance measures.5 Experimental resultsIn this section we learn the attribute hierarchy usingour proposed ATP algorithm.
In order to evaluatethe performance we evaluate the ATP as a hierar-chical clustering algorithm, and compare it to theFBLM and AHC.
We use subsets of the PASCALVOC2008, and SUN09 datasets, for which attributeannotations are available.
We learn hierarchies us-ing the ground truth attribute annotation of the train-ing set, and using the attribute scores obtained forthe image instances in the testing set, where the at-tribute detectors are trained using the training set.We used the FBLM implementation which is avail-able online.
Our implementation of the ATP is basedthe TSSBP implementation which is available on-line, where we extended it to implement our ATPgenerative model.
We used the AHC implementa-tion available at (Mullner, ), where we used the aver-age distance metric, which is also known as the Un-weighted Pair Group Method with Arithmetic Mean(UMPGA) (Murtagh, 1984).5.1 Object category hierarchyHere we consider the PASCAL VOC 2008 dataset.We use the bounding boxes and attribute annotationdata that were collected in (Farhadi et al 2009), andare available online, along with the low-level imagefeatures.
Each of the training and testing sets con-tains over 6000 instances of the object classes: per-son, bird, cat, cow, dog, horse, sheep, airplane, bi-cycle, boat, bus, car, motorcycle, train, bottle, chair,dining-table, potted-plant, sofa, and tv/monitor.
Weused the annotation and features available for thetraining set, to train the attribute detectors using alinear SVM classifier (Fan et al 2008).
We used88 attributes, which included 24 attributes in addi-tion to those used in (Farhadi et al 2009): ?pet?,?vehicle?, ?alive?, ?animal?, and the remaining 20attributes were identical to the object classes.
Theannotation for the first 4 additional attributes was in-ferred from the object classes.
In all the experimentspresented here, we ran the Markov chain for 10,000iterations and used the tree and model parametersfrom the final iteration.The attribute hierarchies for the PASCAL datasetare shown in Figure 3, when using the annotation forthe training set, and when using the attribute scoresobtained for the testing set.
The hierarchies wereobtained by thresholding h(d) with the threshold pa-rameter 0.7 (this parameter is only used to create thevisualization, and it is not used when learning thehierarchies), and only displaying the attributes thatare not already associated with an ancestor node.
Itcan be seen that the attribute hierarchies can accu-rately capture the semantic space that is representedby the 20 categories in the PASCAL dataset.
An im-portant observation is that attributes which are asso-ciated with more categories, such as alive or vehicle,are assigned to nodes that are closer to the root node,as compared to more specialized attributes such aseye or window.In order to evaluate the performance of the at-tribute hierarchies quantitatively, we use the groundtruth category hierarchy for the 20 categories in thePASCAL dataset, which is available at (Binder et al2012), and is shown in Figure 4.
In Figure 5 weshow the AEE performance measure, which we dis-cussed in the previous section, for the different hi-erarchical clustering algorithms which we considerhere.
It can be seen that for the AHC, the AEE isvery sensitive to the threshold parameter, which ef-fectively determines the number of clusters.
A poorchoice of the parameter can adversely affect the per-formance significantly.
On the other hand, the per-formance of the ATP and FBLM is significantly lesssensitive to the choice of the hyper-parameters, sinceall the parameters are learned in a Bayesian fash-ion with weak dependence on the hyper-parameters.This is demonstrated for the ATP in Section 5.1.1.The ATP significantly improves the AEE as com-25!
"#$%&'%()*+&,%-./&+*)%/&&0-1%/&)2#+/&1"*34&5-(/&6*734/&%8%/&3*()*/&&,-+./&-(6/&"%9&:11"7.%./&%-(/&6*734/&4-#(/&%8%& ,%-./&3*()*/&)2#+/&1"*34&!
(6/&"%9&!+#6-"&,%-./&;%3&5-(/&)+*73/&%8%/&3*()*/&"%9/&07((8&<*()*/&)4%%;& ,%-./&%-(/&)+*73/&"%9/&=**"&,%-./&>#(.& <-#"/&?%-2/&%8%/&3*()*/&0%-34%(/&"%9&&,%-./&3*()*/&07((8&,%-./&)+*73/&&07((8/&4*()%/&3*()*/&"%9&@**3A)4*%/&%-(/&%8%&B%4#1"%&C%3-"&D#+9/&-#(;"-+%& EF&>*G8/&.**(/&&=4%%"/&&=#+.*=/&)4#+8&H%3&%+9#+%/&=#+.*=/&(*=I=#+./&=4%%"/&3%G3/&4*(#J*+3-"&18"#+.%(&?
*-3&C%3-"& D#+.*=/&(*=I=#+.&EF&>*G8/&3(-#+&=#+.
*=& EF&>*G8/&9"-))/&>7)&C%3-"/&1-(/&$%4#1"%&D#+9/&-#(;"-+%&C%3-"/&$%4#1"%&D4%%"/&>#181"%/&4-+."%>-()&&,-+.
"%>-()/&=4%%"/&6*3*(>#2%&K4#+8&L4-#(/&07(+#37(%&>-12& ?*M"%/&$%(N1-"&18"#+.%(&!
"#$%&O%-0/&;*3/&$%9%3-N*+/&$%(N1-"&18"#+.%(/&;*M%.I;"-+3&:11"7.%./&.#+#+9I3->"%&K1(%%+/&;"-)N1&(a) Using the attribute annotation of the training set.!""#$%&%'!
""#$%&%' ()*+,-./,01'2"0&&-1'3#422'5#,/61'4#.)&1'7&02,-'8&4%1'/,02,1'29.-':0+1''#&3'8&4%1'-,2&1'64.01';4"&1'29.-'<401'+,$/61'&=&1'/,02,'<401'+,$/61'&=&1'/,02,1'64-%1'40+'8&4%1'4#.
)&' <401'2-,$/1'&=&1'/,02,1'4-.+4#'>&31';,,/?26,&1';$00='(,02,1'#&3'(,02,1';&4/6&0'@&/4#1')&6.
"#&'A6&&#'A.-%,B'CD'E,F=1'B6&&#1'26.-='A.-31'4.07#4-&1'6,0.G,-/4#'"=#.-%&0' >&4;1'7,/1'4#.
)&1')&3&/4H,-1'7,I&%*7#4-/'J,I#&1'')&0H"4#''"=#.-%&0'(b) Using the attribute scores of the testing set.Figure 3: Attribute hierarchy learned for the PASCAL dataset, using the (a) attribute annotation available for thetraining set, and (b) attribute scores obtained by applying the attribute detectors to the image instances of the testingset.
The largest circle denotes the root node.pared to the FBLM, both for the training and testingsets.
We also note, that for the ATP, the AEE ob-tained for the training set is better than that obtainedusing the testing set?s attribute scores (training: 1.76,testing: 2.82), which is consistent with our expecta-tion.
This is not the case for the FBLM (training:6.55, testing: 5.63).5.1.1 Sensitivity to hyper-parametersIn order to validate our claim that the ATP ishighly insensitive to the choice of hyper-parameters,we performed experiments with different hyper-parameter values.
In Table 1 we compare the per-formance when using different values for the hyper-parameters a, and b in (4).
It can be seen thatwhen comparing to AHC in Figure 5, the ATP issignificantly less sensitive to the choice of hyper-parameters.
When comparing to the FBLM, evenFigure 4: The ground truth category hierarchy, for the 20categories in the PASCAL dataset.for the the worst choice of a, b the AEE is still sig-nificantly better.5.2 Scene category hierarchyHere we used the SUN09 dataset which is comprisedof indoor and outdoor scenes.
We use the training262 2.5 3 3.5 4 4.5 5024681012AHC thresholdAEEAHCATPFBLM(a) Training2 2.5 3 3.5 4 4.5 5051015202530AHC thresholdAEEAHCATPFBLM(b) TestingFigure 5: The average edge error (AEE) (10) vs. theAHC threshold parameter, for the hierarchies learned us-ing the (a) training set?s attribute annotations, and (b)attribute detectors applied to the testing set image in-stances.
Smaller values indicate better performance.
Itcan be seen that our ATP algorithm outperforms theFBLM, and unlike the AHC, it is not as sensitive to thechoice of the hyper-parameters.Table 1: Average edge error using the attribute annotationof the training set, for different hyper-parameters.a b AEE1 10 1.975 5 1.9310 5 1.7610 10 1.58510 20 1.569and testing sets which were used in (Myung et al2012), each containing over 4000 images.
The an-notation of 111 objects in the training set, and objectdetector scores for the testing set, are available on-line.
Objects in the scene have the role of attributesin describing the scene.
The object classifiers weretrained using logistic regression classifiers based onGist features that were extracted from the trainingset.Since the ground truth category hierarchy is un-available for this dataset, we use the locality andpurity criteria, which we described in the previoussection.
We computed both of these measures withrespect to the indoor and outdoor categories.
Fig-ure 6 shows the locality and purity measures for thetraining and testing sets.
It can be seen that the AHCis very sensitive to the threshold parameter, and canproduce unsatisfactory performance for a wide rangeof parameter values.
The FBLM slightly outper-forms the ATP with respect to the purity measure,however, its locality is very poor.
Therefore, we con-clude that the ATP provides an improved compro-2.5 3 3.5 4?25?20?15?10?50AHC thresholdLocalityAHCATPFBLM(a) Training- Locality2.5 3 3.5 40.550.60.650.70.750.80.850.90.95AHC thresholdPurityAHCATPFBLM(b) Training- Purity2.5 3 3.5 4?7?6?5?4?3?2?10LocalityAHC thresholdAHCATPFBLM(c) Testing- Locality2.5 3 3.5 40.550.60.650.70.750.80.850.90.95AHC thresholdPurityAHCATPFBLM(d) Testing- PurityFigure 6: The locality and purity measures vs. the AHCthreshold parameter, using the training set?s attribute an-notation, and for the testing set?s attribute scores.
Largervalues indicate better performance.
It can be seen thatour ATP has significantly better locality, and only slightlyworse purity, compared to the FBLM.
Furthermore, theperformance of the AHC depends significantly on thechoice of threshold parameter.mise with respect to the two criteria, which showsthat the ATP captures the properties of a desirablehierarchy better than the FBLM.6 ConclusionsWe developed an algorithm, which we refer to asthe attribute tree process (ATP), that uses an attributebased representation to learn a hierarchy of linguis-tic descriptions, and can be used to describe a visualdataset verbally.
In order to quantitatively evaluatethe performance of our algorithm, we proposed ap-propriate performance metrics for the cases wherethe ground truth category hierarchy is known, andwhen it is unknown.
We compared the ATP?s per-formance as a hierarchical clustering algorithm toother competing methods, and demonstrated that ourmethod can more accurately capture the ground truthsemantic distance between the different categories.Furthermore, we demonstrated that our method hasweak sensitivity to the choice of hyper-parameters.AcknowledgmentsWe acknowledge the support of the NSF Grant CPS-0931474.27References[Adams et al010] R. P. Adams, Z. Ghahramani, andM.
I. Jordan.
2010.
Tree-Structured Stick Breakingfor Hierarchical Data.
NIPS.
[Bart et al011] E. Bart, and M. Welling, and P. Perona.2011.
Unsupervised organization of Image Collec-tions: Taxonomies and Beyond.
IEEE Tran.
on PAMI,33(11):2302?2315.
[Berg et al011] T. L. Berg, A. C. Berg and J. Shih.
2010.Automatic Attribute Discovery and Characterizationfrom Noisy Web Data.
CVPR.
[Binder et al012] A. Binder, K. R. Muller, andM.
Kawanabe.
2012.
On Taxonomies for Multi-classImage Categorization.
International Journal ofComputer Vision, 99:281?301.
[Fan et al008] R. Fan, K. Chang, C. Hsieh, X. Wang,and C. Lin.
2008.
LIBLINEAR: A Library for LargeLinear Classification.
Journal of Machine LearningResearch, 9:1871?1874.
[Farhadi et al009] A. Farhadi, I. Endres, D. Hoiem, andDavid Forsyth.
2009.
Describing objects by their at-tributes.
CVPR.
[Ferrari & Zisserman2007] V. Ferrari and A. Zisserman.2010.
Learning Visual Attributes.
CVPR.
[Jain & Dubes1988 p. 59] A. K. Jain and R. C. Dubes.1988.
Algorithms for Clustering Data.
Prentice-Hall,Englewood Cliffs, NJ.
[Lampert et al009] C. H. Lampert, H. Nickisch, andS.
Harmeling.
2009.
Learning to detect unseen ob-ject classes by between class attribute transfer.
CVPR.
[Li et al010] L. J. Li, and C. Wang, and Y. Lim, andD.
M. Blei, and L. Fei-Fei.
2010.
Building and us-ing a semantivisual image hierarchy.
CVPR.
[Manning et al009 p. 357] C. D. Manning, P. Ragha-van, and H. Schtze.
2009.
An Introduc-tion to information retrieval.
Available online athttp://nlp.stanford.edu/IR-book/.
[Mullner] D. Mulner.
fastcluster: Fast hierar-chical clustering routines for R and Python.http://math.stanford.edu/ muellner/index.html.
[Murtagh1984] F. Murtagh.
1984.
Complexities of Hi-erarchic Clustering Algorithms: the state of the art.Computational Statistics Quarterly, 1: 101?113.
[Myung et al012] M. J. Choi, A. Torralba andA.
S. Willsky.
2012.
A Tree-Based ContextModel for Object Recognition.
IEEE Tran.
on PAMI,34(2):240?252.
[Neal2000] R. Neal.
2000.
Nonparametric factor analysiswith beta process priors.
Annals of Statistics, 31:705?767.
[Paisley & Carin2009] J. W. Paisley, and L. Carin.
2009.Nonparametric factor analysis with beta process pri-ors.
ICML.
[Parikh & Grauman2011] P. Devi and G. Kristen.
2011.Interactively building a discriminative vocabulary ofnameable attributes.
CVPR.
[Sivic et al008] J. Sivic, and B. C. Russel, and A. Zis-serman, and W. T. Freeman, and A.
A. Efros.
2008.Unsupervised Discovery of visual object class hierar-chies.
CVPR.
[Teh et al006] Y. W. Teh and M. I. Jordan and M. J. Bealand D. M. Blei.
2006.
Hierarchical Dirichlet Pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.
[Thibaux & Jordan2007] R. Thibaux and M. I. Jordan.2007.
Hierarchical Beta Processes and the Indian Buf-fet Process.
AISTATS.28
