Learning to Predict Problematic Situations in a Spoken DialogueSystem: Experiments with How May I Help You?Mar i lyn  Walker ,  I rene  Langk i lde ,  Je r ry  Wr ight ,  A l ien  Gor in ,  D iane  L i tmanAT&T Labs- -Research180 Park AvenueF lorham Park, NJ 07932-0971 USAwalker, jwright, algor, diane@research, art.
corn, ilangkil@isi, eduAbst rac tCurrent spoken dialogue systems are deficient intheir strategies for preventing, identifying and re-pairing problems that arise in the conversation.
Thispaper reports results on learning to automaticallyidentify and predict problematic human-computerdialogues in a corpus of 4774 dialogues collected withthe How May I Help You spoken dialogue system.Our expectation is that the ability to predict prob-lematic dialogues will allow the system's dialoguemanager to modify its behavior to repair problems,and even perhaps, to prevent them.
We train aproblematic dialogue classifier using automatically-obtainable features that can identify problematicdialogues ignificantly better (23%) than the base-line.
A classifier trained with only automatic fea-tures from the first exchange in the dialogue canpredict problematic dialogues 7% more accuratelythan the baseline, and one trained with automaticfeatures from the first two exchanges can perform14% better than the baseline.1 In t roduct ionSpoken dialogue systems promise fficient and nat-ural access to a large variety of information sourcesand services from any phone.
Systems that sup-port short utterances to select a particular function(through a statement such as "Say credit card, col-lect or person-to-person") are saving companies mil-lions of dollars.
Research prototypes exist for appli-cations uch as personal email and calendars, traveland restaurant information, and personal banking(Baggia et al, 1998; Walker et al, 1998; Seneff etal., 1995; Sanderman et al, 1998; Chu-Carroll andCarpenter, 1999) inter alia.
Yet there are still manyresearch challenges: current systems are limited inthe interaction they support and brittle in many re-spects.
We show how spoken dialogue systems canlearn to support more natural interaction on the ba-sis of their previous experience.One way that current spoken dialogue systems arequite limited is in their strategies for detecting andrepairing problems that arise in conversation.
Ifa problem can be detected, the system can eithertransfer the call to a human operator or modify itsdialogue strategy in an attempt to repair the prob-lem.
We can train systems to improve their ability todetect problems by exploiting dialogues collected ininteractions with human users.
The initial segmentsof these dialogues can be used to predict hat a prob-lem is likely to occur.
We expect hat the ability topredict that a dialogue is likely to be problematicwill allow the system's dialogue manager to applymore sophisticated strategies to repairing problems,and even perhaps, to prevent hem.This paper reports experiments on predictingproblems in spoken dialogue interaction by train-ing a problematic dialogue predictor on a corpus of4774 dialogues collected in an experimental trial ofAT~;T's How May I Help You (HMIHY) spoken dia-logue system (Gorin et al, 1997; Riccardi and Gorin,to appear; E. Ammicht and Alonso, 1999).
In thistrial, the HMIHY system was installed at an AT&Tcustomer care center.
HMIHY answered calls fromlive customer traffic and successfully automated alarge number of customer requests.
An example ofa dialogue that HMIHY completed successfully isshown in Figure 13St: AT&T How may I help you?UI: I need to \[ uh \] put a call on my calling card please$2: May I have your card number, please?U2:765432 10987654$3: What number would you like to call?U3:8 1 4 7 7 7 6 6 6 6 (misrecognized)$4: May I have that number again?U4:8147776666$5: Thank you.Figure 1: Sample TASKSUCCESS DialogueWe shall refer to the dialogues with a TASKSUC-CESS outcome, in which HMIHY successfully auto-mates the customer's call, as the TASKSUCCESS dia-logues.
In addition to the TASKSUCCESS dialogues,there are several other call outcomes, to be describedin detail below, which we consider problematic.tThe phone numbers, card numbers, and pin numbers inthe sample dialogues are artificial.210This paper reports results from a set of experi-ments that test whether we can learn to automat-ically predict that a dialogue will be problematicon the basis of information the system has: (1)early on in the dialogue; and (2) in real time.
Wetrain an automatic lassifer for predicting problem-atic dialogues from features that can be automat-ically extracted from the HMIHY corpus.
The re-sults show that we can learn to predict problematicdialogues using fully automatic features with an ac-curacy ranging from 72% to 87%, depending on howmuch of the dialogue the system has seen so far.Section 2 describes HMIHY and the dialogue corpuswhich the experiments are based on.
Section 3 de-tails the encoding of the dialogues and the methodsused for utilizing the machine learning program RIP-PER to train an automatic problematic dialogue pre-dictor (Cohen, 1996).
Section 4 presents the resultsand section 5 summarizes the paper and describesfuture work.2 Exper imenta l  Sys tem and DataHMIHY is a spoken dialogue system based on the no-tion of call routing (Gorin et al, 1997; Chu-Carrolland Carpenter, 1999).
In the HMIHY call rout-ing system, services that the user can access areclassified into 14 categories, plus a category calledother for calls that cannot be automated and mustbe transferred to a human operator (Gorin et al,1997).
~ Each category describes a different task,such as person-to-person dialing, or receiving creditfor a misdialed number.
The system determineswhich task the caller is requesting on the basis ofits understanding of the cMler's response to theopen-ended system greeting A T~ T, How May I HelpYou?.
Once the task has been determined, the infor-mation needed for completing the caller's request isobtained using dialogue submodules that are specificfor each task (Abella and Gorin, 1999).In addition to the TASKSUCCESS dialogues inwhich HMIHY successfully automates the cus-tomer's call, illustrated in Figure 1, and the callsthat are transferred to a human operator, there arethree other possible outcomes for a call, all of whichare problematic.
The first category, which we callHANGUP, results from a customer's decision to hangup on the system.
A sample HANGUP dialogue isin Figure 2.
A caller may hang up because s/he isfrustrated with the system; our goal is to learn fromthe corpus which system behaviors led to the caller'sfrustration.The second problematic ategory (WIZARD), re-suits from a human customer care agent's decision totake over the call from the system.
Because HMIHYis experimental, each call was monitored by a human2 One potential use of our results is to provide better ulesfor making the transfer decision.SI: AT&T How may I help you?UI: I need credit please.$2: Okay.
What is the reason for the credit?U2: Miss, uh, Miss, different area code than I needed.$3: Sorry.
I need to know whether you need credit fora wrong number, bad connection or a call that was cutoff.U3: It is a wrong number.
(impatient humming)$4: What was the number that you dialed?USER HANGS UPFigure 2: Sample HANGUP Dialogueagent serving as a wizard who could override thesystem.
There were a number of agents who partici-pated as wizards during the trial of HMIHY and eachwizard was simply told to take over the call if s/heperceived problems with the system's performance.The wizard's decision was logged by the experimen-tal setup, resulting in labelling the call as one thatthe wizard took over.
s A dialogue where the wizarddecided that the dialogue was problematic and tookover the call is shown in Figure 3.Sh AT&T How may I help you?Uh (silence)$2: Sorry.
Please briefly tell me how I may  help you?U2: I'm trying to call 1 8 hundred call A T T.$3: Do you want to place a call?U3: Through my calling card.$4: May I have your card number, please?U4: It's 8 7 6 5 4 3 2 1 0 and then my pin number is 87 6 5.
(misrecognized)$5: Sorry.
Please enter or say your card number again.WIZARD STEPS INFigure 3: Sample WIZARD DialogueThe third problematic category, the TASKFAILUREdialogues, are cases where the system completed thecall, but carried out a task that was not the taskthat the customer was actually requesting.
An ex-ample TASKFAILURE dialogue is given in Figure 4:HMIHY interpreted utterance U2 as a request tomake a third-party call e.g.
to bill it to my homephone.
HMIHY then asked the caller for the infor-mation it needed to carry out this task, the callercomplied, and the system completed the call.The corpus of 4774 dialogues used in our exper-iments was collected in severM experimental trialsof HMIHY on live customer traffic (Pdccardi andGorin, to appear; E. Ammicht and Alonso, 1999),and is referred to as HM2 in (Riccardi and Gorin, toappear)).
During the trial, all of the system behav-iors were automatically recorded in a log file, andlater the dialogues were transcribed by humans andlabelled with a semantic ategory representing thesOf  course we can only infer what might have mot ivatedthe wizard to take over the call, but we assume that the wizardhad good reason for doing so.211SI: AT&T How may I help you?UI: (silence)$2: Sorry.
Please briefly tell me how I may help you?U2: I wanta find out how much my bill's worth.$3: What phone number would you like the call billedto?U3: Urn the access number.$4: Sorry.
Could I have that phone number again?U4:18hundred2255288.$5: What number would you like to call?U5:800 225 5288 (touchtone input)$6: Thank you.Figure 4: Sample TASKFAILURE Dialoguetask that the caller was asking HMIHY to perform,on a per utterance basis.
The logfiles also includedlabels indicating whether the wizard had taken overthe call or the user had hung up.3 T ra in ing  an  Automat icP rob lemat ic  D ia logue  Pred ic torOur experiments apply the machine learning pro-gram RIPPER (Cohen, 1996) to automatically inducea "problematic dialogue" classification model.
RIP-PER takes as input the names of a set of classes tobe learned, the names and ranges of values of a fixedset of features, and training data specifying the classand feature values for each example in a training set.Its output is a classification model for predicting theclass of future examples.
In RIPPER, the classifica-tion model is learned using greedy search guided byan information gain metric, and is expressed as anordered set of if-then rules.To apply RIPPER, the dialogues in the corpus mustbe encoded in terms of a set of classes (the outputclassification) and a set of input features that areused as predictors for the classes.
We start with thedialogue categories described above, but since ourgoal is to develop algorithms that predict/identifyproblematic dialogues, we treat HANGUP, WIZARDand TASKFAILURE as equivalently problematic.
Thuswe train the classifier to distinguish between twoclasses: TASKSUCCESS and PROBLEMATIC.
Note thatour categorization is inherently noisy because we donot know the real reasons why a caller hangs up or awizard takes over the call.
The caller may hang upbecause she is frustrated with the system, or she maysimply dislike automation, or her child may havestarted crying.
Similarly, one wizard may have lowconfidence in the system's ability to recover from er-rors and use a conservative approach that results intaking over many calls, while another wizard may bemore willing to let the system try to recover.
Nev-ertheless we take these human actions as a humanlabelling of these calls as problematic.
Given thisclassification, approximately 36% of the calls in thecorpus of 4774 dialogues are PROBLEMATIC and 64%are TASKSUCCESS.Next, we encoded each dialogue in terms of a setof 196 features that were either automatically oggedby one of the system modules, hand-labelled by hu-mans, or derived from raw features.
We use thehand-labelled features to produce a TOPLINE, an es-timation of how well a classifier could do that hadaccess to perfect information.
The entire feature setis summarized in Figure 5.?
Acoust ic /ASR Features- recog, recog-numwords, ASR-duration, dtmf-flag, rg-modality, rg-grammar?
NLU Features- a confidence measure for all of the possibletasks that the user could be trying to do- salience-coverage, inconsistency, context-shift,top-task, nexttop-task, top-confidence, dill-confidence?
D ia logue  Manager  Features- sys-label, utt-id, prompt, reprompt, confirma-tion, subdial- running tallies: num-reprompts, num-confirms, num-subdials, reprompt%, confir-mation%, subdialogue%?
Hand-Labe l led  Features- tscript, human-label, age, gender, user-modality, clean-tscript, cltscript-numwords,rsuccess?
Whole-Dialogue Featuresnum-utts, num-reprompts, percent-reprompts,num-confirms, percent-confirms, num-subdials, percent-subdials, dial-duration.Figure 5: Features for spoken dialogues.There are 8 features that describe the whole dia-logue, and 47 features for each of the first four ex-changes.
We encode features for the first four ex-changes because we want to predict failures beforethey happen.
Since 97% of the dialogues in our cor-pus are five exchanges or less, in most cases, anypotential problematic outcome will have occurredby the time the system has participated in five ex-changes.
Because the system needs to be able topredict whether the dialogue will be problematic us-ing information it has available in the initial part ofthe dialogue, we train classifiers that only have ac-cess to input features from exchange 1, or only thefeatures from exchange 1 and exchange 2.
To seewhether our results generalize, we also experimentwith a subset of features that are task-independent.We compare results for predicting problematic din-212logues, with results for identifying problematic di-alogues, when the classifier has access to featuresrepresenting the whole dialogue.We utilized features logged by the system becausethey are produced automatically, and thus could beused during runtime to alter the course of the dia-logue.
The system modules that we collected infor-mation from were the acoustic processer/automaticspeech recognizer (ASR) (Riccardi and Gorin, to ap-pear), the natural anguage understanding (NLU)module (Gorin et al, 1997), and the dialogue man-ager (DM) (Abella and Gorin, 1999).
Below we de-scribe each module and the features obtained fromit.ASR takes as input the acoustic signal andoutputs a potentially errorful transcription of whatit believes the caller said.
The ASR features foreach of the first four exchanges were the outputof the speech recognizer (recog), the number ofwords in the recognizer output (recog-numwords),the duration in seconds of the input to therecognizer (asr-duration), a flag for touchtoneinput (dtmf-flag), the input modality expectedby the recognizer (rg-modality) (one of: none,speech, touchtone, speech+touchtone, touchtone-card, speech+touchtone-card, touchtone-date,speech+touchtone-date, or none-final-prompt), andthe grammar used by the recognizer (rg-grammar).The motivation for the ASR features is that anyone of them may have impacted performance.
Forexample, it is well known that longer utterancesare less likely to be recognized correctly, thus asr-duration could be a clue to incorrect recognition re-suits.
In addition, the larger the grammar is, themore likely an ASR error is, so the name of thegrammar vg-grammar could be a predictor of incor-rect recognition.The natural language understanding (NLU) mod-ule takes as input a transcription ofthe user's utter-ance from ASR and produces 15 confidence scoresrepresenting the likelihood that the caller's task isone of the 15 task types.
It also extracts otherrelevant information, such as phone or credit cardnumbers.
Thus 15 of the NLU features for each ex-change represent the 15 confidence scores.
Thereare also features that the NLU module calculatesbased on processing the utterance.
These includean intra-utterance measure of the inconsistency be-tween tasks that the user appears to be requesting(inconsistency), a measure of the coverage of theutterance by salient grammar fragments (salience-coverage), a measure of the shift in context betweenutterances (context-shift), he task with the highestconfidence score (top-task), the task with the secondhighest confidence score (nexttop-task), the value ofthe highest confidence score (top-confidence), andthe difference in values between the top and next-to-top confidence scores (diff-confidence).The motivation for these NLU features i  to makeuse of information that the NLU module has basedon processing the output of ASR and the current dis-course context.
For example, for utterances that fol-low the first utterance, the NLU module knows whattask it believes the caller is trying to complete.
If itappears that the caller has changed her mind, thenthe NLU module may have misunderstood a previ-ous utterance.
The context-shift feature indicatesthe NLU module's belief that it may have made anerror (or be making one now).The dialogue manager (DM) takes the output ofNLU and the dialogue history and decides what itshould say to the caller next.
It decides whether itbelieves there is a single unambiguous task that theuser is trying to accomplish, and how to resolve anyambiguity.
The DM features for each of the first fourexchanges are the task-type label which includes alabel that indicates task ambiguity (sys-label), utter-ance id within the dialogue (implicit in the encod-ing), the name of the prompt played before the userutterance (prompt), and whether that prompt was areprompt (reprompt), a confirmation (confirm), or asubdialogue prompt (subdia O, a superset of the re-prompts and confirmation prompts.The DM features are primarily motivated by pre-vious work.
The task-type label feature is to cap-ture the fact that some tasks may be harder thanothers.
The utterance id feature is motivated by theidea that the length of the dialogue may be impor-tant, possibly in combination with other features liketask-type.
The different prompt features for initialprompts, reprompts, confirmation prompts and sub-dialogue prompts are motivated by results indicatingthat reprompts and confirmation prompts are frus-trating for callers and that callers are likely to hy-perarticulate when they have to repeat hemselves,which results in ASR errors (Shriberg et al, 1992;Levow, 1998).The DM features also include running tallies forthe number of reprompts (num-reprompts), numberof confirmation prompts (num.confirms), and num-ber of subdialogue prompts (num-subdials), that hadbeen played up to each point in the diMogue, as wellas running percentages (percent-reprompts, ercent-confirms, percent-subdials).
The use of running tal-lies and percentages is based on the assumption thatthese features are likely to produce generalized pre-dictors (Litman et al, 1999).The features obtained via hand-labelling were hu-man transcripts of each user utterance (tscript), aset of semantic labels that are closely related to thesystem task-type labels (human-label), age (age) andgender (gender) of the user, the actual modality ofthe user utterance (user-modality) (one of: nothing,speech, touchtone, speech+touchtone, on-speech),213and a cleaned transcript with non-word noise infor-mation removed (clean-tscript).
From these featureswe calculated two derived features.
The first was thenumber of words in the cleaned transcript (cltscriptnumwords), again on the assumption that utterancelength is strongly correlated with ASR and NLU er-rors.
The second derived feature was based on cal-culating whether the human-label matches the sys-label from the dialogue manager (rsuccess).
Therewere four values for rsuccess: rcorrect, rmismatch,rpartial-match and rvacuous-match, indicating re-spectively correct understanding, incorrect under-standing, partial understanding, and the fact thatthere had been no input for ASR and NLU to oper-ate on, either because the user didn't say anythingor because she used touch-tone.The whole-dialogue f atures derived from the per-utterance features were: num-utts, num-reprompts,percent-reprampts, hum.confirms, percent-confirms,num-subdials, and per-cent-subdials for the whole di-alogue, and the duration of the entire dialogue inseconds (dial-duration).In the experiments, the features in Figure 5 exceptthe Hand-Labelled features are referred to as the AU-TOMATIC feature set.
We examine how well we canidentify or predict problematic dialogues using thesefeatures, compared to the full feature set includingthe Hand-Labelled features.
As mentioned earlier,we wish to generalize our problematic dialogue pre-dictor to other systems.
Thus we also discuss howwell we can predict problematic dialogues using onlyfeatures that are both automatically acquirable dur-ing runtime and independent of the HMIHY task.The subset of features from Figure 5 that fit thisqualification are in Figure 6.
We refer to them asthe AUTO, TASK-INDEP feature set.The output of each RIPPER.
experiment is a clas-sification model learned from the training data.
Toevaluate these results, the error rates of the learnedclassification models are estimated using the resam-pling method of cross-validation.
In 5-fold cross-validation, the total set of examples is randomly di-vided into 5 disjoint test sets, and 5 runs of the learn-ing program are performed.
Thus, each run uses theexamples not in the test set for training and the re-maining examples for testing.
An estimated errorrate is obtained by averaging the error rate on thetesting portion of the data from each of the 5 runs.Since we intend to integrate the rules learnedby RIPPER into the HMIHY system, we examinethe precision and recall performance of specific hy-potheses.
Because hypotheses from different cross-validation experiments cannot readily be combinedtogether, we apply the hypothesis learned on onerandomly selected training set (80% of the data) tothat set's respective test data.
Thus the precisionand recall results reported below are somewhat less?
Acoust ic /ASR Features- recog, recog-numwords, ASR-duration, dtmf-flag, rg-modality?
NLU Features- salience-coverage, inconsistency, context-shift,top-confidence, dig-confidence?
D ia logue  Manager  Features- utt-id, reprompt, confirmation, subdial- running tallies: num-reprompts, num-confirms, num-subdials, reprompt%, confir-mation%, subdialogue%Figure 6: Automatic task independent (AUTO,TASK-INDEP) features available at runtime.reliable than the error rates from cross-validation.4 ResultsWe present results for both predicting and identi-fying problematic dialogues.
Because we are inter-ested in predicting that a dialogue will be problem-atic at a point in the dialogue where the system cando something about it, we compare prediction ac-curacy after having only seen the first exchange ofthe diMogue with prediction accuracy after havingseen the first two exchanges, with identification ac-curacy after having seen the whole dialogue.
Foreach of these situations we also compare results forthe AUTOMATIC and AUTO, TASK-INDEP feature sets(as described earlier), with results for the whole fea-ture set including hand-labelled features.
Table 1summarizes the results.The baseline on the first line of Table 1 repre-sents the prediction accuracy from always guess-ing the majority class.
Since 64% of the dialoguesare TASKSUCCESS dialogues, we can achieve 64% ac-curacy from simply guessing TASKSUCCESS withouthaving seen any of the dialogue yet.The first EXCHANGE 1 row shows the results ofusing the AUTOMATIC features from only the firstexchange to predict whether the dialogue outcomewill be TASKSUCCESS or PROBLEMATIC.
The resultsshow that the machine-learned classifier can predictproblematic dialogues 8% better than the baselineafter having seen only the first user utterance.
Usingonly task-independent automatic features (Figure 6)the EXCHANGE 1 classifier can still do nearly as well.The ALL row for EXCHANGE 1 indicates that evenif we had access to human perceptual ability (thehand-labelled features) we would still only be ableto distinguish between TASKSUCCESS and PROBLEM-ATIC dialogues with 77% accuracy after having seenthe first exchange.214Features UsedBASELINE (majority class)EXCHANGE 1 AUTOMATICAUTO, TASK-INDEPALLEXCHANGES l&2 AUTOMATICAUTO, TASK-INDEPALLFULL DIALOGUE AUTOMATICAUTO, TASK-INDEPTOPLINE ALL\]Accuracy (SE)64.0 %72.3 % 1.04 %71.6 % 1.05 %77.0 % 0.56 %79.9 % 0.58 %78.6 % 0.37 %86.7 % 0.33 %87.0 % 0.72 %86.7 % 0.82 %92.3 % 0.72 %Table 1: Results for predicting and identifying problematic dialogues (SE --- Standard Error)The EXCHANGE l&2 rows of Table 1 show the re-suits using features from the first two exchanges inthe dialogue to predict he outcome of the dialogue.
4The additional exchange gives roughly an additional7% boost in predictive accuracy using either of theAUTOMATIC feature sets.
This is only 8% less thanthe accuracy we can achieve using these features af-ter having seen the whole dialogue (see below).
TheALL row for EXCHANGE l&2 shows that we couldachieve over 86% accuracy if we had the ability toutilize the hand-labelled features.The FULL DIALOGUE row in Table 1 for AUTO-MATIC and AUTO, TASK-INDEP features hows theability of the classifier to identify problematic dia-logues, rather than predict them, using features forthe whole dialogue.
The ALL row for the FULL DI-ALOGUE shows that we could correctly identify over92% of the outcomes accurately if we had the abilityto utilize the hand-labelled features.Note that the task-independent automatic fea-tures always perform within 2% error of the auto-matic features, and the hand-labelled features con-sistently perform with accuracies ranging from 6-8%greater.The rules that RIPPER learned on the basis of theExchange 1 automatic features are below.Exchange 1, Automat ic  Features:i f  (el-top-confidence _< .924) A (el-dtmf-f lag = '1')then  problematic,if (el-cliff-confidence _<.916) A (el-asr-duration > 6.92)then problematic,default is tasksuccess.According to these rules, a dialogue will be prob-lematic if the confidence score for the top-ranked4Since 23% of the dialogues consisted of only two ex-changes, we exclude the second exchange features for thosedialogues where the second exchange consists only of the sys-tem playing a closing prompt.
We also excluded any featuresthat indicated to the classifier that the second exchange wasthe last exchange in the dialogue.task (given by the NLU module) is moderate or lowand there was touchtone input in the user utterance.The second rule says that if the difference betweenthe top confidence score and the second-ranked con-fidence score is moderate or low, and the durationof the user utterance is more than 7 seconds, predictPROBLEMATIC.The performance of these rules is summarized inTable 2.
These results show that given the first ex-change, this ruleset predicts that 22% of the dia-logues will be problematic, while 36% of them ac-tually will be.
Of the dialogues that actually willbe problematic, it can predict 41% of them.
Onceit predicts that a dialogue will be problematic, it iscorrect 69% of the time.
As mentioned earlier, thisreflects an overMl improvement in accuracy of 8%over the baseline.The rules learned by training on the automatictask-independent features for exchanges 1 and 2 aregiven below.
As in the first rule set, the features thatthe classifier appears to be exploiting are primarilythose from the ASR and NLU modules.Exchanges l&2, Automatic Task-Independent Features:i f  (e2-recog-numwords < 0) A (el-cliff-confidence < .95)then  problematic.if (el-salience-coverage < .889) A (e2-recog contains"I') A (e2-asr-duration > 7.48) then problematic.if (el-top-confidence < .924) A (e2-asr-duration >_ 5.36)A (el-asr-duration > 8.6) then problematic.if (e2-recog is blank) A (e2-asr-duration > 2.8) thenproblematic.if (el-salience-coverage < .737) A (el-recog contains"help") A (el-asr-duration < 7.04) then problematic.if (el-cliff-confidence < .924) A (el-dtmf-flag = '1') A(el-asr-duration < 6.68) then problematic.default is tasksuccess.The performance of this ruleset is summarized inTable 3.
These results show that, given the firsttwo exchanges, this ruleset predicts that 26% of the215ClassSuccessProblematicOccur red  Pred ic ted  Recal l  P rec i s ion64.1% 78.3 % 89.44 % 73.14 %35.9 % 21.7 % 41.47 % 68.78 %Table 2: Precision and Recall with Exchange 1 Automatic FeaturesClassSuccessProblematicOccur red  Pred ic ted  Recal l  Prec is ion64.1% 75.3 % 91.42 % 77.81%35.9 % 24.7 %' 53.53 % 77.78 %Table 3: Precision and Recall with Exchange l&2 Automatic, Task-Independent Featuresdialogues will be problematic, while 36% of themactually will be.
Of the problematic dialogues, itcan predict 57% of them.
Once it predicts that adialogue will be problematic, it is correct 79% ofthe time.
Compared with the classifier for the firstutterance alone, this classifier has an improvementof 16% in recall and 10% in precision, for an overallimprovement in accuracy of 7% over using the firstexchange alone.One observation from these hypotheses i  the clas-sifier's preference for the asr-duration feature overthe feature for the number of words recognized(recog-numwords).
One would expect longer utter-ances to be more difficult, but the learned rulesetsindicate that duration is a better measure of utter-ance length than the number of words.
Another ob-servation is the usefulness of the NLU confidencescores and the NLU salience-coverage in predictingproblematic dialogues.
These features eem to pro-vide good general indicators of the system's uccessin recognition and understanding.
The fact that themain focus of the rules is detecting ASR and NLUerrors and that none of the DM behaviors are usedas predictors also indicates that, in all likelihood, theDM is performing as well as it can, given the noisyinput that it is getting from ASR and NLU.To identify potential improvements in the prob-lematic dialogue predictor, we analyzed which hand-labelled features made large performance improve-ments, under the assumption that future work canfocus on developing automatic features that ap-proximate the information provided by these hand-labelled features.
The analysis indicated that thevsuceess feature alone improves the performance ofthe TOPLINE from 88.5%, as reported in (Langkildeet al, 1999), to 92.3%.
Using rsuccess as the onlyfeature results in 73.75% accuracy for exchange 1,81.9% accuracy for exchanges 18z2 and 85.3% accu-racy for the full dialogue.
In addition, for Exchangesl&2, the accuracy of the AUTOMATIC, TASK-INDEPfeature set plus the rsuccess feature is 86.5%, whichis only 0.2% less than the accuracy of ALL the lea-tures for Exchanges l&2 as shown in Table 1.
Therules that RIPPER learns for Exchanges 1&52 whenthe AUTOMATIC, TASK-INDEP feature set is aug-mented with the single hand-labelled rsuccess fea-ture is shown below.Exchanges  1~2,  Rsuccess -b Automat icTask - Independent  Features:ife2-salience-coverage ~ 0.651 A e2-asr-duration >_0.04A e2-rsuccess=Rvacuous-match then problematic,if e2-rsuccess=Rmismatch A el-top-confidence < 0.909then problematic,if e2-rsuccess=Rmismatch A e2-context-shift < 0.014 Ae2-salience-coverage ~ 0.2 A e2-recog-numwords < 12 (then problematic,if e2-rsuccess=Rmismatch ^ el-rsuccess=Rmismatchthen problematic,if e2-rsuccess=Rmismatch A e2-top-confidence < 0.803^ e2-asr-duration >__2.68 ^  e2-asr-duration < 6.32 thenproblematic,if el-rsuccess=Rmismatch A el-diff-confidence > 0.83then problematic,if e2-rsuccess=Rmismatch A e2-context-shift >_ 0.54then problematic,ife2-asr-duration > 5.24 A e2-salience-coverage < 0.833A e2-top-confidence < 0.801 A e2-recog-numwords < 7A e2-asr-duration < 16.08 then problematic,if el-diff-confidence < 0.794 A el-asr-duration > 7.2A el-inconsistency > 0.024 A el-inconsistency > 0.755then problematic,default is tasksuccessNote that the rsuccess feature is frequently used inthe rules and that RIPPER learns rules that combinethe rsuccess feature with other features, such as theconfidence, asr-duration, and salience-coverage fea-tures.5 D iscuss ion  and Future  WorkIn summary, our results show that: (1) All featuresets significantly improve over the baseline; (2) Us-ing automatic features from the whole dialogue, wecan identify problematic dialogues 23% better thanthe baseline; (3) Just the first exchange provides ig-216nificantly better prediction (8%) than the baseline;(4) The second exchange provides an additional sig-nificant (7%) improvement, (5) A classifier based ontask-independent automatic features performs withless than 1% degradation in error rate relative tothe automatic features.
Even with current accuracyrates, the improved ability to predict problematicdialogues means that it may be possible to field thesystem without human agent oversight, and we ex-pect to be able to improve these results.The research reported here is the first that weknow of to automatically analyze a corpus of logsfrom a spoken dialogue system for the purpose oflearning to predict problematic situations.
Our workbuilds on earlier research on learning to identify di-alogues in which the user experienced poor speechrecognizer performance (Litman et al, 1999).
How-ever, that work was based on a much smaller set ofexperimental dialogues where the notion of a good orbad dialogue was automatically approximated ratherthan being labelled by humans.
In addition, becausethat work was based on features ynthesized over theentire dialogues, the hypotheses that were learnedcould not be used for prediction during runtime.We are exploring several ways to improve the per-formance of and test the problematic dialogue pre-dictor.
First, we noted above the extent to whichthe hand-labelled feature rsuccess improves classifierperformance.
In other work we report results fromtraining an rsuccess classifier on a per-utterance level(Walker et al, 2000), where we show that we canachieve 85% accuracy using only fully automatic fea-tures.
In future work we intend to use the (noisy)output from this classifier as input to our problem-atic dialogue classifier with the hope of improvingthe performance of the fully automatic feature sets.In addition, since it is more important o minimizeerrors in predicting PROBLEMATIC dialogues than er-rors in predicting TASKSUCCESS dialogues, we intendto experiment with RIPPER'S loss ratio parameter,which instructs RIPPER to achieve high accuracy forthe PROBLEMATIC class, while potentially reducingoverall accuracy.
Finally, we plan to integrate thelearned rulesets into the HMIHY dialogue system toimprove the system's overall performance.Re ferencesA.
Abella and A.L.
Gorin.
1999.
Construct algebra:An analytical method for dialog management.
InProc.
of the Association for Computational Lin-guistics.P.
Baggia, G. Castagneri, and M. Danieli.
1998.Field trials of the Italian Arise Train TimetableSystem.
In Interactive Voice Technology forTelecommunications Applications, IVTTA, pages97-102.J.
Chu-Carroll and R. Carpenter.
1999.
Vector-based natural language call routing.
Computa-tional Linguistics, 25-3:361-387.W.
Cohen.
1996.
Learning trees and rules with set-valued features.
In l~th Conference of the Amer-ican Association of Artificial Intelligence, AAAI.A.L.
Gorin E. Ammicht and T. Alonso.
1999.Knowledge collection for natural anguage spokendialog systems.
In Proc.
of EUROSPEECH 99.A.L.
Gorin, G. Riccardi, and J.H.
Wright.
1997.How may I Help You?
Speech Communication,23:113-127.I.
Langkilde, M. Walker, J. Wright, A. Gorin, andD.
Litman.
1999.
Automatic prediction of prob-lematic human-computer dialogues in How MayI Help You?
In Proc.
IEEE Workshop on Auto-matic Speech Recognition and Understanding.G.
A. Levow.
1998.
Characterizing and recogniz-ing spoken corrections in human-computer dia-logue.
In Proc.
of the 36th Annual Meeting of theAssociation of Computational Linguistics, COL-ING/ACL 98, pages 736-742.D.
J. Litman, M. A. Walker, and M. J. Kearns.
1999.Automatic detection of poor speech recognition atthe dialogue level.
In Proc.
of the 37th AnnualMeeting of the Association of Computational Lin-guistics, ACL99, pages 309-316.G.
Riccardi and A.L.
Gorin.
to appear.
Spoken lan-guage adaptation over time and state in a natu-ral spoken dialog system.
IEEE Transactions onSpeech and Audio.A.
Sanderman, J. Sturm, E. den Os, L. Bores, andA.
Cremers.
1998.
Evaluation of the Dutch TrainTimetable Information System developed in theARISE project.
In Interactive Voice Technologyfor Telecommunications Applications, pages 91-96.S.
Seneff, V. Zue, J. Polifroni, C. Pao, L. Hethering-ton, D. Goddeau, and J.
Glass.
1995.
The pre-liminary development of a displayless PEGASUSsystem.
In ARPA SLT Workshop.E.
Shriberg, E. Wade, and P. Price.
1992.
Human-machine problem solving using spoken languagesystems (SLS): Factors affecting performance anduser satisfaction.
In Proc.
of the DARPA Speechand NL Workshop, pages 49-54.M.
A. Walker, J. C. Fromer, and S. Narayanan.1998.
Learning optimal dialogue strategies: Aease study of a spoken dialogue agent for email.In Proc.
of the 36th Annual Meeting of theAssociation of Computational Linguistics, COL-ING/ACL 98, pages 1345-1352.M.
Walker, I. Langkilde, and J. Wright.
2000.
Us-ing NLP and Discourse features to identify under-standing errors in the How May I Help You spokendialogue system.
In Submission.217
