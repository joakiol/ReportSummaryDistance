Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 12?21,Athens, Greece, 30 March, 2009. c?2009 Association for Computational LinguisticsComputational Linguistics and Generative Linguistics:The Triumph of Hope over ExperienceGeoffrey K. PullumSchool of Philosophy, Psychology, and Language SciencesUniversity of Edinburghgpullum@ling.ed.ac.ulAbstractIt is remarkable if any relationship at allpersists between computational linguists(CL) and that part of general linguis-tics comprising the mainstream of MITtransformational-generative (TG) theoreti-cal syntax.
If the lines are still open, it rep-resents something of a tribute to CL prac-titioners?
tolerance ?
a triumph of hopeand goodwill over the experience of abuse?
because the TG community has shownconsiderable hostility toward CL and ev-erything it stands for over the past fiftyyears.
I offer some brief historical notes,and hint at prospects for a better basis forcollaboration in the future.1 IntroductionThe theme of this workshop is the interaction be-tween computational linguistics (CL) and generallinguistics.
The organizers ask whether it hasit been virtuous, vicious, or vacuous.
They useonly three of the rather extraordinary number ofv-initial adjectives.
Is the relationship vital, valu-able, venturesome, visionary, versatile, and vi-brant?
Or vague, variable, verbose, and sometimesvexatious?
Has it perhaps been merely vestigialand vicarious, with hardly any general linguistsreally participating?
Or vain, venal, vaporous, vir-ginal, volatile, and voguish, yet vulnerable, a re-lationship at risk?
Or would the best descriptionuse adjectives like vengeful, venomous, vilifica-tory, villainous, vindictive, violent, vitriolic, vo-ciferous, and vulpine?I will argue that at least with respect to that partof general linguistics comprising the mainstreamof American theoretical syntax, it would be quiteremarkable if any relationship with computationallinguistics (CL) had thrived.
It would represent (asSamuel Johnson remarked cynically, and wrongly,about second marriages) a triumph of hope overexperience.
It seems to me that the relation-ship that could have been was at least somewhatblighted by the negative and defensive stance thatMIT-influenced transformational-generative (TG)syntacticians have adopted on a diverse array oftopics highly relevant to CL.There was never any need for such attitudes.And at the conclusion of these brief remarks I willsuggest a basis for thinking that relations could bemuch more satisfactory in the future.
But I thinkit is worth taking a sober look at the half-centuryof history from 1959 to 2009, during which al-most everything about the course of theoreticalsyntax, at least in the USA, where I worked dur-ing the latter half of the period, has been tacitlyguided by a single line of thinking.
?Generativegrammar?
is commonly used to denote it, but thatwill not do.
First, ?generative grammar?
is oftenused to mean ?MIT-influenced transformational-generative grammar?.
For that I will use the ab-breviation TG.
And second, it is sometimes (in-correctly) claimed that ?generative?
means noth-ing more or less than ?explicit?
(see Chomsky1966, 12: ?a generative grammar (that is, an ex-plicit grammar that makes no appeal to the reader?s?facule?
de langage?
but rather attempts to incorpo-rate the mechanisms of this faculty)?
).We need more precise terminology in order tohome in on what I am talking about.
As Seuren(2004) has stressed, the relevant vision of what agrammar is like, built into most linguistic theoriza-tion today at a level so deep that most linguists areincapable of seeing past it or out of it, is not justthat it is explicit, but that a grammar is and must bea syntax-centered random generator.
I will there-fore refer to language specification by randomgeneration (LSRG).The definitive technical paper defining gram-mars in LSRG terms is Chomsky (1959).
Thiswas a fine paper, which would have earned its12writer tenure in any department of linguistics,logic, computer science, or mathematics that knewwhat it was doing and could see the possibilities.But it brought into linguistics two things that werenot going to go away for half a century.
Onewas the notion that any formally precise linguis-tics had to be limited to LSRG.
And the other wasthe combative and insular personality of the pa-per?s author, which had such a great influence onthe personality of his extraordinarily important de-partment at MIT.2 Obsession with random generationThe sense of ?generate?
relevant to LSRG goesback to the work of the great mathematical lo-gician Emil Post (as acknowledged by Chomsky1959, 137n).
Post?s project was initially to for-malize the notion of proof in logical systems ?originally, the propositional logic that was infor-mally used but not formally defined in Whiteheadand Russell?s Principia Mathematica.
He repre-sented well-formed formulae (?enunciations?, in-cluding the ?primitive assertions?, i.e.
axioms, andthe ?assertions?
i.e.
theorems) to be simply stringsover a finite set of symbols, and rules of inference(?productions?)
as instructions for deriving a newstring (the conclusion) from a set of strings alreadyin hand (the premises).
He then studied the ques-tion of what kinds of sets of strings could be gen-erated if a set of initial strings were closed underthe operation of applying inference rules.
Post?srather ungainly general presentation of the generalconcept of rules of inference, or in his terms pro-ductions, looks like this:g11 Pi?1 g12 Pi?2 ?
?
?
g1m1 Pi?m1 g1m1+1g21 Pi?
?1 g22 Pi?
?2 ?
?
?
g2m2 Pi?
?m2 g2m2+1?
?
?
?
?
?
?
?
?
?
?
?gk1 Pi?k1gk2 Pi?k2?
?
?
gkmk Pi?kmkgkmk+1produceg1 Pi1 g2 Pi2 ?
?
?
gm Pim gm+1In specific instances of productions the g metavari-ables in this schema are replaced by actual stringsover what is now known as the terminal vocabu-lary.
The P metavariables function as cover sym-bols for arbitrary stretches of material ?
they arestring variables, some of which may be repeatedto copy material into the conclusion.
A produc-tion provides a license, given a set of strings thatmatch patterns of the form g0P1g1P2 ?
?
?Pkgk, toproduce a certain other string composed in someway out of the various gi and Pi.Post defined a class of canonical systems, eachconsisting of a set of initial strings and a finiteset of productions.
Sets generated by canonicalproduction systems he called canonical sets.
Posthad realized early on that the canonical sets werenothing more or less than the sets definable by re-cursive functions or Turing machines; that is, theywere just the recursively enumerable (r. e.) sets.He proceeded to prove that even if you restrictthe number and distribution of the gi and Pi ex-tremely tightly, expressive power may not be re-duced.
Specifically, he proved that no reductionin the definable sets is obtained even if you set thenumber of P variables and the number of premisesat 1, and require that every production has the form?g0P produces Pg1?.
Such very restricted sys-tems were called normal systems.
Normal sys-tems can still derive every canonical set, providedyou are allowed to use extra symbols that appearin derivations but not in the ultimately generatedstrings (these extra symbols are what would be-come known to formal language theorists withincomputer science as variables and to linguists asnonterminals).In a notation more familiar to linguists, the re-sult amounts to showing that every r. e. subset of?+ can be generated by some generative grammarusing a symbol vocabulary V = ?
?
N in whichall rules have the form ?xW ?
Wy?
for speci-fied strings x, y ?
V ?
and some fixed W ?
V ?.This was the first weak generative capacity result:normal systems are equivalent in weak generativecapacity to full canonical systems.In a later paper, settling a conjecture of Thue,Post showed (1947) that you can derive everycanonical set if your productions all have theform ?P1giP2 produces P1gjP2?.
This amountsto showing that every canonical subset of ?+ canbe generated by (what would later be called) agenerative grammar using a symbol vocabularyV = ?
?
N in which all rules have the form?WxZ ?
WyZ?
for specified strings x, y ?
V ?and fixed W,Z ?
V ?.Hence the first demonstration that unrestrictedrewriting systems (Chomsky?s ?type-0?
grammars)can derive any r. e. set was not original with Chom-sky (1959).
It had been published twelve yearsearlier by Post.Post had in effect invented what could be called13top-down random generators.
These randomlygenerate r. e. sets of symbols by expanding an ini-tial axiomatic string, which can be just a singlesymbol.
Their equivalence to Turing machines isobvious (Kozen 1997, 256?257).Between the time of Post?s doctoral work in1920 and the 1943 paper in which he publishedhis result on canonical systems (already presentin compressed form in his thesis), Ajdukiewicz(1935) had proposed a different style of genera-tive grammar, also motivated by the developmentof a better understanding of proof.
Adjukiewicz?sinvention was categorial grammar, the first kind ofbottom-up random generators.
It composes ex-pressions of the generated language by combiningparts ?
initially primitive categorized symbols,and then previously composed subparts.When Chomsky and Lasnik (1977) start talk-ing about the ?computational system?
of humanlanguage (a mode of speaking that rapidly caughton, and persists in current ?minimalist?
work), the?computation?
of which they spoke was one thattakes place nowhere: no such computations areever done, except perhaps using pencil and paperas a syntactic theorist tries to figure out how orwhether a certain string can be derived.
This ?com-putational system?
attempts randomly and nonde-terministically to find some way to apply rules inorder to build a particular structure, starting froman arbitrary syntactic starting point.In the case of pre-1990 work the starting pointwas apparently a start symbol; in post-1990 ?min-imalist?
work it is a numeration: a randomly cho-sen multiset of categorized items from the lexi-con.
The concept of a ?numeration?
is a reflectionof how firmly embedded the random-generationidea is.
The numeration serves no real purpose.It would be possible to formalize a grammar as aset of combinatory principles for putting togetherwords in a string as encountered, from first to last,so that it was in effect a parser.
Categorial gram-mars seem ideally suited to that role (Steedman,2000), and minimalist grammars are really just avariety of categorial grammar, stripped of some ofthe formal coherence and links to logic and seman-tics.Chomsky has often written as if it were a neces-sary truth that a grammar must be a random gener-ator.
For example: ?Clearly, a grammar must con-tain .
.
.
a ?syntactic component?
that generates aninfinite number of strings representing grammat-ical sentences .
.
.
This is the classical model forgrammar?
(Chomsky 1962, 539).
This says that agrammar must be a random generator.
But this isnot true.
A grammar could in principle be formu-lated as, say a transducer mapping phonetic rep-resentation inputs to corresponding sets of logicalforms.
(Presumably this must be possible, givenwhat human beings do.
)It is particularly strange to see Chomsky ignor-ing this possibility and yet asserting in Knowl-edge of Language (Chomsky, 1986b) that a per-son?s internalized grammar ?assigns a status to ev-ery relevant physical event, say, every sound wave?(p.
26).
The claim is false, simply because ran-dom generators are not transducers or functions:they do not take inputs.
A random generator only?assigns a status?
to a string by generating it witha derivation that associates it with certain prop-erties.
And surely it is not a sensible hypothesisabout human linguistic competence to posit that inthe brain of every human being there is an inter-nalized random generator generating every phys-ically possible sequence of sounds, from a ship?sfoghorn to Mahler?s ninth symphony.3 Downplaying expressive powerPerhaps the most centrally important reason forlinguists?
concern with the possibility of excessexpressive power in grammar formalisms wastheir sense that it should be guaranteed by the gen-eral theory of grammar that linguistic behaviorssuch as understanding a sentence should be repre-sented as at least possible.
This meant that gram-mars had to be defined in a way that at least madethe general membership problem (?Given gram-mar G, is string w grammatical??)
decidable.It was in Chomsky?s 1959 paper that progresswas first made toward restricting the expressivepower of production systems in ways that achievedthis, and the early work on topics like pushdownautomata and finite state machines shows thatthose topics were of interest.As is well known, Chomsky showed that ifproductions of the general form X?Z ?
X?Z(where X,?,Z are strings in V ?
and ?
?
V +) arelimited by the condition that ?
is no shorter than?, we are no longer able to derive every r. e. set ofstrings over the alphabet; we get only the context-sensitive stringsets.
If the further limitation that?
?
N is imposed, we get only the context-freestringsets.
And if on top of that the requirement14that ?
?
(?
?
?N) is imposed, we get only theregular sets.Chomsky?s 1959 position was that the set ofall grammatical English word sequences was nota regular stringset over the set of English words,and that if any context-free grammar for Englishcould be constructed, it would not be an elegant orrevealing one.
The search for intuitively adequategrammars therefore had to range over the class ofgrammars generating context-sensitive stringsets.This is a large class of grammars, but at least it isa proper subset of the class of grammars for whichthe membership recognition problem is decidable.Casting around outside that range was probablynot sensible, since natural languages surely had tobe decidable (it was taken to be quite obvious thatnative speakers could rapidly recognize whether ornot a string of words was a sentence in their lan-guage).As I have detailed elsewhere in somewhattongue-in-cheek fashion (Pullum, 1989), Chom-sky pulled back sharply from his initial interestin mathematical study of linguistic formalisms asit became clear that TG theories were being criti-cized for their Turing-equivalence, and began dis-missing precise studies of the generative capac-ity of grammars as trivial and ridiculous.
This, itseems to me, was one more clear sign of distanc-ing from the concerns of CL.
It was mainly com-putational linguists who showed interest in Gaz-dar?s observation that a theory limited to gener-ating context-free languages could guarantee notjust recognition but recognition in polynomial (in-deed, better than cubic) time, and in the relatedobservation that none of the arguments for non-context-free characteristics in human languagesseemed to be good ones (Pullum and Gazdar,1982).The MIT reaction to Gazdar?s suggestion wasto mount a major effort to find intractabilityin Gazdar-style (GPSG) grammars ?
to repre-sent the recognition problem as NP-hard evenfor context-free-equivalent theories of grammar(Barton et al, 1987).
This was something ofa confidence trick.
First, the results dependedon switching attention from the fixed-grammararbitrary-string recognition problem (the analogof what Vardi (1982) calls data complexity) tothe variable-grammar arbitrary-string recognitionproblem (what Vardi calls combined complexity).Second, it seemed to be vaguely assumed that onlyGPSG had any charges to answer, and that theGB theory of that time (Chomsky, 1981) wouldnot suffer from similar computational complexityproblems, but GB eventually turned out to be, in-sofar as it was well defined, strongly equivalent toGazdar?s framework (Rogers, 1998).For pre-GB varieties of TG, however, the prob-lem had mainly been not that recognition was NP-hard but that it was not computable at all: trans-formational grammars from 1957 on kept provingto be Turing-equivalent.
That was what seems tohave driven the denigration of mathematical lin-guistics, and the downplaying of the relevance ofdecidability to such an extreme degree (see e.g.Chomsky 1980: 120ff, where the very idea thatrecognition is decidable is dismissed as an unim-portant detail, and not necessarily even a trueclaim).4 Hostility to machine testingWith many versions of TG offering no guaran-tee that there was any parser for the languageeven in principle, it was not clear that machinetesting of grammatical theories by algorithmicchecking of claims made about grammaticality ofselected strings was a plausible idea.
Perhapsmachine theorem-proving algorithms could havebeen adapted to showing that a certain grammarcould indeed derive a certain string, but in prac-tice early transformational grammar was vastly toocomplex to permit the building of tools for gram-mar testing, and later transformational grammarfar too vague.I know of only one success story in grammarevaluation by implementing random generation,in fact.
Ed Stabler (1992) coded up a Prologgrammaticality-proving system based on the Bar-riers theory of transformational grammar (Chom-sky, 1986a), which (Pullum, 1989) had mocked forsloppiness of statement.
The Barriers system hadin particular abandoned the usual practice of defin-ing trees in a way that had dominance as a reflexiverelation.
Chomsky casually asserted that he wouldtake it to be irreflexive.
Moreover, Stabler?s care-ful and sympathetic reconstruction of Chomsky?sintent defines the notion of ?exclusion?
in such away that every node excludes itself (Chomsky?sdefinition said that ??
excludes ?
if [and only if]no segment of ?
dominates ?
?, and of course agiven ?
never dominates itself).
And sure enough,the Stabler implementation revealed that this sys-15tem of definitions had a problem: unbounded de-pendency constructions that Chomsky took to beallowed were in fact blocked by his theoretical ma-chinery.Stabler concluded from his discovery ?that theproject of implementing GB theories transparentlyis both manageable and worthwhile?.
But his pa-per has essentially never been referred to by anymainstream syntacticians.
It was not exactly whatthey wanted to hear.
Nor has anyone, to my knowl-edge, utilized Stabler?s experience in doing syn-tactic research using the Barriers framework.There has in any case traditionally been con-siderable resistance to machine testing of theo-ries.
I have heard a story told by MIT linguistsof how one early graduate student devised a com-puter program to test the rule system of SPE, andtold Morris Halle about some of the bugs he hadthereby found, but Halle had already noticed all ofthem.
The moral of the story is clearly supposedto be that machine testing is unneeded and of novalue.Mark Johnson as an undergraduate did somework showing that the Unix stream editor sedcould serve as an excellent tool for implementingsystems of ordered historical sound changes forthe assistance of comparative-historical linguists;but this very sensible idea never led to widespreadtesting of synchronic phonological ordered-ruleanalyses.In short, computational testbeds, however en-thusiastically developed in some areas of science(chemistry, astrophysics, ecology, molecular biol-ogy), simply never (yet) took off in linguistic sci-ence.5 Loathing of corporaThere has traditionally been hostility even tomachine data-hunting or language study throughcomputer-searchable corpora.
This is fading awayas a new generation of young linguists who do ev-erything by searching the web do their data by websearch too; but it held back collaboration for a longtime.
Early proposals for amassing computer cor-pora were treated with contempt by TG grammar-ians (?I?m a native speaker, I have intuitions; whydo I need your arbitrary collection of computer-searchable text??
).And quite often evidence from attested sen-tences is simply dismissed.
To take a random ex-ample, on page 48 of Postal (1971) the sentencesI am annoying to myself is prefixed with an as-terisk to show that it is ungrammatical.
Searchingfor this exact strings using Google, as we can dotoday, reveals that it gets 229 hits.
I take this multi-ple attestation to shift the burden overwhelminglyagainst the linguist who claims that it is barred bythe grammar of the language.
But anyone who hasexperience (as I do) with trying to talk TG lin-guists out of their beliefs by citing attested sen-tences will know that it is between the difficult andthe impossible.
From ?There are many errors inpublished works?
to ?It may be OK for him, butit?s not for me?, there are many ways in which thelinguist can escape from the conclusion that a ma-chine has proved superior in assessing the data.Hostility to corpus work has probably to someextent paved the way for the present situation,where the machine translation teams at Google?sresearch labs has no linguists, the work dependingentirely on heavily numerical tracking of statisti-cal parallels seen in aligned bilingual texts.And an unwholesome split is visible in the lin-guistics community between those who broadlywant nothing to do with corpora and think personalintuitions are fine as a basis for data gathering, andthe people that I have called corpus fetishists whotreat all facts as unclean and unholy unless theycome direct and unedited out of a corpus.
At theextremes, we get a divide between dreamers andtoken-counters ?
on the one hand, people whothink that speculations on how universal principlesmight account for subtle shades of their own innerreactions to particular sentences, and on the other,people who think that counting the different pro-nouns in ten million words of text and tabulatingthe results is a contribution to science.6 Aversion to the stochasticMention of statistics reminds us that stochasticmethods have revolutionized CL since the 1980s,but have made few inroads into general linguis-tics, and none into TG linguistics.
This is despitethe excellent introduction to probabilistic genera-tive grammars provided in Levelt?s excellent andfar-sighted introduction to mathematical linguis-tics (Levelt, 1974), the first volume of which hasnow been republished separately (Levelt, 2008).The reason for the extraordinarily low profile ofprobabilistic grammars within the ranks of TG lin-guists has to do with the very successful attack onthe very possibility of their relevance in Syntac-16tic Structures (Chomsky, 1957).
Insisting that anystatistical model for grammaticality would haveto treat Colorless green ideas sleep furiously andFuriously sleep ideas green colorless in exactlythe same terms, as they are word strings with thesame (pre-1957) frequency of zero, Chomsky ar-gued that probability of a string had no conceiv-able relevance to its grammaticality.Unfortunately he had made a mistake.
He wastacitly assuming that the probability of an eventtype that has not yet occurred must be zero.
Max-imum likelihood estimation (MLE) does indeedyield that result; but Chomsky was not obliged toadopt MLE.
The technique now known as smooth-ing had been developed during the Second WorldWar by Alan Turing and I. J.
Good, and althoughit took a while to become known, Good had pub-lished on it by 1953.
Chomsky was simply notacquainted with the statistical literature and not in-terested in applying statistical methods to linguis-tic material.
Most linguists for the next forty yearsfollowed him in his disdain for such work.
Butwhen Pereira (2000) finally applied Good-Turingestimation (smoothing) to the question of how dif-ferent the probabilities of the two famous word se-quences are from normal English text, he foundthat the first (the syntactically well-formed one)had a probability 200,000 times that of the second.7 Contempt for applicationsTheoretical linguists have tended to have an al-most total lack of interest in anything that mightoffer a practical application for their theories.Most kinds of science tend eventually to supportsome sort of engineering or practical techniques:physics led to jet planes; geology gave us oil lo-cation methods; biology brought forth gene splic-ing; even logic and psychology have applicationsin factories and other workplaces.
But not main-stream theoretical linguistics.
Its theories do notseem to yield applications of any sort.Very early on, Chomsky found that he had todistance himself from computers altogether: notethe remark in Chomsky (1966, 9) that ?Quite afew commentators have assumed that recent workin generative grammar is somehow an outgrowthof an interest in the use of computers for oneor another purpose, or that it has some engineer-ing motivation?, and note that he calls such viewsboth ?incomprehensible?
and ?entirely false?.
Be-ing taken to have ambitions relating to natural lan-guage processing was at that time clearly anath-ema for the leader of the TG community.What takes the place of application of theoriesto practical domains today, since nothing has comeof any computational TG linguistics, is an attemptto derive conclusions about human brain organiza-tion and mental anatomy.
Linguists claim to be bi-ologists rather than psychologists (psycholinguis-tics developed its own experimental paradigmsand began its own steady progress away from in-teraction with TG linguistics).
There is a journalcalled Biolinguistics now, and much talk about in-terfaces and evolution and perfection.
Linguistssomehow live with the fact that the real biologistsand neurophysiologists are not getting involved.It is probably this pretense at uncovering deepprinciples of structure in a putative mental organ(and pretense is what it is) that is responsible forthe dramatic falling off of interest in precise de-scription of languages.
Getting the details right ?what was described as ?observational adequacy?
inAspects (Chomsky, 1965) ?
is taken to be a low-prestige occupation when compared to one that isalleged to offer glimpses of universal principlesthat hold the key to language acquisition and theinnate cognitive abilities of the species.Yet these universal principles are never actuallypresented for examination in the way that genuineresults in science are.
It is as if what is impor-tant to the hunter after universal principles is thehunt itself, the call of the horn and the thrill of thechase, but not the grubby business of examiningand weighing the kill.
The fact is that no reallyrobust and carefully formulated universals of lan-guage have been discovered, described, promul-gated, confirmed, and widely accepted as correctin the fifty years that universals have been sought.The notion that linguists have discovered innateprinciples that solve the mystery of first languageacquisition (Scholz and Pullum, 2006) is partic-ularly pernicious.
The position generally advo-cated by TG linguists is widely known as linguis-tic nativism, and it says that some significant as-pects of knowledge of language are not derivedfrom any experience but are innately known.
Butwhen pressed on the question of what the evidenceshows about linguistic nativism, about whetherit can really be defended against its plausible ri-vals, nativists tend to react by drawing back verysharply into a trivial form of the thesis: of courselinguistic nativism must be true, they insist, be-17cause when you raise a baby and a kitten in thesame household under the same conditions it isonly the baby ends up with knowledge of lan-guage.
They therefore differ in some respect, in-nately.
?Universal grammar?
is simply one namethat linguists use for that which separates them:whatever it is that human infants have but kittensand monkeys and bricks don?t.But of course, that makes the thesis trivial: itis true in virtue of being merely a restatement ofthe observation that led to linguistic nativism be-ing put forward.
We know that it is only humanneonates who accomplish the language acquisitiontask, and that is why we are seeking an explanatorytheory of how humans accomplish the task.
To saythat there must be something special about them iscertainly true, but that does not count as a scien-tific discovery.
We need specifics.
Serious scien-tists are like the private sector as characterized inthe immortal line uttered by Ray Stantz (playedby Dan Ackroyd) in Ghostsbusters: ?I?ve workedin the private sector.
They expect results!
?8 Hope for the futureIt is absolutely not the case that general and the-oretical linguistics should continue to act as if themain object were to prevent any interaction withCL.
Let me point to a few hopeful developments.Over the period from about 1989 to 2001, ateam of linguists worked on and completed a trulycomprehensive informal grammar of the Englishlanguage.
It was published as Huddleston and Pul-lum et al (2002), henceforth CGEL.
It is an infor-mal grammar, intended for serious academic usersbut not limited to those with a linguistics back-ground.
And it comes close to being fully exhaus-tive in its coverage of Standard English grammat-ical constructions and morphology.It should not be forgotten that the era of TG,though it produced (in my view) no theories thatare really worth having, an enormous number ofinteresting data discoveries about English weremade.
CGEL profited greatly from those, as theFurther Reading section makes clear.
But does notattempt to develop theoretical conclusions or par-ticipate in theoretical disputes.
Wherever possible,CGEL takes a largely pretheoretic or at least basi-cally neutral stance.Where theoretical commitments have to bemade explicit, they are, but they are then imple-mented in consistent terms across the entire book.Although more than a dozen linguists were in-volved, it is not an anthology; Huddleston and Pul-lum provide a unitary authorial voice for the bookand rewrote every part of the book at least once.When disputes about analyses arose between theauthors who drafted different chapters, they weresettled one way or the other by recourse to evi-dence, and not permitted to create departures fromconsistency in the book as a whole.CGEL was preceded by large-scale 3-volumegrammars for Italian (Renzi et al, 2001) and forSpanish (Bosque and Demonte, 1999), and now agrammar of French on a similar scale, the GrandeGrammaire du franc?ais is being written by a teamof linguists in Paris under the leadership of AnneAbeille?
(Paris 7), Annie Delaveau (Paris 10), andDanie`le Godard (CNRS).
In 2006 I visited Paris atthe request of that team to give a workshop on themaking of CGEL.
Work continues, and the book isnow planned for publication by Editions Bayard in2010.
If anything the scope of this work is broaderthan CGEL?s, since CGEL did not aim to coveruncontroversially non-standard dialects of English(for example, those that have negative concord),whereas the Grande Grammaire explicitly aimsto cover regional and non-standard varieties ofFrench.
Additionally, an effort to produce a com-parable grammar of Mandarin Chinese is now be-ing mounted in Hong Kong under the directorshipof Professor Chu-Ren Huang, the dean of the newFaculty of Humanities at Hong Kong PolytechnicUniversity.
I gave a workshop on CGEL there (inMarch 2009) too.The importance of these projects is simply thatthey bear witness to the fact that, at least in someareas, there are linguists ?
and not just isolatedindividuals but teams of experienced linguists ?who are prepared to get involved in detailed lan-guage description of the type that will be a prereq-uisite to any future computational linguistics thatrelies on details of syntax and semantics (ratherthan probabilistic number-crunching on n-gramsand raw text, which has its own interest but doesnot involve input from linguistics or even a rudi-mentary knowledge of the language being pro-cessed).
Among them are both traditional generallinguists like Huddleston and people with seriousCL experience like Abeille?
and Huang.But there is more.
I have made a prelimi-nary analysis of the inventory of syntactic cate-gories used in the tagging for labelling trees in the18Penn Treebank (Marcus et al, 1993), comparingthem to the categories used in CGEL.
I would de-scribe the fit as not perfect, but within negotiatingrange.
In some ways the fit is remarkable, giventhe complete independence of the two projects(the Treebank under Mitch Marcus in Philadelphiawas largely complete by 1992, when the CGELproject under the direction of Rodney Huddlestonin Australia was only just getting up to speed, butHuddleston and Marcus did not know about eachother?s work).The biggest discrepancy in categorization is inthe problematic area of prepositions, adverbs, andsubordinating conjunctions, where the Treebankhas remained much too close to the confused oldertradition (where many prepositions are claimed tohave second lives as adverbs and quite a few arealso included on the list of subordinating conjunc-tions, so that a word like since has one mean-ing but three grammatical categories).
The heartof the problem is that the sage counsel of Jes-persen (1924, 87?90) and the cogent argumentsof Emonds (1972) were not taken under consid-eration by the devisers of the Treebank?s taggingcategories.
But fixing that would involve nothingmore than undoing some unmotivated partitioningof the preposition category.Since there are few if any significant disagree-ments about bracketing, and the category systemscould be brought into alignment, I believe it wouldnot be a major project to convert the entire PennTreebank into an alternate form where it was to-tally compatible with CGEL in the syntactic anal-yses it presupposed.
There could be considerablevalue in a complex of reference tools that includeda treebank of some 4.5 million words that is fullycompatible in its syntactic assumptions with an1,860-page reference grammar of high reliabilityand consistency.And there is yet more.
Here I will be brief,and things will get slightly technical.
The questionnaturally arises of how one might formalize CGELto get it in a form where it was explicit enough foruse as a database that natural language process-ing systems could in principle make use of.
JamesRogers and I have recently considered that ques-tion (Pullum and Rogers, 2008) within the con-text of model-theoretic syntax, a line of work thatfirst began to receive sophisticated formulationshere at the EACL in various papers of the early1990s (e.g.
Blackburn et al (1993), Kracht (1993),Blackburn & Gardent (1995); see Pullum (2007)for a brief historical survey, and Pullum & Scholz(2001) for a deeper treatment of relevant theoreti-cal issues).One thing that might appear to be a stumbling-block to formalizing CGEL, and an obstacle to therelationship with treebanks as well, is that strictlyspeaking CGEL?s assumed syntactic representa-tions are not (or not all) trees.
They are graphs thatdepart from being ordinary constituent-structuretrees in at least two respects.First, they are annotated not just with cate-gories labelling the nodes, but also with syntacticfunctions (grammatical relations like Subject-of,Determiner-of, Head-of, Complement-of, etc.
)that are perhaps best conceptualized as labellingthe edges of the graph (the lines between the nodesin the diagrams).Second, and perhaps more seriously, there is oc-casional downward convergence of branches: itis permitted for a given constituent, under certainconditions, to bear two different grammatical rela-tions to two different superordinate nodes.
(A de-terminative like some, for example, may be boththe Determiner of an NP and the Head of theNominal that is the phrasal head of that NP.)
Often(as in HPSG work) the introduction of re-entrancyhad dramatic consequences for key properties likedecidability of satisfiability for descriptions, oreven for model-checking.
(I take it that the for-mal issues around HPSG are very well known tothe EACL community.
In this short paper I do nottry to deal with HPSG at all.
There is plenty to besaid, but also plenty of excellent HPSG specialistsin Europe who are more competent than I am totreat the topic.
)Pullum & Rogers (2008) shows, however, thatgiven certain very weak conditions, which seemalmost certainly to be satisfied by the kinds ofgrammatical analysis posited in grammars of theCGEL sort, there is a way of constructing a com-patible directed ordered spanning tree for anyCGEL-style syntactic structure in such a waythat no information is lost and reachability viaedge chains is preserved.
Moreover, the map-ping between CGEL structures and spanning treesis definable in weak monadic second-order logic(wMSO).Put this together with the results of Rogers(1998) on definability of trees in wMSO, and thereis a clear prospect of the CGEL analysis of En-19glish syntax being reconstructible in terms of thewMSO theory of trees.
And what that means forparsing is clear from results of nearly 40 yearsago (Doner, 1970): there is a strong equivalencevia tree automata to context-free grammars, whichmeans that all the technology of context-free pars-ing can potentially be brought to bear on process-ing them.This does not mean it would be a crisis if somelanguage of interest is found to be non-context-free, incidentally.
By the results of Rogers (2003),wMSO theories interpreted on tree-like structuresof higher dimensionality than 2 could be em-ployed.
For example, where the structures are 3-dimensional (so that individual nodes are allowedto bear the parent-of relation to all of the nodesin entire 2-dimensional trees), the string yield ofthe set of all structures satisfying a given wMSOsentence is always a tree-adjoining language, andfor every tree-adjoining language there is such acharacterizing wMSO sentence.Notice, by the way, that the theoretical toolsof use here are coming out of currently very ac-tive subdisciplines of computational logic and au-tomata theory, such as finite model theory, descrip-tive complexity theory, and database theory.
Thevery tools that linguistics needs in order to for-malize syntactic theories in a revealing way arethe ones that theoretical computer science is in-tensively working on because their investigation isintrinsically interesting.To sum up, what this is all telling us is thatthere is no reason for anyone to continue be-ing guided by the TG bias toward isolating the-oretical linguistics from CL.
There is not neces-sarily a major gulf between (i) cutting-edge cur-rent theoretical developments like model-theoreticsyntax, (ii) large-scale descriptive grammars likeCGEL, and (iii) feasible computational natural-language engineering.
Given the excellent per-sonal relations between general linguists and com-putational linguists in some European locations(Edinburgh being an excellent example), it seemsto me that developments in interdisciplinary rela-tions that would integrate the two disciplines quitethoroughly could probably happen quite fast.
Per-haps it is happening already.AcknowledgmentsI am very grateful to Barbara Scholz for her de-tailed criticisms of a draft of this paper.
I havetaken account of many of her helpful suggestions,but since she still does not agree with what I sayhere, none of the failings or errors above should beblamed on her.ReferencesKazimierz Ajdukiewicz.
1935.
Die syntaktische kon-nexita?t.
Studia Philosophica, 1:1?27.
Reprinted inStorrs McCall, ed., Polish Logic 1920?1939, 207?231.
Oxford: Oxford University Press.G.
Edward Barton, Robert C. Berwick, and Eric SvenRistad.
1987.
Computational Complexity and Natu-ral Language.
MIT Press, Cambridge, MA.Patrick Blackburn and Claire Gardent.
1995.
A spec-ification language for lexical functional grammars.In Seventh Conference of the European Chapter ofthe Association for Computational Linguistics: Pro-ceedings of the Conference, pages 39?44, Morris-town, NJ.
European Association for ComputationalLinguistics.Patrick Blackburn, Claire Gardent, and WilfriedMeyer-Viol.
1993.
Talking about trees.
In SixthConference of the European Chapter of the Associa-tion for Computational Linguistics: Proceedings ofthe Conference, pages 21?29, Morristown, NJ.
Eu-ropean Association for Computational Linguistics.Ignacio Bosque and Violeta Demonte, editors.
1999.Grama?tica Descriptiva de La Lengua Espan?ola.Real Academia Espan?ola / Espasa Calpe, Madrid.
3volumes.Noam Chomsky and Howard Lasnik.
1977.
Filters andcontrol.
Linguistic Inquiry, 8:425?504.Noam Chomsky.
1957.
Syntactic Structures.
Mouton,The Hague.Noam Chomsky.
1959.
On certain formal propertiesof grammars.
Information and Control, 2:137?167.Reprinted in Readings in Mathematical Psychology,Volume II, ed.
by R. Duncan Luce, Robert R. Bush,and Eugene Galanter, 125?155, New York: John Wi-ley & Sons, 1965 (citation to the original on p. 125of this reprinting is incorrect).Noam Chomsky.
1962.
Explanatory models in linguis-tics.
In Ernest Nagel, Patrick Suppes, and AlfredTarski, editors, Logic, Methodology and Philosophyof Science: Proceedings of the 1960 InternationalCongress, pages 528?550, Stanford, CA.
StanfordUniversity Press.Noam Chomsky.
1965.
Aspects of the Theory of Syn-tax.
MIT Press, Cambridge, MA.Noam Chomsky.
1966.
Topics in the Theory of Gener-ative Grammar.
Mouton, The Hague.Noam Chomsky.
1980.
Rules and Representations.Basil Blackwell, Oxford.20Noam Chomsky.
1981.
Lectures on Government andBinding.
Foris, Dordrecht.Noam Chomsky.
1986a.
Barriers.
MIT Press, Cam-bridge, MA.Noam Chomsky.
1986b.
Knowledge of Language: ItsOrigins, Nature, and Use.
Praeger, New York.John Doner.
1970.
Tree acceptors and some of theirapplications.
Journal of Computer and System Sci-ences, 4:406?451.Joseph E. Emonds.
1972.
Evidence that indirect ob-ject movement is a structure-preserving rule.
Foun-dations of Language, 8:546?561.Rodney Huddleston, Geoffrey K. Pullum, et al 2002.The Cambridge Grammar of the English Language.Cambridge University Press, Cambridge.Otto Jespersen.
1924.
The Philosophy of Grammar.Holt, New York.Dexter Kozen.
1997.
Automata and Computability.Springer, Berlin.Marcus Kracht.
1993.
Mathematical aspects of com-mand relations.
In Sixth Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics: Proceedings of the Conference, pages240?249, Morristown, NJ.
Association for Compu-tational Linguistics.W.
J. M. Levelt.
1974.
Formal Grammars in Lin-guistics and Psycholinguistics.
Volume I: An Intro-duction to the Theory of Formal Languages andAutomata; Volume II: Applications in LinguisticTheory; Volume III: Psycholinguistic Applications.Mouton, The Hague.W.
J. M. Levelt.
2008.
An Introduction to the Theory ofFormal Languages and Automata.
John Benjamins,Amsterdam.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Emil Post.
1947.
Recursive unsolvability of a problemof thue.
Journal of Symbolic Logic, 12:1?11.Paul M. Postal.
1971.
Crossover Phenomena.
Holt,Rinehart and Winston, New York.Geoffrey K. Pullum and Gerald Gazdar.
1982.
Naturallanguages and context-free languages.
Linguisticsand Philosophy, 4:471?504.Geoffrey K. Pullum and James Rogers.
2008.
Ex-pressive power of the syntactic theory implicit in thecambridge grammar of the english language.
Pa-per presented at the annual meeting of the Linguis-tics Assocition of Great Britain, University of Es-sex, September 2008.
Online at http://ling.ed.ac.uk/?gpullum/EssexLAGB.pdf.Geoffrey K. Pullum and Barbara C. Scholz.
2001.On the distinction between model-theoretic andgenerative-enumerative syntactic frameworks.
InPhilippe de Groote, Glyn Morrill, and ChristianRetore?, editors, Logical Aspects of ComputationalLinguistics: 4th International Conference, num-ber 2099 in Lecture Notes in Artificial Intelligence,pages 17?43, Berlin and New York.
Springer.Geoffrey K. Pullum.
1989.
Formal linguistics meetsthe Boojum.
Natural Language & Linguistic The-ory, 7:137?143.Geoffrey K. Pullum.
2007.
The evolution of model-theoretic frameworks in linguistics.
In James Rogersand Stephan Kepser, editors, Model-Theoretic Syn-tax at 10: ESSLLI 2007Workshop, pages 1?10, Trin-ity College Dublin, Ireland.
Association for Logic,Language and Information.Lorenzo Renzi, Giampaolo Salvi, and Anna Cardi-naletti.
2001.
Grande grammatica italiana di con-sultazione.
Il Mulino, Bologna.
3 volumes.James Rogers.
1998.
A Descriptive Approach toLanguage-Theoretic Complexity.
CSLI Publica-tions, Stanford, CA.James Rogers.
2003. wMSO theories as grammar for-malisms.
Theoretical Computer Science, 293:291?320.Barbara C. Scholz and Geoffrey K. Pullum.
2006.
Ir-rational nativist exuberance.
In Robert Stainton, ed-itor, Contemporary Debates in Cognitive Science,pages 59?80.
Basil Blackwell, Oxford.Pieter A. M. Seuren.
2004.
Chomsky?s Minimalism.Oxford University Press, Oxford.Edward P. Stabler, Jr. 1992.
Implementing governmentbinding theories.
In Robert Levine, editor, FormalGrammar: Theory and Implementation, pages 243?289.
Oxford University Press, New York.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA.Moshe Y. Vardi.
1982.
The complexity of relationalquery languages.
In Proceedings of the 14th ACMSymposium on Theory of Computing, pages 137?146, New York.
Association for Computing Machin-ery.21
