L ingu is t ic  Knowledge  Acqu is i t ion  f rom Pars ing  Fai luresMasaki KIYONO* and Jun-ichi TSUJII(kiyono@ccl.umist.ac.uk and tsujii@ccl.umist.a~.uk)Centre for Computational LinguisticsUniversity of Manchester Institute of Science and TechnologyPO Box 88, Manchester M60 1QDUnited KingdomAbstractA semi-automatic procedure of linguisticknowledge acquisition is proposed, whichcombines corpus-based techniques with theconventional rule-based approach.
Therule-based component generates all the pos-sible hypotheses of defects which the ex-isting linguistic knowledge might contain,when it fails to parse a sentence.
Therule-based component does not try to iden-tify the defects, but generates a set of hy-potheses and the corpus-based componentchooses the plausible ones among them.The procedure will be used for adapting orre-using existing linguistic resources for newapplication domains.1 IntroductionWhile quite a number of useful grammar formalismsfor natural language processing now exist, it still re-mains a time-consuming and hard task to developgrammars and dictionaries with comprehensive cov-erage.
It is also the case that, though quite a fewcomputational grammars and dictionaries with com-prehensive coverage have been used in various ap-plication systems, to re-use them for other applica-tion domains is not always so easy, even if we usethe same formalisms and programs uch as parsers,etc.
We usually have to revise, add, and deletegrammar ules and lexical entries in order to adaptthem to the peculiarities of languages ( ublanguages)of new application domains \[Sekine et al, 1992;Tsujii et al, 1992; Ananiadou, 1990\].
*also a staff member of Matsushita Electric IndustrialCo.,Ltd., Tokyo, JAPAN.Such adaptations of existing linguistic knowledgeto a new domain are currently performed throughrather undisciplined, trial and error processes in-volving much human effort.
In this paper we showthat techniques imilar to those in robust parsingof ill-formed input, together with corpus-based tech-niques, can be used to discover disparities betweenexisting linguistic knowledge and actual language us-age in a new domain, and to hypothesize new gram-mar rules or lexical descriptions.Although our framework appears similar to gram-mar learning from corpora, our current goal is farmore modest, i.e.
to help linguists revise existinggrammars by showing possible defects and hypothe-sizing them through corpus analysis.2 Robust Parsing and LinguisticKnowledge Acquisition2.1 Search Space of  Possible HypothesesWhen a parser fails to analyse an input sentence,a robust parser hypothesizes possible errors in theinput in order to complete the analysis and correcterrors \[Douglas and Dale, 1992\]: for example, dele-tion of necessary words (Ex.
I have book), insertionof unnecessary words (Ex.
I have a the book), dis-order of words (Ex.
I a book have), spelling errors(Ex.
I have a bok), etc.As there is usually a set of possible hypotheses tocomplete the analysis, this error detection processbecomes non-deterministic.
Furthermore, allowingoperations such as deletion and insertion of arbi-trary sequences of words or unrestricted permuta-tion of word sequences, radically expands its searchspace.
The process generates many nonsensical hy-potheses unless we restrict the search space eitherby heuristies-based cost functions \[Mellish, 1989\], or222Type of FailuresRemaining Constituentsto be CollectedFailure of Applicationof an Existing RuleUnrecognized Sequenceof CharactersRobust Parsinghypotheses of- deletion of necessary words- insertion of unnecessary words- disorder of wordsrelaxation of- feature agreementshypotheses of- spelling errorsKnowledge Acquisitionhypotheses of- lack of necessary rulesidentification of- disagreeing featureshypotheses of- n e w  wordsTable 1: Types of Hypothesesby introducing prior knowledge about regularities oferrors in the form of annotated rules \[Goeser, 1992\].On the other hand, our framework of knowledgeacquisition from parsing failures does not assumethat the input contains errors, but instead, assumesthat linguistic knowledge of the system is incomplete.This means that we do not need to, or should not,allow the costly operations of changing input, andtherefore the search space explosion encountered bya robust parser does not occur.For example, when a string of characters which isnot registered in the dictionary as a word appears,a robust parser may assume that there are spellingerrors and try to identify the errors by changingthe character string (deleting characters, adding newcharacters, etc.)
to find the "closest" legitimate wordin the dictionary.
This is because the dictionary isassumed to be complete, e.g.
that it contains all lex-ical items that will appear.
On the other hand, wesimply hypothesize that the string of characters i aword which should be registered in the dictionary,together with the lexical properties that are compat-ible with those hypothesized from the surroundingsyntactic/semantic context in the input.Table 1 shows different types of hypotheses tobe produced by a robust parser and a program forknowledge acquisition from parsing failures.Although the assumption of legitimacy of input re-duces significantly the size of the search space, theassumption of incomplete linguistic knowledge intro-duces another type of non-determinism and poten-tially a very large search space.
For example, evenif a word is registered in the dictionary as a noun, itcan have in theory arbitrary parts of speech such asverb, adjective, adverb, etc., as there is no guaranteethat the current dictionary exhausts all possible us-ages of the word.
A simple method will end up withan explosion of hypotheses.2.2 Corpus-based Knowledge Acquisit ionApart from the differences in types of hypotheses,an essential difference xists in the very nature oferrors in the two paradigms.
While errors in ill-formed input, by definition, are supposed not to showany significant regularity incompleteness or "linguis-tic knowledge rrors" are supposed to be observedrecurrently in a corpus.hFrom the practical viewpoint of adaptation ofknowledge to a new application domain, disparitiesbetween existing knowledge and actual anguage us-ages which are manifested only rarely in a reasonablesize sample corpus, are less significant han those re-currently observed.
Furthermore, unlike robust pars-ing, we do not need to identify causes of parsing fail-ures at the time of parsing.
That is, though there isin general a set of hypotheses which equally explainparsing failures of single sentences, we can choose themost plausible ones by observing statistical proper-ties (for example, frequencies) of the same hypothe-ses generated in the analysis of a whole corpus.
Thiswould be a reasonable approach, as significant dis-parities between knowledge and actual usages aresupposed to be observed recurrently.One of the crucial differences between the twoparadigms, therefore, is that unlike robust parsing,we need not narrow down the number of hypothe-ses to one by using heuristics based on cues insidesingle sentences.
Multiple hypotheses are not seri-ously damaging, though it is desirable for them tobe reasonably restricted.
The final decision will bemade through the observation of hypotheses gener-ated from the analysis of a whole corpus.3 Formal i sm and the  Parser3.1 Linguistic Knowledge to be AcquiredThe formalism and linguistic theories which onechooses as the bases for grammatical learning largelydetermine the types of linguistic knowledge to be ac-quired as well as their representational forms.If one chooses ageneral form of CFG without com-mittment to any specific linguistic theory, the knowl-edge to be learned is just a set of general rewrit-ing rules.
On the other hand, if one chooses morespecific linguistic frameworks, they impose furtherrestrictions on possible forms of knowledge to belearned, and introduce more diverse forms of rep-resenting knowledge.
For example, if one chooses alexicon-oriented framework, it may assume the exis-tence of subcategorization frames as lexical proper-ties, and impose restrictions on the form of rewritingrules such as "the LHS of each rewriting rule should223Rewriting Rule:Cat(F) ::> Carl(F1)+ Cat2(F2) +.. .
+ Catn(Fn) :f(F, F1, F2,..., Fn).Lexical Rule:Cat(F) =~ \[Word1, Word2,..., Wordn\] : f(F).Figure 1: General Forms of Grammar Ruleshave one and only one head", etc.While minimal commitment to specific linguistictheories is possible for research on general algorithmsof robust parsing (as in \[Mellish, 1989\]), it does notseem feasible for our paradigm, as our aim (learn-ing linguistic knowledge) is directly related to theproblems of what type of knowledge is to be learnedand how it is properly represented.
To learn suchrecta-principles from corpora, starting from a weakassumption formalism like CFG, requires inductionand an impractically huge search space.Instead, our aim is far less ambitious than auto-matic grammar learning from corpora.
Our goal isto make existing grammar and lexical resources morecomprehensive or to adapt them to new applicationdomains.
That is, from the very beginning, a sys-tem has a set of linguistic knowledge represented inspecific forms by assuming that meta-principles pro-posed by current linguistic theories are valid.
Weuse established linguistic concepts uch as 'Number-Property', subcategorization frames of predicates,syntactic ategories, etc.
Most of the inductive pro-cesses required in grammar learning will have beenperformed in advance (by linguists), though hypoth-esizing lacking knowledge may require induction evenin our framework.3.2 Grammar  Formal ismFigure 1 and Figure 2 show the general forms of therules in our grammar and specific examples respec-tively.
For experiments, we use a grammar whichconsists of 190 rewriting rules, giving us reasonablecoverage of English.As can be seen, the formalism used is a conven-tional kind of unification grammar where contextfree rules are augmented by feature conditions.
InFigure 1, each syntactic ategory Cati in a rewrit-ing rule has a feature structure Fi, which is unifiedeither wholly or partially to another by using thesame variable or by applying the unification functionf(F, F1, F2,..., F,~) (See examples in Figure 2).Although we do not commit ourselves to any spe-cific linguistic theory, it can be seen from the examplerules that we use basic concepts in modern linguistictheories uch as Head, Subcat, a set of grammaticalfunctions (Subject, Object, etc.
), etc.s(F) :~ np(F_np) + vp(F_vp) :(head,F)= (head,F_vp),(first,subcat,F_vp) = F_np.vp(F) :~ vp(F_vp) + np(F_np) :(head,F) = (head,F_vp),(subcat,F) = (rest,subcat,F_vp),(first,subcat,F_vp) = F_np.v(F) =~ \[has\]:(pred,head,F) - have,(obj,head,F) - (head,first,subcat,F),(subj,head,F) - (head,first,rest,subcat,F),(psn,subj,head,F) = 3,(nbr,subj,head,F) = sgl,(cat,first,subcat,F) = np,(cat,first,rest,subcat,F) = np.Figure 2: Examples of Grammar Rules3.3 Pars ing Resul tsThe parser we use is a left corner, bottom-up arserwith top-down filtering.
When it fails to parse, it re-parses the same sentence without top-down filteringand outputs the following intermediate tuples.Successful Category:succes fu l~oa l  (Cat, Words, WordsRest)This tuple means that a word sequence between'Words' and 'WordsRest' was successfully anal-ysed as an expected category 'Cat'.ex.)
successful_goal(np, \[the,boy, has,a,book\],\[has,a,book\])Failed Category: fa i led_goal(Cat .Words)This tuple means that an expected category'Cat' could not be analysed from a word list'Words'.ex.)
failed.goal(np,\[has,a,book\])These tuples are similar to active and inactiveedges of a chart parser but the 'Failed Category'above directly expresses the local ungrammaticalitywhile an active edge expresses an incomplete xpec-tation of a category within a grammar ule.4 Generat ion  o f  Hypotheses4.1 Hypothesiz ing Grammar  Rules fromParsing FailuresWhen the parser fails to analyse a sentence,the grammar ule hypothesizing program (shortlyGRHP) investigates the parsing results and hypoth-esizes all the possible modifications of the existinggrammar that produce a complete parsing result.GRHP starts from the top category's' and proceedsby breaking down each failed category in accordancewith the existing grammar.224The hypothesizing procedure (hypo_proc) worksfor each category CatA as follows (See also Figure 3):hypo_proc( CatA )beginif (CatA is a failed category) thenforeach i (CatA ==~ CatBil + ... + CatBin).
.
.
.
.
.
(1)foreaeh j (CatBij)call hypo_proc( Cat Bi j ).
.
.
.
.
.
(2)if (CatBij is a failed category) thenHYPO(left_recursive_rule( eat  Bij_ x ) ).
.
.
.
.
.
(3)endi fendHYPO(feature_disagreement(B ,,..., B,,,)).
.
.
.
.
.
(4)endendifif (CatA is a non-lexical category) thenHYPO(rule: CatA =~ CatC1 +.
.
.
+ CatCz).
.
.
.
.
.
(5)else if (CatA is a failed category) thenHYPO(lexical_entry: CatA =~ \[Word\]).
.
.
.
.
.
(6)endi fend(1) If CatA is a failed category, the procedurebreaks CatA down into its daughter categoriesaccording to the rule 'CatA :?, CatBil  + ... +CatBin' in the existing grammar.
The proce-dure iterates this breakdown for each rule com-posing CatA.
(2) The procedure calls itself recursively for eachdaughter category CatBii.
(3) The procedure also checks whether CatBij is afailed category.
If it is a failed category, theprocedure hypothesizes a new left recursive rulefor the preceding category CatBi j_ l  and gener-ates a rule 'CatBi j_ l  =:~ CatBi i -1 + CatR1 +?
.. +CatRo '  by searching adjacent successfulcategories next to CatBij-1 unless this rule isincluded in the existing grammar.
(4) If all the daughter categories are successful cat-egories, the procedure hypothesizes the featuredisagreement between them.
For example, if theexisting grammar contains a rule's ::?, np+ vp'and both 'np' and 'vp' are successfully parsedbut still 's' is a failed category, the procedurehypothesizes the feature disagreement between'np' and 'vp'.
(5) When the procedure finishes applying all theknown rules of CatA, it hypothesize a newrule of CatA unless CatA is a lexical cate-gory.
The procedure searches adjacent success-ful categories tarting from the word positionwhere CatA is expected and generates a rule(1) Breakdown of a Failed Category( CatA )CatBil CatBi2 .. .
CatBin(2) Recursive BreakdownCatACatBil  .
.
.
( CatBi~ ) .
.
.
CatBin(3) Hypothesizing a New Left Recursive RuleCatA?
.. (CatBi i_L)  CatBij(CatBi~_,)  CatR1 ..
.
(4) Hypothesizing a Feature DisagreementCatACatBil  CatBi2 .. .
CatBin(5) Hypothesizing a New RuleCatA =~ CatCx + CatC2 +.
.
.
+ CatCzCatC1 CatC2 .. .
CatCt(6) Hypothesizing a New Lexical EntryCatA =?, \[Word\]T( Word )Figure 3: Hypothesizing Process225'CatA :=~ CatC1 + ... + CatCl' unless the ruleis included in the existing grammar.
This stepis directly executed if CatA is not a failed cate-gory or there are no known rules which composeCatA.
(6) If CatA is a failed lexical category, the proce-dure hypothesizes a new lexical entry 'CatA ==~\[Word\]' at the word position where CatA is ex-pected.
By this hypothesis, an unknown wordas well as a known word is assigned into an ex-pected category.Actually, this process is implemented on Pro-log and each hypothesis is generated alternatively.When GRHP generates a hypothesis, it passes thehypothesis to the parser to analyse the remainingpart of the sentence.
As the result, GI~HP outputsonly the hypotheses that lead to complete structuresof the sentences.On this search algorithm, we imposed a strict con-dition that a sentence does not have more than onecause of its parsing failure and the combination ofhypotheses i  not allowed to account for one ungram-maticality.
Therefore, GRHP generates each hypoth-esis independently and all the hypotheses generatedfrom a sentence are alternatives.4.2 El imination of Redundant HypothesesGRHP in Section 4.1 generates a lot of alternativehypotheses, many of which are nonsensical from thelinguistic viewpoint.
GRHP as it is stated theredoes not include any criteria for judging the appro-priateness of hypotheses as linguistic rules.
In theextreme, it can hypothesize a rule which directly de-rives the input string of words from the start symbol's'.
Although such a rule allows the grammar to ac-cept the input as a sentence, the rule obviously lacksthe generality which we expect a linguistic rule tohave.
More seriously, it ignores all the generaliza-tions which the existing grammar embodies.One can conceive of an automatic procedure ofgrammar learning which starts from a set of suchrules and gradually discovers grammatical concepts,such as NP, VP, etc., based on the replaceabilityamong sub-strings.
However, as we discussed in Sec-tion 3, such a procedure has to solve the difficultiescaused by a huge search space which an inductionprocess generally has, and we are convinced that it isimpossible to induce from scratch the rules involvedin complex systems uch as human languages.Instead, our framework assumes that most of theinduction processes required in grammar learninghave been done by linguists and embodied in theform of the existing grammar.
The system has onlyto discover defects or incompleteness of the exist-ing grammar or to discover the differences betweenthe sublanguage in a new domain and the sublan-guage which the existing grammar has been preparedfor.
In other words, the hypotheses GRHP generatesshould use the generalizations embodied in the exist-ing grammar as much as possible, and the hypotheseswhich ignore them should be rejected as nonsensicalor redundant ones.GRHP hypothesizes a set of new rules which col-lect sequences of successful categories starting at thesame word position into the same failed category.If a substring of the input which is collected intothe failed category contains a sequence of "a goodstudent", for example, and if the existing gram-mar contains rules like 'nhead :=~ adj + nhead','np =~ det + nhead', etc., GRHP will generate hy-potheses whose RHSs contain the sequence, such as'det + adj + nhead', 'det + nhead', etc., as well as theones whose RHSs contain 'np' for the same part ofthe input.However, because the hypothesized rules contain-ing smaller constituents, uch as 'det', 'nhead', etc.instead of 'np', ignore the generalization captured by'np' in the existing grammar, they should be disre-garded as redundant, while only the ones which con-tain 'np' in their RHSs are kept as viable hypotheses.Much simpler criteria could also be used to pre-vent nonsensical hypotheses from being generated.For example, a rule whose RHS consists of a largenumber of constituents would not be viable, if weassume that the existing grammar has already beenequipped with a reasonable set of syntactic ategories(non-terminals) which allow sentences to be assignedreasonably structured escriptions.The following is a list of the criteria which Gl~HPcan use to disregard nonsensical hypotheses.\[1\] P r io r i ty  to the hypotheses of  feature dis-agreement :  Assuming that the existing gram-mar is quite comprehensive, we can give priorityto the hypotheses offeature disagreement, whichdo not create new rules.
In the current imple-mentation, if GI:tHP finds a feature disagree-ment hypothesis to restore a failed category, itstops the recursion and generates no more hy-potheses.\[2\] Number  o f  daughter  nodes:  A rule whichcollects an excessive number of constituents intoone large constituent at once is not viable.
Wecurrently restrict he number of daughter nodesto 4.\[3\] Priority to the hypotheses using general-izations embodied by the existing gram-mar :  As discussed in the above, priority is givento the hypotheses which contain 'np' as daugh-ters over those which contain 'det + nhead','det + adj + nhead', etc.
In general, hypothe-ses containing sequences of constituents whichcan be collected into larger constituents by ex-isting rules are disregarded as redundant (SeeFigure 4).\[4\] Dist inct ion of  lexical categories from othercateogries: While the general form of CFG226CatA  =?, ?
?
?
+ Cat  B i _ l  + np  + CatB i+ l  + .
.
.CatBi_ l  x np x CatB i+ lT,/ Ta s tudentFigure 4: Adjacent Maximal Categorydoes not distinguish lexical categories fromother non-terminals, our grammar does.
There-fore, we prohibit GRHP to hypothesize a newrule whose mother category is one of the lexicalcategories.
The lexical categories are allowedonly to appear in new lexical rules.\[5\] D is t inc t ion  of  closed and open lexical cat-egories:  We assume that the existing gram-mar has a complete list of function words.
Thismeans that LHSs of rules for new lexical entriesare restricted to the open lexical categories, uchas noun, verb, adjective, and adverb.\[6\] Use o f  subcategor i za t ion  f rames:  As in ourgrammar formalism a subcategorization frameis embedded in the feature structure of a headcategory, the correspondence b tween the headcategory and its subcategories does not appearexplicitly in rules.
Therefore, a subcategoriza-tion frame checking mechanism should be incor-porated into the search algorithm and executedbefore hypothesizing any rule or any lexical en-try in order to filter out redundant hypotheses.\[7\] P roh ib i t ion  o f  unary  rules: While the gen-eral form of CFG allows unary rules and theyare sometimes used as category conversion rulesin actual descriptions of a grammar, they differfrom the constituent rules which specify mother-daughter elationships.
For example, a rule' np  =?, in f in i t i ve '  means that an infinitivalclause behaves as a noun phrase in larger con-stituents without changing its structure.
Unre-stricted introduction of such unary rules, how-ever, increases drastically not only parsing am-biguities but also possible hypotheses generatedby GRHP.
Except for lexical rules which areunary in nature, we can prohibit unary hy-potheses by assuming that the existing rammarexhausts all possible category conversion rulesamong the categories it uses (See Section 5).\[8\] D is t inc t ion  o f  closed and open categor ies:We can extend the distinction of open and closedlexical categories in \[5\] to the other categories.Depending on the completeness of the existinggrammar, we can specify a set of categories asclosed categories and prohibit GRHP to gener-ate new rules whose RHSs belong to the set.\[9\] Rest r i c ted  pat terns  o f  new rules: This re-striction could be realized by introducing meta-rules which specify the form of a new rule andthe relations between adjacent categories.
Forexample, according to the X-bar theory, we canconfine a category appearing at the complementposition to be a maximal projection.\[10\] Rest r i c t ion  on Lexical  Rules:  As we dis-cussed in \[7\], unary rules are one of the majorcauses of explosion of the search space.
Unarylexical rules can also be restricted by introduc-ing a pr/or knowledge of possible lexical categoryconversions.
For example, while the conversionbetween a noun and a verb is very frequent inEnglish, the conversion of an adverb with thesuffix - ly to a verb is extremely rare.
This meansthat, though verb is an open lexical category, wecan prohibit a lexical rule which forces a wordregistered in the dictionary as an adverb to beinterpreted as a verb.5 Preliminary ExperimentTo see what sort of hypotheses are actually gener-ated, and how many of them are reasonable (in otherwords, how many of them are nonsensical), we haveconducted a preliminary experiment with the follow-ing six sentences.
(1) The girl in the garden has a bouquet.
(2) Buy a new car.
(3) Dogs do dream.
(4) The box is so heavy that I could not move it.
(5) The student has a BMW.
(6) The boy caught several fish.We deliberately introduce defects into the existinggrammar which are relevant o the analysis of thesesentences.
That is, the following rules are removedfrom the existing grammar for the sake of the exper-iment.?
pp-attachment rule for noun phrases.?
rule for imperative sentences.?
DO-emphasis rule.?
rule for SO-THAT construction.?
lexical rule for "BMW".?
lexical description for the plural usage of "fish".The criteria \[1\]-\[5\] of redundant hypotheses are in-cluded in the basic algorithm of GRHP so that thefollowing lists of hypotheses for these examples do227not contain those which are rejected by these crite-ria.
The hypotheses marked with '--*' are the plau-sible hypotheses.
The hypotheses marked by x and?
are the hypotheses removed by adding \[6\] and \[7\]as further criteria of redundant hypotheses, respec-tively.
We do not use the criteria of \[8\]-\[10\] in thisexperiment, partly because these are highly depen-dent on the completeness of the existing grammarand, though very effective for reducing the numberof hypotheses, can be arbitrary.
(1) "The girl in the garden has a bouquet."?
Rule: colonp => pp-* Rule: np => np,ppRule :  s => np,pp,vpRule :  vp => pp,vpLex ica l  Entry:  v => \[ in\]Instead of the removed pl~attachment rule,'nhead ==~ nhead + pp', GRHP generates a newpp-attachment rule, 'rip =~ .p  + pp'.
(2) "Buy a new car.
"- *?Rule :  s => vpGRHP generates only one hypothesis, a rule forimperative sentences.
This rule looks plausiblebut the fact that the criteria \[7\] of redundanthypotheses suppresses this rule indicates thata rule for imperative sentences hould not betreated as a normal unary (category conversion)rule but rather a whole-sentencial constituentrule.
(3) "Dogs do dream.
"X Rule: a jp  => nheadx Rule: ajp => vp?
Rule: colonp => auxdo@ Rule: colonp => vpX Rule: infinitive => nheadx Rule: infinit ive => vpRule: np => np,auxdoRule: np => np,vp?
Rule: np => relc?
Rule: np => s?
Rule :  np => vpRule: s => np,auxdo,nheadRule: s => np,auxdo,vpRule: s => np,vp,nheadRule: s => np,vp,vpRule: s => relc,nheadRule: s => relc,vpRule: s => s,nheadRule: s => s,vp?
Rule: sub_clause => nhead?
Rule: sub_clause => vp?
Rule: that_clause => nhead?
Rule: that_clause => vpRule: vp => auxdo,nhead-*Rule:  vp => auxdo,vp?
Rule: vp => auxdo(4)X Rule: vppsv => nheadX Ru le :  vppsv => ypLex ica l  Entry:  adj => \[dream\]Lex ica l  Entry:  adv => \[dream\]F Disagrmnt: np => nheadFD isagrmnt :  vp => vp,vpF Visagrmnt: vppsv => vAlthough this sentence is short, quite a few hy-potheses are generated.
This is partly becauseboth "do" and "dream" are ambiguous in theirparts of speech.
Some of the generated hypothe-ses are based on the interpretation of "dream"as a noun.
However, even in the cases in whichthe main verb is not ambiguous, GRHP alwayshypothesizes 'vp =~ vp + vp' as well as the cor-rect DO-emphasis rule, as "do" has two parts ofspeech.
As we discuss in the following section, itis impossible to choose one of these hypotheseson the basis of single parsing failures.
We needcorpus-based techniques to rate the plausibilityof these two hypotheses.
"The box is so heavy that I could not move it.
"X Ru le :x Ru le :?
Ru le :x Ru le :x Ru le :x Ru le :x Ru le :x Ru le :x Rule:x Rule:Rule:Rule:Rule:Rule:?
Rule :?
Ru le :Ru le :Ru le :Ru le :Ru le :Ru le :Rule:Rule:Rule:Rule:-*Rule:Rule:Rule :Ru le :?
Ru le :x Rule:x Rule:x Rule :x Rule:x Rule:?
Rule:ajp  ffi> re lc ,npa jp  => re lca jp  => that_c lausein f in i t i ve  => a jp , re lc ,npinfinitive => ajp,relcinfinitive => ajp,that_clauseinfinitive => ajpinfinitive => relc,npinfinitive => relcinfinit ive => that_clausenhead => a jp , re lc ,npnhead => a jp , re lcnhead => a jp , that_c lausenhead => re lc ,npnhead => relcnhead => that_clausenp => a jp , re lc ,npnp => a jp , re lcnp => a jp , that_c lauses => np ,vp ,a jp , that~lauses => np ,vp , re lc ,nps => np ,vp , that_c lauses => s,ajp,relc,nps => s ,a jp , that_~lauses => s,relc,nps => s,that_clausesub_clause => a jp , re lc ,npsub_clause =>ajp , that_c lausesub_clause => re lc ,npsub_clause => that_c lausethat_c lause  => a jp , re lc ,npthat_c lause  => a jp , re lcthat_c lause  => a jp , that_c lausethat_c lause  => a jpvp => adv ,a jp , re lc ,npvp => adv ,a jp , re lc228x Rule:  vp => adv ,a jp , that .~ lausex Rule :  vp => adv ,a jp?
Rule: vp => ajp,relc,np?
Rule: vp => ajp,relcx Rule: vp => ajp,that_clause?
Rule: vp => ajp?
Rule: vp => relc,npx Rule: vp => relcx Rule: vp => that_clause?
Rule: vp => vp,relc,np?
Rule: vp => vp,relcX Rule:  vppsv => adv ,a jp , re lc ,npx Rule:  vppsv => adv ,a jp , re lcx Rule:  vppsv => adv ,a jp , that_c lausex Rule:  vppsv => adv ,a jp?
Rule:  vppsv => a jp , re lc ,npx Rule:  vppsv => a jp , re lcx Rule:  vppsv  => a jp , that_c lause?
Rule:  vppsv => a jpx Rule:  vppsv => re lc ,npx Rule:  vppsv  => re lcx Rule:  vppsv => that_c lauseLex ica l  Entry:  adj => \ [ that \ ]Lex ica l  Entry:  adv => \ [heavy\ ]Lex ica l  Entry:  adv => \ [ that \ ]Lex ica l  Entry:  n => \ [heavy\ ]Lex ica l  Entry:  n => \ [so\ ]Lex ica l  Entry: n => \[that\]Lex ica l  Entry: v => \ [heavy\ ]Lex ica l  Entry: v => \[so\]Lex ica l  Entry:  v => \ [ that \ ]F V isagrmnt :  a jp  => a jp , that_c lauseF V isagrmnt :  sub_c lause  => con j3 ,sF Disagrmnt: vp => vp,ajpF Disagrmnt: vp => vp,np-~ F Disagrmnt: vp => vp,that_clauseIn this example, 'vp ~ vp + that_clause' (or's ~ s + that_clause') could be the appropriatehypothesis.
However, simple addition of sucha rule to the existing rammar results in over-generalization.
The rule should have a conditionon the existence of "so" in 'vp' (or 's') while asimilar effect can also be attained by adding anew lexical entry for "heavy" which has a sub-categorization frame containing a 'that clause'.That is, the system has to decide which hypoth-esis is more plausible, either "heavy" can sub-categorize a 'that clause' or "so" is crucial inmaking 'vp' to be related with a 'that clause'.This decision may not be possible, if this sen-tence is the only one sentence in a corpus whichcontains this construction.
Like Example 3, weneed corpus-based techniques tochoose the rightone.
(5) "The student has a BMW.
"-~ Lexical Entry: n => \['BMW'\]GRHP generates the correct hypothesis whichassigns the expected lexical category to the un-Sample \]\] Number of Hypotheses ISentence Nit LE FD Total(3) \[1 28 I 2 I 311 331,(4) )) 58\] 9 I 5li 721(5) II O l 11 oil 1 l(8) s 2 1 11NR: New RuleLE: New Lexical EntryFD: Feature DisagreementTable 2: Number of Hypothesesregistered word.
(6) "The boy caught several fish.
"x Rule:  a jp  => det ,nheadx Rule:  a jp  => det?
Rule:  in f in i t i ve  => det ,nheadRule:  s => np ,vp ,det ,nheadRule: s => relc,det,nhead?
Rule: that_clause => det,nhead?
Rule: vp => det,nhead?
Rule: vppsv => det,nheadLexical Entry: adj => \[several\]Lexical Entry: n => \[several\]-~ F Disagrmnt: np => det,ltheadGRHP generates the correct hypothesis of thefeature disagreement between the plural deter-miner "several" and the noun "fish" as one ofpossible hypotheses.Table 2 summarizes the number of hypotheses gen-erated for each sample sentence.
As can be seen,while appropriate hypotheses are generated, quite afew other hypotheses are also generated, especiallyin the case of the third and the fourth sentences.However, as shown in Table 3, the criteria \[6\] and\[7\] of redundant hypotheses can eliminate significantportions of nonsensical hypotheses (Table 3 showsthe effects of these criteria on the number of hypoth-esized new rules).
In Example (4), for example, 31out of 58 initially hypothesized rules are eliminatedby \[6\] and \[7\], while 16 out of 28 rules are eliminatedin Example (3).
Furthermore, we expect hat intro-duction of other criteria for redundant eliminationbased on \[8\]-\[10\] will reduce the number of hypothe-ses significantly and make the succeeding stage of thecorpus-based statistical analysis feasible.The experiment on another set of sample sentencesfrom the UNIX on-line manual confirms our expecta-tion (See Table 4).
The number of hypotheses gener-ated in this experiment is very much similar to thatof the experiment on artificial samples (note that Ta-ble 4 shows the number of hypotheses generated be-fore elimination by the criteria \[6\] and'J7\]).229Sample H Number  of New Rules ISentence I - 5 I - 6 I -\[7Table 3: Effects of Redundancy Elimination6 Corpus-based Techniques andLinguistic Knowledge AcquisitionWe discussed that using an existing rammar shouldenable us to avoid a huge search space which gram-matical earning would otherwise have.
Instead ofinducing grammatical concepts from scratch, ourframework uses the categories prepared in an exist-ing grammar for formulating new structural rules.However, linguistic knowledge acquisition is inher-ently an inductive process.
We cannot expect GttHPalone to choose correct hypotheses without observinganalysis results of other sentences in a corpus.Although we have not yet implemented the corpus-based component, he result of the preliminary ex-periment indicates what sorts of functions this com-ponent should have.\[1\] In Example (6), we have a feature disagreementhypothesis for "several fish" and two lexical hypothe-ses for "several".
Further analysis of the feature dis-agreement hypothesis will lead to two competing hy-potheses, one of which requires a revised lexical de.scription of "several" and the other of which suggeststhat of '~ish".
The other two lexical hypotheses alsosuggest different revisions in the description of "sev-eral".
However, the analysis of this sentence alonemay not enable us to decide which of these four hy-potheses i the right one.We reported in \[Tsujii et al, 1992\] that a simplestatistical measure like the Failure Rate o/ a Word(ratio of the number of sentences containing a wordthat cannot be parsed to the total number of sen-tences containing the same word) is useful for dis-covering words whose lexical descriptions contain de.fects.
This kind of simple measures would also beeffective in a situation like Example (6).
That is,we can expect hat, while the frequency of the word"several" would be high, the frequency of the hy-potheses uggesting the revisions of the lexical de.scriptions of this word would be relatively low.\[2\] As we noted in the comment on Example (3),whenever DO-emphasis construction appears, thesame pair of the hypotheses, 'vp ::~ vp + vp' and'vp =~ auzdo + vp', will be generated.
Unless othertypes of failures lead to one of these hypotheses, theywould be judged to have exactly the same remedialpowers, i.e.
the same set of failures are restoredby them.
In such a situation, we may be able tochoose the right one by comparing the specificitiesof competing hypotheses.
In this example, the for-mer hypothesis which uses 'vp' instead of'auzdo' canbe judged as having excessive generative powers andtherefore inappropriate b cause the other competinghypothesis with far restricted generative powers canrestore the same set of parsing failures.In order for such comparison to be meaningful,the system first have to judge, by corpus-based tech-niques, whether competing hypotheses have the sameremedial powers or not.
If the more general ones ap-pear frequently as remedial rules for parsing failureswhich cannot be restored by the specific ones, thegeneral ones would be the right ones.\[3\] Example (4) shows a situation opposite to Ex-ample (3).
We have two (or three) viable competinghypotheses in this example.
One is the specific hy-pothesis with very restricted generative powers whichsuggests to revise the lexical description of "heavy".The other is a more general hypothesis which allows'vp' (or 's') to be followed by 'that_clause'.
Althougheither of these two can restore the parsing failure ofthis sentence, the specific one cannot restore pars-ing failures in other sentences in which SO-THATconstructions appear with different adjectives.
Thatis, unlike Example (3), these two hypotheses havedifferent remedial powers and, because of this, thegeneral one should be chosen as the right one.Furthermore, though simple addition of this gen-eral rule results in serious over-generalization, tocurb this over-generalization needs complex revisionsof related grammar rules in order for a feature indi-cating the existence of "so" to be percolated to thenode of 'vp' (or 's').
Such invention of a new featureand re-organization f related rules seem beyond thecurrent framework and we expect human linguists toexamine suggested hyoptheses.7 ConclusionWe proposed in this paper a new framework whichacquires linguistic knowledge from parsing failures.Linguistic knowledge acquisition been studied so farby two extreme approaches.
One approach assumesvery little prior knowledge and tries to induce mostof linguistic knowledge from scratch, while the otherassumes existence of almost complete knowledge andtries only to learn the probabilistic properties fromcorpora.
Our approach is between these two ex-tremes.
Although it assumes existence of rather com-prehensive linguistic knowledge, it tries to create newunits of knowledge which deal with specificities ofgiven sublanguages.Considering the diverse nature of sublanguages andthe essential difficulties involved in inductive pro-cesses, we believe that our approach as practicaladvantages over the other approaches as well as in-teresting theoretical implications.
However, the re-230~-~l -~entenceVana es are mltla lze to te  nu string.
1 2 8 3 2 3The default blocking factor is 20 blocks.
1127131111311There is no way selectively to follow symbolic links.
II 19 \[ 6 \[ 1 II 26 IWhen closed, clock displays a clock face.
II 1 I 0 I 0 II 1 IThe default is DELETE.
II 0 l  41 0 II 41This support is normally invisible to the user.
II 26 \[ 13 \[ 3 11 42 \[The output device in use is not capable of backspacing.
II 40 1 14 1 -3 II 5 r IAs a result, the first line must not have any superscripts.
II 13 I ~ I 0 II 16 IPathnames are restricted to 128 characters.
II 0 I 1 I 0 II x IThey default o the standard input and the standard output.
II 12 I 5 I 1 II 18 IRemove initial definitions for all predefined symbols.
II 10 I 2 I 0 II 12 IRemove any definition for the symbol name.
II 2 I 0 I 0 II 2 IThe most recent command is retained in any case.
II 82 I 11 I 5 II 98 ISuch loops are detected, and cause an error message.
II 1_3 I 0 I 0 II 1_3 IComponents of an expression are separated by white space.
II 2 I 0 I 0 II 2 IThe kernel then attempts to overlay the new process with the II 8 I 5 I 0 II 13 Idesired program.Table 4: Number of Hypotheses (Sentences from the UNIX manual)search of this direction has just started and quitea few problems remain to be solved.
The followingshows some of these problems.?
Analysis Methods of Feature Disagree-ments: Unlike robust parsing of ill-formed in-put, we have to identify real causes of disagree-ments and create a set of sub-hypotheses on realcauses.
In many cases, feature disagreementsare caused by lack of or improper lexical de-scriptions.?
Plausibil ity Rat ing of Hypotheses: As wesaw in Section 6, the corpus-based componenthas to take into consideration several factors,such as remedial powers and specificities of in-dividual hypotheses, relative frequencies of hy-potheses (like fault rates), competing relation-ships among them, etc.
in order to rate theplausibility of individual hypotheses.
However,the observation i Section 6 is still very sketchy.In order to design the corpus-based component,we need more detailed observation of the natureof hypotheses generated by GRHP.?
Further  Restr ict ions on Viable Hypothe-ses: Although the current criteria of redundanthypotheses reduce significantly the number ofhypotheses, there still remain cases where morethan thirty hypotheses are generated.?
Ref inement of Generated Hypotheses:The current version of GRHP only generatesstructural skeletons of new rules.
These struc-tural skeletons hould be accompanied by con-ditions on features.
In particular, it would becrucial in practical applications for GRHP togenerate hypotheses of lexical descriptions withfuller feature specifications.AcknowledgementsWe would like to thank our colleagues at CCL whoare interested in corpus-based techniques.
Theircomments on the paper were very useful.
We wouldalso thank Mr. Tomoki Tsumura, Dr. KatsuraKawakami and the colleagues at Matsushita, who al-lowed Kiyono to do research at CCL.Re ferences\[Ananiadou, 1990\] Sofia Ananiadou.
Sublanguagestudies as the basis for computer support for mul-tilingual communication.
In Proc.
of Termplan'90, Kuala Lumpur, 1990.\[Douglas and Dale, 1992\] Shona Douglas andRobert Dale.
Towards robust pitt.
In Proc.
ofCOLING-92, pages 468-474, 1992.\[Goeser, 1992\] Sebastian Goeser.
Chart parsing ofrobust grammars.
In Proc.
of COLING-92, pages120-126, 1992.\[Mellish, 1989\] Chris S. Mellish.
Some chart-basedtechniques for parsing ill-formed input.
In Proc.of the 27th ACL meeting, pages 102-109, 1989.\[Sekine t al., 1992\] Satoshi Sekine, et al Linguis-tic knowledge generator.
In Proc.
of COLING-g2,pages 560-566, 1992.\[Strzalkowski, 1992\] Tomek Strzalkowski.
Ttp: Afast and robust parser for natural anguage.
InProc.
of COLING-g2, pages 198-204, 1992.\[Tsujii et ai., 1992\] $un-ichi Tsujii, et al Linguisticknowledge acquisition from corpora.
In Proc.
of2nd FGNLP, pages 61-81, UMIST, 1992.231
