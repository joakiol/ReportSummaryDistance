Proceedings of the ACL 2007 Student Research Workshop, pages 25?30,Prague, June 2007. c?2007 Association for Computational LinguisticsAutomatic Prediction of Cognate Orthography UsingSupport Vector MachinesAndrea MulloniResearch Group in Computational LinguisticsHLSS, University of WolverhamptonMB114 Stafford Street, Wolverhampton, WV1 1SB, United Kingdomandrea2@wlv.ac.ukAbstractThis  paper  describes  an  algorithm  toautomatically generate a list of cognates ina  target  language  by  means  of  SupportVector  Machines.
While  Levenshteindistance was used to align the training file,no  knowledge  repository  other  than  aninitial  list  of  cognates  used  for  trainingpurposes  was  input  into  the  algorithm.Evaluation  was  set  up  in  a  cognateproduction  scenario  which  mimed  a  real-life  situation  where  no  word  lists  wereavailable in the target language, deliveringthe ideal environment to test the feasibilityof  a  more  ambitious  project  that  willinvolve  language  portability.
An  overallimprovement of 50.58% over the baselineshowed promising horizons.1 IntroductionCognates are words  that have similar spelling andmeaning across different languages.
They accountfor  a  considerable  portion  of  technical  lexicons,and  they  found  application  in  several  NLPdomains.
Some  major  applications  fields  includerelevant  areas  such  as  bilingual  terminologycompilation and statistical machine translation.So far algorithms for cognate recognition havebeen focussing predominantly on the detection ofcognate words  in a  text,  e.g.
(Kondrak and Dorr2004).
Sometimes,  though,  the  detection  ofcognates in free-flowing text is rather impractical:being able to predict the possible translation in thetarget  language  would  optimize  algorithms  thatmake  extensive  use  of  the  Web or  very  largecorpora, since there would be no need to scan thewhole  data  each  time  in  order  to  find  thecorrespondent item.
The proposed approach aims tolook at the same problem from a totally differentperspective,  that  is  to  produce  an  informationrepository about the target language that could thenbe  exploited  in  order  to  predict  how  theorthography of a ?possible?
cognate in the targetlanguage should look like.
This  is necessary whenno plain word list is available in the target languageor the list  is incomplete.
The proposed algorithmmerges for the first time two otherwise well-knownmethods, adopting a specific tagger implementationwhich  suggests  new areas  of  application for  thistool.
Furthermore, once language portability will bein place, the cognate generation exercise will allowto  reformulate  the  recognition  exercise  as  well,which is  indeed a more straightforward one.
Thealgorithm described in this paper is based on theassumption  that  linguistic  mappings  show  somekind of regularity and that they can be exploited inorder to draw a net of implicit rules by means of amachine learning approach.Section 2 deals with previous work done on thefield  of  cognate  recognition,  while  Section  3describes in detail the algorithm used for this study.An evaluation scenario will be drawn in Section 4,while  Section  5  will  outline  the  directions  weintend to take in the next months.2 Previous WorkThe identification of cognates is a quite challengingNLP task.
The most renowned approach to cognaterecognition is to use spelling similarities betweenthe  two  words  involved.
The  most  importantcontribution to this methodology has been given byLevenshtein  (1965),  who  calculated  the  changesneeded in order to transform one word into anotherby applying four different edit operations ?
match,25substitution, insertion and deletion ?
which becameknown under  the  name of  edit  distance  (ED).
Agood case in point of a practical application of EDis represented by the studies in the field of lexiconacquisition from comparable corpora carried out byKoehn and Knight (2002) ?
who expand a list ofEnglish-German cognate words by applying well-established  transformation rules  (e.g.
substitutionof  k or  z by  c and of  ?t?t by  ?ty, as in GermanElektizit?t ?
English electricity) ?
as well as thosethat focused on word alignment in parallel corpora(e.g.
Melamed (2001)  and Simard et  al.
(1999)).Furthermore, Laviosa (2001) showed that cognatescan be extremely helpful in translation studies, too.Among others, ED was extensively used also byMann  and  Yarowsky (2001),  who  try  to  inducetranslation  lexicons  between  cross-familylanguages  via  third languages.
Lexicons  are  thenexpanded  to  intra-family languages  by  means  ofcognate  pairs  and  cognate  distance.
Relatedtechniques  include  a  method  developed  byDanielsson and M?hlenbock (2000),  who associatetwo words by calculating the number of matchingconsonants, allowing for one mismatched character.A  quite  interesting  spin-off  was  analysed  byKondrak  (2004),  who  first  highlighted  theimportance of genetic cognates by comparing thephonetic  similarity of  lexemes  with  the  semanticsimilarity of the glosses.A  general  overview  of  the  most  importantstatistical  techniques  currently  used  for  cognatedetection purposes was delivered by Inkpen  et al(2005),  who addressed the  problem of  automaticclassification of  word  pairs  as  cognates  or  falsefriends  and  analysed  the  impact  of  applyingdifferent  features  through  machine  learningtechniques.
In her  paper,  she also  proposed  amethod  to  automatically  distinguish  betweencognates  and  false  friends,  while  examining  theperformance  of  seven  different  machine  learningclassifiers.Further applications of ED include Mulloni andPekar (2006), who designed an algorithm based onnormalized  edit  distance  aiming  to  automaticallyextract translation rules, for then applying them tothe original cognate list in order to expand it, andBrew and McKelvie (1996), who used approximatestring  matching  in  order  to  align  sentences  andextract  lexicographically  interesting  word-wordpairs from multilingual corpora.Finally, it  is  worth  mentioning  that  the  workdone  on  automatic  named  entity  transliterationoften crosses  paths  with  the  research on cognaterecognition.
One good pointer leads to Kashani etal.
(2006), who used a three-phase algorithm basedon  HMM  to  solve  the  transliteration  problembetween Arabic and English.All the methodologies described above showedgood potential, each one in its own way.
This paperaims to merge some successful ideas together, aswell  as  providing  an  independent  and  flexibleframework  that  could  be  applied  to  differentscenarios.3 Proposed ApproachWhen approaching the algorithm design phase, wewere  faced with two major  decisions:  firstly, wehad to decide which kind of machine learning (ML)approach should be  used to gather  the  necessaryinformation, secondly we needed to determine howto exploit the knowledge base gathered in the mostappropriate and productive way.
As it turned out,the  whole  work  ended  up  to  revolve  around  theintuition that  a simple tagger could lead to quiteinteresting  results,  if  only  we  could  scale  downfrom  sentence  level  to  word  level,  that  is  toproduce  a  tag for  single  letters  instead of  wholewords.
In other  words,  we wanted to exploit  theanalogy  between  PoS  tagging  and  cognateprediction:  given  a  sequence  of  symbols  ?
i.e.source language unigrams ?
and tags aligned withthem ?
i.e.
target language n-grams ?, we aim topredict tags for more symbols.
Thereby the contextprovided  by  the  neighbors  of  a symbol  and  theprevious tags are used as evidence to decide its tag.After  an  extensive  evaluation  of  the  major  ML-based  taggers  available,  we  decided  to  opt  forSVMTool, a generator of sequential taggers basedon  Support  Vector  Machines  developed  byGimenez  and  Marquez  (2004).
In  fact,  variousexperiments carried out on similar software showedthat  SVMTool was the most  suitable  one for  thetype of data being examined, mainly because of itsflexible approach to our input file.
Also, SVMToolallows to define context by providing an adjustablesliding window for the extraction of features.
Oncethe model was trained, we went on to create themost  orthographically  probable  cognate  in  thetarget language.
The following sections exemplifythe  cognate  creation  algorithm,  the  learning  stepand the exploitation of the information gathered.3.1 Cognate Creation AlgorithmFigure 1 shows the cognate creation algorithm indetail.26Input: C1, a list of English-German cognate pairs{L1,L2}; C2, a test file of cognates in L1Output: AL, a list of artificially constructedcognates in the target language1 for c in C1 do:2 determine the edit operations to arrivefrom L1 to L23 use the edit operations to produce aformatted training file for the SVM tagger4 end5 Learn orthographic mappings between L1and L2 (L1 unigram = instance, L2 n-gram =category)6 Align all words of the test file vertically in aletter-by-letter fashion (unigram = instance)7 Tag the test file with the SVM tagger8 Group the tagger output into words andproduce a list of cognate pairsFigure 1.
The cognate creation algorithm.Determination of the Edit OperationsThe algorithm takes as input two distinct cognatelists, one for training and one for testing purposes.It is important to note that the input languages needto share the same alphabet, since the algorithm iscurrently still  depending on  edit  distance.
Futuredevelopments  will  allow for  language portability,which is already matter of study.
The first sub-step(Figure 1, Line 2) deals with the determination ofthe  edit  operations  and  its  association  with  thecognate  pair,  as  shown  in  Figure  2.
The  fouroptions provided by edit distance, as described byLevenshtein  (1965),  are  Match,  Substitution,Insertion and Deletion.toilet/toilettet    |o    |i    |l    |e    |t    |   |t    |o    |i    |l    |e    |t    |t  |eMATCH|MATCH|MATCH|MATCH|MATCH|MATCH|INS|INStractor/traktort    |r    |a    |c    |t    |o    |rt    |r    |a    |k    |t    |o    |rMATCH|MATCH|MATCH|SUBST|MATCH|MATCH|MATCHabsolute/absoluta    |b    |s    |o    |l    |u    |t    |ea    |b    |s    |o    |l    |u    |t    |MATCH|MATCH|MATCH|MATCH|MATCH|MATCH|MATCH|DELFigure 2.
Edit operation associationPreparation of the Training FileThis sub-step  (Figure 1, Line 3)  turned out to bethe  most  challenging  task,  since  we  needed  toproduce the input file that offered the best layoutpossible for the machine learning module.
We firsttried to insert several empty slots between letters inthe source language file, so that we could cope withmaximally  two  subsequent  insertions.
While  allwords are in lower case, we identified the spaceswith a capital X, which would have allowed us tosubsequently discard it without running the risk todelete useful letters in the last step of the algorithm.The  choice  of  manipulating  the  source  languagefile was supported by the fact that we were aimingto limit  the  features  of  the  ML module  to  27  atmost, that is the letters of the alphabet from ?a?
to?z?
plus  the  upper  case  ?X?
meaning  blank.Nonetheless,  we  soon  realized  that  the  spacefeature outweighed all other features and biased theoutput towards shorter words.
Also, the input wordwas  so  interspersed  that  it  did  not  allow  thelearning machine  to  recognize  recurrent  patterns.Further  empirical  activity  showed  that  far  betterresults could be achieved by sticking to the originalletter sequence in the source word and allow for anindefinite number of feature to be learned.
This wasimplemented by  grouping  letters  on  the  basis  oftheir edit operation relation to the source language.Figure  3  exemplifies  a  typical  situation  whereinsertions and deletions are catered for.START START START STARTa a m mb b a ai i c ko o r rog g o ee e e en n c ke e o ot t n ni i o oc X m ma X i isl s c chl c .
ENDy h. ENDFigure 3.
Layout of the training entriesmacroeconomic/makrooekonomisch andabiogenetically/abiogenetisch, showing insertionsand deletionsAs shown in Figure 3,  German diacritics havebeen substituted by their extended version ?
i.e.
??
?as  been  rendered  as  ?oe?
:  this  was  due  to  theinability  of  SVMTool  to  cope  with  diacritics.Figure 3 also shows how insertions and deletions27were  treated.
This  design  choice  caused  a  non-foreseeable number of features to be learned by theML module.
While apparently a negative issue thatcould cause data to be too sparse to be relevant, wetrusted our intuition that the feature growing graphwould just flat out after an initial spike, that is thenumber  of  insertion  edits  would  not  produce  anexplosion of source/target n-gram equivalents, butonly  a  short  expansion  to  the  original  list  ofmapping pairings.
This proved to be correct by theevaluation phase described below.Learning Mappings Across LanguagesOnce the preliminary steps had been taken care of,the training file was passed on to SVMTlearn, thelearning  module  of  SVMTool.
At  this  point  thefocus switches over to the tool itself, which learnsregular  patterns  using  Support  Vector Machinesand then uses the information gathered to tag anypossible list of words  (Figure 1, Line 5).
The toolchooses automatically the best scoring tag, but ?
asa matter of  fact  ?
it  calculates up to 10 possiblealternatives  for  each  letter  and  ranks  them  byprobability scores: in the current paper the reportedresults were based on the best scoring ?tag?, but thealgorithm  can  be  easily  modified  in  order  toaccommodate the outcome of the combination ofall 10 scores.
As it will be shown later in Section 4,this is potentially of great interest if we intend towork in a cognate creation scenario.As far the last three steps of the algorithm areconcerned, they are closely related to the practicalimplementation  of  our  methodology, hence  theywill be described extensively in Section 4.4 EvaluationIn order to evaluate the cognate creation algorithm,we decided to set up a specific evaluation scenariowhere possible cognates needed to be identified butno word list  to choose from existed in the targetlanguage.
Specifically,  we  were  interested  inproducing the correct word in the target language,starting  from a  list  of  possible  cognates  in  thesource language.
An alternative evaluation settingcould  have  been  based  on  a  scenario  whichincluded  a  scrambling and  matching routine,  butafter the good results showed by Mulloni and Pekar(2006), we thought that yet a different environmentwould  have  offered  more  insight  into  the  field.Also, we wanted to evaluate the actual strength ofour  approach,  in  order  to  decide  if  future  workshould be heading this way.4.1 DataThe method was evaluated on an English-Germancognate  list  including  2105  entries.
Since  wewanted to keep as much data available for testingas  possible,  we  decided  to  split  the  list  in  80%training  (1683  entries)  and  20%  (422  entries)testing.4.2 Task DescriptionThe list used for training/testing purposes includedcognates  only.
Therefore,  the  optimal  outcomewould have been a word in the target language thatperfectly matched the cognate of the correspondingsource language word in the original file.
The taskwas therefore a quite straightforward one:  train theSVM tagger  using  the  training  data  file  and  ?starting from a list of words in the source language(English) ?
produce a word in the target language(German) that  looked as  close as possible  to theoriginal  cognate  word.
Also,  we  counted  alloccurrences  where  no  changes  across  languagestook place ?
i.e.
the target word was spelled in thevery same way as the source word ?
and we set thisnumber  as  a  baseline  for  the  assessment  of  ourresults.Preparation of the Training and Test FilesThe  training  file  was  formatted  as  described  inSection 3.1.
In addition to that, the training and testfiles  featured  a  START/START delimiter  at  thebeginning of the word and ./END delimiter at theend of it (Figure 1, Line 6).Learning ParametersOnce  formatting was  done,  the  training file  waspassed  on  to  SVMTlearn.
Notably,  SVMToolcomes  with  a  standard  configuration:  for  thepurpose of this exercise we decided to keep most ofthe standard default parameters, while tuning onlythe settings related to the definition of the featureset.
Also, because of the choices made during thedesign of the training file ?
i.e.
to stick to a strictlinear layout in the L1 word ?
we felt that a rathersmall context window of 5 with the core positionset to 2 ?
that is, considering a context of 2 featuresbefore  and  2  features  after  the  feature  currentlyexamined ?
could offer a good trade-off betweenaccuracy and acceptable working times.
Altogether185  features  were  learnt,  which  confirmed  theintuition  mentioned  in  Section  3.1.
Furthermore,when considering the feature definition, we decidedto stick to unigrams, bigrams and trigrams, even if28up to five-grams were obviously possible.
Notably,the configuration file pictured below shows how aModel 0 and a global left-right-left tagging optionwere  applied.
Both  choices  were  made  after  anextensive  empirical  observation  of  severalmodel/direction combinations.
This  file  is  highlyconfigurable  and  offers  a  vast  range  of  possiblecombinations.
Future activities will concentrate to agreater  extent  on  the  experimentations  of  otherpossible configuration scenarios in order to find thetuning that  performs best.
Gimenez and Marquez(2004)  offer a  detailed description of  the  modelsand  all  available  options,  as  well  as  a  generalintroduction to the use of SVMtool, while Figure 4shows the feature set used to learn mappings from alist of English/German cognate pairs.#ambiguous-right [default]A0k = w(-2) w(-1) w(0) w(1) w(2) w(-2,-1)w(-1,0) w(0,1) w(1,2) w(-1,1) w(-2,2)w(-2,1) w(-1,2) w(-2,0) w(0,2) w(-2,-1,0)w(-2,-1,1) w(-2,-1,2) w(-2,0,1) w(-2,0,2)w(-1,0,1) w(-1,0,2) w(-1,1,2) w(0,1,2) p(-2)p(-1) p(0) p(1) p(2) p(-2,-1) p(-1,0) p(0,1)p(1,2) p(-1,1) p(-2,2) p(-2,1) p(-1,2)p(-2,0) p(0,2) p(-2,-1,0) p(-2,-1,1)p(-2,-1,2) p(-2,0,1) p(-2,0,2) p(-1,0,1)p(-1,0,2) p(-1,1,2) p(0,1,2) k(0) k(1) k(2)m(0) m(1) m(2)Figure 4.
Feature set for known words (A0k).
Thesame feature set is used for unknown words (A0u),as well.Tagging of the Test File and Cognate GenerationFollowing the learning step, a tagging routine wasinvoked,  which  produced the  best  scoring outputfor every single line ?
i.e.
letter or word boundary ?of the test file, which now looked very similar tothe file we used for training (Figure 1, Line 7).
Atthis  stage,  we  grouped  test  instances  together  toform words and associated each  L1 word with itsnewly generated counterpart in L2 (Figure 1, Line8).4.3 ResultsThe generated words were then compared with thewords included in the original cognate file.When evaluating the results we decided to splitthe data into three classes, rather than two: ?Yes?
(correct), ?No?
(incorrect) and ?Very Close?.
Thereason why we chose to add an extra class was thatwhen  analysing  the  data  we  noticed  that  manyimportant  mappings  were  correctly  detected,  butthe  word  was  still  not  perfect  because  of  minororthographic discrepancies that the tagging moduledid get right in a different entry.
In such cases wefelt that more training data would have produced astronger  association  score  that  could  haveeventually led to a correct output.
Decisions weremade  by  an  annotator  with  a  well-groundedknowledge of Support  Vector Machines and theirbehaviour,  which  turned  out  to  be  quite  usefulwhen deciding which output should be classified as?Very Close?.
For fairness reasons, this extra classwas added to the ?No?
class when delivering thefinal results.
Examples of  the ?Very Close?
classare reported in Table 1.Original EN Original DE Output DEmajestically majestatetisch majestischsetting setzend settendmachineries maschinerien machineriennaked nakkt nacktsouthwest suedwestlich suedwestTable 1.
Examples of the class ?Very Close?.In Figure 5 we show the  accuracy of the SVM-based  cognate  generation  algorithm  versus  thebaseline, adding the ?Very Close?
class to both the?Yes?
class (correct) and the ?No?
class (incorrect).Figure 5.
Accuracy of the SVM-based algorithmvs.
the baseline (blue line).The test file included a total of 422 entries, with85 orthographically identical entries in  L1 and  L2(baseline).
The SVM-based algorithm managed toproduce 128 correct cognates, making errors in 26429cases.
The ?Very Close?
class was assigned to 30entries.
Figure  5  shows that  30.33% of  the  totalentries were correctly identified, while an increaseof 50.58% over the baseline was achieved.5 Conclusions and Future WorkIn  this  paper  we  proposed  an  algorithm for  theautomatic  generation  of  cognates  from  twodifferent languages sharing the same alphabet.
Anincrease of 50.58% over the baseline and a 30.33%of overall accuracy were reported.
Even if accuracyis  rather  poor, if  we consider that  no knowledgerepository other  than an initial list of cognates wasavailable,  we  feel  that  the  results  are  still  quiteencouraging.As  far  as  the  learning  module  is  concerned,future ameliorations will focus on the fine tuning ofthe features used by the classifier as well as on thechoice of the model, while main research activitiesare  still  concerned  with  the  development  of  amethodology allowing for language portability: asa  matter  of  fact,  n-gram  co-occurrencies  arecurrently  being  investigated  as  a  possiblealternative to Edit Distance.ReferencesChris  Brew  and  David  McKelvie.
1996.
Word-PairExtraction  for  Lexicography.
Proceedings  of  theSecond International Conference on New Methods inLanguage Processing, 45-55.Pernilla  Danielsson and Katarina  Muehlenbock.
2000.Small  but  Efficient:  The  Misconception  of  High-Frequency  Words  in  Scandinavian  Translation.Proceedings of the 4th Conference of the Associationfor  Machine  Translation  in  the  Americas  onEnvisioning Machine Translation in the InformationFuture, 158-168.Jesus Gimenez and Lluis Marquez.
2004.
SVMTool: AGeneral  POS  Tagger Generator  Based  on  SupportVector Machines.
Proceedings of LREC '04, 43-46.Diana  Inkpen,  Oana  Frunza  and  Grzegorz  Kondrak.2005.
Automatic Identification of Cognates and FalseFriends  in  French  and  English.
Proceedings of  theInternational Conference Recent Advances in NaturalLanguage Processing, 251-257.Mehdi M. Kashani,  Fred Popowich, and Fatiha Sadat.2006.
Automatic  Translitteration  of  Proper  Nounsfrom Arabic to English.
The Challenge of Arabic ForNLP/MT, 76-84.Philipp  Koehn  and  Kevin  Knight.
2002.
EstimatingWord  Translation  Probabilities  From  UnrelatedMonolingual  Corpora  Using  the  EM  Algorithm.Proceedings of the 17th AAAI conference, 711-715.Grzegorz Kondrak.
2004.
Combining  Evidence  inCognate Identification.
Proceedings of  Canadian AI2004:  17th Conference of  the  Canadian  Society  forComputational Studies of Intelligence, 44-59.Grzegorz  Kondrak  and  Bonnie  J.  Dorr.
2004.Identification of confusable drug names.
Proceedingsof  COLING 2004: 20th International Conference onComputational LInguistics, 952-958.Sara Laviosa.
2001.
Corpus-based Translation Studies:Theory, Findings, Applications.
Rodopi, Amsterdam.Vladimir I. Levenshtein.
1965.
Binary codes capable ofcorrecting deletions, insertions and reversals.
DokladyAkademii Nauk SSSR, 163(4):845-848.Gideon S. Mann and David Yarowsky.
2001.
MultipathTranslation Lexicon Induction via Bridge Languages.Proceedings  of  NAACL  2001:  2nd Meeting  of  theNorth  American  Chapter  of  the  Association  forComputational Linguistics, 151-158.I.
Dan Melamed.
1999.
Bitext Maps and Alignment viaPattern  Recognition.
Computational  Linguistics,25(1):107-130.I.
Dan  Melamed.
2001.
Empirical  Methods  forExploiting  Parallel  Texts.
MIT  Press,  Cambridge,MA.Andrea  Mulloni  and  Viktor  Pekar.
2006.
AutomaticDetection  of  Orthographic  Cues  for  CognateRecognition.
Proceedings of LREC '06, 2387-2390.Michel  Simard,  George  F. Foster  and  Pierre  Isabelle.1992.
Using Cognates to Align Sentences in BilingualCorpora.
Proceedings  of  the  4th  InternationalConference  on  Theoretical  and  MethodologicalIssues in Machine Translation, Montreal, Canada, 67-81.30
