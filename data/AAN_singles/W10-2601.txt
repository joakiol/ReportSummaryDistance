Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 1?7,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsAdaptive Parameters for Entity Recognition with Perceptron HMMsMassimiliano Ciaramita?GoogleZu?rich, Switzerlandmassi@google.comOlivier ChapelleYahoo!
ResearchSunnyvale, CA, USAchap@yahoo-inc.comAbstractWe discuss the problem of model adapta-tion for the task of named entity recog-nition with respect to the variation of la-bel distributions in data from different do-mains.
We investigate an adaptive exten-sion of the sequence perceptron, where theadaptive component includes parametersestimated from unlabelled data in combi-nation with background knowledge in theform of gazetteers.
We apply this ideaempirically on adaptation experiments in-volving two newswire datasets from dif-ferent domains and compare with otherpopular methods such as self training andstructural correspondence learning.1 IntroductionModel adaptation is a central problem in learning-based natural language processing.
In the typicalsetting a model is trained on annotated in domain,or source, data, and is used on out of domain, ortarget, data.
The main difference with respect tosimilar problems such as semi-supervised learningis that source and target data are not assumed tobe drawn from the same distribution, which mightactually differ in relevant distributional properties:topic, domain, genre, style, etc.
In some formu-lations of the problem a few target labeled data isassumed to be available (Daume?
III, 2007).
How-ever, we are interested in the case in which no la-beled data is available from the target domain ?except for evaluation purposes and fine tuning ofhyperparameters.Most of the work in adaptation has focusedso far on the input side; e.g, proposing solutionsbased on generating shared source-target represen-tations (Blitzer et al, 2006).
Here we focus in-stead on the output aspect.
We hypothesize that?This work was carried out while the first author wasworking at Yahoo!
Research Barcelona.part of the loss incurred in using a model out ofdomain is due to its built-in class priors whichdo not match the class distribution in the targetdata.
Thus we attempt to explicitly correct theprediction of a pre-trained model for a given la-bel by taking into account a noisy estimate of thelabel frequency in the target data.
The correc-tion is carried out by means of adaptive param-eters, estimated from unlabelled target data andbackground ?world knowledge?
in the form ofgazetteers, and taken in consideration in the de-coding phase.
We built a suitable dataset for exper-imenting with different adaptation approaches fornamed entity recognition (NER).
The main find-ings from our experiments are as follows.
First,the problem is challenging and only marginal im-provements are possible under all evaluated frame-works.
Second, we found that our method com-pares well with current state-of-the-art approachessuch as self training and structural correspondencelearning (McClosky et al, 2006; Blitzer et al,2006) and taps on an interesting aspect whichseems worth of further research.
Although weconcentrate on a segmentation task within a spe-cific framework, the perceptron HMM introducedby Collins (2002), we speculate that the same in-tuition could be straightforwardly applied in otherlearning frameworks (e.g., Support Vector Ma-chines) and different tasks (e.g., standard classi-fication).2 Related workRecent work in domain adaptation has focusedon approaches such as self-training and struc-tural correspondence learning (SCL).
The formerapproach involves adding self-labeled data fromthe target domain produced by a model trainedin-domain (McClosky et al, 2006).
The latterapproach focuses on ways of generating sharedsource-target representations based on good cross-domain (pivot) features (Blitzer et al, 2006) (see1also (Ando, 2004)).
Self training has proved ef-fective in syntactic parsing, particularly in tan-dem with discriminative re-ranking (Charniak andJohnson, 2005), while the SCL has been appliedsuccessfully to tasks such PoS tagging and opin-ion analysis (Blitzer et al, 2006; Blitzer et al,2007).
We address a different aspect of the adapta-tion problem, namely the difference in label distri-butions between source and target domains.
Chanand Ng (2006) proposed correcting the class priorsfor domain adaptation purposes in a word sensedisambiguation task.
They adopt a generativeframework where the base model is a naive Bayesclassifier and priors are re-estimated with EM.
Theapproach proposed by Chelba and Acero (2004) isalso related as they propose a MAP adaptation viaGaussian priors of a MaxEnt model for recoveringthe correct capitalization of text.Domain adaptation naturally invokes the exis-tence of a specific task and data.
As such it isnatural to consider the modeling aspects withinthe context of a specific application.
Here wefocus on the problem of named entity recogni-tion (NER).
There is still little work on adapta-tion for NER.
Ando (2004) reports successful ex-periments on adapting with an SCL-like approach,while Ciaramita and Altun (2005) effectively usedexternal knowledge in the form of gazetteers ina semi-Markov model.
Mika et al (2008) usedWikipedia to generate additional training data fordomain adaptation purposes.3 Problem statementNamed entity taggers detect mentions of instancesof pre-defined categories such as person (Per),location (Loc), organization (Org) and miscel-laneous (Misc).
The problem can be naturallyframed as a segmentation and labeling task.
Stateof the art systems, e.g., based on sequential op-timization, achieve excellent accuracy in domain.However, accuracy degrades if the target data di-verges in relevant distributional aspects from thesource.
As an example, the following is the out-put of a perceptron HMM1 trained on the CoNLL2003 English data (news) (Sang and Muelder,2003) when applied to a molecular biology text:21We used the implementation available from http://sourceforge.net/projects/supersensetag,more details on this tagger can be found in (Ciaramita andAltun, 2006).2The same model achieves F-scores well in excess of 90%evaluated in domain.
(1) Cdc2-cyclin Org B-activated Polo-like Misckinase specifically phosphorylates at least threecomponents of APC Org .The tagger predicts several CoNLL entities whichare unlikely to occur in that context.
One sourceof confusion is probably the shape of words, in-cluding case, numbers, and non alphabetical char-acters, which are also typical, and thus mislead-ing, of unrelated CoNLL entities.
However, weargue that the problem is partially due to the pa-rameters learned which reflect the distribution ofclasses in the source data.
The parameter, actingas biased priors, lead the tagger to generate inap-propriate distributions of labels.
We propose thatthis aspect of the problem might be alleviated bycorrecting the score for each class with an estimateof the class frequency in the target data.
Thus,with respect to the example, we would like to de-crease the score of ?Org?
labels according to theirexpected frequency in a molecular biology corpus.4 A perceptron with adjustable priorsAs generic taggers we adopt perceptron-trainedHMMs (Collins, 2002) which have excellent ef-ficiency/performance trade-off (Nguyen and Guo,2007).
The objective of learning is a discrimi-nant F : X ?
Y ?
IR, where Y denotes se-quences of labels from a pre-defined set of cate-gories Y .
F (x,y;?)
= ??,?(x,y)?
is linear ina feature representation ?
defined over a joint in-put/output space,3 a global feature representationmapping each (x,y) pair to a vector of featurecounts ?
(x,y) ?
IRd:[?
(x,y)]i =|y|?j=1?i(yj?1, yj ,x), (2)where ?i is a (binary) predicate.
Given an inputsequence x, we find the optimal label sequence,f(x;?)
= arg maxy?Y F (x,y;?
), with Viterbidecoding.
The model ?
is learned with the per-ceptron algorithm.Each feature represents a spelling or contex-tual property, or the previous label.
The sim-plest baseline (model B) uses the features listedin the upper half of Table 1.
In previous workon NER adaptation, Ciaramita and Altun (2005)found that gazetteers, in combination with semi-Markov models, significantly improved adapta-tion.
Similarly, we define additional features using3?u,v?
denoting the inner product between u and v.2Model B featuresFeature example token feature value(s) PositionLowercase word Pierre pierre i-1, i, i+1Part of Speech Pierre NNP i-1, i, i+1Word Shape Pierre Xx i-1, i, i+1Suffix2/3 Pierre {re, rre} iPrefix2/3 Pierre {pi, pie} iPrevious label Vinken (in ?Pierre Vinken?)
B-PER (label on ?Pierre?)
iAdditional features of model BGFeature example token feature value(s) PositionInGazetteer Islands (in ?Cayman Islands?)
I-Country2 (inside a 2-word country name) i-1, i, i+1Most frequent supersense Eve B-Per1 (1 token Person label) i2 most frequent supersenses Eve B-Per-Time1 (1 token Person/Time label) iNumber of supersenses Eve B-NSS41 iTable 1.
Feature list and examples.
The upper half lists the features for the baseline tagger (B), the lower halflists the additional features extracted from the gazetteers included to the second non-adapted tagger (BG).
Thelast number on the feature indicates the length of the entry in the list; e.g., ?Islands?
in the example is the end ofa two-word item, in the country gazetteer, because of ?Cayman Islands?.
The remaining features capture the mostfrequent Wordnet supersense of the word, the first and second most frequent supersenses, and the total number ofsupersenses.the gazetteers from GATE,4 (Cunningham et al,2002) namely, countries, person first/last names,trigger words; and also from Wordnet: using thelexicographers or supersense labels; and a list ofcompany names from Fortune 500.
For this sec-ond baseline (model BG) we also extract the fea-tures in the bottom half of Table 1.4.1 Decoding with external priorsIn our method training is performed on the sourcedata using the perceptron algorithm.
Adaptationtakes place at decoding time, when the score ofthe entity labels is adjusted according to a k-dimensional parameter vector ?, k = |Y |, esti-mated by comparing the source and the unlabeledtarget data.
The score of a sequence y?
for input xin the target domain is computed with a variant ofthe original discriminant:F ?(x,y;?)
=|y|?j=1(d?i=1?i(yj?1, yj ,x)?i)+ ?
?yj (3)where ?yj is the adaptive parameter associatedwith yj , and ?
is a scaling factor.
The new predic-tion for x is f ?(x;?)
= arg maxy?Y F ?(x,y;?
).5 Adaptive parameters5.1 ThetaThe vector ?
encodes information about the ex-pected difference in frequency of each cate-gory between source and target.
Let gQ(c) =4http://www.gate.ac.uk/.count(c,Q)Pc?
count(c?,Q) be an estimate of the relative fre-quency of class c in corpus Q.
We propose to for-mulate ?c as:?c =gT (c)?
gS(c)gS(c)(4)where T and S are, respectively, the source andtarget data.
This is the ratio of the difference be-tween in and out domain relative frequencies forclass c, with respect to the in domain frequency.Intuitively, gS(c) represents an estimate of the fre-quency of c in the source S, and ?c an estimateof the expected decrease/increase as a fraction ofthe initial guess; ?c is negative if class c is lessfrequent in the target data than in the source data,and positive otherwise.
From this, it is clear thatequation (3) will offset the scores in the desireddirection.A crucial issue is the estimation of count(c,Q),a guess of the frequency of c in Q.
A simplesolution could be to count directly the class fre-quencies from the labeled source data, and to ob-tain a noisy estimate on the target data by count-ing the occurrence of entities that have known la-bels in the source data.
This approach unfortu-nately works very badly for at least two reasons.First, the number of entities in each class reflectsthe frequency of the class in the source.
There-fore using lists of entities from the source as prox-ies for the class in the target data can transfer thesource bias to the target.
Second, entities can havedifferent senses in different domains; e.g., severalEnglish city names occur in the Wall Street Jour-nal as locations (Liverpool, Manchester, etc.)
and3Attribute CoNLL BBN-4# tokens 300K 1.046MSource Reuters Wall Street JournalDomain General news FinancialYears 1992 1987# entities 34,841 58,637Loc 30.48% 22.51%Per 28.58% 20.08%Org 26.55% 46.27%Misc 14.38% 10.41%Table 2.
BBN and CoNLL datasets.in Reuters news as both locations and organiza-tions (football clubs).
We propose to use lists ofwords which are strongly associated with entitiesof specific classes but are extracted from an inde-pendent third source.
In this way, we hope the biasthey carry will be transferred in similar ways toboth source and target.
Similarly, potential am-biguities should be randomly distributed betweensource and target.
Thus, as a first approximation,we propose that given a list of words Lc, suppos-edly related to c and generated independently fromsource and target, count(c,Q) can be defined as:count(c,Q) ?
?w?Lccount(w,Q) (5)5.2 TauThe scalar ?
needs to be large enough to revisethe decision of the base model, if necessary.
How-ever, ?
should not be too large, otherwise the bestprediction of the base model would be ignored.In order for ?
to have an effective, but balanced,magnitude we introduce a simple notion of mar-gin.
Let the score of a given label ys on token sbe: G(x, ys;?)
=?di=1 ?i(ys?1, ys,x)?i, and lety?s = arg maxy?Y G(x, y;?
), we define the mar-gin on s as:Ms ?
minys 6=y?s(G(x, y?s;?
)?G(x, ys;?)).
(6)The mean of M provides a rough quantificationof the necessary amount by which we need tooffset the scores G(x, ys;?)
in order to changethe predictions.
As a first guess, we take ?
=?
(MS) = 1|S|?|S|s Ms, which we interpret as anupper bound on the desired value of ?
.
Whileexperimenting on the development data we foundthat ?
/2 yields good results.6 Experimental setup6.1 DataWe used two datasets for evaluation.
The firstis the English CoNLL 2003 dataset (Sang andMuelder, 2003), a corpus of Reuters news an-notated with person, location, organization andmiscellaneous entity tags.
The second is theBBN corpus (BBN, 2005), which supplements theWSJ Penn TreeBank with annotation for 105 cat-egories: named entities, nominal entities and nu-meric types.
We made the two datasets ?seman-tically?
compatible as follows.
We tagged a largecollection of text from the English Wikipedia withCoNLL and BBN taggers.
We counted the fre-quencies of BBN/CoNLL tag pairs for the samestrings, and assigned each BBN tag the most fre-quent CoNLL tag;5 e.g.,BBN tag CoNLL tagWork of art:Book ?
MiscOrganization:Educational ?
OrgLocation:Continent ?
LocPerson ?
Per48 BBN-to-CoNLL pairs were labelled in thisway.
Remaining categories, e.g., descriptive andnumerical types, were mapped to the Outside tagas they are not marked in CoNLL.
Finally, we sub-stituted all tags in the BBN corpus with the corre-sponding CoNLL tag, we call this corpus BBN-4.
The data is summarized in Table 2.
Noticethe different label distributions: the BBN-4 datais characterized by a skewed distribution of labelswith organization by far the most frequent class,while the CoNLL data has a more uniform dis-tribution with location as the most frequent class.The CoNLL data was randomly split in three dis-joint sets of sentences for training (16,540 sen-tences), development (2.068) and test (2,136).
ForBBN-4 we used WSJ sections 2-21 for training(39,823), section 22 for development (1,700) andsection 23 for test (2,416).
We evaluated models inboth directions; i.e., swapping CoNLL and BBN-4as source/target.6.2 Model tuningWe regularize the perceptrons by averaging (Fre-und and Schapire, 1999).
The perceptron HMM5A simpler approach might that of manually mapping thetwo tagsets, however a number of cases that are not trivial toresolve emerges in this way.
For this reason we decided toadopt the described data-driven heuristic approach.4has only one hyper-parameter, the number of train-ing iterations (or epochs).
Models trained for ap-plication out of domain can benefit from earlystopping which provides an additional mean ofregularization.
For all models compared we usedthe development sets for choosing the number ofepochs for training the perceptron on the sourcedata.
This is an important step as different adapta-tion approaches yield different overfitting patternand it is important to control for this factor for afair comparison.
As an example, we found thatthe self-training models consistently overfit afterjust a few iterations after which performance has asteep drop.
The order of presentation of instancesin the training algorithm is randomized; for eachmethod we repeat the process 10 times and reportaverage F-score and standard error.The vector ?
was estimated using one of thesame gazetteers used in the base tagger (BG), alist of 1,438 trigger words from GATE.6 Theseare words associated with certain categories; e.g.,?abbess/Per?, ?academy/Org?, ?caves/Loc?, and?manifesto/Misc?.
The lists for different classescontain varying numbers of items and might con-tain misleading words.
To obtain more reliable es-timates of comparable magnitude between classeswe computed equation (4) several times by sam-pling an equal number of words from each listand taking the mean.
On the development set thisproved better than computing the counts from theentire list.Other sources could be evaluated, for exam-ple lists of entities of each class extracted fromWikipedia.
We used all single-word triggers: 191for Loc, 171 for Misc, 89 for Org and 592 for Per.With each list we estimated ?
as in Section 5.1 foreach of the four labels starting with ?B?, i.e., en-tity beginnings, ?
= 0 for the other five labels.
Tofind ?
we use as source S, the in-domain data, andas target T the out-domain data.
The lists containdifferent number of items and might contain mis-leading words.To set ?
we compute the mean margin (6)on CoNLL, using the tagger trained on CoNLL(mean(Ms) ?
50), similarly for BBN-4(mean(Ms) ?
38).
We used the developmentset to fine tune the adaptive rate setting it equalto ?
= 12mean(Ms).6This list corresponds to the list of words Lc of Sec-tion 5.1.6.3 Self trainingTo compare with self-training we trained a tagger(BG) on the training set of CoNLL.
With the tag-ger we annotated the training set of BBN-4, andadded the self-labeled data, 39,823 BBN-4 sen-tences, to the gold standard CoNLL training.
Sim-ilarly, in the reverse direction we trained a tagger(BG) on the training set of BBN-4, annotated thetraining set of CoNLL, and added the self-labeled16,540 CoNLL sentences to the BBN-4 training.We denote these models BGSELF , and the aug-mented sources as CoNLL+ and BBN-4+.6.4 Structural correspondence learningWe first implemented a simple baseline followingthe idea presented in (Ando, 2004).
The basic ideaconsists in performing an SVD decomposition ofthe feature-token matrix, where the matrix con-tains all the sentences from the source and targetdomains.
The goal is to capture co-occurrences offeatures and derive new features which are morestable.
More specifically, we extracted the 50 prin-cipal directions of the feature-token matrix andprojected all the data onto these directions.
Thisresults in 50 new additional features for each to-ken that we append to the original (sparse binary)feature vector ?i, 1 ?
i ?
d. In order to giveequal importance to the original and new features,we multiplied the new features by a constant fac-tor such that the average L1 norms of the new andold features are the same.
Note that this weight-ing might not be optimal but should be sufficientto detect if these new features are helpful or not.We then implemented several versions of struc-tural correspondence learning.
First, following theoriginal formulation (we refer to this model asSCL1), 100 pivot features are selected, these arefrequent features in both source and target data.For a given pivot feature k, a vector wk ?
Rdis computed by performing a regularized linearregression between all the other features and thegiven pivot feature.
The matrixW whose columnsare the wk is formed and the original feature vec-tors are projected onto the 50 top left singular vec-tors of W , yielding 50 new features.
We also triedthe following variants.
In the version we refer toas SCL2 we rescale the left singular vectors of Wby their corresponding singular values.
In the lastvariant (SCL3) we select the pivot features whichare frequent in the source and target domains andwhich are also predictive for the task (as measured5Model Source Target TestB BBN-4 CoNLL 60.4 ?.28BG BBN-4 CoNLL 66.1 ?.32BGSVD BBN-4 CoNLL 66.5 ?.26BGSCL1 BBN-4 CoNLL 66.8 ?.18BGSCL2 BBN-4 CoNLL 64.7 ?.24BGSCL3 BBN-4 CoNLL 66.8 ?.27BGSELF BBN-4+ CoNLL 65.5 ?.26BG?
BBN-4 CoNLL 66.8 ?.53Model Source Target TestB CoNLL BBN-4 65.0 ?.77BG CoNLL BBN-4 67.6 ?.69BGSVD CoNLL BBN-4 67.9 ?.54BGSCL1 CoNLL BBN-4 67.9 ?.45BGSCL2 CoNLL BBN-4 68.1 ?.53BGSCL3 CoNLL BBN-4 67.8 ?.34BGSELF CoNLL+ BBN-4 68.3 ?.36BG?
CoNLL BBN-4 70.3 ?.61Table 3.
Results of baselines and adaptive models.by the mutual information between the feature andthe class label).
The 50 additional features are ap-pended to the original (sparse binary) feature vec-tor ?i, 1 ?
i ?
d, and again, they are first rescaledin order to have the same average L1 norm as theold features over the entire dataset.7 Results and discussionTable 3 summarizes the experimental results onboth datasets.
We refer to our adaptive model asBG?.
Adapting a model from BBN-4 to CoNLL,self training (BGSELF, 65.5%) performs slightlyworse than the base model (BG, 66.1%).
Thebest SCL model, the original formulation, pro-duces a small, but likely significant, improvement(BGSCL1, 66.8%).
Our model (BG?, 66.8%),achieves the same result but with larger variance.The improvement of the best models over the firstbaseline (B, 60.4%) is considerable, +6.4%, butmostly due to gazetteers.In the adaptation experiments from CoNLL toBBN-4 both self training (BGSELF, 68.3%) andthe best SCL model (BGSCL1, 68.1%) are com-parable to the baseline (BG, 67.6%).
The adap-tive perceptron HMM (BG?, 70.3%) improves by2.7%, as much as model BG over B, again with aslightly larger variance.
It is not clear why othermethods do not improve as much.
Speculatively,although we implemented several variants, SCLmight benefit from further tuning as it involvesseveral pre-processing steps.
As for self training,the base tagger might be too inaccurate to supportthis technique.
It is fair to assume that the ad-ditional hyperparameters available to our model,e.g., ?
, provided some additional flexibility.
Wealso experimented with a few variants of estimat-ing ?
on the development set; i.e., different splitsof the unlabeled source/target data and differentsampling modes: with and without replacement,number of trials.
All of these aspects can havea significant impact on the quality of the model.This point brings up a more general issue withthe type of approach explored here: while adapt-ing the class priors seems easier than adapting thefull model it is not trivial to encode noisy worldknowledge into meaningful priors.
Alternatively,in the presence of some labeled data one could op-timize ?
directly.
This information could be alsoelicited from domain experts.
Another interestingalternative is the unsupervised estimation via EMas in (Chan and Ng, 2006).Overall, adaptation from BBN-4 to CoNLL isharder than from CoNLL to BBN-4.
A possi-ble explanation is that adapting from specific togeneral is harder then in the opposite direction:the specific corpus is more heavily biased towardsa domain (finance).
This intuition is compatiblewith the baselines performing better in the CoNLLto BBN-4 direction.
However, the opposite argu-ment, that adapting from specific to general shouldbe easier, has some appeal as well; e.g., if moregeneral means higher entropy it seems easier tomake a distribution more uniform than finding theright peak.In general, all adaptive techniques we evalu-ated provided only marginal improvements overthe baseline (BG) model.
To put things in con-text, it is useful to recall that when evaluated indomain the CoNLL and BBN-4 taggers (modelBG) achieve, respectively, 92.7% and 91.6% aver-age F-scores on the test data.
As the results illus-trate there is a considerable drop in out domain ac-curacy, significantly alleviated by adding featuresfrom gazetteers and to some extent by other meth-ods.
Following Dredze et al (2007) we hypoth-esize that a significant fraction of the loss is dueto labeling inconsistencies between datasets.
Al-though we did our best to optimize the benchmarkmethods it is possible that even better results couldbe achieved with self-training and SCL.
Howeverwe stress that different methods get at differentaspects of the problem: self-training targets datasparseness, SCL methods aims at generating bettershared input representations, while our approachfocuses on generating output distribution morecompatible with the target data.
It seems reason-6able to expect that better adaptation performancewould result from composite approaches, aimingat both better machine learning and task-specificaspects for the named entity recognition problem.8 ConclusionWe investigated the model adaptation problem fornamed entity recognition where the base model isa discriminatively trained HMM (Collins, 2002).We hypothesized that part of the loss incurred inusing a pre-trained model out of domain is dueto its built-in class priors which do not match theclass distribution of the out of domain data.
Totest this hypothesis, and attempt a solution, wepropose to explicitly correct the prediction of themodel for a given label by taking into account anoisy estimate of the label frequency in the tar-get data.
We found encouraging results from pre-liminary experiments.
It might thus be worth in-vestigating more principled formulations of thistype of method, in particular to eliminate someheuristic aspects, improve unsupervised estima-tions, and generalize to other classification tasksbeyond NER.AcknowledgmentsWe would like to thank the anonymous reviewersfor useful comments and pointers to related work.ReferencesRie Kubota Ando.
2004.
Exploiting unannotated cor-pora for tagging and chunking.
In Proceedings ofACL 2004.
Association for Computational Linguis-tics.BBN.
2005.
Pronoun coreference and entity typecorpus.
Linguistic Data Consortium (LDC) catalognumber LDC2005T33.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of EMNLP 2006.Association for Computational Linguistics.John Blitzer, Mark Dredzde, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes, andblenders: Domain adaptation for sentiment classi-fication.
In Proceedings of ACL 2007.Yee Seng Chan and Hwee Tou Ng.
2006.
Estimatingclass priors in domain adaptation for word sense dis-ambiguation.
In Proceedings of Coling-ACL, pages89?96.
Association for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proceedings of ACL 2005, pages 173?180.
Association for Computational Linguistics.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmaximum entropy capitalizer: Little data can helpa lot.
In Proceedings of EMNLP, pages 285?292.Association for Computational Linguistics.Massimiliano Ciaramita and Yasemin Altun.
2005.Named-entity recognition in novel domains with ex-ternal lexical knowledge.
In Advances in StructuredLearning for Text and Speech Processing (NIPS2005).Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.In Proceedings of EMNLP, pages 594?602.
Associ-ation for Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP 2002, pages 1?8.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE: Aframework and graphical development environmentfor robust NLP tools and applications.
In Proceed-ings of ACL 2002.
Association for ComputationalLinguistics.Hal Daume?
III.
2007.
Frustratingly easy domainadaptation.
In Proceedings of ACL.
Association forComputational Linguistics.Mark Dredze, John Blitzer, Pratha Pratim Taluk-dar, Kuzman Ganchev, Joao Graca, and FernandoPereira.
2007.
Frustratingly hard domain adaptationfor parsing.
In Proceedings of CoNLL Shared Task2007.
Association for Computational Linguistics.Y.
Freund and R.E.
Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37:277?296.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Reranking and self-training for parseradaptation.
In Proceedings of COLING-ACL 2006,pages 337?344.
Association for Computational Lin-guistics.Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza,and Jordi Atserias.
2008.
Learning to tag and tag-ging to learn: A case study on Wikipedia.
IEEEIntelligent Systems, 23(5):26?33.Nam Nguyen and Yunsong Guo.
2007.
Comparisonof sequence labeling algorithms and extensions.
InProceedings of ICML 2007, pages 681?688.Erik F. Tjong Kim Sang and Fien De Muelder.2003.
Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition.
InProceedings of CoNLL 2003 Shared Task, pages142?147.
Association for Computational Linguis-tics.7
