Partially Distribution-Free Learning of Regular Languages fromPositive SamplesAlexander ClarkISSCO / TIM, University of GenevaUNI-MAIL, Boulevard du Pont-d?Arve,CH-1211 Gene`ve 4, Switzerlandasc@aclark.demon.co.ukFranck ThollardEURISE, Universite?
Jean Monnet,23,Rue du Docteur Paul Michelon,42023 Saint-Etienne Ce?dex 2, Francethollard@univ-st-etienne.frAbstractRegular languages are widely used in NLP to-day in spite of their shortcomings.
Efficientalgorithms that can reliably learn these lan-guages, and which must in realistic applicationsonly use positive samples, are necessary.
Theselanguages are not learnable under traditionaldistribution free criteria.
We claim that an ap-propriate learning framework is PAC learningwhere the distributions are constrained to begenerated by a class of stochastic automata withsupport equal to the target concept.
We discusshow this is related to other learning paradigms.We then present a simple learning algorithmfor regular languages, and a self-contained proofthat it learns according to this partially distri-bution free criterion.1 IntroductionRegular languages, especially generated by de-terministic finite state automata are widely usedin Natural Language processing, for various dif-ferent tasks (Mohri, 1997).
Efficient learningalgorithms, that have some guarantees of cor-rectness, would clearly be useful.
Existing al-gorithms for learning deterministic automata,such as (Carrasco and Oncina, 1994) have onlyguarantees of identification in the limit (Gold,1967), generally considered not to be a goodguide to practical utility.
Unforunately theprospects for learning according to the moreuseful PAC-learning criterion are poor after thewell known result of (Kearns and Valiant, 1989).Distribution-free learning criteria require algo-rithms to learn for every possible combinationof concept and distribution.
Under this worst-case analysis many simple concept classes areunlearnable.
However in many situations it ismore realistic to assume that there is some re-lationship between the concept and the distri-bution, and furthermore in general only positiveexamples will be available.There are two ways of modelling this.
Thesimplest is to study the learnability of distribu-tions (Kearns et al, 1994; Ron et al, 1995).
Inthis case the samples are drawn from the distri-bution that is being learned.
The choice of errorfunction then becomes critical ?
the most nat-ural (and difficult) being the Kullback-Leiblerdivergence.
This means that any successful al-gorithm must produce hypotheses that assign anon-zero probability to every string.
If what weare interested in is learning the underlying non-probabilistic concept then these hypotheses willbe useless.
We have elsewhere proved (Clarkand Thollard, 2004) a suitable result similar tothat of (Ron et al, 1995), bounding the diver-gence, but that proof involves some more elab-orate technical machinery.The second way is to consider a traditionalconcept-learning problem, but to restrict theclass of distributions to some set that only gen-erates positive examples, and has some relationto the target concept.
It is this latter possibilitythat we explore here.In the particular case of learning languageswe will have an instance space of ??
for somefinite alphabet ?, and we shall have a conceptclass, in this paper, corresponding to the classof all regular languages.
In a distribution-freesetting this is not learnable from positive andnegative samples, nor a fortiori from positivesamples alone.
In our partially distribution-freeframework however, we are able to prove learn-ability with an additional parameter in the sam-ple complexity polynomial, that bounds a sim-ple property of the distribution.
We are able topresent a simple stand alone proof for this wellstudied class of languages.The rest of the paper is structured as follows.Section 2 argues for a modified version of PAClearning as being an appropriate learning frame-work for a range of NLP problems.
After defin-ing some notation in Section 3 we then definean algorithm that learns regular languages (Sec-tion 4) and then in Section 5 prove that it doesso according to this modified PAC-learnabilitycriterion.
We conclude with a critical analysisof our results.2 AppropriatenessRegular languages are widely used in a num-ber of different applications drawn from nu-merous domains such as computational biology,robotics etc.
In many of these areas, efficientlearning algorithms are desirable but in each theexact requirements will be different since thesources of information, and the desired proper-ties of the algorithms vary widely.
We arguehere that learning algorithms in NLP have cer-tain special properties that make the particu-lar learnability result we study here useful.
Themost important feature in our opinion is the ne-cessity for learning from positive examples only.Negative examples in NLP are rarely available.Even in a binary classification problem, therewill often be some overlap between the classesso that examples labelled with ?
are not nec-essarily negative examples of the class labelledwith +.
For this reason alone we consider a tra-ditional distribution-free PAC-learning frame-work to be wholly inappropriate.
An essentialpart of the PAC-learning framework is a sortof symmetry between the positive and negativeexamples.
Furthermore, there are a numberof negative results which rule out distributionfree learning of regular languages (Kearns et al,1994).A related problem is that in the sorts of learn-ing situations that occur in practice in NLPproblems, and also those such as first languageacquisition that one wishes to model formally,the distribution of examples is dependent on theconcept being learned.
Thus if we are modellingthe acquisition of the grammar of a language,the positive examples are the grammatical, orperhaps acceptable, sentences of the target lan-guage.
The distribution of examples is clearlyhighly dependent on the particular language,simply as a matter of fact, in that the sentencesin the sample are generated by people who haveacquired the language.It thus seems reasonable to require the dis-tribution to be drawn from some limited classthat depends on the target concept and gen-erates only positive examples ?
i.e.
where thesupport of the distribution is identical to thepositive part of the target concept.Our proposal is that when the class of lan-guages is defined by some simple class of au-tomata, we can consider only those distribu-tions generated by the corresponding stochas-tic automata.
The set of distributions is re-stricted and thus we call this partially distribu-tion free.
Thus when learning the class of regu-lar languages, which are generated by determin-istic finite-state automata, we select the classof distributions which are generated by PDFAs.Similarly, context free languages are normallydefined by context-free grammmars which canbe extended again to probabilistic or stochasticcontext free grammars.Formally, for every class of languages, L, de-fined by some formal device define a class ofdistributions, D, defined by a stochastic variantof that device.
Then for each language L, weselect the set of distributions whose support isequal to the language:D+L = {D ?
D : ?s ?
??
s ?
L ?
PD(s) > 0}Samples are drawn from one of these distri-butions.
There are two technical problems here:first, this doesn?t penalise over-generalisation.Since the distribution is over positive examples,negative examples have zero weight ?
whichwould give a hypothesis of all strings zero er-ror.
We therefore need some penalty functionover negative examples or alternatively requirethe hypothesis to be a subset of the target, anduse a one-sided loss function as in Valiant?s orig-inal paper (Valiant, 1984), which is what wedo here.
Secondly, this definition is too vague.The exact way in which you extend the ?crisp?language to a stochastic one can have seriousconsequences.
When dealing with regular lan-guages, for example, though the class of lan-guages defined by deterministic automata is thesame as that defined by non-deterministic lan-guages, the same is not true for their stochas-tic variants.
Additionally, one can have expo-nential blow-ups in the number of states whendeterminizing automata.
Similarly, with con-text free languages, (Abney et al, 1999) showedthat converting between two parametrisationsof models for stochastic context free languagesare equivalent but that there are blow-ups inboth directions.It is interesting to compare this to the PAC-learning with simple distributions model (De-nis, 2001).
There, the class of distributionsis limited to a single distribution derived fromalgorithmic complexity theory.
There are anumber of reasons why this is not appropriate.First there is a computational issue: since Kol-mogorov complexity is not computable, sam-pling from the distribution is not possible,though a lower bound on the probabilities canbe defined.
Secondly, there are very large con-stants in the sample complexity polynomial.
Fi-nally and most importantly, there is no reasonto think that in the real world, samples will bedrawn from this distribution; in some sense itis the easiest distribution to learn from since itdominates every other distribution up to a mul-tiplicative factor.We reject the identification in the limitparadigm introduced by (Gold, 1967) as un-suitable for three reasons.
First it is only anasymptotic bound that says nothing about theperformance of the algorithms on finite amountsof data; secondly because it must learn underall presentations of the data even when theseare chosen by an adversary to make it hard tolearn, and thirdly because it has no bounds onthe amount of computation allowed.An alternative way to conceive of this prob-lem is to consider the task of learning distri-butions directly (Kearns et al, 1994), a taskrelated to probability density estimation andlanguage modelling, where the algorithm isgiven examples drawn from a distribution andmust approximate the distribution closely ac-cording to some distance metric: usually theKullback-Leibler divergence or the variationaldistance.
We consider the choice between thedistribution-learning analysis, and the analysiswe present here to depend on what the under-lying task or phenomena to be modelled is.
Ifit is the probability of the event occurring, thenthe distribution modelling analysis is better.
Ifon the other hand it concerns binary judgmentsabout the membership of strings in some setthen the analysis we present here is preferable.The result of (Kearns et al, 1994) shows upa further problem.
Under a standard crypto-graphic assumption the class of acyclic PDFAsover a two-letter alphabet are not learnablesince the class of noisy parity functions can beembedded in this simple subclass of PDFAs.
(Ron et al, 1995) show that this can be cir-cumvented by adding an additional parameterto the sample complexity polynomial, the dis-tinguishability, which we define below.3 PreliminariesWe will write ?
for letters and s for strings.We have a finite alphabet ?, and ??
is thefree monoid generated by ?, i.e.
the set of allstrings with letters from ?, with ?
the emptystring (identity).
For s ?
??
we define |s| tobe the length of s. The subset of ??
of stringsof length d is denoted by ?d.
A distributionor stochastic language D over ??
is a functionD : ??
?
[0, 1] such that ?s???
D(s) = 1.
TheL?
norm between two distributions is defined asmaxs |D1(s) ?
D2(s)|.
For a multiset of stringsS we write S?
for the empirical distribution de-fined by that multiset ?
the maximum likelihoodestimate of the probability of the string.A probabilistic deterministic finite stateautomaton is a mathematical object thatstochastically generates strings of symbols.It has a finite number of states one of whichis a distinguished start state.
Parsing orgenerating starts in the start state, and atany given moment makes a transition with acertain probability to another state and emitsa symbol.
We have a particular symbol andstate which correspond to finishing.A PDFA A is a tuple (Q, ?, q0, qf , ?, ?, ?)
,where?
Q is a finite set of states,?
?
is the alphabet, a finite set of symbols,?
q0 ?
Q is the single initial state,?
qf 6?
Q is the final state,?
?
6?
?
is the final symbol,?
?
: Q???{?}
?
Q?
{qf} is the transitionfunction and?
?
: Q ?
?
?
{?}
?
[0, 1] is the next sym-bol probability function.
?
(q, ?)
= 0 when?
(q, ?)
is not defined.We will sometimes refer to automata by theset of states.
All transitions that emit ?
go tothe final state.
In the following ?
and ?
willbe extended to strings recursively in the normalway.The sum of the output transition from eachstates must be one: so for all q ?
Q?????{?}?
(q, ?)
= 1 (1)Assuming further that there is a non zero proba-bility of reaching the final state from each state:i.e.
?q ?
Q?s ?
??
: ?
(q, s?)
= qf ?
?
(q, s?)
> 0(2)the PDFA then defines a probability distribu-tion over ?
?, where the probability of generat-ing a string s ?
??
is PA(s) = ?
(q0, s?).
We willwrite L(A) for the support of this distribution,L(A) = {s ?
??
: PA(s) > 0}.
We will alsodefine Pq(s) = ?
(q, s?)
which we call the suffixdistribution of the state q.We say that two states q, q?
are ?-distinguishable if L?
(Pq, Pq?)
> ?
for some ?
>0.
An automaton is ?-distinguishable iff everypair of states is ?-distinguishable.
Since we canmerge states q, q?
which have L?
(Pq, Pq?)
= 0,we can assume without loss of generality thatevery PDFA has a non-zero distinguishability.Note that ?
(q0, s) where s ?
??
is the prefixprobability of the string s, i.e.
the probabilitythat the automaton will generate a string thatstarts with s.We will use a similar notation, neglecting theprobability function for (non-probabilistic) de-terministic finite-state automata (DFAs).4 AlgorithmWe shall first state our main result.Theorem 1 For any regular language L, whensamples are generated by a PDFA A whereL(A) = L, with distinguishability ?
and num-ber of states n, for any , ?
> 0, the algorithmLearnDFA will with probability at least 1?
?
re-turn a DFA H which defines a language L(H)that is a subset of L with PA(L(A)?L(H)) < .The algorithm will draw a number of samplesbounded by a polynomial in |?|, n, 1/?, 1/, 1/?,and the computation is bounded by a polynomialin the number of samples and the total length ofthe strings in the sample.We now define the algorithm LearnDFA.
Weincrementally construct a sequence of DFAsthat will generate subsets of the target lan-guage.
Each state of the hypothesis automatawill represent a state of the target and will haveattached a multiset of strings that approximatesthe distribution of strings generated by thatstate.
We calculate the following quantities m0and N from the input parameters.m0 =8?2 log48n|?|(n|?| + 2)??
(3)N = 2n|?|m0 (4)We start with an automaton that consists of asingle state and no transitions, and the attachedmultiset is a sample of strings from the target.At each step we sample N strings from the tar-get distribution.
This re-sampling ensures theindependence of all of the samples, and allowsus to apply bounds in a straightforward way.For each state u in the hypothesis automatonand letter ?
in the alphabet, such that there isno arc labelled with ?
out of u we construct acandidate node (u, ?)
which represents the statereached from u by the transition labelled with ?.For each string in the sample, we trace the cor-responding path through the hypothesis.
Whenwe reach a candidate node, we remove the pre-ceding part of the string, and add the rest tothe multiset of the candidate node.
Otherwise,in the case when the string terminates in thehypothesis automaton we discard the string.After we have done this for every string inthe sample, we select a candidate node (u, ?
)that has a multiset of size at least m0.
If thereis no such candidate node, the algorithm ter-minates, Otherwise we compare this candidatenode with each of the nodes already in the hy-pothesis.
The comparison we use calculates theL?-norm between the empirical distributions ofthe two multisets and says they are similar ifthis distance is less than ?/4.
We will makesure that with high probability these empiricaldistributions are close in the L?-norm to thesuffix distributions of the states they represent.Since we know that the suffix distributions ofdifferent states will be at least ?
apart, we canbe confident that we will only rarely make mis-takes.
If there is a node, v, which is similar thenwe conclude that v and (u, ?)
represent the samestate.
We therefore add an arc labelled with ?leading from u to v. If it is not similar to anynode in the hypothesis, then we conclude thatit represents a new node, and we create a newnode u?
and add an arc labelled with ?
leadingfrom u to u?.
In this case we attach the mul-tiset of the candidate node to the new node inthe hypothesis.
Intuitively this multiset will bea sample from the suffix distribution of the stateof the target that it represents.
We then discardall of the candidate nodes and their associatedmultisets, but keep the multisets attached to thestates of the hypothesis, and repeat.5 ProofWe can now prove that this algorithm has theproperties we claim.
We use one technicallemma that we prove in the appendix.Lemma 1 Given a distribution D over ?
?, forany ??
< 1/2, when we independently drawa number of samples m more than m0 =12?
?2 log12????
, into a multiset S then L?
(S?, D) <??
with probability greater than 1 ?
?
?.Let H0, H1, .
.
.
, Hk be the sequence of finiteautomata, the states labelled with multisets,generated by the algorithm when samples aregenerated by a target PDFA A.We will say that a hypothesis automaton Hiis ?-good if there is a bijective function ?
froma subset of states of A including q0, to all thestates of Hi such that ?
(q0) is the root nodeof Hi, and if there is an edge in Hi such that?
(u, ?)
= v then ?(?
?1(u), ?)
= ?
?1(v) i.e.
ifHi is isomorphic to a subgraph of the target thatincludes the root.
If ?
(q) = u then we say thatu represents q.
In this case the language gen-erated by Hi is a subset of the target language.Additionally we require that for every state v inthe hypothesis, the corresponding multiset sat-isfies L?
(S?v, P?
?1(v)) < ?/4.
When a multisetsatisfies this we will say it is ?-good.We will extend the function ?
to candidatenodes in the obvious way, and also the definitionof ?-good.Definition 1 (Good sample) We say that asample of size N is ?--good given a good hy-pothesis DFA H and a target A if all the candi-date nodes with multisets larger than the thresh-old m0 are ?-good, and that if PA(L(A) ?L(H)) >  then the number of strings thatexit the hypothesis automaton is more than12NPA(L(A) ?
L(H)).5.1 Approximately CorrectWe will now show if all the samples are good,that for all i?0, 1, .
.
.
, k, the hypothesis Hi willbe good, and that when the algorithm termi-nates the final hypothesis will have low error.We will do this by induction on the index i ofthe hypothesis Hi.
Clearly H0 is good.
Sup-pose Hi?1 is good, and we draw a good sample.Consider a candidate node (u, ?)
with multisetgreater than m0.Since the previous hypothesis was good, thiswill be a representative of a state q and thusthe multiset will be a sequence of independentdraws from the suffix distribution of this statePq.
Thus L?
( ?Su,?, Pq) < ?/4 by the good-ness of the sample.
We compare it to a statein the hypothesis v. If this state is a rep-resentative of the same state in the target v,then L?
(S?v, Pq) < ?/4 (by the goodness of themultisets), the triangle inequality shows thatL?
( ?Su,?, S?v) < ?/2, and therefore the compar-ison will return true.
On the other hand, let ussuppose that v is a representative of a differentstate qv.
We know that L?
( ?Su,?, Pq) < ?/4and L?
(S?v, Pqv) < ?/4 (by the goodness ofthe multisets), and L?
(Pq, Pqv) ?
?
(by the?-distinguishability of the target).
By the tri-angle inequality L?
(Pq, Pqv) ?
L?
( ?Su,?, Pq) +L?
( ?Su,?, S?v) + L?
(S?v, Pqv), which implies thatL?
( ?Su,?, S?v) > ?/2 and the comparison willreturn false.
In these cases Hi will be good.Alternatively there is no candidate node abovethreshold in which case the algorithm termi-nates, and i = k. The total number of stringsthat exit the hypotheis must then be less thann|?|m0 since there are at most n|?| candidatenodes each of which has multiset of size less thanm0.
By the definition of N and the goodness ofthe sample PA(L(A) ?
L(H)) < .
Since it isgood and thus defines a subset of the target lan-guage, this is a suitably close hypothesis.5.2 Probably CorrectWe must now show that by setting m0 suffi-ciently large we can be sure that with probabil-ity greater than 1?
?
all of the samples will begood.
We need to show that with high prob-ability a sample of size N will be good for agiven hypotheis G. We can assume that thehypothesis is good at each step.
Each step ofthe algorithm will increase the number of tran-sitions in the active set by at least 1.
There areat most n|?| transitions in the target; so thereare at most n|?|+2 steps in the algorithm sincewe need an initial step to get the multiset forthe root node and another at the end when weterminate.
So we want to show that a particu-lar sample will be good with probability at least1 ?
?n|?|+2 .There are two sorts of errors that can makethe sample bad.
First, one of the multisets couldbe bad, and secondly too few strings might exitthe graph.
There are at most n|?| candidatenodes, so we will make the probability of gettinga bad multiset less than ?/2n|?|(n|?|+ 2), andwe will make the probability of the second sortof error less than ?/2(n|?| + 2).First we bound the probability of getting abad multiset of size m0.
This will be satisfiedif we set ??
= ?/4 and ??
= ?/2n|?|(n|?| + 2),and use Lemma 1.We next need to show that at each step thenumber of strings that exit the graph will benot too far from its expectation, if PA(L(A) ?L(H)) > .
We can use Chernoff bounds toshow that the probability too few strings exitthe graph will be less than ?/2(n|?| + 2)e?N(G)PA(L(A)?L(H))/4 < e?N/4< ?/2(n|?| + 2)which will be satisfied by the value of N de-fined earlier, as can be easily verified.5.3 Polynomial complexitySince we need to draw at most n|?|+2 samplesof size N the overall sample complexity will be(n|?| + 2)N , which ignoring log factors gives asample complexity of O(n2|?|2?
?2?1), whichis quite benign.
It is easy to see that the com-putational complexity is polynomial.
Produc-ing an exact bound is difficult since it dependson the length of the strings.
The precise com-plexity also depends on the relative magnitudesof ?, |?| and so on.
The complexity is domi-nated by the cost of the comparisons.
We canlimit each multiset comparison to at most m0strings, which can be compared naively with m20string comparisons or much more efficiently us-ing hashing or sorting.
The number of nodesin the hypothesis is at most n, and the num-ber of candidate nodes is at most n|?|, so thenumber of comparisons at each step is boundedby n2|?| and thus the total number of multisetcomparisons by n2|?|(n|?|+2).
Construction ofmultisets can be performed in time linear in thesample size.
These observations suffice to showthat the computation is polynomially bounded.6 DiscussionThe convergence of these sorts of algorithmshas been studied before in the identification inthe limit framework, but previous proofs havenot been completely convincing (Carrasco andOncina, 1999), and this criterion gives no guideto the practical utility of the algorithms since itapplies only asymptotically.
The partially dis-tribution free learning problem we study hereis novel.
as is the extension of the results of(Ron et al, 1995) to cyclic automata and thusto infinite languages.Before we examine our results critically, wewould like to point out some positive aspects ofthe algorithm.
First, this class of algorithms isin practice efficient and reliable.
This particularalgorithm is designed to have a provably goodworst-case performance, and thus we anticipateits average performance on naturally occurringdata to be marginally worse than comparablealgorithms.
We have established that we canlearn an exponentially large family of infinitelanguages using polynomial amounts of dataand computation.
Mild properties of the in-put distributions suffice to guarantee learnabil-ity.
The algorithm we present here is howevernot intended to be efficient or cognitively plau-sible: our intention was to find one that alloweda simple proof.The major weakness of this approach in ouropinion is that the parameter n in the samplecomplexity polynomial is the number of statesin the PDFA generating the distribution, andnot the number of states in the minimal FA gen-erating the language.
Since determinisation offinite automata can cause exponential blow upsthis is potentially a serious problem, dependingon the application domain.
A second problemis the need for a distinguishability parameter,which again in specific cases could be exponen-tially small.
An alternative to this is to definea class of ?-distinguishable automata where thedistinguishability is bounded by an inverse poly-nomial in the number of states.
Formally this isequivalent, but it has the effect of removing theparameter from the sample complexity polyno-mial at the cost of having a further restrictionon the class of distributions.
Indeed we can dealwith the previous objection in the same way ifnecessary by requiring the number of states inthe generating PDFA to be bounded by a poly-nomial in the minimal number of states neededto generate the target language.
However bothof these limitations are unavoidable given thenegative results previously discussed.ReferencesS.
Abney, D. McAllester, and F. Pereira.1999.
Relating probabilistic grammars andautomata.
In Proceedings of ACL ?99.R.
C. Carrasco and J. Oncina.
1994.
Learn-ing stochastic regular grammars by means ofa state merging method.
In R. C. Carrascoand J. Oncina, editors, Grammatical Infer-ence and Applications, ICGI-94, number 862in LNAI, pages 139?152, Berlin, Heidelberg.Springer Verlag.R.
C. Carrasco and J. Oncina.
1999.
Learningdeterministic regular grammars from stochas-tic samples in polynomial time.
TheoreticalInformatics and Applications, 33(1):1?20.Alexander Clark and Franck Thollard.
2004.Pac-learnability of probabilistic determinis-tic finite state automata.
Journal of MachineLearning Research, 5:473?497, May.F.
Denis.
2001.
Learning regular languagesfrom simple positive examples.
MachineLearning, 44(1/2):37?66.E.
M. Gold.
1967.
Language indentification inthe limit.
Information and control, 10(5):447?
474.M.
Kearns and G. Valiant.
1989.
Crypto-graphic limitations on learning boolean for-mulae and finite automata.
In 21st annualACM symposium on Theory of computation,pages 433?444, New York.
ACM, ACM.M.J.
Kearns, Y. Mansour, D. Ron, R. Rubin-feld, R.E.
Schapire, and L. Sellie.
1994.
Onthe learnability of discrete distributions.
InProc.
of the 25th Annual ACM Symposiumon Theory of Computing, pages 273?282.Mehryar Mohri.
1997.
Finite-state transducersin language and speech processing.
Compu-tational Linguistics, 23(2):269?311.D.
Ron, Y.
Singer, and N. Tishby.
1995.
On thelearnability and usage of acyclic probabilisticfinite automata.
In COLT 1995, pages 31?40,Santa Cruz CA USA.
ACM.L.
Valiant.
1984.
A theory of the learnable.Communications of the ACM, 27(11):1134 ?1142.AppendixProof of Lemma 1.We write p(s) for the true probability andp?
(s) = c(s)/m for the empirical probability ofthe string in the sample ?
i.e.
the maximumlikelihood estimate.
We want to bound theprobability over an infinite number of strings,which rules out a naive application of Hoeffdingbounds.
It will suffice to show that every stringwith probability less than ?
?/2 will have empir-ical probability less than ?
?, and that all otherstrings will have probability within ??
of theirtrue values.
The latter is straightforward: sincethere are at most 2/??
of these frequent strings.For any given frequent string s, by Hoeffdingbounds:Pr[|p?
(s) ?
p(s)| > ??]
< 2e?2m?
?2 < 2e?2m0?
?2(5)So the probability of making an error on afrequent string is less than 4/??e?2m0?
?2 .Consider all of the strings whose probabilityis in [??2?
(k+1), ?
?2?k).Sk = {s ?
??
: ?
(q, s?)
?
[??2?
(k+1), ?
?2?k)}(6)We define Srare =?
?k=1 Sk.
The Chernoffbound says that for any ?
> 0, for the sum of nbernouilli variables with prob p andPr(X > (1 + ?
)np) <( e?
(1 + ?)(1+?
))np(7)Now we bound each group separately, usingthe binomial Chernoff bound where n = m??
>mp (which is true since p < ??)Pr[p?
(s) ?
??]?
(mpn)nen?mp (8)This bound decreases with p, so we can re-place this for all strings in Sk with the upperbound for the probability, and we can replacem with m0.Pr[p?
(s) ?
??]?(m0??2?km0??)m0??em0???m0??2?k?(2?ke1?2?k)m0?
?< 2?km0?
?Assuming that m0??
> 3Pr[p?
(s) ?
??
]< 2?2k22?m0?
?Pr[?s ?
Sk : p?
(s) ?
??]?
|Sk|2?2k22?m0???
8?2?k2?m0?
?Using the factor of the form 2?k, we can sumover all of the k.Pr[?s ?
Srare : p?
(s) ?
??]
<8??
2?m0???
?k=12?k< 8?2?m0?
?Putting these together we can show that theprobability of the bound being exceeded will be4??
e?2m0?
?2 + 8?2?m0??
< 12??
e?2m0?
?2 (9)This will be less than ??
ifm0 =12?
?2 log12????
(10)which establishes the result.
