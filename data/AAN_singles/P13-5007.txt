Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11?13,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsThe mathematics of language learningAndra?s KornaiComputer and Automation Research InstituteHungarian Academy of Sciencesandras@kornai.comGerald PennDepartment of Computer ScienceUniversity of Torontogpenn@cs.utoronto.eduJames RogersComputer Science DepartmentEarlham Collegejrogers@cs.earlham.eduAnssi Yli-Jyra?Department of Modern LanguagesUniversity of Helsinkianssi.yli-jyra@helsinki.fiOver the past decade, attention has gradu-ally shifted from the estimation of parameters tothe learning of linguistic structure (for a surveysee Smith 2011).
The Mathematics of Language(MOL) SIG put together this tutorial, composed ofthree lectures, to highlight some alternative learn-ing paradigms in speech, syntax, and semantics inthe hopes of accelerating this trend.Compounding the enormous variety of formalmodels one may consider is the bewildering rangeof ML techniques one may bring to bear.
In addi-tion to the surprisingly useful classical techniquesinherited from multivariate statistics such as Prin-cipal Component Analysis (PCA, Pearson 1901)and Linear Discriminant Analysis (LDA, Fisher1936), computational linguists have experimentedwith a broad range of neural net, nearest neighbor,maxent, genetic/evolutionary, decision tree, maxmargin, boost, simulated annealing, and graphicalmodel learners.
While many of these learners be-came standard in various domains of ML, withinCL the basic HMM approach proved surprisinglyresilient, and it is only very recently that deeplearning techniques from neural computing are be-coming competitive not just in speech, but alsoin OCR, paraphrase, sentiment analysis, parsingand vector-based semantic representations.
Thefirst lecture will provide a mathematical introduc-tion to some of the fundamental techniques thatlie beneath these linguistic applications of neuralnetworks, such as: BFGS optimization, finite dif-ference approximations of Hessians and Hessian-free optimization, contrastive divergence and vari-ational inference.Lecture 1: The mathematics ofneural computing ?
PennRecent results in acoustic modeling, OCR, para-phrase, sentiment analysis, parsing and vector-based semantic representations have shown thatnatural language processing, like so many othercorners of artificial intelligence, needs to pay moreattention to neural computing.I Gaussian Mixture Models?
Lagrange?s theorem?
Stochastic gradient descent?
typical acoustic models using GMMs andHMMsII Optimization theory?
Hessian matrices?
Broyden-Fletcher-Goldfarb-Shanno theory?
finite difference approximations of Hessians?
Hessian-free optimization?
Krylov methodsIII Application: Product models?
products of Gaussians vs. GMMs?
products of ?experts??
Gibbs sampling and Markov-chain MonteCarlo?
contrastive divergenceIV Experimentation: Deep NNs for acousticmodeling?
intersecting product models with Boltzmannmachines?
?generative pre-training??
acoustic modeling with Deep Belief Networks?
why DBNs work wellV Variational inference?
variational Bayes for HMMsIn spite of the enormous progress brought byML techniques, there remains a rather significantrange of tasks where automated learners cannotyet get near human performance.
One such is theunsupervised learning of word structure addressedby MorphoChallenge, another is the textual entail-ment task addressed by RTE.The second lecture recasts these and similarproblems in terms of learning weighted edges in asparse graph, and presents learning techniques thatseem to have some potential to better find spare fi-nite state and near-FS models than EM.
We willprovide a mathematical introduction to the Min-imum Description Length (MDL) paradigm and11spectral learning, and relate these to the better-known techniques based on (convex) optimizationand (data-oriented) memorization.Lecture 2: Lexical structuredetection ?
KornaiWhile modern syntactic theory focuses almost en-tirely on productive, rule-like regularities withcompositional semantics, the vast bulk of the infor-mation conveyed by natural language, over 85%,is encoded by improductive, irregular, and non-compositional means, primarily by lexical mean-ing.
Morphology and the lexicon provide a richtesting ground for comparing structure learningtechniques, especially as inferences need to bebased on very few examples, often just one.I Motivation?
Why study structure??
Why study lexical structure?II Lexical structure?
Function words, content words?
Basic vocabulary (Ogden 1930, Swadesh 1950,Yasseri et al012)?
Estimation styleIII Formal models of lexical semantics?
Associative (Findler 1979, Dumais 2005, CVSmodels)?
Combinatorial (FrameNet)?
Algebraic (Kornai 2010)IV Spectral learning?
Case frames and valency?
Spectral learning as data cleaning (Ng 2001)?
Brew and Schulte im Walde 2002 (German),Nemeskey et alHungarian)?
Optionality in case framesV Models with zeros?
Relating ungrammaticality and low probabil-ity (Pereira 2000, Stefanowitsch 2006)?
Estimation errors, language distances (Kornai1998, 2011)?
Quantization errorVI Minimum description length?
Kolmogorov complexity and universal gram-mar (Clark 1994)?
MDL in morphology (Goldsmith 2000, Creutzand Lagus 2002, 2005,...)?
MDL for weighted languages?
Ambiguity?
Discarding data ?
yes, you can!?
Collapsing categoriesVII New directions?
Spectral learning of HMMs (Hsu et al009,2012)?
of weighted automata (Balle and Mohri 2012)?
Feature selection, LASSO (Pajkossy 2013)?
Long Short-Term Memory (Monner and Reg-gia 2012)?
Representation learning (Bengio et al013)Given the broad range of competing formalmodels such as templates in speech, PCFGs andvarious MCS models in syntax, logic-based andassociation-based models in semantics, it is some-what surprising that the bulk of the applied workis still performed by HMMs.
A particularly signifi-cant case in point is provided by PCFGs, whichhave not proved competitive with straight tri-gram models.
Undergirding the practical failureof PCFGs is a more subtle theoretical problem,that the nonterminals in better PCFGs cannot beidentified with the kind of nonterminal labels thatgrammarians assume, and conversely, PCFGs em-bodying some form of grammatical knowledge tendnot to outperform flatly initialized models thatmake no use of such knowledge.
A natural responseto this outcome is to retrench and use less power-ful formal models, and the last lecture will be spentin the subregular space of formal models even lesspowerful than finite state automata.Lecture 3: Subregular Languagesand Their Linguistic Relevance ?Rogers and Yli-Jyra?The difficulty of learning a regular or context-freelanguage in the limit from positive data gives amotivation for studying non-Chomskyan languageclasses.
The lecture gives an overview of the tax-onomy of the most important subregular classes oflanguages and motivate their linguistic relevancein phonology and syntax.I Motivation?
Some classes of (sub)regular languages?
Learning (finite descriptions of) languages?
Identification in the limit from positive data?
Lattice leanersII Finer subregular language classes?
The dot-depth hierarchy and the local andpiecewise hierarchies?
k-Local and k-Piecewise SetsIII Relevance to phonology?
Stress patterns?
Classifying subregular constraintsIV Probabilistic models of language?
Strictly Piecewise Distributions (Heinz andRogers 2010)V Relevance to syntax?
Beyond the inadequate right-linear grammars?
Parsing via intersection and inverse morphism12?
Subregular constraints on the structure anno-tations?
Notions of (parameterized) locality in syntax.The relevance of some parameterized subregularlanguage classes is shown through machine learn-ing and typological arguments.
Typological resultson a large set of languages (Heinz 2007, Heinz et al2011) relate language types to the theory of sub-regular language classes.There are finite-state approaches to syn-tax showing subregular properties.
Althoughstructure-assigning syntax differs from phonotac-tical constraints, the inadequacy of right-lineargrammars does not generalize to all finite-staterepresentations of syntax.
The linguistic relevanceand descriptive adequacy are discussed, in particu-lar, in the context of intersection parsing and con-junctive representations of syntax.InstructorsAnssi Yli-Jyra?
is Academy Research Fellow of theAcademy of Finland and Visiting Fellow at ClareHall, Cambridge.
His research focuses on finite-state technology in phonology, morphology andsyntax.
He is interested in weighted logic, depen-dency complexity and machine learning.James Rogers is Professor of Computer Science atEarlham College, currently on sabbatical at theDepartment of Linguistics and Cognitive Science,University of Delaware.
His primary research in-terests are in formal models of language and for-mal language theory, particularly model-theoreticapproaches to these, and in cognitive science.Gerald Penn teaches computer science at the Uni-versity of Toronto, and is a Senior Member ofthe IEEE.
His research interests are in spokenlanguage processing, human-computer interaction,and mathematical linguistics.Andra?s Kornai teaches at the Algebra Depart-ment of the Budapest Institute of Technology,and leads the HLT group at the Computer andAutomation Research Institute of the HungarianAcademy of Sciences.
He is interested in ev-erything in the intersection of mathematics andlinguistics.
For a list of his publications seehttp://kornai.com/pub.html.Online resourcesSlides for the tutorial:http://molweb.org/acl13tutorial.pdfBibliography:http://molweb.org/acl13refs.pdfSoftware:http://molweb.org/acl13sw.pdf13
