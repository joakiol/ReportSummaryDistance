Proceedings of the 7th Workshop on Statistical Machine Translation, pages 171?180,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsMatch without a Referee:Evaluating MT Adequacy without Reference TranslationsYashar Mehdad Matteo Negri Marcello FedericoFondazione Bruno Kessler, FBK-irstTrento , Italy{mehdad|negri|federico}@fbk.euAbstractWe address two challenges for automatic ma-chine translation evaluation: a) avoiding theuse of reference translations, and b) focusingon adequacy estimation.
From an economicperspective, getting rid of costly hand-craftedreference translations (a) permits to alleviatethe main bottleneck in MT evaluation.
Froma system evaluation perspective, pushing se-mantics into MT (b) is a necessity in orderto complement the shallow methods currentlyused overcoming their limitations.
Castingthe problem as a cross-lingual textual entail-ment application, we experiment with differ-ent benchmarks and evaluation settings.
Ourmethod shows high correlation with humanjudgements and good results on all datasetswithout relying on reference translations.1 IntroductionWhile syntactically informed modelling for statis-tical MT is an active field of research that has re-cently gained major attention from the MT commu-nity, work on integrating semantic models of ade-quacy into MT is still at preliminary stages.
This sit-uation holds not only for system development (mostcurrent methods disregard semantic information, infavour of statistical models of words distribution),but also for system evaluation.
To realize its full po-tential, however, MT is now in the need of semantic-aware techniques, capable of complementing fre-quency counts with meaning representations.In order to integrate semantics more deeply intoMT technology, in this paper we focus on the eval-uation dimension.
Restricting our investigation tosome of the more pressing issues emerging from thisarea of research, we provide two main contributions.1.
An automatic evaluation method that avoidsthe use of reference translations.
Most currentmetrics are based on comparisons between auto-matic translations and human references, and rewardlexical similarity at the n-gram level (e.g.
BLEU(Papineni et al, 2002), NIST (Doddington, 2002),METEOR (Banerjee and Lavie, 2005), TER (Snoveret al, 2006)).
Due to the variability of natural lan-guages in terms of possible ways to express the samemeaning, reliable lexical similarity metrics dependon the availability of multiple hand-crafted (costly)realizations of the same source sentence in the tar-get language.
Our approach aims to avoid this bot-tleneck by adapting cross-lingual semantic inferencecapabilities and judging a translation only given thesource sentence.2.
A method for evaluating translation adequacy.Most current solutions do not consistently rewardtranslation adequacy (semantic equivalence betweensource sentence and target translation).
The scarceintegration of semantic information in MT, specif-ically at the multilingual level, led to MT systemsthat are ?illiterate?
in terms of semantics and mean-ing.
Moreover, current metrics are often difficult tointerpret.
In contrast, our method targets the ade-quacy dimension, producing easily interpretable re-sults (e.g.
judgements in a 4-point scale).Our approach builds on recent advances incross-lingual textual entailment (CLTE) recognition,which provides a natural framework to address MTadequacy evaluation.
In particular, we approachthe problem as an application of CLTE where bi-171directional entailment between source and target isconsidered as evidence of translation adequacy.
Be-sides avoiding the use of references, the proposedsolution differs from most previous methods whichtypically rely on surface-level features, often ex-tracted from the source or the target sentence takenin isolation.
Although some of these features mightcorrelate well with adequacy, they capture seman-tic equivalence only indirectly, and at the level ofa probabilistic prediction.
Focusing on a combina-tion of surface, syntactic and semantic features, ex-tracted from both source and target (e.g.
?source-target length ratio?, ?dependency relations in com-mon?
), our approach leads to informed adequacyjudgements derived from the actual observation ofa translation given the source sentence.2 BackgroundSome recent works proposed metrics able to approx-imately assess meaning equivalence between can-didate and reference translations.
Among these,(Gime?nez and Ma`rquez, 2007) proposed a hetero-geneous set comprising overlapping and matchingmetrics, compiled from a rich set of variants at fivedifferent linguistic levels: lexical, shallow-syntactic,syntactic, shallow-semantic and semantic.
Moresimilar to our approach, (Pado?
et al, 2009) proposedsemantic adequacy metrics that exploit feature rep-resentations motivated by Textual Entailment (TE).Both metrics, however, highly depend on the avail-ability of multiple reference translations.Early attempts to avoid reference translations ad-dressed quality estimation (QE) by means of largenumbers of source, target, and system-dependentfeatures to discriminate between ?good?
and ?bad?translations (Blatz et al, 2004; Quirk, 2004).
Morerecently (Specia et al, 2010b; Specia and Farzindar,2010; Specia, 2011) conducted a series of experi-ments using features designed to estimate translationpost-editing effort (in terms of volume and time) asan indicator of MT output quality.
Good results inQE have been achieved by adding linguistic infor-mation such as shallow parsing, POS tags (Xionget al, 2010), or dependency relations (Bach et al,2011; Avramidis et al, 2011) as features.
However,in general these approaches do not distinguish be-tween fluency (i.e.
syntactic correctness of the out-put translation) and adequacy, and mostly rely onfluency-oriented features (e.g.
?number of punctu-ation marks?).
As a result, a simple surface formvariation is given the same importance of a contentword variation that changes the meaning of the sen-tence.
To the best of our knowledge, only (Specia etal., 2011) proposed an approach to frame MT evalu-ation as an adequacy estimation problem.
However,their method still includes many features which arenot focused on adequacy, and often look either at thesource or at the target in isolation (see for instance?source complexity?
and ?target fluency?
features).Moreover, the actual contribution of the adequacyfeatures used is not always evident and, for sometesting conditions, marginal.Our approach to adequacy evaluation builds onand extends the above mentioned works.
Similarlyto (Pado?
et al, 2009) we rely on the notion of textualentailment, but we cast it as a cross-lingual problemin order to bypass the need of reference translations.Similarly to (Blatz et al, 2004; Quirk, 2004), we tryto discriminate between ?good?
and ?bad?
transla-tions, but we focus on adequacy.
To this aim, like(Xiong et al, 2010; Bach et al, 2011; Avramidis etal., 2011; Specia et al, 2010b; Specia et al, 2011)we rely on a large number of features, but focusingon source-target dependent ones, aiming at informedadequacy evaluation of a translation given the sourceinstead of a more generic quality assessment basedon surface features.3 CLTE for adequacy evaluationWe address adequacy evaluation by adapting cross-lingual textual entailment recognition as a way tomeasure to what extent a source sentence and its au-tomatic translation are semantically similar.
CLTEhas been proposed by (Mehdad et al, 2010) as an ex-tension of textual entailment (Dagan and Glickman,2004) that consists in deciding, given a text T and ahypothesis H in different languages, if the meaningof H can be inferred from the meaning of T.The main motivation in approaching adequacyevaluation using CLTE is that an adequate trans-lation and the source text should convey the samemeaning.
In terms of entailment, this means that anadequate MT output and the source sentence shouldentail each other (bi-directional entailment).
Los-172ing or altering part of the meaning conveyed by thesource sentence (i.e.
having more, or different infor-mation in one of the two sides) will change the en-tailment direction and, consequently, the adequacyjudgement.
Framed in this way, CLTE-based ade-quacy evaluation methods can be designed to dis-tinguish meaning-preserving variations from true di-vergence, regardless of reference translations.Similarly to many monolingual TE approaches,CLTE solutions proposed so far adopt supervisedlearning methods, with features that measure to whatextent the hypotheses can be mapped into the texts.The underlying assumption is that the probability ofentailment is proportional to the number of words inH that can be mapped to words in T (Mehdad et al,2011).
Such mapping can be carried out at differ-ent word representation levels (e.g.
tokens, lemmas,stems), possibly with the support of lexical knowl-edge in order to cross the language barrier betweenT and H (e.g.
dictionaries, phrase tables).Under the same assumption, since in the adequacyevaluation framework the entailment relation shouldhold in both directions, the mapping is performedboth from the source to the target and vice-versa,building on features extracted from both sentences.Moreover, to improve over previous CLTE methodsand boost MT adequacy evaluation performance, weexplore the joint contribution of a number of lexi-cal, syntactic and semantic features (Mehdad et al,2012).Concerning the features used, it?s worth observ-ing that the cost of implementing our approach (interms of required resources and linguistic proces-sors), and the need of reference translations are in-trinsically different bottlenecks for MT.
While thelimited availability of processing tools for some lan-guage pairs is a ?temporary?
bottleneck, the acqui-sition of multiple references is a ?permanent?
one.The former cost is reducing over time due to theprogress in NLP research; the latter represents afixed cost that has to be eliminated.
Similar consid-erations hold regarding the need of annotated data todevelop our supervised learning approach.
Concern-ing this, the cost of labelling source-target pairs withadequacy judgments is significantly lower comparedto the creation of multiple references.3.1 FeaturesIn order to learn models for classification and regres-sion we used the Support Vector Machine (SVM)algorithms implemented in the LIBSVM package(Chang and Lin, 2011) with a linear kernel and de-fault parameters setting.
Aiming at objective ade-quacy evaluation, our method limits the recourse toMT system-dependent features to reduce the biasof evaluating MT technology with its own coremethods.
The experiments described in the follow-ing sections are carried out on publicly availableEnglish-Spanish datasets, exploring the potential ofa combination of surface, syntactic and semanticfeatures.
Language-dependent ones are extractedby exploiting processing tools for the two lan-guages (part-of-speech taggers, dependency parsersand named entity recognizers), most of which areavailable for many languages.Our feature set can be described as follows:Surface Form (F) features consider the num-ber of words, punctuation marks and non-wordmarkers (e.g.
quotations and brackets) in sourceand target, as well as their ratios (source/target andtarget/source), and the number of out of vocabularyterms encountered.Shallow Syntactic (SSyn) features considerthe number and ratios of common part-of-speech(POS) tags in source and target.
Since the list ofvalid POS tags varies for different languages, wemapped English and Spanish tags into a commonlist using the FreeLing tagger (Carreras et al, 2004).Syntactic (Syn) features consider the numberand ratios of dependency roles common to sourceand target.
To create a unique list of roles, we usedthe DepPattern (Otero and Lopez, 2011) package,which provides English and Spanish dependencyparsers.Phrase Table (PT) matching features are cal-culated as in (Mehdad et al, 2011), with a phrasalmatching algorithm that takes advantage of a lexicalphrase table extracted from a bilingual parallelcorpus.
The algorithm determines the number ofphrases in the source (1 to 5-grams, at the level of173tokens, lemmas and stems) that can be mapped intotarget word sequences, and vice-versa.
To build ourEnglish-Spanish phrase table, we used the Europarl,News Commentary and United Nations Spanish-English parallel corpora.
After tokenization, theGiza++ (Och and Ney, 2000) and the Moses toolkit(Koehn et al, 2007) were respectively used toalign the corpora and extract the phrase table.Although the phrase table was generated using MTtechnology, its use to compute our features is stillcompatible with a system-independent approachsince the extraction is carried out without tuning theprocess towards any particular task.
Moreover, ourphrase matching algorithm integrates matches fromoverlapping n-grams of different size and nature(tokens, lemmas and stems) which current MTdecoding algorithms cannot explore for complexityreasons.Dependency Relation (DR) matching fea-tures target the increase of CLTE precision byadding syntactic constraints to the matching pro-cess.
These features capture similarities betweendependency relations, combining syntactic andlexical levels.
We define a dependency relationas a triple that connects pairs of words through agrammatical relation.
In a valid match, while therelation has to be the same, the connected wordscan be either the same, or semantically equivalentterms in the two languages.
For example, ?nsubj(loves, John)?
can match ?nsubj (ama, John)?and ?nsubj (quiere, John)?
but not ?dobj (quiere,John)?.
Term matching is carried out by meansof a bilingual dictionary extracted from parallelcorpora during PT creation.
Given the dependencytree representations of source and target producedwith DepPattern, for each grammatical relation r wecalculate two DR matching scores as the numberof matching occurrences of r in both source andtarget, respectively normalized by: i) the number ofoccurrences of r in the source, and ii) the number ofoccurrences of r in the target.Semantic Phrase Table (SPT) matching featuresrepresent a novel way to leverage the integration ofsemantics and MT-derived techniques.
Semanticallyenhanced phrase tables are used as a recall-orientedcomplement to the lexical PT matching features.SPTs are extracted from the same parallel corporaused to build lexical PTs, augmented with shallowsemantic labels.
To this aim, we first annotate thecorpora with the FreeLing named-entity tagger,replacing named entities with general semanticlabels chosen from a coarse-grained taxonomy(person, location, organization, date and numericexpression).
Then, we combine the sequences ofunique labels into one single token of the samelabel.
Finally, we extract the semantic phrasetable from the augmented corpora in the same waymentioned above.
The resulting SPTs are used tomap phrases between NE-annotated source-targetpairs, similar to PT matching.
SPTs offer threemain advantages: i) semantic tags allow to matchtokens that do not occur in the original parallelcorpora used to extract the phrase table, ii) SPTentries are often short generalizations of longeroriginal phrases, so the matching process canbenefit from the increased probability of mappinghigher order n-grams (i.e.
those providing morecontextual information), and iii) their smaller sizehas positive impact on system?s efficiency, due tothe considerable search space reduction.4 Experiments and results4.1 DatasetsDatasets with manual evaluation of MT output havebeen made available through a number of sharedevaluation tasks.
However, most of these datasetsare not specifically annotated for adequacy measure-ment purposes, and the available adequacy judge-ments are limited to few hundred sentences for somelanguage pairs.
Moreover, most datasets are createdby comparing reference translations with MT sys-tems?
output, disregarding the input sentences.
Suchjudgements are hence biased towards the reference.Furthermore, the inter-annotator agreement is oftenlow (Callison-Burch et al, 2007).
In light of theselimitations, most of the available datasets are per senot fully suitable for adequacy evaluation methodsbased on supervised learning, nor to provide sta-ble and meaningful results.
To partially cope withthese problems, our experiments have been carriedout over two different datasets:?
16K: 16.000 English-Spanish pairs, withSpanish translations produced by multiple MT174systems, annotated by professional translatorswith quality scores in a 4-point scale (Specia etal., 2010a).?
WMT07: 703 English-Spanish pairs derivedfrom MT systems?
output, with explicit ade-quacy judgements on a 5-point scale.The two datasets present complementary advan-tages and disadvantages.
On the one hand, al-though it is not annotated to explicitly capturemeaning-related aspects of MT output, the qualityoriented dataset has the main advantage of beinglarge enough for supervised approaches.
Moreover,it should allow to check the effectiveness of our fea-ture set in estimating adequacy as a latent aspect ofthe more general notion of MT output quality.
Onthe other hand, the smaller dataset is less suitablefor supervised learning, but represents an appropri-ate benchmark for MT adequacy evaluation.4.2 Adequacy and quality predictionTo experiment with our CLTE-based evaluationmethod minimizing overfitting, we randomized eachdataset 5 times (D1 to D5), and split them into 80%for training and 20% for testing.
Using differentfeature sets, we then trained and tested various re-gression models over each of the five splits, andcomputed correlation coefficients between the CLTEmodel predictions and the human gold standard an-notations ([1-4] for quality, and [1-5] for adequacy).16K quality-based datasetIn Table 1 we compare the Pearson?s correlationcoefficient of our SVM regression models againstthe results reported in (Specia et al, 2010b), calcu-lated with the same three common MT evaluationmetrics with a single reference: BLEU, TER andMeteor.
For the sake of comparison, we also re-port the average quality correlation (QE) obtainedby (Specia et al, 2010b) over the same dataset.1The results show that the integration of syntac-tic and semantic information allows our adequacy-oriented model to achieve a correlation with hu-man quality judgements that is always significantly1We only show the average results reported in (Specia et al,2010b), since the distributions of the 16K dataset is differentfrom our randomized distribution.higher2 than the correlation obtained by the MTevaluation metrics used for comparison.
As ex-pected a considerable improvement over surface fea-tures is achieved by the integration of syntactic in-formation.
A further increase, however, is broughtby the complementary contribution of SPT (recall-oriented, due to the higher coverage of semantics-aware phrase tables with respect to lexical PTs), andDR matching features (precision-oriented, due tothe syntactic constraints posed to matching text por-tions).
Although they are meant to capture meaning-related aspects of MT output, our features allowto outperform the results obtained by the genericquality-oriented features used by (Specia et al,2010b), which do not discriminate between ade-quacy and fluency.3 When dependency relations andphrase tables (both lexical and semantics-aware) areused in combination, our scores also outperform theaverage QE score.
Finally, looking at the differentrandom splits of the same dataset (D1 to D5), ourcorrelation scores remain substantially stable, prov-ing the robustness of our approach not only for ade-quacy, but also for quality estimation.WMT07 adequacy-based datasetIn Table 2 we compare our regression model,obtained in the same way previously described,against three commonly used MT evaluation metrics(Callison-Burch et al, 2007).
In this case, the re-ported results do not show the same consistency overthe 5 randomized datasets (D1 to D5).
However, it isworth pointing out that: i) the small dataset is partic-ularly challenging to train models with higher corre-lation with humans, ii) our aim is checking how farwe get using only adequacy-oriented features ratherthan outperforming BLEU/TER/Meteor at any cost,and iii) our results are not far from those achievedby metrics that rely on reference translations.
Com-pared with Meteor, the correlation is even higherproving the effectiveness of the proposed method.2p < 0.05, calculated using the approximate randomizationtest implemented in (Pado?, 2006).3As reported in (Specia et al, 2010b), more than 50% (39out of 74) of the features used is translation-independent (onlysource-derived features).175Features D1 D2 D3 D4 D5 AVGF 0.2506 0.2578 0.2436 0.2527 0.2443 0.25SSyn+Syn 0.4387 0.4114 0.3994 0.4114 0.3793 0.41F+SSyn+Syn 0.4215 0.4398 0.4059 0.4464 0.4255 0.428F+SSyn+Syn+DR 0.4668 0.4602 0.4386 0.4437 0.4454 0.451F+SSyn+Syn+DR+PT 0.4724 0.4715 0.4852 0.5028 0.4653 0.48F+SSyn+Syn+DR+PT+SPT 0.4967 0.4802 0.4688 0.4894 0.4887 0.485BLEU 0.2268TER 0.1938METEOR 0.2713QE (Specia et al, 2010b) 0.4792Table 1: Pearson?s correlation between SVM regression and human quality annotation over 16K dataset.Features D1 D2 D3 D4 D5 AVGF 0.10 0.03 0.04 0.10 0.14 0.083SSyn+Syn 0.299 0.351 0.1834 0.2962 0.2417 0.274F+SSyn+Syn 0.2648 0.2870 0.4061 0.3601 0.1327 0.29F+SSyn+Syn+DR 0.3196 0.4568 0.2860 0.5057 0.4066 0.395F+SSyn+Syn+DR+PT 0.3254 0.4710 0.3921 0.4599 0.3501 0.40F+SSyn+Syn+DR+PT+SPT 0.3487 0.4032 0.4803 0.4380 0.3929 0.413BLEU 0.466TER 0.437METEOR 0.357Table 2: Pearson?s correlation between SVM regression and human adequacy annotation over WMT07.4.3 Multi-class classificationTo further explore the potential of our CLTE-basedMT evaluation method, we trained an SVM multi-class classifier to predict the exact adequacy andquality scores assigned by human judges.
The eval-uation was carried out measuring the accuracy of ourmodels with 10-fold cross validation to minimizeoverfitting.
As a baseline, we calculated the per-formance of the Majority Class (MjC) classifier pro-posed in (Specia et al, 2011), which labels all exam-ples with the most frequent class among all classes.The performance improvement over the result ob-tained by the MjC baseline (?)
has been calculatedto assess the contribution of different feature sets.16K quality-based datasetThe accuracy results reported in Table 3a showthat also in this testing condition, syntactic and se-mantic features improve over surface form ones.
Be-sides that, we observe a steady improvement overthe MjC baseline (from 5% to 12%).
This demon-strates the effectiveness of our adequacy-based fea-tures to predict exact quality scores in a 4-pointscale, although this is a more challenging and dif-ficult task than regression and binary classification.Such improvement is even more interesting consid-ering that (Specia et al, 2010b) reported discour-aging results with multi-class classification to pre-dict quality scores.
Moreover, while they claimedthat removing target-independent features (i.e.
thoseonly looking at the source text) significantly de-grades their QE performance, we achieved good re-sults without using any of these features.WMT07 adequacy-based datasetAs we can observe in Table 3b, all variationsof adequacy estimation models significantly outper-form the MjC baseline, with improvements rang-176Features 10-fold acc.
?F 42.16% 5.16Syn+SSyn 46.61% 9.61F+Syn+SSyn 47.10% 10.10F+Syn+SSyn+DR 47.26% 10.26F+Syn+SSyn+DR+PT 48.15% 11.15F+Syn+SSyn+DR+PT+SPT 48.74% 11.74MjC 37% -(a) 16K dataset.Features 10-fold acc.
?F 50.07% 14.07Syn+SSyn 54.19% 18.19F+Syn+SSyn 54.34% 18.34F+Syn+SSyn+DR 56.47% 20.47F+Syn+SSyn+DR+PT 56.61% 20.61F+Syn+SSyn+DR+PT+SPT 56.75% 20.75MjC 36% -(b) WMT07 datasetTable 3: Multi-class classification accuracy of the quality/adequacy scores.Features 10-fold acc.
?F 65.85% 11.85Syn+SSyn 69.59% 15.59F+Syn+SSyn 70.89% 16.89F+Syn+SSyn+DR 71.39% 17.39F+Syn+SSyn+DR+PT 71.92% 17.92F+Syn+SSyn+DR+PT+SPT 72.21% 18.21MjC 54% -(a) 16k dataset.Features 10-fold acc.
?F 83.24% 12.84Syn+SSyn 83.67% 13.27F+Syn+SSyn 84.31% 13.91F+Syn+SSyn+DR 84.86% 14.46F+Syn+SSyn+DR+PT 84.96% 14.56F+Syn+SSyn+DR+PT+SPT 85.20% 14.80MjC 70.4% -(b) WMT07 dataset.Table 4: Accuracy of the binary classification into ?good?
or ?adequate?, and ?bad?
or ?inadequate?.ing from 14% to 20%.
Interestingly, although thedataset is small and the number of classes is higher(5-point scale), the improvement and overall resultsare better than those obtained on the 16K dataset.Such result confirms our hypothesis that adequacy-based features extracted from both source and targetperform better on a dataset explicitly annotated withadequacy judgements.
In addition, the improvementover the MjC baseline (?)
of our best model is muchhigher (20%) than the one reported in (Specia et al,2011) on adequacy estimation (6%).
We are awarethat their results are calculated over a dataset for adifferent language pair (i.e.
English-Arabic) whichbrings up more challenges.
However, our smallerdataset (700 vs 2580 pairs) and the higher numberof classes (5 vs 4) compensate to some extent thedifficulty of dealing with English-Arabic pairs.4.4 Recognizing ?good?
vs ?bad?
translationsLast but not least, we considered the traditional sce-nario for quality and confidence estimation, whichis a binary classification of translations into ?good?and ?bad?
or, from the meaning point of view, ?ade-quate?
and ?inadequate?.
Adequacy-oriented binaryclassification has many potential applications in thetranslation industry, ranging from the design of con-fidence estimation methods that reward meaning-preserving translations, to the optimization of thetranslation workflow.
For instance, an ?adequate?translation can be just post-edited in terms of fluencyby a target language native speaker, without havingany knowledge of the source language.
On the otherhand, an ?inadequate?
translation should be sent to ahuman translator or to another MT system, in orderto reach acceptable adequacy.
Effective automaticbinary classification has an evident positive impacton such workflow.16K quality-based datasetWe grouped the quality scores in the 4-point scaleinto two classes, where scores {1,2} are consideredas ?bad?
or ?inadequate?, while {3,4} are taken as?good?
or ?adequate?.
We carried out learning and177classification using different sets of features with 10-fold cross validation.
We also compared our accu-racy with the MjC baseline, and calculated the im-provement of each model (?)
against it.The results reported in Table 4a demonstrate thatthe accuracy of our models is always significantlysuperior to the MjC baseline.
Moreover, also in thiscase there is a steady improvement using syntacticand semantic features over the results obtained bysurface form features.
Additionally, it is worth men-tioning that the best model improvement over thebaseline (?)
is much higher (about 18%) than theimprovement reported in (Specia et al, 2010b) overthe same dataset (about 8%), considering the aver-age score obtained with their data distribution.
Thisconfirms the effectiveness of our CLTE approachalso in classifying ?good?
and ?bad?
translations.WMT07 adequacy-based datasetWe mapped the 5-point scale adequacy scores intotwo classes, with {1,2,3} judgements assigned to the?inadequate?
class, and {4,5} judgements assignedto the ?adequate?
class.
The main motivation for thisdistribution was to separate the examples in a waythat adequate translations are substantially accept-able, while inadequate translations present evidentmeaning discrepancies with the source.The results reported in Table 4b show that theaccuracy of the binary classifiers to distinguish be-tween ?adequate?
and ?inadequate?
classes was sig-nificantly superior (up to about 15%) to the MjCbaseline.
We also notice that surface form fea-tures have a significant contribution to deal with theadequacy-oriented dataset, while the gain obtainedusing syntactic and semantic features (2%) is lowerthan the improvement observed in the 16K dataset.This might be due to the more unbalanced distribu-tion of the classes which: i) leads to a high baseline,and ii) together with the small size of the WMT07dataset, makes supervised learning more challeng-ing.
Finally, the improvement of all models (?)
overthe MjC baseline is much higher than the gain re-ported in (Specia et al, 2011) over their adequacy-oriented dataset (around 2%).5 ConclusionsIn the effort of integrating semantics into MT tech-nology, we focused on automatic MT evaluation, in-vestigating the potential of applying cross-lingualtextual entailment techniques for adequacy assess-ment.
The underlying assumption is that MT outputadequacy can be determined by verifying that an en-tailment relation holds from the source to the target,and vice-versa.
Within such framework, this papermakes two main contributions.First, in contrast with most current metrics basedon the comparison between automatic translationsand multiple references, we avoid the bottleneckrepresented by the manual creation of such refer-ences.Second, beyond current approaches biased to-wards fluency or general quality judgements, wetried to isolate the adequacy dimension of the prob-lem, exploring the potential of adequacy-orientedfeatures extracted from the observation of sourceand target.To achieve our objectives, we successfully ex-tended previous CLTE methods with a variety of lin-guistically motivated features.
Altogether, such fea-tures led to reliable judgements that show high cor-relation with human evaluation.
Coherent results ondifferent datasets and classification schemes demon-strate the effectiveness of the approach and its poten-tial for different applications.Future works will address both the improvementof our adequacy evaluation method and its integra-tion in SMT for optimization purposes.
On onehand, we plan to explore new features capturingother semantic dimensions.
A possible direction isto consider topic modelling techniques to measurethe relatedness of source and target.
Another inter-esting direction is to investigate the use of Wikipediaentity linking tools to support the mapping betweensource and target terms.
On the other hand, we planto explore the integration of our model as an errorcriterion in SMT system training.AcknowledgmentsThis work has been partially supported by theCoSyne project (FP7-ICT-4-24853) and T4ME net-work of excellence (FP7-IST-249119), funded bythe European Commission under the 7th Frame-work Programme.
The authors would like to thankHanna Bechara, Antonio Valerio Miceli Barone andDaniele Pighin for their contributions during the MTMarathon 2011.178ReferencesE.
Avramidis, M. Popovic, V. Vilar Torres, and A. Bur-chardt.
2011.
Evaluate with Confidence Estimation:Machine Ranking of Translation Outputs using Gram-matical Features.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation (WMT ?11).N.
Bach, F. Huang, and Y. Al-Onaizan.
2011.
Good-ness: A Method for Measuring Machine TranslationConfidence.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics(ACL 2011).S.
Banerjee and A. Lavie.
2005.
METEOR: An Auto-matic Metric for MT Evaluation with Improved Corre-lation with Human Judgments.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summariza-tion.J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,A.
Kulesza, A. Sanchis, and N. Ueffing.
2004.
Con-fidence Estimation for Machine Translation.
In Pro-ceedings of the 20th international conference on Com-putational Linguistics (COLING ?04).
Association forComputational Linguistics.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(Meta-) Evaluation of MachineTranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation (WMT ?07).X.
Carreras, I. Chao, L.
Padro?, and M. Padro?.
2004.FreeLing: An Open-Source Suite of Language An-alyzers.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC?04).C.C.
Chang and C.J.
Lin.
2011.
LIBSVM: A Libraryfor Support Vector Machines.
ACM Transactions onIntelligent Systems and Technology (TIST), 2(3).I.
Dagan and O. Glickman.
2004.
Probabilistic TextualEntailment: Generic Applied Modeling of LanguageVariability.
In Proceedings of the PASCAL Workshopof Learning Methods for Text Understanding and Min-ing.G.
Doddington.
2002.
Automatic Evaluation of MachineTranslation Quality Using N-gram Co-OccurrenceStatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, HLT ?02.J.
Gime?nez and L. Ma`rquez.
2007.
Linguistic Featuresfor Automatic Evaluation of Heterogenous MT Sys-tems.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation (StatMT ?07).P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In Proceedings of the45th Annual Meeting of the ACL on Interactive Posterand Demonstration Sessions (ACL 2007).Y.
Mehdad, M. Negri, and M. Federico.
2010.
TowardsCross-Lingual Textual Entailment.
In Proceedings ofthe 11th Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics (NAACL HLT 2010).Y.
Mehdad, M. Negri, and M. Federico.
2011.
UsingBilingual Parallel Corpora for Cross-Lingual TextualEntailment.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies (ACL HLT 2011).Y.
Mehdad, M. Negri, and M. Federico.
2012.
Detect-ing Semantic Equivalence and Information Disparityin Cross-lingual Documents.
In Proceedings of theACL?12.F.J.
Och and H. Ney.
2000.
Improved Statistical Align-ment Models.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Linguis-tics (ACL 2000).P.G.
Otero and I.G.
Lopez.
2011.
A Grammatical For-malism Based on Patterns of Part-of-Speech Tags.
In-ternational journal of corpus linguistics, 16(1).S.
Pado?, M. Galley, D. Jurafsky, and C. D. Manning.2009.
Textual Entailment Features for Machine Trans-lation Evaluation.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation (StatMT ?09).S.
Pado?, 2006.
User?s guide to sigf: Significance test-ing by approximate randomisation.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation (ACL 2002.
In Proceedings of the40th annual meeting on association for computationallinguistics.C.B.
Quirk.
2004.
Training a Sentence-Level MachineTranslation Confidence Measure.
In Proceedings ofLREC 2004.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation Edit Ratewith Targeted Human Annotation.
In Proceedings ofAssociation for Machine Translation in the Americas(AMTA 2006).L.
Specia and A. Farzindar.
2010.
Estimating MachineTranslation Post-Editing Effort with HTER.
In Pro-ceedings of the AMTA-2010 Workshop, Bringing MTto the User: MT Research and the Translation Indus-try.L.
Specia, N. Cancedda, and M. Dymetman.
2010a.A Dataset for Assessing Machine Translation Eval-uation Metrics.
In Proceedings of the 7th interna-tional conference on Language Resources and Eval-uation (LREC10).179L.
Specia, D. Raj, and M. Turchi.
2010b.
Machine Trans-lation Evaluation Versus Quality Estimation.
Machinetranslation, 24(1).L.
Specia, N. Hajlaoui, C. Hallett, and W. Aziz.
2011.Predicting Machine Translation Adequacy.
In Pro-ceedings of the 13th Machine Translation Summit (MT-Summit 2011).L.
Specia.
2011.
Exploiting Objective Annotations forMinimising Translation Post-editing Effort.
In Pro-ceedings of the 15th Conference of the European As-sociation for Machine Translation (EAMT 2011).D.
Xiong, M. Zhang, and H. Li.
2010.
Error Detectionfor Statistical Machine Translation Using LinguisticFeatures.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics (ACL2010).
Association for Computational Linguistics.180
