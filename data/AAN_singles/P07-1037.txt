Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288?295,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSupertagged Phrase-Based Statistical Machine TranslationHany HassanSchool of Computing,Dublin City University,Dublin 9, Irelandhhasan@computing.dcu.ieKhalil Sima?anLanguage and Computation,University of Amsterdam,Amsterdam, The Netherlandssimaan@science.uva.nlAndy WaySchool of Computing,Dublin City University,Dublin 9, Irelandaway@computing.dcu.ieAbstractUntil quite recently, extending Phrase-basedStatistical Machine Translation (PBSMT)with syntactic structure caused system per-formance to deteriorate.
In this work weshow that incorporating lexical syntactic de-scriptions in the form of supertags can yieldsignificantly better PBSMT systems.
We de-scribe a novel PBSMT model that integratessupertags into the target language modeland the target side of the translation model.Two kinds of supertags are employed: thosefrom Lexicalized Tree-Adjoining Grammarand Combinatory Categorial Grammar.
De-spite the differences between these two ap-proaches, the supertaggers give similar im-provements.
In addition to supertagging, wealso explore the utility of a surface globalgrammaticality measure based on combina-tory operators.
We perform various experi-ments on the Arabic to English NIST 2005test set addressing issues such as sparseness,scalability and the utility of system subcom-ponents.
Our best result (0.4688 BLEU)improves by 6.1% relative to a state-of-the-art PBSMT model, which compares veryfavourably with the leading systems on theNIST 2005 task.1 IntroductionWithin the field of Machine Translation, by far themost dominant paradigm is Phrase-based StatisticalMachine Translation (PBSMT) (Koehn et al, 2003;Tillmann &Xia, 2003).
However, unlike in rule- andexample-based MT, it has proven difficult to date toincorporate linguistic, syntactic knowledge in orderto improve translation quality.
Only quite recentlyhave (Chiang, 2005) and (Marcu et al, 2006) shownthat incorporating some form of syntactic structurecould show improvements over a baseline PBSMTsystem.
While (Chiang, 2005) avails of structurewhich is not linguistically motivated, (Marcu et al,2006) employ syntactic structure to enrich the en-tries in the phrase table.In this paper we explore a novel approach towardsextending a standard PBSMT system with syntacticdescriptions: we inject lexical descriptions into boththe target side of the phrase translation table and thetarget language model.
Crucially, the kind of lexicaldescriptions that we employ are those that are com-monly devised within lexicon-driven approaches tolinguistic syntax, e.g.
Lexicalized Tree-AdjoiningGrammar (Joshi & Schabes, 1992; Bangalore &Joshi, 1999) and Combinary Categorial Grammar(Steedman, 2000).
In these linguistic approaches, itis assumed that the grammar consists of a very richlexicon and a tiny, impoverished1 set of combina-tory operators that assemble lexical entries togetherinto parse-trees.
The lexical entries consist of syn-tactic constructs (?supertags?)
that describe informa-tion such as the POS tag of the word, its subcatego-rization information and the hierarchy of phrase cat-egories that the word projects upwards.
In this workwe employ the lexical entries but exchange the al-gebraic combinatory operators with the more robust1These operators neither carry nor presuppose further lin-guistic knowledge beyond what the lexicon contains.288and efficient supertagging approach: like standardtaggers, supertaggers employ probabilities based onlocal context and can be implemented using finitestate technology, e.g.
Hidden Markov Models (Ban-galore & Joshi, 1999).There are currently two supertagging approachesavailable: LTAG-based (Bangalore & Joshi, 1999)and CCG-based (Clark & Curran, 2004).
Both theLTAG (Chen et al, 2006) and the CCG supertagsets (Hockenmaier, 2003) were acquired from theWSJ section of the Penn-II Treebank using hand-built extraction rules.
Here we test both the LTAGand CCG supertaggers.
We interpolate (log-linearly)the supertagged components (language model andphrase table) with the components of a standardPBSMT system.
Our experiments on the Arabic?English NIST 2005 test suite show that each of thesupertagged systems significantly improves over thebaseline PBSMT system.
Interestingly, combiningthe two taggers together diminishes the benefits ofsupertagging seen with the individual LTAG andCCG systems.
In this paper we discuss these andother empirical issues.The remainder of the paper is organised as fol-lows: in section 2 we discuss the related work on en-riching PBSMT with syntactic structure.
In section3, we describe the baseline PBSMT system whichour work extends.
In section 4, we detail our ap-proach.
Section 5 describes the experiments carriedout, together with the results obtained.
Section 6concludes, and provides avenues for further work.2 Related WorkUntil very recently, the experience with adding syn-tax to PBSMT systems was negative.
For example,(Koehn et al, 2003) demonstrated that adding syn-tax actually harmed the quality of their SMT system.Among the first to demonstrate improvement whenadding recursive structure was (Chiang, 2005), whoallows for hierarchical phrase probabilities that han-dle a range of reordering phenomena in the correctfashion.
Chiang?s derived grammar does not rely onany linguistic annotations or assumptions, so that the?syntax?
induced is not linguistically motivated.Coming right up to date, (Marcu et al, 2006)demonstrate that ?syntactified?
target languagephrases can improve translation quality for Chinese?English.
They employ a stochastic, top-down trans-duction process that assigns a joint probability toa source sentence and each of its alternative trans-lations when rewriting the target parse-tree into asource sentence.
The rewriting/transduction processis driven by ?xRS rules?, each consisting of a pairof a source phrase and a (possibly only partially)lexicalized syntactified target phrase.
In order toextract xRS rules, the word-to-word alignment in-duced from the parallel training corpus is used toguide heuristic tree ?cutting?
criteria.While the research of (Marcu et al, 2006) hasmuch in common with the approach proposed here(such as the syntactified target phrases), there re-main a number of significant differences.
Firstly,rather than induce millions of xRS rules from par-allel data, we extract phrase pairs in the standardway (Och & Ney, 2003) and associate with eachphrase-pair a set of target language syntactic struc-tures based on supertag sequences.
Relative to usingarbitrary parse-chunks, the power of supertags liesin the fact that they are, syntactically speaking, richlexical descriptions.
A supertag can be assigned toevery word in a phrase.
On the one hand, the cor-rect sequence of supertags could be assembled to-gether, using only impoverished combinatory opera-tors, into a small set of constituents/parses (?almost?a parse).
On the other hand, because supertags arelexical entries, they facilitate robust syntactic pro-cessing (using Markov models, for instance) whichdoes not necessarily aim at building a fully con-nected graph.A second major difference with xRS rules is thatour supertag-enriched target phrases need not begeneralized into (xRS or any other) rules that workwith abstract categories.
Finally, like POS tagging,supertagging is more efficient than actual parsing ortree transduction.3 Baseline Phrase-Based SMT SystemWe present the baseline PBSMT model which weextend with supertags in the next section.
Ourbaseline PBSMT model uses GIZA++2 to obtainword-level alignments in both language directions.The bidirectional word alignment is used to obtainphrase translation pairs using heuristics presented in2http://www.fjoch.com/GIZA++.html289(Och & Ney, 2003) and (Koehn et al, 2003), and theMoses decoder was used for phrase extraction anddecoding.3Let t and s be the target and source languagesentences respectively.
Any (target or source) sen-tence x will consist of two parts: a bag of elements(words/phrases etc.)
and an order over that bag.
Inother words, x = ?
?x, Ox?, where ?x stands for thebag of phrases that constitute x, andOx for the orderof the phrases as given in x (Ox can be implementedas a function from a bag of tokens ?x to a set with afinite number of positions).
Hence, we may separateorder from content:argmaxtP (t|s) = argmaxtP (s | t)P (t) (1)= arg max??t,Ot?TM?
??
?P (?s | ?t)distortion?
??
?P (Os | Ot)LM?
??
?Pw(t) (2)Here, Pw(t) is the target language model, P (Os|Ot)represents the conditional (order) linear distortionprobability, and P (?s|?t) stands for a probabilis-tic translation model from target language bags ofphrases to source language bags of phrases using aphrase translation table.
As commonly done in PB-SMT, we interpolate these models log-linearly (us-ing different ?weights) together with a word penaltyweight which allows for control over the length ofthe target sentence t:arg max?
?t,Ot?P (?s | ?t) P (Os | Ot)?oPw(t)?lm exp|t|?wFor convenience of notation, the interpolation factorfor the bag of phrases translation model is shown informula (3) at the phrase level (but that does not en-tail any difference).
For a bag of phrases ?t consist-ing of phrases ti, and bag ?s consisting of phrasessi, the phrase translation model is given by:P (?s | ?t) =YsitiP (si|ti)P (si| ti) = Pph(si|ti)?t1Pw(si|ti)?t2Pr(ti|si)?t3 (3)where Pph and Pr are the phrase-translation proba-bility and its reverse probability, and Pw is the lexi-cal translation probability.3http://www.statmt.org/moses/4 Our Approach: Supertagged PBSMTWe extend the baseline model with lexical linguis-tic representations (supertags) both in the languagemodel as well as in the phrase translation model.
Be-fore we describe how our model extends the base-line, we shortly review the supertagging approachesin Lexicalized Tree-Adjoining Grammar and Com-binatory Categorial Grammar.4.1 Supertags: Lexical SyntaxNPDTheNPNPNpurchaseNPNPNpriceSNP VPVincludesNPNPNtaxesFigure 1: An LTAG supertag sequence for the sen-tence The purchase price includes taxes.
The sub-categorization information is most clearly availablein the verb includes which takes a subject NP to itsleft and an object NP to its right.Modern linguistic theory proposes that a syntacticparser has access to an extensive lexicon of word-structure pairs and a small, impoverished set of oper-ations to manipulate and combine the lexical entriesinto parses.
Examples of formal instantiations of thisidea include CCG and LTAG.
The lexical entries aresyntactic constructs (graphs) that specify informa-tion such as POS tag, subcategorization/dependencyinformation and other syntactic constraints at thelevel of agreement features.
One important way ofportraying such lexical descriptions is via the su-pertags devised in the LTAG and CCG frameworks(Bangalore & Joshi, 1999; Clark & Curran, 2004).A supertag (see Figure 1) represents a complex,linguistic word category that encodes a syntacticstructure expressing a specific local behaviour of aword, in terms of the arguments it takes (e.g.
sub-ject, object) and the syntactic environment in whichit appears.
In fact, in LTAG a supertag is an elemen-tary tree and in CCG it is a CCG lexical category.Both descriptions can be viewed as closely relatedfunctional descriptions.The term ?supertagging?
(Bangalore & Joshi,1999) refers to tagging the words of a sentence, each290with a supertag.
When well-formed, an ordered se-quence of supertags can be viewed as a compactrepresentation of a small set of constituents/parsesthat can be obtained by assembling the supertagstogether using the appropriate combinatory opera-tors (such as substitution and adjunction in LTAGor function application and combination in CCG).Akin to POS tagging, the process of supertaggingan input utterance proceeds with statistics that arebased on the probability of a word-supertag pairgiven their Markovian or local context (Bangalore& Joshi, 1999; Clark & Curran, 2004).
This is themain difference with full parsing: supertagging theinput utterance need not result in a fully connectedgraph.The LTAG-based supertagger of (Bangalore &Joshi, 1999) is a standard HMM tagger and consistsof a (second-order) Markov language model over su-pertags and a lexical model conditioning the proba-bility of every word on its own supertag (just likestandard HMM-based POS taggers).The CCG supertagger (Clark & Curran, 2004) isbased on log-linear probabilities that condition a su-pertag on features representing its context.
The CCGsupertagger does not constitute a language modelnor are the Maximum Entropy estimates directly in-terpretable as such.
In our model we employ theCCG supertagger to obtain the best sequences of su-pertags for a corpus of sentences from which we ob-tain language model statistics.
Besides the differ-ence in probabilities and statistical estimates, thesetwo supertaggers differ in the way the supertags areextracted from the Penn Treebank, cf.
(Hocken-maier, 2003; Chen et al, 2006).
Both supertaggersachieve a supertagging accuracy of 90?92%.Three aspects make supertags attractive in thecontext of SMT.
Firstly, supertags are rich syntac-tic constructs that exist for individual words and sothey are easy to integrate into SMT models that canbe based on any level of granularity, be it word-or phrase-based.
Secondly, supertags specify thelocal syntactic constraints for a word, which res-onates well with sequential (finite state) statistical(e.g.
Markov) models.
Finally, because supertagsare rich lexical descriptions that represent under-specification in parsing, it is possible to have someof the benefits of full parsing without imposing thestrict connectedness requirements that it demands.4.2 A Supertag-Based SMT modelWe employ the aforementioned supertaggers to en-rich the English side of the parallel training cor-pus with a single supertag sequence per sentence.Then we extract phrase-pairs together with the co-occuring English supertag sequence from this cor-pus via the same phrase extraction method used inthe baseline model.
This way we directly extendthe baseline model described in section 3 with su-pertags both in the phrase translation table and inthe language model.
Next we define the probabilisticmodel that accompanies this syntactic enrichment ofthe baseline model.Let ST represent a supertag sequence of the samelength as a target sentence t. Equation (2) changesas follows:argmaxt?STP (s | t, ST )PST (t, ST ) ?arg max?t,ST ?TM w.sup.tags?
??
?P (?s | ?t,ST )distortion?
??
?P (Os | Ot)?oLM w.sup.tags?
??
?PST (t, ST )word?penalty?
??
?exp|t|?wThe approximations made in this formula are of twokinds: the standard split into components and thesearch for the most likely joint probability of a tar-get hypothesis and a supertag sequence cooccuringwith the source sentence (a kind of Viterbi approachto avoid the complex optimization involving the sumover supertag sequences).
The distortion and wordpenalty models are the same as those used in thebaseline PBSMT model.Supertagged Language Model The ?languagemodel?
PST (t, ST ) is a supertagger assigning prob-abilities to sequences of word?supertag pairs.
Thelanguage model is further smoothed by log-linearinterpolation with the baseline language model overword sequences.Supertags in Phrase Tables The supertaggedphrase translation probability consists of a combina-tion of supertagged components analogous to theircounterparts in the baseline model (equation (3)),i.e.
it consists of P (s | t, ST ), its reverse anda word-level probability.
We smooth this proba-bility by log-linear interpolation with the factored291John bought quickly sharesNNP_NN VBD_(S[dcl]\NP)/NP RB|(S\NP)\(S\NP) NNS_N2 ViolationsFigure 2: Example CCG operator violations: V = 2and L = 3, and so the penalty factor is 1/3.backoff version P (s | t)P (s | ST ), where we im-port the baseline phrase table probability and ex-ploit the probability of a source phrase given the tar-get supertag sequence.
A model in which we omitP (s | ST ) turns out to be slightly less optimal thanthis one.As in most state-of-the-art PBSMT systems, weuse GIZA++ to obtain word-level alignments in bothlanguage directions.
The bidirectional word align-ment is used to obtain lexical phrase translation pairsusing heuristics presented in (Och & Ney, 2003) and(Koehn et al, 2003).
Given the collected phrasepairs, we estimate the phrase translation probabilitydistribution by relative frequency as follows:P?ph(s|t) =count(s, t)?s count(s, t)For each extracted lexical phrase pair, we extract thecorresponding supertagged phrase pairs from the su-pertagged target sequence in the training corpus (cf.section 5).
For each lexical phrase pair, there isat least one corresponding supertagged phrase pair.The probability of the supertagged phrase pair is es-timated by relative frequency as follows:Pst(s|t, st) =count(s, t, st)?s count(s, t, st)4.3 LMs with a Grammaticality FactorThe supertags usually encode dependency informa-tion that could be used to construct an ?almost parse?with the help of the CCG/LTAG composition oper-ators.
The n-gram language model over supertagsapplies a kind of statistical ?compositionality check?but due to smoothing effects this could mask cru-cial violations of the compositionality operators ofthe grammar formalism (CCG in this case).
It isinteresting to observe the effect of integrating intothe language model a penalty imposed when formalcompostion operators are violated.
We combine then-gram language model with a penalty factor thatmeasures the number of encountered combinatoryoperator violations in a sequence of supertags (cf.Figure 2).
For a supertag sequence of length (L)which has (V ) operator violations (as measured bythe CCG system), the language model P will be ad-justed as P?
= P ?
(1 ?
VL ).
This is of course nolonger a simple smoothed maximum-likelihood es-timate nor is it a true probability.
Nevertheless, thismechanism provides a simple, efficient integrationof a global compositionality (grammaticality) mea-sure into the n-gram language model over supertags.Decoder The decoder used in this work is Moses,a log-linear decoder similar to Pharaoh (Koehn,2004), modified to accommodate supertag phraseprobabilities and supertag language models.5 ExperimentsIn this section we present a number of experimentsthat demonstrate the effect of lexical syntax on trans-lation quality.
We carried out experiments on theNIST open domain news translation task from Ara-bic into English.
We performed a number of ex-periments to examine the effect of supertagging ap-proaches (CCG or LTAG) with varying data sizes.Data and Settings The experiments were con-ducted for Arabic to English translation and testedon the NIST 2005 evaluation set.
The systems weretrained on the LDC Arabic?English parallel corpus;we use the news part (130K sentences, about 5 mil-lion words) to train systems with what we call thesmall data set, and the news and a large part ofthe UN data (2 million sentences, about 50 millionwords) for experiments with large data sets.The n-gram target language model was built us-ing 250M words from the English GigaWord Cor-pus using the SRILM toolkit.4 Taking 10% of theEnglish GigaWord Corpus used for building our tar-get language model, the supertag-based target lan-guage models were built from 25M words that weresupertagged.
For the LTAG supertags experiments,we used the LTAG English supertagger5 (Bangalore4http://www.speech.sri.com/projects/srilm/5http://www.cis.upenn.edu/?xtag/gramrelease.html292& Joshi, 1999) to tag the English part of the paralleldata and the supertag language model data.
For theCCG supertag experiments, we used the CCG su-pertagger of (Clark & Curran, 2004) and the Edin-burgh CCG tools6 to tag the English part of the par-allel corpus as well as the CCG supertag languagemodel data.The NIST MT03 test set is used for development,particularly for optimizing the interpolation weightsusing Minimum Error Rate training (Och, 2003).Baseline System The baseline system is a state-of-the-art PBSMT system as described in sec-tion 3.
We built two baseline systems with twodifferent-sized training sets: ?Base-SMALL?
(5 mil-lion words) and ?Base-LARGE?
(50 million words)as described above.
Both systems use a trigram lan-guage model built using 250 million words fromthe English GigaWord Corpus.
Table 1 presents theBLEU scores (Papineni et al, 2002) of both systemson the NIST 2005 MT Evaluation test set.System BLEU ScoreBase-SMALL 0.4008Base-LARGE 0.4418Table 1: Baseline systems?
BLEU scores5.1 Baseline vs. Supertags on Small Data SetsWe compared the translation quality of the baselinesystems with the LTAG and CCG supertags systems(LTAG-SMALL and CCG-SMALL).
The results areSystem BLEU ScoreBase-SMALL 0.4008LTAG-SMALL 0.4205CCG-SMALL 0.4174Table 2: LTAG and CCG systems on small datagiven in Table 2.
All systems were trained on thesame parallel data.
The LTAG supertag-based sys-tem outperforms the baseline by 1.97 BLEU pointsabsolute (or 4.9% relative), while the CCG supertag-based system scores 1.66 BLEU points over the6http://groups.inf.ed.ac.uk/ccg/software.htmlbaseline (4.1% relative).
These significant improve-ments indicate that the rich information in supertagshelps select better translation candidates.POS Tags vs. Supertags A supertag is a complextag that localizes the dependency and the syntax in-formation from the context, whereas a normal POStag just describes the general syntactic category ofthe word without further constraints.
In this experi-ment we compared the effect of using supertags andPOS tags on translation quality.
As can be seenSystem BLEU ScoreBase-SMALL 0.4008POS-SMALL 0.4073LTAG-SMALL .0.4205Table 3: Comparing the effect of supertags and POStagsin Table 3, while the POS tags help (0.65 BLEUpoints, or 1.7% relative increase over the baseline),they clearly underperform compared to the supertagmodel (by 3.2%).The Usefulness of a Supertagged LM In theseexperiments we study the effect of the two addedfeature (cost) functions: supertagged translation andlanguage models.
We compare the baseline systemto the supertags system with the supertag phrase-table probability but without the supertag LM.
Ta-ble 4 lists the baseline system (Base-SMALL), theLTAG system without supertagged language model(LTAG-TM-ONLY) and the LTAG-SMALL sys-tem with both supertagged translation and languagemodels.
The results presented in Table 4 indi-System BLEU ScoreBase-SMALL 0.4008LTAG-TM-ONLY 0.4146LTAG-SMALL .0.4205Table 4: The effect of supertagged componentscate that the improvement is a shared contributionbetween the supertagged translation and languagemodels: adding the LTAG TM improves BLEUscore by 1.38 points (3.4% relative) over the base-line, with the LTAG LM improving BLEU score by293a further 0.59 points (a further 1.4% increase).5.2 Scalability: Larger Training CorporaOutperforming a PBSMT system on small amountsof training data is less impressive than doing so onreally large sets.
The issue here is scalability as wellas whether the PBSMT system is able to bridge theperformance gap with the supertagged system whenreasonably large sizes of training data are used.
Tothis end, we trained the systems on 2 million sen-tences of parallel data, deploying LTAG supertagsand CCG supertags.
Table 5 presents the compari-son between these systems and the baseline trainedon the same data.
The LTAG system improves by1.17 BLEU points (2.6% relative), but the CCG sys-tem gives an even larger increase: 1.91 BLEU points(4.3% relative).
While this is slightly lower thanthe 4.9% relative improvement with the smaller datasets, the sustained increase is probably due to ob-serving more data with different supertag contexts,which enables the model to select better target lan-guage phrases.System BLEU ScoreBase-LARGE 0.4418LTAG-LARGE 0.4535CCG-LARGE 0.4609Table 5: The effect of more training dataAdding a grammaticality factor As described insection 4.3, we integrate an impoverished grammat-icality factor based on two standard CCG combi-nation operations, namely Forward and BackwardApplication.
Table 6 compares the results of thebaseline, the CCG with an n-gram LM-only system(CCG-LARGE) and CCG-LARGE with this ?gram-maticalized?
LM system (CCG-LARGE-GRAM).We see that bringing the grammaticality tests tobear onto the supertagged system gives a further im-provement of 0.79 BLEU points, a 1.7% relativeincrease, culminating in an overall increase of 2.7BLEU points, or a 6.1% relative improvement overthe baseline system.5.3 DiscussionA natural question to ask is whether LTAG and CCGsupertags are playing similar (overlapping, or con-System BLEU ScoreBase-LARGE 0.4418CCG-LARGE 0.4609CCG-LARGE-GRAM 0.4688Table 6: Comparing the effect of CCG-GRAMflicting) roles in practice.
Using an oracle to choosethe best output of the two systems gives a BLEUscore of 0.441, indicating that the combination pro-vides significant room for improvement (cf.
Ta-ble 2).
However, our efforts to build a system thatbenefits from the combination using a simple log-linear combination of the two models did not giveany significant performance change relative to thebaseline CCG system.
Obviously, more informedways of combining the two could result in better per-formance than a simple log-linear interpolation ofthe components.Figure 3 shows some example system output.While the baseline system omits the verb giving ?theauthorities that it had...?, both the LTAG and CCGfound a formulation ?authorities reported that?
witha closer meaning to the reference translation ?Theauthorities said that?.
Omitting verbs turns out tobe a problem for the baseline system when trans-lating the notorious verbless Arabic sentences (seeFigure 4).
The supertagged systems have a moregrammatically strict language model than a standardword-level Markov model, thereby exhibiting a pref-erence (in the CCG system especially) for the inser-tion of a verb with a similar meaning to that con-tained in the reference sentence.6 ConclusionsSMT practitioners have on the whole found it dif-ficult to integrate syntax into their systems.
In thiswork, we have presented a novel model of PBSMTwhich integrates supertags into the target languagemodel and the target side of the translation model.Using LTAG supertags gives the best improve-ment over a state-of-the-art PBSMT system for asmaller data set, while CCG supertags work best ona large 2 million-sentence pair training set.
Addinggrammaticality factors based on algebraic composi-tional operators gives the best result, namely 0.4688BLEU, or a 6.1% relative increase over the baseline.294Reference: The authorities said he was allowed to contact family members by phone from the armored vehicle he was in.Baseline: the authorities that it had allowed him to communicate by phone with his family of the armored car whereLTAG: authorities reported that it had allowed him to contact by telephone with his family of armored car whereCCG: authorities reported that it had enabled him to communicate by phone his family members of the armored car whereFigure 3: Sample output from different systemsSource: wmn AlmErwf An Al$Eb AlSyny mHb llslAm .
Ref: It is well known that the Chinese people are peace loving .Baseline: It is known that the Chinese people a peace-loving .LTAG: It is known that the Chinese people a peace loving .
CCG: It is known that the Chinese people are peace loving .Figure 4: Verbless Arabic sentence and sample output from different systemsThis result compares favourably with the best sys-tems on the NIST 2005 Arabic?English task.
Weexpect more work on system integration to improveresults still further, and anticipate that similar in-creases are to be seen for other language pairs.AcknowledgementsWe would like to thank Srinivas Bangalore andthe anonymous reviewers for useful comments onearlier versions of this paper.
This work is par-tially funded by Science Foundation Ireland Princi-pal Investigator Award 05/IN/1732, and NetherlandsOrganization for Scientific Research (NWO) VIDIAward.ReferencesS.
Bangalore and A. Joshi, ?Supertagging: An Ap-proach to Almost Parsing?, Computational Linguistics25(2):237?265, 1999.J.
Chen, S. Bangalore, and K. Vijay-Shanker, ?Au-tomated extraction of tree-adjoining grammarsfrom treebanks?.
Natural Language Engineering,12(3):251?299, 2006.D.
Chiang, ?A Hierarchical Phrase-Based Model for Sta-tistical Machine Translation?, in Proceedings of ACL2005, Ann Arbor, MI., pp.263?270, 2005.S.
Clark and J. Curran, ?The Importance of Supertaggingfor Wide-Coverage CCG Parsing?, in Proceedings ofCOLING-04, Geneva, Switzerland, pp.282?288, 2004.J.
Hockenmaier, Data and Models for Statistical Parsingwith Combinatory Categorial Grammar, PhD thesis,University of Edinburgh, UK, 2003.A.
Joshi and Y. Schabes, ?Tree Adjoining Grammars andLexicalized Grammars?
in M. Nivat and A.
Podelski(eds.)
Tree Automata and Languages, Amsterdam, TheNetherlands: North-Holland, pp.409?431, 1992.P.
Koehn, ?Pharaoh: A Beam Search Decoder for phrase-based Statistical Machine TranslationModels?, in Pro-ceedings of AMTA-04, Berlin/Heidelberg, Germany:Springer Verlag, pp.115?124, 2004.P.
Koehn, F. Och, and D. Marcu, ?Statistical Phrase-Based Translation?, in Proceedings of HLT-NAACL2003, Edmonton, Canada, pp.127?133, 2003.D.
Marcu, W. Wang, A. Echihabi and K. Knight, ?SPMT:Statistical Machine Translation with Syntactified Tar-get Language Phrases?, in Proceedings of EMNLP,Sydney, Australia, pp.44?52, 2006.D.
Marcu andW.Wong, ?A Phrase-Based, Joint Probabil-ity Model for Statistical Machine Translation?, in Pro-ceedings of EMNLP, Philadelphia, PA., pp.133?139,2002.F.
Och, ?Minimum Error Rate Training in Statistical Ma-chine Translation?, in Proceedings of ACL 2003, Sap-poro, Japan, pp.160?167, 2003.F.
Och and H. Ney, ?A Systematic Comparison of Var-ious Statistical Alignment Models?, ComputationalLinguistics 29:19?51, 2003.K.
Papineni, S. Roukos, T. Ward and W-J.
Zhu, ?BLEU:A Method for Automatic Evaluation of MachineTranslation?, in Proceedings of ACL 2002, Philadel-phia, PA., pp.311?318, 2002.L.
Rabiner, ?A Tutorial on Hidden Markov Models andSelected Applications in Speech Recognition?, in A.Waibel & F-K. Lee (eds.)
Readings in Speech Recog-nition, San Mateo, CA.
: Morgan Kaufmann, pp.267?296, 1990.M.
Steedman, The Syntactic Process.
Cambridge, MA:The MIT Press, 2000.C.
Tillmann and F. Xia, ?A Phrase-based Unigram Modelfor Statistical Machine Translation?, in Proceedings ofHLT-NAACL 2003, Edmonton, Canada.
pp.106?108,2003.295
