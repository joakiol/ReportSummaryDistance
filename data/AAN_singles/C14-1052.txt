Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 541?552, Dublin, Ireland, August 23-29 2014.An LR-inspired generalized lexicalized phrase structure parserBenoit Crabb?ALPAGE / INRIA - Universit?
Paris DiderotPlace Paul RicoeurF-75013 Parisbenoit.crabbe@univ-paris-diderot.frAbstractThe paper introduces an LR-based algorithm for efficient phrase structure parsing of morpholog-ically rich languages.
The algorithm generalizes lexicalized parsing (Collins, 2003) by allowinga structured representation of the lexical items.
Together with a discriminative weighting com-ponent (Collins, 2002), we show that this representation allows us to achieve state of the artaccurracy results on a morphologically rich language such as French while achieving more effi-cient parsing times than the state of the art parsers on the French data set.
A comparison withEnglish, a lexically poor language, is also provided.1 IntroductionThe paper provides a phrase structure parsing algorithm inspired by LR (Knuth, 1965), GLR (Tomita,1988) and the recent developments of (Huang and Sagae, 2010) for dependency grammar.
The parsingalgorithm comes with a discriminative weighting framework inspired by (Collins, 2002).
Although dis-criminative phrase structure parsing has been shown to be challenging when it comes to efficiency issues(Turian and Melamed, 2006; Finkel et al., 2008), we use here several approximations that make theframework not only tractable but also efficient and accurate on a lexically rich language such as French.Despite the successes of dependency grammar, we are interested in phrase structure grammar sinceit naturally allows to support compositional semantic representations as recently highlighted by (Socheret al., 2012).
It remains that most phrase structure parsers have been designed in priority for modellinglexically poor languages such as English or Chinese (Collins, 2003; Charniak, 2000; Zhu et al., 2013).Although highly accurate multilingual parsers exist (Petrov et al., 2006), they remain relatively bothslow for wide coverage purposes and their inner formal structure is not designed to handle naturallymorphological information.We assume that parsing lexically rich languages benefits from taking into account the structured mor-phological information that can be extracted from lexical forms.
Using French as a case study we showthat we can reach both parsing efficiency with an approximative inference method and we can get a stateof the art accurracy by generalizing lexicalized parsing to handle feature structure-based word represen-tations.
Our proposal also differs theoretically from related ones (Sagae and Lavie, 2006; Zhang andClark, 2011; Zhu et al., 2013) by explicitly using an LR automaton.
The explicit introduction of theLR automaton allows us to establish a formal difference between shift reduce phrase structure parsingand shift reduce dependency parsing.
It further provides some insights on the nature of the grammarunderlying many contemporary parsers.The paper is organized as follows.
First, section 2, we set up a formal framework for describingweighted phrase structure parsing as a 2-LCFG (Nederhof and Satta, 2010).
Observing that the treestructures are actually constrained in practice we formulate in section 3 an LR automaton constructionmethod for treebank grammars suitable for encoding these constraints.
We then provide in section 4a description of the algorithm and its components.
Section 5 give an extension to 2-LCFG suitablefor parsing morphologically rich languages and meeting common practical requirements.
The wholeThis work is licensed under a Creative Commons Attribution 4.0 International Licence.541framework is then evaluated in section 6 on French and English allowing to better identify its propertieswith respects to the state of the art.2 Grammatical representationThe first step we consider is the grammar actually used for parsing and how it is generated.
We supposehere a bilexical context free grammar or 2-LCFG (Nederhof and Satta, 2010).
A 2-LCFG is a CFG whoserules are of the form given in Figure 1 (left).
Symbols of the form x and h denote terminal symbolswhile symbols of the form A[h] or A[x] denote lexicalized non terminals.
A,B,C are non lexicalizednon terminals and h denotes a head.
A 2-LCFG rule is typically of the form NP [cat]?
D[a]N [cat].For practical robust parsing, 2-LCFG are grammars with a very large number of rules generated dy-namically at runtime (Section 4).
Most of the static grammatical preprocessing involved for the gen-eration of an LR automaton only applies to the underlying delexicalized 2-CFG by ignoring lexicalannotation symbols.A[h] ?
B[h]C[x]A[h] ?
B[x]C[h]A[h] ?
hX:ZDbCbYBaAaXZ:DbCbY:BaAaFigure 1: 2-LCFG rule patterns and invalid 2-CFG treesThe first step towards robust parsing thus requires to generate a grammar suitable for this purpose.
Inour case, the grammar is a treebank grammar and since most treebanks do encode trees with variablebranching arities we must transform it to match the 2-LCFG required pattern.
The first step amounts toapply an order 0 head markovization (Collins, 2003) which is followed by a reduction of unary rules.Both transformations guarantee that the trees do follow strictly a Chomsky Normal Form (CNF).
Trees inCNF have two properties of interest.
First, one can show by induction that they can be generated with aconstant number of derivation steps ?
for a sentence of length n : ?
= 2n?1.
This property is in principlecritical for the comparison of weighted parsing hypotheses (Section 5) and explains why we use 2-LCFGas a grammatical representation in the first place.
Second, the binarization (markovization) procedurealso introduces temporary symbols we consider to be different from other non terminal symbols.
Thesetemporary symbols are further constraining the tree structure.
Using ?:?
to denote a temporary symbolin Figure 1 (right), we observe for instance that the root of a tree cannot be temporary and two siblingscannot be temporaries either.
By contrast, arc standard dependency parsers such as the one of (Huangand Sagae, 2010) do verify the first property while the second property is irrelevant in that case.3 LR automaton constructionWe use an LR automaton to enforce the parser to generate parse trees satisfying the above mentionedstructural constraints.
Although, other proposals such as (Sagae and Lavie, 2006) apparently returns afailure when the parser generates invalid trees and (Zhu et al., 2013) apparently handles the problemwith local constraints preventing the parser to generate invalid configurations, we use here an LR(0)automaton to ensure that the parser globally enforces these constraints.
This seemed to us theoreticallyjustified, easier to generalize (Section 6) and easier to implement.As such, a traditional LR(0) parser (Knuth, 1965) is not suited for parsing natural language: it aimsto statically eliminate ambiguity from the grammar.
Here, following (Tomita, 1985) the LR tables arebuilt without trying to resolve conflicts.
Instead the conflicts are kept and determinism is brought by aweighting component.
The use of LR(0) tables aims to ensure that the parser actually generates validparse derivations.
In this case, the generation of the derivations requires to constrain the underlying 2-CFG grammar with respects to temporary symbols.
This being said, building an LR(0) automaton forrobust treebank grammars raise two issues.
The first is inductive, a grammar read off from a treebank isnot guaranteed to be robust and to generalize to other text, since a treebank remains a finite sample oflanguage.
The second is practical : traditionalLR(0) compilation methods involve the determinisation of542the LR NFA which is exponential in the number n of states of this NFA.
In case of very large ambiguoustreebank grammars n is very large and the compilation becomes intractable (Briscoe and Carroll, 1993).These two observations lead us to design this automata by the following construction.
First, let ?
bethe set of non terminal symbols read off from the treebank, T be the set of temporaries introduced bybinarization and N the set of non temporary symbols such that ?
= N ?
T and N ?
T = ?.
Secondwe note W the set of terminal symbols extracted from the treebank and A ?
N the unique axiom of thisgrammar.
We then partition ?
with the following set of equivalence classes: [a] = {A}, [t] = T and[n] = ??
(T ?A).
For convenience we also note [w] = W .
Given these equivalence classes, we definethe matrix grammar Gm= ?
?m, [w], [a], Rm?
(where ?m= {[a], [n], [t]}).
The rules Rmof Gmarethen designed to enforce the above mentioned tree well formedness constraints.
Some possible such rulesare given in Table 1 using ID/LP notation (Gazdar et al., 1985).In other words, an immediate dominancerule of the form a ?
b , c is expanded as two rules a ?
b c and a ?
c b.
Such a grammar allows[a]?
[n] , [t] [n]?
[n] , [t] [t]?
[n] , [t][a]?
[n] , [n] [n]?
[n] , [n] [t]?
[n] , [n][a]?
[w] [n]?
[w]Table 1: Example of Immediate Dominance rules for Gmto enforce the above-mentioned constraints, it is also small and it is robust : L(Gm) = [w]+.
We canthen very easily build a deterministic LR(0) automaton Am= ??m?
{[w]}, Q, i, F,Em?
with classicalmethods (Aho et al., 2006).
From this automaton we can then efficiently generate an expanded automatonAexp= ?{?
?W}, Q, i, F,E?
where E = {(q, a, q?)
| (q, [x], q?)
?
Em, ?a ?
[x]}.
In order to read offthe LR(0) table from Aexp, we consider the set of actions Adef= {RL(X)|X ?
?}
?
{RR(X)|X ??}
?
{RU(X)|X ?
?}
?
{S} first introduced by (Sagae and Lavie, 2006).
In short S denotes theshift action, RU(X) denotes an unary reduction by terminal X , RL(X) denotes a binary reduction byterminalX with left symbol marked as head, andRR(X) denotes a binary reduction by terminalX withright symbol marked as head.
By contrast with a classical LR action set, we extract the actions RL(X)and RR(X) from a state q ?
Q if we have an LR item of the form ?X ?
AB??
without requiring that?X ?
BA??
?
q.
This simplification, mirrorring that of (Sagae and Lavie, 2006), reduces the numberof actions, eases learning and makes parsing more efficient.
This being said, the matrix grammar Gmgiven in Table 1 is not the only one possible (see also section 6).
A valid rule set must enforce tree wellformedness constraints by building upon a partition of ?
in equivalence classes.
On the other hand theaction set A defined here implies that for every rule R ?
Rmof the form [x] ?
[y] [z] there is a ruleR??
Rmof the form [x]?
[z] [y].
That is why we formulate the rules Rmwith ID/LP notation and thisalso means that we cannot express any word ordering constraint with this grammar.
This last property isactually shared by many robust parsers.4 Discriminative LR-based parsingThe LR tables being built by preserving conflicts, determinism is achieved by a weighting componentderived from the global perceptron described by (Collins, 2002).
We start by describing the weightedparsing procedure before turning our attention to the weight estimation problem.We assume that anLR(0) table has been built.
The GOTO function of this table GOTO: (?
?W )?N 7?N sends a couple of symbol and LR state to a new LR state.
The ACTION: (N ?W ) 7?
2Afunction ofthis table returns a set a of possible actions given a state and a terminal symbol.
The initial LR state ofthe table is ?iwhile ?edenotes a final state.The algorithm relies on two data structures: a stack S and a queue.
The stack S = .
.
.
|s2|s1|s0hass0for topmost element.
A node si= ?
?, ??
in the stack is a couple where ?
is an LR state number and?
= (si.ct[si.wt] si.cl[si.wl] si.cr[si.wr]) encodes a local tree of depth 1. si.ct, si.cl, si.crdenotethe root left child and right child categories of tree and si.wt, si.wl, si.wrdenote the root, the left childand right child terminals of this tree such that a node si.c?[si.w?]
denotes a non terminal 2-LCFG symbolat node siin the stack.
The queue is static and initially filled up with the sequence of tokens to be parsed:543ITEM ?j,S?
: wINIT ?1, ?
?i, ??
: 0GOAL ?n+ 1, ?
?e, ???
: wSHIFT?j,S| s0=??,_??
:w?j+1,S| s0| ?GOTO(tj,?
), (tj[tj] _ _)?
:w+F (S,?j,S?
)RL(X)?j,S| s2=??2,_?
:w2| s1=?
?1,(s1.ct[s1.wt] _ _)?
:w1| s0=?
?0,(s0.ct[s0.wt] _ _)??
:w0?j,S| s2| ?GOTO(X,?2),(X[s1.wt] s1.ct[s1.wt] s0.ct[s0.wt])?
:w0+F (RL(X),?j,S?
)RR(X)?j,S| s2=??2,_?
:w2| s1=?
?1,(s1.ct[s1.wt] _ _)?
:w1| s0=?
?0,(s0.ct[s0.wt] _ _)??
:w0?j,S| s2| ?GOTO(X,?2),(X[s0.wt] s1.ct[s1.wt] s0.ct[s0.wt])?
:w0+F (RR(X),?j,S?
)RU(X)?j,S| s1=?
?1,(s1.ct[s1.wt] _ _)?
| s0=?
?0,(s0.ct[s0.wt] _ _)??
:w0?j,S| s1| ?GOTO(X,?1),(X[s0.wt] s0.ct[s0.wt])?
:w0+F (RU(X),?j,S?
)GR?j,S| s1=?
?1,(s1.ct[s1.wt] _ _)?
| s0=?
?0,(s0.ct[s0.wt] _ _)??
:w0?j,S| s1| ?GOTO(GR,?1),(s0.ct[s0.wt] _ _)?
:w0+F (GR,?j,S?
)(Rule introduced in section 5)Figure 2: Actions as inference rules in extended deductive notationT = t1.
.
.
tn.
Parsing is performed by generating sequentially configurations Ci= ?j,S?
where S is astack and j the index of the first element of the queue.
Given an initial configuration C0= ?1, ?
?i, ?
?,a derivation step Ci?1ai?1?
Cigenerates a new configuration Ci= ?j?,S??
provided a configurationCi?1= ?j,S|?
?, ???
by applying the action ai?1?
ACTION(?, tj).
A k?step derivation sequenceC0?kis a sequence of derivation steps such that C0a0?
.
.
.ak?1?
Ck.
A derivation sequence is finishedwhen the configuration C3n?1= ?n+ 1, ?
?, ???
is generated1.
If ?
= ?ethen the derivation is a success,otherwise it is a failure.
A derivation is also finished when ACTION(?, tj) = ?
for a configurationCk= ?
?, tj?
which is another case of failure.
The actions detailed in Figure 2 using extended deductivenotation are responsible for modifying the stack and updating LR states.
The shift action, SHIFT, thuspushes onto the stack a local tree rooted by the category of the next token in the queue.
The reduce leftRL(X) and the reduce right RR(X) actions pop the top two elements from the stack and push a newelement of category X[w] on top of it.
The two actions differ only by the way the head w is assigned:RL(X) chooses w to be X?s left child head word while RR(X) sets w to be X?s right child head word.RU(X) is an unary reduction action that pops the stack top and pushes a new top element with categoryX whose head is its unique child head.
By design of the automaton we ensure that RU(X) can only beapplied after a shift reduction took place.In order to achieve disambiguation, a derivation sequence C0?k= C0a0?
.
.
.ak?1?
Ckis also weightedby a function of the form:W (C0?k) = w ?
?g(C0?k) =k?1?i=0w ??
(ai, Ci)that is the weight of a derivation sequence is given by an inner product that the parser approximates asa sum of inner products local to each derivation step.
w ?
Rdis a d-dimensional vector of weights andeach ?
(ai, Ci) ?
{0, 1}dis a d-dimensional vector of feature functions in which every ?ihas signature?i(a,?, j).
The values a and j denote an action and the current index of the head of the queue in T while?
is a kernel vector similar to the one defined by (Huang and Sagae, 2010).
It summarizes informationaccessible from the stack for the purpose of feature function evaluation.
Figure 3 illustrates the actualkernel vector used in this paper: together with j, the index of the first element in the queue, the kernel1For shift reduce parsing with a 2-CFG grammar, the number of steps is the number of reduction steps plus n shifts:?
= 2n?
1 + n = 3n?
1.544vector ?
is the set of values accessible to feature functions ?i(a,?, j) in the stack.
As can be seen thestack stores local trees with instanciated 2-LCFG nodes labelled with the notation introduced in section4.
Since the score of a derivation is a sum of independent terms, the (prefix) weight w = W (C0?k) of aderivation sequence can be computed at each derivation step.
This allows to store the (prefix) weight ofthis sequence on configurations such that a configuration has the extended form Ck= ?j,S?
: w in theweighted case.
We make explicit the actual prefix weight computation in Figure 2 by using the followingabbreviation: F (ai, Ci) = w ??
(ai, Ci).s0.ct[s0.wt]s0.cr[s0.wr]s0.cl[s0.wl]s1.ct[s1.wt]s1.cr[s1.wr]s1.cl[s1.wl]s2.ct[s2.wt]Figure 3: Representation of the kernel vector ?For a given input sequence T , the parser is naturally non deterministic.
Non determinism is introducedby the ACTION function which returns a set a ?
2Aof possible actions given the current configuration.In the nondeterministic case, we thus derive from a given derivation sequence C0?k?1a set ?
(C0?k?1)of k?steps derivation sequences.
If we let GENk?1(T ) be the set of derivation sequences at step k ?
1,the set of derivation sequences at step k is GENk(T ) =?C0?k?1?GENk?1(T )?(C0?k?1).
In this context,achieving deterministic parsing amounts to solve the following optimization problem:?C = argmaxC0?3n?1?GEN3n?1(T )W (C0?3n?1) (1)Since in the worst case, the size of GENk(T ) is |A|k, the search space has exponential size.
Like(Zhu et al., 2013), we use in this paper a beam search approximation.
A beam GENKk(T ) is asubset of size K of GENk(T ).
Provided a beam GENKk?1(T ) we build GENKk(T ) with the fol-lowing recurrence: GENKk(T ) = K-argmaxC0?k??
(GENKk?1(T ))W (C0?k) where ?
(GENKk?1(T )) =?C0?k?1?GENKk?1(T )?
(C0?k?1), Using a beam aims to reduce complexity toO(K|A|(3n?1)) ?
O(n)and makes inference computationally tractable in practice.
On the other hand it makes inference incom-plete (the parser may fail to find a solution even if it exists) and does not guarantee the solution to beoptimal.
In other words, Equation 1 is replaced by an approximation:?C = argmaxC0?3n?1?GENK3n?1(T )W (C0?3n?1) (2)The weight estimation procedure is performed by the averaged percetron algorithm (Collins, 2002).As pointed out by (Huang et al., 2012) using a beam introduces an approximation that can also harmthe convergence of the learning procedure since we provide at each training iteration the approxima-tive solution given by equation 2 instead of the exact solution to equation 1 expected in theory bythe perceptron algorithm.
To overcome the problem we perform updates on subderivation sequences.Let C(r)0?kbe a subderivation sequence at step k and let C(0)0?k= argmaxC0?k?GENKk(T )W (C0?k)be the best subderivation in the beam at step k. In this context the perceptron update has the form:w ?
w + ?g(C(r)0?k) ?
?g(C(0)0?k) .
We tested two methods for choosing k satisfying the weakerconvergence criterions established by (Huang et al., 2012) : C(0)0?k6= C(r)0?kand W (C(0)0?k) >W (C(r)0?k).
If we let V = {k |C(0)0?k6= C(r)0?k,W (C(0)0?k) > W (C(r)0?k)}, then the early updatemethod amounts to choose k = mink?Vk and the max violation update method amounts to choosek = argmaxk?VW (C(0)0?k)?W (C(r)0?k).5 GeneralisationsThis section introduces two extensions to the algorithm meeting practical motivations: grammar relax-ation and extended word representations.545In practical cases, it may be convenient to interface the parser with a morphological tagger.
In this caseterminal symbols t1.
.
.
tnare part of speech tags.
Since grammar transformations introduced in section2 can potentially modify the tagset and since enforcing a strict Chomsky normal form in this case makeslittle sense, we allow the trees to have structures such as the one given in Figure 4.NPNP:APANDFigure 4: Relaxed tree structureThis kind of structure licences the following newpatterns of 2-CFG rules: A ?
B t and A ?
tBwhere t denotes a terminal symbol (in this case atag).
These new rule patterns modify a property of2-LCFG on which we relied so far, ?
is now variable :n?
1 ?
?
?
2n?
1.
We observe that longer deriva-tion sequences tend to have an higher weight.
Indeedweights increase linearly with the length of the derivation sequence as illustrated in Figure 5 where theweights are averaged out of measurements made over the parses on the French development set describedin Section 6.llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0 10 20 30 40 50 6002000400060008000CkWeight(C k)Figure 5: Average derivation weight as a function of the derivation lengthAlthough weights can in principle be positive or negative, this apparently counter-intuitive behaviouris caused by the beam which keeps for further steps only the highest weighted configurations.
To furtherstudy the behaviour of variable length sequences, we define two variants of the parser: first the ?naive?variant modifies the termination condition.
Let S = {C0?k|Ck= ?n+1, ?
?e, ??
?, 2n?1 ?
k ?
3n?1}.In this context, equation 2 is reframed as:?C = argmaxC0?k?SW (C0?k) .
A second version, called the?synchronized version?
introduces an additional inference rule called the Ghost Reduction and referredas GR in Figure 2.
A slight modification of the LR automaton construction, designed to trigger either anunary reduction action or a Ghost reduction after a shift allow us to enforce the property that ?
= 3n?
1in this case too.
The ghost reduction is designed to both make the parser ?wait?
one step during derivationin case it chooses not to perform an unary reduction after shift and also to avoid modifying the contentof the stack.The second extension allows terminals to be not only lexical tokens or part-of-speech tags but arbi-trary tuples ?.
This allows to encode words with an arbitrary set of additional structured features suchas their lemmas, gender, number, case, semantic representation.
The exact nature of these additionalfeatures depends on the capacity of a parsing preprocessor to actually supply them.
In this context thenon terminal symbols of the 2-LCFG have thus the form A[?].
The fields of the tuples are then made ac-cessible to feature functions.
This extension is motivated by the hypothesis that parsing morphologicallyrich languages will benefit significantly from structured word representations, for instance allowing theparser to take advantage of morphology.We are now in position to describe the feature templates used by the parser (Figure 6).
Before the dotsiand qidenote respectively the address in the stack and in the queue of the adressed node.
t, l, r denotethe top, left and right nodes of the local trees in the stack.
After the dot wc, wfdenote a category and aword form, while c is a constituent category.
wmdenote the mood of a verb and wXan refined categorydubbed subcat in the French Treebank (Abeill?
et al., 2003): these subcategories refine crude tags byencoding information such as the definiteness of a determiner, subtypes of adjectives etc.
gen, num, agr546s0t.wc& s0t.c s0t.wf& s1t.wfs0t.c& s1t.c& s2t.c s0t.c& q2.wc& q3.wcAgreements0t.wf& s0t.c s0t.wf& s1t.c s0t.wf& s1t.c& s2t.c s0t.c& q2.wf& q3.wcs0tc& e(s0t.agr, s1t.agr)& s1t.cs1t.wc& s1t.c s0t.c& s1t.wfs0t.c& s1t.wf& q0.wcs0t.c& q2.wc& q3.wfs0tc& e(s0t.num, s1t.num)& s1t.cs1t.wf& s1t.c s0t.c& s1t.c s0t.c& s1t.c& s2t.wfs0t.c& s0r.c& s1t.c s0tc& e(s0t.gen, s1t.gen)& s1t.cs2t.wc& s2t.c s0t.wf& q0.wfs0t.c& s1t.c& q0.wcs0t.c& s0r.c& s1t.wfs0tc& e(s0t.agr, q0.agr)& q1.wcs2t.wc& s2t.c s0t.c& q0.wfs0t.wf& s1t.c& q0.wcs0t.w& s0r.c& s1t.wfs0tc& e(s0t.gen, q0.gen)& q1.wcq0.wc& q0.wfs0t.c& q0.wcs0t.c& s1t.wf& q0.wcs0t.c& s0l.wf& s1t.c s0tc& e(s0t.num, q0.num)& q1.wcq1.wc& q1.wfq0.wf& q1.wfs0t.c& s1t.c& q0.wfs0t.c& s0l.c& s1t.wfs0tc& e(s0t.agr, q1.agr)& q1.wcq2.wc& q2.wfq0.wf& q1.wcs0t.c& q0.wc& q1.wcs0t.c& s0l.c& s1t.c s0tc& e(s0t.num, q0.num)& q1.wcq3.wc& q3.wfq0.wc& q1.wcs0t.c& q0.wf& q1.wcMood s0tc& e(s0t.gen, q0.gen)& q1.wcs0l.wf& s0l.c s1t.wf& q0.wfs0t.c& q0.wc& q1.wfs0t.wm& s1t.wfSubcats0r.wf& s0r.c s1t.wf& q0.wcs0t.c& q1.wc& q2.wcs0t.wf& s1t.wms0t.wX& s1t.wfs1l.wf& s1l.c s1t.c& q0.wfs0t.c& q1.wf& q2.wcs0t.c& s1t.wms0t.wf& s1t.wXs1r.wf& s1r.c s1t.c& q0.wcs0t.c& q1.wc& q2.wfs0t.wm& s1t.c s0t.c& s1t.wXs0twX& s1t.cFigure 6: Parser templatesdenote the gender, the number and their interaction.
The notation e(?, ?)
is an equality function returningtrue if the values of both its argument are equal.6 ExperimentsThe following experiments aim to identify the contribution of the components of the parser both toparsing accurracy and to parsing speed.
Experiments are carried mainly on French.
A final set of tests isalso carried out on English in order to highlight the generality of the framework and to ease comparisonswith other proposals.6.1 ProtocolThe experiments use the French SPMRL dataset (Seddah et al., 2013) which is newer and larger thandatasets previously used for parsing French (Crabb?
and Candito, 2008).
It instanciates the full FrenchTreebank described in (Abeill?
et al., 2003) and will surely become the new standard data set for parsingFrench in the next few years.
We use this data set as is, with two scenarios: one with gold standard tagsand the second with tags predicted by a 97.35% accurate tagger (Seddah et al., 2013).
The French datais head annotated with head rules provided by (Arun and Keller, 2005).
Additionally, compound wordstructures are systematically left headed.
For English, we use the Penn Treebank with standard split:section 02-21 for training, section 22 for development and section 23 for test.
The predicted scenariouses the MELT tagger (Denis and Sagot, 2012) with an accurracy of 97.1%.
The head annotations havebeen inferred by aligning the phrase structure treebank with its dependency conversion described by (deMarneffe et al., 2006).We use a C++ implementation of the algorithm described above for running the experiments.
Scoresreported for the Berkeley parser (Petrov et al., 2006) use the runs described by (Seddah et al., 2013).F-score is measured with the classical evalb and times are measured on the same machine (MacOSX2.4Ghz) and do not take into account input/output times for both parsers.Each experiment modifies a single experimental variable by contrast with a default parser configura-tion.
The default parser configuration sets the beam size to K = 4 and uses the naive synchronisationprocedure (Section 5).
The LR automaton uses the grammar G(base)m(Figure 7) and the update method isearly update (Section 4).
The set of templates is given in Figure 6 except for English where agreement,mood and subcat are ignored since there is no morphology directly available.Experiment 1 This first experiment tests the impact of the beam size by running the parser withdifferent sizes: K = 2,K = 4,K = 8,K = 16.Experiment 2 The second experiment contrasts the naive synchronisation (naive) with the ghostreduction synchronisation (sync) described in section 5.547Experiment 3 This third experiment contrasts two different matrix grammars (Figure 7).
This exper-iment aims to test whether we can take advantage of the LR automaton to better account for compoundwords encoded in the French data.
To this end we designed two matrix grammars generating two dif-ferent automata.
In Figure 7, the top left tree is an example of the representation of compound wordsin the French data set.
The corresponding binary structure is given on bottom left.
The general gram-mar G(base)mencodes a matrix grammar that does not specifically handle compound words and for whichequivalence classes are [a] the axiom symbol, [n] non terminal symbols and [w] terminal symbols.
Eachtemporary non terminal t ?
T yields its own equivalence class.
The grammar G(cpd)madds further equiv-alence classes: [n]cpdgathers non terminals marked as compounds (cpd) and is disjoint from [n], the noncompound non terminals.
T(cpd)gathers temporary symbols marked as compounds and is disjoint fromT .
From these two matrix grammar we generate two different LR automata with one of them encoding aspecific subgrammar for compounds (cpd) while the other is a generic grammar (base).NCcpdNCmarch?PdeNCpartNCcpdNCmarch?NC:cpdPdeNCpartImmediate dominance rules for Gbasem[a]?
[n] [n] [n]?
[n] , [n] t?
[n] , [n] (?t ?
T )[a]?
[n] , [w] [n]?
[n] , [w] t?
[n] , [w] (?t ?
T )[a]?
[w] [w] [n]?
[w] , [w] t?
[w] , [w] (?t ?
T )[a]?
[w] [n]?
[w] t?
[n] , t (?t ?
T )[a]?
[n] , t (?t ?
T ) [n]?
[n] , t (?t ?
T ) t?
[w] , t (?t ?
T )[a]?
[w] , t (?t ?
T ) [n]?
[w] , t (?t ?
T )Additional immediate dominance rules for Gcpdm[a]?
[n]cpd, [n] [n]?
[n]cpd, [n] t?
[n]cpd, [n] (?t ?
T )[a]?
[n]cpd, [n]cpd[n]?
[n]cpd, [n]cpdt?
[n]cpd, [n]cpd(?t ?
T )[a]?
[n]cpd, [w] [n]?
[n]cpd, [w] t?
[n]cpd, [w] (?t ?
T )[a]?
[n]cpd, t (?t ?
T ) [n]?
[n]cpd, t (?t ?
T ) t?
[n]cpd, t (?t ?
T )[n]cpd?
[w] , [w] [n]cpd?
tcpd, [w] (?tcpd?
Tcpd) tcpd?
tcpd, [w] (?tcpd?
Tcpd)[n]cpd?
[w] tcpd?
[w] , [w] (?tcpd?
Tcpd) tcpd?
[w] (?tcpd?
Tcpd)Figure 7: Structured representation of compound words (French data set) and related matrix grammars.Experiment 4 This experiment contrasts early update with max violation update.
Max violationupdate is trained over 12 epochs, while early update is trained over 25 epochsExperiment 5 This last experiment contrasts the use of morphology.
We remove (no-morph) the tem-plates under mood, agreement and subcategories in Figure 6 in order to assess their impact.6.2 ResultsResults with respect to accurracy are given in table 2.
Experiments are carried out on the developmentset with gold part of speech tags (Table 2, left) and predicted part of speech tags (Table 2, right).Experiment 1 The parser achieves its best result with a beam of size 8 (K = 8).
Quite surprisingly itachieves already a very correct score with a beam of size 4.
We observe that increasing the size of thebeam does not proide significant improvements.
Using beams of size 16 (or even 32) only bring marginalaccurracy improvements, if any, at the expense of more important parsing times.Experiment 2 This second experiment is apparently more disappointing: synchronisation does notseem to play a significant role on accurracy, or a detrimental one if any.
This effect seems caused bya property of the dataset.
A more careful analysis of the parser error patterns shows that the parsingmodel naturally misses many unary reductions.
Since the naive automaton is biased towards predictinglonger sequences with higher weights, it somehow helps to favor longer derivations containing moreunary reductions, hence improving the accurracy.548French dev (gold tags)Exp?rience F?
40 F CovK=2 85.40 82.74 98.6K=4 86.52 83.69 99.5K=8 86.80 84.31 99.9K=16 86.49 83.95 99.9sync 86.41 83.66 99.6naive 86.52 83.69 99.5cpd 86.30 83.30 99.2base 86.52 83.69 99.5Max Violation 85.98 83.49 99.5Early Update 86.52 83.69 99.5no-morph 85.23 82.43 99.8all-morph 86.52 83.69 99.5French dev (predicted tags)Exp?rience F?
40 F CovK=2 83.24 80.42 98.9K=4 84.32 81.34 99.4K=8 84.43 81.79 99.8K=16 84.59 81.94 99.8sync 84.06 81.14 99.9naive 84.32 81.34 99.4cpd 83.84 81.17 99.0base 84.32 81.34 99.4Max Violation 83.42 80.56 99.5Early Update 84.32 81.34 99.4no-morph 83.68 81.05 99.8all-morph 84.32 81.34 99.4Table 2: Experimental results (development)Experiment 3 This experiment highlights the problems related to further constraining a parser withapproximative search: we observe that parsing coverage is reduced.
This can be explained by the factthat further constraining the grammar creates less success states in the automaton and that the parsersometimes has to perform less local decisions without all the necessary information available.
Thissuggests for further work that a more constrained matrix grammar should be used with a more robustsearch strategy than simple beam search.Experiment 4 In experiment 4, we observe that max violation update converges twice as fast as earlyupdate but we experienced more overfitting problems explaining the lower scores.
It is however harderto achieve fair comparisons since the number of iterations is significantly different.Experiment 5 This last experiment is probably the most significative.
We observe that morphologyis the variable that allows the parser to improve significantly on French (dev F=81.79).
This result thusconfirms those observed by (Hohensee and Bender, 2012) on several languages for dependency parsing,yet we had to isolate agreement, refined subcategories and verbal mood to get significant improvements(Figure 6).Final tests In order to compare this proposal with current state of the art parsers, we provide com-parative measures of speed and accurracy with the Berkeley parser (Petrov et al., 2006), known to berepresentative of the state of the art in accurracy and in speed on French and in accurracy on English(Table 3).French Test (gold tags) F?
40 F CovK=8 87.14 84.20 99.8Berkeley 86.44 83.96 99.9French Test (predicted tags) F?
40 F CovK=8 84.33 81.43 99.8Berkeley 83.16 80.73 99.9French test (raw text) F?
40 F CovBerkeley 83.59 81.33 99.9English test (gold tags) F?
40 F CovK=8 90.2 89.5 100English test (pred tags) F?
40 F CovK=8 89.7 89.1 100English test (raw text) F?
40 F CovBerkeley - 90.1 -Table 3: Experimental results (test)549Interestingly parsing the English dataset with templates designed on French data is almost state of theart on English (dev F=89.3, test F=89.1).
This suggests that feature engineering is a less important issuethan often thought and it also suggests that the parser is likely to be easy to adapt to other languages bygeneralizing the method used to parse English.Although the differences are modest, the parser is state of the art on French (F=81.43) if we comparewith the Berkeley parser (F=80.73) known to be indicative of the state of the art on French (Seddah etal., 2013) and if we ignore ensemble and semi-supervised parsers.The most important difference is related to speed.
(Petrov et al., 2006) is reported to be the fastestphrase structure parser for English by (Huang and Sagae, 2010) where the authors compare with (Char-niak, 2000).
Yet (Petrov et al., 2006) is a polynomial time parser, while this one has linear time behaviour.We compared the speed of both parsers on the same hardware (ignoring input/output times) and we find(Petrov et al., 2006) has an average parse time of t?= 0.28s with maximum tmax= 10.27s whileour linear time algorithm has mean time t?= 0.06s and maximum tmax= 0.1s with beam 4 andt?= 0.1s, tmax= 0.5s with beam 8 (Figure 8).
Further constraining the automaton shows to be usefulfor speed, since for experiment 3, t?= 0.04s with K = 4 which is clearly faster.+++ +++ ++ + +++++++++ +++ ++ ++ +++++ + + ++++++++++ ++ +++ + ++++ ++++++ ++++ +++++++++ ++ +++++++++++++ +++ ++++ + ++ +++ +++++++++++ ++++ +++++++++ ++++++ +++++++++++++++++++++++++ ++++++++ ++++++++++++++ ++++ ++++++ ++ +++++ + ++++++++++ + +++++++++ + +++++++++++++0 20 40 60 80 100 120 1400246810LongueurTemps(secs)+ +++ +++++ +++ ++ ++ ++ ++ ++ +++ ++ ++++ +++++ ++ +++ ++ + +++ +++ ++ + + ++++ ++ + ++++ + ++++ ++ +++ + + ++ +++ ++ ++ + ++ + ++++ + ++ ++++ +++ ++ ++ +++ ++++ + ++ +++ ++ ++ + ++++ ++++ ++ + +++ + + +++ ++Berkeley Parserbeam 4beam 8Figure 8: Parsing timesWe finally observe on English that the LR strategy performs reasonably accurately and faster than theimplementation of (Petrov et al., 2006) whereas optimisations of PCFG-LA described by (Bodenstabet al., 2011) are significantly less accurate and not significantly faster.
Although it is currently hard toto compare directly on French with the most similar proposal, the Chinese/English parser described by(Zhu et al., 2013), since it does not handle morphology.
With respects to speed, their impressive resulton English suggests that there is still room for speed improvements without loss of accurracy.7 ConclusionTo our knowledge, this is the first formulation of a discriminative LR-automaton driven parser for nat-ural language.
This LR inspired algorithm shares many properties with shift reduce parsers for phrasestructure grammar described by (Sagae and Lavie, 2006) and (Zhu et al., 2013).
(Sagae and Lavie, 2006)describe a first version of this kind of algorithm using a weighting system based on a local maximum en-tropy classifier.
It thus enables them to use a best first search strategy that allows in principle to achievenear-optimal parsing.
By contrast, like (Zhu et al., 2013), we use here a global perceptron algorithmtogether with a beam based breadth first search to which we add an explicit LR component.
The LRautomaton allows us to guarantee that the parser generates a viable prefix.
We believe the LR frameworkcan also shed light on theoretical, practical and experimental issues related to phrase structure parsing bycomparison with dependency parsing.
However using the LR automaton to constrain the parsing model550for multi-word expressions turns out to be disappointing since it forces the parser to take less local de-cisions for which the beam approximation is not well suited.
This suggests for future work to exploresearch methods aiming to achieve optimality (Zhao et al., 2013).Mirroring a common practice in dependency parsing, the parser also provides a first support for phrasestructure parsing of morphologically rich languages thanks to structured word representations.
The richerlexical structure makes morphological information available during the parsing process.
For the case ofFrench it enables, among others, to integrate agreement in the parsing model.
This simple integrationof morphology then allows the parser to achieve state of the art accurracy on French.
Since in principlenothing in the algorithm is specific to French, we expect to generalize and experiment with the modelon other morphologically rich languages.
Further work for such languages is expected to involve arefinement of the interface with morphology along the lines of (Hatori et al., 2012; Bohnet et al., 2013).AcknowledgementsI am grateful to Maximin Coavoux who helped at some stages of the implementation.
I am also gratefulto Benoit Sagot for his encouragments and several discussions that helped to clarify the contents of thispaper, and finally to Djam?
Seddah who brought his expertise with the actual data sets.ReferencesAnne Abeill?, L. Cl?ment, and F. Toussenel.
2003.
Building a treebank for french.
In Treebanks.
Kluwer.Alfred V. Aho, Ravi Sethi, Jeffrey D. Ullman, and Monica S. Lam.
2006.
Compilers: Principles, Techniques, andTools.
Addison Wesley.Abhishek Arun and Frank Keller.
2005.
Lexicalization in crosslinguistic probabilistic parsing: The case of french.In Association for Computational Linguistics.Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian Roark.
2011.
Beam-width prediction for efficientcontext-free parsing.
In Proceedings of the 49th Annual Meeting of the Association for Computational Lin-guistics, Portland, Oregon, June.
Association for Computational Linguistics.Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich?rd Farkas, Filip Ginter, and Jan Hajic.
2013.
Joint morpho-logical and syntactic analysis for richly inflected languages.
TACL, 1:415?428.Ted Briscoe and John A. Carroll.
1993.
Generalized probabilistic lr parsing of natural language (corpora) withunification-based grammars.
Computational Linguistics, 19(1):25?59.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In North American Association for ComputationalLinguistics.Michael Collins.
2002.
Discriminative training methods for hidden markov models: Theory and experimentswith perceptron algorithms.
In Proceedings of the Conference on Empirical Methods in Natural LanguageProcessing (EMNLP).Michael Collins.
2003.
Head-driven statistical models for natural language parsing.
Computational Linguistics,29(4).Benoit Crabb?
and Marie Candito.
2008.
Exp?riences d?analyses syntaxique statistique du fran?ais.
In ActesdeTALN 2008.Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning .
In LREC 2006.
2006.
Generatingtyped dependency parses from phrase structure parses.
In Language ressources and evaluation conference.Pascal Denis and Beno?t Sagot.
2012.
Coupling an annotated corpus and a lexicon for state-of-the-art pos tagging.Language Resources and Evaluation, 46(1).Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of the Association for Computational Linguistics.Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and Ivan A.
Sag.
1985.
Generalized Phrase Structure Grammar.Cambridge, MA: Harvard University Press and Oxford: Basil Blackwell?s.551Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2012.
Incremental joint approach to wordsegmentation, pos tagging, and dependency parsing in chinese.
In ACL (1), pages 1045?1053.Matt Hohensee and Emily M. Bender.
2012.
Getting more from morphology in multilingual dependency parsing.In HLT-NAACL, pages 315?326.Liang Huang and Kenji Sagae.
2010.
Dynamic programming for linear-time incremental parsing.
In Proceedingsof the 48th Annual Meeting of the Association for Computational Linguistics.Liang Huang, Suphan Fayong, and Yang Guo.
2012.
Structured perceptron with inexact search.
In North Ameri-can Association for Computational Linguistics.Donald Knuth.
1965.
On the translation of languages from left to right.
Information and Control, 8(6).Mark-Jan Nederhof and Giorgio Satta.
2010.
Algorithmic aspects of natural language processing.
In M.J. Atallahand M. Blanton, editors, Algorithms and Theory of Computation Handbook.
CRC press.Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006.
Learning accurate, compact, and interpretabletree annotation.
In Proceedings of the 21st International Conference on Computational Linguistics and 44thAnnual Meeting of the Association for Computational Linguistics, Sydney, Australia.
Association for Computa-tional Linguistics.Kenji Sagae and Alon Lavie.
2006.
A best-first probabilistic shift-reduce parser.
In Proceedings of the COL-ING/ACL.Djam?
Seddah, Reut Tsarfaty, Sandra K?bler, Marie Candito, Jinho D. Choi, Rich?rd Farkas, Jennifer Foster,Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann,Wolfgang Maier, Joakim Nivre, Adam Przepi?rkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, VeronikaVincze, Marcin Woli?nsk, Alina Wr?blewska, and ?ric Villemonte De La Clergerie.
2013.
Overview of theSPMRL 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morphologically Rich Languages.
InProceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages.Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Conference on Empirical Methods in Natural Language Processing.Masaru Tomita.
1985.
An efficient context free parsing algorithm for natural language.
In International JointConference on Artificial Intelligence (IJCAI).Masaru Tomita.
1988.
Graph structured stack and natural language parsing.
In Proceedings of the Association forComputational Linguistics (ACL).Joseph P. Turian and I. Dan Melamed.
2006.
Advances in discriminative parsing.
In ACL.Yue Zhang and Stephen Clark.
2011.
Syntactic processing using the generalized perceptron and beam search.Computational Linguistics, 37(1).Kai Zhao, James Cross, and Liang Huang.
2013.
Optimal incremental parsing via best-first dynamic programming.In EMNLP, pages 758?768.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.
2013.
Fast and accurate shift-reduceconstituent parsing.
In Association for Computational Linguistics.552
