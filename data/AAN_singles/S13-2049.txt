Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 290?299, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 13:Word Sense Induction for Graded and Non-Graded SensesDavid JurgensDipartimento di InformaticaSapienza Universita` di Romajurgens@di.uniroma1.itIoannis KlapaftisSearch Technology Center EuropeMicrosoftioannisk@microsoft.comAbstractMost work on word sense disambiguation hasassumed that word usages are best labeledwith a single sense.
However, contextual am-biguity or fine-grained senses can potentiallyenable multiple sense interpretations of a us-age.
We present a new SemEval task for evalu-ating Word Sense Induction and Disambigua-tion systems in a setting where instances maybe labeled with multiple senses, weighted bytheir applicability.
Four teams submitted ninesystems, which were evaluated in two settings.1 IntroductionWord Sense Disambiguation (WSD) attempts toidentify which of a word?s meanings applies in agiven context.
A long-standing task, WSD is fun-damental to many NLP applications (Navigli, 2009).Typically, each usage of a word is treated as express-ing only a single sense.
However, contextual ambi-guity as well as the relatedness of certain meaningscan potentially elicit multiple sense interpretations.Recent work has shown that annotators find multi-ple applicable senses in a given target word contextwhen using fine-grained sense inventories such asWordNet (Ve?ronis, 1998; Murray and Green, 2004;Erk et al 2009; Passonneau et al 2012b; Jurgens,2013; Navigli et al 2013).
Such contexts would bebetter annotated with multiple sense labels, weight-ing each sense according to its applicability (Erk etal., 2009; Jurgens, 2013), in effect allowing ambigu-ity or multiple interpretations to be explicitly mod-eled.
Accordingly, the first goal of this task is toevaluate WSD systems in a setting where instancesmay be labeled with one or more senses, weightedby their applicability.WSD methods are ultimately defined and poten-tially restricted by their choice in sense inventory;for example, a sense inventory may have insufficientsense-annotated data to build WSD systems for spe-cific types of text (e.g., social media), or the inven-tory may lack domain-specific senses.
Word SenseInduction (WSI) has been proposed as a method forovercoming such limitations by learning the sensesautomatically from text.
In essence, a WSI algo-rithm acts as a lexicographer by grouping word us-ages according to their shared meaning.
The sec-ond goal of this task is to assess the performance ofWSI algorithms when they are able to model multi-ple meanings of a usage with graded senses.Task 12 focuses on disambiguating senses for 50target lemmas: 20 nouns, 20 verbs, and 10 adjectives(Sec.
2).
Since the Task evaluates only unsupervisedsystems, no training data was provided; however, toenable more comparison, Unsupervised WSD sys-tems were also allowed to participate.
Participat-ing systems were evaluated in two settings (Sec.
3),depending on whether they used induced senses orWordNet 3.1 senses for their annotations.
The re-sults (Sec.
5) demonstrate a substantial improvementover the competitive most frequent sense baseline.2 Task DescriptionThis task required participating systems to annotateinstances of nouns, verb, and adjectives using Word-Net 3.1 (Fellbaum, 1998), which was selected dueto its fine-grained senses.
Participants could labeleach instance with one or more senses, weighting290We all are relieved to lay aside our fight-or-flight reflexes and to commemorate our births from out of the darkcenters of the women, to feel the complexity of our love and frustration with each other, to stretch our cognition toencompass the thoughts of every entity we know.dark%3:00:01:: ?
devoid of or deficient in light or brightness; shadowed or blackdark%3:00:00:: ?
secretI ask because my practice has always been to allow about five minutes grace, then remove it.ask%2:32:02:: ?
direct or put; seek an answer toask%2:32:04:: ?
address a question to and expect an answer fromTable 1: Example instances with multiple senses due to intended double meanings (top) or contextual am-biguity (bottom).
Senses are specified using their WordNet 3.1 sense keys.each by their applicability.
Table 1 highlights twoexample contexts where multiple senses apply.
Thefirst example shows a case of an intentional dou-ble meaning that evokes both the physical aspect ofdark.a as being devoid of light and the causal re-sult of being secret.
In contrast, the second exampleshows a case of multiple interpretations from ambi-guity; a different preceding context could generatethe alternate interpretations ?I ask [you] because?
(sense ask%2:32:04::) or ?I ask [the question]because?
(sense ask%2:32:02::).2.1 DataThree datasets were provided with the task.
The trialdataset provided weighted word sense annotationsusing the data gathered by Erk et al(2009).
Thetrial dataset consisted of 50 contexts for eight words,where each context was labeled with WordNet 3.0sense ratings from three untrained lexicographers.Due to the unsupervised nature of the task, partic-ipants were not provided with sense-labeled trainingdata.
However, WSI systems were provided with theukWaC corpus (Baroni et al 2009) to use in induc-ing senses.
Previous SemEval WSI tasks had pro-vided participants with corpora specific to the task?starget terms; in contrast, this task opted to use a largecorpus to enable WSI methods that require corpus-wide statistics, e.g., statistical associations.Test data was drawn from the Open AmericanNational Corpus (Ide and Suderman, 2004, OANC)across a variety of genres and from both the spokenand written portions of the corpus, summarized inTable 2.
All contexts were manually inspected to en-sure that the lemma being disambiguated was of thecorrect part of speech and had an interpretation thatmatched at least one WordNet 3.1 sense.
This filter-ing also removed instances that were in a colloca-tion, or had an idiomatic meaning.
Ultimately, 4664contexts were used as test data, with a minimum of22 and a maximum of 100 contexts per word.2.2 Sense AnnotationRecent work proposes to gather sense annotationsusing crowdsourcing in order to reduce the timeand cost of acquiring sense-annotated corpora (Bie-mann and Nygaard, 2010; Passonneau et al 2012b;Rumshisky et al 2012; Jurgens, 2013).
There-fore, we initially annotated the Task?s data using themethod of Jurgens (2013), where workers on Ama-zon Mechanical Turk (AMT) rated all senses of aword on a Likert scale from one to five, indicat-ing the sense does not apply at all or completelyapplies, respectively.
Twenty annotators were as-signed per instance, with their ratings combined byselecting the most frequent rating.
However, wefound that while the annotators achieved moderateinter-annotator agreement (IAA), the resulting an-notations were not of high enough quality to use inthe Task?s evaluations.
Specifically, for some sensesand contexts, AMT annotators required more infor-mation about sense distinctions than was feasible tointegrate into the AMT setting, which led to consis-tent but incorrect sense assignments.Therefore, the test data was annotated by the twoauthors, with the first author annotating all instancesand the second author annotating a 10% sample ofeach lemma?s instances in order to calculate IAA.IAA was calculated using Krippendorff?s ?
(Krip-pendorff, 1980; Artstein and Poesio, 2008), which isan agreement measurement that adjusts for chance,291Spoken WrittenGenre Face-to-face Telephone Fiction Journal Letters Non-fiction Technical Travel Guides AllInstances 52 699 127 2403 103 477 611 192 4664Tokens 1742 30,700 3438 69,479 2238 11,780 17,337 4490 141,204Mean senses/inst.
1.17 1.08 1.15 1.13 1.31 1.10 1.11 1.11 1.12Table 2: Test data used in Task 12, divided according to source typeranging in (?1, 1] for interval data, where 1 indi-cates perfect agreement and -1 indicates systematicdisagreement; two random annotations have an ex-pected ?
of zero.
We treat each sense and instancecombination as a separate item to rate.
The total IAAfor the dataset was 0.504, and on individual words,ranged from 0.903 for number.n to 0.00 for win.v.While this IAA is less than the 0.8 recommended byKrippendorff (2004), it is consistent with the IAAdistribution for the sense annotations of MASC onother parts of the OANC corpus: Passonneau et al(2012a) reports an ?
of 0.88 to -0.02 with the MASIstatistic (Passonneau et al 2006).Table 2 summarizes the annotation statistics forthe Task?s data.
The annotation process resulted infar fewer senses per instance in the trial data, whichwe attribute to using trained annotators.
An analysisacross the corpora genres showed that the multiple-sense annotation rates were similar.
Due to the vari-ety of contextual sources, all lemmas were observedwith at least two distinct senses.3 EvaluationWe adopt a two-part evaluation setting used in pre-vious SemEval WSI and WSD tasks (Agirre andSoroa, 2007; Manandhar et al 2010).
The first eval-uation uses a traditional WSD task that directly com-pares WordNet sense labels.
For WSI systems, theirinduced sense labels are converted to WordNet 3.1labels via a mapping procedure.
The second evalu-ation performs a direct comparison of the two senseinventories using clustering comparisons.3.1 WSD TaskIn the first evaluation, we adopt a WSD task withthree objectives: (1) detecting which senses are ap-plicable, (2) ranking senses by their applicability,and (3) measuring agreement in applicability rat-ings with human annotators.
Each objectives usesa specific measurement: (1) the Jaccard Index, (2)positionally-weighted Kendall?s ?
similarity, and(3) a weighted variant of Normalized DiscountedCumulative Gain, respectively.
Each measure isbounded in [0, 1], where 1 indicates complete agree-ment with the gold standard.
We generalize the tra-ditional definition of WSD Recall such that it mea-sures the average score for each measure across allinstances, including those not labeled by the system.Systems are ultimately scored using the F1 measurebetween each objective?s measure and Recall.3.1.1 Transforming Induced Sense LabelsIn the WSD setting, induced sense labels may betransformed into a reference inventory (e.g., Word-Net 3.1) using a sense mapping procedure.
We fol-low the 80/20 setup of Manandhar et al(2010),where the corpus is randomly divided into five par-titions, four of which are used to learn the sensemapping; the sense labels for the held-out partitionare then converted and compared with the gold stan-dard.
This process is repeated so that each partitionis tested once.
For learning the sense mapping func-tion, we use the distribution mapping technique ofJurgens (2012), which takes into account the senseapplicability weights in both labelings.3.1.2 Jaccard IndexGiven two sets of sense labels for an instance,X and Y , the Jaccard Index is used to measure theagreement: |X?Y ||X?Y | .
The Jaccard Index is maximizedwhen X and Y use identical labels, and is mini-mized when the sets of sense labels are disjoint.3.1.3 Positionally-Weighted Kendall?s ?Rank correlations have been proposed for evalu-ating a system?s ability to order senses by applicabil-ity; in previous work, both Erk and McCarthy (2009)and Jurgens (2012) propose rank correlation coeffi-cients that assume all positions in the ranking areequally important.
However, in the case of graded292sense evaluation, often only a few senses are appli-cable, with the applicability ratings of the remain-ing senses being relatively inconsequential.
There-fore, we consider an alternate rank scoring based onKumar and Vassilvitskii (2010), which weights thepenalty of reordering the lower positions less thanthe penalty of reordering the first ranks.Kendall?s ?
distance, K, is a measure of thenumber of item position swaps required to maketwo sequences identical.
Kumar and Vassilvitskii(2010) extend this distance definition using a vari-able penalty function ?
for the cost of swapping twopositions, which we denote K?.
By using an appro-priate ?, K?
can be biased towards the correctnessof higher ranks by assigning a smaller ?
to lowerranks.
Because K?
is a distance measure, its valuerange will be different depending on the number ofranks used.
Therefore, to convert the measure to asimilarity we normalize the distance to [0, 1] by di-viding by the maximum K?
distance and then sub-tracting the distance from one.
Given two rankingsx and y where x is the reference by which y is to bemeasured, we may compute the normalized similar-ity usingKsim?
= 1?K?
(x, y)Kmax?
(x).
(1)Equation 1 has its maximal value of one when rank-ing y is identical to ranking x, and its minimal valueof zero when y is in the reverse order as x.
We referto this value as the positionally-weighted Kendall?s?
similarity, Ksim?
.
As defined, Ksim?
does not ac-count for ties.
Therefore, we arbitrarily break ties ina deterministic fashion for both rankings.
Second,we define ?
to assign higher cost to the first ranks:the cost to move an item into position i, ?i, is de-fined as n?
(i+1)n , where n is the number of senses.3.1.4 Weighted NDCGTo compare the applicability ratings for sense an-notations, we recast the annotation process in an In-formation Retrieval setting: Given an example con-text acting as a query over a word?s senses, the taskis to retrieve all applicable senses, ranking and scor-ing them by their applicability.
Moffat and Zobel(2008) propose using Discounted Cumulative Gain(DCG) as a method to compare a ranking against abaseline.
Given (1) a gold standard weighting of thek senses applicable to a context, where wi denotesthe applicability for sense i in the gold standard, and(2) a ranking of the k senses by some method, theDCG may be calculated as?ki=12wi+1?1log2(i+1).
DCG iscommonly normalized to [0, 1] so that the value iscomparable when computed on rankings with dif-ferent k and weight values.
To normalize, the maxi-mum value is calculated by first computing the DCGon the ranking when the k items are sorted by theirweights, referred as the Ideal DCG (IDCG), and thennormalizing as NDCG = DCGIDCG .The DCG only considers the weights assignedin the gold standard, which potentially masks im-portance differences in the weights assigned to thesenses.
Therefore, we propose weighting the DCGby the relative difference in the two weights.
Givenan alternate weighting of the k items, denoted as w?i,WDCG =k?i=1min(wi,w?i)max(wi,w?i)(2wi+1 ?
1)log2(i).
(2)The key impact in Equation 2 comes from weight-ing an item?s contribution to the score by its rela-tive deviation in absolute weight.
A set of weightsthat achieves an equivalent ranking may have a lowWDCG if the weights are significantly higher orlower than the reference.
Equation 2 may be nor-malized in the same way as the DCG.
We refer tothis final normalized measure as the Weighted Nor-malized Discounted Cumulative Gain (WNDCG).3.2 Sense Cluster ComparisonsSense induction can be viewed as an unsupervisedclustering task where usages of a word are groupedinto clusters, each representing uses of the samemeaning.
In previous SemEval tasks on sense in-duction, instances were labeled with a single sense,which yields a partition over the instances into dis-joint sets.
The proposed partition can then be com-pared with a gold-standard partition using many ex-isting clustering comparison methods, such as theV-Measure (Rosenberg and Hirschberg, 2007) orpaired FScore (Artiles et al 2009).
Such clustercomparison methods measure the degree of similar-ity between the sense boundaries created by lexicog-raphers and those created by WSI methods.In the present task, instances are potentially la-beled both with multiple senses and with weights293reflecting the applicability.
This type of sense label-ing produces a fuzzy clustering: An instance maybelong to one or more sense clusters with its clus-ter membership relative to its weight for that sense.Formally, we refer to (1) a solution where the setsof instances overlap as a cover and (2) a solutionwhere the sets overlap and instances may have par-tial memberships in a set as fuzzy cover.We propose two new fuzzy measures for com-paring fuzzy sense assignments: Fuzzy B-Cubedand Fuzzy Normalized Mutual Information.
Thetwo measures provide complementary information.B-Cubed summarizes the performance per instanceand therefore provides an estimate of how well a sys-tem would perform on a new corpus with a similarsense distribution.
In contrast, Fuzzy NMI is mea-sured based on the clusters rather than the instances,thereby providing a performance analysis that is in-dependent of the corpus sense distribution.3.2.1 Fuzzy B-CubedBagga and Baldwin (1998) proposed a clusteringevaluation known as B-Cubed, which compares twopartitions on a per-item basis.
Amigo?
et al(2009)later extended the definition of B-Cubed to compareoverlapping clusters (i.e., covers).
We generalize B-Cubed further to handle the case of fuzzy covers.B-Cubed is based on precision and recall, which es-timate the fit between two clusterings, X and Y atthe item level.
For an item i, precision reflects howmany items sharing a cluster with i inX appear in itscluster in Y ; conversely, recall measures how manyitems sharing a cluster in Y with i also appear in itscluster in X .
The final B-Cubed value is the har-monic mean of the two scores.To generalize B-Cubed to fuzzy covers, we adoptthe formalization of Amigo?
et al(2009), who defineitem-based precision and recall functions, P and R,in terms of a correctness function, C ?
{0, 1}.
Fornotational brevity, let avg be a function that returnsthe mean value of a series, and ?x(i) denote the setof clusters in clusteringX of which item i is a mem-ber.
B-Cubed precision and recall may therefore cal-culated over all n items:B-Cubed Precision = avgi[ avgj 6=i??
?y(i)P (i, j)] (3)B-Cubed Recall = avgi[ avgj 6=i??
?x(i)R(i, j)].
(4)When comparing partitions, P and R are defined as1 if two items cluster labels are identical.
To gen-eralize B-Cubed for fuzzy covers, we redefine Pand R to account for differences in the partial clus-ter membership of items.
Let `X(i) denote the setof clusters of which i is a member, and wk(i) de-note the membership weight of item i in cluster k inX .
We therefore define C with respect to X of twoitems asC(i, j,X) =?k?`X(i)?`X(j)1?|wk(i)?wk(j)|.
(5)Equation 5 is maximized when i and j haveidentical membership weights in the clusters ofwhich they are members.
Importantly, Equation5 generalizes to the correctness operations bothwhen comparing partitions and covers, as definedby Amigo?
et al(2009).
Item-based Precisionand Recall are then defined using Equation 5 asP (i, j,X) = Min(C(i,j,X),C(i,j,Y ))C(i,j,X) and R(i, j,X) =Min(C(i,j,X),C(i,j,Y ))C(i,j,Y ) , respectively.
These fuzzy gen-eralizations are used in Equations 3 and 4.3.2.2 Fuzzy Normalized Mutual InformationMutual information measures the dependence be-tween two random variables.
In the context ofclustering evaluation, mutual information treats thesense labels as random variables and measures thelevel of agreement in which instances are labeledwith the same senses (Danon et al 2005).
For-mally, mutual information is defined as I(X;Y ) =H(X)?
(H(X|Y ) whereH(X) denotes the entropyof the random variable X that represents a parti-tion, i.e., the sets of instances assigned to each sense.Typically, mutual information is normalized to [0, 1]in order to facilitate comparisons between multipleclustering solutions on the same scale (Luo et al2009), with Max(H(X), H(Y )) being the recom-mended normalizing factor (Vinh et al 2010).In its original formulation Mutual informationis defined only to compare non-overlapping clusterpartitions.
Therefore, we propose a new definition ofmutual information between fuzzy covers using ex-tension of Lancichinetti et al(2009) for calculatingthe normalized mutual information between covers.In the case of partitions, a clustering is representedas a discrete random variable whose states denotethe probability of being assigned to each cluster.
In294the fuzzy cover setting, each item may be assignedto multiple clusters and no longer has a binary as-signment to a cluster, but takes on a value in [0, 1].Therefore, each cluster Xi can be represented sepa-rately as a continuous random variable, with the en-tire fuzzy cover denoted as the variableX1...k, wherethe ith entry of X is the continuous random vari-able for cluster i.
However, by modeling clusters us-ing continuous domain, differential entropy must beused for the continuous variables; importantly, dif-ferential entropy does not obey the same propertiesas discrete entropy and may be negative.To avoid calculating entropy in the continuous do-main, we therefore propose an alternative method ofcomputing mutual information based on discretiz-ing the continuous values of Xi in the fuzzy set-ting.
For the continuous random variable Xi, wediscretize the value by dividing up probability massinto discrete bins.
That is, the support of Xi is parti-tioned into disjoint ranges, each of which representsa discrete outcome of Xi.
As a result, Xi becomes acategorical distribution over a set of weights ranges{w1, .
.
.
, wn} that denote the strength of member-ship in the fuzzy set.
With respect to sense annota-tion, this discretization process is analogous to hav-ing an annotator rate the applicability of a sense foran instance using a Likert scale instead of using arational number within a fixed bound.Discretizing the continuous cluster membershipratings into bins allows us to avoid the problematicinterpretation of entropy in the continuous domainwhile still expanding the definition of mutual infor-mation from a binary cluster membership to one ofdegrees.
Using the definition of Xi and Yj as a cate-gorical variables over discrete ratings, we may thenestimate the entropy and joint entropy as follows.H(Xi) =n?i=1p(wi)log2p(wi) (6)where p(wi) is the probability of an instance beinglabeled with rating wi Similarly, we may define thejoint entropy of two fuzzy clusters asH(Xk, Yl) =n?i=1m?j=1p(wi, wj)log2p(wi, wj) (7)where p(wi, wj) is the probability of an instance be-ing labeled with rating wi in cluster Xk and wj incluster Yl, and m denotes the number of bins for Yl.The conditional entropy between two clusters maythen be calculated asH(Xk|Yl) = H(Xk, Yl)?H(Yl).Together, Equations 6 and 7 may be used to defineI(X,Y ) as in the original definition.
We then nor-malize using the method of McDaid et al(2011).Based on the limited range of fuzzy membershipsin [0, 1], we selected uniformly distributed bins in[0, 1] at 0.1 intervals when discretizing the member-ship weights for sense labelings.3.3 BaselinesTask 12 included multiple baselines based on mod-eling different types of WSI and WSD systems.Due to space constraints, we include only the fourmost descriptive here: (1) Semcor MFS which la-bels each instance with the most frequent sense ofthat lemma in SemCor, (2) Semcor Ranked Sensesbaseline, which labels each instance with all of thetarget lemma?s senses, ranked according to their fre-quency in SemCor, using weights n?i+1n , where n isthe number of senses and i is the rank, (3) 1c1instwhich labels each instance with its own inducedsense and (4) All-instances, One sense which la-bels all instances with the same induced sense.
Thefirst two baselines directly use WordNet 3.1 senses,while the last two use induced senses.4 Participating SystemsFour teams submitted nine systems, seven of whichused induced sense inventories.
AI-KU submittedthree WSI systems based on a lexical substitutionmethod; a language model is built from the targetword?s contexts in the test data and the ukWaC cor-pus and then Fastsubs (Yuret, 2012) is used to iden-tify lexical substitutes for the target.
Together, thecontexts of the target and substitutes are used tobuild a distributional model using the S-CODE al-gorithm (Maron et al 2010).
The resulting contex-tual distributions are then clustered using K-meansto identify word senses.
The University of Mel-bourne (Unimelb) team submitted two WSI systemsbased on the approach of Lau et al(2012).
Theirsystems use a Hierarchical Dirichlet Process (Tehet al 2006) to automatically infer the number ofsenses from contextual and positional features.
Un-295WSD F1 Cluster ComparisonTeam System Jac.
Ind.
Ksim?
WNDCG Fuzzy NMI Fuzzy B-Cubed #Cl #SAI-KU Base 0.197 0.620 0.387 0.065 0.390 7.76 6.61AI-KU add1000 0.197 0.606 0.215 0.035 0.320 7.76 6.61AI-KU remove5-add1000 0.244 0.642 0.332 0.039 0.451 3.12 5.33Unimelb 5p 0.218 0.614 0.365 0.056 0.459 2.37 5.97Unimelb 50k 0.213 0.620 0.371 0.060 0.483 2.48 6.08UoS #WN Senses 0.192 0.596 0.315 0.047 0.201 8.08 6.77UoS top-3 0.232 0.625 0.374 0.045 0.448 3.00 5.44La Sapienza system-1 0.149 0.507 0.311 - - - 8.69La Sapienza system-2 0.149 0.510 0.383 - - - 8.67All-instances, One sense 0.192 0.609 0.288 0.0 0.623 1.00 6.621c1inst 0.0 0.0 0.0 0.071 0.0 1.00 0.0Semcor MFS 0.455 0.465 0.339 - - - 1.00Semcor Ranked Senses 0.149 0.559 0.489 - - - 8.66Table 3: Performance on the five evaluation measures for all system and selected baselines.
Top systemperformances are marked in bold.like other teams, the Unimelb systems were trainedon a Wikipedia corpus instead of the ukWaC cor-pus.
The University of Sussex (UoS) team submit-ted two WSI systems that use dependency-parsedfeatures from the corpus, which are then clusteredinto senses using the MaxMax algorithm (Hope andKeller, 2013); the resulting fine-grained clusters arethen combined based on their degree of separabil-ity.
The La Sapienza team submitted two Unsu-pervised WSD systems based applying Personal-ized Page Rank (Agirre and Soroa, 2009) over aWordNet-based network to compare the similarity ofeach sense with the similarity of the context, rankingeach sense according to its similarity.5 Results and DiscussionTable 3 shows the main results for all instances.
Ad-ditionally, we report the number of induced clustersused to label each sense as #Cl and the number ofresulting WordNet 3.1 senses for each sense with#S.
As in previous WSD tasks, the MFS baselinewas quite competitive, outperforming all systems ondetecting which senses were applicable, measuredusing the Jaccard Index.
However, most systemswere able to outperform the MFS baseline on rank-ing senses and quantifying their applicability.Previous cluster comparison evaluations oftenfaced issues with the measures being biased eithertowards the 1c1inst baseline or labeling all instanceswith the same sense.
However, Table 3 shows thatTeam System F1 NMI B-CubedAI-KU Base 0.641 0.045 0.351AI-KU add1000 0.601 0.023 0.288AI-KU remove5-add1000 0.628 0.026 0.421Unimelb 5p 0.596 0.035 0.421Unimelb 50k 0.605 0.039 0.441UoS #WN Senses 0.574 0.031 0.180UoS top-3 0.600 0.028 0.414La Sapienza System-1 0.204 - -La Sapienza System-2 0.217 - -All-instances, One sense 0.569 0.0 0.5701c1inst 0.0 0.018 0.0Semcor MFS 0.477 0.0 0.570Table 4: System performance in the single-sense set-ting.
Top system performances are marked in bold.systems are capable of performing well in both theFuzzy NMI and Fuzzy B-Cubed measures, therebyavoiding the extreme performance of either baseline.An analysis of the systems?
results showed thatmany systems labeled instances with a high num-ber of senses, which could have been influenced bythe trial data having significantly more instances la-beled with multiple senses than the test data.
There-fore, we performed a second analysis that parti-tioned the test set into two sets: those labeled witha single sense and those with multiple senses.
Forsingle-sense set, we modified the test setting to havesystems also label instances with a single sense:(1) the sense mapping function for WSI systems(Sec.
3.1.1) was modified so that after the mapping,296WSD F1 Cluster ComparisonTeam System Jac.
Ind.
Ksim?
WNDCG Fuzzy NMI Fuzzy B-CubedAI-KU Base 0.394 0.617 0.317 0.029 0.078AI-KU add1000 0.394 0.620 0.214 0.014 0.061AI-KU remove5-add1000 0.434 0.585 0.290 0.004 0.116Unimelb 5p 0.436 0.585 0.286 0.019 0.130Unimelb 5000k 0.414 0.602 0.298 0.021 0.134UoS #WN Senses 0.367 0.627 0.313 0.036 0.037UoS top-3 0.421 0.574 0.302 0.006 0.113La Sapienza system-1 0.263 0.660 0.447 - -La Sapienza system-2 0.412 0.694 0.536 - -All-instances, One sense 0.387 0.635 0.254 0.0 0.1301c1inst 0.0 0.0 0.0 0.300 0.0Semcor MFS 0.283 0.373 0.197Semcor Ranked Senses 0.263 0.593 0.395Table 5: System performance on all instances labeled with multiple senses.
Top system performances aremarked in bold.only the highest-weighted WordNet 3.1 sense wasused, and (2) the La Sapienza system output wasmodified to retain only the highest weighted sense.In this single-sense setting, systems were evaluatedusing the standard WSD Precision and Recall mea-sures; we report the F1 measure of Precision and Re-call.
The remaining subset of instances annotatedwith multiple senses were evaluated separately.Table 4 shows the systems?
performance onsingle-sense instances, revealing substantially in-creased performance and improvement over theMFS baseline for WSI systems.
Notably, the per-formance of the best sense-remapped WSI systemssurpasses the performance of many supervised WSDsystems in previous WSD evaluations (Kilgarriff,2002; Mihalcea et al 2004; Pradhan et al 2007;Agirre et al 2010).
This performance suggests thatWSI systems using graded labels provide a way toleverage huge amounts of unannotated corpus datafor finding sense-related features in order to trainsemi-supervised WSD systems.Table 5 shows the performance on the subset ofinstances that were annotated with multiple senses.We note that in this setting, the mapping proce-dure transforms the All-Instances One Sense base-line into the average applicability rating for eachsense in the test corpus.
Notably, the La Sapienzasystems sees a significant performance increase inthis setting; their systems label each instance withall of the lemma?s senses, which significantly de-grades performance in the most common case whereonly a single sense applies.
However, when multi-ple senses are known to be present, their method forquantifying sense applicability appears closest to thegold standard judgments.
Furthermore, the majorityof WSI systems are able to surpass all four baselineson identifying which senses are present and quanti-fying their applicability.6 ConclusionWe have introduced a new evaluation setting forWSI and WSD systems where systems are measuredby their ability to detect and weight multiple appli-cable senses for a single context.
Four teams submit-ted nine systems, annotating a total of 4664 contextsfor 50 words from the OANC.
Many systems wereable to surpass the competitive MFS baseline.
Fur-thermore, when WSI systems were trained to pro-duce only a single sense label, the performance ofresulting semi-supervised WSD systems surpassedthat of many supervised systems in previous WSDevaluations.
Future work may assess the impact ofgraded sense annotations in a task-based setting.
Allmaterials have been released on the task website.1AcknowledgmentsWe thank Rebecca Passonneau for her feedback andsuggestions for target lemmas used in this task.1http://www.cs.york.ac.uk/semeval-2013/task13/297ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007 task2: Evaluating word sense induction and discriminationsystems.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations, pages 7?12.
ACL.Eneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for Word Sense Disambiguation.
In Pro-ceedings of EACL, pages 33?41.
ACL.Eneko Agirre, Oier Lo?pez De Lacalle, Christine Fell-baum, Andrea Marchetti, Antonio Toral, and PiekVossen.
2010.
SemEval-2010 task 17: All-wordsword sense disambiguation on specific domains.
InProceedings of SemEval-2010.
ACL.Enrique Amigo?, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A comparison of extrinsic clusteringevaluation metrics based on formal constraints.
Infor-mation Retrieval, 12(4):461?486.Javier Artiles, Enrique Amigo?, and Julio Gonzalo.
2009.The role of named entities in web people search.
InProceedings of EMNLP, pages 534?542.
Associationfor Computational Linguistics.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Amit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of the Lin-guistic Coreference Workshop at LREC, pages 563?566.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Chris Biemann and Valerie Nygaard.
2010.
Crowdsourc-ing wordnet.
In The 5th International Conference ofthe Global WordNet Association (GWC-2010).Leon Danon, Albert D?
?az-Guilera, Jordi Duch, and AlexArenas.
2005.
Comparing community structure iden-tification.
Journal of Statistical Mechanics: Theoryand Experiment, 2005(09):P09008.Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proceedings of Empirical Meth-ods in Natural Language Processing (EMNLP), pages440?449.
ACL.Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2009.
Investigations on word senses and word us-ages.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 10?18.
ACL.Christine Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.David Hope and Bill Keller.
2013.
MaxMax: A Graph-Based Soft Clustering Algorithm Applied to WordSense Induction.
In Proceedings of CICLing, pages368?381.Nancy Ide and Keith Suderman.
2004.
The americannational corpus first release.
In Proceedings of theFourth Language Resources and Evaluation Confer-ence, pages 1681?1684.David Jurgens.
2012.
An Evaluation of Graded SenseDisambiguation using Word Sense Induction.
In Pro-ceedings of *SEM, the First Joint Conference on Lexi-cal and Computational Semantics.
ACL.David Jurgens.
2013.
Embracing Ambiguity: A Com-parison of Annotation Methodologies for Crowdsourc-ing Word Sense Labels.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL).
ACL.Adam Kilgarriff.
2002.
English lexical sampletask description.
In Proceedings of ACL-SIGLEXSENSEVAL-2 Workshop.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to Its Methodology.
Sage, Beverly Hills, CA.Klaus Krippendorff.
2004.
Content Analysis: An In-troduction to Its Methodology.
Sage, Thousand Oaks,CA, second edition.Ravi Kumar and Sergei Vassilvitskii.
2010.
General-ized distances between rankings.
In Proceedings ofthe 19th International Conference on World Wide Web(WWW), pages 571?580.
ACM.Andrea Lancichinetti, Santo Fortunato, and Ja?nosKerte?sz.
2009.
Detecting the overlapping and hierar-chical community structure in complex networks.
NewJournal of Physics, 11(3):033015.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word sense induc-tion for novel sense detection.
In Proceedings of the13th Conference of the European Chapter of the Asso-ciation for computational Linguistics (EACL 2012).Ping Luo, Hui Xiong, Guoxing Zhan, Junjie Wu, andZhongzhi Shi.
2009.
Information-theoretic distancemeasures for clustering validation: Generalization andnormalization.
IEEE Transactions on Knowledge andData Engineering, 21(9):1249?1262.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
SemEval-2010task 14: Word sense induction & disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 63?68.
ACL.Yariv Maron, Michael Lamar, and Elie Bienenstock.2010.
Sphere embedding: An application to part-of-speech induction.
In Proceedings of Advances in Neu-ral Information Processing Systems (NIPS).298Aaron F. McDaid, Derek Greene, and Neil Hurley.
2011.Normalized mutual information to evaluate overlap-ping community finding algorithms.
arXiv:1110.2515.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The Senseval-3 English lexical sampletask.
In Senseval-3: Third International Workshop onthe Evaluation of Systems for the Semantic Analysis ofText, pages 25?28.
ACL.Alistair Moffat and Justin Zobel.
2008.
Rank-biasedprecision for measurement of retrieval effectiveness.ACM Transactions on Information Systems (TOIS),27(1):2.G.
Craig Murray and Rebecca Green.
2004.
Lexicalknowledge and human disagreement on a wsd task.Computer Speech & Language, 18(3):209?222.Roberto Navigli, David Jurgens, and Daniele Vanilla.2013.
Semeval-2013 task 12: Multilingual word sensedisambiguation.
In Proceedings of the 7th Interna-tional Workshop on Semantic Evaluation.Roberto Navigli.
2009.
Word Sense Disambiguation: ASurvey.
ACM Computing Surveys, 41(2):1?69.Rebecca Passonneau, Nizar Habash, and Owen Rambow.2006.
Inter-annotator agreement on a multilingual se-mantic annotation task.
In Proceedings of the FifthInternational Conference on Language Resources andEvaluation (LREC), pages 1951?1956.Rebecca J Passonneau, Collin Baker, Christiane Fell-baum, and Nancy Ide.
2012a.
The MASC word sensesentence corpus.
In Proceedings of LREC.Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-Aouissi, and Nancy Ide.
2012b.
Multiplicity andword sense: evaluating and learning from multiply la-beled word sense annotations.
Language Resourcesand Evaluation, pages 1?34.Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
SemEval-2007 task 17: Englishlexical sample, SRL, and all-words.
In Proceedings ofthe 4th International Workshop on Semantic Evalua-tions.
ACL.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).
ACL.Anna Rumshisky, Nick Botchan, Sophie Kushkuley, andJames Pustejovsky.
2012.
Word sense inventories bynon-experts.
In Proceedings of the 8th InternationalConference on Language Resources and Evaluation(LREC).Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Jean Ve?ronis.
1998.
A study of polysemy judgments andinter-annotator agreement.
In Program and advancedpapers of the Senseval workshop.Nguyen Xuan Vinh, Julien Epps, and James Bailey.2010.
Information theoretic measures for clusteringscomparison: Variants, properties, normalization andcorrection for chance.
The Journal of Machine Learn-ing Research, 11:2837?2854.Deniz Yuret.
2012.
FASTSUBS: An Efcient AdmissibleAlgorithm for Finding the Most Likely Lexical Substi-tutes Using a Statistical Language Model.
ComputingResearch Repository (CoRR).299
