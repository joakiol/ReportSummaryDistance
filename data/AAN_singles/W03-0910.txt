Deriving Verb-Meaning Clusters from Syntactic StructurePaul Kingsbury, Karin KipperComputer and Information SciencesUniversity of PennsylvaniaAbstractThis paper presents a methodology for us-ing the argument structure of sentences, asencoded by the PropBank project, to de-velop clusters of verbs with similar mean-ing and usage.
These clusters can be fa-vorably compared to the classes developedby the VerbNet project.
The most inter-esting cases are those where the cluster-ing methodology suggests new members forVerbNet classes which will then be associ-ated with the semantic predicates for thatclass.1 IntroductionThere can be no doubt that meaning is dependentupon a great number of factors.
The di?cultyin higher-level comprehension in Natural LanguageProcessing is due to the identication of these factorsas well as the successful implementation of methodsfor handling them.
Early generative models of syn-tax failed to answer the meaning question because oftheir assumption that semantics was a natural out-come of structure, thus neglecting the other factorsinvolved.
More recent works, such as those foundin WordNet (Miller, 1985; Fellbaum, 1998) and itsdescendents, fall short for the inverse reason: theyfocus solely on lexical meaning while ignoring struc-ture altogether.Another factor in the meaning game is the rela-tionship between individual lexical items of the samefunction.
How are all determiners alike, for exam-ple, and how are they individually dierent?
Arethere subclasses within the class of determiners, andhow do these subclasses dier from each other?
Hu-man beings are supreme pattern-matchers and assignthings to categories almost to a fault.
The categoriesinto which semantically loaded items are groupedmust therefore be one of the factors leading to higher-level comprehension, and knowledge of what thesecategories are and how they are built can help solvethe greater meaning question.2 Issue and Previous WorkMuch previous work in the domain of classicationof lexical items has focused on verbs.
This is a nat-ural starting place, since the verb is the hook uponwhich the rest of a sentence hangs.
Verbs also dis-play a higher degree of variation in their semanticsthan other lexical types.
Whereas nouns, for exam-ple, all name some kind of `thing,' verbs can de-scribe an action or a state and involve some num-ber of non-verbal actors in the description.
It isthis latter quality of verbs which most interests ushere, and indeed which has been the focus of mostprevious work in verb classication.
Even the mostelementary grammars of English draw a distinctionbetween transitive and intransitive verbs.
More ad-vanced work has rened this distinction into ever-larger numbers of classes.
For example, the land-mark work of Levin (1993) divided over three thou-sand verbs into 191 classes based partly on shared se-mantics and partly on shared syntactic alternations.More recently, the VerbNet project at Penn (Kip-per et al, 2000) incorporated Levin's verb classica-tion to systematically create verb entries in a lexi-con.
On a purely semantic note, WordNet (Miller,1985; Fellbaum, 1998) has classied much of the vo-cabulary of English, not just verbs, in terms of rela-tionships such as synonymy, hyponymy, and others.Various attempts worldwide have begun focussing onthe argument structure of verbs as part of develop-ing dependency grammars.
The PropBank projectat Penn (Kingsbury and Palmer, 2002) is an exam-ple of this process for English; similar projects areunderway for Czech (Hajicova etc), German (Broker1998), and others.
The FrameNet project at Berke-ley (Baker et al, 1998) has classied many words interms of their relation to a relatively small numberof core semantic concepts such as `commerce' and`judgment'.
Various attempts have been made toautomatically cluster verbs into semantically mean-ingful classes, using the Levin class as a gold stan-dard for evaluation (Gildea, 2002; McCarthy, 2000;Merlo and Stevenson, 2001; Schulte im Walde, 2000).In the next two sections, we provide background onVerbNet and PropBank which play central roles inthe cluster methodology presented here.2.1 VerbNetVerbNet is a verb lexicon with syntactic and seman-tic information for English verbs, referring to Levinverb classes (Levin, 1993) for systematic construc-tion of lexical entries.
This lexicon exploits the sys-tematic link between syntax and semantics that mo-tivates these classes, and thus provides a clear andregular association between syntactic and semanticproperties of verbs and verb classes (Kipper et al,2000; Dang et al, 2000).
Each class in the hier-archy is composed of a set of members (linked totheir WordNet synsets) and a set of syntactic framesand semantic information for each frame.
Currently,VerbNet has over 4,000 verb senses described (3,004lemmas) within 191 rst level classes.VerbNet has a hierarchical structure, with the rstlevel classes constituted by the original Levin classes.In order to ensure that each class is coherent, so thatall its members share a common set of thematic roles,syntactic frames and semantic predicates, some re-structuring of the classes was required.
This reorga-nization, which was facilitated by the use of inter-sective Levin classes (Dang et al, 1998), rened theclasses to account for semantic and syntactic dier-ences within a class.
A child subclass inherits all theinformation from its parent class, and adds informa-tion to it, which can be in terms of imposing furtherrestrictions on the roles, or adding syntactic framesor semantic predicates to the subclass.The hierarchical organization of VerbNet is illus-trated in Figure 1.
The Transfer of a Message verbclass is subdivided into three levels.
At the top levelare thematic roles, syntactic frames and semanticpredicates shared by all members of the class.
Inthis particular case, there is a transitive frame withthe Topic (message) as the direct object (Agent VerbTopic), as in \John explained trigonometry", and aframe for Topic and Recipient (Agent Verb Topic toRecipient), as in \John taught math to Mary".
Bothsyntactic frames have semantic predicates expressingthe transfer of information event, but in the rst casethe Recipient is underspecied.
Some of the verbs inthis class are able to participate in other syntacticframes as well.
Verbs at the second level can takethe ditransitive frame (Agent Verb Recipient Topic)in addition to the frames and predicates inheritedfrom the parent class.VerbNet uses aat semantic representation inwhich the semantics of each syntactic frame is cap-tured by a conjunction of predicates1, such as mo-tion, contact, transfer info, which can be negated ornot.
These predicates can take arguments over theverb complements, as well as over implicit existen-tially quantied event variables.Each semantic predicate in VerbNet alo includea time function specifying whether the predicateis true in the preparatory (during(E)), culmina-tion (end(E)), or consequent (result(E)) stage of anevent, in a tripartite event structure is similar to thatof Moens and Steedman (1988), which allows us toexpress the semantics of classes of verbs like changeof state verbs whose description requires reference toa complex event structure.2.2 PropBankIn a dierent vein, the PropBank project (Kingsburyand Palmer, 2002) has endeavoured to describe allthe most frequent verbs of English in terms of theirargument structure.
This project has three majordierences from previous works.
First, the descrip-tion of each verb is accompanied by a rich set ofexamples drawn from real language, in this case theWall Street Journal sections of the Penn Treebank(Marcus, 1994).
Furthermore, the descriptions arebased on the usages in the corpus, rather than a pos-sible situation where the corpus was mined for sen-tences tting preconceived patterns.
This results inmany instances which are perfectly well-formed butunexpected.
The best example of this is an odd usageof the verb add: The Nasdaq composite index added1.01 to 456.6 on paltry volume., where the contextmakes it clear that add is being used as a synonym forrise.
Second, argument structure allows for a richerset of descriptions than merely `transitive', `unac-cusative', and so forth, since any individual verb isallowed to have anywhere between zero and six ar-guments.
Third and perhaps most importantly, thePropBank descriptions make explicit mention of thedierent senses of verbs.
This is crucial because dif-ferent senses can have dierent argument structuresor dierent syntactic alternations, a detail which isoften glossed over in other resources.
Thus, while (1)1Presently there are 64 distinct predicates described.Transfer of a Message - level 1 classhhMEMBERS ii [hcite; wn1i; hdemonstrate;wn1i; hexplain;wn1i; : : :]hhTHEMATIC ROLES ii Agent(A), Recipient(R), Topic(T)hhSELECT RESTRICTIONS ii Agent[+animate],Recipient[+animate],Topic[+message]hhFRAMES and PREDICATES iiTransitive with Topic A V T transfer info(during(E),A,?,T)Topic and Recipient A V T to R transfer info(during(E),A,R,T)Transfer of a Message - level 2 classhhPARENT ii Transfer of a Message - level 1hhMEMBERS ii [hdictate;wn2i; hquote; wn1i; hread;wn3i]hhFRAMES and PREDICATES iiDitransitive A V R T transfer info(during(E),A,R,T)Figure 1: Example entries for the Transfer of a Message - levels 1 and 2 classesand (3) share the same argument structure, (2) and(4) do not, because of the ungrammaticality of (4):1 Congress passed the bill.2 John passed the butter.3 The bill passed.4 *The butter passed.Under the PropBank scheme, the similarity be-tween pass (make bill into law) and pass (movefrom one place to another) is regarded an almost-accidental byproduct of centuries of semantic drift.The combination of accounting for all the usagesin a large corpus and separating verb senses re-sults in a very large database: 4358 distinct sensesspread across 3183 lexical items, with a total of over70 000 unique tokens.
PropBank annotations arerecorded as theory-neutrally as possible.
Crucially,the nomenclature of traditional thematic roles is notused; instead, simple labels of Arg0, Arg1 and soforth are used.
Each ArgX is mapped to a specicrole on a per-verb basis, such as the roleset for give:Arg0: giverArg1: giftArg2: given-toex: [Arg0The executives] gave [Arg2thechefs] [Arg1a standing ovation].Such a schema has its advantages and its draw-backs.
On the negative side, it makes it di?cult todraw comparisons between various arguments withthe same number.
The meaning of Arg2 for one verbmight bear no relation to the meaning of Arg2 forsome other verb, for example.
On the plus side, notusing traditional thematic roles both frees the anno-tators from the necessity of distinguishing betweenTheme and Patient, for example, while also allowingfor roles which do not have corresponding thematicroles.
For example, PropBank has a host of verbs re-lating to the judicial process, such as jail, sentence,imprison, and so forth.
All of these take four roles;Arg0: lawgiverArg1: guilty partyArg2: term, length of jail timeArg3: crimeex: Dallas District Judge Jack Hamptonhad sparked calls for a judicial inquiry withhis remarks to the press last December,two weeks after [Arg0*trace=Hampton*]sentencing [Arg1an 18-year-old defendant]to [Arg230 years in state prison] for [Arg3killing two homosexual men in a city park.
]While Args 0 and 1 can be thought of as Agentand Patient, traditional thematic role terminologyfails to accommodate Args 2 and 3, for all that theyare central to the meaning of the verbs.3 MethodologyThe PropBank annotations thus provide a rich setof actual usages which are easily divided into sensesand quantied.
That is, for any verb (sense) it ispossible to collect all the usages, divide them intotheir various syntactic realizations, and enumeratethese realizations.
The realizations themselves arerendered at a very basic level, encoding only the lin-ear order of arguments.
The verb itself is renderedas rel for 'relation'.
Thus a verb such as repeal canappear in either a simple transitive or passive:[Arg0West Germany] will repeal [Arg1the unpopular turnover tax on securitytransactions].
(wsj 0302)!
Arg0 rel Arg1...the tax will be o?cially repealed [Arg1*trace=the tax*] before the end of thecurrent parliamentary term... (wsj 0302)!
rel Arg1As seen in the second example, adjuncts such asthe temporal before the end of the current parliamen-tary term are omitted.
While the bulk of annotationsfall into basic categories such as Arg0 rel Arg1 (ba-sic transitive), Arg0 rel Arg1 Arg2 (ditransitive) andso forth, the nature of the data and the annotationprocess itself means that there is a large residue of ir-regular syntactic realizations.
As an example of theformer, consider the following sentence, where thebenefactive argument has been topicalized to the be-ginning:For [Arg4Mr.
Sherwin], [Arg0a conviction]could carry [Arg1penalties of ve years inprison and a $250,000 ne on each count].
(wsj 1331) !
Arg4 Arg0 rel Arg1Such a construction is certainly a natural part ofEnglish, yet it occurs very rarely{only twice in allof Propbank (so far).
The annotation process itselfalso introduces many odd, low-frequency syntacticrealizations which should be regarded as errors.
Forexample, occasionally annotators include one argu-ment label twice:...the board also was informed...of [Arg1in-terest] expressed by [Arg0buy-out fundsincluding Kohlberg Kravis Roberts & Co.and Forstmann Little & Co.], as well as by[Arg0Robert Bass, Morgan Stanley's buy-out fund, and Pan Am Corp].
(wsj 2104)!
Arg1 rel Arg0 Arg0In cases of such conjoined constructions the an-notation guidelines specify that the sentence shouldbe regarded as two overlapping sentences, with twooverlapping argument structures.
Thus, for the sen-tence above, there should properly be two instancesof `Arg1 rel Arg0', each with the same Arg1 and relbut with dierent Arg0's.
The annotator of this sen-tence clearly forgot this specication.
Fortunately,PropBank is proceeding through a double blind an-notation followed by an adjudication phase.
The ad-judication will catch and correct structures such asthe above.
For the time being, however, such errorsare retained, largely for want of an e?cient and ac-curate ltering mechanism.Other ltering on the data is performed, however.Since the TreeBank corpus is subject to the sameZipan distribution2of lexical items as any natural-language database, many verbs are only poorly at-tested.
A largely arbitrary decision was made toeliminate all verbs which had fewer than 10 attesta-tions.
Finally, again due to the ongoing nature of thePropBank work, not all polysemous verbs have beendisambiguated.
These verbs were also deleted fromthe subsequent analysis.
All the same, the resultingdataset provides 921 verbs with 200 distinct syntacticrealizations of varying frequencies.
Each realizationwas recorded as a proportion of the number of attes-tations of that syntactic pattern to the total num-ber of attestations of that verb.
Expressing these asproportions rather than as raw counts allows for bet-ter comparisons among verbs which themselves varywidely in frequency.
The proportions themselves arethen run through a standard clustering algorithm im-plemented in R (http://www.R-project.org) whichclassies each verb by the dierence between its at-tested proportions and the proportions attested byevery other verb.
Verbs with little dierence, mean-ing they attest nearly the same syntactic patterns innearly the same proportions, are judged to be verysimilar and are likely to be grouped into the sameclass.
A varying number of \centroids" or proto-types for each class can be established.
Each verbis then classied by its similarity to the centroids,resulting in a number of classes equal to the num-ber of centroids.
The resulting classes are comparedto the existing VerbNet classes.
This is at best anoisy measure of accuracy, since many of the Prop-Banked verbs (even fairly richly-attested ones) arenot classied in VerbNet, and many verbs which areclassied in VerbNet are not present in PropBank insu?cient numbers to undergo the clustering analysis.These factors are constant across all possible numberof centroids/classes, however, so at least the amountof noise in the assessment remains constant as well.2The probability of occurrence of words or other itemsstarts high and tapers o.
Thus, a few occur very oftenwhile many others occur rarely.4 ResultsAs an initial assessment of the worth of the method-ology, consider the case with only three clusters.
Theclusters can be viewed graphically in Figure 2.The crosses are the centroids.
Such a display asthis is naturally a gross simplication of the actual200-dimensional space the clusters are mapped in,but it's the best that can be done on paper.
If we lookat (a sample of) the membership of each of the threeclusters, we see that the three groups clearly fall intothe sets of transitive, unaccusative, and ditransitiveverbs:3Class 1 Class 2 Class 3waste repay driftask.01 invest dieovercome appoint crumblefeel.01 invite triplemake.05 assignoatinuence recruit disappearFor higher numbers of clusters a more formalmethod of measuring accuracy is required.
Since weare comparing sets, the similarity of two sets A andB can be computed with the following:similarity(A;B) =jA\BjjA[BjIn other words, the similarity of two sets can bemeasured as the number of elements shared by thetwo sets divided by the total number of unique el-ements in the two sets.
Thus, two identical sets(fa, bg fb, ag) would have a similarity score of 1,because the intersection contains two elements, asdoes the union.
Two sets a, b and a, b, ..., z wouldhave a very low similarity score, since the intersec-tion contains two elements, but that is divided by theunion with 26 elements.
Thus the similarity scoreis dependent both on the number of matches andthe number of spurious elements present.
For exam-ple, the three clusters described above most closelyalign to the VerbNet classes 13.5.1 (Get class), 45.6(Calibratable change state class), and 48.1.1 (Appearclass), respectively, with 4, 4, and 7 matching ele-ments.
Each of these clusters is huge, however (521,335, and 65 verbs, respectively), and the target setsare also large as VerbNet clusters go (31, 23, and 27verbs, respectively).
These factors conspire to makethe overall similarity scores quite low (4/(521+31),3ask.01: ask a question, as opposed to ask a favor;feel.01: feel emotion; make.05: make money, as in \Pauldoesn't make nearly enough money; triple: while in mostEnglish this would be considered a transitive verb, thecommon usage in the Wall Street Journal is actually un-acusative, as in \Paul's net worth tripled when he founda $5 bill in the dryer'."etc).
This demonstrates how a three-cluster analy-sis, while interesting for describing a gross syntacticgrouping, does not go very far in describing any ner-grained meaning contrasts.To get at the classication with real implicationsfor meaning we clearly need to use more, smaller clus-ters.
How many more, and how small?
Since theoriginal set of verbs under analysis comes from 150VerbNet classes, we can set an upper bound on thenumber of clusters at 150.
Then we iteratively deriveclusters for each number between 3 and 150 and ndthe similarity scores for each cluster in each of thosesets.
To facilitate comparison between the varioussets of clusters, we calculate the average similarityacross all the classes.
This is necessary because, asthe number of clusters rises, the size of any individualcluster is likely to fall, thus bringing the denominatorof the similarity equation down.
Taken across all theclusters, then, the similarity scores are bound to riseregardless of improvements in the matching betweenthe VerbNet clusters and the automatically-derivedones.
The outcome of this procedure can be seen inFigure 3.Interesting peaks appear for analyses with 14 clus-ters and around 89 clusters before the long tailingo with more than 90 clusters.
In addition, there isa local maximum around 32 clusters.
The analyseswith 14 clusters still uses very large clusters (aver-age of 65 verbs/cluster), so that number is less useful.The smallest cluster developed therein, however, con-sists only of `remain' and `stay.01',4and that clustercorrectly maps to VerbNet class 47.1-1 (Exist class),which contains verbs of staying in place.By the time around 90 clusters are used the derivedclasses themselves tend to be very small, with 34clusters containing two or fewer verbs.
In contrast,a few clusters continue to have very large member-ship, such as two groups (arbitrarily labeled `3' and`38') which contain 145 and 146 verbs, respectively,and which correspond roughly to VerbNet classes29.5 (Conjecture class) and 29.2 (Characterize class).While 29.2 is one of the largest classes in VerbNet,precisely why this class should seem to attract va-grant verbs remains to be seen.
More interesting,however, are the times when the clustering processestablishes multiple clusters which are all identiedwith the same VerbNet class { in eect, suggestingsubclasses where they have not previously appeared.This trend appears even with fairly low numbers ofclusters, and it tends to be extremely consistent as towhich VerbNet clusters are split.
For example, class45.4 (Other change state), containing the `miscella-4Sense:remain.
?1.0 ?0.5 0.0 0.5?0.8?0.6?0.4?0.20.00.20.4first principal componentsecondprincipalcomponent211212222211222111222222 223112111112132 1222121213121221312221 12121221121221213122132121223211212122122221 1111233222 2112211121222 221211 12 11 12311223111221112122111311121211121211221221211121122111122 221 11112312 232212113321211121221221122 122311212222221112121111222122211111221131223322211321122132 12 1311222322 23122221111 121122 1121211211122121221111221222231112321121121323222322212221112312112112322232222 231232212211311112222212 21212222113111212 112121212121231222232 121211133322132112212131322212322 1 1221232122122211222122222222212211322122221321222 23111232121212 113112 123123 clustersFigure 2: Clusters for transitive, unaccusative, and ditransitiveneous causatives,' is rst split into two subclasseseven when only 9 clusters are in play, indicatingthat there is more variation in the syntactic patternswithin this single class than there is between almostany other arbitrarily-selected pair of classes.
It ishardly surprising that a collection labeled `miscella-neous' should contain such a wide variety of syntac-tic patterns.
What is surprising, and encouraging, isthe speed at which this methodology identies thismiscellany and moves to correct it.In one example of how previously-unclassiedverbs may be added to a VerbNet class, consider clus-ter 20 within the 90-cluster analysis.
This groupingis optimally identied with VerbNet class 36.3 (Meetclass), verbs of combative meetings.
It contains, un-surprisingly, verbs such as ght and consult.
Theclustering analysis adds, in addition, verbs such aspull.02 (phrasal: pull out) and withdraw.
The for-mer, like all phrasal verbs, is neglected by VerbNet,while the latter is considered to be part of the `re-move' classes (10.1 and 10.5).
Yet it might be just asnatural to think of withdraw in the sense of exitinga meeting or an engagement rather than removingsomething from somewhere.
For example, \It wasjust another one of the risk factors" that led to thecompany's decision to withdraw from the bidding, headded (wsj 0013).
Another verb placed into this clus-ter is hedge.
Impressionistically, this might appear tobe restricted to \hedge one's bets" and thus a poormatch for this fairly combative class.
Upon examina-tion of the actual usages, however, it becomes clearthat most of the time hedge is being used to meanprotect (from), as in But some investors might pre-fer a simpler strategy than hedging their individualholdings (wsj 1962) or Thus, buying puts after a bigmarket slide can be an expensive way to hedge againstrisk (wsj 2415).
Seen this way, hedge is a plausiblemember of this class, and since it is hitherto un-treated by VerbNet, the clustering analysis providesa valuable suggestion for an unexpected meaning.
Italso avoids erroneously attributing a change of loca-tion predicate to the sentence.For another example, consider cluster 25 of thesame 90-cluster analysis.
The verbs in this groupcontain verbs of directed motion such as put.01 andfunnel, and is identied with VerbNet class 9.3 (Fun-nel class).
The VerbNet description for this classspecies three arguments: Agent, Theme, and Des-tination.
The Destination role is further speciedas being restricted to those cases where the Themeand Destination end up in close contact.
Thus, thisclass includes usages such as put the toy in the boxbut not put the money towards a new car.
Anotherverb which is placed into this class is cast.01, whichis restricted to a small number of near-idioms suchas cast a pall (over) and cast doubt (on).
Because ofthis restriction, it has not previously been handledby VerbNet.
The clustering successfully includes thisin the same class as put and funnel, even includingthe selectional restriction on the types of Destina-tion allowed, for only over and on will be found withthis sense of cast.
This would also assign the correctsemantic predicates to cast.Figure 3: Outcome of clustering procedure5 Conclusion and Future DirectionsClustering of syntactic patterns can quickly andautomatically provide a rst approximation of thegroupings into which meaning-bearing items fall.The methodology easily captures low-level gen-eralizations and points out shortcomings in the'gold standard' verb classication against which itwas judged.
Nevertheless, it is only a rst passat transforming syntactic structure into semantics.Additional information is available to extend theparadigm.
For example, the syntactic patterns whichwere the subject of the clustering algorithm werenothing more than the linear order of the argu-ments of the verb.
Further renement on the syn-tactic structure would be possible, at the expense ofmultiplying the patterns.
For instance, in the pre-ceding all prepositions were stripped from the ar-gument structure, even though PropBank includesthe preposition as part of the argument label ratherthan as part of the argument itself.
Also, it wouldbe possible to label the arguments with thematicroles instead of the generic labels used by PropBank,when possible, thus providing a measure of semanticsmixed in with the syntax.
Kipper et al (submitted,2003) describes the process of mapping PropBankargument labels onto VerbNet thematic roles.
Cur-rently, over 64% of the PropBank argument labelshave been mapped automatically, and that is with-out recourse to the sense-disambiguated data.
Thissuggests that the two resources are highly compati-ble and that the combined data could easily be usedin the clustering methodology.
A larger measure ofmeaning could be added if some notion of the mean-ing of the verbs could be included in the clusteringas well.
For example, WordNet synonym sets, espe-cially as encoded in VerbNet, could be used as theinitial state of the clusters, which then could be mod-ied by the dis/similarity of the syntactic patternsattested by those verbs.
Such renements to the pro-cess would in themselves be steps towards integratingthe many factors that contribute to the understand-ing of meaning.ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The Berkeley FrameNet project.
InProceedings of the 17th International Conferenceon Computational Linguistics (COLING/ACL-98), pages 86{90, Montreal.
ACL.Hoa Trang Dang, Karin Kipper, Martha Palmer, andJoseph Rosenzweig.
1998.
Investigating regularsense extensions based on intersective levin classes.In Proceedings of ACL98, Montreal, Canada, Au-gust.Hoa Trang Dang, Karin Kipper, and Martha Palmer.2000.
Integrating compositional semantics intoa verb lexicon.
In Proceedings of the EighteenthInternational Conference on Computational Lin-guistics (COLING-2000), Saarbrucken, Germany,July-August.Christiane Fellbaum, editor.
1998.
WordNet: AnEletronic Lexical Database.
Language, Speech andCommunications.
MIT Press, Cambridge, Mas-sachusetts.Daniel Gildea.
2002.
Probabilistic models of verb-argument structure.
In Proceedings of the 19th In-ternational Conference on Computational Linguis-tics (COLING-02), pages 308{314, Taipei.Paul Kingsbury and Martha Palmer.
2002.
Fromtreebank to propbank.
In Proceedings of the 3rdInternational Conference on Language Resourcesand Evaluation (LREC-2002), Las Palmas, Ca-nary Islands, Spain.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class-based construction of a verb lexicon.In Proceedings of the Seventh National Conferenceon Articial Intelligence (AAAI-2000), Austin,TX, July-August.Beth Levin.
1993.
English Verb Classes and Alterna-tion, A Preliminary Investigation.
The Universityof Chicago Press.Mitch Marcus.
1994.
The penn treebank: A revisedcorpus design for extracting predicate-argumentstructure.
In Proceedings of the ARPA HumanLanguage Technology Workshop, Princeton, NJ,March.Diana McCarthy.
2000.
Using semantic preferencesto identify verbal participation in role switching al-ternations.
In Proceedings of the 1st Annual Meet-ing of the North American Chapter of the ACL(NAACL), pages 256{263, Seattle, Washington.Paola Merlo and Suzanne Stevenson.
2001.
Auto-matic verb classication based on statistical dis-tribution of argum ent structure.
ComputationalLinguistics, 27(3), September.George Miller.
1985.
Wordnet: A dictionarybrowser.
In Proceedings of the First InternationalConference on Information in Data, Waterloo,Ontario.M.
Moens and M. Steedman.
1988.
Temporal On-tology and Temporal Reference.
ComputationalLinguistics, 14:15{38.Sabine Schulte im Walde.
2000.
Clustering verbssemantically according to their alternation be-haviour.
In In Proceedings of the 18th Interna-tional Conference on Computational Linguistics(COLING-00), pages 747{753, Saarbrucken, Ger-many.
