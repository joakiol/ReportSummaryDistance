Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 39?47,Portland, Oregon, 23 June 2011. c?2011 Association for Computational LinguisticsDetecting Forum Authority Claims in Online DiscussionsAlex Marin, Bin Zhang, Mari OstendorfDepartment of Electrical EngineeringUniversity of Washington{amarin, binz}@uw.edu, mo@ee.washington.eduAbstractThis paper explores the problem of detectingsentence-level forum authority claims in on-line discussions.
Using a maximum entropymodel, we explore a variety of strategies forextracting lexical features in a sparse train-ing scenario, comparing knowledge- and data-driven methods (and combinations).
The aug-mentation of lexical features with parse con-text is also investigated.
We find that cer-tain markup features perform remarkably wellalone, but are outperformed by data-drivenselection of lexical features augmented withparse context.1 IntroductionIn multi-party discussions, language is used to es-tablish identity, status, authority and connectionswith others in addition to communicating informa-tion and opinions.
Automatically extracting thistype of social information in language from discus-sions is useful for understanding group interactionsand relationships.The aspect of social communication most ex-plored so far is the detection of participant role,particularly in spoken genres such as broadcastnews, broadcast conversations, and meetings.
Sev-eral studies have explored different types of fea-tures (lexical, prosodic, and turn-taking) in a vari-ety of statistical modeling frameworks (Barzilay etal., 2000; Maskey and Hirschberg, 2006; Liu, 2006;Liu and Liu, 2007; Vinciarelli, 2007; Laskowski etal., 2008; Hutchinson et al, 2010).
Typically, thesestudies assume that a speaker inhabits a role for theduration of the discussion, so multiple turns con-tribute to the decision.
Participant status is similaralthough the language of others is often more rele-vant than that of the participant in question.Communication of other types of social informa-tion can be more localized.
For example, an at-tempt to establish authority frequently occurs withina single sentence or turn when entering a discus-sion, though authority bids may involve multipleturns when the participant is challenged.
Simi-larly, discussion participants may align with or dis-tance themselves from other participants with a sin-gle statement, or someone could agree with one per-son at a particular point in the conversation and dis-agree with them at a different point.
Such localizedphenomena are also important for understanding thebroader context of that participant?s influence or rolein the conversation (Bunderson, 2003).In this paper, we focus on a particular type of au-thority claim, namely forum claims, as defined ina companion paper (Bender et al, 2011).
Forumclaims are based on policy, norms, or contextualrules of behavior in the interaction.
In our experi-ments, we explore the phenomenon using Wikipediadiscussion (?talk?)
pages, which are discussions as-sociated with a Wikipedia article in which changesto the article are debated by the editors in a series ofdiscussion threads.
Examples of such forum claimsare:?
I do think my understanding of Wikipedia andpolicy is better than yours.?
So it has all those things going for it, andI do think it complies with [[WP:V]] and39[[WP:WTA]].?
Folks, please be specific and accurate when you[[WP:CITE?cite your sources]].We treat each discussion thread as a unique ?conver-sation?.
Each contiguous change to a conversationis treated as a unique ?post?
or turn.
The dataset andannotation scheme are described in more detail inthe companion paper.Related previous work on a similar task focusedon detecting attempts to establish topic expertise inWikipedia discussions (Marin et al, 2010).
Theirwork used a different annotation process than thatwhich we build on here.
In particular, the anno-tation was performed at the discussion participantlevel, with evidence marked at the turn level with-out distinguishing the different types of claims as in(Bender et al, 2011).Treating the problem of detecting forum claims asa sentence-level classification problem is similar toother natural language processing tasks, such as sen-timent classification.
Early work in sentiment analy-sis used unigram features (Pang and Lee, 2004; Pangand Lee, 2005).
However, error analyses suggestedthat highly accurate sentiment classification requiresdeeper understanding of the text, or at least higherorder n-gram features.
Kim and Hovy (2006) usedunigrams, bigrams, and trigrams for extracting thepolarity of online reviews.
Gilbert et al (2009) em-ployed weighted n-grams together with additionalfeatures to classify blog comments based on agree-ment polarity.
We conjecture that authority claimdetection will also benefit from moving beyond uni-gram features.The focus of the paper is on two questions in fea-ture extraction:?
Can we exploit domain knowledge to addressovertraining issues in sparse data conditions??
Is parse context more effective than n-gramcontext?Our experiments compare the performance obtainedusing multiple methods for incorporating linguistic-or data-driven knowledge and context into the fea-ture space, relative to the baseline n-gram features.Section 2 describes the general classification archi-tecture.
Section 3 describes the various features im-plemented.
Experimental results are presented insection 4.
We conclude with some analysis in sec-tion 5 and remarks on future work in section 6.2 System DescriptionWe implement a classification system that assignsa binary label to each sentence in a conversation,indicating whether or not a forum authority claimis being made in that sentence.
To obtain higher-level decisions, we apply a simple rule that any postwhich contains at least one sentence-level forum au-thority claim should be labeled positive.
We usethe sentence-level system to obtain turn-level (post-level) decisions instead of training directly on thehigher-level data units because the forum claims arerelatively infrequent events.
Thus, we believe thatthe classification using localized features will yieldbetter results; when using higher-level classifica-tion units, the positive phenomena would be over-whelmed by the negative features in the rest of thesample, leading to poorer performance.Given a potentially large class imbalance due tothe sparsity of the positive-labeled samples, tuningon accuracy scores would lead to very low recall.Thus, we tune and evaluate on F-score, defined asthe harmonic mean of precision (the percent of de-tected claims that are correct) and recall (the percentof true claims that are detected).The classifier used is a maximum entropy clas-sifier (MaxEnt), implemented using the MALLETpackage (McCallum, 2002), an open-source java im-plementation.
MaxEnt models the conditional prob-ability distribution p(c|x) of a forum claim c giventhe feature vector x in a log-linear form.
Model pa-rameters ?
(c)i are estimated using gradient descenton the training data log likelihood with L2 regular-ization.Since our task is a two-class problem, and the ob-jective is the F-score, we use a classification deci-sion with decision threshold ?, i.e.c?
={true if p(true|x) > ?,false otherwise.where ?
is tuned on the development set, and theoptimal value is usually found to be much smallerthan 0.5.403 FeaturesPast work on various NLP tasks has shown thatlexical features can be quite effective in categoriz-ing linguistic phenomena.
However, using a largenumber of features when the number of labeledtraining samples is small often leads to overtrain-ing, due to the curse of dimensionality when deal-ing with high-dimensional feature spaces (Hastie etal., 2009).
Thus, we investigate two task-dependentmethods for generating lexical feature lists: a com-bined data- and knowledge-driven method using re-lated Wikipedia content, and a knowledge-drivenmethod requiring manual feature list generation.We conjecture that using unigram features aloneis often insufficient to capture the more complexphenomena associated with the forum claim detec-tion task.
Empirically, we find that even the wordfeatures most strongly correlated with the class vari-able are frequent in both classes.
In particular, dueto the class imbalance, such features are often moreprevalent in the negative class samples than the pos-itive class samples.
We believe that additional infor-mation about the context in which such words ap-pear in the data could be relevant for further increas-ing their discriminative power.One method often used in the literature to cap-ture the context in which a particular word appearsis to define the context as its neighboring words, e.g.by using higher-order n-grams (such as bigrams ortrigrams) or phrase patterns.
However, this methodalso suffers from the curse of dimensionality prob-lem, as seen from the feature set size increase for ourtraining set when moving beyond unigrams (listed intable 1.
)Features CountsUnigrams 13,899Bigrams 109,449Trigrams 211,580Table 1: N-gram feature statisticsTo understand the meaning of a sentence, featuresbased only on surface word forms may not be suf-ficient.
We propose an alternate method that aug-ments each word with information from the struc-ture of a parse tree for each sentence in which thatword appears.Additionally, we use a small set of other (non-lexical) features, motivated by anecdotal examplesfrom Wikipedia discussions.3.1 Generating Word Feature ListsWe propose two knowledge-assisted methods for se-lecting lexical features, as described below, both ofwhich are combined with data-driven selection ofthe most discriminative features based on mutual in-formation.3.1.1 Leveraging ?Parallel?
DataThe Wikipedia data naturally has ?parallel?
datain that each talk page is associated with an article,and there are additional pages that describe forumpolicies and norms of behavior.
By comparing arti-cle and talk pages, one can extract words that tend tobe associated with editor discussions (words whichhave high TF-IDF in a discussion but low TF-IDF inthe associated article).
By comparing to the poli-cies pages, one can identify words that are likelyto be used in policy-related forum claims (wordswith high average TF-IDF in the corpus of policyand norms of behavior pages.)
To select a singlereduced set of words, we pick only the words withsufficiently high TF-IDF in the discussion pages.
Inpractice, to avoid tuning additional parameters, weselected the settings which yielded the largest list(with approximately 520 words) and let the featureselection process trim down the list.
Some wordsidentified by the feature selection process include:?
words shared with the knowledge-driven list(discussed below): wikipedia, policy, sources,guidelines, reliable, rules, please?
relevant words not appearing in the knowledge-driven list: categories, pages, article, wiki,editing?
other words: was, not, who, is, see3.1.2 Knowledge-Driven Word ListThe knowledge-driven method uses lists of wordspicked by trained linguists who developed the guide-lines for the process of annotating our dataset.
Sixlists were developed, containing keywords and shortphrases related to:41?
behavior in discussion forums (reliable, re-spectful, balanced, unacceptable)?
politeness (please, would you, could you,would you mind)?
positioning and expressing neutrality (point ofview, neutral, opinion, bias, good faith)?
accepted practices in discussion forums (prac-tice, custom, conflict, consensus)?
sourcing information (source, citing, rules, pol-icy, original research)?
Wikipedia-specific keywords (wikipedia, ad-ministrator, registered, unregistered)In all our experiments, the various word lists wereconcatenated and used as a single set of 75 words.Phrases were treated as single keywords for pur-poses of feature extraction, i.e.
a single feature wasextracted for each phrase.
If another word on the listwere a substring of a given phrase, and the phrasewere found to appear in the text of a given sample,both the single word and the phrase were kept in thatsample.3.2 Adding Higher-Level Linguistic ContextAs an alternative to using n-grams as lexical context,we propose using syntactic context, represented byinformation about the parse tree of each sentence inthe data.
Given the low amount of available trainingdata, learning n-gram features we believe is likely toovertrain, due to the combinatorial explosion in thefeature space.
On the other hand, adding parse treecontext information to each feature results in a muchsmaller increase in feature space, due to the smallernumber of non-terminal tokens as compared to thevocabulary size.
To extract such features, the datawas run through a version of the Berkeley parser(Petrov et al, 2006) trained on the Wall Street Jour-nal portion of the Penn Treebank.For each sentence, the one-best parse was used toextract the list of non-terminals above each word inthe sequence.
The list was then filtered to a shortersubset of non-terminal tags.
The words augmentedwith non-terminal parse tree tags were treated as in-dividual features and used in the usual way.
We useda context of at most three non-terminal tags (i.e.
thePOS tag and two additional levels if present.
)For simplicity, multi-word phrases from theknowledge-driven word list were either removed en-tirely, or split with each word augmented indepen-dently.
Using this method resulted in the featurecounts shown in table 2.
In particular, we see thatsplitting phrases instead of removing them resultsin almost twice as many parse-augmented word fea-tures, in great part due to function words appearingin a variety of unrelated contexts.Features CountsAll unigrams 38,384Data-driven list 5,935Knowledge-driven list, no phrases 504Knowledge-driven list, split phrases 908Table 2: Parse feature statistics3.3 Other FeaturesWe use a number of additional features not directlyrelated to lexical cues.
We extract the following sen-tence complexity features:?
the length of the sentence?
the average length of the 20% longest words inthe sentenceAdditionally, we use a number of other features mo-tivated by our analysis of the data.
These featuresare:?
the number of words containing only upper-case letters in that sentence?
the number of (external) URLs in the sentence?
the number of links to Wikipedia pages con-taining norms of forum behavior or policies?
the number of other Wikipedia-internal links4 Experiments4.1 Dataset and ProcedureWe use data from the Authority and Alignment inWikipedia Discussions (AAWD) corpus describedin our companion paper (Bender et al, 2011).
Thedataset contains English Wikipedia discussions an-notated with authority claims by four annotators.Not all the discussions are annotated by multiple an-notators.
Thereby in the train/dev/eval split, we se-lect most of the discussions that are multiply anno-tated for the dev and eval sets.
The statistics of eachset are shown in table 3.42Train Dev Eval# files 226 56 55# sentences 17512 4990 4200Table 3: Data statisticsA number of experiments were conducted to as-sess the performance of the various feature typesproposed.
We evaluate the effect of individual fea-tures when used in a MaxEnt classifier, as well ascombined features.We tune the number of features selected by themutual information between a feature and the classlabels, which is a common approach applied in textcategorization (Yang and Pedersen, 1997).
Fea-ture selection and parameter tuning of the decisionthreshold ?
are performed independently for eachcondition.
We include the number of features se-lected in each case alongside the results.
The per-formance of the various systems described in thispaper is evaluated using F-score.
The numbers cor-responding to the overall best performance obtainedon the dev and eval sets are highlighted in boldfacein the appropriate table.4.2 N-gram FeaturesFirst, we examine the performance of lexical fea-tures extracted at different n-gram lengths.
We usedmaximum n-gram sizes 1, 2, and 3, and the countsof n-grams were used as features for MaxEnt.
Theresults are summarized in table 4.Maximum # selected Dev Evaln-gram length features1 50 0.321 0.2702 50 0.331 0.3003 20 0.333 0.290Table 4: N-gram feature results4.3 ?Smart?
Word FeaturesThe second set of experiments compares the perfor-mance of various methods of selecting unigram lex-ical features.
We compare using the full vocabularywith the two selection methods, outlined in section3.1.
The combination of the two simpler selectionmethods was also examined, under the assumptionthat the parallel-data-driven features may be morecomplete, but also more likely to overtrain, sincethey were derived directly from the data.
The resultsare summarized in table 5.Feature # selected Dev EvalfeaturesAll words 50 0.321 0.270Parallel corpus words 10 0.281 0.231Hand-picked words 50 0.340 0.272Parallel corpus + 100 0.303 0.259hand-picked wordsTable 5: Smart word feature results4.4 Parse-Augmented FeaturesA third set of experiments examines the effect ofadding parsing-related context to the features.
Weuse the same set of features as in section 3.2.
For theknowledge-driven features, we present both versionsof the parse features, the one in which phrases weresplit into their constituent words before augmenta-tion with parse features, and the one from whichphrases were removed altogether.
The results aresummarized in table 6.Word list to # selected Dev Evalderive features from featuresAll words 50 0.352 0.445Parallel corpus words 20 0.336 0.433Hand-picked words 50 0.314 0.306(no phrases)Hand-picked words 50 0.328 0.310(split phrases)Parallel corpus +hand-picked words 50 0.367 0.457(no phrases)Parallel corpus +hand-picked words 50 0.359 0.450(split phrases)Table 6: Parse-augmented feature resultsWe perform a small empirical analysis of featuresin the model with parse-augmented features for allwords.
Table 7 contains some of the most com-mon features, their counts for each class, and model43weight (if selected.)
As expected, the feature withthe highest relative frequency in the positive classgets the highest model weight.
Other features withhigh absolute frequency in the positive class also getsome positive weight.
All other features are dis-carded during model training.Feature # # Weightfalse trueWikipedia NNP NP PP 60 10 1.035Wikipedia NNP NP S 57 12 1.121Wikipedia NNP NP NP 26 16 1.209Wikipedia NNP NP VP 13 3 -Wikipedia JJ NP NP 6 0 -Wikipedia NNP NP FRAG 1 3 2.115Table 7: Parse feature examples4.5 Other FeaturesA fourth set of experiments shows the effect ofWikipedia-specific markup features described inSection 4.5.
The results for the Wikipedia policypage feature are listed in table 8.
The other featureswere found to not be useful, resulting in F-scores ofless than 0.1.Feature Dev EvalWikipedia policy page 0.341 0.622Table 8: Other feature results4.6 Combined FeaturesThe previous sets of experiments reveal that the fea-ture of links to Wikipedia policy page is the mostdiscriminative individual feature.
Therefore, in thenext set of experiments, we combine other featureswith the Wikipedia policy page feature to train Max-Ent models.
We did not include any of the other fea-tures whose results were summarized in section 4.5,due to their very low individual performance.
Theresults are shown in table 9.4.7 Turn-level ClassificationWe propagate the sentence-level classification out-put to the turn-level if that turn has at least one sen-tence classified as forum claim.
For simplicity, in-stead of running experiments on all the feature con-Features other than # selectedWikipedia policy features Dev Evalpage markupN-gram featuresunigram 20 0.448 0.550unigram + bigram 50 0.447 0.551unigram + bigram 100 0.446 0.596+ trigramSmart word featuresParallel corpus words 20 0.427 0.483Hand-picked words 50 0.468 0.596Parallel corpus + 100 0.451 0.569hand-pickedParse-augmented featuresAll words 50 0.398 0.610Parallel corpus words 100 0.381 0.623Hand-picked words 20 0.392 0.632(no phrases)Hand-picked words 100 0.392 0.558(split phrases)Parallel corpus +hand-picked words 50 0.400 0.596(no phrases)Parallel corpus +hand-picked words 50 0.398 0.607(split phrases)Table 9: Combined feature resultsfigurations, we use only the one that provides thehighest dev set F-score, which is the MaxEnt clas-sifier with Wikipedia policy page markup and hand-picked keyword features combined.
The resultingF-score is 0.57 for the development set and 0.66 forthe evaluation set.5 Discussion5.1 Data VariabilityOne of the most notable observations in the exper-iments above is the high degree of data variabil-ity.
A simple rule-based classifier that uses only theWikipedia policy page markup feature gives the bestresults on the evaluation set, but it is not nearly aseffective on the development set.
Simply put, themarkup is a reliable cue when it is available, but itis not always present.
Table 10 demonstrates this44through the precision and recall results of the devand eval sets.
The variability also extends to the util-ity of parse features.Dev EvalPrecision 0.703 0.862Recall 0.225 0.487Table 10: Precision and recall of the rule-based systemTo better understand this issue, we reran the bestcase configurations on the dev and eval sets withthe role of the dev and eval sets reversed, i.e.
us-ing the eval set for feature selection.
For the bestcase configuration on the dev set (Wikipedia policypage markup and hand-picked keywords), 50 and 20features are selected when tuned on dev and evalsets, respectively, and the latter feature set is a sub-set of the former one.
For the best case configurationon the eval set (Wikipedia policy page markup andparse-augmented features derived from hand-pickedwords without phrases), the same 20 features are se-lected when tuned on dev or eval sets.
For eachconfiguration, the combined feature set from thetwo different selection experiments was then usedto train a new model, which was evaluated on thecombined dev and eval test sets.
The precision/recalltrade-off is illustrated in figure 1, which can be com-pared to a precision of 0.78 and recall of 0.32 usingthe rule-based system on the two test sets combined.While this is a ?cheating experiment?
in that the testdata was used in feature selection, it gives a bet-ter idea of the potential gain from parse-augmentedlexical features for this task.
From the figure, bothbest-case configurations outperform the rule-basedsystem, and an operating point with more balancedprecision and recall can be chosen.
Furthermore, thesystem with parse-augmented features is able to op-erate at a high recall while still maintaining reason-able precision, which is desirable in some applica-tions.5.2 Feature AnalysisThe variability of data in this task poses challengesfor learning features that improve over a simpleknowledge-driven baseline.
However, the results insection 4 provide some insights.First, unigram features alone provide poor perfor-0.0 0.2 0.4 0.6 0.8 1.0Precision0.00.20.40.60.81.0Recallpolicy markup & hand-picked wordspolicy markup & hand-pickedwords+parse w/o phrasespolicy markup onlyFigure 1: Precision-recall curvemance.
Adding bigrams improves the performanceon both the development and the evaluation sets,while further adding trigrams degrades the eval setperformance.
This indicates that there are some dis-criminative high-order n-grams, but also too manynoisy n-grams to extract the discriminative n-gramseffectively with a small amount of training data.The smarter word features do not perform as wellas n-gram features when used alone (i.e.
as uni-grams), but they provide an improvement over n-grams when used with parse features.
With parsefeatures, the parallel corpus words are more effec-tive than the hand-picked words, but the best per-formance is achieved with the combination.
Whencombined with the Wikipedia policy page markupfeatures, the hand-picked words are the most useful,with the best eval set results obained with the parse-augmented version.Overall, the best performance seems to beobtained by using the combined feature set ofWikipedia policy page markup and hand-pickedkeyword features with parse augmentation.
How-ever, the test set variability discussed in section 5.1suggests that it would be useful to assess the findingson additional data.5.3 Further ChallengesBy definition, a forum authority claim is composedof a mention ofWikipedia norms and policies to sup-45port a previously-mentioned opinion proposed bythe participant.
While the detection of mentions ofWikipedia norms is relatively easy, we conjecturethat part of the difficulty of this task lies in identify-ing whether a mention of Wikipedia norms is for thepurpose of supporting an opinion, or just a mentionas part of the general conversation.
For example, theWikipedia policy neutral point of view (NPOV) is afrequently used term in talk pages.
It can be usedas support for the participant?s suggested modifica-tion, or it can be just a mention of the policy withoutthe purpose of supporting any opinion.
For example,the sentence This section should be deleted becauseit violates NPOV is a forum claim, because the termNPOV is used to support the participant?s request.However, the sentence Thank you for removing theNPOV tag is not a forum claim, as the participantis not presenting any opinion.
For these reasons, thewordNPOV alone does not provide enough informa-tion for reliable decisions; contextual information,such as n-grams and parse-augmented features, mustbe explored.
On the other hand, a direct reference toa Wikipedia policy page is much less ambiguous, asit is almost always used in the context of strengthen-ing an opinion or claim.Another factor that makes the task challenging isthe sparsity of the data.
It is time-consuming to pro-duce high quality annotations for forum claims, asmany claims are subtle and therefore difficult to de-tect, even by human annotators.
Given the limitedamount of data, many features have low occurrencesand cannot be learned properly.
The data sparsityis an even bigger problem when the feature spaceis increased, for example by using contextual fea-tures such as n-grams and parse-augmented words.On the other hand, while it may be easier to capturethe mention of Wikipedia policies using a limitedset of keywords or phrases, it is difficult to modelthe behavior of presenting an opinion when the datais sparse, as the following forum claim examplesshow:?
I think we can all agree that this issue bearsmentioning, however the blurb as it stands isdecidedly not NPOV, nor does it fit the format-ting guidelines for a Wikipedia article.?
As a reminder, the threshold for inclusion inWikipedia is whether material is attributable toa reliable published source, not whether it istrue.?
If you think that some editor is violating NPOV,you can pursue dispute resolution, but it?s nojustification for moving or removing valid in-formation.?
If you?d like to talk the position that quotes frompeople?s opinions do not belong here, fine, butit is extremely POV to insist only on eliminat-ing editorials that you disagree with, while notchallenging quotes from your own POV.The examples above require deeper understandingof the sentences to identify the embedding of opin-ions.
Modeling such phenomena using word-basedcontextual features when the training data is sparseis particularly hard.
Even with parse-augmented fea-tures that do not increase the feature dimensionalityas fast as n-grams, a certain amount of data is neededto obtain reliable statistics.
Clustering of the featuresinto a lower dimensional space would provide onepossible solution to this issue, but how the cluster-ing can be done robustly remains an open question.6 ConclusionsWe have presented systems to detect forum authorityclaims, which are claims of credibility using forumnorms, in Wikipedia talk pages.
The Wikipedia pol-icy page markup feature was found to be the most ef-fective individual feature for this task.
We have alsodeveloped approaches to further improve the perfor-mance by knowledge-driven selection of lexical fea-tures and adding context in the form of parse infor-mation.Future work includes extending the contextualfeatures, such as parse-augmented word features, toother types of linguistic information, and automat-ically learning the types of contexts that might bemost useful for each word.
Feature clustering meth-ods will also be investigated, in order to reduce fea-ture space dimensionality and deal with data spar-sity.
To improve the effectiveness of the parse fea-tures, domain adaptation of the parser or use of aparser trained on data closer matched to our targetdomain could be investigated.
We will also plan toextend this work to other types of authority claims in46Wikipedia and to other multi-party discussion gen-res.AcknowledgmentsThis research was funded by the Office of the Di-rector of National Intelligence (ODNI), IntelligenceAdvanced Research Projects Activity (IARPA).
Allstatements of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the official views or poli-cies of IARPA, the ODNI or the U.S. Government.ReferencesR.
Barzilay, M. Collins, J. Hirschberg, and S. Whittaker.2000.
The rules behind roles: Identifying speaker rolein radio broadcasts.
In Proceedings of AAAI, pages679?684.E.
M. Bender, J. Morgan, M. Oxley, M. Zachry,B.
Hutchinson, A. Marin, B. Zhang, and M. Osten-dorf.
2011.
Annotating social acts: Authority claimsand alignment moves in wikipedia talk pages.
In Pro-ceedings of ACL ?
Workshop on Language in SocialMedia.J.
S. Bunderson.
2003.
Recognizing and utilizing exper-tise in work groups: A status characteristics perspec-tive.
Administrative Science Quarterly, 48(4):557?591.E.
Gilbert, T. Bergstrom, and K. Karahalios.
2009.
BlogsAre Echo Chambers: Blogs Are Echo Chambers.
InProceedings of HICSS, pages 1?10.T.
Hastie, R. Tibshirani, and J. Friedman.
2009.
TheElements of Statistical Learning: Data Mining, Infer-ence, and Prediction, Second Edition.
Springer Seriesin Statistics.
Springer, September.B.
Hutchinson, B. Zhang, and M. Ostendorf.
2010.
Un-supervised broadcast conversation speaker role label-ing.
In Proceedings of ICASSP, pages 5322?5325.S.
M. Kim and E. Hovy.
2006.
Automatic identificationof pro and con reasons in online reviews.
In Proceed-ings of COLING-ACL, pages 483?490.K.
Laskowski, M. Ostendorf, and T. Schultz.
2008.Modeling vocal interaction for text-independent par-ticipant characterization in multi-party conversation.In ISCA/ACL SIGdial Workshop on Discourse and Di-alogue, pages 194?201.F.
Liu and Y. Liu.
2007.
Soundbite identification usingreference and automatic transcripts of broadcast newsspeech.
In Proceedings of ASRU, pages 653?658.Y.
Liu.
2006.
Initial study on automatic identification ofspeaker role in broadcast news speech.
In Proceedingsof HLT, pages 81?84.A.
Marin, M. Ostendorf, B. Zhang, J. T. Morgan, M. Ox-ley, M. Zachry, and E. M. Bender.
2010.
Detectingauthority bids in online discussions.
In Proceedings ofSLT, pages 49?54.S.
Maskey and J. Hirschberg.
2006.
Soundbite detectionin broadcast news domain.
In Proceedings of Inter-speech, pages 1543?1546.A.
K. McCallum.
2002.
MALLET: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.B.
Pang and L. Lee.
2004.
A Sentimental Education:Sentiment Analysis Using Subjectivity SummarizationBased on Minimum Cuts.
In Proceedings of ACL,pages 271?278.B.
Pang and L. Lee.
2005.
Seeing stars: exploiting classrelationships for sentiment categorization with respectto rating scales.
In Proceedings of ACL, pages 115?124.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proceedings of COLING-ACL, pages433?440.A.
Vinciarelli.
2007.
Speakers role recognition in mul-tiparty audio recordings using social network analy-sis and duration distribution modeling.
IEEE Transac-tions on Multimedia, 9(6):1215?1226.Y.
Yang and J. O. Pedersen.
1997.
A Comparative Studyon Feature Selection in Text Categorization.
In Pro-ceedings of ICML, pages 412?420.47
