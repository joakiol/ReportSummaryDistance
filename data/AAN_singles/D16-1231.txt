Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2133?2143,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAddressee and Response Selection for Multi-Party ConversationHiroki OuchiNara Institute of Science and Technologyouchi.hiroki.nt6@is.naist.jpYuta TsuboiIBM Research - Tokyoyutat@jp.ibm.comAbstractTo create conversational systems working inactual situations, it is crucial to assume thatthey interact with multiple agents.
In thiswork, we tackle addressee and response se-lection for multi-party conversation, in whichsystems are expected to select whom they ad-dress as well as what they say.
The key chal-lenge of this task is to jointly model who istalking about what in a previous context.
Forthe joint modeling, we propose two model-ing frameworks: 1) static modeling and 2) dy-namic modeling.
To show benchmark resultsof our frameworks, we created a multi-partyconversation corpus.
Our experiments on thedataset show that the recurrent neural networkbased models of our frameworks robustly pre-dict addressees and responses in conversationswith a large number of agents.1 IntroductionShort text conversation (STC) has been gaining pop-ularity: given an input message, predict an appropri-ate response in a single-round, two-party conversa-tion (Wang et al, 2013; Shang et al, 2015).
Model-ing STC is simpler than modeling a complete con-versation, but instantly helps applications such aschat-bots and automatic short-message replies (Ji etal., 2014).Beyond two-party conversations, there is also aneed for modeling multi-party conversation, a formof conversation with several interlocutors convers-ing with each other (Traum, 2003; Dignum andVreeswijk, 2003; Uthus and Aha, 2013).
For exam-ple, in the Ubuntu Internet Relay Chat (IRC), sev-Figure 1: Addressee and response selection for multi-partyconversation.
A SYSTEM is required to select an appropriateaddressee from the interlocutors in the conversational contextand an appropriate response from the fixed set of candidates.eral users cooperate to find a solution for a techni-cal issue contributed by another user.
Each agentmight have one part of the solution, and these pieceshave to be combined through conversation in orderto come up with the whole solution.A unique issue of such multi-party conversationsis addressing, a behavior whereby interlocutors in-dicate to whom they are speaking (Jovanovic?
andAkker, 2004; Akker and Traum, 2009).
In face-to-face communication, the basic clue for speci-fying addressees is turning one?s face toward theaddressee.
In contrast, in voice-only or text-based communication, the explicit declaration of ad-dressee?s names is more common.In this work, we tackle addressee and responseselection for multi-party conversation: given a con-text, predict an addressee and response.
As Fig-ure 1 shows, a system is required to select an ad-dressee from the agents appearing in the previouscontext and a response from a fixed set of candidateresponses (Section 3).2133The key challenge for predicting appropriate ad-dressees and responses is to jointly capture whois talking about what at each time step in a con-text.
For jointly modeling the speaker-utterance in-formation, we present two modeling frameworks:1) static modeling and 2) dynamic modeling (Sec-tion 5).
While speakers are represented as fixedvectors in the static modeling, they are representedas hidden state vectors that dynamically changewith time steps in the dynamic modeling.
In prac-tice, our models trained for the task can be appliedto retrieval-based conversation systems, which re-trieves candidate responses from a large-scale repos-itory with the matching model and returns the high-est scoring one with the ranking model (Wang et al,2013; Ji et al, 2014; Wang et al, 2015).
Our trainedmodels work as the ranking model and allow theconversation system to produce addressees as wellas responses.To evaluate the trained models, we provide a cor-pus and dataset.
By exploiting Ubuntu IRC Logs1,we build a large-scale multi-party conversation cor-pus, and create a dataset from it (Section 6).
Ourexperiments on the dataset show the models instanti-ated by the static and dynamic modeling outperforma strong baseline.
In particular, the model based onthe dynamic modeling robustly predicts appropriateaddressees and responses even if the number of in-terlocutors in a conversation increases.2We make three contributions in this work:1.
We formalize the task of addressee and re-sponse selection for multi-party conversation.2.
We present modeling frameworks and the per-formance benchmarks for the task.3.
We build a large-scale multi-party conversationcorpus and dataset for the task.2 Related WorkThis work follows in the footsteps of Ritter et al(2011), who tackled the response generation prob-lem: given a context, generate an appropriate re-sponse.
While previous response generation ap-1http://irclogs.ubuntu.com/2Our code, corpus, and dataset are publicly available athttps://github.com/hiroki13/response-rankingproaches utilize statistical models on top of heuris-tic rules or templates (Levin et al, 2000; Young etal., 2010; Walker et al, 2003), they apply statisticalmachine translation based techniques without suchheuristics, which leads to recent work utilizing theSMT-based techniques with neural networks (Shanget al, 2015; Vinyals and Le, 2015; Sordoni et al,2015; Serban et al, 2016).As another popular approach, retrieval-basedtechniques are used to retrieve candidate responsesfrom a repository and return the highest scoring onewith the ranking model (Ji et al, 2014; Wang et al,2015; Hu et al, 2014; Wang et al, 2013; Lu and Li,2013).
Stemming from this approach, the next utter-ance classification (NUC) task has been proposed, inwhich a system is required to select an appropriateresponse from a fixed set of candidates (Lowe et al,2015; Kadlec et al, 2015).
The NUC is regarded asfocusing on the ranking problem of retrieval-basedsystem, since it omits the candidate retrieving step.The merit of NUC is that it allows us to easily evalu-ate the model performance on the basis of accuracy.Our proposed addressee and response selectiontask is an extension of the NUC.
We generalize thetask by integrating the addressee detection, whichhas been regarded as a problematic issue in multi-party conversation (Traum, 2003; Jovanovic?
andAkker, 2004; Uthus and Aha, 2013).
Basically,the addressee detection has been tackled in thespoken/multimodal dialog system research, and themodels largely rely on acoustic signal or gaze infor-mation (Jovanovic?
et al, 2006; Akker and Traum,2009; Ravuri and Stolcke, 2014).
This current workis different from such previous work in that our mod-els predict addressees with only textual information.For predicting addressees or responses, how thecontext is encoded is crucial.
In single-round con-versation, a system is expected to encode only oneutterance as a context (Ritter et al, 2011; Wang etal., 2013).
In contrast, in multi-turn conversation,a system is expected to encode multiple utterances(Shang et al, 2015; Lowe et al, 2015).
Very re-cently, individual personalities have been encodedas distributed embeddings used for response genera-tion in two-party conversation (Li et al, 2016).
Ourwork is different from that work in that our proposedpersonality-independent representation allows us tohandle new agents unseen in the training data.2134Type NotationInputResponding Agent aresContext CCandidate Responses ROutput Addressee a ?
A(C)Response r ?
RTable 1: Notations for the ARS task.3 Addressee and Response SelectionWe propose and formalize the task of addressee andresponse selection (ARS) for multi-party conversa-tion.
The ARS task assumes the situation where aresponding agent gives a response to an addresseefollowing a context.3NotationTable 1 shows the notations for the formalization.We denote vectors with bold lower-case (e.g.
xt, h),matrices with bold upper-case (e.g.
W,Ha), scalarswith italic lower-case or upper-case (e.g.
am, Q),and sets with bold italic lower-case or cursive upper-case (e.g.
x, C) letters.FormalizationGiven an input conversational situation x, an ad-dressee a and a response r are predicted:GIVEN : x = (ares, C, R)PREDICT : a, rwhere ares is a responding agent, C is a context andR is a set of candidate responses.
The context C isa sequence of previous utterances up to the currenttime step T :C = (ua1,1, ?
?
?
,uaT ,T )where uat,t is an utterance given by an agent at at atime step t. Each utterance uat,t is a sequence of Nttokens:uat,t = (wat,t,1, ?
?
?
, wat,t,Nt)where wat,t,n is a token index in the vocabulary V .3In actual situations, responses can be addressed to multipleagents.
In this work, we assume the situation where one specificagent can be the addressee of a response.To predict an addressee a as a target output, weselect an agent from a set of the agents appearing ina context A(C).
Note that a ground-truth addresseeis always included in A(C).
To predict an appropri-ate response r, we select a response from a set ofcandidate responses R, which consists of Q candi-dates:R = {r1, ?
?
?
, rQ}rq = (wq,1, ?
?
?
, wq,Nq)where rq is a candidate response, which consists ofNq tokens, and wq,n is an token index in the vocab-ulary V .4 Dual Encoder ModelsOur proposed models are extensions of the dualencoder (DE) model in (Lowe et al, 2015).
TheDE model consists of two recurrent neural networks(RNN) that respectively compute the vector repre-sentation of an input context and candidate response.A generic RNN, with input xt ?
Rdw and recur-rent state ht ?
Rdh , is defined as:ht = f(ht?1,xt) = pi(Whht?1 +Wxxt) (1)where pi is a non-linear function,Wx ?
Rdh?dw is aparameter matrix for xt, Wh ?
Rdh?dh is a param-eter matrix for ht?1, and the recurrence is seededwith the 0 vector, i.e.
h0 = 0.
The recurrent stateht acts as a compact summary of the inputs seen upto time step t.In the DE model, each word vector of the con-text C and the response rq is consumed by eachRNN, and is then summarized into the context vec-tor hc ?
Rdh and the response vector hq ?
Rdh .
Us-ing these vectors, the model calculates the probabil-ity that the given candidate response is the ground-truth response given the context as follows:Pr(y(rq) = 1|C, rq) = ?
(hTc Whq) (2)where y is a binary function mapping from rq to{0, 1}, in which 1 represents the ground-truth sam-ple and 0 represents the false one, ?
is the logisticsigmoid function, and W ?
Rdh?dh is a parametermatrix.
As extensions of this model, we propose ourmulti-party encoder models.21355 Multi-Party Encoder ModelsFor capturing multi-party conversational streams,we jointly encode who is speaking what at each timestep.
Each agent and its utterance are integrated intothe hidden states of an RNN.We present two multi-party modeling frame-works: (i) static modeling and (ii) dynamic mod-eling, both of which jointly utilize agent and ut-terance representation for encoding multiple-partyconversation.
What distinguishes the models is thatwhile the agent representation in the static modelingframework is fixed, the one in the dynamic modelingframework changes along with each time step t in aconversation.Modeling FrameworksAs an instance of the static modeling, we propose astatic model to capture the speaking-orders of agentsin conversation.
As an instance of the dynamic mod-eling, we propose a dynamic model using an RNNto track agent states.
Note that the agent represen-tations are independent of each personality (uniqueuser).
The personality-independent representationallows us to handle new agents unseen in the trainingdata.Formally, similar to Eq.
2, both of the modelscalculate the probability that the addressee ap or re-sponse rq is the ground-truth given the input x:Pr(y(ap) = 1|x) = ?
([ares ; hc]T Wa ap) (3)Pr(y(rq) = 1|x) = ?
([ares ; hc]T Wr hq) (4)where y is a binary function mapping from ap orrq to {0, 1}, in which 1 represents the ground-truthsample and 0 represents the false one.
The func-tion ?
is the logistic sigmoid function.
ares ?
Rdais a responding agent vector, ap ?
Rda is a candi-date addressee vector, hc ?
Rdh is a context vector,hq ?
Rdh is a candidate response vector.
These vec-tors are respectively defined in each model.
Wa ?R(da+dh)?dh is a parameter matrix for the addresseeselection probability, and Wr ?
R(da+dh)?dh is aparameter matrix for the response selection proba-bility.
These model parameters are learned duringtraining.On the basis of Eqs.
3 and 4, a resulting addresseeFigure 2: Illustrative example of our static model.and response are selected as follows:a?
= argmaxap?A(C)Pr(y(ap) = 1|x) (5)r?
= argmaxrq?RPr(y(rq) = 1|x) (6)where a?
is the highest probability addressee of a setof agents in the context A(C), and r?
is the highestprobability response of a set of candidate responsesR.5.1 A Static ModelIn the static model, agent matrixA is defined for theagent vectors in Eqs.
3 and 4.
This agent matrix canbe defined arbitrarily.
We define the agent matrix Aon the basis of agents?
speaking orders.
Intuitively,the agents that spoke in recent time steps are morelikely to be an addressee.
Our static model capturessuch property.The static model is shown in Figure 2.
First,agents in the context A(C) and a responding agentares are sorted in descending order based on eachlatest speaking time.
Then the order is assigned asan agent index am ?
(1, ?
?
?
, |A(C)|) to each agent.In the table shown in Figure 2, the responding agent(represented as SYSTEM) has the agent index 1 be-cause he spoke at the most recent time step t = 6.Similarly, User 1 has the index 2 because he spokeat the second most recent time step t = 5, and User2 has the index 3 because he spoke at the third t = 3.Each speaking-order index am is associated withthe am-th column of the agent matrixA:am = A[?, am]2136Figure 3: Illustrative example of our dynamic model.Similarly, a responding agent vector ares and a can-didate addressee vector ap in Eqs.
3 and 4 are re-spectively extracted from A, i.e.
ares = A[?, ares]and ap = A[?, ap].Consuming the agent vectors, an RNN updates itshidden state.
For example, at the time step t = 1 inFigure 2, the agent vector a1 of User 1 is extractedfrom A on the basis of agent index 2 and then con-sumed by the RNN.
Then, the RNN consumes eachword vector w of User 1?s utterance.
By consum-ing the agent vector before word vectors, the modelcan capture which agent speaks the utterance.
Thelast state of the RNN is regarded as hc.
As the tran-sition function f of RNN (Eq.
1), we use the GatedRecurrent Unit (GRU) (Cho et al, 2014; Chung etal., 2014).For the candidate response vector hq, each wordvector (wq,1, ?
?
?
,wq,Nq) in the response rq is sum-marized with the RNN.
Using these vectors ares, ap,hc, and hq, we predict a next addressee and responsewith the Eqs.
3 and 4.5.2 A Dynamic ModelIn the static model, agent representation A is afixed matrix that does not change in a conversationalstream.
In contrast, in the dynamic model, agentrepresentation At tracks each agent?s hidden statewhich dynamically changes with time steps t.Figure 3 shows the overview of the dynamicmodel.
Initially, we set a zero matrix as initial agentstateA0, and each column vector of the agent matrixcorresponds to an agent hidden state vector.
Then,each agent state is updated by consuming the utter-ance vector at each time step.
Note that the statesof the agents that are not speaking at the time areupdated by zero vectors.Formally, each column of At corresponds to anagent state vector:am,t = At[?, am]where an agent state vector am,t of an agent am at atime step t is the am-th column of the agent matrixAt.Each vector of the matrix is updated at each timestep, as shown in Figure 3.
An agent state vectoram,t ?
Rda for each agent am at each time step t isrecurrently computed:am,t = g(am,t?1,um,t), am,0 = 0where um,t ?
Rdw is a summary vector of an ut-terance of an agent am and computed with an RNN.As the transition function g, we use the GRU.
Forexample, at a time step t = 2 in Figure 3, the agentstate vector a1,2 is influenced by its utterance vectoru1,2 and updated from the previous state a1,1.The agent matrix updated up to the time step T isdenoted as AT , which is max-pooled and used as asummarized context vector:hc = maxi AT [i]The agent matrix AT is also used for a respondingagent vector ares and a candidate addressee vectorap, i.e.
ares = AT [?, ares] and ap = AT [?, ap].
rqis summarized into a response vector hq in the sameway as the static model.5.3 LearningWe train the model parameters by minimizing thejoint loss function:L(?)
= ?
La(?)
+ (1 ?
?)
Lr(?)
+?2 ||?||2whereLa is the loss function for the addressee selec-tion, Lr is the loss function for the response selec-tion, ?
is the hyper-parameter for the interpolation,and ?
is the hyper-parameter for the L2 weight de-cay.2137Figure 4: The flow of the corpus and dataset creation.
From theoriginal logs, we extract addressee IDs and add them to the cor-pus.
As the dataset, we add candidate responses and the labels.For addressee and response selection, we use thecross-entropy loss functions:La(?)
= ?
?n[ log Pr(y(a+) = 1|x)+ log (1 ?
Pr(y(a?)
= 1|x) ]Lr(?)
= ?
?n[ log Pr(y(r+) = 1|x)+ log (1 ?
Pr(y(r?)
= 1|x) ]where x is the input set for the task, i.e.
x =(ares, C,R), a+ is a ground-truth addressee, a?
is afalse addressee, r+ is a ground-truth response, andr?
is a false response.
As a false addressee a?,we pick up and use the addressee with the high-est probability from the set of candidate addresseesexcept the ground-truth one (A(C) \ a+).
As afalse response, we randomly pick up and use a re-sponse from the set of candidate responses exceptthe ground-truth one (R \ r+).6 Corpus and DatasetOur goal is to provide a multi-party conversationcorpus/dataset that can be used over a wide rangeof conversation research, such as turn-taking model-ing (Raux and Eskenazi, 2009) and disentanglementmodeling (Elsner and Charniak, 2010), as well as forthe ARS task.
Figure 4 shows the flow of the cor-pus and dataset creation process.
We firstly crawlUbuntu IRC Logs and preprocess the obtained logs.Corpus DatasetTrain Dev TestNo.
of Docs 7355 6,606 367 382No.
of Utters 2.4 M 2.1 M 13.2 k 15.1 kNo.
of Words 27.0 M 23.8 M 1.5 M 1.7 MNo.
of Samples - 665.6 k 45.1 k 51.9 kAvg.
W. / U.
11.1 11.1 11.2 11.3Avg.
A.
/ D. 26.8 26.3 30.68 32.1Table 2: Statistics of the corpus and dataset.
?Docs?
is docu-ments, ?Utters?
is utterances, ?W.
/ U.?
is the number of wordsper utterance, ?A.
/ D.?
is the number of agents per document.Then, from the logs, we extract and add addresseeinformation to the corpus.
In the final step, we setcandidate responses and labels as the dataset.
Table2 shows the statistics of the corpus and dataset.6.1 Ubuntu IRC LogsThe Ubuntu IRC Logs is a collection of logs fromUbuntu-related chat rooms.
In each chat room, anumber of users chat on and discuss various topics,mainly related to technical support with Ubuntu is-sues.The logs are put together into one file per day foreach room.
Each file corresponds to a documentD.
In a document, one line corresponds to one loggiven by a user.
Each log consists of three items(Time, UserID, Utterance).
Using such informa-tion, we create a multi-party conversation corpus.6.2 The Multi-Party Conversation CorpusTo pick up only the documents written in En-glish, we use a language detection library (Nakatani,2010).
Then, we remove the system logs from eachdocument and leave only user logs.
For segmentingthe words in each utterance, we use a word tokenizer(TreebankWordTokenizer) of the NaturalLanguage Toolkit4.
Using the preprocessed docu-ments, we create a corpus, whose row consists ofthe three items (UserID, Addressee, Utterance).First, the IDs of the users in a document are col-lected into the user ID list by referring to the UserIDin each log.
Then, as the addressee user ID, we ex-tract the first word of each utterance.
In the UbuntuIRC Logs, users follow the name mention conven-tion (Uthus and Aha, 2013), in which they express4http://www.nltk.org/2138their addressee by mentioning the addressee?s userID at the beginning of the utterance.
By exploitingthe name mentions, if the first word of each utter-ance is identical to a user ID in the user ID list, weextract the addressee ID and then create a table con-sisting of (UsetID, Addressee, Utterance).
Inthe case that addressee IDs are not explicitly men-tioned at the beginning of the utterance, we do notextract anything.6.3 The ARS DatasetBy exploiting the corpus, we create a dataset forthe ARS task.
If the line of the corpus includesan addressee ID, we regard it as a sample for thetask.
As the ground truth addressees and responses,we straightforwardly use the obtained addressee IDsand the preprocessed utterances.As false responses, we sample utterances else-where within a document.
This document-withinsampling method makes the response selection taskmore difficult than the random sampling method5.One reason for this is that common or similar top-ics in a document are often discussed and the usedwords tend to be similar, which makes the word-based features for the task less effective.
We par-titioned the dataset randomly into a training set(90%), a development set (5%) and a test set (5%).7 ExperimentsWe provide performance benchmarks of our learn-ing architectures on the addressee and response se-lection (ARS) task for multi-party conversation.7.1 Experimental SetupDatasetsWe use the created dataset for the experiments.
Thenumber of candidate responses RES-CAND (|R|) isset to 2 or 10.Evaluation MetricsWe evaluate performance by accuracies onthree aspects: addressee-response pair selection(ADR-RES), addressee selection (ADR), and re-sponse selection (RES).
In the addressee-responsepair selection, we regard the answer as correct ifboth the addressee and the response are correctly5Lowe et al (2015) adopted the random sampling method.selected.
In the addressee/response selection, we re-gard the answer as correct if the addressee/responseis correctly selected.OptimizationThe models are trained by backpropagation throughtime (Werbos, 1990; Graves and Schmidhuber,2005).
For the backpropagation, we use stochasticgradient descent (SGD) with a mini-batch trainingmethod.
The mini-batch size is set to 128.
Thehyper-parameter ?
for the interpolation between thetwo loss functions (Section 5.3) is set to 0.5.
For theL2 weight decay, the hyper-parameter ?
is selectedfrom {0.001, 0.0005, 0.0001}.Parameters of the models are randomly ini-tialized over a uniform distribution with support[?0.01, 0.01].
To update parameters, we use Adam(Kingma and Ba, 2014) with the default setting sug-gested by the authors.
As the word embeddings,we used the 300 dimension vectors pre-trained byGloVe6 (Pennington et al, 2014).
To avoid over-fitting, the word vectors are fixed across all exper-iments.
The hidden dimensions of parameters areset to dw = 300 and dh = 50 in the both models,and da is set to 300 in the static model and 50 in thedynamic model.To identify the best training epoch and model con-figuration, we use the early stopping method (Yao etal., 2007).
In this method, if the best accuracy ofADR-RES on the development set has not been up-dated for consecutive 5 epochs, training is stoppedand the best performing model is picked up.
Themax epochs is set to 30, which is sufficient for con-vergence.Implementation DetailsFor computational efficiency, we limit the length ofa context C as CT?Nc+1:T = (uT?Nc+1, ?
?
?
,uT ),where Nc, called context window, is the numberof utterances prior to a time step t. We set Nc to{5, 10, 15}.
In addition, we truncate the utterancesand responses at a maximum of 20 words.
For batchprocessing, we zero-pad them so that the number ofwords is constant.
Out-of-vocabulary words are re-placed with <unk>, whose vector is the averagedvector over all word vectors.6http://nlp.stanford.edu/projects/glove/2139RES-CAND = 2 RES-CAND = 10Nc ADR-RES ADR RES ADR-RES ADR RESChance - 0.62 1.24 50.00 0.12 1.24 10.00Baseline5 36.97 55.73 65.68 16.34 55.73 28.1910 37.42 55.63 67.79 16.11 55.63 29.4815 37.13 55.62 67.89 15.44 55.62 29.19Static5 46.99 60.39 75.07 21.98 60.26 33.2710 48.67 60.97 77.75 23.31 60.66 35.9115 49.27 61.95 78.14 23.49 60.98 36.58Dynamic5 49.80 63.19 76.07 23.72 63.28 33.6210 53.85 66.94 78.16 25.95 66.70 36.1415 54.88 68.54 78.64 27.19 68.41 36.93Table 3: Benchmark results: accuracies on addressee-response selection (ADR-RES), addressee selection (ADR), and responseselection (RES).
Nc is the context window.
Bolded are the best per column.Baseline ModelWe set a baseline using the term frequency-inversedocument frequency (TF-IDF) retrieval model forthe response selection (Lowe et al, 2015).
We firstlycompute two TF-IDF vectors, one for a context win-dow and one for a candidate response.
Then, wecompute a cosine similarity for these vectors, andselect the highest scoring candidate response as aresult.
For the addressee selection, we adopt a rule-based method: to determine the agent that gives anutterance most recently except a responding agent,which captures the tendency that agents often re-spond to the other that spoke immediately before.7.2 ResultsOverall PerformanceTable 3 shows the empirical benchmark results.
Thedynamic model achieves the best results in all themetrics.
The static model outperforms the baseline,but is inferior to the dynamic model.In addressee selection (ADR), the baseline modelachieves around 55% in accuracy.
This means that ifyou select the agents that spoke most recently as anaddressee, the half of them are correct.
Comparedwith the baseline, our proposed models achieve bet-ter results, which suggests that the models can se-lect the correct addressees that spoke at more pre-vious time steps.
In particular, the dynamic modelachieves 68% in accuracy, which is 7 point higherthan the accuracy of static model.In response selection (RES), our models outper-form the baseline.
Compared with the static model,Figure 5: Accuracies in addressee-response selection using dif-ferent amount of samples for training.the dynamic model achieves around 0.5 point higherin accuracy.Effects of the Context WindowIn response selection, a performance boost of ourproposed models is observed for the context win-dow Nc = 10 over Nc = 5.
Comparing the resultsof the models with the context windowNc = 10 andNc = 15, the performance is improved but relativelysmall, which suggests that the performance almostreaches the convergence.
In addressee selection, theperformance improvements of the static model withthe broader context window is limited.
In contrast,in the dynamic model, a steady performance boostis observed, yielding an increase of over 5 points be-tween Nc = 15 and Nc = 5,2140No.
of Agents 2-5 6-10 11-15 16-20 21-30 31-100 101-305No.
of Samples 3731 5962 5475 4495 5619 7956 18659ADR-RESBaseline 52.13 43.51 39.98 42.96 39.70 36.55 29.22Static 64.17 55.92 50.72 53.04 48.69 49.61 42.86Dynamic 66.90 57.73 54.32 55.64 51.61 55.88 52.14ADRBaseline 84.94 70.82 62.14 65.52 58.89 51.28 41.47Static 86.33 74.37 66.12 68.54 63.43 59.24 50.99Dynamic 87.64 76.48 69.99 72.21 66.90 66.78 62.11RESBaseline 60.71 61.24 64.51 65.58 67.93 71.66 71.38Static 73.60 73.45 74.54 75.95 75.17 81.50 81.60Dynamic 75.64 74.12 75.53 75.17 76.05 81.96 81.81Table 4: Performance comparison for different numbers of agents appearing in the context.
The numbers are accuracies on the testset with the number of candidate responses CAND-RES = 2 and the context window Nc = 15.Effects of the Sample SizeFigure 5 shows the accuracy curves of addressee-response selection (ADR-RES) for different train-ing sample sizes.
We use 1/2, 1/4, and 1/8 ofthe whole training samples for training.
The resultsshow that as the amount of the data increases, theperformance of our models are improved and grad-ually approaches the convergence.
Remarkably, theperformance of the dynamic models using the 1/8samples is comparable to that of the static model us-ing the whole samples.Effects of the Number of ParticipantsTo shed light on the relationship between the modelperformance and the number of agents in multi-partyconversation, we investigate the effect of the num-ber of agents participating in each context.
Table 4compares the performance of the models for differ-ent numbers of agents in a context.In addressee selection, the performance of allmodels gradually gets worse as the number of agentsin the context increases.
However, compared withthe baseline, our proposed models suppress the per-formance degradation.
In particular, the dynamicmodel predicts correct addressees most robustly.In response selection, unexpectedly, the perfor-mance of all the models gets better as the numberof agents increases.
Detailed investigation on the in-teraction between the number of agents and the re-sponse selection complexity is an interesting line offuture work.8 ConclusionWe proposed addressee and response selection formulti-party conversation.
Firstly, we provided theformal definition of the task, and then created a cor-pus and dataset.
To present benchmark results, weproposed two modeling frameworks, which jointlymodel speakers and their utterances in a context.Experimental results showed that our models of theframeworks outperform a baseline.Our future objective to tackle the task of predict-ing whether to respond to a particular utterance.
Inthis work, we assume that the situations where thereis a specific addressee that needs an appropriate re-sponse and a system is required to respond.
In actualmulti-party conversation, however, a system some-times has to wait and listen to the conversation thatother participants are engaging in without needlessinterruption.
Hence, the prediction of whether torespond in a multi-party conversation would be animportant next challenge.AcknowledgmentsWe thank Graham Neubig, Yuya Taguchi, RyosukeKohita, Ander Martinez, the members of the NAISTComputational Linguistics Laboratory, the membersof IBM Research - Tokyo, Long Duong, and the re-viewers for their helpful comments.2141ReferencesRieks Akker and David Traum.
2009.
A comparison ofaddressee detection methods for multiparty conversa-tions.
In Workshop on the Semantics and Pragmaticsof Dialogue.Kyunghyun Cho, Bart vanMerrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase represen-tations using rnn encoder?decoder for statistical ma-chine translation.
In Proceedings of EMNLP, pages1724?1734.Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,and Yoshua Bengio.
2014.
Empirical evaluation ofgated recurrent neural networks on sequence model-ing.
arXiv preprint arXiv: 1412.3555.Frank PMDignum and Gerard AWVreeswijk.
2003.
To-wards a testbed for multi-party dialogues.
Advances inAgent Communication, pages 212?230.Micha Elsner and Eugene Charniak.
2010.
Disentan-gling chat.
Computational Linguistics, pages 389?409.Alex Graves and Ju?rgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional lstmand other neural network architectures.
Neural Net-works, 18(5):602?610.Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.2014.
Convolutional neural network architectures formatching natural language sentences.
In Proceedingsof NIPS, pages 2042?2050.Zongcheng Ji, Zhengdong Lu, and Hang Li.
2014.
Aninformation retrieval approach to short text conversa-tion.
arXiv preprint arXiv: 1408.6988.Natasa Jovanovic?
and op den Rieks Akker.
2004.Towards automatic addressee identification in multi-party dialogues.
In Proceedings of SIGDIAL.Natasa Jovanovic?, op den Rieks Akker, and Anton Ni-jholt.
2006.
Addressee identification in face-to-facemeetings.
In Proceedings of EACL.Rudolf Kadlec, Martin Schmid, and Jan Kleindiest.2015.
Improved deep learning baselines for ubuntucorpus dialogs.
arXiv preprint arXiv: 1510.03753.Diederik P. Kingma and Jimmy Lei Ba.
2014.
Adam:A method for stochastic optimization.
arXiv preprintarXiv: 1412.6980.Esther Levin, Roberto Pieraccini, and Wieland Eckert.2000.
A stochastic model of human-machine interac-tion for learning dialog strategies.
IEEE Transactionson Speech and Audio Processing, pages 11?23.Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,and Bill Dolan.
2016.
A persona-based neural conver-sation model.
In Proceedings of ACL.Ryan Lowe, Nissan Pow, Iulian V. Serban, and JoellePineau.
2015.
The ubuntu dialogue corpus: A largedataset for research in unstructured multi-turn dia-logue systems.
In Proceedings of SIGDIAL, pages285?294.Zhengdong Lu and Hang Li.
2013.
A deep architec-ture for matching short texts.
In Proceedings of NIPS,pages 1367?1375.Shuyo Nakatani.
2010.
Language detection library forjava.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for word rep-resentation.
In Proceedings of EMNLP, pages 1532?1543.Antoine Raux and Maxine Eskenazi.
2009.
A finite-stateturn-taking model for spoken dialog systems.
In Pro-ceedings of NAACL, pages 629?637.Suman V Ravuri and Andreas Stolcke.
2014.
Neural net-work models for lexical addressee detection.
In Pro-ceedings of INTERSPEECH, pages 298?302.Alan Ritter, Colin Cherry, and William B. Dolan.
2011.Data-driven response generation in social media.
InProceedings of EMNL, pages 583?593.Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio,Aaron Courville, and Joelle Pineau.
2016.
Build-ing end-to-end dialogue systems using generative hi-erarchical neural network models.
In Proceedings ofAAAI, pages 3776?3783.Lifeng Shang, Zhengdong Lu, and Hang Li.
2015.
Neu-ral responding machine for short-text conversation.
InProceedings of ACL/IJCNLP, pages 1577?1586.Alessandro Sordoni, Michel Galley, Michael Auli, ChrisBrockett, Yangfeng Ji, Margaret Mitchell, Jian-YunNie, Jianfeng Gao, and Bill Dolan.
2015.
Aneural network approach to context-sensitive genera-tion of conversational responses.
In Proceedings ofNAACL/HLT, pages 196?205.David Traum.
2003.
Issues in multiparty dialogues.
Ad-vances in Agent communication, pages 201?211.David C Uthus and David W Aha.
2013.
Multipartic-ipant chat analysis: A survey.
Artificial Intelligence,pages 106?121.Oriol Vinyals and V. Quoc Le.
2015.
A neural conversa-tional model.
arXiv preprint arXiv: 1506.05869.Marilyn A Walker, Rashmi Prasad, and Amanda Stent.2003.
A trainable generator for recommendationsin multimodal dialog.
In Proceedings of INTER-SPEECH.
Citeseer.Hao Wang, Zhengdong Lu, Hang Li, and Enhong Chen.2013.
A dataset for research on short-text conversa-tions.
In Proceedings of EMNLP, pages 935?945.Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu.2015.
Syntax-based deep matching of short texts.
InProceedings of IJCAI, pages 1354?1361.2142Paul J Werbos.
1990.
Backpropagation through time:what it does and how to do it.
Proceedings of theIEEE, 78(10):1550?1560.Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.2007.
On early stopping in gradient descent learning.Constructive Approximation, 26(2):289?315.Steve Young, Milica Gas?ic?, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, and KaiYu.
2010.
The hidden information state model: Apractical framework for pomdp-based spoken dialoguemanagement.
Computer Speech & Language, pages150?174.2143
