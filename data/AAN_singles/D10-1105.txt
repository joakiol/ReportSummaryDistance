Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1077?1087,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsContext Comparison of Bursty Events in Web Search and Online MediaYunliang JiangUniversity of IllinoisUrbana, IL, 61801jiang8@illinois.eduCindy Xide LinUniversity of IllinoisUrbana, IL, 61801xidelin2@illinois.eduQiaozhu MeiUniversity of MichiganAnn Arbor, MI, 48109qmei@umich.eduAbstractIn this paper, we conducted a systematic com-parative analysis of language in different con-texts of bursty topics, including web search,news media, blogging, and social bookmark-ing.
We analyze (1) the content similarity andpredictability between contexts, (2) the cov-erage of search content by each context, and(3) the intrinsic coherence of information ineach context.
Our experiments show that so-cial bookmarking is a better predictor to thebursty search queries, but news media and so-cial blogging media have a much more com-pelling coverage.
This comparison providesinsights on how the search behaviors and so-cial information sharing behaviors of users arecorrelated to the professional news media inthe context of bursty events.1 IntroductionSearch is easy.
Every day people are repeating thequeries they have used before, trying to access thesame web pages.
A smart search engine tracks thepreference and returns it next time when it sees thesame query.
When I search for ?msr?
I always try toaccess Microsoft research; and even if I misspelledit, a smart search engine could suggest a correctquery based on my query history, the current ses-sion of queries, and/or the queries that other peoplehave been using.Search is hard.
I search for ?social computing?because there was such a new program in NSF; butthe search engine might have not yet noticed that.People use ?msg?
to access monosodium glutamatein most of the cases, but tonight there is a big gamein Madison square garden.
H1N1 suddenly becamea hot topic, followed by a burst of the rumor thatit was a hoax, and then the vaccine.
The informa-tion need of users changed dramatically during sucha period.
When a new event happens, the burst ofnew contents and new interests make it hard to pre-dict what people would search and to suggest whatqueries they should use.Web search is easy when the information need ofthe users is stable and when we have enough histor-ical clicks.
It becomes much more difficult when anew information need knocks the door or when thereis a sudden change of the information need.
Such ashift of the information need is usually caused by aburst of new events or new interests.When we are lack of enough historical observa-tions, why don?t we seek help from other sources?A bursting event will not only influence what wesearch, but hopefully also affect what we read, whatwe write, and what we tag.
Indeed, there is al-ready considerable effort in seeking help from thesesources, by the integration of news and blogs intosearch results or the use of social bookmarks toenhance search.
These conclusions, however, aremostly drawn in a general context (e.g., with gen-eral search queries).
To what extent are they use-ful when dealing with busty events?
How is thebursting content in web search, news media, socialmedia, and social bookmarks correlating and dif-ferent from each other?
Prior to the developmentof desirable applications (e.g.
enhancing search re-sults, query suggestion, keyword bidding on adver-tisement, etc) by integrating the information from allthese sources, it is appealing to have an investigationof feasibility.In this work, we conduct a systematic compara-tive study of what we search, what we read, what1077we write, and what we tag in the scenarios of burstyevents.
Specifically, we analyze the language usedin different contexts of bursty events, including twodifferent query log contexts, two news media con-texts, two blog contexts, and an additional con-text of social bookmarks.
A variety of experimentshave been conducted, including the content similar-ity and cross-entropy between sources, the coverageof search queries in online media, and an in-depthsemantic comparison of sources based on languagenetworks.In the rest of this paper, a summary of relatedwork is briefly described in Section 2.
We thenpresent the experiments setup in Section 3, The re-sults of the experiments is presented in Section 4.
Fi-nally, our major findings from the comparative anal-ysis are drawn in Section 5.2 Related WorkRecently, a rich body of work has focused on howto find the bursting patterns from time-series datausing various approaches such as time-graph analy-sis (Kleinberg, 2003; Kuman et al, 2003), context-based analysis (Gabrilovich et al, 2004), moving-average analysis (Vlachos et al, 2004), and fre-quency analysis (Gruhl et al, 2005), etc.
Thesemethods are all related to the preprocessing step ofour analysis: detecting bursty queries from the querylog effectively.The comparison of two web sources at a time iswidely studied recently.
(Sood et al, 2007) dis-cussed how to leverage the relation between socialtags and web blogs.
(Lloyd et al, 2006; Gamon etal., 2008; Cointet et al, 2008) investigated the rela-tions between news and blogs.
Also some work hasaimed to utilize one external web source to help websearch.
For example, (Diaz, 2009) integrated thenews results into general search.
(Bao et al, 2007;Heymann et al, 2008; Krause et al, 2008; Bischoffet al, 2008) focused on improving search by the so-cial tags.
Compared with the above, our comparisonanalysis tries to explore the interactions among mul-tiple web sources including the search logs.Similar to our work, some recent work (Adar etal., 2007; Sun et al, 2008) has addressed the com-parison among multiple web sources.
For exam-ple, (Adar et al, 2007) did a comprehensive corre-lation study among queries, blogs, news and TV re-sults.
However, different from the content-free anal-ysis above, our work compares the sources based onthe content.Our work can lead to many useful search applica-tions, such as query suggestion which takes as in-put a specific query and returns as output one orseveral suggested queries.
The approaches includequery term cooccurrence (Jones et al, 2006), querysessions (Radlinski and Joachims, 2005), and click-through (Mei et al, 2008), respectively.3 Analysis SetupTasks of web information retrieval such as websearch generally perform very well on frequentand navigational queries (Broder, 2002) such like?chicago?
or ?yahoo movies.?
A considerable chal-lenge in web search remains in how to handle infor-mational queries, especially queries that reflect newinformation need and suddenly changed informationneed of users.
Many such scenarios are caused bythe emergence of bursty events (e.g., ?van gogh?
be-came a hot query in May 2006 since a Van Goghsportrait was sold for 40.3 million in New York dur-ing that time).
The focus of this paper is to analyzehow other online media sources react to those burstyevents and how those reactions compare to the re-action in web search.
This analysis thus serves asan primitive investigation of the feasibility of lever-aging other sources to enhance the search of burstytopics.Therefore, we focus on the ?event-related?
topicswhich present as bursty queries submitted to a searchengine.
These queries not only reflect the suddenlychanged information need of users, but also triggerthe correlated reactions in other online sources, suchas news media, blog media, social bookmarks, etc.We begin with the extraction of bursty topics fromthe query log.3.1 Bursty Topic ExtractionSearch engine logs (or query logs) store the historyof users?
search behaviors, which reflect users?
in-terests and information need.
The query log of acommercial search engine consists of a huge amountof search records, each of which typically containsthe following information: the query submitted by1078a user, the time at which the query was submitted,and/or the URL which the user clicked on after thequery was submitted, etc.
It is common practiceto segment query log into search sessions, each ofwhich represents one user?s searching activities in ashort period of time.We explore a sample of the log of the MicrosoftLive search engine1, which contains 14.9M searchrecords over 1 month (May 2006).3.1.1 Find bursty queries from query logHow to extract the queries that represent burstyevents?
We believe that bursty queries present thepattern that its day-by-day search volume shows asignificant spike ?
that is, the frequency that the usersubmit this query should suddenly increase at onespecific time and drop down after a while.
This as-sumption is consistent with existing work of findingbursty patterns in emails, scientific literature (Klein-berg, 2003), and blogs (Gruhl et al, 2005).Following (Gruhl et al, 2005), we utilize a simplebut effective method to collect bursty topics in thequery log data as follows:?
We choose bigrams as the basic presentation ofbursty topics since bigrams present the informationneed of users more clearly and completely than un-igrams and also have a larger coverage in the querylog comparing to n-grams (n ?
3).?
We only consider the bigram queries which ap-pear more frequently than a threshold s per month.This is reasonable since a bursty event usuallycauses a large volume of search activities.?
Let fmax(q) be the maximum search volume ofa query q in one day (i.e., day d).
Let f?
?5(q) be theupper bound of the daily search volume of q out-side a time window of 5 days centered at day d. Iffmax(q) is ?significantly higher?
than f?
?5(q) (i.e.,rm = f?max(q)/f?5(q) > m), we consider q as aquery with a spike pattern (m is an empirical thresh-old).?
The ratio above may be vulnerable to the querythat has more than one spike.
To solve this, we de-fine f?
?5(q) as the average of daily search volumeof q outside the same time window.
This gives usan alternative ratio ra = fmax(q)/f??5(q).
We fur-ther balance these two ratios by ranking the bursty1Now known as Bing: www.bing.comqueries usingscore(q) = ?
?
rm(q) + (1?
?)
?
ra(q) (1)By setting s = 20, m = 2.5, ?
= 0.8 (based onseveral tests), we select the top 130 bigram querieswhich form the pool of bursty topics for our anal-ysis.
Table 1 shows some of these topics, coveringmultiple domains: politics, science, art, sports, en-tertainment, etc.ID Topic ID Topic1 kentucky election 66 orlando hernandez2 indiana election 75 daniel biechele8 van goph 81 hurricane forecast24 north korea 92 93 memorial34 pacific quake 113 holloway case52 florida fires 128 stephen colbert63 hunger strike 130 bear attackTable 1: Examples of News Topics3.2 Context extraction from multiple sourcesOnce we select the pool of bursty topics, we gatherthe contexts of each topic from multiple sources:query log, news media, blog media, and social book-marks.
We assume that the language in these con-texts will reflect the reactions of the bursty events incorresponding online media.3.2.1 Super query contextThe most straightforward context of bursty eventsin web search is the query string, which directlyreflects the users?
interests and perspectives in thetopic.
We therefore define the first type of context ofa bursty topic in query log as the set of surroundingterms of that bursty bigram in the (longer) queries.For example, the word aftermath in the query ?haitiearthquake aftermath?
is a term in the context of thebursty topic haiti earthquake.Formally, we define a Super Query of a burstytopic t, sq(t), as the query which contains the bi-gram query t lexically as a substring.
For eachbursty topic t, we scan the whole query log Q andretrieve all the super queries of t to form the contextwhich is represented by SQ(t).SQ(t) = {q|q ?
Q and q = sq(t)}1079SQ(t) is defined as the super query context of t.For example, the super query context of ?kentuckyelection?
contains terms such as ?2006,?
?results,?
?christian county,?
etc.
These terms indicate whataspects the users are most interested in KentuckyElection during May 2006.The super query context is widely explored bysearch engines to provide query expansion andquery completion (Jones et al, 2006).3.2.2 Query session contextAnother interesting context of a bursty topic inquery log is the sequence of queries that a usersearches after he submitted the bursty query q. Thiscontext usually reflects how a user reformulates therepresentation of his information need and implicitlyclarifies his interests in the topic.We define a Query Session containing a burstytopic t, qs(t), as the queries which are issued by thesame user after he issued t, within 30 minutes.
Foreach bursty topic t, we collect all the qs(t) to formthe query session context of t, QS(t):QS(t) = {q|q ?
Q and q ?
qs(t)}In web search, the query session context is usu-ally utilized to provide query suggestion and queryreformulation (Radlinski and Joachims, 2005).3.2.3 News contextsNews articles written by critics and journalists re-flect the reactions and perspectives of such profes-sional group of people to a bursty event.
We col-lect news articles about these 130 bursty topics fromGoogle News2, by finding the most relevant newsarticles which (1) match the bursty topic t, (2) werepublished in May, 2006, and (3) were published byany of the five major news medias: CNN, NBC,ABC, New York Times and Washington Post.We then retrieve the title and body of each newsarticle.
This provides us two contexts of each burstytopic t: the set of relevant news titles, NT (t), andthe set of relevant news bodies, NB(t).3.2.4 Blog contextsCompared with news articles, blog articles arewritten by common users in the online communi-ties, which are supposed to reflect the reactions and2http://news.google.com/opinions of the public to the bursty events.
We col-lect blog articles about these 130 topics from GoogleBlog3, by finding the most relevant blog articleswhich (1) match the bursty topic t, (2) were pub-lished in May, 2006 (3) were published in the mostpopular blog community, Blogspot4.
We then re-trieve the title and body of each relevant blog post re-spectively.
This provides another two contexts: theset of relevant blog titles, BT (t), and the set of rel-evant blog bodies, BB(t).3.2.5 Social bookmarking contextSocial bookmarks form a new source of socialmedia that allows the users to tag the webpages theyare interested in and share their tags with others.
Thetags are supposed to reflect how the users describethe content of the pages and their perspectives of thecontent in a concise way.We use a sample of Delicious5 bookmarks in May,2006, which contains around 1.37M unique URLs.We observe that the bursty bigram queries are alsofrequently used as tags in Delicious.
We thus con-struct another context of bursty events by collectingall the tags that are used to tag the same URLs as thebursty topic.Formally, we define DT (t) as the context of so-cial tags of a topic t,DT (t) = {tag|?url, s.t.
tag, t ?
B(url)},where url is a URL and B(url) stands for the set ofall bookmarks of url.3.3 Context StatisticsNow we have constructed the set of 130 burstytopics and 7 corresponding contexts from varioussources.
We believe that these contexts well repre-sent the various types of online media and sources.For each context, we then clean the data by re-moving stopwords and the bursty topic keywordsthemselves.
We then represent it as either the setof unigrams or bigrams from this context.
Table 2shows the basic statistics of each context:From Table 2 we observe the following facts:?
The query session context covers more terms(both unigrams and bigrams) than the super query3http://blogsearch.google.com/4http://www.blogspot.com/5http://delicious.com/1080N T S M S A U M U A B M BSQ 130 76k 5.3k 32.7 390 24.3 235QS 126 108k 5.8k 224 1.5k 150 1062NT 118 4.7k 411 105 627 102 722NB 118 4.7k 411 4.7k 22k 22k 257kBT 128 5.8k 99 184 459 169 451BB 128 5.8k 99 4.1k 15k 12k 69kDT 71 2.3k 475 137 2.0k N/A N/AN: The number of topics coveredT S: The total number of records/documentsM S: The max number of records/documents per topicA U: The avg number of unique unigrams per topicM U: The max number of unique unigramsA B: The avg number of unique bigramsM B: The max number of unique bigramsTable 2: Basic statistics of collectionscontext.
In both contexts, the average number ofunique bigrams is smaller than unigrams.
This isbecause queries in search are usually very short.
Af-ter removing stopwords and topic keywords, quite afew queries have no bigram in these contexts.?
News articles and blog articles cover most of thebursty topics and contain a rich set of unigrams andbigrams in the corresponding contexts.?
The Delicious context only covers less than 60%of bursty topics.
We couldn?t extract bigrams frombookmarks since delicious provides a ?bag-of-tags?interface.In Section 4, we present a comprehensive analy-sis of these different contexts of bursty topics, withthree different types of comparison.4 ExperimentIn this section, we present a comprehensive compar-ative analysis of the different contexts, which repre-sent the reactions to the bursty topics in correspond-ing sources.4.1 Similarity & Predictability analysisOur first task is to compare the content similarityof these sources.
This will help us to understandhow well the language usage in one context can beleveraged to predict the language usage in anothercontext.
This is especially useful to predict the con-tent in web search.
By representing each contextof a bursty topic as a vector space model of uni-grams/bigrams, we first compute and compare theaverage cosine similarity between contexts.
We onlyinclude contexts with more than 5 unigram/bigramsinto this comparison.
The results are shown in TableA and Table B, respectively.
Each table is followedby a heat map to visualize the pattern.To investigate how well one source can predictthe content of another, we also represent each con-text of a bursty topic as a unigram/bigram languagemodel and compute the Cross Entropy (Kullbackand Leibler, 1951) between every pairs of contexts.Cross Entropy measures how certain one probabil-ity distribution predicts another.
We calculate suchmeasure based on the following definition:HCE(m||n) = H(m) +DKL(m||n)We smooth the unigram language models usingLaplace smoothing (Field, 1988) and the bigram lan-guage models using Katz back-off model (Katz,1987).The results are shown in Table C and Table D,followed by the corresponding heat maps.
For eachvalue HCE(m||n) in the table cell, m stands for thecontext in the row and n stands for the context inthe column.
Please note that in Figure 3, 4, a largerHCE value corresponds to a lighter cell.4.1.1 ResultsFrom the results shown in Table A-D, or in Fig-ure 1- 4 more visually, some interesting phenomenacan be observed:?
Compared with other contexts, query sessionis much more similar to the super query.
Thismakes sense because many super queries would beincluded in the query session.?
Compared with news and blog, the deliciouscontext is closer to the query log context.
In fact, de-licious is reasonably close to all the other contexts.This means social tags could be an effective sourceto enhance bursty topics in web search in terms ofquery suggestion.
However, as Table 2 shows, onlyless than 60% of topics can be covered by delicioustag.
We have to explore other sources to make acomprehensive prediction.?
In the news and blog contexts, the title contextsare more similar to the query contexts than the bodycontexts.
This may be because titles usually con-cisely describe the topic while bodies contain muchmore details and irrelevant contents.1081Context SQ QS NT NB BT BB DTSQ 1.0 0.405 0.122 0.072 0.119 0.061 0.188QS 0.405 1.0 0.049 0.062 0.066 0.054 0.112NT 0.122 0.049 1.0 0.257 0.186 0.152 0.120NB 0.072 0.062 0.257 1.0 0.191 0.362 0.114BT 0.119 0.066 0.186 0.191 1.0 0.242 0.141BB 0.061 0.054 0.152 0.362 0.242 1.0 0.107DT 0.188 0.112 0.120 0.114 0.141 0.107 1.0Table A: Cosine similarity for unigram vectorsFigure 1: Heat map of table ASource SQ QS NT NB BT BBSQ 1.0 0.290 0.028 0.024 0.047 0.027QS 0.290 1.0 0.004 0.010 0.011 0.009NT 0.028 0.004 1.0 0.041 0.026 0.011NB 0.024 0.010 0.041 1.0 0.023 0.040BT 0.047 0.011 0.026 0.023 1.0 0.044BB 0.027 0.009 0.011 0.040 0.044 1.0We do not build bigram vector for DTTable B: Cosine similarity for bigram vectorsFigure 2: Heat map of table BSource SQ QS NT NB BT BB DTSQ 1.698 4.911 7.538 8.901 7.948 9.050 7.498QS 7.569 3.842 9.487 11.130 9.997 11.546 8.972NT 8.957 10.868 3.718 7.946 9.006 9.605 8.825NB 11.217 12.897 11.317 7.241 12.282 11.582 11.739BT 9.277 11.084 9.085 10.295 4.637 9.365 9.180BB 11.053 12.842 11.593 11.742 12.001 7.232 11.525DT 8.457 9.794 8.521 9.511 8.831 9.473 2.990Table C: Cross entropy for unigram distributionFigure 3: Heat map of table CSource SQ QS NT NB BT BBSQ 1.891 2.685 4.290 4.540 4.319 4.607QS 6.800 3.430 8.144 9.049 8.528 9.304NT 5.444 5.499 3.652 4.733 5.106 5.218NB 11.572 11.797 11.254 8.731 11.544 11.073BT 5.664 5.674 5.503 5.495 4.597 5.301BB 10.745 10.796 10.517 10.455 10.526 8.518We do not build bigram distribution for DTTable D: Cross Entropy for bigram distributionFigure 4: Heat map of table D1082HCE(SQ||n)n: NT BT NB BBUni: 7.538 7.948 8.901 9.050Bi: 4.290 4.319 4.540 4.607HCE(m||SQ)m: NT BT NB BBUni: 8.927 9.277 11.217 11.053Bi: 5.445 5.664 11.572 10.745Table 3: Cross-entropy among three sources?
News would be a better predictor of the querythan blog in general.
This is interesting, which indi-cates that many search activities may be initializedby reading the news.?
News and blogs are much more similar to eachother than query logs.
We hypothesize that this re-sult reflects the behavior how people write blogsabout bursty events ?
typically they may have readseveral news articles before writing their own blog.In the blog, they may directly quote or retell a partof the news article and then add their opinion.?
Table 3 reveals the generation relations amongthree sources: query, news and blog.
From theupper table, we can observe that queries are morelikely to be generated by news articles, rather thanblog articles.
From the lower table, we can observethat queries are more likely to generate blog arti-cles(body), rather than news articles(body).
This re-sult is quite interesting, which indicates the users?actual behaviors: when a bursty event happens, userswould search them from web after they read it fromsome news articles.
And users would write theirown blogs to discuss the event after they retrieve anddigest information from the web.?
From Table 3 we also find that queries are morelikely to generate news title, rather than blog title.
Itis natural since blogs are written by kinds of people.The content especially the title part contains moreuncertainty.4.1.2 Case studyWe then conduct the analysis to the level of indi-vidual topics.
Table 4 shows the correlation of eachpair of contexts, computed based on the similaritybetween topics in SQ and corresponding topics inthese contexts.
We can observe that News and Blogare correlated with each other tightly.
If one is agood predictor of bursty queries, the other one alsotends to be.QS NT NB BT BB DTQS 0.46 0.59 0.58 0.75 0.46NT 0.73 0.79 0.59 0.61NB 0.71 0.68 0.61BT 0.78 0.59BB 0.48Table 4: Correlations of the similarity with SQFor some topics like ?stephen colbert,?
and ?threegorges,?
both News and Blog are quite similar tothe queries, which implies some intrinsic properties(coherence) of these topics: users would refer to thesame content when using the topic terms in differentsources.We also find that a few topics like ?hotdogs,?
?bear attack,?
for which the similarity of(SQ,News) and (SQ,Blog) are both low.
It isprobably because these topics are too diverse andcarries a lot of ambiguity.Although in most cases they are correlated, some-times News and Blog show different trends in thesimilarity to the queries.
For example, News isquite similar to the queries on the topics such as?holloway case?
and ?jazz fest?
while Blog is dis-similar.
For these unfamiliar topics, users possi-bly search the web ?after?
they read the news arti-cles and express their diverse opinions in the blog.In contrast, on the topics like ?insurance rate?
or?consolidation loans,?
Blog is similar to the querieswhile News is not.
For these daily-life-relatedqueries, users would express the similar opinionswhen they search or write blogs, while news articlestypically report such ?professional?
viewpoints.4.2 Coverage analysisAre social bookmarks the best source to predictbursty content in search?
It looks so from the sim-ilarity comparison, if they have a good coverage ofsearch contents.
In this experiment, we analyze thecoverage of query contexts in other contexts in a sys-tematic way.
If the majority of terms in the superquery context would be covered by a small propor-tion of top words from another source, this sourcehas the potential.10834.2.1 Unigram coverageWe first analyze the coverage of unigrams fromthe super query context in four other contexts: QS,DT , News (the combination of NT and NB) andBlog (the combination of BT and BB) to comparewith SQ.
For each source, we rank the unigrams byfrequency.
Figure 5(a) shows the average trend ofSQ-unigram coverage in different sources.
The x-coordinate refers to the ratio of top unigrams in onesource to the number of unigrams in SQ.
For ex-ample, if SQ contains n unigrams, the ratio 2 standsfor the top 2n unigrams in the other source.
The y-coordinate refers to the coverage rate of SQ.
We canobserve that:?
Query Session naturally covers most of the su-per query terms (over 70%).?
Though delicious tags are more similar toqueries than news and blog, as well as a relativelyhigher coverage rate than the other two while sizeratio is small, the overall coverage rate is quite low:only 21.28%.
Note that this is contradict to existingcomparative studies between social bookmarks andsearch logs (Bischoff et al, 2008).
Clearly, whenconsidering bursty queries, the coverage and effec-tiveness of social bookmarks is much lower thanconsidering all queries.
Handling bursty queries ismuch more difficult; only using social bookmarks topredict queries is not a good choice.
Other usefulsources should be enrolled.?
As the growth of the size ratio, the coveragerate of news and blogs are both gradually increased.When stable, both of them arrive at a relatively highlevel (news: 66.36%, blog: 63.80%), which meansnews and blogs have a higher potential to predict thebursty topics in search.
Moreover, in most cases,news is still prior to blog ?
not only the overall rate,but also the size ratio comparison while the coveragerate reaches 50% (news:109 < blog:183).4.2.2 Bigram CoverageAlso we analyze the bigram coverage.
This timewe only have 3 sources (no DT ).
We rank the bi-grams by the pointwise mutual information insteadof frequency, since not all the bigrams are ?real?
col-locations.
Figure 5(b) shows the results.Different from the unigram coverage, except thatthe query session can naturally keep a high coveragerate (66.07%), both news and blog cover poorly.
Forthis issue, we should re-consider the behavior thatusers search and write articles.
News or blog arti-cles consist of completed sentences and paragraphswhich would contain plenty of meaningful bigrams.However search queries consist of keywords ?
rel-atively discrete and regardless of order.
Therefore,except some proper nouns such as person?s name, alot of bigrams in the query log are formed in an ad-hoc way.
Since the different expressions of searchand writing, detecting unigrams is more informa-tional than bigrams.4.3 Coherence analysisThe above two experiments discuss the inter-relations among different contexts.
In this sectionwe will discuss the inner-relation within each par-ticular context ?
when it comes to a particular burstytopic, how coherent is the information in each con-text?
Does the discussion keep consistent, or slipinto ambiguity?We represent all the terms forming each context ofa bursty topic as a weighted graph: G = (V,E,W ),where each v ?
V stands for each term, wv standsfor the weight of vertex v in G, and each e ?
Estands for the semantic closeness between a pair ofterms (u, v) measured by sim(u, v).
We define thedensity of such a semantic graph as follows:Den(G) = ?u,v?V,u 6=vsim(u, v)wuwv?u,v?V,u 6=vwuwv (2)If sim(u, v) values the semantic similarity betweenu and v, a high value of Den(G) implies that thewhole context is semantically consistent.
Otherwise,it may be diverse or ambiguous.We build the graph of each context based onWordNet6.
For a pair of words, WordNet provides aseries of measures of the semantic similarity (Peder-sen et al, 2004).
We use the Path Distance Similar-ity (path for short) and Lin Similarity (lin for short)to measure sim(u, v).
Both measures range in [0, 1].For the convenience of computation, we choosethe top 1100 unigrams ranked by term frequency ineach source (if any) to represent the whole contexton one specific topic.6http://wordnet.princeton.edu/1084(a) Unigram (b) BigramFigure 5: Coverage results4.3.1 OverallTable 5 shows the average overall density of eachsources over all the topics.
From the table we canSource path linSQ 0.098 0.128QS 0.071 0.082NT 0.103 0.129NB 0.109 0.139BT 0.099 0.109BB 0.116 0.147DT 0.102 0.127Table 5: Overall Densityobserve that QS has the lowest density in both ofthe measures.
It is because the queries in one usersession can easily shift to other (irrelevant) topicseven in a short time.Another interesting phenomenon comes out thatfor either news or blog, the body is denser than thetitle, even if the body context contains much moreterms.
It can be explained by the roles of the titleand the body in one article: the title contains a seriesof words which briefly summarize a topic while thebody part would describe and discuss the title in de-tails.
When it maps to the semantic word network,the title tends to contain the vertices scattered in thegraph, while the context of the body part would addmore semantically related vertices around the origi-nal vertices to strength the relations.
Thus, the bodypart has a higher density than the title part.4.3.2 The trend analysisFigure 6 shows the tendency of the density in eachsource.
The x-coordinate refers to the TopN uni-grams ranked by the term frequency in each source.From Figure 6 we can find that in most cases, the av-erage density will gradually decrease while less im-portant terms are added, which implies that the mostimportant terms are denser, and other terms woulddisperse the topic.To better evaluate this tendency of each source,Table 6 shows the change rate of the highest densityto the overall density measured by lin.
We can easilyfind the following facts:?
The highest density is achieved when a smallproportion of top terms are counted (6 sources forTop5 and one for Top20), which also supports ourhypothesis: the more important, the more coherent.?
BB?s density drops the fastest of all (15.1%),following by DT (10.6%).
It may be because bothblog and delicious tag are generated by many users.And the diversity of the users leads to differentprospectives, which dilutes the context significantly.?
Both NT and NB drop quite slowly (5.8%,6.8%), which means the professional journalistswould have the relatively similar prospectives on thesame topic.
Thus the topic does not disperse toomuch.
BT also keeps a high stability.?
Compared with news, blog is easier to disperse,which can be reflected by the density comparisonbetween NT and BT .
Although the density of BBis still higher than NT , we should notice that thesetwo sources are not completely covered ?
about 3/4unigrams in these two contexts are not included in1085(a) path (b) linFigure 6: Trend of Densitythe semantic networks.
The curves clearly showsBB dropped faster than NB.
One can expect thatNB becomes denser than BB if all the unigrams inboth sources are included in the network.Source Highest Den.
Overall Den.
ChangeSQ 0.140(Top5) 0.128 -8.6%QS 0.091(Top5) 0.082 -9.9%NT 0.137(Top5) 0.129 -5.8%NB 0.148(Top5) 0.139 -6.8%BT 0.114(Top20) 0.109 -4.4%BB 0.172(Top5) 0.147 -15.1%DT 0.142(Top5) 0.127 -10.6%Table 6: Tendency analysis of Density (Lin)4.3.3 Case StudyFrom these 130 news topics, some of them showsa special tendency of coherence.
For example,when more words are included, the density of thetopic?three gorge?
drops rapidly in most of thesources.
The topic ?florida fires?
has the sametrend.
These topics are typically ?focus?
topics,which means users clearly pursue the unique eventwhile they use these terms.
Thus, the density in topunigrams is very high.
It drops rapidly since users?personal interests and opinions toward to this eventwill be enrolled gradually.In contrast, some topics like ?heather mills,?, ?in-surance rate?
express differently: their densitiesgradually increase with the growth of the terms.
Byobserving these topics we find they are usually di-verse topics (e.g: famous person name or entityname), which may lead to diverse search intentionsof users.
So the density of top unigrams is low andgradually increased since one main aspect is proba-bly strengthened.5 Conclusion and Future workIn this paper, we have studied and compared howthe web content reacts to bursty events in multi-ple contexts of web search and online media.
Af-ter a series of comprehensive experiments includingcontent similarity and predictability, the coverageof search content, and semantic diversity, we foundthat social bookmarks are not enough to predict thequeries because of a low coverage.
Other sourceslike news and blogs need to be added.
Furthermore,news can be seen as a consistent source which wouldnot only trigger the discussion of bursty events inblogs but also in search queries.When the target is to diversify the search resultsand query suggestions, blogs and social bookmarksare potentially useful accessory sources because ofthe high diversity of content.Our work serves as a feasibility investigation ofquery suggestion for bursty events.
Future workwould address on how to systematically predict andrecommend the bursty queries using online media,as well as a reasonable evaluation metrics upon it.AcknowledgmentsWe thank Prof. Kevin Chang for his support in dataand useful discussion.
We thank the three anony-mous reviewers for their useful comments.
Thiswork is in part supported by the National ScienceFoundation under award number IIS-0968489.1086ReferencesJon Kleinberg 2003.
Bursty and Hierarchical Structurein Streams Data Mining and Knowledge Discovery,Vol 7(4):373-397Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak andAndrew Tomkins 2005.
The Predictive Power of On-line Chatter KDD ?05: Proceedings of the eleventhACM SIGKDD international conference on Knowl-edge discovery in data mining, 78-87.Ravi Kumar, Jasmine Novak, Prabhakar Raghavan andAndrew Tomkins 2003.
On the Bursty Evolution ofBlogspace WWW ?03: Proc.
of the 12th InternationalWorld Wide Web Conference, 568-576.Michail Vlachos and Christopher Meek and ZografoulaVagena and Dimitrios Gunopulos 2004.
Identifyingsimilarities, periodicities and bursts for online searchqueries SIGMOD ?04: Proceedings of the 2004 ACMSIGMOD international conference on Management ofdata, 131-142.Evgeniy Gabrilovich, Susan Dumais and Eric Horvitz2004.
Newsjunkie: providing personalized newsfeedsvia analysis of information novelty WWW ?04: Pro-ceedings of the 13th international conference on WorldWide Web, 482-490.Eytan Adar, Daniel S. Weld, and Brian N. Bershad andSteven S. Gribble 2007.
Why we search: visualizingand predicting user behavior WWW ?07: Proceedingsof the 16th international conference on World WideWeb, 161-170.Aixin Sun, Meishan Hu and Ee-Peng Lim 2008.
Search-ing blogs and news: a study on popular queries SI-GIR ?08: Proceedings of the 31st annual internationalACM SIGIR conference on Research and developmentin information retrieval, 729-730.JeanPhilippe Cointet, Emmanuel Faure and Camille Roth2008.
Intertemporal topic correlations in online media: A Comparative Study on Weblogs and News Web-sites ICWSM ?08: International Coference on We-blogs and Social MediaLevon Lloyd, Prachi Kaulgud and Steven Skiena 2006.Newspapers vs. Blogs: Who Gets the Scoop?
AAAISpring Symposium on Computational Approaches toAnalyzing WeblogsMichael Gamon, Sumit Basu, Dmitriy Belenko, DanyelFisher, Matthew Hurst, and Arnd Christian Konig2008.
BLEWS: Using Blogs to Provide Context forNews Articles ICWSM ?08: International Coferenceon Weblogs and Social MediaSanjay Sood, Sara Owsley, Kristian Hammond and LarryBirnbaum 2007.
TagAssist: Automatic Tag Sugges-tion for Blog Posts ICWSM ?07: International Cofer-ence on Weblogs and Social MediaFernando Diaz 2009.
Integration of news content intoweb results WSDM ?09: Proceedings of the SecondACM International Conference on Web Search andData Mining, 182-191.Beate Krause, Andreas Hotho and Gerd Stumme 2008.A Comparison of Social Bookmarking with Tradi-tional Search Advances in Information Retrieval, Vol4956/2008:101-113.Paul Heymann, Georgia Koutrika and Hector Garcia-Molina 2008.
Can social bookmarking improve websearch?
WSDM ?08: Proceedings of the internationalconference on Web search and web data mining, 195-206.Shenghua Bao, Guirong Xue, Xiaoyuan Wu, Yong Yu,Ben Fei and Zhong Su 2007.
Optimizing web searchusing social annotations WWW ?07: Proceedings ofthe 16th international conference on World Wide Web,501-510.Kerstin Bischoff, Claudiu S. Firan, Wolfgang Nejdl andRaluca Paiu 2008.
Can all tags be used for search?CIKM ?08: Proceeding of the 17th ACM conferenceon Information and knowledge management, 193-202.Rosie Jones, Benjamin Rey and Omid Madani 2006.Generating query substitutions Proceedings of the15th international conference on World Wide Web,,387-396.Filip Radlinski and Thorsten Joachims 2005.
Querychains: learning to rank from implicit feedback Pro-ceedings of the 11th ACM SIGKDD international con-ference on Knowledge discovery in data mining, 239-248.Qiaozhu Mei, Dengyong Zhou and Kenneth Church2008.
Query suggestion using hitting time CIKM ?08:Proceeding of the 17th ACM conference on Informa-tion and knowledge management, 469-478Andrei Broder 2002.
A Taxonomy of Web Search SIGIRForum, Vol 36(2):3-10.Solomon Kullback and Richard Leibler 1951.
On In-formation and Sufficience Annals of MathematicalStatistics, Vol 22(1):79-86.David A.
Field 1988.
Laplacian Smoothing and Delau-nay Triangulations Communications in Applied Nu-merical Methods, Vol 4:709-712.Stephen M. Katz 1987 Estimation of probabilities fromsparse data for the language model component of aspeech recogniser IEEE Transactions on Acoustics,Speech, and Signal Processing, 35(3), 400-401.Ted Pedersen, Siddharth Patwardhan and Jason Miche-lizzi 2004.
WordNet::Similarity: measuring the relat-edness of concepts PHLT-NAACL ?04: DemonstrationPapers at HLT-NAACL 2004 on XX, 38-41.1087
