Answering Clarification QuestionsMatthew Purver1, Patrick G.T.
Healey2, James King2, Jonathan Ginzburg1 and Greg J. Mills21Department of Computer ScienceKing?s College, LondonLondon WC2R 2LS, UK2Department of Computer ScienceQueen Mary, University of LondonLondon E1 4NS, UKAbstractThis paper describes the results of cor-pus and experimental investigation intothe factors that affect the way clarifica-tion questions in dialogue are interpreted,and the way they are responded to.
Wepresent some results from an investigationusing the BNC which show some generalcorrelations between clarification requesttype, likelihood of answering, answer typeand distance between question and an-swer.
We then describe a new experi-mental technique for integrating manip-ulations into text-based synchronous dia-logue, and give more specific results con-cerning the effect of word category andlevel of grounding on interpretation andresponse type.1 IntroductionRequesting clarification is a vital part of the com-municative process and has received attention fromboth the formal semantic (Ginzburg and Cooper,2001; Ginzburg and Cooper, forthcoming) and con-versation analytic traditions (Schegloff, 1987), butlittle in the computational dialogue system commu-nity.
In theory, a perfect dialogue system should beable to interpret and deal with clarification requests(CRs) made by the user in order to elicit clarifica-tion of some part of a system utterance, and be ableto request clarification itelf of some part of a user ut-terance.
This is no easy task ?
CRs may take manydifferent forms (often highly elliptical), and can beintended to be interpreted with many different read-ings which query different aspects of the original ut-terance.
As a result, dialogue system design has tra-ditionally attempted to avoid the necessity for CRinterpretation by making system utterances as clearand precise as possible, and avoid having to generateall but the most simple CRs by using robust shallowmethods of interpretation or by relying on highlydomain-dependent lexicons and grammars.
How-ever, as systems become more human-like, it seemslikely that we will have to cope with user CRs atsome stage; and the ability to generate system CRscan be useful in order to repair misunderstanding,disambiguate other utterances, and learn new words?
see (Knight, 1996; Dusan and Flanagan, 2002;Purver, 2002).The investigations presented here had two mainaims: to examine (a) how CRs are interpreted, and(b) how they are responded to.
The two are clearlydependent ?
the response must depend on the inter-pretation ?
but there are many other influencing fac-tors such as CR form, context and level of ground-ing.
Answers to (a) should help us with the follow-ing questions:?
What factors can help us disambiguate and cor-rectly interpret user CRs??
What factors should govern generation of sys-tem CRs such that they are correctly interpretedby the user?Answers to (b) should help with the following re-lated questions:?
How (and when) should we answer user CRs??
How (and when) should we expect users to re-spond to system CRs?The paper is organised as follows.
The next sec-tion gives a brief overview of CRs in general andsome previous corpus work.
Section 3 describes fur-ther corpus work which gives some general resultsconcerning response type.
Section 4 then describesa text-based dialogue experiment examining the de-tailed effects on interpretation and response of part-of-speech (PoS) type and level of grounding for oneparticular CR form, and section 5 then draws somegeneral conclusions.2 Clarification RequestsPurver et al (2001; 2002) presented a taxonomyof CR forms and readings derived from a corpusstudy using the British National Corpus (BNC) ?see (Burnard, 2000).
This showed that some formsshowed a high correlation with certain readings, butthat some were highly ambiguous.Purver et al (2002)?s taxonomy of CR forms isgiven in table 1 and CR readings in table 21.
SomeCRs (the non-reprise class) explicitly identify theclarification required, e.g.
?What did you say??
or?What do you mean?
?, and some forms (e.g.
literalreprises) appear to favour a particular reading almostexclusively, but most are more ambiguous.
Indeed,they found that the two most common forms (theconventional and reprise fragment form) could takeany reading.Although this corpus study provided informa-tion about the distribution of different CR formsand readings, it did not provide any informationabout the specific conditions which prompt partic-ular readings and affect how the CR is answered.In this paper we concentrate mostly on the reprisefragment (RF) form, where only a single part ofthe problem utterance, possibly a single word, isreprised2 as in example (1).
This form is not only1They also give a correction reading, which we have ex-cluded here: such CRs are almost exclusively self-correctionsand as such do not fit well with our discussion here.
They arealso very rare compared with the other classes, making up onlyabout 2% of CRs.2Such reprises need not be verbatim repeats: users may useanaphoric terms or use a clearer expression in order to clarifythe fragment in question.common (approximately 30% of CRs in the previ-ous study) and can appear with many readings (al-though biased towards a clausal reading ?
87% ofoccurrences), but specifies the problematic elementthat it clarifies quite precisely, and therefore shouldgive us scope for examining the effect of features ofthat element.
(1)3Gary: Aye, but <pause> youknow <pause> like youse- she mentioned one inparticular, likeJake: What?Gary: the word skeilthJake: Skeilth?Lilias: Mm.Gary: Aha.Jake: Aye, yeah, yeah, take skeilth.Intuitively, at least two such features would beexpected to affect the type of reading assigned toa RF: PoS category and level of grounding.4 ThePoS category of the reprised word should influenceexpectations about what is being clarified.
For ex-ample, reprise of a content word (e.g.
noun or verb)should be more likely to signal a constituent problemthan a reprise of a function word (e.g.
preposition ordeterminer).
Dialogue participants would normallyassume that the meaning of function words is wellknown in a particular linguistic community and that,as a result, a reprise of a function word is more likelyto signal clausal or lexical problems.
RF interpreta-tion should also depend on whether a reprised frag-ment is already considered to have been groundedby the participants in a conversation.
For example,a reprise of a proper noun would be more likely tobe read as signalling a constituent problem if it oc-curs on the first mention than on second mention.All things being equal, the content of a constituentis already considered to have been established by thetime a second mention occurs.3 Corpus InvestigationAccordingly we have re-examined the corpus fromthe above study in order to add information about3BNC file KPD, sentences 578?5844Another is intonation.
However, there is no intonationalinformation in the BNC.
In the future we hope to investigatethis using other corpora and experimental methods.Class Description Examplenon Non-Reprise ?What did you say?
?wot Conventional ?Pardon?
?frg Reprise Fragment ?Paris?
?slu Reprise Sluice ?Where?
?lit Literal Reprise ?You want to go to Paris?
?sub Wh-Subsituted Reprise ?You want to go where?
?gap Gap ?You want to go to .
.
.
?
?fil Gap Filler ?.
.
.
Paris?
?oth Other OtherTable 1: CR formsClass Description Paraphrasecla Clausal ?Are you asking/telling me that .
.
.
X .
.
.
?
?con Constituent ?What/who do you mean by ?X??
?lex Lexical ?Did you utter ?X??
?oth Other OtherTable 2: CR readingscategory, grounding and method of answering.3.1 MethodThe same corpus was re-marked for four attributes:response type and CR-answer distance, and the PoSand last mention of the original source element.The markup scheme used for response typeevolved during the study and is shown in table 3:it includes classification of apparently unansweredCRs into those that may have been answered, butthe sentence possibly containing an answer was tran-scribed in the BNC as <unclear>; those that ap-pear to have remained unanswered because the CRinitiator continued their turn without pause; andthose that are not answered at all (or at least wherewe have no indication of an answer ?
eye contact,head movement etc.
are not recorded in the BNC butcould function as answers).
In cases where the ini-tial response was followed by further information,both were recorded, but the results here are pre-sented only for the initial response.
Further worklater may take both into account, along the lines of(Hockey et al, 1997) who showed this to be impor-tant for questions in general.CR-answer distance was marked in terms of thesentence numbering scheme in the BNC ?
in thesecases it corresponds very closely to distance inspeaker turns, although the correspondence is notexact.PoS category and time of last mention of thesource element were marked, but have not currentlybeen used due to lack of useful data (see below).Reliability of the markup has not yet been exam-ined.
However, the method is close to that of (Purveret al, 2002) (and the corpus is identical), where re-liability was examined and found to be acceptable.We then examined the correlation between CR typeand response type, between reading and responsetype, and the spread of CR-answer distance.3.2 Results3.2.1 Response TypeResults for response type are shown in table 4 asraw numbers, and also in table 5 as percentages foreach CR type, with the none, cont, uncl and quryclasses conflated as one ?unanswered?
class, andonly the most common 4 CR forms shown.The most striking result is perhaps the high over-all number of CRs that do not receive an answer:39% of all CRs do not appear to be answered overall,although this reduces to 17% when taking accountof those marked uncl (possible answers transcribednone No answercont CR initiator continues immediatelyuncl Possible answer but transcribed as <unclear>qury CR explicitly queriedfrg Answered with parallel fragmentsent Answered with full sentenceyn Answered with polar particleTable 3: CR response typesas <unclear>) and cont (the CR-raiser continueswithout waiting).
The most common forms (conven-tional and RF) appear to be answered least ?
around45% go unanswered for both.
The form which ap-pears to be most likely to be answered overall is theexplicit non-conventional form.Some forms appear to have high correlationswith particular response types.
As might be ex-pected, sluices (which are wh-questions) are gen-erally answered with fragments, and never with apolar yes/no answer.
Yes/no answers also seem tobe unsuitable for the conventional CR form, whichis generally answered with a full sentence.
RFs,conversely, are not often answered with full sen-tences, but can be responded to either by fragmentsor yes/no answers.Similarly, from tables 6 and 7 (again, percentagesgiven for each CR reading, with ?unanswered?
re-sponse types conflated and only the most common 3readings shown) we can see that there is a correla-tion between reading and response type, but that thiscorrelation is also not as simple as a direct reading-answer correspondence.
Clausal CRs are unlikely tobe answered with full sentences, but can get eitherfragment or yes/no responses.
Constituent CRs areless likely to get yes/no responses but could get ei-ther other type.
Interestingly, constituent CRs seemto be roughly twice as likely to get a response asclausal or lexical CRs (even though there are fewerexamples of constituent CRs than the others, thisdifference is statistically significant, with a ?2(1) testshowing <0.5% probability of independence).3.2.2 Answer DistanceResults for CR-answer distance are shown in ta-ble 8.
It is clear that the vast majority (94%) of CRsthat are answered are answered in the immediatelyunans frg sent ynwot 45.6 8.7 44.8 0.8 (100)frg 43.2 21.1 3.4 32.2 (100)slu 37.0 50.0 12.9 0 (100)non 13.4 26.9 26.9 32.6 (100)Table 5: BNC results: Response type as percentagesfor each CR formunans frg sent yncla 39.8 22.2 7.8 30.0 (100)con 20.0 35.0 33.3 11.6 (100)lex 42.7 17.2 36.5 3.4 (100)Table 7: BNC results: Response type as percentagesfor each CR reading1 2 3 >3 TotalDistance 273 14 2 0 289Table 8: CR-answer distance (sentences)following sentence, and that none are left longerthan 3 sentences.
While we do not yet have concreteequivalent figures for non-clarificational questions,a study is in progress and initial indications are thatin general, answers are less immediate: only about70% have distance 1, with some up to distance 6.5We therefore expect that (a) answering user CRsmust be done immediately, and that any dialoguemanagement scheme must take this into account,and (b) we should expect answers to any systemCRs to come immediately ?
interpretation routines(we are thinking especially of any ellipsis resolutionroutines here) should not assume that later turns are5Thanks to Raquel Ferna?ndez for providing us with thesepreliminary figures.none cont uncl qury frg sent yn Totalwot 21 13 24 0 11 57 1 127frg 23 22 6 0 25 4 38 118slu 8 6 5 1 27 7 0 54non 4 2 1 0 14 14 17 52lit 5 2 1 0 1 1 10 20fil 3 0 1 0 7 1 4 16sub 4 0 3 0 4 4 0 15gap 1 0 0 0 1 0 0 2oth 0 0 0 1 0 1 0 2Total 69 45 41 2 90 89 70 406Table 4: BNC results: Response type vs. CR formnone cont uncl qury frg sent yn Totalcla 33 31 11 2 43 15 58 193con 9 3 0 0 21 20 7 60lex 21 11 30 0 25 53 5 145oth 5 0 0 0 0 1 0 6Total 69 45 41 2 90 89 70 406Table 6: BNC results: Response type vs. CR readingrelevant to the CR.3.2.3 Further DetailsWhile interesting, we would like to know moredetail than the general trends described above: inparticular we would like to know the effect ofthe factors we have mentioned (word category andgrounding) for particular forms.
As stated above,we concentrate here on the reprise fragment form.Examination of original CR source fragment PoScategory, in order to test the effect of the con-tent/function distinction, showed that almost all RFswere of content words or whole phrases: only 6 of118 RFs were of function words, all of which weredeterminers (mostly numbers).
This is interesting initself: perhaps RFs are unlikely to be used to clarifyuses of e.g.
prepositions.
However, the effect maybe due to lack of data, and does not provide us withany way of testing the distinction between clausaland constituent reading that we expect.Markup of last mention of the original sourcefragment has also not given results in which we canbe confident.
For RFs, we have seen that all con-stituent readings occur on the first mention of thefragment (as expected) ?
but there are too few ofthese examples to draw any firm conclusions.
It isalso impossible to know whether first mention in thetranscription is really the first mention between theparticipants: we do not know what happened beforethe tape was turned on, what their shared history is,or what is said during the frequent portions markedas <unclear>.So we need more information than our currentcorpus can provide.
In order to examine these ef-fects properly we have therefore designed an exper-imental technique to allow dialogues to be manipu-lated directly, with reprises with the desired proper-ties automatically introduced into the conversation.The next section describes this technique and the ex-periment performed.4 Experimental WorkEmpirical analyses of dialogue phenomena havetypically focused either on detailed descriptive anal-yses of corpora of conversations (Schegloff, 1987)or on the experimental manipulation of relativelyglobal parameters of interaction such as task type orcommunicative modality (Clark and Wilkes-Gibbs,1986), (Garrod and Doherty, 1994).
These stud-ies have been used to to motivate a variety of pro-posals about turn-level mechanisms and proceduresthat sustain dialogue co-ordination.
Further devel-opment and testing of these proposals has, how-ever, been limited by the indirect nature of the avail-able evidence.
Corpus studies provide, retrospec-tive, correlational data which is susceptible to chal-lenge and re-interpretation.
Current psycholinguis-tic techniques do not provide ways of integrating ex-perimental manipulations into interactions in a man-ner that is sensitive to the linguistic and conversa-tional context.
This section introduces a techniquefor carrying out experiments in which text-based in-teractions can be directly manipulated at the turnlevel, and gives the results of an experiment whichuses this approach to investigate the effects of thefactors mentioned above on interpretation and re-ponse to RFs.
We also briefly discuss the range ofpotential applications and some of the practical lim-itations of the approach in the context of the experi-mental results.4.1 Manipulating ?Chat?
InteractionsThe experimental technique presented here draws ontwo general developments.
Firstly, the increasinguse of text-based forms of synchronous conversa-tional interaction, for example: chat rooms (MUD?s,MOO?s etc.
), instant messaging, and some onlineconferencing tools.
Secondly, advances in naturallanguage processing technology which make someforms of text processing and transformation fastenough to be performed on a time scale consistentwith exchanges of turns in synchronous text chat.The basic paradigm involves pairs of subjects,seated in different rooms, communicating using asynchronous text chat tool (see figure 1 for an ex-ample).
However, instead of passing each completedturn directly to the appropriate chat clients, each turnis routed via a server.
Depending on the specificgoals of the experiment, the server can be used tosystematically modify turns in a variety of ways.
Forexample, some simple forms of mis-communicationcan be introduced into an interaction by transform-ing the order of characters in some of the inputwords or by substituting words with plausible non-words.
Importantly, the server controls which mod-ifications are broadcast to which participant.
So, ifparticipant A types the word ?table?
the sever canecho back A: table to participant A and a trans-formed version, say, ?blate?
to participant B whosees A: blate.
The ability to set up controlledasymmetries of this kind between the participants ina interaction creates a powerful range of experimen-tal possibilities.
Here, we describe an application ofthis technique to the investigation of reprise clarifi-cation requests (CR?s).A chat-tool experiment was designed to test thefollowing hypotheses:1.
RFs for function words will normally receiveclausal readings, whereas both clausal and con-stituent readings will be available for contentwords.2.
RFs for content words will receive more con-stituent readings on first mention than on sec-ond mention.3.
No difference is predicted for RFs for functionwords on first vs. second mention.4.2 MethodTwo tasks were used to elicit dialogue, a balloondebate and a story-telling task.
In the balloon de-bate subjects are presented with a fictional scenarioin which a balloon is losing altitude and about tocrash.
The only way for any of three passengers tosurvive is for one of them to jump to a certain death.The three passengers are; Dr. Nick Riviera, a can-cer scientist, Mrs. Susie Derkins, a pregnant primaryschool teacher, and Mr. Tom Derkins, the balloonpilot and Susie?s husband.
Subjects are asked to de-cide who should jump.
The advantages of this taskare that it is effective at generating debates betweensubjects and involves repeated references to particu-lar individuals.Following (Bavelas et al, 1992), the second di-alogue task used was the story-telling task.
In thiscase subjects are asked to relate a ?near-miss?
storyabout some experience in which something bad al-most happened but in the end everything was okay.This was chosen because, unlike the balloon task,the topic of the exchange is unrestricted, in effecta random factor, and the interaction relates to realevents.4.2.1 SubjectsTwenty-eight subjects were recruited, 20 maleand 8 female, average age 19 years, from computerscience and IT undergraduate students.
They wererecruited in pairs to ensure that the members of apair were familiar with one another and only sub-jects who had experience with some form of textchat such as chat rooms, IRC, ICQ or other mes-saging systems were used.
Each subject was paidat a rate of ?7.50 per hour for participating in theexperiment.4.2.2 MaterialsA custom experimental chat tool, written in Javaand Perl, was used for the experiment.
The user in-terface is similar to instant messaging applications:a lower window is used to enter text, and the con-versation is displayed in the main upper window asit emerges (see figure 1).
The chat clients were runon two Fujitsu LCD tablet computers with text in-put via standard external keyboards, with the serverrunning on a standard PC in a separate room.User Interface The Chattool client user interfaceis written in Java and is designed to be familiarto subjects experienced with instant messaging/chatapplications.
The application window is split intotwo panes: a lower pane for text entry and an up-per pane in which the conversation is displayed (seefigure 1).
A status display between the two panesshows whether the other participant is active (typ-ing) at any time.
This can be artificially controlledduring the generation of artificial turns to make itappear as if they are generated by the other partici-pant.
The client also has the ability to display an er-ror message and prevent text entry: this can be usedto delay one participant while the other is engagedin an artificially-generated turn sequence.Server Each turn is submitted to a server (alsowritten in Java) on a separate machine when a ?Send?button or the ?Return?
key is pressed.
This serverpasses the text to a NLP component for processingand possible transformation, and then displays theoriginal version to the originator client, and the pro-cessed (or artificially generated) version to the otherclient.
The server records all turns, together witheach key press from both clients, for later analysis.This data is also used on the fly to control the speedand capitalisation of artificially generated turns, tobe as realistic a simulation of the relevant subject aspossible.NLP Component The NLP component consistsof a Perl text-processing module which commu-nicates with various external NLP modules as re-quired: PoS tagging can be performed using LT-POS (Mikheev, 1997), word rarity/frequency tag-ging using a custom tagger based on the BNC (Kil-garriff, 1997), and synonym generation using Word-Net (Fellbaum, 1998).Experimental parameters are specified as a set ofrules which are applied to each word in turn.
Pre-conditions for the application of the rule can be spec-ified in terms of PoS, word frequency and the worditself, together with contextual factors such as thetime since the last artificial turn was generated, anda probability threshold to prevent behaviour appear-ing too regular.
The effect of the rule can be totransform the word in question (by substitution withanother word, a synonym or a randomly generatednon-word, or by letter order scrambling) or to triggeran artificially generated turn sequence (currently areprise fragment, followed by an acknowledgement,although other turn types are possible).The current experimental setup consists of ruleswhich generate pairs of RFs and subsequentacknowledgements6, for proper nouns, commonnouns, verbs, determiners and prepositions, withprobabilities determined during a pilot experimentto give reasonable numbers of RFs per subject.
Nouse is made of word rarity or synonyms.The turn sequences are carried out by (a) present-ing the artificially-generated RF to the relevant clientonly; (b) waiting for a response from that client, pre-venting the other client from getting too far aheadby locking the interface if necessary; (c) presentingan acknowledgement to that response; and (d) pre-senting any text typed by the other client during thesequence.4.2.3 ProcedurePrior to taking part subjects were informed thatthe experimenters were carrying out a study of theeffects of a network-based chat tool on the way peo-6Acknowledgements are randomly chosen amongst: ?ah?,?oh?, ?oh ok?, ?right?, ?oh right?, ?uh huh?, ?i see?, ?sure?.ple interact with one another.
They were told thattheir interaction would be logged, anonymously, andkept for subsequent analysis.
Subjects were advisedthat they could also request the log to be deleted af-ter completion of the interaction.
They were not in-formed of the artificial interventions until afterwards(see below).At the start of the experiment subjects were givena brief demonstration of the operation of the chattool.To prevent concurrent verbal or gestural interac-tion subjects were seated in separate rooms.
Eachpair performed both dialogue tasks and were givenwritten instructions in each case.
The balloon taskwas carried out once and the story-telling task twice;one story for each participant.
To control for or-der effects the order of presentation of the two taskswas counterbalanced across pairs.
A 10-minute timelimit was imposed on both tasks.
At the end ofthe experiment subjects were fully debriefed and theintervention using ?artificial?
clarifications was ex-plained to them.This resulted in a within-subjects design with twofactors; category of reprise fragment and level ofgrounding (first vs. second mention).After the experiment, the logs were manually cor-rected for the PoS category of the RF and for thefirst/second mention clarification.
PoS required cor-rection as the tagger produced incorrect word cate-gories in approximately 30% of cases.
In some in-stances this was due to typing errors or text-specificconventions, such as ?k?
for ?okay?, that were notrecognised.
Detection and classification of propernouns was also sensitive to capitalisation.
Subjectswere not consistent or conventional in their capitali-sation of words and this caused some misclassifica-tions.
In addition a small proportion of erroneoustags were found.
Each system-generated CR waschecked and, where appropriate, corrected.
Becausepairs completed both tasks together CRs classifiedas ?first mentions?
were checked to ensure that theyhadn?t already occured in a previous dialogue.4.3 ResultsThe readings attributed to each RF were classifiedin the same way as the original BNC-based cor-pus, with the addition of one further category: non-clarificational, referring to situations in which thefragment is treated as something other than a CR(this did not apply when building the original cor-pus, as only utterances treated as CRs were con-sidered).
In the experimental results, gap, lexicaland non-clarificational readings were low frequencyevents (4, 1 and 8 instances respectively) and no in-stances of correction readings were noted.
These fig-ures are comparable with (Purver et al, 2002)?s ob-servations for the BNC.
For statistical analysis thesethree categories of reading were grouped together as?Other?.Across the corpus as a whole a total of 215system-generated RFs were produced.
In 50% ofcases the system-generated clarification received noresponse from the target participant.
This may bedue in part to the medium: unlike verbal exchanges,participants in text-chat can produce their turns si-multaneously.
This can result in turns getting outof sequence since users may still be responding to aprior turn when a new turn arrives.
Users must thentrade off the cost of undoing their turn in progressto respond to the new one, against going ahead any-way and responding to the new turn later if it seemsnecessary.
Thus in some cases we observed that theresponse to a clarification was displaced to the endof the turn in progress or to a subsequent turn.
How-ever, comparison with the BNC results from sec-tion 3 above show similar figures: only 56% of thefrg class received a clear answer.
Although thetrue figure will be higher (of the 56%, 5% may havebeen answered, but the next turn was transcribed as<unclear>, and we cannot know in how manycases the reprise may have been answered usingnon-verbal signals), it seems likely that a significantproportion may simply be ignored.Response CategoryCategory None Con Cla OtherCont (1st) 29 14 23 4Cont (2nd) 43 7 16 9Func (1st) 6 0 0 6Func (2nd) 20 0 1 9Table 9: Frequency of Reading Types By RF Cate-gory and MentionThe distribution of reading types according toword category was tested firstly by comparing thefrequency of Clausal, Constituent, and Other read-ings for content words and function words.
Thisproved to be reliably different (?2(2) = 35.3, p =0.00).7 As table 9 shows, RFs of Function wordswere almost exclusively interpreted as Other, i.e.
ei-ther Gap, Lexical or Non-clarificational.
By contrastContent word reprises were interpreted as ClausalCRs 53% of the time, as Constituent CRs 29% ofthe time and as Other 18% of the time.Content word and Function word clarificationswere also compared for the the frequency withwhich they received a response.
This showed noreliable difference (?2(1) = 1.95, p = 0.16) indicat-ing that although the pattern of interpretation forContent and Function reprises is different they areequally likely to receive some kind of response.The influence of grounding on reading type wasassessed firstly by comparing the relative frequencyof Constituent, Clausal and Other readings on firstand second mention.
This was reliably different(?2(2) = 6.28, p = 0.04) indicating that level ofgrounding affects the reading assigned.
A focussedcomparison of Constituent and Clausal readings onfirst and second mention shows no reliable differ-ence (?2(1) = 0.0, p = 0.92).
Together these findingsindicate that, across all word categories, Constituentand Clausal readings are more likely for RF?s of afirst mention than a second mention and, conversely,Other readings are less likely for RF?s to a first men-tion than a second mention.The effect of grounding on the relative frequencywith which a clarification received a response wasalso tested.
This indicated a strong effect of mention(?2(1) = 12.01, p = 0.00); 58% of reprise clarificationsof first mentions recieved a response whereas only33% of second mention clarifications did.4.4 DiscussionThe experimental results support two basic conclu-sions.
Firstly, people?s interpretation of the type ofCR a reprise fragment is intended to make is influ-enced both by the category of the reprise fragmentand its level of grounding.
Secondly, reprise frag-ment CRs to first mentions are much more likely tobe responded to than reprise fragment CRs for sec-7A criterion level of p < 0.05 was adopted for all statisticaltests.ond mentions.Text-based and verbal interaction have differentproperties as communicative media.
Amongst otherthings, in text-chat turns take longer to produce,are normally produced in overlap, and they persistfor longer.
However, even given these differences,the general pattern of clarifications observed in theexperimental task is similar to that noted in ver-bal dialogue.
In particular, Lexical, Gap and Non-clarificational readings are infrequent and reprisefragment clarifications are ignored with surprisingfrequency.
In the present data, the clearest contrastbetween text-based and verbal interaction is in therelative frequency of Constituent and Clausal read-ings.
In the BNC reprise fragments receive Clausalreadings in 87% of cases, and constituent readings in6% of cases.
In the experimental corpus they receiveClausal readings in 48% of cases and Constituentreadings in 34% of cases.These findings demonstrate the viability, andsome limitations, of investigating dialogue co-ordination through the manipulation of chat-toolbased interactions.
The chat tool was successfulin producing plausible clarification sequences.
Al-though in some cases participants had difficultymaking sense of the artificial clarifications this didnot make them distinguishable from other, real, butequally problematic turns from other participants.The clarifications were mostly successful in creat-ing realistic exchanges such as those illustrated infigures 2 and 3.
When questioned during debriefing,no participants reported any suspicions about the ex-perimental manipulation.The main practical difficulty encountered in thepresent study related to text-chat conventions suchas novel spellings, abbreviations, and use of ?smi-leys?.
This created specific problems for the PoStagger which assumes a more standard form of En-glish.
These problems were also compounded by thenoise introduced by typing errors and inconsistencyin spelling and capitalisation.The experiment presented here exploits only onepossibility for the use of this technique.
Otherprossible manipulations include; manipulation ofdistance, in turns or time, between target and probe,substitution of synonyms, hyponyms and hyper-nyms, introduction of artifical turns, blocking ofcertain forms of response.
The important potentialit carries, particularly in comparison with corpus-based techniques, is in the investigation of dialoguephenomena which for various reasons are infrequentin existing corpora.5 ConclusionsThe main conclusions we draw from the results pre-sented here are as follows:?
Reprise CRs appear to go without response farmore often than might be expected, both in theBNC and in our experimental corpus.
Bothmay be effects of the media (transcription inone case, turn sequencing overlap in the other),but the figures are large enough and similarenough to warrant further investigation.?
Corpus investigation shows some strong corre-lations between CR form and expected answertype.
It also shows that responses to CRs, whenthey come, come immediately.?
Both word PoS category and first/second men-tion appear to be reliable indicators of RF read-ing.
This can help us in disambiguating userCRs, and in choosing forms when generatingsystem CRs.?
RFs generated on the first mention of a wordhave a higher likelihood of receiving a responsethan on second mention.?
We have presented a new experimental tech-nique for manipulating dialogue, which we be-lieve has many potential uses in dialogue re-search.6 AcknowledgmentsThis work was supported by the EPSRC under theproject ?ROSSINI: Role of Surface Structural Infor-mation in Dialogue?
(GR/R04942/01).ReferencesJ.B.
Bavelas, N. Chovil, D. Lawrie, and L. Wade.
1992.Interactive gestures.
Discourse Processes, 15:469?489.Lou Burnard.
2000.
Reference Guide for the BritishNational Corpus (World Edition).
Oxford UniversityComputing Services.Herbert H. Clark and Deanna Wilkes-Gibbs.
1986.
Re-ferring as a collaborative process.
Cognition, 22:1?39.Sorin Dusan and James Flanagan.
2002.
Adaptive dialogbased upon multimodal language acquisition.
In Pro-ceedings of the Fourth IEEE International Conferenceon Multimodal Interfaces, Pittsburgh, October.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Simon Garrod and Gwyneth Doherty.
1994.
Conversa-tion, co-ordination and convention: an empirical in-vestigation of how groups establish linguistic conven-tions.
Cognition, 53:181?215.Jonathan Ginzburg and Robin Cooper.
2001.
Resolv-ing ellipsis in clarification.
In Proceedings of the 39thMeeting of the ACL, pages 236?243.
Association forComputational Linguistics, July.Jonathan Ginzburg and Robin Cooper.
forthcoming.Clarification, ellipsis, and the nature of contextual up-dates.
Linguistics and Philosophy.Beth Ann Hockey, Deborah Rossen-Knill, Beverly Spe-jewski, Matthew Stone, and Stephen Isard.
1997.
Canyou predict answers to Yes/No questions?
Yes, No andStuff.
In Proceedings of Eurospeech ?97.Adam Kilgarriff.
1997.
Putting frequencies in thedictionary.
International Journal of Lexicography,10(2):135?155.Kevin Knight.
1996.
Learning word meanings by in-struction.
In Proceedings of the Thirteenth NationalConference on Artifical Intelligence, pages 447?454.AAAI/IAAI.A.
Mikheev.
1997.
Automatic rule induction for un-known word guessing.
Computational Linguistics,23(3):405?423.Matthew Purver, Jonathan Ginzburg, and Patrick Healey.2001.
On the means for clarification in dialogue.
InProceedings of the 2nd ACL SIGdial Workshop on Dis-course and Dialogue, pages 116?125.
Association forComputational Linguistics, September.Matthew Purver, Jonathan Ginzburg, and Patrick Healey.2002.
On the means for clarification in dialogue.In R. Smith and J. van Kuppevelt, editors, Currentand New Directions in Discourse & Dialogue.
KluwerAcademic Publishers.Matthew Purver.
2002.
Processing unknown words in adialogue system.
In Proceedings of the 3rd ACL SIG-dial Workshop on Discourse and Dialogue, pages 174?183.
Association for Computational Linguistics, July.E.
Schegloff.
1987.
Some sources of misunderstandingin talk-in-interaction.
Linguistics, 25:201?218.Figure 1: Chattool Client InterfaceSubject A?s View Subject B?s ViewA: Obviously the relativeswere coming around likethey do to see meB: Obviously the relativeswere coming around likethey do to see meProbe ?
A: relatives?Block B: Yeah just unts and unclesAck ?
A: ahA: yeah B: yeahFigure 2: Story Telling Task Excerpt, Noun Clarification, Subjects 1 & 2Subject A?s View Subject B?s ViewA: so we agree B: so we agreeB: agree?
?
ProbeA: yeah to chuck out SusiederkinsBlockB: uh huh ?
AckA: yes B: yesFigure 3: Balloon Task Excerpt, Verb Clarification, Subjects 3 & 4
