Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1136?1145,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsGender Inference of Twitter Users in Non-English ContextsMorgane CiotSchool of Computer ScienceMcGill UniversityMontreal, Quebec, Canadamorgane.ciot@mail.mcgill.caMorgan SondereggerDepartment of LinguisticsMcGill UniversityMontreal, Quebec, Canadamorgan.sonderegger@mcgill.caDerek RuthsSchool of Computer ScienceMcGill UniversityMontreal, Quebec, Canadaderek.ruths@mcgill.caAbstractWhile much work has considered the problemof latent attribute inference for users of socialmedia such as Twitter, little has been done onnon-English-based content and users.
Here,we conduct the first assessment of latent at-tribute inference in languages beyond English,focusing on gender inference.
We find thatthe gender inference problem in quite diverselanguages can be addressed using existing ma-chinery.
Further, accuracy gains can be madeby taking language-specific features into ac-count.
We identify languages with complexorthography, such as Japanese, as difficult forexisting methods, suggesting a valuable direc-tion for future research.1 IntroductionA 2012 study reported that US-based Twitter usersnow account for only 28% of all active accounts onthe platform (Semiocast, 2012).
Brazil, Japan, India,and Indonesia all rank in the top 10, each with over5% of all users.
These and other findings confirmthat Twitter enjoys widespread international popu-larity and usage.
This is also reflected in the multi-national community of researchers who study hu-man behavior on Twitter and related platforms, e.g.
(Sakaki et al 2010; Tumasjan et al 2010; Kim andPark, 2012).It is remarkable, then, that advances in la-tent attribute inference on social media have beenlargely confined to English content, e.g.
(Liu andRuths, 2013; Zamal et al 2012; Pennacchiotti andPopescu, 2011; Conover et al 2011a).
This biasmay be partially explained in the context of the re-search being conducted largely by anglophone re-searchers.
Nonetheless, it has created a notablesilence in the literature concerning the large-scaleanalysis of languages, cultures, and people on socialmedia who do not employ English.In this paper, we examine the problem of latent at-tribute inference outside the English-language con-text.
To our knowledge, this is the first such studyever conducted.
Here we specifically focus on gen-der inference, as it has been the basis for significantwork in recent years (Liu et al 2012; Zamal et al2012; Pennacchiotti and Popescu, 2011; Rao et al2010; Burger et al 2011).
Our work makes twocontributions.
First, we quantify the extent to whichestablished gender inference methods can be usedwith non-English Twitter content.
Second, we ex-plore the capacity for unique features of other lan-guages (besides English) to improve inference ac-curacy.
This second aspect, in particular, acknowl-edges the fact that latent attribute inference may beeasier in some languages due not to conventions inword usage, but to syntactic structure.In order to assess the extent to which existing gen-der inference machinery works for users who uselanguages other than English, we assembled Twit-ter datasets for languages that are both prevalent onTwitter and representative of diverse language fam-ilies: Japanese, Indonesian, Turkish, and French.Each dataset consisted of approximately 1000 userswho tweeted primarily in a given language.
We usedAmazon Mechanical Turk to manually label eachuser with their gender, using a language-agnostic la-beling strategy (Liu and Ruths, 2013).
For classi-fication, we employed a performant support vectormachine-based (SVM) technique that has been usedin a range of studies, e.g.
(Rao et al 2010; Burgeret al 2011; Zamal et al 2012).1136We found that, without any modification to thetypes of features given to the SVM, the classifier ac-curacy was comparable on English, French, and In-donesian.
Turkish actually performed much better,achieving 87% on average.
Gender in Japanese, incontrast, could not be reliably inferred with any rea-sonable accuracy (61% on average) despite numer-ous attempts to preprocess the tweets and tune theclassifier to accommodate the language?s complexorthography.
This indicates that existing approachesmay not generalize well to language systems withthousands of distinct unigrams (as opposed to tensor hundreds in the other languages considered).1To evaluate the extent to which language-specificfeatures might be used to boost the accuracy ofthe SVM classifier further, we focused on French.French is a valuable case study because, unlike En-glish, it has a number of syntax-based mechanismsthat can encode the gender of the speaker.
Themost common instantiation of gender marking is themodification of adjective and some past participleendings to match the gender of the subject in con-structions beginning with ?je suis?
(trans.
?I am?)constructions.
A classifier based on this insightachieved average accuracy of 90% on the vast ma-jority of French users, surpassing the accuracy ofstandard techniques on English or French.Overall, our results show that, with little modifi-cation, existing gender inference machinery can per-form comparably to English on several other lan-guages.
There are clear areas for substantial im-provement: incorporating language-specific featuresand, in the case of Japanese, finding better waysof accommodating the complex orthography.
Thesefindings identify promising directions for future re-search and will, hopefully, call attention to an im-portant area in the latent attribute inference domainin need of further work.2 BackgroundNon-English Twitter data mining and studies.Existing work on non-English Twitter content canbeen divided into two groups: surveys of the use ofseveral languages on the platform and studies of asocial phenomenon in a non-English body of tweets.1In this work, unigram, bigram, and k-gram refer to one,two, and k-character sequences in a language?s written form.To our knowledge, only a handful of the former va-riety exist.
One recent paper characterized the re-lationship between language and geography (Mo-canu et al 2012).
Another measured how high-leveltweet features (i.e., link, mention, and hashtag fre-quencies) vary across languages (Weerkamp et al2011).
These papers show that tweet structure andcontent can differ widely across languages.More work has been done in the latter category:analysis of social phenomena in a non-English con-text.
A well-known study evaluated usage of Twit-ter in the aftermath of the 2010 earthquake in Japan(Sakaki et al 2010).
Another Japanese-orientedstudy evaluated the impact of television on tweetedcontent (Akioka et al 2010).
Within this cate-gory, another recurring research topic is the anal-ysis of political discussion and elections.
Outsideof English-based analysis, some attention has beengiven to European and East Asian elections, e.g.
(Tumasjan et al 2010; Giglietto, 2012; Kim andPark, 2012).
However, few of these studies haveconsidered measures beyond simple hashtag fre-quencies, relative mention counts among politicians,and retweet counts.
The only study using morecomplex features for computational text analysis in-volved sentiment analysis of a set of German tweets(Tumasjan et al 2010).
However, the tweets inthis study (conducted at a German university) weretranslated into English prior to analysis, a step whichunderscores the significant bias towards English inthe literature on analyzing microtext, and the toolsavailable to researchers in this domain.Gender inference methods.
Gender inference is afield of research situated with the broader area of la-tent attribute inference.
The majority of recent workin this area has focused on Twitter users (Rao et al2010; Pennacchiotti and Popescu, 2011; Conover etal., 2011b; Burger et al 2011; Rao and Yarowsky,2010; Liu and Ruths, 2013; Liu et al 2012; Zamalet al 2012).
Classifiers have been built, predomi-nantly, using support vector machines, e.g.
(Rao etal., 2010; Pennacchiotti and Popescu, 2011; Burgeret al 2011; Zamal et al 2012), though boosted de-cision trees and latent dirichlet alcation systemshave also been evaluated, e.g.
(Pennacchiotti andPopescu, 2011; Conover et al 2011b).
With oneexception, gender inference accuracy has been re-1137ported between 80% and 85%.
The one study whichreported 90% accuracy involved the use of a datasetwhich has been shown to be quite different from typ-ical anglophone Twitter users (Burger et al 2011).This same study did involve non-English Twitterusers, but did not analyze the performance of theclassifier on different languages (e.g.
break downperformance by language, examine to what extentits results were due to better performance on somelanguages), or indeed discuss fully which languageswere present in their sample.
Thus, little can be in-ferred from Burger et als study about the relativeperformance of attribute inference methods on dif-ferent languages, which is the focus of our paper.Language families.
Human languages can beclassified into different language families, definedas a set of languages which are all descended from asingle, ancient parent language.
Languages whichare genetically related (in the same family), how-ever distantly, tend to share many more character-istics than languages from different families.Each language considered in this paper belongsto a different language family: French to Indo-European, Turkish to Altaic, Japanese to Japonic,and Indonesian to Austronesian.
Thus, these lan-guages are completely genetically unrelated, by def-inition.
Further, they are both geographically andculturally dispersed.
While they all have some loan-words from English, these constitute a tiny fractionof each language?s vocabulary.
This selection of lan-guages allows us to conduct the most far-reachingsurvey of non-English latent attribute inference per-formance to date.A variety of features make each language se-lected interesting within the gender inference con-text.
French is noteworthy for its grammatical gen-der.
All nouns, including people, are grammati-cally ?male?
or ?female.?
English, in contrast, hasseparate pronouns for people of different genders(e.g., ?he?, ?she?
), but does not have grammati-cal gender.
(Besides a handful of exceptions like?waiter?/?waitress?, there are no words besides pro-nouns which have different ?masculine?
and ?femi-nine?
forms.)
Indonesian, Turkish, and Japanese areall so-called genderless languages.
Like many lan-guages of the world, they do not have distinct maleand female pronouns (like English and French), orgrammatical gender (like French).3 General Gender InferenceIn order to evaluate the extent to which existing gen-der inference machinery can be used on users whosetweets are in languages other than English, we de-veloped gender-labeled datasets of Twitter users foreach language and then evaluated the performanceof a classifier on each.3.1 DataThe core data for this project consisted of fourdatasets of content from Twitter users who tweetedpredominantly in one of four languages?French,Indonesian, Turkish, and Japanese?collected usingthe methods described below.Data collection.
In order to identify users for can-didate inclusion in a particular language?s (hereafterthe target language) dataset, we walked the stream-ing output of the Twitter firehose and evaluated thelanguage of each tweet using the language mod-els provided by the Natural Language Toolkit (Bird,2006).
Users associated with tweets written in thetarget language were added to a list.
5000 such userswere identified for each language.
The latest 1000tweets for each user were downloaded.
This com-prised the base for the target language dataset.Assigning gender labels.
In prior work, e.g.
(Raoet al 2010; Pennacchiotti and Popescu, 2011; Za-mal et al 2012), the dominant way of obtain-ing datasets consisting of Twitter users with high-confidence gender-labels is to use gender-name as-sociations.
The use of name-gender associations areproblematic when non-English content is consideredbecause databases of anglophone name-gender asso-ciations are no longer useful (Mislove et al 2011).We instead used Amazon Mechanical Turk workersto identify the gender of the person shown in the pro-file picture associated with a user?s account (Liu andRuths, 2013).
In our datasets, each user?s profile pic-ture was coded by 5 separate workers.
Users withnon-photographic or celebrity-based profile pictureswas discarded, as well as any users with profile pic-tures where the gender could not be confidently as-sessed (less than 4 out of 5 votes for one gender).Table 1 shows the final composition of eachdataset.
In Japanese and Indonesian, we observed1138Table 1: The composition of the different languagedatasets used in this study.Language # Males # Females Total SizeFrench 437 506 943Indonesian 977 2260 3237Turkish 1672 1937 3609Japanese 309 520 829a notable difference in the number of males and fe-males in the dataset.
Measures were taken to ensurethat classifier results were not biased by these differ-ences within the datasets.3.2 MethodsThe majority of prior work in gender inference (andlatent inference in general) has used support vectormachines (SVMs).
We followed prior work in thisregard, particularly since our intent here is to eval-uate the relevance of existing gender inference ma-chinery on other languages.
For the present study,we adopted an SVM-based classifier, described in(Zamal et al 2012), that incorporated nearly all fea-tures used in prior work and showed comparable(and sometimes better) accuracy than other methods.Parameter values and kernel choices for the SVMare discussed in the source paper.Feature set.
SVM classifiers require thateach object to be classified be represented bya fixed-length feature vector.
The features weemployed were: k-top words, k-top digramsand trigrams, k-top hashtags, k-top mentions,tweet/retweet/hashtag/link/mention frequencies,and out/in-neighborhood size.
Note that ?k-top Xfeatures?
(e.g., k-top hashtags) refers to the k mostdiscriminating items of that type for each label (i.e.,Male/Female).
Thus, k-top words is actually 2kfeatures: the k words most associated with malesand the k words most associated with females.This list of features is the same set of features usedin (Zamal et al 2012), except that k-top stems andk-top co-stems were both dropped in our version.Both of these feature types are specific to English.Of course, word stems do exist in other languages,however we found that stemmers (the algorithmsthat identify and extract the appropriate stem froma word) were not available across the whole bankof languages.
Therefore, we omitted these stem andco-stem features.
We also added features for the us-age frequencies of Eastern-style and Western-styleemoticons but saw no discernible change in accu-racy; thus, these features are not discussed further.It is important to note that all features includedin our classifier are language-agnostic.
An n-gramis simply an n-character sequence drawn from thealphabet and additional symbols (numbers, punctu-ation, etc.)
present in tweets written in the targetlanguage.
Words are sequences of characters thatare bounded by whitespace or punctuation.
Hash-tags are words proceeded by a pound (?#?)
character,mentions by an ?@?
symbol.
A system that properlysupports unicode strings can implement all of thesenotions without knowing anything about the targetlanguage it is operating on.Tokenization of Japanese.
While all the defini-tions provided above for the SVM features are op-erational, there is a glaring disconnect between thewhitespace-border definition of a word and writtenconventions in Japanese.
Specifically, in Japanesewords are generally not separated by whitespace.We used a tokenizer to insert whitespace intoJapanese text to break up words.
Tokenizationwas done using Kuromoji, the software Japanesemorphological analyzer used and supported by theApache software Foundation (Atilika, 2012).
No-tably, this tool tokenizes the mixed character setsthat are often used in informal Japanese writing.As tokenization does involves some language-specific processing, its use here somewhat under-mines the objective set out for this project.
Thus, wereport the accuracy achieved for both untokenizedand tokenized Japanese tweets.
Curiously, tokeniza-tion was found to not make a difference in overallaverage accuracy.3.3 ResultsFor each dataset, 5-fold cross validation was usedto assess the classifier?s performance.
The value ofk = 20 was used for all k-top features, though theresults reported are robust to changes of this valuewithin reason (between 10 and 30).
If the num-bers of male and female users were unbalanced ina dataset, the larger set was subsampled randomlyto obtain a set of users the same size as the smallerlabeled set.
During the training process, the actual1139Table 2: The accuracy of the SVM-based classifier oneach of the language datasets.
In the case of Japanese,the performance is given for both the tokenized and un-tokenized versions of the dataset.
(Note that tokenizationdid not affect overall accuracy.
)Language Male Female OverallFrench 0.79 0.73 0.76Indonesian 0.87 0.80 0.83Turkish 0.89 0.85 0.87Japanese (t) 0.50 0.76 0.63Japanese (u) 0.58 0.68 0.63values of the features were extracted from the train-ing users (e.g., the k-top differentiating words formales and females were identified).
In this way,the gender model implemented by the SVM waslanguage-specific, in the sense that a particular lan-guage?s gender model contained a different set offeatures.
We call our method language-agnostic onthe grounds that, given a labeled set of users andtweets drawn from a particular language, a modelcan be built without any knowledge of the structureor content of the language itself.Tables in Supplementary Material show the fea-tures for the classifier built over each language?s en-tire dataset.
Note that to conduct the cross-fold eval-uation, new models (and hence different features)were recomputed for each fold.
As a result, the fea-tures reported are slightly different from those thatmight have appeared in the models for a given fold.Manual inspection, however, revealed that differ-ences were slight.
The features reported in the Sup-plementary Material can be safely considered a con-sensus among the models for the individual folds.The accuracy of the classifier for each languageis shown in Table 2.
Overall, the classifier demon-strated good performance on all languages ex-cept for Japanese.
Below, we consider the re-sults for each of the four languages in turn.
Ineach case we discuss language-specific trends inwhich words were most informative for inferringuser gender, and thus help explain the classifier?sperformance.
Throughout, we omit discussion ofnon-alphanumeric ?words?
(such as punctuation oremoticons), and call the k-top discriminating wordsfor male and female users the k-top male words andk-top female words.French.
The k-top words for men and women areof very different grammatical types.
Most malewords are prepositions or articles (16/25; e.g.
de?of?, un ?a/one?
); a few others are basic grammati-cal words (ne ?
[part of] not?, et ?and?
), or pronounsor verb forms referring to a single person or object(he/she/it), as well as one noun (France).
In con-trast, many female words (11/25) are pronouns orbasic verb forms referring to the speaker or a singleaddressee (je ?I?, mon/mes/ma ?my?, tu ?you?, j?ai?I have?).
Others are pronouns or basic verbs re-fer to a single person or object (elle, ?she/it?, c?est?it?s?
), as well as a few other frequent words (trop?too much?, pas ?
[part of] not?, oui ?yes?).
The mostsalient pattern is that use of words (pronouns, basicverbs) associated with talking about the speaker oraddressee indicates a tweet is more likely to be froma female user.
Heavy use of other common functionwords, specifically prepositions and articles, sug-gests a male user.
These patterns reflect known gen-der differences in word usage by male and femaleFrench speakers (Witsen, 1981).Indonesian.
Indonesian achieved performanceclosest to the inference accuracy for English re-ported in the literature.
The k-top lists for men andwomen give some justification for why the classifierperformed well.
Some differences can be tentativelylinked to general trends in how men and women uselanguage differently across cultures.
5/25 of men?sk-top words are nouns which are either related tosoccer (vs ?versus?, chelsea ?
[name of UK soccerteam]?, pemain ?player?)
or which could be relatedto soccer (jakarta, indonesia, malam ?night?
); incontrast, no women?s words are nouns.
It seemsplausible that men tweet about soccer significantlymore than women.
In such a situation, a reasonableconcern is that our classifier discriminated soccerfrom non-soccer enthusiasts rather than males fromfemales.
To address this, we confirmed that thesetopic-based words were not required for accurateclassification: a classifier in which soccer wordswere explicitly removed performed just as well(83.8% vs. 83.3%).More interestingly, many of the k-top words cor-respond to men and women using different termsof address and self-reference.
Among the k-topwords, 7/25 for men and 4/25 for women are terms1140of address or self-reference.
The terms men use aremostly highly informal, including the slang term lu(you) and the English borrowing bro; the addressterms women use are mostly medium-formality,such as aku (I) and kamu (you).
Thus, women seemto be using ?more polite?
self-reference and addressterms than men on average on Indonesian Twitter,in line with the more general tendency for womento use polite forms more frequently than men cross-culturally (Holmes, 1995).Turkish.
Turkish achieved notably high accuracy:the highest of all four languages considered.
Infact, to our knowledge, this is the highest accuracyachieved in the entire Twitter gender inference lit-erature on a dataset drawn from the Twitter gen-eral population.
The k-top lists of male and femalewords again give some justification for the classi-fier?s performance.
Many differences between themale and female lists can be linked to men andwomen talking about different topics, or to differ-ent people.
Several of the male words refer to soc-cer (gol ?goal?, galatasaray ?popular Istanbul team?,mac?
?match?, at ?
[part of imperative for] score?
),which men plausibly tweet about more.
As with In-donesian, a concern is that topics represent a biasedsample of the population.
Thus, we tested a classi-fier with soccer-specific terms removed, and againfound no difference in accuracy (86% vs. 87%).Many other k-top words are familiar terms of ad-dress for men (lan, abi, karde sim, adam, kanka) ora greeting used mainly between men (eyvallah), sug-gesting that male users are addressing or discussingmen more often than female users are.
In contrast,9/25 of the k-top female words are pronouns refer-ring to the speaker, a familiar addressee, or a thirdparty (he/she/it), while none of the k-top male wordsare, suggesting female users are more often talkingdirectly about themselves or to others.
Finally, 2/25of the k-top male words are profanity (amk, ulan),while none of the female k-top words are, suggest-ing male users swear more.Japanese.
Beyond the Japanese classifier?s gener-ally poor accuracy, it is striking that tokenizationdid not improve overall accuracy.
This indicatesthat once words were properly tokenized, no ad-ditional gender-distinguishing signal could be ex-tracted.
This may be an indication that word-basedfeatures carry little information in languages withcomplex orthography, such as Japanese (with manythousands of unigrams).Despite the classifier?s poor performance, the k-top discriminating words for male and female usersdiffer in interesting ways.
Some differences can beunderstood as resulting from known general trendsin how Japanese men and women?s use language.Japanese speakers have a choice of many first-person singular pronouns (equivalent to ?I?
), whichsignal different levels of politeness and of male ver-sus female speech.
The pronoun boku (?)
is asso-ciated with informal male speech; accordingly, it isamong the k-top male words.
Japanese also uses anextensive system of verb forms corresponding to dif-ferent levels of politeness, and honorifics (affixes fornames used when referring to others).
Women tendto use polite verb forms and honorifics more fre-quently than men in Japanese speech (Peng, 1981).In agreement with this pattern, several polite verbforms (-masu, -mashi) and a polite honorific (o-) areamong the k-top female words, as is a diminutivehonorific often used to refer to women (-chan).4 Language-specific Features andInferenceWhile the classifier performed well across a diverseset of languages, recall that all features used by theSVM were language-agnostic.
A natural questionconcerns the extent to which language-specific fea-tures relevant to the attribute of interest (e.g., gen-der) might improve the classifier?s performance.We examine this question within the context ofFrench.
Where gender inference is concerned,French is quite interesting because informationabout the gender of nouns (including the speaker) isoften obligatorily marked in the syntax: many wordshave different ?masculine?
and ?feminine?
forms forreferring to male and female nouns, including thespeaker.
Thus, it is in principle often possible to in-fer the gender of the speaker by which form they use,although it is not clear a priori that this method willwork for Twitter data.4.1 MethodFrench grammar dictates that which forms of wordsare used often reflects the gender of the speaker.1141Adjectives and past participles all have masculineand feminine forms, which are often spelled dif-ferently, and in addition often pronounced differ-ently.Adjectives must agree in gender with the nounthey refer to.
For example, ?I am happy?
wouldbe je suis heureuse for a female speaker and je suisheureux for a male speaker (literally ?I-am-happy?
);heureuse and heureux are the feminine and mascu-line singular forms of the adjective, and are pro-nounced differently.
Past participles of verbs alsoagree with the gender of the subject or object of theverb, for certain verbs and constructions.
For ex-ample, ?I went?
would be je suis alle?e for a femalespeaker and je suis alle?
for a male speaker (here suisis used to form the simple past of the verb aller,?to go?
); alle?
and alle?e are the masculine and fem-inine forms of the past participle of aller, and arepronounced the same.Note that the phrase je suis (?I am?)
occurs in boththe adjectival and verbal constructions referring tothe speaker; however, the function of suis differs be-tween the two.
suis is the first-person singular formof the verb e?tre (?to be?
), and functions as a cop-ula when followed by an adjective (?I am happy?
)but as an auxiliary verb to mark the past tense, whenfollowed by the past participle of certain verbs (?Iwent?).
For our purposes, what is important is that,in both cases, a following adjective or past participlewill take on the gender of the speaker.When this construction occurs in a tweet, it islikely that je is referring to the author of the tweet,and the rules of French grammar dictate that thegender of the associated adjective or past partici-ple should reflect the gender of the tweet?s author.We implemented a classifier that used this logic toclassify the gender of francophone Twitter users.
Itis worth emphasizing that the existence of adjec-tives and participles which reflect the speaker?s gen-der does not automatically make gender identifica-tion in French tweets a trivial task.
First, given theprevalence of non-standard spelling and grammaron Twitter and other online platforms, French usersmay sometimes not use the ?correct?
gender markedform reflecting their actual gender?especially giventhat the male and female forms for a given adjectiveor participle are often pronounced the same.
Sec-ond, even if gender-marked constructions are usedcorrectly, they may not occur sufficiently often inTable 3: The set of patterns that were considered to besuis-constructions when encountered in a tweet.jn suis pas, jm suis, jmsuis, jnmsuis pas, jnsuis pas,je ne suis pas, je suis pas, jsuis, jensuis pas, jemsuis,jnesuis pas, jmesuis, je me suis, je ne me suis pastweets to be a reliably used for speaker gender iden-tification.
Both of these concerns are borne out inour French dataset, as described further below; thequestion addressed in the experiment is how usefulthe signal provided by gender-marked forms is, de-spite these two sources of noise.Unlike the probabilistic SVM classifier, the suis-construction classifier can be made entirely deter-ministic.
For a given user, the set of tweets con-taining a suis-construction are identified, Tsuis(u).Of these, we can identify the number of those tweetsthat involve an adjective or past participle with a fe-male ending TFsuis(u) ?
Tsuis(u).
Labeling a userinvolves selecting a threshold based on TFsuis(u) andTsuis(u) below which a user receives one label andabove which the user receives the other label.Detecting suis-constructions.
As expected, cur-sory inspection of tweets revealed that Twitterusers often employed shorthand forms of the suis-construction.
We accounted for this by conduct-ing a manual survey of the shorthand forms ofthe suis-construction.
A catalog of regular expres-sions was drawn up that matched the different suis-construction forms we identified, shown in Table 3.Recognizing the gender of the adjective or pastparticiple involved in a suis-construction requireda second processing stage.
The Lexique lexicaldatabase was used to tag the word trailing the suis-construction (New and Landing, 2012).
If the tagwas not an adjective or verb, the construction wasdiscarded as it would not contain a gender indica-tion.
If the word was recognized as an adjective orverb, Lexique would also return the gender, whichwould be returned as the gender indication for thatparticular suis-construction.Threshold selection.
We evaluated a number ofpolicies for assigning the user?s gender based onthe relative values of TFsuis(u) and Tsuis(u).
In theend, however, the best performing threshold wasTFsuis(u) > 1: simply labeling as female any user1142Table 4: The component-wise and overall accuracy of thecombined suis-construction and SVM classifier.Component # Male Female Overallusers Acc Acc Accsuis-const.
723 0.91 0.90 0.90SVM 220 0.70 0.54 0.62Overall 943 0.86 0.82 0.83who employed the female construction even once.This threshold makes sense given the plausible intu-ition that females will (almost always) be the onlyusers to employ a female suis-construction; how-ever, it is quite sensitive to uses of female suis-constructions by males.Mixed classifier.
Since not all users had tweetswhich contained suis-constructions, we combinedthe SVM-based classifier used previously with thesuis-construction-based classifier.
The SVM com-ponent was applied to any users who lacked suis-constructions entirely in their tweet history.
Anyuser who used even one suis-construction would belabeled according to the TFsuis(u) > 1 threshold.4.2 ResultsWe ran our classifier on the French dataset, obtain-ing the results shown in Table 4.Coverage of the suis-construction.
In spite ofour concerns over the occurrence frequency and de-tectability of the suis-construction in tweets, ourresults show that suis-constructions were found intweets belonging to nearly 75% of all users in thedataset.
This suggests that the suis-constructionclassifier has quite broad coverage of the popula-tion.
Of course, given the essential role of the verb?e?tre?
in French (like the role of ?to be?
in En-glish), its frequent use is expected.
Nonetheless,the flexible use of grammar and spelling in Twitterand other online contexts raised a genuine concernthat occurrences of the suis-construction might notbe detected.
In fact, when we looked through thetweets of users who were flagged as not having use-ful suis-constructions in their tweets, we discoveredthat many actually did.
The issue was that they em-ployed highly irregular spellings that our implemen-tation was not able to pick up.
Thus, with additionalrefinement, it may be possible to improve the suis-construction coverage further, well beyond 80%.Performance of the suis-construction classifier.On the set of users for which the suis-constructionwas detected, the classifier did very well, achiev-ing an average accuracy of 90%.
Recall that thethreshold used to generate the results in Table 4 wasTFsuis(u) > 1.
We tested other (larger) thresholdsand found that the performance of the method dra-matically and monotonically decreased.
This waslargely due to female users being misclassified asmales, indicating that females do not exclusivelyuse female suis-constructions (this was confirmedvia manual inspection of a number of female tweethistories).
This is different from males, most ofwhom are quite strict about using only male suis-constructions.
Since forming the female form ofan adjective or participle typically requires addingan additional character (or more) to the base of theword, this may reflect a tendency towards droppinggender modifiers in favor of typing less.Performance of the SVM classifier.
While thesuis-construction classifier performed well, theSVM component did not do nearly as well on theTwitter users that could not be labeled using the suis-construction, achieving an average performance of62%.
At this level of accuracy, the classifier is per-forming barely better than a random classifier, whichwould have achieved around 50% accuracy on thelabel-balanced testing data.
This result stands inopposition to our earlier finding that French userscould be labeled with 75% accuracy.
This disparitysuggests that the non-suis-construction users com-prise a particularly difficult-to-classify group.The suis-construction as a filter.
The finding thatthe SVM classifier performed poorly in the com-bined classification setting suggests that the suis-construction classifier is acting as a very effective fil-ter for users that are hard for it to classify.
While wemight have preferred better classification accuracyall around, this result is still interesting and useful.Such filters can decrease classification error by sim-ply flagging those users who cannot be easily clas-sified, leaving them to be handled more carefully bymore powerful classifiers or human coding.
This isprecisely the function that the suis-construction clas-sifier appears to play (in addition to classifying the1143other users).This result suggests a question for future work:whether it is possible to build classifiers that accu-rately label the sets of users that are discarded bythe suis-construction classifier.Performance of the combined classifier.
Despitethe relatively poor performance of the SVM com-ponent, the accuracy of the combined classifier im-proved on the original SVM-only classifier by 8%,which is a substantial increase in accuracy.
Withsome additional focus on classifying the difficultusers who could not be labeled by suis-constructionusage, we feel that this accuracy can be increasedupwards of 90%.5 DiscussionIn this project, we have extended, for the first time,the latent attribute inference problem to users whotweet primarily in languages other than English.
Ourstudy offers several notable insights.Existing approaches generalize.
While accuracylevels certainly vary across languages, overall an ex-isting SVM-based classifier, when trained on usersfrom a given language, can classify the gender ofother users from that same language with accuracycomparable to performance reported for English.We suspect that this result will generalize to the in-ference of other demographic characteristics (e.g.,age and political orientation), though this must beexplored in future work.Complex orthography creates unique issues.Japanese stands out as being utterly unclassifiableusing existing SVM-based approaches and featuresets.
Even efforts to bridge some of the orthographicdisconnects between the Japanese language and theassumptions made by the SVM failed to improveperformance.
This stands out as a clear direction forfuture work, particularly since apparent issues withthe large number of unigrams used by Japanese willcreate issues for handling (Mandarin) Chinese, theworld?s most-spoken language.Language-specific features boost performance.While unsurprising that customizing a classifier tothe peculiarities of a given language boosts perfor-mance, our use of the suis-construction in Frenchhighlights how particular linguistic features may beuniquely well suited to the inference of particularattributes.
The results obtained for French stand incontrast to various, relatively unsuccessful attemptsto boost gender inference by incorporating syntac-tic features of English into the classifier (e.g., us-ing stems and co-stems).
It seems that some lan-guages have features better suited for certain classi-fication tasks.
Identifying and leveraging such fea-tures will be an interesting and fruitful direction forfuture work.Classifiers as a linguist?s tool.
In each language,a number of the k-top words align with or sug-gest gender-specific conventions in that particularlanguage.
That a language-agnostic classifier pro-vided such insights highlights its potential for ex-ploring language-specific word usage patterns andnuances.
For example, sociolinguistics (a subfield oflinguistics) has long studied the different ways menand women use language, especially in spontaneousspeech (Eckert and McConnell-Ginet, 2003); recentwork has begun to examine how language is useddifferently by men and women online as well (Bam-man et al 2012).
Such studies could be radicallyscaled up in terms of the number of languages con-sidered using a language-agnostic gender classifier.6 ConclusionThough there has been relatively little investigationinto latent attribute inference outside of English-language content, we consider it both a fruitfuland important area for future research.
Here, wehave evaluated the capacity for existing inferencemethods to be used outside their intended English-language context.
Furthermore, we have shown howlanguage-specific features might be incorporated inorder to boost classifier accuracy further.
The posi-tive results suggest that latent attribute inference inthe non-English context as a research direction wor-thy of further attention.7 AcknowledgementsThe authors gratefully acknowledge three anony-mous reviewers whose feedback improved the clar-ity and correctness of the manuscript.
The studywas supported by grants from the Social Sciences1144and Humanities and Natural Sciences and Engi-neering Research Councils of Canada (SSHRC In-sight Grant #435-2012-1802 and NSERC DiscoveryGrant #125517855) and the Public Safety CanadaKanishka Program.ReferencesS.
Akioka, N. Kato, Y. Muraoka, and H. Yamana.
2010.Cross-media impact on Twitter in Japan.
In Proceed-ings of the International Workshop on Search andMin-ing User-generated Contents.Atilika.
2012.
Kuromoji morphological analyzer.http://www.atilika.org.D.
Bamman, J. Eisenstein, and T. Schnoebelen.
2012.Gender in Twitter: Styles, stances, and social net-works.
arXiv preprint arXiv:1210.4567.S Bird.
2006.
Nltk: the natural language toolkit.
In Pro-ceedings of the COLING/ACL Interactive PresentationSessions.J.D.
Burger, J. Henderson, and G. Zarrella.
2011.
Dis-criminating gender on Twitter.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.M.
Conover, B. Gonc?alves, J. Ratkiewicz, A. Flammini,and F. Menczer.
2011a.
Predicting the politial align-ment of Twitter users.
In Proceedings of the Interna-tional Conference on Social Computing.M.D.
Conover, J. Ratkiewicz, M. Francisco,B.
Gonc?alves, F Menczer, and A Flammini.
2011b.Political polarization on Twitter.
In Proceedings ofthe International Conference on Weblogs and SocialMedia.P.
Eckert and S. McConnell-Ginet.
2003.
Language andgender.
Cambridge University Press, Cambridge.F.
Giglietto.
2012.
If likes were votes: An empiricalstudy of the 2011 Italian administrative elections.
InProceedings of the International Conference on We-blogs and Social Media.J.
Holmes.
1995.
Women, men and politeness.
Long-man, London.M.
Kim and H.W.
Park.
2012. e-measuring Twitter-based political participation and deliberation in theSouth Korean context by using social network andTriple Helix indicators.
Scientometrics, 90(1):121?140.W.
Liu and D. Ruths.
2013.
What?s in a name?
Usingfirst names as features for gender inference in Twit-ter.
In Analyzing Microtext: 2013 AAAI Spring Sym-posium.W.
Liu, F.A.
Zamal, and D. Ruths.
2012.
Using so-cial media to infer gender composition from commuterpopulations.
In Proceedings of the When the CityMeets the Citizen Worksop.A.
Mislove, S. Lehmann, Y.Y.
Ahn, J.P. Onnela, and J.N.Rosenquist.
2011.
Understanding the demographicsof Twitter users.
In Proceedings of the InternationalConference on Weblogs and Social Media.D.
Mocanu, A. Baronchelli, B. Gonc?alves, N. Perra, andA.
Vespignani.
2012.
The Twitter of Babel: Map-ping world languages through microblogging plat-forms.
ArXiv e-prints, December.B.
New and C. Landing.
2012.
Lexique 3.http://www.lexique.org/telLexique.php.F.C.C.
Peng, editor.
1981.
Male/female differences inJapanese.
The East-West Sign Language Association,Tokyo.M.
Pennacchiotti and A.M. Popescu.
2011.
A machinelearning approach to Twitter user classification.
InProceedings of the International Conference on We-blogs and Social Media.D.
Rao and D. Yarowsky.
2010.
Detecting latent userproperties in social media.
In Proceedings of the NIPSworkshop on Machine Learning for Social Networks.D.
Rao, D. Yarowsky, A. Shreevats, and M. Gupta.
2010.Classifying latent user attributes in Twitter.
In Pro-ceedings of the International Workshop on Search andMining User-generated Contents.T.
Sakaki, M. Okazaki, and Y. Matsuo.
2010.
Earth-quake shakes Twitter users: Real-time event detectionby social sensors.
In Proceedings of the InternationalWorld Wide Web Conference.Semiocast.
2012.
Brazil becomes the 2nd country onTwitter, Japan 3rd, Netherlands most active country.http://semiocast.com/publications/2012 01 31 Brazil becomes 2nd country onTwitter superseds Japan.A.
Tumasjan, T.O.
Sprenger, P.G.
Sandner, and I.M.Welpe.
2010.
Predicting elections with Twitter: What140 characters reveal about political sentiment.
InProceedings of the International Conference on We-blogs and Social Media.W.
Weerkamp, S. Carter, and M. Tsagkias.
2011.
Howpeople use Twitter in different languages.
In Proceed-ings of the Web Science Conference.R.
Schenk-Van Witsen.
1981.
Les diffe?rences sex-uelles dans le franc?ais parle?
: une e?tude-pilote desdiffe?rences lexicales entre hommes et femmes.
Lan-gage et socie?te?, 17(1):59?78.F.A.
Zamal, W. Liu, and D. Ruths.
2012.
Homophily andlatent attribute inference: Inferring latent attributes ofTwitter users from neighbors.
In Proceedings of theInternational Conference on Weblogs and Social Me-dia.1145
