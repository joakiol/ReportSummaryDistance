2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 646?655,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsStructured Event Retrieval over Microblog ArchivesDonald Metzler, Congxing Cai, Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Marina del Rey, CA 90292AbstractMicroblog streams often contain a consider-able amount of information about local, re-gional, national, and global events.
Most ex-isting microblog search capabilities are fo-cused on recent happenings and do not providethe ability to search and explore past events.This paper proposes the problem of structuredretrieval of historical event information overmicroblog archives.
Rather than retrieving in-dividual microblog messages in response to anevent query, we propose retrieving a rankedlist of historical event summaries by distill-ing high quality event representations usinga novel temporal query expansion technique.The results of an exploratory study carriedout over a large archive of Twitter messagesdemonstrates both the value of the microblogevent retrieval task and the effectiveness of ourproposed search methodologies.1 IntroductionReal-time user generated content is one of the keydriving forces behind the growing popularity of so-cial media-centric communication.
The ability to in-stantly share, often from your mobile phone, yourthoughts (via Twitter), your photos (via Facebook),your location (via Foursquare), and a variety of otherinformation is changing the way that information iscreated, communicated, and consumed.There has been a substantial amount of researcheffort devoted to user generated content-relatedsearch tasks, including blog search, forum search,and community-based question answering.
How-ever, there has been relatively little research on mi-croblog search.
Microblog services, such as Tumblrand Twitter, provide users with the ability to broad-cast short messages in real-time.
This is in contrastto traditional blogs that typically have considerablymore content that is updated less frequently.
Bytheir very nature, microblog streams often containa considerable amount of information about local,regional, national, and global news and events.
Arecent study found that over 85% of trending topicson Twitter are news-related (Kwak et al, 2010).
An-other recent study by Teevan et al that investigatedthe differences between microblog and Web searchreported similar findings (Teevan et al, 2011).
Thestudy also found that microblog search queries areused to find information related to news and events,while Web search queries are more navigational innature and used to find a variety of information on aspecific topic.It is likely that microblogs have not received muchattention because, unlike blog search, there is nowell-defined microblog search task.
Existing mi-croblog search services, such as those offered byTwitter and Google, only provide the ability to re-trieve individual microblog posts in response to aquery.
Unfortunately, this task has limited utilitysince very few real information needs can be satis-fied by a single short piece of text (e.g., the max-imum length of a message on Twitter is 140 char-acters).
Hence, novel search tasks defined over mi-croblog streams that go beyond ?message retrieval?have the potential to add substantial value to users.Given the somewhat limited utility of microblogmessage search and the preponderance of news andevent-related material posted on microblogs, this pa-646July 16 2010 at 17 UTC, for 11 hoursSummary tweets:i. Ok a 3.6 ?rocks?
nothing.
But boarding a planethere now, Woodward ho!
RT @todayshow: 3.6 mag-nitude #earthquake rocks Washington DC area.ii.
RT @fredthompson: 3.6-magnitude earthquake hitDC.
President Obama said it was due to 8 years ofBush failing to regulate plate tectonic ...iii.
3.6-magnitude earthquake wakes Md.
residents:Temblor centered in Gaithersburg felt by as many as3 million people... http://bit.ly/9iMLEkFigure 1: Example structured event representation re-trieved for the query ?earthquake?.per proposes a novel search task that we call mi-croblog event retrieval.
Given a query that describesan event, such as earthquake, terrorist bombing, orbieber concert, the goal of the task is to retrieve aranked list of structured event representations, suchas the one shown in Figure 1, from a large archive ofhistorical microblog posts.In this work, structured representations come inthe form of a list of timespans during which an in-stance of the event occurred and was actively dis-cussed within the microblog stream.
Additionally,for each timespan, a small set of relevant messagesare retrieved for the purpose of providing a high-level summary of the event that occurred during thetimespan.
This task leverages the large amount ofreal-time, often first-hand information found in mi-croblog archives to deliver a novel form of user gen-erated content-based search results to users.
Unlikenews search, which finds professionally written ar-ticles on a news-related topic, and general-purposeWeb search, which is likely to find a large amountof unrelated information, this task is designed to re-trieve highly relevant news and event-related infor-mation viewed through the lens of users who ex-perienced or discussed the event while it happened(or during its aftermath).
Such search functional-ity would not only be useful for everyday end-users,but also social scientists, historians, journalists, andemergency planners.This paper has three primary contributions.
First,we introduce the microblog event retrieval task,which retrieves a ranked list of structured event rep-resentations in response to an event query.
By goingbeyond individual microblog message retrieval, thetask adds value to microblog archives and providesusers with the ability to find information that wasdisseminated in real-time about past events, whichis not possible with news and Web search engines.Second, we propose an unsupervised methodologyfor distilling high quality event representations usinga novel temporal query expansion technique.
Thetechnique synthesizes ideas from pseudo-relevancefeedback, term burstiness, and temporal aspectsof microblog streams.
Third, we perform an ex-ploratory evaluation of 50 event queries over a cor-pus of 46 million Twitter messages.
The results ofour evaluation demonstrate both the value of the mi-croblog event retrieval task itself and the effective-ness of our proposed search methodologies, whichshow improvements of up to 42% compared to abaseline approach.2 Related WorkThere are several directions of microblog researchthat are related to our proposed work.
First, there isa growing body of literature that has focused on thetopical content of microblog posts.
This researchhas focused on microblog topic models (Hong andDavison, 2010), event and topic detection and track-ing (Sankaranarayanan et al, 2009; Cataldi et al,2010; Petrovic?
et al, 2010; Lin et al, 2010), predict-ing flu outbreaks using keyword tracking (Culotta,2010), and using microblog streams as a sourceof features for improving recency ranking in Websearch (Dong et al, 2010).
Most of these approachesanalyze content as it arrives in the system.
Whiletracking a small number of topics or keywords is fea-sible using online algorithms, the general problemof topic detection and tracking (Allan et al, 1998) isconsiderably more challenging given the large num-ber of topics being discussed at any one point.
Ourwork differs in that it does not attempt to track ormodel topics as they arrive in the system.
Instead,given an event query, our system retrospectively an-alyzes the corpus of microblog messages for the pur-pose of retrieving structured event representations.There is no shortage of previous work on usingpseudo-relevance feedback approaches for query ex-pansion.
Relevant research includes classical vector-space approaches (Rocchio, 1971), language mod-647eling approaches (Lavrenko and Croft, 2001; Zhaiand Lafferty, 2001; Li and Croft, 2003), among oth-ers (Metzler and Croft, 2007; Cao et al, 2008; Lvand Zhai, 2010).
The novel aspect of our proposedtemporal query expansion approach is the fact thatexpansion is done over a temporal stream of veryshort, noisy messages.There has also been recent work on summarizingsets of microblog posts (Sharifi et al, 2010).
Wechose to make use of a simple approach in favor ofa more sophisticated one because summarization isonly a minor aspect of our proposed framework.Finally, there are two previous studies that arethe most relevant to our work.
First, Massoudi etal.
propose a retrieval model that uses query ex-pansion and microblog quality indicators to retrieveindividual microblog messages (Massoudi et al,2011).
Their proposed query expansion approachdiffers from ours in the sense that we utilize times-pans from the (possibly distant) past when generat-ing expanded queries and focus on event retrieval,rather than individual message retrieval.
The otherresearch that is closely related to ours is the workdone by Chieu and Lee (Chieu and Lee, 2004).
Theauthors propose an approach for automatically con-structing timelines from news articles in responseto a query.
The novelty of our proposed work de-rives from our novel temporal query expansion ap-proach, and the fact that our work focuses on mi-croblog streams which are fundamentally differentin nature from news articles.3 Microblog Event RetrievalThe primary goal of this paper is to introduce a newmicroblog search paradigm that goes beyond retriev-ing messages individually.
We propose a novel taskcalled microblog event retrieval, which is defined asfollows.
Given a query that specifies an event, re-trieve a set of relevant structured event representa-tions from a large archive of microblog messages.This definition is purposefully general to allow for abroad interpretation of the task.There is nothing in our proposed retrieval frame-work that precludes it from producing reasonable re-sults for any type of query, not just those related toevents.
However, we chose to primarily focus onevents in this paper because previous studies haveshown that a majority of trending topics within mi-croblog streams are about news and events (Kwak etal., 2010).
The information found in microblogs isdifficult to find anywhere else, including news andWeb archives, thereby making it a valuable resourcefor a wide variety of users.3.1 Overview of FrameworkOur microblog event retrieval framework takes aquery as input and returns a ranked list of struc-tured event representations.
To accomplish this, theframework breaks the work into two steps ?
times-pan retrieval and summarization.
The timespan re-trieval step identifies the timespans when the eventhappened, while the summarization step retrievesa small set of microblog messages for each times-pan that are meant to act as a summary.
Figure 1shows an example result that is returned in responseto the query ?earthquake?.
The result consists of astart time that indicates when the event began be-ing discussed, a duration that specifies how long theevent was discussed, and a small number of mes-sages posted during the time interval that are meantto summarize what happened.
This example corre-sponds to an earthquake that struck the metropoli-tan District of Colombia area in the United States.The earthquake was heavily discussed for nearly 11hours, because it hit a densely populated area thatdoes not typically experience earthquakes.3.2 Temporal Query ExpansionWe assume that queries issued to our retrieval frame-work are simple keyword queries that consist of asmall number of terms.
This sparse representationof the user?s information need makes finding rel-evant messages challenging, since microblog mes-sages that are highly related to the query might notcontain any of the query keywords.
It is common formicroblog messages about a given topic to expressthe topic in a different, possibly shortened or slang,manner.
For example, rather than writing ?earth-quake?, users may instead use the word ?quake?
orsimply include a hashtag such as ?#eq?
in their mes-sage.
It is impractical to manually identify the fullset of related keywords and folksonomy tags (i.e.,hashtags) for each query.
In information retrieval,this is known as the vocabulary mismatch problem.To address this problem, we propose a novel unsu-648pervised temporal query expansion technique.
Theapproach is unsupervised in the sense that it makesuse of a pseudo-relevance feedback-like mechanismwhen extracting expansion terms.
Traditional queryexpansion approaches typically find terms that com-monly co-occur with the query terms in documents(or passages).
However, such approaches are notsuitable for expanding queries in the microblog set-ting since microblog messages are very short, yield-ing unreliable co-occurrence information.
Further-more, microblog messages have an important tem-poral dimension that should be considered whenthey are being used to generate expansion terms.Our proposed approach generates expansionterms based on the temporal co-occurrence of terms.Given keyword query q, we first automatically re-trieve a set of N timespans for which the query key-words were most heavily discussed.
To do so, werank timespans according to the proportion of mes-sages posted during the timespan that contain oneor more of the query keywords.
This is a simple,but highly reliable way of identifying timespans dur-ing which a specific topic is being heavily discussed.These timespans are then considered to be pseudo-relevant.
In our experiments, the microblog streamis divided into hours, with each hour correspondingto an atomic timespan.
Although it is possible todefine timespans in many different ways, we foundthat this was a suitable level of granularity for mostevents that was neither overly broad nor overly spe-cific.For each pseudo-relevant timespan, a burstinessscore is computed for all of the terms that occur inmessages posted during the timespan.
The bursti-ness score is meant to quantify how trending a termis during the timespan.
Thus, if the query is be-ing heavily discussed during the timespan and someterm is also trending during the timespan, then theterm may be related to the query.
For each of the topN time intervals, the burstiness score of each termis computed as follows:burstiness(w, TSi) =P (w|TSi)P (w)(1)which is the ratio of the term?s likelihood of occur-ring within timespan TSi versus the likelihood ofthe term occurring during any timespan.
Hence, ifa term that generally infrequently occurs within themessage stream suddenly occurs many times withina single time interval, then the term will be assigneda high burstiness score.
This weighting is similar innature to that proposed by Ponte for query expansionwithin the language modeling framework for infor-mation retrieval (Ponte, 1998).
The following prob-ability estimates are used for the expressions withinthe burstiness score:P (w|TSi) =tfw,TSi + ?tfwN|TSi|+ ?, P (w) =tfw +KN +K|V |where tfw,TSi is the number of occurrences of w intimespan TSi, tfw is the number of occurrences ofw in the entire microblog archive, |TSi| is the num-ber of terms in timespan TSi, N is the total numberof terms in the microblog archive, V is the vocabu-lary size, and ?
and K are smoothing parameters.While it is common practice to smooth P (w|TSi)using Dirichlet (or Bayesian) smoothing (Zhai andLafferty, 2004), it is less common to smooth the gen-eral English language model P (w).
However, wefound that this was necessary since term distribu-tions in microblog services exhibit unique character-istics.
By smoothing P (w), we dampen the effect ofoverweighting very rare terms.
In our experiments,we set the value of ?
to 500 and K to 10 after somepreliminary exploration.
We found that the overallsystem effectiveness is generally insensitive to thechoice of smoothing parameters.The final step of the query expansion processinvolves aggregating the burstiness scores acrossall pseudo-relevant timespans to generate an over-all score for each term.
To do so, we computethe geometric mean of the burstiness scores acrossthe pseudo-relevant timespans.
Preliminary experi-ments showed that the arithmetic mean was suscep-tible to overweighting terms that had a very largeburstiness score in a single timespan.
By utiliz-ing the geometric average instead, we ensure thatthe highest weighted terms are those that have largeweights in a large number of the timespans, therebyeliminating spurious terms.
Seo and Croft (2010)observed similar results with traditional pseudo-relevance feedback techniques.The k highest weighted terms are then used asexpansion terms.
Using this approach, terms thatcommonly trend during the same timespans that649the query terms commonly occur (i.e., the pseudo-relevant timespans) are assigned high weights.Hence, the approach is capable of capturing sim-ple temporal dependencies between terms and querykeywords, which is not possible with traditional ap-proaches.3.3 Timespan RankingThe end result of the query expansion process justdescribed is an expanded query q?
that consists of aset of k terms and their respective weights (denotedas ?w).
Our framework uses the expanded query q?to retrieve relevant timespans.
We hypothesize thatusing the expanded version of the query for timespanretrieval will yield significantly better results thanusing the keyword version.To retrieve timespans, we first identify the 1000highest scoring timespans (with respect to q?).
Wethen merge contiguous timespans into a single,longer timespan, where the score of the mergedtimespan is the maximum score of its componenttimespans.
The final ranked list consists of themerged timespans.
Therefore, although our times-pans are defined as hour intervals, it is possible forour system to return longer (merged) timespans.We now describe two scoring functions that canbe used to compute the relevance of a timespan withrespect to an expanded query representation.3.3.1 Coverage Scoring FunctionThe coverage scoring function measures rele-vance as the (weighted) number of expansion termsthat are covered within the timespan.
This measureassumes that the expanded query is a faithful repre-sentation of the information need and that the moretimes the highly weighted expansion terms occur,the more relevant the timespan is.
Using this defi-nition, the coverage score of a time interval is com-puted as:s(q?, TS) =?w?q?
?w ?
tfw,TSwhere tfwi,TS is the term frequency of wi in times-pan TS and ?w is the expansion weight of term w.3.3.2 Burstiness Scoring FunctionSince multiple events may occur at the same time,microblog streams can easily be dominated by thelarger of two events.
However, less popular eventsmay also exhibit burstiness at the same time.
There-fore, another measure of relevance is the burstinessof the event signature during the timespan.
If allof the expansion terms exhibit burstiness during thetime interval, it strongly suggests the timespan maybe relevant to the query.Therefore, to measure the relevance of the times-pan, we first compute the burstiness scores for all ofthe terms within the time interval.
This yields a vec-tor ?TS of burstiness scores.
The cosine similaritymeasure is used to compute the similarity betweenthe query burstiness scores and the timespan bursti-ness scores.
Hence, the burstiness scoring functionis computed as:s(q?, TS) = cos(?q?
, ?TS)3.4 Timespan SummarizationThe final step of the retrieval process is to producea short query-biased summary for each retrievedtime interval.
The primary purpose for generatingthis type of summary is to provide the user witha quick overview of what happened during the re-trieved timespans.We utilize a simple, straightforward approach thatgenerates unexpectedly useful summaries.
Given atimespan, we use a relatively simple information re-trieval model to retrieve a small set of microblogmessages posted during the timespan that are themost relevant to the expanded representation of theoriginal query.
These messages are then used as ashort summary of the timespan.This is accomplished by scoring a microblog mes-sageM with respect to an expanded query represen-tation q?
using a weighted variant of the query like-lihood scoring function (Ponte and Croft, 1998):s(q?,M) =?w?q?
?w ?
logP (w|M)where ?w is the burstiness score for expansion termw and P (w|M) is a Dirichlet smoothed languagemodeling estimate for term w in message M .
Thisscoring function is also equivalent to the cross en-tropy and KL-divergence scoring functions (Laffertyand Zhai, 2001).650Category EventsBusiness layoffs, bankruptcy, acquisition,merger, hostile takeoverCelebrity wedding, divorceCrime shooting, robbery, assassination,court decision, school shootingDeath death, suicide, drownedEnergy blackout, brownoutEntertainment awards, championship game,world recordHealth recall, pandemic, disease, flu,poisoningNatural Disaster hurricane, tornado, earthquake,flood, tsunami, wildfire, firePolitics election, riots, protestsTerrorism hostage, explosion, terrorism,bombing, terrorist attack, suicidebombing, hijackedTransportation plane crash, traffic jam, sinks,pileup, road rage, train crash, de-railed, capsizesTable 1: The 50 event types used as queries during ourevaluation, divided into categories.4 ExperimentsThis section describes our empirical evaluation ofthe proposed microblog event retrieval task.4.1 Microblog CorpusOur microblog message archive consists of datathat we collected from Twitter using their Stream-ing API.
The API delivers a continuous 1% ran-dom sample of public Twitter messages (also called?tweets?).
Our evaluation makes use of data col-lected between July 16, 2010 and Jan 1st, 2011.
Af-ter eliminating all non-English tweets, our corpusconsists of 46,611,766 English tweets, which corre-sponds to roughly 10,000 tweets per hour.
Althoughthis only represents a 1% sample of all tweets, webelieve that the corpus is sizable enough to demon-strate the utility of our proposed approach.4.2 Event QueriesTo evaluate our system, we prepared a list of 50event types that fall into 11 different categories.The event types and their corresponding categoriesare listed in Table 1.
The different event typescan have substantially different characteristics, suchas the frequency of occurrence, geographic or de-mographic interest, popularity, etc.
For example,there are more weddings than earthquakes.
Pub-lic events, such as federal elections involve peopleacross the country.
However, a car pileup typicallyonly attracts local attention.
Moreover, microblog-gers show different amounts of interest to each typeof event.
For example, Twitter users are more likelyto tweet about politics than a business acquisition.4.3 MethodologyTo evaluate the quality of a particular configurationof our framework, we run the microblog event re-trieval task for the 50 different event type queries de-scribed in the previous section.
For each query, thetop 10 timespans retrieved are manually judged tobe relevant or non-relevant.
If the summary returnedclearly indicated a real event instance occurred, thenthe timespan was marked as relevant.
The primarymetric of interest is precision at 10.In addition to the temporal query expansionapproach (denoted TQE), we also ran exper-iments using relevance-based language models,which is a state-of-the-art query expansion ap-proach (Lavrenko and Croft, 2001).
We ran twovariants of relevance-based language models.
Inthe first, query expansion was done using the Twit-ter corpus itself (denoted TwitterRM).
This allowsus to compare the effectiveness of the TQE ap-proach against a more traditional query expansionapproach.
In the other variant, query expansion wasdone using the English Gigaword corpus (denotedNewsRM), which is a rich source of event informa-tion created by traditional news media.For all three query expansion approaches (TQE,TwitterRM, and NewsRM), the two scoring func-tions, burstiness and coverage, are used to ranktimespans.
Hence, we evaluate six specific instancesof our framework.
As a baseline, we use a sim-ple (unexpanded) keyword retrieval approach thatscores timespans according to the relative frequencyof event keywords that occur during the timespan.4.4 Timespan Retrieval ResultsBefore delving into the details of our quantitativeevaluation of effectiveness, we provide an illustra-tive example of the type of results our system is ca-pable of producing.
Table 2 shows the top four re-651July 16 2010 at 17 UTC, for 11 hoursOk a 3.6 ?rocks?
nothing.
But boarding a plane therenow, Woodward ho!
RT @todayshow: 3.6 magnitude#earthquake rocks Washington DC area.September 28 2010 at 11 UTC, for 6 hoursRT @Quakeprediction: 2.6 earthquake(possible foreshock) hits E of Los Ange-les; http://earthquake.usgs.gov/earthquakes/recenteqscanv/Fau ...September 04 2010 at 01 UTC, for 3 hours7.0 quake strikes New Zealand - A 7.0-magnitudeearthquake has struck near New Zealand?s secondlargest city.
Reside... http://ht.ly/18R2rwOctober 27 2010 at 01 UTC, for 5 hoursRT @SURFER Magazine: Tsunami StrikesMentawais: Wave Spawned By A 7.5-MagnitudeEarthquake Off West Coast Of Indonesiahttp://bit.ly/8Z9LbvTable 2: Top four timespans (with a single summarytweet) retrieved for the query ?earthquake?.sults retrieved using temporal query expansion withthe burstiness scoring function for the query ?earth-quake?.
Only a single summary tweet is displayedfor each timespan due to space restrictions.
As wecan see from the tweets, all of the results are rele-vant to the query, in that they all correspond to timeswhen an earthquake happened and was actively dis-cussed on Twitter.
Different from Web and newssearch results, these types of ranked lists provide aclear temporal picture of relevant events that wereactively discussed on Twitter.The results of our microblog retrieval task areshown in Table 3.
The table reports the per-categoryand overall precision at 10 for the baseline, andthe six configurations of our proposed framework.Bolded values represent the best result per category.As the results show, using temporal query expan-sion with burstiness ranking yields a mean preci-sion at 10 of 61%, making it the best overall sys-tem configuration.
The approach is 41.9% betterthan the baseline, which is statistically significantaccording to a one-sided paired t-test at the p < 0.01level.
Interestingly, the relevance model-based ex-pansion techniques exhibit even worse performance,on average, than our simple keyword baseline.
Forexample, the news-based expansion approach was11.6% worse using the coverage scoring functionand 18.6% worse using the burstiness scoring func-tion compared to the baseline.
All of the traditionalquery expansion results are statistically significantlyworse than the temporal query expansion-based ap-proaches.
Hence, the results suggest that capturingtemporal dependencies between terms yields bet-ter expanded representations than simply capturingterm co-occurrences, as is done in traditional queryexpansion approaches.The results also indicate the burstiness scoringfunction outperforms the coverage scoring functionfor temporal query expansion.
An analysis of theresults revealed that in many cases the timespansreturned using the coverage scoring function had asmall number of frequent terms that matched the ex-panded query.
This happened less often with theburstiness scoring function, which is based on thecosine similarity between the query and timespan?sburstiness scores.
The combination of burstinessweighting and l2 normalization (when computingthe cosine similarity) appears to yield a more robustscoring function.4.5 Event Popularity EffectsIt is also interesting to note that the retrieval perfor-mance varies substantially across the different eventtype categories.
For example, the performance onqueries about ?natural disasters?
and ?politics?
isconsistently strong.
Similar performance can alsobe achieved for popular events related to celebri-ties.
However, energy-related event queries, such as?blackout?, achieves very poor effectiveness.
Thisobservation seems to suggest that the more popu-lar an event is, the better the retrieval performancethat can be achieved.
This is a reasonable hypothe-sis since the more people tweet about the event, theeasier it is to identify the trend from the background.To better understand this phenomenon, we com-pute the correlation between timespan retrieval pre-cision and event (query) popularity, where popular-ity is measured according to:Popularity(q) =1NN?i=1burstiness(q, TSi),where q is the event query, burstiness(q, TSi) isthe burstiness score of the event during timespan652Event Category BaselineNewsRM TwitterRM TQEburst cover burst cover burst coverBusiness 0.50 0.46 0.30 0.70 0.18 0.74 0.64Celebrity 0.75 0.30 0.40 0.50 0.60 0.80 0.45Crime 0.44 0.28 0.54 0.22 0.32 0.46 0.28Death 0.43 0.20 0.33 0.30 0.30 0.47 0.47Energy 0.05 0.10 0.05 0.20 0.05 0.15 0.00Entertainment 0.47 0.53 0.67 0.30 0.53 0.70 0.70Health 0.48 0.28 0.36 0.44 0.16 0.60 0.60Nat.
Disaster 0.50 0.53 0.59 0.66 0.46 0.87 0.66Politics 0.67 0.70 0.53 0.63 0.30 0.87 0.60Terrorism 0.41 0.44 0.39 0.39 0.17 0.69 0.51Transportation 0.21 0.08 0.08 0.08 0.10 0.31 0.19All 0.43 0.35 0.38 0.40 0.26 0.61 0.47Table 3: Per-category and overall (All) precision at 10 for the keyword only approach (Baseline), traditional newswireexpansion (NewsRM), traditional pseudo relevance feedback using the Twitter corpus (TwitterRM), and tempo-ral query expansion (TQE).
For the expansion-based approaches, results for the burstiness scoring (burst) and thecoverage-based scoring (cover) are given.
Bold values indicate the best result per category.CorrelationBaseline 0.63 (p < 0.01)NewsRM 0.53 (p < 0.01)TwitterRM 0.61 (p < 0.01)TQE 0.50 (p < 0.01)Table 4: Spearman rank correlation between event re-trieval precisions and event popularity.
All methods usethe burstiness scoring function.TSi, as defined in Equation 1, and the sum goes overthe topN timespans retrieved for the event using ourproposed retrieval approach.Using this measure, we find that Twitter users aremore interested in events related to entertainmentand politics, and less interested in events related toenergy or transportation.
Also, we notice that Twit-ter users actively discuss dramatic crisis-related top-ics, including natural disasters (e.g., earthquakes,hurricanes, tornado, etc.)
and terrorist attacks.Table 4 shows the correlations between effec-tiveness and event popularity across different ap-proaches.
The correlations indicate a strong cor-relation with event popularity for the keyword ap-proach.
This is expected, since the approach is basedon the number of times the keywords are mentionedwithin the timespan.
The correlations are signif-icantly reduced by incorporating query expansionterms.
The configurations that use temporal queryexpansion tend to have lower correlation than theother approaches.
Although the correlation is stillsignificant, the lower correlation suggests that tem-poral query expansion approaches are more robust topopularity effects than simple keywords approaches.Additional work is necessary to better understandthe role of popularity in retrieval tasks like this.5 ConclusionsIn this paper, we proposed a novel microblog searchtask called microblog event retrieval.
Unlike previ-ous microblog search tasks that retrieve individualmicroblog messages, our task involves the retrievalof structured event representations during which anevent occurs and is discussed within the microblogcommunity.
In this way, users are presented with aranked list or timeline of event instances in responseto a query.To tackle the microblog search task, we proposeda novel timespan retrieval framework that first con-structs an expanded representation of the incomingquery, performs timespan retrieval, and then pro-duces a short summary of the timespan.
Our experi-mental evaluation, carried out over a corpus of over46 million microblog messages collected from Twit-ter, showed that microblog event retrieval is a feasi-ble, challenging task, and that our proposed times-pan retrieval framework is both robust and effective.653ReferencesJames Allan, Jaime Carbonell, George Doddington,Jonathan Yamron, and Yiming Yang.
1998.
Topic De-tection and Tracking Pilot Study.
In In Proceedings ofthe DARPA Broadcast News Transcription and Under-standing Workshop, pages 194?218.Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and StephenRobertson.
2008.
Selecting good expansion terms forpseudo-relevance feedback.
In Proc.
31st Ann.
Intl.ACM SIGIR Conf.
on Research and Development in In-formation Retrieval, SIGIR ?08, pages 243?250, NewYork, NY, USA.
ACM.Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.2010.
Emerging topic detection on twitter based ontemporal and social terms evaluation.
In Proceedingsof the Tenth International Workshop on MultimediaData Mining, MDMKDD ?10, pages 4:1?4:10, NewYork, NY, USA.
ACM.Hai Leong Chieu and Yoong Keok Lee.
2004.
Querybased event extraction along a timeline.
In Proceed-ings of the 27th annual international ACM SIGIR con-ference on Research and development in informationretrieval, SIGIR ?04, pages 425?432, New York, NY,USA.
ACM.Aron Culotta.
2010.
Towards detecting influenza epi-demics by analyzing twitter messages.
In 1st Work-shop on Social Media Analytics (SOMA?10), July.Anlei Dong, Ruiqiang Zhang, Pranam Kolari, JingBai, Fernando Diaz, Yi Chang, Zhaohui Zheng, andHongyuan Zha.
2010.
Time is of the essence: im-proving recency ranking using twitter data.
In Pro-ceedings of the 19th international conference on Worldwide web, WWW ?10, pages 331?340, New York, NY,USA.
ACM.Liangjie Hong and Brian D. Davison.
2010.
Empiricalstudy of topic modeling in twitter.
In 1st Workshop onSocial Media Analytics (SOMA?10), July.Haewoon Kwak, Changhyun Lee, Hosung Park, and SueMoon.
2010.
What is twitter, a social network ora news media?
In Proceedings of the 19th inter-national conference on World wide web, WWW ?10,pages 591?600, New York, NY, USA.
ACM.J.
Lafferty and C. Zhai.
2001.
Document language mod-els, query models, and risk minimization for informa-tion retrieval.
In Proc.
24th Ann.
Intl.
ACM SIGIRConf.
on Research and Development in InformationRetrieval, pages 111?119.V.
Lavrenko and W. B. Croft.
2001.
Relevance-basedlanguage models.
In Proc.
24th Ann.
Intl.
ACM SI-GIR Conf.
on Research and Development in Informa-tion Retrieval, pages 120?127.Xiaoyan Li and W. Bruce Croft.
2003.
Time-based lan-guage models.
In Proc.
12th Intl.
Conf.
on Informationand Knowledge Management, CIKM ?03, pages 469?475, New York, NY, USA.
ACM.Cindy Xide Lin, Bo Zhao, Qiaozhu Mei, and Jiawei Han.2010.
Pet: a statistical model for popular events track-ing in social communities.
In Proc.
16th Ann.
Intl.ACM SIGKDD Conf.
on Knowledge Discovery andData Mining, KDD ?10, pages 929?938, New York,NY, USA.
ACM.Yuanhua Lv and ChengXiang Zhai.
2010.
Positional rel-evance model for pseudo-relevance feedback.
In Proc.33rd Ann.
Intl.
ACM SIGIR Conf.
on Research and De-velopment in Information Retrieval, SIGIR ?10, pages579?586, New York, NY, USA.
ACM.Kamran Massoudi, Manos Tsagkias, Maarten de Rijke,and Wouter Weerkamp.
2011.
Incorporating query ex-pansion and quality indicators in searching microblogposts.
In Proc.
33rd European Conf.
on InformationRetrieval, page To appear.Donald Metzler and W. Bruce Croft.
2007.
Latent con-cept expansion using markov random fields.
In Proc.30th Ann.
Intl.
ACM SIGIR Conf.
on Research and De-velopment in Information Retrieval, SIGIR ?07, pages311?318, New York, NY, USA.
ACM.Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applica-tion to twitter.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,HLT ?10, pages 181?189, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.J.
Ponte and W. Bruce Croft.
1998.
A language modelingapproach to information retrieval.
In Proc.
21st Ann.Intl.
ACM SIGIR Conf.
on Research and Developmentin Information Retrieval, pages 275?281.Jay Ponte.
1998.
A Language Modeling Approach to In-formation Retrieval.
Ph.D. thesis, University of Mas-sachusetts, Amherst, MA.J.
J. Rocchio, 1971.
Relevance Feedback in InformationRetrieval, pages 313?323.
Prentice-Hall.Jagan Sankaranarayanan, Hanan Samet, Benjamin E.Teitler, Michael D. Lieberman, and Jon Sperling.2009.
Twitterstand: news in tweets.
In Proceedings ofthe 17th ACM SIGSPATIAL International Conferenceon Advances in Geographic Information Systems, GIS?09, pages 42?51, New York, NY, USA.
ACM.Jangwon Seo and W. Bruce Croft.
2010.
Geometric rep-resentations for multiple documents.
In Proceeding ofthe 33rd international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?10, pages 251?258, New York, NY, USA.
ACM.Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.Kalita.
2010.
Experiments in microblog summariza-tion.
Social Computing / IEEE International Confer-654ence on Privacy, Security, Risk and Trust, 2010 IEEEInternational Conference on, 0:49?56.Jaime Teevan, Daniel Ramage, and Meredith Ringel Mor-ris.
2011.
#twittersearch: A comparison of microblogsearch and web search.
In WSDM 2011: Fourth Inter-national Conference on Web Search and Data Mining,Feb.ChengXiang Zhai and John Lafferty.
2001.
Model-basedfeedback in the language modeling approach to infor-mation retrieval.
In Proc.
10th Intl.
Conf.
on Informa-tion and Knowledge Management, pages 403?410.C.
Zhai and J. Lafferty.
2004.
A study of smoothingmethods for language models applied to informationretrieval.
ACM Trans.
Inf.
Syst., 22(2):179?214.655
