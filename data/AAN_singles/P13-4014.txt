Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79?84,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsQuEst - A translation quality estimation frameworkLucia Specia?, Kashif Shah?, Jose G. C. de Souza?
and Trevor Cohn?
?Department of Computer ScienceUniversity of Sheffield, UK{l.specia,kashif.shah,t.cohn}@sheffield.ac.uk?Fondazione Bruno KesslerUniversity of Trento, Italydesouza@fbk.euAbstractWe describe QUEST, an open sourceframework for machine translation qualityestimation.
The framework allows the ex-traction of several quality indicators fromsource segments, their translations, exter-nal resources (corpora, language models,topic models, etc.
), as well as languagetools (parsers, part-of-speech tags, etc.).
Italso provides machine learning algorithmsto build quality estimation models.
Webenchmark the framework on a number ofdatasets and discuss the efficacy of fea-tures and algorithms.1 IntroductionAs Machine Translation (MT) systems becomewidely adopted both for gisting purposes and toproduce professional quality translations, auto-matic methods are needed for predicting the qual-ity of a translated segment.
This is referred to asQuality Estimation (QE).
Different from standardMT evaluation metrics, QE metrics do not haveaccess to reference (human) translations; they areaimed at MT systems in use.
QE has a number ofapplications, including:?
Deciding which segments need revision by atranslator (quality assurance);?
Deciding whether a reader gets a reliable gistof the text;?
Estimating how much effort it will be neededto post-edit a segment;?
Selecting among alternative translations pro-duced by different MT systems;?
Deciding whether the translation can be usedfor self-training of MT systems.Work in QE for MT started in the early 2000?s,inspired by the confidence scores used in SpeechRecognition: mostly the estimation of word pos-terior probabilities.
Back then it was called confi-dence estimation, which we believe is a narrowerterm.
A 6-week workshop on the topic at JohnHopkins University in 2003 (Blatz et al 2004)had as goal to estimate automatic metrics such asBLEU (Papineni et al 2002) and WER.
Thesemetrics are difficult to interpret, particularly at thesentence-level, and results of their very many trialsproved unsuccessful.
The overall quality of MTwas considerably lower at the time, and thereforepinpointing the very few good quality segmentswas a hard problem.
No software nor datasetswere made available after the workshop.A new surge of interest in the field started re-cently, motivated by the widespread used of MTsystems in the translation industry, as a conse-quence of better translation quality, more user-friendly tools, and higher demand for translation.In order to make MT maximally useful in thisscenario, a quantification of the quality of trans-lated segments similar to ?fuzzy match scores?from translation memory systems is needed.
QEwork addresses this problem by using more com-plex metrics that go beyond matching the sourcesegment with previously translated data.
QE canalso be useful for end-users reading translationsfor gisting, particularly those who cannot read thesource language.QE nowadays focuses on estimating more inter-pretable metrics.
?Quality?
is defined according tothe application: post-editing, gisting, etc.
A num-ber of positive results have been reported.
Exam-ples include improving post-editing efficiency byfiltering out low quality segments which would re-quire more effort or time to correct than translatingfrom scratch (Specia et al 2009; Specia, 2011),selecting high quality segments to be published asthey are, without post-editing (Soricut and Echi-habi, 2010), selecting a translation from eitheran MT system or a translation memory for post-editing (He et al 2010), selecting the best trans-lation from multiple MT systems (Specia et al792010), and highlighting sub-segments that need re-vision (Bach et al 2011).QE is generally addressed as a supervised ma-chine learning task using a variety of algorithms toinduce models from examples of translations de-scribed through a number of features and anno-tated for quality.
For an overview of various al-gorithms and features we refer the reader to theWMT12 shared task on QE (Callison-Burch etal., 2012).
Most of the research work lies ondeciding which aspects of quality are more rel-evant for a given task and designing feature ex-tractors for them.
While simple features such ascounts of tokens and language model scores can beeasily extracted, feature engineering for more ad-vanced and useful information can be quite labour-intensive.
Different language pairs or optimisationagainst specific quality scores (e.g., post-editingtime vs translation adequacy) can benefit fromvery different feature sets.QUEST, our framework for quality estimation,provides a wide range of feature extractors fromsource and translation texts and external resourcesand tools (Section 2).
These go from simple,language-independent features, to advanced, lin-guistically motivated features.
They include fea-tures that rely on information from the MT sys-tem that generated the translations, and featuresthat are oblivious to the way translations wereproduced (Section 2.1).
In addition, by inte-grating a well-known machine learning toolkit,scikit-learn,1 and algorithms that are knownto perform well on this task, QUEST provides asimple and effective way of experimenting withtechniques for feature selection and model build-ing, as well as parameter optimisation through gridsearch (Section 2.2).
In Section 3 we presentexperiments using the framework with nine QEdatasets.In addition to providing a practical platformfor quality estimation, by freeing researchers fromfeature engineering, QUEST will facilitate workon the learning aspect of the problem.
Qualityestimation poses several machine learning chal-lenges, such as the fact that it can exploit a large,diverse, but often noisy set of information sources,with a relatively small number of annotated datapoints, and it relies on human annotations that areoften inconsistent due to the subjectivity of thetask (quality judgements).
Moreover, QE is highly1http://scikit-learn.org/non-linear: unlike many other problems in lan-guage processing, considerable improvements canbe achieved using non-linear kernel techniques.Also, different applications for the quality predic-tions may benefit from different machine learn-ing techniques, an aspect that has been mostly ne-glected so far.
Finally, the framework will alsofacilitate research on ways of using quality predic-tions in novel extrinsic tasks, such as self-trainingof statistical machine translation systems, and forestimating quality in other text output applicationssuch as text summarisation.2 The QUEST frameworkQUEST consists of two main modules: a featureextraction module and a machine learning mod-ule.
The first module provides a number of featureextractors, including the most commonly used fea-tures in the literature and by systems submitted tothe WMT12 shared task on QE (Callison-Burch etal., 2012).
More than 15 researchers from 10 in-stitutions contributed to it as part of the QUESTproject.2 It is implemented in Java and providesabstract classes for features, resources and pre-processing steps so that extractors for new featurescan be easily added.The basic functioning of the feature extractionmodule requires raw text files with the source andtranslation texts, and a few resources (where avail-able) such as the source MT training corpus andlanguage models of source and target.
Configura-tion files are used to indicate the resources avail-able and a list of features that should be extracted.The machine learning module providesscripts connecting the feature files with thescikit-learn toolkit.
It also uses GPy, aPython toolkit for Gaussian Processes regression,which outperformed algorithms commonly usedfor the task such as SVM regressors.2.1 Feature setsIn Figure 1 we show the types of features thatcan be extracted in QUEST.
Although the textunit for which features are extracted can be of anylength, most features are more suitable for sen-tences.
Therefore, a ?segment?
here denotes a sen-tence.From the source segments QUEST can extractfeatures that attempt to quantify the complexity2http://www.dcs.shef.ac.uk/?lucia/projects/quest.html80Confidence indicatorsComplexity indicators Fluency indicatorsAdequacyindicatorsSource text TranslationMT systemFigure 1: Families of features in QUEST.of translating those segments, or how unexpectedthey are given what is known to the MT system.Examples of features include:?
number of tokens in the source segment;?
language model (LM) probability of sourcesegment using the source side of the parallelcorpus used to train the MT system as LM;?
percentage of source 1?3-grams observed indifferent frequency quartiles of the sourceside of the MT training corpus;?
average number of translations per sourceword in the segment as given by IBM 1model with probabilities thresholded in dif-ferent ways.From the translated segments QUEST can ex-tract features that attempt to measure the fluencyof such translations.
Examples of features include:?
number of tokens in the target segment;?
average number of occurrences of the targetword within the target segment;?
LM probability of target segment using alarge corpus of the target language to buildthe LM.From the comparison between the source andtarget segments, QUEST can extract adequacyfeatures, which attempt to measure whether thestructure and meaning of the source are pre-served in the translation.
Some of these are basedon word-alignment information as provided byGIZA++.
Features include:?
ratio of number of tokens in source and targetsegments;?
ratio of brackets and punctuation symbols insource and target segments;?
ratio of percentages of numbers, content- /non-content words in the source & target seg-ments;?
ratio of percentage of nouns/verbs/etc in thesource and target segments;?
proportion of dependency relations between(aligned) constituents in source and targetsegments;?
difference between the depth of the syntactictrees of the source and target segments;?
difference between the number ofPP/NP/VP/ADJP/ADVP/CONJP phrases inthe source and target;?
difference between the number of per-son/location/organization entities in sourceand target sentences;?
proportion of person/location/organizationentities in source aligned to the same type ofentities in target segment;?
percentage of direct object personal or pos-sessive pronouns incorrectly translated.When available, information from the MT sys-tem used to produce the translations can be veryuseful, particularly for statistical machine transla-tion (SMT).
These features can provide an indi-cation of the confidence of the MT system in thetranslations.
They are called ?glass-box?
features,to distinguish them from MT system-independent,?black-box?
features.
To extract these features,QUEST assumes the output of Moses-like SMTsystems, taking into account word- and phrase-alignment information, a dump of the decoder?sstandard output (search graph information), globalmodel score and feature values, n-best lists, etc.For other SMT systems, it can also take an XMLfile with relevant information.
Examples of glass-box features include:?
features and global score of the SMT system;?
number of distinct hypotheses in the n-bestlist;?
1?3-gram LM probabilities using translationsin the n-best to train the LM;?
average size of the target phrases;?
proportion of pruned search graph nodes;?
proportion of recombined graph nodes.We note that some of these features arelanguage-independent by definition (such as theconfidence features), while others can be depen-dent on linguistic resources (such as POS taggers),or very language-specific, such as the incorrecttranslation of pronouns, which was designed forArabic-English QE.Some word-level features have also been im-plemented: they include standard word posteriorprobabilities and n-gram probabilities for each tar-81get word.
These can also be averaged across thewhole sentence to provide sentence-level value.The complete list of features available is givenas part of QUEST?s documentation.
At the currentstage, the number of BB features varies from 80to 123 depending on the language pair, while GBfeatures go from 39 to 48 depending on the SMTsystem used (see Section 3).2.2 Machine learningQUEST provides a command-line interface mod-ule for the scikit-learn library implementedin Python.
This module is completely indepen-dent from the feature extraction code and it usesthe extracted feature sets to build QE models.The dependencies are the scikit-learn li-brary and all its dependencies (such as NumPy3and SciPy4).
The module can be configured torun different regression and classification algo-rithms, feature selection methods and grid searchfor hyper-parameter optimisation.The pipeline with feature selection and hyper-parameter optimisation can be set using a con-figuration file.
Currently, the module has aninterface for Support Vector Regression (SVR),Support Vector Classification, and Lasso learn-ing algorithms.
They can be used in conjunctionwith the feature selection algorithms (RandomisedLasso and Randomised decision trees) and the gridsearch implementation of scikit-learn to fitan optimal model of a given dataset.Additionally, QUEST includes Gaussian Pro-cess (GP) regression (Rasmussen and Williams,2006) using the GPy toolkit.5 GPs are an ad-vanced machine learning framework incorporatingBayesian non-parametrics and kernel machines,and are widely regarded as state of the art forregression.
Empirically we found the perfor-mance to be similar to SVR on most datasets,with slightly worse MAE and better RMSE.6 Incontrast to SVR, inference in GP regression canbe expressed analytically and the model hyper-parameters optimised directly using gradient as-cent, thus avoiding the need for costly grid search.This also makes the method very suitable for fea-ture selection.3http://www.numpy.org/4http://www.scipy.org/5https://github.com/SheffieldML/GPy6This follows from the optimisation objective: GPs use aquadratic loss (the log-likelihood of a Gaussian) compared toSVR which penalises absolute margin violations.Data Training TestWMT12 (en-es) 1,832 422EAMT11 (en-es) 900 64EAMT11 (fr-en) 2,300 225EAMT09-s1-s4 (en-es) 3,095 906GALE11-s1-s2 (ar-en) 2,198 387Table 1: Number of sentences used for trainingand testing in our datasets.3 BenchmarkingIn this section we benchmark QUEST on nine ex-isting datasets using feature selection and learningalgorithms known to perform well in the task.3.1 DatasetsThe statistics of the datasets used in the experi-ments are shown in Table 1.7WMT12 English-Spanish sentence translationsproduced by an SMT system and judged forpost-editing effort in 1-5 (worst-best), taking aweighted average of three annotators.EAMT11 English-Spanish (EAMT11-en-es)and French-English (EAMT11-fr-en) sentencetranslations judged for post-editing effort in 1-4.EAMT09 English sentences translated by fourSMT systems into Spanish and scored for post-editing effort in 1-4.
Systems are denoted by s1-s4.GALE11 Arabic sentences translated by twoSMT systems into English and scored for ade-quacy in 1-4.
Systems are denoted by s1-s2.3.2 SettingsAmongst the various learning algorithms availablein QUEST, to make our results comparable we se-lected SVR with radial basis function (RBF) ker-nel, which has been shown to perform very wellin this task (Callison-Burch et al 2012).
The op-timisation of parameters is done with grid searchusing the following ranges of values:?
penalty parameter C: [1, 10, 10]?
?
: [0.0001, 0.1, 10]?
: [0.1, 0.2, 10]where elements in list denote beginning, end andnumber of samples to generate, respectively.For feature selection, we have experimentedwith two techniques: Randomised Lasso and7The datasets can be downloaded from http://www.dcs.shef.ac.uk/?lucia/resources.html82Gaussian Processes.
Randomised Lasso (Mein-shausen and Bu?hlmann, 2010) repeatedly resam-ples the training data and fits a Lasso regressionmodel on each sample.
A feature is said to be se-lected if it was selected (i.e., assigned a non-zeroweight) in at least 25% of the samples (we do this1000 times).
This strategy improves the robust-ness of Lasso in the presence of high dimensionaland correlated inputs.Feature selection with Gaussian Processes isdone by fitting per-feature RBF widths (alsoknown as the automatic relevance determinationkernel).
The RBF width denotes the importanceof a feature, the narrower the RBF the more impor-tant a change in the feature value is to the modelprediction.
To make the results comparable withour baseline systems we select the 17 top rankedfeatures and then train a SVR on these features.8As feature sets, we select all features availablein QUEST for each of our datasets.
We differen-tiate between black-box (BB) and glass-box (GB)features, as only BB are available for all datasets(we did not have access to the MT systems thatproduced the other datasets).
For the WMT12 andGALE11 datasets, we experimented with both BBand GB features.
For each dataset we build foursystems:?
BL: 17 baseline features that performed wellacross languages in previous work and wereused as baseline in the WMT12 QE task.?
AF: All features available for dataset.?
FS: Feature selection for automatic rankingand selection of top features with:?
RL: Randomised Lasso.?
GP: Gaussian Process.Mean Absolute Error (MAE) and Root MeanSquared Error (RMSE) are used to evaluate themodels.3.3 ResultsThe error scores for all datasets with BB featuresare reported in Table 2, while Table 3 shows the re-sults with GB features, and Table 4 the results withBB and GB features together.
For each table anddataset, bold-faced figures are significantly betterthan all others (paired t-test with p ?
0.05).It can be seen from the results that adding moreBB features (systems AF) improves the results inmost cases as compared to the baseline systems8More features resulted in further performance gains onmost tasks, with 25?35 features giving the best results.Dataset System #feats.
MAE RMSEWMT12BL 17 0.6802 0.8192AF 80 0.6703 0.8373FS(RL) 69 0.6628 0.8107FS(GP) 17 0.6537 0.8014EAMT11(en-es)BL 17 0.4867 0.6288AF 80 0.4696 0.5438FS(RL) 29 0.4657 0.5424FS(GP) 17 0.4640 0.5420EAMT11(fr-en)BL 17 0.4387 0.6357AF 80 0.4275 0.6211FS(RL) 65 0.4266 0.6196FS(GP) 17 0.4240 0.6189EAMT09-s1BL 17 0.5294 0.6643AF 80 0.5235 0.6558FS(RL) 73 0.5190 0.6516FS(GP) 17 0.5195 0.6511EAMT09-s2BL 17 0.4604 0.5856AF 80 0.4734 0.5973FS(RL) 59 0.4601 0.5837FS(GP) 17 0.4610 0.5825EAMT09-s3BL 17 0.5321 0.6643AF 80 0.5437 0.6827FS(RL) 67 0.5338 0.6627FS(GP) 17 0.5320 0.6630EAMT09-s4BL 17 0.3583 0.4953AF 80 0.3569 0.5000FS(RL) 40 0.3554 0.4995FS(GP) 17 0.3560 0.4949GALE11-s1BL 17 0.5456 0.6905AF 123 0.5359 0.6665FS(RL) 56 0.5358 0.6649FS(GP) 17 0.5410 0.6721GALE11-s2BL 17 0.5532 0.7177AF 123 0.5381 0.6933FS(RL) 54 0.5369 0.6955FS(GP) 17 0.5424 0.6999Table 2: Results with BB features.Dataset System #feats.
MAE RMSEWMT12 AF 47 0.7036 0.8476FS(RL) 26 0.6821 0.8388FS(GP) 17 0.6771 0.8308GALE11-s1 AF 39 0.5720 0.7392FS(RL) 46 0.5691 0.7388FS(GP) 17 0.5711 0.7378GALE11-s2AF 48 0.5510 0.6977FS(RL) 46 0.5512 0.6970FS(GP) 17 0.5501 0.6978Table 3: Results with GB features.Dataset System #feats.
MAE RMSEWMT12 AF 127 0.7165 0.8476FS(RL) 26 0.6601 0.8098FS(GP) 17 0.6501 0.7989GALE11-s1 AF 162 0.5437 0.6741FS(RL) 69 0.5310 0.6681FS(GP) 17 0.5370 0.6701GALE11-s2AF 171 0.5222 0.6499FS(RL) 82 0.5152 0.6421FS(GP) 17 0.5121 0.6384Table 4: Results with BB and GB features.83BL, however, in some cases the improvements arenot significant.
This behaviour is to be expectedas adding more features may bring more relevantinformation, but at the same time it makes the rep-resentation more sparse and the learning prone tooverfitting.
In most cases, feature selection withboth or either RL and GP improves over all fea-tures (AF).
It should be noted that RL automati-cally selects the number of features used for train-ing while FS(GP) was limited to selecting the top17 features in order to make the results compara-ble with our baseline feature set.
It is interestingto note that system FS(GP) outperformed the othersystems in spite of using fewer features.
This tech-nique is promising as it reduces the time require-ments and overall computational complexity fortraining the model, while achieving similar resultscompared to systems with many more features.Another interesting question is whether thesefeature selection techniques identify a commonsubset of features from the various datasets.
Theoverall top ranked features are:?
LM perplexities and log probabilities forsource and target;?
size of source and target sentences;?
average number of possible translations ofsource words (IBM 1 with thresholds);?
ratio of target by source lengths in words;?
percentage of numbers in the target sentence;?
percentage of distinct unigrams seen in theMT source training corpus.Interestingly, not all top ranked features areamong the baseline 17 features which are report-edly best in literature.GB features on their own perform worse thanBB features, but in all three datasets, the combi-nation of GB and BB followed by feature selec-tion resulted in significantly lower errors than us-ing only BB features with feature selection, show-ing that the two features sets are complementary.4 RemarksThe source code for the framework, the datasetsand extra resources can be downloaded fromhttp://www.quest.dcs.shef.ac.uk/.The project is also set to receive contribution frominterested researchers using a GitHub repository:https://github.com/lspecia/quest.The license for the Java code, Python and shellscripts is BSD, a permissive license with no re-strictions on the use or extensions of the softwarefor any purposes, including commercial.
For pre-existing code and resources, e.g., scikit-learn, GPyand Berkeley parser, their licenses apply, but fea-tures relying on these resources can be easily dis-carded if necessary.AcknowledgmentsThis work was supported by the QuEst (EUFP7 PASCAL2 NoE, Harvest program) and QT-LaunchPad (EU FP7 CSA No.
296347) projects.ReferencesN.
Bach, F. Huang, and Y. Al-Onaizan.
2011.
Good-ness: a method for measuring machine translationconfidence.
In ACL11, pages 211?219, Portland.J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,C.
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.2004.
Confidence Estimation for Machine Transla-tion.
In Coling04, pages 315?321, Geneva.C.
Callison-Burch, P. Koehn, C. Monz, M. Post,R.
Soricut, and L. Specia.
2012.
Findings of the2012 workshop on statistical machine translation.
InWMT12, pages 10?51, Montre?al.Y.
He, Y. Ma, J. van Genabith, and A.
Way.
2010.Bridging SMT and TM with Translation Recom-mendation.
In ACL10, pages 622?630, Uppsala.N.
Meinshausen and P. Bu?hlmann.
2010.
Stability se-lection.
Journal of the Royal Statistical Society: Se-ries B (Statistical Methodology), 72:417?473.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In ACL02, pages 311?318,Philadelphia.C.E.
Rasmussen and C.K.I.
Williams.
2006.
Gaus-sian processes for machine learning, volume 1.
MITPress, Cambridge.R.
Soricut and A. Echihabi.
2010.
Trustrank: Induc-ing trust in automatic translations via ranking.
InACL11, pages 612?621, Uppsala.L.
Specia, M. Turchi, N. Cancedda, M. Dymetman,and N. Cristianini.
2009.
Estimating the Sentence-Level Quality of Machine Translation Systems.
InEAMT09, pages 28?37, Barcelona.L.
Specia, D. Raj, and M. Turchi.
2010.
Ma-chine translation evaluation versus quality estima-tion.
Machine Translation, 24(1):39?50.L.
Specia.
2011.
Exploiting objective annotationsfor measuring translation post-editing effort.
InEAMT11, pages 73?80, Leuven.84
