Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 420?429,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsModel-Based Aligner Combination Using Dual DecompositionJohn DeNeroGoogle Researchdenero@google.comKlaus MachereyGoogle Researchkmach@google.comAbstractUnsupervised word alignment is most oftenmodeled as a Markov process that generates asentence f conditioned on its translation e. Asimilar model generating e from f will makedifferent alignment predictions.
Statisticalmachine translation systems combine the pre-dictions of two directional models, typicallyusing heuristic combination procedures likegrow-diag-final.
This paper presents a graph-ical model that embeds two directional align-ers into a single model.
Inference can be per-formed via dual decomposition, which reusesthe efficient inference algorithms of the direc-tional models.
Our bidirectional model en-forces a one-to-one phrase constraint while ac-counting for the uncertainty in the underlyingdirectional models.
The resulting alignmentsimprove upon baseline combination heuristicsin word-level and phrase-level evaluations.1 IntroductionWord alignment is the task of identifying corre-sponding words in sentence pairs.
The standardapproach to word alignment employs directionalMarkov models that align the words of a sentencef to those of its translation e, such as IBM Model 4(Brown et al, 1993) or the HMM-based alignmentmodel (Vogel et al, 1996).Machine translation systems typically combinethe predictions of two directional models, one whichaligns f to e and the other e to f (Och et al,1999).
Combination can reduce errors and relaxthe one-to-many structural restriction of directionalmodels.
Common combination methods include theunion or intersection of directional alignments, aswell as heuristic interpolations between the unionand intersection like grow-diag-final (Koehn et al,2003).
This paper presents a model-based alterna-tive to aligner combination.
Inference in a prob-abilistic model resolves the conflicting predictionsof two directional models, while taking into accounteach model?s uncertainty over its output.This result is achieved by embedding two direc-tional HMM-based alignment models into a largerbidirectional graphical model.
The full model struc-ture and potentials allow the two embedded direc-tional models to disagree to some extent, but rewardagreement.
Moreover, the bidirectional model en-forces a one-to-one phrase alignment structure, sim-ilar to the output of phrase alignment models (Marcuand Wong, 2002; DeNero et al, 2008), unsuper-vised inversion transduction grammar (ITG) models(Blunsom et al, 2009), and supervised ITG models(Haghighi et al, 2009; DeNero and Klein, 2010).Inference in our combined model is not tractablebecause of numerous edge cycles in the modelgraph.
However, we can employ dual decomposi-tion as an approximate inference technique (Rush etal., 2010).
In this approach, we iteratively apply thesame efficient sequence algorithms for the underly-ing directional models, and thereby optimize a dualbound on the model objective.
In cases where ouralgorithm converges, we have a certificate of opti-mality under the full model.
Early stopping beforeconvergence still yields useful outputs.Our model-based approach to aligner combina-tion yields improvements in alignment quality andphrase extraction quality in Chinese-English exper-iments, relative to typical heuristic combinationsmethods applied to the predictions of independentdirectional models.4202 Model DefinitionOur bidirectional model G = (V,D) is a globallynormalized, undirected graphical model of the wordalignment for a fixed sentence pair (e,f).
Each ver-tex in the vertex set V corresponds to a model vari-able Vi, and each undirected edge in the edge set Dcorresponds to a pair of variables (Vi, Vj).
Each ver-tex has an associated potential function ?i(vi) thatassigns a real-valued potential to each possible valuevi of Vi.1 Likewise, each edge has an associated po-tential function ?ij(vi, vj) that scores pairs of val-ues.
The probability under the model of any full as-signment v to the model variables, indexed by V ,factors over vertex and edge potentials.P(v) ?
?vi?V?i(vi) ??
(vi,vj)?D?ij(vi, vj)Our model contains two directional hiddenMarkov alignment models, which we review in Sec-tion 2.1, along with additional structure that that weintroduce in Section 2.2.2.1 HMM-Based Alignment ModelThis section describes the classic hidden Markovmodel (HMM) based alignment model (Vogel et al,1996).
The model generates a sequence of words fconditioned on a word sequence e. We convention-ally index the words of e by i and f by j. P(f |e)is defined in terms of a latent alignment vector a,where aj = i indicates that word position i of ealigns to word position j of f .P(f |e) =?aP(f ,a|e)P(f ,a|e) =|f |?j=1D(aj |aj?1)M(fj |eaj ) .
(1)In Equation 1 above, the emission model M isa learned multinomial distribution over word types.The transition model D is a multinomial over tran-sition distances, which treats null alignments as aspecial case.D(aj = 0|aj?1 = i) = poD(aj = i?
6= 0|aj?1 = i) = (1?
po) ?
c(i?
?
i) ,1Potentials in an undirected model play the same role as con-ditional probabilities in a directed model, but do not need to belocally normalized.where c(i?
?
i) is a learned distribution over signeddistances, normalized over the possible transitionsfrom i.
The parameters of the conditional multino-mial M and the transition model c can be learnedfrom a sentence aligned corpus via the expectationmaximization algorithm.
The null parameter po istypically fixed.2The highest probability word alignment vectorunder the model for a given sentence pair (e,f) canbe computed exactly using the standard Viterbi al-gorithm for HMMs in O(|e|2 ?
|f |) time.An alignment vector a can be converted triviallyinto a set of word alignment links A:Aa = {(i, j) : aj = i, i 6= 0} .Aa is constrained to be many-to-one from f to e;many positions j can align to the same i, but each jappears at most once.We have defined a directional model that gener-ates f from e. An identically structured model canbe defined that generates e from f .
Let b be a vectorof alignments where bi = j indicates that word po-sition j of f aligns to word position i of e. Then,P(e,b|f) is defined similarly to Equation 1, butwith e and f swapped.
We can distinguish the tran-sition and emission distributions of the two modelsby subscripting them with their generative direction.P(e,b|f) =|e|?j=1Df?e(bi|bi?1)Mf?e(ei|fbi) .The vector b can be interpreted as a set of align-ment links that is one-to-many: each value i appearsat most once in the set.Ab = {(i, j) : bi = j, j 6= 0} .2.2 A Bidirectional Alignment ModelWe can combine two HMM-based directional align-ment models by embedding them in a larger model2In experiments, we set po = 10?6.
Transitions from a null-aligned state aj?1 = 0 are also drawn from a fixed distribution,where D(aj = 0|aj?1 = 0) = 10?4 and for i?
?
1,D(aj = i?|aj?1 = 0) ?
0.8(????i??|f||e|?j???
).With small po, the shape of this distribution has little effect onthe alignment outcome.421How areyou?
?How areyou?
?How areyoua1c11b2b2c22(a)c22(b)a2a3b1c12c13c21c22c23a1a2a3b1c21(a)c21(b)c23(a)c23(b)c13(a)c13(b)c12(a)c12(b)c11(a)c11(b)c22(a)a1a2a3c21(a)c23(a)c13(a)c12(a)c11(a)Figure 1: The structure of our graphical model for a sim-ple sentence pair.
The variables a are blue, b are red, andc are green.that includes all of the random variables of two di-rectional models, along with additional structure thatpromotes agreement and resolves discrepancies.The original directional models include observedword sequences e and f , along with the two latentalignment vectors a and b defined in Section 2.1.Because the word types and lengths of e and f arealways fixed by the observed sentence pair, we candefine our model only over a and b, where the edgepotentials between any aj , fj , and e are compiledinto a vertex potential function ?
(a)j on aj , definedin terms of f and e, and likewise for any bi.?
(a)j (i) = Me?f (fj |ei)?
(b)i (j) = Mf?e(ei|fj)The edge potentials between a and b encode thetransition model in Equation 1.?
(a)j?1,j(i, i?)
= De?f (aj = i?|aj?1 = i)?
(b)i?1,i(j, j?)
= Df?e(bi = j?|bi?1 = j)In addition, we include in our model a latentboolean matrix c that encodes the output of the com-bined aligners:c ?
{0, 1}|e|?|f | .This matrix encodes the alignment links proposedby the bidirectional model:Ac = {(i, j) : cij = 1} .Each model node for an element cij ?
{0, 1} isconnected to aj and bi via coherence edges.
Theseedges allow the model to ensure that the three setsof variables, a, b, and c, together encode a coher-ent alignment analysis of the sentence pair.
Figure 1depicts the graph structure of the model.2.3 Coherence PotentialsThe potentials on coherence edges are not learnedand do not express any patterns in the data.
Instead,they are fixed functions that promote consistency be-tween the integer-valued directional alignment vec-tors a and b and the boolean-valued matrix c.Consider the assignment aj = i, where i = 0indicates that word fj is null-aligned, and i ?
1 in-dicates that fj aligns to ei.
The coherence potentialensures the following relationship between the vari-able assignment aj = i and the variables ci?j , forany i?
?
[1, |e|].?
If i = 0 (null-aligned), then all ci?j = 0.?
If i > 0, then cij = 1.?
ci?j = 1 only if i?
?
{i?
1, i, i+ 1}.?
Assigning ci?j = 1 for i?
6= i incurs a cost e?
?.Collectively, the list of cases above enforce an intu-itive correspondence: an alignment aj = i ensuresthat cij must be 1, adjacent neighbors may be 1 butincur a cost, and all other elements are 0.This pattern of effects can be encoded in a poten-tial function ?
(c) for each coherence edge.
Theseedge potential functions takes an integer value i forsome variable aj and a binary value k for some ci?j .?
(c)(aj ,ci?j)(i, k) =????????????????????????
?1 i = 0 ?
k = 00 i = 0 ?
k = 11 i = i?
?
k = 10 i = i?
?
k = 01 i 6= i?
?
k = 0e??
|i?
i?| = 1 ?
k = 10 |i?
i?| > 1 ?
k = 1(2)Above, potentials of 0 effectively disallow certaincases because a full assignment to (a,b, c) is scoredby the product of all model potentials.
The poten-tial function ?(c)(bi,cij?
)(j, k) for a coherence edge be-tween b and c is defined similarly.4222.4 Model PropertiesWe interpret c as the final alignment produced by themodel, ignoring a and b.
In this way, we relax theone-to-many constraints of the directional models.However, all of the information about how wordsalign is expressed by the vertex and edge potentialson a and b.
The coherence edges and the link ma-trix c only serve to resolve conflicts between the di-rectional models and communicate information be-tween them.Because directional alignments are preserved in-tact as components of our model, extensions orrefinements to the underlying directional Markovalignment model could be integrated cleanly intoour model as well, including lexicalized transitionmodels (He, 2007), extended conditioning contexts(Brunning et al, 2009), and external information(Shindo et al, 2010).For any assignment to (a,b, c) with non-zeroprobability, c must encode a one-to-one phrasealignment with a maximum phrase length of 3.
Thatis, any word in either sentence can align to at mostthree words in the opposite sentence, and thosewords must be contiguous.
This restriction is di-rectly enforced by the edge potential in Equation 2.3 Model InferenceIn general, graphical models admit efficient, exactinference algorithms if they do not contain cycles.Unfortunately, our model contains numerous cycles.For every pair of indices (i, j) and (i?, j?
), the fol-lowing cycle exists in the graph:cij ?
bi ?
cij?
?
aj?
?ci?j?
?
bi?
?
ci?j ?
aj ?
cijAdditional cycles also exist in the graph throughthe edges between aj?1 and aj and between bi?1and bi.
The general phrase alignment problem underan arbitrary model is known to be NP-hard (DeNeroand Klein, 2008).3.1 Dual DecompositionWhile the entire graphical model has loops, there aretwo overlapping subgraphs that are cycle-free.
Onesubgraph Ga includes all of the vertices correspond-ing to variables a and c. The other subgraph Gb in-cludes vertices for variables b and c. Every edge inthe graph belongs to exactly one of these two sub-graphs.The dual decomposition inference approach al-lows us to exploit this sub-graph structure (Rush etal., 2010).
In particular, we can iteratively applyexact inference to the subgraph problems, adjustingtheir potentials to reflect the constraints of the fullproblem.
The technique of dual decomposition hasrecently been shown to yield state-of-the-art perfor-mance in dependency parsing (Koo et al, 2010).3.2 Dual Problem FormulationTo describe a dual decomposition inference proce-dure for our model, we first restate the inferenceproblem under our graphical model in terms of thetwo overlapping subgraphs that admit tractable in-ference.
Let c(a) be a copy of c associated with Ga,and c(b) with Gb.
Also, let f(a, c(a)) be the un-normalized log-probability of an assignment to Gaand g(b, c(b)) be the unnormalized log-probabilityof an assignment to Gb.
Finally, let I be the indexset of all (i, j) for c. Then, the maximum likelihoodassignment to our original model can be found byoptimizingmaxa,b,c(a),c(b)f(a, c(a)) + g(b, c(b)) (3)such that: c(a)ij = c(b)ij ?
(i, j) ?
I .The Lagrangian relaxation of this optimizationproblem is L(a,b, c(a), c(b),u) =f(a, c(a))+ g(b, c(b))+?
(i,j)?Iu(i, j)(c(a)i,j ?c(b)i,j ) .Hence, we can rewrite the original problem asmaxa,b,c(a),c(b)minuL(a,b, c(a), c(b),u) .We can form a dual problem that is an up-per bound on the original optimization problem byswapping the order of min and max.
In this case,the dual problem decomposes into two terms that areeach local to an acyclic subgraph.minu??maxa,c(a)?
?f(a, c(a)) +?i,ju(i, j)c(a)ij?
?+ maxb,c(b)?
?g(b, c(b))?
?i,ju(i, j)c(b)ij????
(4)423How areyou?
?How areyou?
?How areyoua1c11b2b2c22(a)c22(b)a2a3b1c12c13c21c22c23a1a2a3b1c21(a)c21(b)c23(a)c23(b)c13(a)c13(b)c12(a)c12(b)c11(a)c11(b)c22(a)a1a2a3c21(a)c23(a)c13(a)c12(a)c11(a)Figure 2: Our combined model decomposes into twoacyclic models that each contain a copy of c.The decomposed model is depicted in Figure 2.As in previous work, we solve for the dual variableu by repeatedly performing inference in the two de-coupled maximization problems.3.3 Sub-Graph InferenceWe now address the problem of evaluating Equa-tion 4 for fixed u.
Consider the first line of Equa-tion 4, which includes variables a and c(a).maxa,c(a)?
?f(a, c(a)) +?i,ju(i, j)c(a)ij??
(5)Because the graph Ga is tree-structured, Equa-tion 5 can be evaluated in polynomial time.
In fact,we can make a stronger claim: we can reuse theViterbi inference algorithm for linear chain graph-ical models that applies to the embedded directionalHMM models.
That is, we can cast the optimizationof Equation 5 asmaxa?
?|f |?j=1De?f (aj |aj?1) ?M?j(aj = i)??
.In the original HMM-based aligner, the vertex po-tentials correspond to bilexical probabilities.
Thosequantities appear in f(a, c(a)), and therefore will bea part of M?j(?)
above.
The additional terms of Equa-tion 5 can also be factored into the vertex poten-tials of this linear chain model, because the optimalHow areyou?
?How areyou?
?How areyoua1c11b2b2c22(a)c22(b)a2a3b1c12c13c21c22c23a1a2a3b1c21(a)c21(b)c23(a)c23(b)c13(a)c13(b)c12(a)c12(b)c11(a)c11(b)c22(a)a1a2a3c21(a)c23(a)c13(a)c12(a)c11(a)Figure 3: The tree-structured subgraph Ga can be mappedto an equivalent chain-structured model by optimizingover ci?j for aj = i.choice of each cij can be determined from aj and themodel parameters.
If aj = i, then cij = 1 accordingto our edge potential defined in Equation 2.
Hence,setting aj = i requires the inclusion of the corre-sponding vertex potential ?
(a)j (i), as well as u(i, j).For i?
6= i, either ci?j = 0, which contributes noth-ing to Equation 5, or ci?j = 1, which contributesu(i?, j)?
?, according to our edge potential betweenaj and ci?j .Thus, we can capture the net effect of assigningaj and then optimally assigning all ci?j in a singlepotential M?j(aj = i) =?
(a)j (i) + exp?
?u(i, j) +?j?:|j?
?j|=1max(0, u(i, j?)?
?)?
?Note that Equation 5 and f are sums of terms inlog space, while Viterbi inference for linear chainsassumes a product of terms in probability space,which introduces the exponentiation above.Defining this potential allows us to collapse thesource-side sub-graph inference problem definedby Equation 5, into a simple linear chain modelthat only includes potential functions M?j and ?
(a).Hence, we can use a highly optimized linear chaininference implementation rather than a solver forgeneral tree-structured graphical models.
Figure 3depicts this transformation.An equivalent approach allows us to evaluate the424Algorithm 1 Dual decomposition inference algo-rithm for the bidirectional modelfor t = 1 to max iterations dor ?
1t .
Learning ratec(a) ?
argmax f(a, c(a)) +?i,j u(i, j)c(a)ijc(b) ?
argmax g(b, c(b))?
?i,j u(i, j)c(b)ijif c(a) = c(b) thenreturn c(a) .
Convergedu?
u + r ?
(c(b) ?
c(a)) .
Dual updatereturn combine(c(a), c(b)) .
Stop earlysecond line of Equation 4 for fixed u:maxb,c(b)?
?g(b, c(b)) +?i,ju(i, j)c(b)ij??
.
(6)3.4 Dual Decomposition AlgorithmNow that we have the means to efficiently evalu-ate Equation 4 for fixed u, we can define the fulldual decomposition algorithm for our model, whichsearches for a u that optimizes Equation 4.
We caniteratively search for such a u via sub-gradient de-scent.
We use a learning rate 1t that decays with thenumber of iterations t. The full dual decompositionoptimization procedure appears in Algorithm 1.If Algorithm 1 converges, then we have found a usuch that the value of c(a) that optimizes Equation 5is identical to the value of c(b) that optimizes Equa-tion 6.
Hence, it is also a solution to our originaloptimization problem: Equation 3.
Since the dualproblem is an upper bound on the original problem,this solution must be optimal for Equation 3.3.5 Convergence and Early StoppingOur dual decomposition algorithm provides an infer-ence method that is exact upon convergence.3 WhenAlgorithm 1 does not converge, the two alignmentsc(a) and c(b) can still be used.
While these align-ments may differ, they will likely be more similarthan the alignments of independent aligners.These alignments will still need to be combinedprocedurally (e.g., taking their union), but because3This certificate of optimality is not provided by other ap-proximate inference algorithms, such as belief propagation,sampling, or simulated annealing.they are more similar, the importance of the combi-nation procedure is reduced.
We analyze the behav-ior of early stopping experimentally in Section 5.3.6 Inference PropertiesBecause we set a maximum number of iterationsn in the dual decomposition algorithm, and eachiteration only involves optimization in a sequencemodel, our entire inference procedure is only a con-stant multiple n more computationally expensivethan evaluating the original directional aligners.Moreover, the value of u is specific to a sen-tence pair.
Therefore, our approach does not requireany additional communication overhead relative tothe independent directional models in a distributedaligner implementation.
Memory requirements arevirtually identical to the baseline: only u must bestored for each sentence pair as it is being processed,but can then be immediately discarded once align-ments are inferred.Other approaches to generating one-to-one phrasealignments are generally more expensive.
In par-ticular, an ITG model requires O(|e|3 ?
|f |3) time,whereas our algorithm requires onlyO(n ?
(|f ||e|2 + |e||f |2)) .Moreover, our approach allows Markov distortionpotentials, while standard ITG models are restrictedto only hierarchical distortion.4 Related WorkAlignment combination normally involves selectingsome A from the output of two directional models.Common approaches include forming the union orintersection of the directional sets.A?
= Aa ?
AbA?
= Aa ?
Ab .More complex combiners, such as the grow-diag-final heuristic (Koehn et al, 2003), produce align-ment link sets that include all of A?
and some sub-set ofA?
based on the relationship of multiple links(Och et al, 1999).In addition, supervised word alignment modelsoften use the output of directional unsupervisedaligners as features or pruning signals.
In the case425that a supervised model is restricted to proposingalignment links that appear in the output of a di-rectional aligner, these models can be interpreted asa combination technique (Deng and Zhou, 2009).Such a model-based approach differs from ours inthat it requires a supervised dataset and treats the di-rectional aligners?
output as fixed.Combination is also related to agreement-basedlearning (Liang et al, 2006).
This approach tojointly learning two directional alignment mod-els yields state-of-the-art unsupervised performance.Our method is complementary to agreement-basedlearning, as it applies to Viterbi inference under themodel rather than computing expectations.
In fact,we employ agreement-based training to estimate theparameters of the directional aligners in our experi-ments.A parallel idea that closely relates to our bidi-rectional model is posterior regularization, whichhas also been applied to the word alignment prob-lem (Grac?a et al, 2008).
One form of posteriorregularization stipulates that the posterior probabil-ity of alignments from two models must agree, andenforces this agreement through an iterative proce-dure similar to Algorithm 1.
This approach alsoyields state-of-the-art unsupervised alignment per-formance on some datasets, along with improve-ments in end-to-end translation quality (Ganchev etal., 2008).Our method differs from this posterior regulariza-tion work in two ways.
First, we iterate over Viterbipredictions rather than posteriors.
More importantly,we have changed the output space of the model tobe a one-to-one phrase alignment via the coherenceedge potential functions.Another similar line of work applies belief prop-agation to factor graphs that enforce a one-to-oneword alignment (Cromie`res and Kurohashi, 2009).The details of our models differ: we employdistance-based distortion, while they add structuralcorrespondence terms based on syntactic parse trees.Also, our model training is identical to the HMM-based baseline training, while they employ beliefpropagation for both training and Viterbi inference.Although differing in both model and inference, ourwork and theirs both find improvements from defin-ing graphical models for alignment that do not admitexact polynomial-time inference algorithms.Aligner Intersection Union AgreementModel |A?| |A?| |A?|/|A?|Baseline 5,554 10,998 50.5%Bidirectional 7,620 10,262 74.3%Table 1: The bidirectional model?s dual decompositionalgorithm substantially increases the overlap between thepredictions of the directional models, measured by thenumber of links in their intersection.5 Experimental ResultsWe evaluated our bidirectional model by comparingits output to the annotations of a hand-aligned cor-pus.
In this way, we can show that the bidirectionalmodel improves alignment quality and enables theextraction of more correct phrase pairs.5.1 Data ConditionsWe evaluated alignment quality on a hand-alignedportion of the NIST 2002 Chinese-English test set(Ayan and Dorr, 2006).
We trained the model on aportion of FBIS data that has been used previouslyfor alignment model evaluation (Ayan and Dorr,2006; Haghighi et al, 2009; DeNero and Klein,2010).
We conducted our evaluation on the first 150sentences of the dataset, following previous work.This portion of the dataset is commonly used to trainsupervised models.We trained the parameters of the directional mod-els using the agreement training variant of the expec-tation maximization algorithm (Liang et al, 2006).Agreement-trained IBM Model 1 was used to ini-tialize the parameters of the HMM-based alignmentmodels (Brown et al, 1993).
Both IBM Model 1and the HMM alignment models were trained for5 iterations on a 6.2 million word parallel corpusof FBIS newswire.
This training regimen on thisdata set has provided state-of-the-art unsupervisedresults that outperform IBM Model 4 (Haghighi etal., 2009).5.2 Convergence AnalysisWith n = 250 maximum iterations, our dual decom-position inference algorithm only converges 6.2%of the time, perhaps largely due to the fact that thetwo directional models have different one-to-manystructural constraints.
However, the dual decompo-426Model Combiner Prec Rec AERunion 57.6 80.0 33.4Baseline intersect 86.2 62.7 27.2grow-diag 60.1 78.8 32.1union 63.3 81.5 29.1Bidirectional intersect 77.5 75.1 23.6grow-diag 65.6 80.6 28.0Table 2: Alignment error rate results for the bidirectionalmodel versus the baseline directional models.
?grow-diag?
denotes the grow-diag-final heuristic.Model Combiner Prec Rec F1union 75.1 33.5 46.3Baseline intersect 64.3 43.4 51.8grow-diag 68.3 37.5 48.4union 63.2 44.9 52.5Bidirectional intersect 57.1 53.6 55.3grow-diag 60.2 47.4 53.0Table 3: Phrase pair extraction accuracy for phrase pairsup to length 5.
?grow-diag?
denotes the grow-diag-finalheuristic.sition algorithm does promote agreement betweenthe two models.
We can measure the agreementbetween models as the fraction of alignment linksin the union A?
that also appear in the intersectionA?
of the two directional models.
Table 1 showsa 47% relative increase in the fraction of links thatboth models agree on by running dual decomposi-tion (bidirectional), relative to independent direc-tional inference (baseline).
Improving convergencerates represents an important area of future work.5.3 Alignment Error EvaluationTo evaluate alignment error of the baseline direc-tional aligners, we must apply a combination pro-cedure such as union or intersection to Aa and Ab.Likewise, in order to evaluate alignment error forour combined model in cases where the inferencealgorithm does not converge, we must apply combi-nation to c(a) and c(b).
In cases where the algorithmdoes converge, c(a) = c(b) and so no further combi-nation is necessary.We evaluate alignments relative to hand-aligneddata using two metrics.
First, we measure align-ment error rate (AER), which compares the pro-posed alignment setA to the sure set S and possibleset P in the annotation, where S ?
P .Prec(A,P) =|A ?
P||A|Rec(A,S) =|A ?
S||S|AER(A,S,P) = 1?|A ?
S|+ |A ?
P||A|+ |S|AER results for Chinese-English are reported inTable 2.
The bidirectional model improves both pre-cision and recall relative to all heuristic combinationtechniques, including grow-diag-final (Koehn et al,2003).
Intersected alignments, which are one-to-onephrase alignments, achieve the best AER.Second, we measure phrase extraction accuracy.Extraction-based evaluations of alignment better co-incide with the role of word aligners in machinetranslation systems (Ayan and Dorr, 2006).
LetR5(S,P) be the set of phrases up to length 5 ex-tracted from the sure link set S and possible link setP .
Possible links are both included and excludedfrom phrase pairs during extraction, as in DeNeroand Klein (2010).
Null aligned words are never in-cluded in phrase pairs for evaluation.
Phrase ex-traction precision, recall, and F1 for R5(A,A) arereported in Table 3.
Correct phrase pair recall in-creases from 43.4% to 53.6% (a 23.5% relative in-crease) for the bidirectional model, relative to thebest baseline.Finally, we evaluated our bidirectional model in alarge-scale end-to-end phrase-based machine trans-lation system from Chinese to English, based onthe alignment template approach (Och and Ney,2004).
The translation model weights were tuned forboth the baseline and bidirectional alignments usinglattice-based minimum error rate training (Kumar etal., 2009).
In both cases, union alignments outper-formed other combination heuristics.
Bidirectionalalignments yielded a modest improvement of 0.2%BLEU4 on a single-reference evaluation set of sen-tences sampled from the web (Papineni et al, 2002).4BLEU improved from 29.59% to 29.82% after trainingIBM Model 1 for 3 iterations and training the HMM-basedalignment model for 3 iterations.
During training, link poste-riors were symmetrized by pointwise linear interpolation.427As our model only provides small improvements inalignment precision and recall for the union com-biner, the magnitude of the BLEU improvement isnot surprising.6 ConclusionWe have presented a graphical model that combinestwo classical HMM-based alignment models.
Ourbidirectional model, which requires no additionallearning and no supervised data, can be applied us-ing dual decomposition with only a constant factoradditional computation relative to independent di-rectional inference.
The resulting predictions im-prove the precision and recall of both alignmentlinks and extraced phrase pairs in Chinese-Englishexperiments.
The best results follow from combina-tion via intersection.Because our technique is defined declaratively interms of a graphical model, it can be extended in astraightforward manner, for instance with additionalpotentials on c or improvements to the componentdirectional models.
We also look forward to dis-covering the best way to take advantage of thesenew alignments in downstream applications like ma-chine translation, supervised word alignment, bilin-gual parsing (Burkett et al, 2010), part-of-speechtag induction (Naseem et al, 2009), or cross-lingualmodel projection (Smith and Eisner, 2009; Das andPetrov, 2011).ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
Going be-yond AER: An extensive analysis of word alignmentsand their impact on MT.
In Proceedings of the Asso-ciation for Computational Linguistics.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of theAssociation for Computational Linguistics.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics.Jamie Brunning, Adria de Gispert, and William Byrne.2009.
Context-dependent alignment models for statis-tical machine translation.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.David Burkett, John Blitzer, and Dan Klein.
2010.Joint parsing and alignment with weakly synchronizedgrammars.
In Proceedings of the North American As-sociation for Computational Linguistics and IJCNLP.Fabien Cromie`res and Sadao Kurohashi.
2009.
Analignment algorithm using belief propagation and astructure-based distortion model.
In Proceedings ofthe European Chapter of the Association for Compu-tational Linguistics and IJCNLP.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of the Association for Computa-tional Linguistics.John DeNero and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In Proceedings of the As-sociation for Computational Linguistics.John DeNero and Dan Klein.
2010.
Discriminative mod-eling of extraction sets for machine translation.
InProceedings of the Association for Computational Lin-guistics.John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing.Yonggang Deng and Bowen Zhou.
2009.
Optimizingword alignment combination for phrase table training.In Proceedings of the Association for ComputationalLinguistics.Kuzman Ganchev, Joao Grac?a, and Ben Taskar.
2008.Better alignments = better translations?
In Proceed-ings of the Association for Computational Linguistics.Joao Grac?a, Kuzman Ganchev, and Ben Taskar.
2008.Expectation maximization and posterior constraints.In Proceedings of Neural Information Processing Sys-tems.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In Proceedings of the Association forComputational Linguistics.Xiaodong He.
2007.
Using word-dependent transitionmodels in HMM-based word alignment for statisticalmachine.
In ACL Workshop on Statistical MachineTranslation.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the North American Chapter of the Associationfor Computational Linguistics.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decomposi-tion for parsing with non-projective head automata.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.428Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Josef Och.
2009.
Efficient minimum error ratetraining and minimum bayes-risk decoding for trans-lation hypergraphs and lattices.
In Proceedings of theAssociation for Computational Linguistics.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, andRegina Barzilay.
2009.
Multilingual part-of-speechtagging: Two unsupervised approaches.
Journal of Ar-tificial Intelligence Research.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics.Franz Josef Och, Christopher Tillman, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of theAssociation for Computational Linguistics.Alexander M. Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.2010.
Word alignment with synonym regularization.In Proceedings of the Association for ComputationalLinguistics.David A. Smith and Jason Eisner.
2009.
Parser adapta-tion and projection with quasi-synchronous grammarfeatures.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the Conference on Computa-tional linguistics.429
