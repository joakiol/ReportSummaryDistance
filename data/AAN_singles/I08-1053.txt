Translating Compounds by Learning ComponentGloss Translation Models via Multiple LanguagesNikesh Garera and David YarowskyDepartment of Computer ScienceCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USA{ngarera,yarowsky}@cs.jhu.eduAbstractThis paper presents an approach to thetranslation of compound words without theneed for bilingual training text, by mod-eling the mapping of literal componentword glosses (e.g.
?iron-path?)
into flu-ent English (e.g.
?railway?)
across mul-tiple languages.
Performance is improvedby adding component-sequence and learned-morphology models along with context sim-ilarity from monolingual text and optionalcombination with traditional bilingual-text-based translation discovery.1 IntroductionCompound words such as lighthouse and fireplaceare words that are composed of two or more compo-nent words and are often a challenge for machinetranslation due to their potentially complex com-pounding behavior and ambiguous interpretations(Rackow et al, 1992).
For many languages, suchwords form a significant portion of the lexicon andthe compounding process is further complicated bydiverse morphological processes (Levi, 1978) andthe properties of different compound sequences suchas Noun-Noun, Adj-Adj, Adj-Noun, Verb-Verb, etc.Compounds also tend to have a high type frequencybut a low token frequency which makes their transla-tion difficult to learn using corpus-based algorithms(Tanaka and Baldwin, 2003).
Furthermore, most ofthe literature on compound translation has been re-stricted to a few languages dealing with compound-ing phenomena specific to the language in question.Compound Splitting English Gloss TranslationInput: Distilled glosses from German-English dictionaryKrankenhaus Kranken-Haus sick-house hospitalRegenschirm Regen-Schirm rain-guard umbrellaWo?rterBuch Wo?rter-Buch words-book dictionaryEisenbahn Eisen-Bahn iron-path railroadInput: Distilled glosses from Swedish-English dictionarySjukhus Sjhu-Khus sick-house hospitalJa?rnva?g Ja?rn-va?g iron-path railwayOrdbok Ord-Bok words-book dictionaryGoal: To translate new Albanian compoundsHekurudhe?
Hekur-Udhe?
iron-path ??
?Table 1: Example lexical resources used in this task and theirapplication to translating compound words in new languages.With these challenges in mind, the primary goal ofthis work is to improve the coverage of translationlexicons for compounds, as illustrated in Table 1and Figure 1, in multiple new languages.
We showhow using cross-language compound evidence ob-tained from bilingual dictionaries can aid in com-pound translation.
A primary motivating idea forthis work is that the literal component glosses forcompound words (such as ?iron path?
for railway)is often replicated in multiple languages, providinginsight into the fluent translation of a similar literalgloss in a new (often resource-poor) language.2 Resources UtilizedThe only resource utilized for our compound trans-lation lexicon algorithm is a collection of bilingualdictionaries.
We used bilingual dictionary collec-tions for 50 languages that were acquired in elec-tronic form over the Internet or via optical characterrecognition (OCR) on paper dictionaries.
Note thatno parallel or even monolingual corpora is required,their use described later in the paper is optional.4033 Related WorkThe compound-translation literature typically dealswith these steps: 1) Compound splitting, 2) transla-tion candidate generation and 3) translation candi-date scoring.
Compound splitting is generally doneusing translation lexicon lookup and allowing fordifferent splitting options based on corpus frequency(Zhang et al, 2000; Koehn and Knight, 2003).Translation candidate generation is an importantphase and this is where our work differs signifi-cantly from the previous literature.
Most of the pre-vious work has been focused on generating com-positional translation candidates, that is, the trans-lation candidates of the compound words are lexi-cally composed of the component word translations.This has been done by either just concatenating thetranslations of component words to form a candi-date (Grefenstette, 1999; Cao and Li, 2002), or us-ing syntactic templates such as ?E2 in E1?, ?E1 ofE2?
to form translation candidates from the transla-tion of the component words E2 and E1 (Baldwinand Tanaka, 2004), or using synsets of the compo-nent word translations to include synonyms in thecompositional candidates (Navigli et al, 2003).The above class of work in compositional-candidategeneration fails to translate compounds such asKrankenhaus (hospital) whose component wordtranslations are Kranken (sick) and Haus (hospital),and composing sick and house in any order will notresult in the correct translation (hospital).
Anotherproblem with using fixed syntactic templates is thatthey are restricted to the specific patterns occurringin the target language.
We show how one can usethe gloss patterns of compounds in multiple otherlanguages to hypothesize translation candidates thatare not lexically compositional.4 ApproachOur approach to compound word translation is illus-trated in Figure 1.4.1 Splitting compound words and glossgeneration with translation lexicon lookupWe first split a given source word, such as the Al-banian compound hekurudhe?, into a set of compo-nent word partitions, such as hekur (iron) and udhe?(path).
Our initial approach is to consider all possi-ble partitions based on contiguous component wordsfound in a small dictionary for the language, as inGoal: To translate thisAlbanian compound word:udh?
(English gloss)Compound splittingusing lexicon lookupUsing small AlbanianEnglish dictionaryItalian-English dictionaryferrovia      --->  ferro    via(railroad)   <--- (iron)  (path)German-English dictionaryeisenbahn  --->  eisen     bahn(railroad)   <---  (iron)    (path)Swedish-English dictionaryj?rnv?g  --->    j?rn          v?g(railway)    <--- (iron)    (path)Uighur-English dictionaryt?m?ryol   --->   t?m?r     yol(railroad)    <--- (iron)     (path)Lookup words in otherlanguages that result in"iron path" after splittingCandidatetranslationsof hekurudh?Other dictionariesiron pathhekurhekurudh?zog birdudh?
pathhekur ironvadis water0.190.140.05railroadrailwayrailAlgorithm outputfor hekurudh?
(iron) (path)Figure 1: Illustration of using cross-language evidence us-ing bilingual dictionaries of different languages for compoundtranslation404Brown (2002) and Koehn and Knight (2003)1.
For agiven split, we generate its English glosses by usingall possible English translations of the componentwords given in the dictionary of that language2.4.2 Using cross-language evidence fromdifferent bilingual dictionariesFor many compound words (especially for borrow-ings), the compounding process is identical acrossseveral languages and the literal English gloss re-mains the same across these languages.
For ex-ample, the English word railway is translated as acompound word in many languages, and the Englishgloss of those compounds is often ?iron path?
or asimilar literal meaning3.
Thus knowing the fluentEnglish translation of the literal gloss ?iron path?in some relatively resource-rich language provides avehicle for the translation from all other languagessharing that literal gloss44.3 Ranking translation candidatesThe confidence in the correctness of a mapping be-tween a literal gloss (e.g.
?iron path?)
and fluenttranslation (e.g.
?railroad?)
can be based on thenumber of distinct languages exhibiting this associa-tion.
Thus we rank the candidate translations gener-ated via different languages as in Figure 1 as fol-lows: For a given target compound word, say fcwith a set of English glosses G obtained via mul-tiple splitting options or multiple component wordtranslations, the translation probability for a candi-date translation can be computed as:p(ec|fc) =?g?Gp(ec, g|fc)=?g?Gp(g|fc) ?
p(ec|g, fc)=?g?Gp(g|fc) ?
p(ec|g)1In order to avoid inflections as component-words we limitthe component-word length to at least three characters.2The algorithm is allowed to generate multiple glosses ?ironway,?
?iron road,?
etc.
based on multiple translations of thecomponent words.
Multiple glosses only add to the number oftranslation candidates generated.3For the gloss, ?iron path?, we found 10 other languages inwhich some compound word has the English gloss after split-ting and component-word translation4We do assume an existing small translation lexicon in thetarget language for the individual component-words, but theseare often higher frequency words and present either in a basicdictionary or discoverable through corpus-based techniques.where, p(g|fc) = p(g1|f1) ?
p(g2|f2).
f1, f2 arethe individual component-words of compound andg1, g2 are their translations from the existing dic-tionary.
For human dictionaries, p(g|fc) is uni-form for all g ?
G, while variable probabilitiescan also be acquired from bitext or other translationdiscovery approaches.
Also, p(ec|g) =freq(g,ec)freq(g) ,where freq(g, ec) is the number of times the com-pound word with English gloss g is translated asec in the bilingual dictionaries of other languagesand freq(g) is the total number of times the Englishgloss appears in these dictionaries.5 Evaluation using Exact-matchTranslation AccuracyFor evaluation, we assess the performance of thealgorithm on the following 10 languages: Alba-nian, Arabic, Bulgarian, Czech, Farsi, German,Hungarian, Russian, Slovak and Swedish.
We de-tail both the average performance for these 10 lan-guages (Avg10), as well as provide individual per-formance details on Albanian, Bulgarian, Germanand Swedish.
For each of the compound trans-lation models, we report coverage (the # of com-pound words for which a hypothesis was generatedby the algorithm) and Top1/Top10 accuracy.
Top1and Top 10 accuracy are the fraction of words forwhich a correct translation (listed in the evaluationdictionary) appears in the Top 1 and Top 10 trans-lation candidates respectively, as ranked by the al-gorithm.
Because evaluation dictionaries are oftenmissing acceptable translations (e.g.
railroad ratherthan railway), and any deviation from exact-match isscored as incorrect, these measures will be a lowerbound on acceptable translation accuracy.
Also,target language models can often select effectivelyamong such hypothesis lists in context.6 Comparison of different compoundtranslation models6.1 A simple model using literal English glossconcatenation as the translationOur baseline model is a simple gloss concatenationmodel for generating compositional translation can-didates on the lines of Grefenstette (1999) and Caoand Li (2002).
We take the translations of the in-dividual component-words (e.g.
for the compoundword hekurudhe?, they would be hekur (iron) and405udhe?
(path)) and hypothesizes three translation can-didate variants: ?iron path?, ?iron-path?
and ?iron-path?.
A test instance is scored as correct if anyof these translation candidates occur in the transla-tions of hekurudhe?
in the bilingual dictionary.
Thisbaseline performance measures how well simple lit-eral glosses serve as translation candidates.
In casessuch as the German compoundNu?schale (nutshell),which is a simple concatenation of the individualcomponents Nu?
(nut) and Schale (shell), the literalgloss is correct.
For this baseline, if the component-words have multiple translations, then each of thepossible English gloss is ranked randomly.
WhileGrefenstette (1999) and Cao and Li (2002) proposedre-ranking these candidates using web-data, the po-tential gains of this ranking are limited, as we see inTable 2 that even the Found Acc.
is very low5, thatis for most of the cases the correct translation doesnot appear anywhere in the set of English glosses6Language Cmpnd wrds Top1 Top10 Foundtranslated Acc.
Acc.
Acc.Albanian 4472 (10.11%) 0.001 0.010 0.020Bulgarian 9093 (12.50%) 0.001 0.015 0.031German 15731 (29.11%) 0.004 0.079 0.134Swedish 18316 (31.57%) 0.005 0.068 0.111Avg10 14228 (17.84%) 0.002 0.030 0.055Table 2: Baseline performance using unreordered literal En-glish glosses as translations.
The percentages in parenthesesindicate what fraction of all the words in the test (entire) vocab-ulary were detected and translated as compounds.6.2 Using bilingual dictionariesThis section describes the results from the model ex-plained in Section 4.
To recap, this model attemptsto translate every test word such that there is at leastone additional language whose bilingual dictionarysupports an equivalent split and literal English gloss,and bases its translation hypotheses on the consen-sus fluent translation(s) corresponding to the literalglosses in these other languages.
The performanceis shown in Table 3.
The substantial increase in ac-curacy over the baseline indicates the usefulness of5Found Acc.
is the fraction of examples for which the cor-rect translation appears anywhere in the n-best list6One explanation for this could be that for only a small per-centage of compound words, their dictionary translations areformed by concatenating their English glosses.
Also, Grefen-stette (1999) reports much higher accuracies for German on thismodel because the 724 German test compounds were chosen insuch a way that their correct translation is a concatenation of thepossible component word translations.such gloss-to-translation guidance from other lan-guages.
The rest of the sections detail our investi-gation of improvements to this model.Language Compound words Top1 Top10translated Acc.
Acc.Albanian 3085 (6.97%) 0.185 0.332Bulgarian 6719 (9.24%) 0.247 0.416German 11103 (20.55%) 0.195 0.362Swedish 12681 (21.86%) 0.188 0.346Avg10 9320.9 (11.98%) 0.184 0.326Table 3: Coverage and accuracy for the standard model us-ing gloss-to-fluent translation mappings learned from bilingualdictionaries in other languages (in forward order only).6.3 Using forward and backward ordering forEnglish gloss searchIn our standard model, the literal English gloss fora source compound word (for example, iron path)matches glosses in other language dictionaries onlyin the identical order.
But given that modifier/headword order often differs between languages, wetest how searching for both orderings (e.g.
?ironpath?
and ?path iron?)
can improve performance,as shown in Table 4.
The percentages in parenthesesshow relative increase from the performance of thestandard model in Section 6.2.
We see a substantialimprovement in both coverage and accuracy.Language Cmpnd wrds Top1 Top10translated Acc.
Acc.Albanian 3229(+4.67%) .217(+17.30%) .409(+23.19%)Bulgarian 6806(+1.29%) .255(+3.24%) .442(+6.25%)German 11346(+2.19%) .199(+2.05%) .388(+7.18%)Swedish 12970(+2.28%) .189(+0.53%) .361(+4.34%)Avg10 9603(+3.03%) .193(+4.89%) .362(+11.04%)Table 4: Performance for looking up English gloss via bothorderings.
The percentages in parentheses are relative improve-ments from the performance in Table 3.6.4 Increasing coverage by automaticallydiscovering compound morphologyFor many languages, the compounding process in-troduces its own morphology (Figure 2).
For ex-ample, in German, the word Gescha?ftsfu?hrer (man-ager) consists of the lexemes Gescha?ft (business)and Fu?hrer (guide) joined by the lexeme -s. For thepurposes of these experiments, we will call such lex-emes fillers or middle glue characters.
Koehn andKnight (2003) used a fixed set of two known fillers sand es for handling German compounds.
To broadenthe applicability of this work to new languages with-out linguistic guidance, we show how such fillers406Gesch?ft s F?hrerPaterfamiliasPater Familia s+ + + +s as Middle Gluein Germans as End Gluein LatinGesch?ftsf?hrer(Business) (Guide) (Father) (Family)(Manager) (Household head)Figure 2: Illustration of compounding morphology usingmiddle and end glue characters.can be estimated directly from corpora in differentlanguages.
In additional to fillers, compound canalso introduce morphology at the suffix or prefix ofcompounds, for example, in the Latin language, thelexeme paterfamilias contains the genitive form fa-milias of the lexeme familia (family), thus s in thiscase is referred to as the ?end glue?
character.
ToAlbanian Bulgarian German SwedishTop 5 Middle Glue Character(s)j 0.059 O 0.129 s 0.133 s 0.132s 0.048 N 0.046 n 0.090 l 0.051t 0.042 H 0.036 k 0.066 n 0.049r 0.042 E 0.025 h 0.042 t 0.045i 0.038 A 0.025 f 0.037 r 0.035Top 5 End Glue Character(s)m 0.146 T 0.124 n 0.188 a 0.074t 0.079 EH 0.092 t 0.167 g 0.073s 0.059 H 0.063 en 0.130 t 0.059k 0.048 M 0.049 e 0.069 e 0.057r 0.037 AM 0.047 d 0.043 d 0.057Table 5: Top 5 middle glues (fillers) and end glues discoveredfor each language along with their probability scores.augment the splitting step outlined in Section 4.1,we allow deletion of up to two middle charactersand two end characters.
Then, for each glue candi-date (for example es), we estimate its probability asthe relative frequency of unique hypothesized com-pound words successfully using that particular glue.We rank the set of glues by their probability and takethe top 10 middle and end glues for each language.A sample of glues discovered for some of the lan-guages are shown in Table 5.
The performance forthe morphology step is shown in Table 6.
The rela-tive percentage improvements are with respect to theprevious Section 6.3.
We observe significant gain incoverage as the flexibility of glue process allows dis-covery of more compounds.6.5 Re-ranking using context vector projectionWe may further improve performance by re-rankingcandidate translations based on the goodness of se-mantic ?fit?
between two words, as measured byLanguage Cmpnd wrds Top1 Top10translated Acc.
Acc.Albanian 3272(+1.33%) .214(-1.38%) .407(-0.49%)Bulgarian 7211(+5.95%) .258(+1.18%) .443(+0.23%)German 13372(+17.86%) .200(+0.50%) .391(+0.77%)Swedish 15094(+16.38%) .190(+0.53%) .363(+0.55%)Avg10 10273(+6.98%) .194(+0.52%) .363(+0.28%)Table 6: Performance for increasing coverage by includingcompounding morphology.
The percentages in parentheses arerelative improvements from the performance in Table 4.their context similarity.
This can be accomplished asin Rapp (1999) and Schafer and Yarowsky (2002) bycreating bag-of-words context vectors around boththe source and target language words and then pro-jecting the source vectors into the (English) targetspace via the current small translation dictionary.Once in the same language space, source words andtheir translation hypotheses are compared via co-sine similarity using their surrounding context vec-tors.
We performed this experiment for Germanand Swedish and report average accuracies with andwithout this addition in Table 7.
For monolingualcorpora, we used the German and Swedish side ofthe Europarl corpus (Koehn, 2005) consisting of ap-proximately 15 million and 21 million words respec-tively.
We were able to project context vectors foran average of 4224.5 words in the two languagesamong all the possible compound words detected inSection 6.4.
The poor Eurpoarl coverage could bedue to the fact that compound words are generallytechnical words with low Europarl corpus frequency,especially in parliamentary proceedings.
We believethat the small performance gains here are due tothese limitations of the monolingual corpora.Method Top1avg Top10avgOriginal ranking 0.196 0.388Comb.
with Context Sim 0.201 0.391Table 7: Average performance on German and Swedish withand without using context vector similarity from monolingualcorpora.6.6 Using phrase-tables if a parallel corpus isavailableAll previous results in this paper have been for trans-lation lexicon discovery without the need for paral-lel bilingual text (bitext), which is often in limitedsupply for lower-resource languages.
However, itis useful to assess how this translation lexicon dis-407covery work compares with traditional bitext-basedlexicon induction (and how well the approaches canbe combined).
For this purpose, we used phrase ta-bles learned by the standard statistical MT ToolkitMoses (Koehn et al, 2007).
We tested the phrase-table accuracy on two languages, one for which wehad a lot of parallel data available (German-EnglishEuroparl corpus with approx.
15 million words) andone for which we had relatively little parallel data(Czech-English news-commentary corpus with ap-prox.
1 million words).
This was done to see howthe amount of parallel data available affects the ac-curacy and coverage of compound translation.
Table8 shows the performance for this experiment.
ForGerman, we see a significant improvement in accu-racy and for Czech a small improvement in Top1 buta decline in Top10 accuracy.
Note that these ac-curacies are still quite low as compared to generalperformance of phrase tables in an end-to-end MTsystem because we are measuring exact-match ac-curacy on a generally more challenging and often-lower-frequency lexicon subset.
The third row inTable 8 for each of the languages shows that if onehad a parallel corpus available, its n-best list can becombined with the n-best list of Bilingual Dictio-naries algorithm to provide much higher consensusaccuracy gains using weighted voting.Method # of words Top1 Top10translated Acc.
Acc.GermanBiDict 13372 0.200 0.391Parallel Corpus SMT 3281 0.423 0.576Parallel + BiDict 3281 0.452 0.579CzechBiDictthresh=1 3455 0.276 0.514Parallel Corpus SMT 309 0.285 0.404Parallel + BiDict 309 0.359 0.599Table 8: Performance of this paper?s BiDict approach com-pared with and augmented with traditional statistical MT learn-ing from bitext.7 Quantifying the Role of Cross-languages7.1 Coverage/Accuracy Trade offThe number of languages offering a translation hy-pothesis for a given literal English gloss is a use-ful parameter for measuring confidence in the algo-rithm?s selection.
The more distinct languages ex-hibiting a translation for the gloss, the higher like-lihood that the majority translation will be correctCoverage/Accuracy Tradeoff0.30.40.50.60.70.80.90 200 400 600 800 1000 1200 1400 1600# of words translated as compoundsExactmatchaccuracyAvg Top 1 Acc.Avg Top 10 Acc.>= 8>= 5>= 4>= 3>= 6>= 14>= x: threshold for# of languagesFigure 3: Coverage/Accuracy trade off curve by incrementingthe minimum number of languages exhibiting a candidate trans-lation for the source-word?s literal English gloss.
Accuracy hereis the Top1 accuracy averaged over all 10 test languages.rather than noise.
Varying this parameter yields thecoverage/accuracy trade off as shown in Figure 3.7.2 Varying size of bilingual dictionariesFigure 4 illustrates how the size of the bilingualdictionaries used for providing cross-language evi-dence affects translation performance.
In order totake both coverage and accuracy into account, per-formance measure used was the F-score which isa harmonic average of Precision (the accuracy onthe subset of words that could be translated) andPsuedo-recall (which is the correctly translated frac-tion out of total words that could be translated using100% of the dictionary size).
We can see in Figure 4that increasing the percentage of dictionary size7 al-ways helps without plateauing, suggesting substan-tial extrapolation potential from large dictionaries.7.3 Greedy vs Random Selection of UtilizedLanguagesA natural question for our compound translation al-gorithm is how does the choice of additional lan-guages affect performance.
We report two experi-ments on this question.
A simple experiment is touse bilingual dictionaries of randomly selected lan-guages and test the performance of K-randomly se-lected languages8, incrementing K until it is the fullset of 50 languages.
The dashed lines in Figures 57Each run of choosing a percentage of dictionary size wasaveraged over 10 runs8Each run of randomly selecting K languages was averagedover 10 runs.40800.050.10.150.20.250.30.350 10 20 30 40 50 60 70 80 90 100% of dictionary usedF- score Top 1Top 10Figure 4: F-measure performance given varying sizes of thebilingual dictionaries used for cross-language evidence (as apercentage of words randomly utilized from each dictionary).00.020.040.060.080.10.120.140.160.180.20 10 20 30 40 50# of languages utilized (K)F-score (Top 1) K-RandomK-GreedyFigure 5: Top-1 match F-score performance utilizing K lan-guages for cross-language evidence, for both a random K lan-guages and greedy selection of the most effective K languages(typically the closest or largest dictionaries)00.050.10.150.20.250.30.350 10 20 30 40 50# of languages utilized (K)F-score (Top 10) K-RandomK-GreedyFigure 6: The performance relationship detailed in Figure 5caption for Top-10 match F-score.and 6 show this trend.
The performance is measuredby F-score as in section 7.1, where Pseudo-Recallhere is the fraction of correct candidates out of thetotal candidates that could be translated had we usedbilingual dictionaries of all the languages.
We cansee that adding random bilingual dictionaries helpsimprove the performance in a close to linear fashion.Furthermore, we observe that certain contributinglanguages are much more effective than others (e.g.Arabic/Farsi vs. Arabic/Czech).
We use a greedyheuristic for ranking an additional cross-language,that is the number of test words for which the correctEnglish translation can be provided by the bilingualdictionary of the respective cross-language.
Figures5 and 6 show that greedy selection of the most ef-fective K utilized languages using this heuristic sub-stantially accelerates performance.
In fact, beyondthe best 10 languages, performance plateaus and ac-tually decreases slightly, indicating that increasednoise is outweighing increased coverage.Albanian ArabicRussian 0.067 0.116 Farsi 0.051 0.090+Spanish 0.100 0.169 +Spanish 0.059 0.111+Bulgarian 0.119 0.201 +French 0.077 0.138Bulgarian CzechRussian 0.186 0.294 Slovak 0.177 0.289+Hungarian 0.190 0.319 +Russian 0.222 0.368+Swedish 0.203 0.339 +Hungarian 0.235 0.407Farsi GermanArabic 0.031 0.047 Dutch 0.130 0.228+Dutch 0.038 0.070 +Swedish 0.191 0.316+Spanish 0.044 0.079 +Hungarian 0.204 0.355Hungarian RussianSwedish 0.073 0.108 Bulgarian 0.185 0.250+Dutch 0.103 0.158 +Hungarian 0.199 0.292+German 0.117 0.182 +Swedish 0.216 0.319Slovak SwedishCzech 0.145 0.218 German 0.120 0.188+Russian 0.168 0.280 +Hungarian 0.152 0.264+Hungarian 0.176 0.300 +Dutch 0.182 0.309Table 9: Illustrating 3-best cross-languages obtained for eachtest language (shown in bold).
Each row shows the effect ofadding the respective cross-language to the set of languages inthe rows above it and the corresponding F-scores (Top 1 andTop 10) achieved.7.4 Languages found using Greedy selectionTable 9 shows the sets of the most effective threecross-languages per test language selected using thegreedy heuristic explained in previous section.
Un-surprisingly, related languages tend to help morethan distant languages.
For example, Dutch is most409effective for the test language German, and Slovak ismost effective for Czech.
We can also see interest-ing symmetries between related languages, for ex-ample: Farsi is the top language used for test lan-guage Arabic and vice-versa.
Such symmetries canalso be seen for other pairs of related languages suchas (Czech, Slovak) and (Russian, Bulgarian).
Thus,related languages are most helpful and they can berelated in several ways such as etymologically, cul-turally and physically (such as Hungarian contactwith the Germanic languages).
The second pointto note is that languages having large dictionariesalso tend to be especially helpful, even when un-related.
This can be seen by the presence of Hun-garian in top three cross-languages for most of thetest languages.
This is likely because Hungarian wasone of the largest dictionaries and hence can providegood coverage for obtaining translation candidatesof rarer or technical compounds, which may havemore language universal literal glosses.8 ConclusionThis paper has shown that successful translationof compounds can be achieved without the needfor bilingual training text, by modeling the map-ping of literal component-word glosses (e.g.
?iron-path?)
into fluent English (e.g.
?railway?)
acrossmultiple languages.
An interesting property of us-ing such cross-language evidence is that one doesneed to restrict the candidate translations to compo-sitional (or ?glossy?)
translations, as our model al-lows the successful generation of more fluent non-compositional translations.
We further show im-proved performance by adding component-sequenceand learned-morphology models along with contextsimilarity from monolingual text and optional com-bination with traditional bilingual-text-based trans-lation discovery.
These models show consistent per-formance gains across 10 diverse test languages.9 AcknowledgmentsWe thank Chris Callison-Burch for providing accessto phrase tables and giving valuable comments onthis work as well as suggesting useful additional ex-periments.
We also thankMarkus Dreyer for helpingwith German examples and David Smith for givingvaluable comments on initial version of the paper.ReferencesT.
Baldwin and T. Tanaka.
2004.
Translation by Machineof Complex Nominals: Getting it Right.
Proceedingsof the ACL-2004 Workshop on Multiword Expressions,pages 24?31.R.D.
Brown.
2002.
Corpus-driven splitting of compoundwords.
Proceedings of TMI-2002.Y.
Cao and H. Li.
2002.
Base Noun Phrase translationusing web data and the EM algorithm.
Proceedings ofCOLING-Volume 1, pages 1?7.G.
Grefenstette.
1999.
The World Wide Web as a Re-source for Example-Based Machine Translation Tasks.In ASLIB?99 Translating and the Computer 21.P.
Koehn and K. Knight.
2003.
Empirical methods forcompound splitting.
Proceedings of the EACL-Volume1, pages 187?193.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
Proceedingsof ACL, companion volume, pages 177?180.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
MT Summit X.J.N.
Levi.
1978.
The Syntax and Semantics of ComplexNominals.R.
Navigli, P. Velardi, and A. Gangemi.
2003.
Ontologylearning and its application to automated terminologytranslation.
Intelligent Systems, IEEE, 18(1):22?31.U.
Rackow, I. Dagan, and U. Schwall.
1992.
Auto-matic translation of noun compounds.
Proceedings ofCOLING-Volume 4, pages 1249?1253.R.
Rapp.
1999.
Automatic identification of word trans-lations from unrelated English and German corpora.Proceedings of ACL, pages 519?526.C.
Schafer and D. Yarowsky.
2002.
Inducing translationlexicons via diverse similarity measures and bridgelanguages.
Proceedings of COLING, pages 1?7.C.
Schafer and D. Yarowsky.
2004.
Exploiting aggregateproperties of bilingual dictionaries for distinguishingsenses of English words and inducing English senseclusters.
Proceedings of ACL-2004, pages 118?121.T.
Tanaka and T. Baldwin.
2003.
Noun-Noun CompoundMachine Translation: A Feasibility Study on ShallowProcessing.
Proceedings of the ACL-2003 Workshopon Multiword Expressions, pages 17?24.J.
Zhang, J. Gao, and M. Zhou.
2000.
Extraction ofChinese compound words: an experimental study ona very large corpus.
Proceedings of the Second Work-shop on Chinese Language Processing, pages 132?139.410
