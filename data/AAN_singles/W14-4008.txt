Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67?77,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSyntax and Semantics in Quality Estimation of Machine TranslationRasoul Kaljahi?
?, Jennifer Foster?, Johann Roturier?
?NCLT, School of Computing, Dublin City University, Ireland{rkaljahi, jfoster}@computing.dcu.ie?Symantec Research Labs, Dublin, Ireland{johann roturier}@symantec.comAbstractWe employ syntactic and semantic infor-mation in estimating the quality of ma-chine translation from a new data setwhich contains source text from Englishcustomer support forums and target textconsisting of its machine translation intoFrench.
These translations have been bothpost-edited and evaluated by professionaltranslators.
We find that quality estima-tion using syntactic and semantic informa-tion on this data set can hardly improveover a baseline which uses only surfacefeatures.
However, the performance canbe improved when they are combined withsuch surface features.
We also introducea novel metric to measure translation ade-quacy based on predicate-argument struc-ture match using word alignments.
Whileword alignments can be reliably used,the two main factors affecting the per-formance of all semantic-based methodsseems to be the low quality of seman-tic role labelling (especially on ill-formedtext) and the lack of nominal predicate an-notation.1 IntroductionThe problem of evaluating machine translationoutput without reference translations is calledquality estimation (QE) and has recently been thecentre of attention (Bojar et al., 2014) followingthe seminal work of Blatz et al.
(2003).
MostQE studies have focused on surface and language-model-based features of the source and target.
Thequality of translation is however closely related tothe syntax and semantics of the languages, the for-mer concerning fluency and the latter adequacy.While there have been some attempts to utilizesyntax in this task, semantics has been paid lessattention.
In this work, we aim to exploit bothsyntax and semantics in QE, with a particular fo-cus on the latter.
We use shallow semantic analy-sis obtained via semantic role labelling (SRL) andemploy this information in QE in various ways in-cluding statistical learning using both tree kernelsand hand-crafted features.
We also design a QEmetric which is based on the Predicate-Argumentstructure Match (PAM ) between the source and itstranslation.
The semantic-based system is thencombined with the syntax-based system to evalu-ate the full power of structural linguistic informa-tion.
We also combine this system with a baselinesystem consisting of effective surface features.A second contribution of the paper is the releaseof a new data set for QE.1This data set comprisesa set of 4.5K sentences chosen from customer sup-port forum text.
The machine translation of thesentences are not only evaluated in terms of ade-quacy and fluency, but also manually post-editedallowing various metrics of interest to be appliedto measure different aspects of quality.
All exper-iments are carried out on this data set.The rest of the paper is organized as follows:after reviewing the related work, the data is de-scribed and the semantic role labelling approachis explained.
The baseline is then introduced, fol-lowed by the experiments with tree kernels, hand-crafted features, the PAM metric and finally thecombination of all methods.
The paper ends witha summary and suggestions for future work.2 Related WorkSyntax has been exploited in QE in various waysincluding tree kernels (Hardmeier et al., 2012;Kaljahi et al., 2013; Kaljahi et al., 2014b),parse probabilities and syntactic label frequency(Avramidis, 2012), parseability (Quirk, 2004) andPOS n-gram scores (Specia and Gim?enez, 2010).1The data will be made publicly available - see http://www.computing.dcu.ie/mt/confidentmt.html67Turning to the role of semantic knowledge inQE and MT evaluation in general, Pighin andM`arquez (2011) propose a method for ranking twotranslation hypotheses that exploits the projectionof SRL from a sentence to its translation usingword alignments.
They first project the SRL of asource corpus to its parallel corpus and then buildtwo translation models: 1) translations of proposi-tion labelling sequences in the source to its projec-tion in the target and 2) translations of argumentrole fillers in the source to their counterparts inthe target.
The source SRL is then projected toits machine translation and the above models areforced to translate source proposition labelling se-quences to the projected ones.
Finally the confi-dence scores of these translations and their reach-ability are used to train a classifier which selectsthe better of the two translation hypotheses withan accuracy of 64%.
Factors hindering their clas-sifier are word alignment limitations and low SRLrecall due to the lack of a verb or the loss of apredicate during translation.In MT evaluation, where reference translationsare available, Gim?enez and M`arquez (2007) usesemantic roles in building several MT evaluationmetrics which measure the full or partial lexicalmatch between the fillers of same semantic roles inthe hypothesis and translation, or simply the rolelabel matches between them.
They conclude thatthese features can only be useful in combinationwith other features and metrics reflecting differentaspects of the quality.Lo and Wu (2011) introduce HMEANT, a man-ual MT evaluation metric based on predicate-argument structure matching which involves twosteps of human engagement: 1) semantic role an-notation of the reference and machine translation,2) evaluating the translation of predicates and ar-guments.
The metric calculates the F1score ofthe semantic frame match between the referenceand machine translation based on this evaluation.To keep the costs reasonable, the first step is car-ried out by amateur annotators who were mini-mally trained with a simplified list of 10 thematicroles.
On a set of 40 examples, the metric ismeta-evaluated in terms of correlation with humanjudgements of translation adequacy ranking, and acorrelation as high as that of HTER is reported.Lo et al.
(2012) propose MEANT, a variant ofHMEANT, which automatizes its manual stepsusing 1) automatic SRL systems for (only) verbpredicates, 2) automatic alignment of predicatesand their arguments in the reference and ma-chine translation based on their lexical similarity.Once the predicates and arguments are aligned,their similarities are measured using a variety ofmethods such as cosine distance and even Me-teor and BLEU.
In computation of the final score,the similarity scores replace the counts of correctand partial translations used in HMEANT.
Thismetric outperforms several automatic metrics in-cluding BLEU, Meteor and TER, but it signifi-cantly under-performs HMEANT and HTER.
Fur-ther analysis shows that automatizing the secondstep does not affect the performance of MEANT.Therefore, it seems to be the lower accuracy of thesemantic role labelling that is responsible.Bojar and Wu (2012) identify a set of flawswith HMEANT and propose solutions for them.The most important problems stem from the su-perficial SRL annotation guidelines.
These prob-lems are exacerbated in MEANT due to the auto-matic nature of the two steps.
More recently, Loet al.
(2014) extend MEANT to ranking transla-tions without a reference by using phrase transla-tion probabilities for aligning semantic role fillersof the source and its translation.3 DataWe randomly select 4500 segments from a largecollection of Symantec English Norton forumtext.2In order to be independent of any one MTsystem, we translate these segments into Frenchwith the following three systems and randomlychoose 1500 distinct segments from each.?
ACCEPT3: a phrase-based Moses systemtrained on training sets of WMT12 releasesof Europarl and News Commentary plusSymantec translation memories?
SYSTRAN: a proprietary rule-based systemaugmented with domain-specific dictionaries?
Bing4: an online translation systemThese translations are evaluated in two ways.The first method involves light post-editing bya professional human translator who is a native2http://community.norton.com3http://www.accept.unige.ch/Products/D_4_1_Baseline_MT_systems.pdf4http://www.bing.com/translator(on24-Feb-2014)68Adequacy Fluency5 All meaning Flawless Language4 Most of meaning Good Language3 Much of meaning Non-native Language2 Little meaning Disfluent Language1 None of meaning IncomprehensibleTable 2: Adequacy/fluency score interpretationFrench speaker.5Each sentence translation is thenscored against its post-edit using BLEU6(Papineniet al., 2002), TER (Snover et al., 2006) andMETEOR (Denkowski and Lavie, 2011), which arethe most widely used MT evaluation metrics.
Fol-lowing Snover et al.
(2006), we consider this wayof scoring MT output to be a variation of human-targeted scoring, where no reference translationis provided to the post-editor, so we call themHBLEU, HTER and HMETEOR.
The average scoresfor the entire data set together with their standarddeviations are presented in Table 1.7In the second method, we asked three profes-sional translators, who are again native Frenchspeakers, to assess the quality of MT output interms of adequacy and fluency in a 5-grade scale(LDC, 2002).
The interpretation of the scores isgiven in Table 2.
Each evaluator was given theentire data set for evaluation.
We therefore col-lected three sets of scores and averaged them toobtain the final scores.
The averages of thesescores for the entire data set together with theirstandard deviations are presented in Table 1.
Tobe easily comparable to human-targeted scores,we scale these scores to the [0,1] range, i.e.
ad-equacy/fluency scores of 1 and 5 are mapped to 0and 1 respectively and all the scores in betweenare accordingly scaled.The average Kappa inter-annotator agreementfor adequacy scores is 0.25 and for fluency scores0.19.
However, this measurement does not dif-ferentiate between small and large differences inagreement.
In other words, the difference between5The post-editing guidelines are based on theTAUS/CNGL guidelines for achieving ?good enough?quality downloaded from https://evaluation.taus.net/images/stories/guidelines/taus-cngl-machine-translation-postediting-guidelines.pdf.6Version 13a of MTEval script was used at the segmentlevel which performs smoothing.7Note that HTER scores have no upper limit and can behigher than 1 when the number of errors is higher than thesegment length.
In addition, the higher HTER indicates lowertranslation quality.
To be comparable to the other scores, wecut-off them at 1 and convert to 1-HTER.1-HTER HBLEU HMeteor Adq Flu1-HTER - - - - -HBLEU 0.9111 - - - -HMeteor 0.9207 0.9314 - - -Adq 0.6632 0.7049 0.6843 - -Flu 0.6447 0.7213 0.6652 0.8824 -Table 3: Pearson r between pairs of metrics on theentire 4.5K data setscores of 5 and 4 is the same as the differencebetween 5 and 2.
To account for this, we useweighted Kappa instead.
Specifically, we considertwo scores of difference 1 to represent 75% agree-ment instead of 100%.
All the other differencesare considered to be a disagreement.
The aver-age weighted Kappa computed in this way is 0.65for adequacy and 0.63 for fluency.
Though theweighting used is quite strict, the weighted Kappavalues are in the substantial agreement range.Once we have both human-targeted and manualevaluation scores together, it is interesting to knowhow they are correlated.
We calculate the Pearsoncorrelation coefficient r between each pair of thefive scores and present them in Table 3.
HBLEUhas the highest correlation with both adequacy andfluency scores among the human-targeted metrics.HTER on the other hand has the lowest correla-tion.
Moreover, HBLEU is more correlated withfluency than with adequacy which is the oppositeto HMeteor.
This is expected according to thedefinition of BLEU and Meteor.
There is alsoa high correlation between adequacy and fluencyscores.
Although this could be related to the factthat both scores are from the same evaluators, itindicates that if either the fluency and adequacy ofthe MT output is low or high, the other tends to bethe same.The data is split into train, development and testsets of 3000, 500 and 1000 sentences respectively.4 Semantic Role LabellingThe type of semantic information we use in thiswork is the predicate-argument structure or se-mantic role labelling of the sentence.
This infor-mation needs to be extracted from both sides of thetranslation, i.e.
English and French.
Though theSRL of English has been well-studied (M`arquezet al., 2008) thanks to the existence of two majorhand-crafted resources, namely FrameNet (Bakeret al., 1998) and PropBank (Palmer et al., 2005),French is one of the under-studied languages in691-HTER HBLEU HMeteor Adequacy FluencyAverage 0.6976 0.5517 0.7221 0.6230 0.4096Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780Table 1: Average and standard deviation of the evaluation scores for the entire data setthis respect mainly due to a lack of such resources.The only available gold standard resource is asmall set of 1000 sentences taken from Europarl(Koehn, 2005) and manually annotated with Prop-bank verb predicates (van der Plas et al., 2010).van der Plas et al.
(2011) attempt to tackle thisscarcity by automatically projecting SRL from theEnglish side of a large parallel corpus to its Frenchside.
Our preliminary experiments (Kaljahi et al.,2014a), however, show that SRL models trainedon the small manually annotated corpus have ahigher quality than ones trained on the much largerprojected corpus.
We therefore use the 1K goldstandard set to train a French SRL model.
For En-glish, we use all the data provided in the CoNLL2009 shared task (Haji?c et al., 2009).We use LTH (Bj?orkelund et al., 2009), adependency-based SRL system, for both the En-glish and French data.
This system was amongthe best performing systems in the CoNLL 2009shared task and is straightforward to use.
It comeswith a set of features tuned for each shared tasklanguage (English, German, Japanese, Spanish,Catalan, Czech, Chinese).
We compared the per-formance of the English and Spanish feature setson French and chose the former due to its higherperformance (by 1 F1point).It should be noted that the English SRL datacome with gold standard syntactic annotation.
Onthe other hand, for our QE data set, such anno-tation is not available.
Our preliminary experi-ments show that, since the SRL system heavilyrelies on syntactic features, the performance con-siderably drops when the syntactic annotation ofthe test data is obtained using a different parserthan that of the training data.
We therefore re-place the parses of the training data with those ob-tained automatically by first parsing the data us-ing the Lorg PCFG-LA parser8(Attia et al., 2010)and then converting them to dependencies usingStanford converter (de Marneffe and Manning,2008).
The POS tags are also replaced with thoseoutput by the parser.
For the same reason, we re-8https://github.com/CNGLdlab/LORG-Release.place the original POS tagging of the French 1Kdata with those obtained by the MElt tagger (De-nis and Sagot, 2012).The English SRL achieves 77.77 and 67.02 la-belled F1points when trained only on the trainingsection of PropBank and tested on the WSJ andBrown test sets respectively.9The French SRL isevaluated using 5-fold cross-validation on the 1Kdata set and obtains an F1average of 67.66.
Whenapplied to the QE data set, these models identify9133, 8875 and 8795 propositions on its sourceside, post-edits and MT output respectively.5 BaselineWe compare the results of our experiments to abaseline built using the 17 baseline features of theWMT QE shared task (Bojar et al., 2014).
Thesefeatures provide a strong baseline and have beenused in all three years of the shared task.
Weuse support vector regression implemented in theSVMLight toolkit10with Radial Basis Function(RBF) kernel to build this baseline.
To extractthese features, a parallel English-French corpusis required to build a lexical translation table us-ing GIZA++ (Och and Ney, 2003).
We use theEuroparl English-French parallel corpus (Koehn,2005) plus around 1M segments of Symantectranslation memory.Table 4 shows the performance of this system(WMT17) on the test set measured by Root MeanSquare Error (RMSE) and Pearson correlation co-efficient (r).
We only report the results on predict-ing four of the metrics introduced above, omittingHMeteor due to space constraints.
C and ?
pa-rameters are tuned on the development set with re-spect to r. The results show a significant differ-ence between manual and human-targeted metricprediction.
The higher r for the former suggeststhat the patterns of these scores are easier to learn.The RMSE seems to follow the standard deviation9Although the English SRL data are annotated for nounpredicates as well as verb predicates, since the French datahas only verb predicate annotations, we only consider verbpredicates for English.10http://svmlight.joachims.org/70of the scores as the same ranking is seen in both.6 Tree KernelsTree kernels (Moschitti, 2006) have been success-fully used in QE by Hardmeier et al.
(2012) andin our previous work (Kaljahi et al., 2013; Kal-jahi et al., 2014b), where syntactic trees are em-ployed.
Tree kernels eliminate the burden of man-ual feature engineering by efficiently utilizing allsubtrees of a tree.
We employ both syntactic andsemantic information in learning quality scores,using the SVMLight-TK11, a support vector ma-chine (SVM) implementation of tree kernels.We implement a syntactic tree kernel QE sys-tem with constituency and dependency trees ofthe source and target side, following our previouswork (Kaljahi et al., 2013; Kaljahi et al., 2014b).The performance of this system (TKSyQE) isshown in Table 4.
Unlike our previous results,where the syntax-based system significantly out-performed the WMT17 baseline, TKSyQE can onlybeat the baseline in HTER and fluency prediction,with neither difference being statistically signifi-cant and it is below the baseline for HBLEU andadequacy prediction.12It should be noted that inour previous work, a WMT News data set wasused as the QE data set which, unlike our new dataset, is well-formed and in the same domain as theparsers?
training data.
The discrepancy betweenour new and old results suggests that the perfor-mance is strongly dependent on the data set.Unlike syntactic parsing, semantic role la-belling does not produce a tree to be directly usedin the tree kernel framework.
There can be var-ious ways to accomplish this goal.
We first trya method inspired by the PAS format introducedby Moschitti et al.
(2006).
In this format, a fixednumber of nodes are gathered under a dummy rootnode as slots of one predicate and 6 arguments ofa proposition (one tree per predicate).
Each nodedominates an argument label or a dummy label forthe predicate, which in turn dominates the POStag of the argument or the predicate lemma.
If aproposition has more than 6 arguments they areignored, if it has fewer than 6 arguments, the extraslots are attached to a dummy null label.
Note thatthese trees are derived from the dependency-basedSRL of both the source and target side (Figure11http://disi.unitn.it/moschitti/Tree-Kernel.htm12We use paired bootstrap resampling Koehn (2004) forstatistical significance testing.1-HTER HBLEU Adq FluRMSEWMT17 0.2310 0.2696 0.2219 0.2469TKSyQE 0.2267 0.2721 0.2258 0.2431D-PAS 0.2489 0.2856 0.2423 0.2652D-PST 0.2409 0.2815 0.2383 0.2606C-PST 0.2400 0.2809 0.2410 0.2615CD-PST 0.2394 0.2795 0.2373 0.2578TKSSQE 0.2269 0.2722 0.2253 0.2425Pearson rWMT17 0.3661 0.3806 0.4710 0.4769TKSyQE 0.3693 0.3559 0.4306 0.5013D-PAS 0.1774 0.1843 0.2770 0.3252D-PST 0.2136 0.2450 0.3169 0.3670C-PST 0.2319 0.2541 0.2966 0.3616CD-PST 0.2311 0.2714 0.3303 0.3923TKSSQE 0.3682 0.3537 0.4351 0.5046Table 4: RMSE and Pearson r of the 17 base-line features (WMT17) and tree kernel systems;TKSyQE: syntax-based tree kernels, D-PAS:dependency-based PAS tree kernels of Moschittiet al.
(2006), D-PST, C-PST and CD-PST:dependency-based, constituency-based proposi-tion subtree kernels and their combination,TKSSQE: syntactic-semantic tree kernels1(a)).
The results are shown in Table 4 (D-PAS).The performance is statistically significantly lowerthan the baseline.13In order to encode more information in the trees,we propose another format in which propositionsubtrees (PST) of the sentence are gathered un-der a dummy root node.
A dependency PST (Fig-ure 1(b)) is formed by the predicate label underthe root dominating its lemma and all its argu-ments roles.
Each of these nodes in turn dominatesthree nodes: the argument word form (the predi-cate word form for the case of a predicate lemma),its syntactic dependency relation to its head and itsPOS tag.
We preserve the order of arguments andpredicate in the sentence.14This system is namedD-PST in Table 4.
Tree kernels in this format sig-nificantly outperform D-PAS.
However, the per-formance is still far lower than the baseline.The above formats are based on dependencytrees.
We try another PST format derived fromconstituency trees.
These PSTs (Figure 1(c)) arethe lowest common subtrees spanning the predi-cate node and its argument nodes and are gath-ered under a dummy root node.
The argument role13Note that the only lexical information in this format isthe predicate lemma.
We tried replacing the POS tags withargument word forms, which led to a slight degradation.14This format is chosen among several other variations dueto its higher performance.71(a) D-PAS (b) D-PST (c) C-PST (d) D-TKSSQE (e) C-TKSSQEFigure 1: Semantic tree kernel formats for the sentence: Can anyone help?labels are concatenated with the syntactic non-terminal category of the argument node.
Predi-cates are not marked.
However, our dependency-based SRL is required to be converted into aconstituency-based format.
While constituency-to-dependency conversion is straightforward us-ing head-finding rules (Surdeanu et al., 2008),the other way around is not.
We therefore ap-proximate the conversion using a heuristic we call(D2C).15As shown in Table 4, the system built us-ing these PSTs C-PST improves over D-PST forhuman-targeted metric prediction, but not man-ual metric prediction.
However, when they arecombined in CD-PST, we can see improvementover the highest scores of both systems, exceptfor HTER prediction for Pearson r. The fluencyprediction improvement is statistically significant.The other changes are not statistically significant.An alternative approach to formulating seman-tic tree kernels is to augment syntactic trees withsemantic information.
We augment the trees inTKSyQE with semantic role labels.
We attach se-mantic roles to dependency labels of the argumentnodes in the dependency trees as in Figure 1(d).For constituency trees, we use the D2C heuristicto elevate roles up the terminal nodes and attachthe labels to the syntactic non-terminal categoryof the node as in Figure 1(e).
The performanceof the resulting system, TKSSQE, is shown in Ta-ble 4.
It substantially outperforms its counterpart,CD-PST, all differences being statistically signif-icant.
However, compared to the plain syntactictree kernels (TKSyQE), the changes are slight andinconsistent, rendering the augmentation not use-ful.
We consider this system to be our syntactic-15This heuristic (D2C) recursively elevates the argumentrole already assigned to a terminal node (based on thedependency-based argument position) to the parent node aslong as 1) the argument node is not a root node or is nottagged as a POS (possessive), 2) the role is not an AM-NEG,AM-MOD or AM-DIS adjunct, and 3) the argument does notdominate its predicate?s node or another argument node of thesame proposition.1-HTER HBLEU Adq FluRMSEWMT17 0.2310 0.2696 0.2219 0.2469HCSyQE 0.2435 0.2797 0.2334 0.2479HCSeQE 0.2482 0.2868 0.2416 0.2612Pearson rWMT17 0.3661 0.3806 0.4710 0.4769HCSyQE 0.2572 0.3080 0.3961 0.4696HCSeQE 0.1794 0.1636 0.2972 0.3577Table 5: RMSE and Pearson r of the 17 baselinefeatures (WMT17) and hand-crafted featuressemantic tree kernel system.7 Hand-crafted FeaturesIn our previous work (Kaljahi et al., 2014b), weexperiment with a set of hand-crafted syntacticfeatures extracted from both constituency and de-pendency trees on a different data set.
We applythe same feature set on the new data set here.
Theresults are reported in Table 5.
The performance ofthis system (HCSyQE) is significantly lower thanthe baseline.
This is opposite to what we ob-serve with the same feature set on a different dataset, again showing that the role of data is funda-mental in understanding system performance.
Themain difference between these two data sets is thatthe former is extracted from a well-formed text inthe news domain, the same domain on which ourparsers and SRL system have been trained, whilethe new data set does not necessarily contain well-formed text nor is it from the same domain.We design another set of feature types aimingat capturing the semantics of the source and trans-lation via predicate-argument structure.
The fea-ture types are listed in Table 6.
Feature types1 to 8 each contain two features, one extractedfrom the source and the other from the transla-tion.
To compute argument span sizes (featuretypes 4 and 5), we use the constituency conver-sion of SRL obtained using the D2C heuristic in-troduced in Section 6.
The proposition label se-721 Number of propositions2 Number of arguments3 Average number of arguments per proposition4 Sum of span sizes of arguments5 Ratio of sum of span sizes of arguments to sentencelength6 Proposition label sequences7 Constituency label sequences of proposition elements8 Dependency label sequences of proposition elements9 Percentage of predicate/argument word alignmentmapping typesTable 6: Semantic feature typesquence (feature type 6) is the concatenation of ar-gument roles and predicate labels of the propo-sition with their preserved order (e.g.
A0-go.01-A4).
Similarly, constituency and dependency la-bel sequences (feature types 4 and 5) are extractedby replacing argument and predicate labels withtheir constituency and dependency labels respec-tively.
Feature type 9 consists of three featuresbased on word alignment of source and targetsentences: number of non-aligned, one-to-many-aligned and many-to-one-aligned predicates andarguments.
The word alignments are obtained us-ing the grow-diag-final-and heuristic asthey performed slightly better than other types.16As in the baseline system, we use SVMs to buildthe QE systems using these hand-crafted features.The nominal features are binarized to be usable bySVM.
However, the set of possible feature valuescan be large, leading to a large number of binaryfeatures.
For example, there are more than 5000unique proposition label sequences in our data.Not only does this high dimensionality reduce theefficiency of the system, it can also affect its per-formance as these features are sparse.
To tacklethis issue, we impose a frequency cutoff on thesefeatures: we keep only frequent features using athreshold set empirically on the development set.Table 5 shows the performance of the system(HCSeQE) built with these features.
The semanticfeatures perform substantially lower than the syn-tactic features and thus the baseline, especially inpredicting human-targeted scores.
Since these fea-tures are chosen from a comprehensive set of se-mantic features, and as they should ideally captureadequacy better than general features, a probablereason for their low performance is the quality of16It should be noted that a number of features in additionto those presented here have been tried, e.g.
the ratio and dif-ference of the source and target values of numerical features.However, through manual feature selection, we have removedfeatures which do not appear to contribute much.the underlying syntactic and semantic analysis.8 Predicate-Argument Match (PAM)Translation adequacy measures how much of thesource meaning is preserved in the translated text.Predicate-argument structure or semantic role la-belling expresses a substantial part of the meaning.Therefore, the matching between the predicate-argument structure of the source and its transla-tion could be an important clue to the translationadequacy, independent of the language pair used.We attempt to exploit predicate-argument match(PAM) to create a metric that measures the trans-lation adequacy.The algorithm to compute PAM score startsby aligning the predicates and arguments of thesource side to its target side using word align-ments.17It then treats the problem as one of SRLscoring, similar to the scoring scheme used in theCoNLL 2009 shared task (Haji?c et al., 2009).
As-suming the source side SRL as a reference, it com-putes unlabelled precision and recall of the targetside SRL with respect to it:UPrec =# aligned preds and their args# target side preds and argsURec =# aligned preds and their args# source side preds and argsLabelled precision and recall are calculated inthe same way except that they also require argu-ment label agreement.
UF1and LF1are the har-monic means of unlabelled and labelled scores re-spectively.
Inspired by the observation that mostsource sentences with no identified proposition areshort and can be assumed to be easier to translate,and based on experiments on the dev set, we assigna score of 1 to such sentences.
When no proposi-tion is identified in the target side while there is aproposition in the source, we assign a score of 0.5.We obtain word alignments using the Mosestoolkit (Hoang et al., 2009), which can gener-ate alignments in both directions and combinethem using a number of heuristics.
We try in-tersection, union, source-to-target only, as wellas the grow-diag-final-and heuristic, butonly the source-to-target results are reported hereas they slightly outperform the others.Table 7 shows the RMSE and Pearson r foreach of the unlabelled and labelled F1against ade-17We also tried lexical and phrase translation tables for thispurpose in addition to word alignments but they do not out-perform word alignments.731-HTER HBLEU Adq FluRMSE1 UF10.3175 0.3607 0.3108 0.4033LF10.4247 0.3903 0.3839 0.3586Pearson rUF10.2328 0.2179 0.2698 0.2865LF10.1784 0.1835 0.2225 0.2688Table 7: RMSE and Pearson r of PAM unlabelledand labelled F1scores as estimation of the MTevaluation metrics1-HTER HBLEU Adq FluRMSEPAM 0.2414 0.2833 0.2414 0.2661HCSeQE 0.2482 0.2868 0.2416 0.2612HCSeQEpam0.2445 0.2822 0.2370 0.2575Pearson rPAM 0.2292 0.2195 0.2787 0.3210HCSeQE 0.1794 0.1636 0.2972 0.3577HCSeQEpam0.2387 0.2368 0.3571 0.3908Table 8: RMSE and Pearson r of PAM scores asfeatures, alone and combined (PAM)quacy and also fluency scores on the test data set.18According to the results, the unlabelled F1(UF1)is a closer estimation than the labelled one.
ItsPearson correlation scores are overall competitiveto the hand-crafted semantic features (HCSeQE inTable 5): they are better for the automatic metriccases but lower for manual ones.
However, theRMSE scores are considerably larger.
Overall, theperformance is not comparable to the baseline andother well performing systems.
We investigate thereasons behind this result in the next section.Another way to employ the PAM scores in QEis to use them in a statistical framework.
We builda SVM model using all 6 PAM scores The per-formance of this system (PAM) on the test set isshown in Table 8.
The performance is consider-ably higher than when the PAM scores are useddirectly as estimations.
Interestingly, compared tothe 47 semantic hand-crafted features (HCSeQE),this small feature set performs better in predictinghuman-targeted metrics.We add these features to our set of hand-crafted features in Section 7 to yield a new sys-tem (HCSeQEpamin Table 8).
All scores improvecompared to the stronger of the two components.However, only the manual metric prediction im-provements are statistically significant.
The per-formance is still not close to the baseline.18Precision and recall scores were also tried.
Precisionproved to be the weakest estimator, whereas recall scoreswere highest for some settings.8.1 Analyzing PAMIdeally, PAM scores should capture the adequacyof translation with a high accuracy.
The resultsare however far from ideal.
There are two fac-tors involved in the PAM scoring procedure, thequality of which can affect its performance: 1)predicate-argument structure of the source andtarget side of the translation, 2) alignment ofpredicate-argument structures of source and target.The SRL systems for both English and Frenchare trained on edited newswire.
On the otherhand, our data is neither from the same domain noredited.
The problem is exacerbated on the trans-lation target side, where our French SRL systemis trained on only a small data set and applied tomachine translation output.
To discover the con-tribution of each of these factors in the accuracyof PAM, we carry out a manual analysis.
We ran-domly select 10% of the development set (50 sen-tences) and count the number of problems of eachof these two categories.We find only 8 cases in which a wrong wordalignment misleads PAM scoring.
On the otherhand, there are 219 cases of SRL problems, in-cluding predicate and argument identification andlabelling: 82 cases (37%) in the source and 138cases (63%) in the target.We additionally look for the cases where atranslation divergence causes predicate-argumentmismatch in the source and translation.
For ex-ample, without sacrificing is translated into sansimpact sur (without impact on), a case of transpo-sition, where the source side verb predicate is leftunaligned thus affecting the PAM score.
We findonly 9 such cases in the sample, which is similarto the proportion of word alignment problems.As mentioned in the previous section, PAMscoring has to assign default values for cases inwhich there is no predicate in the source or tar-get.
This can be another source of estimation error.In order to verify its effect, we find such cases inthe development set and manually categorize thembased on the reason causing the sentence to be leftwithout predicates.
There are 79 (16%) source and96 (19%) target sentences for which the SRL sys-tems do not identify any predicate, out of which64 cases have both sides without any predicate.Among such source sentences, 20 (25%) have nopredicate due to a predicate identification error ofthe SRL system, 57 (72%) because of the sentencestructure (e.g.
copula verbs which are not labelled741-HTER HBLEU Adq FluRMSEWMT17 0.2310 0.2696 0.2219 0.2469SyQE 0.2255 0.2711 0.2248 0.2419SeQE 0.2249 0.2710 0.2242 0.2404SSQE 0.2246 0.2696 0.2230 0.2402SSQE+WMT17 0.2225 0.2673 0.2202 0.2379Pearson rWMT17 0.3661 0.3806 0.4710 0.4769SyQE 0.3824 0.3650 0.4393 0.5087SeQE 0.3884 0.3648 0.4447 0.5182SSQE 0.3920 0.3768 0.4538 0.5196SSQE+WMT17 0.4144 0.3953 0.4771 0.5331Table 9: RMSE and Pearson r of the 17 baselinefeatures (WMT17) and system combinationsas predicates in the SRL training data, titles, etc.
),and the remaining 2 due to spelling errors mislead-ing the SRL system.
Among the target side sen-tences, most of the cases are due to the sentencestructure (65 or 68%) and only 14 (15%) cases arecaused by an SRL error.
In 13 cases, no verb pred-icate in the source is translated correctly.
Amongthe remaining cases, two are due to untranslatedspelling errors in the source and the other two dueto tokenization errors misleading the SRL system.These numbers show that the main reason lead-ing to the sentences without verbal predicates isthe sentence structure.
This problem can be al-leviated by employing nominal predicates in bothsides.
While this is possible for the English side,there is currently no French resource where nomi-nal predicates have been annotated.9 Combining SystemsWe now combine the systems we have built sofar (Table 9).
We first combine syntax-basedand semantic-based systems individually.
SyQEis the combination of the syntactic tree kernelsystem (TKSyQE) and the hand-crafted features(HCSyQE).
Likewise, SeQE is the combinationof the semantic tree kernel system (TKSSQE) andthe semantic hand-crafted features including PAMfeatures (HCSeQEpam).
These two systems arecombined in SSQE but without syntactic tree ker-nels (TKSyQE) to avoid redundancy with TKSSQEas these are the augmented syntactic tree kernels.We finally combine SSQE with the baseline.SyQE significantly improves over its tree ker-nel and hand-crafted components.
It also outper-forms the baseline in HTER and fluency predic-tion, but is beaten by it in HBLEU and adequacyprediction.
None of these differences are statis-tically significant however.
SeQE also performsbetter than the stronger of its components.
Exceptfor adequacy prediction, the other improvementsare statistically significant.
This system performsslightly better than SyQE.
Its comparison to thebaseline is the same as that of SyQE, except thatits superiority to the baseline in fluency predictionis statistically significant.The full syntactic-semantic system (SSQE) alsoimproves over its syntactic and semantic compo-nents.
However, the improvements are not statisti-cally significant.
Compared to the baseline, HTERand fluency prediction perform better, the latterbeing statistically significant.
HBLEU predictionis around the same as the baseline, but adequacyprediction performance is lower, though not statis-tically significantly.Finally, when we combine the syntactic-semantic system with the baseline system, thecombination continues to improve further.
Com-pared to the stronger component however, only theHTER and fluency prediction improvements arestatistically significant.10 ConclusionWe introduced a new QE data set drawn from cus-tomer support forum text, machine translated andboth post-edited and manually evaluated for ad-equacy and fluency.
We used syntactic and se-mantic QE systems via both tree kernels and hand-crafted features.
We found it hard to improve overa baseline, albeit strong, using such informationwhich is extracted by applying parsers and seman-tic role labellers on out-of-domain and uneditedtext.
We also defined a metric for estimating thetranslation adequacy based on predicate-argumentstructure match between source and target.
Thismetric relies on automatic word alignments andsemantic role labelling.
We find that word align-ment and translation divergence only have minoreffects on the performance of this metric, whereasthe quality of semantic role labelling is the mainhindering factor.
Another major issue affecting theperformance of PAM is the unavailability of nom-inal predicate annotation.Our PAM scoring method is based on only wordmatches as there are no constituent SRL resourcesavailable for French ?
perhaps constituent-basedarguments can make a more accurate comparisonbetween the source and target predicate-argumentstructure possible.75AcknowledgmentsThis research has been supported by the IrishResearch Council Enterprise Partnership Scheme(EPSPG/2011/102) and the computing infrastruc-ture of the CNGL at DCU.
We thank the reviewersfor their helpful comments.ReferencesMohammed Attia, Jennifer Foster, Deirdre Hogan,Joseph Le Roux, Lamia Tounsi, and Josef van Gen-abith.
2010.
Handling unknown words in statisticallatent-variable parsing models for Arabic, Englishand French.
In Proceedings of the 1st Workshopon Statistical Parsing of Morphologically Rich Lan-guages.Eleftherios Avramidis.
2012.
Quality estimation forMachine Translation output using linguistic analysisand decoding features.
In Proceedings of the 7thWMT.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley Framenet project.
In Proceed-ings of the 36th ACL.Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning: Shared Task.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2003.
Confidenceestimation for Machine Translation.
In JHU/CLSPSummer Workshop Final Report.Ond?rej Bojar and Dekai Wu.
2012.
Towards apredicate-argument evaluation for MT.
In Proceed-ings of the Sixth Workshop on Syntax, Semantics andStructure in Statistical Translation.Ondrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, Lucia Specia, and Ale?sTamchyna.
2014.
Findings of the 2014 workshopon Statistical Machine Translation.
In Proceedingsof the 9th WMT.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependenciesrepresentation.
In Proceedings of the COLINGWorkshop on Cross-Framework and Cross-DomainParser Evaluation.Pascal Denis and Beno?
?t Sagot.
2012.
Coupling anannotated corpus and a lexicon for state-of-the-artpos tagging.
Lang.
Resour.
Eval., 46(4):721?736.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the 6th WMT.Jes?us Gim?enez and Llu?
?s M`arquez.
2007.
Linguisticfeatures for automatic evaluation of heterogenous mtsystems.
In Proceedings of the Second Workshop onStatistical Machine Translation.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?18.Christian Hardmeier, Joakim Nivre, and J?org Tiede-mann.
2012.
Tree kernels for machine translationquality estimation.
In Proceedings of the SeventhWMT.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A unified framework for phrase-based, hierarchical,and syntax-based statistical machine translation.
InProceedings of the International Workshop on Spo-ken Language Translation (IWSLT).Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, Jo-hann Roturier, and Fred Hollowood.
2013.
Parseraccuracy in quality estimation of machine transla-tion: a tree kernel approach.
In International JointConference on Natural Language Processing (IJC-NLP).Rasoul Kaljahi, Jennifer Foster, and Johann Roturier.2014a.
Semantic role labelling with minimal re-sources: Experiments with french.
In Third JointConference on Lexical and Computational Seman-tics (*SEM).Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, andJohann Roturier.
2014b.
Quality estimation ofenglish-french machine translation: A detailed studyof the role of syntax.
In International Conference onComputational Linguistics (COLING).Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Conference Pro-ceedings: the tenth Machine Translation Summit.LDC.
2002.
Linguistic data annotation specification:Assessment of fluency and adequacy in chinese-english translations.
Technical report.Chi-kiu Lo and Dekai Wu.
2011.
Meant: An inex-pensive, high-accuracy, semi-automatic metric forevaluating translation utility via semantic frames.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1.Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.2012.
Fully automatic semantic mt evaluation.
InProceedings of the Seventh WMT.76Chi-kiu Lo, Meriem Beloucif, Markus Saers, andDekai Wu.
2014.
Xmeant: Better semantic mt eval-uation without reference translations.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), June.Llu?
?s M`arquez, Xavier Carreras, Kenneth C.Litkowski, and Suzanne Stevenson.
2008.
Se-mantic role labeling: An introduction to the specialissue.
Comput.
Linguist., 34(2):145?159, June.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2006.
Tree kernel engineering for propo-sition re-ranking.
In Proceedings of Mining andLearning with Graphs (MLG).Alessandro Moschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In Proceedingsof EACL.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of theACL, pages 311?318.Daniele Pighin and Llu?
?s M`arquez.
2011.
Automaticprojection of semantic structures: An application topairwise translation ranking.
In Proceedings of theFifth Workshop on Syntax, Semantics and Structurein Statistical Translation, pages 1?9.Chris Quirk.
2004.
Training a sentence-level machinetranslation confidence measure.
In Proceedings ofLREC.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA.Lucia Specia and Jes?us Gim?enez.
2010.
Combiningconfidence estimation and reference-based metricsfor segment level MT evaluation.
In Proceedings ofAMTA.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
Theconll-2008 shared task on joint parsing of syntacticand semantic dependencies.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning.Lonneke van der Plas, Tanja Samard?zi?c, and PaolaMerlo.
2010.
Cross-lingual validity of propbankin the manual annotation of french.
In Proceedingsof the Fourth Linguistic Annotation Workshop, LAWIV ?10.Lonneke van der Plas, Paola Merlo, and James Hen-derson.
2011.
Scaling up automatic cross-lingualsemantic role annotation.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies.77
