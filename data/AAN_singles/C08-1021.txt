Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 161?168Manchester, August 2008KnowNet: Building a Large Net of Knowledge from the WebMontse CuadrosTALP Research Center, UPCBarcelona, Spaincuadros@lsi.upc.eduGerman RigauIXA NLP Group, UPV/EHUDonostia, Spaingerman.rigau@ehu.esAbstractThis paper presents a new fully auto-matic method for building highly denseand accurate knowledge bases from ex-isting semantic resources.
Basically, themethod uses a wide-coverage and accu-rate knowledge-based Word Sense Dis-ambiguation algorithm to assign the mostappropriate senses to large sets of topi-cally related words acquired from the web.KnowNet, the resulting knowledge-basewhich connects large sets of semantically-related concepts is a major step towardsthe autonomous acquisition of knowledgefrom raw corpora.
In fact, KnowNet is sev-eral times larger than any available knowl-edge resource encoding relations betweensynsets, and the knowledge KnowNet con-tains outperform any other resource whenis empirically evaluated in a commonframework.1 IntroductionUsing large-scale knowledge bases, such as Word-Net (Fellbaum, 1998), has become a usual, of-ten necessary, practice for most current NaturalLanguage Processing (NLP) systems.
Even now,building large and rich enough knowledge basesfor broad?coverage semantic processing takes agreat deal of expensive manual effort involvinglarge research groups during long periods of de-velopment.
In fact, hundreds of person-years havebeen invested in the development of wordnets forvarious languages (Vossen, 1998).
For example, inc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.more than ten years of manual construction (from1995 to 2006, that is from version 1.5 to 3.0),WordNet grew from 103,445 to 235,402 semanticrelations1.
But this data does not seem to be richenough to support advanced concept-based NLPapplications directly.
It seems that applicationswill not scale up to work in open domains withoutmore detailed and rich general-purpose (and alsodomain-specific) semantic knowledge built by au-tomatic means.
Obviously, this fact has severelyhampered the state-of-the-art of advanced NLP ap-plications.However, the Princeton WordNet (WN) is by farthe most widely-used knowledge base (Fellbaum,1998).
In fact, WordNet is being used world-widefor anchoring different types of semantic knowl-edge including wordnets for languages other thanEnglish (Atserias et al, 2004), domain knowledge(Magnini and Cavagli`a, 2000) or ontologies likeSUMO (Niles and Pease, 2001) or the EuroWord-Net Top Concept Ontology (?Alvez et al, 2008).It contains manually coded information about En-glish nouns, verbs, adjectives and adverbs and isorganized around the notion of a synset.
A synsetis a set of words with the same part-of-speech thatcan be interchanged in a certain context.
For ex-ample, <party, political party> form a synset be-cause they can be used to refer to the same concept.A synset is often further described by a gloss, inthis case: ?an organization to gain political power?and by explicit semantic relations to other synsets.Fortunately, during the last years the researchcommunity has devised a large set of innovativemethods and tools for large-scale automatic acqui-sition of lexical knowledge from structured and un-structured corpora.
Among others we can men-1Symmetric relations are counted only once.161tion eXtended WordNet (Mihalcea and Moldovan,2001), large collections of semantic preferencesacquired from SemCor (Agirre and Martinez,2001; Agirre and Martinez, 2002) or acquired fromBritish National Corpus (BNC) (McCarthy, 2001),large-scale Topic Signatures for each synset ac-quired from the web (Agirre and de Lacalle, 2004)or knowledge about individuals from Wikipedia(Suchanek et al, 2007).
Obviously, all these se-mantic resources have been acquired using a verydifferent methods, tools and corpora.
As expected,each semantic resource has different volume andaccuracy figures when evaluated in a common andcontrolled framework (Cuadros and Rigau, 2006).However, not all these large-scale resources en-code semantic relations between synsets.
In somecases, only relations between synsets and wordshave been acquired.
This is the case of the TopicSignatures acquired from the web (Agirre and deLacalle, 2004).
This is one of the largest seman-tic resources ever built with around one hundredmillion relations between synsets and semanticallyrelated words2.A knowledge net or KnowNet (KN), is an exten-sible, large and accurate knowledge base, whichhas been derived by semantically disambiguatingsmall portions of the Topic Signatures acquiredfrom the web.
Basically, the method uses a ro-bust and accurate knowledge-based Word SenseDisambiguation algorithm to assign the most ap-propriate senses to the topic words associated toa particular synset.
The resulting knowledge-basewhich connects large sets of topically-related con-cepts is a major step towards the autonomous ac-quisition of knowledge from raw text.Table 1 compares the different volumes of se-mantic relations between synset pairs of avail-able knowledge bases and the newly createdKnowNets3.Varying from five to twenty the number of pro-cessed words from each Topic Signature, we cre-ated automatically four different KnowNet ver-sions with millions of new semantic relations be-tween synsets.
In fact, KnowNet is several timeslarger than WordNet, and when evaluated empir-ically in a common framework, the knowledge itcontains outperforms any other semantic resource.After this introduction, section 2 describes theTopic Signatures acquired from the web.
Section2Available at http://ixa.si.ehu.es/Ixa/resources/sensecorpus3These KnowNet versions are available athttp://adimen.si.ehu.esSource #relationsPrinceton WN3.0 235,402Selectional Preferences from SemCor 203,546eXtended WN 550,922Co-occurring relations from SemCor 932,008New KnowNet-5 231,163New KnowNet-10 689,610New KnowNet-15 1,378,286New KnowNet-20 2,358,927Table 1: Number of synset relations3 presents the approach we followed for buildinghighly dense and accurate knowledge bases fromthe Topic Signatures.
In section 4, we present theevaluation framework used in this study.
Section 5describes the results when evaluating different ver-sions of KnowNet and finally, section 6 presentssome concluding remarks and future work.2 Topic SignaturesTopic Signatures (TS) are word vectors related to aparticular topic (Lin and Hovy, 2000).
Topic Sig-natures are built by retrieving context words of atarget topic from a large corpora.
This study con-siders word senses as topics.
Basically, the acqui-sition of TS consists of:?
acquiring the best possible corpus examplesfor a particular word sense (usually character-izing each word sense as a query and perform-ing a search on the corpus for those examplesthat best match the queries)?
building the TS by selecting the contextwords that best represent the word sense fromthe selected corpora.The Topic Signatures acquired from the web(hereinafter TSWEB) constitutes one of the largestsemantic resource available with around 100 mil-lion relations (between synsets and words) (Agirreand de Lacalle, 2004).
Inspired by the work of(Leacock et al, 1998), TSWEB was constructedusing monosemous relatives from WN (synonyms,hypernyms, direct and indirect hyponyms, and sib-lings), querying Google and retrieving up to onethousand snippets per query (that is, a word sense),extracting the salient words with distinctive fre-quency using TFIDF.
Thus, TSWEB consist oflarge ordered lists of words with weights associ-ated to the polysemous nouns of WN1.6.
Thenumber of constructed topic signatures is 35,250with an average size per signature of 6,877 words.162tammany#n 0.0319federalist#n 0.0315whig#n 0.0300missionary#j 0.0229Democratic#n 0.0218nazi#j 0.0202republican#n 0.0189constitutional#n 0.0186conservative#j 0.0148socialist#n 0.0140Table 2: TS of party#n#1 (first 10 out of 12,890total words)When evaluating TSWEB, we used at maximumthe first 700 words while for building KnowNet weused at maximum the first 20 words.For example, table 2 presents the first words(lemmas and part-of-speech) and weights of theTopic Signature acquired for party#n#14.3 Building highly connected and denseknowledge basesWe acquired by fully automatic means highlyconnected and dense knowledge bases by disam-biguating small portions of the Topic Signaturesobtained from the web, increasing the total num-ber of semantic relations from less than one mil-lion (the current number of available relations) tomillions of new and accurate semantic relationsbetween synsets.
We applied a knowledge?basedall?words Word Sense Disambiguation algorithmto the Topic Signatures for deriving a sense vectorfrom each word vector.3.1 SSI-DijkstraWe have implemented a version of the Struc-tural Semantic Interconnections algorithm (SSI), aknowledge-based iterative approach to Word SenseDisambiguation (Navigli and Velardi, 2005).
TheSSI algorithm is very simple and consists of an ini-tialization step and a set of iterative steps (see al-gorithm 1).Given W, an ordered list of words to be dis-ambiguated, the SSI algorithm performs as fol-lows.
During the initialization step, all monose-mous words are included into the set I of alreadyinterpreted words, and the polysemous words areincluded in P (all of them pending to be disam-biguated).
At each step, the set I is used to disam-biguate one word of P, selecting the word sensewhich is closer to the set I of already disam-4This format stands for word#pos#sense.biguated words.
Once a sense is selected, the wordsense is removed from P and included into I. Thealgorithm finishes when no more pending wordsremain in P.Algorithm 1 SSI-Dijkstra AlgorithmSSI (T: list of terms)for each {t ?
T} doI[t] = ?if t is monosemous thenI[t] := the only sense of telseP := P ?
{t}end ifend forrepeatP?
:= Pfor each {t ?
P} doBestSense := ?MaxV alue := 0for each {sense s of t} doW [s] := 0N [s] := 0for each {sense s??
I} dow := DijsktraShortestPath(s, s?
)if w > 0 thenW [s] := W [s] + (1/w)N [s] := N [s] + 1end ifend forif N [s] > 0 thenNewV alue := W [s]/N [s]if NewV alue > MaxV alue thenMaxV alue := NewV alueBestSense := send ifend ifend forif MaxV alue > 0 thenI[t] := BestSenseP := P \ {t}end ifend foruntil P 6= P?return (I, P);Initially, the list I of interpreted words should in-clude the senses of the monosemous words in W,or a fixed set of word senses5.
However, when dis-5If no monosemous words are found or if no initial sensesare provided, the algorithm could make an initial guess basedon the most probable sense of the less ambiguous word of W.163ambiguating a TS of a word sense s (for instanceparty#n#1), the list I already includes s.In order to measure the proximity of one synsetto the rest of synsets of I, we use part of theknowledge already available to build a very largeconnected graph with 99,635 nodes (synsets) and636,077 edges.
This graph includes the set ofdirect relations between synsets gathered fromWordNet and eXtended WordNet.
On that graph,we used a very efficient graph library, Boost-Graph6to compute the Dijkstra algorithm.
TheDijkstra algorithm is a greedy algorithm for com-puting the shortest path distance between one nodean the rest of nodes of a graph.
In that way, we cancompute very efficiently the shortest distance be-tween any two given nodes of a graph.
We call thisversion of the SSI algorithm, SSI-Dijkstra.SSI-Dijkstra has very interesting properties.
Forinstance, it always provides the minimum distancebetween two synsets.
That is, the algorithm alwaysprovides an answer being the minimum distanceclose or far.
In contrast, the original SSI algorithmnot always provides a path distance because it de-pends on a predefined grammar of semantic rela-tions.
In fact, the SSI-Dijkstra algorithm comparesthe distances between the synsets of a word and allthe synsets already interpreted in I.
At each step,the SSI-Dijkstra algorithm selects the synset whichis closer to I (the set of already interpreted words).Furthermore, this approach is completely lan-guage independent.
The same graph can be usedfor any language having words connected to Word-Net.3.2 Building KnowNetWe developed KnowNet (KN), a large-scale andextensible knowledge base, by applying SSI-Dijkstra to each topic signature from TSWEB.We have generated four different versions ofKnowNet applying SSI-Dijkstra to only the first5, 10, 15 and 20 words for each TS.
SSI-Dijkstraused only the knowledge present in WordNet andeXtended WordNet which consist of a very largeconnected graph with 99,635 nodes (synsets) and636,077 edges (semantic relations).We generated each KnowNet by applying theSSI-Dijkstra algorithm to the whole TSWEB (pro-cessing the first words of each of the 35,250topic signatures).
For each TS, we obtained thedirect relations from the topic (a word sense)6http://www.boost.orgKB WN+XWN #relations #synsetsKN-5 3,1% 231,163 39,864KN-10 5,0% 689,610 45,817KN-15 6,9% 1,378,286 48,521KN-20 8,5% 2,358,927 50,789Table 3: Size and percentage of overlapping rela-tions between KnowNet versions and WN+XWNto the disambiguated word senses of the TS(for instance, party#n#1?>federalist#n#1), butalso the indirect relations between disambiguatedwords from the TS (for instance, federalist#n#1?>republican#n#1).
Finally, we removed symmet-ric and repeated relations.Table 3 shows the overlaping percentage be-tween each KnowNet and the knowledge con-tained into WordNet and eXtended WordNet, andthe total number of relations and synsets of eachresource.
For instance, only 8,5% of the total di-rect relations included into WN+XWN are alsopresent in KnowNet-20.
This means that the restof relations from KnowNet-20 are new.
As ex-pected, each KnowNet is very large, ranging fromhundreds of thousands to millions of new semanticrelations between synsets among increasing sets ofsynsets.4 Evaluation frameworkIn order to empirically establish the relative qual-ity of these new semantic resources, we used theevaluation framework of task 16 of SemEval-2007:Evaluation of wide coverage knowledge resources(Cuadros and Rigau, 2007).In this framework all knowledge resources areevaluated on a common WSD task.
In particu-lar, we used the noun-sets of the English Lexi-cal Sample task of Senseval-3 and SemEval-2007exercises which consists of 20 and 35 nouns re-spectively.
All performances are evaluated on thetest data using the fine-grained scoring system pro-vided by the organizers.Furthermore, trying to be as neutral as possiblewith respect to the resources studied, we appliedsystematically the same disambiguation method toall of them.
Recall that our main goal is to es-tablish a fair comparison of the knowledge re-sources rather than providing the best disambigua-tion technique for a particular knowledge base.
Allknowledge bases are evaluated as topic signatures.That is, word vectors with weights associated to aparticular synset which are obtained by collecting164those word senses appearing in the synsets directlyrelated to the topics.
This simple representationtries to be as neutral as possible with respect to theresources used.A common WSD method has been applied to allknowledge resources.
A simple word overlappingcounting is performed between the topic signaturerepresenting a word sense and the test example7.The synset having higher overlapping word countsis selected.
In fact, this is a very simple WSDmethod which only considers the topical informa-tion around the word to be disambiguated.
Finally,we should remark that the results are not skewed(for instance, for resolving ties) by the most fre-quent sense in WN or any other statistically pre-dicted knowledge.4.1 BaselinesWe have designed a number of baselines in orderto establish a complete evaluation framework forcomparing the performance of each semantic re-source on the English WSD tasks.RANDOM: For each target word, this methodselects a random sense.
This baseline can be con-sidered as a lower-bound.SEMCOR-MFS: This baseline selects the mostfrequent sense of the target word in SemCor.WN-MFS: This baseline is obtained by se-lecting the most frequent sense (the first sensein WN1.6) of the target word.
WordNet word-senses were ranked using SemCor and other sense-annotated corpora.
Thus, WN-MFS and SemCor-MFS are similar, but not equal.TRAIN-MFS: This baseline selects the mostfrequent sense in the training corpus of the targetword.TRAIN: This baseline uses the training corpusto directly build a Topic Signature using TFIDFmeasure for each word sense and selecting at max-imum the first 450 words.
Note that in WSD eval-uation frameworks, this is a very basic baseline.However, in our evaluation framework, this ?WSDbaseline?
could be considered as an upper-bound.We do not expect to obtain better topic signaturesfor a particular sense than from its own annotatedcorpus.4.2 Other Large-scale Knowledge ResourcesIn order to measure the relative quality of the newresources, we include in the evaluation a wide7We also consider those multiword terms appearing inWN.range of large-scale knowledge resources con-nected to WordNet.WN (Fellbaum, 1998): This resource uses thedifferent direct relations encoded in WN1.6 andWN2.0.
We also tested WN2using relations at dis-tance 1 and 2, WN3using relations at distances 1to 3 and WN4using relations at distances 1 to 4.XWN (Mihalcea and Moldovan, 2001): Thisresource uses the direct relations encoded in eX-tended WN.spBNC (McCarthy, 2001): This resource con-tains 707,618 selectional preferences acquired forsubjects and objects from BNC.spSemCor (Agirre and Martinez, 2002): Thisresource contains the selectional preferences ac-quired for subjects and objects from SemCor.MCR (Atserias et al, 2004): This resource in-tegrates the direct relations of WN, XWN andspSemCor.TSSEM (Cuadros et al, 2007): These TopicSignatures have been constructed using Sem-Cor.For each word-sense appearing in SemCor, wegather all sentences for that word sense, building aTS using TFIDF for all word-senses co-occurringin those sentences.4.3 Integrated Knowledge ResourcesWe also evaluated the performance of the integra-tion (removing duplicated relations) of some ofthese resources.WN+XWN: This resource integrates the di-rect relations of WN and XWN.
We also tested(WN+XWN)2(using either WN or XWN rela-tions at distances 1 and 2).MCR (Atserias et al, 2004): This resource in-tegrates the direct relations of WN, XWN andspSemCor.WN+XWN+KN-20: This resource integratesthe direct relations of WN, XWN and KnowNet-20.5 KnowNet EvaluationWe evaluated KnowNet using the same frameworkexplained in section 4.
That is, the noun part of thetest set from the English Senseval-3 and SemEval-2007 English lexical sample tasks.5.1 Senseval-3 evaluationTable 4 presents ordered by F1 measure, the per-formance in terms of precision (P), recall (R) and165KB P R F1 Av.
SizeTRAIN 65.1 65.1 65.1 450TRAIN-MFS 54.5 54.5 54.5WN-MFS 53.0 53.0 53.0TSSEM 52.5 52.4 52.4 103SEMCOR-MFS 49.0 49.1 49.0MCR245.1 45.1 45.1 26,429WN+XWN+KN-20 44.8 44.8 44.8 671MCR 45.3 43.7 44.5 129KnowNet-20 44.1 44.1 44.1 610KnowNet-15 43.9 43.9 43.9 339spSemCor 43.1 38.7 40.8 56KnowNet-10 40.1 40.0 40.0 154(WN+XWN)238.5 38.0 38.3 5,730WN+XWN 40.0 34.2 36.8 74TSWEB 36.1 35.9 36.0 1,721XWN 38.8 32.5 35.4 69KnowNet-5 35.0 35.0 35.0 44WN335.0 34.7 34.8 503WN433.2 33.1 33.2 2,346WN233.1 27.5 30.0 105spBNC 36.3 25.4 29.9 128WN 44.9 18.4 26.1 14RANDOM 19.1 19.1 19.1Table 4: P, R and F1 fine-grained results for theresources evaluated at Senseval-3, English LexicalSample Task.F1 measure (F1, harmonic mean of recall and pre-cision) of each knowledge resource on Senseval-3and the average size of the TS per word-sense.
Thedifferent KnowNet versions appear marked in boldand the baselines appear in italics.As expected, RANDOM obtains the poorest re-sult.
The most frequent senses obtained from Sem-Cor (SEMCOR-MFS) and WN (WN-MFS) areboth below the most frequent sense of the trainingcorpus (TRAIN-MFS).
However, all of them arefar below to the Topic Signatures acquired usingthe training corpus (TRAIN).The best results are obtained by TSSEM (withF1 of 52.4).
The lowest result is obtained by theknowledge directly gathered from WN mainly be-cause of its poor coverage (R of 18.4 and F1 of26.1).
Interestingly, the knowledge integrated inthe MCR although partly derived by automaticmeans performs much better in terms of precision,recall and F1 measures than using them separately(F1 with 18.4 points higher than WN, 9.1 thanXWN and 3.7 than spSemCor).Despite its small size, the resources derivedfrom SemCor obtain better results than its coun-terparts using much larger corpora (TSSEM vs.TSWEB and spSemCor vs. spBNC).Regarding the baselines, all knowledge re-sources surpass RANDOM, but none achieves nei-ther WN-MFS, TRAIN-MFS nor TRAIN.
OnlyTSSEM obtains better results than SEMCOR-MFSand is very close to the most frequent sense of WN(WN-MFS) and the training (TRAIN-MFS).Regarding the expansions and combinations, theperformance of WN is improved using words atdistances up to 2, and up to 3, but it decreases usingdistances up to 4.
Interestingly, none of these WNexpansions achieve the results of XWN.
Finally,(WN+XWN)2performs better than WN+XWNand MCR2slightly better than MCR8.The different versions of KnowNet consistentlyobtain better performances as they increase thewindow size of processed words of TSWEB.
Asexpected, KnowNet-5 obtain the lower results.However, it performs better than WN (and allits extensions) and spBNC.
Interestingly, fromKnowNet-10, all KnowNet versions surpass theknowledge resources used for their construction(WN, XWN, TSWEB and WN+XWN).
In fact,KnowNet-10 also outperforms (WN+XWN)2withmuch more relations per sense.
Also interestingis that KnowNet-10 and KnowNet-20 obtain bet-ter performance than spSemCor which was derivedfrom annotated corpora.
However, KnowNet-20only performs slightly better than KnowNet-15while almost doubling the number of relations.These initial results seem to be very promis-ing.
If we do not consider the resources derivedfrom manually sense annotated data (spSemCor,MCR, TSSEM, etc.
), KnowNet-10 performs bet-ter that any knowledge resource derived by man-ual or automatic means.
In fact, KnowNet-15 andKnowNet-20 outperforms spSemCor which wasderived from manually annotated corpora.
This isa very interesting result since these KnowNet ver-sions have been derived only with the knowledgecoming from WN and the web (that is, TSWEB),and WN and XWN as a knowledge source for SSI-Dijkstra9.Regarding the integration of resources,WN+XWN+KN-20 performs better than MCRand similarly to MCR2(having less than 50 timesits size).
Also interesting is that WN+XWN+KN-20 have better performance than their individualresources, indicating a complementary knowledge.In fact, WN+XWN+KN-20 performs much betterthan the resources from which it derives (WN,XWN and TSWEB).8No further distances have been tested9eXtended WordNet only has 17,185 manually labeledsenses.166KB P R F1 Av.
SizeTRAIN 87.6 87.6 87.6 450TRAIN-MFS 81.2 79.6 80.4WN-MFS 66.2 59.9 62.9WN+XWN+KN-20 53.0 53.0 53.0 627(WN+XWN)254.9 51.1 52.9 5,153TSWEB 54.8 47.8 51.0 700KnowNet-20 49.5 46.1 47.7 561KnowNet-15 47.0 43.5 45.2 308XWN 50.1 39.8 44.4 96KnowNet-10 44.0 39.8 41.8 139WN+XWN 45.4 36.8 40.7 101SEMCOR-MFS 42.4 38.4 40.3MCR 40.2 35.5 37.7 149TSSEM 35.1 32.7 33.9 428KnowNet-5 35.5 26.5 30.3 41MCR232.4 29.5 30.9 24,896WN329.3 26.3 27.7 584RANDOM 27.4 27.4 27.4WN225.9 27.4 26.6 72spSemCor 31.4 23.0 26.5 51.0WN426.1 23.9 24.9 2,710WN 36.8 16.1 22.4 13spBNC 24.4 18.1 20.8 290Table 5: P, R and F1 fine-grained results for the re-sources evaluated at SemEval-2007, English Lexi-cal Sample Task.5.2 SemEval-2007 evaluationTable 5 presents ordered by F1 measure, the per-formance in terms of precision (P), recall (R) andF1 measure (F1) of each knowledge resource onSemEval-2007 and its average size of the TS perword-sense10.
Again, the different KnowNet ver-sions appear marked in bold and the baselines ap-pear in italics.As in the previous evaluation, RANDOM ob-tains the poorest result.
The most frequent sensesobtained from SemCor (SEMCOR-MFS) and WN(WN-MFS) are both far below the most frequentsense of the training corpus (TRAIN-MFS), and allof them are below the Topic Signatures acquiredusing the training corpus (TRAIN).Interestingly, on SemEval-2007, all the knowl-edge resources behave differently.
Now, the bestindividual results are obtained by TSWEB, whilein this case TSSEM obtains very modest results.The lowest result is obtained by the knowledge en-coded in spBNC.Regarding the baselines, spBNC, WN (and alsoWN2and WN4) and spSemCor do not surpassRANDOM, and none achieves neither WN-MFS,TRAIN-MFS nor TRAIN.
Now, WN+XWN,XWN, TSWEB and (WN+XWN)2obtain better10The average size is different with respect Senseval-3 be-cause the words selected for this task are differentresults than SEMCOR-MFS but far below the mostfrequent sense of WN (WN-MFS) and the training(TRAIN-MFS).Regarding other expansions and combinations,the performance of WN is improved using wordsat distances up to 2, and up to 3, but it decreasesusing distances up to 4.
Again, none of these WNexpansions achieve the results of XWN.
Finally,(WN+XWN)2performs better than WN+XWNand MCR2slightly better than MCR11.On SemEval-2007, the different versions ofKnowNet consistently obtain better performancesas they incease the window size of processedwords of TSWEB.
As expected, KnowNet-5 ob-tain the lower results.
However, it performs betterthan spBNC, WN (and all its extensions), spSem-Cor and MCR2.
This time, all KnowNet ver-sions perform worse than TSWEB.
However, as inthe previous evaluation, KnowNet-10 outperformsWN+XWN, and this time, also TSSEM and theMCR, with much more relations per sense.
Alsointeresting is that from KnowNet-10, all KnowNetversions perform better than the resources derivedfrom manually sense annotated corpora (spSem-Cor, MCR, TSSEM, etc.
).Regarding the integration of resources,WN+XWN+KN-20 performs better than anyknowledge resource derived by manual or auto-matic means.
Again, it is interesting to note thatWN+XWN+KN-20 have better performance thantheir individual resources, indicating a comple-mentary knowledge.
In fact, WN+XWN+KN-20performs much better than the resources fromwhich it derives (WN, XWN and TSWEB).5.3 DiscussionWhen comparing the ranking of the differentknowledge resources, the different versions ofKnowNet seem to be more robust and stableacross corpora changes.
For instance, in bothevaluation frameworks (Senseval-3 and SemEval-2007), KnowNet-20 ranks 5th and 4th, respec-tively ((WN+XWN)2ranks 8th and 2nd, TSSEMranks 1st and 10th, MCR ranks 4th and 9th,TSWEB ranks 11th and 3rd, etc.).
In fact,WN+XWN+KN-20 ranks 3rd and 1st, respec-tively.11No further distances have been tested1676 Conclusions and future researchIt is our belief, that accurate semantic processing(such as WSD) would rely not only on sophisti-cated algorithms but on knowledge intensive ap-proaches.
The results presented in this paper sug-gests that much more research on acquiring andusing large-scale semantic resources should be ad-dressed.The knowledge acquisition bottleneck problemis particularly acute for open domain (and alsodomain specific) semantic processing.
The ini-tial results obtained for the different versions ofKnowNet seem to be a major step towards the au-tonomous acquisition of knowledge from raw cor-pora, since they are several times larger than theavailable knowledge resources which encode re-lations between synsets, and the knowledge theycontain outperform any other resource when is em-pirically evaluated in a common framework.It remains for future research the evaluation ofthese KnowNet versions in combination with otherlarge-scale semantic resources or in a cross-lingualsetting.AcknowledgmentsWe want to thank Aitor Soroa for his technicalsupport and the anonymous reviewers for theircomments.
This work has been supported byKNOW (TIN2006-15049-C03-01) and KYOTO(ICT-2007-211423).ReferencesAgirre, E. and O. Lopez de Lacalle.
2004.
Publiclyavailable topic signatures for all wordnet nominalsenses.
In Proceedings of LREC, Lisbon, Portugal.Agirre, E. and D. Martinez.
2001.
Learning class-to-class selectional preferences.
In Proceedings ofCoNLL, Toulouse, France.Agirre, E. and D. Martinez.
2002.
Integrating selec-tional preferences in wordnet.
In Proceedings ofGWC, Mysore, India.
?Alvez, J., J. Atserias, J. Carrera, S. Climent, A. Oliver,and G. Rigau.
2008.
Consistent annotation of eu-rowordnet with the top concept ontology.
In Pro-ceedings of Fourth International WordNet Confer-ence (GWC?08).Atserias, J., L. Villarejo, G. Rigau, E. Agirre, J. Car-roll, B. Magnini, and Piek Vossen.
2004.
The mean-ing multilingual central repository.
In Proceedingsof GWC, Brno, Czech Republic.Cuadros, M. and G. Rigau.
2006.
Quality assessmentof large scale knowledge resources.
In Proceedingsof the EMNLP.Cuadros, M. and G. Rigau.
2007.
Semeval-2007task 16: Evaluation of wide coverage knowledge re-sources.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007).Cuadros, M., G. Rigau, and M. Castillo.
2007.
Eval-uating large-scale knowledge resources across lan-guages.
In Proceedings of RANLP.Fellbaum, C., editor.
1998.
WordNet.
An ElectronicLexical Database.
The MIT Press.Leacock, C., M. Chodorow, and G. Miller.
1998.Using Corpus Statistics and WordNet Relations forSense Identification.
Computational Linguistics,24(1):147?166.Lin, C. and E. Hovy.
2000.
The automated acquisi-tion of topic signatures for text summarization.
InProceedings of COLING.
Strasbourg, France.Magnini, B. and G. Cavagli`a.
2000.
Integrating subjectfield codes into wordnet.
In Proceedings of LREC,Athens.
Greece.McCarthy, D. 2001.
Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Aternations, Sub-categorization Frames and Selectional Preferences.Ph.D.
thesis, University of Sussex.Mihalcea, R. and D. Moldovan.
2001. extended word-net: Progress report.
In Proceedings of NAACLWorkshop on WordNet and Other Lexical Resources,Pittsburgh, PA.Navigli, R. and P. Velardi.
2005.
Structural seman-tic interconnections: a knowledge-based approach toword sense disambiguation.
IEEE Transactions onPattern Analysis and Machine Intelligence (PAMI),27(7):1063?1074.Niles, I. and A. Pease.
2001.
Towards a standard up-per ontology.
In Proceedings of the 2nd Interna-tional Conference on Formal Ontology in Informa-tion Systems (FOIS-2001), pages 17?19.
Chris Weltyand Barry Smith, eds.Suchanek, Fabian M., Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A Core of Semantic Knowl-edge.
In 16th international World Wide Web con-ference (WWW 2007), New York, NY, USA.
ACMPress.Vossen, P., editor.
1998.
EuroWordNet: A MultilingualDatabase with Lexical Semantic Networks .
KluwerAcademic Publishers .168
