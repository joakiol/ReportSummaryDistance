Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1930?1935,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning to Answer Questions from Wikipedia InfoboxesAlvaro MoralesCSAIL MITalvarom@mit.eduVarot PremtoonCSAIL MITvarot@mit.eduCordelia AveryCSAIL MITcavery@mit.eduSue FelshinCSAIL MITsfelshin@mit.eduBoris KatzCSAIL MITboris@mit.eduAbstractA natural language interface to answers on theWeb can help us access information more ef-ficiently.
We start with an interesting sourceof information?infoboxes in Wikipedia thatsummarize factoid knowledge?and develop acomprehensive approach to answering ques-tions with high precision.
We first build asystem to access data in infoboxes in a struc-tured manner.
We use our system to constructa crowdsourced dataset of over 15,000 high-quality, diverse questions.
With these ques-tions, we train a convolutional neural networkmodel that outperforms models that achievetop results in similar answer selection tasks.1 IntroductionThe goal of open-domain question answering is toprovide high-precision access to information.
Withmany sources of knowledge on the Web, selectingthe right answer to a user?s question remains chal-lenging.
Wikipedia contains over five million arti-cles in its English version.
Providing a natural lan-guage interface to answers in Wikipedia is an impor-tant step towards more effective information access.Many Wikipedia articles have an infobox, a tablethat summarizes key information in the article in theform of attribute?value pairs like ?Narrated by: FredAstaire?.
This data source is appealing for questionanswering because it covers a broad range of factsthat are inherently relevant: a human editor manuallyhighlighted this information in the infobox.Although many infoboxes appear to be similar,they are only semi-structured?few attributes haveQ Who took over after Nelson Mandela?A Succeeded by: Thabo MbekiQ Who designed Central Park?A Architect: Frederick Law OlmstedQ Where did Oscar Wilde earn his degree?A Alma mater: Trinity College, DublinQ What does Intel do?A Industry: SemiconductorsTable 1: Example questions and answers with little lexicaloverlap from the INFOBOXQA dataset.consistent value types across articles, infobox tem-plates do not mandate which attributes must beincluded, and editors are allowed to add article-specific attributes.
Infobox-like tables are very com-mon on the Web.
Since it is infeasible to incorporateevery such source into structured knowledge baseslike Freebase (Bollacker et al, 2008), we need tech-niques that do not rely on ontology or value typeinformation.We focus on the answer selection problem, wherethe goal is to select the best answer out of a givencandidate set of attribute?value pairs from infoboxescorresponding to a named entity in the question.
Ta-ble 1 illustrates how questions from users may havelittle lexical overlap with the correct attribute?valuepair.
Answer selection is an important subtask inbuilding an end-to-end question answering system.Our work has two main contributions: (1) Wecompiled the INFOBOXQA dataset, a crowdsourcedcorpus of over 15,000 questions with answers from1930infoboxes in 150 articles in Wikipedia.
Unlike ex-isting answer selection datasets with answers fromknowledge bases or long-form text, INFOBOXQAtargets tabular data that is not augmented with valuetypes or linked to an ontology.
(2) We built a multi-channel convolutional neural network (CNN) modelthat achieves the best results on INFOBOXQA com-pared to other neural network models in the answerselection task.2 The INFOBOXQA datasetInfoboxes are designed to be human-readable, notmachine-interpretable.
This allowed us to devise acrowdsourced assignment where we ask participantsto generate questions from infoboxes.
With little tono training, humans can form coherent questions outof terse, potentially ambiguous attribute?value pairs.Wikipedia does not provide a way to access spe-cific information segments; its API returns the entirearticle.
We first worked on the data access problemand developed a system called WikipediaBase to ro-bustly extract attribute?value pairs from infoboxes.Inspired by Omnibase (Katz et al, 2002), we or-ganize infoboxes in an object?attribute?value datamodel, where an object (Lake Titicaca) has an at-tribute (?Surface area?)
with a value (8,372 km2).Attributes are grouped by infobox class (for in-stance, the film class contains attributes like ?Di-rected by?
and ?Cinematography?).
The data modelallowed us to extend WikipediaBase to informationoutside of infoboxes.
We implemented methods foraccessing images, categories, and article sections.We then created a question-writing assignmentwhere participants see infoboxes constructed usingdata from WikipediaBase.
These infoboxes visuallyresembled the original ones in Wikipedia but weredesigned to control for variables.
To prevent parti-cipants from only generating questions for attributesat the top of the table, the order of attributes wasrandomly shuffled.
To ensure that the task couldbe completed in a reasonable amount of time, in-foboxes were partitioned into assignments with upto ten attributes.
A major goal of this data collec-tion was to gather question paraphrases.
For eachattribute, we asked participants to write two ques-tions.
It is likely that at least one of the questionswill use words from the attribute, but requiring anTotal questions 15266Total attributes 762Average questions per attribute 20.0Average answers per question 17.8Table 2: Statistics of the INFOBOXQA dataset.additional question encouraged them to think of al-ternative phrasings.Every infobox in the experiment included a pic-ture to help disambiguate the article.
For instance,the cover image for ?Grand Theft Auto III?
(in con-cert with the values in the infobox) makes it reason-ably clear that the assignment is about a video gameand not a type of crime.
We asked participants toinclude an explicit referent to the article title in eachquestion (e.g., ?Where was Albert Einstein born?
?instead of ?Where was he born??
).We analyzed the occurrences of infobox attributesin Wikipedia and found that they fit a rapidly-decaying exponential distribution with a long tail ofattributes that occur in few articles.
This distributionmeans that with a carefully chosen subset of articleswe can achieve a large coverage of frequently ap-pearing attributes.
We developed a greedy approx-imation algorithm that selects a subset of infoboxclasses, picks a random sample of articles in theclass, and chooses three representative articles thatcontain the largest quantity of attributes.
150 articlesfrom 50 classes were selected, covering roughly halfof common attributes found in Wikipedia.The dataset contains example questions qi, withan attribute?value pair (ai, vi) that answers the ques-tion.
To generate negative examples for the an-swer selection task, we picked every other tuple(aj , vj); 8j 6= i from the infobox that containsthe correct answer.
If we know that a questionasks about a specific entity, we must consider ev-ery attribute in the entity?s infobox as a possible an-swer.
In INFOBOXQA, candidate answers are justattribute?value pairs with no type information.
Be-cause of this, every attribute in the infobox is indis-tinguishable a priori, and is thus in the candidate set.Not having type information makes the task harderbut also more realistic.
Table 2 shows statistics ofINFOBOXQA.
The dataset is available online.11http://groups.csail.mit.edu/infolab/infoboxqa/19313 Model descriptionDeep learning models for answer selection assumethat there is a high similarity between question andanswer representations (Yu et al, 2014).
Instead ofcomparing them directly, the main intuition in ourmodel is to use the attribute as an explicit bridgeto facilitate the match between question and an-swer.
Consider the question ?Who replaced DwightD.
Eisenhower?
?, with answer ?Succeeded by: JohnF.
Kennedy?.
Clearly, the attribute ?Succeeded by?plays a crucial role in indicating the match betweenthe question and the answer.
If the question and at-tribute have certain semantic similarities, and thosesimilarities match the similarities of the answer andthe attribute, then the answer must be a good matchfor the question.We propose an architecture with three weight-sharing CNNs, each one processing either the ques-tion, the attribute, or the answer.
We then use anelement-wise product merge layer to compute simi-larities between the question and attribute, and be-tween the attribute and answer.
We refer to thismodel as Tri-CNN.
Tri-CNN has five types of layers:an input layer, a convolution layer, a max-poolinglayer, a merge layer, and a final multi-layer percep-tron (MLP) scoring layer that solves the answer se-lection task.
We now describe each layer.Input layer.
Let sq be a matrix 2 R|sq |?d, whererow i is a d-dimensional word embedding of the i-thword in the question.
Similarly, let sattr and sansbe word embedding matrices for the attribute andanswer, respectively.
sq, sattr, and sans are zero-padded to have the same length.
We use pre-trainedGloVe2 embeddings with d = 300 (Pennington etal., 2014), which we keep adaptable during training.Convolution layer.
We use the multi-channelCNN architecture of (Kim, 2014) with three weight-sharing CNNs, one each for sq, sattr, and sans.
Dif-ferent lengths of token substrings (e.g., unigrams orbigrams) are used as channels.
The CNNs shareweights among the three inputs in a Siamese archi-tecture (Bromley et al, 1993).
Weight-sharing al-lows the model to compute the representation of oneinput influenced by the other inputs; i.e., the repre-sentation of the question is influenced by the repre-sentations of the attribute and answer.2http://nlp.stanford.edu/projects/glove/Figure 1: A schematic of the Tri-CNN model.We describe the convolution layer with respect tothe input s, which can stand for sq, sattr, or sans.For each channel h 2 [1...M ], a filter w 2 Rh?dis applied to a sliding window of h rows of s toproduce a feature map C. Formally, C is a matrixwhere:C[i, :] = tanh(w ?
s[i...i + h  1, :] + b) (1)and b 2 Rd is the bias.
We use wide convolutionto ensure that terminal and non-terminal words areconsidered equally when applying the filterw (Blun-som et al, 2014).Max-pooling layer.
Pooling is used to extractmeaningful features from the output of convolution(Yin et al, 2015).
We apply a max-pooling layer tothe output of each channel h. The result is a vectorch 2 Rd wherech[i] = max{C[:, i]} (2)Max-pooling is applied to all M channels.
The re-sulting vectors ch for h 2 [1...M ] are concatenatedinto a single vector c.Merge layer.
Our goal is to model the semanticsimilarities between the question and the attribute,and between the answer and the attribute.
We com-pute the element-wise product of the feature vectors1932generated by convolution and max-pooling as fol-lows:dq,attr = cq   cattr (3)dans,attr = cans   cattr (4)where   is the element-wise product operator, suchthat dij is a vector.
Each element in dij encodes asimilarity in a single semantic aspect between twofeature representations.MLP scoring layer.
We wish to compute a real-valued similarity between the distance vectors fromthe merge layer.
Instead of directly computing thisusing, e.g., cosine similarity, we follow (Baudis?
andS?edivy`, 2016) and first project the two distance vec-tors into a shared embedding space.
We computeelement-wise sums and products of the embeddings,which are then input to a two-layer perceptron.4 ExperimentsWe implemented Tri-CNN in the dataset-sts3framework for semantic text similarity, built on topof the Keras deep learning library (Chollet, 2015).The framework aims to unify various sentencematching tasks, including answer selection, andprovides implementations for variants of sentence-matching models that achieve state-of-the-art resultson the TREC answer selection dataset (Wang et al,2007).
We evaluated the performance of variousmodels in dataset-sts against INFOBOXQA forthe task of answer selection.
We report the averageand the standard deviation for mean average preci-sion (MAP) and mean reciprocal rank (MRR) fromfive-fold cross validation.
We used 10% of the train-ing set for validation.In answer selection, a model learns a function toscore candidate answers; the set of candidate an-swers is already given.
Entity linking is needed togenerate candidate answers and is often treated as aseparate module.
For INFOBOXQA, we asked hu-mans to generate questions from pre-specified in-foboxes.
Given this setup, we already know whichentity the question refers to; we also know that thequestion is answerable by the infobox.
Entity link-ing was therefore out of scope in our experiments.By effectively asking humans to identify the namedentity, our evaluation results are not affected bynoise caused by a faulty entity linking strategy.3https://github.com/brmson/dataset-stsModel MAP MRRAvg SD Avg SDTF-IDF 0.503 0.004 0.501 0.065BM-25 0.531 0.007 0.532 0.056AVG 0.593 0.021 0.609 0.042RNN 0.685 0.024 0.674 0.028ATTN1511 0.772 0.016 0.771 0.014CNN 0.757 0.015 0.754 0.024Tri-CNN 0.806 0.014 0.781 0.025Table 3: Results of five-fold cross validation.
Our Tri-CNNmodel achieves the best results in MAP and MRR.4.1 BenchmarksWe compare against TF-IDF and BM25 (Robertsonet al, 1995), two models from the information re-trieval literature that calculate weighted measures ofword co-occurrence between the question and an-swer.
We also experiment with various neural net-work sentence matching models.
AVG is a baselinemodel that computes averages of unigram word em-beddings.
CNN is the model most similar to Tri-CNN, with two CNNs in a Siamese architecture,one for the question and one for the answer.
Max-pooling is computed on the output of convolution,and then fed to the output layer directly.
RNN com-putes summary embeddings of the question and an-swer using bidirectional GRUs (Cho et al, 2014).ATTN1511 feeds the outputs of the bi-GRU into theconvolution layer.
It implements an asymmetric at-tention mechanism as in (Tan et al, 2015), where theoutput of convolution and max-pooling of the ques-tion is used to re-weight the input to convolution ofthe answer.
The convolution weights are not shared.For these neural architectures, we use the same MLPscoring layer used in Tri-CNN as the output layerand train using bipartite RankNet loss (Burges et al,2005).4.2 ResultsTable 3 summarizes the results of experiments onINFOBOXQA.
The performance of the baselinesindicates that unigram bag-of-words models arenot sufficiently expressive for matching; Tri-CNNmakes use of larger semantic units through its mul-tiple channels.
The attention mechanism and thecombination of an RNN and CNN in ATTN1511achieves better results than RNN, but still performs1933slightly worse than the CNN model with weight-sharing.
The Siamese architecture allows an input?srepresentation to be influenced by the other inputs.The convolution feature maps are thus encoded ina comparable scheme that is more amenable to amatching task.
Our Tri-CNN model built on topof this weight-sharing architecture achieves the bestperformance.
Tri-CNN computes the match by com-paring the similarities between question?attributeand answer?attribute, which leads to improved re-sults over models that compare the question and an-swer directly.5 Related workDeep learning approaches to answer selection havebeen successful on the standard TREC dataset andthe more recent WIKIQA corpus (Yang et al, 2015).Models like (Feng et al, 2015) and (Wang and Ny-berg, 2015) generate feature representations of ques-tions and answers using neural networks, comput-ing the similarity of these representations to selectan answer.
Recently, attention mechanisms to influ-ence the calculation of the representation (Tan et al,2015) or to re-weight feature maps before matching(Santos et al, 2016) have achieved good results.
Ourwork differs from past approaches in that we use theattribute as an additional input to the matching task.Other approaches to question answering over struc-tured knowledge bases focus on mapping questionsinto executable database queries (Berant et al, 2013)or traversing embedded sub-graphs in vector space(Bordes et al, 2014).6 ConclusionWe presented an approach to answering questionsfrom infoboxes in Wikipedia.
We first compiledthe INFOBOXQA dataset, a large and varied corpusof interesting questions from infoboxes.
We thentrained a convolutional neural network model on thisdataset that uses the infobox attribute as a bridge inmatching the question to the answer.
Our Tri-CNNmodel achieved the best results when compared torecent CNN and RNN-based architectures.
We planto test our model?s ability to generalize to other typesof infobox-like tables on the Web.
We expect ourmethods to achieve good results for sources such asproduct descriptions on shopping websites.AcknowledgmentsWe thank Andrei Barbu and Yevgeni Berzak forhelpful discussions and insightful comments on thispaper.
We also thank Ayesha Bose, Michael Sil-ver, and the anonymous reviewers for their valu-able feedback.
Federico Mora, Kevin Ellis, ChrisPerivolaropoulos, Cheuk Hang Lee, Michael Silver,and Mengjiao Yang also made contributions to theearly iterations and current version of Wikipedia-Base.
The work described in this paper hasbeen supported in part by AFRL contract No.FA8750-15-C-0010 and in part through funding pro-vided by the Center for Brains, Minds, and Ma-chines (CBMM), funded by NSF STC award CCF-1231216.ReferencesPetr Baudis?
and Jan S?edivy`.
2016.
Sentence pair scor-ing: Towards unified framework for text comprehen-sion.
arXiv preprint arXiv:1603.06127.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing.Phil Blunsom, Edward Grefenstette, and Nal Kalchbren-ner.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: A col-laboratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Management ofData, pages 1247?1250.
ACM.Antoine Bordes, Sumit Chopra, and Jason Weston.
2014.Question answering with subgraph embeddings.
arXivpreprint arXiv:1406.3676.Jane Bromley, James W Bentz, Le?on Bottou, IsabelleGuyon, Yann LeCun, Cliff Moore, Eduard Sa?ckinger,and Roopak Shah.
1993.
Signature verification usinga ?Siamese?
time delay neural network.
InternationalJournal of Pattern Recognition and Artificial Intelli-gence, 7(04):669?688.Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hullender.2005.
Learning to rank using gradient descent.
InProceedings of the 22nd International Conference onMachine Learning, pages 89?96.
ACM.1934Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014.
On the proper-ties of neural machine translation: encoder-decoder ap-proaches.
In Eighth Workshop on Syntax, Semanticsand Structure in Statistical Translation (SSST-8).Franc?ois Chollet.
2015.
Keras.
https://github.com/fchollet/keras.Minwei Feng, Bing Xiang, Michael R Glass, LidanWang, and Bowen Zhou.
2015.
Applying deep learn-ing to answer selection: A study and an open task.arXiv preprint arXiv:1508.01585.Boris Katz, Sue Felshin, Deniz Yuret, Ali Ibrahim,Jimmy Lin, Gregory Marton, Alton Jerome McFar-land, and Baris Temelkuran.
2002.
Omnibase: Uni-form access to heterogeneous data for question an-swering.
In Natural Language Processing and Infor-mation Systems, pages 230?234.
Springer.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing, 13.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
GloVe: Global vectors for word rep-resentation.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing,pages 1532?1543.Stephen E Robertson, Steve Walker, Susan Jones, Miche-line M Hancock-Beaulieu, and Mike Gatford.
1995.Okapi at TREC-3.
NIST Special Publication SP,109:109.Cicero dos Santos, Ming Tan, Bing Xiang, and BowenZhou.
2016.
Attentive pooling networks.
arXivpreprint arXiv:1602.03609.Ming Tan, Bing Xiang, and Bowen Zhou.
2015.
LSTM-based deep learning models for non-factoid answer se-lection.
arXiv preprint arXiv:1511.04108.Di Wang and Eric Nyberg.
2015.
A long short-termmemory model for answer sentence selection in ques-tion answering.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics.Mengqiu Wang, Noah A Smith, and Teruko Mitamura.2007.
What is the Jeopardy model?
A quasi-synchronous grammar for QA.
In Proceedings ofEMNLP-CoNLL, volume 7, pages 22?32.Yi Yang, Wen-tau Yih, and Christopher Meek.
2015.WikiQA: A challenge dataset for open-domain ques-tion answering.
In Proceedings of the 2015 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 2013?2018.
Citeseer.Wenpeng Yin, Hinrich Schu?tze, Bing Xiang, and BowenZhou.
2015.
ABCNN: Attention-based convolutionalneural network for modeling sentence pairs.
arXivpreprint arXiv:1512.05193.Lei Yu, Karl Moritz Hermann, Phil Blunsom, andStephen Pulman.
2014.
Deep Learning for AnswerSentence Selection.
In NIPS Deep Learning Work-shop, December.1935
